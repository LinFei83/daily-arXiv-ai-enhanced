{"id": "2507.13511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13511", "abs": "https://arxiv.org/abs/2507.13511", "authors": ["Nabil Abdelaziz Ferhat Taleb", "Abdolazim Rezaei", "Raj Atulkumar Patel", "Mehdi Sookhak"], "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination", "comment": null, "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.", "AI": {"tldr": "GraphTrafficGPT提出了一种图基架构，通过将任务及其依赖表示为图，优化了大型语言模型（LLM）驱动的智能交通管理系统，显著降低了令牌消耗和响应延迟，并支持并发多查询处理。", "motivation": "当前的链式LLM交通管理系统（如TrafficGPT）存在任务顺序执行、高令牌消耗和可扩展性差的问题，导致其在复杂现实场景中效率低下。", "method": "本文提出了GraphTrafficGPT，一种基于图的新型架构。它将任务及其依赖表示为有向图中的节点和边，以实现高效的并行执行和动态资源分配。核心思想是一个“大脑代理”（Brain Agent），负责分解用户查询、构建优化的依赖图，并协调专门的代理网络（用于数据检索、分析、可视化和模拟）。该架构还引入了先进的上下文感知令牌管理，并支持并发多查询处理。", "result": "实验结果表明，与TrafficGPT相比，GraphTrafficGPT将令牌消耗降低了50.2%，平均响应延迟降低了19.0%。同时，它支持并发多查询执行，效率提高了23.0%。", "conclusion": "GraphTrafficGPT通过创新的图基架构和大脑代理协调机制，有效解决了现有LLM驱动交通管理系统的效率、令牌消耗和并发处理能力不足的问题，为复杂的城市交通环境提供了更优的解决方案。"}}
{"id": "2507.13541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13541", "abs": "https://arxiv.org/abs/2507.13541", "authors": ["Shuyue Stella Li", "Melanie Sclar", "Hunter Lang", "Ansong Ni", "Jacqueline He", "Puxin Xu", "Andrew Cohen", "Chan Young Park", "Yulia Tsvetkov", "Asli Celikyilmaz"], "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes", "comment": "17 pages, 6 tables, 5 figures", "summary": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications.", "AI": {"tldr": "PrefPalette是一个框架，它将偏好分解为属性维度，并根据不同的社交社区价值观定制偏好预测，以实现人类可解释性，在预测准确性上显著优于GPT-4o。", "motivation": "当前的偏好模型将人类判断视为黑箱，无法理解偏好背后的原因，且通常采用聚合偏好建模，未能捕捉人类判断的多样性评估框架。", "method": "PrefPalette框架通过两种方式实现多属性决策：1) 可扩展的反事实属性合成步骤，生成合成训练数据以隔离个体属性效应（如正式性、幽默、文化价值观）；2) 基于注意力的偏好建模，学习不同社交社区如何动态地加权这些属性。", "result": "在Reddit的45个社交社区上评估，PrefPalette的平均预测准确率比GPT-4o高出46.6%。此外，它揭示了直观的、社区特定的偏好特征：学术社区优先考虑冗长和刺激性，冲突导向的社区重视讽刺和直接，而支持型社区强调同理心。", "conclusion": "PrefPalette通过建模人类判断的属性介导结构，不仅提供了卓越的偏好建模，还带来了透明、可解释的洞察，是迈向更值得信赖、更注重价值观的个性化应用的第一步。"}}
{"id": "2507.13550", "categories": ["cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.13550", "abs": "https://arxiv.org/abs/2507.13550", "authors": ["Eduardo C. Garrido-Merchán", "Cristina Puente"], "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.", "AI": {"tldr": "本文提出一种利用大型语言模型（LLMs）在受控和透明方式下构建专家系统的新方法，通过生成可由人类专家验证和纠正的Prolog符号知识表示，从而解决LLM幻觉问题并提升系统可靠性。", "motivation": "大型语言模型（LLMs）在知识密集型系统中表现出色，但存在幻觉或自信生成不正确/不可验证事实的缺点。研究旨在开发一种结合LLM能力同时克服其缺陷的专家系统方法。", "method": "该方法通过限制领域并采用结构化的基于提示的提取方法，将LLM生成的知识转换为Prolog形式的符号表示。这种表示可以由人类专家验证和纠正，并保证了可解释性、可扩展性和可靠性。实验使用了Claude Sonnet 3.7和GPT-4.1进行定量和定性分析。", "result": "通过实验证明，所生成的知识库在事实遵循性和语义连贯性方面表现出色。该混合解决方案成功结合了LLM的召回能力和符号系统的精确性，展现了在敏感领域构建可靠AI应用的基础。", "conclusion": "本文提出了一种透明的混合解决方案，将大型语言模型的召回能力与符号系统的精确性相结合，为在敏感领域开发可靠的人工智能应用奠定了基础，解决了LLM生成不可靠信息的问题。"}}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "AI": {"tldr": "当前AI主要关注像素、文字和音素，但世界和企业最有价值的数据本质上是实体、属性和关系构成的关系型数据。本文探讨了关系学习（处理这类数据）未能普及的原因，并提出了使其获得应有地位的对策。", "motivation": "现有AI系统多关注像素、文字和音素等感知或描述层面的数据，而忽略了世界由实体、属性及其关系构成的本质。此外，企业最有价值的数据通常以关系型格式（如电子表格、数据库）存在，而非文本或图像，且传统机器学习方法难以有效处理这类数据中包含的非数值标识符。", "method": "本文通过分析和论证，解释了关系学习（包括关系学习、统计关系AI等领域）当前未能普及的原因（除了少数受限情况），并提出了提升其应有地位所需的具体措施。", "result": "关系学习目前尚未在AI领域占据主导地位，仅在少数关系受限的案例中有所应用。本文将阐明造成这一现状的具体原因。", "conclusion": "关系学习应在AI领域获得更重要的地位。本文将指出实现这一目标所需的关键步骤和改进方向。"}}
{"id": "2507.13534", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13534", "abs": "https://arxiv.org/abs/2507.13534", "authors": ["Leo Semmelmann", "Frederik vom Scheidt"], "title": "Heatwave-driven air conditioning adoption could increase German electricity demand by 14 GW in the near future", "comment": "10 pages, 6 figures", "summary": "Intensifying heatwaves driven by climate change are accelerating the adoption\nof mobile air conditioning (AC) systems. A rapid mass adoption of such AC\nsystems could create additional stress on electricity grids and the power\nsystem. This study presents a novel method to estimate the electricity demand\nfrom AC systems both at system level and at high temporal and spatial\ngranularity. We apply the method to a near-future heatwave scenario in Germany\nin which household AC adoption increases from current 19% to 35% during a\nheatwave similar to the one of July 2025. We analyze the effects for 196,428\ngrid cells of one square kilometer across Germany, by combining weather data,\ncensus data, socio-demographic assumptions, mobility patterns, and\ntemperature-dependent AC activation functions. We find that electricity demand\nof newly purchased mobile AC systems could increase the peak load by over 14 GW\n(23%), with urban hot-spots reaching 5.8 MW per square kilometer. The temporal\npattern creates a pronounced afternoon peak that coincides with lower\nphotovoltaic generation, potentially exacerbating power system stability\nchallenges. Our findings underscore the urgency for proactive energy system\nplanning to manage emerging demand peaks.", "AI": {"tldr": "研究表明，气候变化导致的移动空调普及将使德国电网峰值负荷增加超过14 GW（23%），特别是在下午，这需要紧急的能源系统规划。", "motivation": "气候变化加剧的热浪正在加速移动空调的普及，这可能给电力系统带来额外压力。本研究旨在估算这种普及对电力需求的影响。", "method": "本研究提出了一种估算空调系统电力需求的新方法，该方法在系统层面和高时空粒度上进行。具体应用于德国2025年夏季热浪情景，假设家庭空调普及率从19%增至35%。方法结合了天气数据、人口普查数据、社会人口学假设、出行模式和温度依赖的空调激活函数，分析了德国196,428个一平方公里网格单元的影响。", "result": "新购买的移动空调系统可能使峰值负荷增加超过14 GW（23%），城市热点区域每平方公里可达5.8 MW。这种需求的时间模式会在下午形成显著峰值，与光伏发电量较低的时段重合，可能加剧电力系统稳定性挑战。", "conclusion": "研究结果强调了积极进行能源系统规划以管理新出现的用电高峰的紧迫性。"}}
{"id": "2507.13384", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13384", "abs": "https://arxiv.org/abs/2507.13384", "authors": ["Osama Hardan", "Omar Elshenhabi", "Tamer Khattab", "Mohamed Mabrok"], "title": "Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation", "comment": "Submitted to the 2025 IEEE International Conference on Future Machine\n  Learning and Data Science (FMLDS)", "summary": "Vision Mamba models promise transformer-level performance at linear\ncomputational cost, but their reliance on serializing 2D images into 1D\nsequences introduces a critical, yet overlooked, design choice: the patch scan\norder. In medical imaging, where modalities like brain MRI contain strong\nanatomical priors, this choice is non-trivial. This paper presents the first\nsystematic study of how scan order impacts MRI segmentation. We introduce\nMulti-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures\nthat facilitates exploring diverse scan paths without additional computational\ncost. We conduct a large-scale benchmark of 21 scan strategies on three public\ndatasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our\nanalysis shows conclusively that scan order is a statistically significant\nfactor (Friedman test: $\\chi^{2}_{20}=43.9, p=0.0016$), with performance\nvarying by as much as 27 Dice points. Spatially contiguous paths -- simple\nhorizontal and vertical rasters -- consistently outperform disjointed diagonal\nscans. We conclude that scan order is a powerful, cost-free hyperparameter, and\nprovide an evidence-based shortlist of optimal paths to maximize the\nperformance of Mamba models in medical imaging.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2507.13455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13455", "abs": "https://arxiv.org/abs/2507.13455", "authors": ["Dean Chen", "Armin Pomeroy", "Brandon T. Peterson", "Will Flanagan", "He Kai Lim", "Alexandra Stavrakis", "Nelson F. SooHoo", "Jonathan B. Hopkins", "Tyler R. Clites"], "title": "Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms", "comment": "42 pages, 17 figures. Under review at ASME Journal of Mechanical\n  Design", "summary": "Compliant mechanisms have significant potential in precision applications due\nto their ability to guide motion without contact. However, an inherent\nvulnerability to fatigue and mechanical failure has hindered the translation of\ncompliant mechanisms to real-world applications. This is particularly\nchallenging in service environments where loading is complex and uncertain, and\nthe cost of failure is high. In such cases, mechanical hard stops are critical\nto prevent yielding and buckling. Conventional hard-stop designs, which rely on\nstacking single-DOF limits, must be overly restrictive in multi-DOF space to\nguarantee safety in the presence of unknown loads. In this study, we present a\nsystematic design synthesis method to guarantee overload protection in\ncompliant mechanisms by integrating coupled multi-DOF motion limits within a\nsingle pair of compact hard-stop surfaces. Specifically, we introduce a\ntheoretical and practical framework for optimizing the contact surface geometry\nto maximize the mechanisms multi-DOF working space while still ensuring that\nthe mechanism remains within its elastic regime. We apply this synthesis method\nto a case study of a caged-hinge mechanism for orthopaedic implants, and\nprovide numerical and experimental validation that the derived design offers\nreliable protection against fatigue, yielding, and buckling. This work\nestablishes a foundation for precision hard-stop design in compliant systems\noperating under uncertain loads, which is a crucial step toward enabling the\napplication of compliant mechanisms in real-world systems.", "AI": {"tldr": "本研究提出了一种系统性的设计合成方法，通过集成耦合的多自由度运动限制到单个紧凑的硬限位表面中，为柔顺机构提供过载保护，从而最大限度地扩大工作空间并确保弹性工作。", "motivation": "柔顺机构在精密应用中潜力巨大，但其固有的疲劳和机械故障脆弱性阻碍了实际应用，尤其是在负载复杂和不确定的高成本故障服务环境中。传统的硬限位设计过于限制多自由度空间，无法在未知负载下保证安全。", "method": "开发了一种系统性的设计合成方法，通过优化接触表面几何形状，将耦合的多自由度运动限制集成到单个紧凑的硬限位表面中。该方法旨在最大化机构的多自由度工作空间，同时确保机构保持在弹性范围内。", "result": "将该合成方法应用于骨科植入物中的笼式铰链机构案例研究。数值和实验验证表明，所导出的设计能够可靠地防止疲劳、屈服和屈曲。", "conclusion": "这项工作为在不确定负载下运行的柔顺系统中的精密硬限位设计奠定了基础，是使柔顺机构在实际系统中应用的关键一步。"}}
{"id": "2507.13359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13359", "abs": "https://arxiv.org/abs/2507.13359", "authors": ["Yang Zhou", "Junjie Li", "CongYang Ou", "Dawei Yan", "Haokui Zhang", "Xizhe Xue"], "title": "Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives", "comment": "27 pages, 5 figures", "summary": "Due to its extensive applications, aerial image object detection has long\nbeen a hot topic in computer vision. In recent years, advancements in Unmanned\nAerial Vehicles (UAV) technology have further propelled this field to new\nheights, giving rise to a broader range of application requirements. However,\ntraditional UAV aerial object detection methods primarily focus on detecting\npredefined categories, which significantly limits their applicability. The\nadvent of cross-modal text-image alignment (e.g., CLIP) has overcome this\nlimitation, enabling open-vocabulary object detection (OVOD), which can\nidentify previously unseen objects through natural language descriptions. This\nbreakthrough significantly enhances the intelligence and autonomy of UAVs in\naerial scene understanding. This paper presents a comprehensive survey of OVOD\nin the context of UAV aerial scenes. We begin by aligning the core principles\nof OVOD with the unique characteristics of UAV vision, setting the stage for a\nspecialized discussion. Building on this foundation, we construct a systematic\ntaxonomy that categorizes existing OVOD methods for aerial imagery and provides\na comprehensive overview of the relevant datasets. This structured review\nenables us to critically dissect the key challenges and open problems at the\nintersection of these fields. Finally, based on this analysis, we outline\npromising future research directions and application prospects. This survey\naims to provide a clear road map and a valuable reference for both newcomers\nand seasoned researchers, fostering innovation in this rapidly evolving domain.\nWe keep tracing related works at\nhttps://github.com/zhouyang2002/OVOD-in-UVA-imagery", "AI": {"tldr": "本文对无人机（UAV）航空场景下的开放词汇目标检测（OVOD）进行了全面的综述。", "motivation": "传统的无人机航空目标检测方法仅限于预定义类别，限制了其适用性。跨模态文本-图像对齐（如CLIP）的出现使得开放词汇目标检测成为可能，能够通过自然语言描述识别未见过的物体，从而显著增强无人机在航空场景理解中的智能性和自主性。", "method": "本文首先将开放词汇目标检测的核心原理与无人机视觉的独特特性相结合，然后构建了一个系统的分类法，对现有的航空图像开放词汇目标检测方法进行分类，并提供了相关数据集的全面概述。在此基础上，批判性地剖析了这些领域交叉的关键挑战和开放问题。", "result": "本文提供了一个关于无人机航空场景中开放词汇目标检测的系统性综述，包括其原理与无人机视觉的结合、现有方法的分类、相关数据集的概述以及关键挑战和开放问题的深入分析。", "conclusion": "基于分析，本文展望了未来有前景的研究方向和应用前景，旨在为该领域的初学者和资深研究人员提供清晰的路线图和有价值的参考。"}}
{"id": "2507.13357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13357", "abs": "https://arxiv.org/abs/2507.13357", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs.", "AI": {"tldr": "本研究探索了利用少量样本自适应语言提示（ALP）结合多模态大型语言模型（如GPT-4o和Gemini 1.5 Pro）来检测网络钓鱼网页，并显著提高了检测准确性。", "motivation": "网络钓鱼攻击是重大的网络安全威胁，需要适应性强的检测技术。", "method": "提出了一种名为自适应语言提示（ALP）的少量样本结构化语义推理方法。该方法引导LLM分析文本欺骗，通过分解语言模式、检测紧急提示和识别网络钓鱼内容中常见的操纵性措辞。通过整合文本、视觉和URL分析，提出了一个统一的多模态模型。", "result": "实验证明，ALP通过结构化推理和上下文分析显著提高了网络钓鱼检测的准确性，F1分数达到0.93，超越了传统方法。", "conclusion": "ALP集成的多模态LLM为更强大、可解释和自适应的基于语言的网络钓鱼检测系统奠定了基础。"}}
{"id": "2507.13625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13625", "abs": "https://arxiv.org/abs/2507.13625", "authors": ["Yuxin Zhang", "Xi Wang", "Mo Hu", "Zhenyu Zhang"], "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety", "comment": "19 pages, 13 figures", "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.", "AI": {"tldr": "BifrostRAG是一种双图RAG系统，通过结合图遍历和向量搜索，有效处理复杂法规文本中的多跳查询，显著优于现有RAG基线。", "motivation": "自动化合规性检查需要从复杂的法规文本中检索信息和回答问题，但法规文本的语言和结构复杂性以及多跳查询需求对传统RAG系统构成了挑战。", "method": "引入BifrostRAG系统，它集成双图模型：实体网络图（建模语言关系）和文档导航图（建模文档结构）。该系统采用混合检索机制，结合图遍历和基于向量的语义搜索，使大型语言模型能够同时理解文本的意义和结构。", "result": "在多跳问题数据集上，BifrostRAG实现了92.8%的准确率、85.5%的召回率和87.3%的F1分数。这些结果显著优于仅基于向量或仅基于图的RAG基线。", "conclusion": "BifrostRAG被确立为LLM驱动合规性检查的强大知识引擎。其双图混合检索机制为处理知识密集型工程领域中的复杂技术文档提供了可迁移的蓝图。"}}
{"id": "2507.13623", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13623", "abs": "https://arxiv.org/abs/2507.13623", "authors": ["Rahul Gulia"], "title": "MD-OFDM: An Energy-Efficient and Low-PAPR MIMO-OFDM Variant for Resource-Constrained Applications", "comment": null, "summary": "Orthogonal Frequency Division Multiplexing (OFDM) combined with\nMultiple-Input Multiple-Output (MIMO) techniques forms the backbone of modern\nwireless communication systems. While offering high spectral efficiency and\nrobustness, conventional MIMO-OFDM, especially with complex equalizers like\nMinimum Mean Square Error (MMSE), suffers from high Peak-to-Average Power Ratio\n(PAPR) and significant power consumption due to multiple active Radio Frequency\n(RF) chains. This paper proposes and mathematically models an alternative\nsystem, termed Multi-Dimensional OFDM (MD-OFDM), which employs a per-subcarrier\ntransmit antenna selection strategy. By activating only one transmit antenna\nfor each subcarrier, MD-OFDM aims to reduce PAPR, lower power consumption, and\nimprove Bit Error Rate (BER) performance. We provide detailed mathematical\nformulations for BER, Energy Efficiency (EE), and PAPR, and discuss the\nsuitability of MD-OFDM for various applications, particularly in\nenergy-constrained and cost-sensitive scenarios such as the Internet of Things\n(IoT) and Low-Power Wide Area Networks (LPWAN). Simulation results demonstrate\nthat MD-OFDM achieves superior BER and significantly lower PAPR compared to\nMMSE MIMO, albeit with a trade-off in peak overall energy efficiency due to\nreduced spectral multiplexing.", "AI": {"tldr": "本文提出多维OFDM (MD-OFDM) 系统，通过逐子载波发射天线选择，旨在降低传统MIMO-OFDM的高峰均功率比(PAPR)和功耗，并改善误码率(BER)性能。", "motivation": "传统的MIMO-OFDM系统，特别是结合MMSE均衡器时，存在高峰均功率比(PAPR)和由于多路射频链导致的显著功耗问题。", "method": "提出多维OFDM (MD-OFDM) 系统，其核心策略是为每个子载波仅激活一个发射天线。论文提供了BER、能量效率(EE)和PAPR的详细数学公式和模型。", "result": "仿真结果表明，MD-OFDM与MMSE MIMO相比，实现了更优的BER和显著更低的PAPR。尽管由于频谱复用减少，其峰值整体能量效率有所权衡，但MD-OFDM特别适用于物联网(IoT)和低功耗广域网(LPWAN)等能量受限和成本敏感的场景。", "conclusion": "MD-OFDM是一种可行的替代方案，通过逐子载波天线选择有效降低PAPR和功耗，并提升BER性能，使其在对能耗和成本敏感的应用中具有优势，尽管可能牺牲部分频谱复用带来的峰值能量效率。"}}
{"id": "2507.13394", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13394", "abs": "https://arxiv.org/abs/2507.13394", "authors": ["Akhil John Thomas", "Christiaan Boerkamp"], "title": "Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning", "comment": null, "summary": "Nerve segmentation is crucial in medical imaging for precise identification\nof nerve structures. This study presents an optimized DeepLabV3-based\nsegmentation pipeline that incorporates automated threshold fine-tuning to\nimprove segmentation accuracy. By refining preprocessing steps and implementing\nparameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a\nPixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate\nsignificant improvements over baseline models and highlight the importance of\ntailored parameter selection in automated nerve detection.", "AI": {"tldr": "本研究提出了一种优化的基于DeepLabV3的神经分割流程，通过自动化阈值微调和参数优化显著提高了超声神经图像的分割精度。", "motivation": "神经分割在医学成像中对于精确识别神经结构至关重要。", "method": "该研究采用了一种优化的DeepLabV3分割流程，结合了自动化阈值微调，并通过改进预处理步骤和实施参数优化来实现。", "result": "在超声神经成像上，模型达到了0.78的Dice分数，0.70的IoU和0.95的像素准确率，显示出比基线模型显著的改进。", "conclusion": "研究结果突出了在自动化神经检测中，量身定制的参数选择的重要性。"}}
{"id": "2507.13468", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13468", "abs": "https://arxiv.org/abs/2507.13468", "authors": ["Shiye Cao", "Maia Stiber", "Amama Mahmood", "Maria Teresa Parreira", "Wendy Ju", "Micol Spitale", "Hatice Gunes", "Chien-Ming Huang"], "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations", "comment": null, "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.", "AI": {"tldr": "ERR@HRI 2.0挑战赛旨在通过提供多模态数据集和鼓励机器学习模型开发，解决LLM驱动的对话机器人在人机交互中出现的错误检测问题。", "motivation": "LLM驱动的对话机器人易出错（如误解意图、过早打断、无响应），这些错误会导致对话中断、任务受阻并损害用户信任，因此检测和解决这些故障至关重要。", "method": "ERR@HRI 2.0挑战赛提供了一个包含16小时人机交互的多模态数据集，涵盖面部、语音和头部运动特征。数据标注了机器人错误（系统视角）和用户纠正意图。参与者需开发机器学习模型，利用多模态数据检测这些故障。", "result": "该挑战赛旨在为检测机器人故障的机器学习模型提供基准，并鼓励研究人员开发有效的检测方案。提交的模型将通过检测准确率和误报率等指标进行评估。", "conclusion": "这项挑战是利用社交信号分析改进人机交互中故障检测的关键一步，有助于提升对话机器人的可靠性和用户体验。"}}
{"id": "2507.13360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13360", "abs": "https://arxiv.org/abs/2507.13360", "authors": ["Le-Anh Tran", "Chung Nguyen Tran", "Ngoc-Luu Nguyen", "Nhan Cach Dang", "Jordi Carrabina", "David Castells-Rufas", "Minh Son Nguyen"], "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance", "comment": "6 pages, 3 figures, ICCCE 2025", "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.", "AI": {"tldr": "本文提出了一种名为EDNIG的深度学习框架，用于低光图像增强。该框架基于U-Net，通过整合亮度通道先验（BCP）生成的照度图作为引导输入，并结合空间金字塔池化（SPP）模块提取多尺度特征，以提高增强效果。模型在GAN框架下使用复合损失函数进行优化，实现了具有竞争力的性能和较低的模型复杂度。", "motivation": "现有低光图像增强方法可能在处理欠曝光区域和多样化光照条件时存在不足，且可能模型复杂度较高。本研究旨在开发一种能有效关注欠曝光区域、处理多尺度特征、同时保持较低模型复杂度的深度学习框架。", "method": "本文提出了Encoder-Decoder Network with Illumination Guidance (EDNIG)框架：\n1. 基于U-Net架构。\n2. 将从亮度通道先验（BCP）派生的照度图作为引导输入，帮助网络关注欠曝光区域。\n3. 整合空间金字塔池化（SPP）模块以提取多尺度上下文特征。\n4. 采用Swish激活函数以确保训练期间梯度传播更平滑。\n5. 在生成对抗网络（GAN）框架下进行优化。\n6. 使用结合对抗损失、像素级均方误差（MSE）和感知损失的复合损失函数。", "result": "EDNIG在定量指标和视觉质量方面，与最先进的方法相比，取得了具有竞争力的性能。同时，它保持了较低的模型复杂度。", "conclusion": "EDNIG在低光图像增强方面表现出色，具有竞争力的性能和较低的模型复杂度，证明了其在实际应用中的适用性。"}}
{"id": "2507.13380", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13380", "abs": "https://arxiv.org/abs/2507.13380", "authors": ["Keito Inoshita", "Rushia Harada"], "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.", "AI": {"tldr": "该论文提出了PersonaGen框架，利用大型语言模型（LLM）通过多阶段基于角色的条件生成情感丰富的文本，旨在解决高质量情感数据集稀缺的问题。", "motivation": "情感识别领域面临高质量、多样化情感数据集稀缺的挑战。情感表达具有主观性，受个体特质、社会文化背景和情境因素影响，导致大规模、通用数据的收集在伦理和实践上都十分困难。", "method": "PersonaGen框架通过结合人口统计学属性、社会文化背景和详细情境来构建分层虚拟角色，并以此引导情感表达的生成。研究通过聚类和分布度量评估语义多样性，通过LLM进行质量评分评估类人度，通过与真实情感语料库比较评估真实性，并通过下游情感分类任务评估实用性。", "result": "实验结果表明，PersonaGen在生成多样、连贯和可区分的情感表达方面显著优于基线方法。", "conclusion": "PersonaGen展示了作为增强或替代真实世界情感数据集的强大潜力。"}}
{"id": "2507.13651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13651", "abs": "https://arxiv.org/abs/2507.13651", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks", "comment": null, "summary": "Many intelligent tutoring systems can support a student in solving a stepwise\ntask. When a student combines several steps in one step, the number of possible\npaths connecting consecutive inputs may be very large. This combinatorial\nexplosion makes error diagnosis hard. Using a final answer to diagnose a\ncombination of steps can mitigate the combinatorial explosion, because there\nare generally fewer possible (erroneous) final answers than (erroneous)\nsolution paths. An intermediate input for a task can be diagnosed by\nautomatically completing it according to the task solution strategy and\ndiagnosing this solution. This study explores the potential of automated error\ndiagnosis based on a final answer. We investigate the design of a service that\nprovides a buggy rule diagnosis when a student combines several steps. To\nvalidate the approach, we apply the service to an existing dataset (n=1939) of\nunique student steps when solving quadratic equations, which could not be\ndiagnosed by a buggy rule service that tries to connect consecutive inputs with\na single rule. Results show that final answer evaluation can diagnose 29,4% of\nthese steps. Moreover, a comparison of the generated diagnoses with teacher\ndiagnoses on a subset (n=115) shows that the diagnoses align in 97% of the\ncases. These results can be considered a basis for further exploration of the\napproach.", "AI": {"tldr": "本文提出一种基于最终答案评估的自动错误诊断方法，旨在解决智能辅导系统中学生合并多个步骤时诊断困难的问题。", "motivation": "当学生在智能辅导系统中合并多个步骤时，连接连续输入的可能路径数量会呈组合爆炸式增长，这使得错误诊断变得非常困难。", "method": "研究探索了基于最终答案评估的自动错误诊断潜力。通过自动完成任务解决方案策略来诊断中间输入，并设计了一个在学生合并多个步骤时提供错误规则诊断的服务。该方法在一个包含1939个解二次方程学生步骤的数据集上进行了验证。", "result": "结果显示，最终答案评估能够诊断出29.4%的先前无法诊断的步骤。此外，与教师诊断的比较表明，在115个案例子集上，诊断结果的吻合度高达97%。", "conclusion": "这些结果为进一步探索基于最终答案评估的自动错误诊断方法奠定了基础，表明其在解决学生合并步骤诊断难题方面具有巨大潜力。"}}
{"id": "2507.13672", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13672", "abs": "https://arxiv.org/abs/2507.13672", "authors": ["Hang Zhou", "Tao Meng", "Kun Wang", "Chengrui Shi", "Renhao Mao", "Weijia Wang", "Jiakun Lei"], "title": "Spacecraft Safe Robust Control Using Implicit Neural Representation for Geometrically Complex Targets in Proximity Operations", "comment": "15 pages, 18 figures, submitted to TAES", "summary": "This study addresses the challenge of ensuring safe spacecraft proximity\noperations, focusing on collision avoidance between a chaser spacecraft and a\ncomplex-geometry target spacecraft under disturbances. To ensure safety in such\nscenarios, a safe robust control framework is proposed that leverages implicit\nneural representations. To handle arbitrary target geometries without explicit\nmodeling, a neural signed distance function (SDF) is learned from point cloud\ndata via a enhanced implicit geometric regularization method, which\nincorporates an over-apporximation strategy to create a conservative,\nsafety-prioritized boundary. The target's surface is implicitly defined by the\nzero-level set of the learned neural SDF, while the values and gradients\nprovide critical information for safety controller design. This neural SDF\nrepresentation underpins a two-layer hierarchcial safe robust control\nframework: a safe velocity generation layer and a safe robust controller layer.\nIn the first layer, a second-order cone program is formulated to generate\nsafety-guaranteed reference velocity by explicitly incorporating the\nunder-approximation error bound. Furthermore, a circulation inequality is\nintroduced to mitigate the local minimum issues commonly encountered in control\nbarrier function (CBF) methods. The second layer features an integrated\ndisturbance observer and a smooth safety filter explicitly compensating for\nestimation error, bolstering robustness to external disturbances. Extensive\nnumerical simulations and Monte Carlo analysis validate the proposed framework,\ndemonstrating significantly improved safety margins and avoidance of local\nminima compared to conventional CBF approaches.", "AI": {"tldr": "该研究提出了一种基于隐式神经表示和两层分层安全鲁棒控制框架，用于在存在干扰的情况下，确保追逐航天器与复杂几何目标航天器之间的安全近距离操作，有效避免碰撞并克服局部最小值问题。", "motivation": "确保航天器近距离操作（特别是追逐航天器与复杂几何目标航天器之间的碰撞避免）在存在干扰的情况下是安全的，是一个重大挑战。", "method": "提出了一种安全鲁棒控制框架：\n1. 利用增强隐式几何正则化方法，从点云数据中学习神经符号距离函数（SDF），以处理任意目标几何形状，并采用过近似策略创建保守的安全边界。\n2. 构建一个两层分层安全鲁棒控制框架：\n   a. 第一层（安全速度生成）：通过二阶锥规划（SOCP）生成安全保证的参考速度，显式纳入欠近似误差界，并引入循环不等式以缓解控制障碍函数（CBF）方法中常见的局部最小值问题。\n   b. 第二层（安全鲁棒控制器）：集成了扰动观测器和平滑安全滤波器，显式补偿估计误差，增强对外部扰动的鲁棒性。", "result": "广泛的数值模拟和蒙特卡洛分析验证了所提出的框架，结果表明与传统CBF方法相比，该框架显著提高了安全裕度并避免了局部最小值问题。", "conclusion": "所提出的框架能够有效确保航天器在复杂几何和扰动条件下的安全近距离操作，通过提高安全裕度和避免局部最小值，显著提升了碰撞避免性能。"}}
{"id": "2507.13458", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13458", "abs": "https://arxiv.org/abs/2507.13458", "authors": ["Malte Hoffmann"], "title": "Domain-randomized deep learning for neuroimage analysis", "comment": "12 pages, 6 figures, 2 tables, deep learning, domain generalization,\n  domain randomization, neuroimaging, medical image analysis, accepted for\n  publication in IEEE Signal Processing Magazine", "summary": "Deep learning has revolutionized neuroimage analysis by delivering\nunprecedented speed and accuracy. However, the narrow scope of many training\ndatasets constrains model robustness and generalizability. This challenge is\nparticularly acute in magnetic resonance imaging (MRI), where image appearance\nvaries widely across pulse sequences and scanner hardware. A recent\ndomain-randomization strategy addresses the generalization problem by training\ndeep neural networks on synthetic images with randomized intensities and\nanatomical content. By generating diverse data from anatomical segmentation\nmaps, the approach enables models to accurately process image types unseen\nduring training, without retraining or fine-tuning. It has demonstrated\neffectiveness across modalities including MRI, computed tomography, positron\nemission tomography, and optical coherence tomography, as well as beyond\nneuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray\nmicrotomography. This tutorial paper reviews the principles, implementation,\nand potential of the synthesis-driven training paradigm. It highlights key\nbenefits, such as improved generalization and resistance to overfitting, while\ndiscussing trade-offs such as increased computational demands. Finally, the\narticle explores practical considerations for adopting the technique, aiming to\naccelerate the development of generalizable tools that make deep learning more\naccessible to domain experts without extensive computational resources or\nmachine learning knowledge.", "AI": {"tldr": "本文综述了一种基于合成图像域随机化的深度学习训练范式，旨在提高神经影像分析模型（尤其是MRI）的泛化能力和鲁棒性，使其能处理训练中未见的图像类型。", "motivation": "深度学习在神经影像分析中面临模型鲁棒性和泛化能力不足的问题，尤其在MRI中，图像外观因序列和硬件差异巨大，现有训练数据集范围狭窄。", "method": "采用域随机化策略，通过从解剖分割图生成具有随机强度和解剖内容的合成图像来训练深度神经网络。这种方法使模型能够准确处理训练中未见的图像类型，无需重新训练或微调。", "result": "该方法已被证明在多种模态（包括MRI、CT、PET、OCT、超声、电子/荧光显微镜和X射线显微断层扫描）中有效，能够提高泛化能力并抵抗过拟合。模型无需再训练或微调即可处理未见数据。", "conclusion": "该教程回顾了合成驱动训练范式的原理、实现和潜力，强调了其在改善泛化能力和抵抗过拟合方面的优势，同时讨论了计算需求增加等权衡。旨在加速开发可泛化工具，使深度学习更易于领域专家使用。"}}
{"id": "2507.13539", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13539", "abs": "https://arxiv.org/abs/2507.13539", "authors": ["Jim O'Connor", "Jay B. Nash", "Derin Gezgin", "Gary B. Parker"], "title": "SCOPE for Hexapod Gait Generation", "comment": "IJCCI Conference on Evolutionary Computation and Theory and\n  Applications, 2025", "summary": "Evolutionary methods have previously been shown to be an effective learning\nmethod for walking gaits on hexapod robots. However, the ability of these\nalgorithms to evolve an effective policy rapidly degrades as the input space\nbecomes more complex. This degradation is due to the exponential growth of the\nsolution space, resulting from an increasing parameter count to handle a more\ncomplex input. In order to address this challenge, we introduce Sparse Cosine\nOptimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine\nTransform (DCT) to learn directly from the feature coefficients of an input\nmatrix. By truncating the coefficient matrix returned by the DCT, we can reduce\nthe dimensionality of an input while retaining the highest energy features of\nthe original input. We demonstrate the effectiveness of this method by using\nSCOPE to learn the gait of a hexapod robot. The hexapod controller is given a\nmatrix input containing time-series information of previous poses, which are\nthen transformed to gait parameters by an evolved policy. In this task, the\naddition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.\nSCOPE achieves this result by reducing the total input size of the time-series\npose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of\ncompressing an input to any output shape, provided that each output dimension\nis no greater than the corresponding input dimension. This paper demonstrates\nthat SCOPE is capable of significantly compressing the size of an input to an\nevolved controller, resulting in a statistically significant gain in efficacy.", "AI": {"tldr": "本文提出SCOPE算法，利用离散余弦变换（DCT）压缩输入维度，有效提升了进化算法在复杂输入空间中学习六足机器人步态的效率。", "motivation": "现有进化算法在学习六足机器人步态时，面对复杂输入空间时性能会迅速下降。这归因于随着输入复杂性增加，参数数量指数增长，导致解空间呈指数级膨胀。", "method": "引入稀疏余弦优化策略进化（SCOPE）算法。SCOPE利用离散余弦变换（DCT）直接从输入矩阵的特征系数中学习。通过截断DCT返回的系数矩阵，可以在保留原始输入高能量特征的同时，降低输入维度。该方法应用于六足机器人步态学习，将前一姿态的时间序列信息作为输入矩阵，并通过进化的策略转换为步态参数。", "result": "将SCOPE添加到参考算法中，使六足机器人步态学习的效率提高了20%。SCOPE将时间序列姿态数据的总输入大小从2700减少到54，降低了98%。此外，SCOPE能够将输入压缩到任何输出形状，只要每个输出维度不大于相应的输入维度。", "conclusion": "SCOPE能够显著压缩进化控制器输入的尺寸，从而带来统计学上显著的效率提升。"}}
{"id": "2507.13361", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13361", "abs": "https://arxiv.org/abs/2507.13361", "authors": ["Shmuel Berman", "Jia Deng"], "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "comment": null, "summary": "Visual Language Models (VLMs) excel at complex visual tasks such as VQA and\nchart understanding, yet recent work suggests they struggle with simple\nperceptual tests. We present an evaluation that tests vision-language models'\ncapacity for nonlocal visual reasoning -- reasoning that requires chaining\nevidence collected from multiple, possibly distant, regions of an image. We\nisolate three distinct forms of non-local vision: comparative perception, which\ndemands holding two images in working memory and comparing them; saccadic\nsearch, which requires making discrete, evidence-driven jumps to locate\nsuccessive targets; and smooth visual search, which involves searching smoothly\nalong a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude\nVision 3.7, GPT-o4-mini), even those that perform well on prior\nprimitive-vision benchmarks, fail these tests and barely exceed random accuracy\non two variants of our tasks that are trivial for humans. Our structured\nevaluation suite allows us to test if VLMs can perform similar visual\nalgorithms to humans. Our findings show that despite gains in raw visual\nacuity, current models lack core visual reasoning capabilities.", "AI": {"tldr": "视觉语言模型（VLMs）在需要整合多个图像区域信息的非局部视觉推理方面表现不佳，即使是顶级模型也难以通过人类认为简单的测试。", "motivation": "尽管VLMs在复杂视觉任务上表现出色，但近期研究表明它们在简单的感知测试中存在困难。本研究旨在评估VLMs进行非局部视觉推理的能力，即需要整合来自图像多个（可能相距较远）区域证据的推理。", "method": "本研究设计了一套评估，测试VLMs的三种非局部视觉形式：比较感知（需要比较两幅图像）、扫视搜索（需要基于证据跳跃定位目标）和平滑视觉搜索（需要沿连续轮廓平滑搜索）。测试对象包括Gemini 2.5 Pro、Claude Vision 3.7和GPT-o4-mini等主流模型。", "result": "实验结果显示，即使在先前原始视觉基准测试中表现良好的旗舰VLM模型，在这些非局部视觉推理测试中也表现失败，在对人类而言微不足道的两种任务变体上，其准确率 barely 超过随机水平。", "conclusion": "研究发现，尽管当前VLMs在原始视觉敏锐度上有所提升，但它们仍然缺乏核心的视觉推理能力，特别是无法执行类似于人类的视觉算法来处理非局部视觉信息。"}}
{"id": "2507.13381", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13381", "abs": "https://arxiv.org/abs/2507.13381", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan Günnemann"], "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models.", "AI": {"tldr": "SAFT是一种结构感知微调方法，通过磁拉普拉斯算子计算的方向敏感位置编码将图拓扑注入预训练LLM，无需架构更改，显著提升了AMR-to-text生成任务的性能，并达到了新的SOTA。", "motivation": "大型语言模型（LLMs）越来越多地应用于图等结构化输入任务，但现有方法在处理抽象意义表示（AMRs）时，常任意线性化AMRs（丢失结构信息）或依赖于与标准LLMs不兼容的架构。因此，需要一种有效的方法将图结构信息融入LLMs。", "method": "本文提出了SAFT（结构感知微调）方法。它通过计算转换后的AMRs的磁拉普拉斯算子（magnetic Laplacian）得到方向敏感的位置编码，并将其投影到LLM的嵌入空间中，从而将图拓扑注入预训练的LLM，且无需对LLM架构进行任何更改。", "result": "SAFT在AMR 3.0数据集上取得了新的最先进成果，相较基线模型，BLEU分数提高了3.5。性能提升随着图复杂度的增加而更为显著，突显了结构感知表示在增强LLM性能方面的价值。", "conclusion": "SAFT为连接结构化数据和语言模型提供了一条通用且有效的途径，证明了结构感知表示在提升LLM性能方面的关键作用。"}}
{"id": "2507.13652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13652", "abs": "https://arxiv.org/abs/2507.13652", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses", "comment": null, "summary": "Model tracing and constraint-based modeling are two approaches to diagnose\nstudent input in stepwise tasks. Model tracing supports identifying consecutive\nproblem-solving steps taken by a student, whereas constraint-based modeling\nsupports student input diagnosis even when several steps are combined into one\nstep. We propose an approach that merges both paradigms. By defining\nconstraints as properties that a student input has in common with a step of a\nstrategy, it is possible to provide a diagnosis when a student deviates from a\nstrategy even when the student combines several steps. In this study we explore\nthe design of a system for multistep strategy diagnoses, and evaluate these\ndiagnoses. As a proof of concept, we generate diagnoses for an existing dataset\ncontaining steps students take when solving quadratic equations (n=2136). To\ncompare with human diagnoses, two teachers coded a random sample of deviations\n(n=70) and applications of the strategy (n=70). Results show that that the\nsystem diagnosis aligned with the teacher coding in all of the 140 student\nsteps.", "AI": {"tldr": "本文提出了一种融合模型追踪和基于约束建模的方法，用于诊断学生在多步任务中的输入，尤其是在学生合并步骤时，并通过实验验证了其诊断结果与教师判断高度一致。", "motivation": "模型追踪和基于约束建模是诊断学生分步任务输入的两种主要方法。模型追踪擅长识别连续的问题解决步骤，而基于约束建模即使在学生合并多个步骤时也能进行诊断。然而，现有方法在处理学生合并步骤时的诊断能力仍有提升空间，因此需要一种能结合两者优势的新方法。", "method": "本文提出了一种将模型追踪和基于约束建模相结合的方法。通过将约束定义为学生输入与策略步骤共有的属性，即使学生合并了多个步骤，系统也能在学生偏离策略时提供诊断。作为概念验证，作者开发了一个系统，并使用包含学生解决二次方程步骤的现有数据集（n=2136）生成诊断。为了与人类诊断进行比较，两位教师对随机抽取的偏离（n=70）和策略应用（n=70）样本进行了编码。", "result": "结果显示，在所有140个学生步骤中，系统诊断与教师编码完全一致。", "conclusion": "该研究证明了所提出的融合方法在诊断学生多步策略方面是有效的，即使学生合并了多个步骤，系统也能提供准确的诊断，并且其诊断结果与人类教师的判断高度一致。"}}
{"id": "2507.13678", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13678", "abs": "https://arxiv.org/abs/2507.13678", "authors": ["Honghao Wu", "Kemi Ding", "Li Qiu"], "title": "Minimum Clustering of Matrices Based on Phase Alignment", "comment": "This work has been received by CDC2025", "summary": "Coordinating multi-agent systems requires balancing synchronization\nperformance and controller implementation costs. To this end, we classify\nagents by their intrinsic properties, enabling each group to be controlled by a\nuniform controller and thus reducing the number of unique controller types\nrequired. Existing centralized control methods, despite their capability to\nachieve high synchronization performance with fewer types of controllers,\nsuffer from critical drawbacks such as limited scalability and vulnerability to\nsingle points of failure. On the other hand, distributed control strategies,\nwhere controllers are typically agent-dependent, result in the type of required\ncontrollers increasing proportionally with the size of the system.\n  This paper introduces a novel phase-alignment-based framework to minimize the\ntype of controllers by strategically clustering agents with aligned\nsynchronization behaviors. Leveraging the intrinsic phase properties of complex\nmatrices, we formulate a constrained clustering problem and propose a\nhierarchical optimization method combining recursive exact searches for\nsmall-scale systems and scalable stochastic approximations for large-scale\nnetworks. This work bridges theoretical phase analysis with practical control\nsynthesis, offering a cost-effective solution for large-scale multi-agent\nsystems. The theoretical results applied for the analysis of a 50-agent network\nillustrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "本文提出了一种基于相位对齐的框架，通过策略性地聚类具有相似同步行为的智能体，以最小化多智能体系统中的控制器类型，从而在保证同步性能的同时降低实现成本。", "motivation": "多智能体系统协调需要在同步性能和控制器实现成本之间取得平衡。现有集中式控制方法虽然能以较少控制器类型实现高性能，但存在可伸缩性差和单点故障的缺点；而分布式控制策略通常依赖于智能体，导致所需控制器类型随系统规模线性增加。因此，需要一种方法来减少所需控制器类型的数量，同时保持性能和可伸缩性。", "method": "本文引入了一种新颖的基于相位对齐的框架，通过利用复杂矩阵的内在相位特性，将具有对齐同步行为的智能体进行策略性聚类，从而最小化控制器类型。该方法将问题表述为一个受约束的聚类问题，并提出了一种分层优化方法，结合了针对小规模系统的递归精确搜索和针对大规模网络的自适应随机近似。", "result": "该工作为大规模多智能体系统提供了一种经济有效的解决方案，弥合了理论相位分析与实际控制综合之间的鸿沟。在50个智能体网络上的理论结果应用证明了所提出算法的有效性。", "conclusion": "通过引入基于相位对齐的聚类框架和分层优化方法，本文成功地解决了多智能体系统中控制器类型最小化的问题，提供了一种兼具高性能、可伸缩性和成本效益的解决方案，适用于大规模多智能体系统。"}}
{"id": "2507.13604", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13604", "abs": "https://arxiv.org/abs/2507.13604", "authors": ["Qihang Li", "Jichen Yang", "Yaqian Chen", "Yuwen Chen", "Hanxue Gu", "Lars J. Grimm", "Maciej A. Mazurowski"], "title": "BreastSegNet: Multi-label Segmentation of Breast MRI", "comment": null, "summary": "Breast MRI provides high-resolution imaging critical for breast cancer\nscreening and preoperative staging. However, existing segmentation methods for\nbreast MRI remain limited in scope, often focusing on only a few anatomical\nstructures, such as fibroglandular tissue or tumors, and do not cover the full\nrange of tissues seen in scans. This narrows their utility for quantitative\nanalysis. In this study, we present BreastSegNet, a multi-label segmentation\nalgorithm for breast MRI that covers nine anatomical labels: fibroglandular\ntissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and\nimplant. We manually annotated a large set of 1123 MRI slices capturing these\nstructures with detailed review and correction from an expert radiologist.\nAdditionally, we benchmark nine segmentation models, including U-Net, SwinUNet,\nUNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among\nthem, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across\nall labels. It performs especially well on heart, liver, muscle, FGT, and bone,\nwith Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All\nmodel code and weights are publicly available, and we plan to release the data\nat a later date.", "AI": {"tldr": "本研究提出了BreastSegNet，一个用于乳腺MRI的多标签分割算法，能够识别九种解剖结构，并评估了多种模型，其中nnU-Net ResEncM表现最佳。", "motivation": "现有乳腺MRI分割方法范围有限，通常只关注少数组织（如纤维腺体组织或肿瘤），未能覆盖扫描中所有可见组织，从而限制了其在定量分析中的实用性。", "method": "开发了BreastSegNet多标签分割算法，涵盖九种解剖标签（纤维腺体组织、血管、肌肉、骨骼、病变、淋巴结、心脏、肝脏和植入物）。手动标注了1123张MRI切片，并经过专家放射科医生详细审查和修正。同时，基准测试了包括U-Net、SwinUNet、UNet++、SAM、MedSAM和带有多个ResNet编码器的nnU-Net在内的九种分割模型。", "result": "在所有标签中，nnU-Net ResEncM取得了最高的平均Dice分数0.694。在心脏、肝脏、肌肉、纤维腺体组织和骨骼上的表现尤为出色，Dice分数超过0.73，心脏和肝脏接近0.90。", "conclusion": "BreastSegNet（采用nnU-Net ResEncM）为乳腺MRI提供了全面的多标签分割能力，显著提升了定量分析的实用性。所有模型代码和权重已公开，数据计划后续发布。"}}
{"id": "2507.13602", "categories": ["cs.RO", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13602", "abs": "https://arxiv.org/abs/2507.13602", "authors": ["Shivakanth Sujit", "Luca Nunziante", "Dan Ogawa Lillrank", "Rousslan Fernand Julien Dossa", "Kai Arulkumaran"], "title": "Improving Low-Cost Teleoperation: Augmenting GELLO with Force", "comment": "Accepted at the 2025 IEEE/SICE International Symposium on System\n  Integration", "summary": "In this work we extend the low-cost GELLO teleoperation system, initially\ndesigned for joint position control, with additional force information. Our\nfirst extension is to implement force feedback, allowing users to feel\nresistance when interacting with the environment. Our second extension is to\nadd force information into the data collection process and training of\nimitation learning models. We validate our additions by implementing these on a\nGELLO system with a Franka Panda arm as the follower robot, performing a user\nstudy, and comparing the performance of policies trained with and without force\ninformation on a range of simulated and real dexterous manipulation tasks.\nQualitatively, users with robotics experience preferred our controller, and the\naddition of force inputs improved task success on the majority of tasks.", "AI": {"tldr": "该工作通过增加力反馈和将力信息纳入模仿学习，扩展了低成本GELLO遥操作系统，提升了用户体验和任务成功率。", "motivation": "GELLO遥操作系统最初仅设计用于关节位置控制，缺乏力信息。本研究旨在通过引入力信息（力反馈和用于模仿学习训练）来增强其功能和性能。", "method": "1. 为GELLO系统实现力反馈功能；2. 将力信息整合到数据收集和模仿学习模型的训练过程中；3. 在配备Franka Panda机械臂的GELLO系统上进行实现和验证；4. 开展用户研究；5. 对比有无力信息训练的策略在模拟和真实灵巧操作任务中的表现。", "result": "定性上，有机器人经验的用户更偏爱新的控制器；定量上，增加力输入提高了大多数任务的成功率。", "conclusion": "通过添加力反馈和将力信息纳入模仿学习过程，显著改进了GELLO遥操作系统，提升了用户满意度和任务执行的成功率。"}}
{"id": "2507.13362", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.8; I.2.6; I.2.7; I.5.4; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13362", "abs": "https://arxiv.org/abs/2507.13362", "authors": ["Binbin Ji", "Siddharth Agrawal", "Qiance Tang", "Yvonne Wu"], "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning", "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate).\n  Authored by students from the Courant Institute, NYU", "summary": "This study investigates the spatial reasoning capabilities of vision-language\nmodels (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement\nlearning. We begin by evaluating the impact of different prompting strategies\nand find that simple CoT formats, where the model generates a reasoning step\nbefore the answer, not only fail to help, but can even harm the model's\noriginal performance. In contrast, structured multi-stage prompting based on\nscene graphs (SceneGraph CoT) significantly improves spatial reasoning\naccuracy. Furthermore, to improve spatial reasoning ability, we fine-tune\nmodels using Group Relative Policy Optimization (GRPO) on the SAT dataset and\nevaluate their performance on CVBench. Compared to supervised fine-tuning\n(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates\nsuperior robustness under out-of-distribution (OOD) conditions. In particular,\nwe find that SFT overfits to surface-level linguistic patterns and may degrade\nperformance when test-time phrasing changes (e.g., from \"closer to\" to \"farther\nfrom\"). GRPO, on the other hand, generalizes more reliably and maintains stable\nperformance under such shifts. Our findings provide insights into how\nreinforcement learning and structured prompting improve the spatial reasoning\ncapabilities and generalization behavior of modern VLMs. All code is open\nsource at: https://github.com/Yvonne511/spatial-vlm-investigator", "AI": {"tldr": "本研究通过结构化思维链（CoT）提示和强化学习（GRPO）显著提升了视觉-语言模型（VLMs）的空间推理能力和泛化性，发现简单CoT无效甚至有害，而GRPO优于监督微调（SFT）且对分布外（OOD）变化更鲁棒。", "motivation": "评估和提高视觉-语言模型（VLMs）的空间推理能力，并探索不同提示策略（特别是思维链CoT）和微调方法对其性能的影响，因为发现简单CoT可能无效甚至有害。", "method": "1. 评估了不同提示策略对VLM空间推理能力的影响，包括简单CoT和基于场景图的结构化多阶段提示（SceneGraph CoT）。2. 使用群组相对策略优化（GRPO）在SAT数据集上对模型进行微调，并将其性能与监督微调（SFT）进行比较。3. 在CVBench上评估模型性能，并测试其在分布外（OOD）条件下的鲁棒性。", "result": "1. 简单的CoT格式不仅未能帮助，反而可能损害模型的原始性能。2. 基于场景图的结构化多阶段提示（SceneGraph CoT）显著提高了空间推理准确性。3. 与SFT相比，GRPO在Pass@1评估中取得了更高的准确性，并在OOD条件下表现出卓越的鲁棒性。4. SFT容易过拟合表层语言模式，在测试时措辞变化时性能下降，而GRPO则更可靠地泛化并保持稳定性能。", "conclusion": "强化学习和结构化提示可以显著提高现代视觉-语言模型的空间推理能力和泛化行为，为未来的VLM研究提供了重要见解。"}}
{"id": "2507.13382", "categories": ["cs.CL", "cs.LG", "05-05C12"], "pdf": "https://arxiv.org/pdf/2507.13382", "abs": "https://arxiv.org/abs/2507.13382", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms.", "AI": {"tldr": "本文提出一种新颖的基于图的方法，结合自然语言处理和异常检测算法，用于识别数字世界中快速传播的假新闻。", "motivation": "假新闻在数字世界中以惊人的速度传播，解决这一问题具有重要意义。", "method": "研究使用了Kaggle的真实和假新闻数据集，并补充了最新的COVID-19相关新闻。通过自然语言处理（NLP）技术将新闻文章转换为上下文图结构，然后应用基于最小描述长度（MDL）的图基异常检测（GBAD）算法进行图挖掘，以识别偏离常规的异常模式。", "result": "所提出的方法能够识别数据集中的规范模式，并随后发现偏离这些既定规范的异常模式。图基方法在处理丰富的上下文数据方面特别有效，能够发现传统查询或统计技术可能忽略的复杂模式。", "conclusion": "该研究提出了一种有效的上下文图基方法来检测假新闻，通过将新闻文章转换为图结构并应用异常检测算法来识别异常模式，证明了图基方法在处理复杂新闻数据方面的优势。"}}
{"id": "2507.13737", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13737", "abs": "https://arxiv.org/abs/2507.13737", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.", "AI": {"tldr": "DailyLLM是一个基于轻量级LLM的系统，利用智能手机/手表传感器收集的多维度上下文信息（位置、运动、环境、生理），生成和总结高质量的活动日志，在准确性和效率上显著优于现有方法。", "motivation": "丰富的、上下文感知的活动日志对于用户行为分析和健康监测至关重要，但现有活动日志生成方法在准确性、效率和语义丰富度方面存在显著局限性。尽管大型语言模型（LLMs）提供了新的机遇，但仍需解决这些挑战。", "method": "本文提出了DailyLLM系统，这是首个全面整合四维度（位置、运动、环境、生理）上下文活动信息的日志生成和摘要系统，仅使用智能手机和智能手表上的常用传感器。DailyLLM引入了一个轻量级的基于LLM的框架，结合了结构化提示和高效的特征提取，以实现高级活动理解。", "result": "实验证明DailyLLM优于最先进的（SOTA）日志生成方法，并且可以高效地部署在个人电脑和树莓派上。仅使用1.5B参数的LLM模型，DailyLLM在日志生成BERTScore精度上比70B参数的SOTA基线提高了17%，同时推理速度快了近10倍。", "conclusion": "DailyLLM成功解决了现有活动日志生成方法在准确性、效率和语义丰富度方面的挑战，通过整合多维度上下文信息和采用轻量级LLM框架，实现了高性能和高效部署，为普适计算中的用户行为分析和健康监测提供了新的解决方案。"}}
{"id": "2507.13687", "categories": ["eess.SY", "cs.SY", "93C95, 93E35, 93E20", "H.4.1"], "pdf": "https://arxiv.org/pdf/2507.13687", "abs": "https://arxiv.org/abs/2507.13687", "authors": ["Ming Lei", "Shufan Wu"], "title": "Robust Probability Hypothesis Density Filtering: Theory and Algorithms", "comment": "This version is submitted and in review currently", "summary": "Multi-target tracking (MTT) serves as a cornerstone technology in information\nfusion, yet faces significant challenges in robustness and efficiency when\ndealing with model uncertainties, clutter interference, and target\ninteractions. Conventional approaches like Gaussian Mixture PHD (GM-PHD) and\nCardinalized PHD (CPHD) filters suffer from inherent limitations including\ncombinatorial explosion, sensitivity to birth/death process parameters, and\nnumerical instability. This study proposes an innovative minimax robust PHD\nfiltering framework with four key contributions: (1) A theoretically derived\nrobust GM-PHD recursion algorithm that achieves optimal worst-case error\ncontrol under bounded uncertainties; (2) An adaptive real-time parameter\nadjustment mechanism ensuring stability and error bounds; (3) A generalized\nheavy-tailed measurement likelihood function maintaining polynomial\ncomputational complexity; (4) A novel partition-based credibility weighting\nmethod for extended targets. The research not only establishes rigorous\nconvergence guarantees and proves the uniqueness of PHD solutions, but also\nverifies algorithmic equivalence with standard GM-PHD. Experimental results\ndemonstrate that in high-clutter environments, this method achieves a\nremarkable 32.4% reduction in OSPA error and 25.3% lower cardinality RMSE\ncompared to existing techniques, while maintaining real-time processing\ncapability at 15.3 milliseconds per step. This breakthrough lays a crucial\nfoundation for reliable MTT in safety-critical applications.", "AI": {"tldr": "本文提出了一种创新的最小最大鲁棒PHD滤波框架，旨在解决多目标跟踪（MTT）中模型不确定性、杂波干扰和目标交互带来的挑战，显著提升了高杂波环境下的跟踪精度和实时性。", "motivation": "传统的多目标跟踪方法（如GM-PHD和CPHD滤波器）在处理模型不确定性、杂波干扰和目标交互时面临鲁棒性和效率挑战，具体表现为组合爆炸、对目标生灭过程参数敏感以及数值不稳定性。", "method": "本研究提出了一个最小最大鲁棒PHD滤波框架，包括：1) 理论推导的鲁棒GM-PHD递推算法，实现有界不确定性下的最优最坏情况误差控制；2) 自适应实时参数调整机制，确保稳定性和误差边界；3) 广义重尾测量似然函数，保持多项式计算复杂度；4) 针对扩展目标的新型基于分区的可信度加权方法。研究还建立了严格的收敛性保证，证明了PHD解的唯一性，并验证了算法与标准GM-PHD的等效性。", "result": "实验结果表明，在高杂波环境下，该方法相比现有技术，OSPA误差降低了32.4%，基数RMSE降低了25.3%，同时保持了实时处理能力（每步15.3毫秒）。", "conclusion": "这项突破为安全关键应用中可靠的多目标跟踪奠定了关键基础。"}}
{"id": "2507.13782", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13782", "abs": "https://arxiv.org/abs/2507.13782", "authors": ["Malo Gicquel", "Ruoyi Zhao", "Anika Wuestefeld", "Nicola Spotorno", "Olof Strandberg", "Kalle Åström", "Yu Xiao", "Laura EM Wisse", "Danielle van Westen", "Rik Ossenkoppele", "Niklas Mattsson-Carlgren", "David Berron", "Oskar Hansson", "Gabrielle Flood", "Jacob Vogel"], "title": "Converting T1-weighted MRI from 3T to 7T quality using deep learning", "comment": null, "summary": "Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides\ndetailed anatomical views, offering better signal-to-noise ratio, resolution\nand tissue contrast than 3T MRI, though at the cost of accessibility. We\npresent an advanced deep learning model for synthesizing 7T brain MRI from 3T\nbrain MRI. Paired 7T and 3T T1-weighted images were acquired from 172\nparticipants (124 cognitively unimpaired, 48 impaired) from the Swedish\nBioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:\na specialized U-Net, and a U-Net integrated with a generative adversarial\nnetwork (GAN U-Net). Our models outperformed two additional state-of-the-art\n3T-to-7T models in image-based evaluation metrics. Four blinded MRI\nprofessionals judged our synthetic 7T images as comparable in detail to real 7T\nimages, and superior in subjective visual quality to 7T images, apparently due\nto the reduction of artifacts. Importantly, automated segmentations of the\namygdalae of synthetic GAN U-Net 7T images were more similar to manually\nsegmented amygdalae (n=20), than automated segmentations from the 3T images\nthat were used to synthesize the 7T images. Finally, synthetic 7T images showed\nsimilar performance to real 3T images in downstream prediction of cognitive\nstatus using MRI derivatives (n=3,168). In all, we show that synthetic\nT1-weighted brain images approaching 7T quality can be generated from 3T\nimages, which may improve image quality and segmentation, without compromising\nperformance in downstream tasks. Future directions, possible clinical use\ncases, and limitations are discussed.", "AI": {"tldr": "本研究开发了一种深度学习模型，能从3T脑部MRI图像合成接近7T质量的图像，提高了图像质量和分割精度，同时不影响下游任务表现。", "motivation": "7T MRI比3T MRI在信噪比、分辨率和组织对比度方面更优，但可及性较差。因此，研究目标是开发一种方法，利用更易获得的3T MRI数据合成具有7T MRI质量的图像。", "method": "研究使用了来自瑞典BioFINDER-2研究的172名参与者（124名认知正常，48名认知障碍）的配对7T和3T T1加权脑部图像数据。训练了两种深度学习模型：一个专门的U-Net模型和一个集成生成对抗网络（GAN）的U-Net模型（GAN U-Net）。通过图像评估指标、四位盲法MRI专业人员的主观评估、与手动分割结果的比较（杏仁核自动分割），以及在认知状态预测下游任务中的表现来评估合成图像的质量。", "result": "所提出的模型在图像评估指标上优于其他两种最先进的3T-到-7T模型。盲法评估显示，合成的7T图像在细节上与真实7T图像相当，在主观视觉质量上甚至优于真实7T图像（可能是由于伪影减少）。重要的是，合成GAN U-Net 7T图像的杏仁核自动分割结果比用于合成的3T图像的自动分割结果更接近手动分割结果。此外，合成的7T图像在认知状态预测的下游任务中表现与真实3T图像相似。", "conclusion": "研究表明，可以从3T图像生成接近7T质量的T1加权脑部图像，这有望提高图像质量和分割精度，同时不损害在下游任务中的性能。"}}
{"id": "2507.13647", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13647", "abs": "https://arxiv.org/abs/2507.13647", "authors": ["Minze Li", "Wei Zhao", "Ran Chen", "Mingqiang Wei"], "title": "Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones", "comment": "8 papers,7 figures", "summary": "Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic\nenvironments remains a key challenge due to high computational demands and the\nneed for fast, adaptive responses. Traditional Particle Swarm Optimization\n(PSO) methods, while effective for offline planning, often struggle with\npremature convergence and latency in real-time scenarios. To overcome these\nlimitations, we propose PE-PSO, an enhanced PSO-based online trajectory\nplanner. The method introduces a persistent exploration mechanism to preserve\nswarm diversity and an entropy-based parameter adjustment strategy to\ndynamically adapt optimization behavior. UAV trajectories are modeled using\nB-spline curves, which ensure path smoothness while reducing optimization\ncomplexity. To extend this capability to UAV swarms, we develop a multi-agent\nframework that combines genetic algorithm (GA)-based task allocation with\ndistributed PE-PSO, supporting scalable and coordinated trajectory generation.\nThe distributed architecture allows for parallel computation and decentralized\ncontrol, enabling effective cooperation among agents while maintaining\nreal-time performance. Comprehensive simulations demonstrate that the proposed\nframework outperforms conventional PSO and other swarm-based planners across\nseveral metrics, including trajectory quality, energy efficiency, obstacle\navoidance, and computation time. These results confirm the effectiveness and\napplicability of PE-PSO in real-time multi-UAV operations under complex\nenvironmental conditions.", "AI": {"tldr": "本文提出了一种名为PE-PSO的增强型粒子群优化算法，用于无人机在动态环境下的实时轨迹规划，并扩展至多无人机蜂群，通过结合遗传算法进行任务分配和分布式PE-PSO实现可扩展的协调轨迹生成。", "motivation": "在动态环境中，无人机实时轨迹规划面临计算量大、响应速度慢的挑战。传统的粒子群优化（PSO）方法在实时场景中常出现早熟收敛和延迟问题。", "method": "提出PE-PSO算法，引入持久探索机制以保持种群多样性，并采用基于熵的参数调整策略动态适应优化行为。无人机轨迹采用B样条曲线建模以确保平滑性并降低优化复杂度。对于无人机蜂群，开发了一个多智能体框架，结合基于遗传算法的任务分配和分布式PE-PSO，实现并行计算和去中心化控制。", "result": "综合仿真结果表明，所提出的框架在轨迹质量、能源效率、避障能力和计算时间等方面均优于传统PSO和其他基于群体的规划器。", "conclusion": "PE-PSO框架在复杂环境下的实时多无人机操作中表现出有效性和适用性。"}}
{"id": "2507.13363", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13363", "abs": "https://arxiv.org/abs/2507.13363", "authors": ["Atharv Goel", "Mehar Khurana"], "title": "Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop", "comment": null, "summary": "Modern 3D object detection datasets are constrained by narrow class\ntaxonomies and costly manual annotations, limiting their ability to scale to\nopen-world settings. In contrast, 2D vision-language models trained on\nweb-scale image-text pairs exhibit rich semantic understanding and support\nopen-vocabulary detection via natural language prompts. In this work, we\nleverage the maturity and category diversity of 2D foundation models to perform\nopen-vocabulary 3D object detection without any human-annotated 3D labels.\n  Our pipeline uses a 2D vision-language detector to generate text-conditioned\nproposals, which are segmented with SAM and back-projected into 3D using camera\ngeometry and either LiDAR or monocular pseudo-depth. We introduce a geometric\ninflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D\nbounding boxes without training. To simulate adverse real-world conditions, we\nconstruct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes\ndataset.\n  Experiments demonstrate that our method achieves competitive localization\nperformance across multiple settings, including LiDAR-based and purely RGB-D\ninputs, all while remaining training-free and open-vocabulary. Our results\nhighlight the untapped potential of 2D foundation models for scalable 3D\nperception. We open-source our code and resources at\nhttps://github.com/atharv0goel/open-world-3D-det.", "AI": {"tldr": "该论文提出了一种无需人工标注3D标签的开放词汇3D目标检测方法，通过利用2D视觉-语言基础模型和几何推断策略实现。", "motivation": "现代3D目标检测数据集受限于狭窄的类别分类和昂贵的手动标注，难以扩展到开放世界场景。相比之下，2D视觉-语言模型在网络规模的图像-文本对上训练，展现出丰富的语义理解和通过自然语言提示支持开放词汇检测的能力。", "method": "该方法利用2D视觉-语言检测器生成文本条件提议，然后使用SAM进行分割，并通过相机几何和LiDAR或单目伪深度将其反投影到3D空间。为推断3D边界框，引入了一种基于DBSCAN聚类和旋转卡尺的几何膨胀策略，无需训练。此外，构建了Pseudo-nuScenes数据集（nuScenes的雾增强、纯RGB变体）来模拟恶劣的真实世界条件进行实验。", "result": "实验表明，该方法在多种设置下（包括基于LiDAR和纯RGB-D输入）均实现了有竞争力的定位性能，同时保持了免训练和开放词汇的特性。", "conclusion": "研究结果突出了2D基础模型在可扩展3D感知方面的巨大潜力。"}}
{"id": "2507.13390", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13390", "abs": "https://arxiv.org/abs/2507.13390", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "PARAM-1 BharatGen 2.9B Model", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications.", "AI": {"tldr": "PARAM-1是一个2.9B参数的语言模型，从头开始训练，专注于印度语言多样性，旨在解决现有大型语言模型以英语为中心的问题。", "motivation": "现有的大型语言模型（LLMs）主要以英语数据、架构和优化范式为主导，导致印度等语言多样性丰富的地区被结构性地低估。印度拥有20多种官方语言和100多种方言，并存在语码转换和双语现象，因此需要专门的模型来公平地代表这些语言。", "method": "PARAM-1是一个2.9B参数的仅解码器、仅文本语言模型，从头开始训练。它使用了一个高质量、事实丰富的印地语和英语双语数据集。其训练遵循三个核心原则：1) 通过25%的语料库分配实现印度语言的公平表示；2) 通过适应印度形态结构的SentencePiece分词器实现分词公平性；3) 通过IndicQA、语码混合推理和社会语言学鲁棒性任务进行文化对齐的评估基准。", "result": "PARAM-1在预训练阶段就嵌入了多样性，而不是推迟到后期对齐，它既是一个称职的通用模型，也是印度中心应用的强大基线。", "conclusion": "PARAM-1为公平的基础模型提供了一个“设计优先”的蓝图，通过在预训练级别嵌入多样性，解决了现有LLM的语言排他性问题，并为印度语言多样性提供了有效的支持。"}}
{"id": "2507.13759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13759", "abs": "https://arxiv.org/abs/2507.13759", "authors": ["Carlos Bobed", "Carlota Quintana", "Eduardo Mena", "Jorge Bobed", "Fernando Bobillo"], "title": "OntView: What you See is What you Meant", "comment": null, "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community.", "AI": {"tldr": "本文介绍了一个名为OntView的本体可视化工具，旨在通过直观的界面和推理能力，有效解决现有工具在表示大型和复杂本体时信息过载和缺乏推理知识显示的问题。", "motivation": "现有本体编辑器和查看器在图形化表示本体结构时，往往无法以有意义且不过载的方式呈现，限制了用户理解大型本体框架中依赖关系和属性的能力。尤其缺乏有效可视化推理知识的工具。", "method": "本文提出了OntView，一个本体查看器，它通过用户友好的界面提供本体概念及其形式定义的直观视觉表示。它基于描述逻辑（DL）推理器，遵循“所见即所得”范式，显示实际推理出的知识。其关键特性包括：可视化通用概念包含（GCI），以及通过以下方式避免信息过载：1) 根据概念重要性创建本体摘要；2) 聚焦显示给定两类之间的TBox元素；3) 动态隐藏/显示分支而不失语义。", "result": "OntView被开发并发布为开源工具，它提供了一种有效且直观的本体可视化方案，能够显示推理知识（如GCI），并通过多种方法（摘要、聚焦、动态隐藏/显示）管理信息过载，从而提升用户对大型本体的理解能力。", "conclusion": "OntView为本体可视化领域提供了一个重要的工具，它通过结合推理能力和信息简化策略，克服了现有工具的局限性，使得用户能够更好地理解复杂的本体结构和推理出的知识。该工具已开源，供社区使用。"}}
{"id": "2507.13872", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13872", "abs": "https://arxiv.org/abs/2507.13872", "authors": ["Aditya Singh", "Aastha Mishra", "Manan Tayal", "Shishir Kolathaya", "Pushpak Jagtap"], "title": "Safe and Performant Controller Synthesis using Gradient-based Model Predictive Control and Control Barrier Functions", "comment": "6 Pages, 2 Figures. The first two authors contributed equally", "summary": "Ensuring both performance and safety is critical for autonomous systems\noperating in real-world environments. While safety filters such as Control\nBarrier Functions (CBFs) enforce constraints by modifying nominal controllers\nin real time, they can become overly conservative when the nominal policy lacks\nsafety awareness. Conversely, solving State-Constrained Optimal Control\nProblems (SC-OCPs) via dynamic programming offers formal guarantees but is\nintractable in high-dimensional systems. In this work, we propose a novel\ntwo-stage framework that combines gradient-based Model Predictive Control (MPC)\nwith CBF-based safety filtering for co-optimizing safety and performance. In\nthe first stage, we relax safety constraints as penalties in the cost function,\nenabling fast optimization via gradient-based methods. This step improves\nscalability and avoids feasibility issues associated with hard constraints. In\nthe second stage, we modify the resulting controller using a CBF-based\nQuadratic Program (CBF-QP), which enforces hard safety constraints with minimal\ndeviation from the reference. Our approach yields controllers that are both\nperformant and provably safe. We validate the proposed framework on two case\nstudies, showcasing its ability to synthesize scalable, safe, and\nhigh-performance controllers for complex, high-dimensional autonomous systems.", "AI": {"tldr": "本文提出一个两阶段框架，结合基于梯度的模型预测控制（MPC）和基于控制障碍函数（CBF）的安全滤波，以协同优化自主系统的性能和安全性。", "motivation": "现有方法在确保自主系统性能和安全方面存在局限：单独使用CBF可能过于保守，而通过动态规划求解带状态约束的最优控制问题（SC-OCPs）在高维系统中难以处理。因此，需要一种既能高效优化性能又能提供安全保证的方法。", "method": "该框架分为两个阶段：第一阶段，使用基于梯度的MPC，将安全约束作为成本函数中的惩罚项，实现快速优化、提高可扩展性并避免硬约束带来的可行性问题。第二阶段，使用基于CBF的二次规划（CBF-QP）修改第一阶段得到的控制器，以最小偏差强制执行硬安全约束。", "result": "该方法生成的控制器既具有高性能又可证明是安全的。通过两个案例研究验证了其有效性，证明了该框架能够为复杂高维自主系统合成可扩展、安全且高性能的控制器。", "conclusion": "所提出的两阶段框架成功地将基于梯度的MPC与基于CBF的安全滤波相结合，为高维自主系统提供了兼具可扩展性、安全性和高性能的控制器解决方案。"}}
{"id": "2507.13830", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13830", "abs": "https://arxiv.org/abs/2507.13830", "authors": ["Maximilian Rokuss", "Benjamin Hamm", "Yannick Kirchhoff", "Klaus Maier-Hein"], "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation", "comment": "Accepted at MICCAI 2025 WOMEN", "summary": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider", "AI": {"tldr": "本文介绍了首个公开可用的乳房MRI数据集，包含超过13,000例明确的左右乳房分割标签，并提供了一个经过训练的深度学习模型用于左右乳房分割。", "motivation": "该研究旨在填补乳房MRI分析中的一个关键空白，并为女性健康领域高级工具的开发提供有价值的资源。", "method": "提供了一个包含13,000多个标注案例的乳房MRI数据集，并训练了一个鲁棒的深度学习模型用于左右乳房分割。", "result": "成功构建并公开了首个带有明确左右乳房分割标签的乳房MRI数据集，以及一个经过训练的、用于左右乳房分割的深度学习模型。", "conclusion": "该工作为乳房MRI分析和女性健康领域高级工具的开发提供了重要的公共资源和工具。"}}
{"id": "2507.13650", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13650", "abs": "https://arxiv.org/abs/2507.13650", "authors": ["Yu-Ting Lai", "Yasamin Foroutani", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography", "comment": "12 pages, 27 figures", "summary": "Secondary cataract is one of the most common complications of vision loss due\nto the proliferation of residual lens materials that naturally grow on the lens\ncapsule after cataract surgery. A potential treatment is capsule cleaning, a\nsurgical procedure that requires enhanced visualization of the entire capsule\nand tool manipulation on the thin membrane. This article presents a robotic\nsystem capable of performing the capsule cleaning procedure by integrating a\nstandard transpupillary and an intraocular optical coherence tomography probe\non a surgical instrument for equatorial capsule visualization and real-time\ntool-to-tissue distance feedback. Using robot precision, the developed system\nenables complete capsule mapping in the pupillary and equatorial regions with\nin-situ calibration of refractive index and fiber offset, which are still\ncurrent challenges in obtaining an accurate capsule model. To demonstrate\neffectiveness, the capsule mapping strategy was validated through five\nexperimental trials on an eye phantom that showed reduced root-mean-square\nerrors in the constructed capsule model, while the cleaning strategy was\nperformed in three ex-vivo pig eyes without tissue damage.", "AI": {"tldr": "该研究提出了一种机器人系统，通过集成光学相干断层扫描（OCT）探头，用于辅助白内障手术后继发性白内障的晶状体囊膜清洁，提高了可视化和操作精度。", "motivation": "继发性白内障是白内障手术后常见的视力丧失并发症，由残留晶状体物质增生引起。现有的囊膜清洁手术需要增强对整个囊膜的可视化以及在薄膜上的工具操作能力，这仍是挑战。", "method": "该系统将标准经瞳孔和眼内OCT探头集成到手术器械上，以实现赤道囊膜的可视化和实时的工具-组织距离反馈。利用机器人精度，该系统能够对瞳孔和赤道区域进行完整的囊膜映射，并进行折射率和光纤偏移的原位校准。", "result": "囊膜映射策略通过五次眼部模型实验验证，显示构建的囊膜模型均方根误差降低。清洁策略在三只离体猪眼中成功实施，未造成组织损伤。", "conclusion": "该机器人系统通过整合OCT探头和机器人精度，有效提升了继发性白内障囊膜清洁过程中的可视化和操作能力，并在实验中证明了其准确性和安全性，为准确的囊膜模型构建和无损伤的清洁提供了潜力。"}}
{"id": "2507.13364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13364", "abs": "https://arxiv.org/abs/2507.13364", "authors": ["Siddharth Srivastava", "Gaurav Sharma"], "title": "OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning", "comment": null, "summary": "We present a novel multimodal multitask network and associated training\nalgorithm. The method is capable of ingesting data from approximately 12\ndifferent modalities namely image, video, audio, text, depth, point cloud, time\nseries, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed\napproach utilizes modality specialized tokenizers, a shared transformer\narchitecture, and cross-attention mechanisms to project the data from different\nmodalities into a unified embedding space. It addresses multimodal and\nmultitask scenarios by incorporating modality-specific task heads for different\ntasks in respective modalities. We propose a novel pretraining strategy with\niterative modality switching to initialize the network, and a training\nalgorithm which trades off fully joint training over all modalities, with\ntraining on pairs of modalities at a time. We provide comprehensive evaluation\nacross 25 datasets from 12 modalities and show state of the art performances,\ndemonstrating the effectiveness of the proposed architecture, pretraining\nstrategy and adapted multitask training.", "AI": {"tldr": "本文提出了一种新颖的多模态多任务网络及其训练算法，能够处理12种不同模态的数据。该方法通过模态专用分词器、共享Transformer和跨注意力机制将数据统一到嵌入空间，并采用迭代模态切换的预训练策略和成对模态训练算法，在多模态多任务场景中取得了最先进的性能。", "motivation": "现有方法在处理大规模、多样化的多模态和多任务场景时可能存在局限性，需要一种更通用、高效的网络架构和训练策略来统一处理来自多种模态的数据并执行多项任务。", "method": "该研究提出了一种新型多模态多任务网络。它利用模态专用分词器、共享Transformer架构和跨注意力机制将来自12种模态（如图像、视频、音频、文本等）的数据投影到统一的嵌入空间。网络通过集成模态特定的任务头来处理不同模态的多种任务。训练方面，采用了一种新颖的迭代模态切换预训练策略，以及一种平衡所有模态联合训练与成对模态训练的训练算法。", "result": "研究在来自12种模态的25个数据集上进行了全面评估，结果表明所提出的架构、预训练策略和多任务训练方法均达到了最先进的性能，证明了其有效性。", "conclusion": "所提出的多模态多任务网络及其新颖的预训练和训练策略在处理多样化模态数据和多任务场景中表现出卓越的有效性和最先进的性能，为未来的多模态人工智能研究提供了新的方向。"}}
{"id": "2507.13392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13392", "abs": "https://arxiv.org/abs/2507.13392", "authors": ["Emil Häglund", "Johanna Björklund"], "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.", "AI": {"tldr": "该研究通过将主题建模流程重构为基于“意见单元”（包含文本摘录和情感分数）的操作，显著提升了从客户评论中提取洞察的能力，并能将主题和情感与业务指标关联。", "motivation": "传统的客户评论主题建模方法在捕获情感和关联业务结果方面存在不足。研究旨在提高主题建模的性能，使其产生更连贯、可解释的主题，并捕获相关情感，从而揭示客户关注点如何影响业务成果。", "method": "研究方法包括：1. 重构主题建模流程，使其在“意见单元”（通过大型语言模型提取的文本摘录和相关情感分数）上运行。2. 将生成的主题和情感与星级评分等业务指标相关联。3. 评估系统在创建连贯主题方面的有效性。4. 评估整合主题和情感模态以准确预测星级评分的方法。", "result": "结果显示，该方法显著提升了后续主题建模的性能，产生了连贯且可解释的主题，并捕获了与每个主题相关的情感。通过关联主题和情感与业务指标，可以深入了解特定客户关注点如何影响业务成果。系统在创建连贯主题方面表现出有效性，并评估了整合主题和情感模态以进行准确星级评分预测的方法。", "conclusion": "该系统在主题建模和分类解决方案方面具有优势，能够有效地创建连贯的主题，并通过整合主题和情感模态实现准确的星级评分预测，从而为企业提供有价值的客户洞察。"}}
{"id": "2507.13768", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13768", "abs": "https://arxiv.org/abs/2507.13768", "authors": ["Renato Ghisellini", "Remo Pareschi", "Marco Pedroni", "Giovanni Battista Raggi"], "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning", "comment": "Peer-reviewed full paper accepted through a double-blind review\n  process at the HAR 2025 conference (https://har-conf.eu/). The official\n  version will appear in a volume of the Lecture Notes in Computer Science\n  (LNCS) series", "summary": "We present a hybrid architecture for agent-augmented strategic reasoning,\ncombining heuristic extraction, semantic activation, and compositional\nsynthesis. Drawing on sources ranging from classical military theory to\ncontemporary corporate strategy, our model activates and composes multiple\nheuristics through a process of semantic interdependence inspired by research\nin quantum cognition. Unlike traditional decision engines that select the best\nrule, our system fuses conflicting heuristics into coherent and\ncontext-sensitive narratives, guided by semantic interaction modeling and\nrhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,\nwith preliminary validation through semantic metrics. Limitations and\nextensions (e.g., dynamic interference tuning) are discussed.", "AI": {"tldr": "该论文提出了一种混合架构，用于增强型战略推理，通过语义激活和组合合成，将冲突的启发式规则融合成连贯的叙事，而非简单选择最佳规则。", "motivation": "传统的决策引擎倾向于选择最佳规则，但在实际战略推理中，常常存在相互冲突的启发式规则。本研究旨在解决如何将这些冲突规则融合为连贯且情境敏感的叙事。", "method": "采用混合架构，结合启发式提取、语义激活和组合合成。借鉴量子认知研究，通过语义相互依赖性激活和组合多种启发式规则。利用语义交互建模和修辞框架，将冲突的启发式规则融合为连贯的叙事，而非选择单一最佳规则。", "result": "该框架通过Meta vs. FTC案例研究进行了演示，并通过语义指标进行了初步验证。", "conclusion": "论文提出了一种通过融合冲突启发式规则进行战略推理的新方法，并通过案例研究和初步语义验证展示了其可行性，并讨论了局限性和未来扩展方向。"}}
{"id": "2507.13888", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13888", "abs": "https://arxiv.org/abs/2507.13888", "authors": ["Janani S K", "Shishir Kolathaya"], "title": "Fixed time convergence guarantees for Higher Order Control Barrier Functions", "comment": "6 PAGES, 2 FIGURES", "summary": "We present a novel method for designing higher-order Control Barrier\nFunctions (CBFs) that guarantee convergence to a safe set within a\nuser-specified finite. Traditional Higher Order CBFs (HOCBFs) ensure asymptotic\nsafety but lack mechanisms for fixed-time convergence, which is critical in\ntime-sensitive and safety-critical applications such as autonomous navigation.\nIn contrast, our approach imposes a structured differential constraint using\nrepeated roots in the characteristic polynomial, enabling closed-form\npolynomial solutions with exact convergence at a prescribed time. We derive\nconditions on the barrier function and its derivatives that ensure forward\ninvariance and fixed-time reachability, and we provide an explicit formulation\nfor second-order systems. Our method is evaluated on three robotic systems - a\npoint-mass model, a unicycle, and a bicycle model and benchmarked against\nexisting HOCBF approaches. Results demonstrate that our formulation reliably\nenforces convergence within the desired time, even when traditional methods\nfail. This work provides a tractable and robust framework for real-time control\nwith provable finite-time safety guarantees.", "AI": {"tldr": "本文提出一种新型高阶控制障碍函数（HOCBF）设计方法，能确保系统在用户指定时间内收敛到安全集，解决了传统HOCBF缺乏固定时间收敛机制的问题。", "motivation": "传统高阶控制障碍函数（HOCBFs）只能保证渐近安全，无法实现固定时间收敛。在自动导航等时间敏感和安全关键应用中，固定时间收敛至关重要。", "method": "该方法通过在特征多项式中使用重复根，施加结构化微分约束，从而得到具有精确固定时间收敛特性的封闭式多项式解。文中推导了障碍函数及其导数确保前向不变性和固定时间可达性的条件，并给出了二阶系统的显式公式。", "result": "该方法在点质量模型、独轮车和自行车模型三种机器人系统上进行了评估，并与现有HOCBF方法进行了基准测试。结果表明，即使传统方法失效，本文提出的方法也能可靠地在期望时间内强制收敛。", "conclusion": "这项工作提供了一个可操作且鲁棒的框架，用于实现具有可证明的有限时间安全保证的实时控制。"}}
{"id": "2507.13901", "categories": ["eess.IV", "cs.CV", "62H35, 68U10", "I.4.10; I.4.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.13901", "abs": "https://arxiv.org/abs/2507.13901", "authors": ["Lei Xu", "Torkel B Brismar"], "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive", "comment": "24 pages, 7 figures", "summary": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.", "AI": {"tldr": "该论文介绍了一个名为AnatomyArchive的CT图像分析软件包，它基于TotalSegmentator模型，提供自动解剖区域选择、体成分分析、放射组学特征提取和机器学习模型开发辅助功能。", "motivation": "研究动机是为了开发一个能够自动化CT图像分析、精确进行体成分分析、简化医学图像数据库管理，并支持放射组学和机器学习模型开发的综合工具。", "method": "AnatomyArchive构建在TotalSegmentator全身分割模型之上，采用基于知识图谱的解剖分割掩膜管理和图像数据库维护。它实现了自动目标体积选择/取消选择、自动身体体积裁剪、手臂检测和排除。该软件包还提供鲁棒的体素级放射组学特征提取、可视化以及统计测试分析工具链，并包含基于Python的GPU加速电影级渲染。", "result": "AnatomyArchive成功实现了根据用户配置解剖结构的自动目标体积选择和取消选择，通过自动裁剪和手臂排除实现了更精确的2D和3D体成分分析。它提供了强大的放射组学特征提取、可视化和统计分析功能，并包含高质量的集成渲染。该软件能够有效协助现代机器学习模型的开发。", "conclusion": "AnatomyArchive是一个功能全面、自动化程度高的CT图像分析软件包，它显著提升了体成分分析的精确性，优化了医学图像数据库管理，并为放射组学研究和机器学习模型开发提供了强大的支持工具。"}}
{"id": "2507.13654", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13654", "abs": "https://arxiv.org/abs/2507.13654", "authors": ["Haoran Wang", "Yasamin Foroutani", "Matthew Nepo", "Mercedes Rodriguez", "Ji Ma", "Jean-Pierre Hubschman", "Tsu-Chin Tsao", "Jacob Rosen"], "title": "A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment", "comment": "9 pages, 11 figures", "summary": "This paper examines the performance of Inside and Outside Control modes at\nvarious scaling factors in a simulated vitreoretinal surgical setting. The\nIRISS teleoperated surgical system's console (cockpit) was adapted to project a\nsimulated microscope view of an intraocular setup to a virtual reality (VR)\nheadset. Five experienced vitreoretinal surgeons and five engineers with no\nsurgical experience used the system to perform tasks common to vitreoretinal\nsurgery. Experimental results indicate that Inside Control methods at higher\nscaling factors (20 or 30) achieved the best performance overall, though the\noptimal scaling factor may vary by task and complexity. Optimizing control\nmethods and scaling factors could lead to improvements in surgical efficiency\nand accuracy, as well as minimize risks in future robotic-assisted intraocular\nprocedures.", "AI": {"tldr": "本文在模拟玻璃体视网膜手术环境中，研究了“内部控制”和“外部控制”模式在不同缩放因子下的表现。", "motivation": "为了提高未来机器人辅助眼内手术的效率、准确性并最小化风险，需要优化控制方法和缩放因子。", "method": "研究使用了IRISS远程手术系统的控制台，将模拟显微镜视图投射到VR头戴设备中。五名经验丰富的玻璃体视网膜外科医生和五名无手术经验的工程师使用该系统执行玻璃体视网膜手术中的常见任务。", "result": "实验结果表明，在较高的缩放因子（20或30）下，“内部控制”方法整体表现最佳，但最佳缩放因子可能因任务和复杂性而异。", "conclusion": "优化控制方法和缩放因子有望提高未来机器人辅助眼内手术的效率和准确性，并降低风险。"}}
{"id": "2507.13371", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13371", "abs": "https://arxiv.org/abs/2507.13371", "authors": ["Yeming Cai", "Yang Wang", "Zhenglin Li"], "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation", "comment": null, "summary": "This paper proposes an end-to-end deep learning framework integrating optical\nmotion capture with a Transformer-based model to enhance medical\nrehabilitation. It tackles data noise and missing data caused by occlusion and\nenvironmental factors, while detecting abnormal movements in real time to\nensure patient safety. Utilizing temporal sequence modeling, our framework\ndenoises and completes motion capture data, improving robustness. Evaluations\non stroke and orthopedic rehabilitation datasets show superior performance in\ndata reconstruction and anomaly detection, providing a scalable, cost-effective\nsolution for remote rehabilitation with reduced on-site supervision.", "AI": {"tldr": "本文提出了一个端到端深度学习框架，结合光学动作捕捉和Transformer模型，用于医疗康复，旨在处理数据噪声、缺失并实时检测异常动作。", "motivation": "解决光学动作捕捉数据因遮挡和环境因素导致的噪声和数据缺失问题，同时实时检测异常动作以确保患者安全，从而提升医疗康复效果。", "method": "一个端到端的深度学习框架，整合了光学动作捕捉技术与基于Transformer的模型。该框架利用时间序列建模进行动作捕捉数据的去噪和补全。", "result": "在卒中和骨科康复数据集上的评估显示，该框架在数据重建和异常检测方面表现出色，为远程康复提供了一个可扩展、经济高效且能减少现场监督的解决方案。", "conclusion": "所提出的框架通过有效处理动作捕捉数据问题和实时异常检测，显著增强了医疗康复能力，为可扩展和经济高效的远程康复提供了有力支持。"}}
{"id": "2507.13395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13395", "abs": "https://arxiv.org/abs/2507.13395", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.", "AI": {"tldr": "Babel是一个新颖的框架，它利用单语语料库，通过一个风格检测器和基于扩散的风格应用器，作为神经机器翻译（NMT）的后处理模块，显著提高了翻译的风格保真度。", "motivation": "尽管神经机器翻译（NMT）彻底改变了跨语言交流，但保留文体细微差别仍然是一个重大挑战。现有方法通常需要平行语料库来实现风格保留，而这往往难以获取。", "method": "Babel框架仅使用单语语料库。它包含两个关键组件：1) 基于上下文嵌入的风格检测器，用于识别源文本和目标文本之间的风格差异；2) 基于扩散的风格应用器，用于纠正风格不一致性同时保持语义完整性。该框架作为后处理模块与现有NMT系统集成，无需架构修改或平行风格数据。", "result": "在法律、文学、科学写作、医学和教育内容五个不同领域进行的广泛实验表明，Babel能够以88.21%的精度识别风格不一致，并将风格保留度提高150%，同时保持0.92的高语义相似性分数。人工评估证实，经Babel改进的翻译能更好地保留源文本风格，同时保持流畅性和充分性。", "conclusion": "Babel框架通过其独特的风格检测和应用机制，有效解决了NMT中风格保留的挑战，且无需平行风格数据，显著提升了翻译的风格保真度，同时保持了语义完整性。"}}
{"id": "2507.13825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13825", "abs": "https://arxiv.org/abs/2507.13825", "authors": ["Haoyang Li", "Yuming Xu", "Yiming Li", "Hanmo Liu", "Darian Li", "Chen Jason Zhang", "Lei Chen", "Qing Li"], "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction", "comment": "Submitted in 2024. Accepted in 2025", "summary": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.", "AI": {"tldr": "EAGLE是一个轻量级框架，通过结合短期时间近邻和长期全局结构模式，显著提高了动态图时间链接预测的效率和效果，速度比现有模型快50倍以上。", "motivation": "现有的时间图神经网络（T-GNNs）在建模时间及结构依赖方面表现出色，但由于计算开销大，面临可扩展性和效率挑战。", "method": "本文提出了EAGLE框架，它包含一个时间感知模块（聚合最近邻居信息以反映即时偏好）和一个结构感知模块（利用时间个性化PageRank捕获全局重要节点的影响）。EAGLE采用自适应加权机制动态调整这两个模块的贡献，并避免了复杂的多跳消息传递或内存密集型机制，从而提高了效率。", "result": "在七个真实世界时间图上的大量实验表明，EAGLE在有效性和效率方面均持续超越了最先进的T-GNNs，并且比基于Transformer的T-GNNs提速超过50倍。", "conclusion": "EAGLE提供了一种高效且有效的解决方案，用于动态图中的时间链接预测，它通过整合短期时间近邻和长期全局结构模式，克服了现有T-GNNs的计算开销问题。"}}
{"id": "2507.13908", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13908", "abs": "https://arxiv.org/abs/2507.13908", "authors": ["Frederik Thiele", "Felix Biertümpfel", "Harald Pfifer"], "title": "A Robust Periodic Controller for Spacecraft Attitude Tracking", "comment": "Presented at European Control Conference 2025", "summary": "This paper presents a novel approach for robust periodic attitude control of\nsatellites. Respecting the periodicity of the satellite dynamics in the\nsynthesis allows to achieve constant performance and robustness requirements\nover the orbit. The proposed design follows a mixed sensitivity control design\nemploying a physically motivated weighting scheme. The controller is calculated\nusing a novel structured linear time-periodic output feedback synthesis with\nguaranteed optimal L2-performance. The synthesis poses a convex optimization\nproblem and avoids grid-wise evaluations of coupling conditions inherent for\nclassical periodic H-infinity-synthesis. Moreover, the controller has a\ntransparent and easy to implement structure. A solar power plant satellite is\nused to demonstrate the effectiveness of the proposed method for periodic\nsatellite attitude control.", "AI": {"tldr": "本文提出了一种新型鲁棒周期性卫星姿态控制方法，通过考虑系统周期性，实现了轨道上恒定的性能和鲁棒性。", "motivation": "研究动机在于通过在控制器设计中尊重卫星动力学的周期性，从而在整个轨道上实现恒定的性能和鲁棒性要求。", "method": "采用混合灵敏度控制设计，结合物理驱动的加权方案。控制器通过一种新型结构化线性时变输出反馈综合方法计算，保证最优L2性能。该综合问题是一个凸优化问题，避免了传统周期性H-infinity综合中固有的网格化耦合条件评估。", "result": "所提出的控制器结构透明且易于实现。通过一个太阳能电站卫星的案例，验证了该方法在周期性卫星姿态控制中的有效性，实现了轨道上恒定的性能和鲁棒性。", "conclusion": "该论文提出了一种有效且易于实现的鲁棒周期性卫星姿态控制方法，通过新颖的周期性输出反馈综合，确保了整个轨道上的恒定性能和鲁棒性。"}}
{"id": "2507.13915", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13915", "abs": "https://arxiv.org/abs/2507.13915", "authors": ["Huu-Phu Do", "Po-Chih Hu", "Hao-Chien Hsueh", "Che-Kai Liu", "Vu-Hoang Tran", "Ching-Chun Huang"], "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation", "comment": "Accepted by ACCV 2024", "summary": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.", "AI": {"tldr": "本研究提出一种新颖的盲超分辨率（BSR）策略，利用高分辨率（HR）参考图像来建立尺度感知的退化核，从而在不同超分辨率尺度下提升性能。", "motivation": "以往的盲超分辨率研究主要集中于直接从低分辨率（LR）输入估计退化核，但这些核应同时考虑退化过程和下采样因子。在不同超分辨率尺度下应用相同的退化核是不切实际的。", "method": "该研究引入一种策略，将退化核和缩放因子视为BSR任务的关键要素，并利用HR图像作为参考来建立尺度感知的退化核。模型通过内容无关的HR参考图像和目标LR图像自适应地识别退化过程，并通过下采样HR参考图像生成额外的LR-HR对以改进SR性能。此基于参考的训练程序适用于已训练的盲SR模型和零样本盲SR方法。", "result": "该方法在两种情况下（熟练训练的盲SR模型和零样本盲SR方法）均持续优于现有方法。", "conclusion": "结合模糊核和缩放因子的双重考虑，以及利用参考图像，是该方法在盲超分辨率任务中取得有效性的关键。"}}
{"id": "2507.13662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13662", "abs": "https://arxiv.org/abs/2507.13662", "authors": ["Jing Cheng", "Yasser G. Alqaham", "Zhenyu Gan", "Amit K. Sanyal"], "title": "Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion", "comment": null, "summary": "This paper presents a scalable and adaptive control framework for legged\nrobots that integrates Iterative Learning Control (ILC) with a biologically\ninspired torque library (TL), analogous to muscle memory. The proposed method\naddresses key challenges in robotic locomotion, including accurate trajectory\ntracking under unmodeled dynamics and external disturbances. By leveraging the\nrepetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the\nframework enhances accuracy and generalization across diverse locomotion\nscenarios. The control architecture is data-enabled, combining a physics-based\nmodel derived from hybrid-system trajectory optimization with real-time\nlearning to compensate for model uncertainties and external disturbances. A\ncentral contribution is the development of a generalized TL that stores learned\ncontrol profiles and enables rapid adaptation to changes in speed, terrain, and\ngravitational conditions-eliminating the need for repeated learning and\nsignificantly reducing online computation. The approach is validated on the\nbipedal robot Cassie and the quadrupedal robot A1 through extensive simulations\nand hardware experiments. Results demonstrate that the proposed framework\nreduces joint tracking errors by up to 85% within a few seconds and enables\nreliable execution of both periodic and nonperiodic gaits, including slope\ntraversal and terrain adaptation. Compared to state-of-the-art whole-body\ncontrollers, the learned skills eliminate the need for online computation\nduring execution and achieve control update rates exceeding 30x those of\nexisting methods. These findings highlight the effectiveness of integrating ILC\nwith torque memory as a highly data-efficient and practical solution for legged\nlocomotion in unstructured and dynamic environments.", "AI": {"tldr": "本文提出一种可扩展的自适应控制框架，将迭代学习控制（ILC）与受生物启发扭矩库（TL）结合，用于腿足机器人运动，显著提高轨迹跟踪精度和泛化能力，同时大幅减少在线计算。", "motivation": "腿足机器人面临未建模动力学和外部扰动下的精确轨迹跟踪挑战。现有方法在适应多样化运动场景时存在局限性，需要更高效、鲁棒的控制方案。", "method": "该方法将迭代学习控制（ILC）扩展到非周期性任务，并整合一个受肌肉记忆启发的扭矩库（TL）。控制架构是数据驱动的，结合了基于物理的模型和实时学习，以补偿模型不确定性和外部扰动。核心贡献是开发了一个广义扭矩库，用于存储学习到的控制配置文件，实现对速度、地形和重力条件的快速适应，避免重复学习并显著减少在线计算。", "result": "该框架在双足机器人Cassie和四足机器人A1上进行了仿真和硬件验证。结果显示，关节跟踪误差在几秒内减少高达85%，能够可靠执行周期性和非周期性步态，包括斜坡行走和地形适应。与现有最先进的全身控制器相比，学习到的技能消除了执行过程中的在线计算需求，控制更新速率比现有方法快30倍以上。", "conclusion": "将迭代学习控制与扭矩记忆相结合，是解决非结构化和动态环境中腿足机器人运动问题的有效、数据高效且实用的方案。"}}
{"id": "2507.13372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13372", "abs": "https://arxiv.org/abs/2507.13372", "authors": ["Yeming Cai", "Zhenglin Li", "Yang Wang"], "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks", "comment": null, "summary": "Breast cancer is a leading cause of death among women globally, and early\ndetection is critical for improving survival rates. This paper introduces an\ninnovative framework that integrates Vision Transformers (ViT) and Graph Neural\nNetworks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.\nOur framework leverages ViT's ability to capture global image features and\nGNN's strength in modeling structural relationships, achieving an accuracy of\n84.2%, outperforming traditional methods. Additionally, interpretable attention\nheatmaps provide insights into the model's decision-making process, aiding\nradiologists in clinical settings.", "AI": {"tldr": "本文提出了一种结合Vision Transformers (ViT)和图神经网络 (GNN)的创新框架，用于乳腺癌检测，在CBIS-DDSM数据集上达到了84.2%的准确率，并提供可解释的注意力热图。", "motivation": "乳腺癌是全球女性死亡的主要原因，早期检测对提高生存率至关重要。", "method": "该研究整合了ViT（用于捕获全局图像特征）和GNN（用于建模结构关系）来增强乳腺癌检测。使用了CBIS-DDSM数据集，并提供了可解释的注意力热图。", "result": "该框架实现了84.2%的准确率，优于传统方法。可解释的注意力热图为模型决策过程提供了洞察。", "conclusion": "结合ViT和GNN的框架显著提升了乳腺癌检测的性能，具有高准确性和可解释性，有助于放射科医生在临床环境中的应用。"}}
{"id": "2507.13410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13410", "abs": "https://arxiv.org/abs/2507.13410", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.", "AI": {"tldr": "本文研究通过修改大型多语言模型（LLMs）中的单个稀疏自编码器（SAE）特征，在零样本设置下实现生成语言的确定性控制，并成功保持语义一致性。", "motivation": "在零样本设置下，缺乏明确语言提示或微调的情况下，确定性地控制大型多语言模型（LLMs）的目标生成语言是一个基本挑战。", "method": "研究人员利用在Gemma-2B和Gemma-9B的残差流上预训练的稀疏自编码器（SAEs），识别出在英语和四种目标语言（中文、日语、西班牙语、法语）之间激活差异最大的特征。通过在Transformer层中修改单个SAE特征，在推理时引导模型生成语言。通过FastText进行语言分类评估成功率，并使用LaBSE（Language-Agnostic BERT Sentence Embedding）相似性评估语义保真度。", "result": "通过修改单个SAE特征，实现了高达90%的语言转换成功率，并保持了语义保真度。分析显示，语言引导在中到晚期的Transformer层中最有效，并由与语言敏感SAE特征不成比例相关的特定注意力头放大。", "conclusion": "稀疏特征引导是一种有前景、轻量级且可解释的机制，用于可控的多语言生成。"}}
{"id": "2507.13846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13846", "abs": "https://arxiv.org/abs/2507.13846", "authors": ["Kathrin Korte", "Christian Medeiros Adriano", "Sona Ghahremani", "Holger Giese"], "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.", "AI": {"tldr": "本文提出了一种因果知识迁移框架，使多智能体强化学习（MARL）在非平稳环境中能够零样本地适应新障碍，通过学习和共享碰撞恢复的因果宏操作，减少了对昂贵再训练的需求。", "motivation": "传统的多智能体强化学习知识迁移方法在非平稳环境中泛化能力差，且智能体适应环境变化时需要高成本的再训练。", "method": "该研究引入了一个因果知识迁移框架。将每次碰撞建模为因果干预，并实例化为一系列恢复动作（宏操作），这些宏操作代表了规避障碍并实现目标的因果知识。这些恢复宏操作通过第二个智能体在线传输，并以零样本方式应用，仅通过查询一个基于局部上下文信息（碰撞）的查找模型。", "result": "研究发现：1) 具有异构目标的智能体在适应新环境时，能够弥补随机探索与完全再训练策略之间约一半的差距；2) 因果知识迁移的效果取决于环境复杂度和智能体异构目标之间的相互作用。", "conclusion": "所提出的因果知识迁移框架能够有效提升多智能体在非平稳环境中的适应能力，尤其在面对异构目标和不同环境复杂度时，展现了其潜力。"}}
{"id": "2507.13931", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13931", "abs": "https://arxiv.org/abs/2507.13931", "authors": ["L. D. Couto", "K. Haghverdi", "F. Guo", "K. Trad", "G. Mulder"], "title": "Identifiability Analysis of a Pseudo-Two-Dimensional Model & Single Particle Model-Aided Parameter Estimation", "comment": "9 pages, 2 figures, This work has been presented at the 2025 American\n  Control Conference (ACC) and will appear in the conference proceedings.\n  \\c{opyright} 2025 IEEE", "summary": "This contribution presents a parameter identification methodology for the\naccurate and fast estimation of model parameters in a pseudo-two-dimensional\n(P2D) battery model. The methodology consists of three key elements. First, the\ndata for identification is inspected and specific features herein that need to\nbe captured are included in the model. Second, the P2D model is analyzed to\nassess the identifiability of the physical model parameters and propose\nalternative parameterizations that alleviate possible issues. Finally, diverse\noperating conditions are considered that excite distinct battery dynamics which\nallows the use of different low-order battery models accordingly. Results show\nthat, under low current conditions, the use of low-order models achieve\nparameter estimates at least 500 times faster than using the P2D model at the\nexpense of twice the error. However, if accuracy is a must, these estimated\nparameters can be used to initialize the P2D model and perform the\nidentification in half of the time.", "AI": {"tldr": "该研究提出了一种参数识别方法，用于快速准确地估计伪二维（P2D）电池模型中的参数，通过结合数据分析、模型可识别性评估和分层模型应用。", "motivation": "需要一种准确且快速的方法来估计伪二维（P2D）电池模型的参数。", "method": "该方法包含三个关键要素：1) 检查识别数据，确保模型能捕捉到关键特征；2) 分析P2D模型以评估物理参数的可识别性，并提出替代参数化方案以缓解潜在问题；3) 考虑不同的操作条件，根据激发的电池动力学使用不同的低阶电池模型。在低电流条件下使用低阶模型进行快速估计，然后用这些估计值初始化P2D模型进行高精度识别。", "result": "在低电流条件下，使用低阶模型识别参数的速度比直接使用P2D模型快至少500倍，但误差增加一倍。如果需要高精度，可以使用这些估计参数初始化P2D模型，将识别时间缩短一半。", "conclusion": "该参数识别方法能够在速度和精度之间取得平衡，通过在不同操作条件下利用低阶模型进行快速初步估计，并将其用于初始化P2D模型以实现高效且准确的参数识别。"}}
{"id": "2507.13974", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13974", "abs": "https://arxiv.org/abs/2507.13974", "authors": ["Jiaqi Lv", "Yijie Zhu", "Carmen Guadalupe Colin Tenorio", "Brinder Singh Chohan", "Mark Eastwood", "Shan E Ahmed Raza"], "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images", "comment": "Accepted by MIUA 2025", "summary": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.", "AI": {"tldr": "该研究提出了一种利用病理基础模型Virchow2作为特征提取器，并结合Efficient-UNet进行黑色素瘤H&E图像五种组织类型自动分割的深度学习网络，并在PUMA挑战赛中取得第一名。", "motivation": "黑色素瘤组织形态的准确表征对预后和治疗至关重要，但从H&E全切片图像中手动分割组织区域劳动密集且易受观察者间差异影响，因此需要可靠的自动化组织分割方法。", "method": "该方法提出了一种深度学习网络，利用在310万张组织病理图像上训练的病理基础模型Virchow2作为特征提取器，将提取的特征与原始RGB图像融合，然后通过编码器-解码器分割网络（Efficient-UNet）进行处理，以生成准确的五种组织类别分割图。", "result": "所提出的模型在PUMA大挑战的组织分割任务中获得第一名，展示了其强大的性能和泛化能力。", "conclusion": "研究结果表明，将病理基础模型整合到分割网络中具有潜力和有效性，可以加速计算病理学工作流程。"}}
{"id": "2507.13702", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13702", "abs": "https://arxiv.org/abs/2507.13702", "authors": ["Junho Choi", "Kihwan Ryoo", "Jeewon Kim", "Taeyun Kim", "Eungchang Lee", "Myeongwoo Jeong", "Kevin Christiansen Marsim", "Hyungtae Lim", "Hyun Myung"], "title": "SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization", "comment": "This paper has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Multi-robot localization is a crucial task for implementing multi-robot\nsystems. Numerous researchers have proposed optimization-based multi-robot\nlocalization methods that use camera, IMU, and UWB sensors. Nevertheless,\ncharacteristics of individual robot odometry estimates and distance\nmeasurements between robots used in the optimization are not sufficiently\nconsidered. In addition, previous researches were heavily influenced by the\nodometry accuracy that is estimated from individual robots. Consequently,\nlong-term drift error caused by error accumulation is potentially inevitable.\nIn this paper, we propose a novel visual-inertial-range-based multi-robot\nlocalization method, named SaWa-ML, which enables geometric structure-aware\npose correction and weight adaptation-based robust multi-robot localization.\nOur contributions are twofold: (i) we leverage UWB sensor data, whose range\nerror does not accumulate over time, to first estimate the relative positions\nbetween robots and then correct the positions of each robot, thus reducing\nlong-term drift errors, (ii) we design adaptive weights for robot pose\ncorrection by considering the characteristics of the sensor data and\nvisual-inertial odometry estimates. The proposed method has been validated in\nreal-world experiments, showing a substantial performance increase compared\nwith state-of-the-art algorithms.", "AI": {"tldr": "本文提出了一种名为 SaWa-ML 的新型多机器人定位方法，通过利用 UWB 传感器数据进行几何结构感知位姿校正和基于权重自适应的鲁棒定位，有效减少了长期漂移。", "motivation": "现有的多机器人定位方法未能充分考虑单个机器人里程计估计和机器人间距离测量的特性，且易受里程计精度影响，导致长期漂移误差积累。", "method": "本文提出 SaWa-ML 方法，该方法基于视觉-惯性-测距传感器数据。它首先利用 UWB 传感器数据（其测距误差不随时间累积）估计机器人间的相对位置，然后校正每个机器人的位姿，从而减少长期漂移误差。此外，该方法还设计了自适应权重，用于机器人位姿校正，同时考虑传感器数据和视觉-惯性里程计估计的特性。", "result": "所提出的方法已通过实际实验验证，与现有最先进的算法相比，性能显著提升。", "conclusion": "SaWa-ML 通过结合 UWB 传感器的非累积误差特性和自适应权重策略，成功实现了鲁棒的多机器人定位，有效解决了长期漂移问题，并显著提高了定位性能。"}}
{"id": "2507.13373", "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.13373", "abs": "https://arxiv.org/abs/2507.13373", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "AI": {"tldr": "Butter是一种新型目标检测框架，通过增强分层特征表示来提高自动驾驶场景下的检测鲁棒性，同时平衡精度和计算效率。", "motivation": "现有目标检测架构（如YOLO和DETR）在自动驾驶中难以在保持多尺度特征一致性的同时，平衡检测精度和计算效率。", "method": "Butter引入了两项创新：频率自适应特征一致性增强组件（FAFCE），利用自适应频率滤波细化多尺度特征一致性；渐进式分层特征融合网络模块（PHFFNet），逐步整合多级特征以弥合语义鸿沟并强化分层特征学习。", "result": "在BDD100K、KITTI和Cityscapes数据集上的实验表明，Butter展示了卓越的特征表示能力，显著提高了检测精度并降低了模型复杂度。", "conclusion": "Butter通过专注于分层特征的细化和整合，为目标检测提供了一种先进方法，在实时自动驾驶场景中实现了精度、可部署性和计算效率之间的平衡。"}}
{"id": "2507.13411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13411", "abs": "https://arxiv.org/abs/2507.13411", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.", "AI": {"tldr": "本文提出ALIGNed-LLM，一种通过将知识图谱（KG）信息注入大语言模型（LLM）的潜在空间来提高其事实准确性并减少幻觉的简单有效方法。", "motivation": "大语言模型在NLP任务中表现出色，但其易于产生幻觉是主要挑战。将知识图谱整合到语言模型中被认为是一种有前景的解决方案，因为它能提供结构化、可靠且领域特定的外部信息。", "method": "ALIGNed-LLM借鉴LLaVA的融合策略，通过精简的方式将知识图谱信息注入语言模型的潜在空间。具体方法是使用预训练的知识图谱嵌入（KGE）模型（如TransE）的嵌入，并通过一个可训练的投影层来对齐实体和文本嵌入，从而帮助语言模型区分相似实体。", "result": "该方法在三个流行的问答基准数据集上对不同大小的语言模型进行了测试，显示出显著的改进。此外，在欧洲一家大型中央银行的真实金融用例中，该方法也大幅提高了LLM答案的准确性和精确性。", "conclusion": "ALIGNed-LLM是一种简单而有效的方法，通过将知识图谱信息注入LLM的潜在空间，显著提高了语言模型的事实准确性并减少了幻觉。"}}
{"id": "2507.13874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13874", "abs": "https://arxiv.org/abs/2507.13874", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery", "comment": null, "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.", "AI": {"tldr": "本文提出一个与模型无关的潜在空间构思框架，通过在连续嵌入空间中导航，实现受控且可扩展的创意生成，以解决大型语言模型在生成新颖且相关想法方面的挑战。", "motivation": "大型语言模型（LLMs）在生成既新颖又相关的想法方面存在核心挑战，它们倾向于复制训练模式，缺乏创造性发散能力。现有解决方案（如领域特定启发式和结构化提示管道）脆弱且难以泛化。", "method": "本文提出一个与模型无关的潜在空间构思框架，通过导航想法的连续嵌入空间来实现受控、可扩展的创造力。该框架无需手动编写规则，易于适应不同的领域、输入格式和创意任务。目前是一个早期原型。", "result": "初步结果突出了该方法作为人机协作通用共同构思者的潜力。", "conclusion": "该框架提供了一种通用的、可扩展的、受控的创意生成方式，解决了LLMs在创新构思方面的局限性，并有望促进人机协作。"}}
{"id": "2507.13982", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13982", "abs": "https://arxiv.org/abs/2507.13982", "authors": ["Yanni Jiwan-Mercier", "Barış Dönmez", "Güneş Karabulut-Kurt", "Sébastien Loranger"], "title": "Diffraction and Scattering Modeling for Laser Power Beaming in Lunar Environment", "comment": "10 pages, 8 figures", "summary": "Reliable energy delivery is a critical requirement for\n  long-term lunar missions, particularly in regions with limited\n  solar access, such as polar craters and during extended lunar\n  nights. Optical Power Beaming (OPB) using high-power lasers\n  offers a promising alternative to conventional solar power, but\n  the effects of suspended lunar dust on beam propagation remain\n  poorly understood. This study introduces a detailed simulation\n  model that incorporates both diffraction and height-dependent\n  scattering by the electrostatically suspended lunar regolith. Un like prior\napproaches, which assumed uniform dust layers or\n  center-to-center transmission loss, our model uses generalized\n  diffraction theory and refractive index gradients derived from\n  particle density to assess beam deformation and attenuation. The\n  results show that even in ground-to-ground scenarios, lunar dust\n  significantly degrades energy transfer efficiency, dropping from\n  57% to 3.7% over 50 km in dust-free vs. dusty conditions with\n  175 nm particles. Increasing the particle size to 250 nm limits the\n  viable transmission range to below 30 km at 6% efficiency. The\n  study further demonstrates that raising the laser source height\n  can improve efficiency, achieving 91% for a distance of 5 km\n  and 25% at 50 km when the source is positioned 12 m above\n  ground. These findings underscore the importance of system\n  elevation and dust modeling in lunar OPB design and reveal\n  the mission-critical role of particle size distribution, especially in\n  environments disturbed by human activity.", "AI": {"tldr": "该研究通过详细的模拟模型，评估了月球尘埃对光学能量束传输（OPB）效率的影响，并提出通过提高激光源高度来缓解尘埃影响的方法。", "motivation": "月球任务，尤其是在太阳光照有限的区域（如极地陨石坑和漫长月夜），需要可靠的能源供应。光学能量束传输（OPB）是传统太阳能的替代方案，但月球悬浮尘埃对光束传播的影响尚不明确。", "method": "引入了一个详细的模拟模型，该模型结合了衍射和由静电悬浮月球风化层引起的与高度相关的散射。与以往假设均匀尘埃层的方法不同，该模型利用广义衍射理论和从粒子密度导出的折射率梯度来评估光束变形和衰减。", "result": "在地面到地面的情况下，月球尘埃显著降低了能量传输效率：在50公里距离下，无尘条件下的效率为57%，而有175纳米尘埃粒子时降至3.7%。将粒子尺寸增加到250纳米，有效传输范围限制在30公里以下，效率仅为6%。将激光源高度提高可以显著改善效率：当光源位于地面上方12米时，5公里距离的效率可达91%，50公里距离的效率可达25%。", "conclusion": "系统高度和准确的尘埃建模在月球OPB设计中至关重要，并揭示了粒子尺寸分布（尤其是在人类活动扰动的环境中）在任务中的关键作用。"}}
{"id": "2507.13993", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13993", "abs": "https://arxiv.org/abs/2507.13993", "authors": ["Ningyong Wu", "Jinzhi Wang", "Wenhong Zhao", "Chenzhan Yu", "Zhigang Xiu", "Duwei Dai"], "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models", "comment": null, "summary": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.", "AI": {"tldr": "OrthoInsight是一个多模态深度学习框架，用于自动诊断肋骨骨折并生成报告，结合了图像识别、知识图谱和大型语言模型。", "motivation": "医疗影像数据量不断增长，特别是肋骨骨折等肌肉骨骼损伤，需要自动化诊断工具，因为手动判读耗时且易出错。", "method": "OrthoInsight框架整合了：1) YOLOv9模型用于骨折检测；2) 医疗知识图谱用于检索临床背景；3) 微调的LLaVA语言模型用于生成诊断报告。它结合了CT图像的视觉特征和专家文本数据。", "result": "在28,675张带注释的CT图像和专家报告上进行评估，OrthoInsight在诊断准确性、内容完整性、逻辑连贯性和临床指导价值方面表现出色，平均得分4.28，优于GPT-4和Claude-3等模型。", "conclusion": "这项研究展示了多模态学习在医学图像分析中转化潜力和为放射科医生提供有效支持的能力。"}}
{"id": "2507.13729", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13729", "abs": "https://arxiv.org/abs/2507.13729", "authors": ["Yu Yao", "Salil Bhatnagar", "Markus Mazzola", "Vasileios Belagiannis", "Igor Gilitschenski", "Luigi Palmieri", "Simon Razniewski", "Marcel Hallgarten"], "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework", "comment": null, "summary": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.", "AI": {"tldr": "本文提出了一种基于LLM代理的框架，利用自然语言描述来增强真实世界的交通场景，以解决现有自动驾驶规划器测试方法中稀有关键场景生成效率低、控制力差的问题。", "motivation": "自动驾驶规划器在稀有但关键的场景中测试面临挑战。单纯依赖真实世界数据需要海量数据集；数据驱动模型生成场景缺乏细粒度控制且可能引入分布偏移；专家手动增强场景则无法满足规模化需求。", "method": "引入了一种新颖的基于LLM代理的框架，用于通过自然语言描述来增强真实世界的交通场景。其关键创新在于采用了“代理式设计”，这使得对输出具有细粒度控制，并能在使用较小、更具成本效益的LLM时也能保持高性能。", "result": "通过广泛的人工专家评估，结果表明该框架能够准确遵循用户意图，生成与手动创建场景质量相当的高质量增强场景。", "conclusion": "该LLM代理框架为自动驾驶系统的评估提供了一种可扩展且可控的交通场景增强方法，有效解决了现有方法的局限性，特别是在生成稀有且具有挑战性的场景方面。"}}
{"id": "2507.13374", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13374", "abs": "https://arxiv.org/abs/2507.13374", "authors": ["Kevin Dela Rosa"], "title": "Smart Routing for Multimodal Video Retrieval: When to Search What", "comment": "Accepted to ICCV 2025 Multimodal Representation and Retrieval\n  Workshop", "summary": "We introduce ModaRoute, an LLM-based intelligent routing system that\ndynamically selects optimal modalities for multimodal video retrieval. While\ndense text captions can achieve 75.9% Recall@5, they require expensive offline\nprocessing and miss critical visual information present in 34% of clips with\nscene text not captured by ASR. By analyzing query intent and predicting\ninformation needs, ModaRoute reduces computational overhead by 41% while\nachieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR\n(speech), OCR (text), and visual indices, averaging 1.78 modalities per query\nversus exhaustive 3.0 modality search. Evaluation on 1.8M video clips\ndemonstrates that intelligent routing provides a practical solution for scaling\nmultimodal retrieval systems, reducing infrastructure costs while maintaining\ncompetitive effectiveness for real-world deployment.", "AI": {"tldr": "ModaRoute是一个基于LLM的智能路由系统，用于多模态视频检索，它能动态选择最佳模态，显著降低计算开销并保持竞争力。", "motivation": "传统的密集文本字幕处理成本高昂且会遗漏关键视觉信息（如ASR未捕获的场景文本），因此需要一种更高效、更全面的多模态检索解决方案。", "method": "ModaRoute利用GPT-4.1分析查询意图并预测信息需求，动态地将查询路由到ASR（语音）、OCR（文本）和视觉索引等模态。平均每个查询使用1.78种模态，而非穷举式的3.0种模态搜索。", "result": "该系统将计算开销降低了41%，同时实现了60.9%的Recall@5。在180万个视频片段上的评估表明，它在保持竞争性有效性的同时，显著降低了基础设施成本。", "conclusion": "智能路由（ModaRoute）为扩展多模态检索系统提供了一个实用的解决方案，能够有效降低成本并保持在实际部署中的竞争力。"}}
{"id": "2507.13474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13474", "abs": "https://arxiv.org/abs/2507.13474", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack", "AI": {"tldr": "本文提出了一种名为“论文摘要攻击”（PSA）的新型越狱方法，利用大型语言模型（LLMs）对权威来源（如学术论文）的信任，通过构造包含恶意内容的论文摘要模板，实现了对多种LLMs的高成功率攻击。", "motivation": "先前的研究表明LLMs倾向于信任来自权威来源的信息，这可能引入新的安全漏洞。本文旨在验证这一可能性并基于此开发攻击方法。", "method": "提出“论文摘要攻击”（PSA）方法，该方法系统地综合攻击或防御导向的LLM安全论文内容，构建对抗性提示模板，并策略性地在预定义的小节中嵌入有害查询作为对抗性载荷。", "result": "PSA在基础LLMs和最先进的推理模型（如Deepseek-R1）上展现出显著的漏洞。在Claude3.5-Sonnet上实现了97%的攻击成功率（ASR），在Deepseek-R1上达到了98%的ASR。研究还发现，不同基础模型甚至同一模型的不同版本，在面对攻击导向或防御导向的论文时，表现出截然相反的漏洞偏向。", "conclusion": "PSA是一种有效的越狱方法，揭示了LLMs，包括先进推理模型，存在的显著安全漏洞。观察到的模型对不同类型论文的漏洞偏向为未来的对抗性方法和安全对齐研究提供了潜在线索。"}}
{"id": "2507.13956", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.", "AI": {"tldr": "本文提出了一种名为ADPC的视觉-语言因果干预框架，利用大语言模型处理的临床文本与MRI/fMRI图像，通过因果干预消除混杂因素，从而显著提升了认知正常、轻度认知障碍和阿尔茨海默病患者的诊断准确性。", "motivation": "阿尔茨海默病（AD）的诊断面临巨大挑战，主要源于多模态数据选择偏差导致的混杂因素以及变量间复杂的关系，这使得非因果模型易捕获虚假关联。轻度认知障碍（MCI）作为AD的前驱阶段，早期识别和干预对延缓疾病进展至关重要。", "method": "本文提出了一种名为“基于跨模态因果干预的阿尔茨海默病预测”（ADPC）的视觉-语言因果干预框架。该框架利用大语言模型（LLM）严格遵循模板总结临床数据，即使在不完整或分布不均的数据集下也能生成结构化文本输出。ADPC模型整合了磁共振成像（MRI）、功能性磁共振成像（fMRI）图像以及LLM生成的文本数据，对参与者进行认知正常（CN）、轻度认知障碍（MCI）和阿尔茨海默病（AD）分类。通过因果干预，该框架隐式地消除了神经影像伪影和年龄相关生物标志物等混杂因素。", "result": "实验结果表明，ADPC方法在区分CN/MCI/AD病例方面表现出色，在大多数评估指标上达到了最先进（SOTA）的性能。", "conclusion": "这项研究展示了将因果推理与多模态学习相结合在神经系统疾病诊断中的巨大潜力。"}}
{"id": "2507.14004", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14004", "abs": "https://arxiv.org/abs/2507.14004", "authors": ["Niloofar Nobahari", "Alireza Rezaee"], "title": "Smart fault detection in satellite electrical power system", "comment": null, "summary": "This paper presents an new approach for detecting in the electrical power\nsystem of satellites operating in Low Earth Orbit (LEO) without an Attitude\nDetermination and Control Subsystem (ADCS). Components of these systems are\nprone to faults, such as line-to-line faults in the photovoltaic subsystem,\nopen circuits, and short circuits in the DC-to-DC converter, as well as ground\nfaults in batteries. In the previous research has largely focused on detecting\nfaults in each components, such as photovoltaic arrays or converter systems,\ntherefore, has been limited attention given to whole electrical power system of\nsatellite as a whole system. Our approach addresses this gap by utilizing a\nMulti-Layer Perceptron (MLP) neural network model, which leverages input data\nsuch as solar radiation and surface temperature to predict current and load\noutputs. These machine learning techniques that classifiy use different\napproaches like Principal Component Analysis (PCA) and K-Nearest Neighbors\n(KNN), to classify faults effectively. The model presented achieves over 99%\naccuracy in identifying faults across multiple subsystems, marking a notable\nadvancement from previous approaches by offering a complete diagnostic solution\nfor the entire satellite power system. This thorough method boosts system\nreliability and helps lower the chances of mission failure", "AI": {"tldr": "本文提出了一种基于多层感知器（MLP）神经网络的新方法，用于检测低地球轨道（LEO）卫星（无姿态确定和控制子系统）整个电力系统中的故障，实现了超过99%的故障识别准确率。", "motivation": "现有研究主要集中于检测卫星电力系统各个组件（如光伏阵列或DC-DC转换器）的故障，而对整个电力系统作为一个整体的故障检测关注不足。卫星电力系统组件易发生故障，如光伏子系统中的线间故障、DC-DC转换器中的开路和短路，以及电池接地故障。", "method": "该方法利用多层感知器（MLP）神经网络模型，输入太阳辐射和表面温度等数据来预测电流和负载输出。故障分类则采用主成分分析（PCA）和K-近邻（KNN）等机器学习技术。", "result": "该模型在识别多个子系统故障方面实现了超过99%的准确率，相比以往方法取得了显著进步，为整个卫星电力系统提供了完整的诊断解决方案。", "conclusion": "这种全面的方法提高了系统可靠性，并有助于降低任务失败的几率，为卫星电力系统提供了彻底的故障检测方案。"}}
{"id": "2507.14046", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14046", "abs": "https://arxiv.org/abs/2507.14046", "authors": ["Hao Fang", "Hao Yu", "Sihao Teng", "Tao Zhang", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging", "comment": "11 pages, 9 figures", "summary": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.", "AI": {"tldr": "针对深度图像先验（DIP）等无监督学习方法在3D时间序列断层成像中计算成本高的问题，本文提出了Deep Dynamic Image Prior (D2IP) 框架，通过参数热启动、时间参数传播和轻量级网络显著加速并提升了3D时间序列电阻抗断层成像（tsEIT）的重建质量和效率。", "motivation": "无监督学习方法（如DIP）在断层成像中虽有潜力且无需训练数据，但其依赖大量网络参数迭代导致计算成本高昂，限制了其在复杂3D或时间序列断层成像任务中的实际应用。", "method": "本文提出了Deep Dynamic Image Prior (D2IP) 框架，引入了三项关键策略：1) 无监督参数热启动 (UPWS) 加速收敛；2) 时间参数传播 (TPP) 增强时间一致性；3) 定制的轻量级重建骨干网络 3D-FastResUNet 提升计算效率。", "result": "D2IP在模拟和临床肺部数据集上均实现了快速准确的3D时间序列电阻抗断层成像 (tsEIT) 重建。与现有基线相比，D2IP图像质量显著提升（平均MSSIM增加24.8%，ERR降低8.1%），同时计算时间大幅缩短（快7.1倍）。", "conclusion": "D2IP框架在速度和准确性方面表现出色，优于现有最先进的基线方法，在临床动态肺部成像中具有广阔的应用前景。"}}
{"id": "2507.13787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13787", "abs": "https://arxiv.org/abs/2507.13787", "authors": ["Doina Pisla", "Alexandru Pusca", "Andrei Caprariu", "Adrian Pisla", "Bogdan Gherman", "Calin Vaida", "Damien Chablat"], "title": "Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery", "comment": null, "summary": "This paper focuses on the design of a parallel robot designed for robotic\nassisted minimally invasive pancreatic surgery. Two alternative architectures,\ncalled ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are\nproposed. Their kinematic schemes are presented, and the conceptual 3D CAD\nmodels are illustrated. Based on these, two Finite Element Method (FEM)\nsimulations were performed to determine which architecture has the higher\nstiffness. A workspace quantitative analysis is performed to further assess the\nusability of the two proposed parallel architectures related to the medical\ntasks. The obtained results are used to select the architecture which fit the\nrequired design criteria and will be used to develop the experimental model of\nthe surgical robot.", "AI": {"tldr": "本文设计并比较了两种用于机器人辅助微创胰腺手术的四自由度并联机器人架构（ATHENA-1和ATHENA-2），通过有限元分析和工作空间分析，选出了最佳架构。", "motivation": "研究旨在设计一种适用于机器人辅助微创胰腺手术的并联机器人，以满足其特定的设计需求。", "method": "提出了两种并联机器人架构（ATHENA-1和ATHENA-2），展示了它们的运动学方案和概念性三维CAD模型。通过有限元方法（FEM）模拟评估了两种架构的刚度，并进行了工作空间定量分析以评估其在医疗任务中的可用性。", "result": "FEM模拟确定了两种架构中刚度更高的一种，工作空间分析评估了它们的可用性。这些结果被用于选择最符合设计标准的架构。", "conclusion": "根据获得的分析结果，选择了一种满足设计标准的架构，该架构将被用于开发外科机器人的实验模型。"}}
{"id": "2507.13378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13378", "abs": "https://arxiv.org/abs/2507.13378", "authors": ["Yuqi Cheng", "Yunkang Cao", "Haiming Yao", "Wei Luo", "Cheng Jiang", "Hui Zhang", "Weiming Shen"], "title": "A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects", "comment": "27 pages, 7 figures", "summary": "Industrial defect detection is vital for upholding product quality across\ncontemporary manufacturing systems. As the expectations for precision,\nautomation, and scalability intensify, conventional inspection approaches are\nincreasingly found wanting in addressing real-world demands. Notable progress\nin computer vision and deep learning has substantially bolstered defect\ndetection capabilities across both 2D and 3D modalities. A significant\ndevelopment has been the pivot from closed-set to open-set defect detection\nframeworks, which diminishes the necessity for extensive defect annotations and\nfacilitates the recognition of novel anomalies. Despite such strides, a\ncohesive and contemporary understanding of industrial defect detection remains\nelusive. Consequently, this survey delivers an in-depth analysis of both\nclosed-set and open-set defect detection strategies within 2D and 3D\nmodalities, charting their evolution in recent years and underscoring the\nrising prominence of open-set techniques. We distill critical challenges\ninherent in practical detection environments and illuminate emerging trends,\nthereby providing a current and comprehensive vista of this swiftly progressing\nfield.", "AI": {"tldr": "本综述深入分析了工业缺陷检测领域，涵盖2D和3D模态下的封闭集与开放集策略，追溯其演变，并强调开放集技术的日益重要性，同时提炼了实际挑战并展望了新兴趋势。", "motivation": "工业缺陷检测对产品质量至关重要，但传统方法已无法满足现代制造对精度、自动化和可扩展性的高要求。尽管计算机视觉和深度学习在2D和3D缺陷检测方面取得了显著进展（特别是从封闭集到开放集方法的转变），但目前仍缺乏对该领域连贯和全面的最新理解。", "method": "本文采用综述形式，深入分析了2D和3D模态下的封闭集与开放集缺陷检测策略。它梳理了这些技术近年来的发展演变，着重强调了开放集技术的日益突出地位。同时，综述还提炼了实际检测环境中固有的关键挑战，并阐明了新兴趋势。", "result": "深度学习显著提升了2D和3D缺陷检测能力。一个重要发展是从封闭集转向开放集缺陷检测框架，这减少了大量缺陷标注的需求，并促进了对新型异常的识别。开放集技术的重要性日益凸显。本综述还识别并总结了实际检测环境中的关键挑战和新兴趋势。", "conclusion": "本综述为快速发展的工业缺陷检测领域提供了当前且全面的视角。它强调了开放集技术在处理新颖异常和减少标注需求方面的优势，并指出了该领域面临的关键挑战和未来发展方向。"}}
{"id": "2507.13490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13490", "abs": "https://arxiv.org/abs/2507.13490", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）价值取向探测方法的鲁棒性和表达能力，发现现有方法对输入扰动敏感，且探测到的价值观与模型在特定情境下的行为关联性不强。", "motivation": "评估LLMs的价值取向至关重要，因为它影响用户体验，但现有研究存在挑战：多项选择题（MCQ）设置易受扰动影响，且缺乏对探测方法的系统比较；此外，不清楚探测到的价值观是否能捕捉上下文信息并反映模型对现实世界行为的偏好。", "method": "研究评估了三种广泛使用的价值探测策略的鲁棒性和表达能力。通过改变提示和选项进行输入扰动测试。引入了两个新任务：一是研究价值观是否对人口统计学上下文有响应，二是评估价值观与模型在价值相关场景中行为的一致性。", "result": "所有探测方法在输入扰动下都表现出很大的方差（即不鲁棒）。人口统计学上下文对自由文本生成的影响很小。模型探测到的价值观与它们对基于价值的行为的偏好只有微弱的相关性。", "conclusion": "研究强调需要更仔细地审查LLM价值探测方法，并认识到其局限性。"}}
{"id": "2507.13958", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13958", "abs": "https://arxiv.org/abs/2507.13958", "authors": ["Pedro Cabalar", "Martín Diéguez", "François Olivier", "Torsten Schaub", "Igor Stéphan"], "title": "Towards Constraint Temporal Answer Set Programming", "comment": null, "summary": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.", "AI": {"tldr": "本文提出了一种新颖的、基于时间与约束的Here-and-There逻辑扩展及其非单调平衡扩展，旨在解决ASP在处理高分辨率动态系统推理时的挑战。", "motivation": "逻辑编程方法（如ASP）在对具有细粒度时间与数值分辨率的动态系统进行推理时面临重大挑战。", "method": "通过结合线性时间Here-and-There逻辑（提供非单调时间推理能力）和带约束的Here-and-There逻辑（实现数值约束的直接集成），构建了一个表达力强的、基于时间与约束的Here-and-There逻辑扩展及其非单调平衡扩展。", "result": "该工作首次提出了专门为ASP量身定制的、结合约束的非单调时间推理方法。", "conclusion": "该研究为在ASP范式内处理高分辨率复杂动态系统建立了基础逻辑框架。"}}
{"id": "2507.14020", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14020", "abs": "https://arxiv.org/abs/2507.14020", "authors": ["Marwan Hassini", "Colette Mintsa-Eya", "Eduardo Redondo-Iglesias", "Pascal Venet"], "title": "Influence of Cell Position on the Capacity of Retired Batteries: Experimental and Statistical Studies", "comment": "5 pages, 4 figures, IECON 2025", "summary": "Understanding how batteries perform after automotive use is crucial to\ndetermining their potential for reuse. This article presents experimental\nresults aimed at advancing knowledge of retired battery performance. Three\nmodules extracted from electric vehicles were tested. Their performance was\nassessed, and the results were analyzed statistically using analysis of\nvariance (ANOVA). The 36 retired cells exhibited a high level of performance,\nalbeit with significant variation. On average, the cells had a 95% state of\nhealth capacity with a dispersion of 2.4%. ANOVA analysis suggests that cell\nperformance is not correlated with their position inside the module. These\nresults demonstrate the need to evaluate dispersion within retired batteries\nand to develop thermal management and balancing systems for second-life\nbatteries.", "AI": {"tldr": "本文研究了电动汽车退役电池的性能，发现其仍保持高容量（平均95%SOH），但存在显著差异，且性能与电池在模块中的位置无关，强调了评估离散度和开发热管理系统的必要性。", "motivation": "了解汽车使用后的电池性能对于确定其再利用潜力至关重要。", "method": "测试了三组从电动汽车中取出的电池模块，评估其性能，并使用方差分析（ANOVA）进行统计分析。", "result": "36个退役电池表现出高水平的性能，平均健康状态容量为95%，但存在2.4%的离散度。ANOVA分析表明，电池性能与其在模块内部的位置无关。", "conclusion": "研究结果表明，有必要评估退役电池内部的性能离散度，并为二次寿命电池开发热管理和平衡系统。"}}
{"id": "2507.14102", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14102", "abs": "https://arxiv.org/abs/2507.14102", "authors": ["Shravan Venkatraman", "Pavan Kumar S", "Rakesh Raj Madavan", "Chandrakala S"], "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography", "comment": "18 pages, 10 figures, 5 tables, 2025 ICCV Workshops", "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL", "AI": {"tldr": "UGPL是一种不确定性引导的渐进式学习框架，通过从全局到局部的分析，显著提高了CT图像在肾脏异常、肺癌和COVID-19检测中的分类准确性。", "motivation": "现有CT图像分类方法难以处理病理特征的细微性和空间多样性，且通常统一处理图像，限制了其检测需要聚焦分析的局部异常的能力。", "method": "UGPL框架采用不确定性引导的渐进式学习，首先识别诊断模糊区域，然后对这些关键区域进行详细检查。它利用证据深度学习量化预测不确定性，并通过非最大抑制机制引导信息补丁的提取，以保持空间多样性。该方法结合了渐进式细化策略和自适应融合机制，以整合上下文信息和精细细节。", "result": "UGPL在三个CT数据集上的表现持续优于现有最先进方法，在肾脏异常、肺癌和COVID-19检测中，准确率分别提高了3.29%、2.46%和8.08%。分析表明，不确定性引导组件提供了显著优势，当完整渐进式学习流程实施时，性能显著提升。", "conclusion": "UGPL通过其不确定性引导的全局到局部分析和渐进式学习策略，有效解决了CT图像中病理特征难以检测的问题，显著提高了诊断准确性。"}}
{"id": "2507.13871", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13871", "abs": "https://arxiv.org/abs/2507.13871", "authors": ["Mehul Anand", "Shishir Kolathaya"], "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models", "comment": "6 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2409.12616", "summary": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.", "AI": {"tldr": "本文提出一个半监督框架，利用在世界模型潜在空间中学习的控制障碍证书（CBCs），合成安全的视觉运动策略，以减少对大量标注数据的依赖。", "motivation": "从视觉数据合成安全控制器通常需要大量监督标注的安全关键数据，这在实际环境中往往不切实际。世界模型在潜在空间中的预测能力为可扩展和数据高效的安全控制提供了新途径。", "method": "引入了一个半监督框架，利用在世界模型潜在空间中学习的控制障碍证书（CBCs）来合成安全的视觉运动策略。该方法联合学习一个神经障碍函数和一个安全控制器，使用有限的标注数据，并利用现代视觉Transformer的预测能力进行潜在动力学建模。", "result": "该工作介绍了一种能够通过在潜在空间中学习CBCs来合成安全视觉运动策略的半监督框架，显著减少了对大量安全关键数据标注的需求，并利用了世界模型的预测能力。", "conclusion": "该框架通过结合潜在空间中的CBCs和世界模型，实现了在有限标注数据下合成安全的视觉运动策略，为数据高效且可扩展的安全控制提供了解决方案。"}}
{"id": "2507.13385", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13385", "abs": "https://arxiv.org/abs/2507.13385", "authors": ["Arjun Rao", "Esther Rolf"], "title": "Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery", "comment": "17 pages, 9 figures, 7 tables. Accepted to TerraBytes@ICML 2025", "summary": "A large variety of geospatial data layers is available around the world\nranging from remotely-sensed raster data like satellite imagery, digital\nelevation models, predicted land cover maps, and human-annotated data, to data\nderived from environmental sensors such as air temperature or wind speed data.\nA large majority of machine learning models trained on satellite imagery\n(SatML), however, are designed primarily for optical input modalities such as\nmulti-spectral satellite imagery. To better understand the value of using other\ninput modalities alongside optical imagery in supervised learning settings, we\ngenerate augmented versions of SatML benchmark tasks by appending additional\ngeographic data layers to datasets spanning classification, regression, and\nsegmentation. Using these augmented datasets, we find that fusing additional\ngeographic inputs with optical imagery can significantly improve SatML model\nperformance. Benefits are largest in settings where labeled data are limited\nand in geographic out-of-sample settings, suggesting that multi-modal inputs\nmay be especially valuable for data-efficiency and out-of-sample performance of\nSatML models. Surprisingly, we find that hard-coded fusion strategies\noutperform learned variants, with interesting implications for future work.", "AI": {"tldr": "研究发现，将光学卫星图像与多种地理空间数据融合可以显著提升机器学习模型的性能，尤其是在数据有限和跨区域泛化场景下，且简单的融合策略优于学习型策略。", "motivation": "大多数基于卫星图像的机器学习模型（SatML）主要依赖于光学输入。本研究旨在探究在监督学习环境中，将其他地理空间数据模态与光学图像结合使用的价值。", "method": "通过将额外的地理数据层（如DEM、土地覆盖图、环境传感器数据等）附加到现有的SatML基准任务数据集（涵盖分类、回归和分割）中，生成了增强版数据集。在此基础上，比较了不同融合策略（硬编码与学习型）的效果。", "result": "融合额外的地理输入与光学图像显著提升了SatML模型的性能。在标记数据有限和地理样本外（out-of-sample）设置中，这种益处最为显著。令人惊讶的是，硬编码的融合策略优于学习型策略。", "conclusion": "多模态输入对SatML模型的性能提升非常有价值，尤其有助于提高数据效率和样本外泛化能力。未来的研究可以考虑优先采用简单的硬编码融合策略。"}}
{"id": "2507.13501", "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "pdf": "https://arxiv.org/pdf/2507.13501", "abs": "https://arxiv.org/abs/2507.13501", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "title": "Encoding syntactic objects and Merge operations in function spaces", "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic.", "AI": {"tldr": "本文提出一种数学论证，展示了如何在函数空间中构建任意句法对象的忠实表示，并使其与句法结构（如Merge操作）兼容，从而为句法核心计算结构的神经计算实现提供了理论可能性。", "motivation": "研究动机在于提供一个数学论证，证明句法核心计算结构（特别是Merge操作）在神经计算层面实现的可能性，并阐明Merge与算术后继函数之间的相似性。", "method": "方法包括：将词汇项表示为函数（如小波）在函数空间中；利用第二Renyi熵构建一个可交换非结合半环结构；将函数集定义为作用于运算子上的代数，其中运算子操作模拟电路；通过余积和Hopf代数马尔可夫链，将Merge操作忠实地实现为对这些电路的操作；并提出了一个具体的实例，将Merge实现为正弦波的交叉频率相位同步。", "result": "结果表明，给定词汇项的函数表示，可以在同一函数空间中构建句法对象的忠实表示，该表示与岩浆结构兼容，并形成一个运算子上的代数。Merge操作可以通过对电路的操作忠实实现。这些结果提供了一个建设性论证，显示了句法核心计算结构神经计算实现的理论可能性。此外，Merge操作可以表示为半环的后继函数，从而阐明了其与算术后继函数的相似性。", "conclusion": "本文通过严谨的数学构建，证明了将句法结构（尤其是Merge操作）在神经计算层面实现的理论可行性，为理解大脑如何处理语法提供了新的视角，并澄清了Merge操作的数学本质。"}}
{"id": "2507.14032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14032", "abs": "https://arxiv.org/abs/2507.14032", "authors": ["Lam Nguyen", "Erika Barcelos", "Roger French", "Yinghui Wu"], "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models", "comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)", "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.", "AI": {"tldr": "KROMA是一个基于RAG和LLM的新型本体匹配框架，通过动态上下文增强和优化技术（如双相似性匹配和本体细化）显著提升了匹配性能，并降低了LLM调用开销。", "motivation": "现有本体匹配系统依赖手工规则或专业模型，导致适应性有限。", "method": "KROMA利用检索增强生成（RAG）管道中的大型语言模型（LLMs），动态地用结构、词汇和定义知识丰富本体匹配任务的语义上下文。为优化性能和效率，KROMA整合了基于双相似性的概念匹配和轻量级本体细化步骤，以修剪候选概念并显著减少调用LLMs的通信开销。", "result": "实验证明，KROMA通过知识检索和上下文增强的LLMs显著增强了本体匹配能力，优于经典的本体匹配系统和前沿的基于LLM的方法，同时保持了可比的通信开销。", "conclusion": "该研究强调了所提出的优化技术（定向知识检索、提示丰富和本体细化）对于大规模本体匹配的可行性和益处。"}}
{"id": "2507.14025", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14025", "abs": "https://arxiv.org/abs/2507.14025", "authors": ["Wataru Hashimoto", "Kazumune Hashimoto", "Masako Kishida", "Shigemasa Takai"], "title": "Reference-Free Iterative Learning Model Predictive Control with Neural Certificates", "comment": "This paper was submitted to IET Control Theory & Applications on May\n  19, 2025 (under review)", "summary": "In this paper, we propose a novel reference-free iterative learning model\npredictive control (MPC). In the proposed method, a certificate function based\non the concept of Control Lyapunov Barrier Function (CLBF) is learned using\ndata collected from past control executions and used to define the terminal set\nand cost in the MPC optimization problem at the current iteration. This scheme\nenables the progressive refinement of the MPC's terminal components over\nsuccessive iterations. Unlike existing methods that rely on mixed-integer\nprogramming and suffer from numerical difficulties, the proposed approach\nformulates the MPC optimization problem as a standard nonlinear program,\nenabling more efficient online computation. The proposed method satisfies key\nMPC properties, including recursive feasibility and asymptotic stability.\nAdditionally, we demonstrate that the performance cost is non-increasing with\nrespect to the number of iterations, under certain assumptions. Numerical\nexperiments including the simulation with PyBullet confirm that our control\nscheme iteratively enhances control performance and significantly improves\nonline computational efficiency compared to the existing methods.", "AI": {"tldr": "本文提出一种新型无参考迭代学习模型预测控制（MPC）方法，通过学习基于控制Lyapunov障碍函数（CLBF）的证书函数来迭代优化MPC的终端组件，从而提高性能和计算效率。", "motivation": "现有MPC方法依赖混合整数规划，导致数值困难和在线计算效率低下。", "method": "该方法提出一种无参考迭代学习MPC。它利用从过去控制执行中收集的数据学习一个基于控制Lyapunov障碍函数（CLBF）的证书函数，并用此函数定义当前迭代MPC优化问题中的终端集合和成本。MPC优化问题被表述为一个标准的非线性规划（NLP），而非混合整数规划。", "result": "所提出的方法能够逐步改进MPC的终端组件，实现更高效的在线计算。它满足递归可行性和渐近稳定性等关键MPC特性。在特定假设下，性能成本随迭代次数非递增。数值实验（包括PyBullet仿真）证实，该控制方案能迭代增强控制性能，并显著提高在线计算效率。", "conclusion": "该新型无参考迭代学习MPC方法通过迭代优化终端组件，显著提高了控制性能和在线计算效率，同时保持了MPC的关键特性。"}}
{"id": "2507.13387", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13387", "abs": "https://arxiv.org/abs/2507.13387", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction", "comment": "Accepted to ICCV Workshop 2025", "summary": "Accurate perception of the surrounding environment is essential for safe\nautonomous driving. 3D occupancy prediction, which estimates detailed 3D\nstructures of roads, buildings, and other objects, is particularly important\nfor vision-centric autonomous driving systems that do not rely on LiDAR\nsensors. However, in 3D semantic occupancy prediction -- where each voxel is\nassigned a semantic label -- annotated LiDAR point clouds are required, making\ndata acquisition costly. In contrast, large-scale binary occupancy data, which\nonly indicate occupied or free space without semantic labels, can be collected\nat a lower cost. Despite their availability, the potential of leveraging such\ndata remains unexplored. In this study, we investigate the utilization of\nlarge-scale binary occupancy data from two perspectives: (1) pre-training and\n(2) learning-based auto-labeling. We propose a novel binary occupancy-based\nframework that decomposes the prediction process into binary and semantic\noccupancy modules, enabling effective use of binary occupancy data. Our\nexperimental results demonstrate that the proposed framework outperforms\nexisting methods in both pre-training and auto-labeling tasks, highlighting its\neffectiveness in enhancing 3D semantic occupancy prediction. The code is\navailable at https://github.com/ToyotaInfoTech/b2s-occupancy", "AI": {"tldr": "本研究提出了一种新颖的框架，通过预训练和学习型自动标注两种方式，利用大规模二值占用数据来提升3D语义占用预测的性能，特别适用于不依赖激光雷达的视觉中心自动驾驶系统。", "motivation": "3D语义占用预测对自动驾驶至关重要，但其数据标注（需要激光雷达点云）成本高昂。虽然大规模二值占用数据（仅区分占用与否，无语义标签）获取成本较低且易得，但其潜在价值尚未被充分探索。", "method": "提出了一种基于二值占用的新型框架，将预测过程分解为二值占用模块和语义占用模块，从而有效利用二值占用数据。具体研究了两种利用方式：1) 预训练；2) 基于学习的自动标注。", "result": "实验结果表明，所提出的框架在预训练和自动标注任务中均优于现有方法。", "conclusion": "该框架通过有效利用大规模二值占用数据，显著增强了3D语义占用预测的能力。"}}
{"id": "2507.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13903", "abs": "https://arxiv.org/abs/2507.13903", "authors": ["Ziliang Li", "Hongming Chen", "Yiyang Lin", "Biyu Ye", "Ximin Lyu"], "title": "AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery", "comment": null, "summary": "Autonomous aerial systems play an increasingly vital role in a wide range of\napplications, particularly for transport and delivery tasks in complex\nenvironments. In airdrop missions, these platforms face the dual challenges of\nabrupt control mode switching and inherent system delays along with control\nerrors. To address these issues, this paper presents an autonomous airdrop\nsystem based on an aerial manipulator (AM). The introduction of additional\nactuated degrees of freedom enables active compensation for UAV tracking\nerrors. By imposing smooth and continuous constraints on the parabolic landing\npoint, the proposed approach generates aerial throwing trajectories that are\nless sensitive to the timing of payload release. A hierarchical disturbance\ncompensation strategy is incorporated into the Nonlinear Model Predictive\nControl (NMPC) framework to mitigate the effects of sudden changes in system\nparameters, while the predictive capabilities of NMPC are further exploited to\nimprove the precision of aerial throwing. Both simulation and real-world\nexperimental results demonstrate that the proposed system achieves greater\nagility and precision in airdrop missions.", "AI": {"tldr": "本文提出了一种基于空中机械手（AM）的自主空投系统，利用非线性模型预测控制（NMPC）和分层扰动补偿策略，解决空投任务中的控制模式切换、系统延迟和控制误差问题，显著提高了空投的敏捷性和精度。", "motivation": "自主空中系统在运输和递送任务中面临挑战，尤其是在复杂环境下的空投任务中，存在突然的控制模式切换、固有的系统延迟以及控制误差等问题，影响空投精度。", "method": "本研究引入了空中机械手（AM）以增加驱动自由度，主动补偿无人机跟踪误差。通过对抛物线着陆点施加平滑连续约束，生成对有效载荷释放时机不敏感的空中抛掷轨迹。同时，将分层扰动补偿策略整合到非线性模型预测控制（NMPC）框架中，以减轻系统参数突然变化的影响，并利用NMPC的预测能力提高空中抛掷的精度。", "result": "仿真和实际实验结果均表明，所提出的系统在空投任务中实现了更高的敏捷性和精度。", "conclusion": "所提出的基于空中机械手的自主空投系统，结合NMPC和分层扰动补偿策略，有效解决了空投任务中的关键挑战，显著提升了空投任务的性能。"}}
{"id": "2507.13386", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13386", "abs": "https://arxiv.org/abs/2507.13386", "authors": ["Yang Zhang", "Er Jin", "Yanfei Dong", "Yixuan Wu", "Philip Torr", "Ashkan Khakzar", "Johannes Stegmaier", "Kenji Kawaguchi"], "title": "Minimalist Concept Erasure in Generative Models", "comment": "ICML2025", "summary": "Recent advances in generative models have demonstrated remarkable\ncapabilities in producing high-quality images, but their reliance on\nlarge-scale unlabeled data has raised significant safety and copyright\nconcerns. Efforts to address these issues by erasing unwanted concepts have\nshown promise. However, many existing erasure methods involve excessive\nmodifications that compromise the overall utility of the model. In this work,\nwe address these issues by formulating a novel minimalist concept erasure\nobjective based \\emph{only} on the distributional distance of final generation\noutputs. Building on our formulation, we derive a tractable loss for\ndifferentiable optimization that leverages backpropagation through all\ngeneration steps in an end-to-end manner. We also conduct extensive analysis to\nshow theoretical connections with other models and methods. To improve the\nrobustness of the erasure, we incorporate neuron masking as an alternative to\nmodel fine-tuning. Empirical evaluations on state-of-the-art flow-matching\nmodels demonstrate that our method robustly erases concepts without degrading\noverall model performance, paving the way for safer and more responsible\ngenerative models.", "AI": {"tldr": "本文提出了一种基于生成输出分布距离的极简概念擦除方法，通过可微分优化和神经元掩蔽，在不损害模型整体性能的情况下，鲁棒地擦除生成模型中的不良概念。", "motivation": "当前的生成模型虽然能生成高质量图像，但依赖大量未标记数据，引发了安全和版权问题。现有擦除方法常导致模型过度修改，损害其整体实用性。", "method": "研究者提出了一个新颖的极简概念擦除目标，仅基于最终生成输出的分布距离。在此基础上，推导出一个可微分优化的可处理损失函数，实现端到端反向传播。为增强擦除的鲁棒性，引入了神经元掩蔽作为模型微调的替代方案。", "result": "在最先进的流匹配模型上的实证评估表明，该方法能够鲁棒地擦除概念，同时不降低模型的整体性能。", "conclusion": "该方法为开发更安全、更负责任的生成模型铺平了道路。"}}
{"id": "2507.13544", "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "pdf": "https://arxiv.org/pdf/2507.13544", "abs": "https://arxiv.org/abs/2507.13544", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.", "AI": {"tldr": "本文提出了一种用于构建和简化“准模式对话”会话图的计算框架，通过结合大型语言模型和图简化技术，显著提高了对话建模的语义清晰度和结构完整性。", "motivation": "随着大型语言模型（LLM）驱动的系统日益普及，分析会话动态变得越来越重要，特别是在处理用户与系统之间松散组织的对话时。", "method": "研究提出了一种新颖的计算框架，用于构建会话图以捕获松散组织对话的流和结构。引入了“Filter & Reconnect”方法，这是一种图简化技术，旨在最小化噪声同时保留语义连贯性和结构完整性。该方法结合了大型语言模型进行会话图的构建和简化。", "result": "通过比较分析，该方法结合大型语言模型和图简化技术后，语义度量S比现有方法提高了2.06倍，同时强制实现了0 δ-双曲性的树状结构，确保了会话建模的最佳清晰度。", "conclusion": "这项工作提供了一种分析大规模对话数据集的计算方法，在监控聊天机器人、对话管理工具和用户行为分析等自动化系统方面具有实际应用价值。"}}
{"id": "2507.14077", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14077", "abs": "https://arxiv.org/abs/2507.14077", "authors": ["Temiloluwa Prioleau", "Baiying Lu", "Yanjun Cui"], "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions", "comment": "19 pages, 3 figures, 6 tables", "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.", "AI": {"tldr": "该研究推出了Glucose-ML，一个包含10个公开糖尿病数据集的大型集合，旨在解决AI在糖尿病管理中数据获取的障碍，并提供了数据选择指南和血糖预测基准。", "motivation": "高质量大型数据集的缺乏阻碍了糖尿病管理领域中鲁棒AI解决方案的开发。", "method": "研究者收集了10个过去7年内发布的公开糖尿病数据集，组成了Glucose-ML集合，包含超过30万天的连续血糖监测(CGM)数据。他们进行了比较分析以指导数据选择，并以血糖预测为例进行了案例研究，为Glucose-ML中的所有数据集提供了短期血糖预测基准。", "result": "Glucose-ML集合包含了来自4个国家2500多人的3800万个血糖样本。案例研究表明，相同的算法在不同数据集上开发/评估时，预测结果可能显著不同。研究结果为开发鲁棒AI解决方案提供了建议。", "conclusion": "Glucose-ML数据集集合及其伴随的分析和基准测试为糖尿病及更广泛健康领域的AI研究提供了宝贵的资源，并强调了数据选择对AI模型性能的关键影响，为开发鲁棒AI解决方案提供了指导和建议。"}}
{"id": "2507.14052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14052", "abs": "https://arxiv.org/abs/2507.14052", "authors": ["Mingdao Lin", "Max Bolderman", "Mircea Lazar"], "title": "Physics-guided gated recurrent units for inversion-based feedforward control", "comment": "8 pages", "summary": "Inversion-based feedforward control relies on an accurate model that\ndescribes the inverse system dynamics. The gated recurrent unit (GRU), which is\na recent architecture in recurrent neural networks, is a strong candidate for\nobtaining such a model from data. However, due to their black-box nature, GRUs\nface challenges such as limited interpretability and vulnerability to\noverfitting. Recently, physics-guided neural networks (PGNNs) have been\nintroduced, which integrate the prior physical model structure into the\nprediction process. This approach not only improves training convergence, but\nalso facilitates the learning of a physics-based model. In this work, we\nintegrate a GRU in the PGNN framework to obtain a PG-GRU, based on which we\nadopt a two-step approach to feedforward control design. First, we adopt stable\ninversion techniques to design a stable linear model of the inverse dynamics.\nThen, a GRU trained on the residual is tailored to inverse system\nidentification. The resulting PG-GRU feedforward controller is validated by\nmeans of real-life experiments on a two-mass spring-damper system, where it\ndemonstrates roughly a two-fold improvement compared to the linear feedforward\nand a preview-based GRU feedforward in terms of the integral absolute error.", "AI": {"tldr": "本文提出了一种基于物理引导门控循环单元（PG-GRU）的前馈控制器设计方法，通过结合物理模型和数据驱动学习，显著提升了控制性能。", "motivation": "传统基于逆动力学的GRUs在前馈控制中存在可解释性差和过拟合问题。物理引导神经网络（PGNNs）能整合物理先验知识，提高训练收敛性和模型学习能力。因此，将GRUs与PGNNs结合，以克服其局限性。", "method": "研究将GRU整合到PGNN框架中，形成PG-GRU。采用两步法设计前馈控制器：首先，利用稳定逆方法设计线性逆动力学模型；其次，训练一个GRU来识别残差系统的逆动力学。最终的PG-GRU前馈控制器在双质量弹簧阻尼系统上进行验证。", "result": "在双质量弹簧阻尼系统上的实际实验表明，PG-GRU前馈控制器在积分绝对误差方面，比线性前馈控制器和基于预测的GRU前馈控制器提高了约两倍。", "conclusion": "所提出的PG-GRU前馈控制器通过结合物理模型结构和数据驱动的GRU学习，有效地解决了逆动力学建模中的挑战，并在实际系统中展现出显著优越的控制性能。"}}
{"id": "2507.13722", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13722", "abs": "https://arxiv.org/abs/2507.13722", "authors": ["Julia Laubmann", "Johannes Reschke"], "title": "Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box", "comment": null, "summary": "In today's digital age, concerns about the dangers of AI-generated images are\nincreasingly common. One powerful tool in this domain is StyleGAN (style-based\ngenerative adversarial networks), a generative adversarial network capable of\nproducing highly realistic synthetic faces. To gain a deeper understanding of\nhow such a model operates, this work focuses on analyzing the inner workings of\nStyleGAN's generator component. Key architectural elements and techniques, such\nas the Equalized Learning Rate, are explored in detail to shed light on the\nmodel's behavior. A StyleGAN model is trained using the PyTorch framework,\nenabling direct inspection of its learned weights. Through pruning, it is\nrevealed that a significant number of these weights can be removed without\ndrastically affecting the output, leading to reduced computational\nrequirements. Moreover, the role of the latent vector -- which heavily\ninfluences the appearance of the generated faces -- is closely examined. Global\nalterations to this vector primarily affect aspects like color tones, while\ntargeted changes to individual dimensions allow for precise manipulation of\nspecific facial features. This ability to finetune visual traits is not only of\nacademic interest but also highlights a serious ethical concern: the potential\nmisuse of such technology. Malicious actors could exploit this capability to\nfabricate convincing fake identities, posing significant risks in the context\nof digital deception and cybercrime.", "AI": {"tldr": "该研究分析了StyleGAN生成器的工作原理，发现大量权重可被剪枝以降低计算需求，并揭示了隐向量如何影响生成图像的特征，强调了其潜在的伦理风险。", "motivation": "在数字时代，人们对AI生成图像的危险日益担忧。为了深入理解StyleGAN这类能生成高度逼真合成人脸的模型如何运作，并揭示其潜在的滥用风险，本研究旨在分析其内部机制。", "method": "研究详细探讨了StyleGAN生成器的关键架构元素和技术（如均衡学习率）。模型使用PyTorch框架进行训练，以便直接检查其学习到的权重。通过剪枝技术评估了权重的冗余性。此外，还深入检查了隐向量的作用，包括全局和针对性的修改如何影响生成人脸的特征。", "result": "研究发现，StyleGAN模型的大量权重可以在不显著影响输出质量的情况下被移除，从而降低了计算要求。对隐向量的分析表明，全局修改主要影响颜色色调等整体方面，而对单个维度的针对性修改则能实现对特定面部特征的精确操控。", "conclusion": "StyleGAN对视觉特征的精细调整能力不仅具有学术价值，更凸显了严重的伦理问题：恶意行为者可能利用此技术伪造逼真的虚假身份，从而在数字欺骗和网络犯罪领域构成重大风险。"}}
{"id": "2507.13940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13940", "abs": "https://arxiv.org/abs/2507.13940", "authors": ["Qingyi Chen", "Ahmed H. Qureshi"], "title": "NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning", "comment": null, "summary": "Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in\nrobotics. Despite substantial advancements, existing methods often face a\ndilemma. Decentralized algorithms typically rely on predicting the behavior of\nother agents, sharing contracts, or maintaining communication for safety, while\ncentralized approaches struggle with scalability and real-time decision-making.\nTo address these challenges, we introduce Neural Hamilton-Jacobi Reachability\nLearning (HJR) for Decentralized Multi-Agent Motion Planning. Our method\nprovides scalable neural HJR modeling to tackle high-dimensional configuration\nspaces and capture worst-case collision and safety constraints between agents.\nWe further propose a decentralized trajectory optimization framework that\nincorporates the learned HJR solutions to solve MAMP tasks in real-time. We\ndemonstrate that our method is both scalable and data-efficient, enabling the\nsolution of MAMP problems in higher-dimensional scenarios with complex\ncollision constraints. Our approach generalizes across various dynamical\nsystems, including a 12-dimensional dual-arm setup, and outperforms a range of\nstate-of-the-art techniques in successfully addressing challenging MAMP tasks.\nVideo demonstrations are available at https://youtu.be/IZiePX0p1Mc.", "AI": {"tldr": "该论文提出了一种名为“神经哈密顿-雅可比可达性学习”（Neural HJR）的新方法，用于去中心化多智能体运动规划（MAMP），以解决现有方法在可扩展性和实时性方面的挑战，并确保复杂高维场景下的安全性。", "motivation": "现有的多智能体运动规划（MAMP）方法面临困境：去中心化算法依赖行为预测、协议或通信来确保安全，而中心化方法则受限于可扩展性和实时决策能力。", "method": "作者引入了用于去中心化MAMP的神经哈密顿-雅可比可达性学习（Neural HJR），该方法提供可扩展的神经HJR建模，以处理高维配置空间并捕获最坏情况下的碰撞和智能体间的安全约束。此外，还提出了一个去中心化轨迹优化框架，将学习到的HJR解融入其中以实时解决MAMP任务。", "result": "该方法在解决高维复杂碰撞约束下的MAMP问题时，表现出良好的可扩展性和数据效率。它能泛化应用于各种动力系统，包括12维双臂设置，并在成功解决具有挑战性的MAMP任务方面超越了一系列现有技术。", "conclusion": "该研究成功开发了一种可扩展、数据高效的去中心化多智能体运动规划方法，能够处理复杂的高维场景，并有效解决具有挑战性的MAMP任务，同时确保安全性。"}}
{"id": "2507.13397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13397", "abs": "https://arxiv.org/abs/2507.13397", "authors": ["Kaiyuan Zhai", "Juan Chen", "Chao Wang", "Zeyi Xu"], "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction", "comment": null, "summary": "Accurate pedestrian trajectory prediction is crucial for intelligent\napplications, yet it remains highly challenging due to the complexity of\ninteractions among pedestrians. Previous methods have primarily relied on\nrelative positions to model pedestrian interactions; however, they tend to\noverlook specific interaction patterns such as paired walking or conflicting\nbehaviors, limiting the prediction accuracy in crowded scenarios. To address\nthis issue, we propose InSyn (Interaction-Synchronization Network), a novel\nTransformer-based model that explicitly captures diverse interaction patterns\n(e.g., walking in sync or conflicting) while effectively modeling\ndirection-sensitive social behaviors. Additionally, we introduce a training\nstrategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue\nof initial-step divergence in numerical time-series prediction. Experiments on\nthe ETH and UCY datasets demonstrate that our model outperforms recent\nbaselines significantly, especially in high-density scenarios. Furthermore, the\nSSOS strategy proves effective in improving sequential prediction performance,\nreducing the initial-step prediction error by approximately 6.58%.", "AI": {"tldr": "本文提出InSyn模型和SSOS训练策略，用于改进行人轨迹预测，尤其在拥挤场景下，通过捕捉多样化交互模式和解决初始步发散问题，显著提升预测精度。", "motivation": "现有行人轨迹预测方法主要依赖相对位置建模交互，但忽略了特定交互模式（如结伴行走或冲突行为），导致在拥挤场景下预测准确性受限。", "method": "提出InSyn（Interaction-Synchronization Network），一个基于Transformer的模型，显式捕捉多样化交互模式（如同步行走或冲突），并有效建模方向敏感的社会行为。此外，引入SSOS（Seq-Start of Seq）训练策略，旨在缓解数值时间序列预测中常见的初始步发散问题。", "result": "模型在ETH和UCY数据集上显著优于现有基线，尤其在高密度场景下表现更佳。SSOS策略有效提升了序列预测性能，将初始步预测误差降低了约6.58%。", "conclusion": "InSyn模型通过捕捉多样化交互模式和方向敏感行为，结合SSOS训练策略解决初始步发散问题，显著提高了行人轨迹预测的准确性，尤其在拥挤环境中表现出色。"}}
{"id": "2507.13551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13551", "abs": "https://arxiv.org/abs/2507.13551", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.", "AI": {"tldr": "本研究利用自动语音识别（ASR）技术，将语音停顿特征与语义连贯性指标相结合，显著提升了对精神分裂症谱系障碍中形式思维障碍（FTD）严重程度的自动化预测能力，为临床评估提供了可扩展的解决方案。", "motivation": "传统的FTD临床评估方法资源密集且难以扩展。自动语音分析（ASR）提供了一种客观量化语音特征的手段，特别是ASR导出的停顿时间戳能反映认知过程。然而，将这些ASR衍生的特征，尤其是停顿特征，与语义连贯性指标结合用于评估FTD严重程度的效用，仍需进一步评估。", "method": "研究整合了ASR提取的停顿特征与语义连贯性指标，并在三个不同数据集上进行评估：自然语境下的自我记录日记（AVH, n=140）、结构化图片描述（TOPSY, n=72）和梦境叙述（PsyCL, n=43）。使用支持向量回归（SVR）模型预测临床FTD评分，并比较了仅使用语义特征、仅使用停顿特征以及整合两者的模型性能。", "result": "关键发现表明，单独的停顿特征能够稳健地预测FTD的严重程度。与仅使用语义特征的模型相比，整合停顿特征和语义连贯性指标显著提升了预测性能。在TOPSY数据集上，整合模型在严重病例检测中的相关性最高达到ρ = 0.649，AUC为83.71%，优于仅使用语义特征的模型（ρ = 0.584，AUC = 79.23%）。性能提升在所有语境中均保持一致，尽管停顿模式的性质因数据集而异。", "conclusion": "结合时间（停顿）和语义分析的框架为完善紊乱言语评估提供了路线图，并推动了精神病学中自动化语音分析的进步。"}}
{"id": "2507.14097", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.", "AI": {"tldr": "本研究引入了生成式AI驱动的人体运动模拟（G-AI-HMS），通过整合文本到文本和文本到运动模型，显著提高了工业任务中人体运动模拟的保真度。", "motivation": "现有的人体运动模拟（HMS）方法在运动保真度方面存在不足，限制了其在工业任务中对工人行为、安全和生产力进行经济高效评估的能力。", "method": "本研究提出了G-AI-HMS，其核心在于整合文本到文本模型（如与MotionGPT训练词汇对齐的大型语言模型，用于将任务描述转化为运动感知语言）和文本到运动模型。此外，通过计算机视觉技术（姿态估计算法从实时视频中提取关节地标，并使用运动相似性度量）对AI增强的运动进行验证，与真实人体运动进行比较。", "result": "在涉及八项任务的案例研究中，AI增强的运动在大多数场景下显示出比人工创建描述更低的误差。具体表现为：在六项任务中空间准确性更佳，在四项任务中姿态归一化后对齐度更好，在七项任务中整体时间相似性更优。统计分析表明，AI增强的提示显著（p < 0.0001）减少了关节误差和时间错位，同时保持了可比的姿态准确性。", "conclusion": "G-AI-HMS通过利用生成式AI显著提升了人体运动模拟的质量和准确性，为工业任务中工人行为的评估提供了更可靠的工具。"}}
{"id": "2507.14073", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14073", "abs": "https://arxiv.org/abs/2507.14073", "authors": ["Oumayma Khattabi", "Matteo Tacchi-Bénard", "Sorin Olaru"], "title": "Convex computation of regions of attraction from data using Sums-of-Squares programming", "comment": null, "summary": "The paper concentrates on the analysis of the region of attraction (ROA) for\nunknown autonomous dynamical systems. The aim is to explore a data-driven\napproach based on moment-sum-of-squares (SoS) hierarchy, which enables novel\nRoA outer approximations despite the reduced information on the structure of\nthe dynamics. The main contribution of this work is bypassing the system model\nand, consequently, the recurring constraint on its polynomial structure.\nNumerical experimentation showcases the influence of data on learned\napproximating sets, offering a promising outlook on the potential of this\nmethod.", "AI": {"tldr": "本文提出一种基于数据驱动的矩-平方和层次方法，用于分析未知自治动力系统的吸引域(ROA)，并实现其外部近似。", "motivation": "现有吸引域分析方法通常依赖于已知的系统模型及其多项式结构。本文旨在为未知动力系统探索一种新的数据驱动方法，以规避对系统模型和其多项式结构的依赖。", "method": "该研究采用基于矩-平方和(SoS)层次结构的数据驱动方法。其核心贡献在于绕过对系统模型的直接需求，从而摆脱了系统必须具有多项式结构的限制，以实现吸引域的外部近似。", "result": "数值实验结果表明，数据对所学习到的近似集合有显著影响，验证了该方法的有效性和潜力。", "conclusion": "所提出的数据驱动方法为分析未知动力系统的吸引域提供了一个有前景的新途径，尤其适用于系统结构信息有限的情况。"}}
{"id": "2507.13852", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13852", "abs": "https://arxiv.org/abs/2507.13852", "authors": ["Luigi Russo", "Francesco Mauro", "Babak Memar", "Alessandro Sebastianelli", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data", "comment": "Accepted at IEEE Joint Urban Remote Sensing Event (JURSE) 2025", "summary": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.", "AI": {"tldr": "本研究利用量子卷积（Quanvolutional）预处理技术增强注意力U-Net模型，用于城市区域的建筑物分割，在保持准确性的同时显著减少了模型参数。", "motivation": "在城市规划、灾害响应和人口制图等领域，城市建筑物的分割至关重要。然而，由于卫星图像尺寸大、分辨率高，在密集的城市区域准确分割建筑物面临挑战。", "method": "本研究探讨了使用量子卷积预处理来增强注意力U-Net模型在建筑物分割中的能力。具体地，利用Sentinel-1合成孔径雷达（SAR）图像，并应用量子卷积从雷达图像中提取更具信息量的特征图，以捕获重要的结构细节。", "result": "初步结果表明，所提出的方法在测试准确性上与标准注意力U-Net模型相当，但显著减少了网络参数。这与以往的研究结果一致，证实了量子卷积不仅能保持模型准确性，还能提高计算效率。", "conclusion": "这些有前景的结果突出了量子辅助深度学习框架在城市环境中进行大规模建筑物分割的潜力。"}}
{"id": "2507.13969", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13969", "abs": "https://arxiv.org/abs/2507.13969", "authors": ["Maria Eduarda Silva de Macedo", "Ana Paula Chiarelli de Souza", "Roberto Silvio Ubertino Rosso Jr.", "Yuri Kaszubowski Lopes"], "title": "A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios", "comment": "7 pages total (6 pages of content + 1 page of references). Short\n  paper manuscript submitted to TAROS 2025", "summary": "The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.", "AI": {"tldr": "本文提出了一种基于视线传感器的多任务自聚合行为，使同构机器人群能够自主地形成紧凑的集群，解决了现有方法形成非紧凑或非完全自主队形的问题。", "motivation": "现有的多任务自聚合方法在生成队形时存在问题，例如形成非完全紧凑的圆形队形，或者不是完全自主的。此外，不同群体动态如何相互影响也是一个挑战。", "method": "该研究提出了一种多任务自聚合行为，其中同构机器人群仅依靠视线传感器将自身分类成不同的紧凑集群。", "result": "所提出的多任务自聚合行为具有良好的可扩展性，并能实现紧凑的队形。通过一系列不同配置的模拟试验，验证了其可扩展性，并显著提高了集群的紧凑性，同时保持了与其他研究中相似的机器人聚集比例。", "conclusion": "该研究成功开发了一种可扩展、自主且能形成紧凑集群的多任务自聚合行为，有效提升了群体机器人自聚合的性能和队形质量。"}}
{"id": "2507.13401", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13401", "abs": "https://arxiv.org/abs/2507.13401", "authors": ["Shreya Kadambi", "Risheek Garrepalli", "Shubhankar Borse", "Munawar Hyatt", "Fatih Porikli"], "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing", "comment": "26 pages", "summary": "Despite the remarkable success of diffusion models in text-to-image\ngeneration, their effectiveness in grounded visual editing and compositional\ncontrol remains challenging. Motivated by advances in self-supervised learning\nand in-context generative modeling, we propose a series of simple yet powerful\ndesign choices that significantly enhance diffusion model capacity for\nstructured, controllable generation and editing. We introduce Masking-Augmented\nDiffusion with Inference-Time Scaling (MADI), a framework that improves the\neditability, compositionality and controllability of diffusion models through\ntwo core innovations. First, we introduce Masking-Augmented gaussian Diffusion\n(MAgD), a novel training strategy with dual corruption process which combines\nstandard denoising score matching and masked reconstruction by masking noisy\ninput from forward process. MAgD encourages the model to learn discriminative\nand compositional visual representations, thus enabling localized and\nstructure-aware editing. Second, we introduce an inference-time capacity\nscaling mechanism based on Pause Tokens, which act as special placeholders\ninserted into the prompt for increasing computational capacity at inference\ntime. Our findings show that adopting expressive and dense prompts during\ntraining further enhances performance, particularly for MAgD. Together, these\ncontributions in MADI substantially enhance the editability of diffusion\nmodels, paving the way toward their integration into more general-purpose,\nin-context generative diffusion architectures.", "AI": {"tldr": "该论文提出MADI框架，通过双重破坏训练策略（MAgD）和推理时容量扩展（Pause Tokens），显著提升了扩散模型在接地视觉编辑和组合控制方面的能力。", "motivation": "尽管扩散模型在文本到图像生成方面取得了显著成功，但在接地视觉编辑和组合控制方面仍面临挑战。研究动机来源于自监督学习和上下文生成建模的进展。", "method": "该研究引入了MADI（Masking-Augmented Diffusion with Inference-Time Scaling）框架。核心方法包括：1) MAgD（Masking-Augmented gaussian Diffusion）：一种新型训练策略，采用双重破坏过程，结合标准去噪分数匹配和通过掩码噪声输入进行的重建，以学习判别性和组合性视觉表示。2) 推理时容量扩展机制：通过在提示中插入“Pause Tokens”作为特殊占位符，增加推理时的计算容量。此外，研究发现训练期间使用表达性强和密集的提示能进一步提升性能，尤其对MAgD。", "result": "MADI框架显著增强了扩散模型的编辑性、组合性和可控性。MAgD方法使得模型能够进行局部和结构感知的编辑。整体贡献为将扩散模型整合到更通用、上下文生成扩散架构铺平了道路。", "conclusion": "MADI框架通过其创新设计（MAgD训练策略和推理时容量扩展）大幅提升了扩散模型的编辑能力，使其更适用于通用、上下文生成任务。"}}
{"id": "2507.13563", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13563", "abs": "https://arxiv.org/abs/2507.13563", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations.", "AI": {"tldr": "本文介绍了Balalaika数据集，一个包含2000多小时高质量俄语语音的新型数据集，旨在解决俄语语音合成中的独特挑战，并实验证明其在语音合成和增强任务中优于现有数据集。", "motivation": "俄语语音合成面临特有的挑战，如元音弱化、辅音清化、可变重音模式、同形异义词歧义和不自然的语调。现有数据集不足以有效应对这些挑战。", "method": "本文提出了Balalaika数据集，包含超过2000小时的录音室质量俄语语音，并附带全面的文本标注，包括标点和重音标记。论文详细介绍了数据集的构建流程和标注方法。", "result": "实验结果表明，在Balalaika数据集上训练的模型在语音合成和增强任务中均显著优于在现有数据集上训练的模型。", "conclusion": "Balalaika数据集的引入显著提升了俄语语音合成和增强的性能，为解决俄语语音的独特挑战提供了高质量的资源。"}}
{"id": "2507.14107", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14107", "abs": "https://arxiv.org/abs/2507.14107", "authors": ["Viraj Nishesh Darji", "Callie C. Liao", "Duoduo Liao"], "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment", "comment": null, "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.", "AI": {"tldr": "本研究探索了大型语言模型（LLMs）在解释无损检测（NDE）等高线图方面的能力，以自动化和改进桥梁状况分析，结果表明LLMs能显著提高效率和准确性。", "motivation": "桥梁无损检测（NDE）数据的解释耗时且需要专业知识，可能延误决策。研究旨在利用LLMs自动化并改进这一过程。", "method": "本试点研究评估了多种LLMs解释NDE等高线图的能力。通过设计特定提示词来增强图像描述质量，并将其应用于五种不同的NDE等高线图。评估标准包括详细描述能力、缺陷识别、提供可行建议和整体准确性。表现较好的LLMs的输出再由其他LLMs进行总结。", "result": "在九个模型中，有四个模型提供了更好的图像描述，有效覆盖了桥梁状况的广泛主题。其中，ChatGPT-4和Claude 3.5 Sonnet在生成综合性总结方面表现更佳。研究结果表明LLMs能显著提高效率和准确性。", "conclusion": "LLMs在图像描述和总结方面的应用，能够加速桥梁维护决策，提升基础设施管理和安全评估效率，具有巨大潜力。"}}
{"id": "2507.14117", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14117", "abs": "https://arxiv.org/abs/2507.14117", "authors": ["Aayushya Agarwal", "Larry Pileggi"], "title": "Integrating Forecasting Models Within Steady-State Analysis and Optimization", "comment": null, "summary": "Extreme weather variations and the increasing unpredictability of load\nbehavior make it difficult to determine power grid dispatches that are robust\nto uncertainties. While machine learning (ML) methods have improved the ability\nto model uncertainty caused by loads and renewables, accurately integrating\nthese forecasts and their sensitivities into steady-state analyses and\ndecision-making strategies remains an open challenge. Toward this goal, we\npresent a generalized methodology that seamlessly embeds ML-based forecasting\nengines within physics-based power flow and grid optimization tools. By\ncoupling physics-based grid modeling with black-box ML methods, we accurately\ncapture the behavior and sensitivity of loads and weather events by directly\nintegrating the inputs and outputs of trained ML forecasting models into the\nnumerical methods of power flow and grid optimization. Without fitting\nsurrogate load models, our approach obtains the sensitivities directly from\ndata to accurately predict the response of forecasted devices to changes in the\ngrid. Our approach combines the sensitivities of forecasted devices attained\nvia backpropagation and the sensitivities of physics-defined grid devices. We\ndemonstrate the efficacy of our method by showcasing improvements in\nsensitivity calculations and leveraging them to design a robust power dispatch\nthat improves grid reliability under stochastic weather events. Our approach\nenables the computation of system sensitivities to exogenous factors which\nsupports broader analyses that improve grid reliability in the presence of load\nvariability and extreme weather conditions.", "AI": {"tldr": "该研究提出了一种通用方法，将机器学习（ML）预测引擎无缝嵌入到基于物理的电力潮流和电网优化工具中，以提高电网在不确定性条件下的鲁棒性和可靠性。", "motivation": "极端天气变化和负荷行为的不可预测性使得确定对不确定性具有鲁棒性的电网调度变得困难。尽管ML方法提高了对负荷和可再生能源引起的不确定性建模能力，但如何将这些预测及其敏感性准确地整合到稳态分析和决策策略中仍然是一个开放的挑战。", "method": "该方法将ML预测引擎直接嵌入到基于物理的电力潮流和电网优化工具中，实现物理建模与黑盒ML方法的耦合。通过直接整合训练好的ML预测模型的输入和输出到电力潮流和电网优化的数值方法中，捕捉负荷和天气事件的行为和敏感性。该方法无需拟合代理负荷模型，直接从数据中（通过反向传播）获取敏感性，并将其与物理定义的电网设备的敏感性相结合。", "result": "该方法改进了敏感性计算，并利用这些敏感性设计出一种鲁棒的电力调度方案，从而在随机天气事件下提高了电网的可靠性。它还能够计算系统对外部因素的敏感性，支持更广泛的分析，以提高电网在负荷可变性和极端天气条件下的可靠性。", "conclusion": "通过将ML预测与物理模型深度融合，该方法有效解决了电网调度中的不确定性挑战，提高了敏感性计算的准确性，并最终增强了电网在复杂环境下的鲁棒性和可靠性。"}}
{"id": "2507.13970", "categories": ["cs.RO", "cs.AI", "I.2; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13970", "abs": "https://arxiv.org/abs/2507.13970", "authors": ["Casper Bröcheler", "Thomas Vroom", "Derrick Timmermans", "Alan van den Akker", "Guangzhi Tang", "Charalampos S. Kouzinopoulos", "Rico Möckel"], "title": "A segmented robot grasping perception neural network for edge AI", "comment": "Accepted by SMC 2025", "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.", "AI": {"tldr": "该研究在RISC-V边缘芯片上实现了基于热图的6自由度抓取检测模型，并通过硬件感知优化实现了低功耗、实时抓取推断。", "motivation": "机器人抓取是一个复杂的任务，需要精确的感知和控制。深度神经网络在抓取合成方面表现出色，但在资源受限的环境中实现低延迟、低功耗的实时抓取是其部署到边缘设备的关键挑战。", "method": "该工作在GAP9 RISC-V片上系统上实现了“热图引导抓取检测”这一端到端框架，用于检测6自由度抓取姿态。模型通过硬件感知技术进行了优化，包括输入维度降低、模型分区和量化。", "result": "在GraspNet-1Billion基准测试上进行的实验评估验证了完全片上推理的可行性。", "conclusion": "研究结果突出了低功耗微控制器在实时、自主操作方面的巨大潜力。"}}
{"id": "2507.13403", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13403", "abs": "https://arxiv.org/abs/2507.13403", "authors": ["Morteza Bodaghi", "Majid Hosseini", "Raju Gottumukkala", "Ravi Teja Bhupatiraju", "Iftikhar Ahmad", "Moncef Gabbouj"], "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data", "comment": null, "summary": "In this study, we present a comprehensive public dataset for driver\ndrowsiness detection, integrating multimodal signals of facial, behavioral, and\nbiometric indicators. Our dataset includes 3D facial video using a depth\ncamera, IR camera footage, posterior videos, and biometric signals such as\nheart rate, electrodermal activity, blood oxygen saturation, skin temperature,\nand accelerometer data. This data set provides grip sensor data from the\nsteering wheel and telemetry data from the American truck simulator game to\nprovide more information about drivers' behavior while they are alert and\ndrowsy. Drowsiness levels were self-reported every four minutes using the\nKarolinska Sleepiness Scale (KSS). The simulation environment consists of three\nmonitor setups, and the driving condition is completely like a car. Data were\ncollected from 19 subjects (15 M, 4 F) in two conditions: when they were fully\nalert and when they exhibited signs of sleepiness. Unlike other datasets, our\nmultimodal dataset has a continuous duration of 40 minutes for each data\ncollection session per subject, contributing to a total length of 1,400\nminutes, and we recorded gradual changes in the driver state rather than\ndiscrete alert/drowsy labels. This study aims to create a comprehensive\nmultimodal dataset of driver drowsiness that captures a wider range of\nphysiological, behavioral, and driving-related signals. The dataset will be\navailable upon request to the corresponding author.", "AI": {"tldr": "本研究构建了一个全面的多模态驾驶员疲劳检测公共数据集，包含面部、行为、生物识别和驾驶模拟数据，并记录了连续的疲劳度变化。", "motivation": "现有驾驶员疲劳检测数据集可能缺乏多模态信号的全面性、连续的疲劳度标签或足够长的持续时间来捕捉驾驶员状态的渐进变化。本研究旨在创建一个更全面、更细致的驾驶员疲劳多模态数据集。", "method": "数据集收集了19名受试者（15男，4女）在清醒和疲劳两种状态下的数据。每位受试者的每次数据采集持续40分钟，总时长达1400分钟，并每四分钟通过Karolinska睡意量表（KSS）自报疲劳水平。采集的信号包括：深度摄像头3D面部视频、红外摄像头视频、后方视频、生物识别信号（心率、皮肤电活动、血氧饱和度、皮肤温度、加速度计数据）、方向盘握力传感器数据以及来自美国卡车模拟器游戏的遥测数据。模拟环境采用三显示器设置，模拟真实驾驶条件。", "result": "成功构建了一个综合性的多模态驾驶员疲劳检测数据集，该数据集集成了面部、行为、生物识别和驾驶模拟数据，并能够捕获驾驶员状态的渐进变化，而非简单的离散标签。该数据集具有较长的连续采集时长。", "conclusion": "本研究成功创建了一个全面的多模态驾驶员疲劳数据集，涵盖了广泛的生理、行为和驾驶相关信号。该数据集将对驾驶员疲劳检测领域的研究提供重要支持，并可根据请求提供给相关研究人员。"}}
{"id": "2507.13614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13614", "abs": "https://arxiv.org/abs/2507.13614", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "AI": {"tldr": "本研究通过语言学特征（形态、句法、语义）来刻画人类和大型语言模型（LLM）生成文本的差异，发现人类文本语法更简单、语义更多样，且随着模型更新，机器生成文本的变异性趋于同质化。", "motivation": "随着LLM生成文本与人类文本的相似度越来越高，现有研究多集中于文本分类，本研究旨在通过语言学特征深入刻画和区分这两种文本，而非仅仅分类。", "method": "研究选择了涵盖8个领域、由11种不同LLM生成的文本数据集。计算了包括依存长度和情感性在内的多种语言学特征，并结合不同的采样策略、重复控制和模型发布日期进行统计分析。此外，还应用了风格嵌入（style embeddings）来进一步测试变异性。", "result": "统计分析显示，人类文本倾向于展现更简单的句法结构和更多样化的语义内容。人类和机器文本在不同领域都表现出文体多样性，但人类文本在特征上的变异更大。值得注意的是，较新的模型输出的文本变异性相似，表明机器生成文本趋于同质化。", "conclusion": "人类和机器生成文本在语言学特征上存在显著差异，人类文本在某些方面（如语义多样性、特征变异性）表现出更大的复杂性或多样性。随着LLM的发展，新模型生成的文本变异性趋于一致，可能导致机器生成文本的同质化。"}}
{"id": "2507.14111", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14111", "abs": "https://arxiv.org/abs/2507.14111", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "comment": "Preprint Version", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.", "AI": {"tldr": "CUDA-L1是一个基于强化学习的自动化CUDA优化框架，在A100上实现了显著的性能提升，并展示了优异的跨架构可移植性，能够发现并应用多种优化技术。", "motivation": "大型语言模型（LLMs）的快速发展导致对GPU计算资源的需求呈指数级增长，迫切需要自动化的CUDA优化策略。尽管LLMs在代码生成方面有进展，但当前的SOTA模型在提升CUDA速度方面的成功率较低。", "method": "本文提出了CUDA-L1，一个自动化的强化学习框架，用于CUDA优化。该框架通过基于加速比的奖励信号，将一个初始性能不佳的LLM转化为有效的CUDA优化器，无需人类专业知识或领域知识。", "result": "CUDA-L1在NVIDIA A100上训练，在KernelBench的250个CUDA核函数上平均提速17.7倍，峰值提速达449倍。模型还表现出卓越的跨GPU架构可移植性，在H100上平均提速17.8倍，RTX 3090上19.0倍，L40上16.5倍，H800上14.7倍，H20上13.9倍（尽管是为A100优化）。此外，它能发现多种CUDA优化技术并战略性组合，揭示CUDA优化基本原理，识别非显而易见的性能瓶颈，并拒绝有害的优化。", "conclusion": "强化学习能够通过纯粹的基于加速比的奖励信号，将一个性能不佳的LLM转变为高效的CUDA优化器，无需人类专业知识。训练后的RL模型能将其习得的推理能力扩展到新的核函数。这种范式为CUDA操作的自动化优化开辟了可能性，并有望大幅提升GPU效率，缓解GPU计算资源的日益增长的压力。"}}
{"id": "2507.14043", "categories": ["cs.RO", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.14043", "abs": "https://arxiv.org/abs/2507.14043", "authors": ["Genliang Li", "Yaxin Cui", "Jinyu Su"], "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems", "comment": "59 pages, 22 figures", "summary": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.", "AI": {"tldr": "本文提出了一种多策略改进的蛇群优化器（MISO），旨在解决原蛇群优化器收敛慢和易陷入局部最优的问题，并在标准测试函数、工程设计和无人机路径规划中验证了其优越性能。", "motivation": "原有的蛇群优化器（SO）存在收敛速度慢和易陷入局部最优的缺点。此外，无人机（UAV）路径规划对飞行安全和效率至关重要，但路径模型建立和优化仍面临挑战，需要更高效的优化算法。", "method": "提出了多策略改进的蛇群优化器（MISO）。具体改进包括：1) 基于正弦函数的新型自适应随机扰动策略，以避免局部最优；2) 引入基于比例因子和领导者的自适应Levy飞行策略，赋予雄性蛇领导者飞行能力，帮助跳出局部最优；3) 结合精英领导和布朗运动的位置更新策略，加速收敛并保证精度。通过30个CEC2017测试函数、CEC2022测试套件和11种流行算法进行对比，并应用于无人机3D路径规划和6个工程设计问题进行实际应用评估。", "result": "实验结果表明，MISO在求解质量和稳定性方面均优于其他竞争算法，在标准测试函数、工程设计问题和无人机路径规划中展现出强大的应用潜力。", "conclusion": "MISO通过引入多种改进策略，有效克服了传统蛇群优化器收敛慢和易陷入局部最优的问题，并在理论测试和实际应用中展现出卓越的性能和稳定性，具有广阔的应用前景。"}}
{"id": "2507.13404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13404", "abs": "https://arxiv.org/abs/2507.13404", "authors": ["Delin An", "Pan Du", "Jian-Xun Wang", "Chaoli Wang"], "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation", "comment": null, "summary": "Accurate 3D aortic construction is crucial for clinical diagnosis,\npreoperative planning, and computational fluid dynamics (CFD) simulations, as\nit enables the estimation of critical hemodynamic parameters such as blood flow\nvelocity, pressure distribution, and wall shear stress. Existing construction\nmethods often rely on large annotated training datasets and extensive manual\nintervention. While the resulting meshes can serve for visualization purposes,\nthey struggle to produce geometrically consistent, well-constructed surfaces\nsuitable for downstream CFD analysis. To address these challenges, we introduce\nAortaDiff, a diffusion-based framework that generates smooth aortic surfaces\ndirectly from CT/MRI volumes. AortaDiff first employs a volume-guided\nconditional diffusion model (CDM) to iteratively generate aortic centerlines\nconditioned on volumetric medical images. Each centerline point is then\nautomatically used as a prompt to extract the corresponding vessel contour,\nensuring accurate boundary delineation. Finally, the extracted contours are\nfitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh\nrepresentation. AortaDiff offers distinct advantages over existing methods,\nincluding an end-to-end workflow, minimal dependency on large labeled datasets,\nand the ability to generate CFD-compatible aorta meshes with high geometric\nfidelity. Experimental results demonstrate that AortaDiff performs effectively\neven with limited training data, successfully constructing both normal and\npathologically altered aorta meshes, including cases with aneurysms or\ncoarctation. This capability enables the generation of high-quality\nvisualizations and positions AortaDiff as a practical solution for\ncardiovascular research.", "AI": {"tldr": "AortaDiff是一个基于扩散模型的框架，能直接从CT/MRI图像生成平滑且兼容CFD的3D主动脉表面，减少了对大量标注数据和人工干预的依赖。", "motivation": "现有的3D主动脉构建方法依赖于大量标注训练数据和广泛的人工干预，且生成的网格几何一致性差，不适用于下游CFD分析。然而，精确的3D主动脉构建对临床诊断、术前规划和CFD模拟至关重要。", "method": "AortaDiff是一个扩散模型框架，其步骤如下：1. 使用体积引导条件扩散模型(CDM)从医学图像中迭代生成主动脉中心线。2. 每个中心线点被用作提示，自动提取相应的血管轮廓，确保边界准确。3. 将提取的轮廓拟合为平滑的3D表面，生成连续的、兼容CFD的网格表示。", "result": "AortaDiff提供了端到端的工作流程，对大型标注数据集的依赖性最小，并能生成高几何保真度的CFD兼容主动脉网格。实验结果表明，即使在有限的训练数据下，AortaDiff也能有效构建正常和病变（如动脉瘤或主动脉缩窄）的主动脉网格，并能生成高质量的可视化。", "conclusion": "AortaDiff是心血管研究的实用解决方案，能够生成高质量、CFD兼容的3D主动脉网格，且对训练数据的需求少，人工干预极小。"}}
{"id": "2507.13618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13618", "abs": "https://arxiv.org/abs/2507.13618", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "AI": {"tldr": "本文介绍了Seed-X，一个7B参数的开源大语言模型家族，通过多语言预训练、CoT推理微调和强化学习，在28种语言的翻译任务上取得了与Gemini-2.5和GPT-4o相当的性能，并显著优于其他大型开源模型。", "motivation": "现有的大语言模型在处理多语言翻译中复杂的语言模式和生硬的翻译时面临挑战。", "method": "研究团队引入了Seed-X，一个包含指令和推理模型的开源大语言模型家族。其基础模型在包含28种语言的单语和双语高质量数据集上进行预训练。指令模型通过思维链（CoT）推理进行翻译微调，并通过强化学习（RL）进一步增强，以提高在不同语言对之间的泛化能力。", "result": "Seed-X在28种语言的翻译任务上，其性能与领先的闭源模型（包括Gemini-2.5和GPT-4o）相当，并在自动评估指标和人工评估中显著优于更大的开源模型。", "conclusion": "Seed-X推动了7B参数规模大语言模型在翻译能力方面的极限，实现了与顶级闭源模型媲美的性能，并分享了优化过程中的最佳实践，为翻译研究和应用提供了公开可用的参数。"}}
{"id": "2507.13407", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13407", "abs": "https://arxiv.org/abs/2507.13407", "authors": ["Vinu Sankar Sadasivan", "Mehrdad Saberi", "Soheil Feizi"], "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images", "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)", "summary": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets.", "AI": {"tldr": "IConMark是一种新颖的生成过程中可解释的语义水印方法，通过嵌入有意义的语义属性来增强AI生成图像的水印鲁棒性和可读性，优于传统方法。", "motivation": "随着生成式AI和合成媒体的快速发展，区分AI生成图像和真实图像对于防止错误信息传播和确保数字真实性至关重要。传统水印技术易受对抗性攻击，影响其有效性。", "method": "本文提出了IConMark，一种在图像生成过程中嵌入可解释概念的鲁棒语义水印方法。与传统添加噪声或扰动的方法不同，IConMark融入有意义的语义属性，使其对人类可解释并对对抗性操纵具有弹性。它还可以与现有水印技术（如StegaStamp和TrustMark）结合，形成混合方法（IConMark+SS和IConMark+TM）。", "result": "IConMark在检测准确性和图像质量保持方面表现出优越性，且对各种图像增强具有鲁棒性，并支持人工验证。与最佳基线相比，IConMark及其变体（+TM和+SS）在水印检测的平均AUROC分数上分别提高了10.8%、14.5%和15.9%。", "conclusion": "IConMark是一种有效、鲁棒且人类可读的语义水印方法，能够抵御多种图像操纵，并可通过与现有技术结合进一步增强其鲁棒性，为可解释水印迈出了重要一步。"}}
{"id": "2507.14049", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14049", "abs": "https://arxiv.org/abs/2507.14049", "authors": ["Paweł Budzianowski", "Wesley Maa", "Matthew Freed", "Jingxiang Mo", "Winston Hsiao", "Aaron Xie", "Tomasz Młoduchowski", "Viraj Tipnis", "Benjamin Bolte"], "title": "EdgeVLA: Efficient Vision-Language-Action Models", "comment": null, "summary": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.", "AI": {"tldr": "Edge VLA (EVLA) 是一种新方法，通过消除自回归预测和利用小型语言模型，显著提升了视觉-语言-动作（VLA）模型在边缘设备上的推理速度和内存效率。", "motivation": "视觉-语言模型（VLMs）在解决机器人领域数据稀缺问题方面显示出潜力，但将大规模VLM部署到资源受限的移动操作系统上面临挑战。", "method": "EVLA通过两项创新实现：1) 消除末端执行器位置预测的自回归要求，从而实现7倍的推理速度提升；2) 利用小型语言模型（SLMs）的效率，在显著降低计算需求的同时保持可比的训练性能。", "result": "EVLA在推理速度和内存效率方面取得了显著提升，同时保持了与OpenVLA相当的训练特性。", "conclusion": "EVLA使VLA模型能够在边缘设备上实现实时性能，同时保持其表示能力，为在资源受限系统上部署大规模VLM提供了解决方案。"}}
{"id": "2507.13405", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13405", "abs": "https://arxiv.org/abs/2507.13405", "authors": ["Ishant Chintapatla", "Kazuma Choji", "Naaisha Agarwal", "Andrew Lin", "Hannah You", "Charles Duong", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark", "comment": null, "summary": "Recently, many benchmarks and datasets have been developed to evaluate\nVision-Language Models (VLMs) using visual question answering (VQA) pairs, and\nmodels have shown significant accuracy improvements. However, these benchmarks\nrarely test the model's ability to accurately complete visual entailment, for\ninstance, accepting or refuting a hypothesis based on the image. To address\nthis, we propose COREVQA (Crowd Observations and Reasoning Entailment), a\nbenchmark of 5608 image and synthetically generated true/false statement pairs,\nwith images derived from the CrowdHuman dataset, to provoke visual entailment\nreasoning on challenging crowded images. Our results show that even the\ntop-performing VLMs achieve accuracy below 80%, with other models performing\nsubstantially worse (39.98%-69.95%). This significant performance gap reveals\nkey limitations in VLMs' ability to reason over certain types of image-question\npairs in crowded scenes.", "AI": {"tldr": "现有VLM基准未充分测试视觉蕴涵能力，本文提出COREVQA基准，发现VLM在拥挤场景下的视觉蕴涵推理能力存在显著局限。", "motivation": "尽管VLM在VQA任务上表现出色，但现有基准很少测试模型基于图像准确完成视觉蕴涵（即接受或驳斥假设）的能力，尤其是在具有挑战性的拥挤图像中。", "method": "提出了COREVQA（Crowd Observations and Reasoning Entailment）基准，包含5608对图像和合成的真/假陈述。图像来源于CrowdHuman数据集，旨在评估VLM在拥挤场景下的视觉蕴涵推理能力。", "result": "评估结果显示，即使是性能最佳的VLM，准确率也低于80%，而其他模型的表现更差（39.98%-69.95%）。", "conclusion": "VLM在拥挤场景中对某些类型的图像-问题对进行推理的能力存在关键局限性，表现出显著的性能差距。"}}
{"id": "2507.13655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13655", "abs": "https://arxiv.org/abs/2507.13655", "authors": ["Teerapong Panboonyuen"], "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.", "AI": {"tldr": "CU-ICU是一种针对ICU领域定制无监督指令微调大模型的方法，通过稀疏微调和少量参数更新，显著提升了预测准确性和可解释性，同时保持高效。", "motivation": "将大型语言模型整合到医疗保健等专业领域面临独特挑战，包括领域适应和标记数据有限的问题。", "method": "引入CU-ICU方法，利用Text-to-Text Transfer Transformer (T5) 架构，通过稀疏微调结合少量提示（few-shot prompting）和选择性参数更新，实现对ICU数据集的高效适应，仅需少量监督。", "result": "在早期脓毒症检测、死亡率预测和临床笔记生成等关键ICU任务中，CU-ICU的表现优于标准微调方法。脓毒症检测准确率提高了15%，生成临床相关解释的能力增强了20%，而更新的模型参数不到1%。", "conclusion": "CU-ICU是一种可扩展、低开销的解决方案，能够为真实的ICU环境提供准确且可解释的临床决策支持。"}}
{"id": "2507.13408", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13408", "abs": "https://arxiv.org/abs/2507.13408", "authors": ["Hemanth Kumar M", "Karthika M", "Saianiruth M", "Vasanthakumar Venugopal", "Anandakumar D", "Revathi Ezhumalai", "Charulatha K", "Kishore Kumar J", "Dayana G", "Kalyan Sivasailam", "Bargava Subramanian"], "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs", "comment": "12 pages, 2 figures", "summary": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.", "AI": {"tldr": "该研究开发了一个基于多模型深度学习集成系统，用于在肩部X光片中高精度检测骨折，以解决误诊问题。", "motivation": "肩部骨折在急诊和高流量临床环境中常被漏诊（高达10%），导致诊断延误。人工智能工具能有效辅助早期检测并缩短诊断时间。", "method": "研究开发了一个多模型深度学习系统，使用了10,000张标注的肩部X光片。采用了Faster R-CNN (ResNet50-FPN, ResNeXt)、EfficientDet和RF-DETR等架构。为增强检测效果，应用了边界框和分类级别的集成技术，如Soft-NMS、WBF和NMW融合。", "result": "NMW集成方法实现了95.5%的准确率和0.9610的F1分数，在所有关键指标上均优于单个模型。它展现了强大的召回率和定位精度，证实了其在肩部X光片临床骨折检测中的有效性。", "conclusion": "结果表明，基于集成的AI系统能够可靠地高精度检测X光片中的肩部骨折，具有重要的临床相关性。该模型的准确性和部署就绪性使其非常适合集成到实时诊断工作流程中。当前模型仅限于二元骨折检测，旨在支持快速筛查和分诊，而非详细的骨科分类。"}}
{"id": "2507.14059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14059", "abs": "https://arxiv.org/abs/2507.14059", "authors": ["Tianyuan Wang", "Mark A Post", "Mathieu Deremetz"], "title": "Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub", "comment": "In proceedings of the Towards Autonomous Robotic Systems 2025\n  conference (TAROS 2025), York, UK 6 pages, one page of references, 6 figures", "summary": "The use of autonomous robots in space is an essential part of the \"New Space\"\ncommercial ecosystem of assembly and re-use of space hardware components in\nEarth orbit and beyond. The STARFAB project aims to create a ground\ndemonstration of an orbital automated warehouse as a hub for sustainable\ncommercial operations and servicing. A critical part of this fully-autonomous\nrobotic facility will be the capability to monitor, inspect, and assess the\ncondition of both the components stored in the warehouse, and the STARFAB\nfacility itself. This paper introduces ongoing work on the STARFAB Mobile\nInspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it\ncan be carried by Walking Manipulators (WM) as an independently-mobile robot,\nand multiple MIMs can be stored and retrieved as needed for operations on\nSTARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a\nthermal imaging sensor, with the capability to add other modular sensors. A\ngrasping tool and torque wrench are stored within the modular body for use by\nan attached WM for maintenance operations. Implementation and testing is still\nongoing at the time of writing. This paper details the concept of operations\nfor the MIM as an on-orbit autonomous inspection and maintenance system, the\nmechanical and electronic design of the MIM, and the sensors package used for\nnon-destructive testing.", "AI": {"tldr": "本文介绍了STARFAB项目中的移动检查模块（MIM），一个用于轨道自动化仓库的自主检查和维护机器人。", "motivation": "“新太空”商业生态系统需要自主机器人在轨组装和重复利用硬件，STARFAB项目旨在创建一个自动化轨道仓库。该仓库需要具备监测、检查和评估存储部件及设施自身状况的能力。", "method": "STARFAB移动检查模块（MIM）被提出，它使用标准互连件（SI），可由步行机械手（WM）携带，也可作为独立移动机器人。MIM配备高分辨率相机、3D轮廓仪和热成像传感器（支持模块化传感器），并内置抓取工具和扭矩扳手供WM进行维护操作。论文详细阐述了MIM的运行概念、机械和电子设计以及用于无损检测的传感器包。", "result": "截至论文撰写时，MIM的实施和测试仍在进行中。本文主要介绍了MIM的概念、设计和传感器配置，尚未呈现具体的测试结果。", "conclusion": "MIM是STARFAB项目实现自主在轨检查和维护的关键组成部分，其设计和运行概念为未来空间站的自动化运营奠定了基础，尽管具体测试结果尚待公布。"}}
{"id": "2507.13420", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13420", "abs": "https://arxiv.org/abs/2507.13420", "authors": ["Alessandro Pistola", "Valentina Orru'", "Nicolo' Marchetti", "Marco Roccetti"], "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery", "comment": "25 pages, 9 Figures", "summary": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization", "AI": {"tldr": "通过结合CORONA旧卫星图像对深度学习模型进行再训练，显著提升了在已发生巨变的环境中自动识别考古遗址的能力，并成功发现新遗址。", "motivation": "在过去五十年中，许多考古遗址因环境剧变（包括完全破坏）而不再可见。研究旨在利用AI技术和历史图像，克服传统方法在识别这些消失遗址方面的局限性。", "method": "将一个基于Bing图像的卷积网络模型，使用CORONA卫星图像（涵盖巴格达以西的Abu Ghraib地区）进行再训练。通过图像分割（IoU）和整体准确率评估模型性能，并通过实地验证确认新发现的遗址。", "result": "模型检测精度显著提高：图像分割级别的交并比（IoU）值超过85%，考古遗址检测的总体准确率达到90%。此外，再训练后的模型成功识别出四个新的考古遗址，这些遗址此前未被传统考古技术发现，并已通过实地验证确认。", "conclusion": "研究证实了结合AI技术和20世纪60年代CORONA卫星图像在发现当前已不可见的考古遗址方面的有效性，这对于研究受人类活动影响而考古证据逐渐消失的景观具有重要意义。"}}
{"id": "2507.13666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13666", "abs": "https://arxiv.org/abs/2507.13666", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.", "AI": {"tldr": "本文提出了一种名为Keyword-inspired Cascade (KiC)的新型级联框架，旨在通过选择性地使用更强的模型，实现成本效益高的自由形式文本生成，同时保持接近SOTA模型的性能。", "motivation": "高性能大型语言模型(LLMs)通常只能通过API访问，导致高昂的推理成本。现有级联方法依赖精确文本匹配，难以选择可靠的代表性响应并评估自由形式输出的整体可靠性。", "method": "KiC框架首先从较弱模型生成多个输出，识别其中最具代表性的答案，然后评估其他响应与该代表性答案的语义对齐程度。根据对齐程度，KiC决定是接受较弱模型的输出，还是升级到更强的模型。", "result": "在三个自由形式文本生成基准测试中，KiC达到了GPT-4 97.53%的准确率，同时平均降低了28.81%的API成本，甚至在一个特定基准测试中超越了GPT-4。", "conclusion": "KiC是一种有效的、成本效益高的自由形式文本生成方法，它在保持高准确率的同时显著降低了LLM的API使用成本，解决了现有级联方法在处理自由形式输出时的局限性。"}}
{"id": "2507.13425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13425", "abs": "https://arxiv.org/abs/2507.13425", "authors": ["Sirui Wang", "Zhou Guan", "Bingxi Zhao", "Tongjia Gu"], "title": "CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction", "comment": null, "summary": "Accurate prediction of driving intention is key to enhancing the safety and\ninteractive efficiency of human-machine co-driving systems. It serves as a\ncornerstone for achieving high-level autonomous driving. However, current\napproaches remain inadequate for accurately modeling the complex\nspatio-temporal interdependencies and the unpredictable variability of human\ndriving behavior. To address these challenges, we propose CaSTFormer, a Causal\nSpatio-Temporal Transformer to explicitly model causal interactions between\ndriver behavior and environmental context for robust intention prediction.\nSpecifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)\nmechanism for precise temporal alignment of internal and external feature\nstreams, a Causal Pattern Extraction (CPE) module that systematically\neliminates spurious correlations to reveal authentic causal dependencies, and\nan innovative Feature Synthesis Network (FSN) that adaptively synthesizes these\npurified representations into coherent spatio-temporal inferences. We evaluate\nthe proposed CaSTFormer on the public Brain4Cars dataset, and it achieves\nstate-of-the-art performance. It effectively captures complex causal\nspatio-temporal dependencies and enhances both the accuracy and transparency of\ndriving intention prediction.", "AI": {"tldr": "本文提出CaSTFormer，一种因果时空Transformer模型，用于精确预测驾驶意图，通过显式建模驾驶员行为与环境上下文之间的因果互动，解决了现有方法在处理复杂时空依赖和行为变异性方面的不足。", "motivation": "准确预测驾驶意图对于提升人机共驾系统的安全性和交互效率至关重要，也是实现高级别自动驾驶的基础。然而，现有方法在准确建模复杂时空相互依赖性以及人类驾驶行为的不可预测变异性方面存在不足。", "method": "本文提出CaSTFormer（因果时空Transformer），显式建模驾驶员行为与环境上下文之间的因果互动。该模型包含三个核心模块：1) 互惠偏移融合（RSF）机制，用于精确对齐内部和外部特征流；2) 因果模式提取（CPE）模块，系统性地消除虚假关联，揭示真实的因果依赖；3) 特征合成网络（FSN），自适应地合成这些纯化后的表示，进行连贯的时空推断。", "result": "CaSTFormer在公开的Brain4Cars数据集上进行了评估，取得了最先进的性能。它有效地捕获了复杂的因果时空依赖性，并提高了驾驶意图预测的准确性和透明度。", "conclusion": "CaSTFormer通过显式建模驾驶员行为和环境上下文之间的因果交互，显著提升了驾驶意图预测的准确性和透明度，为实现高级别自动驾驶奠定了基础。"}}
{"id": "2507.14061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14061", "abs": "https://arxiv.org/abs/2507.14061", "authors": ["Nataliya Nechyporenko", "Yutong Zhang", "Sean Campbell", "Alessandro Roncone"], "title": "MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation", "comment": null, "summary": "What if a robot could rethink its own morphological representation to better\nmeet the demands of diverse tasks? Most robotic systems today treat their\nphysical form as a fixed constraint rather than an adaptive resource, forcing\nthe same rigid geometric representation to serve applications with vastly\ndifferent computational and precision requirements. We introduce MorphIt, a\nnovel algorithm for approximating robot morphology using spherical primitives\nthat balances geometric accuracy with computational efficiency. Unlike existing\napproaches that rely on either labor-intensive manual specification or\ninflexible computational methods, MorphIt implements an automatic\ngradient-based optimization framework with tunable parameters that provides\nexplicit control over the physical fidelity versus computational cost tradeoff.\nQuantitative evaluations demonstrate that MorphIt outperforms baseline\napproaches (Variational Sphere Set Approximation and Adaptive Medial-Axis\nApproximation) across multiple metrics, achieving better mesh approximation\nwith fewer spheres and reduced computational overhead. Our experiments show\nenhanced robot capabilities in collision detection accuracy, contact-rich\ninteraction simulation, and navigation through confined spaces. By dynamically\nadapting geometric representations to task requirements, robots can now exploit\ntheir physical embodiment as an active resource rather than an inflexible\nparameter, opening new frontiers for manipulation in environments where\nphysical form must continuously balance precision with computational\ntractability.", "AI": {"tldr": "MorphIt是一种新颖的算法，允许机器人通过球形基元动态调整其形态表示，以平衡几何精度和计算效率，从而更好地适应多样化的任务需求。", "motivation": "目前的机器人系统将物理形态视为固定约束而非适应性资源，导致相同的刚性几何表示必须服务于计算和精度要求差异巨大的应用，效率低下。", "method": "MorphIt采用了一种自动的、基于梯度的优化框架，使用可调参数通过球形基元来近似机器人形态。该框架显式控制物理保真度与计算成本之间的权衡。", "result": "定量评估表明，MorphIt在多个指标上优于基线方法（变分球集近似和自适应中轴近似），能用更少的球体实现更好的网格近似，并降低计算开销。实验证明，它增强了机器人在碰撞检测精度、接触丰富的交互模拟和狭窄空间导航方面的能力。", "conclusion": "通过动态调整几何表示以适应任务要求，机器人现在可以将其实体形态作为一种主动资源而非僵化参数来利用，这为在需要持续平衡精度与计算可行性的环境中进行操作开辟了新领域。"}}
{"id": "2507.13428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13428", "abs": "https://arxiv.org/abs/2507.13428", "authors": ["Jing Gu", "Xian Liu", "Yu Zeng", "Ashwin Nagarajan", "Fangrui Zhu", "Daniel Hong", "Yue Fan", "Qianqi Yan", "Kaiwen Zhou", "Ming-Yu Liu", "Xin Eric Wang"], "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models", "comment": "31 pages, 21 figures", "summary": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.", "AI": {"tldr": "该论文提出了PhyWorldBench，一个用于评估视频生成模型物理真实性的综合基准，并分析了当前模型面临的挑战。", "motivation": "尽管视频生成模型在生成高质量、逼真的内容方面取得了显著进展，但它们准确模拟物理现象的能力仍然是一个关键且未解决的挑战。", "method": "引入了PhyWorldBench基准，涵盖从基本原理（如物体运动、能量守恒）到复杂场景（如刚体交互、人类/动物运动）的多个物理现象层级。设计了一个“反物理”类别，评估模型在违反物理定律指令下的遵循能力和逻辑一致性。除了大规模人工评估外，还提出了一种利用多模态大语言模型（MLLM）进行零样本物理真实性评估的方法。评估了12个最先进的文本到视频生成模型（包括开源和专有模型），使用了1050个精心策划的提示，涵盖基础、复合和反物理场景。", "result": "通过对模型输出的系统测试，识别了这些模型在遵循现实世界物理定律方面面临的关键挑战。详细比较和分析了模型在不同物理现象和提示类型下的表现。", "conclusion": "针对提升模型对物理原理的忠实度，提出了有针对性的提示工程建议。"}}
{"id": "2507.13681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13681", "abs": "https://arxiv.org/abs/2507.13681", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.", "AI": {"tldr": "LoopServe是一种自适应双阶段推理加速框架，用于多轮对话中的大型语言模型，通过在线稀疏化和渐进式KV压缩，显著提升了长上下文对话任务的推理速度和效果。", "motivation": "多轮对话中，随着对话历史变长，现有大型语言模型面临计算和内存挑战，影响效率和响应速度。当前加速方法多依赖固定或基于位置的启发式规则，难以适应动态多变的对话模式。", "method": "LoopServe引入了两项主要创新：1. 在预填充阶段进行在线稀疏化，动态选择注意力矩阵中最重要的部分。2. 在解码阶段采用渐进式键值压缩，根据最新生成的输出token自适应维护相关且高效的缓存。此外，还提出了一个包含11个多轮数据集的新基准测试。", "result": "LoopServe在广泛的长上下文对话任务中，相比现有基线，持续展现出卓越的有效性，并显著加速了大型语言模型的推理。", "conclusion": "LoopServe为多轮对话中大型语言模型的推理提供了一个自适应、高效的加速解决方案，有效解决了长对话历史带来的计算和内存挑战。"}}
{"id": "2507.13659", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13659", "abs": "https://arxiv.org/abs/2507.13659", "authors": ["Xiao Wang", "Qian Zhu", "Shujuan Wu", "Bo Jiang", "Shiliang Zhang", "Yaowei Wang", "Yonghong Tian", "Bin Luo"], "title": "When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework", "comment": null, "summary": "Recent researchers have proposed using event cameras for person\nre-identification (ReID) due to their promising performance and better balance\nin terms of privacy protection, event camera-based person ReID has attracted\nsignificant attention. Currently, mainstream event-based person ReID algorithms\nprimarily focus on fusing visible light and event stream, as well as preserving\nprivacy. Although significant progress has been made, these methods are\ntypically trained and evaluated on small-scale or simulated event camera\ndatasets, making it difficult to assess their real identification performance\nand generalization ability. To address the issue of data scarcity, this paper\nintroduces a large-scale RGB-event based person ReID dataset, called EvReID.\nThe dataset contains 118,988 image pairs and covers 1200 pedestrian identities,\nwith data collected across multiple seasons, scenes, and lighting conditions.\nWe also evaluate 15 state-of-the-art person ReID algorithms, laying a solid\nfoundation for future research in terms of both data and benchmarking. Based on\nour newly constructed dataset, this paper further proposes a pedestrian\nattribute-guided contrastive learning framework to enhance feature learning for\nperson re-identification, termed TriPro-ReID. This framework not only\neffectively explores the visual features from both RGB frames and event\nstreams, but also fully utilizes pedestrian attributes as mid-level semantic\nfeatures. Extensive experiments on the EvReID dataset and MARS datasets fully\nvalidated the effectiveness of our proposed RGB-Event person ReID framework.\nThe benchmark dataset and source code will be released on\nhttps://github.com/Event-AHU/Neuromorphic_ReID", "AI": {"tldr": "该论文提出了一个大规模RGB-事件行人重识别数据集EvReID，并提出了一个名为TriPro-ReID的行人属性引导对比学习框架，以提升行人重识别的性能。", "motivation": "当前的事件相机行人重识别算法主要在小规模或模拟数据集上进行训练和评估，导致难以准确评估其实际识别性能和泛化能力。数据稀缺是主要问题。", "method": "1. 构建了一个大规模RGB-事件行人重识别数据集EvReID，包含118,988对图像和1200个行人身份，数据涵盖多季节、场景和光照条件。2. 在EvReID数据集上评估了15种最先进的行人重识别算法，建立了基准。3. 提出了一个行人属性引导的对比学习框架TriPro-ReID，该框架有效结合了RGB帧和事件流的视觉特征，并充分利用行人属性作为中级语义特征。", "result": "1. 成功构建并发布了大规模RGB-事件行人重识别数据集EvReID。2. 对15种最先进的行人重识别算法进行了评估，为未来的研究奠定了数据和基准基础。3. 在EvReID和MARS数据集上的大量实验验证了所提出的RGB-事件行人重识别框架TriPro-ReID的有效性。", "conclusion": "该论文通过引入大规模数据集和提出新的有效框架，解决了事件相机行人重识别领域的数据稀缺问题，并为未来的研究提供了坚实的基础和新的方向。"}}
{"id": "2507.14099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14099", "abs": "https://arxiv.org/abs/2507.14099", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Maria Koskinopoulou", "Yvan R. Petillot"], "title": "Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation", "comment": "Accepted at 2025 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)", "summary": "Autonomous motion planning is critical for efficient and safe underwater\nmanipulation in dynamic marine environments. Current motion planning methods\noften fail to effectively utilize prior motion experiences and adapt to\nreal-time uncertainties inherent in underwater settings. In this paper, we\nintroduce an Adaptive Heuristic Motion Planner framework that integrates a\nHeuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning\nfor autonomous underwater manipulation. Our approach employs the Probabilistic\nRoadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite\ncost function that accounts for distance, uncertainty, energy consumption, and\nexecution time. By leveraging HMS, our framework significantly reduces the\nsearch space, thereby boosting computational performance and enabling real-time\nplanning capabilities. Bayesian Networks are utilized to dynamically update\nuncertainty estimates based on real-time sensor data and environmental\nconditions, thereby refining the joint probability of path success. Through\nextensive simulations and real-world test scenarios, we showcase the advantages\nof our method in terms of enhanced performance and robustness. This\nprobabilistic approach significantly advances the capability of autonomous\nunderwater robots, ensuring optimized motion planning in the face of dynamic\nmarine challenges.", "AI": {"tldr": "本文提出一种自适应启发式运动规划器，结合启发式运动空间（HMS）和贝叶斯网络，以优化水下机器人在动态环境中的运动规划，提高效率和安全性。", "motivation": "当前运动规划方法未能有效利用先验运动经验，也无法适应水下环境中固有的实时不确定性，这对于高效安全的自主水下操作至关重要。", "method": "该方法引入了自适应启发式运动规划器框架，将启发式运动空间（HMS）与贝叶斯网络相结合。在HMS内使用概率路线图（PRM）算法，通过最小化包含距离、不确定性、能耗和执行时间的复合成本函数来优化路径。HMS显著减小了搜索空间，提高了计算性能。贝叶斯网络用于根据实时传感器数据和环境条件动态更新不确定性估计，从而完善路径成功的联合概率。", "result": "通过广泛的仿真和真实世界测试场景，该方法在性能和鲁棒性方面表现出显著优势。它显著减少了搜索空间，提升了计算性能并实现了实时规划能力。", "conclusion": "这种概率方法显著提升了自主水下机器人的能力，确保在动态海洋挑战面前实现优化的运动规划。"}}
{"id": "2507.13486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13486", "abs": "https://arxiv.org/abs/2507.13486", "authors": ["Debao Huang", "Rongjun Qin"], "title": "Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation", "comment": "16 pages, 9 figures, this manuscript has been submitted to ISPRS\n  Journal of Photogrammetry and Remote Sensing for consideration", "summary": "Uncertainty quantification of the photogrammetry process is essential for\nproviding per-point accuracy credentials of the point clouds. Unlike airborne\nLiDAR, which typically delivers consistent accuracy across various scenes, the\naccuracy of photogrammetric point clouds is highly scene-dependent, since it\nrelies on algorithm-generated measurements (i.e., stereo or multi-view stereo).\nGenerally, errors of the photogrammetric point clouds propagate through a\ntwo-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),\nfollowed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM\nstage has been well studied using the first-order statistics of the\nreprojection error function, that in the MVS stage remains largely unsolved and\nnon-standardized, primarily due to its non-differentiable and multi-modal\nnature (i.e., from pixel values to geometry). In this paper, we present an\nuncertainty quantification framework closing this gap by associating an error\ncovariance matrix per point accounting for this two-step photogrammetry\nprocess. Specifically, to estimate the uncertainty in the MVS stage, we propose\na novel, self-calibrating method by taking reliable n-view points (n>=6)\nper-view to regress the disparity uncertainty using highly relevant cues (such\nas matching cost values) from the MVS stage. Compared to existing approaches,\nour method uses self-contained, reliable 3D points extracted directly from the\nMVS process, with the benefit of being self-supervised and naturally adhering\nto error propagation path of the photogrammetry process, thereby providing a\nrobust and certifiable uncertainty quantification across diverse scenes. We\nevaluate the framework using a variety of publicly available airborne and UAV\nimagery datasets. Results demonstrate that our method outperforms existing\napproaches by achieving high bounding rates without overestimating uncertainty.", "AI": {"tldr": "本文提出了一种新的不确定性量化框架，用于光度测量点云，特别是在多视图立体（MVS）阶段，通过一种自校准方法关联每个点的误差协方差矩阵，从而提供稳健且可认证的精度评估。", "motivation": "机载LiDAR通常提供一致的精度，而光度测量点云的精度高度依赖于场景，且其误差通过SfM和MVS两步传播。SfM阶段的不确定性估计已被充分研究，但MVS阶段的不确定性估计仍未解决且未标准化，主要因为其不可微分和多模态特性，这构成了研究空白。", "method": "本文提出了一种新颖的、自校准的方法来估计MVS阶段的不确定性。该方法通过利用每个视图中可靠的n视图点（n>=6），并使用来自MVS阶段的高度相关线索（如匹配成本值）来回归视差不确定性。该方法是自监督的，并自然地遵循光度测量误差传播路径，利用从MVS过程中直接提取的自包含可靠3D点。", "result": "该框架在多种公开的机载和UAV图像数据集上进行了评估。结果表明，与现有方法相比，本文方法在不估计过高不确定性的前提下，实现了高边界率，表现优于现有方法。", "conclusion": "本文通过为每个点关联一个考虑光度测量两步过程的误差协方差矩阵，弥补了MVS阶段不确定性量化的空白。所提出的自校准方法提供了一种稳健且可认证的跨场景不确定性量化，能够准确地评估光度测量点云的精度。"}}
{"id": "2507.13705", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13705", "abs": "https://arxiv.org/abs/2507.13705", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.", "AI": {"tldr": "该研究评估了大型语言模型（LLMs）在群组推荐系统（GRS）中作为决策者和解释生成器的表现，发现其推荐结果常与加性功利（ADD）聚合策略相似，但解释存在不一致和模糊性，影响了透明度。", "motivation": "LLMs正越来越多地被应用于GRS中，作为联合决策者和解释生成器，因此有必要评估其推荐和解释的有效性和透明度。", "method": "研究方法是将LLM生成的推荐和解释与基于社会选择的聚合策略进行比较，特别是与加性功利（ADD）聚合策略进行对比。", "result": "研究结果显示，LLM生成的推荐通常与加性功利（ADD）聚合策略的结果相似。然而，其解释通常提及平均评分（类似于但不完全等同于ADD聚合）。群组结构（统一或多样）对推荐没有影响。此外，LLMs经常在解释中声称使用了用户或项目相似性、多样性等额外标准，或使用了未定义的流行度指标或阈值。解释中额外的标准依赖于群组场景中的评分数量，这可能暗示标准聚合方法在更大项目集尺寸下的潜在低效。不一致和模糊的解释削弱了透明度和可解释性。", "conclusion": "该研究对LLMs在GRS中的应用以及标准聚合策略具有重要启示。LLMs生成的解释存在不一致和模糊性，这损害了其作为GRS中透明度和可解释性工具的关键作用。同时，研究结果也提示了标准聚合方法在处理大型项目集时可能存在的效率问题。"}}
{"id": "2507.13677", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13677", "abs": "https://arxiv.org/abs/2507.13677", "authors": ["Chuheng Wei", "Ziye Qin", "Walter Zimmer", "Guoyuan Wu", "Matthew J. Barth"], "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors", "comment": "Ranked first in CVPR DriveX workshop TUM-Traf V2X challenge. Accepted\n  by ITSC2025", "summary": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments.", "AI": {"tldr": "HeCoFuse是一个针对V2X异构传感器配置（相机、激光雷达或两者）的协同感知统一框架，通过分层融合、自适应空间分辨率调整和协同学习策略，有效解决了特征融合和感知可靠性问题，并在真实世界数据集上取得了最先进的性能。", "motivation": "现实世界的V2X协同感知系统常因成本和部署差异导致传感器配置异构，这给特征融合和感知可靠性带来了巨大挑战。", "method": "本文提出了HeCoFuse框架：1) 引入分层融合机制，结合通道和空间注意力自适应地加权特征，以解决跨模态特征错位和表示质量不平衡问题。2) 采用自适应空间分辨率调整模块，平衡计算成本和融合效率。3) 实现协同学习策略，根据可用模态动态调整融合类型，增强不同配置下的鲁棒性。", "result": "在TUMTraf-V2X数据集上，HeCoFuse在全传感器配置（LC+LC）下实现了43.22%的3D mAP，比CoopDet3D基线高出1.17%；在L+LC场景下达到更高的43.38% 3D mAP；在九种异构传感器配置下，3D mAP保持在21.74%至43.38%之间。该成果在CVPR 2025 DriveX挑战赛中获得第一名。", "conclusion": "HeCoFuse在TUM-Traf V2X数据集上达到了当前最先进的水平，并展示了在不同传感器部署下的强大鲁棒性能，有效解决了V2X异构传感器协同感知中的关键挑战。"}}
{"id": "2507.13857", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13857", "abs": "https://arxiv.org/abs/2507.13857", "authors": ["Max van den Hoven", "Kishaan Jeeveswaran", "Pieter Piscaer", "Thijs Wensveen", "Elahe Arani", "Bahram Zonooz"], "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation", "comment": null, "summary": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.", "AI": {"tldr": "该论文提出Depth3DLane，一个双路径框架，通过集成自监督单目深度估计和相机参数预测，在不依赖昂贵传感器、额外深度真值或已知相机参数的情况下，实现准确的单目3D车道线检测。", "motivation": "单目3D车道线检测缺乏显式空间信息，极具挑战性。现有方法依赖昂贵深度传感器或难以大规模收集的深度真值数据，且通常假设相机参数已知，限制了其在众包高清车道图等场景中的应用。", "method": "Depth3DLane是一个双路径框架：1) 集成自监督单目深度估计网络，无需额外深度真值，生成场景点云；2) 鸟瞰图路径从点云中提取显式空间信息；3) 前视图路径同时提取丰富的语义信息。通过3D车道锚点从两条路径采样特征并推断3D车道几何。此外，框架扩展为逐帧预测相机参数，并引入理论驱动的拟合过程以增强逐段稳定性。", "result": "Depth3DLane在OpenLane基准数据集上取得了有竞争力的性能。实验结果表明，使用学习到的相机参数而非真值参数，使得Depth3DLane能够应用于相机标定不可行的场景，这是先前方法无法实现的。", "conclusion": "Depth3DLane通过创新的双路径设计、自监督深度估计和相机参数预测，克服了传统单目3D车道线检测对昂贵传感器、深度真值和已知相机参数的依赖，显著提高了该技术在自动驾驶和众包地图等实际应用中的普适性和实用性。"}}
{"id": "2507.13514", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13514", "abs": "https://arxiv.org/abs/2507.13514", "authors": ["Bhumika Laxman Sadbhave", "Philipp Vaeth", "Denise Dejon", "Gunther Schorcht", "Magda Gregorová"], "title": "Sugar-Beet Stress Detection using Satellite Image Time Series", "comment": null, "summary": "Satellite Image Time Series (SITS) data has proven effective for agricultural\ntasks due to its rich spectral and temporal nature. In this study, we tackle\nthe task of stress detection in sugar-beet fields using a fully unsupervised\napproach. We propose a 3D convolutional autoencoder model to extract meaningful\nfeatures from Sentinel-2 image sequences, combined with\nacquisition-date-specific temporal encodings to better capture the growth\ndynamics of sugar-beets. The learned representations are used in a downstream\nclustering task to separate stressed from healthy fields. The resulting stress\ndetection system can be directly applied to data from different years, offering\na practical and accessible tool for stress detection in sugar-beets.", "AI": {"tldr": "该研究提出了一种基于3D卷积自编码器的无监督方法，利用Sentinel-2卫星图像时间序列数据检测甜菜田的胁迫状况。", "motivation": "卫星图像时间序列数据在农业任务中表现出色，但甜菜胁迫检测仍需一种实用且可直接应用于不同年份数据的无监督方法。", "method": "核心方法是3D卷积自编码器模型，用于从Sentinel-2图像序列中提取特征，并结合采集日期特定的时间编码以捕捉生长动态。学习到的特征随后用于下游聚类任务，区分受胁迫和健康的田地。", "result": "该系统能有效将受胁迫田地与健康田地分离，且可以直接应用于不同年份的数据。", "conclusion": "该研究提供了一个实用且易于访问的甜菜胁迫检测工具，适用于不同年份的数据。"}}
{"id": "2507.13732", "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13732", "abs": "https://arxiv.org/abs/2507.13732", "authors": ["Guillaume Zambrano"], "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.", "AI": {"tldr": "本研究利用机器学习预测法国上诉法院的儿童抚养权判决结果，发现个体法官的决策模式对案件结果有显著影响，支持了法律现实主义观点。", "motivation": "本研究旨在检验个体法官的决策模式是否显著影响案件结果，挑战了法官是中立变量并统一适用法律的假设。这建立在法律现实主义与形式主义的辩论之上。", "method": "研究使用了从10,306个案件中提取的18,937份生活安排裁决。为遵守隐私法，数据经过严格的假名化处理。预测流程采用混合方法：使用大型语言模型（LLMs）进行结构化特征提取，然后使用机器学习模型（如RF、XGB和SVC）进行结果预测。研究比较了基于个体法官过往判决训练的“专家模型”和基于聚合数据训练的“通用模型”，并通过域内（In-Domain）和跨域（Cross-Domain）有效性测试进行验证。", "result": "结果显示，“专家模型”的预测准确率始终高于“通用模型”，表现最佳的专家模型F1分数高达92.85%，而通用模型（训练数据量是专家模型的20到100倍）的F1分数为82.63%。专家模型捕捉到了稳定的个体模式，这些模式不可转移到其他法官。域内和跨域有效性测试为法律现实主义提供了经验支持，表明司法身份在法律结果中扮演着可衡量的角色。", "conclusion": "本研究得出结论，法官的个体身份在法律判决中扮演着可衡量的角色，为法律现实主义提供了经验支持，挑战了法官是中立变量的传统假设。"}}
{"id": "2507.13739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13739", "abs": "https://arxiv.org/abs/2507.13739", "authors": ["Junsu Kim", "Yunhoe Ku", "Seungryul Baek"], "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning", "comment": "6th CLVISION ICCV Workshop accepted", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones.", "AI": {"tldr": "本文提出Diffusion-FSCIL，一种利用冻结的文生图扩散模型作为骨干网络来解决小样本类增量学习（FSCIL）挑战的新方法，通过提取多尺度特征和引入潜在回放机制，在有限数据下有效缓解灾难性遗忘并学习新知识。", "motivation": "小样本类增量学习（FSCIL）面临训练数据极度有限的挑战，同时需要减少灾难性遗忘并有效学习新信息。", "method": "Diffusion-FSCIL利用一个冻结的文生图扩散模型作为骨干网络。该方法利用大型生成模型的1）大规模预训练生成能力；2）多尺度表示；3）文本编码器带来的表示灵活性。为最大化表示能力，它提取互补的扩散特征作为潜在回放，并辅以特征蒸馏以避免生成偏差。该框架通过使用冻结骨干、最少可训练组件和批量处理多特征提取实现高效性。", "result": "在CUB-200、miniImageNet和CIFAR-100上的大量实验表明，Diffusion-FSCIL超越了现有最先进的方法，在保留先前学习类别性能的同时，有效适应了新类别。", "conclusion": "Diffusion-FSCIL通过利用大型文生图扩散模型的强大表示能力和生成潜力，为小样本类增量学习提供了一种高效且性能卓越的解决方案，有效克服了数据稀缺和灾难性遗忘的挑战。"}}
{"id": "2507.13527", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13527", "abs": "https://arxiv.org/abs/2507.13527", "authors": ["Levi Harris", "Md Jayed Hossain", "Mufan Qiu", "Ruichen Zhang", "Pingchuan Ma", "Tianlong Chen", "Jiaqi Gu", "Seth Ariel Tongay", "Umberto Celano"], "title": "SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM", "comment": null, "summary": "The increasing use of two-dimensional (2D) materials in nanoelectronics\ndemands robust metrology techniques for electrical characterization, especially\nfor large-scale production. While atomic force microscopy (AFM) techniques like\nconductive AFM (C-AFM) offer high accuracy, they suffer from slow data\nacquisition speeds due to the raster scanning process. To address this, we\nintroduce SparseC-AFM, a deep learning model that rapidly and accurately\nreconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM\nscans. Our approach is robust across various scanning modes, substrates, and\nexperimental conditions. We report a comparison between (a) classic flow\nimplementation, where a high pixel density C-AFM image (e.g., 15 minutes to\ncollect) is manually parsed to extract relevant material parameters, and (b)\nour SparseC-AFM method, which achieves the same operation using data that\nrequires substantially less acquisition time (e.g., under 5 minutes).\nSparseC-AFM enables efficient extraction of critical material parameters in\nMoS$_2$, including film coverage, defect density, and identification of\ncrystalline island boundaries, edges, and cracks. We achieve over 11x reduction\nin acquisition time compared to manual extraction from a full-resolution C-AFM\nimage. Moreover, we demonstrate that our model-predicted samples exhibit\nremarkably similar electrical properties to full-resolution data gathered using\nclassic-flow scanning. This work represents a significant step toward\ntranslating AI-assisted 2D material characterization from laboratory research\nto industrial fabrication. Code and model weights are available at\ngithub.com/UNITES-Lab/sparse-cafm.", "AI": {"tldr": "该研究引入了SparseC-AFM，一个深度学习模型，能从稀疏的导电原子力显微镜（C-AFM）扫描数据中快速准确地重建二维材料（如MoS2）的导电率图，大幅缩短了材料表征时间。", "motivation": "随着二维材料在纳米电子学中应用日益广泛，需要鲁棒的电学表征计量技术，尤其是在大规模生产中。传统的C-AFM技术虽然精度高，但由于光栅扫描过程导致数据采集速度慢，无法满足工业需求。", "method": "研究开发了一个名为SparseC-AFM的深度学习模型。该模型能够从稀疏的C-AFM扫描数据中快速准确地重建二维材料的导电率图。该方法在不同扫描模式、基底和实验条件下均表现出鲁棒性。", "result": "SparseC-AFM相较于传统手动从全分辨率C-AFM图像中提取数据的方法，采集时间减少了11倍以上（从15分钟缩短到5分钟以内）。它能高效提取MoS2的关键材料参数，包括薄膜覆盖率、缺陷密度以及晶体岛边界、边缘和裂纹的识别。模型预测的样本表现出与全分辨率数据高度相似的电学特性。", "conclusion": "这项工作是AI辅助二维材料表征从实验室研究走向工业制造的重要一步，显著提高了二维材料电学表征的效率和实用性。"}}
{"id": "2507.13743", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13743", "abs": "https://arxiv.org/abs/2507.13743", "authors": ["Maluna Menke", "Thilo Hagendorff"], "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs", "comment": null, "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.", "AI": {"tldr": "本研究评估了两种参数高效微调（PEFT）技术（LoRA和软提示调优）在减轻大型语言模型（LLMs）中对LGBTQIA+群体的偏见方面的有效性，发现LoRA能以极低的计算成本显著降低偏见。", "motivation": "大型语言模型（LLMs）经常复现训练数据中存在的性别和性身份偏见，导致输出歧视LGBTQIA+用户。因此，减少此类偏见至关重要。", "method": "研究评估了两种参数高效微调（PEFT）技术——低秩适应（LoRA）和软提示调优，作为全模型微调的轻量级替代方案。使用WinoQueer基准测试量化了三款开源LLM的偏见，并使用精选的QueerNews语料库对模型进行微调。", "result": "基线偏见分数高达98（满分100，50为中立）。通过LoRA（额外参数<0.1%）进行微调，偏见分数降低了多达50点，中立性从几乎0%提高到36%。软提示调优（10个虚拟token）仅带来微不足道的改进。", "conclusion": "LoRA能以最小的计算成本实现显著的公平性提升。研究倡导更广泛地采用社区参与的PEFT方法，创建更大的由酷儿群体创作的语料库，并开发比WinoQueer更丰富的评估套件，同时进行持续审计以保持LLMs的包容性。"}}
{"id": "2507.13769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13769", "abs": "https://arxiv.org/abs/2507.13769", "authors": ["Mingyang Yu", "Zhijian Wu", "Dingjiang Huang"], "title": "Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its\ndegraded 2D measurements. Recently great progress has been made in deep\nlearning-based methods, however, these methods often struggle to accurately\ncapture high-frequency details of the HSI. To address this issue, this paper\nproposes a Spectral Diffusion Prior (SDP) that is implicitly learned from\nhyperspectral images using a diffusion model. Leveraging the powerful ability\nof the diffusion model to reconstruct details, this learned prior can\nsignificantly improve the performance when injected into the HSI model. To\nfurther improve the effectiveness of the learned prior, we also propose the\nSpectral Prior Injector Module (SPIM) to dynamically guide the model to recover\nthe HSI details. We evaluate our method on two representative HSI methods: MST\nand BISRNet. Experimental results show that our method outperforms existing\nnetworks by about 0.5 dB, effectively improving the performance of HSI\nreconstruction.", "AI": {"tldr": "本文提出了一种基于扩散模型的频谱扩散先验（SDP）和频谱先验注入模块（SPIM），以提升高光谱图像（HSI）重建中高频细节的恢复能力。", "motivation": "现有的深度学习高光谱图像（HSI）重建方法难以准确捕捉HSI的高频细节。", "method": "1. 提出频谱扩散先验（SDP），利用扩散模型从高光谱图像中隐式学习。2. 提出频谱先验注入模块（SPIM），用于动态引导模型恢复HSI细节。3. 在MST和BISRNet两种代表性HSI方法上进行评估。", "result": "实验结果显示，该方法在HSI重建性能上超越现有网络约0.5 dB，有效提升了HSI重建性能。", "conclusion": "通过学习到的频谱扩散先验和提出的注入模块，该方法能显著提高高光谱图像重建的性能，尤其是在高频细节恢复方面。"}}
{"id": "2507.13530", "categories": ["cs.CV", "math.DG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.13530", "abs": "https://arxiv.org/abs/2507.13530", "authors": ["Lukas Baumgärtner", "Ronny Bergmann", "Roland Herzog", "Stephan Schmidt", "Manuel Weiß"], "title": "Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising", "comment": null, "summary": "We propose a novel formulation for the second-order total generalized\nvariation (TGV) of the normal vector on an oriented, triangular mesh embedded\nin $\\mathbb{R}^3$. The normal vector is considered as a manifold-valued\nfunction, taking values on the unit sphere. Our formulation extends previous\ndiscrete TGV models for piecewise constant scalar data that utilize a\nRaviart-Thomas function space. To exctend this formulation to the manifold\nsetting, a tailor-made tangential Raviart-Thomas type finite element space is\nconstructed in this work. The new regularizer is compared to existing methods\nin mesh denoising experiments.", "AI": {"tldr": "本文提出了一种针对三维三角网格上法向量的二阶全广义变分（TGV）新公式，该公式将法向量视为流形值函数，并构建了一个定制的切向Raviart-Thomas有限元空间，用于网格去噪。", "motivation": "现有的离散TGV模型主要针对分段常数标量数据，并利用Raviart-Thomas函数空间。为了将TGV扩展到流形值函数（如单位球上的法向量），需要一种新的定制化方法。", "method": "研究者提出了一种新的二阶全广义变分（TGV）公式，用于嵌入在三维空间中的定向三角网格上的法向量。法向量被视为在单位球面上取值的流形值函数。为实现这一目标，文中构建了一个定制的切向Raviart-Thomas型有限元空间。", "result": "该新正则化器在网格去噪实验中与现有方法进行了比较。", "conclusion": "该研究提出了一种处理网格法向量TGV的新颖且定制化的方法，为网格去噪提供了新的正则化工具。"}}
{"id": "2507.13761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13761", "abs": "https://arxiv.org/abs/2507.13761", "authors": ["Palash Nandi", "Maithili Joshi", "Tanmoy Chakraborty"], "title": "Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models", "comment": null, "summary": "Language models are highly sensitive to prompt formulations - small changes\nin input can drastically alter their output. This raises a critical question:\nTo what extent can prompt sensitivity be exploited to generate inapt content?\nIn this paper, we investigate how discrete components of prompt design\ninfluence the generation of inappropriate content in Visual Language Models\n(VLMs). Specifically, we analyze the impact of three key factors on successful\njailbreaks: (a) the inclusion of detailed visual information, (b) the presence\nof adversarial examples, and (c) the use of positively framed beginning\nphrases. Our findings reveal that while a VLM can reliably distinguish between\nbenign and harmful inputs in unimodal settings (text-only or image-only), this\nability significantly degrades in multimodal contexts. Each of the three\nfactors is independently capable of triggering a jailbreak, and we show that\neven a small number of in-context examples (as few as three) can push the model\ntoward generating inappropriate outputs. Furthermore, we propose a framework\nthat utilizes a skip-connection between two internal layers of the VLM, which\nsubstantially increases jailbreak success rates, even when using benign images.\nFinally, we demonstrate that memes, often perceived as humorous or harmless,\ncan be as effective as toxic visuals in eliciting harmful content, underscoring\nthe subtle and complex vulnerabilities of VLMs.", "AI": {"tldr": "本文研究了视觉语言模型（VLMs）中提示敏感性如何被利用来生成不当内容，发现多模态环境下模型的安全识别能力显著下降，且特定的提示设计元素（如详细视觉信息、对抗性示例、积极开头的短语、少量上下文示例甚至模因）都能有效触发越狱。", "motivation": "语言模型对提示表述高度敏感，微小输入变化可大幅改变输出。这引发了一个关键问题：提示敏感性在多大程度上可以被利用来生成不当内容，尤其是在视觉语言模型中。", "method": "研究通过分析提示设计的离散组件对VLMs生成不当内容的影响，具体考察了三个关键因素：包含详细视觉信息、存在对抗性示例以及使用积极框架的起始短语。此外，还探讨了少量上下文示例的作用，并提出了一个利用VLM内部两层之间跳跃连接的框架。最后，还测试了模因作为触发有害内容的有效性。", "result": "研究发现，VLM在单模态（仅文本或仅图像）设置下能可靠区分良性和有害输入，但在多模态环境下此能力显著下降。上述三个因素（详细视觉信息、对抗性示例、积极开头短语）均能独立触发越狱。即使少量（低至三个）上下文示例也能促使模型生成不当输出。提出的跳跃连接框架显著提高了越狱成功率，即使使用良性图像。模因与有害视觉内容一样能有效引发有害内容。", "conclusion": "VLMs存在复杂而微妙的漏洞，尤其是在多模态上下文中，提示设计中的离散组件（包括看似无害的元素如模因）可以被利用来绕过安全机制，导致生成不当内容。"}}
{"id": "2507.13789", "categories": ["cs.CV", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.13789", "abs": "https://arxiv.org/abs/2507.13789", "authors": ["Kyriakos Flouris", "Moritz Halter", "Yolanne Y. R. Lee", "Samuel Castonguay", "Luuk Jacobs", "Pietro Dirix", "Jonathan Nestmann", "Sebastian Kozerke", "Ender Konukoglu"], "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI", "comment": null, "summary": "Hemodynamic analysis is essential for predicting aneurysm rupture and guiding\ntreatment. While magnetic resonance flow imaging enables time-resolved\nvolumetric blood velocity measurements, its low spatiotemporal resolution and\nsignal-to-noise ratio limit its diagnostic utility. To address this, we propose\nthe Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that\nenhances both spatial and temporal resolution with the ability to predict wall\nshear stress (WSS) directly from clinical imaging data. LoFNO integrates\nLaplacian eigenvectors as geometric priors for improved structural awareness on\nirregular, unseen geometries and employs an Enhanced Deep Super-Resolution\nNetwork (EDSR) layer for robust upsampling. By combining geometric priors with\nneural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow\ndata, achieving superior velocity and WSS predictions compared to interpolation\nand alternative deep learning methods, enabling more precise cerebrovascular\ndiagnostics.", "AI": {"tldr": "LoFNO是一种新型3D傅里叶神经算子，用于提高MR血流成像的时空分辨率并直接预测壁面剪应力，以改善脑血管诊断。", "motivation": "磁共振血流成像的时空分辨率和信噪比低，限制了其在动脉瘤破裂预测和治疗指导中血流动力学分析的诊断效用。", "method": "本文提出了局部傅里叶神经算子（LoFNO），一个新颖的3D架构。它整合了拉普拉斯特征向量作为几何先验以增强对不规则、未见几何体的结构感知，并采用增强深度超分辨率网络（EDSR）层进行鲁棒上采样。LoFNO结合几何先验和神经算子框架，对血流数据进行去噪和时空上采样。", "result": "LoFNO在速度和壁面剪应力（WSS）预测方面优于插值法和替代深度学习方法。", "conclusion": "LoFNO通过提高分辨率并直接从临床影像数据预测WSS，实现了更精确的脑血管诊断。"}}
{"id": "2507.13546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13546", "abs": "https://arxiv.org/abs/2507.13546", "authors": ["Dmitrii Mikhailov", "Aleksey Letunovskiy", "Maria Kovaleva", "Vladimir Arkhipkin", "Vladimir Korviakov", "Vladimir Polovnikov", "Viacheslav Vasilev", "Evelina Sidorova", "Denis Dimitrov"], "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention", "comment": null, "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA", "AI": {"tldr": "NABLA提出了一种新颖的邻域自适应块级注意力机制，通过利用自适应稀疏驱动阈值的块级注意力，显著加速了视频生成Transformer的训练和推理，同时保持了生成质量。", "motivation": "Transformer架构在视频生成中表现出色，但全注意力机制的二次复杂度对于高分辨率和长持续时间视频序列而言是一个关键瓶颈。", "method": "本文提出NABLA（Neighborhood Adaptive Block-Level Attention），一种动态适应视频扩散Transformer（DiTs）中稀疏模式的机制。它通过块级注意力与自适应稀疏驱动阈值相结合，减少计算开销。该方法不需要自定义底层操作，可无缝集成PyTorch的Flex Attention操作。", "result": "实验表明，与基线相比，NABLA在训练和推理速度上提高了2.7倍，几乎不损害量化指标（CLIP分数、VBench分数、人类评估分数）和视觉质量。", "conclusion": "NABLA有效解决了视频生成Transformer中全注意力机制的计算瓶颈，通过自适应稀疏块级注意力显著提升了效率，同时保持了卓越的生成质量。"}}
{"id": "2507.13793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13793", "abs": "https://arxiv.org/abs/2507.13793", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Xuemeng Song", "Tian Gan", "Liqiang Nie"], "title": "An Enhanced Model-based Approach for Short Text Clustering", "comment": null, "summary": "Short text clustering has become increasingly important with the popularity\nof social media like Twitter, Google+, and Facebook. Existing methods can be\nbroadly categorized into two paradigms: topic model-based approaches and deep\nrepresentation learning-based approaches. This task is inherently challenging\ndue to the sparse, large-scale, and high-dimensional characteristics of the\nshort text data. Furthermore, the computational intensity required by\nrepresentation learning significantly increases the running time. To address\nthese issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet\nMultinomial Mixture model (GSDMM), which effectively handles the sparsity and\nhigh dimensionality of short texts while identifying representative words for\neach cluster. Based on several aspects of GSDMM that warrant further\nrefinement, we propose an improved approach, GSDMM+, designed to further\noptimize its performance. GSDMM+ reduces initialization noise and adaptively\nadjusts word weights based on entropy, achieving fine-grained clustering that\nreveals more topic-related information. Additionally, strategic cluster merging\nis employed to refine clustering granularity, better aligning the predicted\ndistribution with the true category distribution. We conduct extensive\nexperiments, comparing our methods with both classical and state-of-the-art\napproaches. The experimental results demonstrate the efficiency and\neffectiveness of our methods. The source code for our model is publicly\navailable at https://github.com/chehaoa/VEMC.", "AI": {"tldr": "本文提出了一种基于Dirichlet多项式混合模型的折叠Gibbs采样算法（GSDMM）及其改进版本GSDMM+，用于解决短文本聚类中的稀疏性、高维度和计算强度问题，并显著提升了聚类效果。", "motivation": "随着社交媒体的普及，短文本聚类变得日益重要。然而，短文本数据固有的稀疏性、大规模和高维度特性，以及表示学习方法所需的巨大计算量，使得短文本聚类任务极具挑战性。", "method": "本文提出了GSDMM（Dirichlet多项式混合模型的折叠Gibbs采样算法），有效处理短文本的稀疏性和高维度，并识别每个簇的代表词。在此基础上，提出了改进的GSDMM+，通过减少初始化噪声、基于熵自适应调整词权重以实现更细粒度的聚类，并采用策略性簇合并来优化聚类粒度，使预测分布更好地符合真实类别分布。", "result": "通过与经典和最先进方法的广泛实验比较，结果表明本文提出的方法（GSDMM和GSDMM+）具有高效性和有效性。", "conclusion": "GSDMM及其改进版GSDMM+能够高效且有效地处理短文本聚类中的挑战，提供高质量的聚类结果，并揭示更多与主题相关的信息。"}}
{"id": "2507.13801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13801", "abs": "https://arxiv.org/abs/2507.13801", "authors": ["Haoang Lu", "Yuanqi Su", "Xiaoning Zhang", "Hao Hu"], "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion", "comment": null, "summary": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.", "AI": {"tldr": "本文提出了一种名为CF-SSC的新型时序3D语义场景补全（SSC）框架，通过预测伪未来帧来扩展感知范围，有效解决了单目SSC方法在处理遮挡和视场外区域时的局限性，显著提升了3D场景补全的准确性。", "motivation": "现有的单目3D语义场景补全（SSC）方法无法充分解决真实交通场景中大量被遮挡或超出摄像机视野的区域，这严重限制了其在自动驾驶中的应用。", "method": "本文提出了CF-SSC框架，其核心方法包括：1) 利用伪未来帧预测来扩展模型有效感知范围；2) 结合姿态和深度信息建立准确的3D对应关系；3) 在3D空间中几何一致地融合过去、现在和预测的未来帧；4) 采用3D感知架构显式建模时空关系，而非简单的特征堆叠。", "result": "CF-SSC在SemanticKITTI和SSCBench-KITTI-360基准测试上均展现了最先进的性能，验证了其有效性，并显著提高了遮挡推理能力和3D场景补全的准确性。", "conclusion": "CF-SSC通过引入时序信息和伪未来帧预测，成功克服了传统单目SSC方法在处理遮挡和视场外区域的局限性，为自动驾驶中的3D场景理解提供了更鲁棒和准确的解决方案。"}}
{"id": "2507.13568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13568", "abs": "https://arxiv.org/abs/2507.13568", "authors": ["Kaihong Wang", "Donghyun Kim", "Margrit Betke"], "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning", "comment": null, "summary": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.", "AI": {"tldr": "本文提出了一种LoRA增强的合成回放框架，通过将任务特定的低秩适配器注入到Stable Diffusion模型中，以生成更符合领域特性的合成样本，从而提高视觉-语言模型(VLM)的持续学习性能。", "motivation": "现有的合成回放方法在处理真实世界中领域特定和细粒度语义的任务时，由于生成器无法充分捕捉这些细微差别，导致生成的样本与实际不符，进而误导微调过程并损害对先前知识的保留。", "method": "该方法将任务特定的低秩适配器（LoRA）注入到冻结的Stable Diffusion模型中，以高效捕捉每个新任务独特的视觉和语义模式。具体而言，它引入了一个两阶段、基于置信度的样本选择机制：首先，根据VLM的后微调置信度对真实任务数据进行排序，以选择最具代表性的样本来微调LoRA；然后，生成合成样本并再次根据置信度选择它们进行知识蒸馏。该方法可无缝集成到现有回放管道中。", "result": "在多领域任务增量学习（MTIL）基准测试上，该方法优于先前的合成回放技术，在可塑性、稳定性和零样本能力之间实现了最佳平衡。", "conclusion": "通过LoRA进行生成器适配，可以有效提升视觉-语言模型在持续学习中的鲁棒性。"}}
{"id": "2507.13827", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13827", "abs": "https://arxiv.org/abs/2507.13827", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.", "AI": {"tldr": "该研究旨在通过问答（QA）对的形式从科学文章中提取关键概念和贡献。提出了两种方法：一种基于大型语言模型（LLM）从文章内容生成QA，另一种基于知识图谱（KG）并利用实体关系（ER）提取。评估显示，基于KG的方法能有效捕捉文章主旨，且ER提取模型在特定语料库上的微调至关重要。", "motivation": "学者在决定阅读或引用文章时，需要快速识别和理解其主要思想和贡献。本研究旨在解决这一需求，通过自动化生成QA对来帮助学者高效获取信息。", "method": "本研究提出了两种生成QA对的方法：\n1.  **基于LLM的方法**：选择文章中显著的段落，使用大型语言模型生成问题，根据获得有意义答案的可能性对问题进行排序，然后生成答案。此方法仅依赖文章内容。\n2.  **基于KG的方法**：通过在科学文章上微调实体关系（ER）提取模型来构建知识图谱。利用显著三元组提取方法（基于三元组TF-IDF类度量，评估三元组在文章中的重要性与在文献中的普遍性）来选择每篇文章最相关的ER。两种方法生成的QA对由主题专家（SMEs）通过预定义指标进行评估。", "result": "评估结果表明：\n1.  基于知识图谱（KG）的方法能够有效捕捉文章讨论的主要思想。\n2.  在科学语料库上对实体关系（ER）提取模型进行微调，对于从这类文档中提取高质量的三元组至关重要。", "conclusion": "通过生成问答对来提取科学文章的关键概念和贡献是可行的。其中，基于知识图谱的方法在捕捉文章主旨方面表现更优，并且针对特定科学语料库对实体关系提取模型进行微调是获得高质量结果的关键。"}}
{"id": "2507.13820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13820", "abs": "https://arxiv.org/abs/2507.13820", "authors": ["Jun Xie", "Zhaoran Zhao", "Xiongjun Guan", "Yingjian Zhu", "Hongzhu Yi", "Xinming Wang", "Feng Chen", "Zhepeng Wang"], "title": "Team of One: Cracking Complex Video QA with Model Synergy", "comment": null, "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.", "AI": {"tldr": "本文提出了一种新颖的开放式视频问答框架，通过协调多个异构视频-语言模型（VLM）并利用外部大型语言模型（LLM）进行评估和集成，显著提升了推理深度、泛化能力和鲁棒性，无需模型再训练。", "motivation": "现有视频大型多模态模型（Video-LMMs）在上下文理解、时间建模以及对模糊或复合查询的泛化能力方面存在局限性，促使研究人员寻求更深层次的推理和更强的鲁棒性。", "method": "该研究引入了一种提示-响应集成机制，通过结构化的思维链协调多个异构视频-语言模型（VLM），每个思维链都针对不同的推理路径。此外，一个外部大型语言模型（LLM）被用作评估器和集成器，负责选择并融合最可靠的响应。", "result": "实验结果表明，该方法在所有评估指标上均显著优于现有基线，展现出卓越的泛化能力和鲁棒性。", "conclusion": "该方法提供了一种轻量级、可扩展的多模态推理策略，无需模型再训练，为未来的视频大型多模态模型（Video-LMM）发展奠定了坚实基础。"}}
{"id": "2507.13595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13595", "abs": "https://arxiv.org/abs/2507.13595", "authors": ["Tengkai Wang", "Weihao Li", "Ruikai Cui", "Shi Qiu", "Nick Barnes"], "title": "NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision", "comment": "14 pages, 4 figures", "summary": "Reconstructing accurate implicit surface representations from point clouds\nremains a challenging task, particularly when data is captured using\nlow-quality scanning devices. These point clouds often contain substantial\nnoise, leading to inaccurate surface reconstructions. Inspired by the\nNoise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel\nmethod designed to extend this concept to 3D neural fields. Our approach\nenables learning clean neural SDFs directly from noisy point clouds through\nnoisy supervision by minimizing the MSE loss between noisy SDF representations,\nallowing the network to implicitly denoise and refine surface estimations. We\nevaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the\nShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that\nour framework significantly improves surface reconstruction quality from noisy\ninputs.", "AI": {"tldr": "本文提出NoiseSDF2NoiseSDF方法，将Noise2Noise范式扩展到3D神经场，通过最小化噪声SDF表示之间的MSE损失，直接从噪声点云中学习干净的神经SDF，从而提高表面重建质量。", "motivation": "从点云重建精确的隐式表面表示是一个挑战，尤其当数据来源于低质量扫描设备时，点云包含大量噪声，导致重建不准确。", "method": "引入NoiseSDF2NoiseSDF，灵感来源于2D图像的Noise2Noise范式，将其扩展到3D神经场。该方法通过最小化噪声SDF表示之间的均方误差（MSE）损失，利用噪声监督直接从噪声点云中学习干净的神经SDF，使网络能够隐式去噪并优化表面估计。", "result": "在ShapeNet、ABC、Famous和Real等基准数据集上的实验结果表明，该框架显著提高了从噪声输入中重建表面的质量。", "conclusion": "NoiseSDF2NoiseSDF框架能够有效提升从噪声点云中进行3D表面重建的质量，通过噪声监督实现隐式去噪和表面优化。"}}
{"id": "2507.13839", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13839", "abs": "https://arxiv.org/abs/2507.13839", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.", "AI": {"tldr": "本研究发现，在中文心理咨询中，负面情绪词的使用频率与抑郁和焦虑的严重程度呈正相关，但第一人称单数代词的使用频率与心理状态无关，这与西方研究结果不同，凸显了文化和对话情境对语言使用的影响。", "motivation": "本研究旨在探讨中文心理咨询互动中语言表达（特别是第一人称单数代词和负面情绪词）与抑郁和焦虑心理状态之间的关系，以填补主要基于英语语境的现有研究空白。", "method": "研究利用包含735个在线咨询会话的语料库，通过语言查询与词语计数（LIWC）软件量化语言模式，并采用广义线性混合效应模型进行统计分析。", "result": "结果显示，负面情绪词的使用频率与来访者的抑郁和焦虑状态严重程度呈显著正相关。然而，与先前主要基于英语语境的研究不同，第一人称单数代词的使用频率并未随来访者的心理状况显著变化。", "conclusion": "研究结果强调了集体主义的中国文化背景和个体主义的西方背景之间的文化差异，以及心理咨询对话独特的互动动态，对心理健康沟通中语言使用的细微影响。这为中文语境下的治疗实践提供了相关的心理语言学标记见解。"}}
{"id": "2507.13868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13868", "abs": "https://arxiv.org/abs/2507.13868", "authors": ["Francesco Ortu", "Zhijing Jin", "Diego Doimo", "Alberto Cazzaniga"], "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.", "AI": {"tldr": "本文分析了视觉-语言模型（VLMs）如何解决其内部参数化知识与外部视觉输入之间的跨模态冲突。研究通过引入反事实查询数据集，定位了控制冲突解决的特定注意力头，并展示了这些头可以引导模型行为，并精确识别驱动视觉覆盖的图像区域。", "motivation": "VLMs在处理复杂任务时，常遇到其内部参数知识与外部信息之间的冲突，这可能导致幻觉和不可靠的响应。然而，控制这种交互的机制尚不明确，本文旨在填补这一空白。", "method": "1. 引入一个多模态反事实查询数据集，故意与模型内部的常识知识相矛盾。2. 使用logit检查定位控制冲突的一小部分注意力头。3. 通过修改这些注意力头，引导模型偏向其内部知识或视觉输入。4. 展示这些头的注意力可以精确识别驱动视觉覆盖的局部图像区域。", "result": "1. 成功定位了一小部分控制VLMs知识冲突的注意力头。2. 证明通过修改这些头，可以有效引导模型倾向于其内部知识或视觉输入。3. 发现这些头的注意力在定位驱动视觉覆盖的图像区域方面，比基于梯度的归因方法更精确。", "conclusion": "VLMs中存在特定的注意力头负责解决跨模态知识冲突。这些头不仅能够控制模型的行为，使其倾向于内部知识或外部视觉输入，还能精确地定位图像中驱动视觉覆盖的关键区域，其精度优于传统归因方法。"}}
{"id": "2507.13599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13599", "abs": "https://arxiv.org/abs/2507.13599", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model", "comment": "Accepted by ICCV2025", "summary": "Since acquiring large amounts of realistic blurry-sharp image pairs is\ndifficult and expensive, learning blind image deblurring from unpaired data is\na more practical and promising solution. Unfortunately, dominant approaches\nrely heavily on adversarial learning to bridge the gap from blurry domains to\nsharp domains, ignoring the complex and unpredictable nature of real-world blur\npatterns. In this paper, we propose a novel diffusion model (DM)-based\nframework, dubbed \\ours, for image deblurring by learning spatially varying\ntexture prior from unpaired data. In particular, \\ours performs DM to generate\nthe prior knowledge that aids in recovering the textures of blurry images. To\nimplement this, we propose a Texture Prior Encoder (TPE) that introduces a\nmemory mechanism to represent the image textures and provides supervision for\nDM training. To fully exploit the generated texture priors, we present the\nTexture Transfer Transformer layer (TTformer), in which a novel\nFilter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes\nspatially varying blurring through adaptive filtering. Furthermore, we\nimplement a wavelet-based adversarial loss to preserve high-frequency texture\ndetails. Extensive evaluations show that \\ours provides a promising\nunsupervised deblurring solution and outperforms SOTA methods in widely-used\nbenchmarks.", "AI": {"tldr": "本文提出一个名为\\ours的基于扩散模型（DM）的框架，通过从无配对数据中学习空间变化的纹理先验来解决盲图像去模糊问题，并在基准测试中超越了现有技术。", "motivation": "获取大量真实的模糊-清晰图像对既困难又昂贵，因此从无配对数据中学习盲图像去模糊是一种更实用和有前景的解决方案。然而，主流方法过度依赖对抗学习来弥合模糊域与清晰域之间的差距，却忽视了真实世界模糊模式的复杂性和不可预测性。", "method": "本文提出一个名为\\ours的基于扩散模型的框架。\\ours利用DM生成先验知识以恢复模糊图像的纹理。为此，引入了纹理先验编码器（TPE），它通过记忆机制表示图像纹理并为DM训练提供监督。为了充分利用生成的纹理先验，提出了纹理传输Transformer层（TTformer），其中包含一个新颖的滤波器调制多头自注意力（FM-MSA）机制，通过自适应滤波有效去除空间变化的模糊。此外，还引入了基于小波的对抗损失以保留高频纹理细节。", "result": "广泛的评估表明，\\ours提供了一个有前景的无监督去模糊解决方案，并在广泛使用的基准测试中超越了现有最先进的方法。", "conclusion": "\\ours框架通过学习空间变化的纹理先验，实现了有效的无监督图像去模糊，并在性能上优于现有方法，证明了其作为实用去模糊解决方案的潜力。"}}
{"id": "2507.13841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13841", "abs": "https://arxiv.org/abs/2507.13841", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "title": "Modeling Fair Play in Detective Stories with Language Models", "comment": null, "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.", "AI": {"tldr": "本文提出一个用于侦探小说的概率框架，以量化“公平竞争”和故事连贯性与惊喜之间的平衡，并发现LLM生成的故事难以达到这种平衡。", "motivation": "有效叙事需要在满足读者预期与引入意外发展之间取得平衡。在侦探小说中，这种平衡被称为“公平竞争”，即作者与读者之间关于故事可能结局的隐含约定。研究旨在形式化定义和衡量这些品质，并评估LLM生成侦探故事的能力。", "method": "研究构建了一个侦探小说的概率框架，在此框架下正式定义了“公平竞争”及其度量指标。基于这些定义，提出了故事连贯性与惊喜之间的内在张力。通过将此框架应用于LLM生成的侦探故事来验证其有效性。", "result": "研究结果表明，尽管LLM生成的故事可能不可预测，但它们通常无法平衡惊喜与“公平竞争”之间的权衡，这极大地导致了其质量不佳。", "conclusion": "LLM在生成侦探小说时，虽然能产生不可预测性，但难以在故事的连贯性和惊喜之间取得恰当的“公平竞争”平衡，这是其故事质量欠佳的主要原因。"}}
{"id": "2507.13880", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13880", "abs": "https://arxiv.org/abs/2507.13880", "authors": ["Marten Kreis", "Benjamin Kiefer"], "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision", "comment": null, "summary": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.", "AI": {"tldr": "本文提出一种通过融合实时视频与海图信息来增强海洋视觉的新方法，利用基于Transformer的网络实现浮标检测与海图数据的精确匹配，显著提升了目标定位和关联精度。", "motivation": "现有海洋视觉系统在动态和复杂环境下，对导航辅助设备（如浮标）的定位和关联精度不足，需要一种更鲁棒的方法来融合实时视觉数据和海图信息。", "method": "该方法通过将检测到的导航辅助设备（如浮标）与其在海图数据中的对应表示进行精确匹配，将海图数据叠加到实时视频流上。为此，引入了一个基于Transformer的端到端神经网络，用于预测浮标查询的边界框和置信度分数，从而实现图像域检测与世界空间海图标记的直接匹配。该方法与基线方法（包括通过相机投影估计浮标位置的射线投射模型和扩展了距离估计模块的YOLOv7网络）进行了比较。", "result": "在真实世界海洋场景数据集上的实验结果表明，该方法显著提高了动态和挑战性环境中目标的定位和关联精度。", "conclusion": "所提出的融合实时视觉数据与海图信息的方法，特别是采用基于Transformer的网络进行浮标匹配，有效增强了海洋视觉，并大幅提升了目标定位和关联的准确性。"}}
{"id": "2507.13607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13607", "abs": "https://arxiv.org/abs/2507.13607", "authors": ["Kento Kawai", "Takeru Oba", "Kyotaro Tokoro", "Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Burst Super-Resolution with One-step Diffusion", "comment": "NTIRE2025", "summary": "While burst Low-Resolution (LR) images are useful for improving their Super\nResolution (SR) image compared to a single LR image, prior burst SR methods are\ntrained in a deterministic manner, which produces a blurry SR image. Since such\nblurry images are perceptually degraded, we aim to reconstruct sharp and\nhigh-fidelity SR images by a diffusion model. Our method improves the\nefficiency of the diffusion model with a stochastic sampler with a high-order\nODE as well as one-step diffusion using knowledge distillation. Our\nexperimental results demonstrate that our method can reduce the runtime to 1.6\n% of its baseline while maintaining the SR quality measured based on image\ndistortion and perceptual quality.", "AI": {"tldr": "该论文提出一种基于扩散模型的高效爆发式超分辨率（Burst SR）方法，通过改进采样器和知识蒸馏，显著减少运行时间，同时保持图像清晰度和感知质量。", "motivation": "现有的爆发式超分辨率方法采用确定性训练，导致生成的超分辨率图像模糊，感知质量下降。本研究旨在通过扩散模型重建清晰、高保真的超分辨率图像。", "method": "该方法利用扩散模型进行图像重建。为提高效率，采用了基于高阶ODE的随机采样器，并通过知识蒸馏实现一步扩散（one-step diffusion）。", "result": "实验结果表明，该方法能将运行时间缩短至基线的1.6%，同时在图像失真和感知质量方面保持了超分辨率图像的质量。", "conclusion": "所提出的基于扩散模型的爆发式超分辨率方法能够高效地生成清晰、高保真的超分辨率图像，解决了现有方法产生的模糊问题，并显著提升了运行效率。"}}
{"id": "2507.13858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13858", "abs": "https://arxiv.org/abs/2507.13858", "authors": ["Nicolò Brunello", "Davide Rigamonti", "Andrea Sassella", "Vincenzo Scotti", "Mark James Carman"], "title": "InTraVisTo: Inside Transformer Visualisation Tool", "comment": "8 pages", "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.", "AI": {"tldr": "本文介绍了一种名为InTraVisTo的新工具，旨在可视化和追踪基于Transformer的大型语言模型（LLMs）内部的计算过程，以帮助研究人员理解其推理机制。", "motivation": "尽管LLMs的推理能力显著提升，但由于其不可预测性以及期望行为与实际输出之间的差异，在生产环境中使用LLMs仍面临挑战。因此，需要深入理解LLMs内部的计算过程。", "method": "开发了InTraVisTo工具，它通过解码模型每一层的token嵌入来可视化Transformer模型的内部状态，并使用Sankey图可视化模型不同层之间各组件的信息流。", "result": "本文引入了InTraVisTo工具，该工具能够让研究人员调查和追踪Transformer-based LLM中每个token的生成计算过程，提供模型内部状态和信息流的可视化。", "conclusion": "InTraVisTo旨在帮助研究人员和实践者更好地理解Transformer模型内部的计算，从而揭示LLMs所采用的内部模式和推理过程。"}}
{"id": "2507.13881", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13881", "abs": "https://arxiv.org/abs/2507.13881", "authors": ["Cole Walsh", "Rodica Ivan", "Muhammad Zafar Iqbal", "Colleen Robb"], "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test", "comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States", "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.", "AI": {"tldr": "本文提出一种利用大型语言模型（LLMs）从情境判断测试（SJTs）的开放式回答中提取与构念相关的特征的新方法，旨在实现个人和专业技能的自动化评估。", "motivation": "学术项目日益重视个人和专业技能，但缺乏可扩展的系统来测量和评估这些技能。开放式SJTs传统上依赖人工评分，难以大规模应用，而以往基于自然语言处理（NLP）的评分系统在构念效度方面存在不足。", "method": "研究探索了一种新颖的方法，利用大型语言模型（LLMs）从SJTs的回答中提取与构念相关的特征。该方法以Casper SJT为例进行了效果验证。", "result": "研究表明，利用LLMs从SJT回答中提取特征的方法是有效的，为个人和专业技能的自动化评分奠定了基础。", "conclusion": "该研究为未来开发个人和专业技能的自动化评分系统奠定了基础，解决了传统开放式SJT评估中规模化和构念效度不足的问题。"}}
{"id": "2507.13609", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13609", "abs": "https://arxiv.org/abs/2507.13609", "authors": ["Yanan Wang", "Julio Vizcarra", "Zhi Li", "Hao Niu", "Mori Kurokawa"], "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks", "comment": null, "summary": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning.", "AI": {"tldr": "CoTasks是一个新框架，通过将复杂视频问题分解为实体级基础任务，为视频大语言模型（VideoLLMs）提供链式思考（CoT）能力，显著提升了模型在细粒度视频理解和组合推理方面的性能。", "motivation": "现有视频大语言模型（VideoLLMs）在细粒度物体级视频理解中缺乏链式思考（CoT）能力，因为它们通常在缺乏结构化标注的高级视频-文本对上进行训练，难以进行组合式、逐步推理。", "method": "本文提出了CoTasks框架，它将现有数据集中（如NeXT-QA, STAR）的复杂视频问题分解为四个实体级基础任务：帧定位、实体追踪、空间关系提取和时间关系提取。通过将这些中间的CoT式推理步骤嵌入到输入中，CoTasks使模型能够明确执行以物体为中心的时空推理。", "result": "在NeXT-QA基准测试中，CoTasks显著提升了推理性能：LLaVA-video-7B的平均GPT-4评估分数提高了+3.3分，Qwen2.5-VL-3B则提升了+17.4分，尤其在因果关系（+14.6）、时间关系（+10.9）和描述性（+48.1）子类别中表现出巨大提升。", "conclusion": "CoTasks作为一种结构化的CoT式监督框架，能有效提升视频大语言模型的组合式视频推理能力。"}}
{"id": "2507.13870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13870", "abs": "https://arxiv.org/abs/2507.13870", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.", "AI": {"tldr": "网络安全NER领域缺乏标准化标签，导致数据集难以整合。本研究尝试统一标签，并评估了多种模型（BiLSTM、多头模型、图基迁移模型）在统一数据集上的跨数据集泛化能力，发现效果不佳。", "motivation": "网络安全命名实体识别（NER）领域缺乏标准化标签，导致现有数据集难以有效结合，限制了数据资源的可用性。", "method": "研究人员对四个网络安全数据集进行了粗粒度标签统一，并使用BiLSTM模型进行成对的跨数据集评估。为解决统一限制，提出了包括多头模型（带权重共享）和基于图的迁移模型（基于BERT-base-NER）的替代架构。", "result": "结果显示，在统一数据集上训练的模型在跨数据集泛化能力方面表现不佳。带权重共享的多头模型相比统一训练仅提供微小改进，而基于图的迁移模型（基于BERT-base-NER）与BERT-base-NER相比没有显著性能提升。", "conclusion": "网络安全NER数据集的标签统一面临挑战，简单统一训练或提出的多头及图基迁移模型未能显著提高跨数据集的泛化能力，表明该领域仍需更有效的标签标准化和模型架构来应对数据整合问题。"}}
{"id": "2507.13913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13913", "abs": "https://arxiv.org/abs/2507.13913", "authors": ["Matous Volf", "Jakub Simko"], "title": "Political Leaning and Politicalness Classification of Texts", "comment": null, "summary": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.", "AI": {"tldr": "本文通过整合现有数据集并创建新数据集，使用Transformer模型解决了政治倾向和政治性文本自动分类中现有模型泛化能力差的问题。", "motivation": "现有方法在政治文本分类任务上存在孤立解决方案，导致在处理分布外文本时表现不佳。", "method": "作者首先对现有数据集和模型进行了全面概述。为解决局限性，他们整合了12个政治倾向分类数据集，并扩展了18个现有数据集以创建新的政治性数据集。通过“留一法”（leave-one-in）和“剔除法”（leave-one-out）进行广泛基准测试，评估了现有模型并训练了新的模型。", "result": "通过评估，他们训练出了具有增强泛化能力的新模型。", "conclusion": "通过构建多样化的数据集和采用新的训练方法，可以显著提升政治文本分类模型的泛化能力。"}}
{"id": "2507.13628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13628", "abs": "https://arxiv.org/abs/2507.13628", "authors": ["Masahiro Ogawa", "Qi An", "Atsushi Yamashita"], "title": "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation", "comment": "8 pages, 15 figures, RA-L submission", "summary": "Separating moving and static objects from a moving camera viewpoint is\nessential for 3D reconstruction, autonomous navigation, and scene understanding\nin robotics. Existing approaches often rely primarily on optical flow, which\nstruggles to detect moving objects in complex, structured scenes involving\ncamera motion. To address this limitation, we propose Focus of Expansion\nLikelihood and Segmentation (FoELS), a method based on the core idea of\nintegrating both optical flow and texture information. FoELS computes the focus\nof expansion (FoE) from optical flow and derives an initial motion likelihood\nfrom the outliers of the FoE computation. This likelihood is then fused with a\nsegmentation-based prior to estimate the final moving probability. The method\neffectively handles challenges including complex structured scenes, rotational\ncamera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016\ndataset and real-world traffic videos demonstrate its effectiveness and\nstate-of-the-art performance.", "AI": {"tldr": "本文提出FoELS方法，通过结合光流和纹理信息，有效解决了移动相机视角下在复杂结构场景中分离运动和静态物体的问题。", "motivation": "现有方法主要依赖光流，在涉及相机运动的复杂结构场景中难以检测运动物体，这限制了3D重建、自主导航和场景理解的应用。", "method": "FoELS方法整合了光流和纹理信息。它首先从光流计算膨胀焦点（FoE），并从FoE计算的异常值中得出初始运动可能性。然后，将此可能性与基于分割的先验融合，以估计最终的运动概率。", "result": "该方法能有效处理复杂结构场景、相机旋转运动和平行运动等挑战。在DAVIS 2016数据集和真实世界交通视频上的综合评估表明其有效性并达到了最先进的性能。", "conclusion": "FoELS通过结合光流和纹理信息，成功克服了传统光流方法在复杂场景中检测运动物体的局限性，为机器人领域提供了一种鲁棒的运动物体分离方案。"}}
{"id": "2507.13875", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13875", "abs": "https://arxiv.org/abs/2507.13875", "authors": ["Carlos Mena", "Pol Serra", "Jacobo Romero", "Abir Messaoudi", "Jose Giraldo", "Carme Armentano-Oller", "Rodolfo Zevallos", "Ivan Meza", "Javier Hernando"], "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies", "comment": "Accepted at Interspeech 2025", "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.", "AI": {"tldr": "该研究通过生成合成语码转换数据、拼接单语音频和利用带语言标记的真实语码转换数据三种策略，显著提升了OpenAI Whisper模型在加泰罗尼亚语-西班牙语语码转换语音识别（ASR）中的性能。", "motivation": "语码转换（CS）因训练数据稀缺和语言相似性对自动语音识别（ASR）构成挑战。现有模型主要依赖单语或混合语言语料库，无法反映真实的语码转换模式，这在多语社会中尤为关键，例如加泰罗尼亚语-西班牙语语码转换在媒体和议会演讲中广泛使用。", "method": "研究探索了三种策略来改进加泰罗尼亚语-西班牙语语码转换ASR：1) 生成合成语码转换数据；2) 拼接单语音频；3) 利用带有语言标记的真实语码转换数据。研究从加泰罗尼亚语语音语料库中提取语码转换数据，并对OpenAI的Whisper模型进行微调。", "result": "结果表明，将适量的合成语码转换数据与主导语言标记结合使用，能产生最佳的转录性能。", "conclusion": "结合适量合成语码转换数据与主导语言标记是提升加泰罗尼亚语-西班牙语语码转换ASR性能的有效方法。"}}
{"id": "2507.13919", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13919", "abs": "https://arxiv.org/abs/2507.13919", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "title": "The Levers of Political Persuasion with Conversational AI", "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.", "AI": {"tldr": "研究发现，当前及未来AI的说服力主要源于后训练和提示工程，而非个性化或模型规模，且说服力增强往往伴随着事实准确性的下降。", "motivation": "普遍担忧对话式AI可能对人类信仰施加前所未有的影响。", "method": "通过三项大规模实验（N=76,977），部署19个大型语言模型（LLMs，包括一些为说服目的进行后训练的模型），评估它们在707个政治问题上的说服力。同时检查了466,769个LLM声明的事实准确性。", "result": "当前及未来AI的说服力主要来自后训练（提升高达51%）和提示工程（提升高达27%），而非个性化或模型规模。这些方法通过利用LLMs快速访问和策略性部署信息的能力来增强说服力。令人惊讶的是，AI说服力增强的地方，其事实准确性也系统性地下降了。", "conclusion": "LLMs的说服力主要通过后训练和提示方法实现，这表明AI的潜在影响力更多地取决于其训练和使用方式，而非其核心能力或个性化。此外，提升说服力可能以牺牲事实准确性为代价。"}}
{"id": "2507.13648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13648", "abs": "https://arxiv.org/abs/2507.13648", "authors": ["Seungjun Moon", "Sangjoon Yu", "Gyeong-Moon Park"], "title": "EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation", "comment": null, "summary": "The rapid advancement of neural radiance fields (NeRF) has paved the way to\ngenerate animatable human avatars from a monocular video. However, the sole\nusage of NeRF suffers from a lack of details, which results in the emergence of\nhybrid representation that utilizes SMPL-based mesh together with NeRF\nrepresentation. While hybrid-based models show photo-realistic human avatar\ngeneration qualities, they suffer from extremely slow inference due to their\ndeformation scheme: to be aligned with the mesh, hybrid-based models use the\ndeformation based on SMPL skinning weights, which needs high computational\ncosts on each sampled point. We observe that since most of the sampled points\nare located in empty space, they do not affect the generation quality but\nresult in inference latency with deformation. In light of this observation, we\npropose EPSilon, a hybrid-based 3D avatar generation scheme with novel\nefficient point sampling strategies that boost both training and inference. In\nEPSilon, we propose two methods to omit empty points at rendering; empty ray\nomission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that\nprogress through the empty space. Then, EIO narrows down the sampling interval\non the ray, which wipes out the region not occupied by either clothes or mesh.\nThe delicate sampling scheme of EPSilon enables not only great computational\ncost reduction during deformation but also the designation of the important\nregions to be sampled, which enables a single-stage NeRF structure without\nhierarchical sampling. Compared to existing methods, EPSilon maintains the\ngeneration quality while using only 3.9% of sampled points and achieves around\n20 times faster inference, together with 4 times faster training convergence.\nWe provide video results on https://github.com/seungjun-moon/epsilon.", "AI": {"tldr": "本文提出EPSilon，一种高效的混合式3D人体化身生成方案，通过创新的点采样策略（空射线剔除和空区间剔除）显著加速了基于NeRF的训练和推理过程，同时保持生成质量。", "motivation": "现有的混合式NeRF人体化身模型虽然能生成逼真的效果，但推理速度极慢。这是因为它们依赖于基于SMPL蒙皮权重的形变，对每个采样点都需要进行计算，即使大部分采样点位于空旷空间，也会导致计算成本高昂和推理延迟。", "method": "本文提出了EPSilon，一种带有高效点采样策略的混合式3D化身生成方案。该方案包含两种方法来省略空点：1) 空射线剔除（ERO），即剔除穿过空旷空间的射线；2) 空区间剔除（EIO），即缩小射线上的采样区间，剔除衣服或网格未占据的区域。这种精细的采样方案不仅大大减少了形变过程中的计算成本，还指定了重要的采样区域，从而实现了无需分层采样的单阶段NeRF结构。", "result": "与现有方法相比，EPSilon在保持生成质量的同时，仅使用了3.9%的采样点，实现了约20倍的推理速度提升，以及4倍的训练收敛速度提升。", "conclusion": "EPSilon通过创新的高效点采样策略，成功解决了混合式NeRF人体化身模型推理速度慢的问题。它在显著减少计算量的同时，保持了高生成质量，并加速了训练过程，为可动画人体化身生成提供了更实用的解决方案。"}}
{"id": "2507.13937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13937", "abs": "https://arxiv.org/abs/2507.13937", "authors": ["Jan Trienes", "Anastasiia Derzhanskaia", "Roland Schwarzkopf", "Markus Mühling", "Jörg Schlötterer", "Christin Seifert"], "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support", "comment": null, "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.", "AI": {"tldr": "Marcel是一个轻量级、开源的对话代理，旨在通过检索增强生成（RAG）和创新的FAQ检索器，为潜在学生提供快速、个性化且可验证的招生相关查询响应，同时减轻大学工作人员的负担，并易于部署在资源受限的环境中。", "motivation": "支持潜在学生处理招生相关查询，提供快速、个性化的回复，并减少大学工作人员的工作量。", "method": "采用检索增强生成（RAG）技术，将答案基于大学资源生成；引入FAQ检索器，将用户问题映射到知识库条目，允许管理员引导检索，并优于标准的密集/混合检索策略；系统设计易于在资源受限的学术环境中部署；提供了系统架构细节、组件技术评估和真实世界部署的见解。", "result": "系统能够提供可验证的、上下文相关的招生信息；FAQ检索器提高了检索质量；系统易于部署在资源受限的学术环境中，并在实际部署中获得洞察。", "conclusion": "Marcel是一个有效的解决方案，能够支持潜在学生、减轻大学工作人员负担，并因其创新的检索方法和易于部署的特性，适用于大学环境。"}}
{"id": "2507.13942", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13942", "abs": "https://arxiv.org/abs/2507.13942", "authors": ["Jacob C Walker", "Pedro Vélez", "Luisa Polania Cabrera", "Guangyao Zhou", "Rishabh Kabra", "Carl Doersch", "Maks Ovsjanikov", "João Carreira", "Shiry Ginosar"], "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion", "comment": null, "summary": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.", "AI": {"tldr": "研究发现视觉模型的感知能力与短期预测性能之间存在强相关性，并提出一种新颖的通用预测框架，通过在冻结视觉骨干网络的表示空间中训练潜在扩散模型来预测未来特征。", "motivation": "预测未来事件对于在不同抽象级别上规划或行动的通用系统至关重要。本文旨在探究视觉模型的感知能力与其通用预测性能之间的关联。", "method": "提出一种新颖的通用预测框架，该框架可用于任何冻结的视觉骨干网络：在冻结的表示空间中训练潜在扩散模型来预测未来特征，然后通过轻量级的任务特定读出器进行解码。为实现跨任务的一致评估，引入了直接在下游任务空间中比较分布特性的分布度量。", "result": "发现视觉模型的感知能力与短期通用预测性能之间存在强相关性。这一趋势适用于多种预训练模型（包括生成式模型）以及从原始像素到深度、点轨迹和物体运动等多个抽象级别。该框架已应用于九个模型和四个任务。", "conclusion": "研究结果强调了将表示学习和生成模型相结合对于基于时间信息的视频理解的价值。"}}
{"id": "2507.13663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13663", "abs": "https://arxiv.org/abs/2507.13663", "authors": ["Xingyu Jiang", "Ning Gao", "Hongkun Dou", "Xiuhui Zhang", "Xiaoqing Zhong", "Yue Deng", "Hongjue Li"], "title": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration", "comment": null, "summary": "Natural image quality is often degraded by adverse weather conditions,\nsignificantly impairing the performance of downstream tasks. Image restoration\nhas emerged as a core solution to this challenge and has been widely discussed\nin the literature. Although recent transformer-based approaches have made\nremarkable progress in image restoration, their increasing system complexity\nposes significant challenges for real-time processing, particularly in\nreal-world deployment scenarios. To this end, most existing methods attempt to\nsimplify the self-attention mechanism, such as by channel self-attention or\nstate space model. However, these methods primarily focus on network\narchitecture while neglecting the inherent characteristics of image restoration\nitself. In this context, we explore a pyramid Wavelet-Fourier iterative\npipeline to demonstrate the potential of Wavelet-Fourier processing for image\nrestoration. Inspired by the above findings, we propose a novel and efficient\nrestoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).\nSpecifically, PW-FNet features two key design principles: 1) at the inter-block\nlevel, integrates a pyramid wavelet-based multi-input multi-output structure to\nachieve multi-scale and multi-frequency bands decomposition; and 2) at the\nintra-block level, incorporates Fourier transforms as an efficient alternative\nto self-attention mechanisms, effectively reducing computational complexity\nwhile preserving global modeling capability. Extensive experiments on tasks\nsuch as image deraining, raindrop removal, image super-resolution, motion\ndeblurring, image dehazing, image desnowing and underwater/low-light\nenhancement demonstrate that PW-FNet not only surpasses state-of-the-art\nmethods in restoration quality but also achieves superior efficiency, with\nsignificantly reduced parameter size, computational cost and inference time.", "AI": {"tldr": "本文提出了一种高效的金字塔小波-傅里叶网络（PW-FNet），通过结合多尺度小波分解和傅里叶变换，在多种图像恢复任务中实现了卓越的性能和效率。", "motivation": "现有的基于Transformer的图像恢复方法系统复杂性高，难以实时处理和实际部署。尽管有尝试简化自注意力机制，但它们往往忽视了图像恢复固有的特性。", "method": "本文探索了一种金字塔小波-傅里叶迭代流程。在此基础上，提出了PW-FNet，其核心设计原则包括：1) 在块间层面，集成金字塔小波多输入多输出结构，实现多尺度和多频带分解；2) 在块内层面，引入傅里叶变换作为自注意力机制的替代，以降低计算复杂度并保持全局建模能力。", "result": "PW-FNet在图像去雨、雨滴移除、图像超分辨率、运动去模糊、图像去雾、图像去雪以及水下/低光照增强等任务上进行了广泛实验。结果表明，PW-FNet不仅超越了现有最先进方法的恢复质量，还在参数量、计算成本和推理时间方面显著降低，展现出卓越的效率。", "conclusion": "小波-傅里叶处理在图像恢复领域具有巨大潜力。PW-FNet成功证明了通过结合小波分解和傅里叶变换，可以构建出既高效又高质量的图像恢复基线网络。"}}
{"id": "2507.13949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13949", "abs": "https://arxiv.org/abs/2507.13949", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "title": "Exploiting Primacy Effect To Improve Large Language Models", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在微调后会放大首位偏差（primacy bias）。通过根据语义相似性重新排序多项选择题的选项，可以利用这种偏差显著提高LLMs在多项选择问答（MCQA）任务上的性能。", "motivation": "LLMs在自然语言处理任务中广泛应用，但存在偏见，特别是位置偏见，如首位效应和近因效应，这些偏见会影响其回答的准确性。在多项选择问答（MCQA）中，选项的顺序会影响预测结果，其中首位效应尤为关键。本研究旨在探讨微调LLMs中的首位偏差问题，并发现微调可能加剧了这种偏差。", "method": "1. 证明微调会放大LLMs中的首位偏差，这可能源于模型接触了类似人类的模式。2. 策略性地利用这种偏差，通过根据答案选项与查询的语义相似性来重新排序选项，且无需预知正确答案。3. 在MCQA任务中进行实验验证该方法的有效性。", "result": "实验结果表明，通过根据语义相似性重新排序响应选项的方法，显著提高了LLMs在多项选择问答（MCQA）任务中的性能。", "conclusion": "本研究的发现强调了偏见的两面性，既是挑战也是机遇。研究结果为设计“偏见感知”模型和优化自然语言处理应用提供了见解。"}}
{"id": "2507.13966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13966", "abs": "https://arxiv.org/abs/2507.13966", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.", "AI": {"tldr": "本文提出一种自下而上的方法，通过知识图谱（KG）合成任务来训练大型语言模型（LLMs），以获得深度领域专业知识，并在医学领域验证了其有效性，实现了领域特异性“超智能”。", "motivation": "传统的语言模型虽然在跨领域泛化和任务特定推理方面表现出色，但其自上而下的训练方式不足以获取深度领域专业知识所需的抽象概念。这促使研究者探索一种自下而上的方法，通过学习组合简单的领域概念来获得专业知识。", "method": "研究者提出一个任务生成流程，直接从知识图谱（KG）原语合成任务，使模型能够获取并组合这些原语进行推理。他们使用医学KG构建了一个包含24,000个推理任务及其思维轨迹的课程，并在此基础上微调了QwQ-32B模型，得到了QwQ-Med-3。同时，他们引入了ICD-Bench评估套件来量化模型在15个医学领域的推理能力。", "result": "实验表明，QwQ-Med-3在ICD-Bench类别上显著优于最先进的推理模型，尤其在最困难的任务上表现出更大的性能优势。此外，QwQ-Med-3还将所获得的专业知识转移到医学问答基准测试中，提升了基础模型的性能。", "conclusion": "该研究表明，领域特异性“超智能”可以从高效的领域特异性“超智能”代理的可组合交互中产生，这与行业对通用人工智能（AGI）强调广泛专业知识的方法形成了对比，为未来AGI的发展提供了新的视角。"}}
{"id": "2507.13673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13673", "abs": "https://arxiv.org/abs/2507.13673", "authors": ["Yuechen Xie", "Haobo Jiang", "Jian Yang", "Yigong Zhang", "Jin Xie"], "title": "MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training", "comment": "10 pages, 8 figures, 6 tables", "summary": "In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of\nhands and objects from monocular RGB input remains highly challenging due to\nthe inherent geometric ambiguity of RGB images and the severe mutual occlusions\nthat occur during interaction.To address these challenges, we propose MaskHOI,\na novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI\npose estimation. Our core idea is to leverage the masking-then-reconstruction\nstrategy of MAE to encourage the feature encoder to infer missing spatial and\nstructural information, thereby facilitating geometric-aware and\nocclusion-robust representation learning. Specifically, based on our\nobservation that human hands exhibit far greater geometric complexity than\nrigid objects, conventional uniform masking fails to effectively guide the\nreconstruction of fine-grained hand structures. To overcome this limitation, we\nintroduce a Region-specific Mask Ratio Allocation, primarily comprising the\nregion-specific masking assignment and the skeleton-driven hand masking\nguidance. The former adaptively assigns lower masking ratios to hand regions\nthan to rigid objects, balancing their feature learning difficulty, while the\nlatter prioritizes masking critical hand parts (e.g., fingertips or entire\nfingers) to realistically simulate occlusion patterns in real-world\ninteractions. Furthermore, to enhance the geometric awareness of the pretrained\nencoder, we introduce a novel Masked Signed Distance Field (SDF)-driven\nmultimodal learning mechanism. Through the self-masking 3D SDF prediction, the\nlearned encoder is able to perceive the global geometric structure of hands and\nobjects beyond the 2D image plane, overcoming the inherent limitations of\nmonocular input and alleviating self-occlusion issues. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches.", "AI": {"tldr": "MaskHOI是一种基于Masked Autoencoder (MAE)的预训练框架，通过区域特定掩码分配和骨架驱动的手部掩码指导，结合掩码SDF驱动的多模态学习，显著提升了从单目RGB图像进行3D手物交互姿态估计的几何感知和遮挡鲁棒性。", "motivation": "从单目RGB输入估计3D手物交互（HOI）的精确关节姿态极具挑战性，原因在于RGB图像固有的几何模糊性以及交互过程中严重的相互遮挡。", "method": "本文提出了MaskHOI框架：1) 利用MAE的掩码-重建策略来推断缺失的空间和结构信息，以学习几何感知和抗遮挡的表示。2) 引入“区域特定掩码比分配”，对手部区域分配较低的掩码比，并采用骨架驱动的手部掩码指导，优先掩码关键手部区域（如指尖或整个手指），以模拟真实遮挡模式。3) 引入“掩码符号距离场（SDF）驱动的多模态学习机制”，通过自掩码3D SDF预测，使预训练编码器能够感知手和物体的全局几何结构，克服单目输入的局限性并缓解自遮挡问题。", "result": "广泛的实验表明，该方法显著优于现有最先进的方法。", "conclusion": "MaskHOI通过创新的MAE预训练策略，包括区域特定掩码和SDF驱动的学习，有效解决了3D手物交互姿态估计中的几何模糊和严重遮挡问题，从而实现了更精确和鲁棒的姿态估计。"}}
{"id": "2507.13977", "categories": ["cs.CL", "eess.AS", "I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13977", "abs": "https://arxiv.org/abs/2507.13977", "authors": ["Lilit Grigoryan", "Nikolay Karpov", "Enas Albasiri", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic", "comment": "Accepted to ICASSP 2025", "summary": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.", "AI": {"tldr": "本文提出了一种通用的阿拉伯语语音和文本处理方法，并基于FastConformer架构训练了两个新的阿拉伯语自动语音识别（ASR）模型：一个针对现代标准阿拉伯语（MSA），另一个是首个统一的MSA和古典阿拉伯语（CA）公共模型。这些模型在相关数据集上均达到了最先进（SOTA）的性能，并已开源以促进复现。", "motivation": "尽管阿拉伯语使用广泛，但由于其复杂性，阿拉伯语ASR系统开发面临巨大挑战，且公开模型数量有限。现有研究主要关注MSA，而对语言内部变体（如CA）的关注不足。", "method": "开发了一种通用的阿拉伯语语音和文本处理方法。基于FastConformer架构训练了两个模型：一个专为MSA设计，另一个是首个统一的MSA和CA模型。", "result": "MSA模型在相关数据集上达到了新的SOTA性能。统一模型在CA上实现了带音符的SOTA准确率，同时在MSA上保持了强大性能。", "conclusion": "本文成功开发了高性能的阿拉伯语ASR模型，包括针对MSA和统一的MSA+CA模型，通过创新的处理方法实现了SOTA结果。为促进研究，所有模型和训练配方均已开源。"}}
{"id": "2507.13984", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13984", "abs": "https://arxiv.org/abs/2507.13984", "authors": ["Quang-Binh Nguyen", "Minh Luu", "Quang Nguyen", "Anh Tran", "Khoi Nguyen"], "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models", "comment": "Accepted to ICCV 2025", "summary": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.", "AI": {"tldr": "本文提出CSD-VAR，一种基于视觉自回归模型（VAR）的内容-风格分解方法，通过创新的优化策略、SVD校正和增强的键值记忆，实现了更好的内容保留和风格化，并引入了CSD-100数据集进行基准测试。", "motivation": "现有的内容-风格分解（CSD）方法主要针对扩散模型，但视觉自回归模型（VAR）作为一种有前景的替代方案，其逐尺度生成过程可能有利于解耦。因此，需要探索将VAR作为CSD的生成框架，以利用其特性提升解耦效果。", "method": "本文提出了CSD-VAR方法，包含三项关键创新：1) 尺度感知交替优化策略，使内容和风格表示与其各自的尺度对齐，增强分离；2) 基于SVD的纠正方法，减少内容信息泄露到风格表示中；3) 增强的键值（K-V）记忆机制，提高内容身份的保留。此外，为基准测试此任务，本文还引入了CSD-100数据集。", "result": "实验结果表明，CSD-VAR在内容保留和风格化保真度方面均优于现有方法。", "conclusion": "CSD-VAR是一种新颖且有效的内容-风格分解方法，它成功地将视觉自回归模型应用于此任务，并通过其创新设计实现了卓越的性能，提升了视觉合成的灵活性。"}}
{"id": "2507.13693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13693", "abs": "https://arxiv.org/abs/2507.13693", "authors": ["Hongyi Liu", "Haifeng Wang"], "title": "Gaussian kernel-based motion measurement", "comment": null, "summary": "The growing demand for structural health monitoring has driven increasing\ninterest in high-precision motion measurement, as structural information\nderived from extracted motions can effectively reflect the current condition of\nthe structure. Among various motion measurement techniques, vision-based\nmethods stand out due to their low cost, easy installation, and large-scale\nmeasurement. However, when it comes to sub-pixel-level motion measurement,\ncurrent vision-based methods either lack sufficient accuracy or require\nextensive manual parameter tuning (e.g., pyramid layers, target pixels, and\nfilter parameters) to reach good precision. To address this issue, we developed\na novel Gaussian kernel-based motion measurement method, which can extract the\nmotion between different frames via tracking the location of Gaussian kernels.\nThe motion consistency, which fits practical structural conditions, and a\nsuper-resolution constraint, are introduced to increase accuracy and robustness\nof our method. Numerical and experimental validations show that it can\nconsistently reach high accuracy without customized parameter setup for\ndifferent test samples.", "AI": {"tldr": "本文提出了一种基于高斯核的新型视觉运动测量方法，无需定制参数即可实现亚像素级高精度运动测量。", "motivation": "结构健康监测对高精度运动测量的需求不断增长。然而，现有视觉方法在亚像素级运动测量中存在精度不足或需要大量手动参数调优的问题。", "method": "开发了一种基于高斯核的运动测量方法，通过跟踪高斯核的位置来提取帧间运动。该方法引入了运动一致性（符合实际结构条件）和超分辨率约束，以提高精度和鲁棒性。", "result": "数值和实验验证表明，该方法能够对不同测试样本持续达到高精度，且无需进行定制的参数设置。", "conclusion": "该方法有效解决了现有视觉测量在亚像素级精度和参数调优方面的挑战，实现了高精度且无需定制参数的运动测量，适用于结构健康监测。"}}
{"id": "2507.14017", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14017", "abs": "https://arxiv.org/abs/2507.14017", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models", "comment": null, "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.", "AI": {"tldr": "RHYTHM是一个利用大型语言模型（LLM）进行人类移动预测和轨迹推理的框架，它通过分层时间标记化和预计算的提示嵌入，显著提高了预测精度（尤其在周末）并大幅减少了训练时间。", "motivation": "研究旨在利用大型语言模型（LLM）进行时空预测和轨迹推理，解决传统方法在处理长序列时的效率问题，并提高人类移动预测的准确性。", "method": "RHYTHM将轨迹划分为日常片段，并将其编码为具有分层注意力的离散令牌，以捕获每日和每周的依赖关系，同时显著减少序列长度。通过一个冻结的LLM，令牌表示通过预计算的提示嵌入得到丰富，增强了模型捕获相互依赖性的能力，同时保持了计算效率。", "result": "在三个真实世界数据集上的评估显示，RHYTHM相较于现有最先进的方法，整体准确率提高了2.4%，周末准确率提高了5.0%，训练时间减少了24.6%。", "conclusion": "RHYTHM是一个有效且高效的框架，它成功地将LLM应用于人类移动预测，通过创新的标记化和嵌入方法，在提高预测性能（尤其在复杂的时间模式如周末）的同时，显著提升了计算效率。"}}
{"id": "2507.14067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14067", "abs": "https://arxiv.org/abs/2507.14067", "authors": ["Shuliang Liu", "Qi Zheng", "Jesse Jiaxi Xu", "Yibo Yan", "He Geng", "Aiwei Liu", "Peijie Jiang", "Jia Liu", "Yik-Cheung Tam", "Xuming Hu"], "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model", "comment": null, "summary": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking", "AI": {"tldr": "VLA-Mark是一种用于视觉-语言模型的数字水印框架，它通过跨模态协调在保护知识产权的同时，保持多模态连贯性和语义保真度，并展现出高检测率和抗攻击性。", "motivation": "现有文本水印方法通过有偏的token选择和静态策略破坏视觉-文本对齐，导致语义关键概念易受攻击，因此需要一种不损害多模态连贯性的水印解决方案。", "method": "VLA-Mark框架通过跨模态协调嵌入可检测水印，同时保留语义保真度。它整合了多尺度视觉-文本对齐指标（包括局部补丁亲和度、全局语义连贯性和上下文注意力模式）来指导水印注入，且无需模型再训练。此外，一个熵敏感机制动态平衡水印强度和语义保留，在低不确定性生成阶段优先考虑视觉基础。", "result": "实验表明，VLA-Mark比传统方法PPL低7.4%，BLEU高26.6%，检测率近乎完美（AUC 98.8%）。该框架对复述和同义词替换等攻击展现出96.1%的抗攻击性，同时保持了文本-视觉一致性。", "conclusion": "VLA-Mark为质量保留的多模态水印设定了新标准，有效解决了现有方法的局限性，实现了知识产权保护与多模态内容质量的平衡。"}}
{"id": "2507.13706", "categories": ["cs.CV", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.13706", "abs": "https://arxiv.org/abs/2507.13706", "authors": ["Ángel F. García-Fernández", "Jinhao Gu", "Lennart Svensson", "Yuxuan Xia", "Jan Krejčí", "Oliver Kost", "Ondřej Straka"], "title": "GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms", "comment": null, "summary": "This paper introduces two quasi-metrics for performance assessment of\nmulti-object tracking (MOT) algorithms. In particular, one quasi-metric is an\nextension of the generalised optimal subpattern assignment (GOSPA) metric and\nmeasures the discrepancy between sets of objects. The other quasi-metric is an\nextension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy\nbetween sets of trajectories. Similar to the GOSPA-based metrics, these\nquasi-metrics include costs for localisation error for properly detected\nobjects, the number of false objects and the number of missed objects. The\nT-GOSPA quasi-metric also includes a track switching cost. Differently from the\nGOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of\npenalising missed and false objects with different costs, and the localisation\ncosts are not required to be symmetric. These properties can be useful in MOT\nevaluation in certain applications. The performance of several Bayesian MOT\nalgorithms is assessed with the T-GOSPA quasi-metric via simulations.", "AI": {"tldr": "本文提出了两种新的准度量，用于评估多目标跟踪（MOT）算法的性能，它们是GOSPA和T-GOSPA度量的扩展，并允许对漏检和虚警目标设置不同的惩罚成本以及非对称的定位成本。", "motivation": "现有的GOSPA和T-GOSPA度量在惩罚漏检和虚警目标时，可能缺乏足够的灵活性，且定位成本通常被要求是对称的。在某些特定的MOT应用中，需要能够对漏检和虚警目标施加不同的成本，并且允许非对称的定位成本，以更准确地评估算法性能。", "method": "本文引入了两种准度量：一种是广义最优子模式分配（GOSPA）度量的扩展，用于测量对象集之间的差异；另一种是轨迹GOSPA（T-GOSPA）度量的扩展，用于测量轨迹集之间的差异。这些准度量继承了GOSPA基度量的特性，包括定位误差、虚警和漏检成本，T-GOSPA准度量还包含轨迹切换成本。与GOSPA和T-GOSPA不同的是，所提出的准度量允许对漏检和虚警目标施加不同的惩罚成本，并且不要求定位成本是对称的。", "result": "通过仿真，使用T-GOSPA准度量评估了几种贝叶斯多目标跟踪算法的性能。", "conclusion": "所提出的准度量，特别是T-GOSPA准度量，通过提供对漏检和虚警目标不同成本的灵活性以及非对称定位成本的能力，为多目标跟踪算法的评估提供了更强大和灵活的工具，这在某些特定应用中尤其有用。"}}
{"id": "2507.14022", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14022", "abs": "https://arxiv.org/abs/2507.14022", "authors": ["Jianfei Li", "Kevin Kam Fung Yuen"], "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis", "comment": "35 pages, 33 tables, 6 Figures", "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.", "AI": {"tldr": "本研究提出了一种基于认知成对比较分类模型选择（CPC-CMS）框架，用于文档级情感分析，通过专家知识判断计算评估指标权重，并选择最佳分类模型。", "motivation": "研究动机在于为文档级情感分析问题选择最佳的分类模型，尤其是在面对多种评估标准和模型时，需要一个系统化的选择框架。", "method": "该研究采用认知成对比较（CPC）方法，基于专家知识判断计算准确率、精确率、召回率、F1分数、特异性、MCC、Kappa和效率等评估标准的权重。然后，构建一个包含分类评估分数和标准权重的加权决策矩阵，以从朴素贝叶斯、LSVC、随机森林、逻辑回归、XGBoost、LSTM和ALBERT等基线模型中选择最佳模型。通过三个社交媒体开放数据集验证了该框架的可行性。", "result": "仿真结果显示，在不考虑时间因素的情况下，ALBERT在三个数据集中表现最佳；如果考虑时间消耗，则没有单一模型始终优于其他模型。", "conclusion": "所提出的CPC-CMS框架对于文档级情感分析是可行的，并且可以应用于其他领域的分类应用中。"}}
{"id": "2507.14079", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14079", "abs": "https://arxiv.org/abs/2507.14079", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.", "AI": {"tldr": "DENSE是一个模拟医生工作流程的系统，通过整合电子健康记录（EHR）中分散的异构笔记，利用大型语言模型（LLM）生成临床上连贯且时间上准确的患者进展笔记，以弥补现有数据集中进展笔记的稀缺性。", "motivation": "进展笔记在电子健康记录中具有重要的临床意义，但它们在大型EHR数据集中（如MIMIC-III）严重不足（仅约8.56%的住院记录包含进展笔记），导致患者纵向叙述的缺失和不完整。", "method": "该研究提出了DENSE系统，它模拟医生撰写进展笔记时参考过往记录的方式。DENSE引入了细粒度的笔记分类和时间对齐机制，将不同就诊的异构笔记组织成结构化、按时间顺序排列的输入。核心方法是利用临床知情的检索策略，从当前和之前的就诊中识别出时间上和语义上相关的证据，然后用这些证据提示大型语言模型（LLM）生成临床连贯且时间感知的进展笔记。", "result": "DENSE在经过筛选的、具有完整进展笔记文档的多次就诊患者队列上进行了评估。生成的笔记展现出强大的纵向保真度，时间对齐比率达到1.089，超过了原始笔记中观察到的连续性。", "conclusion": "DENSE系统通过恢复碎片化文档中的叙事连贯性，支持改进下游任务，如总结、预测建模和临床决策支持。它为在真实医疗环境中利用LLM驱动的笔记合成提供了一个可扩展的解决方案。"}}
{"id": "2507.13708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13708", "abs": "https://arxiv.org/abs/2507.13708", "authors": ["Sofia Jamil", "Bollampalli Areen Reddy", "Raghvendra Kumar", "Sriparna Saha", "Koustava Goswami", "K. J. Joseph"], "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement", "comment": "ECAI 2025", "summary": "Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.", "AI": {"tldr": "本文提出了一种名为PoemTale Diffusion的免训练方法，通过多阶段提示词优化和修改自注意力机制，旨在提高文本到图像扩散模型在处理诗歌等复杂创意文本时的表现，并引入了P4I诗歌数据集。", "motivation": "现有的文本到图像扩散模型在处理复杂、抽象或高度描述性的创意语言（特别是诗歌中包含的分层、抽象和双重含义）时，往往难以准确解释并生成高质量的图像，导致信息丢失。", "method": "该研究引入了PoemTale Diffusion方法，其核心是一个免训练的多阶段提示词优化循环，集成到语言模型中以增强诗歌文本的可解释性。此外，通过修改现有扩散模型的自注意力机制，采用“一致性自注意力技术”来生成多张一致的图像，这些图像共同传达诗歌的含义。为支持研究，还构建了包含1111首诗歌的P4I（PoemForImage）数据集。方法通过诗歌专家进行定性评估和定量评估进行验证。", "result": "人类评估和定量评估的结果均验证了该方法的有效性，表明其在诗歌到图像生成中能更有效地捕获信息，并体现在生成的图像中。", "conclusion": "该研究为诗歌到图像生成提供了一个新颖的视角，通过增强生成图像中的信息捕获能力，有效地解决了现有模型在处理复杂创意文本时的局限性。"}}
{"id": "2507.14045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14045", "abs": "https://arxiv.org/abs/2507.14045", "authors": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "Jimmy Huang"], "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks", "comment": "Accepted at Canadian AI 2025", "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.", "AI": {"tldr": "本文全面评估了多种成本效益高的LLMs在生物医学文本和图像任务上的表现，发现没有单一模型能全面胜出，开源模型在某些任务上可与闭源模型媲美甚至更优，并提供额外优势。", "motivation": "旨在评估和比较不同大型语言模型（LLMs）在多样化生物医学任务中的表现，特别是关注其成本效益，以指导特定应用的模型选择。", "method": "评估了一系列闭源和开源LLMs，涵盖生物医学文本分类、文本生成、问答以及多模态图像处理等任务。", "result": "实验结果显示，没有一个LLM能在所有任务上持续优于其他模型，不同LLMs擅长不同任务。虽然某些闭源LLMs在特定任务上表现强劲，但其开源对应模型也能达到可比（有时甚至更好）的结果，并具有更快的推理速度和增强的隐私性等额外优势。", "conclusion": "研究结果为选择最适合特定生物医学应用的模型提供了宝贵的见解。"}}
{"id": "2507.14093", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14093", "abs": "https://arxiv.org/abs/2507.14093", "authors": ["Šimon Kubov", "Simon Klíčník", "Jakub Dandár", "Zdeněk Straka", "Karolína Kvaková", "Daniel Kvak"], "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment", "comment": null, "summary": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.", "AI": {"tldr": "一项多中心回顾性研究评估了全自动深度学习软件在脊柱侧弯Cobb角测量中的表现，结果显示其能达到专家水平的测量和分级准确性。", "motivation": "脊柱侧弯影响青少年，治疗决策依赖于精确的Cobb角测量。然而，人工测量耗时且存在观察者间差异。", "method": "研究对来自10家医院的103张站立位全脊柱前后位X光片进行了回顾性多中心评估。使用名为Carebot AI Bones的完全自动化深度学习软件进行测量。两名肌肉骨骼放射科医生独立测量作为参考。通过Bland-Altman分析、平均绝对误差（MAE）、均方根误差（RMSE）、Pearson相关系数和Cohen Kappa系数（用于四级严重程度分类）评估AI与每位放射科医生之间的一致性。", "result": "与放射科医生1相比，AI的MAE为3.89度（RMSE 4.77度），偏差为0.70度，一致性限度为-8.59至+9.99度。与放射科医生2相比，AI的MAE为3.90度（RMSE 5.68度），偏差为2.14度，一致性限度为-8.23至+12.50度。AI与放射科医生的Pearson相关系数分别为0.906和0.880（读者间相关系数为0.928），严重程度分级的Cohen Kappa系数分别为0.51和0.64（读者间Kappa系数为0.59）。", "conclusion": "该深度学习软件能够跨多个中心复现专家水平的Cobb角测量和分类分级，表明其在简化脊柱侧弯报告和临床工作流程中的分流具有实用价值。"}}
{"id": "2507.13719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13719", "abs": "https://arxiv.org/abs/2507.13719", "authors": ["Daniele Pannone", "Alessia Castronovo", "Maurizio Mancini", "Gian Luca Foresti", "Claudio Piciarelli", "Rossana Gabrieli", "Muhammad Yasir Bilal", "Danilo Avola"], "title": "Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction", "comment": null, "summary": "This paper presents an innovative augmented reality pipeline tailored for\nmuseum environments, aimed at recognizing artworks and generating accurate 3D\nmodels from single images. By integrating two complementary pre-trained depth\nestimation models, i.e., GLPN for capturing global scene structure and\nDepth-Anything for detailed local reconstruction, the proposed approach\nproduces optimized depth maps that effectively represent complex artistic\nfeatures. These maps are then converted into high-quality point clouds and\nmeshes, enabling the creation of immersive AR experiences. The methodology\nleverages state-of-the-art neural network architectures and advanced computer\nvision techniques to overcome challenges posed by irregular contours and\nvariable textures in artworks. Experimental results demonstrate significant\nimprovements in reconstruction accuracy and visual realism, making the system a\nhighly robust tool for museums seeking to enhance visitor engagement through\ninteractive digital content.", "AI": {"tldr": "该论文提出了一种创新的博物馆增强现实（AR）流程，能够从单张图像识别艺术品并生成高精度的3D模型，通过结合两种深度估计模型优化深度图，从而实现沉浸式AR体验。", "motivation": "研究旨在解决博物馆环境中艺术品识别和单图3D模型生成的需求，克服艺术品不规则轮廓和多变纹理带来的挑战，并帮助博物馆通过互动数字内容提升参观者体验。", "method": "该方法整合了两个预训练的深度估计模型：GLPN（用于捕捉全局场景结构）和Depth-Anything（用于详细局部重建），以生成优化的深度图。这些深度图随后被转换为高质量的点云和网格。整个过程利用了最先进的神经网络架构和高级计算机视觉技术。", "result": "实验结果表明，该系统在重建精度和视觉真实感方面取得了显著提升。", "conclusion": "该系统是一个高度稳健的工具，能够有效帮助博物馆通过互动数字内容增强参观者参与度，为博物馆AR应用提供了强大支持。"}}
{"id": "2507.14063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14063", "abs": "https://arxiv.org/abs/2507.14063", "authors": ["Lautaro Estienne", "Gabriel Ben Zenou", "Nona Naderi", "Jackie Cheung", "Pablo Piantanida"], "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog", "comment": null, "summary": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.", "AI": {"tldr": "本文提出了一种名为协同理性言语行为（CRSA）的信息论扩展，用于建模多轮对话，通过优化基于率失真理论的增益函数，从而实现更具语用和社交意识的语言智能体。", "motivation": "现有AI系统在承担协作角色时，需要推理共享目标和信念，而不仅仅是生成流畅语言。然而，现有的理性言语行为（RSA）框架扩展在处理多轮、协作场景时面临扩展性挑战。", "method": "引入了协同理性言语行为（CRSA），它是RSA的一个信息论（IT）扩展。CRSA通过优化一个从率失真理论改编而来的增益函数来建模多轮对话，该函数考虑了对话双方都拥有私有信息并基于对话生成话语的情况。", "result": "CRSA在指称游戏和医疗领域的模板式医患对话中表现出有效性。实证结果表明，CRSA比现有基线模型产生更一致、可解释和协作的行为。", "conclusion": "CRSA为实现更具语用和社交意识的语言智能体铺平了道路，使其在协作场景中能更好地推理共享目标和信念。"}}
{"id": "2507.14096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14096", "abs": "https://arxiv.org/abs/2507.14096", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "AI": {"tldr": "该研究通过举办PLABA竞赛评估了大型语言模型（LLMs）将专业生物医学文献改编为通俗语言的能力，发现其在准确性方面表现良好，但在简洁性和自动评估工具方面仍有不足。", "motivation": "语言模型在将专业生物医学文献改编为通俗语言方面展现潜力，但其不可预测性及该领域潜在的危害性，使得严格评估至关重要。因此，研究旨在刺激相关研究并提供高质量的系统评估。", "method": "研究在2023和2024年的文本检索会议（TREC）上举办了生物医学摘要通俗语言改编（PLABA）赛道。任务包括：任务1为摘要的完整句子级改写；任务2为识别并替换难懂术语。任务1的自动评估开发了四套专业编写的参考文本。所有提交的系统（任务1和任务2）都获得了生物医学专家的大量人工评估。", "result": "12个团队参与了竞赛。在任务1的人工评估中，表现最佳的模型在事实准确性和完整性方面可媲美人类水平，但在简洁性和简短性方面仍有差距。自动、基于参考的指标与人工判断的相关性普遍不高。在任务2中，系统在识别难懂术语和分类替换方式方面表现不佳。然而，在生成替换内容时，基于LLM的系统在人工评估的准确性、完整性和简洁性方面表现良好，但在简短性方面仍不足。", "conclusion": "PLABA赛道显示了使用大型语言模型改编生物医学文献以供大众阅读的潜力，同时也揭示了它们的不足，并强调了改进自动基准测试工具的必要性。"}}
{"id": "2507.13753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13753", "abs": "https://arxiv.org/abs/2507.13753", "authors": ["Tongtong Su", "Chengyu Wang", "Bingyan Liu", "Jun Huang", "Dongming Lu"], "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis", "comment": null, "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.", "AI": {"tldr": "本文提出EVS，一种免训练的视频合成器，通过结合文本到图像（T2I）和文本到视频（T2V）模型，显著提升生成视频的图像质量和运动流畅性，并加快推理速度。", "motivation": "现有的文本到视频（T2V）模型在实现高图像质量和有效运动表示方面面临挑战。直接改编预训练的文本到图像（T2I）模型来优化视频帧会导致帧间不一致，产生闪烁和伪影。", "method": "EVS是一个免训练的封装视频合成器。它利用训练好的扩散模型T2I模型来优化低质量视频帧（将其视为分布外样本，通过加噪和去噪步骤优化），同时使用T2V骨干网络确保运动动态的一致性。该方法将T2V的时间先验封装到T2I的生成过程中，从而结合了两类模型的优势。", "result": "实验结果验证了EVS相较于先前方法的有效性，生成视频的图像和运动质量均有所提升。此外，其合成过程还带来了1.6倍至4.5倍的推理速度提升。", "conclusion": "EVS通过巧妙地结合T2I和T2V模型的优势，成功解决了文本到视频合成中图像质量和运动流畅性的挑战，并显著提高了推理效率。"}}
{"id": "2507.13773", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13773", "abs": "https://arxiv.org/abs/2507.13773", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "comment": "ACL2025 Main", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios.", "AI": {"tldr": "针对视觉问答(VQA)中用户提问模糊的问题，现有方法主要通过改写问题来解决。本文提出ClearVQA基准，旨在评估VLM通过交互澄清歧义的能力，并解决缺乏相关基准和VLM不倾向于提问的问题。", "motivation": "用户在VQA中常提出模糊问题，现有方法忽略了交互澄清的潜力。交互澄清研究面临两大挑战：缺乏评估VLM交互澄清能力的基准，以及VLM倾向于回答而非提问，阻碍其寻求澄清。", "method": "引入了ClearVQA基准，该基准针对VQA中三类常见的歧义，并涵盖了多种VQA场景。", "result": "本文的主要成果是创建了ClearVQA基准，用于解决评估VLM交互澄清能力和VLM行为模式的挑战。", "conclusion": "ClearVQA基准的引入旨在促进VLM在交互式澄清模糊问题方面的研究和发展，填补了现有研究在评估工具和VLM行为模式上的空白。"}}
{"id": "2507.14119", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14119", "abs": "https://arxiv.org/abs/2507.14119", "authors": ["Maksim Kuprashevich", "Grigorii Alekseenko", "Irina Tolstykh", "Georgii Fedorov", "Bulat Suleimanov", "Vladimir Dokholyan", "Aleksandr Gordeev"], "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining", "comment": null, "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.", "AI": {"tldr": "该论文提出一个自动化、模块化的流水线，用于生成高质量的图像编辑三元组（原始图像、指令、编辑图像），以训练生成模型。通过该流水线，他们构建并发布了大规模数据集NHR-Edit，并训练了一个达到最先进水平的Bagel模型。", "motivation": "当前生成模型驱动的图像编辑助手需要数百万个高质量的训练三元组。然而，手动或半自动化地挖掘像素精确、符合指令、风格一致、物理合理且视觉吸引力的示例非常困难，且缺乏可靠的自动化编辑质量评估指标，阻碍了大规模数据生产。", "method": "研究人员开发了一个自动化、模块化的流水线，利用公共生成模型，无需人工干预。该系统使用一个任务调优的Gemini验证器直接评估指令遵循度和美学，无需分割或接地模型。通过反转（Inversion）和组合自举（Compositional Bootstrapping）技术，将挖掘的数据集扩大了约2.2倍。", "result": "通过该方法，他们自动化了重复性标注步骤，实现了无需人工标注的大规模训练数据生成。最终发布了NHR-Edit，一个包含35.8万个高质量三元组的开放数据集，在最大的跨数据集评估中超越了所有公开替代方案。同时，他们还发布了Bagel-NHR-Edit，一个开源的微调Bagel模型，在实验中取得了最先进的性能指标。", "conclusion": "该研究通过自动化图像编辑三元组的生成，极大地降低了训练数据标注的成本和复杂性，为资源密集型领域的图像编辑研究提供了便利。NHR-Edit数据集和Bagel-NHR-Edit模型的发布，为该领域设定了新的基准，并有望推动图像编辑技术的发展。"}}
{"id": "2507.13772", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13772", "abs": "https://arxiv.org/abs/2507.13772", "authors": ["Abhijit Sen", "Giridas Maiti", "Bikram K. Parida", "Bhanu P. Mishra", "Mahima Arya", "Denys I. Bondar"], "title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification", "comment": null, "summary": "Feature engineering continues to play a critical role in image\nclassification, particularly when interpretability and computational efficiency\nare prioritized over deep learning models with millions of parameters. In this\nstudy, we revisit classical machine learning based image classification through\na novel approach centered on Permutation Entropy (PE), a robust and\ncomputationally lightweight measure traditionally used in time series analysis\nbut rarely applied to image data. We extend PE to two-dimensional images and\npropose a multiscale, multi-orientation entropy-based feature extraction\napproach that characterizes spatial order and complexity along rows, columns,\ndiagonals, anti-diagonals, and local patches of the image. To enhance the\ndiscriminatory power of the entropy features, we integrate two classic image\ndescriptors: the Histogram of Oriented Gradients (HOG) to capture shape and\nedge structure, and Local Binary Patterns (LBP) to encode micro-texture of an\nimage. The resulting hand-crafted feature set, comprising of 780 dimensions, is\nused to train Support Vector Machine (SVM) classifiers optimized through grid\nsearch. The proposed approach is evaluated on multiple benchmark datasets,\nincluding Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers\ncompetitive classification performance without relying on deep architectures.\nOur results demonstrate that the fusion of PE with HOG and LBP provides a\ncompact, interpretable, and effective alternative to computationally expensive\nand limited interpretable deep learning models. This shows a potential of\nentropy-based descriptors in image classification and contributes a lightweight\nand generalizable solution to interpretable machine learning in image\nclassification and computer vision.", "AI": {"tldr": "该研究提出一种结合置换熵（PE）、方向梯度直方图（HOG）和局部二值模式（LBP）的新型手工特征工程方法，用于图像分类，在保持竞争性性能的同时，提供比深度学习模型更好的可解释性和计算效率。", "motivation": "在图像分类中，当优先考虑可解释性和计算效率而非参数量巨大的深度学习模型时，特征工程仍扮演关键角色。本研究旨在探索一种轻量级且可解释的替代方案。", "method": "研究将传统用于时间序列分析的置换熵（PE）扩展到二维图像，并提出一种多尺度、多方向的基于熵的特征提取方法，以表征图像的空间顺序和复杂性。为增强判别力，该方法融合了HOG（捕获形状和边缘）和LBP（编码微纹理）两种经典图像描述符。最终生成780维特征集，用于训练通过网格搜索优化的支持向量机（SVM）分类器。", "result": "该方法在Fashion-MNIST、KMNIST、EMNIST和CIFAR-10等多个基准数据集上取得了有竞争力的分类性能，且不依赖深度学习架构。结果表明，PE与HOG和LBP的融合提供了一种紧凑、可解释且有效的替代方案，优于计算昂贵且可解释性有限的深度学习模型。", "conclusion": "研究证明了基于熵的描述符在图像分类中的潜力，并为图像分类和计算机视觉领域的可解释机器学习贡献了一种轻量级和通用化的解决方案。"}}
{"id": "2507.13779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13779", "abs": "https://arxiv.org/abs/2507.13779", "authors": ["Durgesh Singh", "Ahcène Boubekki", "Robert Jenssen", "Michael Kampffmeyer"], "title": "SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering", "comment": null, "summary": "Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)\nenhance the model performance by exploiting information from labeled and\nunlabeled data. The clustering assumption has proven advantageous for learning\nwith limited supervision and states that data points belonging to the same\ncluster in a high-dimensional space should be assigned to the same category.\nRecent works have utilized different training mechanisms to implicitly enforce\nthis assumption for the SSL and UDA. In this work, we take a different approach\nby explicitly involving a differentiable clustering module which is extended to\nleverage the supervised data to compute its centroids. We demonstrate the\neffectiveness of our straightforward end-to-end training strategy for SSL and\nUDA over extensive experiments and highlight its benefits, especially in low\nsupervision regimes, both as a standalone model and as a regularizer for\nexisting approaches.", "AI": {"tldr": "本文提出一种显式可微分聚类模块，通过利用有监督数据计算聚类中心，提升半监督学习（SSL）和无监督域适应（UDA）在低监督场景下的性能，并可作为独立模型或现有方法的正则化器。", "motivation": "现有的半监督学习（SSL）和无监督域适应（UDA）方法通常隐式地强制执行“聚类假设”（即高维空间中同一簇的数据点应属于同一类别）。本文旨在采取一种不同的方法，显式地引入一个可微分聚类模块。", "method": "引入一个显式可微分聚类模块，并扩展该模块以利用有监督数据计算其聚类中心。采用直接的端到端训练策略，可作为独立模型或现有方法的正则化器。", "result": "通过大量实验证明了该方法在半监督学习（SSL）和无监督域适应（UDA）中的有效性，尤其在低监督场景下表现出显著优势。", "conclusion": "所提出的显式可微分聚类模块及其端到端训练策略，通过利用有监督数据计算聚类中心，有效提升了半监督学习和无监督域适应的性能，尤其适用于监督数据有限的情况，并能作为独立模型或现有方法的有效正则化器。"}}
{"id": "2507.13797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13797", "abs": "https://arxiv.org/abs/2507.13797", "authors": ["Huu-Phu Do", "Yu-Wei Chen", "Yi-Cheng Liao", "Chi-Wei Hsiao", "Han-Yang Wang", "Wei-Chen Chiu", "Ching-Chun Huang"], "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance", "comment": "Accepted by ICCV 2025", "summary": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial\nimages from unknown degraded inputs, presenting significant challenges in\npreserving both identity and detail. Pre-trained diffusion models have been\nincreasingly used as image priors to generate fine details. Still, existing\nmethods often use fixed diffusion sampling timesteps and a global guidance\nscale, assuming uniform degradation. This limitation and potentially imperfect\ndegradation kernel estimation frequently lead to under- or over-diffusion,\nresulting in an imbalance between fidelity and quality. We propose\nDynFaceRestore, a novel blind face restoration approach that learns to map any\nblindly degraded input to Gaussian blurry images. By leveraging these blurry\nimages and their respective Gaussian kernels, we dynamically select the\nstarting timesteps for each blurry image and apply closed-form guidance during\nthe diffusion sampling process to maintain fidelity. Additionally, we introduce\na dynamic guidance scaling adjuster that modulates the guidance strength across\nlocal regions, enhancing detail generation in complex areas while preserving\nstructural fidelity in contours. This strategy effectively balances the\ntrade-off between fidelity and quality. DynFaceRestore achieves\nstate-of-the-art performance in both quantitative and qualitative evaluations,\ndemonstrating robustness and effectiveness in blind face restoration.", "AI": {"tldr": "DynFaceRestore是一种新颖的盲人脸修复方法，通过动态选择扩散采样起始时间步和应用局部自适应指导，解决了现有方法中保真度与质量不平衡的问题。", "motivation": "现有基于预训练扩散模型的人脸修复方法通常使用固定的采样时间步和全局指导尺度，假设降级是均匀的。这种限制以及不完美的降级核估计常导致欠扩散或过扩散，从而造成图像保真度与质量之间的不平衡。", "method": "DynFaceRestore首先学习将任意盲降级输入映射到高斯模糊图像。然后，它利用这些模糊图像及其高斯核动态选择每个图像的扩散采样起始时间步，并在扩散采样过程中应用闭式指导以保持保真度。此外，该方法引入了一个动态指导尺度调节器，用于在局部区域调节指导强度，以在复杂区域增强细节生成，同时在轮廓处保留结构保真度。", "result": "DynFaceRestore在定量和定性评估中均达到了最先进的性能，展示了其在盲人脸修复中的鲁棒性和有效性。", "conclusion": "DynFaceRestore通过动态扩散采样和指导策略，有效平衡了盲人脸修复中的保真度与图像质量之间的权衡，实现了优异的修复效果。"}}
{"id": "2507.13803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13803", "abs": "https://arxiv.org/abs/2507.13803", "authors": ["Weiqi Yang", "Xu Zhou", "Jingfu Guan", "Hao Du", "Tianyu Bai"], "title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation", "comment": null, "summary": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments.", "AI": {"tldr": "GRAM-MAMBA是一种针对物联网多模态感知的框架，它利用Mamba模型处理时序数据，通过GRAM矩阵实现模态间对齐，并引入低秩自适应层补偿策略以应对模态缺失，在资源受限环境下实现了高效且鲁棒的多模态感知。", "motivation": "现有物联网多模态系统面临模型复杂度高（难以在资源受限设备部署）、单向模态对齐忽略模态间关系以及传感器数据缺失时鲁棒性差等挑战，阻碍了在实际物联网场景中的高效和鲁棒感知。", "method": "本文提出了GRAM-MAMBA框架。它采用线性复杂度的Mamba模型处理传感器时间序列数据，结合优化的GRAM矩阵策略实现模态间的两两对齐。此外，受LoRA启发，引入了一种自适应低秩层补偿策略，用于处理训练后模态缺失的情况，该策略通过冻结预训练模型核心和无关自适应层，仅微调与可用模态和融合过程相关的部分。", "result": "在SPAWC2021室内定位数据集上，预训练模型误差低于基线，适应缺失模态时性能提升24.5%，且仅训练不到0.2%的参数。在USC-HAD人类活动识别数据集上，F1得分达到93.55%，总体准确率(OA)达到93.81%，优于现有工作；更新策略使F1得分提高23%，且仅训练不到0.3%的参数。", "conclusion": "GRAM-MAMBA在资源受限环境下实现了高效且鲁棒的多模态感知，其在处理复杂时序数据、模态对齐和模态缺失方面的有效性得到了实验验证，显示出巨大的应用潜力。"}}
{"id": "2507.13812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13812", "abs": "https://arxiv.org/abs/2507.13812", "authors": ["Yingying Zhang", "Lixiang Ru", "Kang Wu", "Lei Yu", "Lei Liang", "Yansheng Li", "Jingdong Chen"], "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing", "comment": "Accepted by ICCV25", "summary": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.", "AI": {"tldr": "SkySense V2是一种统一的多模态遥感基础模型，采用单个Transformer骨干网络和专门设计的自监督学习策略，有效处理多模态遥感数据，解决了现有方法效率低下和特征适应性不足的问题。", "motivation": "现有多模态遥感基础模型通常为每种模态训练独立的骨干网络，导致冗余和参数利用效率低下。此外，主流预训练方法直接应用自然图像的自监督学习技术，未能充分适应遥感图像复杂的语义分布、不同分辨率和有限特征多样性等特性。", "method": "SkySense V2采用单个Transformer骨干网络处理多种模态，并使用针对遥感数据特性定制的新型自监督学习策略进行预训练。具体方法包括：引入创新的自适应补丁合并模块（adaptive patch merging module）和可学习的模态提示令牌（learnable modality prompt tokens）以解决不同分辨率和模态间特征多样性有限的问题；额外整合专家混合（MoE）模块以进一步提升模型性能。", "result": "SkySense V2展现出令人印象深刻的泛化能力，在涉及7项任务的16个数据集中进行了广泛评估，平均性能比SkySense高出1.8个点。", "conclusion": "SkySense V2通过其统一的骨干网络、为遥感数据量身定制的自监督学习策略以及特定的模块设计（如自适应补丁合并、模态提示令牌和MoE），成功构建了一个更高效、更强大的多模态遥感基础模型，并在多项任务中取得了显著的性能提升和泛化能力。"}}
{"id": "2507.13861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13861", "abs": "https://arxiv.org/abs/2507.13861", "authors": ["Junjie Hu", "Tianyang Han", "Kai Ma", "Jialin Gao", "Hao Dou", "Song Yang", "Xianhua He", "Jianhui Zhang", "Junfeng Luo", "Xiaoming Wei", "Wenqiang Zhang"], "title": "PositionIC: Unified Position and Identity Consistency for Image Customization", "comment": null, "summary": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.", "AI": {"tldr": "PositionIC是一个统一框架，通过可扩展的数据合成管线和轻量级位置调制层，实现了多主体图像定制中的精确空间控制和身份一致性。", "motivation": "现有主体驱动图像定制在保真度上取得进展，但缺乏细粒度的实体级空间控制，这主要是因为缺少结合身份和精确位置线索的可扩展数据集，从而阻碍了实际应用。", "method": "本文提出了PositionIC框架，它包含：1) 一个采用双向生成范式以消除主体漂移并保持语义连贯性的可扩展合成管线，用于数据生成；2) 一个轻量级位置调制层，用于解耦主体间的空间嵌入，从而实现独立、精确的定位，同时保持视觉保真度。", "result": "PositionIC方法在图像定制任务中实现了精确的空间控制，同时保持了高一致性。", "conclusion": "PositionIC为开放世界、多实体场景中的可控、高保真图像定制铺平了道路，并将被发布以促进进一步研究。"}}
{"id": "2507.13891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13891", "abs": "https://arxiv.org/abs/2507.13891", "authors": ["Yu Wei", "Jiahui Zhang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations", "comment": "Accepted by ICCV2025", "summary": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.", "AI": {"tldr": "PCR-GS提出了一种无COLMAP的3D Gaussian Splatting技术，通过特征重投影和基于小波的频率正则化对相机姿态进行协同正则化，以解决现有方法在复杂相机轨迹下场景重建和相机姿态估计不佳的问题。", "motivation": "现有的无COLMAP 3D Gaussian Splatting方法在处理具有剧烈旋转和平移的复杂相机轨迹场景时表现不佳，导致相机姿态估计退化，并使相机姿态和3D-GS的联合优化陷入局部最优。", "method": "PCR-GS通过相机姿态协同正则化实现。具体包括两方面：1) 特征重投影正则化：从相邻视图中提取对视图鲁棒的DINO特征，并对齐其语义信息以正则化相机姿态。2) 基于小波的频率正则化：利用高频细节的差异进一步优化相机姿态中的旋转矩阵。", "result": "所提出的PCR-GS在多个真实世界场景中实现了卓越的3D场景建模和相机姿态估计，特别是在相机轨迹发生剧烈变化的情况下，表现出优异的无姿态3D-GS场景建模能力。", "conclusion": "PCR-GS通过引入创新的相机姿态协同正则化方法，有效提升了无COLMAP 3D Gaussian Splatting在复杂相机轨迹下的场景建模和姿态估计性能。"}}
{"id": "2507.13899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13899", "abs": "https://arxiv.org/abs/2507.13899", "authors": ["Yujian Mo", "Yan Wu", "Junqiao Zhao", "Jijun Wang", "Yinghao Hu", "Jun Yan"], "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection", "comment": null, "summary": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.", "AI": {"tldr": "该研究通过将视觉基础模型DepthAnything的深度先验与原始LiDAR点特征融合，并设计双路径RoI特征提取与融合模块，显著提升了LiDAR三维目标检测的精度。", "motivation": "LiDAR数据稀疏且其原始点特征（特别是反射率）表达能力有限，判别性弱。而视觉基础模型（如DepthAnything）能提供密集可靠的几何先验，但在LiDAR三维目标检测中这些先验尚未被充分利用。", "method": "1. 引入DepthAnything预测的深度先验，并与原始LiDAR属性融合，以丰富每个点的表示。2. 提出一个点级特征提取模块来利用增强后的点特征。3. 采用双路径RoI特征提取框架，包括一个基于体素的分支用于全局语义上下文，以及一个基于点的分支用于细粒度结构细节。4. 引入一个双向门控RoI特征融合模块，有效整合互补的RoI特征，平衡全局和局部信息。", "result": "在KITTI基准测试上进行了大量实验，结果表明所提出的方法持续提高了检测精度。", "conclusion": "将视觉基础模型的先验知识融入基于LiDAR的三维目标检测中具有显著价值，能够有效提升检测性能。"}}
{"id": "2507.13929", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13929", "abs": "https://arxiv.org/abs/2507.13929", "authors": ["Hsiang-Hui Hung", "Huu-Phu Do", "Yung-Hui Li", "Ching-Chun Huang"], "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views", "comment": "Accepted by MM 2024", "summary": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.", "AI": {"tldr": "TimeNeRF是一种可泛化的神经渲染方法，能够在少量输入视图下，渲染任意视点和任意时间的全新视图，尤其擅长模拟自然场景的昼夜变化。", "motivation": "现有NeRF技术在时间维度上的3D场景建模能力有限，缺乏专用数据集，且真实世界应用中多视图数据采集成本高昂，对未见场景的逐一优化效率低下。元宇宙等数字领域对沉浸式、自然昼夜过渡的3D环境建模需求日益增长。", "method": "该方法结合了多视图立体（multi-view stereo）、神经辐射场（neural radiance fields）和跨多样数据集的解耦策略（disentanglement strategies）。它构建了一个隐式内容辐射场来表示场景，并能在此基础上在任意时间点构建神经辐射场，最终通过体渲染合成该时间点的全新视图。", "result": "TimeNeRF能够在少量输入视图（few-shot setting）下渲染全新视图，无需针对每个场景进行单独优化。最显著的成果是它能创建逼真的全新视图，并实现不同时间（从黎明到黄昏）之间的平滑过渡，精准捕捉复杂的自然场景变化。", "conclusion": "TimeNeRF成功实现了在少量输入下可泛化的时间3D场景建模和渲染，尤其在模拟逼真的昼夜自然场景变化方面表现出色，解决了当前NeRF在时间和泛化能力上的局限性。"}}
{"id": "2507.13934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13934", "abs": "https://arxiv.org/abs/2507.13934", "authors": ["Marzieh Gheisari", "Auguste Genovesio"], "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "comment": null, "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.", "AI": {"tldr": "DiViD是一种端到端视频扩散框架，用于无监督地解耦视频中的静态外观和动态运动，通过创新的编码器、解码器和正则化器，解决了现有方法的信息泄露和模糊重建问题，并超越了现有技术。", "motivation": "视频中静态外观和动态运动的无监督解耦仍然是一个基本挑战，现有基于VAE和GAN的方法常受信息泄露和模糊重建的困扰。", "method": "引入DiViD，首个用于显式静态-动态分解的端到端视频扩散框架。其序列编码器从首帧提取全局静态token，并从每帧动态token中明确移除静态内容。条件DDPM解码器包含三个关键归纳偏置：共享噪声调度（用于时间一致性）、时变KL瓶颈（早期紧缩静态信息，后期放松丰富动态）和交叉注意力（将全局静态token路由到所有帧，同时保持动态token帧特异性）。此外，使用正交性正则化器防止残余的静态-动态泄露。", "result": "DiViD在真实世界基准测试中表现优于最先进的序列解耦方法：它实现了最高的基于交换的联合准确性，在提高动态传输的同时保持了静态保真度，并减少了平均交叉泄露。", "conclusion": "DiViD通过其创新的框架设计，有效解决了视频中静态和动态内容无监督解耦的难题，显著提高了分解的准确性和重建质量，并减少了信息泄露，超越了现有技术水平。"}}
{"id": "2507.13981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13981", "abs": "https://arxiv.org/abs/2507.13981", "authors": ["Sara Abdulaziz", "Giacomo D'Amicantonio", "Egor Bondarev"], "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset", "comment": "accepted at ICCV'25 workshop CV4BIOM", "summary": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.", "AI": {"tldr": "针对AI驱动的监控引发的隐私担忧，本文提出了一个评估视觉隐私保护方法的综合框架，并引入了HR-VISPR数据集，旨在提供一种客观的隐私评估工具。", "motivation": "AI驱动的监控技术日益普及，加剧了对敏感个人数据收集和处理的担忧。这促使研究转向隐私保护设计方案，并产生了对客观评估隐私保护效果技术的需求。", "method": "本文提出了一个评估视觉隐私保护方法的综合框架，从隐私、实用性和实践性三个维度进行评估。此外，引入了HR-VISPR数据集，这是一个公开的以人为中心的、包含生物识别、软生物识别和非生物识别标签的数据集，用于训练可解释的隐私度量。该框架被用于评估11种隐私保护方法，包括传统技术和深度学习方法。", "result": "该框架能够根据人类视觉感知区分不同的隐私级别，并揭示了隐私、实用性和实践性之间的权衡关系。", "conclusion": "这项研究及其HR-VISPR数据集提供了一个有洞察力的工具和一个适用于多种场景的结构化评估框架。"}}
{"id": "2507.13985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13985", "abs": "https://arxiv.org/abs/2507.13985", "authors": ["Haoran Li", "Yuli Tian", "Kun Lan", "Yong Liao", "Lin Wang", "Pan Hui", "Peng Yuan Zhou"], "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation", "comment": "Extended version of ECCV 2024 paper \"DreamScene\"", "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.", "AI": {"tldr": "DreamScene是一个端到端的框架，能够从文本或对话生成高质量、可编辑的3D场景，解决了现有方法在自动化、3D一致性和精细控制方面的不足。", "motivation": "现有从自然语言生成3D场景的方法在自动化、3D一致性和精细控制方面存在挑战，限制了它们在游戏、电影和设计等领域的应用潜力。", "method": "DreamScene首先通过场景规划模块（利用GPT-4推理对象语义和空间约束，构建混合图）生成碰撞检测的布局。接着，采用Formation Pattern Sampling (FPS)技术，通过多时间步采样和重建优化来生成对象几何。为确保全局一致性，系统采用渐进式相机采样策略。此外，DreamScene还支持精细化的场景编辑，包括对象移动、外观改变和4D动态运动。", "result": "实验证明，DreamScene在质量、一致性和灵活性方面超越了现有方法，为开放域3D内容创作提供了一个实用的解决方案。", "conclusion": "DreamScene提供了一个实用的、端到端解决方案，能够从文本或对话生成高质量、可编辑的3D场景，并在自动化、一致性和灵活性方面表现出色，有望推动3D内容创作的发展。"}}
{"id": "2507.14010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14010", "abs": "https://arxiv.org/abs/2507.14010", "authors": ["Yong Feng", "Xiaolei Zhang", "Shijin Feng", "Yong Zhao", "Yihan Chen"], "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations", "comment": "8 pages, 10 figures, 3 tables", "summary": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.", "AI": {"tldr": "本文提出一种两步深度学习方法，用于隧道裂缝的分类和分割，显著提高了检测精度和效率，并结合视觉解释技术增强模型可解释性。", "motivation": "隧道衬砌裂缝是隧道安全状态的关键指标。为了更准确、高效地对隧道裂缝进行分类和分割，从而提供隧道健康状况的快速准确评估。", "method": "该方法分为两步：第一步，使用DenseNet-169开发自动隧道图像分类模型，用于筛选包含裂缝的图像；第二步，基于DeepLabV3+构建裂缝分割模型，并利用分数加权视觉解释技术评估其内部逻辑。通过将分类和分割相结合，仅对第一步筛选出的含裂缝图像进行分割，以提高检测准确性和效率。", "result": "实验验证了该两步法的优越性能。隧道裂缝分类模型的准确率达到92.23%，每秒帧数（FPS）为39.80，优于其他基于CNN和Transformer的模型。隧道裂缝分割模型的交并比（IoU）为57.01%，F1分数为67.44%，优于其他最先进的模型。此外，提供的视觉解释有助于理解深度学习模型的“黑箱”特性。", "conclusion": "所开发的两阶段深度学习方法，结合视觉解释，为快速、准确地定量评估隧道健康状况提供了基础。"}}
{"id": "2507.14013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14013", "abs": "https://arxiv.org/abs/2507.14013", "authors": ["Ji-Yan Wu", "Zheng Yong Poh", "Anoop C. Patil", "Bongsoo Park", "Giovanni Volpe", "Daisuke Urano"], "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model", "comment": null, "summary": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.", "AI": {"tldr": "本研究提出了一种结合多光谱成像和改进YOLOv5（带Transformer注意力头）的深度学习框架，用于植物叶片营养缺乏症的异常分割，并在检测氯化和色素积累等症状上表现出色。", "motivation": "在精准农业中，准确检测植物叶片营养缺乏症至关重要，能实现早期干预，从而有效管理施肥、病害和胁迫。", "method": "本研究采用深度学习框架，处理九通道多光谱图像输入。核心模型是增强的YOLOv5，其头部集成了基于Transformer的注意力机制，利用自注意力捕捉细微的空间分布症状。实验在受控营养胁迫条件下进行，并与基线YOLOv5进行对比。", "result": "所提出的模型显著优于基线YOLOv5，平均Dice分数和IoU（交并比）提高了约12%。该模型在检测叶片氯化和色素积累等挑战性症状方面尤其有效。", "conclusion": "结合多光谱成像与光谱-空间特征学习在推进植物表型分析和精准农业方面具有广阔前景。"}}
{"id": "2507.14024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14024", "abs": "https://arxiv.org/abs/2507.14024", "authors": ["Jiarong Ye", "Sharon X. Huang"], "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing", "comment": null, "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.", "AI": {"tldr": "该研究提出了一个名为Moodifier的系统，通过构建大型情感标注数据集MoodArchive和微调视觉语言模型MoodifyCLIP，实现了精确的情感驱动图像编辑，并在情感准确性和内容保留方面优于现有方法。", "motivation": "将情感与视觉内容相结合进行图像编辑在创意产业中潜力巨大，但由于情感的抽象性和在不同语境下的多样化表现，精确的情感操控仍然极具挑战性。", "method": "该研究采用三部分集成方法：1. 引入MoodArchive，一个包含800多万张图像的大型数据集，具有LLaVA生成并部分人工验证的详细分层情感标注。2. 开发MoodifyCLIP，一个在MoodArchive上微调的视觉语言模型，用于将抽象情感转化为具体的视觉属性。3. 提出Moodifier，一个免训练的编辑模型，利用MoodifyCLIP和多模态大型语言模型（MLLMs）实现精确的情感转换，同时保持内容完整性。", "result": "Moodifier系统在字符表情、时尚设计、珠宝和家居装饰等多个领域均能工作，使创作者能够快速可视化情感变化，同时保留身份和结构。广泛的实验评估表明，Moodifier在情感准确性和内容保留方面均优于现有方法，能够提供符合上下文的编辑。", "conclusion": "该解决方案通过将抽象情感与具体的视觉变化联系起来，为现实世界应用中的情感内容创作开启了新的可能性。研究团队将发布MoodArchive数据集、MoodifyCLIP模型以及Moodifier的代码和演示。"}}
{"id": "2507.14031", "categories": ["cs.CV", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14031", "abs": "https://arxiv.org/abs/2507.14031", "authors": ["Hao Fang", "Sihao Teng", "Hao Yu", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography", "comment": "10 pages, 12 figures", "summary": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.", "AI": {"tldr": "QuantEIT是一种超轻量级、量子辅助的无监督深度学习框架，用于电学阻抗断层成像（EIT）图像重建，显著减少了模型复杂度和参数数量，同时提高了重建精度和抗噪能力。", "motivation": "电学阻抗断层成像（EIT）因其无创、低成本和高时间分辨率而适用于床旁监测，但其固有的病态逆问题导致图像重建困难。现有基于深度学习的方法虽然有前景，但通常依赖复杂的网络架构和大量参数，限制了效率和可扩展性。", "method": "本文提出了QuantEIT框架，利用量子辅助网络（QA-Net）进行EIT图像重建。QA-Net结合并行的2量子比特量子电路生成富有表现力的潜在表示（作为隐式非线性先验），然后通过单个线性层进行电导率重建。该设计大幅减少了模型复杂度和参数数量。QuantEIT以无监督、无需训练数据的方式运行，是首次将量子电路集成到EIT图像重建中。", "result": "在模拟和真实世界的2D和3D EIT肺部成像数据上进行了广泛实验，结果表明QuantEIT优于传统方法，仅使用0.2%的参数即可达到相当或更优的重建精度，并增强了对噪声的鲁棒性。", "conclusion": "QuantEIT为EIT图像重建提供了一种超轻量级、量子辅助的无监督解决方案，在显著降低模型复杂度的同时，保持了高重建性能和对噪声的鲁棒性，是量子电路在EIT应用中的首次成功探索。"}}
{"id": "2507.14042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14042", "abs": "https://arxiv.org/abs/2507.14042", "authors": ["Qiankun Ma", "Ziyao Zhang", "Chi Su", "Jie Chen", "Zhen Song", "Hairong Zheng", "Wen Gao"], "title": "Training-free Token Reduction for Vision Mamba", "comment": null, "summary": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.", "AI": {"tldr": "本文提出MTR框架，通过Mamba结构感知的注意力分数，实现了Vision Mamba模型的高效Token缩减，显著降低计算量且性能影响小。", "motivation": "Vision Mamba因其线性复杂度在长距离依赖捕获方面表现出色，但ViT中常用的Token缩减技术在Vision Mamba中鲜有探索。直接应用现有方法会导致性能显著下降，因为Mamba是序列模型，缺乏ViT依赖的注意力机制来衡量Token重要性并忽略压缩Token的顺序。", "method": "本文研究了一种Mamba结构感知的Token重要性评分方法，并在此基础上提出了MTR（Mamba Token Reduction）框架。MTR是一种免训练、无需额外调优参数的即插即用组件，可无缝集成到各种Mamba模型中。", "result": "实验证明，MTR显著降低了计算负载，同时最大限度地减少了性能影响。例如，在Vim-B骨干网络上，MTR减少了约40%的FLOPs，而ImageNet性能仅下降1.6%，且无需重新训练。", "conclusion": "MTR是一个简单而有效的Vision Mamba Token缩减框架，能够显著提高模型的计算效率，同时保持良好的性能，为Vision Mamba的更广泛应用奠定了基础。"}}
{"id": "2507.14050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14050", "abs": "https://arxiv.org/abs/2507.14050", "authors": ["Mohamed Elkhayat", "Mohamed Mahmoud", "Jamil Fayyad", "Nourhan Bayasi"], "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification", "comment": "Accepted at the MICCAI EMERGE 2025 workshop", "summary": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.", "AI": {"tldr": "本文系统评估了冻结的基础模型（FM）在皮肤病分类的类增量学习（CIL）中的潜力，提出了一种简单有效的方法，即冻结骨干网络并增量训练轻量级MLP，实现了SOTA性能且无遗忘。研究还发现基于原型的零训练方法也能获得有竞争力的结果。", "motivation": "基础模型（FM）在大型数据集上预训练后能提供丰富的可迁移表示，为类增量学习（CIL）带来了新机遇。然而，它们在皮肤病学领域实现增量学习的潜力尚未被充分探索。", "method": "研究方法包括：1. 系统评估了在大型皮肤病变数据集上预训练的冻结FM在皮肤病分类CIL中的表现。2. 提出了一种简单有效的方法：骨干网络保持冻结，仅增量训练一个轻量级多层感知器（MLP）。3. 进一步探索了零训练场景，使用最近均值分类器，其原型源自FM的嵌入。4. 将提出的方法与基于正则化、重放和架构的方法进行比较。", "result": "主要结果显示：1. 所提出的冻结骨干网络加增量MLP的方法在不遗忘的情况下实现了最先进（SOTA）的性能，优于基于正则化、重放和架构的方法。2. 基于原型的零训练变体也能获得有竞争力的结果。", "conclusion": "研究结论强调了冻结的基础模型在皮肤病学持续学习方面的强大能力，并支持它们在真实世界医疗应用中的更广泛采用。"}}
{"id": "2507.14083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14083", "abs": "https://arxiv.org/abs/2507.14083", "authors": ["Sara Abdulaziz", "Egor Bondarev"], "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection", "comment": "ACIVS 2025", "summary": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.", "AI": {"tldr": "本研究评估了四种人体匿名化技术（模糊、遮蔽、加密、头像替换）对视频异常检测性能的影响，发现异常检测在匿名数据下仍可行，且性能取决于算法设计和学习策略，有时甚至能提升。", "motivation": "深度学习在异常检测方面的进步带来了敏感人体数据收集的隐私担忧，因此需要探索在保护隐私的同时维持异常检测性能的方法。", "method": "在UCF-Crime数据集上，应用四种人体匿名化技术（模糊、遮蔽、加密、头像替换），评估MGFN、UR-DMU、BN-WVAD和PEL4VAD四种异常检测方法在匿名数据上的性能，并比较了传统匿名化技术与新兴的隐私设计解决方案。", "result": "异常检测在匿名数据下依然可行，其性能受算法设计和学习策略影响。在某些匿名化模式（如加密和遮蔽）下，部分模型甚至能达到比原始数据更高的AUC性能，这表明算法对噪声模式的强烈响应性。研究强调了隐私保护与检测效用之间以及鲁棒隐私保护与效用灵活性之间存在的权衡。", "conclusion": "本研究为平衡人体隐私与异常检测需求提供了一个有力的基准和深入见解。"}}
{"id": "2507.14095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14095", "abs": "https://arxiv.org/abs/2507.14095", "authors": ["Yung-Hong Sun", "Ting-Hung Lin", "Jiangang Chen", "Hongrui Jiang", "Yu Hen Hu"], "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs", "comment": null, "summary": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.", "AI": {"tldr": "C-DOG是一个无需训练的框架，通过结合连接的delta-overlap图建模和对极几何，在不依赖视觉特征的情况下，鲁棒地关联多视角下的物体检测，适用于3D重建。", "motivation": "现有方法在物体视觉上难以区分或观测数据受噪声干扰时会失效。因此，需要一种不依赖视觉特征、对噪声和高密度场景鲁棒的物体关联方法。", "method": "C-DOG框架将每个2D观测表示为图节点，边权重基于对极一致性。采用delta-neighbor-overlap聚类识别强一致性组，并容忍噪声和部分连接。为提高鲁棒性，引入基于四分位距（IQR）的滤波和3D反投影误差准则来消除不一致观测。", "result": "在合成基准测试中，C-DOG优于基于几何的基线方法，并在高物体密度、无视觉特征和有限相机重叠等挑战性条件下保持鲁棒性。", "conclusion": "C-DOG提供了一种鲁棒且可扩展的多视角多物体关联解决方案，特别适用于真实世界场景中具有挑战性的3D重建任务。"}}
{"id": "2507.14137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14137", "abs": "https://arxiv.org/abs/2507.14137", "authors": ["Shashanka Venkataramanan", "Valentinos Pariza", "Mohammadreza Salehi", "Lukas Knobel", "Spyros Gidaris", "Elias Ramzi", "Andrei Bursuc", "Yuki M. Asano"], "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning", "comment": null, "summary": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.", "AI": {"tldr": "Franca是一个完全开源的视觉基础模型，其性能与专有模型相当甚至超越，通过引入多头聚类投影器和位置解耦策略，解决了SSL聚类中的语义模糊和位置偏差问题。", "motivation": "当前最先进的视觉基础模型多为专有且不透明，且现有的自监督学习（SSL）聚类方法（如Sinkhorn-Knopp）在处理图像特征到大型码本的分配时，未能考虑聚类语义固有的模糊性，同时存在表示中的位置偏差。", "method": "该研究提出Franca模型，采用受Web-SSL启发的透明训练流程，并使用公开数据（ImageNet-21K和ReLAION-2B子集）。核心方法包括：1) 引入基于嵌套Matryoshka表示的参数高效、多头聚类投影器，以渐进式地将特征细化为更细粒度的簇，同时不增加模型大小。2) 提出一种新颖的位置解耦策略，明确地从密集表示中移除位置偏差，以改善语义内容的编码。", "result": "Franca在许多情况下匹配并超越了DINOv2、CLIP、SigLIPv2等最先进的专有模型。所提出的多头聚类投影器实现了性能和内存效率的提升。位置解耦策略在多个下游基准测试中带来了持续的性能提升，证明了更清晰特征空间的有效性。", "conclusion": "Franca的贡献为透明、高性能的视觉模型树立了新标准，并为AI社区中更可复现和泛化的基础模型开辟了道路。"}}
