{"id": "2508.02870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02870", "abs": "https://arxiv.org/abs/2508.02870", "authors": ["Mohamed Irfan Refai", "Abdulaziz Y. Alkayas", "Anup Teejo Mathew", "Federico Renda", "Thomas George Thuruthel"], "title": "Learning User Interaction Forces using Vision for a Soft Finger Exosuit", "comment": "13 pages, 9 figures", "summary": "Wearable assistive devices are increasingly becoming softer. Modelling their\ninterface with human tissue is necessary to capture transmission of dynamic\nassistance. However, their nonlinear and compliant nature makes both physical\nmodeling and embedded sensing challenging. In this paper, we develop a\nimage-based, learning-based framework to estimate distributed contact forces\nfor a finger-exosuit system. We used the SoRoSim toolbox to generate a diverse\ndataset of exosuit geometries and actuation scenarios for training. The method\naccurately estimated interaction forces across multiple contact locations from\nlow-resolution grayscale images, was able to generalize to unseen shapes and\nactuation levels, and remained robust under visual noise and contrast\nvariations. We integrated the model into a feedback controller, and found that\nthe vision-based estimator functions as a surrogate force sensor for\nclosed-loop control. This approach could be used as a non-intrusive alternative\nfor real-time force estimation for exosuits.", "AI": {"tldr": "本文提出了一种基于图像的机器学习框架，用于估计软体可穿戴设备（如手指外骨骼）与人体组织之间的分布式接触力，并证明其可作为闭环控制的替代力传感器。", "motivation": "可穿戴辅助设备日益软化，但其非线性顺应性使得物理建模和嵌入式传感都极具挑战性，难以捕捉动态辅助的力传输，因此需要一种新的方法来估计其与人体组织的界面力。", "method": "开发了一种基于图像的、学习型框架来估计手指外骨骼系统的分布式接触力。利用SoRoSim工具箱生成了多样化的外骨骼几何形状和驱动场景数据集用于训练。将该模型集成到反馈控制器中进行测试。", "result": "该方法能从低分辨率灰度图像中准确估计多个接触点的相互作用力，能够泛化到未见的形状和驱动水平，并在视觉噪声和对比度变化下保持鲁棒性。视觉估计器可作为闭环控制的替代力传感器。", "conclusion": "所提出的基于图像的学习方法可以作为外骨骼实时力估计的一种非侵入式替代方案，为软体可穿戴设备的控制提供了新的可能性。"}}
{"id": "2508.02873", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02873", "abs": "https://arxiv.org/abs/2508.02873", "authors": ["Rongqian Chen", "Jun Kwon", "Kefan Wu", "Wei-Hsi Chen"], "title": "Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping Across Varying Ground Profiles", "comment": "2025 IEEE International Conference on Robotics & Automation (ICRA)", "summary": "We present the design and implementation of HASTA (Hopper with Adjustable\nStiffness for Terrain Adaptation), a vertical hopping robot with real-time\ntunable leg stiffness, aimed at optimizing energy efficiency across various\nground profiles (a pair of ground stiffness and damping conditions). By\nadjusting leg stiffness, we aim to maximize apex hopping height, a key metric\nfor energy-efficient vertical hopping. We hypothesize that softer legs perform\nbetter on soft, damped ground by minimizing penetration and energy loss, while\nstiffer legs excel on hard, less damped ground by reducing limb deformation and\nenergy dissipation. Through experimental tests and simulations, we find the\nbest leg stiffness within our selection for each combination of ground\nstiffness and damping, enabling the robot to achieve maximum steady-state\nhopping height with a constant energy input. These results support our\nhypothesis that tunable stiffness improves energy-efficient locomotion in\ncontrolled experimental conditions. In addition, the simulation provides\ninsights that could aid in the future development of controllers for selecting\nleg stiffness.", "AI": {"tldr": "HASTA是一个具有可调腿部刚度的垂直跳跃机器人，旨在通过实时调整腿部刚度，在不同地面条件下优化能量效率，实现最大跳跃高度。", "motivation": "在不同地面（具有不同刚度和阻尼）上，通过调整机器人腿部刚度来优化能量效率，以最大化跳跃高度。研究假设是，软腿在软阻尼地面表现更好，硬腿在硬低阻尼地面表现更好。", "method": "设计并实现了HASTA机器人，该机器人具有实时可调的腿部刚度。通过实验测试和仿真，寻找在不同地面条件下最佳的腿部刚度设置。", "result": "研究找到了在每种地面刚度和阻尼组合下，能使机器人在恒定能量输入下达到最大稳态跳跃高度的最佳腿部刚度。这些结果支持了可调刚度能提高能量效率的假设。此外，仿真为未来开发腿部刚度选择控制器提供了见解。", "conclusion": "可调刚度能够提高机器人在受控实验条件下的能量高效运动能力，并且仿真结果为未来控制器开发提供了指导。"}}
{"id": "2508.02898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02898", "abs": "https://arxiv.org/abs/2508.02898", "authors": ["Isobel Voysey", "Lynne Baillie", "Joanne Williams", "Michael Herrmann"], "title": "Co-designing Zoomorphic Robot Concepts for Animal Welfare Education", "comment": null, "summary": "Animal welfare education could greatly benefit from customized robots to help\nchildren learn about animals and their behavior, and thereby promote positive,\nsafe child-animal interactions. To this end, we ran Participatory Design\nworkshops with animal welfare educators and children to identify key\nrequirements for zoomorphic robots from their perspectives. Our findings\nencompass a zoomorphic robot's appearance, behavior, and features, as well as\nconcepts for a narrative surrounding the robot. Through comparing and\ncontrasting the two groups, we find the importance of: negative reactions to\nundesirable behavior from children; using the facial features and tail to\nprovide cues signaling an animal's internal state; and a natural, furry\nappearance and texture. We also contribute some novel activities for\nParticipatory Design with children, including branching storyboards inspired by\nthematic apperception tests and interactive narratives, and reflect on some of\nthe key design challenges of achieving consensus between the groups, despite\nmuch overlap in their design concepts.", "AI": {"tldr": "该研究通过与动物福利教育者和儿童的参与式设计工作坊，探讨了用于动物福利教育的拟动物机器人关键需求，并提出了设计挑战和新颖的参与式设计活动。", "motivation": "动物福利教育可通过定制机器人帮助儿童学习动物及其行为，从而促进积极、安全的儿童-动物互动。", "method": "与动物福利教育者和儿童进行了参与式设计工作坊，以从他们的角度识别拟动物机器人的关键要求。同时贡献了新颖的儿童参与式设计活动，包括受主题统觉测验和互动叙事启发的支线故事板。", "result": "研究结果涵盖了拟动物机器人的外观、行为和特征，以及围绕机器人的叙事概念。通过比较两组，发现以下重要性：儿童对不良行为的负面反应；利用面部特征和尾巴提供信号动物内部状态的线索；以及自然、毛茸茸的外观和质地。研究还反思了在各组间达成共识的一些关键设计挑战。", "conclusion": "为动物福利教育设计拟动物机器人时，应重视儿童对不良行为的反应、利用面部和尾部表达动物情绪、以及自然毛茸茸的外观。尽管设计概念有重叠，但在不同用户群体间达成共识仍是主要挑战。"}}
{"id": "2508.02919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02919", "abs": "https://arxiv.org/abs/2508.02919", "authors": ["Boyang Tian", "Weisong Shi"], "title": "Context-aware Risk Assessment and Its Application in Autonomous Driving", "comment": "ITSC 2025, 7 pages", "summary": "Ensuring safety in autonomous driving requires precise, real-time risk\nassessment and adaptive behavior. Prior work on risk estimation either outputs\ncoarse, global scene-level metrics lacking interpretability, proposes\nindicators without concrete integration into autonomous systems, or focuses\nnarrowly on specific driving scenarios. We introduce the Context-aware Risk\nIndex (CRI), a light-weight modular framework that quantifies directional risks\nbased on object kinematics and spatial relationships, dynamically adjusting\ncontrol commands in real time. CRI employs direction-aware spatial partitioning\nwithin a dynamic safety envelope using Responsibility-Sensitive Safety (RSS)\nprinciples, a hybrid probabilistic-max fusion strategy for risk aggregation,\nand an adaptive control policy for real-time behavior modulation. We evaluate\nCRI on the Bench2Drive benchmark comprising 220 safety-critical scenarios using\na state-of-the-art end-to-end model Transfuser++ on challenging routes. Our\ncollision-rate metrics show a 19\\% reduction (p = 0.003) in vehicle collisions\nper failed route, a 20\\% reduction (p = 0.004) in collisions per kilometer, a\n17\\% increase (p = 0.016) in composed driving score, and a statistically\nsignificant reduction in penalty scores (p = 0.013) with very low overhead (3.6\nms per decision cycle). These results demonstrate that CRI substantially\nimproves safety and robustness in complex, risk-intensive environments while\nmaintaining modularity and low runtime overhead.", "AI": {"tldr": "本文提出了一种名为CRI（Context-aware Risk Index）的轻量级模块化框架，用于自动驾驶中的实时风险评估和自适应行为调整，显著降低了碰撞率并提高了驾驶安全性。", "motivation": "现有的风险评估方法存在局限性，包括输出粗略的全局场景指标、缺乏可解释性、指标未能具体整合到自动驾驶系统中，或仅关注特定驾驶场景。因此，需要一种能够提供精确、实时风险评估并实现自适应行为的解决方案，以确保自动驾驶的安全性。", "method": "CRI框架通过以下方法量化定向风险并动态调整控制指令：1. 基于物体运动学和空间关系，在动态安全包络线内采用方向感知的空间划分（利用责任敏感安全RSS原则）。2. 采用混合概率-最大融合策略进行风险聚合。3. 采用自适应控制策略进行实时行为调制。该框架具有轻量级和模块化特点。", "result": "在包含220个安全关键场景的Bench2Drive基准测试中，使用先进的端到端模型Transfuser++在挑战性路线上评估了CRI。结果显示：每失败路线的车辆碰撞减少19%（p = 0.003），每公里碰撞减少20%（p = 0.004），综合驾驶得分提高17%（p = 0.016），罚分显著减少（p = 0.013）。此外，CRI的开销极低（每个决策周期3.6毫秒）。", "conclusion": "CRI在复杂、高风险的环境中显著提高了自动驾驶的安全性与鲁棒性，同时保持了模块化和低运行时开销。这些结果证明了CRI在实际应用中的有效性。"}}
{"id": "2508.02694", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.02694", "abs": "https://arxiv.org/abs/2508.02694", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.", "AI": {"tldr": "本研究系统性地探讨了LLM驱动的智能体系统在效率与有效性之间的权衡，并提出了一个名为“高效智能体”的新框架，显著降低了成本同时保持了高水平性能。", "motivation": "大型语言模型（LLM）驱动的智能体在处理复杂多步骤任务方面表现出色，但其不断上升的成本威胁到可扩展性和可访问性。因此，迫切需要设计具有成本效益且不牺牲性能的智能体系统。", "method": "本研究对现代智能体系统中的效率-有效性权衡进行了首次系统性研究。通过在GAIA基准上进行实证分析，评估了LLM骨干模型选择、智能体框架设计和测试时扩展策略的影响。使用“通过成本”（cost-of-pass）指标量化了这些维度上的效率-性能权衡。在此基础上，提出了一种名为“高效智能体”的新型智能体框架，旨在实现任务要求与复杂度的最优匹配。", "result": "研究回答了智能体任务固有的复杂性需求、附加模块的边际效益递减点以及通过高效框架设计可获得的效率提升等问题。结果表明，“高效智能体”框架在将操作成本从0.398美元降低到0.228美元的同时（通过成本提高了28.4%），仍保留了领先开源智能体框架OWL 96.7%的性能。", "conclusion": "本研究为设计高效、高性能的智能体系统提供了可操作的见解，从而提升了AI驱动解决方案的可访问性和可持续性。"}}
{"id": "2508.02808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02808", "abs": "https://arxiv.org/abs/2508.02808", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "comment": null, "summary": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.", "AI": {"tldr": "ICARE是一个可解释的、基于大语言模型代理的放射报告自动评估框架，通过动态多项选择问答来衡量生成报告的临床准确性和一致性，并显著优于现有指标。", "motivation": "放射报告生成（RRG）模型的部署需要可靠的临床评估，但现有评估指标多依赖于表层相似性或缺乏可解释性，无法满足安全部署的要求。", "method": "ICARE（Interpretable and Clinically-grounded Agent-based Report Evaluation）框架利用两个大语言模型代理：一个持有真实报告，另一个持有生成报告。它们互相生成并回答临床相关问题，通过答案的一致性来衡量发现的保留和一致性，作为临床准确性和召回率的代理指标。分数与问答对关联，实现透明评估。", "result": "临床医生研究表明，ICARE与专家判断的吻合度显著高于现有指标。扰动分析证实了其对临床内容的敏感性和可复现性。模型比较揭示了可解释的错误模式。", "conclusion": "ICARE提供了一个透明、可解释且以临床为基础的放射报告评估框架，能够更准确地反映专家判断，并提供错误模式的洞察，从而促进RRG模型的安全部署。"}}
{"id": "2508.02806", "categories": ["cs.CV", "cs.LG", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.02806", "abs": "https://arxiv.org/abs/2508.02806", "authors": ["Zongyou Yang", "Jonathan Loo"], "title": "PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation", "comment": "10 pages, 20 figures", "summary": "Recently, a significant improvement in the accuracy of 3D human pose\nestimation has been achieved by combining convolutional neural networks (CNNs)\nwith pyramid grid alignment feedback loops. Additionally, innovative\nbreakthroughs have been made in the field of computer vision through the\nadoption of Transformer-based temporal analysis architectures. Given these\nadvancements, this study aims to deeply optimize and improve the existing Pymaf\nnetwork architecture. The main innovations of this paper include: (1)\nIntroducing a Transformer feature extraction network layer based on\nself-attention mechanisms to enhance the capture of low-level features; (2)\nEnhancing the understanding and capture of temporal signals in video sequences\nthrough feature temporal fusion techniques; (3) Implementing spatial pyramid\nstructures to achieve multi-scale feature fusion, effectively balancing feature\nrepresentations differences across different scales. The new PyCAT4 model\nobtained in this study is validated through experiments on the COCO and 3DPW\ndatasets. The results demonstrate that the proposed improvement strategies\nsignificantly enhance the network's detection capability in human pose\nestimation, further advancing the development of human pose estimation\ntechnology.", "AI": {"tldr": "该研究通过引入Transformer特征提取、时序特征融合和空间金字塔结构，深度优化了PyMAF网络，提出了PyCAT4模型，显著提升了3D人体姿态估计的准确性。", "motivation": "现有PyMAF网络结合CNN和金字塔网格对齐已取得显著进展，同时Transformer在计算机视觉时序分析中也实现了突破。为进一步提升3D人体姿态估计精度，本研究旨在优化和改进现有PyMAF架构。", "method": "1. 引入基于自注意力机制的Transformer特征提取网络层，以增强低层特征捕获。2. 通过特征时序融合技术，增强对视频序列中时序信号的理解和捕获。3. 实施空间金字塔结构，实现多尺度特征融合，平衡不同尺度特征表示差异。", "result": "在COCO和3DPW数据集上的实验结果表明，所提出的改进策略显著增强了网络在人体姿态估计中的检测能力。", "conclusion": "本研究提出的新PyCAT4模型通过结合Transformer、时序融合和空间金字塔结构，有效提升了人体姿态估计技术的性能，进一步推动了该领域的发展。"}}
{"id": "2508.02726", "categories": ["eess.IV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02726", "abs": "https://arxiv.org/abs/2508.02726", "authors": ["Lucio Pinello", "Francesco Cadini", "Luca Lomazzi"], "title": "MPCA-based Domain Adaptation for Transfer Learning in Ultrasonic Guided Waves", "comment": null, "summary": "Ultrasonic Guided Waves (UGWs) represent a promising diagnostic tool for\nStructural Health Monitoring (SHM) in thin-walled structures, and their\nintegration with machine learning (ML) algorithms is increasingly being adopted\nto enable real-time monitoring capabilities. However, the large-scale\ndeployment of UGW-based ML methods is constrained by data scarcity and limited\ngeneralisation across different materials and sensor configurations. To address\nthese limitations, this work proposes a novel transfer learning (TL) framework\nbased on Multilinear Principal Component Analysis (MPCA). First, a\nConvolutional Neural Network (CNN) for regression is trained to perform damage\nlocalisation for a plated structure. Then, MPCA and fine-tuning are combined to\nhave the CNN work for a different plate. By jointly applying MPCA to the source\nand target domains, the method extracts shared latent features, enabling\neffective domain adaptation without requiring prior assumptions about\ndimensionality. Following MPCA, fine-tuning enables adapting the pre-trained\nCNN to a new domain without the need for a large training dataset. The proposed\nMPCA-based TL method was tested against 12 case studies involving different\ncomposite materials and sensor arrays. Statistical metrics were used to assess\ndomains alignment both before and after MPCA, and the results demonstrate a\nsubstantial reduction in localisation error compared to standard TL techniques.\nHence, the proposed approach emerges as a robust, data-efficient, and\nstatistically based TL framework for UGW-based SHM.", "AI": {"tldr": "该研究提出了一种基于多线性主成分分析（MPCA）的迁移学习（TL）框架，用于超声导波（UGW）结构健康监测（SHM）中的损伤定位，以解决数据稀缺和泛化能力差的问题。", "motivation": "UGW结合机器学习（ML）在薄壁结构SHM中应用受限于数据稀缺以及在不同材料和传感器配置下泛化能力有限。", "method": "首先训练一个用于回归的卷积神经网络（CNN）进行平板结构损伤定位。然后，结合MPCA和微调（fine-tuning），使CNN适应不同的平板结构。通过在源域和目标域联合应用MPCA提取共享潜在特征，实现有效的域适应，无需对维度进行预设。接着通过微调使预训练的CNN适应新域，无需大量训练数据。", "result": "该MPCA-based TL方法在12个涉及不同复合材料和传感器阵列的案例研究中进行了测试。结果显示，与标准TL技术相比，定位误差显著降低。", "conclusion": "所提出的方法是一个鲁棒、数据高效且基于统计的UGW-based SHM迁移学习框架，有效提升了损伤定位的准确性。"}}
{"id": "2508.02843", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.02843", "abs": "https://arxiv.org/abs/2508.02843", "authors": ["M. F. Shakib"], "title": "State dimension reduction of recurrent equilibrium networks with contraction and robustness preservation", "comment": null, "summary": "Recurrent equilibrium networks (RENs) are effective for learning the dynamics\nof complex dynamical systems with certified contraction and robustness\nproperties through unconstrained learning. While this opens the door to\nlearning large-scale RENs, deploying such large-scale RENs in real-time\napplications on resource-limited devices remains challenging. Since a REN\nconsists of a feedback interconnection of linear time-invariant (LTI) dynamics\nand static activation functions, this article proposes a projection-based\napproach to reduce the state dimension of the LTI component of a trained REN.\nOne of the two projection matrices is dedicated to preserving contraction and\nrobustness by leveraging the already-learned REN contraction certificate. The\nother projection matrix is iteratively updated to improve the accuracy of the\nreduced-order REN based on necessary $h_2$-optimality conditions for LTI model\nreduction. Numerical examples validate the approach, demonstrating significant\nstate dimension reduction with limited accuracy loss while preserving\ncontraction and robustness.", "AI": {"tldr": "本文提出了一种基于投影的方法，用于降低已训练的循环平衡网络（RENs）中线性时不变（LTI）组件的状态维度，以实现在资源受限设备上的部署，同时保持收缩性和鲁棒性。", "motivation": "循环平衡网络（RENs）在学习复杂动力学系统方面表现出色，并具有经过认证的收缩性和鲁棒性。然而，将大规模RENs部署到资源受限设备上的实时应用中仍然是一个挑战。", "method": "该方法提出了一种基于投影的方法来减少已训练RENs中LTI组件的状态维度。其中一个投影矩阵专门用于利用已学习的REN收缩性证书来保持收缩性和鲁棒性。另一个投影矩阵则基于LTI模型降阶的必要h2-最优条件进行迭代更新，以提高降阶REN的准确性。", "result": "数值示例验证了该方法，表明在有限的精度损失下实现了显著的状态维度降低，同时保留了收缩性和鲁棒性。", "conclusion": "所提出的投影方法能够有效降低RENs的复杂性，使其适用于资源受限的设备，同时确保其关键性能（收缩性和鲁棒性）得以保持。"}}
{"id": "2508.02930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02930", "abs": "https://arxiv.org/abs/2508.02930", "authors": ["Zenan Zhu", "Wenxi Chen", "Pei-Chun Kao", "Janelle Clark", "Lily Behnke", "Rebecca Kramer-Bottiglio", "Holly Yanco", "Yan Gu"], "title": "Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain Geometry Estimation with Wearable Soft Sensors", "comment": "8 pages, 5 figures", "summary": "This letter presents a model-agnostic meta-learning (MAML) based framework\nfor simultaneous and accurate estimation of human gait phase and terrain\ngeometry using a small set of fabric-based wearable soft sensors, with\nefficient adaptation to unseen subjects and strong generalization across\ndifferent subjects and terrains. Compared to rigid alternatives such as\ninertial measurement units, fabric-based soft sensors improve comfort but\nintroduce nonlinearities due to hysteresis, placement error, and fabric\ndeformation. Moreover, inter-subject and inter-terrain variability, coupled\nwith limited calibration data in real-world deployments, further complicate\naccurate estimation. To address these challenges, the proposed framework\nintegrates MAML into a deep learning architecture to learn a generalizable\nmodel initialization that captures subject- and terrain-invariant structure.\nThis initialization enables efficient adaptation (i.e., adaptation with only a\nsmall amount of calibration data and a few fine-tuning steps) to new users,\nwhile maintaining strong generalization (i.e., high estimation accuracy across\nsubjects and terrains). Experiments on nine participants walking at various\nspeeds over five terrain conditions demonstrate that the proposed framework\noutperforms baseline approaches in estimating gait phase, locomotion mode, and\nincline angle, with superior accuracy, adaptation efficiency, and\ngeneralization.", "AI": {"tldr": "该论文提出了一个基于MAML（模型无关元学习）的框架，利用织物软传感器，实现了人体步态相位和地形几何的同步准确估计，对未见过个体和不同地形具有高效适应和强泛化能力。", "motivation": "传统刚性传感器舒适性差；织物软传感器虽舒适但存在滞后、放置误差和织物变形等非线性问题；此外，个体和地形间的巨大差异以及实际应用中校准数据有限，进一步增加了准确估计的难度。", "method": "提出的框架将MAML集成到深度学习架构中，以学习一个可泛化的模型初始化。该初始化能够捕捉与个体和地形无关的结构，从而使得模型能够仅通过少量校准数据和几次微调步骤即可高效适应新用户，并保持在不同个体和地形间的高估计精度。", "result": "实验结果表明，该框架在步态相位、运动模式和倾斜角度估计方面，性能优于基线方法，展现出卓越的准确性、适应效率和泛化能力。", "conclusion": "该MAML框架成功解决了织物软传感器引入的非线性问题、个体和地形差异以及校准数据有限的挑战，实现了对新用户的高效适应和在不同个体与地形间的强泛化能力，为可穿戴软传感器在步态和地形估计领域的应用提供了有效解决方案。"}}
{"id": "2508.02697", "categories": ["cs.AI", "cs.CE", "03B35 (Primary) 03A99, 03B10, 03B25, 68V15, 03C07 (Secondary)", "I.2.3; I.2.4; F.4.1; F.2.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.02697", "abs": "https://arxiv.org/abs/2508.02697", "authors": ["Mikhail Soutchanski", "Yongmei Liu"], "title": "Planning with Dynamically Changing Domains", "comment": "A revised version of the paper accepted to the 1st International\n  Workshop on Trends in Knowledge Representation and Reasoning organized as a\n  IJCAI 2025 workshop that takes place in August 2025 in Montreal, Canada. See\n  the details at https://tkr2025.krportal.org/programme.html", "summary": "In classical planning and conformant planning, it is assumed that there are\nfinitely many named objects given in advance, and only they can participate in\nactions and in fluents. This is the Domain Closure Assumption (DCA). However,\nthere are practical planning problems where the set of objects changes\ndynamically as actions are performed; e.g., new objects can be created, old\nobjects can be destroyed. We formulate the planning problem in first-order\nlogic, assume an initial theory is a finite consistent set of fluent literals,\ndiscuss when this guarantees that in every situation there are only finitely\nmany possible actions, impose a finite integer bound on the length of the plan,\nand propose to organize search over sequences of actions that are grounded at\nplanning time. We show the soundness and completeness of our approach. It can\nbe used to solve the bounded planning problems without DCA that belong to the\nintersection of sequential generalized planning (without sensing actions) and\nconformant planning, restricted to the case without the disjunction over fluent\nliterals. We discuss a proof-of-the-concept implementation of our planner.", "AI": {"tldr": "该研究提出了一种在动态对象集（可创建/销毁对象）环境下，无需领域封闭假设（DCA）的规划方法，并证明了其完备性和正确性。", "motivation": "传统的经典规划和一致性规划都依赖于领域封闭假设（DCA），即预先给定有限的命名对象，且只有这些对象能参与动作和流变。然而，在实际规划问题中，对象集合会随着动作的执行而动态变化，例如创建新对象或销毁旧对象，DCA无法处理此类问题。", "method": "将规划问题形式化为一阶逻辑，假设初始理论是有限且一致的流变文字集合。讨论了何时能保证在每种情境下只有有限数量的可能动作。对计划长度施加有限整数限制，并提出在规划时对接地动作序列进行搜索。", "result": "该方法被证明是可靠且完备的。它能够解决属于序列广义规划（无感知动作）和一致性规划交集中的、无DCA的有限规划问题，但仅限于没有流变文字析取的特殊情况。论文还讨论了其规划器概念验证实现的细节。", "conclusion": "该方法为解决动态对象集下的规划问题提供了一种理论上健全且完备的解决方案，克服了传统规划中DCA的限制，并展示了其在特定问题类别中的有效性。"}}
{"id": "2508.02853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02853", "abs": "https://arxiv.org/abs/2508.02853", "authors": ["Yinuo Xu", "Veronica Derricks", "Allison Earl", "David Jurgens"], "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives", "comment": "28 pages, 17 figures", "summary": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives.", "AI": {"tldr": "该研究提出一种通过人口统计学感知的专家混合模型（DEM-MoE）和LLM生成的合成数据来建模主观NLP任务中标注者分歧的方法，以更好地表示多样化观点。", "motivation": "先前的模型难以有效表示标注者分歧中结构化的、群体层面的差异，并且存在人口统计学覆盖稀疏的问题，因此需要新的方法来提升对多样化视角的建模能力。", "method": "该研究采用两种创新方法：1) 架构创新：开发DEM-MoE模型，根据标注者人口统计学信息将输入路由到专家子网络；2) 数据中心创新：利用LLM通过零样本角色提示生成合成标注，用于数据插补和丰富，并评估了融合真实和合成数据的策略。", "result": "DEM-MoE模型在不同人口统计学群体中表现出竞争力，尤其在标注者分歧度高的数据集上表现出色。LLM生成的合成判断与人类标注有中等程度的一致性，并提供了一种可扩展的数据丰富方式。研究还发现，最佳的数据融合策略取决于数据集结构。", "conclusion": "这些贡献共同提升了在主观NLP任务中对多样化视角的表示能力，有效解决了标注者分歧建模和数据稀疏性问题。"}}
{"id": "2508.02807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02807", "abs": "https://arxiv.org/abs/2508.02807", "authors": ["Tongchun Zuo", "Zaiyu Huang", "Shuliang Ning", "Ente Lin", "Chao Liang", "Zerong Zheng", "Jianwen Jiang", "Yuan Zhang", "Mingyuan Gao", "Xin Dong"], "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework", "comment": "18 pages, 12 figures", "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. \\textbf{In the second stage}, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/", "AI": {"tldr": "DreamVVT是一种基于Diffusion Transformers的两阶段视频虚拟试穿框架，旨在解决现有方法对配对数据集的依赖、细节保留不足和时间一致性差的问题，通过利用非配对数据和预训练模型生成高质量、时间一致的虚拟试穿视频。", "motivation": "现有端到端视频虚拟试穿方法严重依赖稀缺的配对服装中心数据集，未能有效利用先进视觉模型和测试时输入的先验知识，导致在无约束场景下难以准确保留精细服装细节和维持时间一致性。", "method": "DreamVVT是一个两阶段框架：第一阶段，从输入视频中采样代表性帧，并利用集成视觉-语言模型（VLM）的多帧试穿模型合成高保真、语义一致的关键帧试穿图像，作为后续视频生成的补充外观指导。第二阶段，从输入内容中提取骨架图、精细运动和外观描述，连同关键帧试穿图像一起输入到通过LoRA适配器增强的预训练视频生成模型中，以确保未见区域的长期时间连贯性和高度逼真的动态运动。", "result": "广泛的定量和定性实验表明，DreamVVT在真实场景中超越了现有方法，在保留详细服装内容和时间稳定性方面表现更优。", "conclusion": "DreamVVT通过创新的两阶段DiT框架，有效解决了视频虚拟试穿中细节保留和时间一致性的挑战，并通过利用非配对数据和预训练模型，显著提升了在真实世界场景中的适应性和性能。"}}
{"id": "2508.02839", "categories": ["eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02839", "abs": "https://arxiv.org/abs/2508.02839", "authors": ["Zack Dewis", "Zhengsen Xu", "Yimin Zhu", "Motasem Alkayid", "Mabel Heffring", "Lincoln Linlin Xu"], "title": "Spatial-Temporal-Spectral Mamba with Sparse Deformable Token Sequence for Enhanced MODIS Time Series Classification", "comment": null, "summary": "Although MODIS time series data are critical for supporting dynamic,\nlarge-scale land cover land use classification, it is a challenging task to\ncapture the subtle class signature information due to key MODIS difficulties,\ne.g., high temporal dimensionality, mixed pixels, and spatial-temporal-spectral\ncoupling effect. This paper presents a novel spatial-temporal-spectral Mamba\n(STSMamba) with deformable token sequence for enhanced MODIS time series\nclassification, with the following key contributions. First, to disentangle\ntemporal-spectral feature coupling, a temporal grouped stem (TGS) module is\ndesigned for initial feature learning. Second, to improve Mamba modeling\nefficiency and accuracy, a sparse, deformable Mamba sequencing (SDMS) approach\nis designed, which can reduce the potential information redundancy in Mamba\nsequence and improve the adaptability and learnability of the Mamba sequencing.\nThird, based on SDMS, to improve feature learning, a novel\nspatial-temporal-spectral Mamba architecture is designed, leading to three\nmodules, i.e., a sparse deformable spatial Mamba module (SDSpaM), a sparse\ndeformable spectral Mamba module (SDSpeM), and a sparse deformable temporal\nMamba module (SDTM) to explicitly learn key information sources in MODIS. The\nproposed approach is tested on MODIS time series data in comparison with many\nstate-of-the-art approaches, and the results demonstrate that the proposed\napproach can achieve higher classification accuracy with reduced computational\ncomplexity.", "AI": {"tldr": "本文提出了一种新颖的空间-时间-光谱Mamba（STSMamba）模型，结合可变形令牌序列，用于增强MODIS时间序列数据的土地覆盖分类，解决了高维度、混合像素和耦合效应等挑战。", "motivation": "MODIS时间序列数据对于动态、大规模土地覆盖分类至关重要，但由于其高时间维度、混合像素以及空间-时间-光谱耦合效应，难以捕捉细微的类别特征信息。", "method": "1. 设计了时间分组主干（TGS）模块，用于初始特征学习，以解耦时间-光谱特征耦合。2. 提出了稀疏可变形Mamba序列化（SDMS）方法，以减少Mamba序列中的信息冗余，提高Mamba建模的效率、适应性和可学习性。3. 基于SDMS，设计了新颖的空间-时间-光谱Mamba架构，包括稀疏可变形空间Mamba模块（SDSpaM）、稀疏可变形光谱Mamba模块（SDSpeM）和稀疏可变形时间Mamba模块（SDTM），以显式学习MODIS中的关键信息源。", "result": "与许多现有先进方法相比，所提出的方法在MODIS时间序列数据测试中取得了更高的分类精度，并降低了计算复杂度。", "conclusion": "所提出的STSMamba模型能够有效处理MODIS时间序列数据的复杂性，通过创新的模块设计实现更准确、更高效的土地覆盖分类。"}}
{"id": "2508.02881", "categories": ["eess.SY", "cs.CR", "cs.GT", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.02881", "abs": "https://arxiv.org/abs/2508.02881", "authors": ["Faezeh Shojaeighadikolaei", "Shouhuai Xu", "Keith Paarporn"], "title": "Optimizing Preventive and Reactive Defense Resource Allocation with Uncertain Sensor Signals", "comment": "6 pages, 6 figures. Accepted for presentation at the 61st Allerton\n  Conference on Communication, Control, and Computing", "summary": "Cyber attacks continue to be a cause of concern despite advances in cyber\ndefense techniques. Although cyber attacks cannot be fully prevented, standard\ndecision-making frameworks typically focus on how to prevent them from\nsucceeding, without considering the cost of cleaning up the damages incurred by\nsuccessful attacks. This motivates us to investigate a new resource allocation\nproblem formulated in this paper: The defender must decide how to split its\ninvestment between preventive defenses, which aim to harden nodes from attacks,\nand reactive defenses, which aim to quickly clean up the compromised nodes.\nThis encounters a challenge imposed by the uncertainty associated with the\nobservation, or sensor signal, whether a node is truly compromised or not; this\nuncertainty is real because attack detectors are not perfect. We investigate\nhow the quality of sensor signals impacts the defender's strategic investment\nin the two types of defense, and ultimately the level of security that can be\nachieved. In particular, we show that the optimal investment in preventive\nresources increases, and thus reactive resource investment decreases, with\nhigher sensor quality. We also show that the defender's performance\nimprovement, relative to a baseline of no sensors employed, is maximal when the\nattacker can only achieve low attack success probabilities.", "AI": {"tldr": "研究在网络攻击防御中，如何在预防性防御和响应性防御之间分配资源，同时考虑传感器信号不确定性对防御策略和安全水平的影响。", "motivation": "现有网络防御决策框架侧重于阻止攻击成功，但忽略了成功攻击造成的清理成本。同时，攻击检测器不完美导致节点是否被入侵的传感器信号存在不确定性，这促使研究如何在预防和响应防御之间进行资源分配。", "method": "本文提出并建立了一个新的资源分配问题模型，要求防御者决定如何在旨在强化节点以抵御攻击的预防性防御和旨在快速清理受损节点的响应性防御之间分配投资，并考虑了与观察（传感器信号）相关的真伪不确定性。", "result": "研究表明，随着传感器质量的提高，对预防性资源的最佳投资会增加，从而减少对响应性资源的投资。此外，当攻击者只能实现较低的攻击成功概率时，防御者相对于不使用传感器的基线，其性能提升最大。", "conclusion": "传感器信号质量对防御者在预防性和响应性防御之间的战略投资选择以及最终能达到的安全水平有显著影响。在攻击成功概率较低的情况下，高质量传感器带来的防御性能提升最为显著。"}}
{"id": "2508.02947", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02947", "abs": "https://arxiv.org/abs/2508.02947", "authors": ["M Tanjid Hasan Tonmoy", "Rahath Malladi", "Kaustubh Singh", "Forsad Al Hossain", "Rajesh Gupta", "Andrés E. Tejada-Martínez", "Tauhidur Rahman"], "title": "AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed", "comment": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025. Author Accepted Manuscript", "summary": "Indoor air quality plays an essential role in the safety and well-being of\noccupants, especially in the context of airborne diseases. This paper\nintroduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor\nair purification systems through a robotic cough emulator testbed and a\ndigital-twins-based aerosol residence time analysis. Current portable air\nfilters often overlook the concentrations of respiratory aerosols generated by\ncoughs, posing a risk, particularly in high-exposure environments like\nhealthcare facilities and public spaces. To address this gap, we present a\nrobotic dual-agent physical emulator comprising a maneuverable mannequin\nsimulating cough events and a portable air purifier autonomously responding to\naerosols. The generated data from this emulator trains a digital twins model,\ncombining a physics-based compartment model with a machine learning approach,\nusing Long Short-Term Memory (LSTM) networks and graph convolution layers.\nExperimental results demonstrate the model's ability to predict aerosol\nconcentration dynamics with a mean residence time prediction error within 35\nseconds. The proposed system's real-time intervention strategies outperform\nstatic air filter placement, showcasing its potential in mitigating airborne\npathogen risks.", "AI": {"tldr": "本文提出AeroSafe系统，通过机器人咳嗽模拟器和数字孪生技术分析气溶胶停留时间，以提高室内空气净化系统的效率，从而有效降低空气传播疾病风险。", "motivation": "现有便携式空气过滤器常忽略咳嗽产生的呼吸道气溶胶浓度，在高风险环境（如医疗机构和公共场所）构成潜在威胁。研究旨在解决这一问题，提升空气净化系统应对气溶胶传播疾病的有效性。", "method": "开发了AeroSafe系统，包含一个机器人双代理物理模拟器：一个模拟咳嗽事件的机械人体模型和一个自主响应气溶胶的便携式空气净化器。模拟器生成的数据用于训练数字孪生模型，该模型结合了基于物理的隔室模型与机器学习方法（长短期记忆网络和图卷积层）。", "result": "实验结果表明，该模型能够预测气溶胶浓度动态，平均停留时间预测误差在35秒以内。所提出的系统实时干预策略优于静态空气过滤器放置方法。", "conclusion": "AeroSafe系统通过其创新的机器人模拟器和数字孪生分析，展示了在减轻空气传播病原体风险方面的巨大潜力，有效提升了室内空气净化系统的性能。"}}
{"id": "2508.02734", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.02734", "abs": "https://arxiv.org/abs/2508.02734", "authors": ["Weiyu Luo", "Chenfeng Xiong"], "title": "Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model", "comment": "20 pages, 5 figures", "summary": "Location-Based Service (LBS) data provides critical insights into human\nmobility, yet its sparsity often yields incomplete trip and activity sequences,\nmaking accurate inferences about trips and activities difficult. We raise a\nresearch problem: Can we use activity sequences derived from high-quality LBS\ndata to recover incomplete activity sequences at the individual level? This\nstudy proposes a new solution, the Variable Selection Network-fused Insertion\nTransformer (VSNIT), integrating the Insertion Transformer's flexible sequence\nconstruction with the Variable Selection Network's dynamic covariate handling\ncapability, to recover missing segments in incomplete activity sequences while\npreserving existing data. The findings show that VSNIT inserts more diverse,\nrealistic activity patterns, more closely matching real-world variability, and\nrestores disrupted activity transitions more effectively aligning with the\ntarget. It also performs significantly better than the baseline model across\nall metrics. These results highlight VSNIT's superior accuracy and diversity in\nactivity sequence recovery tasks, demonstrating its potential to enhance LBS\ndata utility for mobility analysis. This approach offers a promising framework\nfor future location-based research and applications.", "AI": {"tldr": "该研究提出VSNIT模型，用于从高质量LBS数据中恢复个人层面的不完整活动序列，显著提升了恢复的准确性和多样性。", "motivation": "LBS数据虽然能提供人类出行洞察，但其稀疏性常导致出行和活动序列不完整，难以准确推断。因此，研究旨在解决如何利用高质量LBS数据恢复不完整的个人活动序列。", "method": "本研究提出了一种新的解决方案：变数选择网络融合插入转换器（Variable Selection Network-fused Insertion Transformer, VSNIT）。该模型整合了插入转换器（Insertion Transformer）的灵活序列构建能力和变数选择网络（Variable Selection Network）的动态协变量处理能力，以恢复不完整的活动序列中的缺失片段，同时保留现有数据。", "result": "研究结果表明，VSNIT能够插入更多样化、更真实的活动模式，更接近现实世界的变异性，并更有效地恢复被中断的活动转换，使其与目标对齐。此外，VSNIT在所有指标上均显著优于基线模型。", "conclusion": "这些结果突显了VSNIT在活动序列恢复任务中卓越的准确性和多样性，证明了其在增强LBS数据效用以进行出行分析方面的潜力。该方法为未来的基于位置的研究和应用提供了一个有前景的框架。"}}
{"id": "2508.02872", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02872", "abs": "https://arxiv.org/abs/2508.02872", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "title": "Highlight & Summarize: RAG without the jailbreaks", "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.", "AI": {"tldr": "本文提出并评估了一种名为“高亮与摘要”（Highlight & Summarize, H&S）的新型检索增强生成（RAG）系统设计模式，旨在通过不向生成式LLM暴露用户问题来有效防止LLM越狱和模型劫持攻击。", "motivation": "防止大型语言模型（LLM）的越狱和模型劫持是一个重要但具有挑战性的任务。现有缓解措施（如强化系统提示或使用内容分类器）由于输入和输出空间巨大，很容易被绕过，导致LLM生成不期望的内容或执行偏离预期的任务。", "method": "H&S设计模式将RAG管道拆分为两个核心组件：一个“高亮器”（highlighter），它接收用户问题并从检索到的文档中提取相关段落（“高亮内容”）；一个“摘要器”（summarizer），它接收高亮内容并将其总结为连贯的答案。这种方法确保生成式LLM永远不会直接看到用户的原始问题。", "result": "评估结果显示，当使用基于LLM的高亮器时，大多数H&S生成的响应在正确性、相关性和响应质量方面被判断为优于标准的RAG管道。", "conclusion": "H&S是一种通过设计来有效防止LLM越狱和模型劫持攻击的新型RAG系统设计模式，并且在生成响应的质量上表现出超越传统RAG的潜力。"}}
{"id": "2508.02829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02829", "abs": "https://arxiv.org/abs/2508.02829", "authors": ["Adam Colton"], "title": "Elucidating the Role of Feature Normalization in IJEPA", "comment": null, "summary": "In the standard image joint embedding predictive architecture (IJEPA),\nfeatures at the output of the teacher encoder are layer normalized (LN) before\nserving as a distillation target for the student encoder and predictor. We\npropose that this feature normalization disrupts the natural energy hierarchy\nof visual tokens, where high-energy tokens (those with larger L2 norms) encode\nsemantically important image regions. LN forces all features to have identical\nL2 norms, effectively equalizing their energies and preventing the model from\nprioritizing semantically rich regions. We find that IJEPA models trained with\nfeature LN exhibit loss maps with significant checkerboard-like artifacts. We\npropose that feature LN be replaced with a DynTanh activation as the latter\nbetter preserves token energies and allows high-energy tokens to greater\ncontribute to the prediction loss. We show that IJEPA trained with feature\nDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard\nartifacts in the loss map. Our empirical results show that our simple\nmodification improves ImageNet linear probe accuracy from 38% to 42.7% for\nViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.\nThese results suggest that preserving natural token energies is crucial for\neffective self-supervised visual representation learning.", "AI": {"tldr": "本文提出IJEPA中特征层归一化（LN）破坏了视觉token的自然能量层级，导致性能下降。通过用DynTanh激活替换LN，可以更好地保留token能量，消除伪影，并显著提升自监督学习表现。", "motivation": "标准IJEPA中，教师编码器输出的特征在作为学生编码器和预测器的蒸馏目标前会进行层归一化（LN）。作者认为这种特征归一化破坏了视觉token的自然能量层级（高能量token编码语义重要区域），导致所有特征L2范数相同，无法优先处理语义丰富的区域，并在损失图中产生棋盘状伪影。", "method": "将IJEPA中的特征层归一化（LN）替换为DynTanh激活函数。DynTanh能更好地保留token能量，允许高能量token对预测损失做出更大贡献。", "result": "使用DynTanh训练的IJEPA模型展现出更长的损失分布尾部，并消除了损失图中的棋盘状伪影。经验结果显示，ViT-Small模型在ImageNet线性探测准确率从38%提升至42.7%，在NYU Depth V2单目深度估计任务中RMSE降低了0.08。", "conclusion": "研究表明，保留视觉token的自然能量对于有效的自监督视觉表征学习至关重要。"}}
{"id": "2508.02880", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02880", "abs": "https://arxiv.org/abs/2508.02880", "authors": ["Pengwei Sun", "Wei Peng", "Lun Yu Li", "Yixin Wang", "Kilian M. Pohl"], "title": "Evaluation of 3D Counterfactual Brain MRI Generation", "comment": null, "summary": "Counterfactual generation offers a principled framework for simulating\nhypothetical changes in medical imaging, with potential applications in\nunderstanding disease mechanisms and generating physiologically plausible data.\nHowever, generating realistic structural 3D brain MRIs that respect anatomical\nand causal constraints remains challenging due to data scarcity, structural\ncomplexity, and the lack of standardized evaluation protocols. In this work, we\nconvert six generative models into 3D counterfactual approaches by\nincorporating an anatomy-guided framework based on a causal graph, in which\nregional brain volumes serve as direct conditioning inputs. Each model is\nevaluated with respect to composition, reversibility, realism, effectiveness\nand minimality on T1-weighted brain MRIs (T1w MRIs) from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI). In addition, we test the\ngeneralizability of each model with respect to T1w MRIs of the National\nConsortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). Our results\nindicate that anatomically grounded conditioning successfully modifies the\ntargeted anatomical regions; however, it exhibits limitations in preserving\nnon-targeted structures. Beyond laying the groundwork for more interpretable\nand clinically relevant generative modeling of brain MRIs, this benchmark\nhighlights the need for novel architectures that more accurately capture\nanatomical interdependencies.", "AI": {"tldr": "本文将六种生成模型转换为3D反事实方法，通过基于因果图的解剖引导框架生成逼真的大脑MRI，并对其进行基准测试，发现解剖学条件化能成功修改目标区域但难以保留非目标结构。", "motivation": "在医学影像中，反事实生成对于理解疾病机制和生成生理学上合理的数据具有潜力。然而，生成尊重解剖学和因果约束的真实3D大脑MRI具有挑战性，原因包括数据稀缺、结构复杂性以及缺乏标准化的评估协议。", "method": "将六种生成模型转换为3D反事实方法，方法是整合一个基于因果图的解剖引导框架，其中区域大脑体积作为直接条件输入。在ADNI的T1加权大脑MRI上，从构成、可逆性、真实性、有效性和最小性方面评估每个模型。此外，还在NCANDA的T1加权MRI上测试了模型的泛化能力。", "result": "解剖学上基于条件的生成成功修改了目标解剖区域；然而，它在保留非目标结构方面表现出局限性。", "conclusion": "这项工作为更具可解释性和临床相关性的大脑MRI生成建模奠定了基础，并强调了需要新的架构来更准确地捕捉解剖学上的相互依赖性。"}}
{"id": "2508.02906", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.02906", "abs": "https://arxiv.org/abs/2508.02906", "authors": ["Mehmet Karahan"], "title": "Modeling and Simulation of an Active Quarter Car Suspension with a Robust LQR Controller under Road Disturbance and Parameter Uncertainty", "comment": "16 pages, 13 figures", "summary": "Vehicle suspension is important for passengers to travel comfortably and to\nbe less exposed to effects such as vibration and shock. A good suspension\nsystem in-creases the road holding of vehicles, allows them to take turns\nsafely and reduces the risk of traffic accidents. Passive suspension system is\nthe most widely used suspension system in vehicles due to its simple structure\nand low cost. Passive suspension systems do not have an actuator and therefore\ndo not have a controller. Active suspension systems have an actuator and a\ncontroller. Although their structures are more complex and costly, they are\nsafer. PID controller is widely used in active suspension systems due to its\nsimple structure, reasonable cost and easy adjustment of coefficients. In this\nstudy, a more robust LQR controlled active suspension was designed than passive\nsus-pension and PID controlled active suspension. Robustness analyses were\nperformed for passive suspension, PID controlled active suspension and LQR\ncontrolled active sus-pension. Suspension travel, sprung mass acceleration and\nsprung mass motion simulations were performed for all 3 suspensions under road\ndisturbance and under simultaneous road disturbance and parameter uncertainty.\nA comparative analysis was performed by obtaining the suspension rise time,\novershoot and settling time data. It was observed that the LQR controlled\nactive suspension showed the least overshoot and had the shortest settling\ntime. In this case, it was proven that the LQR controlled active suspension\nprovided a more comfortable and safe ride compared to the other two suspension\nsystems.", "AI": {"tldr": "本研究比较了被动悬架、PID主动悬架和LQR主动悬架的性能，发现LQR主动悬架在舒适性和安全性方面表现最佳。", "motivation": "车辆悬架对于乘客舒适性、减少振动冲击以及提高车辆抓地力、转弯安全性和降低事故风险至关重要。被动悬架结构简单成本低但无控制器，主动悬架虽复杂昂贵但更安全。PID控制器因其简单和易于调整而被广泛用于主动悬架，但存在进一步提升空间，因此研究旨在设计更鲁棒的LQR控制器主动悬架。", "method": "设计了一个LQR控制的主动悬架系统，并与被动悬架和PID控制的主动悬架进行了比较。对三种悬架系统进行了鲁棒性分析。在道路扰动以及道路扰动与参数不确定性同时存在的情况下，模拟了悬架行程、簧载质量加速度和簧载质量运动。通过获取悬架的上升时间、超调量和稳定时间数据进行了比较分析。", "result": "LQR控制的主动悬架表现出最小的超调量和最短的稳定时间，优于被动悬架和PID控制的主动悬架。", "conclusion": "LQR控制的主动悬架相比被动悬架和PID控制的主动悬架，能提供更舒适、更安全的驾驶体验。"}}
{"id": "2508.02952", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02952", "abs": "https://arxiv.org/abs/2508.02952", "authors": ["Hassan Iqbal", "Kobiny Rex", "Joseph Shirley", "Carlos Baiz", "Christian Claudel"], "title": "A novel autonomous microplastics surveying robot for beach environments", "comment": "12 pages, 11 figures", "summary": "Microplastics, defined as plastic particles smaller than 5 millimeters, have\nbecome a pervasive environmental contaminant that accumulates on beaches due to\nwind patterns and tidal forcing. Detecting microplastics and mapping their\nconcentration in the wild remains one of the primary challenges in addressing\nthis environmental issue. This paper introduces a novel robotic platform that\nautomatically detects and chemically analyzes microplastics on beach surfaces.\nThis mobile manipulator system scans areas for microplastics using a camera\nmounted on the robotic arm's end effector. The system effectively segments\ncandidate microplastic particles on sand surfaces even in the presence of\norganic matter such as leaves and clams. Once a candidate microplastic particle\nis detected, the system steers a near-infrared (NIR) spectroscopic sensor onto\nthe particle using both NIR and visual feedback to chemically analyze it in\nreal-time. Through experiments in lab and beach environments, the system is\nshown to achieve an excellent positional precision in manipulation control and\nhigh microplastic classification accuracy.", "AI": {"tldr": "本文介绍了一种新型机器人平台，能够自动检测并化学分析海滩表面的微塑料。", "motivation": "微塑料是普遍存在的环境污染物，在海滩堆积，但检测和绘制其浓度仍是主要挑战。", "method": "该系统是一个移动机械臂，使用末端执行器上的摄像头扫描区域并分割候选微塑料颗粒；然后利用近红外（NIR）光谱传感器，结合NIR和视觉反馈，实时对颗粒进行化学分析。", "result": "在实验室和海滩环境中进行的实验表明，该系统在操作控制方面实现了卓越的定位精度，并在微塑料分类方面达到了高准确度。", "conclusion": "该新型机器人平台能够有效地在海滩环境中自动检测和化学分析微塑料，解决了现有环境监测的挑战。"}}
{"id": "2508.02744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02744", "abs": "https://arxiv.org/abs/2508.02744", "authors": ["Peiran Wang", "Yaoning Yu", "Ke Chen", "Xianyang Zhan", "Haohan Wang"], "title": "Large Language Model-based Data Science Agent: A Survey", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven novel\napplications across diverse domains, with LLM-based agents emerging as a\ncrucial area of exploration. This survey presents a comprehensive analysis of\nLLM-based agents designed for data science tasks, summarizing insights from\nrecent studies. From the agent perspective, we discuss the key design\nprinciples, covering agent roles, execution, knowledge, and reflection methods.\nFrom the data science perspective, we identify key processes for LLM-based\nagents, including data preprocessing, model development, evaluation,\nvisualization, etc. Our work offers two key contributions: (1) a comprehensive\nreview of recent developments in applying LLMbased agents to data science\ntasks; (2) a dual-perspective framework that connects general agent design\nprinciples with the practical workflows in data science.", "AI": {"tldr": "这篇综述全面分析了用于数据科学任务的LLM（大型语言模型）代理，从代理设计和数据科学工作流双重角度进行了探讨。", "motivation": "LLM的快速发展推动了新应用，其中LLM代理是重要的探索领域，尤其是在数据科学任务中展现出巨大潜力。", "method": "通过对近期研究的总结，本研究从代理视角讨论了关键设计原则（角色、执行、知识、反思方法），并从数据科学视角识别了LLM代理的关键流程（数据预处理、模型开发、评估、可视化等）。", "result": "成果包括：1) 对LLM代理应用于数据科学任务最新进展的全面综述；2) 一个连接通用代理设计原则与数据科学实际工作流的双视角框架。", "conclusion": "本工作提供了对LLM代理在数据科学领域应用的全面回顾，并提出了一个实用的双视角框架，有助于理解和开发LLM驱动的数据科学代理。"}}
{"id": "2508.02885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02885", "abs": "https://arxiv.org/abs/2508.02885", "authors": ["Elliot Murphy", "Rohan Venkatesh", "Edward Khokhlovich", "Andrey Vyshedskiy"], "title": "Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages", "comment": null, "summary": "In the modern language sciences, the core computational operation of syntax,\n'Merge', is defined as an operation that combines two linguistic units (e.g.,\n'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).\nThis can then be further combined with additional linguistic units based on\nthis categorial information, respecting non-associativity such that abstract\ngrouping is respected. Some linguists have embraced the view that Merge is an\nelementary, indivisible operation that emerged in a single evolutionary step.\nFrom a neurocognitive standpoint, different mental objects constructed by Merge\nmay be supported by distinct mechanisms: (1) simple command constructions\n(e.g., \"eat apples\"); (2) the merging of adjectives and nouns (\"red boat\"); and\n(3) the merging of nouns with spatial prepositions (\"laptop behind the sofa\").\nHere, we systematically investigate participants' comprehension of sentences\nwith increasing levels of syntactic complexity. Clustering analyses revealed\nbehavioral evidence for three distinct structural types, which we discuss as\npotentially emerging at different developmental stages and subject to selective\nimpairment. While a Merge-based syntax may still have emerged suddenly in\nevolutionary time, responsible for the structured symbolic turn our species\ntook, different cognitive mechanisms seem to underwrite the processing of\nvarious types of Merge-based objects.", "AI": {"tldr": "该研究通过行为学实验和聚类分析，发现语言学中的核心句法操作“合并”（Merge）产生的不同句法结构类型，可能由不同的认知机制支持，而非单一不可分割。", "motivation": "现代语言学中，“合并”操作被认为是句法的核心运算，但关于其是否是单一不可分割、在演化中一次性出现，以及不同合并结构（如动词短语、名词短语、介词短语）是否由不同神经认知机制支持，存在争议。本研究旨在系统性地探究这一问题。", "method": "研究系统地调查了参与者对不同句法复杂程度句子的理解。通过聚类分析（Clustering analyses）来揭示行为学证据中的结构类型。", "result": "聚类分析揭示了三种不同的句法结构类型，这些类型可能在不同的发展阶段出现，并可能受到选择性损伤。这表明即使“合并”作为一种抽象操作是统一的，其具体应用在认知处理上可能存在差异。", "conclusion": "尽管基于“合并”的句法能力可能在演化上突然出现，但不同类型的“合并”结构在处理过程中似乎由不同的认知机制支持。这挑战了“合并”是一个完全统一且不可分的神经认知操作的观点。"}}
{"id": "2508.02831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02831", "abs": "https://arxiv.org/abs/2508.02831", "authors": ["Mikołaj Zieliński", "Krzysztof Byrski", "Tomasz Szczepanik", "Przemysław Spurek"], "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing", "comment": null, "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)", "AI": {"tldr": "GENIE是一个混合模型，结合了NeRF的光真实感渲染质量与GS的可编辑结构化表示，实现了实时、局部感知的3D场景交互式编辑。", "motivation": "NeRF在新视角合成方面表现出色但难以编辑和物理交互；GS虽然可实时渲染和编辑，但在渲染质量上可能不如NeRF。研究旨在弥合几何编辑与神经渲染之间的鸿沟，创建一个兼具两者优点的模型。", "method": "GENIE是一个混合模型，为每个高斯体分配可训练的特征嵌入，而非球谐函数。这些嵌入用于根据查询点最近的k个高斯体来条件化NeRF网络。为提高效率，引入了Ray-Traced Gaussian Proximity Search (RT-GPS)进行快速最近高斯体搜索。同时，集成多分辨率哈希网格来初始化和更新高斯特征。", "result": "GENIE实现了实时、局部感知的编辑，高斯基元的位置或修改能立即反映在渲染输出中。它支持直观的场景操作、动态交互，并与物理模拟兼容。", "conclusion": "GENIE通过结合隐式（NeRF）和显式（GS）表示的优势，成功弥合了几何编辑和神经渲染之间的差距，提供了直观的场景操作、动态交互和物理模拟兼容性。"}}
{"id": "2508.02889", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02889", "abs": "https://arxiv.org/abs/2508.02889", "authors": ["Farzad Beizaee", "Sina Hajimiri", "Ismail Ben Ayed", "Gregory Lodygensky", "Christian Desrosiers", "Jose Dolz"], "title": "REFLECT: Rectified Flows for Efficient Brain Anomaly Correction Transport", "comment": "Accepted in Medical Image Computing and Computer Assisted\n  Intervention Society (MICCAI 2025)", "summary": "Unsupervised anomaly detection (UAD) in brain imaging is crucial for\nidentifying pathologies without the need for labeled data. However, accurately\nlocalizing anomalies remains challenging due to the intricate structure of\nbrain anatomy and the scarcity of abnormal examples. In this work, we introduce\nREFLECT, a novel framework that leverages rectified flows to establish a\ndirect, linear trajectory for correcting abnormal MR images toward a normal\ndistribution. By learning a straight, one-step correction transport map, our\nmethod efficiently corrects brain anomalies and can precisely localize\nanomalies by detecting discrepancies between anomalous input and corrected\ncounterpart. In contrast to the diffusion-based UAD models, which require\niterative stochastic sampling, rectified flows provide a direct transport map,\nenabling single-step inference. Extensive experiments on popular UAD brain\nsegmentation benchmarks demonstrate that REFLECT significantly outperforms\nstate-of-the-art unsupervised anomaly detection methods. The code is available\nat https://github.com/farzad-bz/REFLECT.", "AI": {"tldr": "REFLECT框架利用整流流实现脑部MR图像的单步直接校正，以进行无监督异常检测和精确异常定位，优于现有方法。", "motivation": "脑部图像无监督异常检测（UAD）在识别病理方面至关重要，但面临无标注数据、脑部解剖结构复杂和异常样本稀缺等挑战，导致异常定位困难。", "method": "REFLECT框架引入整流流（rectified flows），学习一个直接、线性的单步传输映射，将异常MR图像校正到正常分布。通过比较异常输入和校正后的图像之间的差异来精确定位异常。与扩散模型不同，REFLECT实现单步推理。", "result": "在流行的UAD脑分割基准测试中，REFLECT的性能显著优于最先进的无监督异常检测方法。", "conclusion": "REFLECT提供了一种基于整流流的高效、精确的单步无监督脑部异常检测和定位方法，超越了现有技术。"}}
{"id": "2508.03001", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.03001", "abs": "https://arxiv.org/abs/2508.03001", "authors": ["Boyu Yao", "Andrey Bernstein", "Yury Dvorkin"], "title": "Integrating Upstream Supply Chains into Generation Expansion Planning", "comment": "10 pages, 9 figures", "summary": "Rising electricity demand underscores the need for secure and reliable\ngeneration expansion planning that accounts for upstream supply chain\nconstraints. Traditional models often overlook limitations in materials,\nmanufacturing capacity, lead times for deployment, and field availability,\nwhich can delay availability of planned resources and thus to threaten system\nreliability. This paper introduces a multi-stage supply chain-constrained\ngeneration expansion planning (SC-GEP) model that optimizes long-term\ninvestments while capturing material availability, production limits, spatial\nand temporal constraints, and material reuse from retired assets. A\ndecomposition algorithm efficiently solves the resulting MILP. A Maryland case\nstudy shows that supply chain constraints shift technology choices, amplify\ndeployment delays caused by lead times, and prompt earlier investment in\nshorter lead-time, low-material-intensity options. In the low-demand scenario,\nsupply chain constraints raise investment costs by $1.2 billion. Under high\ndemand, persistent generation and reserve shortfalls emerge, underscoring the\nneed to integrate upstream constraints into long-term planning.", "AI": {"tldr": "该论文提出了一个考虑上游供应链限制（如材料、制造能力、交付周期和可用性）的多阶段电力系统发电扩张规划（SC-GEP）模型，并通过案例研究证明了其对技术选择、部署时间和成本的影响。", "motivation": "电力需求不断增长，但传统发电扩张规划模型常忽略上游供应链限制（材料、制造能力、交付周期、现场可用性），这可能导致计划资源延迟上线，威胁系统可靠性。", "method": "引入了一个多阶段供应链受限的发电扩张规划（SC-GEP）模型，该模型优化长期投资，并考虑材料可用性、生产限制、时空约束以及报废资产的材料再利用。使用分解算法高效求解由此产生的混合整数线性规划（MILP）问题。", "result": "马里兰州的案例研究表明，供应链约束会改变技术选择，加剧交付周期导致的部署延迟，并促使更早投资于交付周期短、材料强度低的选项。在低需求情景下，供应链约束使投资成本增加12亿美元。在高需求情景下，会出现持续的发电和备用短缺。", "conclusion": "研究强调了将上游供应链约束整合到长期发电规划中的必要性，以应对电力系统可靠性挑战和潜在的成本上升及短缺问题。"}}
{"id": "2508.02953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02953", "abs": "https://arxiv.org/abs/2508.02953", "authors": ["Adarsh Salagame", "Eric Sihite", "Alireza Ramezani"], "title": "Optimal Trajectory Planning in a Vertically Undulating Snake Locomotion using Contact-implicit Optimization", "comment": null, "summary": "Contact-rich problems, such as snake robot locomotion, offer unexplored yet\nrich opportunities for optimization-based trajectory and acyclic contact\nplanning. So far, a substantial body of control research has focused on\nemulating snake locomotion and replicating its distinctive movement patterns\nusing shape functions that either ignore the complexity of interactions or\nfocus on complex interactions with matter (e.g., burrowing movements). However,\nmodels and control frameworks that lie in between these two paradigms and are\nbased on simple, fundamental rigid body dynamics, which alleviate the\nchallenging contact and control allocation problems in snake locomotion, remain\nabsent. This work makes meaningful contributions, substantiated by simulations\nand experiments, in the following directions: 1) introducing a reduced-order\nmodel based on Moreau's stepping-forward approach from differential inclusion\nmathematics, 2) verifying model accuracy, 3) experimental validation.", "AI": {"tldr": "本文提出了一种基于Moreau步进法的新型降阶模型，用于解决蛇形机器人运动中的复杂接触问题，并通过仿真和实验验证了其准确性。", "motivation": "现有蛇形机器人运动控制研究要么忽略接触复杂性，要么专注于非常复杂的交互（如挖洞），缺乏基于简单刚体动力学、能有效处理接触和控制分配问题的中间范式模型。", "method": "引入了一种基于微分包含数学中Moreau步进法的降阶模型。", "result": "通过仿真验证了模型的准确性，并通过实验进行了验证。", "conclusion": "该工作为蛇形机器人运动中的接触问题提供了一个有意义的贡献，提出了一个准确且经过实验验证的降阶模型。"}}
{"id": "2508.02789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02789", "abs": "https://arxiv.org/abs/2508.02789", "authors": ["Newman Cheng", "Gordon Broadbent", "William Chappell"], "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science", "comment": null, "summary": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes.", "AI": {"tldr": "该研究提出了一种名为CLIO（认知循环通过原位优化）的新方法，使大型语言模型（LLMs）能够自我制定解决问题的方法，并在置信度低时调整行为，从而提高科学发现中的推理能力、透明度和可控性。", "motivation": "现有的AI开发模式（基于非推理模型的框架或抽象推理控制的推理模型）在科学发现中存在局限性。科学家不仅需要推理的准确性和透明度，还需要可控性，以最大限度地利用AI进行科学发现。", "method": "CLIO通过“认知循环和原位优化”实现对推理过程的深度和精确控制。它允许LLMs自我制定解决问题的方法，在自我置信度低时调整行为，并最终提供一个最终的信念或答案。其开放式设计使科学家能够观察不确定性水平，通过图结构理解最终信念状态的形成，并进行干预纠正。", "result": "在没有额外后期训练的情况下，结合CLIO的OpenAI GPT-4.1在“人类的最后考试”（HLE）文本生物学和医学问题上达到了22.37%的准确率。这比基础GPT-4.1模型净提高了13.82%（相对提高了161.64%），并超越了OpenAI o3在高低推理努力模式下的表现。研究还发现，内部不确定性测量的振荡是决定CLIO结果准确性的关键。", "conclusion": "CLIO的开放设计和内部机制能够为科学决策过程提供洞察和控制，证明了其在提高AI辅助科学发现能力方面的潜力。"}}
{"id": "2508.02886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02886", "abs": "https://arxiv.org/abs/2508.02886", "authors": ["Wenjie Luo", "Ruocheng Li", "Shanshan Zhu", "Julian Perry"], "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models", "comment": null, "summary": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning.", "AI": {"tldr": "提出Coherent Multimodal Reasoning Framework (CMRF)，通过迭代式自评估推理机制，显著提升大型视觉语言模型(LVLMs)在复杂多模态常识推理任务中的表现，模拟人类解决问题的方式。", "motivation": "当前的大型语言模型(LLMs)和视觉语言模型(LVLMs)在处理复杂、多步骤、跨模态的常识推理任务时表现不佳，缺乏“审慎思考”能力，倾向于依赖表面关联而非深层链式推理，尤其在整合视觉信息与抽象概念时。", "method": "提出Coherent Multimodal Reasoning Framework (CMRF)，模仿人类解决问题，通过迭代、自评估的推理机制增强LVLMs的常识推理能力。框架包含三个核心模块：推理分解单元(RDU)分解问题，上下文推理引擎(CIE)进行上下文推理，连贯性评估模块(CAM)评估逻辑一致性。结合自适应迭代优化策略，系统地改进推理路径。基于LLaVA-1.6-34B构建，并在新型多模态日常活动推理(MDAR)数据集上进行训练。", "result": "CMRF在VCR、A-OKVQA和DailyLife-MRC等挑战性基准测试中，在开源LVLMs中取得了最先进的性能，平均准确率达到69.4%，比最佳开源基线高出2.4个百分点，在复杂推理场景中表现尤为突出。广泛的消融研究和人工评估证实了每个模块的关键贡献以及迭代优化的有效性。", "conclusion": "CMRF通过其迭代自评估推理框架，显著提升了LVLMs在复杂多模态常识推理任务上的性能，证明了这种模仿人类思维的策略在解决复杂跨模态问题上的有效性。"}}
{"id": "2508.02844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02844", "abs": "https://arxiv.org/abs/2508.02844", "authors": ["Anghong Du", "Nay Aung", "Theodoros N. Arvanitis", "Stefan K. Piechnik", "Joao A C Lima", "Steffen E. Petersen", "Le Zhang"], "title": "RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation", "comment": null, "summary": "High-quality pixel-level annotations of medical images are essential for\nsupervised segmentation tasks, but obtaining such annotations is costly and\nrequires medical expertise. To address this challenge, we propose a novel\ncoarse-to-fine segmentation framework that relies entirely on coarse-level\nannotations, encompassing both target and complementary drawings, despite their\ninherent noise. The framework works by introducing transition matrices in order\nto model the inaccurate and incomplete regions in the coarse annotations. By\njointly training on multiple sets of coarse annotations, it progressively\nrefines the network's outputs and infers the true segmentation distribution,\nachieving a robust approximation of precise labels through matrix-based\nmodeling. To validate the flexibility and effectiveness of the proposed method,\nwe demonstrate the results on two public cardiac imaging datasets, ACDC and\nMSCMRseg, and further evaluate its performance on the UK Biobank dataset.\nExperimental results indicate that our approach surpasses the state-of-the-art\nweakly supervised methods and closely matches the fully supervised approach.", "AI": {"tldr": "该论文提出了一种新的粗到精分割框架，仅使用粗粒度标注（包括目标和补充绘图），通过引入转移矩阵来建模不准确和不完整的区域，并联合训练多组粗标注，以实现对精确标签的鲁棒近似。", "motivation": "医学图像的像素级标注成本高昂且需要医学专业知识，这限制了监督分割任务的应用。", "method": "提出了一种粗到精的分割框架，该框架完全依赖粗粒度标注。通过引入转移矩阵来建模粗标注中不准确和不完整的区域。通过联合训练多组粗标注，逐步优化网络输出并推断真实的分割分布，通过基于矩阵的建模实现精确标签的鲁棒近似。", "result": "在ACDC、MSCMRseg和UK Biobank三个心脏影像数据集上的实验结果表明，该方法超越了现有的弱监督方法，并接近全监督方法的性能。", "conclusion": "该方法证明了其灵活性和有效性，能够仅利用粗粒度标注，通过矩阵建模实现对精确标签的鲁棒近似，解决了医学图像分割中高成本标注的挑战。"}}
{"id": "2508.02957", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02957", "abs": "https://arxiv.org/abs/2508.02957", "authors": ["Puzhen Wu", "Mingquan Lin", "Qingyu Chen", "Emily Y. Chew", "Zhiyong Lu", "Yifan Peng", "Hexin Dong"], "title": "AMD-Mamba: A Phenotype-Aware Multi-Modal Framework for Robust AMD Prognosis", "comment": "Accepted at the MICCAI 2025 MIML Workshop", "summary": "Age-related macular degeneration (AMD) is a leading cause of irreversible\nvision loss, making effective prognosis crucial for timely intervention. In\nthis work, we propose AMD-Mamba, a novel multi-modal framework for AMD\nprognosis, and further develop a new AMD biomarker. This framework integrates\ncolor fundus images with genetic variants and socio-demographic variables. At\nits core, AMD-Mamba introduces an innovative metric learning strategy that\nleverages AMD severity scale score as prior knowledge. This strategy allows the\nmodel to learn richer feature representations by aligning learned features with\nclinical phenotypes, thereby improving the capability of conventional prognosis\nmethods in capturing disease progression patterns. In addition, unlike existing\nmodels that use traditional CNN backbones and focus primarily on local\ninformation, such as the presence of drusen, AMD-Mamba applies Vision Mamba and\nsimultaneously fuses local and long-range global information, such as vascular\nchanges. Furthermore, we enhance prediction performance through multi-scale\nfusion, combining image information with clinical variables at different\nresolutions. We evaluate AMD-Mamba on the AREDS dataset, which includes 45,818\ncolor fundus photographs, 52 genetic variants, and 3 socio-demographic\nvariables from 2,741 subjects. Our experimental results demonstrate that our\nproposed biomarker is one of the most significant biomarkers for the\nprogression of AMD. Notably, combining this biomarker with other existing\nvariables yields promising improvements in detecting high-risk AMD patients at\nearly stages. These findings highlight the potential of our multi-modal\nframework to facilitate more precise and proactive management of AMD.", "AI": {"tldr": "该研究提出了AMD-Mamba，一个新颖的多模态框架，用于年龄相关性黄斑变性（AMD）的预后，并开发了一种新的AMD生物标志物。该框架整合了眼底图像、遗传变异和社会人口学变量，利用度量学习和Vision Mamba模型，显著提高了早期高风险AMD患者的检测能力。", "motivation": "年龄相关性黄斑变性（AMD）是导致不可逆视力丧失的主要原因，因此有效的预后对于及时干预至关重要。现有模型可能未能充分捕捉疾病进展模式，或仅关注局部信息。", "method": "本研究提出了AMD-Mamba多模态框架，整合了彩色眼底图像、基因变异和社会人口学变量。核心方法包括：1) 引入创新的度量学习策略，利用AMD严重程度评分作为先验知识，使学习到的特征与临床表型对齐；2) 应用Vision Mamba模型，同时融合局部和长程全局信息，区别于传统CNN只关注局部信息；3) 通过多尺度融合，结合不同分辨率的图像信息和临床变量。该模型在AREDS数据集上进行了评估。", "result": "实验结果表明，所提出的生物标志物是AMD进展最重要的生物标志物之一。将此生物标志物与其他现有变量结合，在早期检测高风险AMD患者方面取得了显著改善。", "conclusion": "该多模态框架及其新生物标志物具有潜力，能够促进更精确和主动的AMD管理。"}}
{"id": "2508.03119", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.03119", "abs": "https://arxiv.org/abs/2508.03119", "authors": ["Zhenyao Li", "Yifan Yao", "Deqiang Gan"], "title": "Power System Voltage Stability Boundary: Computational Results and Applications", "comment": null, "summary": "The objective of this paper is to report some computational results for the\ntheory of DAE stability boundary, with the aim of advancing applications in\npower system voltage stability studies. Firstly, a new regularization\ntransformation for standard differential-algebraic equations (DAEs) is\nproposed. Then the existence of anchor points on voltage stability boundary is\nexamined, and an optimization method for computing the controlling\npseudo-saddle is suggested. Subsequently, a local representation of the stable\nmanifold of the pseudo-saddle on the stability boundary is presented, and a\nvoltage stability margin expression is obtained. Finally, the proposed results\nare verified using several examples, demonstrating the accuracy and\neffectiveness of the suggested methods.", "AI": {"tldr": "本文报告了DAE稳定性边界的计算结果，旨在推进电力系统电压稳定性研究，提出新的正则化变换、伪鞍点计算方法及电压稳定裕度表达式，并验证了其准确性和有效性。", "motivation": "为了推进DAE理论在电力系统电压稳定性研究中的应用，需要报告DAE稳定性边界的计算结果。", "method": ["提出了一种新的标准微分代数方程（DAE）正则化变换。", "检验了电压稳定性边界上锚点的存在性。", "提出了一种计算控制伪鞍点的优化方法。", "给出了稳定性边界上伪鞍点稳定流形的局部表示。", "推导了电压稳定裕度表达式。"], "result": ["提出了一种新的DAE正则化变换。", "考察了锚点在电压稳定性边界上的存在性。", "提出了一种计算控制伪鞍点的优化方法。", "给出了稳定性边界上伪鞍点稳定流形的局部表示。", "获得了电压稳定裕度表达式。", "所提出的结果通过多个例子验证，证明了方法的准确性和有效性。"], "conclusion": "所提出的DAE稳定性边界计算方法，包括新的正则化变换、伪鞍点计算和稳定裕度表达式，在电力系统电压稳定性研究中具有准确性和有效性。"}}
{"id": "2508.02962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02962", "abs": "https://arxiv.org/abs/2508.02962", "authors": ["Peter Burke"], "title": "Robot builds a robot's brain: AI generated drone command and control station hosted in the sky", "comment": null, "summary": "Advances in artificial intelligence (AI) including large language models\n(LLMs) and hybrid reasoning models present an opportunity to reimagine how\nautonomous robots such as drones are designed, developed, and validated. Here,\nwe demonstrate a fully AI-generated drone control system: with minimal human\ninput, an artificial intelligence (AI) model authored all the code for a\nreal-time, self-hosted drone command and control platform, which was deployed\nand demonstrated on a real drone in flight as well as a simulated virtual drone\nin the cloud. The system enables real-time mapping, flight telemetry,\nautonomous mission planning and execution, and safety protocolsall orchestrated\nthrough a web interface hosted directly on the drone itself. Not a single line\nof code was written by a human. We quantitatively benchmark system performance,\ncode complexity, and development speed against prior, human-coded\narchitectures, finding that AI-generated code can deliver functionally complete\ncommand-and-control stacks at orders-of-magnitude faster development cycles,\nthough with identifiable current limitations related to specific model context\nwindow and reasoning depth. Our analysis uncovers the practical boundaries of\nAI-driven robot control code generation at current model scales, as well as\nemergent strengths and failure modes in AI-generated robotics code. This work\nsets a precedent for the autonomous creation of robot control systems and, more\nbroadly, suggests a new paradigm for robotics engineeringone in which future\nrobots may be largely co-designed, developed, and verified by artificial\nintelligence. In this initial work, a robot built a robot's brain.", "AI": {"tldr": "研究展示了一个完全由AI生成的无人机控制系统，AI自主编写了所有代码，并在真实和模拟无人机上成功部署和演示，显著加快了开发速度。", "motivation": "人工智能（AI）的进步，特别是大型语言模型（LLMs）和混合推理模型，为重新构想自主机器人（如无人机）的设计、开发和验证提供了新机遇。", "method": "研究采用AI模型，在最少的人类输入下，自主编写了实时、自托管的无人机指挥与控制平台的所有代码。该系统在真实无人机和云端模拟无人机上进行了部署和演示，实现了实时地图、飞行遥测、自主任务规划与执行以及安全协议，并通过无人机上托管的网页界面进行协调。", "result": "定量基准测试显示，AI生成的代码能够以比人类编码架构快几个数量级的开发周期交付功能完整的指挥与控制堆栈。尽管存在模型上下文窗口和推理深度相关的局限性，但研究揭示了AI驱动的机器人控制代码生成在当前模型规模下的实际边界、新兴优势和故障模式。", "conclusion": "这项工作为机器人控制系统的自主创建开创了先例，并更广泛地提出了一种新的机器人工程范式，即未来的机器人可能在很大程度上由人工智能共同设计、开发和验证。这是一项由机器人构建机器人大脑的初步工作。"}}
{"id": "2508.02841", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02841", "abs": "https://arxiv.org/abs/2508.02841", "authors": ["Ziruo Yi", "Jinyu Liu", "Ting Xiao", "Mark V. Albert"], "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering", "comment": null, "summary": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning.", "AI": {"tldr": "本文提出了一种多智能体系统（MAS），通过专门的智能体协同工作，解决了放射科视觉问答（RVQA）中基于多模态大语言模型（MLLMs）和检索增强生成（RAG）方法存在的准确性、幻觉和跨模态错位问题，显著提高了RVQA的性能。", "motivation": "尽管基于MLLMs和RAG的方法在RVQA中取得了进展，但它们在事实准确性、幻觉和跨模态错位方面仍面临挑战。现有方法难以支持RVQA所需的复杂推理，从而限制了其在减轻放射科医生工作量方面的潜力。", "method": "引入了一个多智能体系统（MAS），该系统包含专门的智能体，分别负责上下文理解、多模态推理和答案验证。通过模型分歧过滤，构建了一个具有挑战性的RVQA数据集，用于评估系统。在该数据集上与强大的MLLM基线进行了广泛的实验。", "result": "实验结果表明，该系统在性能上优于强大的MLLM基线，并展现出卓越的有效性。案例研究进一步证明了其可靠性和可解释性。", "conclusion": "这项工作强调了多智能体方法在支持需要复杂推理的可解释和可信赖临床AI应用方面的巨大潜力。"}}
{"id": "2508.02901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02901", "abs": "https://arxiv.org/abs/2508.02901", "authors": ["Osama Khalid", "Sanvesh Srivastava", "Padmini Srinivasan"], "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations", "comment": null, "summary": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%.", "AI": {"tldr": "该研究探索了感官语言与传统风格特征（如LIWC）的关系，提出了一种新的降秩岭回归（R4）方法来提取低维特征，并引入了精简可解释模型（SLIM-LLMs），证明其在大幅减少参数的同时能匹配全尺寸语言模型的性能。", "motivation": "感官语言在表达经验和感知方面扮演着基础角色。研究旨在探索感官语言与LIWC等传统文体特征之间的关系，并寻求更高效、可解释的模型来理解和预测这种关系。", "method": "1. 采用了一种新颖的降秩岭回归（Reduced-Rank Ridge Regression, R4）方法来从LIWC特征中提取低维潜在表示。2. 引入了文体精简可解释模型（Stylometrically Lean Interpretable Models, SLIM-LLMs），用于建模这些风格维度之间的非线性关系。", "result": "1. R4方法表明，24维的LIWC低维潜在表示能有效捕捉感官语言的文体信息，其预测效果与74维的完整特征集相当。2. SLIM-LLMs模型在五种文体上进行评估，使用低秩LIWC特征时，其性能与全尺寸语言模型匹配，同时将参数量减少了高达80%。", "conclusion": "该研究证明，通过使用降秩的文体特征和精简可解释的模型（SLIM-LLMs），可以高效且准确地预测感官语言，同时显著降低模型的复杂性和参数量，达到与大型模型相当的性能。"}}
{"id": "2508.02858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02858", "abs": "https://arxiv.org/abs/2508.02858", "authors": ["Tianheng Zhu", "Yiheng Feng"], "title": "MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model", "comment": "18 pages, 9 figures", "summary": "As autonomous driving (AD) technology advances, increasing research has\nfocused on leveraging cooperative perception (CP) data collected from multiple\nAVs to enhance traffic applications. Due to the impracticality of large-scale\nreal-world AV deployments, simulation has become the primary approach in most\nstudies. While game-engine-based simulators like CARLA generate high-fidelity\nraw sensor data (e.g., LiDAR point clouds) which can be used to produce\nrealistic detection outputs, they face scalability challenges in multi-AV\nscenarios. In contrast, microscopic traffic simulators such as SUMO scale\nefficiently but lack perception modeling capabilities. To bridge this gap, we\npropose MIDAR, a LiDAR detection mimicking model that approximates realistic\nLiDAR detections using vehicle-level features readily available from\nmicroscopic traffic simulators. Specifically, MIDAR predicts true positives\n(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the\nspatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop\nLine-of-Sight (RM-LoS) graph is constructed to encode the occlusion\nrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP\narchitecture to propagate features from the ego AV and occluding vehicles to\nthe prediction target. MIDAR achieves an AUC of 0.909 in approximating the\ndetection results generated by CenterPoint, a mainstream 3D LiDAR detection\nmodel, on the nuScenes AD dataset. Two CP-based traffic applications further\nvalidate the necessity of such realistic detection modeling, particularly for\ntasks requiring accurate individual vehicle observations (e.g., position,\nspeed, lane index). As demonstrated in the applications, MIDAR can be\nseamlessly integrated into traffic simulators and trajectory datasets and will\nbe open-sourced upon publication.", "AI": {"tldr": "MIDAR是一个LiDAR检测模拟模型，它通过使用微观交通模拟器中可用的车辆级特征，来近似逼真的LiDAR检测结果，从而弥合了可扩展交通模拟器和高保真感知模拟器之间的差距。", "motivation": "自动驾驶技术中，合作感知数据对交通应用至关重要。然而，现有模拟器要么可扩展但缺乏感知建模（如SUMO），要么能生成高保真感知数据但难以扩展到多AV场景（如CARLA），导致大规模合作感知研究受限。", "method": "本文提出了MIDAR模型，它利用微观交通模拟器中易于获得的车辆级特征（空间布局、尺寸）来预测LiDAR检测结果中的真阳性（TPs）和假阴性（FNs）。具体方法包括构建精炼多跳视线（RM-LoS）图来编码车辆间的遮挡关系，并在此基础上采用GRU增强的APPNP架构来传播自我AV和遮挡车辆的特征，以预测目标检测结果。", "result": "MIDAR在nuScenes AD数据集上，以0.909的AUC近似了主流3D LiDAR检测模型CenterPoint的检测结果。通过两个基于合作感知的交通应用进一步验证了这种逼真检测建模的必要性，尤其对于需要精确个体车辆观测（如位置、速度、车道索引）的任务。", "conclusion": "MIDAR提供了一种有效的方法，将逼真的LiDAR检测建模引入到可扩展的交通模拟器和轨迹数据集中，从而弥合了模拟器之间的差距，对于大规模多AV场景下的合作感知研究具有重要意义，并将开源提供。"}}
{"id": "2508.03008", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03008", "abs": "https://arxiv.org/abs/2508.03008", "authors": ["Meng Zhou", "Farzad Khalvati"], "title": "ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal Neuroimaging Fusion", "comment": "Accepted at MICCAI MLMI 2025 Workshop", "summary": "Multimodal medical image fusion integrates complementary information from\ndifferent imaging modalities to enhance diagnostic accuracy and treatment\nplanning. While deep learning methods have advanced performance, existing\napproaches face critical limitations: Convolutional Neural Networks (CNNs)\nexcel at local feature extraction but struggle to model global context\neffectively, while Transformers achieve superior long-range modeling at the\ncost of quadratic computational complexity, limiting clinical deployment.\nRecent State Space Models (SSMs) offer a promising alternative, enabling\nefficient long-range dependency modeling in linear time through selective scan\nmechanisms. Despite these advances, the extension to 3D volumetric data and the\nclinical validation of fused images remains underexplored. In this work, we\npropose ClinicalFMamba, a novel end-to-end CNN-Mamba hybrid architecture that\nsynergistically combines local and global feature modeling for 2D and 3D\nimages. We further design a tri-plane scanning strategy for effectively\nlearning volumetric dependencies in 3D images. Comprehensive evaluations on\nthree datasets demonstrate the superior fusion performance across multiple\nquantitative metrics while achieving real-time fusion. We further validate the\nclinical utility of our approach on downstream 2D/3D brain tumor classification\ntasks, achieving superior performance over baseline methods. Our method\nestablishes a new paradigm for efficient multimodal medical image fusion\nsuitable for real-time clinical deployment.", "AI": {"tldr": "本文提出了一种名为ClinicalFMamba的新型CNN-Mamba混合架构，用于2D和3D多模态医学图像融合。该方法结合了局部和全局特征建模，并通过三平面扫描策略处理3D数据，实现了卓越的实时融合性能，并在临床验证中表现出色。", "motivation": "现有深度学习方法在多模态医学图像融合中存在局限：CNN擅长局部特征但缺乏全局上下文，Transformer能建模长程依赖但计算复杂度高，限制了临床应用。State Space Models (SSMs) 提供高效长程依赖建模，但其在3D数据上的扩展和临床验证尚待深入探索。", "method": "提出了ClinicalFMamba，一种端到端的CNN-Mamba混合架构，协同结合局部和全局特征建模，适用于2D和3D图像。为有效学习3D图像中的体积依赖性，设计了三平面扫描策略。", "result": "在三个数据集上的综合评估表明，该方法在多个定量指标上实现了卓越的融合性能，并达到了实时融合。在下游的2D/3D脑肿瘤分类任务中，该方法也优于基线方法，验证了其临床实用性。", "conclusion": "本文提出的方法为高效多模态医学图像融合建立了新范式，适用于实时临床部署。"}}
{"id": "2508.03135", "categories": ["eess.SY", "cs.SY", "math.OC", "math.PR", "65C30, 34K28, 60J60, 60J65, 93E11, 62F15, 65L50, 65K10, 60G15, 60H35"], "pdf": "https://arxiv.org/pdf/2508.03135", "abs": "https://arxiv.org/abs/2508.03135", "authors": ["Igor G. Vladimirov"], "title": "Filtering and 1/3 Power Law for Optimal Time Discretisation in Numerical Integration of Stochastic Differential Equations", "comment": "6 pages, submitted to IFAC World Congress 2026", "summary": "This paper is concerned with the numerical integration of stochastic\ndifferential equations (SDEs) which govern diffusion processes driven by a\nstandard Wiener process. With the latter being replaced by a sequence of\nincrements at discrete moments of time, we revisit a filtering point of view on\nthe approximate strong solution of the SDE as an estimate of the hidden system\nstate whose conditional probability distribution is updated using a Bayesian\napproach and Brownian bridges over the intermediate time intervals. For a class\nof multivariable linear SDEs, where the numerical solution is organised as a\nKalman filter, we investigate the fine-grid asymptotic behaviour of terminal\nand integral mean-square error functionals when the time discretisation is\nspecified by a sufficiently smooth monotonic transformation of a uniform grid.\nThis leads to constrained optimisation problems over the time discretisation\nprofile, and their solutions reveal a 1/3 power law for the asymptotically\noptimal grid density functions. As a one-dimensional example, the results are\nillustrated for the Ornstein-Uhlenbeck process.", "AI": {"tldr": "本文研究了随机微分方程（SDEs）的数值积分，通过卡尔曼滤波和贝叶斯方法优化时间离散化，发现最优网格密度函数遵循1/3幂律。", "motivation": "旨在改进随机微分方程（SDEs）强解的数值积分方法，特别是通过优化时间离散化来减少均方误差，以更有效地估计隐藏系统状态。", "method": "将维纳过程替换为离散时间增量，采用滤波（贝叶斯）视角将SDE的近似强解视为隐藏系统状态的估计，并利用布朗桥处理中间时间间隔。对于多变量线性SDEs，数值解被组织成卡尔曼滤波器。通过对均匀网格进行平滑单调变换来定义时间离散化，研究了在精细网格下终端和积分均方误差泛函的渐近行为，并构建了时间离散化剖面的约束优化问题。", "result": "优化问题的解揭示了渐近最优网格密度函数遵循1/3幂律。以Ornstein-Uhlenbeck过程为例，验证了该结果。", "conclusion": "通过卡尔曼滤波方法对线性随机微分方程进行数值积分时，最优的时间离散化网格密度函数遵循1/3幂律，这为提高SDEs数值解的精度提供了理论指导。"}}
{"id": "2508.02976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02976", "abs": "https://arxiv.org/abs/2508.02976", "authors": ["Hanwen Ren", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Neural Time Fields for Prehensile Object Manipulation", "comment": null, "summary": "Object manipulation skills are necessary for robots operating in various\ndaily-life scenarios, ranging from warehouses to hospitals. They allow the\nrobots to manipulate the given object to their desired arrangement in the\ncluttered environment. The existing approaches to solving object manipulations\nare either inefficient sampling based techniques, require expert\ndemonstrations, or learn by trial and error, making them less ideal for\npractical scenarios. In this paper, we propose a novel, multimodal\nphysics-informed neural network (PINN) for solving object manipulation tasks.\nOur approach efficiently learns to solve the Eikonal equation without expert\ndata and finds object manipulation trajectories fast in complex, cluttered\nenvironments. Our method is multimodal as it also reactively replans the\nrobot's grasps during manipulation to achieve the desired object poses. We\ndemonstrate our approach in both simulation and real-world scenarios and\ncompare it against state-of-the-art baseline methods. The results indicate that\nour approach is effective across various objects, has efficient training\ncompared to previous learning-based methods, and demonstrates high performance\nin planning time, trajectory length, and success rates. Our demonstration\nvideos can be found at https://youtu.be/FaQLkTV9knI.", "AI": {"tldr": "本文提出了一种新颖的多模态物理信息神经网络（PINN），用于解决机器人物体操作任务，无需专家数据，能高效学习和快速规划，并在复杂环境中实现高成功率。", "motivation": "现有的机器人物体操作方法存在效率低下（基于采样）、需要专家演示或通过试错学习等问题，不适用于实际场景中的复杂、杂乱环境。", "method": "提出了一种新颖的多模态物理信息神经网络（PINN），用于求解Eikonal方程。该方法无需专家数据即可学习，能快速找到物体操作轨迹，并在操作过程中响应式地重新规划机器人的抓取姿态，以达到期望的物体位姿。", "result": "在模拟和真实世界场景中进行了验证，并与现有SOTA方法进行比较。结果表明，该方法对各种物体均有效，与之前的学习方法相比训练更高效，并在规划时间、轨迹长度和成功率方面表现出卓越性能。", "conclusion": "所提出的多模态物理信息神经网络（PINN）是一种有效且高效的机器人物体操作方法，能够在复杂、杂乱的环境中实现高性能，且无需专家数据。"}}
{"id": "2508.02900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02900", "abs": "https://arxiv.org/abs/2508.02900", "authors": ["Michael Katz", "Harsha Kokel", "Sarath Sreedharan"], "title": "Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game", "comment": null, "summary": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches.", "AI": {"tldr": "该论文提出了一个基于“倒计时”游戏的新型规划基准，用于评估基础模型和智能体的长期规划能力，并证明其对现有LLM方法极具挑战性。", "motivation": "当前基础模型和智能体的主要限制是无法形成长期规划，而现有规划基准不足以真正衡量其规划能力。大多数现有基准要么难以形式化和验证，要么是为测试现有自动化规划器弱点而设计。", "method": "提出了一种基于“倒计时”（Countdown）游戏的规划基准创建程序。该游戏要求玩家通过算术运算从给定数字列表中形成目标数字。该方法被证明具有直观的自然语言描述、计算挑战性（NP完全）和丰富的实例空间（避免记忆化）。论文进行了广泛的理论分析，确立了计算复杂性，并演示了其实例生成程序相对于公共基准的优势。", "result": "理论分析证实了“倒计时”问题的计算复杂性（NP完全性）。实验结果表明，与24点游戏（倒计时特例）等其他领域不同，所提出的动态基准对现有基于LLM的规划方法仍然极具挑战性。", "conclusion": "基于“倒计时”游戏的新型规划基准能够有效且严苛地评估基础模型和智能体的长期规划能力，揭示了现有LLM辅助规划方法在复杂规划任务上的显著局限性。"}}
{"id": "2508.02931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02931", "abs": "https://arxiv.org/abs/2508.02931", "authors": ["Shengqi Li", "Amarnath Gupta"], "title": "Can LLMs Generate High-Quality Task-Specific Conversations?", "comment": null, "summary": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.", "AI": {"tldr": "本文提出一个参数化框架，通过九个参数控制大型语言模型（LLM）的对话质量，并实验证明其有效性。", "motivation": "现有对话生成面临主题连贯性、知识进展、角色一致性和控制粒度等挑战，需要一种精确控制对话属性的方法。", "method": "引入一个参数化框架，探索了六个维度中的九个关键参数，以实现对话属性的精确指定。通过对最先进的LLM进行实验来验证参数化控制的效果。", "result": "参数化控制能够对生成的对话属性产生统计学上显著的差异，有效解决了对话生成中的多项挑战。", "conclusion": "该框架为对话质量控制提供了一种标准化方法，在教育、治疗、客户服务和娱乐等领域具有广泛应用前景。未来工作将侧重于通过架构修改实现更多参数和开发基准数据集。"}}
{"id": "2508.02871", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02871", "abs": "https://arxiv.org/abs/2508.02871", "authors": ["J. Alex Hurt", "Trevor M. Bajkowski", "Grant J. Scott", "Curt H. Davis"], "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets", "comment": null, "summary": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as\nthe state-of-the-art in CV, as these networks soon led in visual tasks for many\ndomains, including remote sensing. With the publication of Visual Transformers,\nwe are witnessing the second modern leap in computational vision, and as such,\nit is imperative to understand how various transformer-based neural networks\nperform on satellite imagery. While transformers have shown high levels of\nperformance in natural language processing and CV applications, they have yet\nto be compared on a large scale to modern remote sensing data. In this paper,\nwe explore the use of transformer-based neural networks for object detection in\nhigh-resolution electro-optical satellite imagery, demonstrating\nstate-of-the-art performance on a variety of publicly available benchmark data\nsets. We compare eleven distinct bounding-box detection and localization\nalgorithms in this study, of which seven were published since 2020, and all\neleven since 2015. The performance of five transformer-based architectures is\ncompared with six convolutional networks on three state-of-the-art opensource\nhigh-resolution remote sensing imagery datasets ranging in size and complexity.\nFollowing the training and evaluation of thirty-three deep neural models, we\nthen discuss and analyze model performance across various feature extraction\nmethodologies and detection algorithms.", "AI": {"tldr": "本文比较了Transformer和卷积神经网络在遥感图像目标检测中的性能，发现在高分辨率卫星图像上，Transformer模型展现了最先进的性能。", "motivation": "深度卷积神经网络在计算机视觉领域取得了显著进展，但随着视觉Transformer的出现，需要大规模评估其在遥感数据上的性能，以了解其在卫星图像目标检测任务中的表现。", "method": "研究探索了基于Transformer的神经网络在高分辨率电光卫星图像中进行目标检测的应用。比较了11种不同的边界框检测和定位算法（其中7种发表于2020年之后，全部发表于2015年之后），包括5种基于Transformer的架构和6种卷积网络。这些模型在3个最先进的开源高分辨率遥感图像数据集上进行了训练和评估，共计33个深度神经网络模型。", "result": "研究结果表明，基于Transformer的架构在多种公开基准数据集上，针对高分辨率电光卫星图像的目标检测任务，展现了最先进的性能。", "conclusion": "基于Transformer的神经网络在高分辨率遥感图像目标检测方面表现出色，达到了最先进的水平，这表明它们是该领域极具潜力的选择。"}}
{"id": "2508.03057", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03057", "abs": "https://arxiv.org/abs/2508.03057", "authors": ["Tongxu Zhang", "Zhiming Liang", "Bei Wang"], "title": "A Survey of Medical Point Cloud Shape Learning: Registration, Reconstruction and Variation", "comment": null, "summary": "Point clouds have become an increasingly important representation for 3D\nmedical imaging, offering a compact, surface-preserving alternative to\ntraditional voxel or mesh-based approaches. Recent advances in deep learning\nhave enabled rapid progress in extracting, modeling, and analyzing anatomical\nshapes directly from point cloud data. This paper provides a comprehensive and\nsystematic survey of learning-based shape analysis for medical point clouds,\nfocusing on three fundamental tasks: registration, reconstruction, and\nvariation modeling. We review recent literature from 2021 to 2025, summarize\nrepresentative methods, datasets, and evaluation metrics, and highlight\nclinical applications and unique challenges in the medical domain. Key trends\ninclude the integration of hybrid representations, large-scale self-supervised\nmodels, and generative techniques. We also discuss current limitations, such as\ndata scarcity, inter-patient variability, and the need for interpretable and\nrobust solutions for clinical deployment. Finally, future directions are\noutlined for advancing point cloud-based shape learning in medical imaging.", "AI": {"tldr": "本文综述了2021年至2025年间基于深度学习的医学点云形状分析研究，重点关注配准、重建和变异建模，并讨论了挑战、趋势及未来方向。", "motivation": "点云作为一种紧凑且保留表面的3D医学图像表示方式日益重要，深度学习的进展推动了其在解剖形状分析中的应用。因此，需要对医学点云的形状分析进行全面系统的综述。", "method": "本文采用系统文献综述的方法，审查了2021年至2025年间的相关文献，总结了医学点云形状分析中配准、重建和变异建模这三个基本任务的代表性方法、数据集、评估指标、临床应用及独特挑战。", "result": "综述总结了该领域的方法、数据集和评估指标，并强调了临床应用和特有挑战（如数据稀缺、患者间变异性、对可解释和鲁棒解决方案的需求）。识别出关键趋势包括混合表示集成、大规模自监督模型和生成技术。", "conclusion": "文章讨论了当前局限性，并为推进医学影像中基于点云的形状学习指明了未来的发展方向。"}}
{"id": "2508.03154", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.03154", "abs": "https://arxiv.org/abs/2508.03154", "authors": ["Bhargavi Chaudhary", "Krishanu Nath", "Subashish Datta", "Indra Narayan Kar"], "title": "An Event-based State Estimation Approach for Positive Systems with Positive Observers", "comment": "16 pages, 7 figures and 3 tables", "summary": "This article addresses the problem of state observer design for\ncontinuous-time linear positive networked systems. Considering the bandwidth\nconstraint in the communication network, an event-measurement-based positive\nobserver design is proposed. The physical interpretation of a positive observer\ndiffers from that of a general observer. Its primary goal is to ensure that all\nstate estimates remain non-negative at all times. Using output measurements, a\nlaw with weighted sampling error is used to determine the sampling sequence\nbetween the system and the observer. The observer dynamics are designed using\nthe standard Luenberger structure with the event-based sampled output\ninformation, which is updated only when an event occurs. Assuming observability\nand sufficient conditions for the positivity of the system, the asymptotic\nstability of the observer dynamics with sampled information is established.\nSufficient conditions of stability and positivity are derived using linear\nmatrix inequalities. Moreover, the design ensures that the event-based\narchitecture is free from Zeno behavior, ensuring a positive minimum bound on\nthe inter-execution time. In addition, numerical simulations on a three-tank\nsystem having variable cross-sections are used to demonstrate the efficacy of\nthe proposed event-based positive observer.", "AI": {"tldr": "本文提出了一种针对连续时间线性正网络系统的事件测量型正观测器设计，旨在带宽限制下确保状态估计的非负性和渐近稳定性，并避免Zeno行为。", "motivation": "在网络化系统中，通信带宽受限是一个常见问题。同时，对于某些物理系统，其状态估计（如浓度、液位等）必须始终保持非负性，这使得传统观测器不适用，需要设计专门的正观测器。", "method": "采用基于事件测量的方法，通过加权采样误差确定系统与观测器之间的采样序列。观测器动力学基于标准Luenberger结构，仅在事件发生时更新。利用线性矩阵不等式（LMI）推导了稳定性和正性条件，并确保了事件触发机制无Zeno行为。", "result": "成功建立了带有采样信息的观测器动力学的渐近稳定性。推导了保证稳定性和正性的充分条件。设计确保了事件触发架构无Zeno行为，保证了执行间隔时间的最小正界。通过三罐系统仿真验证了所提出方法的有效性。", "conclusion": "所提出的事件测量型正观测器设计，在满足网络带宽限制的同时，确保了状态估计的非负性和观测器的渐近稳定性，并有效避免了Zeno现象，为线性正网络系统的状态估计提供了有效解决方案。"}}
{"id": "2508.02982", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02982", "abs": "https://arxiv.org/abs/2508.02982", "authors": ["Lucas Chen", "Guna Avula", "Hanwen Ren", "Zixing Wang", "Ahmed H. Qureshi"], "title": "Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects", "comment": null, "summary": "Human-robot object handover is a crucial element for assistive robots that\naim to help people in their daily lives, including elderly care, hospitals, and\nfactory floors. The existing approaches to solving these tasks rely on\npre-selected target objects and do not contextualize human implicit and\nexplicit preferences for handover, limiting natural and smooth interaction\nbetween humans and robots. These preferences can be related to the target\nobject selection from the cluttered environment and to the way the robot should\ngrasp the selected object to facilitate desirable human grasping during\nhandovers. Therefore, this paper presents a unified approach that selects\ntarget distant objects using human verbal and non-verbal commands and performs\nthe handover operation by contextualizing human implicit and explicit\npreferences to generate robot grasps and compliant handover motion sequences.\nWe evaluate our integrated framework and its components through real-world\nexperiments and user studies with arbitrary daily-life objects. The results of\nthese evaluations demonstrate the effectiveness of our proposed pipeline in\nhandling object handover tasks by understanding human preferences. Our\ndemonstration videos can be found at https://youtu.be/6z27B2INl-s.", "AI": {"tldr": "本文提出了一种统一的人机协作物体递交方法，该方法能根据人类的言语和非言语指令选择目标物体，并结合人类的隐性和显性偏好生成机器人抓取和顺从的递交动作序列，以实现更自然流畅的交互。", "motivation": "现有的人机物体递交方法依赖于预先选择的目标物体，且未能充分考虑人类在递交过程中的隐性和显性偏好（如目标物体选择和机器人抓取方式），这限制了人机交互的自然性和流畅性。这些偏好对于从杂乱环境中选择目标物体以及机器人如何抓取物体以方便人类后续抓取至关重要。", "method": "本文提出了一种统一的方法。该方法通过人类的言语和非言语指令选择远距离的目标物体，并通过情境化人类的隐性和显性偏好来生成机器人的抓取方式和顺从的递交运动序列。该集成框架及其组件通过真实世界实验和用户研究进行了评估。", "result": "评估结果表明，所提出的集成框架及其组件在处理物体递交任务方面是有效的，并且能够理解人类的偏好。", "conclusion": "该研究提出的管道能够通过理解人类偏好来有效处理人机物体递交任务，从而促进更自然和流畅的人机交互。"}}
{"id": "2508.02913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02913", "abs": "https://arxiv.org/abs/2508.02913", "authors": ["Carolina Minami Oguchi", "Leo Wei", "Koyo Kobayashi", "Hsin-Tai Wu", "Dipak Ghosal"], "title": "Enhancing Japanese Large Language Models with Reasoning Vectors", "comment": null, "summary": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages.", "AI": {"tldr": "针对资源受限的日语大型语言模型，提出一种从推理LLM中提取“推理向量”并应用于日语LLM的后训练方法，以提升其性能和推理能力。", "motivation": "主流LLM的后训练方法能提升性能和推理能力，但由于资源限制，对日语LLM而言难以实现。", "method": "受任务向量（提取训练前后权重变化）启发，从推理LLM中获取“推理向量”，并将其应用于日语LLM。", "result": "该方法为提升日语LLM性能提供了一种简单有效的方式，取得了显著的改进。", "conclusion": "该方法成功提升了日语LLM的性能，并有望为其他面临类似资源挑战的语言提供启发。"}}
{"id": "2508.02997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02997", "abs": "https://arxiv.org/abs/2508.02997", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.", "AI": {"tldr": "本文提出一种利用上下文共现矩阵和张量潜在空间特性检测大语言模型（LLM）越狱攻击的新方法，在数据稀缺环境下显著提升了检测性能和速度。", "motivation": "大语言模型（LLMs）的复杂性和不透明性使其容易受到越狱攻击，从而产生有害响应。为了确保LLMs的安全可靠使用，迫切需要开发强大的检测方法来识别这些恶意提示。", "method": "本研究利用在数据稀缺环境中表现出色的上下文共现矩阵（Contextual Co-occurrence Matrix）来解决越狱提示检测问题。提出了一种新颖的方法，该方法利用上下文共现矩阵和张量的潜在空间特性，以有效识别对抗性提示和越狱提示。", "result": "该方法在仅使用0.5%的标注提示时，F1分数达到0.83，比基线模型提高了96.6%。此外，该方法显著加快了检测速度，与基线模型相比，速度提升了2.3到128.4倍。", "conclusion": "所提出的方法在LLM越狱攻击检测方面表现出卓越的性能，尤其是在标注数据稀缺的情况下，其学习模式的有效性得到显著体现，并且具有显著的计算效率优势，有助于LLMs的安全和可靠部署。"}}
{"id": "2508.02890", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02890", "abs": "https://arxiv.org/abs/2508.02890", "authors": ["Rongxin Jiang", "Robert Long", "Chenghao Gu", "Mingrui Yan"], "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction", "comment": null, "summary": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications.", "AI": {"tldr": "VisuCraft是一个新颖的框架，通过集成多模态结构化信息提取器和动态提示生成模块，显著提升了大型视觉-语言模型（LVLMs）在复杂视觉引导创意内容生成（如故事、诗歌）方面的视觉保真度、创造力和指令遵循能力。", "motivation": "现有LVLMs在生成长文本时，难以保持高视觉保真度、真正的创造力以及精确遵循用户指令。", "method": "VisuCraft包含两个核心模块：多模态结构化信息提取器（E），用于从输入图像中提取细粒度视觉属性并转化为结构化表示；动态提示生成模块（G），将提取的结构化信息与用户指令结合，为底层LVLMs（如LLaVA、InstructBLIP）生成高度优化的提示。", "result": "在自建的ImageStoryGen-500K数据集上，使用VisuGen指标（视觉关联度、创造力、指令遵循）评估，VisuCraft在故事生成和诗歌创作等任务上持续优于基线LVLMs，尤其在创造力和指令遵循方面表现出显著提升。", "conclusion": "VisuCraft有效生成了富有想象力、视觉关联且符合用户需求的长篇创意文本，展示了其在复杂创意AI应用中解锁LVLMs新潜力的能力。"}}
{"id": "2508.03073", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03073", "abs": "https://arxiv.org/abs/2508.03073", "authors": ["Bo Zhang", "JianFei Huo", "Zheng Zhang", "Wufan Wang", "Hui Gao", "Xiangyang Gong", "Wendong Wang"], "title": "Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution", "comment": null, "summary": "Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for\nmedical image analysis by adapting to diverse spatial resolutions. However,\ntraditional CNN-based methods are inherently ill-suited for ARSR, as they are\ntypically designed for fixed upsampling factors. While INR-based methods\novercome this limitation, they still struggle to effectively process and\nleverage multi-modal images with varying resolutions and details. In this\npaper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which\nemploys varied information and downstream tasks to achieve high-quality,\nadaptive-resolution medical image super-resolution. Specifically, Nexus-INR\ncontains three key components. A dual-branch encoder with an auxiliary\nclassification task to effectively disentangle shared anatomical structures and\nmodality-specific features; a knowledge distillation module using cross-modal\nattention that guides low-resolution modality reconstruction with\nhigh-resolution reference, enhanced by self-supervised consistency loss; an\nintegrated segmentation module that embeds anatomical semantics to improve both\nreconstruction quality and downstream segmentation performance. Experiments on\nthe BraTS2020 dataset for both super-resolution and downstream segmentation\ndemonstrate that Nexus-INR outperforms state-of-the-art methods across various\nmetrics.", "AI": {"tldr": "Nexus-INR是一个针对医学图像的任意分辨率超分辨率（ARSR）框架，通过整合多模态信息和下游任务，解决了传统方法在处理变分辨率和多模态图像时的局限性，并超越了现有技术。", "motivation": "传统基于CNN的超分辨率方法通常设计用于固定上采样因子，不适用于需要适应不同空间分辨率的任意分辨率超分辨率（ARSR）。虽然基于INR的方法克服了这一限制，但它们在有效处理和利用具有不同分辨率和细节的多模态图像方面仍存在困难。", "method": "本文提出了Nexus-INR框架，一个多样知识引导的ARSR框架，包含三个关键组件：1) 一个带有辅助分类任务的双分支编码器，用于有效解耦共享解剖结构和模态特定特征；2) 一个知识蒸馏模块，使用跨模态注意力以高分辨率参考指导低分辨率模态重建，并通过自监督一致性损失增强；3) 一个集成分割模块，嵌入解剖语义以改善重建质量和下游分割性能。", "result": "在BraTS2020数据集上进行的超分辨率和下游分割实验表明，Nexus-INR在各种指标上均优于现有最先进的方法。", "conclusion": "Nexus-INR通过其独特的三组件设计，有效解决了医学图像ARSR中多模态和变分辨率处理的挑战，不仅提升了图像重建质量，也改善了下游分割任务的表现，为医学图像分析提供了关键的灵活性。"}}
{"id": "2508.03389", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.03389", "abs": "https://arxiv.org/abs/2508.03389", "authors": ["Ognjen Stanojev", "Orcun Karaca", "Mario Schweizer"], "title": "Grid-Forming Vector Current Control FRT Modes Under Symmetrical and Asymmetrical Faults", "comment": null, "summary": "Recent research has shown that operating grid-connected converters using the\ngrid-forming vector current control (GFVCC) scheme offers significant benefits,\nincluding the simplicity and modularity of the control architecture, as well as\nenabling a seamless transition from PLL-based grid-following control to\ngrid-forming. An important aspect of any grid-connected converter control\nstrategy is the handling of grid-fault scenarios such as symmetrical and\nasymmetrical short-circuit faults. This paper presents several fault\nride-through (FRT) strategies for GFVCC that enable the converter to provide\nfault current and stay synchronized to the grid while respecting the converter\nhardware limitations and retaining grid-forming behavior. The converter control\nscheme is extended in a modular manner to include negative-sequence loops, and\nthe proposed FRT strategies address both symmetrical and asymmetrical faults.\nThe proposed FRT strategies are analyzed through case studies, including\ninfinite-bus setups and multi-unit grids.", "AI": {"tldr": "本文提出并分析了几种针对并网变换器网格形成矢量电流控制（GFVCC）方案的故障穿越（FRT）策略，以应对对称和非对称短路故障，同时保持电网同步和网格形成行为。", "motivation": "网格形成矢量电流控制（GFVCC）方案在并网变换器中展现出控制架构简单、模块化以及易于从锁相环（PLL）型网格跟随控制平滑过渡到网格形成控制等显著优势。然而，任何并网变换器控制策略的关键在于如何处理电网故障场景（如对称和非对称短路故障），同时保持其网格形成特性并尊重硬件限制。", "method": "本文提出了多种GFVCC的故障穿越（FRT）策略。这些策略通过模块化扩展控制方案，纳入负序环路，以应对对称和非对称故障。所提出的FRT策略旨在使变换器在提供故障电流的同时保持与电网同步，尊重硬件限制，并保留网格形成行为。", "result": "所提出的FRT策略使得变换器能够在故障情况下提供故障电流，保持与电网的同步，同时遵守变换器硬件限制并保持其网格形成行为。这些策略通过无限母线设置和多单元电网等案例研究进行了分析和验证。", "conclusion": "所提出的GFVCC故障穿越（FRT）策略能够有效处理对称和非对称电网故障，使变换器在故障期间保持电网同步、提供故障电流，同时满足硬件限制并维持其网格形成能力。"}}
{"id": "2508.02984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02984", "abs": "https://arxiv.org/abs/2508.02984", "authors": ["Bibek Gupta", "Mintae Kim", "Albert Park", "Eric Sihite", "Koushil Sreenath", "Alireza Ramezani"], "title": "Estimation of Aerodynamics Forces in Dynamic Morphing Wing Flight", "comment": null, "summary": "Accurate estimation of aerodynamic forces is essential for advancing the\ncontrol, modeling, and design of flapping-wing aerial robots with dynamic\nmorphing capabilities. In this paper, we investigate two distinct methodologies\nfor force estimation on Aerobat, a bio-inspired flapping-wing platform designed\nto emulate the inertial and aerodynamic behaviors observed in bat flight. Our\ngoal is to quantify aerodynamic force contributions during tethered flight, a\ncrucial step toward closed-loop flight control. The first method is a\nphysics-based observer derived from Hamiltonian mechanics that leverages the\nconcept of conjugate momentum to infer external aerodynamic forces acting on\nthe robot. This observer builds on the system's reduced-order dynamic model and\nutilizes real-time sensor data to estimate forces without requiring training\ndata. The second method employs a neural network-based regression model,\nspecifically a multi-layer perceptron (MLP), to learn a mapping from joint\nkinematics, flapping frequency, and environmental parameters to aerodynamic\nforce outputs. We evaluate both estimators using a 6-axis load cell in a\nhigh-frequency data acquisition setup that enables fine-grained force\nmeasurements during periodic wingbeats. The conjugate momentum observer and the\nregression model demonstrate strong agreement across three force components\n(Fx, Fy, Fz).", "AI": {"tldr": "本文研究了两种用于扑翼飞行机器人气动力的估算方法：基于物理的共轭动量观测器和基于神经网络的回归模型，并在系留飞行中进行了评估。", "motivation": "准确估计气动力对于具有动态变形能力的扑翼飞行机器人的控制、建模和设计至关重要，特别是为了实现闭环飞行控制。", "method": "1. 基于物理的观测器：利用哈密顿力学和共轭动量概念，从系统的降阶动力学模型和实时传感器数据推断外部气动力，无需训练数据。2. 基于神经网络的回归模型：采用多层感知器（MLP），学习关节运动学、扑翼频率和环境参数到气动力输出的映射。两种方法均通过六轴力传感器在高频数据采集设置下进行评估。", "result": "共轭动量观测器和回归模型在三个力分量（Fx、Fy、Fz）上表现出高度一致性。", "conclusion": "所提出的两种气动力估算方法（基于物理的观测器和神经网络模型）均能有效且准确地估算扑翼飞行机器人的气动力。"}}
{"id": "2508.02921", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02921", "abs": "https://arxiv.org/abs/2508.02921", "authors": ["Shane Caldwell", "Max Harley", "Michael Kouremetis", "Vincent Abruzzo", "Will Pearce"], "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements", "comment": "18 pages, 5 figures, 3 tables", "summary": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments.", "AI": {"tldr": "PentestJudge是一个基于LLM的评估系统，用于衡量渗透测试代理的操作，通过将复杂任务分解为可判定的标准，并与人类专家对比，发现其在评估AI安全代理过程质量方面的潜力。", "motivation": "现有方法难以程序化地评估渗透测试代理的复杂操作，需要一种更实用、可扩展且全面的方法来评估AI安全代理的过程质量，以便在敏感生产环境中安全使用。", "method": "引入PentestJudge系统，利用大型语言模型（LLM）作为评判者，并赋予其工具访问权限以分析代理状态和工具调用历史。开发了树状结构的评估标准（rubrics），将渗透测试任务分解为层级化的子任务，直到叶节点为简单的“是/否”判断标准。这些标准涵盖操作目标、操作安全和技术技巧。将LLM的评分与人类领域专家的评分进行比较，使用F1分数等标准二分类指标评估其相对性能。", "result": "最佳的LLM评判模型达到了0.83的F1分数。发现工具使用能力更强的模型与人类专家表现更接近。即使总体分数相似，不同模型在不同类型的问题上仍存在差异。较弱和更便宜的模型能够评估更强大和昂贵模型执行的渗透测试轨迹。", "conclusion": "验证（评估）渗透测试任务可能比生成（执行）更容易。该方法有助于未来研究，以全面和可扩展地评估基于AI的信息安全代理的过程质量，从而使其能够自信地应用于敏感生产环境。"}}
{"id": "2508.03037", "categories": ["cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03037", "abs": "https://arxiv.org/abs/2508.03037", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "该研究分析了2013年至2025年间关于AI生成艺术的英语语篇，发现艺术家对同意、透明度和创意劳动的担忧常被主流媒体叙事和技术术语边缘化。", "motivation": "生成式AI正在重塑艺术生产和人类表达方式，直接影响艺术家生计，引发了对同意、透明度和创意劳动未来的担忧。然而，在主流公共和学术话语中，艺术家的声音常被边缘化。", "method": "该研究对2013年至2025年间439篇精选的500字英语语篇（包括评论文章、新闻报道、博客、法律文件和口语转录）进行了为期十二年的分析。采用可复现的方法（基于BERTopic），识别了五个稳定的主题集群。", "result": "研究识别出五个稳定的主题集群，并揭示了艺术家感知与主流媒体叙事之间的不一致。结果还强调了技术术语如何充当一种微妙的“看门人”形式，常常将艺术家认为最紧迫的问题边缘化。", "conclusion": "该工作提供了一种基于BERTopic的方法和多模态基线，以供未来研究使用，并明确呼吁在不断发展的AI创意领域中，更深入、更透明地关注艺术家的视角。"}}
{"id": "2508.02903", "categories": ["cs.CV", "68T07", "I.4.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.02903", "abs": "https://arxiv.org/abs/2508.02903", "authors": ["Mehrdad Moradi", "Kamran Paynabar"], "title": "RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation", "comment": "10 pages, 5 figures. Accepted to the ICCV 2025 Workshop on\n  Vision-based Industrial InspectiON (VISION)", "summary": "Recent advancements in diffusion models have demonstrated significant success\nin unsupervised anomaly segmentation. For anomaly segmentation, these models\nare first trained on normal data; then, an anomalous image is noised to an\nintermediate step, and the normal image is reconstructed through backward\ndiffusion. Unlike traditional statistical methods, diffusion models do not rely\non specific assumptions about the data or target anomalies, making them\nversatile for use across different domains. However, diffusion models typically\nassume access to normal data for training, limiting their applicability in\nrealistic settings. In this paper, we propose novel robust denoising diffusion\nmodels for scenarios where only contaminated (i.e., a mix of normal and\nanomalous) unlabeled data is available. By casting maximum likelihood\nestimation of the data as a nonlinear regression problem, we reinterpret the\ndenoising diffusion probabilistic model through a regression lens. Using robust\nregression, we derive a robust version of denoising diffusion probabilistic\nmodels. Our novel framework offers flexibility in constructing various robust\ndiffusion models. Our experiments show that our approach outperforms current\nstate of the art diffusion models, for unsupervised anomaly segmentation when\nonly contaminated data is available. Our method outperforms existing\ndiffusion-based approaches, achieving up to 8.08\\% higher AUROC and 10.37\\%\nhigher AUPRC on MVTec datasets. The implementation code is available at:\nhttps://github.com/mehrdadmoradi124/RDDPM", "AI": {"tldr": "提出了一种鲁棒去噪扩散概率模型（RDDPM），用于在仅有受污染（正常和异常混合）数据的情况下进行无监督异常分割，通过将DDPM重新解释为鲁棒回归问题，并取得了显著优于现有方法的性能。", "motivation": "现有扩散模型在无监督异常分割中表现出色，但它们通常需要纯净的正常数据进行训练，这限制了其在实际应用中的普适性，因为真实场景中数据往往是受污染的。", "method": "将数据最大似然估计重新表述为非线性回归问题，从而通过回归视角重新解释去噪扩散概率模型（DDPM）。在此基础上，利用鲁棒回归技术推导出鲁棒版本的DDPM，使其能够在只有受污染数据的情况下进行训练。", "result": "在仅有受污染数据可用的情况下，该方法在无监督异常分割任务上优于当前最先进的扩散模型。在MVTec数据集上，AUROC提高了8.08%，AUPRC提高了10.37%。", "conclusion": "该研究成功开发了一种能在受污染数据上进行训练的鲁棒扩散模型，显著提升了扩散模型在更现实、数据不纯净环境中的异常分割能力。"}}
{"id": "2508.03357", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03357", "abs": "https://arxiv.org/abs/2508.03357", "authors": ["Yifei Sun", "Zhanghao Chen", "Hao Zheng", "Yuqing Lu", "Lixin Duan", "Fenglei Fan", "Ahmed Elazab", "Xiang Wan", "Changmiao Wang", "Ruiquan Ge"], "title": "GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution Bone Suppression in Chest X-Ray Images", "comment": "11 pages, 3 figures, accepted by MICCAI 2025", "summary": "Chest X-Ray (CXR) imaging for pulmonary diagnosis raises significant\nchallenges, primarily because bone structures can obscure critical details\nnecessary for accurate diagnosis. Recent advances in deep learning,\nparticularly with diffusion models, offer significant promise for effectively\nminimizing the visibility of bone structures in CXR images, thereby improving\nclarity and diagnostic accuracy. Nevertheless, existing diffusion-based methods\nfor bone suppression in CXR imaging struggle to balance the complete\nsuppression of bones with preserving local texture details. Additionally, their\nhigh computational demand and extended processing time hinder their practical\nuse in clinical settings. To address these limitations, we introduce a\nGlobal-Local Latent Consistency Model (GL-LCM) architecture. This model\ncombines lung segmentation, dual-path sampling, and global-local fusion,\nenabling fast high-resolution bone suppression in CXR images. To tackle\npotential boundary artifacts and detail blurring in local-path sampling, we\nfurther propose Local-Enhanced Guidance, which addresses these issues without\nadditional training. Comprehensive experiments on a self-collected dataset\nSZCH-X-Rays, and the public dataset JSRT, reveal that our GL-LCM delivers\nsuperior bone suppression and remarkable computational efficiency,\nsignificantly outperforming several competitive methods. Our code is available\nat https://github.com/diaoquesang/GL-LCM.", "AI": {"tldr": "本文提出了一种名为GL-LCM的全局-局部潜在一致性模型，用于在胸部X光片中快速高效地抑制骨骼结构，同时保留细节，以提高诊断准确性。", "motivation": "胸部X光片中的骨骼结构会遮挡关键诊断细节。现有基于扩散模型的骨骼抑制方法难以在彻底抑制骨骼和保留局部纹理细节之间取得平衡，且计算成本高、处理时间长，阻碍了其在临床中的实际应用。", "method": "提出了一个全局-局部潜在一致性模型（GL-LCM）架构。该模型结合了肺部分割、双路径采样和全局-局部融合，实现了胸部X光片中快速高分辨率的骨骼抑制。为解决局部路径采样中潜在的边界伪影和细节模糊问题，进一步提出了局部增强引导（Local-Enhanced Guidance），无需额外训练即可解决这些问题。", "result": "在自收集数据集SZCH-X-Rays和公共数据集JSRT上的综合实验表明，GL-LCM在骨骼抑制和计算效率方面均表现出色，显著优于其他竞争方法。", "conclusion": "GL-LCM模型成功解决了现有胸部X光片骨骼抑制方法的局限性，实现了快速、高分辨率的骨骼抑制，同时有效保留了图像细节，具有显著的临床应用潜力。"}}
{"id": "2508.03417", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.03417", "abs": "https://arxiv.org/abs/2508.03417", "authors": ["Haojie Bai", "Jiping Luo", "Huafu Li", "Xiongwei Zhao", "Yang Wang"], "title": "A Robust Cooperative Vehicle Coordination Framework for Intersection Crossing", "comment": null, "summary": "Cooperative vehicle coordination at unsignalized intersections has garnered\nsignificant interest from both academia and industry in recent years,\nhighlighting its notable advantages in improving traffic throughput and fuel\nefficiency. However, most existing studies oversimplify the coordination\nsystem, assuming accurate vehicle state information and ideal state update\nprocess. The oversights pose driving risks in the presence of state uncertainty\nand communication constraint. To address this gap, we propose a robust and\ncomprehensive intersection coordination framework consisting of a robust\ncooperative trajectory planner and a context-aware status update scheduler. The\ntrajectory planner directly controls the evolution of the trajectory\ndistributions during frequent vehicle interactions, thereby offering\nprobabilistic safety guarantees. To further align with coordination safety in\npractical bandwidth-limited conditions, we propose a context-aware status\nupdate scheduler that dynamically prioritizes the state updating order of\nvehicles based on their driving urgency. Simulation results validate the\nrobustness and effectiveness of the proposed coordination framework, showing\nthat the collision probability can be significantly reduced while maintaining\ncomparable coordination efficiency to state-of-theart strategies. Moreover, our\nproposed framework demonstrates superior effectiveness in utilizing wireless\nresources in practical uncertain and bandwidth-limited conditions.", "AI": {"tldr": "针对无信号交叉口车辆协调中存在的状态不确定性和通信限制问题，本文提出了一个鲁棒的协调框架，包含鲁棒合作轨迹规划器和上下文感知状态更新调度器，旨在提高安全性的同时优化资源利用。", "motivation": "现有的无信号交叉口车辆协调研究大多过于简化系统，假设车辆状态信息准确且更新过程理想，这在实际存在状态不确定性和通信约束时会带来驾驶风险。", "method": "本文提出了一个鲁棒且全面的交叉口协调框架，包括两个核心组件：1) 一个鲁棒的合作轨迹规划器，直接控制轨迹分布的演变，提供概率性安全保证；2) 一个上下文感知状态更新调度器，根据车辆的驾驶紧急程度动态优先排序状态更新，以适应带宽受限的实际条件。", "result": "仿真结果验证了所提协调框架的鲁棒性和有效性。与最先进的策略相比，该框架在保持可比协调效率的同时，显著降低了碰撞概率。此外，在实际不确定和带宽受限的条件下，该框架在无线资源利用方面表现出卓越的有效性。", "conclusion": "所提出的鲁棒协调框架能够有效应对无信号交叉口车辆协调中的状态不确定性和通信限制，显著提高安全性并优化无线资源利用，使其在实际应用中更具可行性。"}}
{"id": "2508.02988", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02988", "abs": "https://arxiv.org/abs/2508.02988", "authors": ["Linji Wang", "Zifan Xu", "Peter Stone", "Xuesu Xiao"], "title": "GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring", "comment": "7 pages, IROS 2025", "summary": "Curriculum learning has emerged as a promising approach for training complex\nrobotics tasks, yet current applications predominantly rely on manually\ndesigned curricula, which demand significant engineering effort and can suffer\nfrom subjective and suboptimal human design choices. While automated curriculum\nlearning has shown success in simple domains like grid worlds and games where\ntask distributions can be easily specified, robotics tasks present unique\nchallenges: they require handling complex task spaces while maintaining\nrelevance to target domain distributions that are only partially known through\nlimited samples. To this end, we propose Grounded Adaptive Curriculum Learning,\na framework specifically designed for robotics curriculum learning with three\nkey innovations: (1) a task representation that consistently handles complex\nrobot task design, (2) an active performance tracking mechanism that allows\nadaptive curriculum generation appropriate for the robot's current\ncapabilities, and (3) a grounding approach that maintains target domain\nrelevance through alternating sampling between reference and synthetic tasks.\nWe validate GACL on wheeled navigation in constrained environments and\nquadruped locomotion in challenging 3D confined spaces, achieving 6.8% and 6.1%\nhigher success rates, respectively, than state-of-the-art methods in each\ndomain.", "AI": {"tldr": "本文提出了一种名为“接地自适应课程学习”（GACL）的新框架，用于自动化机器人任务的课程学习，旨在解决手动设计课程的局限性和现有自动化方法在复杂机器人领域面临的挑战。", "motivation": "当前的课程学习方法在机器人任务中主要依赖手动设计，这耗费大量工程精力且容易导致次优结果。自动化课程学习在简单领域表现良好，但在机器人任务中面临独特挑战，如处理复杂的任务空间和在目标领域分布部分未知的情况下保持相关性。", "method": "本文提出了GACL框架，包含三项关键创新：1) 一种能够一致处理复杂机器人任务设计的任务表示方法；2) 一种主动性能跟踪机制，可以根据机器人当前能力自适应地生成课程；3) 一种接地方法，通过在参考任务和合成任务之间交替采样来保持与目标领域的相关性。", "result": "GACL在受限环境下的轮式导航和挑战性3D受限空间中的四足机器人运动任务上进行了验证，分别比现有最先进方法取得了6.8%和6.1%的成功率提升。", "conclusion": "GACL框架成功解决了机器人课程学习中的关键挑战，通过创新的任务表示、自适应课程生成和领域接地方法，显著提高了复杂机器人任务的学习成功率，优于现有最先进方法。"}}
{"id": "2508.02936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02936", "abs": "https://arxiv.org/abs/2508.02936", "authors": ["Songkun Yan", "Zhi Li", "Siyu Zhu", "Yixin Wen", "Mofan Zhang", "Mengye Chen", "Jie Cao", "Yang Hong"], "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology", "comment": "8 pages, 5 figures, 2025 ICCV SEA workshop paper", "summary": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers.", "AI": {"tldr": "AQUAH是首个端到端、基于语言的水文建模智能体，能根据自然语言指令自动完成数据检索、模型配置、模拟运行并生成自包含的PDF报告。", "motivation": "旨在简化复杂环境建模流程，降低地球观测数据、物理工具与决策者之间的使用门槛，实现水文模拟的自动化和智能化。", "method": "AQUAH利用支持视觉的大型语言模型（LLM），解释地图和栅格数据，自主执行数据检索、水文模型配置、模拟运行，并生成PDF报告。LLM还指导关键决策，如出口选择、参数初始化和不确定性评估。", "result": "在美国多个流域的初步实验表明，AQUAH无需人工干预即可完成冷启动模拟并生成分析师可用的文档。水文学家评价其结果清晰、透明且符合物理规律。", "conclusion": "早期成果突出显示了以LLM为中心、以视觉为基础的智能体在简化复杂环境建模方面的巨大潜力，并能有效连接地球观测数据、物理工具和决策者，尽管仍需进一步校准和验证以实现操作部署。"}}
{"id": "2508.03098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03098", "abs": "https://arxiv.org/abs/2508.03098", "authors": ["Haoran Wang", "Xiongxiao Xu", "Baixiang Huang", "Kai Shu"], "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.", "AI": {"tldr": "针对检索增强生成（RAG）系统中敏感数据泄露的风险，本文提出了一种名为隐私感知解码（PAD）的轻量级推理时防御机制，通过自适应地注入校准高斯噪声来提供差分隐私保证，有效减少信息泄露并保持生成质量。", "motivation": "检索增强生成（RAG）系统虽然能提升大型语言模型（LLM）的准确性，但在处理私有或敏感数据时，容易受到提取攻击，导致机密信息通过生成的响应泄露。", "method": "本文提出了隐私感知解码（PAD），这是一种在生成过程中向token logits自适应注入校准高斯噪声的推理时防御方法。PAD集成了基于置信度的筛选以选择性保护高风险token，高效的敏感性估计以最小化不必要的噪声，以及上下文感知的噪声校准以平衡隐私和生成质量。它使用Rényi差分隐私（RDP）会计师严格跟踪累积隐私损失，为敏感输出提供明确的每响应$(\\varepsilon, \\delta)$-DP保证。PAD是模型无关的，且仅在解码时操作，计算开销极小。", "result": "在三个真实世界数据集上的实验表明，PAD显著减少了私人信息泄露，同时保持了响应的实用性，并且优于现有的基于检索和后处理的防御方法。", "conclusion": "PAD通过解码策略，在缓解RAG中的隐私风险方面迈出了重要一步，为敏感领域的通用和可扩展隐私解决方案铺平了道路。"}}
{"id": "2508.02905", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02905", "abs": "https://arxiv.org/abs/2508.02905", "authors": ["Mahnoor Fatima Saad", "Ziad Al-Halah"], "title": "How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes", "comment": "Accepted to ICCV 2025. Project Page:\n  https://mahnoor-fatima-saad.github.io/m-capa.html", "summary": "How would the sound in a studio change with a carpeted floor and acoustic\ntiles on the walls? We introduce the task of material-controlled acoustic\nprofile generation, where, given an indoor scene with specific audio-visual\ncharacteristics, the goal is to generate a target acoustic profile based on a\nuser-defined material configuration at inference time. We address this task\nwith a novel encoder-decoder approach that encodes the scene's key properties\nfrom an audio-visual observation and generates the target Room Impulse Response\n(RIR) conditioned on the material specifications provided by the user. Our\nmodel enables the generation of diverse RIRs based on various material\nconfigurations defined dynamically at inference time. To support this task, we\ncreate a new benchmark, the Acoustic Wonderland Dataset, designed for\ndeveloping and evaluating material-aware RIR prediction methods under diverse\nand challenging settings. Our results demonstrate that the proposed model\neffectively encodes material information and generates high-fidelity RIRs,\noutperforming several baselines and state-of-the-art methods.", "AI": {"tldr": "该研究提出了一种材料控制的声学剖面生成任务，通过一个新颖的编码器-解码器模型，根据用户定义的材料配置生成目标房间脉冲响应（RIR），并创建了一个新的数据集来支持该任务。", "motivation": "现有方法难以动态模拟室内场景中声音随不同材料配置的变化。研究旨在解决给定室内场景的音视频特征，根据用户在推理时定义的材料配置生成目标声学剖面（RIR）的问题。", "method": "采用新颖的编码器-解码器方法：编码器从音视频观察中编码场景的关键属性，解码器根据用户提供的材料规格生成目标RIR。为支持此任务，创建了新的基准数据集——Acoustic Wonderland Dataset，用于开发和评估材料感知RIR预测方法。", "result": "所提出的模型能够有效地编码材料信息并生成高保真RIR，性能优于多个基线方法和现有最先进的方法。", "conclusion": "该模型成功实现了材料控制的声学剖面生成，能够根据动态定义的材料配置生成多样化的RIR，为声学模拟和设计提供了有效工具。"}}
{"id": "2508.03461", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03461", "abs": "https://arxiv.org/abs/2508.03461", "authors": ["Gideon N. L. Rouwendaal", "Daniël Boeke", "Inge L. Cox", "Henk G. van der Poel", "Margriet C. van Dijk-de Haan", "Regina G. H. Beets-Tan", "Thierry N. Boellaard", "Wilson Silva"], "title": "Evaluating the Predictive Value of Preoperative MRI for Erectile Dysfunction Following Radical Prostatectomy", "comment": "13 pages, 5 figures, 2 tables. Accepted at PRedictive Intelligence in\n  MEdicine workshop @ MICCAI 2025 (PRIME-MICCAI). This is the submitted\n  manuscript with added link to github repo, funding acknowledgements and\n  authors' names and affiliations. No further post submission improvements or\n  corrections were integrated. Final version not published yet", "summary": "Accurate preoperative prediction of erectile dysfunction (ED) is important\nfor counseling patients undergoing radical prostatectomy. While clinical\nfeatures are established predictors, the added value of preoperative MRI\nremains underexplored. We investigate whether MRI provides additional\npredictive value for ED at 12 months post-surgery, evaluating four modeling\nstrategies: (1) a clinical-only baseline, representing current\nstate-of-the-art; (2) classical models using handcrafted anatomical features\nderived from MRI; (3) deep learning models trained directly on MRI slices; and\n(4) multimodal fusion of imaging and clinical inputs. Imaging-based models\n(maximum AUC 0.569) slightly outperformed handcrafted anatomical approaches\n(AUC 0.554) but fell short of the clinical baseline (AUC 0.663). Fusion models\noffered marginal gains (AUC 0.586) but did not exceed clinical-only\nperformance. SHAP analysis confirmed that clinical features contributed most to\npredictive performance. Saliency maps from the best-performing imaging model\nsuggested a predominant focus on anatomically plausible regions, such as the\nprostate and neurovascular bundles. While MRI-based models did not improve\npredictive performance over clinical features, our findings suggest that they\ntry to capture patterns in relevant anatomical structures and may complement\nclinical predictors in future multimodal approaches.", "AI": {"tldr": "研究发现，术前MRI在预测根治性前列腺切除术后勃起功能障碍方面，未能超越现有临床特征的预测能力，但MRI模型确实关注了相关的解剖区域。", "motivation": "准确的术前预测勃起功能障碍（ED）对接受根治性前列腺切除术的患者咨询至关重要。尽管临床特征已是既定预测因子，但术前MRI的额外价值尚未得到充分探索。", "method": "研究评估了四种建模策略来预测术后12个月的ED：1) 仅基于临床特征的基线模型；2) 使用手工提取的MRI解剖特征的经典模型；3) 直接在MRI切片上训练的深度学习模型；4) 影像和临床输入的多模态融合模型。通过AUC、SHAP分析和显著性图评估模型性能。", "result": "影像学模型（最大AUC 0.569）略优于手工解剖特征方法（AUC 0.554），但均低于临床基线模型（AUC 0.663）。融合模型仅带来微小增益（AUC 0.586），未能超越仅临床特征的表现。SHAP分析证实临床特征对预测性能贡献最大。表现最佳的影像学模型的显著性图显示，其主要关注前列腺和神经血管束等解剖学上合理的区域。", "conclusion": "尽管基于MRI的模型未能提升对临床特征的预测性能，但研究结果表明它们试图捕捉相关解剖结构中的模式，未来可能在多模态方法中补充临床预测因子。"}}
{"id": "2508.03647", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.03647", "abs": "https://arxiv.org/abs/2508.03647", "authors": ["Hend Abououf", "Sidra Ghayour Bhatti", "Qadeer Ahmed"], "title": "Improving Q-Learning for Real-World Control: A Case Study in Series Hybrid Agricultural Tractors", "comment": null, "summary": "The variable and unpredictable load demands in hybrid agricultural tractors\nmake it difficult to design optimal rule-based energy management strategies,\nmotivating the use of adaptive, learning-based control. However, existing\napproaches often rely on basic fuel-based rewards and do not leverage expert\ndemonstrations to accelerate training. In this paper, first, the performance of\nQ-value-based reinforcement learning algorithms is evaluated for powertrain\ncontrol in a hybrid agricultural tractor. Three algorithms, Double Q-Learning\n(DQL), Deep Q-Networks (DQN), and Double DQN (DDQN), are compared in terms of\nconvergence speed and policy optimality. Second, a piecewise domain-specific\nreward-shaping strategy is introduced to improve learning efficiency and steer\nagent behavior toward engine fuel-efficient operating regions. Third, the\ndesign of the experience replay buffer is examined, with a focus on the effects\nof seeding the buffer with expert demonstrations and analyzing how different\ntypes of expert policies influence convergence dynamics and final performance.\nExperimental results demonstrate that (1) DDQN achieves 70\\% faster convergence\nthan DQN in this application domain, (2) the proposed reward shaping method\neffectively biases the learned policy toward fuel-efficient outcomes, and (3)\ninitializing the replay buffer with structured expert data leads to a 33\\%\nimprovement in convergence speed.", "AI": {"tldr": "本文评估了Q值强化学习算法在混合农业拖拉机动力系统控制中的性能，并引入了分段领域特定奖励整形策略和专家演示经验回放缓冲区初始化，以加速训练并提高燃油效率。", "motivation": "混合农业拖拉机负载需求多变且不可预测，导致难以设计最优的基于规则的能量管理策略。现有方法常依赖基本的燃油奖励，且未利用专家演示加速训练。", "method": "1. 评估了Q值强化学习算法（DQL、DQN、DDQN）在混合农业拖拉机动力系统控制中的性能，比较收敛速度和策略最优性。2. 引入分段领域特定奖励整形策略，以提高学习效率并引导智能体行为趋向发动机燃油高效运行区域。3. 检查经验回放缓冲区的设计，重点是使用专家演示初始化缓冲区，并分析不同类型专家策略对收敛动态和最终性能的影响。", "result": "1. 在此应用领域，DDQN比DQN的收敛速度快70%。2. 所提出的奖励整形方法有效促使学习策略产生燃油高效的结果。3. 使用结构化专家数据初始化回放缓冲区可使收敛速度提高33%。", "conclusion": "Q值强化学习算法（特别是DDQN）结合领域特定奖励整形和专家演示初始化经验回放缓冲区，能显著提高混合农业拖拉机动力系统能量管理策略的学习效率和燃油经济性。"}}
{"id": "2508.03003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03003", "abs": "https://arxiv.org/abs/2508.03003", "authors": ["Chenghao Wang", "Alireza Ramezani"], "title": "Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals", "comment": null, "summary": "Husky Carbon, a robot developed by Northeastern University, serves as a\nresearch platform to explore unification of posture manipulation and thrust\nvectoring. Unlike conventional quadrupeds, its joint actuators and thrusters\nenable enhanced control authority, facilitating thruster-assisted narrow-path\nwalking. While a unified Model Predictive Control (MPC) framework optimizing\nboth ground reaction forces and thruster forces could theoretically address\nthis control problem, its feasibility is limited by the low torque-control\nbandwidth of the system's lightweight actuators. To overcome this challenge, we\npropose a decoupled control architecture: a Raibert-type controller governs\nlegged locomotion using position-based control, while an MPC regulates the\nthrusters augmented by learned Contact Residual Dynamics (CRD) to account for\nleg-ground impacts. This separation bypasses the torque-control rate bottleneck\nwhile retaining the thruster MPC to explicitly account for leg-ground impact\ndynamics through learned residuals. We validate this approach through both\nsimulation and hardware experiments, showing that the decoupled control\narchitecture with CRD performs more stable behavior in terms of push recovery\nand cat-like walking gait compared to the decoupled controller without CRD.", "AI": {"tldr": "针对Husky Carbon机器人低扭矩控制带宽问题，提出一种解耦控制架构：腿部采用Raibert型控制器，推力器使用增强了学习型接触残余动力学（CRD）的MPC，实现了更稳定的步态。", "motivation": "传统统一的模型预测控制（MPC）框架，虽然理论上能解决机器人姿态操纵与推力矢量控制的统一问题，但受限于系统轻量级执行器低扭矩控制带宽，实际应用可行性受限。", "method": "提出一种解耦控制架构：腿部运动采用基于位置的Raibert型控制器；推力器通过MPC进行调节，并辅以学习到的接触残余动力学（CRD）来补偿腿地冲击。这种分离绕过了扭矩控制速率瓶颈，同时通过学习到的残余量保留了推力器MPC以明确考虑腿地冲击动力学。", "result": "通过仿真和硬件实验验证了该方法。结果表明，与不带CRD的解耦控制器相比，带CRD的解耦控制架构在推力恢复和猫步式行走步态方面表现出更稳定的行为。", "conclusion": "解耦控制架构结合学习型接触残余动力学（CRD）能有效克服轻量级执行器低扭矩控制带宽的挑战，显著提高了机器人步态的稳定性和鲁棒性。"}}
{"id": "2508.02951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02951", "abs": "https://arxiv.org/abs/2508.02951", "authors": ["Mahtab Bigverdi", "Wisdom Ikezogwo", "Kevin Zhang", "Hyewon Jeong", "Mingyu Lu", "Sungjae Cho", "Linda Shapiro", "Ranjay Krishna"], "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine", "comment": null, "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.", "AI": {"tldr": "多模态语言模型在医疗领域有潜力，但Medblink基准测试显示它们在基本的医学图像感知任务上表现不佳，远低于人类水平。", "motivation": "临床医生对AI工具的采用非常谨慎，如果模型在简单的感知任务（如图像方向、CT增强识别）上出错，将难以被采纳。因此，需要评估这些模型的基本感知能力。", "method": "引入Medblink基准测试，包含8项临床有意义的任务，涵盖多种成像模态和解剖区域，共1429个多项选择题和1605张图像。评估了19个最先进的多模态语言模型（包括通用和领域专用模型），并与人类标注员的表现进行比较。", "result": "人类标注员的准确率达到96.4%，而表现最好的模型仅达到65%。这表明当前的多模态语言模型在常规感知检查中频繁失败。", "conclusion": "当前的多模态语言模型在基本的感知任务上表现不足，需要加强其视觉基础能力，以支持在临床环境中的实际应用和采纳。"}}
{"id": "2508.03110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03110", "abs": "https://arxiv.org/abs/2508.03110", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.", "AI": {"tldr": "TPARAG是一种针对RAG系统的新型攻击框架，通过在token级别优化恶意内容，同时攻击检索和生成阶段，在白盒和黑盒场景下均有效，揭示了RAG的脆弱性。", "motivation": "大型语言模型（LLMs）存在幻觉和知识过时问题。RAG框架通过外部知识增强LLMs，但引入了新的安全漏洞：外部数据库中的恶意内容可能被检索并用于操纵模型输出。现有针对RAG的攻击方法依赖于检索器访问或未能同时考虑检索和生成阶段，在黑盒场景下效果有限。", "method": "本文提出TPARAG（Token-level Precise Attack on the RAG），一个针对白盒和黑盒RAG系统的新型攻击框架。TPARAG利用一个轻量级白盒LLM作为攻击者，在token级别生成并迭代优化恶意段落，以确保其可检索性以及在生成阶段的高攻击成功率。", "result": "在开放域问答数据集上的大量实验表明，TPARAG在检索阶段和端到端攻击效果上均持续优于现有方法。", "conclusion": "这些结果进一步揭示了RAG管道中的关键漏洞，并为提高其鲁棒性提供了新的见解。"}}
{"id": "2508.02917", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02917", "abs": "https://arxiv.org/abs/2508.02917", "authors": ["Vebjørn Haug Kåsene", "Pierre Lison"], "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces", "comment": "This paper has been accepted to ICNSLP 2025", "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.", "AI": {"tldr": "本文探讨了预训练大型视觉语言模型（LVLMs）在视觉语言导航（VLN）任务中的应用潜力，并评估了它们在低级和全景动作空间下的表现。", "motivation": "当前的VLN系统多依赖于专门设计和优化的模型，而现成的LVLMs的潜力尚未被充分探索。此外，VLN任务的动作空间已从低级原子动作转向离散可导航的全景视图，需要研究LVLMs如何支持这两种范式。", "method": "研究通过在Room-to-Room (R2R) 数据集上微调开源模型Qwen2.5-VL-3B-Instruct，并在低级和全景动作空间下评估其经验性能，过程中未进行架构修改或基于模拟器的训练。", "result": "最佳模型在R2R测试集上取得了41%的成功率，表明现成的LVLMs可以学习执行视觉语言导航任务。", "conclusion": "尽管现成的LVLMs能够学习并执行视觉语言导航，但它们的性能仍落后于专门为此任务设计的模型。"}}
{"id": "2508.03594", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03594", "abs": "https://arxiv.org/abs/2508.03594", "authors": ["Ana Lawry Aguila", "Ayodeji Ijishakin", "Juan Eugenio Iglesias", "Tomomi Takenaga", "Yukihiro Nomura", "Takeharu Yoshikawa", "Osamu Abe", "Shouhei Hanaoka"], "title": "CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models", "comment": null, "summary": "Applying machine learning to real-world medical data, e.g. from hospital\narchives, has the potential to revolutionize disease detection in brain images.\nHowever, detecting pathology in such heterogeneous cohorts is a difficult\nchallenge. Normative modeling, a form of unsupervised anomaly detection, offers\na promising approach to studying such cohorts where the ``normal'' behavior is\nmodeled and can be used at subject level to detect deviations relating to\ndisease pathology. Diffusion models have emerged as powerful tools for anomaly\ndetection due to their ability to capture complex data distributions and\ngenerate high-quality images. Their performance relies on image restoration;\ndifferences between the original and restored images highlight potential\nabnormalities. However, unlike normative models, these diffusion model\napproaches do not incorporate clinical information which provides important\ncontext to guide the disease detection process. Furthermore, standard\napproaches often poorly restore healthy regions, resulting in poor\nreconstructions and suboptimal detection performance. We present CADD, the\nfirst conditional diffusion model for normative modeling in 3D images. To guide\nthe healthy restoration process, we propose a novel inference inpainting\nstrategy which balances anomaly removal with retention of subject-specific\nfeatures. Evaluated on three challenging datasets, including clinical scans,\nwhich may have lower contrast, thicker slices, and motion artifacts, CADD\nachieves state-of-the-art performance in detecting neurological abnormalities\nin heterogeneous cohorts.", "AI": {"tldr": "CADD是首个用于3D图像规范建模的条件扩散模型，通过新颖的推理修复策略，在异构临床数据中实现了神经异常检测的SOTA性能。", "motivation": "将机器学习应用于真实世界医疗数据（如医院档案）以检测脑部疾病具有巨大潜力，但异构队列中的病理检测极具挑战。现有扩散模型在异常检测中未能整合临床信息，且对健康区域的修复不佳，导致重建质量和检测性能受限。", "method": "本文提出了CADD，首个用于3D图像规范建模的条件扩散模型。为指导健康区域的修复过程，CADD引入了一种新颖的推理修复策略，旨在平衡异常去除与保留受试者特定特征。", "result": "CADD在三个具有挑战性的数据集（包括可能存在低对比度、厚切片和运动伪影的临床扫描）上进行了评估，在异构队列中检测神经异常方面取得了最先进的性能。", "conclusion": "CADD通过结合条件扩散模型和优化的推理修复策略，有效解决了真实世界异构医学数据中神经异常检测的难题，显著提升了检测精度。"}}
{"id": "2508.03043", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03043", "abs": "https://arxiv.org/abs/2508.03043", "authors": ["Yi-Hsuan Hsiao", "Andrea Tagliabue", "Owen Matteson", "Suhan Kim", "Tong Zhao", "Jonathan P. How", "YuFeng Chen"], "title": "Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control", "comment": "27 pages, 26 supplementary pages, 6 main figures, 16 supplementary\n  figures, 1 table", "summary": "Aerial insects exhibit highly agile maneuvers such as sharp braking,\nsaccades, and body flips under disturbance. In contrast, insect-scale aerial\nrobots are limited to tracking non-aggressive trajectories with small body\nacceleration. This performance gap is contributed by a combination of low robot\ninertia, fast dynamics, uncertainty in flapping-wing aerodynamics, and high\nsusceptibility to environmental disturbance. Executing highly dynamic maneuvers\nrequires the generation of aggressive flight trajectories that push against the\nhardware limit and a high-rate feedback controller that accounts for model and\nenvironmental uncertainty. Here, through designing a deep-learned robust tube\nmodel predictive controller, we showcase insect-like flight agility and\nrobustness in a 750-millgram flapping-wing robot. Our model predictive\ncontroller can track aggressive flight trajectories under disturbance. To\nachieve a high feedback rate in a compute-constrained real-time system, we\ndesign imitation learning methods to train a two-layer, fully connected neural\nnetwork, which resembles insect flight control architecture consisting of\ncentral nervous system and motor neurons. Our robot demonstrates insect-like\nsaccade movements with lateral speed and acceleration of 197 centimeters per\nsecond and 11.7 meters per second square, representing 447$\\%$ and 255$\\%$\nimprovement over prior results. The robot can also perform saccade maneuvers\nunder 160 centimeters per second wind disturbance and large command-to-force\nmapping errors. Furthermore, it performs 10 consecutive body flips in 11\nseconds - the most challenging maneuver among sub-gram flyers. These results\nrepresent a milestone in achieving insect-scale flight agility and inspire\nfuture investigations on sensing and compute autonomy.", "AI": {"tldr": "该研究通过设计深度学习的鲁棒管模型预测控制器和模仿学习，显著提升了750毫克扑翼机器人的飞行敏捷性和鲁棒性，使其能够执行昆虫般的机动动作。", "motivation": "昆虫具有高度敏捷的飞行能力，如急刹车、眼跳和翻身，而现有昆虫大小的飞行机器人受限于低惯性、快速动力学、气动不确定性和环境干扰，只能跟踪非激进轨迹。这种性能差距促使研究人员寻求生成激进飞行轨迹和高反馈率控制器的方法，以应对硬件限制和模型/环境不确定性。", "method": "研究设计了一种深度学习的鲁棒管模型预测控制器（MPC），用于跟踪激进飞行轨迹并应对干扰。为在计算受限的实时系统中实现高反馈率，研究采用了模仿学习方法，训练了一个双层全连接神经网络，其架构类似于昆虫的中央神经系统和运动神经元。", "result": "该机器人展示了昆虫般的眼跳运动，横向速度达到197厘米/秒（比之前结果提升447%），加速度达到11.7米/秒²（比之前结果提升255%）。机器人还能在160厘米/秒的风扰和较大的指令-力映射误差下执行眼跳机动。此外，它在11秒内连续完成了10次机身翻转，这是亚克级飞行器中最具挑战性的机动。", "conclusion": "这些成果代表了在实现昆虫尺度飞行敏捷性方面的一个里程碑，并为未来在传感和计算自主性方面的研究提供了启发。"}}
{"id": "2508.03024", "categories": ["cs.RO", "I.2.9; C.3"], "pdf": "https://arxiv.org/pdf/2508.03024", "abs": "https://arxiv.org/abs/2508.03024", "authors": ["Jie Lin", "Hsun-Yu Lee", "Ho-Ming Li", "Fang-Jing Wu"], "title": "LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning", "comment": "6 pages, 10 figures", "summary": "Accurate and robust indoor localization is critical for smart building\napplications, yet existing Wi-Fi-based systems are often vulnerable to\nenvironmental conditions. This work presents a novel indoor localization\nsystem, called LiGen, that leverages the spectral intensity patterns of ambient\nlight as fingerprints, offering a more stable and infrastructure-free\nalternative to radio signals. To address the limited spectral data, we design a\ndata augmentation framework based on generative adversarial networks (GANs),\nfeaturing two variants: PointGAN, which generates fingerprints conditioned on\ncoordinates, and FreeGAN, which uses a weak localization model to label\nunconditioned samples. Our positioning model, leveraging a Multi-Layer\nPerceptron (MLP) architecture to train on synthesized data, achieves\nsubmeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\\%.\nLiGen also demonstrates strong robustness in cluttered environments. To the\nbest of our knowledge, this is the first system to combine spectral\nfingerprints with GAN-based data augmentation for indoor localization.", "AI": {"tldr": "LiGen是一种新型室内定位系统，利用环境光的频谱强度模式作为指纹，并结合生成对抗网络（GANs）进行数据增强，实现了高精度和鲁棒性，优于传统Wi-Fi系统。", "motivation": "现有的Wi-Fi室内定位系统容易受到环境条件影响，不够稳定。研究需要一种更稳定、无需基础设施的替代方案。", "method": "LiGen系统利用环境光的频谱强度模式作为指纹。为解决频谱数据有限的问题，设计了基于GANs的数据增强框架，包括PointGAN（根据坐标生成指纹）和FreeGAN（使用弱定位模型标记无条件样本）。定位模型采用多层感知机（MLP）在合成数据上训练。", "result": "LiGen系统实现了亚米级定位精度，性能优于基于Wi-Fi的基线系统50%以上。该系统在杂乱环境中也表现出强大的鲁棒性。这是首个将频谱指纹与基于GAN的数据增强结合用于室内定位的系统。", "conclusion": "LiGen系统通过利用环境光频谱指纹和GANs数据增强，提供了一种准确、鲁棒且稳定的室内定位解决方案，克服了传统Wi-Fi系统的局限性。"}}
{"id": "2508.02959", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02959", "abs": "https://arxiv.org/abs/2508.02959", "authors": ["Chia-Tung Ho", "Jing Gong", "Xufeng Yao", "Yunsheng Bai", "Abhishek B Akkur", "Haoxing Ren"], "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow", "comment": "18 pages, 12 figures, under review for AAAI2026", "summary": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines.", "AI": {"tldr": "本文提出Polymath，一个自优化智能体，利用动态分层工作流和无标签数据优化方法，显著提升了大型语言模型（LLMs）在多种任务上的表现，平均超越现有SOTA基线8.1%。", "motivation": "当前LLMs通过文本接口手动嵌入到代理系统（如Chain-of-Thought、ReACT）中，限制了可扩展性和效率。现有自动化工作流生成和优化方法依赖于标注数据集，这在缺乏标注数据的真实世界动态问题中是无效且不灵活的。", "method": "引入Polymath，一个具有动态分层工作流的自优化智能体。它利用任务流图的灵活性和代码表示工作流的表达能力。其优化方法整合了受多网格启发的图优化与自反思引导的进化算法，从而在无需标注数据的情况下优化工作流。", "result": "在编码、数学和多轮问答等六个基准数据集上的实验结果表明，Polymath比现有最先进的基线平均提高了8.1%。", "conclusion": "Polymath通过其动态分层工作流和创新的无标注数据优化方法，成功解决了构建通用LLM代理的扩展性和效率限制，并在真实世界动态问题上展现出卓越的性能提升。"}}
{"id": "2508.03112", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.03112", "abs": "https://arxiv.org/abs/2508.03112", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Cross-lingual Opinions and Emotions Mining in Comparable Documents", "comment": "16 pages, 5 figures", "summary": "Comparable texts are topic-aligned documents in multiple languages that are\nnot direct translations. They are valuable for understanding how a topic is\ndiscussed across languages. This research studies differences in sentiments and\nemotions across English-Arabic comparable documents. First, texts are annotated\nwith sentiment and emotion labels. We apply a cross-lingual method to label\ndocuments with opinion classes (subjective/objective), avoiding reliance on\nmachine translation. To annotate with emotions (anger, disgust, fear, joy,\nsadness, surprise), we manually translate the English WordNet-Affect (WNA)\nlexicon into Arabic, creating bilingual emotion lexicons used to label the\ncomparable corpora. We then apply a statistical measure to assess the agreement\nof sentiments and emotions in each source-target document pair. This comparison\nis especially relevant when the documents originate from different sources. To\nour knowledge, this aspect has not been explored in prior literature. Our study\nincludes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera\n(JSC). Results show that sentiment and emotion annotations align when articles\ncome from the same news agency and diverge when they come from different ones.\nThe proposed method is language-independent and generalizable to other language\npairs.", "AI": {"tldr": "本研究探讨了英阿可比文本中的情感和情绪差异，发现来自同一新闻机构的文章情感一致性较高，而来自不同机构的文章则存在分歧。", "motivation": "可比文本对于理解同一主题在不同语言中的讨论方式具有重要价值。此前研究尚未充分探索不同来源的可比文本在情感和情绪方面的差异，特别是跨语言情感和情绪的一致性问题。", "method": "研究首先对文本进行了情感和情绪标注。对于主观/客观意见分类，采用了不依赖机器翻译的跨语言方法。对于情绪（愤怒、厌恶、恐惧、喜悦、悲伤、惊讶）标注，手动将英语WordNet-Affect（WNA）词典翻译成阿拉伯语，创建了双语情感词典。随后，使用统计方法评估每对源-目标文档的情感和情绪一致性。研究使用了来自Euronews、BBC和Al-Jazeera（JSC）的英阿文档对。", "result": "结果显示，当文章来自同一新闻机构时，情感和情绪标注趋于一致；而当文章来自不同机构时，情感和情绪标注则出现分歧。", "conclusion": "所提出的方法是语言独立的，可推广到其他语言对。文档来源对跨语言可比文本的情感和情绪一致性具有显著影响。"}}
{"id": "2508.02923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02923", "abs": "https://arxiv.org/abs/2508.02923", "authors": ["Minh-Hai Nguyen", "Edouard Pauwels", "Pierre Weiss"], "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution", "comment": null, "summary": "The Maximum A Posteriori (MAP) estimation is a widely used framework in blind\ndeconvolution to recover sharp images from blurred observations. The estimated\nimage and blur filter are defined as the maximizer of the posterior\ndistribution. However, when paired with sparsity-promoting image priors, MAP\nestimation has been shown to favors blurry solutions, limiting its\neffectiveness. In this paper, we revisit this result using diffusion-based\npriors, a class of models that capture realistic image distributions. Through\nan empirical examination of the prior's likelihood landscape, we uncover two\nkey properties: first, blurry images tend to have higher likelihoods; second,\nthe landscape contains numerous local minimizers that correspond to natural\nimages. Building on these insights, we provide a theoretical analysis of the\nblind deblurring posterior. This reveals that the MAP estimator tends to\nproduce sharp filters (close to the Dirac delta function) and blurry solutions.\nHowever local minimizers of the posterior, which can be obtained with gradient\ndescent, correspond to realistic, natural images, effectively solving the blind\ndeconvolution problem. Our findings suggest that overcoming MAP's limitations\nrequires good local initialization to local minima in the posterior landscape.\nWe validate our analysis with numerical experiments, demonstrating the\npractical implications of our insights for designing improved priors and\noptimization techniques.", "AI": {"tldr": "研究发现，在盲去卷积中，当与基于扩散的图像先验结合时，最大后验（MAP）估计器倾向于产生模糊结果，但后验分布的局部最小值能得到清晰的自然图像，这需要良好的局部初始化。", "motivation": "MAP估计在盲去卷积中常用于恢复清晰图像，但当与稀疏性先验结合时，它已被证明偏向模糊解，限制了其有效性。本文旨在重新审视这一问题，并探索基于扩散的先验如何影响MAP估计。", "method": "通过对基于扩散的图像先验的似然景观进行经验性检查，揭示其特性；然后对盲去模糊后验进行理论分析；最后通过数值实验验证分析结果。", "result": "1. 模糊图像在基于扩散的先验下倾向于具有更高的似然值；2. 似然景观包含大量对应于自然图像的局部最小值；3. MAP估计器倾向于产生尖锐的模糊滤波器（接近狄拉克函数）和模糊的图像解；4. 后验分布的局部最小值（可通过梯度下降获得）对应于真实、自然的图像，有效解决了盲去卷积问题。", "conclusion": "要克服MAP估计的局限性，需要良好的局部初始化以找到后验景观中的局部最小值。这些发现对设计改进的先验和优化技术具有实际指导意义。"}}
{"id": "2508.03339", "categories": ["cs.RO", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03339", "abs": "https://arxiv.org/abs/2508.03339", "authors": ["Haoran Lin", "Wenrui Chen", "Xianchi Chen", "Fan Yang", "Qiang Diao", "Wenxin Xie", "Sijie Wu", "Kailun Yang", "Maojun Li", "Yaonan Wang"], "title": "UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands", "comment": "The project page is at https://haochen611.github.io/UFG", "summary": "Dexterous grasp datasets are vital for embodied intelligence, but mostly\nemphasize grasp stability, ignoring functional grasps needed for tasks like\nopening bottle caps or holding cup handles. Most rely on bulky, costly, and\nhard-to-control high-DOF Shadow Hands. Inspired by the human hand's\nunderactuated mechanism, we establish UniFucGrasp, a universal functional grasp\nannotation strategy and dataset for multiple dexterous hand types. Based on\nbiomimicry, it maps natural human motions to diverse hand structures and uses\ngeometry-based force closure to ensure functional, stable, human-like grasps.\nThis method supports low-cost, efficient collection of diverse, high-quality\nfunctional grasps. Finally, we establish the first multi-hand functional grasp\ndataset and provide a synthesis model to validate its effectiveness.\nExperiments on the UFG dataset, IsaacSim, and complex robotic tasks show that\nour method improves functional manipulation accuracy and grasp stability,\nenables efficient generalization across diverse robotic hands, and overcomes\nannotation cost and generalization challenges in dexterous grasping. The\nproject page is at https://haochen611.github.io/UFG.", "AI": {"tldr": "灵巧抓取数据集缺乏功能性抓取且成本高昂。本文提出UniFucGrasp，一个通用的多手型功能性抓取标注策略和数据集，提高了抓取精度、稳定性和泛化能力。", "motivation": "现有的灵巧抓取数据集主要侧重抓取稳定性，忽略了任务所需的功能性抓取（如开瓶盖、握杯柄）。此外，大多数数据集依赖笨重、昂贵且难以控制的高自由度Shadow Hands。", "method": "受到欠驱动人手的启发，建立了UniFucGrasp，一个通用的功能性抓取标注策略和数据集。该方法基于仿生学，将自然人手运动映射到不同机械手结构，并利用基于几何的力闭合来确保功能性、稳定且类人抓取。支持低成本、高效的数据收集，并建立了首个多手型功能性抓取数据集，提供合成模型验证其有效性。", "result": "在UFG数据集、IsaacSim和复杂机器人任务上的实验表明，该方法提高了功能性操作精度和抓取稳定性，实现了在不同机器人手型上的高效泛化，并克服了灵巧抓取中的标注成本和泛化挑战。", "conclusion": "UniFucGrasp策略和数据集通过提供多样化、高质量、功能性且可在多种机器人手上泛化的抓取，有效解决了现有灵巧抓取数据集的局限性，显著推动了具身智能的发展。"}}
{"id": "2508.03324", "categories": ["cs.CV", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03324", "abs": "https://arxiv.org/abs/2508.03324", "authors": ["Satyapreet Singh Yadav", "Chandra Sekhar Seelamantula", "Chetan Singh Thakur"], "title": "Live Demonstration: Neuromorphic Radar for Gesture Recognition", "comment": "Neuromorphic Radar, Hand Gesture Recognition, Event-Driven,\n  Sigma-Delta Encoding, Sparse Representation. Presented in ICASSP 2025 at\n  Hyderabad, India", "summary": "We present a neuromorphic radar framework for real-time, low-power hand\ngesture recognition (HGR) using an event-driven architecture inspired by\nbiological sensing. Our system comprises a 24 GHz Doppler radar front-end and a\ncustom neuromorphic sampler that converts intermediate-frequency (IF) signals\ninto sparse spike-based representations via asynchronous sigma-delta encoding.\nThese events are directly processed by a lightweight neural network deployed on\na Cortex-M0 microcontroller, enabling low-latency inference without requiring\nspectrogram reconstruction. Unlike conventional radar HGR pipelines that\ncontinuously sample and process data, our architecture activates only when\nmeaningful motion is detected, significantly reducing memory, power, and\ncomputation overhead. Evaluated on a dataset of five gestures collected from\nseven users, our system achieves > 85% real-time accuracy. To the best of our\nknowledge, this is the first work that employs bio-inspired asynchronous\nsigma-delta encoding and an event-driven processing framework for radar-based\nHGR.", "AI": {"tldr": "该研究提出了一种基于事件驱动的神经形态雷达框架，通过异步sigma-delta编码将雷达信号转换为稀疏脉冲，并在低功耗微控制器上实现实时、低功耗的手势识别，显著降低了系统开销。", "motivation": "传统的雷达手势识别（HGR）系统持续采样和处理数据，导致高昂的内存、功耗和计算开销，促使研究人员寻求更高效、低功耗的解决方案。", "method": "该系统包含一个24 GHz多普勒雷达前端和一个定制的神经形态采样器，通过异步sigma-delta编码将中频（IF）信号转换为稀疏的脉冲表示。这些事件由部署在Cortex-M0微控制器上的轻量级神经网络直接处理，无需频谱图重建。系统仅在检测到有意义运动时激活，采用事件驱动的处理方式。", "result": "在包含七名用户五种手势的数据集上进行评估，该系统实现了超过85%的实时识别准确率，并显著降低了内存、功耗和计算开销。", "conclusion": "该工作首次将生物启发式异步sigma-delta编码和事件驱动处理框架应用于基于雷达的手势识别，为实时、低功耗的雷达手势识别提供了新的范例。"}}
{"id": "2508.03027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03027", "abs": "https://arxiv.org/abs/2508.03027", "authors": ["Yizhuo Wang", "Haodong He", "Jingsong Liang", "Yuhong Cao", "Ritabrata Chakraborty", "Guillaume Sartoretti"], "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction", "comment": "Accepted for presentation at CORL 2025", "summary": "Path planning in unknown environments is a crucial yet inherently challenging\ncapability for mobile robots, which primarily encompasses two coupled tasks:\nautonomous exploration and point-goal navigation. In both cases, the robot must\nperceive the environment, update its belief, and accurately estimate potential\ninformation gain on-the-fly to guide planning. In this work, we propose\nCogniPlan, a novel path planning framework that leverages multiple plausible\nlayouts predicted by a COnditional GeNerative Inpainting model, mirroring how\nhumans rely on cognitive maps during navigation. These predictions, based on\nthe partially observed map and a set of layout conditioning vectors, enable our\nplanner to reason effectively under uncertainty. We demonstrate strong synergy\nbetween generative image-based layout prediction and graph-attention-based path\nplanning, allowing CogniPlan to combine the scalability of graph\nrepresentations with the fidelity and predictiveness of occupancy maps,\nyielding notable performance gains in both exploration and navigation. We\nextensively evaluate CogniPlan on two datasets (hundreds of maps and realistic\nfloor plans), consistently outperforming state-of-the-art planners. We further\ndeploy it in a high-fidelity simulator and on hardware, showcasing its\nhigh-quality path planning and real-world applicability.", "AI": {"tldr": "CogniPlan是一种新颖的路径规划框架，它利用条件生成修复模型预测的多个可能环境布局，从而在未知环境中实现高效的探索和导航。", "motivation": "移动机器人在未知环境中的路径规划（包括自主探索和目标导航）是一项关键但固有的挑战，需要机器人实时感知环境、更新信念并估计潜在信息增益以指导规划。", "method": "该研究提出了CogniPlan框架，它借鉴人类认知地图的导航方式，利用一个条件生成修复（COGNI）模型根据部分观测地图和布局条件向量来预测多个可信的潜在环境布局。这种方法将生成式图像布局预测与基于图注意力机制的路径规划相结合，使规划器能够在不确定性下有效推理。", "result": "CogniPlan在探索和导航任务中均取得了显著的性能提升，结合了图表示的可扩展性与占据地图的保真度和预测性。在两个大型数据集（数百张地图和真实平面图）上，CogniPlan持续优于现有最先进的规划器，并在高保真模拟器和真实硬件上展示了其高质量的路径规划能力和实际应用价值。", "conclusion": "CogniPlan通过利用生成模型预测环境布局，有效解决了未知环境中的路径规划挑战，展现出卓越的性能和实际应用潜力。"}}
{"id": "2508.02961", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.", "AI": {"tldr": "本文提出一种基于大语言模型（LLM）内在推理能力的“自我意识”防御机制，以对抗提示注入攻击。", "motivation": "传统的提示注入防御方法依赖外部分类器，本文旨在利用LLM固有的推理能力实现自我保护，提高LLM的伦理安全性。", "method": "引入了一个包含元认知（Meta-Cognitive）和仲裁（Arbitration）模块的框架，使LLM能够自主评估和调节其输出。该方法在七个先进的LLM上，使用AdvBench和Prompt-Injection-Mixed-Techniques-2024两个数据集进行了评估。", "result": "实验结果表明，防御成功率显著提高，在增强模式下，部分模型达到了完美或接近完美的防御效果。研究还分析了防御成功率提升与计算开销之间的权衡。", "conclusion": "这种“自我意识”方法为增强LLM的伦理提供了轻量级、经济高效的解决方案，特别适用于各种平台上的生成式AI应用。"}}
{"id": "2508.03137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03137", "abs": "https://arxiv.org/abs/2508.03137", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "title": "Long Story Generation via Knowledge Graph and Literary Theory", "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.", "AI": {"tldr": "本文提出了一种多智能体故事生成器结构，通过引入记忆存储模型和故事主题障碍框架，并模拟写读者交互，以解决长篇故事生成中主题漂移和情节缺乏吸引力的问题，从而生成更高质量的故事。", "motivation": "现有的基于大纲的多阶段长篇故事生成方法存在两个主要问题：由于对之前大纲的记忆丢失导致几乎不可避免的主题漂移，以及情节乏味、逻辑不连贯，对人类读者缺乏吸引力。", "method": "本文提出了一个多智能体故事生成器结构，以大型语言模型（LLMs）作为智能体的核心组件。为避免主题漂移，引入了一个记忆存储模型，包括识别重要记忆的长期记忆和保留最新大纲的短期记忆。为增加故事的吸引力，设计了一个基于文学叙事学理论的故事主题障碍框架，引入不确定因素和评估标准来生成大纲，通过构建知识图谱和整合新节点内容来增强故事吸引力。此外，建立了多智能体交互阶段，通过对话模拟写读者交互并根据反馈修改故事文本，以确保一致性和逻辑性。", "result": "与现有方法相比，本文提出的方法能够生成更高质量的长篇故事。", "conclusion": "通过多智能体结构、记忆存储模型、故事主题障碍框架以及模拟写读者交互，可以有效解决长篇故事生成中的主题漂移和情节吸引力不足的问题，显著提升生成故事的质量。"}}
{"id": "2508.02927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02927", "abs": "https://arxiv.org/abs/2508.02927", "authors": ["Srikanth Muralidharan", "Heitor R. Medeiros", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?", "comment": null, "summary": "Many real-world applications require recognition models that are robust to\ndifferent operational conditions and modalities, but at the same time run on\nsmall embedded devices, with limited hardware. While for normal size models,\npre-training is known to be very beneficial in accuracy and robustness, for\nsmall models, that can be employed for embedded and edge devices, its effect is\nnot clear. In this work, we investigate the effect of ImageNet pretraining on\nincreasingly small backbone architectures (ultra-small models, with $<$1M\nparameters) with respect to robustness in downstream object detection tasks in\nthe infrared visual modality. Using scaling laws derived from standard object\nrecognition architectures, we construct two ultra-small backbone families and\nsystematically study their performance. Our experiments on three different\ndatasets reveal that while ImageNet pre-training is still useful, beyond a\ncertain capacity threshold, it offers diminishing returns in terms of\nout-of-distribution detection robustness. Therefore, we advise practitioners to\nstill use pre-training and, when possible avoid too small models as while they\nmight work well for in-domain problems, they are brittle when working\nconditions are different.", "AI": {"tldr": "研究了ImageNet预训练对超小型模型（参数<1M）在红外目标检测任务中鲁棒性的影响，发现预训练仍有用，但超过一定容量阈值后，对域外鲁棒性的提升回报递减。", "motivation": "现实应用需要模型在有限硬件的嵌入式设备上运行，同时对不同操作条件和模态具有鲁棒性。虽然预训练对常规大小模型有益，但其对超小型模型的影响尚不明确。", "method": "通过缩放定律构建了两类超小型骨干网络（参数<1M），并系统研究了ImageNet预训练对它们在红外视觉模态下，下游目标检测任务鲁棒性的影响。实验在三个不同数据集上进行。", "result": "ImageNet预训练仍然有用，但当模型容量超过某个阈值后，它在域外检测鲁棒性方面的回报会递减。", "conclusion": "建议实践者仍然使用预训练。同时，应避免使用过小的模型，因为它们虽然可能在域内问题上表现良好，但在工作条件不同时会变得脆弱。"}}
{"id": "2508.03403", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03403", "abs": "https://arxiv.org/abs/2508.03403", "authors": ["Gang Yang"], "title": "Sparsity and Total Variation Constrained Multilayer Linear Unmixing for Hyperspectral Imagery", "comment": null, "summary": "Hyperspectral unmixing aims at estimating material signatures (known as\nendmembers) and the corresponding proportions (referred to abundances), which\nis a critical preprocessing step in various hyperspectral imagery applications.\nThis study develops a novel approach called sparsity and total variation (TV)\nconstrained multilayer linear unmixing (STVMLU) for hyperspectral imagery.\nSpecifically, based on a multilayer matrix factorization model, to improve the\naccuracy of unmixing, a TV constraint is incorporated to consider adjacent\nspatial similarity. Additionally, a L1/2-norm sparse constraint is adopted to\neffectively characterize the sparsity of the abundance matrix. For optimizing\nthe STVMLU model, the method of alternating direction method of multipliers\n(ADMM) is employed, which allows for the simultaneous extraction of endmembers\nand their corresponding abundance matrix. Experimental results illustrate the\nenhanced performance of the proposed STVMLU when compared to other algorithms.", "AI": {"tldr": "该研究提出了一种名为STVMLU的新型高光谱解混方法，通过结合多层线性解混模型、全变分（TV）空间相似性约束和L1/2稀疏性约束，并利用ADMM算法优化，实现了更好的解混性能。", "motivation": "高光谱解混是高光谱图像应用中的一个关键预处理步骤，旨在估计材料特征（端元）及其比例（丰度），提高解混精度是重要目标。", "method": "该方法基于多层矩阵分解模型。为提高解混精度，引入了全变分（TV）约束以考虑相邻空间相似性，并采用L1/2范数稀疏约束来有效表征丰度矩阵的稀疏性。模型优化采用交替方向乘子法（ADMM），可同时提取端元及其对应的丰度矩阵。", "result": "实验结果表明，所提出的STVMLU方法与其他算法相比，性能有所提升。", "conclusion": "STVMLU方法通过结合空间相似性和稀疏性约束，有效提高了高光谱图像解混的准确性。"}}
{"id": "2508.03428", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03428", "abs": "https://arxiv.org/abs/2508.03428", "authors": ["Bojan Derajić", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Wolfgang Hönig"], "title": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments", "comment": null, "summary": "In this paper, we propose a hybrid MPC local planner that uses a\nlearning-based approximation of a time-varying safe set, derived from local\nobservations and applied as the MPC terminal constraint. This set can be\nrepresented as a zero-superlevel set of the value function computed via\nHamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time.\nWe exploit the property that the HJ value function can be expressed as a\ndifference of the corresponding signed distance function (SDF) and a\nnon-negative residual function. The residual component is modeled as a neural\nnetwork with non-negative output and subtracted from the computed SDF,\nresulting in a real-time value function estimate that is at least as safe as\nthe SDF by design. Additionally, we parametrize the neural residual by a\nhypernetwork to improve real-time performance and generalization properties.\nThe proposed method is compared with three state-of-the-art methods in\nsimulations and hardware experiments, achieving up to 30\\% higher success rates\ncompared to the best baseline while requiring a similar computational effort\nand producing high-quality (low travel-time) solutions.", "AI": {"tldr": "本文提出了一种混合MPC局部规划器，通过学习近似时变安全集作为MPC终端约束，该安全集基于Hamilton-Jacobi可达性分析，并利用神经网络对残差项进行建模以实现实时估计，显著提升了规划成功率。", "motivation": "Hamilton-Jacobi (HJ) 可达性分析计算出的价值函数可以表示为安全集，但其实时计算不可行，因此需要一种实时可行的学习方法来近似该安全集，作为模型预测控制（MPC）的终端约束，以确保规划器的安全性。", "method": "提出混合MPC局部规划器，将学习近似的时变安全集作为MPC终端约束。该安全集是HJ价值函数的零超水平集。利用HJ价值函数可表示为带符号距离函数（SDF）与非负残差函数之差的特性，将残差建模为具有非负输出的神经网络，并从SDF中减去以获得实时的价值函数估计。为提高实时性能和泛化能力，通过超网络参数化神经网络残差。", "result": "与三种最先进的方法进行仿真和硬件实验比较，结果显示所提方法成功率比最佳基线高出30%，同时计算开销相似，并能产生高质量（低旅行时间）的解决方案。", "conclusion": "所提出的混合MPC局部规划器，通过学习近似HJ可达性分析得到的安全集并将其作为终端约束，在实时性能和泛化能力方面表现优异，显著提升了规划成功率，并能生成高质量的路径，且计算成本可控。"}}
{"id": "2508.03053", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03053", "abs": "https://arxiv.org/abs/2508.03053", "authors": ["Haojun Xu", "Jiaqi Xiang", "Wu Wei", "Jinyu Chen", "Linqing Zhong", "Linjiang Huang", "Hongyu Yang", "Si Liu"], "title": "SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps", "comment": "9 pages, 5 figures", "summary": "A typical human strategy for giving navigation guidance is to sketch route\nmaps based on the environmental layout. Inspired by this, we introduce Sketch\nmap-based visual Navigation (SkeNa), an embodied navigation task in which an\nagent must reach a goal in an unseen environment using only a hand-drawn sketch\nmap as guidance. To support research for SkeNa, we present a large-scale\ndataset named SoR, comprising 54k trajectory and sketch map pairs across 71\nindoor scenes. In SoR, we introduce two navigation validation sets with varying\nlevels of abstraction in hand-drawn sketches, categorized based on their\npreservation of spatial scales in the environment, to facilitate future\nresearch. To construct SoR, we develop an automated sketch-generation pipeline\nthat efficiently converts floor plans into hand-drawn representations. To solve\nSkeNa, we propose SkeNavigator, a navigation framework that aligns visual\nobservations with hand-drawn maps to estimate navigation targets. It employs a\nRay-based Map Descriptor (RMD) to enhance sketch map valid feature\nrepresentation using equidistant sampling points and boundary distances. To\nimprove alignment with visual observations, a Dual-Map Aligned Goal Predictor\n(DAGP) leverages the correspondence between sketch map features and on-site\nconstructed exploration map features to predict goal position and guide\nnavigation. SkeNavigator outperforms prior floor plan navigation methods by a\nlarge margin, improving SPL on the high-abstract validation set by 105%\nrelatively. Our code and dataset will be released.", "AI": {"tldr": "该研究引入了基于手绘草图的视觉导航（SkeNa）任务，并发布了大规模数据集SoR，同时提出了SkeNavigator框架，该框架通过对齐视觉观测与草图来导航，并显著优于现有方法。", "motivation": "受人类通过手绘草图指导导航的启发，现有具身导航任务缺乏使用手绘、可能抽象的草图作为指导，促使研究者探索一种更自然、符合人类习惯的导航方式。", "method": "引入了基于手绘草图的视觉导航（SkeNa）任务；构建了包含5.4万对轨迹和草图的大规模数据集SoR，包含两种抽象级别的验证集；开发了自动化草图生成流程；提出了SkeNavigator导航框架，包含射线地图描述器（RMD）用于增强草图特征表示，以及双地图对齐目标预测器（DAGP）用于对齐草图与探索地图特征以预测目标位置。", "result": "SkeNavigator框架显著优于现有平面图导航方法，在高抽象度验证集上将SPL相对提高了105%。", "conclusion": "该研究成功定义并支持了基于手绘草图的具身导航任务SkeNa，提供了大规模数据集SoR，并提出了高效的SkeNavigator解决方案，证明了其在利用手绘草图进行导航方面的卓越性能。"}}
{"id": "2508.02979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02979", "abs": "https://arxiv.org/abs/2508.02979", "authors": ["Peng Ding", "Rick Stevens"], "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development.", "AI": {"tldr": "提出了一种统一的LLM工具集成方法，旨在解决现有生态系统的碎片化问题，并通过自动化和优化提升开发效率和执行性能。", "motivation": "当前的工具增强型大型语言模型（LLMs）生态系统碎片化严重，开发者需要处理多种协议、手动定义模式和复杂的执行流程。", "method": "提出了一种统一的、协议无关的工具集成方法，包括自动化模式生成、双模式并发执行以及无缝多源工具管理。", "result": "实验结果显示，集成场景下的代码量减少了60-80%，通过优化并发性能提升高达3.1倍，并与现有函数调用标准完全兼容。", "conclusion": "该工作为工具集成架构提供了理论见解，并为LLM应用开发提供了实用的解决方案。"}}
{"id": "2508.03140", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03140", "abs": "https://arxiv.org/abs/2508.03140", "authors": ["Junyao Yang", "Jianwei Wang", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior", "comment": "15 pages, 7 figures", "summary": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability.", "AI": {"tldr": "提出RCP-Merging方法，旨在以资源高效的方式将具有长CoT推理能力的LLM与特定领域知识结合，同时克服现有合并方法导致的推理能力下降问题。", "motivation": "现有模型合并方法在合并长CoT模型和领域特定LLM时，会导致推理能力显著下降，甚至出现乱码或输出崩溃，因此需要一种能有效融合两者并保持性能的方法。", "method": "RCP-Merging框架将推理模型权重视为基础先验，并利用推理能力指标来保留核心长CoT能力模型的权重，同时选择性地合并重要的领域特定权重，以实现双重能力的融合。", "result": "在生物医学和金融领域的Qwen2.5-7B、Llama3.1-8B和Qwen2.5-1.5B模型上的实验表明，RCP-Merging成功合并了推理模型和领域特定模型，使领域任务性能比现有最佳方法提高了9.5%和9.2%，同时没有显著损害原始的长CoT推理能力。", "conclusion": "RCP-Merging是一种有效且资源高效的模型合并框架，能够将长CoT推理能力与领域特定知识融合到大型语言模型中，同时保持两种能力的高性能。"}}
{"id": "2508.02944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02944", "abs": "https://arxiv.org/abs/2508.02944", "authors": ["Chenxu Zhang", "Zenan Li", "Hongyi Xu", "You Xie", "Xiaochen Zhao", "Tianpei Gu", "Guoxian Song", "Xin Chen", "Chao Liang", "Jianwen Jiang", "Linjie Luo"], "title": "X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio", "comment": "Project Page at https://byteaigc.github.io/X-Actor/", "summary": "We present X-Actor, a novel audio-driven portrait animation framework that\ngenerates lifelike, emotionally expressive talking head videos from a single\nreference image and an input audio clip. Unlike prior methods that emphasize\nlip synchronization and short-range visual fidelity in constrained speaking\nscenarios, X-Actor enables actor-quality, long-form portrait performance\ncapturing nuanced, dynamically evolving emotions that flow coherently with the\nrhythm and content of speech. Central to our approach is a two-stage decoupled\ngeneration pipeline: an audio-conditioned autoregressive diffusion model that\npredicts expressive yet identity-agnostic facial motion latent tokens within a\nlong temporal context window, followed by a diffusion-based video synthesis\nmodule that translates these motions into high-fidelity video animations. By\noperating in a compact facial motion latent space decoupled from visual and\nidentity cues, our autoregressive diffusion model effectively captures\nlong-range correlations between audio and facial dynamics through a\ndiffusion-forcing training paradigm, enabling infinite-length emotionally-rich\nmotion prediction without error accumulation. Extensive experiments demonstrate\nthat X-Actor produces compelling, cinematic-style performances that go beyond\nstandard talking head animations and achieves state-of-the-art results in\nlong-range, audio-driven emotional portrait acting.", "AI": {"tldr": "X-Actor是一个新颖的音频驱动肖像动画框架，能从单张图片和音频生成逼真、富有情感的长程说话人视频。", "motivation": "现有方法侧重于唇部同步和短程视觉保真度，难以生成捕捉细微、动态演变情感的长时间、演员级肖像表演。", "method": "采用两阶段解耦生成流程：首先，一个音频条件自回归扩散模型在长时间上下文窗口内预测富有表现力但与身份无关的面部运动潜在令牌；然后，一个基于扩散的视频合成模块将这些运动转化为高保真视频动画。通过在紧凑的面部运动潜在空间中操作并采用扩散强制训练范式，有效捕捉音频与面部动态之间的长程关联，实现无限长度的情感丰富运动预测而无误差累积。", "result": "X-Actor能生成引人入胜、电影风格的表演，超越了标准的说话人动画，并在长程、音频驱动的情感肖像表演方面达到了最先进的水平。", "conclusion": "X-Actor成功解决了传统音频驱动肖像动画在长程情感表达和连贯性方面的不足，实现了高质量、富有情感的逼真视频生成。"}}
{"id": "2508.03608", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.03608", "abs": "https://arxiv.org/abs/2508.03608", "authors": ["Saleh Sakib Ahmed", "Sara Nowreen", "M. Sohel Rahman"], "title": "CloudBreaker: Breaking the Cloud Covers of Sentinel-2 Images using Multi-Stage Trained Conditional Flow Matching on Sentinel-1", "comment": null, "summary": "Cloud cover and nighttime conditions remain significant limitations in\nsatellite-based remote sensing, often restricting the availability and\nusability of multi-spectral imagery. In contrast, Sentinel-1 radar images are\nunaffected by cloud cover and can provide consistent data regardless of weather\nor lighting conditions. To address the challenges of limited satellite imagery,\nwe propose CloudBreaker, a novel framework that generates high-quality\nmulti-spectral Sentinel-2 signals from Sentinel-1 data. This includes the\nreconstruction of optical (RGB) images as well as critical vegetation and water\nindices such as NDVI and NDWI.We employed a novel multi-stage training approach\nbased on conditional latent flow matching and, to the best of our knowledge,\nare the first to integrate cosine scheduling with flow matching. CloudBreaker\ndemonstrates strong performance, achieving a Frechet Inception Distance (FID)\nscore of 0.7432, indicating high fidelity and realism in the generated optical\nimagery. The model also achieved Structural Similarity Index Measure (SSIM) of\n0.6156 for NDWI and 0.6874 for NDVI, indicating a high degree of structural\nsimilarity. This establishes CloudBreaker as a promising solution for a wide\nrange of remote sensing applications where multi-spectral data is typically\nunavailable or unreliable", "AI": {"tldr": "提出CloudBreaker框架，利用Sentinel-1雷达数据生成高质量Sentinel-2多光谱图像（包括RGB、NDVI、NDWI），以克服云层和夜间条件对遥感数据的限制。", "motivation": "卫星遥感中，云层覆盖和夜间条件严重限制了多光谱图像的可用性，而Sentinel-1雷达图像不受这些条件影响，可提供持续数据。", "method": "提出CloudBreaker框架，通过Sentinel-1数据生成Sentinel-2多光谱信号（RGB、NDVI、NDWI）。采用基于条件潜在流匹配的新颖多阶段训练方法，并首次将余弦调度与流匹配结合。", "result": "生成的光学图像FID得分为0.7432，表明高保真度和真实感；NDWI的SSIM为0.6156，NDVI的SSIM为0.6874，表明高度结构相似性。", "conclusion": "CloudBreaker为多光谱数据通常不可用或不可靠的遥感应用提供了一个有前景的解决方案。"}}
{"id": "2508.03068", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03068", "abs": "https://arxiv.org/abs/2508.03068", "authors": ["Sirui Chen", "Yufei Ye", "Zi-Ang Cao", "Jennifer Lew", "Pei Xu", "C. Karen Liu"], "title": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching", "comment": null, "summary": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns\nnavigation, locomotion, and reaching skills for humanoids, directly from human\nmotion and vision perception data. We take a modular approach where the\nhigh-level planner commands the target position and orientation of the hands\nand eyes of the humanoid, delivered by the low-level policy that controls the\nwhole-body movements. Specifically, the low-level whole-body controller learns\nto track the three points (eyes, left hand, and right hand) from existing\nlarge-scale human motion capture data while high-level policy learns from human\ndata collected by Aria glasses. Our modular approach decouples the ego-centric\nvision perception from physical actions, promoting efficient learning and\nscalability to novel scenes. We evaluate our method both in simulation and in\nthe real-world, demonstrating humanoid's capabilities to navigate and reach in\ncomplex environments designed for humans.", "AI": {"tldr": "HEAD是一个框架，使类人机器人能直接从人类运动和视觉感知数据中学习导航、运动和抓取技能，采用高低层解耦的模块化方法。", "motivation": "研究动机是让类人机器人能从人类数据中学习复杂的导航、运动和抓取技能，以在为人类设计的复杂环境中有效操作。", "method": "HEAD框架采用模块化方法：高层规划器指令手和眼睛的目标位置与方向，低层策略控制全身运动以实现这些目标。低层控制器从大规模人体动作捕捉数据中学习跟踪眼睛和双手三个点，高层策略则从Aria眼镜收集的人类数据中学习。这种方法将自我中心视觉感知与物理动作解耦。", "result": "该方法在模拟和现实世界中均进行了评估，展示了类人机器人在复杂环境中进行导航和抓取的能力。", "conclusion": "HEAD框架通过模块化方法，实现了从人类数据中高效学习类人机器人的导航、运动和抓取技能，并具有良好的可扩展性，能适应新场景。"}}
{"id": "2508.02994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02994", "abs": "https://arxiv.org/abs/2508.02994", "authors": ["Fangyi Yu"], "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs", "comment": null, "summary": "As large language models (LLMs) grow in capability and autonomy, evaluating\ntheir outputs-especially in open-ended and complex tasks-has become a critical\nbottleneck. A new paradigm is emerging: using AI agents as the evaluators\nthemselves. This \"agent-as-a-judge\" approach leverages the reasoning and\nperspective-taking abilities of LLMs to assess the quality and safety of other\nmodels, promising calable and nuanced alternatives to human evaluation. In this\nreview, we define the agent-as-a-judge concept, trace its evolution from\nsingle-model judges to dynamic multi-agent debate frameworks, and critically\nexamine their strengths and shortcomings. We compare these approaches across\nreliability, cost, and human alignment, and survey real-world deployments in\ndomains such as medicine, law, finance, and education. Finally, we highlight\npressing challenges-including bias, robustness, and meta evaluation-and outline\nfuture research directions. By bringing together these strands, our review\ndemonstrates how agent-based judging can complement (but not replace) human\noversight, marking a step toward trustworthy, scalable evaluation for\nnext-generation LLMs.", "AI": {"tldr": "这篇综述探讨了利用AI代理作为评估者（“代理即法官”）来解决大型语言模型（LLMs）评估瓶颈的新范式，分析了其演变、优缺点、应用领域、挑战及未来方向。", "motivation": "随着大型语言模型能力和自主性的增强，尤其是在开放式和复杂任务中，评估其输出成为一个关键瓶颈。需要可扩展且细致的替代方案来取代人工评估。", "method": "本文作为一篇综述，首先定义了“代理即法官”的概念，追溯了其从单模型评估到动态多代理辩论框架的演变。随后，批判性地审视了这些方法的优缺点，并在可靠性、成本和人类对齐方面进行了比较，同时调查了其在医学、法律、金融和教育等领域的实际部署。最后，指出了面临的挑战（如偏见、鲁棒性和元评估），并展望了未来的研究方向。", "result": "“代理即法官”方法利用LLM的推理和视角采纳能力，为LLM评估提供了可扩展和细致的替代方案。该范式已从单一模型发展到多代理辩论框架，并在多个实际领域得到应用。尽管存在挑战，但它能够补充（而非取代）人工监督。", "conclusion": "代理即法官”方法标志着LLM评估迈向可信赖、可扩展的一步，能够有效补充人类监督，但不能完全取代之。"}}
{"id": "2508.03178", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03178", "abs": "https://arxiv.org/abs/2508.03178", "authors": ["Chenyang Wang", "Liang Wen", "Shousheng Jia", "Xiangzheng Zhang", "Liang Xu"], "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following", "comment": "12 pages, 10 figures, 7 tables", "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.", "AI": {"tldr": "本文提出一个包含预览和自检的框架，通过数据筛选、Entropy-SFT和TEA-RL来解决LLM在复杂指令遵循中存在的“懒惰推理”问题，显著提升了模型性能。", "motivation": "尽管LLM在数学、编码和通用谜题方面表现出色，但在复杂指令遵循方面仍存在不一致性，主要归因于推理阶段的“懒惰推理”。", "method": "1. 识别“懒惰推理”为导致指令遵循不佳的主因。2. 提出一个包含预览和自检的严谨推理框架。3. 生成带有复杂约束的指令并筛选为hard、easy和pass三类数据集。4. 对pass提示进行拒绝采样，构建高质量冷启动数据集。5. 采用熵保持监督微调（Entropy-SFT）和基于规则密集奖励的逐令牌熵自适应强化学习（TEA-RL），以促进模型形成可泛化的预览和自检推理能力。", "result": "在指令遵循基准测试中，模型性能显著提升，且在各种模型规模上均有体现。Light-IF-32B模型甚至超越了更大的开源模型（如DeepSeek-R1）和闭源模型（如Doubao-1.6）。", "conclusion": "所提出的框架能有效缓解LLM的“懒惰推理”问题，通过促进严谨的推理过程（包括预览和自检），显著提高了模型遵循复杂指令的能力和推理的泛化性。"}}
{"id": "2508.02967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02967", "abs": "https://arxiv.org/abs/2508.02967", "authors": ["Dawei Zhang", "Xiaojie Guo"], "title": "Towards Robust Image Denoising with Scale Equivariance", "comment": null, "summary": "Despite notable advances in image denoising, existing models often struggle\nto generalize beyond in-distribution noise patterns, particularly when\nconfronted with out-of-distribution (OOD) conditions characterized by spatially\nvariant noise. This generalization gap remains a fundamental yet underexplored\nchallenge. In this work, we investigate \\emph{scale equivariance} as a core\ninductive bias for improving OOD robustness. We argue that incorporating\nscale-equivariant structures enables models to better adapt from training on\nspatially uniform noise to inference on spatially non-uniform degradations.\nBuilding on this insight, we propose a robust blind denoising framework\nequipped with two key components: a Heterogeneous Normalization Module (HNM)\nand an Interactive Gating Module (IGM). HNM stabilizes feature distributions\nand dynamically corrects features under varying noise intensities, while IGM\nfacilitates effective information modulation via gated interactions between\nsignal and feature paths. Extensive evaluations demonstrate that our model\nconsistently outperforms state-of-the-art methods on both synthetic and\nreal-world benchmarks, especially under spatially heterogeneous noise. Code\nwill be made publicly available.", "AI": {"tldr": "本文提出一个基于尺度等变性的鲁棒盲去噪框架，通过异构归一化模块（HNM）和交互门控模块（IGM）处理空间变异噪声，显著提升了模型在分布外（OOD）条件下的泛化能力和去噪性能。", "motivation": "现有图像去噪模型难以泛化到分布外（OOD）噪声模式，特别是面对空间变异噪声时，这一泛化差距是一个基本但未被充分探索的挑战。", "method": "引入“尺度等变性”作为核心归纳偏置以提升OOD鲁棒性。提出一个鲁棒盲去噪框架，包含两个关键组件：1) 异构归一化模块（HNM），用于稳定特征分布并动态校正不同噪声强度下的特征；2) 交互门控模块（IGM），通过信号和特征路径间的门控交互实现有效信息调制。", "result": "该模型在合成和真实世界基准测试中，特别是在空间异构噪声条件下，持续优于现有最先进的方法。", "conclusion": "通过整合尺度等变结构以及HNM和IGM模块，模型能够更好地从均匀噪声训练适应非均匀退化推理，显著提升了去噪模型在OOD和空间异构噪声条件下的鲁棒性和性能。"}}
{"id": "2508.03070", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.03070", "abs": "https://arxiv.org/abs/2508.03070", "authors": ["Devin Crowley", "Jeremy Dao", "Helei Duan", "Kevin Green", "Jonathan Hurst", "Alan Fern"], "title": "Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running", "comment": "7 pages, 7 figures, published by IEEE at ICRA 2023, pp. 12205-12211,\n  see https://ieeexplore.ieee.org/document/10160436", "summary": "In this paper, we explore the space of running gaits for the bipedal robot\nCassie. Our first contribution is to present an approach for optimizing gait\nefficiency across a spectrum of speeds with the aim of enabling extremely\nhigh-speed running on hardware. This raises the question of how the resulting\ngaits compare to human running mechanics, which are known to be highly\nefficient in comparison to quadrupeds. Our second contribution is to conduct\nthis comparison based on established human biomechanical studies. We find that\ndespite morphological differences between Cassie and humans, key properties of\nthe gaits are highly similar across a wide range of speeds. Finally, our third\ncontribution is to integrate the optimized running gaits into a full controller\nthat satisfies the rules of the real-world task of the 100m dash, including\nstarting and stopping from a standing position. We demonstrate this controller\non hardware to establish the Guinness World Record for Fastest 100m by a\nBipedal Robot.", "AI": {"tldr": "本文优化了双足机器人Cassie的跑步步态以实现极高速度，并将其与人类跑步力学进行比较，最终集成了控制器，使Cassie打破了双足机器人100米短跑的世界纪录。", "motivation": "研究旨在使双足机器人Cassie实现极高速奔跑，并探究优化后的机器人步态与已知高效的人类跑步力学之间的相似性。", "method": "1. 优化Cassie的步态效率以适应不同速度，旨在实现高速运行。2. 基于现有的人类生物力学研究，将优化后的机器人步态与人类跑步力学进行比较。3. 将优化后的跑步步态集成到一个完整的控制器中，使其能够完成从站立姿态启动和停止的100米短跑任务。", "result": "1. 成功优化了Cassie的跑步步态，实现了高速运行。2. 尽管Cassie与人类在形态上存在差异，但在广泛的速度范围内，其步态的关键特性与人类高度相似。3. 该控制器在硬件上得到验证，Cassie成功创造了双足机器人100米短跑的最快吉尼斯世界纪录。", "conclusion": "通过优化步态和集成控制器，双足机器人Cassie能够实现高效且类似人类的高速奔跑，并成功完成现实世界的100米短跑任务，创造了新的性能里程碑。"}}
{"id": "2508.02999", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02999", "abs": "https://arxiv.org/abs/2508.02999", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "Mónica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs.", "AI": {"tldr": "AGENTiGraph是一个用户友好的代理驱动系统，通过自然语言操作知识图谱，使非技术用户能够直观地管理领域特定数据，并支持多轮对话和动态更新，无需专业查询语言。", "motivation": "现有知识库管理系统对非技术用户不友好，需要专业查询语言，难以实现直观交互、增量构建和动态更新。该研究旨在为非技术用户提供一个完整的、可视化的解决方案，以自然语言方式管理和精炼知识库。", "method": "AGENTiGraph采用代理驱动设计，包含意图分类、任务规划和自动知识集成，以确保不同任务之间的无缝推理。它通过自然语言处理和知识图谱操作实现多轮对话和动态更新。", "result": "在教育场景中，系统在3500个查询的基准测试中表现优异，分类准确率达到95.12%，执行成功率达到90.45%，显著优于强大的零样本基线。这表明其在法律和医疗等合规性关键或多步骤查询领域具有潜在的可扩展性。", "conclusion": "AGENTiGraph为多轮企业知识管理提供了一种强大的新范式，有效连接了大型语言模型（LLMs）和结构化图谱。其在教育场景的成功表现预示着在法律和医疗等复杂领域具有广泛应用前景，能够动态整合新信息。"}}
{"id": "2508.03181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03181", "abs": "https://arxiv.org/abs/2508.03181", "authors": ["Lukas Pätz", "Moritz Beyer", "Jannik Späth", "Lasse Bohlen", "Patrick Zschech", "Mathias Kraus", "Julian Rosenberger"], "title": "Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification", "comment": "Accepted at 20th International Conference on Wirtschaftsinformatik\n  (WI25); September 2025, M\\\"unster, Germany", "summary": "This study investigates political discourse in the German parliament, the\nBundestag, by analyzing approximately 28,000 parliamentary speeches from the\nlast five years. Two machine learning models for topic and sentiment\nclassification were developed and trained on a manually labeled dataset. The\nmodels showed strong classification performance, achieving an area under the\nreceiver operating characteristic curve (AUROC) of 0.94 for topic\nclassification (average across topics) and 0.89 for sentiment classification.\nBoth models were applied to assess topic trends and sentiment distributions\nacross political parties and over time. The analysis reveals remarkable\nrelationships between parties and their role in parliament. In particular, a\nchange in style can be observed for parties moving from government to\nopposition. While ideological positions matter, governing responsibilities also\nshape discourse. The analysis directly addresses key questions about the\nevolution of topics, sentiment dynamics, and party-specific discourse\nstrategies in the Bundestag.", "AI": {"tldr": "本研究利用机器学习模型分析了德国联邦议院近五年的约2.8万份演讲，以揭示政党间的话题趋势、情感分布和话语策略演变。", "motivation": "研究旨在深入探讨德国议会中的政治话语，解答关于话题演变、情感动态以及政党特定话语策略的关键问题。", "method": "分析了约28,000份议会演讲。开发并训练了两个机器学习模型，分别用于话题和情感分类，模型在手动标注数据集上进行训练。训练后的模型被应用于评估不同政党和不同时间段的话题趋势和情感分布。", "result": "模型表现出强大的分类性能（话题分类AUROC为0.94，情感分类AUROC为0.89）。分析揭示了政党与其议会角色之间的显著关系。观察到政党从执政党变为反对党时，其话语风格发生变化。意识形态立场和执政责任共同塑造了政治话语。", "conclusion": "德国联邦议院的政治话语不仅受意识形态影响，也受执政责任塑造，政党角色的转变会导致话语风格的变化。本研究直接回答了关于议题演变、情感动态和政党特定话语策略的关键问题。"}}
{"id": "2508.02973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02973", "abs": "https://arxiv.org/abs/2508.02973", "authors": ["Alakh Desai", "Nuno Vasconcelos"], "title": "Diffusion Models with Adaptive Negative Sampling Without External Resources", "comment": null, "summary": "Diffusion models (DMs) have demonstrated an unparalleled ability to create\ndiverse and high-fidelity images from text prompts. However, they are also\nwell-known to vary substantially regarding both prompt adherence and quality.\nNegative prompting was introduced to improve prompt compliance by specifying\nwhat an image must not contain. Previous works have shown the existence of an\nideal negative prompt that can maximize the odds of the positive prompt. In\nthis work, we explore relations between negative prompting and classifier-free\nguidance (CFG) to develop a sampling procedure, {\\it Adaptive Negative Sampling\nWithout External Resources} (ANSWER), that accounts for both positive and\nnegative conditions from a single prompt. This leverages the internal\nunderstanding of negation by the diffusion model to increase the odds of\ngenerating images faithful to the prompt. ANSWER is a training-free technique,\napplicable to any model that supports CFG, and allows for negative grounding of\nimage concepts without an explicit negative prompts, which are lossy and\nincomplete. Experiments show that adding ANSWER to existing DMs outperforms the\nbaselines on multiple benchmarks and is preferred by humans 2x more over the\nother methods.", "AI": {"tldr": "本文提出了一种名为ANSWER的无训练方法，通过利用扩散模型对否定概念的内部理解，从单个提示中处理正负条件，从而在不使用外部负提示的情况下，显著提高了图像生成对提示的忠实度和质量。", "motivation": "扩散模型在生成高质量图像方面表现出色，但在提示依从性和图像质量上仍有显著差异。负向提示虽能提高依从性，但可能不完整且有损。本研究旨在开发一种更鲁棒的方法来提升提示依从性和图像质量，避免传统负向提示的局限性。", "method": "开发了ANSWER（Adaptive Negative Sampling Without External Resources）采样过程。该方法探索了负向提示与分类器自由引导（CFG）之间的关系，以从单个提示中同时处理正向和负向条件。它利用扩散模型对否定概念的内部理解来生成更忠实于提示的图像。ANSWER是一种无需训练的技术，适用于任何支持CFG的模型，并允许对图像概念进行负向接地，而无需显式的、有损且不完整的负向提示。", "result": "实验表明，将ANSWER添加到现有扩散模型中，其性能在多个基准测试上优于现有基线方法。此外，人类评估者对使用ANSWER生成图像的偏好度是其他方法的2倍。", "conclusion": "ANSWER是一种有效的、无需训练的技术，通过利用扩散模型对否定概念的内部理解，从单个提示中处理正负条件，显著提高了图像生成对提示的忠实度和图像质量，并消除了对显式负向提示的需求。"}}
{"id": "2508.03099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03099", "abs": "https://arxiv.org/abs/2508.03099", "authors": ["Sang Min Kim", "Hyeongjun Heo", "Junho Kim", "Yonghyeon Lee", "Young Min Kim"], "title": "Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping", "comment": null, "summary": "We propose Point2Act, which directly retrieves the 3D action point relevant\nfor a contextually described task, leveraging Multimodal Large Language Models\n(MLLMs). Foundation models opened the possibility for generalist robots that\ncan perform a zero-shot task following natural language descriptions within an\nunseen environment. While the semantics obtained from large-scale image and\nlanguage datasets provide contextual understanding in 2D images, the rich yet\nnuanced features deduce blurry 2D regions and struggle to find precise 3D\nlocations for actions. Our proposed 3D relevancy fields bypass the\nhigh-dimensional features and instead efficiently imbue lightweight 2D\npoint-level guidance tailored to the task-specific action. The multi-view\naggregation effectively compensates for misalignments due to geometric\nambiguities, such as occlusion, or semantic uncertainties inherent in the\nlanguage descriptions. The output region is highly localized, reasoning\nfine-grained 3D spatial context that can directly transfer to an explicit\nposition for physical action at the on-the-fly reconstruction of the scene. Our\nfull-stack pipeline, which includes capturing, MLLM querying, 3D\nreconstruction, and grasp pose extraction, generates spatially grounded\nresponses in under 20 seconds, facilitating practical manipulation tasks.\nProject page: https://sangminkim-99.github.io/point2act/", "AI": {"tldr": "Point2Act利用多模态大语言模型（MLLMs）直接从上下文描述中检索3D动作点，解决了现有方法在精确3D定位上的挑战。", "motivation": "尽管大语言模型为通用机器人提供了零样本任务执行能力，但从2D图像和语言数据中获得的语义理解难以推断出精确的3D动作位置，存在2D区域模糊、几何模糊（如遮挡）和语言描述中的语义不确定性等问题。", "method": "Point2Act绕过高维特征，通过“3D相关性场”高效地注入轻量级的2D点级引导，并结合多视图聚合来补偿几何和语义不确定性造成的错位。其完整堆栈流程包括捕获、MLLM查询、3D重建和抓取姿态提取。", "result": "该方法输出高度局部化的3D空间上下文，可直接转化为物理动作的精确位置，并在20秒内生成空间接地响应，促进了实际操作任务。", "conclusion": "Point2Act提供了一个高效且实用的解决方案，能够将自然语言描述精确地映射到3D动作点，从而实现机器人对场景的精确物理交互。"}}
{"id": "2508.03018", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03018", "abs": "https://arxiv.org/abs/2508.03018", "authors": ["Yutong Wang", "Pengliang Ji", "Kaixin Li", "Baolong Bi", "Tao Feng", "Guillaume Sartoretti"], "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning", "comment": null, "summary": "Large Language Reasoning Models have demonstrated remarkable success on\nstatic tasks, yet their application to multi-round agentic planning in\ninteractive environments faces two fundamental challenges. First, the\nintractable credit assignment problem renders conventional reinforcement\nlearning ineffective in sparse-reward settings. Second, the computational\noverhead of verbose, step-by-step reasoning histories is prohibitive. To\naddress these challenges, we propose BPO, a three-stage framework\n(bootstrapping, extrapolation, and refinement) that establishes a\nself-improving data flywheel to develop robust reasoning models for\nlong-horizon, sparse-reward environments. Our framework first bootstraps\nefficient reasoning using the proposed planning quaternions with long-short\nchain-of-thought fusion. It then extrapolates to out-of-distribution tasks\nthrough complexity-stratified curriculum learning. Finally, the model\niteratively refines itself by learning exclusively on experiences selected via\nreward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and\nWebShop demonstrate that our approach achieves state-of-the-art with\nsignificant token efficiency, providing a new recipe for reasoning models in\nagentic planning.", "AI": {"tldr": "BPO是一个三阶段自改进框架，通过规划四元数、课程学习和奖励门控拒绝采样，解决了大型语言模型在稀疏奖励交互式环境中进行多轮智能体规划时面临的信用分配和计算开销问题，实现了SOTA性能和高token效率。", "motivation": "大型语言模型在静态任务上表现出色，但在交互式环境中的多轮智能体规划面临两大挑战：1) 难以解决的信用分配问题导致传统强化学习在稀疏奖励设置下失效；2) 冗长、逐步的推理历史导致计算开销过大。", "method": "本文提出了BPO框架，包含三个阶段：1) 自举（bootstrapping）：使用规划四元数（planning quaternions）结合长短链式思考（long-short chain-of-thought fusion）来引导高效推理；2) 外推（extrapolation）：通过复杂度分层课程学习（complexity-stratified curriculum learning）将模型泛化到分布外任务；3) 精炼（refinement）：通过奖励门控拒绝采样（reward-gated rejection sampling）选择高质量经验进行迭代学习，实现模型自我改进。", "result": "在ALFWorld、ScienceWorld和WebShop上的实验表明，BPO方法实现了最先进的性能（state-of-the-art），并显著提高了token效率。", "conclusion": "BPO为智能体规划中的推理模型提供了一种新的范式，有效解决了长周期、稀疏奖励环境中大型语言模型面临的挑战。"}}
{"id": "2508.03199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03199", "abs": "https://arxiv.org/abs/2508.03199", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems.", "AI": {"tldr": "研究发现，文本到图像（T2I）模型中的语法性别会显著影响生成的视觉内容中的性别表示，导致超出刻板印象的偏差。", "motivation": "以往对T2I模型偏差的研究主要集中在人口统计学表示和刻板印象属性，忽略了语法性别如何跨语言影响视觉表示这一基本问题。", "method": "引入了一个跨语言基准，其中包含语法性别与刻板印象性别关联相矛盾的词语（例如，法语中语法为阴性但指代刻板印象为男性的“哨兵”）。数据集涵盖五种有性别语言（法语、西班牙语、德语、意大利语、俄语）和两种性别中立的对照语言（英语、中文），包含800个独特提示，在三种最先进的T2I模型上生成了28,800张图像。", "result": "分析显示，语法性别极大地影响图像生成：阳性语法标记平均将男性表示增加到73%（相比于性别中立英语的22%），而阴性语法标记将女性表示增加到38%（相比于英语的28%）。这些影响系统地因语言资源可用性和模型架构而异，高资源语言显示出更强的效果。", "conclusion": "研究结果表明，语言结构本身，而不仅仅是内容，塑造了AI生成的视觉输出，为理解多语言、多模态系统中的偏差和公平性引入了一个新维度。"}}
{"id": "2508.02978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02978", "abs": "https://arxiv.org/abs/2508.02978", "authors": ["Yusaku Takama", "Ning Ding", "Tatsuya Yokota", "Toru Tamaki"], "title": "Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning", "comment": "9 pages", "summary": "Existing architectures of multi-domain learning have two types of adapters:\nshared LoRA for all domains and domain-specific LoRA for each particular\ndomain. However, it remains unclear whether this structure effectively captures\ndomain-specific information. In this paper, we propose a method that ensures\nthat shared and domain-specific LoRAs exist in different subspaces;\nspecifically, the column and left null subspaces of the pre-trained weights. We\napply the proposed method to action recognition with three datasets (UCF101,\nKinetics400, and HMDB51) and demonstrate its effectiveness in some cases along\nwith the analysis of the dimensions of LoRA weights.", "AI": {"tldr": "本文提出一种多域学习方法，通过将共享LoRA和领域特定LoRA分别置于预训练权重的列空间和左零空间，确保它们存在于不同子空间，从而提升领域信息捕获能力，并在动作识别任务中验证了其有效性。", "motivation": "现有的多域学习架构中，共享LoRA和领域特定LoRA的组合未能明确有效地捕获领域特定信息，其结构是否有效仍不清楚。", "method": "提出一种新方法，确保共享LoRA和领域特定LoRA分别存在于预训练权重的列空间和左零空间这两个不同的子空间中。", "result": "将所提出的方法应用于动作识别任务，在UCF101、Kinetics400和HMDB51三个数据集上进行了实验，结果表明在某些情况下该方法有效，并对LoRA权重的维度进行了分析。", "conclusion": "通过将共享LoRA和领域特定LoRA分离到预训练权重的不同正交子空间中，可以更有效地捕获领域特定信息，从而提升多域学习的性能。"}}
{"id": "2508.03129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03129", "abs": "https://arxiv.org/abs/2508.03129", "authors": ["Le Qiu", "Yusuf Umut Ciftci", "Somil Bansal"], "title": "Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection", "comment": null, "summary": "Imitation Learning has provided a promising approach to learning complex\nrobot behaviors from expert demonstrations. However, learned policies can make\nerrors that lead to safety violations, which limits their deployment in\nsafety-critical applications. We propose MPC-SafeGIL, a design-time approach\nthat enhances the safety of imitation learning by injecting adversarial\ndisturbances during expert demonstrations. This exposes the expert to a broader\nrange of safety-critical scenarios and allows the imitation policy to learn\nrobust recovery behaviors. Our method uses sampling-based Model Predictive\nControl (MPC) to approximate worst-case disturbances, making it scalable to\nhigh-dimensional and black-box dynamical systems. In contrast to prior work\nthat relies on analytical models or interactive experts, MPC-SafeGIL integrates\nsafety considerations directly into data collection. We validate our approach\nthrough extensive simulations including quadruped locomotion and visuomotor\nnavigation and real-world experiments on a quadrotor, demonstrating\nimprovements in both safety and task performance. See our website here:\nhttps://leqiu2003.github.io/MPCSafeGIL/", "AI": {"tldr": "MPC-SafeGIL通过在专家演示中注入对抗性扰动来增强模仿学习的安全性，使策略学习鲁棒的恢复行为。", "motivation": "模仿学习策略可能导致安全违规，限制了其在安全关键应用中的部署。", "method": "提出MPC-SafeGIL，一种设计时方法，在专家演示期间注入对抗性扰动，使专家暴露于更广泛的安全关键场景。该方法使用基于采样的模型预测控制（MPC）来近似最坏情况扰动，适用于高维和黑盒动态系统。它将安全考虑直接整合到数据收集中。", "result": "通过四足运动和视觉运动导航的广泛模拟以及四旋翼的真实世界实验验证了该方法，证明了安全性和任务性能均有所提高。", "conclusion": "MPC-SafeGIL通过在数据收集阶段主动引入对抗性扰动，有效提升了模仿学习策略的安全性，并使其能学习到鲁棒的恢复行为。"}}
{"id": "2508.03030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03030", "abs": "https://arxiv.org/abs/2508.03030", "authors": ["Siyuan Li", "Yifan Yu", "Yanchen Deng", "Zhihao Zhang", "Mengjing Chen", "Fangzhou Zhu", "Tao Zhong", "Jianye Hao", "Peng Liu", "Bo An"], "title": "Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming", "comment": null, "summary": "Mixed-integer linear programming (MILP) has been a fundamental problem in\ncombinatorial optimization. Previous works have designed a plethora of\nhard-coded heuristics to accomplish challenging MILP solving with domain\nknowledge. Driven by the high capability of neural networks, recent research is\ndevoted to replacing manually designed heuristics with learned policies.\nAlthough learning-based MILP methods have shown great promise, existing\nworksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without\nconsidering their interdependence, severely hurting the solving speed and\nquality. To address this issue, we propose a novel multi-agent-based policy\nlearning framework for MILP (Collab-Solver), which can collaboratively optimize\nthe policies for multiple modules. Specifically, we formulate the collaboration\nof cut selection and branching in MILP solving as a Stackelberg game. Under\nthis formulation, we develop a two-phase learning paradigm to stabilize the\ncollaborative policy learning, where the first phase achieves the\ndata-communicated policy pretraining and the second phase further orchestrates\nthe policy learning for various modules. The jointly learned policy\nsignificantly improves the solving performance on both synthetic and\nlarge-scale real-world MILP datasets. Moreover, the policies learned by\nCollab-Solver have also demonstrated excellent generalization abilities across\ndifferent instance sets.", "AI": {"tldr": "该论文提出了一种名为Collab-Solver的新型多智能体协作策略学习框架，用于优化混合整数线性规划（MILP）求解器中的不同模块（如割平面选择和分支），以解决现有学习方法忽略模块间相互依赖的问题，从而显著提高求解速度、质量和泛化能力。", "motivation": "混合整数线性规划（MILP）是组合优化中的一个基本问题。传统的MILP求解器依赖于大量硬编码的启发式方法。尽管基于学习的MILP方法前景广阔，但现有工作独立地训练MILP求解器中各个模块的策略，忽略了它们之间的相互依赖性，严重影响了求解速度和质量。", "method": "本文提出了一种新颖的基于多智能体的MILP策略学习框架Collab-Solver，旨在协同优化多个模块的策略。具体而言，将MILP求解中割平面选择和分支的协作建模为Stackelberg博弈。在此框架下，开发了一个两阶段学习范式来稳定协作策略学习：第一阶段实现数据通信的策略预训练，第二阶段进一步协调各模块的策略学习。", "result": "联合学习的策略显著提高了在合成和大规模真实世界MILP数据集上的求解性能。此外，Collab-Solver学习到的策略在不同实例集上也表现出卓越的泛化能力。", "conclusion": "Collab-Solver框架通过将MILP求解中的关键模块（如割平面选择和分支）的策略学习建模为协作博弈，并采用两阶段学习范式，有效解决了现有学习方法中模块间独立优化的问题，从而显著提升了MILP的求解性能和泛化能力。"}}
{"id": "2508.03204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03204", "abs": "https://arxiv.org/abs/2508.03204", "authors": ["Abhirup Sinha", "Pritilata Saha", "Tithi Saha"], "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP", "comment": "To be published in the Proceedings of Die Studierendenkonferenz\n  Informatik (SKILL) 2024", "summary": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.", "AI": {"tldr": "本报告探讨了在大型语言模型训练数据中，为保护隐私而对文本数据进行匿名化的几种预处理方法。", "motivation": "现代大型语言模型需要海量数据，其中常包含私人信息，且这些信息可能被提取，从而侵犯个人隐私权并违反如GDPR等数据保护法规。", "method": "报告聚焦于讨论几种现有的、领域无关的文本数据预处理方法，这些方法旨在对私人信息进行掩盖或假名化处理。", "result": "研究表明，私人信息可以从语言模型中被提取。本报告的成果是综述和探讨了多种用于文本数据匿名化的预处理方法。", "conclusion": "即使无法实现完全匿名化，通过预处理方法对文本数据中的私人信息进行掩盖或假名化对于保护数据隐私至关重要，尤其是在处理大型语言模型所需数据时。"}}
{"id": "2508.02981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02981", "abs": "https://arxiv.org/abs/2508.02981", "authors": ["Takuya Sugimoto", "Ning Ding", "Toru Tamaki"], "title": "MoExDA: Domain Adaptation for Edge-based Action Recognition", "comment": "7 pages", "summary": "Modern action recognition models suffer from static bias, leading to reduced\ngeneralization performance. In this paper, we propose MoExDA, a lightweight\ndomain adaptation between RGB and edge information using edge frames in\naddition to RGB frames to counter the static bias issue. Experiments\ndemonstrate that the proposed method effectively suppresses static bias with a\nlower computational cost, allowing for more robust action recognition than\nprevious approaches.", "AI": {"tldr": "MoExDA是一种轻量级域适应方法，通过结合RGB和边缘帧来解决现代动作识别模型中的静态偏差问题，提升泛化性能。", "motivation": "现代动作识别模型存在静态偏差问题，导致泛化性能下降。", "method": "提出了MoExDA方法，这是一种轻量级的域适应方案。它在RGB帧的基础上，额外使用边缘帧信息来对抗静态偏差。", "result": "该方法有效抑制了静态偏差，计算成本更低，并且实现了比以往方法更鲁棒的动作识别。", "conclusion": "MoExDA通过利用边缘信息，以较低的计算成本有效解决了动作识别中的静态偏差问题，提高了模型的鲁棒性。"}}
{"id": "2508.03138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03138", "abs": "https://arxiv.org/abs/2508.03138", "authors": ["Mintaek Oh", "Chan Kim", "Seung-Woo Seo", "Seong-Woo Kim"], "title": "Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 8 pages, 7 figures", "summary": "Robots operating in human-centric or hazardous environments must proactively\nanticipate and mitigate dangers beyond basic obstacle detection. Traditional\nnavigation systems often depend on static maps, which struggle to account for\ndynamic risks, such as a person emerging from a suddenly opening door. As a\nresult, these systems tend to be reactive rather than anticipatory when\nhandling dynamic hazards. Recent advancements in pre-trained large language\nmodels and vision-language models (VLMs) create new opportunities for proactive\nhazard avoidance. In this work, we propose a zero-shot language-as-cost mapping\nframework that leverages VLMs to interpret visual scenes, assess potential\ndynamic risks, and assign risk-aware navigation costs preemptively, enabling\nrobots to anticipate hazards before they materialize. By integrating this\nlanguage-based cost map with a geometric obstacle map, the robot not only\nidentifies existing obstacles but also anticipates and proactively plans around\npotential hazards arising from environmental dynamics. Experiments in simulated\nand diverse dynamic environments demonstrate that the proposed method\nsignificantly improves navigation success rates and reduces hazard encounters,\ncompared to reactive baseline planners. Code and supplementary materials are\navailable at https://github.com/Taekmino/LaC.", "AI": {"tldr": "该论文提出一种零样本“语言即成本”映射框架，利用视觉-语言模型（VLM）解释视觉场景并预测动态风险，从而使机器人能够主动规避潜在危险，而非被动反应。", "motivation": "传统机器人导航系统依赖静态地图，难以处理动态风险（如突然开门后出现的人），导致其在应对动态危险时通常是被动而非主动的。预训练大型语言模型和视觉-语言模型的发展为主动避险提供了新机遇。", "method": "提出一个零样本“语言即成本”映射框架。该框架利用视觉-语言模型（VLMs）来解释视觉场景，评估潜在的动态风险，并预先分配风险感知导航成本。这些语言成本图与几何障碍图结合，使机器人不仅识别现有障碍物，还能预测并主动规划规避环境动态产生的潜在危险。", "result": "在模拟和多样化的动态环境中进行的实验表明，与被动基线规划器相比，所提出的方法显著提高了导航成功率并减少了危险遭遇。", "conclusion": "通过整合基于语言的成本图和几何障碍图，机器人能够主动预测并规划规避潜在的动态环境危险，从而显著提升导航安全性和效率。"}}
{"id": "2508.03031", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03031", "abs": "https://arxiv.org/abs/2508.03031", "authors": ["Ziyang Ma", "Baojian Zhou", "Deqing Yang", "Yanghua Xiao"], "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a new paradigm in large language\nmodels (LLMs), enabling them to perform novel tasks by conditioning on a few\nexamples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for\nNLP tasks remains poorly understood. To shed light on its underlying\nmechanisms, this paper investigates whether LLMs can solve ordinary\ndifferential equations (ODEs) under the ICL setting. We formulate standard ODE\nproblems and their solutions as sequential prompts and evaluate GPT-2 models on\nthese tasks. Experiments on two types of ODEs show that GPT-2 can effectively\nlearn a meta-ODE algorithm, with convergence behavior comparable to, or better\nthan, the Euler method, and achieve exponential accuracy gains with increasing\nnumbers of demonstrations. Moreover, the model generalizes to\nout-of-distribution (OOD) problems, demonstrating robust extrapolation\ncapabilities. These empirical findings provide new insights into the mechanisms\nof ICL in NLP and its potential for solving nonlinear numerical problems.", "AI": {"tldr": "研究了大型语言模型（LLMs）在语境学习（ICL）设置下解决常微分方程（ODEs）的能力，发现GPT-2能有效学习并展现出高精度和泛化能力。", "motivation": "语境学习（ICL）在大型语言模型（LLMs）中的非线性行为及其底层机制尚不清楚，尤其是在自然语言处理（NLP）任务中。本文旨在通过让LLMs解决常微分方程（ODEs）来深入理解ICL的工作原理。", "method": "将标准常微分方程问题及其解格式化为顺序提示。使用GPT-2模型进行评估，并在两种类型的常微分方程上进行实验。", "result": "GPT-2模型能够有效地学习一种元ODE算法，其收敛行为与欧拉方法相当或更优。随着演示数量的增加，模型精度呈指数级提高。此外，模型对分布外（OOD）问题也表现出良好的泛化能力和鲁棒的推断能力。", "conclusion": "这些实证发现为自然语言处理中语境学习的机制及其解决非线性数值问题的潜力提供了新的见解。"}}
{"id": "2508.03211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03211", "abs": "https://arxiv.org/abs/2508.03211", "authors": ["Pablo J. Diego-Simón", "Emmanuel Chemla", "Jean-Rémi King", "Yair Lakretz"], "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance.", "AI": {"tldr": "结构探针在评估大型语言模型（LLM）的句法表示时，存在距离偏见、难以处理深层结构和干扰词等问题，且不受词语可预测性影响。", "motivation": "现有的结构探针通常在未加区分的句子集上进行评估，导致不清楚结构和/或统计因素是否系统性地影响LLM的句法表示。本研究旨在深入分析此问题。", "method": "在三个受控基准上对结构探针进行了深入分析，以评估其性能和受到的影响。", "result": "研究发现三点：1) 结构探针受词语距离影响，距离越近越可能被认为是句法关联；2) 结构探针难以表示深层句法结构，并受交互名词或不合语法动词形式的干扰；3) 结构探针似乎不受单个词语可预测性的影响。", "conclusion": "这项工作揭示了结构探针当前面临的挑战，并强调了使用受控刺激基准来更好评估其性能的重要性。"}}
{"id": "2508.02987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02987", "abs": "https://arxiv.org/abs/2508.02987", "authors": ["Zachary Yahn", "Selim Furkan Tekin", "Fatih Ilhan", "Sihao Hu", "Tiansheng Huang", "Yichang Xu", "Margaret Loper", "Ling Liu"], "title": "Adversarial Attention Perturbations for Large Object Detection Transformers", "comment": "ICCV 2025", "summary": "Adversarial perturbations are useful tools for exposing vulnerabilities in\nneural networks. Existing adversarial perturbation methods for object detection\nare either limited to attacking CNN-based detectors or weak against\ntransformer-based detectors. This paper presents an Attention-Focused Offensive\nGradient (AFOG) attack against object detection transformers. By design, AFOG\nis neural-architecture agnostic and effective for attacking both large\ntransformer-based object detectors and conventional CNN-based detectors with a\nunified adversarial attention framework. This paper makes three original\ncontributions. First, AFOG utilizes a learnable attention mechanism that\nfocuses perturbations on vulnerable image regions in multi-box detection tasks,\nincreasing performance over non-attention baselines by up to 30.6%. Second,\nAFOG's attack loss is formulated by integrating two types of feature loss\nthrough learnable attention updates with iterative injection of adversarial\nperturbations. Finally, AFOG is an efficient and stealthy adversarial\nperturbation method. It probes the weak spots of detection transformers by\nadding strategically generated and visually imperceptible perturbations which\ncan cause well-trained object detection models to fail. Extensive experiments\nconducted with twelve large detection transformers on COCO demonstrate the\nefficacy of AFOG. Our empirical results also show that AFOG outperforms\nexisting attacks on transformer-based and CNN-based object detectors by up to\n83% with superior speed and imperceptibility. Code is available at\nhttps://github.com/zacharyyahn/AFOG.", "AI": {"tldr": "本文提出了一种名为AFOG的注意力聚焦对抗性梯度攻击方法，专门针对目标检测Transformer，但也能有效攻击基于CNN的检测器，通过可学习的注意力机制将扰动集中在脆弱区域，实现高效且隐蔽的攻击。", "motivation": "现有的目标检测对抗性扰动方法要么仅限于攻击基于CNN的检测器，要么对基于Transformer的检测器效果不佳，因此需要一种对Transformer模型同样有效的通用攻击方法。", "method": "AFOG通过以下方式实现攻击：1) 利用可学习的注意力机制，将扰动集中在多框检测任务中图像的脆弱区域；2) 攻击损失通过两种特征损失的集成来制定，并通过可学习的注意力更新和对抗性扰动的迭代注入来优化；3) 该方法是神经架构无关的，对CNN和Transformer模型都有效。", "result": "AFOG在COCO数据集上针对十二个大型检测Transformer进行了广泛实验，结果表明：1) 相比非注意力基线，其性能提升高达30.6%；2) AFOG在Transformer和CNN检测器上的攻击效果优于现有方法高达83%；3) AFOG攻击高效、隐蔽，能够以视觉上难以察觉的扰动使训练有素的模型失效，且具有更快的速度和更好的不可感知性。", "conclusion": "AFOG是一种有效、高效且隐蔽的对抗性扰动方法，能够成功攻击包括大型Transformer在内的各种目标检测模型，并通过探测其弱点，导致模型失败，显著优于现有方法。"}}
{"id": "2508.03232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03232", "abs": "https://arxiv.org/abs/2508.03232", "authors": ["Muzhen Cai", "Xiubo Chen", "Yining An", "Jiaxin Zhang", "Xuesong Wang", "Wang Xu", "Weinan Zhang", "Ting Liu"], "title": "CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios", "comment": "9 pages, 5 figures", "summary": "Embodied Planning is dedicated to the goal of creating agents capable of\nexecuting long-horizon tasks in complex physical worlds. However, existing\nembodied planning benchmarks frequently feature short-horizon tasks and\ncoarse-grained action primitives. To address this challenge, we introduce\nCookBench, a benchmark for long-horizon planning in complex cooking scenarios.\nBy leveraging a high-fidelity simulation environment built upon the powerful\nUnity game engine, we define frontier AI challenges in a complex, realistic\nenvironment. The core task in CookBench is designed as a two-stage process.\nFirst, in Intention Recognition, an agent needs to accurately parse a user's\ncomplex intent. Second, in Embodied Interaction, the agent should execute the\nidentified cooking goal through a long-horizon, fine-grained sequence of\nphysical actions. Unlike existing embodied planning benchmarks, we refine the\naction granularity to a spatial level that considers crucial operational\ninformation while abstracting away low-level robotic control. Besides, We\nprovide a comprehensive toolset that encapsulates the simulator. Its unified\nAPI supports both macro-level operations, such as placing orders and purchasing\ningredients, and a rich set of fine-grained embodied actions for physical\ninteraction, enabling researchers to focus on high-level planning and\ndecision-making. Furthermore, we present an in-depth analysis of\nstate-of-the-art, closed-source Large Language Model and Vision-Language Model,\nrevealing their major shortcomings and challenges posed by complex,\nlong-horizon tasks. The full benchmark will be open-sourced to facilitate\nfuture research.", "AI": {"tldr": "CookBench是一个新的基准，旨在解决现有具身规划基准任务周期短、动作粒度粗的问题，通过在复杂的烹饪场景中提供长周期、细粒度的具身规划任务，并揭示了现有大型语言模型和视觉语言模型在此类任务中的局限性。", "motivation": "现有的具身规划基准通常任务周期短且动作原语粗糙，无法有效模拟和解决复杂物理世界中的长周期任务挑战。", "method": "引入CookBench，一个基于Unity高保真模拟环境的烹饪场景基准。核心任务分为两阶段：意图识别和具身交互。动作粒度被细化到考虑关键操作信息的空间级别，同时抽象了低级机器人控制。提供了一个包含宏观操作和细粒度具身动作的统一API工具集。对最先进的闭源大型语言模型和视觉语言模型进行了深入分析。", "result": "通过CookBench的分析，揭示了最先进的闭源大型语言模型和视觉语言模型在处理复杂、长周期任务时存在主要缺陷和挑战。", "conclusion": "CookBench为具身规划领域提供了一个具有挑战性的新基准，专注于长周期和细粒度任务，有助于推动具身智能研究，并揭示了当前AI模型在处理此类复杂任务时的局限性，从而促进未来的研究。"}}
{"id": "2508.03038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03038", "abs": "https://arxiv.org/abs/2508.03038", "authors": ["Qi Peng", "Jialin Cui", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree", "comment": "Accepted by ACM MM 2025", "summary": "Large language models (LLMs) have shown great potential in the medical\ndomain. However, existing models still fall short when faced with complex\nmedical diagnosis task in the real world. This is mainly because they lack\nsufficient reasoning depth, which leads to information loss or logical jumps\nwhen processing a large amount of specialized medical data, leading to\ndiagnostic errors. To address these challenges, we propose Tree-of-Reasoning\n(ToR), a novel multi-agent framework designed to handle complex scenarios.\nSpecifically, ToR introduces a tree structure that can clearly record the\nreasoning path of LLMs and the corresponding clinical evidence. At the same\ntime, we propose a cross-validation mechanism to ensure the consistency of\nmulti-agent decision-making, thereby improving the clinical reasoning ability\nof multi-agents in complex medical scenarios. Experimental results on\nreal-world medical data show that our framework can achieve better performance\nthan existing baseline methods.", "AI": {"tldr": "针对LLMs在复杂医疗诊断中推理深度不足的问题，本文提出了Tree-of-Reasoning (ToR)多智能体框架，通过树状推理路径记录和交叉验证机制，显著提升了诊断性能。", "motivation": "现有大型语言模型（LLMs）在处理复杂现实世界医疗诊断任务时表现不足，主要原因是缺乏足够的推理深度，导致处理大量专业医疗数据时信息丢失或逻辑跳跃，进而引发诊断错误。", "method": "提出了Tree-of-Reasoning (ToR)多智能体框架，旨在处理复杂医疗场景。ToR引入了一个树状结构，用于清晰记录LLMs的推理路径和相应的临床证据。同时，提出了一种交叉验证机制，以确保多智能体决策的一致性，从而提高在复杂医疗场景下的临床推理能力。", "result": "在真实世界医疗数据上的实验结果表明，该框架能够比现有基线方法取得更好的性能。", "conclusion": "ToR框架通过增强推理深度和多智能体决策一致性，有效提升了LLMs在复杂医疗诊断任务中的临床推理能力和性能。"}}
{"id": "2508.03240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03240", "abs": "https://arxiv.org/abs/2508.03240", "authors": ["Mutaz Ayesh", "Nicolás Gutiérrez-Rolón", "Fernando Alva-Manchego"], "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting", "comment": null, "summary": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results.", "AI": {"tldr": "本文介绍了CardiffNLP团队在CLEARS西班牙语文本改编共享任务中的贡献，采用LLM提示方法，最终使用Gemma-3模型，在两个子任务中分别获得第三名和第二名。", "motivation": "参与IberLEF 2025主办的CLEARS西班牙语文本改编共享任务，并解决其包含的两个子任务。", "method": "采用LLM（大型语言模型）提示方法，并尝试了不同的提示变体。初期实验使用了LLaMA-3.2，但最终提交采用了Gemma-3模型。", "result": "在子任务1中获得第三名，在子任务2中获得第二名。论文详细描述了团队的多种提示变体、示例和实验结果。", "conclusion": "基于LLM提示的方法，特别是使用Gemma-3模型，在西班牙语文本改编任务中表现出色，取得了较高的排名。"}}
{"id": "2508.03006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03006", "abs": "https://arxiv.org/abs/2508.03006", "authors": ["Fan Yang", "Yihao Huang", "Jiayi Zhu", "Ling Shi", "Geguang Pu", "Jin Song Dong", "Kailong Wang"], "title": "Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models", "comment": "8 pages", "summary": "Diffusion-based text-to-image (T2I) models enable high-quality image\ngeneration but also pose significant risks of misuse, particularly in producing\nnot-safe-for-work (NSFW) content. While prior detection methods have focused on\nfiltering prompts before generation or moderating images afterward, the\nin-generation phase of diffusion models remains largely unexplored for NSFW\ndetection. In this paper, we introduce In-Generation Detection (IGD), a simple\nyet effective approach that leverages the predicted noise during the diffusion\nprocess as an internal signal to identify NSFW content. This approach is\nmotivated by preliminary findings suggesting that the predicted noise may\ncapture semantic cues that differentiate NSFW from benign prompts, even when\nthe prompts are adversarially crafted. Experiments conducted on seven NSFW\ncategories show that IGD achieves an average detection accuracy of 91.32% over\nnaive and adversarial NSFW prompts, outperforming seven baseline methods.", "AI": {"tldr": "本文提出了一种名为In-Generation Detection (IGD)的新方法，通过利用扩散模型生成过程中预测的噪声作为内部信号，有效检测文本到图像（T2I）模型生成的NSFW（不安全工作内容）。", "motivation": "文本到图像（T2I）模型能生成高质量图像，但也存在滥用风险，特别是生成NSFW内容。现有检测方法主要集中于生成前过滤提示或生成后审核图像，而扩散模型的“生成中”阶段在NSFW检测方面仍未被充分探索。初步研究表明，预测噪声可能捕捉到区分NSFW和良性提示的语义线索，即使在对抗性提示下也如此。", "method": "引入了In-Generation Detection (IGD)方法，该方法利用扩散过程中预测的噪声作为内部信号来识别NSFW内容。", "result": "在七个NSFW类别上进行的实验表明，IGD在幼稚和对抗性NSFW提示上的平均检测准确率达到91.32%，优于七种基线方法。", "conclusion": "IGD是一种简单而有效的在扩散模型生成过程中检测NSFW内容的方法，通过利用预测噪声作为内部信号，表现出卓越的检测性能。"}}
{"id": "2508.03246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03246", "abs": "https://arxiv.org/abs/2508.03246", "authors": ["Zehua Fan", "Feng Gao", "Zhijun Chen", "Yunpeng Yin", "Limin Yang", "Qingxing Xi", "En Yang", "Xuefeng Luo"], "title": "Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots", "comment": null, "summary": "Guiding the visually impaired in complex environments requires real-time\ntwo-way interaction and safety assurance. We propose a Force-Compliance Model\nPredictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for\nforce-compliant navigation and obstacle avoidance in Hexapod guide robots.\nFC-MPC enables two-way interaction by estimating user-applied forces and\nmoments using the robot's dynamic model and the recursive least squares (RLS)\nmethod, and then adjusting the robot's movements accordingly, while Robot-User\nCBFs ensure the safety of both the user and the robot by handling static and\ndynamic obstacles, and employ weighted slack variables to overcome feasibility\nissues in complex dynamic environments. We also adopt an Eight-Way Connected\nDBSCAN method for obstacle clustering, reducing computational complexity from\nO(n2) to approximately O(n), enabling real-time local perception on\nresource-limited on-board robot computers. Obstacles are modeled using Minimum\nBounding Ellipses (MBEs), and their trajectories are predicted through Kalman\nfiltering. Implemented on the HexGuide robot, the system seamlessly integrates\nforce compliance, autonomous navigation, and obstacle avoidance. Experimental\nresults demonstrate the system's ability to adapt to user force commands while\nguaranteeing user and robot safety simultaneously during navigation in complex\nenvironments.", "AI": {"tldr": "该研究提出了一种针对六足导盲机器人的力-顺从模型预测控制（FC-MPC）和机器人-用户控制障碍函数（CBFs）方法，以实现在复杂环境中与用户的双向交互、安全导航和避障，并采用高效的障碍物感知技术。", "motivation": "在复杂环境中引导视障人士需要实时的双向交互和严格的安全保障。", "method": "该研究提出了以下方法：1) **力-顺从模型预测控制（FC-MPC）**：通过机器人动力学模型和递归最小二乘法（RLS）估计用户施加的力和力矩，并相应调整机器人运动，实现双向交互。2) **机器人-用户控制障碍函数（CBFs）**：确保用户和机器人安全，处理静态和动态障碍物，并采用加权松弛变量解决复杂动态环境中的可行性问题。3) **八向连接DBSCAN方法**：用于障碍物聚类，将计算复杂度从O(n²)降低到大约O(n)，实现资源有限的机载机器人计算机上的实时局部感知。4) **障碍物建模**：使用最小边界椭圆（MBEs）。5) **轨迹预测**：通过卡尔曼滤波进行。系统在HexGuide机器人上实现。", "result": "该系统无缝集成了力顺从性、自主导航和避障功能。实验结果表明，系统能够在复杂环境中根据用户力指令进行调整，同时保证用户和机器人的安全。", "conclusion": "所提出的系统通过结合力顺从性、安全保障和高效的实时障碍物处理，成功实现了在复杂环境中对视障人士的有效引导。"}}
{"id": "2508.03054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03054", "abs": "https://arxiv.org/abs/2508.03054", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Litian Zhang", "Lirong Qiu", "Xi Zhang"], "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning", "comment": null, "summary": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks.", "AI": {"tldr": "本文提出认知驱动防御（CDD）框架，通过模拟人类认知推理链和应用元操作来防御大型语言模型（LLMs）的越狱攻击，结合监督微调和熵引导强化学习以实现对未知攻击的泛化。", "motivation": "现有防御方法多依赖浅层模式匹配，难以泛化到新颖的越狱攻击策略，而LLMs的安全可靠部署需要更鲁棒的防御机制。", "method": "CDD框架通过应用元操作（隐藏恶意意图的基本操作）来针对越狱提示的底层结构。它模拟人类认知推理，包括全局感知和局部分析以揭示隐藏操作。通过对结构化推理链进行监督微调，模型学习识别已知操作模式。为增强对未知威胁的泛化能力，引入熵引导强化学习算法（EG-GRPO）鼓励探索新类型的元操作。", "result": "实验证明，CDD能够实现最先进的防御性能，并对未见的越狱攻击表现出强大的泛化能力。", "conclusion": "CDD框架通过其独特的认知推理和学习机制，为防御LLMs越狱攻击提供了一种高效且泛化能力强的解决方案。"}}
{"id": "2508.03247", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.03247", "abs": "https://arxiv.org/abs/2508.03247", "authors": ["Shintaro Sakai", "Jisun An", "Migyeong Kang", "Haewoon Kwak"], "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs", "comment": null, "summary": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications.", "AI": {"tldr": "研究发现，大型语言模型（LLMs）在模拟西方和东方抑郁症症状报告的文化差异方面表现不佳，即使使用文化角色提示。尽管使用东方语言提示有所改善，但模型对文化角色敏感度低且存在强烈的文化无关症状等级是主要原因，表明当前LLMs不具备安全的心理健康应用所需的文化意识能力。", "motivation": "先前的临床心理学研究表明，西方抑郁症患者倾向于报告心理症状，而东方患者则报告躯体症状。鉴于LLMs在心理健康领域日益增长的应用，研究旨在测试LLMs是否能再现这些文化模式。", "method": "通过使用西方或东方角色提示LLMs，并在英语以及主要东方语言（如中文、日语和印地语）中进行测试，以观察LLMs是否能复现抑郁症症状报告的文化模式。", "result": "结果显示，LLMs在英语提示下未能很好地复制这些模式。尽管使用主要东方语言提示时，在某些配置下有所改善，但LLMs未能完全对齐。分析指出失败的两个主要原因：模型对文化角色的敏感度低，以及存在一个强大的、文化不变的症状等级，该等级覆盖了文化线索。", "conclusion": "研究得出结论，虽然提示语言很重要，但当前的通用LLMs缺乏强大、具有文化意识的能力，而这些能力对于安全有效的心理健康应用至关重要。"}}
{"id": "2508.03007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03007", "abs": "https://arxiv.org/abs/2508.03007", "authors": ["Xinhui Li", "Xiaojie Guo"], "title": "Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation", "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) aims to improve the\ngeneralization ability of models across unseen domains without access to target\ndata during training. Recent advances in DGSS have increasingly exploited\nvision foundation models (VFMs) via parameter-efficient fine-tuning strategies.\nHowever, most existing approaches concentrate on global feature fine-tuning,\nwhile overlooking hierarchical adaptation across feature levels, which is\ncrucial for precise dense prediction. In this paper, we propose\nMulti-Granularity Feature Calibration (MGFC), a novel framework that performs\ncoarse-to-fine alignment of VFM features to enhance robustness under domain\nshifts. Specifically, MGFC first calibrates coarse-grained features to capture\nglobal contextual semantics and scene-level structure. Then, it refines\nmedium-grained features by promoting category-level feature discriminability.\nFinally, fine-grained features are calibrated through high-frequency spatial\ndetail enhancement. By performing hierarchical and granularity-aware\ncalibration, MGFC effectively transfers the generalization strengths of VFMs to\nthe domain-specific task of DGSS. Extensive experiments on benchmark datasets\ndemonstrate that our method outperforms state-of-the-art DGSS approaches,\nhighlighting the effectiveness of multi-granularity adaptation for the semantic\nsegmentation task of domain generalization.", "AI": {"tldr": "本文提出多粒度特征校准（MGFC）框架，通过对视觉基础模型（VFMs）进行粗到细的特征对齐，以增强领域泛化语义分割（DGSS）在领域迁移下的鲁棒性。", "motivation": "现有的领域泛化语义分割方法在利用视觉基础模型时，主要关注全局特征微调，却忽略了对密集预测至关重要的跨特征层级适应，导致在未见领域泛化能力不足。", "method": "提出多粒度特征校准（MGFC）框架，通过分层和粒度感知的方式校准VFM特征。具体包括：首先校准粗粒度特征以捕获全局上下文语义和场景级结构；其次通过促进类别级特征判别性来优化中粒度特征；最后通过高频空间细节增强来校准细粒度特征。", "result": "在基准数据集上的广泛实验表明，MGFC方法优于现有最先进的DGSS方法。", "conclusion": "多粒度适应对于领域泛化语义分割任务是有效的，MGFC通过分层和粒度感知校准，成功将VFMs的泛化能力转移到DGSS任务中，显著提升了模型在领域迁移下的鲁棒性。"}}
{"id": "2508.03408", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03408", "abs": "https://arxiv.org/abs/2508.03408", "authors": ["Ivana Collado-Gonzalez", "John McConnell", "Paul Szenher", "Brendan Englot"], "title": "Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments", "comment": null, "summary": "Scene reconstruction is an essential capability for underwater robots\nnavigating in close proximity to structures. Monocular vision-based\nreconstruction methods are unreliable in turbid waters and lack depth scale\ninformation. Sonars are robust to turbid water and non-uniform lighting\nconditions, however, they have low resolution and elevation ambiguity. This\nwork proposes a real-time opti-acoustic scene reconstruction method that is\nspecially optimized to work in turbid water. Our strategy avoids having to\nidentify point features in visual data and instead identifies regions of\ninterest in the data. We then match relevant regions in the image to\ncorresponding sonar data. A reconstruction is obtained by leveraging range data\nfrom the sonar and elevation data from the camera image. Experimental\ncomparisons against other vision-based and sonar-based approaches at varying\nturbidity levels, and field tests conducted in marina environments, validate\nthe effectiveness of the proposed approach. We have made our code open-source\nto facilitate reproducibility and encourage community engagement.", "AI": {"tldr": "提出了一种实时光声融合的水下场景重建方法，专为浑浊水域优化，克服了单一视觉或声纳方法的局限性。", "motivation": "水下机器人近距离导航需要场景重建能力。单目视觉方法在浑浊水域不可靠且缺乏深度尺度信息；声纳虽然耐受浑浊水和非均匀光照，但分辨率低且存在高程模糊性。", "method": "提出了一种实时光声场景重建方法。该方法避免识别视觉数据中的点特征，转而识别数据中的感兴趣区域，并将图像中的相关区域与对应的声纳数据进行匹配。通过利用声纳的距离数据和相机图像的高程数据来获得场景重建。", "result": "在不同浑浊度水平下，与其它基于视觉和基于声纳的方法进行了实验比较，并在码头环境中进行了现场测试，验证了所提出方法的有效性。", "conclusion": "所提出的光声融合方法能够有效克服传统单一视觉或声纳方法在浑浊水域进行水下场景重建时的局限性，实现了鲁棒且有效的重建。"}}
{"id": "2508.03080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03080", "abs": "https://arxiv.org/abs/2508.03080", "authors": ["Shuang Liu", "Zelong Li", "Ruoyun Ma", "Haiyan Zhao", "Mengnan Du"], "title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts", "comment": null, "summary": "The potential of large language models (LLMs) in specialized domains such as\nlegal risk analysis remains underexplored. In response to growing interest in\nlocally deploying open-source LLMs for legal tasks while preserving data\nconfidentiality, this paper introduces ContractEval, the first benchmark to\nthoroughly evaluate whether open-source LLMs could match proprietary LLMs in\nidentifying clause-level legal risks in commercial contracts. Using the\nContract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15\nopen-source LLMs. Our results highlight five key findings: (1) Proprietary\nmodels outperform open-source models in both correctness and output\neffectiveness, though some open-source models are competitive in certain\nspecific dimensions. (2) Larger open-source models generally perform better,\nthough the improvement slows down as models get bigger. (3) Reasoning\n(\"thinking\") mode improves output effectiveness but reduces correctness, likely\ndue to over-complicating simpler tasks. (4) Open-source models generate \"no\nrelated clause\" responses more frequently even when relevant clauses are\npresent. This suggests \"laziness\" in thinking or low confidence in extracting\nrelevant content. (5) Model quantization speeds up inference but at the cost of\nperformance drop, showing the tradeoff between efficiency and accuracy. These\nfindings suggest that while most LLMs perform at a level comparable to junior\nlegal assistants, open-source models require targeted fine-tuning to ensure\ncorrectness and effectiveness in high-stakes legal settings. ContractEval\noffers a solid benchmark to guide future development of legal-domain LLMs.", "AI": {"tldr": "本研究引入ContractEval基准，评估开源大型语言模型在合同法律风险分析中与专有模型的性能差距，发现专有模型表现更优，开源模型虽有潜力但需针对性微调。", "motivation": "大型语言模型在法律风险分析等专业领域潜力未被充分探索。随着对本地部署开源LLM以保护数据机密性的兴趣日益增长，需要评估开源LLM在法律任务中是否能与专有LLM媲美。", "method": "引入ContractEval基准，首次全面评估LLM在识别商业合同中条款级法律风险的能力。使用合同理解Atticus数据集（CUAD），评估了4个专有LLM和15个开源LLM，分析了正确性、输出有效性、推理模式、无相关条款响应频率以及模型量化的影响。", "result": "1. 专有模型在正确性和输出有效性上优于开源模型，但部分开源模型在特定维度具有竞争力。2. 更大的开源模型通常表现更好，但性能提升随模型增大而放缓。3. 推理模式（“思考”模式）提高了输出有效性，但降低了正确性。4. 开源模型即使存在相关条款也更频繁地生成“无相关条款”响应。5. 模型量化加速了推理但牺牲了性能。", "conclusion": "大多数LLM的表现与初级法律助理相当，但开源模型需要有针对性的微调以确保在关键法律环境中的正确性和有效性。ContractEval为未来法律领域LLM的开发提供了坚实的基准。"}}
{"id": "2508.03250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03250", "abs": "https://arxiv.org/abs/2508.03250", "authors": ["Deborah Dore", "Elena Cabrio", "Serena Villata"], "title": "RooseBERT: A New Deal For Political Language Modelling", "comment": null, "summary": "The increasing amount of political debates and politics-related discussions\ncalls for the definition of novel computational methods to automatically\nanalyse such content with the final goal of lightening up political\ndeliberation to citizens. However, the specificity of the political language\nand the argumentative form of these debates (employing hidden communication\nstrategies and leveraging implicit arguments) make this task very challenging,\neven for current general-purpose pre-trained Language Models. To address this\nissue, we introduce a novel pre-trained Language Model for political discourse\nlanguage called RooseBERT. Pre-training a language model on a specialised\ndomain presents different technical and linguistic challenges, requiring\nextensive computational resources and large-scale data. RooseBERT has been\ntrained on large political debate and speech corpora (8K debates, each composed\nof several sub-debates on different topics) in English. To evaluate its\nperformances, we fine-tuned it on four downstream tasks related to political\ndebate analysis, i.e., named entity recognition, sentiment analysis, argument\ncomponent detection and classification, and argument relation prediction and\nclassification. Our results demonstrate significant improvements over\ngeneral-purpose Language Models on these four tasks, highlighting how\ndomain-specific pre-training enhances performance in political debate analysis.\nWe release the RooseBERT language model for the research community.", "AI": {"tldr": "本文提出了RooseBERT，一个专门用于政治话语分析的预训练语言模型，它在政治辩论分析任务上显著优于通用语言模型。", "motivation": "政治辩论和相关讨论日益增多，需要新的计算方法来自动分析这些内容，以帮助公民理解政治审议。然而，政治语言的特殊性及其论证形式（包含隐藏的沟通策略和隐含论点）使得即使是当前的通用预训练语言模型也难以有效处理。", "method": "研究者引入了RooseBERT，一个针对政治话语语言的预训练语言模型。该模型在大型英语政治辩论和演讲语料库（8K场辩论）上进行训练。为了评估其性能，研究者将其在四个政治辩论分析下游任务上进行了微调，包括命名实体识别、情感分析、论证组件检测和分类，以及论证关系预测和分类。", "result": "RooseBERT在所有四个下游任务上均表现出比通用语言模型显著的性能提升，这表明领域特定的预训练能有效增强政治辩论分析的性能。", "conclusion": "领域特定的预训练能够显著提高政治辩论分析任务的性能。RooseBERT作为专门的政治话语语言模型，在相关任务上超越了通用模型。该模型已向研究社区发布。"}}
{"id": "2508.03009", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03009", "abs": "https://arxiv.org/abs/2508.03009", "authors": ["Xuyi Yang", "Wenhao Zhang", "Hongbo Jin", "Lin Liu", "Hongbo Xu", "Yongwei Nie", "Fei Yu", "Fei Ma"], "title": "Enhancing Long Video Question Answering with Scene-Localized Frame Grouping", "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) often perform poorly in long\nvideo understanding, primarily due to resource limitations that prevent them\nfrom processing all video frames and their associated information. Efficiently\nextracting relevant information becomes a challenging task. Existing frameworks\nand evaluation tasks focus on identifying specific frames containing core\nobjects from a large number of irrelevant frames, which does not align with the\npractical needs of real-world applications. To address this issue, we propose a\nnew scenario under the video question-answering task, SceneQA, which emphasizes\nscene-based detail perception and reasoning abilities. And we develop the LVSQA\ndataset to support the SceneQA task, which is built upon carefully selected\nvideos from LVBench and contains a new collection of question-answer pairs to\npromote a more fair evaluation of MLLMs' scene perception abilities in long\nvideos. Inspired by human cognition, we introduce a novel method called SLFG.\nThe core idea of SLFG is to combine individual frames into semantically\ncoherent scene frames. By leveraging scene localization methods and dynamic\nframe reassembly mechanisms, SLFG significantly enhances the understanding\ncapabilities of existing MLLMs in long videos. SLFG requires no modification to\nthe original model architecture and boasts excellent plug-and-play usability.\nExperimental results show that this method performs exceptionally well in\nseveral long video benchmark tests. Code and dataset will be released at\nhttp://www.slfg.pkuzwh.cn.", "AI": {"tldr": "针对多模态大语言模型（MLLMs）在长视频理解中的不足，本文提出了一个新的场景问答任务SceneQA和数据集LVSQA，并引入了SLFG方法。SLFG通过将独立帧组合成语义连贯的场景帧，显著提升了现有MLLMs在长视频理解上的能力，且无需修改模型架构，即插即用。", "motivation": "当前MLLMs在长视频理解方面表现不佳，主要原因是资源限制导致无法处理所有视频帧及其信息，难以高效提取相关信息。现有框架和评估任务侧重于识别包含核心对象的特定帧，与实际应用中的场景理解需求不符。", "method": "1. 提出了新的视频问答场景——SceneQA任务，强调基于场景的细节感知和推理能力。2. 开发了LVSQA数据集，用于支持SceneQA任务，该数据集基于LVBench精选视频构建，并包含新的问答对。3. 引入了SLFG方法，其核心思想是将单个帧组合成语义连贯的场景帧，通过场景定位和动态帧重组机制来增强理解能力。SLFG无需修改原始模型架构，具有即插即用的特性。", "result": "SLFG方法显著增强了现有MLLMs在长视频中的理解能力，并在多个长视频基准测试中表现出色。", "conclusion": "SceneQA任务、LVSQA数据集以及SLFG方法有效解决了MLLMs在长视频理解中的挑战，显著提升了其性能，并具有良好的实用性。"}}
{"id": "2508.03514", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03514", "abs": "https://arxiv.org/abs/2508.03514", "authors": ["Pavlos Panagiotidis", "Victor Zhi Heung Ngo", "Sean Myatt", "Roma Patel", "Rachel Ramchurn", "Alan Chamberlain", "Ayse Kucukyilmaz"], "title": "Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours", "comment": "The paper is accepted for presentation to International Conference on\n  Social Robotics + AI (https://icsr2025.eu/)", "summary": "In this paper, we propose theatre-in-the-loop, a framework for developing\nexpressive robot behaviours tailored to artistic performance through a\ndirector-guided puppeteering workflow. Leveraging theatrical methods, we use\nnarrative objectives to direct a puppeteer in generating improvised robotic\ngestures that convey specific emotions. These improvisations are captured and\ncurated to build a dataset of reusable movement templates for standalone\nplayback in future autonomous performances. Initial trials demonstrate the\nfeasibility of this approach, illustrating how the workflow enables precise\nsculpting of robotic gestures into coherent emotional arcs while revealing\nchallenges posed by the robot's mechanical constraints. We argue that this\npractice-led framework provides a model for interdisciplinary teams creating\nsocially expressive robot behaviours, contributing to (1) theatre as an\ninteractive training ground for human-robot interaction and (2) co-creation\nmethodologies between humans and machines.", "AI": {"tldr": "本文提出了“剧场循环”框架，通过导演指导的木偶操作工作流，为机器人艺术表演开发富有表现力的行为，并生成可重用的运动模板。", "motivation": "为机器人艺术表演创造具有情感表达的机器人行为，并探索人机协作生成社交表达行为的方法。", "method": "提出“剧场循环”框架，利用戏剧方法，由导演设定叙事目标，指导木偶操作师即兴生成表达特定情感的机器人姿态。这些即兴表演被捕捉并整理，构建可重用于未来自主表演的运动模板数据集。", "result": "初步试验表明该方法可行，能将机器人姿态精确塑造成连贯的情感弧线，同时也揭示了机器人机械限制带来的挑战。", "conclusion": "该实践主导的框架为跨学科团队创建社交表达机器人行为提供了一个模型，有助于将剧场作为人机交互的互动训练场，并促进人机之间的共同创造方法论。"}}
{"id": "2508.03082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03082", "abs": "https://arxiv.org/abs/2508.03082", "authors": ["Fei Liu", "Yilu Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan"], "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "comment": null, "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.", "AI": {"tldr": "该研究提出了一种新的LLM驱动的自动化启发式设计方法（AHSD），旨在生成一个互补的启发式集合，以解决现有方法仅设计单一启发式导致泛化能力差的问题，并在实验中取得了显著性能提升。", "motivation": "现有的LLM驱动的自动化启发式设计（AHD）方法仅设计单个启发式来服务所有问题实例，导致在不同分布或设置下泛化能力差。", "method": "提出了自动化启发式集合设计（AHSD）的新范式，旨在自动生成一个小型互补启发式集合。证明了AHSD的目标函数是单调且超模的。在此基础上，提出了启发式集合进化（EoH-S）算法，包含互补种群管理和互补感知模因搜索两种新机制，以有效地生成高质量和互补的启发式集合。", "result": "在三个AHD任务上，针对不同大小和分布的多样化实例进行综合实验，结果表明EoH-S始终优于现有的最先进AHD方法，并实现了高达60%的性能提升。", "conclusion": "AHSD（通过EoH-S实现）能够有效地生成一组高质量且互补的启发式方法，以应对多样化的优化问题实例，显著优于传统的单一启发式设计方法。"}}
{"id": "2508.03259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03259", "abs": "https://arxiv.org/abs/2508.03259", "authors": ["Duzhen Zhang", "Chenxing Li", "Jiahua Dong", "Qi Liu", "Dong Yu"], "title": "Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition", "comment": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "Continual Named Entity Recognition (CNER) is an evolving field that focuses\non sequentially updating an existing model to incorporate new entity types.\nPrevious CNER methods primarily utilize Knowledge Distillation (KD) to preserve\nprior knowledge and overcome catastrophic forgetting, strictly ensuring that\nthe representations of old and new models remain consistent. Consequently, they\noften impart the model with excessive stability (i.e., retention of old\nknowledge) but limited plasticity (i.e., acquisition of new knowledge). To\naddress this issue, we propose a Stability-Plasticity Trade-off (SPT) method\nfor CNER that balances these aspects from both representation and weight\nperspectives. From the representation perspective, we introduce a pooling\noperation into the original KD, permitting a level of plasticity by\nconsolidating representation dimensions. From the weight perspective, we\ndynamically merge the weights of old and new models, strengthening old\nknowledge while maintaining new knowledge. During this fusion, we implement a\nweight-guided selective mechanism to prioritize significant weights. Moreover,\nwe develop a confidence-based pseudo-labeling approach for the current\nnon-entity type, which predicts entity types using the old model to handle the\nsemantic shift of the non-entity type, a challenge specific to CNER that has\nlargely been ignored by previous methods. Extensive experiments across ten CNER\nsettings on three benchmark datasets demonstrate that our SPT method surpasses\nprevious CNER approaches, highlighting its effectiveness in achieving a\nsuitable stability-plasticity trade-off.", "AI": {"tldr": "本文提出了一种名为稳定性-可塑性权衡（SPT）的方法，用于持续命名实体识别（CNER），旨在平衡模型在学习新实体类型时的旧知识保留（稳定性）和新知识获取（可塑性）。", "motivation": "以往的CNER方法主要依赖知识蒸馏（KD），导致模型过度稳定而可塑性不足，难以有效获取新知识。此外，CNER中非实体类型的语义漂移问题常被忽视。", "method": "SPT方法从表示和权重两个层面平衡稳定性与可塑性。在表示层面，通过在KD中引入池化操作来增加可塑性；在权重层面，动态融合新旧模型权重，并采用权重引导的选择机制。此外，还开发了一种基于置信度的伪标签方法，利用旧模型预测当前非实体类型，以应对语义漂移。", "result": "在三个基准数据集的十种CNER设置上进行的广泛实验表明，SPT方法优于以前的CNER方法。", "conclusion": "SPT方法有效实现了稳定性与可塑性的平衡，显著提升了CNER的性能。"}}
{"id": "2508.03017", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03017", "abs": "https://arxiv.org/abs/2508.03017", "authors": ["Liheng Zhang", "Weihao Yu", "Zubo Lu", "Haozhi Gu", "Jin Huang"], "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting", "comment": "9 pages, 7 figures. Under review at AAAI 2026", "summary": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and\nhigh-quality novel view synthesis. However, representing scenes requires a\nlarge number of Gaussian points, leading to high storage demands and limiting\npractical deployment. The latest methods facilitate the compression of Gaussian\nmodels but struggle to identify truly insignificant Gaussian points in the\nscene, leading to a decline in subsequent Gaussian pruning, compression\nquality, and rendering performance. To address this issue, we propose SA-3DGS,\na method that significantly reduces storage costs while maintaining rendering\nquality. SA-3DGS learns an importance score to automatically identify the least\nsignificant Gaussians in scene reconstruction, thereby enabling effective\npruning and redundancy reduction. Next, the importance-aware clustering module\ncompresses Gaussians attributes more accurately into the codebook, improving\nthe codebook's expressive capability while reducing model size. Finally, the\ncodebook repair module leverages contextual scene information to repair the\ncodebook, thereby recovering the original Gaussian point attributes and\nmitigating the degradation in rendering quality caused by information loss.\nExperimental results on several benchmark datasets show that our method\nachieves up to 66x compression while maintaining or even improving rendering\nquality. The proposed Gaussian pruning approach is not only adaptable to but\nalso improves other pruning-based methods (e.g., LightGaussian), showcasing\nexcellent performance and strong generalization ability.", "AI": {"tldr": "SA-3DGS是一种3D高斯溅射模型压缩方法，通过学习重要性分数进行有效剪枝、基于重要性的聚类压缩以及码本修复，显著降低存储需求（最高66倍压缩）同时保持或提升渲染质量。", "motivation": "3D高斯溅射模型需要大量高斯点，导致存储需求高，限制了实际部署。现有压缩方法难以有效识别不重要的高斯点，导致剪枝和压缩质量下降，影响渲染性能。", "method": "1. 学习一个重要性分数以自动识别场景重建中最不重要的高斯点，实现有效剪枝和冗余减少。 2. 采用重要性感知聚类模块更准确地将高斯属性压缩到码本中，提高码本表达能力并减小模型尺寸。 3. 设计码本修复模块，利用上下文场景信息修复码本，恢复原始高斯点属性，减轻信息损失导致的渲染质量下降。", "result": "在多个基准数据集上的实验结果表明，该方法在保持甚至提升渲染质量的同时，实现了高达66倍的压缩。所提出的高斯剪枝方法不仅适用于其他基于剪枝的方法（如LightGaussian），还能提升其性能，展现出卓越的性能和强大的泛化能力。", "conclusion": "SA-3DGS通过创新的重要性感知剪枝和压缩策略，成功解决了3D高斯溅射模型存储成本高昂的问题，显著降低了模型大小，同时保持了高质量的渲染效果，并对现有方法具有良好的兼容性和提升作用。"}}
{"id": "2508.03526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03526", "abs": "https://arxiv.org/abs/2508.03526", "authors": ["Kun Song", "Shentao Ma", "Gaoming Chen", "Ninglong Jin", "Guangbao Zhao", "Mingyu Ding", "Zhenhua Xiong", "Jia Pan"], "title": "CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation", "comment": "9 pages,5 figures", "summary": "A central research topic in robotics is how to use this system to interact\nwith the physical world. Traditional manipulation tasks primarily focus on\nsmall objects. However, in factory or home environments, there is often a need\nfor the movement of large objects, such as moving tables. These tasks typically\nrequire multi-robot systems to work collaboratively. Previous research lacks a\nframework that can scale to arbitrary sizes of robots and generalize to various\nkinds of tasks. In this work, we propose CollaBot, a generalist framework for\nsimultaneous collaborative manipulation. First, we use SEEM for scene\nsegmentation and point cloud extraction of the target object. Then, we propose\na collaborative grasping framework, which decomposes the task into local grasp\npose generation and global collaboration. Finally, we design a 2-stage planning\nmodule that can generate collision-free trajectories to achieve this task.\nExperiments show a success rate of 52% across different numbers of robots,\nobjects, and tasks, indicating the effectiveness of the proposed framework.", "AI": {"tldr": "CollaBot是一个通用的协同操作框架，用于多机器人同时搬运大型物体。", "motivation": "传统机器人研究主要关注小型物体操作，但工厂和家庭环境中常需移动大型物体（如桌子），这需要多机器人协作。现有研究缺乏可扩展且通用的框架。", "method": "该方法首先使用SEEM进行场景分割和目标物体点云提取；然后提出一个协同抓取框架，将其分解为局部抓取姿态生成和全局协作；最后设计一个两阶段规划模块，生成无碰撞轨迹。", "result": "实验表明，该框架在不同数量的机器人、物体和任务下，成功率达到52%，证明了其有效性。", "conclusion": "CollaBot框架为同时协同操作提供了有效的解决方案，能够处理大型物体的搬运任务。"}}
{"id": "2508.03083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03083", "abs": "https://arxiv.org/abs/2508.03083", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for missing data\nimputation by modeling the joint distribution of observed and unobserved\nvariables. However, existing methods, typically based on stochastic denoising\ndiffusion probabilistic models (DDPMs), suffer from high inference latency and\nvariable outputs, limiting their applicability in real-world tabular settings.\nTo address these deficiencies, we present in this paper MissDDIM, a conditional\ndiffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for\ntabular imputation. While stochastic sampling enables diverse completions, it\nalso introduces output variability that complicates downstream processing.", "AI": {"tldr": "针对现有基于DDPM的扩散模型在表格数据插补中推理延迟高和输出变异性大的问题，本文提出了MissDDIM，一个基于DDIM的条件扩散框架，以实现更高效和稳定的插补。", "motivation": "现有的基于随机去噪扩散概率模型（DDPM）的扩散模型在缺失数据插补方面表现出色，但其高推理延迟和不稳定的输出限制了它们在真实世界表格数据场景中的应用。", "method": "提出了MissDDIM，一个条件扩散框架，该框架将去噪扩散隐式模型（DDIM）应用于表格数据插补。", "result": "抽象中未直接给出具体结果，但暗示MissDDIM旨在解决现有DDPM方法的推理延迟和输出变异性问题。它提到随机采样虽然能生成多样化补全，但引入的输出变异性会使下游处理复杂化。", "conclusion": "MissDDIM通过采用DDIM来改进表格数据插补中的扩散模型，旨在克服现有DDPM方法的高延迟和输出变异性问题。"}}
{"id": "2508.03262", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03262", "abs": "https://arxiv.org/abs/2508.03262", "authors": ["Junhyuk Choi", "Hyeonchu Park", "Haemin Lee", "Hyebeen Shin", "Hyun Joung Jin", "Bugeun Kim"], "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?", "comment": "Preprint", "summary": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）使用真实人类数据预测个体经济决策的能力，发现在个体层面预测困难，但在群体层面表现合理，且复杂提示方法效果不佳。", "motivation": "现有关于LLMs模拟人类行为的研究多依赖虚构角色而非真实人类数据，本研究旨在弥补这一局限性，评估LLMs在真实经济决策场景中的预测能力。", "method": "研究通过“随心付”（PWYW）定价实验，使用来自522名韩国参与者的真实详细个人信息，评估了三种先进的多模态LLMs预测文化消费场景中个体经济决策的能力。同时，探究了人格注入方法对预测性能的影响。", "result": "结果显示，LLMs难以进行精确的个体层面预测，但能合理地捕捉群体层面的行为趋势。此外，常用的提示技术（如个人叙事重构或检索增强生成）并未比简单提示方法带来显著性能提升。", "conclusion": "本研究首次全面评估了LLMs使用真实人类数据模拟经济行为的能力，为计算社会科学中基于角色的模拟提供了实证指导。"}}
{"id": "2508.03034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03034", "abs": "https://arxiv.org/abs/2508.03034", "authors": ["Qi Xie", "Yongjia Ma", "Donglin Di", "Xuehao Gao", "Xun Yang"], "title": "MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention", "comment": null, "summary": "Achieving ID-preserving text-to-video (T2V) generation remains challenging\ndespite recent advances in diffusion-based models. Existing approaches often\nfail to capture fine-grained facial dynamics or maintain temporal identity\ncoherence. To address these limitations, we propose MoCA, a novel Video\nDiffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating\na Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts\nparadigm. Our framework improves inter-frame identity consistency by embedding\nMoCA layers into each DiT block, where Hierarchical Temporal Pooling captures\nidentity features over varying timescales, and Temporal-Aware Cross-Attention\nExperts dynamically model spatiotemporal relationships. We further incorporate\na Latent Video Perceptual Loss to enhance identity coherence and fine-grained\ndetails across video frames. To train this model, we collect CelebIPVid, a\ndataset of 10,000 high-resolution videos from 1,000 diverse individuals,\npromoting cross-ethnicity generalization. Extensive experiments on CelebIPVid\nshow that MoCA outperforms existing T2V methods by over 5% across Face\nsimilarity.", "AI": {"tldr": "MoCA是一种基于Diffusion Transformer的新型视频扩散模型，通过引入混合交叉注意力机制和潜在视频感知损失，显著提升了文本到视频生成中人物身份的保留和时间一致性，并在新数据集CelebIPVid上表现优异。", "motivation": "尽管扩散模型取得了进展，但ID-preserving的文本到视频（T2V）生成仍具挑战性，现有方法难以捕捉精细的面部动态或保持时间上的身份一致性。", "method": "本文提出了MoCA模型，该模型基于Diffusion Transformer (DiT) 主干，并引入了受专家混合（MoE）启发的多专家交叉注意力（MoCA）机制。MoCA层嵌入到每个DiT块中，其中分层时间池化（Hierarchical Temporal Pooling）捕获不同时间尺度上的身份特征，时间感知交叉注意力专家（Temporal-Aware Cross-Attention Experts）动态建模时空关系。此外，模型还引入了潜在视频感知损失（Latent Video Perceptual Loss）以增强视频帧间的身份连贯性和细节。为训练模型，作者收集了包含10,000个高分辨率视频的CelebIPVid数据集。", "result": "在CelebIPVid数据集上的大量实验表明，MoCA在人脸相似度方面优于现有T2V方法超过5%。", "conclusion": "MoCA模型通过其创新的架构和训练策略，有效解决了文本到视频生成中身份保留和时间一致性的挑战，实现了更优的生成质量和身份保持能力。"}}
{"id": "2508.03541", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03541", "abs": "https://arxiv.org/abs/2508.03541", "authors": ["Ergi Tushe", "Bilal Farooq"], "title": "Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions", "comment": null, "summary": "The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy\nurban spaces introduces unique challenges in terms of safe, efficient, and\nsocially acceptable navigation. We develop the complete pipeline for a single\nvision sensor based multi-pedestrian detection and tracking, pose estimation,\nand monocular depth perception. Leveraging the real-world MOT17 dataset\nsequences, this study demonstrates how integrating human-pose estimation and\ndepth cues enhances pedestrian trajectory prediction and identity maintenance,\neven under occlusions and dense crowds. Results show measurable improvements,\nincluding up to a 10% increase in identity preservation (IDF1), a 7%\nimprovement in multiobject tracking accuracy (MOTA), and consistently high\ndetection precision exceeding 85%, even in challenging scenarios. Notably, the\nsystem identifies vulnerable pedestrian groups supporting more socially aware\nand inclusive robot behaviour.", "AI": {"tldr": "该研究开发了一个基于单目视觉传感器的完整管道，用于在行人密集区域实现自动配送机器人（ADRs）的多行人检测、跟踪、姿态估计和深度感知，显著提升了行人轨迹预测和身份保持能力，并支持更具社会意识的机器人行为。", "motivation": "自动配送机器人（ADRs）在行人密集的城市空间中面临安全、高效和可被社会接受的导航挑战，尤其是在多行人场景下。", "method": "开发了一个基于单个视觉传感器的完整管道，包含多行人检测与跟踪、姿态估计和单目深度感知。利用真实世界的MOT17数据集序列，通过整合人体姿态估计和深度信息来增强行人轨迹预测和身份保持。", "result": "结果显示显著改进：身份保持（IDF1）提升高达10%，多目标跟踪准确度（MOTA）提升7%，即使在复杂场景下检测精度也始终高于85%。该系统还能识别脆弱行人群体，支持更具社会意识和包容性的机器人行为。", "conclusion": "将人体姿态估计和深度信息整合到单目视觉系统中，能够有效提升行人的轨迹预测和身份保持能力，从而使自动配送机器人在城市环境中实现更安全、更高效且更具社会意识的导航。"}}
{"id": "2508.03091", "categories": ["cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03091", "abs": "https://arxiv.org/abs/2508.03091", "authors": ["Xingjun Ma", "Hanxun Huang", "Tianwei Song", "Ye Sun", "Yifeng Gao", "Yu-Gang Jiang"], "title": "T2UE: Generating Unlearnable Examples from Text Descriptions", "comment": "To appear in ACM MM 2025", "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure.", "AI": {"tldr": "T2UE是一种新型框架，允许用户仅使用文本描述生成不可学习示例（UEs），从而在不暴露原始图像数据的情况下保护个人数据，解决了现有UE生成方法中的隐私悖论。", "motivation": "大规模预训练模型（如CLIP）依赖于包含私人用户数据的网络爬取数据集，引发了滥用担忧。现有不可学习示例（UEs）生成方法通常需要同时优化图像和文本的不可学习噪声，但计算成本高昂，且依赖第三方服务，导致用户必须先暴露数据才能获得保护，形成隐私悖论。", "method": "本文提出了Text-to-Unlearnable Example (T2UE)框架，通过使用文本到图像（T2I）模型将文本描述映射到图像（噪声）空间，并结合误差最小化框架来生成有效的不可学习噪声。该方法仅依赖文本描述，无需原始图像数据。", "result": "实验表明，T2UE保护的数据显著降低了最先进模型在下游任务（如跨模态检索）中的性能。此外，这种保护效果能够泛化到不同的模型架构，甚至适用于监督学习设置。", "conclusion": "该工作证明了“零接触数据保护”的可行性，即仅基于文本描述即可保护个人数据，无需直接暴露数据，从而解决了数据保护中的隐私矛盾。"}}
{"id": "2508.03275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03275", "abs": "https://arxiv.org/abs/2508.03275", "authors": ["Jiahao Zhao"], "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning", "comment": "15 pages, 4 figures, 1 table", "summary": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms.", "AI": {"tldr": "LECTOR是一种基于LLM的自适应间隔重复算法，专为考试型学习设计，通过语义分析和个性化配置解决词汇学习中的语义混淆问题，在模拟测试中显著提高了成功率。", "motivation": "现有的间隔重复系统在处理语义干扰和个性化适应方面存在不足，特别是在以成功率为核心的考试型学习场景（如语言考试）中，语义混淆是一个关键挑战。", "method": "LECTOR利用大型语言模型（LLMs）进行语义分析和语义相似度评估，并结合个性化学习档案和既定的间隔重复原则。该算法旨在解决词汇学习中的语义混淆问题。", "result": "在针对100名模拟学习者进行100天的评估中，LECTOR的成功率为90.2%，优于表现最佳的基线算法（SSP-MMC，88.4%），相对提升了2.0%。该算法在处理语义相似概念方面表现出色，减少了由混淆引起的错误，同时保持了计算效率。", "conclusion": "LECTOR为智能辅导系统和自适应学习平台提供了一个有前景的方向，证明了其在提高考试型学习成功率方面的潜力，尤其是在处理语义相似概念时。"}}
{"id": "2508.03039", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.03039", "abs": "https://arxiv.org/abs/2508.03039", "authors": ["Yiran Meng", "Junhong Ye", "Wei Zhou", "Guanghui Yue", "Xudong Mao", "Ruomei Wang", "Baoquan Zhao"], "title": "VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering", "comment": null, "summary": "Cross-video question answering presents significant challenges beyond\ntraditional single-video understanding, particularly in establishing meaningful\nconnections across video streams and managing the complexity of multi-source\ninformation retrieval. We introduce VideoForest, a novel framework that\naddresses these challenges through person-anchored hierarchical reasoning. Our\napproach leverages person-level features as natural bridge points between\nvideos, enabling effective cross-video understanding without requiring\nend-to-end training. VideoForest integrates three key innovations: 1) a\nhuman-anchored feature extraction mechanism that employs ReID and tracking\nalgorithms to establish robust spatiotemporal relationships across multiple\nvideo sources; 2) a multi-granularity spanning tree structure that\nhierarchically organizes visual content around person-level trajectories; and\n3) a multi-agent reasoning framework that efficiently traverses this\nhierarchical structure to answer complex cross-video queries. To evaluate our\napproach, we develop CrossVideoQA, a comprehensive benchmark dataset\nspecifically designed for person-centric cross-video analysis. Experimental\nresults demonstrate VideoForest's superior performance in cross-video reasoning\ntasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior\nanalysis, and 51.67% in summarization and reasoning, significantly\noutperforming existing methods. Our work establishes a new paradigm for\ncross-video understanding by unifying multiple video streams through\nperson-level features, enabling sophisticated reasoning across distributed\nvisual information while maintaining computational efficiency.", "AI": {"tldr": "本文提出了VideoForest框架，通过以人物为中心的层次化推理解决跨视频问答的挑战，并构建了新的CrossVideoQA数据集，实验证明其在跨视频推理任务中表现优越。", "motivation": "传统单视频理解无法满足跨视频问答的需求，主要挑战在于如何建立跨视频流的有效连接，以及管理多源信息的复杂性。", "method": "引入VideoForest框架，采用以人物为中心的层次化推理，无需端到端训练。其核心创新包括：1) 人物锚定的特征提取机制，利用ReID和跟踪算法在多视频源间建立时空关系；2) 多粒度生成树结构，围绕人物轨迹层次化组织视觉内容；3) 多智能体推理框架，高效遍历该层次结构以回答复杂查询。此外，还开发了CrossVideoQA数据集用于评估。", "result": "VideoForest在跨视频推理任务中表现出色，人物识别准确率达71.93%，行为分析达83.75%，总结和推理达51.67%，显著优于现有方法。", "conclusion": "该工作通过以人物级特征统一多个视频流，为跨视频理解建立了新范式，实现了分布式视觉信息间的复杂推理，同时保持了计算效率。"}}
{"id": "2508.03559", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03559", "abs": "https://arxiv.org/abs/2508.03559", "authors": ["Gokhan Solak", "Arash Ajoudani"], "title": "Online Learning for Vibration Suppression in Physical Robot Interaction using Power Tools", "comment": "Submitted, under review", "summary": "Vibration suppression is an important capability for collaborative robots\ndeployed in challenging environments such as construction sites. We study the\nactive suppression of vibration caused by external sources such as power tools.\nWe adopt the band-limited multiple Fourier linear combiner (BMFLC) algorithm to\nlearn the vibration online and counter it by feedforward force control. We\npropose the damped BMFLC method, extending BMFLC with a novel adaptive\nstep-size approach that improves the convergence time and noise resistance. Our\nlogistic function-based damping mechanism reduces the effect of noise and\nenables larger learning rates. We evaluate our method on extensive simulation\nexperiments with realistic time-varying multi-frequency vibration and\nreal-world physical interaction experiments. The simulation experiments show\nthat our method improves the suppression rate in comparison to the original\nBMFLC and its recursive least squares and Kalman filter-based extensions.\nFurthermore, our method is far more efficient than the latter two. We further\nvalidate the effectiveness of our method in real-world polishing experiments. A\nsupplementary video is available at https://youtu.be/ms6m-6JyVAI.", "AI": {"tldr": "本文提出了一种名为“阻尼BMFLC”的新方法，用于协作机器人在嘈杂环境中主动抑制由外部源引起的振动，通过改进的自适应步长和阻尼机制，提高了收敛速度和抗噪能力。", "motivation": "在建筑工地等恶劣环境中部署的协作机器人需要具备振动抑制能力，特别是针对电动工具等外部源引起的振动，以提高其性能和稳定性。", "method": "本文采用带限多傅里叶线性组合器（BMFLC）算法在线学习振动并通过前馈力控制进行抵消。在此基础上，提出了“阻尼BMFLC”方法，引入了一种新颖的自适应步长方法以提高收敛时间和抗噪性。其基于逻辑函数的阻尼机制减少了噪声影响，并允许使用更大的学习率。", "result": "仿真实验表明，与原始BMFLC及其基于递归最小二乘和卡尔曼滤波的扩展相比，所提出的方法提高了抑制率，并且效率远高于后两者。此外，在实际抛光实验中也验证了该方法的有效性。", "conclusion": "所提出的阻尼BMFLC方法有效解决了协作机器人在外部振动源影响下的振动抑制问题，通过其创新的自适应步长和阻尼机制，显著提升了抑制性能、收敛速度和抗噪能力。"}}
{"id": "2508.03092", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03092", "abs": "https://arxiv.org/abs/2508.03092", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking.", "AI": {"tldr": "本文提出了一种创新的可验证大型语言模型（LLM）代理，用于检测虚假信息，该代理通过与网络源互动、评估来源可信度并提供可验证的推理过程，超越了传统的二元判断。", "motivation": "随着大型语言模型的普及，虚假信息检测变得日益重要和复杂，传统的真/假二元判断已不足以应对挑战。", "method": "研究设计了一个LLM代理架构，包含三个核心工具：精确网络搜索工具、来源可信度评估工具和数值声明验证工具。该代理能执行多步骤验证策略、维护证据日志并形成综合评估结论。通过在FakeNewsNet等标准虚假信息数据集上进行评估，并与传统机器学习模型和LLM进行比较，评估指标包括分类指标、推理过程质量和内容重写鲁棒性。", "result": "实验结果表明，该代理在虚假信息检测准确性、推理透明度和信息重写抵抗力方面均优于基线方法。", "conclusion": "该研究为可信赖的AI辅助事实核查提供了一个新范式。"}}
{"id": "2508.03276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03276", "abs": "https://arxiv.org/abs/2508.03276", "authors": ["Terra Blevins", "Susanne Schmalwieser", "Benjamin Roth"], "title": "Do language models accommodate their users? A study of linguistic convergence", "comment": null, "summary": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在对话中表现出语言趋同现象，但其趋同模式与人类不同，且通常存在过度拟合。", "motivation": "尽管LLMs在生成语言方面表现出色，但它们在语言使用上与人类的相似度（特别是语言趋同这一核心语用元素）仍未被充分研究。本文旨在探究模型是否会像人类一样适应或趋同于用户的语言模式。", "method": "研究者系统地比较了16个语言模型在3个对话语料库中对现有对话的补全，并与原始人类响应进行了对比，使用了多种文体特征进行分析。", "result": "研究发现模型强烈趋同于对话风格，但相对于人类基线，这种趋同往往存在显著的过度拟合。趋同模式通常是特征特定的，并且在不同模型设置下存在一致的趋同变化，其中指令微调模型和大型模型的趋同程度低于其预训练对应模型。", "conclusion": "鉴于人类和模型趋同模式的差异，研究者推测这两种行为的底层机制可能非常不同。"}}
{"id": "2508.03050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03050", "abs": "https://arxiv.org/abs/2508.03050", "authors": ["Zeyu Zhu", "Weijia Wu", "Mike Zheng Shou"], "title": "Multi-human Interactive Talking Dataset", "comment": "9 pages, 4 figures, 4 tables", "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.", "AI": {"tldr": "该研究引入了一个名为MIT的大规模数据集和基线模型CovOG，旨在解决现有谈话视频生成研究中多人类交互的局限性，专注于多人类谈话视频的生成。", "motivation": "现有的谈话视频生成研究主要集中于单人独白或孤立的面部动画，限制了其在真实多人类交互场景中的应用。研究旨在弥补这一空白。", "method": "该研究开发了一个自动化流程来收集和标注多人物对话视频，构建了MIT数据集，包含身体姿态和语音交互的细粒度标注。在此基础上，提出了基线模型CovOG，它集成了多人类姿态编码器（MPE）以处理不同数量的说话者，并通过交互式音频驱动器（IAD）根据特定说话者的音频特征调节头部动态。", "result": "MIT数据集包含12小时的高分辨率素材，每段视频有2到4名说话者，并带有细致的身体姿态和语音交互标注，捕捉了多说话者场景中的自然对话动态。CovOG模型展示了生成真实多人类谈话视频的可行性和挑战。", "conclusion": "MIT数据集为未来的多人类谈话视频生成研究提供了一个宝贵的基准，而CovOG模型则展示了该任务的潜力和挑战。"}}
{"id": "2508.03600", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.03600", "abs": "https://arxiv.org/abs/2508.03600", "authors": ["Hamze Hammami", "Eva Denisa Barbulescu", "Talal Shaikh", "Mouayad Aldada", "Muhammad Saad Munawar"], "title": "Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control", "comment": "This work was accepted for presentation at the ALIFE 2025 Conference\n  in Kyoto, and will be published by MIT Press as part of the ALIFE 2025\n  proceedings", "summary": "Imagine a robot controller with the ability to adapt like human synapses,\ndynamically rewiring itself to overcome unforeseen challenges in real time.\nThis paper proposes a novel zero-shot adaptation mechanism for evolutionary\nrobotics, merging a standard Genetic Algorithm (GA) controller with online\nHebbian plasticity. Inspired by biological systems, the method separates\nlearning and memory, with the genotype acting as memory and Hebbian updates\nhandling learning. In our approach, the fitness function is leveraged as a live\nscaling factor for Hebbian learning, enabling the robot's neural controller to\nadjust synaptic weights on-the-fly without additional training. This adds a\ndynamic adaptive layer that activates only during runtime to handle unexpected\nenvironmental changes. After the task, the robot 'forgets' the temporary\nadjustments and reverts to the original weights, preserving core knowledge. We\nvalidate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze\nnavigation task with changing light conditions and obstacles.", "AI": {"tldr": "本文提出了一种结合遗传算法（GA）控制器与在线赫布（Hebbian）可塑性的新型零样本自适应机制，使机器人控制器能像人类突触一样动态调整以应对实时挑战。", "motivation": "目前的机器人控制器难以在实时面对不可预见的挑战时进行适应，而生物系统（如人类突触）具有动态重连的能力，启发了研究者开发一种类似自适应机制。", "method": "该方法将GA控制器与在线赫布可塑性结合，分离了学习（赫布更新）和记忆（基因型）。适应度函数被用作赫布学习的实时缩放因子，使机器人神经控制器能够实时调整突触权重，无需额外训练。任务结束后，机器人“遗忘”临时调整，恢复原始权重，保留核心知识。", "result": "该混合GA-赫布控制器在e-puck机器人的T型迷宫导航任务中得到了验证，该任务包含不断变化的光照条件和障碍物。", "conclusion": "所提出的混合GA-赫布控制器为机器人提供了一个动态的自适应层，使其能够在运行时处理意外的环境变化，同时保留核心知识。"}}
{"id": "2508.03109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03109", "abs": "https://arxiv.org/abs/2508.03109", "authors": ["Wen-Xi Yang", "Tian-Fang Zhao"], "title": "AgentSME for Simulating Diverse Communication Modes in Smart Education", "comment": null, "summary": "Generative agent models specifically tailored for smart education are\ncritical, yet remain relatively underdeveloped. A key challenge stems from the\ninherent complexity of educational contexts: learners are human beings with\nvarious cognitive behaviors, and pedagogy is fundamentally centered on\npersonalized human-to-human communication. To address this issue, this paper\nproposes AgentSME, a unified generative agent framework powered by LLM. Three\ndirectional communication modes are considered in the models, namely Solo,\nMono, and Echo, reflecting different types of agency autonomy and communicative\nreciprocity. Accuracy is adopted as the primary evaluation metric, complemented\nby three diversity indices designed to assess the diversity of reasoning\ncontents. Six widely used LLMs are tested to validate the robustness of\ncommunication modes across different model tiers, which are equally divided\ninto base-capacity and high-capacity configurations. The results show that\ngenerative agents that employ the Echo communication mode achieve the highest\naccuracy scores, while DeepSeek exhibits the greatest diversity. This study\nprovides valuable information to improve agent learning capabilities and\ninspire smart education models.", "AI": {"tldr": "本文提出了AgentSME，一个基于LLM的统一生成式智能体框架，用于智能教育，考虑了Solo、Mono和Echo三种通信模式，并发现Echo模式准确率最高，DeepSeek多样性最佳。", "motivation": "智能教育中针对性强的生成式智能体模型仍不成熟，主要挑战在于教育背景的复杂性（学习者是具有不同认知行为的人类）以及教学核心在于个性化的人际交流。", "method": "提出了AgentSME，一个由LLM驱动的统一生成式智能体框架。考虑了Solo、Mono和Echo三种通信模式，分别反映不同的智能体自主性和交流互惠性。评估指标包括准确率和三个多样性指数（评估推理内容多样性）。测试了六个LLM（分为基础能力和高能力配置）以验证通信模式的鲁棒性。", "result": "结果表明，采用Echo通信模式的生成式智能体取得了最高的准确率分数，而DeepSeek模型表现出最大的多样性。", "conclusion": "本研究为提高智能体的学习能力和启发智能教育模型提供了有价值的信息。"}}
{"id": "2508.03292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03292", "abs": "https://arxiv.org/abs/2508.03292", "authors": ["Shahed Masoudian", "Gustavo Escobedo", "Hannah Strauss", "Markus Schedl"], "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes", "comment": "Under Review", "summary": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.", "AI": {"tldr": "大型语言模型（LLMs）存在性别偏见，尤其是在生成长篇内容时。本研究通过引入基于心理学性别刻板印象的叙事生成任务和数据集，发现LLMs在无条件提示下偏向男性，但基于非性别刻板印象的属性进行条件生成可缓解偏见。结合多个男性刻板印象属性会加剧偏见，而女性刻板印象属性则能缓解偏见，且模型偏见与心理学真实情况一致，并随模型增大而增强。", "motivation": "随着大型语言模型（LLMs）在各种应用中的普及，人们对其可能放大性别偏见的担忧日益增加。以往的研究通常使用显式性别线索或在短文本任务中探测偏见，这可能忽视了LLMs在生成更长内容时所嵌入的更隐性的偏见形式。", "method": "本研究通过开放式叙事生成任务，利用心理学中研究的性别刻板印象（例如攻击性或八卦）来调查LLMs中的性别偏见。为此，引入了一个名为StereoBias-Stories的新数据集，包含无条件或根据25个心理学刻板印象中的（一个、两个或六个）随机属性以及三个任务相关的故事结局进行条件生成的故事。分析了故事中性别贡献如何随这些属性的变化而变化。", "result": ["在无条件提示下，模型平均高度偏向男性，但基于与性别刻板印象无关的属性进行条件生成可缓解这种偏见。", "结合与相同性别刻板印象相关的多个属性会加剧模型行为：男性刻板印象属性会放大偏见，而女性刻板印象属性则会缓解偏见。", "模型偏见与心理学真实情况（用于分类的）相符，并且这种对齐强度随模型尺寸的增大而增强。"], "conclusion": "这些发现共同强调了对大型语言模型进行基于心理学原理的评估的重要性。"}}
{"id": "2508.03055", "categories": ["cs.CV", "cs.AI", "I.4.8"], "pdf": "https://arxiv.org/pdf/2508.03055", "abs": "https://arxiv.org/abs/2508.03055", "authors": ["Hyebin Cho", "Jaehyup Lee"], "title": "Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation", "comment": "Accepted to ACM MM 2025. 9 pages, 8 figures, 6 tables", "summary": "Face filters have become a key element of short-form video content, enabling\na wide array of visual effects such as stylization and face swapping. However,\ntheir performance often degrades in the presence of occlusions, where objects\nlike hands, hair, or accessories obscure the face. To address this limitation,\nwe introduce the novel task of face matting, which estimates fine-grained alpha\nmattes to separate occluding elements from facial regions. We further present\nFaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality\nalpha mattes under complex occlusions. Our approach leverages a two-stage\ntraining pipeline: a teacher model is trained to jointly estimate alpha mattes\nand per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this\nuncertainty is then used to guide the student model through spatially adaptive\nknowledge distillation. This formulation enables the student to focus on\nambiguous or occluded regions, improving generalization and preserving semantic\nconsistency. Unlike previous approaches that rely on trimaps or segmentation\nmasks, our framework requires no auxiliary inputs making it well-suited for\nreal-time applications. In addition, we reformulate the matting objective by\nexplicitly treating skin as foreground and occlusions as background, enabling\nclearer compositing strategies. To support this task, we newly constructed\nCelebAMat, a large-scale synthetic dataset specifically designed for\nocclusion-aware face matting. Extensive experiments show that FaceMat\noutperforms state-of-the-art methods across multiple benchmarks, enhancing the\nvisual quality and robustness of face filters in real-world, unconstrained\nvideo scenarios. The source code and CelebAMat dataset are available at\nhttps://github.com/hyebin-c/FaceMat.git", "AI": {"tldr": "FaceMat是一个无需Trimap、感知不确定性的面部抠图框架，能有效分离遮挡物和面部区域，从而提升面部滤镜在复杂遮挡下的性能。", "motivation": "面部滤镜在存在遮挡（如手、头发、配饰）时性能会下降。现有方法难以处理这种情况，因此需要一种能精确分离遮挡物和面部区域的方法。", "method": "引入了“面部抠图”的新任务，旨在估计精细的alpha蒙版来分离遮挡物和面部区域。提出了FaceMat框架，该框架无需Trimap且感知不确定性。采用两阶段训练流程：教师模型联合估计alpha蒙版和像素级不确定性（使用NLL损失），然后利用此不确定性通过空间自适应知识蒸馏指导学生模型，使其专注于模糊或被遮挡区域。该方法无需辅助输入（如Trimap或分割掩码），并将皮肤视为前景，遮挡物视为背景。为此任务构建了大型合成数据集CelebAMat。", "result": "FaceMat在多个基准测试中优于最先进的方法，显著提高了面部滤镜在真实、无约束视频场景中的视觉质量和鲁棒性。", "conclusion": "FaceMat通过引入新颖、鲁棒且支持实时应用的面部抠图框架，并配合新数据集，有效解决了面部滤镜在遮挡情况下的性能下降问题，从而显著提升了面部滤镜的整体表现。"}}
{"id": "2508.03645", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03645", "abs": "https://arxiv.org/abs/2508.03645", "authors": ["Akshay L Chandra", "Iman Nematollahi", "Chenguang Huang", "Tim Welschehold", "Wolfram Burgard", "Abhinav Valada"], "title": "DiWA: Diffusion Policy Adaptation with World Models", "comment": "Accepted at the 2025 Conference on Robot Learning (CoRL)", "summary": "Fine-tuning diffusion policies with reinforcement learning (RL) presents\nsignificant challenges. The long denoising sequence for each action prediction\nimpedes effective reward propagation. Moreover, standard RL methods require\nmillions of real-world interactions, posing a major bottleneck for practical\nfine-tuning. Although prior work frames the denoising process in diffusion\npolicies as a Markov Decision Process to enable RL-based updates, its strong\ndependence on environment interaction remains highly inefficient. To bridge\nthis gap, we introduce DiWA, a novel framework that leverages a world model for\nfine-tuning diffusion-based robotic skills entirely offline with reinforcement\nlearning. Unlike model-free approaches that require millions of environment\ninteractions to fine-tune a repertoire of robot skills, DiWA achieves effective\nadaptation using a world model trained once on a few hundred thousand offline\nplay interactions. This results in dramatically improved sample efficiency,\nmaking the approach significantly more practical and safer for real-world robot\nlearning. On the challenging CALVIN benchmark, DiWA improves performance across\neight tasks using only offline adaptation, while requiring orders of magnitude\nfewer physical interactions than model-free baselines. To our knowledge, this\nis the first demonstration of fine-tuning diffusion policies for real-world\nrobotic skills using an offline world model. We make the code publicly\navailable at https://diwa.cs.uni-freiburg.de.", "AI": {"tldr": "DiWA提出了一种新颖的框架，通过离线世界模型对基于扩散的机器人策略进行微调，显著提高了样本效率，解决了传统强化学习微调扩散策略时交互成本高昂的问题。", "motivation": "使用强化学习微调扩散策略面临两大挑战：一是长时间的去噪序列阻碍了奖励有效传播；二是标准强化学习方法需要数百万次真实世界交互，效率低下且不切实际。现有将去噪过程视为马尔可夫决策过程的方法仍高度依赖环境交互。", "method": "DiWA框架利用一个世界模型，完全离线地使用强化学习微调基于扩散的机器人技能。该世界模型仅需在数十万次离线交互数据上训练一次。", "result": "DiWA在CALVIN基准测试中，仅通过离线适应就显著提升了八项任务的性能，并且所需物理交互次数比无模型基线少几个数量级，极大地提高了样本效率。这是首次展示使用离线世界模型微调真实世界机器人技能的扩散策略。", "conclusion": "DiWA通过引入离线世界模型，成功克服了强化学习微调扩散策略的挑战，显著提升了样本效率和实用性，为真实世界机器人学习提供了更可行、更安全的方法。"}}
{"id": "2508.03117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03117", "abs": "https://arxiv.org/abs/2508.03117", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "comment": "25 pages", "summary": "We present a framework for training trustworthy large language model (LLM)\nagents for optimization modeling via a verifiable synthetic data generation\npipeline. Focusing on linear and mixed-integer linear programming, our approach\nbegins with structured symbolic representations and systematically produces\nnatural language descriptions, mathematical formulations, and solver-executable\ncode. By programmatically constructing each instance with known optimal\nsolutions, the pipeline ensures full verifiability and enables automatic\nfiltering of low-quality demonstrations generated by teacher models. Each\ndataset instance includes a structured representation of the optimization\nproblem, a corresponding natural language description, the verified optimal\nsolution, and step-by-step demonstrations - generated by a teacher model - that\nshow how to model and solve the problem across multiple optimization modeling\nlanguages. This enables supervised fine-tuning of open-source LLMs specifically\ntailored to optimization tasks. To operationalize this pipeline, we introduce\nOptiTrust, a modular LLM agent that performs multi-stage translation from\nnatural language to solver-ready code, leveraging stepwise demonstrations,\nmulti-language inference, and majority-vote cross-validation. Our agent\nachieves state-of-the-art performance on standard benchmarks. Out of 7\ndatasets, it achieves the highest accuracy on six and outperforms the next-best\nalgorithm by at least 8 percentage on three of them. Our approach provides a\nscalable, verifiable, and principled path toward building reliable LLM agents\nfor real-world optimization applications.", "AI": {"tldr": "该研究提出一个框架，通过可验证的合成数据生成流程来训练用于优化建模的可信赖大型语言模型（LLM）代理，并在标准基准测试中达到了最先进的性能。", "motivation": "构建可信赖的LLM代理以进行优化建模（特别是线性规划和混合整数线性规划）面临挑战，需要确保生成的数据和解决方案的准确性和可验证性。", "method": "该方法包括一个可验证的合成数据生成管道，该管道从结构化符号表示开始，系统地生成自然语言描述、数学公式和可执行求解器代码。通过编程方式构建具有已知最优解的实例，确保了完全可验证性并能自动过滤低质量的教师模型生成演示。基于此数据，通过监督微调优化了开源LLM。此外，引入了一个名为OptiTrust的模块化LLM代理，该代理利用分步演示、多语言推理和多数投票交叉验证，执行从自然语言到求解器就绪代码的多阶段翻译。", "result": "OptiTrust代理在标准基准测试中取得了最先进的性能，在7个数据集中有6个达到了最高准确率，并且在其中3个数据集上比次优算法至少高出8个百分点。", "conclusion": "该方法为构建用于现实世界优化应用的可靠LLM代理提供了一条可扩展、可验证和有原则的途径。"}}
{"id": "2508.03294", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03294", "abs": "https://arxiv.org/abs/2508.03294", "authors": ["Leonidas Zotos", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea", "Malvina Nissim", "Hedderik van Rijn"], "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty", "comment": "10 pages, 2 figures, accepted at the 2nd International Workshop on AI\n  in Society, Education and Educational Research (AISEER)", "summary": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment.", "AI": {"tldr": "该研究比较了大型语言模型（LLM）和教授在估计考试题目难度方面的能力，发现基于LLM不确定性的监督学习方法表现最佳，可帮助教授提高评估质量。", "motivation": "开发高质量的考试需要准确估计试题难度，但教授们在这方面表现不佳。", "method": "研究将多种基于LLM的方法（直接询问Gemini 2.5、利用LLM不确定性进行监督学习）与三位教授进行比较。任务是估计学生在神经网络和机器学习领域的是非题上给出正确答案的百分比。监督学习仅使用了42个训练样本。", "result": "结果显示，教授们区分试题难度的能力有限，且表现不如直接询问Gemini 2.5。而利用LLM不确定性进行监督学习的方法，即使仅有42个训练样本，也取得了更好的结果。", "conclusion": "研究得出结论，利用LLM不确定性进行监督学习可以帮助教授更好地估计考试题目难度，从而提高评估质量。"}}
{"id": "2508.03060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03060", "abs": "https://arxiv.org/abs/2508.03060", "authors": ["Lekang Wen", "Jing Xiao", "Liang Liao", "Jiajun Chen", "Mi Wang"], "title": "CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation", "comment": null, "summary": "Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene\nunderstanding across arbitrary combinations of input modality. Existing methods\ntypically rely on explicit feature alignment to achieve modal homogenization,\nwhich dilutes the distinctive strengths of each modality and destroys their\ninherent complementarity. To achieve cooperative harmonization rather than\nhomogenization, we propose CHARM, a novel complementary learning framework\ndesigned to implicitly align content while preserving modality-specific\nadvantages through two components: (1) Mutual Perception Unit (MPU), enabling\nimplicit alignment through window-based cross-modal interaction, where\nmodalities serve as both queries and contexts for each other to discover\nmodality-interactive correspondences; (2) A dual-path optimization strategy\nthat decouples training into Collaborative Learning Strategy (CoL) for\ncomplementary fusion learning and Individual Enhancement Strategy (InE) for\nprotected modality-specific optimization. Experiments across multiple datasets\nand backbones indicate that CHARM consistently outperform the baselines, with\nsignificant increment on the fragile modalities. This work shifts the focus\nfrom model homogenization to harmonization, enabling cross-modal\ncomplementarity for true harmony in diversity.", "AI": {"tldr": "模态无关语义分割(MaSS)旨在实现跨模态鲁棒场景理解。现有方法通过显式特征对齐导致模态同质化并破坏互补性。本文提出CHARM框架，通过隐式对齐和双路径优化策略实现模态协同协调，在多个数据集上显著优于基线。", "motivation": "现有模态无关语义分割(MaSS)方法通常依赖显式特征对齐来实现模态同质化，但这会稀释各模态的独特优势并破坏它们固有的互补性，无法实现真正的协同。", "method": "本文提出CHARM，一个新颖的互补学习框架，旨在隐式对齐内容同时保留模态特有优势。它包含两个核心组件：1) 互感知单元(MPU)，通过基于窗口的跨模态交互实现隐式对齐；2) 双路径优化策略，将训练解耦为用于互补融合学习的协同学习策略(CoL)和用于受保护模态特定优化的个体增强策略(InE)。", "result": "在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，尤其在“脆弱”模态上取得了显著提升。", "conclusion": "本工作将研究焦点从模型同质化转向模态协调化，通过实现跨模态互补性，达到了多样性中的真正和谐。"}}
{"id": "2508.03672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03672", "abs": "https://arxiv.org/abs/2508.03672", "authors": ["Zhongbi Luo", "Yunjia Wang", "Jan Swevers", "Peter Slaets", "Herman Bruyninckx"], "title": "Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways", "comment": null, "summary": "Accurate geospatial information is crucial for safe, autonomous Inland\nWaterway Transport (IWT), as existing charts (IENC) lack real-time detail and\nconventional LiDAR SLAM fails in waterway environments. These challenges lead\nto vertical drift and non-semantic maps, hindering autonomous navigation.\n  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It\nuses an improved feature extraction and a water surface planar constraint to\nmitigate vertical drift. A novel pipeline transforms 3D point clouds into\nstructured 2D semantic maps using voxel-based geometric analysis, enabling\nreal-time computation of navigational parameters like bridge clearances. An\nautomated module extracts shorelines and exports them into a lightweight,\nIENC-compatible format.\n  Evaluations on a real-world dataset show Inland-LOAM achieves superior\nlocalization accuracy over state-of-the-art methods. The generated semantic\nmaps and shorelines align with real-world conditions, providing reliable data\nfor enhanced situational awareness. The code and dataset will be publicly\navailable", "AI": {"tldr": "本文提出Inland-LOAM，一个针对内陆水域的激光雷达SLAM框架，通过改进特征提取和水面平面约束解决了传统方法在水域的垂直漂移问题，并生成结构化的2D语义地图和海岸线，提高了定位精度和态势感知能力。", "motivation": "现有内陆水域电子航道图（IENC）缺乏实时细节，且传统激光雷达SLAM在水域环境中表现不佳，存在垂直漂移和生成非语义地图的问题，这些都阻碍了内陆水域自主航行。", "method": "该研究引入Inland-LOAM框架，采用改进的特征提取和水面平面约束来减轻垂直漂移。它通过基于体素的几何分析将3D点云转换为结构化的2D语义地图，实现实时计算航行参数（如桥梁净空）。此外，还包含一个自动化模块用于提取海岸线并导出为轻量级、兼容IENC的格式。", "result": "在真实世界数据集上的评估表明，Inland-LOAM在定位精度上优于现有先进方法。生成的语义地图和海岸线与真实条件高度吻合，为增强态势感知提供了可靠数据。", "conclusion": "Inland-LOAM框架有效解决了内陆水域自主航行中高精度地理空间信息获取的挑战，通过提供卓越的定位精度和实用的语义地图，显著提升了内陆水域运输的安全性与自主性。"}}
{"id": "2508.03149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03149", "abs": "https://arxiv.org/abs/2508.03149", "authors": ["Linda Smail", "David Santandreu Calonge", "Firuz Kamalov", "Nur H. Orak"], "title": "Can Large Language Models Bridge the Gap in Environmental Knowledge?", "comment": "20 pages, 3 figures, 7 tables. No external funding", "summary": "This research investigates the potential of Artificial Intelligence (AI)\nmodels to bridge the knowledge gap in environmental education among university\nstudents. By focusing on prominent large language models (LLMs) such as\nGPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses\ntheir effectiveness in conveying environmental concepts and, consequently,\nfacilitating environmental education. The investigation employs a standardized\ntool, the Environmental Knowledge Test (EKT-19), supplemented by targeted\nquestions, to evaluate the environmental knowledge of university students in\ncomparison to the responses generated by the AI models. The results of this\nstudy suggest that while AI models possess a vast, readily accessible, and\nvalid knowledge base with the potential to empower both students and academic\nstaff, a human discipline specialist in environmental sciences may still be\nnecessary to validate the accuracy of the information provided.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在弥合大学生环境知识鸿沟方面的潜力，发现AI模型拥有丰富的知识，但仍需人类专家验证其准确性。", "motivation": "旨在探讨人工智能模型如何帮助弥补大学生在环境教育方面的知识空白。", "method": "使用标准化工具环境知识测试（EKT-19）和特定问题，比较大学生与GPT-3.5、GPT-4、GPT-4o、Gemini、Claude Sonnet和Llama 2等AI模型在环境知识方面的表现。", "result": "研究结果表明，AI模型拥有庞大、易获取且有效的知识库，有潜力赋能学生和教职员工。", "conclusion": "尽管AI模型潜力巨大，但仍可能需要环境科学领域的人类学科专家来验证其提供信息的准确性。"}}
{"id": "2508.03296", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03296", "abs": "https://arxiv.org/abs/2508.03296", "authors": ["Anqi Li", "Wenwei Jin", "Jintao Tong", "Pengda Qin", "Weijia Li", "Guo Lu"], "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling", "comment": null, "summary": "Social platforms have revolutionized information sharing, but also\naccelerated the dissemination of harmful and policy-violating content. To\nensure safety and compliance at scale, moderation systems must go beyond\nefficiency and offer accuracy and interpretability. However, current approaches\nlargely rely on noisy, label-driven learning, lacking alignment with moderation\nrules and producing opaque decisions that hinder human review. Therefore, we\npropose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that\nintroduces a new policy-aligned decision paradigm. The term \"Hierarchical\"\nreflects two key aspects of our system design: (1) a hierarchical moderation\npipeline, where a lightweight binary model first filters safe content and a\nstronger model handles fine-grained risk classification; and (2) a hierarchical\ntaxonomy in the second stage, where the model performs path-based\nclassification over a hierarchical taxonomy ranging from coarse to fine-grained\nlevels. To ensure alignment with evolving moderation policies, Hi-Guard\ndirectly incorporates rule definitions into the model prompt. To further\nenhance structured prediction and reasoning, we introduce a multi-level\nsoft-margin reward and optimize with Group Relative Policy Optimization (GRPO),\npenalizing semantically adjacent misclassifications and improving explanation\nquality. Extensive experiments and real-world deployment demonstrate that\nHi-Guard achieves superior classification accuracy, generalization, and\ninterpretability, paving the way toward scalable, transparent, and trustworthy\ncontent safety systems. Code is available at:\nhttps://github.com/lianqi1008/Hi-Guard.", "AI": {"tldr": "本文提出了Hi-Guard，一个多模态内容审核框架，通过分层管道和与政策对齐的决策范式，提高了审核的准确性、泛化能力和可解释性。", "motivation": "现有内容审核系统主要依赖于嘈杂的标签驱动学习，缺乏与审核规则的对齐，并且产生的决策不透明，阻碍了人工审查，难以在大规模应用中同时保证效率、准确性和可解释性。", "method": "Hi-Guard引入了新的政策对齐决策范式。其“分层”体现在两个方面：1) 分层审核管道，轻量级二元模型首先过滤安全内容，然后更强的模型处理细粒度风险分类；2) 第二阶段的分层分类法，模型对从粗到细粒度的分层分类法进行基于路径的分类。为确保与政策对齐，Hi-Guard将规则定义直接整合到模型提示中。此外，引入了多级软边距奖励并使用组相对策略优化（GRPO）进行优化，以惩罚语义相邻的错误分类并提高解释质量。", "result": "广泛的实验和实际部署表明，Hi-Guard在分类准确性、泛化能力和可解释性方面均表现出色。", "conclusion": "Hi-Guard为构建可扩展、透明和值得信赖的内容安全系统铺平了道路。"}}
{"id": "2508.03064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03064", "abs": "https://arxiv.org/abs/2508.03064", "authors": ["Trinh Quoc Nguyen", "Oky Dicky Ardiansyah Prima", "Katsuyoshi Hotta"], "title": "CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification", "comment": null, "summary": "This study introduces a novel framework, \"Comprehensive Optimization and\nRefinement through Ensemble Fusion in Domain Adaptation for Person\nRe-identification (CORE-ReID)\", to address an Unsupervised Domain Adaptation\n(UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to\ngenerate diverse data that harmonizes differences in image characteristics from\ndifferent camera sources in the pre-training stage. In the fine-tuning stage,\nbased on a pair of teacher-student networks, the framework integrates\nmulti-view features for multi-level clustering to derive diverse pseudo labels.\nA learnable Ensemble Fusion component that focuses on fine-grained local\ninformation within global features is introduced to enhance learning\ncomprehensiveness and avoid ambiguity associated with multiple pseudo-labels.\nExperimental results on three common UDAs in Person ReID demonstrate\nsignificant performance gains over state-of-the-art approaches. Additional\nenhancements, such as Efficient Channel Attention Block and Bidirectional Mean\nFeature Normalization mitigate deviation effects and adaptive fusion of global\nand local features using the ResNet-based model, further strengthening the\nframework. The proposed framework ensures clarity in fusion features, avoids\nambiguity, and achieves high ac-curacy in terms of Mean Average Precision,\nTop-1, Top-5, and Top-10, positioning it as an advanced and effective solution\nfor the UDA in Person ReID. Our codes and models are available at\nhttps://github.com/TrinhQuocNguyen/CORE-ReID.", "AI": {"tldr": "该研究提出了CORE-ReID框架，通过CycleGAN进行数据多样化，结合教师-学生网络、多级聚类和可学习的集成融合组件，解决了行人重识别中的无监督域适应问题，并取得了显著的性能提升。", "motivation": "现有行人重识别方法在无监督域适应（UDA）方面存在挑战，尤其是在处理不同摄像源的图像特征差异以及如何有效利用伪标签方面。", "method": "CORE-ReID框架包括两个阶段：1) 预训练阶段，使用CycleGAN生成多样化数据以协调不同摄像源的图像特征；2) 微调阶段，基于教师-学生网络，整合多视图特征进行多级聚类以生成多样伪标签，并引入可学习的集成融合组件（Ensemble Fusion）来增强学习全面性，避免多伪标签的歧义。此外，还引入了高效通道注意力块（ECA）和双向均值特征归一化（BMFN）来减轻偏差效应并自适应融合全局和局部特征。", "result": "实验结果表明，该框架在三个常见的行人重识别UDA任务上显著优于现有最先进的方法，并在平均精度均值（mAP）、Top-1、Top-5和Top-10等指标上实现了高精度。", "conclusion": "CORE-ReID框架为行人重识别中的无监督域适应提供了一个先进且有效的解决方案，它确保了融合特征的清晰性，避免了歧义，并达到了高准确率。"}}
{"id": "2508.03132", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03132", "abs": "https://arxiv.org/abs/2508.03132", "authors": ["Arion Zimmermann", "Soon-Jo Chung", "Fred Hadaegh"], "title": "COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks", "comment": "in Proc. 75th Int. Astronautical Congress (IAC-24), Milan, Italy,\n  Oct. 2024", "summary": "The accurate state estimation of unknown bodies in space is a critical\nchallenge with applications ranging from the tracking of space debris to the\nshape estimation of small bodies. A necessary enabler to this capability is to\nfind and track features on a continuous stream of images. Existing methods,\nsuch as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,\nwhereas modern deep learning methods yield higher quality features at the cost\nof more demanding computational resources which might not be available on\nspace-qualified hardware. Additionally, both classical and data-driven methods\nare not robust to the highly opaque self-cast shadows on the object of\ninterest. We show that, as the target body rotates, these shadows may lead to\nlarge biases in the resulting pose estimates. For these objects, a bias in the\nreal-time pose estimation algorithm may mislead the spacecraft's state\nestimator and cause a mission failure, especially if the body undergoes a\nchaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast\nFEature Extractor, a real-time pose estimation framework for asteroids designed\nto leverage prior information on the sun phase angle given by sun-tracking\nsensors commonly available onboard spacecraft. By associating salient contours\nto their projected shadows, a sparse set of features are detected, invariant to\nthe motion of the shadows. A Sparse Neural Network followed by an\nattention-based Graph Neural Network feature matching model are then jointly\ntrained to provide a set of correspondences between successive frames. The\nresulting pose estimation pipeline is found to be bias-free, more accurate than\nclassical pose estimation pipelines and an order of magnitude faster than other\nstate-of-the-art deep learning pipelines on synthetic data as well as on\nrenderings of the tumbling asteroid Apophis.", "AI": {"tldr": "COFFEE是一种实时小行星姿态估计框架，通过利用太阳相位角信息，提取对阴影运动不变的特征，并结合稀疏神经网络和图神经网络进行特征匹配，实现了无偏差、高精度且计算效率远超现有方法的姿态估计。", "motivation": "空间未知天体（如空间碎片、小行星）的精确状态估计是关键挑战。现有方法存在问题：传统方法（SIFT、ORB、AKAZE）实时但精度不足；现代深度学习方法精度高但计算资源需求大，不适用于航天硬件；最重要的是，两者都无法有效处理物体上的自投阴影，阴影会导致姿态估计产生大的偏差，进而可能导致航天器状态估计器误判，甚至任务失败。", "method": "提出COFFEE（Celestial Occlusion Fast FEature Extractor）框架。该方法利用航天器上常见的太阳追踪传感器提供的太阳相位角先验信息，将显著轮廓与其投影阴影关联起来，检测出一组对阴影运动不变的稀疏特征。随后，一个稀疏神经网络与一个基于注意力的图神经网络特征匹配模型被联合训练，以提供连续帧之间的对应关系。", "result": "实验结果表明，COFFEE姿态估计算法是无偏差的，比经典姿态估计管道更准确，并且在合成数据以及翻滚小行星Apophis的渲染图上，比其他最先进的深度学习管道快一个数量级。", "conclusion": "COFFEE框架成功解决了空间天体姿态估计中阴影导致的偏差问题，并在计算效率和精度上取得了显著提升，为小行星的实时、鲁棒、高精度姿态估计提供了有效的解决方案。"}}
{"id": "2508.03167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03167", "abs": "https://arxiv.org/abs/2508.03167", "authors": ["Charles Tapley Hoyt", "Craig Bakker", "Richard J. Callahan", "Joseph Cottam", "August George", "Benjamin M. Gyori", "Haley M. Hummel", "Nathaniel Merrill", "Sara Mohammad Taheri", "Pruthvi Prakash Navada", "Marc-Antoine Parent", "Adam Rupe", "Olga Vitek", "Jeremy Zucker"], "title": "Causal identification with $Y_0$", "comment": null, "summary": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0.", "AI": {"tldr": "Y0是一个Python软件包，旨在帮助研究人员定性地识别因果关系，确定因果效应是否可从现有数据中估计，并指导将因果查询转化为可估计的符号表达式。", "motivation": "研究的动机是为了解决在进行因果效应量化估计之前，如何判断因果关系是否可估计以及如何将因果查询转化为可估计形式的问题，尤其是在面对来自不同来源（RCT、观察性研究或混合）的数据时。", "method": "Y0通过实现多种因果识别算法，提供领域特定语言来表示因果查询和可估计量，支持表示带有未观测混杂因素的因果图模型（如ADMGs），并能处理来自随机对照试验、观察性研究或混合数据。", "result": "Y0提供了一个可用的Python软件包，能够对干预性、反事实和可迁移性查询进行因果识别，帮助研究人员在量化估计前进行定性分析，并指导他们将因果查询转换为非参数可估计的符号表达式。", "conclusion": "Y0软件包为因果推断领域提供了一个实用工具，使研究人员能够系统地探索因果关系的可估计性，并从不同类型的数据中推导出可估计的因果表达式，从而在定量分析之前奠定坚实的基础。"}}
{"id": "2508.03333", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03333", "abs": "https://arxiv.org/abs/2508.03333", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Tao Chen"], "title": "CTTS: Collective Test-Time Scaling", "comment": null, "summary": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM.", "AI": {"tldr": "本文探索了集体测试时缩放（CTTS）以提升LLM性能，提出并验证了三种范式，发现多智能体对多奖励模型（MA-MR）表现最佳。基于此，提出CTTS-MM框架，通过智能体协作搜索（ACS）和奖励模型混合（MoR）进一步增强推理效果。", "motivation": "测试时缩放（TTS）无需额外训练即可提升大型语言模型（LLM）性能，但现有方法如Best-of-N和Self-Consistency受限于单智能体单奖励模型（SA-SR）范式。近期研究表明，集体智能体方法能突破单智能体系统的上限，因此有必要探索集体测试时缩放（CTTS）。", "method": "本文首次探索集体测试时缩放（CTTS），设计了三种主要范式来研究其最优模式：1) 单智能体对多奖励模型（SA-MR）；2) 多智能体对单奖励模型（MA-SR）；3) 多智能体对多奖励模型（MA-MR）。在此基础上，提出CTTS-MM框架，其中包含：多智能体协作的“智能体协作搜索（ACS）”模块，用于从候选池中选择最有效的LLM智能体组合；以及多奖励模型协作的“奖励模型混合（MoR）”模块，通过精心策划的问题池和“先验奖励模型集成选择（PRES）”以及“成对奖励排名（PRR）”指标来选择最佳奖励模型组合。", "result": "广泛实验表明，多智能体对多奖励模型（MA-MR）范式持续取得最佳性能。所提出的CTTS-MM框架在七个主流基准测试中均持续获得卓越的性能。", "conclusion": "集体测试时缩放（CTTS），特别是多智能体对多奖励模型（MA-MR）范式，是提升LLM性能的有效途径。所提出的CTTS-MM框架通过结合多智能体和多奖励模型协作，显著增强了LLM的推理能力。"}}
{"id": "2508.03069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03069", "abs": "https://arxiv.org/abs/2508.03069", "authors": ["Bo Zhang", "Yifan Zhang", "Shuo Yan", "Yu Bai", "Zheng Zhang", "Wu Liu", "Xiuzhuang Zhou", "Wendong Wang"], "title": "SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation", "comment": null, "summary": "In light of the spatial domain's limited capacity for modeling global context\nin 3D medical image segmentation, emerging approaches have begun to incorporate\nfrequency domain representations. However, straightforward feature extraction\nstrategies often overlook the unique properties of frequency domain\ninformation, such as conjugate symmetry. They also fail to account for the\nfundamental differences in data distribution between the spatial and frequency\ndomains, which can ultimately dilute or obscure the complementary strengths\nthat frequency-based representations offer. In this paper, we propose SSFMamba,\na Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D\nmedical image segmentation. SSFMamba employs a complementary dual-branch\narchitecture that extracts features from both the spatial and frequency\ndomains, and leverages a Mamba block to fuse these heterogeneous features to\npreserve global context while reinforcing local details. In the frequency\ndomain branch, we harness Mamba's exceptional capability to extract global\ncontextual information in conjunction with the synergistic effect of frequency\ndomain features to further enhance global modeling. Moreover, we design a 3D\nmulti-directional scanning mechanism to strengthen the fusion of local and\nglobal cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets\ndemonstrate that our approach consistently outperforms state-of-the-art methods\nacross various evaluation metrics.", "AI": {"tldr": "SSFMamba是一种基于Mamba的空间-频率特征融合网络，用于3D医学图像分割，通过双分支架构和新的扫描机制，有效融合空间和频率域信息，提升了分割性能。", "motivation": "现有3D医学图像分割方法在空间域中难以建模全局上下文，而频率域方法常忽略其特有属性（如共轭对称性）及与空间域的数据分布差异，导致频率域的互补优势未能充分发挥或被稀释。", "method": "提出SSFMamba网络，采用双分支架构分别从空间域和频率域提取特征。利用Mamba模块融合异构特征，以保留全局上下文并增强局部细节。在频率域分支中，结合Mamba的全局上下文提取能力和频率域特征的协同效应，进一步增强全局建模。设计了3D多方向扫描机制以强化局部和全局信息的融合。", "result": "在BraTS2020和BraTS2023数据集上的大量实验表明，SSFMamba在各种评估指标上均持续优于最先进的方法。", "conclusion": "SSFMamba通过结合空间和频率域特征，并利用Mamba的强大能力和创新的扫描机制，有效解决了3D医学图像分割中全局上下文建模的挑战，实现了卓越的分割性能。"}}
{"id": "2508.03331", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03331", "abs": "https://arxiv.org/abs/2508.03331", "authors": ["Amirreza Rouhi", "Sneh Patel", "Noah McCarthy", "Siddiqa Khan", "Hadi Khorsand", "Kaleb Lefkowitz", "David K. Han"], "title": "LRDDv2: Enhanced Long-Range Drone Detection Dataset with Range Information and Comprehensive Real-World Challenges", "comment": "Accepted and presented at ISRR 2024", "summary": "The exponential growth in Unmanned Aerial Vehicles (UAVs) usage underscores\nthe critical need of detecting them at extended distances to ensure safe\noperations, especially in densely populated areas. Despite the tremendous\nadvances made in computer vision through deep learning, the detection of these\nsmall airborne objects remains a formidable challenge. While several datasets\nhave been developed specifically for drone detection, the need for a more\nextensive and diverse collection of drone image data persists, particularly for\nlong-range detection under varying environmental conditions. We introduce here\nthe Long Range Drone Detection (LRDD) Version 2 dataset, comprising 39,516\nmeticulously annotated images, as a second release of the LRDD dataset released\npreviously. The LRDDv2 dataset enhances the LRDDv1 by incorporating a greater\nvariety of images, providing a more diverse and comprehensive resource for\ndrone detection research. What sets LRDDv2 apart is its inclusion of target\nrange information for over 8,000 images, making it possible to develop\nalgorithms for drone range estimation. Tailored for long-range aerial object\ndetection, the majority of LRDDv2's dataset consists of images capturing drones\nwith 50 or fewer pixels in 1080p resolution. For access to the complete\nLong-Range Drone Detection Dataset (LRDD)v2, please visit\nhttps://research.coe.drexel.edu/ece/imaple/lrddv2/ .", "AI": {"tldr": "本文介绍了LRDDv2数据集，一个包含39,516张精心标注图像的无人机检测数据集，旨在解决长距离无人机检测和测距的挑战。", "motivation": "无人机使用量呈指数级增长，对在远距离检测无人机以确保安全操作的需求日益迫切，尤其是在人口密集区域。尽管深度学习在计算机视觉方面取得了巨大进步，但检测这些小型空中目标仍是一个巨大挑战。现有无人机检测数据集缺乏多样性和广度，特别是在不同环境条件下的长距离检测方面。", "method": "研究者通过细致标注39,516张图像，构建了LRDDv2数据集，作为先前发布的LRDDv1的增强版本。LRDDv2增加了图像的多样性，并首次包含了8,000多张图像的目标距离信息，以便开发无人机距离估计算法。该数据集主要包含在1080p分辨率下仅占50像素或更小的无人机图像，专为长距离空中目标检测设计。", "result": "LRDDv2数据集包含39,516张精心标注的图像，提供了比LRDDv1更丰富和多样化的无人机检测资源。其中，超过8,000张图像包含目标距离信息，使得开发无人机距离估计算法成为可能。数据集中的大多数图像捕捉的是在1080p分辨率下像素尺寸小于或等于50的无人机，非常适合长距离空中目标检测研究。", "conclusion": "LRDDv2数据集为长距离无人机检测研究提供了一个更广泛、更全面的资源，并通过包含目标距离信息，为无人机距离估计算法的开发开辟了新途径。这有助于推动计算机视觉在小型空中目标检测领域的进步，从而提高无人机操作的安全性。"}}
{"id": "2508.03173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03173", "abs": "https://arxiv.org/abs/2508.03173", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Honghao He", "Linzhuang Sun", "Conghui He", "Lijun Wu", "Bihui Yu", "Cheng Tan"], "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions", "comment": null, "summary": "Mathematical geometric reasoning is essential for scientific discovery and\neducational development, requiring precise logic and rigorous formal\nverification. While recent advances in Multimodal Large Language Models (MLLMs)\nhave improved reasoning tasks, existing models typically struggle with formal\ngeometric reasoning, particularly when dynamically constructing and verifying\nauxiliary geometric elements. To address these challenges, we introduce\nGeoint-R1, a multimodal reasoning framework designed to generate formally\nverifiable geometric solutions from textual descriptions and visual diagrams.\nGeoint-R1 uniquely integrates auxiliary elements construction, formal reasoning\nrepresented via Lean4, and interactive visualization. To systematically\nevaluate and advance formal geometric reasoning, we propose the Geoint\nbenchmark, comprising 1,885 rigorously annotated geometry problems across\ndiverse topics such as plane, spatial, and solid geometry. Each problem\nincludes structured textual annotations, precise Lean4 code for auxiliary\nconstructions, and detailed solution steps verified by experts. Extensive\nexperiments demonstrate that Geoint-R1 significantly surpasses existing\nmultimodal and math-specific reasoning models, particularly on challenging\nproblems requiring explicit auxiliary element constructions.", "AI": {"tldr": "该论文提出了Geoint-R1多模态推理框架和Geoint基准，旨在解决现有大模型在形式化几何推理中动态构造和验证辅助几何元素的不足。实验证明Geoint-R1在几何推理任务上，尤其在需要辅助元素构造的难题上，显著超越了现有模型。", "motivation": "数学几何推理对科学发现和教育发展至关重要，需要精确逻辑和严格的形式验证。然而，现有多模态大语言模型（MLLMs）在形式化几何推理，尤其是动态构造和验证辅助几何元素方面表现不足。", "method": "本文引入了Geoint-R1多模态推理框架，该框架能从文本描述和视觉图表中生成可形式化验证的几何解。Geoint-R1独特地整合了辅助元素构造、通过Lean4表示的形式化推理以及交互式可视化。为系统评估和推进形式化几何推理，还提出了Geoint基准，包含1,885个经过严格标注的几何问题，每个问题都包含结构化文本标注、精确的Lean4辅助构造代码和专家验证的详细解题步骤。", "result": "广泛实验表明，Geoint-R1显著超越了现有多模态和数学专用推理模型，特别是在需要明确辅助元素构造的挑战性问题上表现突出。", "conclusion": "Geoint-R1框架和Geoint基准的提出，有效提升了多模态模型在形式化几何推理，特别是辅助元素构造方面的能力，为该领域的研究和发展提供了新的工具和评估标准。"}}
{"id": "2508.03358", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03358", "abs": "https://arxiv.org/abs/2508.03358", "authors": ["Tiago G Canário", "Catarina Duarte", "Flávio L. Pinheiro", "João L. M. Pereira"], "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature", "comment": "24 pages, 5 Figures, 4 Tables", "summary": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2", "AI": {"tldr": "本文提出了一个名为Taggus的流水线，用于从葡萄牙语文学作品中自动识别人物及其互动，并构建社交网络。该方法结合词性标注和启发式规则，在人物识别和共指消解方面F1分数达到94.1%，在互动检测方面达到75.9%，显著优于现有SOTA工具。", "motivation": "自动识别小说中的人物及其互动是一项复杂任务，现有NLP方法（如NER）在此任务中表现不佳，尤其是在葡萄牙语等数据标注较少的语言中，导致构建人物社交网络困难。", "method": "研究人员提出了Taggus流水线，该流水线不依赖于现成的NER工具或大型语言模型，而是利用词性标注（POS tagging）和一系列启发式规则来识别人物并解决共指问题，进而检测人物间的互动。", "result": "Taggus流水线在人物识别和共指消解任务中取得了94.1%的平均F1分数，在互动检测任务中取得了75.9%的F1分数。与现有SOTA工具（包括NER工具和ChatGPT）相比，Taggus在人物识别和共指消解方面提升了50.7%，在互动检测方面提升了22.3%。", "conclusion": "Taggus流水线在葡萄牙语文学作品的人物及互动提取方面表现出色，显著超越了现有最先进的工具。该研究为低资源语言的角色网络构建提供了有效解决方案，并已公开Taggus流水线以促进该领域的发展。未来工作将探索人物间关系的检测。"}}
{"id": "2508.03077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03077", "abs": "https://arxiv.org/abs/2508.03077", "authors": ["Anran Wu", "Long Peng", "Xin Di", "Xueyuan Dai", "Chen Wu", "Yang Wang", "Xueyang Fu", "Yang Cao", "Zheng-Jun Zha"], "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions", "comment": null, "summary": "Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of\noptimization-based 3DGS by enabling fast and high-quality reconstruction\nwithout the need for per-scene optimization. However, existing feedforward\napproaches typically assume that input multi-view images are clean and\nhigh-quality. In real-world scenarios, images are often captured under\nchallenging conditions such as noise, low light, or rain, resulting in\ninaccurate geometry and degraded 3D reconstruction. To address these\nchallenges, we propose a general and efficient multi-view feature enhancement\nmodule, RobustGS, which substantially improves the robustness of feedforward\n3DGS methods under various adverse imaging conditions, enabling high-quality 3D\nreconstruction. The RobustGS module can be seamlessly integrated into existing\npretrained pipelines in a plug-and-play manner to enhance reconstruction\nrobustness. Specifically, we introduce a novel component, Generalized\nDegradation Learner, designed to extract generic representations and\ndistributions of multiple degradations from multi-view inputs, thereby\nenhancing degradation-awareness and improving the overall quality of 3D\nreconstruction. In addition, we propose a novel semantic-aware state-space\nmodel. It first leverages the extracted degradation representations to enhance\ncorrupted inputs in the feature space. Then, it employs a semantic-aware\nstrategy to aggregate semantically similar information across different views,\nenabling the extraction of fine-grained cross-view correspondences and further\nimproving the quality of 3D representations. Extensive experiments demonstrate\nthat our approach, when integrated into existing methods in a plug-and-play\nmanner, consistently achieves state-of-the-art reconstruction quality across\nvarious types of degradations.", "AI": {"tldr": "RobustGS是一个多视图特征增强模块，显著提高了前向3D高斯泼溅（3DGS）方法在各种恶劣成像条件下的鲁棒性和重建质量。", "motivation": "现有的前向3DGS方法假设输入多视图图像是干净且高质量的，但在现实世界中，图像常在噪声、低光或雨等挑战性条件下捕获，导致几何不准确和3D重建质量下降。", "method": "本文提出了RobustGS，一个通用且高效的多视图特征增强模块，可即插即用地集成到现有预训练管线中。具体而言，它引入了一个“广义退化学习器”来提取多种退化的通用表示，并提出了一个“语义感知状态空间模型”，该模型首先利用退化表示在特征空间增强受损输入，然后采用语义感知策略聚合跨视图的语义相似信息，以提取细粒度的跨视图对应关系。", "result": "当集成到现有方法中时，RobustGS在各种类型的退化条件下始终实现最先进的重建质量。", "conclusion": "RobustGS显著提高了前向3DGS方法在恶劣成像条件下的鲁棒性，实现了高质量的3D重建。"}}
{"id": "2508.03669", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03669", "abs": "https://arxiv.org/abs/2508.03669", "authors": ["Katherine Liu", "Sergey Zakharov", "Dian Chen", "Takuya Ikeda", "Greg Shakhnarovich", "Adrien Gaidon", "Rares Ambrus"], "title": "OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World", "comment": "8 pages, 5 figures. This version has typo fixes on top of the version\n  published at ICRA 2025", "summary": "We would like to estimate the pose and full shape of an object from a single\nobservation, without assuming known 3D model or category. In this work, we\npropose OmniShape, the first method of its kind to enable probabilistic pose\nand shape estimation. OmniShape is based on the key insight that shape\ncompletion can be decoupled into two multi-modal distributions: one capturing\nhow measurements project into a normalized object reference frame defined by\nthe dataset and the other modelling a prior over object geometries represented\nas triplanar neural fields. By training separate conditional diffusion models\nfor these two distributions, we enable sampling multiple hypotheses from the\njoint pose and shape distribution. OmniShape demonstrates compelling\nperformance on challenging real world datasets. Project website:\nhttps://tri-ml.github.io/omnishape", "AI": {"tldr": "提出OmniShape，一种从单次观测中概率性估计物体姿态和完整形状的新方法，无需已知3D模型或类别。", "motivation": "在没有已知3D模型或类别假设的情况下，从单次观测中估计物体的姿态和完整形状。", "method": "OmniShape将形状补全解耦为两个多模态分布：测量如何投影到归一化对象参考系，以及对象几何形状的先验（表示为三平面神经场）。通过为这两个分布训练独立的条件扩散模型，实现从联合姿态和形状分布中采样多个假设。", "result": "在具有挑战性的真实世界数据集中展示了引人注目的性能。", "conclusion": "OmniShape是首个实现从单次观测中概率性估计物体姿态和完整形状的方法，通过解耦的扩散模型能够生成多个假设，有效解决了无先验模型下的形状和姿态估计问题。"}}
{"id": "2508.03174", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03174", "abs": "https://arxiv.org/abs/2508.03174", "authors": ["Tian-Fang Zhao", "Wen-Xi Yang"], "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation", "comment": null, "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.", "AI": {"tldr": "本文提出了一个名为InqEduAgent的LLM驱动智能体模型，用于模拟和选择探究式学习的理想学习伙伴，并通过实验验证了其在多数知识学习场景中的优越性能。", "motivation": "在探究式教育中，协作伙伴关系至关重要。然而，目前的学习伙伴选择方法要么依赖经验分配（缺乏科学规划），要么基于规则的机器助手（存在知识扩展困难和灵活性不足），因此需要更智能、自适应的解决方案。", "method": "研究提出了InqEduAgent模型，该模型包含：1) 生成式智能体，用于捕捉学习者在真实场景中的认知和评估特征；2) 一种自适应匹配算法，结合高斯过程增强，以识别先验知识中的模式，从而为面临不同练习的学习者提供最佳学习伙伴匹配。", "result": "实验结果表明，InqEduAgent在大多数知识学习场景以及不同能力水平的LLM环境中都表现出最佳性能。", "conclusion": "这项研究推动了基于人类学习伙伴的智能分配以及基于AI学习伙伴的构建，为探究式教育中的学习伙伴选择提供了新的智能化途径。"}}
{"id": "2508.03363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03363", "abs": "https://arxiv.org/abs/2508.03363", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.", "AI": {"tldr": "本文提出了一种名为JointThinking的新型上下文学习（ICL）范式，通过利用“思考”和“不思考”两种推理模式之间的结构化差异，显著提高了大型语言模型（RLLMs）的推理准确性和鲁棒性，同时保持了较低的延迟开销。", "motivation": "尽管推理型大型语言模型（RLLMs）在结构化和多步推理方面表现出色，但其在上下文学习（ICL）方面的潜力仍未得到充分探索，现有研究主要集中于改进训练和推理策略。", "method": "提出JointThinking范式：模型并行生成两个答案，一个在“思考”模式下，一个在“不思考”模式下。仅当两个初始回答不一致时（这种情况很少发生，如GSM8K中仅占6%），才触发第二轮“思考”，此时使用一个结合了原始问题和两个候选答案的单一提示。由于大多数情况下只需一轮推理，因此延迟开销极小。", "result": "JointThinking在多个推理基准测试中显著优于少样本思维链（CoT）和多数投票，并提升了答案鲁棒性。其在同分布任务上表现与基于训练的SOTA方法相当，但在异分布任务上则显著优于SOTA。系统分析表明，利用不同推理模式能持续降低错误率，且第二轮思考中，实际与理想推理之间的性能差距随模型规模增大而缩小，显示出良好的可扩展性。", "conclusion": "JointThinking是一种有效且可扩展的RLLM上下文学习方法，通过利用不同推理模式间的结构化差异，在提高推理准确性和鲁棒性方面表现出色，尤其在异分布任务上优势明显，并强调了结构化思维多样性的价值。本文也讨论了当前局限性并展望了未来ICL研究方向。"}}
{"id": "2508.03079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03079", "abs": "https://arxiv.org/abs/2508.03079", "authors": ["Zaiying Zhao", "Toshihiko Yamasaki"], "title": "Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models", "comment": "Accepted to the Responsible Generative AI (ReGenAI) Workshop, CVPR\n  2025", "summary": "The rapid expansion of applications using Large Vision-Language Models\n(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.\nWhile existing studies primarily focus on demographic attributes such as race\nand gender, fairness across a broader range of attributes remains largely\nunexplored. In this study, we construct an open-set knowledge base of bias\nattributes leveraging Large Language Models (LLMs) and evaluate the fairness of\nLVLMs across finer-grained attributes. Our experimental results reveal that\nLVLMs exhibit biased outputs across a diverse set of attributes and further\ndemonstrate that cultural, environmental, and behavioral factors have a more\npronounced impact on LVLM decision-making than traditional demographic\nattributes.", "AI": {"tldr": "研究发现大型视觉-语言模型（LVLMs）在更广泛的细粒度属性上存在偏见，且文化、环境和行为因素对其决策影响比传统人口属性更显著。", "motivation": "随着大型视觉-语言模型（LVLMs）应用迅速扩展，其公平性问题日益突出。现有研究主要关注种族和性别等人口统计属性，但对更广泛属性的公平性探索不足。", "method": "本研究利用大型语言模型（LLMs）构建了一个开放式偏见属性知识库，并评估了LVLMs在更细粒度属性上的公平性。", "result": "实验结果表明，LVLMs在多样化属性上表现出偏见输出。此外，文化、环境和行为因素对LVLM决策的影响比传统人口统计属性更为显著。", "conclusion": "LVLMs的公平性问题不仅限于传统的人口统计属性，更受文化、环境和行为等细粒度因素的显著影响，这提示未来研究需关注更全面的偏见维度。"}}
{"id": "2508.03690", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03690", "abs": "https://arxiv.org/abs/2508.03690", "authors": ["Youquan Liu", "Lingdong Kong", "Weidong Yang", "Ao Liang", "Jianxiong Gao", "Yang Wu", "Xiang Xu", "Xin Li", "Linfeng Li", "Runnan Chen", "Ben Fei"], "title": "Veila: Panoramic LiDAR Generation from a Monocular RGB Image", "comment": "Preprint; 10 pages, 6 figures, 7 tables", "summary": "Realistic and controllable panoramic LiDAR data generation is critical for\nscalable 3D perception in autonomous driving and robotics. Existing methods\neither perform unconditional generation with poor controllability or adopt\ntext-guided synthesis, which lacks fine-grained spatial control. Leveraging a\nmonocular RGB image as a spatial control signal offers a scalable and low-cost\nalternative, which remains an open problem. However, it faces three core\nchallenges: (i) semantic and depth cues from RGB are vary spatially,\ncomplicating reliable conditioning generation; (ii) modality gaps between RGB\nappearance and LiDAR geometry amplify alignment errors under noisy diffusion;\nand (iii) maintaining structural coherence between monocular RGB and panoramic\nLiDAR is challenging, particularly in non-overlap regions between images and\nLiDAR. To address these challenges, we propose Veila, a novel conditional\ndiffusion framework that integrates: a Confidence-Aware Conditioning Mechanism\n(CACM) that strengthens RGB conditioning by adaptively balancing semantic and\ndepth cues according to their local reliability; a Geometric Cross-Modal\nAlignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a\nPanoramic Feature Coherence (PFC) for enforcing global structural consistency\nacross monocular RGB and panoramic LiDAR. Additionally, we introduce two\nmetrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to\nevaluate alignment quality across modalities. Experiments on nuScenes,\nSemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila\nachieves state-of-the-art generation fidelity and cross-modal consistency,\nwhile enabling generative data augmentation that improves downstream LiDAR\nsemantic segmentation.", "AI": {"tldr": "Veila是一个新颖的条件扩散框架，它利用单目RGB图像作为空间控制信号，生成逼真且可控的全景LiDAR数据，解决了现有方法在可控性和跨模态对齐方面的挑战。", "motivation": "自动驾驶和机器人技术中，可扩展的3D感知需要逼真且可控的全景LiDAR数据生成。现有方法要么缺乏可控性，要么通过文本引导合成，缺乏精细的空间控制。利用单目RGB图像作为空间控制信号是一种可扩展且低成本的替代方案，但面临三个核心挑战：RGB语义和深度线索的空间变化性、RGB外观和LiDAR几何之间的模态差距，以及单目RGB和全景LiDAR之间结构一致性的维护。", "method": "本文提出了Veila框架，包含：1. 置信度感知条件机制（CACM），根据局部可靠性自适应平衡语义和深度线索，增强RGB条件作用；2. 几何跨模态对齐（GCMA），在噪声扩散下实现鲁棒的RGB-LiDAR对齐；3. 全景特征一致性（PFC），强制单目RGB和全景LiDAR之间的全局结构一致性。此外，引入了跨模态语义一致性和跨模态深度一致性两个新度量来评估对齐质量。", "result": "在nuScenes、SemanticKITTI和提出的KITTI-Weather基准测试中，Veila实现了最先进的生成保真度和跨模态一致性。实验还表明，通过生成数据增强，可以提高下游LiDAR语义分割的性能。", "conclusion": "Veila成功地解决了单目RGB引导全景LiDAR数据生成中的关键挑战，实现了高保真度、强一致性的数据生成，并能有效提升下游3D感知任务的性能，为自动驾驶和机器人领域提供了有价值的数据增强工具。"}}
{"id": "2508.03251", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03251", "abs": "https://arxiv.org/abs/2508.03251", "authors": ["Osama Mohammed", "Jiaxin Pan", "Mojtaba Nayyeri", "Daniel Hernández", "Steffen Staab"], "title": "Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning", "comment": "European Conference of Artificial Intelligence 2025", "summary": "Modeling evolving interactions among entities is critical in many real-world\ntasks. For example, predicting driver maneuvers in traffic requires tracking\nhow neighboring vehicles accelerate, brake, and change lanes relative to one\nanother over consecutive frames. Likewise, detecting financial fraud hinges on\nfollowing the flow of funds through successive transactions as they propagate\nthrough the network. Unlike classic time-series forecasting, these settings\ndemand reasoning over who interacts with whom and when, calling for a\ntemporal-graph representation that makes both the relations and their evolution\nexplicit. Existing temporal-graph methods typically use snapshot graphs to\nencode temporal evolution. We introduce a full-history graph that instantiates\none node for every entity at every time step and separates two edge sets: (i)\nintra-time-step edges that capture relations within a single frame and (ii)\ninter-time-step edges that connect an entity to itself at consecutive steps. To\nlearn on this graph we design an Edge-Type Decoupled Network (ETDNet) with\nparallel modules: a graph-attention module aggregates information along\nintra-time-step edges, a multi-head temporal-attention module attends over an\nentity's inter-time-step history, and a fusion module combines the two messages\nafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin\nfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,\nlifting Waymo joint accuracy to 75.6\\% (vs. 74.1\\%) and raising Elliptic++\nillicit-class F1 to 88.1\\% (vs. 60.4\\%). These gains demonstrate the benefit of\nrepresenting structural and temporal relations as distinct edges in a single\ngraph.", "AI": {"tldr": "该论文提出了一种名为“全历史图”的新型时间图表示方法，以及一个名为ETDNet的神经网络模型，用于建模实体之间随时间演变的交互，并在驾驶员意图预测和金融欺诈检测任务上取得了显著优于基线的性能。", "motivation": "在许多现实世界任务中，建模实体之间不断演变的交互至关重要，例如预测驾驶员行为或检测金融欺诈。传统时间序列预测方法难以捕捉“谁在何时与谁交互”的复杂关系，需要一种能明确表示关系及其演变的时间图表示。", "method": "该研究引入了一种“全历史图”表示，为每个实体在每个时间步实例化一个节点，并分离两种边：(i) 时间步内边，捕捉单个帧内的关系；(ii) 时间步间边，连接实体在连续时间步的自身。在此图上，设计了“边类型解耦网络 (ETDNet)”，包含并行模块：一个图注意力模块处理时间步内边，一个多头时间注意力模块处理实体的时间步间历史，以及一个融合模块在每层之后结合两种信息。", "result": "ETDNet在驾驶员意图预测（Waymo数据集）和比特币欺诈检测（Elliptic++数据集）任务上进行了评估。结果显示，ETDNet持续超越了强基线，将Waymo的联合准确率提升至75.6%（对比74.1%），并将Elliptic++的非法类别F1分数提高到88.1%（对比60.4%）。", "conclusion": "研究结果表明，在单个图中将结构关系和时间关系表示为不同的边，能够显著提升模型性能，证明了这种表示方法的有效性。"}}
{"id": "2508.03399", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03399", "abs": "https://arxiv.org/abs/2508.03399", "authors": ["Eliseo Bao", "Anxo Pérez", "Javier Parapar"], "title": "ReDSM5: A Reddit Dataset for DSM-5 Depression Detection", "comment": "Accepted as a resource paper at CIKM 2025", "summary": "Depression is a pervasive mental health condition that affects hundreds of\nmillions of individuals worldwide, yet many cases remain undiagnosed due to\nbarriers in traditional clinical access and pervasive stigma. Social media\nplatforms, and Reddit in particular, offer rich, user-generated narratives that\ncan reveal early signs of depressive symptomatology. However, existing\ncomputational approaches often label entire posts simply as depressed or not\ndepressed, without linking language to specific criteria from the DSM-5, the\nstandard clinical framework for diagnosing depression. This limits both\nclinical relevance and interpretability. To address this gap, we introduce\nReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each\nexhaustively annotated at the sentence level by a licensed psychologist for the\nnine DSM-5 depression symptoms. For each label, the annotator also provides a\nconcise clinical rationale grounded in DSM-5 methodology. We conduct an\nexploratory analysis of the collection, examining lexical, syntactic, and\nemotional patterns that characterize symptom expression in social media\nnarratives. Compared to prior resources, ReDSM5 uniquely combines\nsymptom-specific supervision with expert explanations, facilitating the\ndevelopment of models that not only detect depression but also generate\nhuman-interpretable reasoning. We establish baseline benchmarks for both\nmulti-label symptom classification and explanation generation, providing\nreference results for future research on detection and interpretability.", "AI": {"tldr": "该论文介绍了一个名为ReDSM5的新型Reddit语料库，通过心理学家对帖子进行句子级DSM-5抑郁症症状标注和临床解释，旨在提高社交媒体抑郁症检测的临床相关性和可解释性。", "motivation": "抑郁症诊断存在传统临床障碍和污名化问题，导致许多病例未被确诊。现有社交媒体抑郁症计算方法通常只进行二元分类（抑郁/非抑郁），未能将语言与DSM-5的具体症状标准关联，缺乏临床相关性和可解释性。", "method": "构建ReDSM5语料库，包含1484篇长篇Reddit帖子，由持证心理学家在句子层面针对DSM-5的九种抑郁症症状进行详尽标注，并为每个标签提供简洁的临床理由。对语料库进行探索性分析，考察词汇、句法和情感模式。为多标签症状分类和解释生成建立了基准线。", "result": "ReDSM5独特地结合了症状特异性监督和专家解释，促进了能够检测抑郁症并生成人类可解释推理的模型开发。通过探索性分析揭示了社交媒体叙事中症状表达的模式。为未来的检测和可解释性研究提供了基准结果。", "conclusion": "ReDSM5语料库通过提供与DSM-5症状挂钩的专家标注和解释，弥补了现有方法的不足，为开发具有临床相关性和可解释性的抑郁症检测模型奠定了基础，并为后续研究提供了可参考的基准。"}}
{"id": "2508.03081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03081", "abs": "https://arxiv.org/abs/2508.03081", "authors": ["Bo Zhang", "Xu Xinan", "Shuo Yan", "Yu Bai", "Zheng Zhang", "Wufan Wang", "Wendong Wang"], "title": "Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification", "comment": null, "summary": "Recent pseudo-bag augmentation methods for Multiple Instance Learning\n(MIL)-based Whole Slide Image (WSI) classification sample instances from a\nlimited number of bags, resulting in constrained diversity. To address this\nissue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sample\ninstances from all bags with the same class to increase the diversity of\npseudo-bags. However, introducing new instances into the pseudo-bag increases\nthe number of critical instances (e.g., tumor instances). This increase results\nin a reduced occurrence of pseudo-bags containing few critical instances,\nthereby limiting model performance, particularly on test slides with small\ntumor areas. To address this, we introduce a bag-level and group-level\ncontrastive learning framework to enhance the discrimination of features with\ndistinct semantic meanings, thereby improving model performance. Experimental\nresults demonstrate that $C^2Aug$ consistently outperforms state-of-the-art\napproaches across multiple evaluation metrics.", "AI": {"tldr": "本文提出了一种名为$C^2Aug$的对比跨包增强方法，通过从同类所有包中采样实例来增加伪包多样性，并结合包级和组级对比学习，以提高多实例学习(MIL)在全玻片图像(WSI)分类上的性能，尤其是在肿瘤区域较小的测试幻灯片上。", "motivation": "现有的伪包增强方法从有限数量的包中采样实例，导致多样性受限；同时，伪包中关键实例（如肿瘤实例）数量的增加，限制了模型在肿瘤区域较小的测试幻灯片上的性能。", "method": "1. 提出对比跨包增强($C^2Aug$)，从所有同类包中采样实例，以增加伪包多样性。2. 引入包级和组级对比学习框架，增强具有不同语义含义特征的判别能力。", "result": "实验结果表明，$C^2Aug$在多个评估指标上持续优于现有最先进的方法。", "conclusion": "$C^2Aug$通过增加伪包多样性和增强特征判别能力，有效提高了基于MIL的WSI分类性能，尤其在处理小肿瘤区域时表现出色。"}}
{"id": "2508.03691", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03691", "abs": "https://arxiv.org/abs/2508.03691", "authors": ["Youquan Liu", "Lingdong Kong", "Weidong Yang", "Xin Li", "Ao Liang", "Runnan Chen", "Ben Fei", "Tongliang Liu"], "title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data", "comment": "Preprint; 10 pages, 6 figures, 7 tables", "summary": "Controllable generation of realistic LiDAR scenes is crucial for applications\nsuch as autonomous driving and robotics. While recent diffusion-based models\nachieve high-fidelity LiDAR generation, they lack explicit control over\nforeground objects and spatial relationships, limiting their usefulness for\nscenario simulation and safety validation. To address these limitations, we\npropose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a\nnovel layout-guided generative framework that introduces semantic-enhanced\nscene graph diffusion with relation-aware contextual conditioning for\nstructured LiDAR layout generation, followed by foreground-aware control\ninjection for complete scene generation. This enables customizable control over\nobject placement while ensuring spatial and semantic consistency. To support\nour structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two\nlarge-scale LiDAR scene graph datasets, along with new evaluation metrics for\nlayout synthesis. Extensive experiments demonstrate that La La LiDAR achieves\nstate-of-the-art performance in both LiDAR generation and downstream perception\ntasks, establishing a new benchmark for controllable 3D scene generation.", "AI": {"tldr": "该论文提出了“La La LiDAR”模型，一个布局引导的生成框架，用于可控地生成逼真的激光雷达场景，解决了现有扩散模型在前景对象和空间关系控制方面的不足。", "motivation": "现有的基于扩散的激光雷达生成模型虽然保真度高，但缺乏对前景对象和空间关系的明确控制，这限制了它们在场景模拟和安全验证（如自动驾驶）中的应用。", "method": "本文提出了“La La LiDAR”模型，该模型通过引入语义增强的场景图扩散和关系感知的上下文条件来生成结构化的激光雷达布局，然后通过前景感知的控制注入完成整个场景的生成。为支持结构化激光雷达生成，论文还构建了Waymo-SG和nuScenes-SG两个大规模激光雷达场景图数据集，并提出了新的布局合成评估指标。", "result": "La La LiDAR在激光雷达生成和下游感知任务中均达到了最先进的性能，为可控的3D场景生成设立了新的基准。", "conclusion": "La La LiDAR通过其布局引导和前景感知控制机制，实现了对激光雷达场景的可定制控制，同时确保了空间和语义一致性，显著提升了可控3D场景生成的能力。"}}
{"id": "2508.03284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03284", "abs": "https://arxiv.org/abs/2508.03284", "authors": ["Shaofeng Yin", "Ting Lei", "Yang Liu"], "title": "ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools", "comment": null, "summary": "Integrating external tools into Large Foundation Models (LFMs) has emerged as\na promising approach to enhance their problem-solving capabilities. While\nexisting studies have demonstrated strong performance in tool-augmented Visual\nQuestion Answering (VQA), recent benchmarks reveal significant gaps in\nreal-world tool-use proficiency, particularly in functionally diverse\nmultimodal settings requiring multi-step reasoning. In this work, we introduce\nToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to\nbridge this gap. Unlike previous datasets that rely on synthetic scenarios and\nsimplified queries, ToolVQA features real-world visual contexts and challenging\nimplicit multi-step reasoning tasks, better aligning with real user\ninteractions. To construct this dataset, we propose ToolEngine, a novel data\ngeneration pipeline that employs Depth-First Search (DFS) with a dynamic\nin-context example matching mechanism to simulate human-like tool-use\nreasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task\ndomains, with an average inference length of 2.78 reasoning steps per instance.\nThe fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on\nour test set but also surpass the large close-sourced model GPT-3.5-turbo on\nvarious out-of-distribution (OOD) datasets, demonstrating strong\ngeneralizability to real-world tool-use scenarios.", "AI": {"tldr": "该研究引入了ToolVQA，一个包含2.3万实例的大规模多模态数据集，旨在弥补现有LFM工具使用基准在真实世界多步推理方面的不足。ToolVQA通过ToolEngine生成，并展示了在ToolVQA上微调的7B LFM在OOD数据集上超越GPT-3.5-turbo的强大泛化能力。", "motivation": "尽管现有研究在工具增强型视觉问答(VQA)中表现出色，但最近的基准测试揭示了大型基础模型(LFMs)在真实世界、功能多样、需要多步推理的多模态环境中，其工具使用能力存在显著差距。这促使了对更贴近真实用户交互的数据集的需求。", "method": "研究引入了ToolVQA数据集，该数据集包含2.3万个实例，采用真实世界视觉上下文和具有挑战性的隐式多步推理任务。为构建该数据集，提出了ToolEngine数据生成管道，该管道利用深度优先搜索(DFS)和动态上下文示例匹配机制来模拟人类般的工具使用推理。ToolVQA涵盖了7个不同任务领域的10种多模态工具，平均每个实例需要2.78个推理步骤。", "result": "在ToolVQA上微调的7B大型基础模型不仅在测试集上取得了令人印象深刻的性能，而且在各种域外(OOD)数据集上超越了大型闭源模型GPT-3.5-turbo。这表明了ToolVQA在模拟真实世界工具使用场景方面的强大泛化能力。", "conclusion": "ToolVQA数据集有效弥补了现有LFM工具使用基准在真实世界多模态、多步推理方面的差距。基于ToolVQA训练的模型展示了强大的泛化能力，能够有效应对真实世界的工具使用场景，甚至超越了更大型的闭源模型。"}}
{"id": "2508.03420", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.03420", "abs": "https://arxiv.org/abs/2508.03420", "authors": ["Bing Wang", "Ximing Li", "Yiming Wang", "Changchun Li", "Jiaxu Cui", "Renchu Guan", "Bo Yang"], "title": "Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations", "comment": "Accepted by CIKM 2025. 11 pages, 4 figures. Code:\n  https://github.com/wangbing1416/MISDER", "summary": "The proliferation of misinformation across diverse social media platforms has\ndrawn significant attention from both academic and industrial communities due\nto its detrimental effects. Accordingly, automatically distinguishing\nmisinformation, dubbed as Misinformation Detection (MD), has become an\nincreasingly active research topic. The mainstream methods formulate MD as a\nstatic learning paradigm, which learns the mapping between the content, links,\nand propagation of news articles and the corresponding manual veracity labels.\nHowever, the static assumption is often violated, since in real-world\nscenarios, the veracity of news articles may vacillate within the dynamically\nevolving social environment. To tackle this problem, we propose a novel\nframework, namely Misinformation detection with Dynamic Environmental\nRepresentations (MISDER). The basic idea of MISDER lies in learning a social\nenvironmental representation for each period and employing a temporal model to\npredict the representation for future periods. In this work, we specify the\ntemporal model as the LSTM model, continuous dynamics equation, and pre-trained\ndynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,\nMISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,\nwe compare it to various MD baselines across 2 prevalent datasets, and the\nexperimental results can indicate the effectiveness of our proposed model.", "AI": {"tldr": "针对社交媒体上新闻真实性动态变化的挑战，本文提出MISDER框架，通过学习并预测动态社会环境表示来改进谣言检测，并验证了其有效性。", "motivation": "社交媒体上的虚假信息泛滥且有害。主流的谣言检测方法将问题视为静态学习，但新闻的真实性在动态演变的社会环境中可能会波动，这与现实不符。", "method": "提出MISDER（Misinformation detection with Dynamic Environmental Representations）框架。核心思想是为每个时期学习一个社会环境表示，并使用时间模型预测未来时期的表示。具体时间模型包括LSTM、连续动力学方程和预训练动力学系统，形成MISDER-LSTM、MISDER-ODE和MISDER-PT三个变体。", "result": "在两个流行的数据集上与多种基线模型进行比较，实验结果表明所提出的MISDER模型是有效的。", "conclusion": "MISDER框架通过捕捉新闻真实性的动态变化，有效提升了谣言检测的性能，解决了现有静态检测方法的局限性。"}}
{"id": "2508.03094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03094", "abs": "https://arxiv.org/abs/2508.03094", "authors": ["Jiantao Tan", "Peixian Ma", "Kanghao Chen", "Zhiming Dai", "Ruixuan Wang"], "title": "Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts", "comment": null, "summary": "Continual learning is essential for medical image classification systems to\nadapt to dynamically evolving clinical environments. The integration of\nmultimodal information can significantly enhance continual learning of image\nclasses. However, while existing approaches do utilize textual modality\ninformation, they solely rely on simplistic templates with a class name,\nthereby neglecting richer semantic information. To address these limitations,\nwe propose a novel framework that harnesses visual concepts generated by large\nlanguage models (LLMs) as discriminative semantic guidance. Our method\ndynamically constructs a visual concept pool with a similarity-based filtering\nmechanism to prevent redundancy. Then, to integrate the concepts into the\ncontinual learning process, we employ a cross-modal image-concept attention\nmodule, coupled with an attention loss. Through attention, the module can\nleverage the semantic knowledge from relevant visual concepts and produce\nclass-representative fused features for classification. Experiments on medical\nand natural image datasets show our method achieves state-of-the-art\nperformance, demonstrating the effectiveness and superiority of our method. We\nwill release the code publicly.", "AI": {"tldr": "该研究提出一种新颖的持续学习框架，利用大型语言模型（LLMs）生成的视觉概念作为判别性语义指导，以增强医学图像分类的持续学习能力。", "motivation": "现有方法在利用文本模态信息进行持续学习时，仅依赖于简单的类名模板，忽视了更丰富的语义信息，限制了多模态信息融合的效果。", "method": "该方法动态构建一个视觉概念池，通过基于相似性的过滤机制避免冗余。然后，通过一个跨模态图像-概念注意力模块和注意力损失，将这些概念整合到持续学习过程中，从而利用相关视觉概念的语义知识，生成具有类别代表性的融合特征进行分类。", "result": "在医学图像和自然图像数据集上的实验表明，该方法实现了最先进的性能，证明了其有效性和优越性。", "conclusion": "该方法通过有效利用LLMs生成的视觉概念作为语义指导，显著提升了图像分类中持续学习的性能，尤其适用于动态变化的临床环境。"}}
{"id": "2508.03692", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03692", "abs": "https://arxiv.org/abs/2508.03692", "authors": ["Ao Liang", "Youquan Liu", "Yu Yang", "Dongyue Lu", "Linfeng Li", "Lingdong Kong", "Huaici Zhao", "Wei Tsang Ooi"], "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences", "comment": "Preprint; 28 pages, 18 figures, 12 tables; Project Page at\n  https://lidarcrafter.github.io", "summary": "Generative world models have become essential data engines for autonomous\ndriving, yet most existing efforts focus on videos or occupancy grids,\noverlooking the unique LiDAR properties. Extending LiDAR generation to dynamic\n4D world modeling presents challenges in controllability, temporal coherence,\nand evaluation standardization. To this end, we present LiDARCrafter, a unified\nframework for 4D LiDAR generation and editing. Given free-form natural language\ninputs, we parse instructions into ego-centric scene graphs, which condition a\ntri-branch diffusion network to generate object structures, motion\ntrajectories, and geometry. These structured conditions enable diverse and\nfine-grained scene editing. Additionally, an autoregressive module generates\ntemporally coherent 4D LiDAR sequences with smooth transitions. To support\nstandardized evaluation, we establish a comprehensive benchmark with diverse\nmetrics spanning scene-, object-, and sequence-level aspects. Experiments on\nthe nuScenes dataset using this benchmark demonstrate that LiDARCrafter\nachieves state-of-the-art performance in fidelity, controllability, and\ntemporal consistency across all levels, paving the way for data augmentation\nand simulation. The code and benchmark are released to the community.", "AI": {"tldr": "LiDARCrafter是一个统一的4D LiDAR生成和编辑框架，通过自然语言指令生成和编辑具有时间一致性的LiDAR序列，解决了自动驾驶中LiDAR世界模型在可控性、时间连贯性和评估标准化方面的挑战。", "motivation": "现有的自动驾驶生成世界模型主要关注视频或占用栅格，忽略了LiDAR的独特属性。将LiDAR生成扩展到动态4D世界建模在可控性、时间连贯性和评估标准化方面面临挑战。", "method": "LiDARCrafter框架将自然语言输入解析为以自我为中心的场景图，这些图作为条件驱动三分支扩散网络生成物体结构、运动轨迹和几何形状。一个自回归模块用于生成时间连贯的4D LiDAR序列。同时，建立了一个包含场景、物体和序列层面的综合评估基准。", "result": "在nuScenes数据集上，LiDARCrafter在保真度、可控性和时间一致性方面均达到了最先进的性能。代码和基准已发布。", "conclusion": "LiDARCrafter为自动驾驶中的数据增强和模拟铺平了道路，通过提供一个统一的4D LiDAR生成和编辑框架，并在各项指标上取得了卓越表现。"}}
{"id": "2508.03341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03341", "abs": "https://arxiv.org/abs/2508.03341", "authors": ["Jiayan Nan", "Wenquan Ma", "Wenlong Wu", "Yize Chen"], "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts.", "AI": {"tldr": "Nemori是一种受人类认知启发的自组织记忆架构，通过双步对齐原则解决记忆粒度问题，并通过预测-校准原则实现自适应知识进化，显著提升了LLM在长语境下的长期记忆能力。", "motivation": "大型语言模型（LLMs）在长语境中缺乏持久记忆，限制了其作为自主代理的有效性。现有记忆系统存在记忆单元粒度任意性及知识提取机制被动、基于规则的问题，阻碍了真正的学习和进化。", "method": "本文提出了Nemori架构，其核心创新包括两点：1. 受事件分割理论启发的“双步对齐原则”，提供了一种从上而下的方法，将原始对话流组织成语义连贯的片段，解决了记忆粒度问题。2. 受自由能原理启发的“预测-校准原则”，使代理能够主动从预测偏差中学习，实现自适应知识进化。", "result": "在LoCoMo和LongMemEval基准测试中，Nemori的表现显著优于现有最先进系统，尤其在更长的语境中优势更为明显。", "conclusion": "Nemori为处理自主代理的长期、动态工作流提供了一条可行的路径，有望克服LLM在长语境下记忆不足的限制。"}}
{"id": "2508.03440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03440", "abs": "https://arxiv.org/abs/2508.03440", "authors": ["Junhong Wu", "Jinliang Lu", "Zixuan Ren", "Ganqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.", "AI": {"tldr": "本文研究了大型语言模型（LLMs）的“软思考”能力，发现其在连续概念空间推理中倾向于贪婪解码。通过引入随机性，特别是Gumbel-Softmax技巧，可以有效提升软思考的性能。", "motivation": "现有推理模型依赖生成离散标记，限制了表达能力。近期工作尝试让LLMs生成软、抽象标记以在连续概念空间进行推理，但其内部行为和效率有待深入探究。", "method": "使用一系列探查技术检查LLMs的内部行为，以理解其“软思考”能力。为解决发现的问题，探索了引入随机性的采样策略，包括Dirichlet重采样和Gumbel-Softmax技巧。", "result": "研究发现LLMs在“软思考”中主要依赖软输入中最具影响力的分量，这阻碍了不同推理路径的探索，使香草软思考退化为一种贪婪解码。引入随机性，尤其是Gumbel-Softmax技巧，能够缓解这些局限性，并在八个推理基准上实现卓越性能。", "conclusion": "香草“软思考”在LLMs中表现出贪婪解码的倾向，未能充分利用软标记传递的信息。通过引入适当的随机性（如Gumbel-Softmax技巧），可以有效克服这一限制，释放“软思考”在连续概念空间推理中的潜力。"}}
{"id": "2508.03100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03100", "abs": "https://arxiv.org/abs/2508.03100", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video", "comment": null, "summary": "Multimodal reasoning over long-horizon video is challenging due to the need\nfor precise spatiotemporal fusion and alignment across modalities. While recent\nmethods such as Group Relative Policy Optimization (GRPO) have shown promise in\nthis domain, they suffer from three key limitations: (1) data inefficiency from\ntheir on-policy design, (2) a vanishing advantage problem, where identical or\nnear-identical rewards within a group eliminate the learning signal by\nproducing zero-valued advantages, and (3) uniform credit assignment that fails\nto emphasize critical reasoning steps. We introduce AVATAR (Audio-Video Agent\nfor Alignment and Reasoning), a framework that addresses these limitations\nthrough two core components: (1) an off-policy training architecture that\nimproves sample efficiency and resolves vanishing advantages by reusing past\nexperiences with greater reward diversity, and (2) Temporal Advantage Shaping\n(TAS), a novel credit assignment strategy that upweights key reasoning phases\nduring learning. AVATAR achieves strong performance across various benchmarks,\noutperforming the Qwen2.5-Omni baseline by +5.4on MMVU, +4.9 on OmniBench, and\n+4.5 on Video-Holmes, while demonstrating over 35% higher sample efficiency.", "AI": {"tldr": "AVATAR是一个用于长时视频多模态推理的框架，通过离策略训练和时间优势塑形解决了现有方法的数据效率低下、优势消失和信用分配不均等问题，显著提升了性能和样本效率。", "motivation": "现有方法（如GRPO）在长时视频多模态推理中存在三个主要限制：1) 在策略设计导致数据效率低下；2) 组内奖励相同或接近导致优势值消失，学习信号丧失；3) 统一的信用分配未能突出关键推理步骤。", "method": "本文提出了AVATAR（Audio-Video Agent for Alignment and Reasoning）框架，包含两个核心组件：1) 离策略训练架构，通过重用过去经验并增加奖励多样性，提高了样本效率并解决了优势消失问题；2) 时间优势塑形（Temporal Advantage Shaping, TAS），一种新颖的信用分配策略，在学习过程中加权关键推理阶段。", "result": "AVATAR在多个基准测试中表现出色，相比Qwen2.5-Omni基线，在MMVU上性能提升+5.4，在OmniBench上提升+4.9，在Video-Holmes上提升+4.5，同时样本效率提高了35%以上。", "conclusion": "AVATAR框架通过其创新的离策略训练和时间优势塑形策略，有效克服了长时视频多模态推理中的挑战，显著提升了模型性能和样本效率。"}}
{"id": "2508.03345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03345", "abs": "https://arxiv.org/abs/2508.03345", "authors": ["Xingdan Wang", "Jiayi He", "Zhiqing Tang", "Jianxiong Guo", "Jiong Lou", "Liping Qian", "Tian Wang", "Weijia Jia"], "title": "Adaptive AI Agent Placement and Migration in Edge Intelligence Systems", "comment": null, "summary": "The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents\ncapable of real-time task handling. However, migrating data-intensive,\nmulti-modal edge workloads to cloud data centers, traditionally used for agent\ndeployment, introduces significant latency. Deploying AI agents at the edge\nimproves efficiency and reduces latency. However, edge environments present\nchallenges due to limited and heterogeneous resources. Maintaining QoS for\nmobile users necessitates agent migration, which is complicated by the\ncomplexity of AI agents coordinating LLMs, task planning, memory, and external\ntools. This paper presents the first systematic deployment and management\nsolution for LLM-based AI agents in dynamic edge environments. We propose a\nnovel adaptive framework for AI agent placement and migration in edge\nintelligence systems. Our approach models resource constraints and\nlatency/cost, leveraging ant colony algorithms and LLM-based optimization for\nefficient decision-making. It autonomously places agents to optimize resource\nutilization and QoS and enables lightweight agent migration by transferring\nonly essential state. Implemented on a distributed system using AgentScope and\nvalidated across globally distributed edge servers, our solution significantly\nreduces deployment latency and migration costs.", "AI": {"tldr": "该论文提出了一种在动态边缘环境中部署和管理基于LLM的AI智能体的系统解决方案，通过自适应框架和轻量级迁移来优化资源利用和QoS。", "motivation": "随着LLM（如ChatGPT）的兴起，对能实时处理任务的AI智能体的需求增加。传统上将数据密集型、多模态边缘工作负载迁移到云数据中心会导致显著延迟。在边缘部署AI智能体可以提高效率并降低延迟，但边缘环境资源有限且异构。为移动用户保持QoS需要智能体迁移，而AI智能体（协调LLM、任务规划、内存和外部工具）的复杂性使得迁移变得复杂。", "method": "本文提出了一个新颖的自适应框架，用于边缘智能系统中AI智能体的放置和迁移。该方法对资源限制、延迟和成本进行建模，利用蚁群算法和基于LLM的优化进行高效决策。它能自主放置智能体以优化资源利用率和QoS，并通过仅传输必要状态实现轻量级智能体迁移。该解决方案在分布式系统上使用AgentScope实现，并在全球分布式边缘服务器上进行了验证。", "result": "该解决方案显著降低了部署延迟和迁移成本。", "conclusion": "本文首次提出了一种在动态边缘环境中部署和管理基于LLM的AI智能体的系统解决方案，有效解决了资源受限和迁移复杂性问题，提高了效率并降低了成本。"}}
{"id": "2508.03453", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03453", "abs": "https://arxiv.org/abs/2508.03453", "authors": ["Rita González-Márquez", "Philipp Berens", "Dmitry Kobak"], "title": "Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings", "comment": null, "summary": "Text embeddings, i.e. vector representations of entire texts, play an\nimportant role in many NLP applications, such as retrieval-augmented\ngeneration, sentiment analysis, clustering, or visualizing collections of texts\nfor data exploration. Currently, top-performing embedding models are derived\nfrom pre-trained language models via extensive supervised fine-tuning using\ncurated text pairs. This contrasts with computer vision, where self-supervised\ntraining based on data augmentations has demonstrated remarkable success. Here\nwe systematically compare the two most well-known augmentation strategies for\npositive pair generation in contrastive learning of text embeddings. We assess\nembedding quality on MTEB and additional in-domain evaluations and show that\ncropping augmentation strongly outperforms the dropout-based approach. We find\nthat on out-of-domain data, the quality of resulting embeddings is below the\nsupervised SOTA models, but for in-domain data, self-supervised fine-tuning\nproduces high-quality text embeddings after very short fine-tuning, sometimes\nonly marginally below the supervised SOTA. Finally, we show that representation\nquality increases towards the last transformer layers, which undergo the\nlargest change during fine-tuning; and that fine-tuning only those last layers\nis sufficient to reach similar embedding quality.", "AI": {"tldr": "该研究系统比较了两种自监督文本嵌入对比学习中的数据增强策略，发现裁剪增强优于基于Dropout的方法，且自监督微调在域内数据上能快速生成高质量嵌入。", "motivation": "文本嵌入在NLP中至关重要，但当前顶尖模型依赖大量监督微调。与计算机视觉中自监督学习的成功形成对比，本研究旨在探索自监督方法在文本嵌入生成中的潜力，特别是评估数据增强策略的效果。", "method": "研究系统比较了两种最知名的自监督数据增强策略（裁剪增强与基于Dropout的方法）用于对比学习中正样本对的生成。通过MTEB基准测试和额外的域内评估来评估嵌入质量。同时，分析了Transformer模型中不同层在微调过程中的变化，并测试了仅微调最后几层的效果。", "result": "研究发现，裁剪增强在性能上显著优于基于Dropout的方法。在域外数据上，自监督生成的嵌入质量低于监督SOTA模型；但在域内数据上，经过非常短的微调（有时仅略低于监督SOTA），自监督微调能产生高质量的文本嵌入。此外，表示质量随Transformer层数的增加而提高，最后几层在微调过程中变化最大，且仅微调这些最后层就足以达到相似的嵌入质量。", "conclusion": "自监督微调，特别是结合裁剪增强，是一种在文本嵌入领域有前景的方法，尤其适用于快速生成高质量域内嵌入。通过集中微调Transformer模型的最后几层，可以有效提升训练效率并保持高表示质量。"}}
{"id": "2508.03102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03102", "abs": "https://arxiv.org/abs/2508.03102", "authors": ["Tianjiao Jiang", "Zhen Zhang", "Yuhang Liu", "Javen Qinfeng Shi"], "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning", "comment": null, "summary": "Few-shot learning (FSL) often requires effective adaptation of models using\nlimited labeled data. However, most existing FSL methods rely on entangled\nrepresentations, requiring the model to implicitly recover the unmixing process\nto obtain disentangled representations using only limited supervision, which\nhinders effective adaptation. Recent theoretical studies show that multimodal\ncontrastive learning methods, such as CLIP, can disentangle latent\nrepresentations up to linear transformations. In light of this, we propose the\nCausal CLIP Adapter (CCA), a novel framework that explicitly disentangles\nvisual features extracted from CLIP using unsupervised Independent Component\nAnalysis (ICA). This removes the need to learn the unmixing process from the\nlabeled data, thereby reducing the number of trainable parameters and\nmitigating overfitting. Taking a step further, while ICA can obtain visual\ndisentangled representations, it may also disrupt CLIP's intra- and inter-modal\nalignment. To counteract this, CCA further leverages CLIP's inherent\ncross-modal alignment by enhancing it in two ways: unidirectionally, through\nfine-tuning a CLIP-based text classifier, and bidirectionally, via a\ncross-attention mechanism that enriches visual and textual representations\nthrough mutual interaction. Both unimodal and cross-modal classification\noutputs can be effectively combined linearly to improve classification\naccuracy. Extensive experiments on 11 benchmark datasets demonstrate that our\nmethod consistently outperforms state-of-the-art approaches in terms of\nfew-shot performance and robustness to distributional shifts, while maintaining\ncomputational efficiency. Code will be available at\nhttps://github.com/tianjiao-j/CCA.", "AI": {"tldr": "本文提出Causal CLIP Adapter (CCA)，通过无监督独立成分分析(ICA)显式解耦CLIP视觉特征，并增强跨模态对齐，显著提升小样本学习性能和分布偏移鲁棒性。", "motivation": "现有小样本学习(FSL)方法依赖纠缠表示，在有限监督下难以有效解耦并适应模型。尽管CLIP等方法能解耦潜在表示，但仍需显式利用以提高适应性并减少过拟合。", "method": "CCA框架首先使用无监督独立成分分析(ICA)显式解耦从CLIP提取的视觉特征，减少了从有限标记数据中学习解耦过程的需求。为弥补ICA可能破坏CLIP模态对齐的问题，CCA通过微调基于CLIP的文本分类器进行单向增强，并通过跨注意力机制进行双向增强，丰富视觉和文本表示。最终，将单模态和跨模态分类输出线性组合以提高准确性。", "result": "在11个基准数据集上的大量实验表明，CCA在小样本性能和对分布偏移的鲁棒性方面持续优于现有最先进方法，同时保持了计算效率。", "conclusion": "Causal CLIP Adapter (CCA)通过显式解耦视觉特征并增强CLIP的跨模态对齐，有效解决了小样本学习中表示纠缠的问题，从而在性能和鲁棒性上取得了显著提升。"}}
{"id": "2508.03346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03346", "abs": "https://arxiv.org/abs/2508.03346", "authors": ["Zeju Li", "Jianyuan Zhong", "Ziyang Zheng", "Xiangyu Wen", "Zhijian Xu", "Yingying Cheng", "Fan Zhang", "Qiang Xu"], "title": "Compressing Chain-of-Thought in LLMs via Step Entropy", "comment": null, "summary": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at\ncomplex reasoning but generate verbose thought processes with considerable\nredundancy, leading to increased inference costs and reduced efficiency. We\nintroduce a novel CoT compression framework based on step entropy, a metric\nthat quantifies the informational contribution of individual reasoning steps to\nidentify redundancy. Through theoretical analysis and extensive empirical\nvalidation on mathematical reasoning benchmarks, we demonstrate that steps with\nlow entropy are indeed highly redundant. Our experiments reveal that an\nastonishing 80\\% of low-entropy intermediate steps can be pruned with minor\ndegradation in the final answer accuracy across DeepSeek-R1-7B, 14B and\nQwen3-8B. This finding sharply contrasts with random or high-entropy pruning,\nwhich severely impairs reasoning performance. Building on this, we propose a\nnovel two-stage training strategy combining Supervised Fine-Tuning (SFT) and\nGroup Relative Policy Optimization (GRPO) reinforcement learning. This approach\nenables LLMs to autonomously learn to generate compressed COTs during inference\nby strategically incorporating [SKIP] tokens. Our method significantly enhances\nLLM inference efficiency while rigorously preserving accuracy, offering\nprofound implications for practical LLM deployment and a deeper understanding\nof reasoning structures.", "AI": {"tldr": "该研究提出了一种基于步长熵的思维链（CoT）压缩框架，并结合两阶段训练策略，使大型语言模型（LLMs）能够自主生成压缩的CoT，显著提高推理效率并保持准确性。", "motivation": "LLMs使用CoT提示时会生成冗长且包含大量冗余的思维过程，这导致推理成本增加和效率降低。", "method": "1. 引入“步长熵”度量来量化每个推理步骤的信息贡献，以识别冗余。2. 基于步长熵进行低熵步骤的剪枝。3. 提出一种两阶段训练策略，结合监督微调（SFT）和组相对策略优化（GRPO）强化学习，使LLMs能通过战略性地加入[SKIP]标记来自主生成压缩的CoT。", "result": "1. 理论分析和实验验证表明，低熵步骤确实高度冗余。2. 在DeepSeek-R1-7B、14B和Qwen3-8B模型上，80%的低熵中间步骤可以在最终答案准确性仅有轻微下降的情况下被剪除。3. 这与随机剪枝或高熵剪枝形成鲜明对比，后者会严重损害推理性能。4. 所提出的方法显著提高了LLM的推理效率，同时严格保持了准确性。", "conclusion": "该方法通过实现高效、准确的压缩CoT生成，对LLM的实际部署和对推理结构的深入理解具有深远意义。"}}
{"id": "2508.03475", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03475", "abs": "https://arxiv.org/abs/2508.03475", "authors": ["Pranshu Rastogi"], "title": "fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval", "comment": "7 pages, 6 tables. Code available at\n  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders", "summary": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\nRetrieval is approached as a Learning-to-Rank task using a bi-encoder model\nfine-tuned from a pre-trained transformer optimized for sentence similarity.\nTraining used both the source languages and their English translations for\nmultilingual retrieval and only English translations for cross-lingual\nretrieval. Using lightweight models with fewer than 500M parameters and\ntraining on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual\nand 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.", "AI": {"tldr": "该研究针对SemEval-2025任务7的多语言和跨语言事实核查声明检索，采用基于预训练Transformer的轻量级双编码器模型，通过学习排序任务实现了高成功率。", "motivation": "参与SemEval-2025任务7：多语言和跨语言事实核查声明检索，旨在开发有效的方法来检索跨语言的事实核查声明。", "method": "将任务视为学习排序问题，使用一个从预训练Transformer（针对句子相似性优化）微调的双编码器模型。多语言检索训练时使用源语言及其英语翻译，而跨语言检索仅使用英语翻译。模型参数少于5亿，在Kaggle T4 GPU上进行训练。", "result": "在多语言检索中实现了92%的Success@10，在跨语言检索中实现了80%的Success@10。在跨语言赛道中排名第5，在多语言赛道中排名第10。", "conclusion": "所提出的基于轻量级双编码器模型的方法，通过特定的多语言和跨语言训练策略，在事实核查声明检索任务中表现出色，取得了较高的成功率和良好的竞赛排名。"}}
{"id": "2508.03118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03118", "abs": "https://arxiv.org/abs/2508.03118", "authors": ["Heng Jia", "Linchao Zhu", "Na Zhao"], "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction", "comment": "ICCV 2025", "summary": "Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable\n3D reconstruction remains challenging, particularly in multi-view\ncorrespondence modeling. Existing approaches face a fundamental trade-off:\nexplicit methods achieve geometric precision but struggle with ambiguous\nregions, while implicit methods provide robustness but suffer from slow\nconvergence. We present H3R, a hybrid framework that addresses this limitation\nby integrating volumetric latent fusion with attention-based feature\naggregation. Our framework consists of two complementary components: an\nefficient latent volume that enforces geometric consistency through epipolar\nconstraints, and a camera-aware Transformer that leverages Pl\\\"ucker\ncoordinates for adaptive correspondence refinement. By integrating both\nparadigms, our approach enhances generalization while converging 2$\\times$\nfaster than existing methods. Furthermore, we show that spatial-aligned\nfoundation models (e.g., SD-VAE) substantially outperform semantic-aligned\nmodels (e.g., DINOv2), resolving the mismatch between semantic representations\nand spatial reconstruction requirements. Our method supports variable-number\nand high-resolution input views while demonstrating robust cross-dataset\ngeneralization. Extensive experiments show that our method achieves\nstate-of-the-art performance across multiple benchmarks, with significant PSNR\nimprovements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and\nDTU datasets, respectively. Code is available at\nhttps://github.com/JiaHeng-DLUT/H3R.", "AI": {"tldr": "H3R提出了一种混合框架，结合了体素潜在融合和基于注意力的特征聚合，解决了多视角对应建模中几何精度与收敛速度的矛盾，实现了更快的收敛和更强的泛化能力，并在3D重建任务中达到了SOTA性能。", "motivation": "尽管前馈3D高斯泼溅技术取得了进展，但可泛化的3D重建仍然具有挑战性，尤其是在多视角对应建模方面。现有方法面临一个基本权衡：显式方法几何精度高但在模糊区域表现不佳，而隐式方法鲁棒性强但收敛缓慢。", "method": "H3R是一个混合框架，通过整合体素潜在融合和基于注意力的特征聚合来解决上述限制。它包含两个互补组件：一个通过极线约束强制执行几何一致性的高效潜在体素，以及一个利用普吕克坐标进行自适应对应细化的相机感知Transformer。此外，研究发现空间对齐的基础模型（如SD-VAE）在空间重建方面显著优于语义对齐模型（如DINOv2）。", "result": "该方法在增强泛化能力的同时，收敛速度比现有方法快2倍。它支持可变数量和高分辨率的输入视图，并展示了强大的跨数据集泛化能力。在RealEstate10K、ACID和DTU数据集上，PSNR分别显著提升了0.59 dB、1.06 dB和0.22 dB，达到了最先进的性能。", "conclusion": "H3R通过结合体素潜在融合和注意力机制，有效解决了3D重建中多视角对应建模的挑战，实现了高精度、快速收敛和强泛化的性能。同时，研究强调了空间对齐基础模型在空间重建任务中的优越性。"}}
{"id": "2508.03360", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03360", "abs": "https://arxiv.org/abs/2508.03360", "authors": ["Feng Rui", "Zhiyao Luo", "Wei Wang", "Yuting Song", "Yong Liu", "Tingting Zhu", "Jianqing Li", "Xingyao Wang"], "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment", "comment": "19 pages, 9 figures, 12 tables", "summary": "Automatic assessment of cognitive impairment from spontaneous speech offers a\npromising, non-invasive avenue for early cognitive screening. However, current\napproaches often lack generalizability when deployed across different languages\nand clinical settings, limiting their practical utility. In this study, we\npropose CogBench, the first benchmark designed to evaluate the cross-lingual\nand cross-site generalizability of large language models (LLMs) for\nspeech-based cognitive impairment assessment. Using a unified multimodal\npipeline, we evaluate model performance on three speech datasets spanning\nEnglish and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,\nCIR-E. Our results show that conventional deep learning models degrade\nsubstantially when transferred across domains. In contrast, LLMs equipped with\nchain-of-thought prompting demonstrate better adaptability, though their\nperformance remains sensitive to prompt design. Furthermore, we explore\nlightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which\nsignificantly improves generalization in target domains. These findings offer a\ncritical step toward building clinically useful and linguistically robust\nspeech-based cognitive assessment tools.", "AI": {"tldr": "本研究提出了CogBench基准，用于评估大型语言模型(LLMs)在跨语言和跨中心语音认知障碍评估中的泛化能力，并发现LLMs通过思维链提示和LoRA微调能显著提升泛化性能。", "motivation": "自动语音认知障碍评估缺乏跨语言和跨临床环境的泛化能力，限制了其实用性。", "method": "提出了CogBench基准，采用统一的多模态流程，在英语和普通话的三个语音数据集（ADReSSo、NCMMSC2021-AD、CIR-E）上评估模型性能。研究了传统深度学习模型、结合思维链提示的LLMs以及通过LoRA进行轻量级微调的LLMs。", "result": "传统深度学习模型在跨领域迁移时性能显著下降；LLMs结合思维链提示表现出更好的适应性，但对提示设计敏感；通过LoRA轻量级微调LLMs显著提高了在目标领域的泛化能力。", "conclusion": "这些发现是构建临床有用且语言鲁棒的语音认知评估工具的关键一步。"}}
{"id": "2508.03489", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03489", "abs": "https://arxiv.org/abs/2508.03489", "authors": ["Kaiwen Zhao", "Bharathan Balaji", "Stephen Lee"], "title": "CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation", "comment": null, "summary": "Product sustainability reports provide valuable insights into the\nenvironmental impacts of a product and are often distributed in PDF format.\nThese reports often include a combination of tables and text, which complicates\ntheir analysis. The lack of standardization and the variability in reporting\nformats further exacerbate the difficulty of extracting and interpreting\nrelevant information from large volumes of documents. In this paper, we tackle\nthe challenge of answering questions related to carbon footprints within\nsustainability reports available in PDF format. Unlike previous approaches, our\nfocus is on addressing the difficulties posed by the unstructured and\ninconsistent nature of text extracted from PDF parsing. To facilitate this\nanalysis, we introduce CarbonPDF-QA, an open-source dataset containing\nquestion-answer pairs for 1735 product report documents, along with\nhuman-annotated answers. Our analysis shows that GPT-4o struggles to answer\nquestions with data inconsistencies. To address this limitation, we propose\nCarbonPDF, an LLM-based technique specifically designed to answer carbon\nfootprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama\n3 with our training data. Our results show that our technique outperforms\ncurrent state-of-the-art techniques, including question-answering (QA) systems\nfinetuned on table and text data.", "AI": {"tldr": "该论文旨在解决从PDF格式的产品可持续性报告中提取碳足迹信息并回答相关问题的挑战。为此，作者提出了一个新数据集CarbonPDF-QA，并开发了一种基于LLM的新技术CarbonPDF（通过微调Llama 3），该技术在处理非结构化和不一致文本方面表现优于现有SOTA方法。", "motivation": "产品可持续性报告通常以PDF格式发布，包含表格和文本的混合内容，且缺乏标准化，导致难以从大量文档中提取和解释相关信息，特别是关于碳足迹的数据。", "method": "研究者创建了CarbonPDF-QA，一个包含1735份产品报告的问答对和人工标注答案的开源数据集。他们分析了GPT-4o在处理数据不一致性时的不足。在此基础上，提出了CarbonPDF，一种专门用于回答碳足迹问题的基于LLM的技术，通过使用其训练数据微调Llama 3模型实现。", "result": "分析显示GPT-4o在处理数据不一致性时表现不佳。所提出的CarbonPDF技术（通过微调Llama 3开发）在回答碳足迹问题上，优于包括在表格和文本数据上微调的现有最先进问答系统。", "conclusion": "CarbonPDF是一种有效且性能卓越的LLM-based技术，能够解决从非结构化和不一致的PDF可持续性报告中提取碳足迹信息并回答相关问题的难题，显著优于现有SOTA方法。"}}
{"id": "2508.03127", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03127", "abs": "https://arxiv.org/abs/2508.03127", "authors": ["Sai Ma", "Zhuang Li", "John A Taylor"], "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery", "comment": null, "summary": "Vision language models (VLMs) that enable natural language interaction with\nsatellite imagery can democratize Earth observation by accelerating expert\nworkflows, making data accessible to non-specialists, and enabling planet-scale\nautomation. However, existing datasets focus mainly on short-term,\nhigh-resolution imagery from a limited number of satellites, overlooking\nlow-resolution, multi-satellite, long-term archives, such as Landsat, that are\nessential for affordable and bias-robust global monitoring. We address this gap\nwith Landsat30-AU, a large-scale vision-language dataset built from 30-meter\nresolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over\nAustralia, spanning more than 36 years. The dataset includes two components:\nLandsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA,\ncomprising 17,725 human-verified visual question answering (VQA) samples across\neight remote sensing domains. Both datasets are curated through a bootstrapped\npipeline that leverages generic VLMs with iterative refinement and human\nverification to ensure quality. Our evaluation of eight VLMs on our benchmark\nreveals that off-the-shelf models struggle to understand satellite imagery. The\nopen-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in\ncaptioning and a VQA accuracy of 0.48, highlighting the limitations of current\napproaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on\nLandsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and\nboosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at\nhttps://github.com/papersubmit1/landsat30-au.", "AI": {"tldr": "该研究引入了Landsat30-AU，一个用于长期、低分辨率卫星图像的视觉语言数据集，旨在解决现有VLM在地球观测领域的数据局限性，并证明了在该数据集上进行微调可以显著提升VLM的性能。", "motivation": "现有的视觉语言模型（VLMs）及其数据集主要关注短期、高分辨率的卫星图像，忽略了对经济、无偏见的全球监测至关重要的低分辨率、多卫星、长期存档数据（如Landsat）。这限制了VLMs在地球观测领域的应用，无法加速专家工作流程、向非专业人士普及数据以及实现全球范围的自动化。", "method": "研究构建了一个名为Landsat30-AU的大规模视觉语言数据集，该数据集包含来自四颗Landsat卫星（5, 7, 8, 9）在澳大利亚上空超过36年间收集的30米分辨率图像。数据集包含两部分：Landsat30-AU-Cap（196,262对图像-标题）和Landsat30-AU-VQA（17,725个人工验证的视觉问答样本）。数据集通过一个自举流程构建，该流程利用通用VLMs进行迭代细化和人工验证以确保数据质量。研究还评估了八个VLM在该基准上的性能，并对Qwen2.5-VL-7B模型进行了轻量级微调。", "result": "评估结果显示，开箱即用的VLM难以理解卫星图像，例如开源遥感VLM EarthDial在图像标题生成方面SPIDEr得分仅为0.07，VQA准确率为0.48。然而，在Landsat30-AU上对Qwen2.5-VL-7B进行轻量级微调后，其标题生成性能从0.11 SPIDEr提升到0.31 SPIDEr，VQA准确率从0.74提升到0.87。", "conclusion": "当前VLM在理解长期、低分辨率卫星图像方面存在局限性。新创建的Landsat30-AU数据集填补了现有数据集的空白，并为该领域的研究提供了宝贵的资源。对现有VLM进行轻量级微调，能够显著提升其在卫星图像理解任务上的性能，为地球观测的民主化和自动化提供了新的途径。"}}
{"id": "2508.03366", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.03366", "abs": "https://arxiv.org/abs/2508.03366", "authors": ["Michael K. Chen"], "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input.", "AI": {"tldr": "本文比较了两种神经符号AI方法（集成式和混合式）在通用逻辑推理方面的潜力，发现混合式方法更有前景，并提出了一个可泛化的框架。", "motivation": "大型语言模型（LLMs）在通用逻辑推理方面表现不佳，缺乏确定性和可解释性。神经符号AI旨在结合逻辑和神经网络来解决这一问题，但不同方法在领域无关的逻辑推理基准上的表现尚未得到充分研究和比较。", "method": "作者首先识别了两种主要的神经符号AI方法：集成式（逻辑推理包含在神经网络内部）和混合式（独立的符号求解器执行推理）。然后，选择两种代表性的领域无关模型作为案例研究：集成式方法的Logic Neural Network (LNN) 和混合式方法的LLM-Symbolic Solver (LLM-SS)，通过对比分析评估它们在发展通用逻辑推理方面的潜力。最后，提出了一个基于LLM-SS的模块化、模型无关、领域无关且几乎无需人工输入的通用框架。", "result": "分析结果表明，混合式方法在发展通用逻辑推理方面更具前景。主要原因有二：(i) 其推理链更具可解释性；(ii) 它保留了现有LLM的能力和优势。", "conclusion": "混合式神经符号AI方法在实现通用逻辑推理方面比集成式方法更有希望，因为它提供了更好的可解释性并能利用LLM的固有优势。为支持未来的研究，本文提出了一个基于LLM-SS的通用框架。"}}
{"id": "2508.03520", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03520", "abs": "https://arxiv.org/abs/2508.03520", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Aneesh Krishna", "Shafin Rahman", "Tom Gedeon"], "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression", "comment": "Code available at https://github.com/hasan-rakibul/UPLME", "summary": "Supervised learning for empathy regression is challenged by noisy\nself-reported empathy scores. While many algorithms have been proposed for\nlearning with noisy labels in textual classification problems, the regression\ncounterpart is relatively under-explored. We propose UPLME, an\nuncertainty-aware probabilistic language modelling framework to capture label\nnoise in the regression setting of empathy detection. UPLME includes a\nprobabilistic language model that predicts both empathy score and\nheteroscedastic uncertainty and is trained using Bayesian concepts with\nvariational model ensembling. We further introduce two novel loss components:\none penalises degenerate Uncertainty Quantification (UQ), and another enforces\nthe similarity between the input pairs on which we predict empathy. UPLME\nprovides state-of-the-art performance (Pearson Correlation Coefficient:\n$0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the\nperformance reported in the literature in two public benchmarks, having label\nnoise. Through synthetic label noise injection, we show that UPLME is effective\nin separating noisy and clean samples based on the predicted uncertainty. UPLME\nfurther outperform (Calibration error: $0.571\\rightarrow0.376$) a recent\nvariational model ensembling-based UQ method designed for regression problems.", "AI": {"tldr": "本文提出了UPLME，一个不确定性感知的概率语言建模框架，用于解决同理心回归中自报告分数带噪声的问题，并在公共基准上取得了最先进的性能。", "motivation": "同理心回归中的监督学习面临自报告同理心分数噪声大的挑战。尽管文本分类中处理噪声标签的算法很多，但回归任务中对噪声标签的处理相对未被充分探索。", "method": "UPLME框架包含一个概率语言模型，该模型能预测同理心分数和异方差不确定性，并使用贝叶斯概念和变分模型集成进行训练。此外，引入了两个新颖的损失组件：一个惩罚退化的不确定性量化（UQ），另一个强制输入对之间的相似性。", "result": "UPLME在两个带有标签噪声的公共基准上，性能达到了文献报道的最先进水平（Pearson相关系数：0.558→0.580和0.629→0.634）。通过合成标签噪声注入，UPLME能有效根据预测的不确定性区分噪声样本和干净样本。UPLME还优于最近的基于变分模型集成的回归UQ方法（校准误差：0.571→0.376）。", "conclusion": "UPLME是一个有效且鲁棒的框架，能够处理同理心回归中带有噪声的标签，并通过其不确定性量化能力，在性能和噪声样本识别方面均表现出色。"}}
{"id": "2508.03139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03139", "abs": "https://arxiv.org/abs/2508.03139", "authors": ["Haozhou Zhai", "Yanzhe Gao", "Tianjiang Hu"], "title": "Uint: Building Uint Detection Dataset", "comment": null, "summary": "Fire scene datasets are crucial for training robust computer vision models,\nparticularly in tasks such as fire early warning and emergency rescue\noperations. However, among the currently available fire-related data, there is\na significant shortage of annotated data specifically targeting building\nunits.To tackle this issue, we introduce an annotated dataset of building units\ncaptured by drones, which incorporates multiple enhancement techniques. We\nconstruct backgrounds using real multi-story scenes, combine motion blur and\nbrightness adjustment to enhance the authenticity of the captured images,\nsimulate drone shooting conditions under various circumstances, and employ\nlarge models to generate fire effects at different locations.The synthetic\ndataset generated by this method encompasses a wide range of building\nscenarios, with a total of 1,978 images. This dataset can effectively improve\nthe generalization ability of fire unit detection, providing multi-scenario and\nscalable data while reducing the risks and costs associated with collecting\nreal fire data. The dataset is available at\nhttps://github.com/boilermakerr/FireUnitData.", "AI": {"tldr": "该研究引入了一个由无人机视角捕获的合成建筑单元火灾数据集，以解决现有火灾数据集中带注释建筑单元数据不足的问题。", "motivation": "现有的火灾场景数据集，尤其是针对建筑单元的带注释数据，严重不足，这限制了计算机视觉模型在火灾预警和应急救援任务中的训练效果。", "method": "通过以下技术构建数据集：使用真实多层场景构建背景，结合运动模糊和亮度调整增强图像真实性，模拟不同条件下的无人机拍摄，并利用大型模型在不同位置生成火灾效果。", "result": "生成了一个包含1978张图像的合成数据集，涵盖了广泛的建筑场景。该数据集能有效提高火灾单元检测的泛化能力，提供多场景和可扩展数据，并降低收集真实火灾数据的风险和成本。", "conclusion": "所创建的合成数据集有效弥补了带注释建筑单元火灾数据的不足，为火灾单元检测提供了宝贵且可扩展的训练资源。"}}
{"id": "2508.03368", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.03368", "abs": "https://arxiv.org/abs/2508.03368", "authors": ["Lucia Cipolina-Kun", "Marianna Nezhurina", "Jenia Jitsev"], "title": "Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play", "comment": null, "summary": "The Board Game Arena library provides a framework for evaluating the decision\nmaking abilities of large language models (LLMs) through strategic board games\nimplemented in Google OpenSpiel library. The framework enables systematic\ncomparisons between LLM based agents and other agents (random, human,\nreinforcement learning agents, etc.) in various game scenarios by wrapping\nmultiple board and matrix games and supporting different agent types. It\nintegrates API access to models via LiteLLM, local model deployment via vLLM,\nand offers distributed execution through Ray. Additionally it provides\nextensive analysis tools for the LLM reasoning traces. This paper summarizes\nthe structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game-theoretic behavior", "AI": {"tldr": "Board Game Arena库是一个评估大型语言模型（LLM）在策略棋盘游戏（基于Google OpenSpiel）中决策能力的框架，支持多种代理类型和分布式执行，并提供LLM推理分析工具。", "motivation": "旨在系统地评估和比较LLM在策略棋盘游戏中的决策能力，理解其推理过程和博弈论行为，并与人类、强化学习等其他代理进行对比。", "method": "该框架通过封装Google OpenSpiel中的多种棋盘和矩阵游戏，支持LLM、随机、人类和强化学习等多种代理类型。它集成了LiteLLM进行模型API访问，vLLM进行本地模型部署，并利用Ray实现分布式执行。此外，还提供了针对LLM推理轨迹的广泛分析工具。", "result": "该论文概述了Board Game Arena库的结构、关键特性和动机，展示了其如何促进对LLM推理和博弈论行为的实证评估。", "conclusion": "Board Game Arena库为在策略棋盘游戏中实证评估LLM的决策能力、推理过程和博弈论行为提供了一个全面的框架，有助于推动LLM研究进展。"}}
{"id": "2508.03523", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03523", "abs": "https://arxiv.org/abs/2508.03523", "authors": ["Lester James V. Miranda", "Elyanah Aco", "Conner Manuel", "Jan Christian Blaise Cruz", "Joseph Marvin Imperial"], "title": "FilBench: Can LLMs Understand and Generate Filipino?", "comment": null, "summary": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development.", "AI": {"tldr": "本文介绍了FilBench，一个旨在评估大型语言模型（LLMs）在菲律宾语、他加禄语和宿务语方面能力的基准测试，发现现有LLMs在菲律宾语阅读理解和翻译上表现不佳，即使是最佳模型GPT-4o也仅达到72.23%的得分。", "motivation": "尽管LLMs在英语任务上表现出色，但其在特定语言（如菲律宾语）上的能力尚不明确。现有研究存在空白，缺乏针对菲律宾语系语言的专门基准测试。", "method": "研究者引入了FilBench，一个以菲律宾为中心的基准测试，旨在评估LLMs在菲律宾语、他加禄语和宿务语上的多种任务和能力。FilBench的任务经过精心策划，涵盖文化知识、经典自然语言处理、阅读理解和生成等领域。研究者在FilBench上评估了27个最先进的LLMs。", "result": "评估结果显示，许多LLMs在阅读理解和翻译能力方面表现不佳。FilBench具有挑战性，即使是表现最好的模型GPT-4o也只取得了72.23%的得分。此外，专门针对东南亚语言训练的模型在FilBench上表现往往不佳，其中表现最好的SEA-LION v3 70B模型仅获得61.07%的得分。", "conclusion": "这项工作证明了策划特定语言LLM基准测试的价值，有助于推动菲律宾自然语言处理的进展，并促进菲律宾语言在LLM开发中的包容性。"}}
{"id": "2508.03142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03142", "abs": "https://arxiv.org/abs/2508.03142", "authors": ["Chengyu Bai", "Jintao Chen", "Xiang Bai", "Yilong Chen", "Qi She", "Ming Lu", "Shanghang Zhang"], "title": "UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying", "comment": null, "summary": "In recent years, unified vision-language models (VLMs) have rapidly advanced,\neffectively tackling both visual understanding and generation tasks within a\nsingle design. While many unified VLMs have explored various design choices,\nthe recent hypothesis from OpenAI's GPT-4o suggests a promising generation\npipeline: Understanding VLM->Visual Feature->Projector->Diffusion Model->Image.\nThe understanding VLM is frozen, and only the generation-related modules are\ntrained. This pipeline maintains the strong capability of understanding VLM\nwhile enabling the image generation ability of the unified VLM. Although this\npipeline has shown very promising potential for the future development of\nunified VLM, how to easily enable image editing capability is still unexplored.\nIn this paper, we introduce a novel training-free framework named UniEdit-I to\nenable the unified VLM with image editing capability via three iterative steps:\nunderstanding, editing, and verifying. 1. The understanding step analyzes the\nsource image to create a source prompt through structured semantic analysis and\nmakes minimal word replacements to form the target prompt based on the editing\ninstruction. 2. The editing step introduces a time-adaptive offset, allowing\nfor coherent editing from coarse to fine throughout the denoising process. 3.\nThe verification step checks the alignment between the target prompt and the\nintermediate edited image, provides automatic consistency scores and corrective\nfeedback, and determines whether to stop early or continue the editing loop.\nThis understanding, editing, and verifying loop iterates until convergence,\ndelivering high-fidelity editing in a training-free manner. We implemented our\nmethod based on the latest BLIP3-o and achieved state-of-the-art (SOTA)\nperformance on the GEdit-Bench benchmark.", "AI": {"tldr": "本文提出UniEdit-I，一个无需训练的框架，通过理解、编辑、验证三步迭代循环，为统一视觉-语言模型（VLM）赋能图像编辑能力。", "motivation": "尽管统一VLM在视觉理解和生成方面取得了快速进展，并且GPT-4o的生成管线（理解VLM->视觉特征->投影器->扩散模型->图像）显示出巨大潜力，但如何轻松实现图像编辑能力仍未被探索。", "method": "UniEdit-I框架包含三个迭代步骤：1. 理解：通过结构化语义分析从源图像创建源提示，并根据编辑指令进行最小词替换以形成目标提示。2. 编辑：引入时间自适应偏移，在去噪过程中实现从粗到细的连贯编辑。3. 验证：检查目标提示与中间编辑图像的一致性，提供自动一致性分数和纠正反馈，并决定是否提前停止或继续编辑循环。该循环迭代直至收敛，实现无需训练的高保真编辑。", "result": "基于最新的BLIP3-o实现，UniEdit-I在GEdit-Bench基准测试上达到了最先进的性能（SOTA）。", "conclusion": "UniEdit-I提供了一种无需训练的、高保真图像编辑方法，成功为统一VLM（如基于GPT-4o生成管线的模型）赋能了图像编辑能力，并通过理解、编辑、验证的迭代循环实现了卓越性能。"}}
{"id": "2508.03379", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03379", "abs": "https://arxiv.org/abs/2508.03379", "authors": ["Wenxin Mao", "Zhitao Wang Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.", "AI": {"tldr": "UML2Dep是一个分步代码生成框架，它通过引入增强的UML序列图（结合决策表和API规范）和显式数据依赖推理，解决了LLM从模糊自然语言生成复杂代码的挑战。", "motivation": "大型语言模型（LLMs）在从自然语言生成代码时，面临自然语言描述固有的模糊性，难以捕捉复杂的系统行为、条件逻辑、架构约束和隐式数据依赖等问题。", "method": "该研究提出了UML2Dep框架：首先，引入了针对服务导向架构的增强UML序列图，通过集成决策表和API规范来明确形式化业务逻辑和结构关系，消除语言歧义。其次，提出了数据依赖推理（DDI）任务，在代码合成前构建显式数据依赖图，并通过将DDI形式化为受约束的数学推理任务来提高可靠性，并利用静态解析和依赖剪枝减少上下文复杂性。", "result": "通过上述方法，UML2Dep旨在严格消除语言歧义，确保数据依赖推理的可靠性，并提升LLM在处理复杂规范时的推理准确性和效率。", "conclusion": "该框架通过利用明确的正式规范和显式数据依赖建模，成功弥合了自然语言描述与复杂代码生成之间的鸿沟，为LLM生成高质量代码提供了更可靠的途径。"}}
{"id": "2508.03529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03529", "abs": "https://arxiv.org/abs/2508.03529", "authors": ["Vukosi Marivate", "Isheanesu Dzingirai", "Fiskani Banda", "Richard Lastrucci", "Thapelo Sindane", "Keabetswe Madumo", "Kayode Olaleye", "Abiodun Modupe", "Unarine Netshifhefhe", "Herkulaas Combrink", "Mohlatlego Nakeng", "Matome Ledwaba"], "title": "Marito: Structuring and Building Open Multilingual Terminologies for South African NLP", "comment": "Under Review", "summary": "The critical lack of structured terminological data for South Africa's\nofficial languages hampers progress in multilingual NLP, despite the existence\nof numerous government and academic terminology lists. These valuable assets\nremain fragmented and locked in non-machine-readable formats, rendering them\nunusable for computational research and development. \\emph{Marito} addresses\nthis challenge by systematically aggregating, cleaning, and standardising these\nscattered resources into open, interoperable datasets. We introduce the\nfoundational \\emph{Marito} dataset, released under the equitable,\nAfrica-centered NOODL framework. To demonstrate its immediate utility, we\nintegrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.\nExperiments show substantial improvements in the accuracy and domain-specific\nconsistency of English-to-Tshivenda machine translation for large language\nmodels. \\emph{Marito} provides a scalable foundation for developing robust and\nequitable NLP technologies, ensuring South Africa's rich linguistic diversity\nis represented in the digital age.", "AI": {"tldr": "Marito项目通过聚合、清洗和标准化南非官方语言的术语列表，创建了开放、可互操作的结构化数据集，显著提升了大型语言模型在特定领域英-特语机器翻译的准确性和一致性。", "motivation": "南非官方语言缺乏结构化术语数据，现有政府和学术术语列表分散且为非机器可读格式，阻碍了多语言NLP的进展和计算研究与开发。", "method": "系统地聚合、清洗和标准化分散的术语资源，形成开放、可互操作的Marito数据集（基于NOODL框架发布）。将该术语数据集成到检索增强生成（RAG）管道中，并进行实验。", "result": "实验表明，将Marito术语数据集成到RAG管道后，大型语言模型在英-特语（Tshivenda）机器翻译的准确性和领域特定一致性方面有显著提升。", "conclusion": "Marito为开发健壮和公平的NLP技术奠定了可扩展的基础，确保了南非丰富的语言多样性在数字时代得到代表。"}}
{"id": "2508.03143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03143", "abs": "https://arxiv.org/abs/2508.03143", "authors": ["Yanshu Wang", "Xichen Xu", "Xiaoning Lei", "Guoyang Xie"], "title": "SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance", "comment": "Accepted by The 2025 International Conference on Machine Intelligence\n  and Nature-InspireD Computing (MIND)", "summary": "Synthesizing realistic and spatially precise anomalies is essential for\nenhancing the robustness of industrial anomaly detection systems. While recent\ndiffusion-based methods have demonstrated strong capabilities in modeling\ncomplex defect patterns, they often struggle with spatial controllability and\nfail to maintain fine-grained regional fidelity. To overcome these limitations,\nwe propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained\nDiffusion with discriminative mask Guidance), a novel diffusion-based framework\nspecifically designed for anomaly generation. Our approach introduces a\nRegion-Constrained Diffusion (RCD) process that preserves the background by\nfreezing it and selectively updating only the foreground anomaly regions during\nthe reverse denoising phase, thereby effectively reducing background artifacts.\nAdditionally, we incorporate a Discriminative Mask Guidance (DMG) module into\nthe discriminator, enabling joint evaluation of both global realism and local\nanomaly fidelity, guided by pixel-level masks. Extensive experiments on the\nMVTec-AD and BTAD datasets show that SARD surpasses existing methods in\nsegmentation accuracy and visual quality, setting a new state-of-the-art for\npixel-level anomaly synthesis.", "AI": {"tldr": "SARD是一种新型的基于扩散模型的异常合成框架，通过区域约束扩散和判别性掩码引导，实现了高空间精度和区域保真度的工业异常生成，提升了异常检测系统的鲁棒性。", "motivation": "现有的扩散模型在生成复杂缺陷模式方面表现出色，但在空间可控性和保持精细区域保真度方面存在不足，导致生成的异常可能缺乏空间精确性或引入背景伪影，从而影响工业异常检测系统的鲁棒性。", "method": "本文提出了SARD框架，包含两个核心组件：1. 区域约束扩散（RCD）过程：在逆向去噪阶段冻结背景，仅选择性地更新前景异常区域，以减少背景伪影并保持背景完整性。2. 判别性掩码引导（DMG）模块：集成到判别器中，通过像素级掩码引导，联合评估全局真实性和局部异常保真度。", "result": "在MVTec-AD和BTAD数据集上的大量实验表明，SARD在分割精度和视觉质量方面均超越了现有方法，为像素级异常合成设定了新的最先进水平。", "conclusion": "SARD通过其创新的区域约束扩散和判别性掩码引导机制，有效解决了现有扩散模型在异常合成中空间可控性和区域保真度不足的问题，实现了逼真且空间精确的异常生成，显著提升了工业异常检测系统的性能。"}}
{"id": "2508.03396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03396", "abs": "https://arxiv.org/abs/2508.03396", "authors": ["Rui Zou", "Mengqi Wei", "Yutao Zhu", "Jirong Wen", "Xin Zhao", "Jing Chen"], "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis", "comment": null, "summary": "Large Language Models (LLMs) excel in reasoning and generation across\ndomains, but still struggle with identifying and diagnosing complex errors.\nThis stems mainly from training objectives that prioritize correct answers,\nlimiting exposure to and learning from errors. While recent studies have begun\nto address this by introducing error signals, most rely on shallow, static\nerrors, restricting improvement in deep diagnostic ability. To overcome this,\nwe propose Hide and Seek Game (HSG), a dynamic adversarial framework for error\ngeneration and diagnosis, and evaluate it on mathematical problem-solving. HSG\ninvolves two adversarial roles: Sneaky, which \"hides\" by generating subtle,\ndeceptive reasoning errors, and Diagnosis, which \"seeks\" to accurately detect\nthem. Through adversarial co-evolution, both error stealth and diagnostic\nprecision are enhanced. Experiments on several math reasoning tasks show that\nHSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher\naccuracy than baselines like GPT-4o. We also release a challenging dataset of\ndeceptive errors and diagnostic annotations as a benchmark for future research.", "AI": {"tldr": "该论文提出Hide and Seek Game (HSG)框架，通过对抗性训练提升大型语言模型(LLMs)对复杂推理错误的诊断能力，在数学推理任务上显著优于基线模型。", "motivation": "LLMs在推理和生成方面表现出色，但难以识别和诊断复杂错误。这主要是因为训练目标优先考虑正确答案，导致模型接触和学习错误的机会有限。现有错误信号多为浅层、静态错误，无法有效提升深层诊断能力。", "method": "提出Hide and Seek Game (HSG)，一个动态对抗性框架，用于错误生成和诊断。框架包含两个对抗角色：'Sneaky'（生成微妙、欺骗性的推理错误）和'Diagnosis'（准确检测这些错误）。通过对抗性共同演化，提升错误隐蔽性和诊断精度。在数学问题解决任务上进行评估。", "result": "HSG显著提升了错误诊断能力，在多项数学推理任务上的准确率比GPT-4o等基线模型高出16.8%—31.4%。同时发布了一个具有挑战性的欺骗性错误和诊断标注数据集。", "conclusion": "HSG框架通过动态对抗性共同演化，有效解决了LLMs在复杂错误诊断方面的不足，显著提升了其诊断精度，为未来研究提供了新的基准和数据集。"}}
{"id": "2508.03533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03533", "abs": "https://arxiv.org/abs/2508.03533", "authors": ["Xiaoming Hou", "Jiquan Zhang", "Zibin Lin", "DaCheng Tao", "Shengli Zhang"], "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models", "comment": null, "summary": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation.", "AI": {"tldr": "EmbedGrad是一种通过梯度优化文本提示嵌入的新框架，解耦训练与部署，显著提升基础模型在多任务上的适应性，尤其对小型模型在复杂任务上效果显著。", "motivation": "预训练基础模型在适应多样化任务时面临挑战。现有方法（离散提示工程和连续参数调整）存在局限：离散方法缺乏精度，参数方法增加复杂性并降低可解释性。", "method": "提出EmbedGrad框架，通过基于梯度的优化方法精炼文本提示嵌入。该方法独特之处在于训练与部署解耦：训练时，利用带标签数据指导精确嵌入调整并保持语义；推理时，仅将优化后的嵌入与用户查询结合。", "result": "在数学推理、情感分析和因果判断任务上的综合评估证明了EmbedGrad的有效性。例如，优化Qwen2.5-Math-1.5B的推理提示，数学问题准确率从14.74%提高到58.96%。在不同模型规模（0.5B-14B）和所有任务上都观察到持续改进，特别是小型模型在复杂问题（如因果判断）上获得了显著提升。", "conclusion": "该工作在不改变模型架构的情况下，弥合了提示工程和参数效率之间的鸿沟，将嵌入精炼确立为任务适应的一种强大新范式。"}}
{"id": "2508.03144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03144", "abs": "https://arxiv.org/abs/2508.03144", "authors": ["Liangyang Ouyang", "Jiafeng Mao"], "title": "LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing", "comment": "We will make our implementation available soon", "summary": "Text-driven image editing enables users to flexibly modify visual content\nthrough natural language instructions, and is widely applied to tasks such as\nsemantic object replacement, insertion, and removal. While recent\ninversion-based editing methods using rectified flow models have achieved\npromising results in image quality, we identify a structural limitation in\ntheir editing behavior: the semantic bias toward the source concept encoded in\nthe inverted noise tends to suppress attention to the target concept. This\nissue becomes particularly critical when the source and target semantics are\ndissimilar, where the attention mechanism inherently leads to editing failure\nor unintended modifications in non-target regions. In this paper, we\nsystematically analyze and validate this structural flaw, and introduce LORE, a\ntraining-free and efficient image editing method. LORE directly optimizes the\ninverted noise, addressing the core limitations in generalization and\ncontrollability of existing approaches, enabling stable, controllable, and\ngeneral-purpose concept replacement, without requiring architectural\nmodification or model fine-tuning. We conduct comprehensive evaluations on\nthree challenging benchmarks: PIEBench, SmartEdit, and GapEdit. Experimental\nresults show that LORE significantly outperforms strong baselines in terms of\nsemantic alignment, image quality, and background fidelity, demonstrating the\neffectiveness and scalability of latent-space optimization for general-purpose\nimage editing.", "AI": {"tldr": "现有基于反演的文本驱动图像编辑方法存在语义偏差，导致编辑失败或非目标区域修改。本文提出LORE，一种无需训练的图像编辑方法，通过直接优化反演噪声，显著提升了概念替换的稳定性、可控性和泛化能力。", "motivation": "当前基于反演的整流流模型在文本驱动图像编辑中，存在结构性限制：反演噪声中编码的源概念语义偏向会抑制对目标概念的注意力。当源和目标语义不相似时，这尤其导致编辑失败或在非目标区域产生意外修改。", "method": "本文提出了LORE，一种无需训练且高效的图像编辑方法。LORE通过直接优化反演噪声来解决现有方法在泛化性和可控性方面的核心限制，从而实现稳定、可控、通用的概念替换，且无需修改架构或模型微调。", "result": "在PIEBench、SmartEdit和GapEdit三个挑战性基准测试中，LORE在语义对齐、图像质量和背景保真度方面显著优于现有强基线，证明了潜在空间优化在通用图像编辑中的有效性和可扩展性。", "conclusion": "LORE通过直接优化反演噪声，有效解决了现有文本驱动图像编辑方法中存在的语义偏差和注意力抑制问题，证明了潜在空间优化对于通用图像编辑的有效性和可扩展性，实现了更稳定、可控和泛化的概念替换。"}}
{"id": "2508.03406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03406", "abs": "https://arxiv.org/abs/2508.03406", "authors": ["Kai Li", "Ruihao Zheng", "Xinye Hao", "Zhenkun Wang"], "title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models", "comment": null, "summary": "In real-world routing problems, users often propose conflicting or\nunreasonable requirements, which result in infeasible optimization models due\nto overly restrictive or contradictory constraints, leading to an empty\nfeasible solution set. Existing Large Language Model (LLM)-based methods\nattempt to diagnose infeasible models, but modifying such models often involves\nmultiple potential adjustments that these methods do not consider. To fill this\ngap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which\ncombines LLM agents and multi-objective optimization within an automatic\nrouting solver, to provide a set of representative actionable suggestions.\nSpecifically, MOID employs multi-objective optimization to consider both path\ncost and constraint violation, generating a set of trade-off solutions, each\nencompassing varying degrees of model adjustments. To extract practical\ninsights from these solutions, MOID utilizes LLM agents to generate a solution\nanalysis function for the infeasible model. This function analyzes these\ndistinct solutions to diagnose the original infeasible model, providing users\nwith diverse diagnostic insights and suggestions. Finally, we compare MOID with\nseveral LLM-based methods on 50 types of infeasible routing problems. The\nresults indicate that MOID automatically generates multiple diagnostic\nsuggestions in a single run, providing more practical insights for restoring\nmodel feasibility and decision-making compared to existing methods.", "AI": {"tldr": "MOID结合大语言模型(LLM)和多目标优化，为不 S可行的路径规划问题提供多样的、可操作的诊断建议，以恢复模型可行性。", "motivation": "现实世界中的路径规划问题常因用户需求冲突或不合理导致模型不可行，现有基于LLM的方法虽能诊断但未能考虑多种潜在的调整方案。", "method": "MOID通过多目标优化（同时考虑路径成本和约束违反）生成一系列权衡解决方案，每种方案代表不同程度的模型调整。随后，利用LLM代理生成解决方案分析函数，对这些不同方案进行分析，从而诊断原始不可行模型，并提供多样化的诊断见解和建议。", "result": "在50种不可行路径规划问题上的比较结果表明，MOID在单次运行中能自动生成多个诊断建议，相较于现有方法，为恢复模型可行性和决策提供了更实用的见解。", "conclusion": "MOID有效解决了不可行路径规划模型的诊断和修复问题，通过结合多目标优化和LLM代理，为用户提供了更全面、实用的诊断见解和决策支持。"}}
{"id": "2508.03550", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03550", "abs": "https://arxiv.org/abs/2508.03550", "authors": ["Peng Lai", "Jianjie Zheng", "Sijie Cheng", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations", "comment": null, "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using large language models, a paradigm known as\n\"LLMas-a-judge.\" However, improving its alignment with human preferences\nwithout complex prompts or fine-tuning remains challenging. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and taskrelevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a lightweight and\nefficient framework for enhancing LLM-as-a-Judge alignment with human scoring,\nvia internal representations. LAGER produces fine-grained judgment scores by\naggregating cross-layer scoretoken logits and computing the expected score from\na softmax-based distribution, with the LLM backbone kept frozen. LAGER fully\nleverages the complementary information across different layers, overcoming the\nlimitations of relying solely on the final layer. We evaluate our method on the\nstandard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman\ncorrelation, and find that LAGER achieves improvements of up to 7.5% over the\nbest baseline across these benchmarks. Without reasoning steps, LAGER matches\nor outperforms reasoning-based methods. Experiments on downstream applications,\nsuch as data selection and emotional understanding, further show the\neffectiveness of our method.", "AI": {"tldr": "LAGER通过聚合LLM内部中间层的表示来提高“LLM即评委”与人类偏好的一致性，无需复杂提示或微调，实现了显著性能提升。", "motivation": "大规模评估任务中，“LLM即评委”的自动化评估范式被广泛采用，但如何在不使用复杂提示或微调的情况下提高其与人类偏好的一致性仍然是一个挑战。初步研究发现，LLM的中间层到上层编码的语义和任务相关表示通常比最终层更符合人类判断，这激发了本研究。", "method": "本文提出了LAGER框架，通过聚合跨层得分-token的logits，并从基于softmax的分布中计算预期得分，以生成细粒度的判断分数。该方法保持LLM骨干网络冻结，充分利用不同层之间的互补信息，克服了仅依赖最终层的局限性。", "result": "LAGER在标准对齐基准（Flask、HelpSteer、BIGGen）上，使用Spearman相关性评估，相对于最佳基线实现了高达7.5%的改进。在不使用推理步骤的情况下，LAGER能够匹配或超越基于推理的方法。在数据选择和情感理解等下游应用中也验证了其有效性。", "conclusion": "LAGER通过利用LLM的内部表示，有效增强了“LLM即评委”与人类评分的一致性，提供了一个轻量且高效的解决方案。它在多个基准测试中表现优异，甚至在没有推理步骤的情况下也能超越或匹配基于推理的方法，并在实际应用中展现了良好效果。"}}
{"id": "2508.03164", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03164", "abs": "https://arxiv.org/abs/2508.03164", "authors": ["Junyoung Lim", "Jaewoo Ahn", "Gunhee Kim"], "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning", "comment": "ICCV 2025 (Highlight)", "summary": "Generating accurate, informative, and hallucination-free captions for charts\nremains challenging for vision language models, primarily due to the lack of\nlarge-scale, high-quality datasets of real-world charts. However, existing\nreal-world chart datasets suffer from the inclusion of extraneous information\nthat cannot be inferred from the chart and failure to sufficiently capture\nstructural elements and key insights. Therefore, we introduce ChartCap, a\nlarge-scale dataset of 565K real-world chart images paired with type-specific,\ndense captions that exclude extraneous information and highlight both\nstructural elements and key insights in detail. To build ChartCap, we design a\nfour-stage pipeline that generates captions using only the discernible data\nfrom the chart and employ a cycle consistency-based human verification, which\naccelerates quality control without sacrificing accuracy. Additionally, we\npropose a novel metric, the Visual Consistency Score, which evaluates caption\nquality by measuring the similarity between the chart regenerated from a\ncaption and the original chart, independent of reference captions. Extensive\nexperiments confirms that models fine-tuned on ChartCap consistently generate\nmore accurate and informative captions with reduced hallucinations, surpassing\nboth open-source and proprietary models and even human-annotated captions.", "AI": {"tldr": "本文介绍了ChartCap，一个包含56.5万张真实世界图表图像及其类型特定、密集、无无关信息的描述的大规模数据集，旨在解决现有图表描述生成中幻觉和信息不足的问题。研究还提出了一种新的评估指标和构建方法，并证明了ChartCap能显著提升模型生成图表描述的准确性和信息量。", "motivation": "当前视觉语言模型在为图表生成准确、信息丰富且无幻觉的描述方面面临挑战，主要原因是缺乏大规模、高质量的真实世界图表数据集。现有数据集存在包含图表无法推断的无关信息，并且未能充分捕捉结构元素和关键见解的问题。", "method": "本文提出了ChartCap数据集，包含56.5万张真实世界图表图像，并配有类型特定、密集、排除无关信息且详细突出结构元素和关键见解的描述。构建ChartCap采用了一个四阶段流水线，仅使用图表可识别的数据生成描述，并采用基于循环一致性的人工验证来加速质量控制。此外，还提出了一种新的评估指标——视觉一致性分数（Visual Consistency Score），通过测量从描述重建的图表与原始图表之间的相似性来评估描述质量。", "result": "广泛的实验证实，在ChartCap上微调的模型能够持续生成更准确、信息更丰富且幻觉更少的描述，其性能超越了开源模型、专有模型乃至人工标注的描述。", "conclusion": "ChartCap数据集的引入及其构建方法和新的评估指标，显著提升了视觉语言模型在图表描述生成方面的能力，解决了现有数据集的局限性，为未来图表理解和描述生成研究奠定了基础。"}}
{"id": "2508.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03438", "abs": "https://arxiv.org/abs/2508.03438", "authors": ["Taine J. Elliott", "Stephen P. Levitt", "Ken Nixon", "Martin Bekker"], "title": "Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction", "comment": "18 pages, 8 figures, Published in the Annual Conference of South\n  African Institute of Computer Scientists and Information Technologists,\n  Preprint (author original)", "summary": "The rapid expansion of publicly-available medical data presents a challenge\nfor clinicians and researchers alike, increasing the gap between the volume of\nscientific literature and its applications. The steady growth of studies and\nfindings overwhelms medical professionals at large, hindering their ability to\nsystematically review and understand the latest knowledge. This paper presents\nan approach to information extraction and automatic knowledge graph (KG)\ngeneration to identify and connect biomedical knowledge. Through a pipeline of\nlarge language model (LLM) agents, the system decomposes 44 PubMed abstracts\ninto semantically meaningful proposition sentences and extracts KG triples from\nthese sentences. The triples are enhanced using a combination of open domain\nand ontology-based information extraction methodologies to incorporate\nontological categories. On top of this, a context variable is included during\nextraction to allow the triple to stand on its own - thereby becoming\n`quadruples'. The extraction accuracy of the LLM is validated by comparing\nnatural language sentences generated from the enhanced triples to the original\npropositions, achieving an average cosine similarity of 0.874. The similarity\nfor generated sentences of enhanced triples were compared with generated\nsentences of ordinary triples showing an increase as a result of the context\nvariable. Furthermore, this research explores the ability for LLMs to infer new\nrelationships and connect clusters in the knowledge base of the knowledge\ngraph. This approach leads the way to provide medical practitioners with a\ncentralised, updated in real-time, and sustainable knowledge source, and may be\nthe foundation of similar gains in a wide variety of fields.", "AI": {"tldr": "该研究提出一种利用大型语言模型（LLM）从医学摘要中提取信息并自动生成知识图谱（KG）的方法，以应对医学文献量激增的问题。", "motivation": "公开医学数据和科学文献的快速增长使临床医生和研究人员难以系统地回顾和理解最新知识，导致知识应用与文献量之间存在差距。", "method": "该方法通过一个LLM代理管道，将PubMed摘要分解为语义命题，并从中提取知识图谱三元组。这些三元组通过结合开放域和本体论信息提取方法进行增强，纳入本体类别，并添加上下文变量形成“四元组”。通过比较由增强三元组生成的自然语言句子与原始命题，验证LLM的提取准确性。此外，还探讨了LLM推断新关系和连接知识图谱中集群的能力。", "result": "LLM提取的准确性验证表明，由增强三元组生成的句子与原始命题的平均余弦相似度达到0.874。上下文变量的引入提高了生成句子与原始命题的相似度。研究还表明LLM能够推断新关系并连接知识库中的集群。", "conclusion": "该方法为医学从业者提供了一个集中、实时更新且可持续的知识来源，有望成为各领域类似进步的基础。"}}
{"id": "2508.03571", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03571", "abs": "https://arxiv.org/abs/2508.03571", "authors": ["Iing Muttakhiroh", "Thomas Fevens"], "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation", "comment": null, "summary": "Large Language Models (LLMs) often suffer from performance degradation when\nfaced with domain shifts, primarily due to catastrophic forgetting. In this\nwork, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),\na novel continual learning framework that integrates dynamic knowledge graphs\nwith instruction tuning. By leveraging retrieved domain-specific knowledge as\nguidance during training, KILO enhances both adaptability to new domains and\nretention of previously acquired knowledge. We pretrain our model on\nWikiText-103 and evaluate sequential adaptation across four diverse target\ndomains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that\nKILO consistently outperforms strong baselines, including continual\nfine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward\ntransfer, F1 score, retention rate, and training efficiency. These results\nhighlight the effectiveness of combining structured knowledge retrieval and\ninstruction prompting to overcome domain shift challenges in continual learning\nscenarios.", "AI": {"tldr": "KILO是一种新型的持续学习框架，通过结合动态知识图谱和指令微调，帮助大型语言模型在领域迁移中克服灾难性遗忘，同时提升新领域适应性和旧知识保留能力。", "motivation": "大型语言模型（LLMs）在面对领域迁移时，常因灾难性遗忘而导致性能下降。", "method": "本文提出了KILO（Knowledge-Instructed Learning for Continual Adaptation）框架，它将动态知识图谱与指令微调相结合。KILO在训练过程中利用检索到的特定领域知识作为指导，以增强对新领域的适应性和对先前知识的保留。模型在WikiText-103上进行预训练，并在BioASQ、SciQ、TweetEval和MIND四个不同目标领域进行顺序适应性评估。", "result": "KILO在逆向迁移、正向迁移、F1分数、知识保留率和训练效率方面，持续优于强基线模型，包括持续微调、ERNIE 2.0和CPT。", "conclusion": "结合结构化知识检索和指令提示，能够有效克服持续学习场景中的领域迁移挑战。"}}
{"id": "2508.03177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03177", "abs": "https://arxiv.org/abs/2508.03177", "authors": ["Zhaoxu Li", "Chenqi Kong", "Yi Yu", "Qiangqiang Wu", "Xinghao Jiang", "Ngai-Man Cheung", "Bihan Wen", "Alex Kot", "Xudong Jiang"], "title": "SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision", "comment": null, "summary": "Large Vision-Language Models (LVLMs) recently achieve significant\nbreakthroughs in understanding complex visual-textual contexts. However,\nhallucination issues still limit their real-world applicability. Although\nprevious mitigation methods effectively reduce hallucinations in photographic\nimages, they largely overlook the potential risks posed by stylized images,\nwhich play crucial roles in critical scenarios such as game scene\nunderstanding, art education, and medical analysis. In this work, we first\nconstruct a dataset comprising photographic images and their corresponding\nstylized versions with carefully annotated caption labels. We then conduct\nhead-to-head comparisons on both discriminative and generative tasks by\nbenchmarking 13 advanced LVLMs on the collected datasets. Our findings reveal\nthat stylized images tend to induce significantly more hallucinations than\ntheir photographic counterparts. To address this issue, we propose Style-Aware\nVisual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs'\nfinal outputs based on the token-level visual attention patterns, leveraging\nearly-layer feedback to mitigate hallucinations caused by stylized images.\nExtensive experiments demonstrate that SAVER achieves state-of-the-art\nperformance in hallucination mitigation across various models, datasets, and\ntasks.", "AI": {"tldr": "本文发现大型视觉语言模型（LVLMs）在处理风格化图像时存在严重的幻觉问题，并提出了一个名为SAVER的新机制，通过利用早期层视觉注意力反馈来有效缓解这一问题。", "motivation": "LVLMs在理解复杂视觉-文本上下文方面取得了显著进展，但幻觉问题限制了其在现实世界中的应用。现有缓解方法主要关注摄影图像，忽略了风格化图像带来的潜在风险，而风格化图像在游戏、艺术教育和医疗分析等关键场景中至关重要。", "method": "首先构建了一个包含摄影图像及其对应风格化版本并带有详细标注的数据集。其次，在该数据集上对13个先进的LVLMs进行了判别和生成任务的基准测试。最后，提出了Style-Aware Visual Early Revision (SAVER) 机制，该机制基于标记级别的视觉注意力模式，利用早期层反馈动态调整LVLMs的最终输出，以缓解风格化图像引起的幻觉。", "result": "研究发现，风格化图像比摄影图像更容易诱发显著更多的幻觉。大量实验表明，SAVER在各种模型、数据集和任务中都实现了最先进的幻觉缓解性能。", "conclusion": "风格化图像是导致LVLM幻觉的重要来源。SAVER通过利用早期层视觉注意力模式，能够有效缓解LVLMs在处理风格化图像时产生的幻觉问题，提升了模型的实际应用性。"}}
{"id": "2508.03465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03465", "abs": "https://arxiv.org/abs/2508.03465", "authors": ["Saleh Nikooroo"], "title": "Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence", "comment": null, "summary": "Belief systems are often treated as globally consistent sets of propositions\nor as scalar-valued probability distributions. Such representations tend to\nobscure the internal structure of belief, conflate external credibility with\ninternal coherence, and preclude the modeling of fragmented or contradictory\nepistemic states. This paper introduces a minimal formalism for belief systems\nas directed, weighted graphs. In this framework, nodes represent individual\nbeliefs, edges encode epistemic relationships (e.g., support or contradiction),\nand two distinct functions assign each belief a credibility (reflecting source\ntrust) and a confidence (derived from internal structural support). Unlike\nclassical probabilistic models, our approach does not assume prior coherence or\nrequire belief updating. Unlike logical and argumentation-based frameworks, it\nsupports fine-grained structural representation without committing to binary\njustification status or deductive closure. The model is purely static and\ndeliberately excludes inference or revision procedures. Its aim is to provide a\nfoundational substrate for analyzing the internal organization of belief\nsystems, including coherence conditions, epistemic tensions, and\nrepresentational limits. By distinguishing belief structure from belief\nstrength, this formalism enables a richer classification of epistemic states\nthan existing probabilistic, logical, or argumentation-based approaches.", "AI": {"tldr": "本文提出一种将信念系统建模为有向加权图的新形式主义，以更好地表示信念的内部结构、区分可信度与置信度，并处理碎片化或矛盾的认知状态。", "motivation": "传统的信念系统表示（如全局一致的命题集或标量概率分布）往往掩盖了信念的内部结构，混淆了外部可信度与内部连贯性，并且无法建模碎片化或矛盾的认知状态。", "method": "该方法将信念系统表示为有向加权图。图中节点代表个体信念，边编码认知关系（如支持或矛盾）。同时，引入两个独立函数：一个分配信念的可信度（反映来源信任），另一个分配置信度（源自内部结构支持）。该模型是纯静态的，不包含推理或修订过程。", "result": "通过区分信念结构与信念强度，该形式主义能够实现比现有概率、逻辑或基于论证的方法更丰富的认知状态分类。它提供了一个分析信念系统内部组织（包括连贯性条件、认知张力及表示限制）的基础基质。", "conclusion": "所提出的有向加权图形式主义为分析信念系统的内部组织提供了一个基础框架，克服了传统模型在表示复杂认知状态方面的局限性，实现了对认知状态更细致的分类。"}}
{"id": "2508.03644", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03644", "abs": "https://arxiv.org/abs/2508.03644", "authors": ["Wenxuan Shen", "Mingjia Wang", "Yaochen Wang", "Dongping Chen", "Junjie Yang", "Yao Wan", "Weiwei Lin"], "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?", "comment": "In submission. Project website: https://double-bench.github.io/", "summary": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.", "AI": {"tldr": "本文提出了一个名为Double-Bench的大规模、多语言、多模态评估系统，用于全面评估文档RAG系统，以克服现有基准的局限性。", "motivation": "当前的RAG系统评估基准存在不足，它们通常只关注文档RAG系统的特定部分，使用合成数据，且缺乏完整的真实标签和证据，未能反映现实世界的瓶颈和挑战，从而阻碍了多模态大型语言模型（MLLM）在复杂文档理解方面RAG系统的发展。", "method": "研究者引入了Double-Bench，一个包含3,276份文档（72,880页）和5,168个单跳及多跳查询的评估系统，涵盖6种语言和4种文档类型，并支持动态更新以解决潜在的数据污染。所有查询都基于详尽扫描的证据页面，并经过人工专家验证。研究者使用Double-Bench对9种最先进的嵌入模型、4种MLLM和4种端到端文档RAG框架进行了全面实验。", "result": "实验结果表明，文本和视觉嵌入模型之间的差距正在缩小，这凸显了构建更强大的文档检索模型的必要性。此外，研究发现当前的文档RAG框架存在过度自信的困境，即使没有证据支持也倾向于提供答案。", "conclusion": "Double-Bench提供了一个严格的基础，有望推动未来先进文档RAG系统的研究。研究团队计划及时检索语料并每年发布新的基准。"}}
{"id": "2508.03179", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.03179", "abs": "https://arxiv.org/abs/2508.03179", "authors": ["Ulugbek Alibekov", "Vanessa Staderini", "Philipp Schneider", "Doris Antensteiner"], "title": "Advancing Precision in Multi-Point Cloud Fusion Environments", "comment": "Accpeted for publication in Communications in Computer and\n  Information Science, Springer", "summary": "This research focuses on visual industrial inspection by evaluating point\nclouds and multi-point cloud matching methods. We also introduce a synthetic\ndataset for quantitative evaluation of registration method and various distance\nmetrics for point cloud comparison. Additionally, we present a novel\nCloudCompare plugin for merging multiple point clouds and visualizing surface\ndefects, enhancing the accuracy and efficiency of automated inspection systems.", "AI": {"tldr": "该研究通过评估点云和多点云匹配方法，并引入合成数据集、距离度量和CloudCompare插件，以提高工业视觉检测的准确性和效率。", "motivation": "提高自动化工业检测系统的准确性和效率，特别是在视觉工业检测中对点云进行评估和匹配。", "method": "评估点云和多点云匹配方法；引入合成数据集用于配准方法和距离度量的定量评估；开发CloudCompare插件用于合并多点云和可视化表面缺陷。", "result": "提出了一种用于定量评估配准方法和距离度量的合成数据集；开发了一个新的CloudCompare插件，能够合并多点云并可视化表面缺陷。", "conclusion": "所提出的方法和工具（包括合成数据集和CloudCompare插件）能够提高自动化工业检测系统的准确性和效率。"}}
{"id": "2508.03484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03484", "abs": "https://arxiv.org/abs/2508.03484", "authors": ["Zhiyao Xu", "Dan Zhao", "Qingsong Zou", "Qing Li", "Yong Jiang", "Yuhang Wang", "Jingyu Xiao"], "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes", "comment": null, "summary": "As smart homes become increasingly prevalent, intelligent models are widely\nused for tasks such as anomaly detection and behavior prediction. These models\nare typically trained on static datasets, making them brittle to behavioral\ndrift caused by seasonal changes, lifestyle shifts, or evolving routines.\nHowever, collecting new behavior data for retraining is often impractical due\nto its slow pace, high cost, and privacy concerns. In this paper, we propose\nSmartGen, an LLM-based framework that synthesizes context-aware user behavior\ndata to support continual adaptation of downstream smart home models. SmartGen\nconsists of four key components. First, we design a Time and Semantic-aware\nSplit module to divide long behavior sequences into manageable, semantically\ncoherent subsequences under dual time-span constraints. Second, we propose\nSemantic-aware Sequence Compression to reduce input length while preserving\nrepresentative semantics by clustering behavior mapping in latent space. Third,\nwe introduce Graph-guided Sequence Synthesis, which constructs a behavior\nrelationship graph and encodes frequent transitions into prompts, guiding the\nLLM to generate data aligned with contextual changes while retaining core\nbehavior patterns. Finally, we design a Two-stage Outlier Filter to identify\nand remove implausible or semantically inconsistent outputs, aiming to improve\nthe factual coherence and behavioral validity of the generated sequences.\nExperiments on three real-world datasets demonstrate that SmartGen\nsignificantly enhances model performance on anomaly detection and behavior\nprediction tasks under behavioral drift, with anomaly detection improving by\n85.43% and behavior prediction by 70.51% on average. The code is available at\nhttps://github.com/horizonsinzqs/SmartGen.", "AI": {"tldr": "SmartGen是一个基于LLM的框架，通过合成上下文感知的用户行为数据，以支持智能家居模型在行为漂移下的持续适应。", "motivation": "智能家居模型通常在静态数据集上训练，但用户行为会因季节、生活方式变化等原因发生漂移，导致模型性能下降。重新收集新行为数据进行再训练成本高昂、耗时且涉及隐私问题，因此需要一种无需实际数据收集即可适应行为漂移的方法。", "method": "SmartGen包含四个核心组件：1) 时间和语义感知分割模块，将长行为序列分解为有意义的子序列；2) 语义感知序列压缩，通过潜在空间聚类减少输入长度同时保留语义；3) 图引导序列合成，构建行为关系图并编码频繁转换，指导LLM生成符合上下文变化的数据；4) 两阶段异常值过滤器，识别并移除不合理或语义不一致的生成数据，以提高数据有效性。", "result": "在三个真实世界数据集上的实验表明，SmartGen显著提升了模型在行为漂移下的异常检测和行为预测任务性能。异常检测平均提升85.43%，行为预测平均提升70.51%。", "conclusion": "SmartGen通过合成上下文感知的用户行为数据，有效解决了智能家居模型在行为漂移下的持续适应问题，显著提高了异常检测和行为预测的性能。"}}
{"id": "2508.03654", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03654", "abs": "https://arxiv.org/abs/2508.03654", "authors": ["Xinyu Wang", "Yue Zhang", "Liqiang Jing"], "title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?", "comment": "Accepted by CIKM 2025", "summary": "Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA.", "AI": {"tldr": "本文评估了大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）中的表现，发现其在视觉理解和概念知识方面的局限性，并提出了一种免训练框架，通过深度对象提取和外部概念知识来提升LVLMs的讽刺理解与解释能力。", "motivation": "讽刺是一种复杂的语言现象，对情感分析等任务构成挑战。传统讽刺检测方法主要侧重文本，尽管已有结合多模态信息的方法，但大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）中的应用仍未被充分探索。现有LVLMs在视觉理解和概念知识方面存在不足，亟需解决。", "method": "研究人员评估了LVLMs在多模态讽刺检测和多模态讽刺解释任务中的性能。他们通过实验识别了LVLMs的关键局限性（如视觉理解不足和概念知识缺乏）。为解决这些问题，提出了一种免训练框架，该框架整合了深度对象提取和外部概念知识，以增强模型解释多模态语境中讽刺的能力。", "result": "综合实验结果表明，LVLMs在多模态讽刺分析中存在视觉理解不足和概念知识缺乏等局限性。然而，所提出的免训练框架在多个模型上均显示出有效性，显著提升了模型解释和理解多模态讽刺的能力。", "conclusion": "大型视觉语言模型在多模态讽刺分析中面临视觉理解和概念知识的挑战。通过引入深度对象提取和外部概念知识的免训练框架，可以有效提升LVLMs在多模态语境中解释和理解讽刺的能力。"}}
{"id": "2508.03180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03180", "abs": "https://arxiv.org/abs/2508.03180", "authors": ["Weihang Liu", "Yuke Li", "Yuxuan Li", "Jingyi Yu", "Xin Lou"], "title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable\nrendering fidelity and efficiency. However, these methods still rely on\ncomputationally expensive sequential alpha-blending operations, resulting in\nsignificant overhead, particularly on resource-constrained platforms. In this\npaper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy\nGaussian representations with order-independent rendering techniques to achieve\nphotorealistic results while sustaining real-time performance. To mitigate the\noverhead caused by view-adaptive radix sort, we introduce cell proxies for\nlocal Gaussians management and propose cell search rasterization for further\nacceleration. By seamlessly combining our framework with Order-Independent\nTransparency (OIT), we develop a physically inspired weighted sum rendering\ntechnique that simultaneously eliminates \"popping\" and \"transparency\"\nartifacts, yielding substantial improvements in both accuracy and efficiency.\nExtensive experiments on a variety of real-world datasets demonstrate the\nrobustness of our method across diverse scenarios, including multi-scale\ntraining views and large-scale environments. Our results validate the\nadvantages of the OIT rendering paradigm in Gaussian Splatting, achieving\nhigh-quality rendering with an impressive 1.5 to 4 speedup over existing OIT\nbased Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix\nsort overhead without quality degradation.", "AI": {"tldr": "Duplex-GS是一种双层级框架，结合代理高斯表示和顺序无关渲染（OIT）技术，显著提升了3D高斯泼溅（3DGS）的渲染效率和质量，尤其适用于资源受限平台。", "motivation": "现有3D高斯泼溅方法依赖计算昂贵的顺序alpha混合操作，导致显著开销，尤其在资源受限平台上性能不佳。", "method": "提出Duplex-GS，一个双层级框架，集成代理高斯表示与顺序无关渲染技术。引入单元代理管理局部高斯，并提出单元搜索光栅化以加速。与OIT结合，开发了物理启发式加权和渲染技术，消除“弹出”和“透明度”伪影。", "result": "实现了逼真的渲染效果和实时性能。在多种真实世界数据集上表现出鲁棒性，包括多尺度训练视图和大规模环境。与现有基于OIT的3DGS方法相比，渲染速度提升1.5到4倍，radix排序开销减少52.2%到86.9%，且无质量损失。", "conclusion": "Duplex-GS验证了OIT渲染范式在高斯泼溅中的优势，通过解决效率和伪影问题，显著提升了渲染质量和性能。"}}
{"id": "2508.03488", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03488", "abs": "https://arxiv.org/abs/2508.03488", "authors": ["Khaled Bachir Delassi", "Lakhdar Zeggane", "Hadda Cherroun", "Abdelhamid Haouhat", "Kaoutar Bouzouad"], "title": "VQA support to Arabic Language Learning Educational Tool", "comment": null, "summary": "We address the problem of scarcity of educational Arabic Language Learning\ntools that advocate modern pedagogical models such as active learning which\nensures language proficiency. In fact, we investigate the design and evaluation\nof an AI-powered educational tool designed to enhance Arabic language learning\nfor non-native speakers with beginner-to-intermediate proficiency level. The\ntool leverages advanced AI models to generate interactive visual quizzes,\ndeploying Visual Question Answering as the primary activity. Adopting a\nconstructivist learning approach, the system encourages active learning through\nreal-life visual quizzes, and image-based questions that focus on improving\nvocabulary, grammar, and comprehension. The system integrates Vision-Language\nPretraining models to generate contextually relevant image description from\nwhich Large Language Model generate assignments based on customized Arabic\nlanguage Learning quizzes thanks to prompting.\n  The effectiveness of the tool is evaluated through a manual annotated\nbenchmark consisting of 1266 real-life visual quizzes, with human participants\nproviding feedback. The results show a suitable accuracy rates, validating the\ntool's potential to bridge the gap in Arabic language education and\nhighlighting the tool's promise as a reliable, AI-powered resource for Arabic\nlearners, offering personalized and interactive learning experiences.", "AI": {"tldr": "本文设计并评估了一个AI驱动的阿拉伯语学习工具，通过生成交互式视觉测验（基于VQA、VLP和LLM）来解决现代教学工具的稀缺问题，并经验证其有效性。", "motivation": "目前缺乏支持主动学习等现代教学模式的阿拉伯语学习工具，这阻碍了语言熟练度的提升。研究旨在填补这一空白，为非母语的初中级学习者提供一个AI赋能的解决方案。", "method": "开发了一个AI驱动的教育工具，主要活动是视觉问答（VQA），采用建构主义学习方法。系统利用视觉-语言预训练（VLP）模型生成图像描述，并结合大型语言模型（LLM）通过提示词生成定制化的阿拉伯语学习测验，侧重于词汇、语法和理解。工具的有效性通过包含1266个真实视觉测验的手动标注基准进行评估，并收集了人类参与者的反馈。", "result": "评估结果显示该工具具有合适的准确率，验证了其弥补阿拉伯语教育差距的潜力，并突显了其作为可靠、AI驱动的阿拉伯语学习资源的广阔前景，能够提供个性化和互动式的学习体验。", "conclusion": "该AI驱动的阿拉伯语学习工具是有效且有前景的，它通过利用先进的AI模型提供互动式、个性化的学习体验，有望成为阿拉伯语学习者的可靠资源，解决现有教育工具的不足。"}}
{"id": "2508.03668", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03668", "abs": "https://arxiv.org/abs/2508.03668", "authors": ["Zixuan Li", "Binzong Geng", "Jing Xiong", "Yong He", "Yuxuan Hu", "Jian Chen", "Dingwei Chen", "Xiyu Chang", "Liang Zhang", "Linjian Mo", "Chengming Li", "Chuan Yuan", "Zhenan Sun"], "title": "CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction", "comment": null, "summary": "Click-Through Rate (CTR) prediction, a core task in recommendation systems,\nestimates user click likelihood using historical behavioral data. Modeling user\nbehavior sequences as text to leverage Language Models (LMs) for this task has\ngained traction, owing to LMs' strong semantic understanding and contextual\nmodeling capabilities. However, a critical structural gap exists: user behavior\nsequences consist of discrete actions connected by semantically empty\nseparators, differing fundamentally from the coherent natural language in LM\npre-training. This mismatch causes semantic fragmentation, where LM attention\nscatters across irrelevant tokens instead of focusing on meaningful behavior\nboundaries and inter-behavior relationships, degrading prediction performance.\nTo address this, we propose $\\textit{CTR-Sink}$, a novel framework introducing\nbehavior-level attention sinks tailored for recommendation scenarios. Inspired\nby attention sink theory, it constructs attention focus sinks and dynamically\nregulates attention aggregation via external information. Specifically, we\ninsert sink tokens between consecutive behaviors, incorporating\nrecommendation-specific signals such as temporal distance to serve as stable\nattention sinks. To enhance generality, we design a two-stage training strategy\nthat explicitly guides LM attention toward sink tokens and a attention sink\nmechanism that amplifies inter-sink dependencies to better capture behavioral\ncorrelations. Experiments on one industrial dataset and two open-source\ndatasets (MovieLens, Kuairec), alongside visualization results, validate the\nmethod's effectiveness across scenarios.", "AI": {"tldr": "针对推荐系统中基于语言模型（LM）的点击率（CTR）预测中用户行为序列与自然语言不匹配导致的语义碎片化问题，本文提出了CTR-Sink框架，通过引入行为级注意力汇聚点（attention sinks）来引导LM的注意力，从而提升预测性能。", "motivation": "尽管语言模型在语义理解和上下文建模方面能力强大，但用户行为序列（离散动作、语义空分隔符）与LM预训练的连贯自然语言存在根本性结构差异。这种不匹配导致语义碎片化，使得LM的注意力分散在无关标记上，未能有效关注有意义的行为边界和行为间关系，从而降低了预测性能。", "method": "本文提出了CTR-Sink框架，引入行为级注意力汇聚点：1) 在连续行为之间插入汇聚点标记（sink tokens），并融入推荐场景特有的信号（如时间距离）作为稳定的注意力汇聚点；2) 设计了两阶段训练策略，显式引导LM的注意力聚焦于汇聚点标记；3) 设计了注意力汇聚机制以增强汇聚点间的依赖关系，更好地捕捉行为间的关联性。", "result": "在工业数据集和两个开源数据集（MovieLens、Kuairec）上的实验，以及可视化结果，都验证了该方法在不同场景下的有效性。", "conclusion": "CTR-Sink框架通过引入行为级注意力汇聚点，成功解决了基于语言模型的CTR预测中用户行为序列的语义碎片化问题，有效提升了预测性能，并在多种场景下展现出良好效果。"}}
{"id": "2508.03186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03186", "abs": "https://arxiv.org/abs/2508.03186", "authors": ["Heng Wu", "Qian Zhang", "Guixu Zhang"], "title": "Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling", "comment": null, "summary": "Accurate monocular depth estimation remains a challenging problem due to the\ninherent ambiguity that stems from the ill-posed nature of recovering 3D\nstructure from a single view, where multiple plausible depth configurations can\nproduce identical 2D projections. In this paper, we present a novel depth\nestimation method that combines both local and global cues to improve\nprediction accuracy. Specifically, we propose the Gated Large Kernel Attention\nModule (GLKAM) to effectively capture multi-scale local structural information\nby leveraging large kernel convolutions with a gated mechanism. To further\nenhance the global perception of the network, we introduce the Global Bin\nPrediction Module (GBPM), which estimates the global distribution of depth bins\nand provides structural guidance for depth regression. Extensive experiments on\nthe NYU-V2 and KITTI dataset demonstrate that our method achieves competitive\nperformance and outperforms existing approaches, validating the effectiveness\nof each proposed component.", "AI": {"tldr": "本文提出一种结合局部和全局线索的单目深度估计算法，通过Gated Large Kernel Attention Module (GLKAM)捕获局部多尺度信息，并通过Global Bin Prediction Module (GBPM)提供全局深度分布指导，在NYU-V2和KITTI数据集上表现优异。", "motivation": "单目深度估计是一个具有挑战性的问题，因为从单一视图恢复3D结构本质上是一个病态问题，存在多种可能的深度配置产生相同的2D投影的模糊性。", "method": "1. 提出Gated Large Kernel Attention Module (GLKAM)，利用大核卷积和门控机制有效捕获多尺度局部结构信息。2. 引入Global Bin Prediction Module (GBPM)，估计深度bin的全局分布，为深度回归提供结构指导。", "result": "在NYU-V2和KITTI数据集上进行了广泛实验，结果表明所提出的方法达到了有竞争力的性能，并优于现有方法，验证了每个提出组件的有效性。", "conclusion": "所提出的结合局部和全局线索的单目深度估计算法及其组件是有效的，并在标准数据集上取得了领先的性能。"}}
{"id": "2508.03500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03500", "abs": "https://arxiv.org/abs/2508.03500", "authors": ["Yijin Yang", "Cristina Cornelio", "Mario Leiva", "Paulo Shakarian"], "title": "Error Detection and Correction for Interpretable Mathematics in Large Language Models", "comment": null, "summary": "Recent large language models (LLMs) have demonstrated the ability to perform\nexplicit multi-step reasoning such as chain-of-thought prompting. However,\ntheir intermediate steps often contain errors that can propagate leading to\ninaccurate final predictions. Additionally, LLMs still struggle with\nhallucinations and often fail to adhere to prescribed output formats, which is\nparticularly problematic for tasks like generating mathematical expressions or\nsource code. This work introduces EDCIM (Error Detection and Correction for\nInterpretable Mathematics), a method for detecting and correcting these errors\nin interpretable mathematics tasks, where the model must generate the exact\nfunctional form that explicitly solve the problem (expressed in natural\nlanguage) rather than a black-box solution. EDCIM uses LLMs to generate a\nsystem of equations for a given problem, followed by a symbolic error-detection\nframework that identifies errors and provides targeted feedback for LLM-based\ncorrection. To optimize efficiency, EDCIM integrates lightweight, open-source\nLLMs with more powerful proprietary models, balancing cost and accuracy. This\nbalance is controlled by a single hyperparameter, allowing users to control the\ntrade-off based on their cost and accuracy requirements. Experimental results\nacross different datasets show that EDCIM significantly reduces both\ncomputational and financial costs, while maintaining, and even improving,\nprediction accuracy when the balance is properly configured.", "AI": {"tldr": "EDCIM是一种针对可解释数学任务中大型语言模型（LLM）推理错误的检测与纠正方法，通过结合符号错误检测和分层LLM使用，显著降低成本并保持或提高准确性。", "motivation": "当前LLM在多步推理中常出现中间步骤错误，导致最终预测不准确；同时存在幻觉问题，难以遵循特定输出格式（如数学表达式或源代码）。", "method": "EDCIM使用LLM生成问题方程组，接着采用符号错误检测框架识别错误并提供有针对性的反馈以供LLM纠正。为优化效率，它结合了轻量级开源LLM和更强大的专有模型，并通过一个超参数平衡成本与准确性。", "result": "实验结果表明，EDCIM显著降低了计算和财务成本，同时在适当配置下保持甚至提高了预测准确性。", "conclusion": "EDCIM成功解决了LLM在可解释数学任务中推理错误的问题，提供了一个成本效益高且准确的解决方案，通过结合LLM生成与符号错误检测，并灵活集成不同能力模型。"}}
{"id": "2508.03677", "categories": ["cs.CL", "stat.ML", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.03677", "abs": "https://arxiv.org/abs/2508.03677", "authors": ["Arturo Pérez-Peralta", "Sandra Benítez-Peña", "Rosa E. Lillo"], "title": "FairLangProc: A Python package for fairness in NLP", "comment": "40 pages, 4 figures, 3 tables", "summary": "The rise in usage of Large Language Models to near ubiquitousness in recent\nyears has risen societal concern about their applications in decision-making\ncontexts, such as organizational justice or healthcare. This, in turn, poses\nquestions about the fairness of these models in critical settings, which leads\nto the developement of different procedures to address bias in Natural Language\nProcessing. Although many datasets, metrics and algorithms have been proposed\nto measure and mitigate harmful prejudice in Natural Language Processing, their\nimplementation is diverse and far from centralized. As a response, this paper\npresents FairLangProc, a comprehensive Python package providing a common\nimplementation of some of the more recent advances in fairness in Natural\nLanguage Processing providing an interface compatible with the famous Hugging\nFace transformers library, aiming to encourage the widespread use and\ndemocratization of bias mitigation techniques. The implementation can be found\non https://github.com/arturo-perez-peralta/FairLangProc.", "AI": {"tldr": "本文提出了FairLangProc，一个Python包，旨在为自然语言处理中的偏见缓解技术提供统一的实现，并与Hugging Face Transformers库兼容，以促进这些技术的广泛应用。", "motivation": "大型语言模型在决策场景（如组织公正或医疗保健）中的广泛应用引发了社会对模型公平性的担忧。尽管已提出多种数据集、度量和算法来衡量和缓解自然语言处理中的有害偏见，但它们的实现方式多样且缺乏集中性。", "method": "开发了一个名为FairLangProc的综合性Python包，它提供了自然语言处理中最新公平性进展的通用实现，并提供与Hugging Face Transformers库兼容的接口。", "result": "FairLangProc包提供了一个统一的平台，整合了多种偏见缓解技术，并与流行的Hugging Face生态系统无缝集成，旨在降低使用门槛，促进偏见缓解技术的普及。", "conclusion": "FairLangProc的推出有助于鼓励偏见缓解技术的广泛使用和民主化，从而解决大型语言模型在关键决策场景中的公平性问题。"}}
{"id": "2508.03189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03189", "abs": "https://arxiv.org/abs/2508.03189", "authors": ["Tianshuo Zhang", "Siran Peng", "Li Gao", "Haoyuan Zhang", "Xiangyu Zhu", "Zhen Lei"], "title": "Unifying Locality of KANs and Feature Drift Compensation for Data-free Continual Face Forgery Detection", "comment": null, "summary": "The rapid advancements in face forgery techniques necessitate that detectors\ncontinuously adapt to new forgery methods, thus situating face forgery\ndetection within a continual learning paradigm. However, when detectors learn\nnew forgery types, their performance on previous types often degrades rapidly,\na phenomenon known as catastrophic forgetting. Kolmogorov-Arnold Networks\n(KANs) utilize locally plastic splines as their activation functions, enabling\nthem to learn new tasks by modifying only local regions of the functions while\nleaving other areas unaffected. Therefore, they are naturally suitable for\naddressing catastrophic forgetting. However, KANs have two significant\nlimitations: 1) the splines are ineffective for modeling high-dimensional\nimages, while alternative activation functions that are suitable for images\nlack the essential property of locality; 2) in continual learning, when\nfeatures from different domains overlap, the mapping of different domains to\ndistinct curve regions always collapses due to repeated modifications of the\nsame regions. In this paper, we propose a KAN-based Continual Face Forgery\nDetection (KAN-CFD) framework, which includes a Domain-Group KAN Detector\n(DG-KD) and a data-free replay Feature Separation strategy via KAN Drift\nCompensation Projection (FS-KDCP). DG-KD enables KANs to fit high-dimensional\nimage inputs while preserving locality and local plasticity. FS-KDCP avoids the\noverlap of the KAN input spaces without using data from prior tasks.\nExperimental results demonstrate that the proposed method achieves superior\nperformance while notably reducing forgetting.", "AI": {"tldr": "本文提出了一种基于Kolmogorov-Arnold网络（KANs）的持续人脸伪造检测框架（KAN-CFD），以解决持续学习中灾难性遗忘问题，并通过改进KANs来处理高维图像输入和避免特征空间重叠。", "motivation": "人脸伪造技术快速发展，要求检测器持续适应新方法，这属于持续学习范畴。然而，现有检测器在学习新伪造类型时，对旧类型的性能会迅速下降，即灾难性遗忘。KANs因其局部可塑性激活函数理论上适合解决此问题，但其在处理高维图像和持续学习中特征重叠时存在局限性。", "method": "提出了KAN-based Continual Face Forgery Detection (KAN-CFD) 框架，包含两个核心组件：1) Domain-Group KAN Detector (DG-KD)，使KANs能够处理高维图像输入并保持局部性和局部可塑性；2) data-free replay Feature Separation strategy via KAN Drift Compensation Projection (FS-KDCP)，在不使用过往任务数据的情况下避免KAN输入空间的重叠。", "result": "实验结果表明，所提出的方法在持续人脸伪造检测中取得了卓越的性能，并显著减少了灾难性遗忘。", "conclusion": "所提出的KAN-CFD框架有效解决了持续人脸伪造检测中的灾难性遗忘问题，通过对KANs的创新改进，使其能够适应高维图像输入和避免特征空间重叠，从而在性能和遗忘减少方面均表现出色。"}}
{"id": "2508.03616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03616", "abs": "https://arxiv.org/abs/2508.03616", "authors": ["Jorge Gallego-Feliciano", "S. Aaron McClendon", "Juan Morinelli", "Stavros Zervoudakis", "Antonios Saravanos"], "title": "Hidden Dynamics of Massive Activations in Transformer Training", "comment": null, "summary": "Massive activations are scalar values in transformer hidden states that\nachieve values orders of magnitude larger than typical activations and have\nbeen shown to be critical for model functionality. While prior work has\ncharacterized these phenomena in fully trained models, the temporal dynamics of\ntheir emergence during training remain poorly understood. We present the first\ncomprehensive analysis of massive activation development throughout transformer\ntraining, using the Pythia model family as our testbed. Through systematic\nanalysis of various model sizes across multiple training checkpoints, we\ndemonstrate that massive activation emergence follows predictable mathematical\npatterns that can be accurately modeled using an exponentially-modulated\nlogarithmic function with five key parameters. We develop a machine learning\nframework to predict these mathematical parameters from architectural\nspecifications alone, achieving high accuracy for steady-state behavior and\nmoderate accuracy for emergence timing and magnitude. These findings enable\narchitects to predict and potentially control key aspects of massive activation\nemergence through design choices, with significant implications for model\nstability, training cycle length, interpretability, and optimization. Our\nfindings demonstrate that the emergence of massive activations is governed by\nmodel design and can be anticipated, and potentially controlled, before\ntraining begins.", "AI": {"tldr": "本文首次全面分析了Transformer模型训练过程中巨量激活（massive activations）的出现动态，发现其遵循可预测的数学模式，并开发了一个机器学习框架，仅通过架构规范即可预测这些模式的关键参数。", "motivation": "先前的研究已经表征了完全训练模型中的巨量激活现象，但其在训练过程中出现的时间动态仍不清楚。", "method": "研究使用Pythia模型家族作为测试平台，系统分析了不同模型尺寸在多个训练检查点上的巨量激活发展。开发了一个机器学习框架，用于仅从架构规范预测巨量激活出现模式的数学参数。", "result": "巨量激活的出现遵循可预测的数学模式，可以用一个五参数的指数调制对数函数精确建模。开发的机器学习框架在预测稳态行为方面实现了高精度，在预测出现时间和幅度方面实现了中等精度。", "conclusion": "巨量激活的出现受模型设计支配，可以在训练开始前预测甚至潜在地控制，这对模型的稳定性、训练周期长度、可解释性和优化具有重要意义。"}}
{"id": "2508.03678", "categories": ["cs.CL", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03678", "abs": "https://arxiv.org/abs/2508.03678", "authors": ["Yangtian Zi", "Harshitha Menon", "Arjun Guha"], "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation", "comment": null, "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.", "AI": {"tldr": "研究LLM在特定代码任务上表现不佳的原因，通过引入PartialOrderEval方法，系统性地探索提示细节对LLM代码生成能力的影响，发现提示特异性是关键因素。", "motivation": "LLM在通用基准测试（如HumanEval）上表现优异，但在专业基准测试（如ParEval）上表现不佳。研究旨在探究这是否是由于LLM缺乏领域知识，还是因为提示细节不足。", "method": "引入PartialOrderEval方法，该方法能为任何代码生成基准测试增加从最小到最大详细程度的提示偏序。将此方法应用于HumanEval以及ParEval的串行和OpenMP子集，并使用Llama-3.x和Qwen2.5-Coder模型测量pass@1如何随提示特异性变化。", "result": "实验表明，不同任务对提示敏感度存在不同程度的差异。定性分析突出显示，明确的输入/输出规范、边缘情况处理和分步细化是提示细节改进的关键驱动因素。", "conclusion": "LLM在代码生成任务上的表现，尤其是在专业领域，受到提示细节的显著影响。通过增加提示特异性，如明确I/O、处理边缘情况和提供逐步分解，可以有效提升模型性能，表明提示细节而非仅仅领域知识是关键因素。"}}
{"id": "2508.03197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03197", "abs": "https://arxiv.org/abs/2508.03197", "authors": ["Tao Chen", "Dan Zhang", "Da Chen", "Huazhu Fu", "Kai Jin", "Shanshan Wang", "Laurent D. Cohen", "Yitian Zhao", "Quanyong Yi", "Jiong Zhang"], "title": "Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network", "comment": null, "summary": "Choroidal neovascularization (CNV), a primary characteristic of wet\nage-related macular degeneration (wet AMD), represents a leading cause of\nblindness worldwide. In clinical practice, optical coherence tomography\nangiography (OCTA) is commonly used for studying CNV-related pathological\nchanges, due to its micron-level resolution and non-invasive nature. Thus,\naccurate segmentation of CNV regions and vessels in OCTA images is crucial for\nclinical assessment of wet AMD. However, challenges existed due to irregular\nCNV shapes and imaging limitations like projection artifacts, noises and\nboundary blurring. Moreover, the lack of publicly available datasets\nconstraints the CNV analysis. To address these challenges, this paper\nconstructs the first publicly accessible CNV dataset (CNVSeg), and proposes a\nnovel multilateral graph convolutional interaction-enhanced CNV segmentation\nnetwork (MTG-Net). This network integrates both region and vessel morphological\ninformation, exploring semantic and geometric duality constraints within the\ngraph domain. Specifically, MTG-Net consists of a multi-task framework and two\ngraph-based cross-task modules: Multilateral Interaction Graph Reasoning (MIGR)\nand Multilateral Reinforcement Graph Reasoning (MRGR). The multi-task framework\nencodes rich geometric features of lesion shapes and surfaces, decoupling the\nimage into three task-specific feature maps. MIGR and MRGR iteratively reason\nabout higher-order relationships across tasks through a graph mechanism,\nenabling complementary optimization for task-specific objectives. Additionally,\nan uncertainty-weighted loss is proposed to mitigate the impact of artifacts\nand noise on segmentation accuracy. Experimental results demonstrate that\nMTG-Net outperforms existing methods, achieving a Dice socre of 87.21\\% for\nregion segmentation and 88.12\\% for vessel segmentation.", "AI": {"tldr": "该研究构建了首个公开的CNV数据集（CNVSeg），并提出了一个新颖的多边图卷积交互增强网络（MTG-Net），用于光学相干断层扫描血管造影（OCTA）图像中的脉络膜新生血管（CNV）区域和血管的精确分割，以解决不规则形状、伪影和数据集缺乏等挑战。", "motivation": "脉络膜新生血管（CNV）是湿性年龄相关性黄斑变性（湿性AMD）的主要特征，是全球失明的主要原因。在临床实践中，OCTA常用于研究CNV相关的病理变化，因此，准确分割CNV区域和血管对于湿性AMD的临床评估至关重要。然而，CNV形状不规则、成像限制（如投影伪影、噪声和边界模糊）以及缺乏公开数据集带来了挑战。", "method": "本研究构建了首个公开的CNV数据集（CNVSeg），并提出了一个新颖的多边图卷积交互增强CNV分割网络（MTG-Net）。MTG-Net整合了区域和血管的形态信息，在图域中探索语义和几何对偶约束。具体而言，MTG-Net包含一个多任务框架和两个基于图的跨任务模块：多边交互图推理（MIGR）和多边强化图推理（MRGR）。多任务框架编码病灶形状和表面的丰富几何特征，将图像解耦为三个任务特定的特征图。MIGR和MRGR通过图机制迭代推理跨任务的高阶关系，实现任务特定目标的互补优化。此外，还提出了一种不确定性加权损失，以减轻伪影和噪声对分割精度的影响。", "result": "实验结果表明，MTG-Net优于现有方法，在区域分割方面实现了87.21%的Dice分数，在血管分割方面实现了88.12%的Dice分数。", "conclusion": "MTG-Net通过整合区域和血管信息、利用图推理机制以及引入不确定性加权损失，有效解决了CNV分割中的挑战，并在新构建的公开数据集上取得了优异的性能，为湿性AMD的临床评估提供了重要工具。"}}
{"id": "2508.03622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03622", "abs": "https://arxiv.org/abs/2508.03622", "authors": ["Jialin Li", "Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "title": "Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework", "comment": null, "summary": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.", "AI": {"tldr": "大语言模型在错误前提下生成代码时易出现幻觉，本文提出FPBench评估框架，发现模型推理和自查能力差，并揭示了不同错误前提对模型的影响。", "motivation": "随着大语言模型（LLMs）代码生成能力的提升，其对输入前提的依赖性增强。当用户提供包含错误前提的输入时，代码生成幻觉的概率显著增加，暴露出LLMs自查能力的不足。", "method": "提出了首个针对错误前提的代码生成评估框架FPBench。通过系统构建三类错误前提，并集成多维度评估指标，对15个代表性LLMs进行了深入评估。", "result": "1. 大多数模型在错误前提下表现出较差的推理能力和次优的代码生成性能，严重依赖显式提示进行错误检测，自查能力有限；2. 错误前提导致资源投入回报递减，盲目增加输入长度无法提高质量；3. 三类错误前提分别激活了模型不同的缺陷模式，揭示了代码生成模型认知机制的三重分离。", "conclusion": "本研究不仅强调了LLMs在代码生成中主动验证前提的迫切性，而且通过提出的FPBench框架和多维度评估系统，为开发可靠、以人为中心的代码生成模型提供了理论基础和实践途径。"}}
{"id": "2508.03686", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03686", "abs": "https://arxiv.org/abs/2508.03686", "authors": ["Shudong Liu", "Hongwei Liu", "Junnan Liu", "Linchen Xiao", "Songyang Gao", "Chengqi Lyu", "Yuzhe Gu", "Wenwei Zhang", "Derek F. Wong", "Songyang Zhang", "Kai Chen"], "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward", "comment": "Technical Report; 31 Pages", "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.", "AI": {"tldr": "本文提出CompassVerifier，一个轻量级、准确且鲁棒的答案验证模型，用于评估大型语言模型（LLM）并作为其优化奖励模型。同时发布VerifierBench基准测试，以系统评估验证能力。", "motivation": "现有LLM答案验证方法存在两点局限：1) 缺乏系统评估不同LLM验证能力的综合基准；2) 现有验证器开发尚处于早期，缺乏处理复杂边缘情况的鲁棒性和跨领域泛化能力，且依赖耗时的人工定制（如正则表达式或提示工程）。", "method": "开发了CompassVerifier，一个轻量级验证模型，旨在提高准确性和鲁棒性。同时构建了VerifierBench基准测试，其中包含从多个数据源收集的模型输出，并通过人工分析元错误模式进行增强，以提升CompassVerifier的性能。", "result": "CompassVerifier在数学、知识和多样推理任务等多个领域展现出强大的能力，能够处理多子问题、公式和序列答案等多种答案类型，并有效识别异常/无效响应。VerifierBench则提供了一个系统评估验证能力的平台。", "conclusion": "CompassVerifier和VerifierBench有望促进答案验证、评估协议和强化学习研究的发展。"}}
{"id": "2508.03201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03201", "abs": "https://arxiv.org/abs/2508.03201", "authors": ["Yidan Wang", "Chenyi Zhuang", "Wutao Liu", "Pan Gao", "Nicu Sebe"], "title": "AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding", "comment": null, "summary": "Weakly supervised visual grounding (VG) aims to locate objects in images\nbased on text descriptions. Despite significant progress, existing methods lack\nstrong cross-modal reasoning to distinguish subtle semantic differences in text\nexpressions due to category-based and attribute-based ambiguity. To address\nthese challenges, we introduce AlignCAT, a novel query-based semantic matching\nframework for weakly supervised VG. To enhance visual-linguistic alignment, we\npropose a coarse-grained alignment module that utilizes category information\nand global context, effectively mitigating interference from\ncategory-inconsistent objects. Subsequently, a fine-grained alignment module\nleverages descriptive information and captures word-level text features to\nachieve attribute consistency. By exploiting linguistic cues to their fullest\nextent, our proposed AlignCAT progressively filters out misaligned visual\nqueries and enhances contrastive learning efficiency. Extensive experiments on\nthree VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the\nsuperiority of AlignCAT against existing weakly supervised methods on two VG\ntasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.", "AI": {"tldr": "AlignCAT是一种新颖的弱监督视觉定位框架，通过粗粒度和细粒度对齐模块，有效解决文本描述中的类别和属性模糊性问题，提升跨模态推理能力。", "motivation": "现有弱监督视觉定位方法在区分文本表达中的细微语义差异时，缺乏强大的跨模态推理能力，原因在于存在基于类别和基于属性的歧义。", "method": "本文提出AlignCAT，一个基于查询的语义匹配框架。它包含两个模块：1) 粗粒度对齐模块，利用类别信息和全局上下文减少不一致对象的干扰；2) 细粒度对齐模块，利用描述性信息和词级文本特征实现属性一致性。该方法通过充分利用语言线索，逐步过滤未对齐的视觉查询并增强对比学习效率。", "result": "在RefCOCO、RefCOCO+和RefCOCOg三个视觉定位基准测试上，AlignCAT在两项视觉定位任务中均表现出优于现有弱监督方法的性能。", "conclusion": "AlignCAT通过其新颖的查询式语义匹配框架和渐进式粗细粒度对齐策略，有效解决了弱监督视觉定位中的类别和属性模糊性挑战，显著提升了视觉-语言对齐和跨模态推理能力。"}}
{"id": "2508.03661", "categories": ["cs.AI", "astro-ph.HE", "astro-ph.IM", "gr-qc"], "pdf": "https://arxiv.org/pdf/2508.03661", "abs": "https://arxiv.org/abs/2508.03661", "authors": ["He Wang", "Liang Zeng"], "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search", "comment": "89 pages (37 main), 6+6 figures, 1 table. Initial submission; subject\n  to revision", "summary": "Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains.", "AI": {"tldr": "该论文提出了Evo-MCTS框架，结合蒙特卡洛树搜索、进化优化和大型语言模型启发式，用于引力波信号识别。该框架在性能上超越现有方法，并能生成可解释的算法路径，为计算科学领域的自动化算法发现提供了可迁移的方法。", "motivation": "计算科学发现，特别是在引力波信号识别中，面临挑战。现有算法如匹配滤波（MF）计算成本高昂且依赖预定义模板，而深度神经网络（DNN）是黑箱模型，缺乏可解释性并引入潜在偏差。因此，需要一种既能提升性能又能提供可解释性的新方法。", "method": "论文提出了进化蒙特卡洛树搜索（Evo-MCTS）框架。该方法结合了树状搜索、进化优化和大型语言模型（LLM）启发式，通过领域感知的物理约束指导算法空间探索，旨在创建可解释的算法解决方案。", "result": "Evo-MCTS框架在MLGWSC-1基准数据集上，比现有最先进的引力波探测算法性能提升了20.2%。该框架生成了人类可解释的算法路径，揭示了独特的性能模式，并发现了新颖的算法组合。", "conclusion": "Evo-MCTS框架不仅显著提升了引力波信号识别的性能，还通过生成可解释的算法路径解决了现有方法的局限性。它提供了一种可迁移的、用于计算科学领域自动化算法发现的新方法。"}}
{"id": "2508.03351", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03351", "abs": "https://arxiv.org/abs/2508.03351", "authors": ["Yufei Xue", "Yushi Huang", "Jiawei Shao", "Jun Zhang"], "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation", "comment": "13 pages, 5 figures", "summary": "Post-training quantization (PTQ) has emerged as an effective approach for\ncompressing large models and accelerating their inference without retraining.\nWhile PTQ has been extensively studied in the context of large language models\n(LLMs), its applicability to vision-language models (VLMs) remains\nunderexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.},\nlimited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs.\nHowever, existing Hessian-based LLM PTQ methods treat all tokens equally during\nquantization, resulting in severe performance drops when applied to VLMs.\nMotivated by this observation, we propose a novel importance-aware PTQ\nframework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token\nredundancy, VLMQ 1) optimizes an importance-aware objective that yields an\nenhanced Hessian with token-level importance factors, while retaining\ncompatibility with parallelized weight updates, and 2) ensures efficiency and\neffectiveness by computing these factors via a single lightweight block-wise\nbackward pass, guided by a theoretical connection to token-level perturbations.\nExtensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the\nstate-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit\nsettings. For example, it achieves a substantial \\textbf{16.45\\%} improvement\non MME-RealWorld under 2-bit quantization.", "AI": {"tldr": "本文提出了一种名为VLMQ的新型重要性感知后训练量化（PTQ）框架，专门针对视觉-语言模型（VLMs）的模态差异（文本令牌有限，视觉令牌冗余），解决了现有LLM PTQ方法在VLM上性能下降的问题，并在多项基准测试中实现了最先进的性能。", "motivation": "后训练量化（PTQ）已被广泛应用于压缩大型语言模型（LLMs）并加速其推理，但在视觉-语言模型（VLMs）中的应用尚未得到充分探索。研究发现VLM存在模态差异（文本令牌有限vs视觉令牌冗余），而现有的基于Hessian的LLM PTQ方法在量化时平等对待所有令牌，导致在应用于VLM时性能严重下降。", "method": "本文提出了VLMQ框架。为解决视觉令牌冗余问题，VLMQ1）优化了一个重要性感知目标，生成带有令牌级重要性因子的增强Hessian，同时保持与并行权重更新的兼容性；2）通过单次轻量级块级反向传播计算这些因子，并结合了与令牌级扰动的理论联系，确保了效率和有效性。", "result": "在0.5B到32B的VLM上，通过8个基准测试的广泛评估表明，VLMQ实现了最先进的（SOTA）性能，尤其是在低比特设置下。例如，在2比特量化下，VLMQ在MME-RealWorld上实现了16.45%的显著提升。", "conclusion": "VLMQ通过引入重要性感知机制，有效解决了VLM在PTQ中因模态差异导致的性能下降问题，在低比特量化下表现出卓越的性能，证明了其在VLM压缩和加速方面的巨大潜力。"}}
{"id": "2508.03207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03207", "abs": "https://arxiv.org/abs/2508.03207", "authors": ["Ting Lei", "Shaofeng Yin", "Qingchao Chen", "Yuxin Peng", "Yang Liu"], "title": "Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration", "comment": null, "summary": "Open Vocabulary Human-Object Interaction (HOI) detection aims to detect\ninteractions between humans and objects while generalizing to novel interaction\nclasses beyond the training set. Current methods often rely on Vision and\nLanguage Models (VLMs) but face challenges due to suboptimal image encoders, as\nimage-level pre-training does not align well with the fine-grained region-level\ninteraction detection required for HOI. Additionally, effectively encoding\ntextual descriptions of visual appearances remains difficult, limiting the\nmodel's ability to capture detailed HOI relationships. To address these issues,\nwe propose INteraction-aware Prompting with Concept Calibration (INP-CC), an\nend-to-end open-vocabulary HOI detector that integrates interaction-aware\nprompts and concept calibration. Specifically, we propose an interaction-aware\nprompt generator that dynamically generates a compact set of prompts based on\nthe input scene, enabling selective sharing among similar interactions. This\napproach directs the model's attention to key interaction patterns rather than\ngeneric image-level semantics, enhancing HOI detection. Furthermore, we refine\nHOI concept representations through language model-guided calibration, which\nhelps distinguish diverse HOI concepts by investigating visual similarities\nacross categories. A negative sampling strategy is also employed to improve\ninter-modal similarity modeling, enabling the model to better differentiate\nvisually similar but semantically distinct actions. Extensive experimental\nresults demonstrate that INP-CC significantly outperforms state-of-the-art\nmodels on the SWIG-HOI and HICO-DET datasets. Code is available at\nhttps://github.com/ltttpku/INP-CC.", "AI": {"tldr": "本文提出INP-CC，一个端到端的开放词汇人-物交互（HOI）检测器，通过交互感知提示和概念校准来解决现有方法在细粒度区域级检测和文本描述编码上的不足，并在多个数据集上显著优于现有技术。", "motivation": "当前开放词汇HOI检测方法依赖视觉和语言模型（VLMs），但存在两个主要挑战：1) 图像编码器次优，因为图像级预训练与HOI所需的细粒度区域级检测不匹配；2) 有效编码视觉外观的文本描述仍很困难，限制了模型捕捉详细HOI关系的能力。", "method": "本文提出INteraction-aware Prompting with Concept Calibration (INP-CC)。具体方法包括：1) 一个交互感知提示生成器，根据输入场景动态生成紧凑提示集，实现相似交互间的选择性共享，从而引导模型关注关键交互模式。2) 通过语言模型引导的概念校准来提炼HOI概念表示，通过调查跨类别的视觉相似性来区分不同的HOI概念。3) 采用负采样策略来改善跨模态相似性建模，使模型能更好地区分视觉相似但语义不同的动作。", "result": "INP-CC在SWIG-HOI和HICO-DET数据集上显著优于最先进的模型。", "conclusion": "INP-CC通过引入交互感知提示和概念校准，有效解决了开放词汇HOI检测中图像编码器次优和文本描述编码困难的问题，显著提升了模型在泛化到新交互类别上的性能。"}}
{"id": "2508.03680", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03680", "abs": "https://arxiv.org/abs/2508.03680", "authors": ["Xufang Luo", "Yuge Zhang", "Zhiyuan He", "Zilong Wang", "Siyun Zhao", "Dongsheng Li", "Luna K. Qiu", "Yuqing Yang"], "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning", "comment": null, "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.", "AI": {"tldr": "Agent Lightning是一个灵活可扩展的框架，它将强化学习（RL）训练与大型语言模型（LLM）驱动的AI智能体执行完全解耦，实现对任意智能体的RL训练，并展示了稳定的性能提升。", "motivation": "现有方法将RL训练与智能体紧密耦合，或依赖于带掩码的序列拼接，导致难以与现有智能体集成。因此，需要一个能完全解耦智能体执行和RL训练的通用框架，以支持对任意AI智能体进行RL微调。", "method": "Agent Lightning将智能体执行建模为马尔可夫决策过程（MDP），并定义了统一的数据接口。它提出了一个分层RL算法LightningRL，包含信用分配模块，能将任意智能体生成的轨迹分解为训练转换。系统设计上，引入了“训练-智能体解耦”架构，并将智能体可观测性框架引入运行时，提供标准化的智能体微调接口。", "result": "在text-to-SQL、检索增强生成（RAG）和数学工具使用等任务上，Agent Lightning展示了稳定、持续的性能改进。", "conclusion": "Agent Lightning框架通过其解耦设计和分层RL算法，证明了在真实世界智能体训练和部署方面的巨大潜力，能够无缝集成并有效训练各种现有AI智能体。"}}
{"id": "2508.03481", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03481", "abs": "https://arxiv.org/abs/2508.03481", "authors": ["Hyungjin Kim", "Seokho Ahn", "Young-Duk Seo"], "title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models", "comment": "Accepted at ICCV 2025", "summary": "Personalized generation in T2I diffusion models aims to naturally incorporate\nindividual user preferences into the generation process with minimal user\nintervention. However, existing studies primarily rely on prompt-level modeling\nwith large-scale models, often leading to inaccurate personalization due to the\nlimited input token capacity of T2I diffusion models. To address these\nlimitations, we propose DrUM, a novel method that integrates user profiling\nwith a transformer-based adapter to enable personalized generation through\ncondition-level modeling in the latent space. DrUM demonstrates strong\nperformance on large-scale datasets and seamlessly integrates with open-source\ntext encoders, making it compatible with widely used foundation T2I models\nwithout requiring additional fine-tuning.", "AI": {"tldr": "DrUM是一种新型方法，通过用户画像和基于Transformer的适配器，在潜在空间进行条件级建模，实现个性化T2I生成，解决现有方法精度不足和令牌容量限制问题。", "motivation": "现有T2I扩散模型的个性化生成主要依赖于提示级建模，受限于模型输入令牌容量，导致个性化不准确。", "method": "提出DrUM方法，将用户画像与基于Transformer的适配器相结合，在潜在空间实现条件级建模，从而进行个性化生成。", "result": "DrUM在大规模数据集上表现出色，可无缝集成到开源文本编码器中，兼容主流T2I基础模型，无需额外微调。", "conclusion": "DrUM通过在潜在空间进行条件级建模，有效解决了T2I扩散模型个性化生成中现有方法的局限性，实现了更准确且兼容性强的个性化生成。"}}
{"id": "2508.03209", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03209", "abs": "https://arxiv.org/abs/2508.03209", "authors": ["Xinwei Liu", "Xiaojun Jia", "Yuan Xun", "Simeng Qin", "Xiaochun Cao"], "title": "GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations", "comment": null, "summary": "Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable\nability to infer users' locations from public shared images, posing a\nsubstantial risk to geoprivacy. Although adversarial perturbations offer a\npotential defense, current methods are ill-suited for this scenario: they often\nperform poorly on high-resolution images and low perturbation budgets, and may\nintroduce irrelevant semantic content. To address these limitations, we propose\nGeoShield, a novel adversarial framework designed for robust geoprivacy\nprotection in real-world scenarios. GeoShield comprises three key modules: a\nfeature disentanglement module that separates geographical and non-geographical\ninformation, an exposure element identification module that pinpoints\ngeo-revealing regions within an image, and a scale-adaptive enhancement module\nthat jointly optimizes perturbations at both global and local levels to ensure\neffectiveness across resolutions. Extensive experiments on challenging\nbenchmarks show that GeoShield consistently surpasses prior methods in\nblack-box settings, achieving strong privacy protection with minimal impact on\nvisual or semantic quality. To our knowledge, this work is the first to explore\nadversarial perturbations for defending against geolocation inference by\nadvanced VLMs, providing a practical and effective solution to escalating\nprivacy concerns.", "AI": {"tldr": "针对视觉-语言模型（VLMs）通过图像推断地理位置造成的隐私风险，本文提出了GeoShield，一个新颖的对抗性扰动框架，旨在提供鲁棒的地理隐私保护，且对图像质量影响最小。", "motivation": "GPT-4o等VLMs能够从公开共享图像中推断用户位置，对地理隐私构成重大风险。现有对抗性防御方法在处理高分辨率图像和低扰动预算时表现不佳，且可能引入不相关的语义内容，因此需要更有效的解决方案。", "method": "GeoShield框架包含三个关键模块：1. 特征解耦模块：分离地理和非地理信息；2. 暴露元素识别模块：识别图像中透露地理信息的区域；3. 尺度自适应增强模块：在全局和局部层面共同优化扰动，以确保跨分辨率的有效性。", "result": "在具有挑战性的基准测试中，GeoShield在黑盒设置下持续超越现有方法，实现了强大的隐私保护，同时对视觉或语义质量的影响最小。", "conclusion": "GeoShield是首次探索使用对抗性扰动来防御先进VLMs地理定位推断的工作，为日益增长的隐私问题提供了一个实用且有效的解决方案。"}}
{"id": "2506.16119", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16119", "abs": "https://arxiv.org/abs/2506.16119", "authors": ["Chengyu Bai", "Yuming Li", "Zhongyu Zhao", "Jintao Chen", "Peidong Jia", "Qi She", "Ming Lu", "Shanghang Zhang"], "title": "FastInit: Fast Noise Initialization for Temporally Consistent Video Generation", "comment": null, "summary": "Video generation has made significant strides with the development of\ndiffusion models; however, achieving high temporal consistency remains a\nchallenging task. Recently, FreeInit identified a training-inference gap and\nintroduced a method to iteratively refine the initial noise during inference.\nHowever, iterative refinement significantly increases the computational cost\nassociated with video generation. In this paper, we introduce FastInit, a fast\nnoise initialization method that eliminates the need for iterative refinement.\nFastInit learns a Video Noise Prediction Network (VNPNet) that takes random\nnoise and a text prompt as input, generating refined noise in a single forward\npass. Therefore, FastInit greatly enhances the efficiency of video generation\nwhile achieving high temporal consistency across frames. To train the VNPNet,\nwe create a large-scale dataset consisting of pairs of text prompts, random\nnoise, and refined noise. Extensive experiments with various text-to-video\nmodels show that our method consistently improves the quality and temporal\nconsistency of the generated videos. FastInit not only provides a substantial\nimprovement in video generation but also offers a practical solution that can\nbe applied directly during inference. The code and dataset will be released.", "AI": {"tldr": "FastInit提出了一种快速噪声初始化方法，通过学习一个视频噪声预测网络（VNPNet），在单次前向传递中生成高质量且时间一致的视频，解决了现有迭代精炼方法的计算开销问题。", "motivation": "视频生成中，扩散模型虽然取得了进展，但时间一致性仍是挑战。FreeInit通过迭代精炼初始噪声解决了训练-推理差距，但计算成本高昂，因此需要一种更高效的方法。", "method": "FastInit引入了一个视频噪声预测网络（VNPNet），该网络以随机噪声和文本提示为输入，通过单次前向传递生成精炼的噪声。为了训练VNPNet，研究人员构建了一个包含文本提示、随机噪声和精炼噪声对的大规模数据集。", "result": "在多种文本到视频模型上的广泛实验表明，FastInit方法能持续提高生成视频的质量和时间一致性。它显著提升了视频生成效率，同时保持了高时间一致性。", "conclusion": "FastInit不仅显著改进了视频生成效果，还提供了一个实用的解决方案，可以直接应用于推理阶段，无需迭代精炼，大幅提升了效率和实用性。"}}
{"id": "2508.03562", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03562", "abs": "https://arxiv.org/abs/2508.03562", "authors": ["Muzhaffar Hazman", "Susan McKeever", "Josephine Griffith"], "title": "Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching", "comment": "Accepted for publication at IEEE International Conference on Image\n  Processing Theory, Tools and Applications (IPTA) 2025", "summary": "Internet memes, now a staple of digital communication, play a pivotal role in\nhow users engage within online communities and allow researchers to gain\ninsight into contemporary digital culture. These engaging user-generated\ncontent are characterised by their reuse of visual elements also found in other\nmemes. Matching instances of memes via these shared visual elements, called\nMeme Matching, is the basis of a wealth of meme analysis approaches. However,\nmost existing methods assume that every meme consists of a shared visual\nbackground, called a Template, with some overlaid text, thereby limiting meme\nmatching to comparing the background image alone. Current approaches exclude\nthe many memes that are not template-based and limit the effectiveness of\nautomated meme analysis and would not be effective at linking memes to\ncontemporary web-based meme dictionaries. In this work, we introduce a broader\nformulation of meme matching that extends beyond template matching. We show\nthat conventional similarity measures, including a novel segment-wise\ncomputation of the similarity measures, excel at matching template-based memes\nbut fall short when applied to non-template-based meme formats. However, the\nsegment-wise approach was found to consistently outperform the whole-image\nmeasures on matching non-template-based memes. Finally, we explore a\nprompting-based approach using a pretrained Multimodal Large Language Model for\nmeme matching. Our results highlight that accurately matching memes via shared\nvisual elements, not just background templates, remains an open challenge that\nrequires more sophisticated matching techniques.", "AI": {"tldr": "本文提出了一种超越模板匹配的模因匹配新方法，旨在解决现有方法仅限于基于模板的模因的局限性。研究测试了传统相似性度量（包括分段计算）和基于多模态大语言模型的提示方法，发现准确匹配非模板模因仍是一个开放挑战。", "motivation": "现有的模因匹配方法大多假设模因由共享视觉背景（模板）和叠加文本组成，从而将匹配限制为仅比较背景图像。这排除了许多非模板模因，限制了自动化模因分析的有效性，并且无法有效地将模因与当代网络模因词典关联起来。", "method": "研究引入了一种更广泛的模因匹配公式，超越了模板匹配。测试了传统的相似性度量（包括新颖的分段计算方法），并探索了使用预训练多模态大语言模型（MLLM）的提示式方法进行模因匹配。", "result": "传统相似性度量（包括分段计算）在匹配基于模板的模因方面表现出色，但在应用于非模板模因时效果不佳。然而，分段方法在匹配非模板模因方面始终优于整图度量。结果表明，通过共享视觉元素（不仅仅是背景模板）准确匹配模因仍然是一个开放挑战，需要更复杂的匹配技术。", "conclusion": "要实现通过共享视觉元素（而不仅仅是背景模板）的准确模因匹配，需要开发更复杂的匹配技术。现有方法在处理非模板模因方面存在局限性，这是一个仍待解决的重要问题。"}}
{"id": "2508.03213", "categories": ["cs.CV", "cs.AI", "C.1.2"], "pdf": "https://arxiv.org/pdf/2508.03213", "abs": "https://arxiv.org/abs/2508.03213", "authors": ["Wang Yu-Hang", "Shiwei Li", "Jianxiang Liao", "Li Bohan", "Jian Liu", "Wenfei Yin"], "title": "The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness", "comment": "13 pages,2 figures,6 tables", "summary": "Adversarial perturbations pose a significant threat to deep learning models.\nAdversarial Training (AT), the predominant defense method, faces challenges of\nhigh computational costs and a degradation in standard performance. While data\naugmentation offers an alternative path, existing techniques either yield\nlimited robustness gains or incur substantial training overhead. Therefore,\ndeveloping a defense mechanism that is both highly efficient and strongly\nrobust is of paramount importance.In this work, we first conduct a systematic\nanalysis of existing augmentation techniques, revealing that the synergy among\ndiverse strategies -- rather than any single method -- is crucial for enhancing\nrobustness. Based on this insight, we propose the Universal Adversarial\nAugmenter (UAA) framework, which is characterized by its plug-and-play nature\nand training efficiency. UAA decouples the expensive perturbation generation\nprocess from model training by pre-computing a universal transformation\noffline, which is then used to efficiently generate unique adversarial\nperturbations for each sample during training.Extensive experiments conducted\non multiple benchmarks validate the effectiveness of UAA. The results\ndemonstrate that UAA establishes a new state-of-the-art (SOTA) for\ndata-augmentation-based adversarial defense strategies , without requiring the\nonline generation of adversarial examples during training. This framework\nprovides a practical and efficient pathway for building robust models,Our code\nis available in the supplementary materials.", "AI": {"tldr": "本文提出了一种名为通用对抗增强器（UAA）的新框架，通过离线预计算通用变换来高效生成对抗扰动，实现了数据增强型对抗防御的最新技术水平，解决了对抗训练成本高和现有数据增强效果有限的问题。", "motivation": "深度学习模型面临对抗扰动的严重威胁。现有的主要防御方法对抗训练（AT）存在计算成本高和标准性能下降的问题。而现有的数据增强技术要么鲁棒性增益有限，要么训练开销巨大。因此，开发一种既高效又鲁棒的防御机制至关重要。", "method": "首先，系统分析了现有数据增强技术，发现多种策略的协同作用对于增强鲁棒性至关重要。在此基础上，提出了通用对抗增强器（UAA）框架，其特点是即插即用和训练高效。UAA通过离线预计算一个通用变换，将昂贵的扰动生成过程与模型训练解耦，然后在训练期间高效地为每个样本生成独特的对抗扰动。", "result": "在多个基准测试上的大量实验验证了UAA的有效性。结果表明，UAA在基于数据增强的对抗防御策略中建立了新的最先进水平（SOTA），并且无需在训练期间在线生成对抗样本。", "conclusion": "UAA框架为构建鲁棒模型提供了一条实用且高效的途径。"}}
{"id": "2508.01263", "categories": ["cs.CL", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "AI": {"tldr": "本文分析了XAI挑战赛2025，一个旨在教育领域推动可解释人工智能（XAI）的黑客马拉松，参赛者需构建基于轻量级LLM或混合系统的问答系统，为大学政策查询提供逻辑清晰的解释。", "motivation": "随着AI在教育中日益普及，对透明度和可解释性的需求增加。然而，很少有黑客马拉松直接在真实的教育场景中解决可解释AI（XAI）问题。", "method": "本文描述了XAI挑战赛2025，一个要求参赛者构建问答系统以回答学生关于大学政策的查询，并生成清晰、基于逻辑的自然语言解释的竞赛。系统需使用轻量级大型语言模型（LLMs）或混合LLM-符号系统。竞赛提供了一个通过逻辑模板构建、Z3验证并经专家学生审查的高质量数据集。", "result": "该挑战赛代表了一项新颖的努力，旨在将大型语言模型与符号推理相结合，以服务于可解释性。本文描述了挑战赛的动机、结构、数据集构建和评估协议。", "conclusion": "研究结果为未来以XAI为中心的教育系统和竞争性研究计划提供了可操作的见解。"}}
{"id": "2508.03218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03218", "abs": "https://arxiv.org/abs/2508.03218", "authors": ["Shanshan Guo", "Xiwen Liang", "Junfan Lin", "Yuzheng Zhuang", "Liang Lin", "Xiaodan Liang"], "title": "ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow", "comment": null, "summary": "Language-instructed robot manipulation has garnered significant interest due\nto the potential of learning from collected data. While the challenges in\nhigh-level perception and planning are continually addressed along the progress\nof general large pre-trained models, the low precision of low-level action\nestimation has emerged as the key limiting factor in manipulation performance.\nTo this end, this paper introduces a novel robot manipulation framework, i.e.,\nActionSink, to pave the way toward precise action estimations in the field of\nlearning-based robot manipulation. As the name suggests, ActionSink\nreformulates the actions of robots as action-caused optical flows from videos,\ncalled \"action flow\", in a self-supervised manner, which are then used to be\nretrieved and integrated to enhance the action estimation. Specifically,\nActionSink incorporates two primary modules. The first module is a\ncoarse-to-fine action flow matcher, which continuously refines the accuracy of\naction flow via iterative retrieval and denoising process. The second module is\na dynamic action flow integrator, which employs a working memory pool that\ndynamically and efficiently manages the historical action flows that should be\nused to integrate to enhance the current action estimation. In this module, a\nmulti-layer fusion module is proposed to integrate direct estimation and action\nflows from both the current and the working memory, achieving highly accurate\naction estimation through a series of estimation-integration processes. Our\nActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\\%\nsuccess rate, and obtained nearly an 8\\% accuracy gain on the challenging\nlong-horizon visual task LIBERO-Long.", "AI": {"tldr": "ActionSink是一种新型机器人操作框架，通过将机器人动作重构为“动作流”（由动作引起的光流），并结合粗到精的匹配器和动态集成器，显著提高了低级动作估计的精度，从而提升了机器人操作性能。", "motivation": "尽管大型预训练模型在高级感知和规划方面取得了进展，但低级动作估计的精度不足已成为机器人操作性能的关键限制因素。", "method": "ActionSink框架将机器人动作自监督地重构为视频中的“动作流”（action flow），并利用其进行动作估计。它包含两个主要模块：1) 粗到精的动作流匹配器，通过迭代检索和去噪连续优化动作流精度；2) 动态动作流集成器，利用工作记忆池动态管理历史动作流，并通过多层融合模块整合直接估计和来自当前及记忆的动作流，实现高精度动作估计。", "result": "ActionSink框架在LIBERO基准测试上将成功率提高了7.9%，在更具挑战性的LIBERO-Long长程视觉任务上获得了近8%的精度提升，超越了先前的SOTA方法。", "conclusion": "ActionSink通过引入“动作流”概念和创新的匹配与集成机制，成功解决了低级动作估计精度不足的问题，为基于学习的机器人操作提供了精确的动作估计方法，并显著提升了机器人操作任务的性能。"}}
{"id": "2508.03227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03227", "abs": "https://arxiv.org/abs/2508.03227", "authors": ["Hongyu Shen", "Junfeng Ni", "Yixin Chen", "Weishuo Li", "Mingtao Pei", "Siyuan Huang"], "title": "Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing", "comment": null, "summary": "We address the challenge of lifting 2D visual segmentation to 3D in Gaussian\nSplatting. Existing methods often suffer from inconsistent 2D masks across\nviewpoints and produce noisy segmentation boundaries as they neglect these\nsemantic cues to refine the learned Gaussians. To overcome this, we introduce\nGaussian Instance Tracing (GIT), which augments the standard Gaussian\nrepresentation with an instance weight matrix across input views. Leveraging\nthe inherent consistency of Gaussians in 3D, we use this matrix to identify and\ncorrect 2D segmentation inconsistencies. Furthermore, since each Gaussian\nideally corresponds to a single object, we propose a GIT-guided adaptive\ndensity control mechanism to split and prune ambiguous Gaussians during\ntraining, resulting in sharper and more coherent 2D and 3D segmentation\nboundaries. Experimental results show that our method extracts clean 3D assets\nand consistently improves 3D segmentation in both online (e.g., self-prompting)\nand offline (e.g., contrastive lifting) settings, enabling applications such as\nhierarchical segmentation, object extraction, and scene editing.", "AI": {"tldr": "该论文提出了一种名为高斯实例追踪（GIT）的方法，用于解决高斯泼溅中2D到3D语义分割的挑战，通过引入实例权重矩阵和自适应密度控制来纠正视图不一致性和改善分割边界。", "motivation": "现有方法在将2D视觉分割提升到3D高斯泼溅时，存在跨视角的2D掩码不一致以及产生的分割边界噪声问题，因为它们忽略了语义线索来优化学习到的高斯。", "method": "该方法通过以下方式解决问题：1) 增强标准高斯表示，增加一个跨输入视图的实例权重矩阵（GIT）。2) 利用高斯固有的3D一致性，使用该矩阵识别和纠正2D分割不一致性。3) 提出一种GIT引导的自适应密度控制机制，在训练期间分割和修剪模糊的高斯，以获得更清晰、更连贯的2D和3D分割边界。", "result": "实验结果表明，该方法能够提取干净的3D资产，并在在线（如自提示）和离线（如对比提升）设置中持续改进3D分割，从而实现分层分割、对象提取和场景编辑等应用。", "conclusion": "GIT方法有效解决了高斯泼溅中2D到3D分割的挑战，通过引入实例权重矩阵和自适应密度控制，显著提高了3D分割的一致性和边界质量，并拓展了相关应用。"}}
{"id": "2508.03235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03235", "abs": "https://arxiv.org/abs/2508.03235", "authors": ["Freida Barnatan", "Emunah Goldstein", "Einav Kalimian", "Orchen Madar", "Avi Huri", "David Zitoun", "Ya'akov Mandelbaum", "Moshe Amitay"], "title": "Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models", "comment": null, "summary": "Accurate and efficient characterization of nanoparticle morphology in\nScanning Electron Microscopy (SEM) images is critical for ensuring product\nquality in nanomaterial synthesis and accelerating development. However,\nconventional deep learning methods for shape classification require extensive\nlabeled datasets and computationally demanding training, limiting their\naccessibility to the typical nanoparticle practitioner in research and\nindustrial settings. In this study, we introduce a zero-shot classification\npipeline that leverages two vision foundation models: the Segment Anything\nModel (SAM) for object segmentation and DINOv2 for feature embedding. By\ncombining these models with a lightweight classifier, we achieve high-precision\nshape classification across three morphologically diverse nanoparticle datasets\n- without the need for extensive parameter fine-tuning. Our methodology\noutperforms a fine-tuned YOLOv11 and ChatGPT o4-mini-high baselines,\ndemonstrating robustness to small datasets, subtle morphological variations,\nand domain shifts from natural to scientific imaging. Quantitative clustering\nmetrics on PCA plots of the DINOv2 features are discussed as a means of\nassessing the progress of the chemical synthesis. This work highlights the\npotential of foundation models to advance automated microscopy image analysis,\noffering an alternative to traditional deep learning pipelines in nanoparticle\nresearch which is both more efficient and more accessible to the user.", "AI": {"tldr": "本研究提出了一种零样本分类流程，利用SAM进行图像分割和DINOv2进行特征嵌入，实现了纳米颗粒形态的高精度分类，无需大量标记数据和计算资源，为纳米材料研究提供了高效且易用的自动化显微图像分析工具。", "motivation": "传统的深度学习方法在纳米颗粒形态分类中需要大量标注数据集和计算密集型训练，这限制了其在纳米颗粒研究和工业实践中的可及性。因此，需要一种更高效、更易于使用的替代方案。", "method": "该研究引入了一个零样本分类流程，结合了两个视觉基础模型：使用Segment Anything Model (SAM) 进行目标分割，以及DINOv2进行特征嵌入。随后，将这些特征与一个轻量级分类器结合，实现了纳米颗粒的形态分类。", "result": "该方法在三个形态多样化的纳米颗粒数据集上实现了高精度形状分类，且无需大量参数微调。它优于经过微调的YOLOv11和ChatGPT o4-mini-high基线模型，并表现出对小数据集、细微形态变化以及从自然图像到科学图像的领域转移的鲁棒性。此外，还讨论了DINOv2特征在PCA图上的聚类指标，可用于评估化学合成的进展。", "conclusion": "这项工作突出了基础模型在推进自动化显微镜图像分析方面的潜力，为纳米颗粒研究中传统的深度学习流程提供了一种更高效且更易于用户使用的替代方案。"}}
{"id": "2508.03241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03241", "abs": "https://arxiv.org/abs/2508.03241", "authors": ["Xingchao Yang", "Shiori Ueda", "Yuantian Huang", "Tomoya Akiyama", "Takafumi Taketomi"], "title": "FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles", "comment": "Project: https://yangxingchao.github.io/FFHQ-Makeup-page, Datasets:\n  https://huggingface.co/datasets/cyberagent/FFHQ-Makeup", "summary": "Paired bare-makeup facial images are essential for a wide range of\nbeauty-related tasks, such as virtual try-on, facial privacy protection, and\nfacial aesthetics analysis. However, collecting high-quality paired makeup\ndatasets remains a significant challenge. Real-world data acquisition is\nconstrained by the difficulty of collecting large-scale paired images, while\nexisting synthetic approaches often suffer from limited realism or\ninconsistencies between bare and makeup images. Current synthetic methods\ntypically fall into two categories: warping-based transformations, which often\ndistort facial geometry and compromise the precision of makeup; and\ntext-to-image generation, which tends to alter facial identity and expression,\nundermining consistency. In this work, we present FFHQ-Makeup, a high-quality\nsynthetic makeup dataset that pairs each identity with multiple makeup styles\nwhile preserving facial consistency in both identity and expression. Built upon\nthe diverse FFHQ dataset, our pipeline transfers real-world makeup styles from\nexisting datasets onto 18K identities by introducing an improved makeup\ntransfer method that disentangles identity and makeup. Each identity is paired\nwith 5 different makeup styles, resulting in a total of 90K high-quality\nbare-makeup image pairs. To the best of our knowledge, this is the first work\nthat focuses specifically on constructing a makeup dataset. We hope that\nFFHQ-Makeup fills the gap of lacking high-quality bare-makeup paired datasets\nand serves as a valuable resource for future research in beauty-related tasks.", "AI": {"tldr": "本文提出了FFHQ-Makeup，一个高质量的合成素颜-妆容配对数据集，通过解耦身份和妆容，解决了现有数据集在真实感和一致性方面的不足。", "motivation": "素颜-妆容配对图像对于虚拟试妆、面部隐私保护和面部美学分析等美容相关任务至关重要。然而，收集大规模高质量的真实配对图像非常困难，而现有合成方法常存在真实感不足或素颜与妆容图像之间不一致（如面部几何扭曲、身份和表情改变）的问题。", "method": "基于多样化的FFHQ数据集，本文引入了一种改进的妆容迁移方法，该方法能够解耦身份和妆容，将现有数据集中的真实妆容风格迁移到1.8万个身份上。每个身份与5种不同的妆容风格配对。", "result": "创建了FFHQ-Makeup数据集，包含总计9万对高质量的素颜-妆容图像对（1.8万个身份，每个身份5种妆容风格），同时保留了面部身份和表情的一致性。这是第一个专门专注于构建妆容数据集的工作。", "conclusion": "FFHQ-Makeup数据集填补了高质量素颜-妆容配对数据集的空白，有望成为未来美容相关研究的宝贵资源。"}}
{"id": "2508.03254", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03254", "abs": "https://arxiv.org/abs/2508.03254", "authors": ["Jisoo Kim", "Wooseok Seo", "Junwan Kim", "Seungho Park", "Sooyeon Park", "Youngjae Yu"], "title": "V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models", "comment": "ICCV2025 accepted", "summary": "With growing interest in deploying text-to-video (T2V) models in\nresource-constrained environments, reducing their high computational cost has\nbecome crucial, leading to extensive research on pruning and knowledge\ndistillation methods while maintaining performance. However, existing\ndistillation methods primarily rely on supervised fine-tuning (SFT), which\noften leads to mode collapse as pruned models with reduced capacity fail to\ndirectly match the teacher's outputs, ultimately resulting in degraded quality.\nTo address this challenge, we propose an effective distillation method, ReDPO,\nthat integrates DPO and SFT. Our approach leverages DPO to guide the student\nmodel to focus on recovering only the targeted properties, rather than\npassively imitating the teacher, while also utilizing SFT to enhance overall\nperformance. We additionally propose V.I.P., a novel framework for filtering\nand curating high-quality pair datasets, along with a step-by-step online\napproach for calibrated training. We validate our method on two leading T2V\nmodels, VideoCrafter2 and AnimateDiff, achieving parameter reduction of 36.2%\nand 67.5% each, while maintaining or even surpassing the performance of full\nmodels. Further experiments demonstrate the effectiveness of both ReDPO and\nV.I.P. framework in enabling efficient and high-quality video generation. Our\ncode and videos are available at https://jiiiisoo.github.io/VIP.github.io/.", "AI": {"tldr": "该研究提出ReDPO蒸馏方法和V.I.P.数据策展框架，旨在降低文生视频（T2V）模型的计算成本，同时避免模式崩溃并保持甚至超越原始模型的性能。", "motivation": "文生视频（T2V）模型计算成本高昂，难以在资源受限环境中部署。现有蒸馏方法（如SFT）常导致模式崩溃，因为剪枝后的模型难以直接匹配教师模型输出，从而导致质量下降。", "method": "提出ReDPO蒸馏方法，结合DPO（直接偏好优化）和SFT（监督微调），使学生模型专注于恢复目标属性而非被动模仿教师模型，同时提升整体性能。此外，引入V.I.P.框架用于筛选和整理高质量配对数据集，并提出分步在线校准训练方法。", "result": "在VideoCrafter2和AnimateDiff两种主流T2V模型上验证了方法有效性，分别实现了36.2%和67.5%的参数缩减，同时性能保持或超越了完整模型。实验证明ReDPO和V.I.P.框架均能有效实现高效高质量的视频生成。", "conclusion": "ReDPO和V.I.P.框架的结合，为在资源受限环境中部署高效且高质量的文生视频模型提供了有效的解决方案，成功解决了现有蒸馏方法导致的模式崩溃和性能下降问题。"}}
{"id": "2508.03243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03243", "abs": "https://arxiv.org/abs/2508.03243", "authors": ["Lukas Ranftl", "Felix Brendel", "Bertram Drost", "Carsten Steger"], "title": "MVTOP: Multi-View Transformer-based Object Pose-Estimation", "comment": "9 pages, 7 figures", "summary": "We present MVTOP, a novel transformer-based method for multi-view rigid\nobject pose estimation. Through an early fusion of the view-specific features,\nour method can resolve pose ambiguities that would be impossible to solve with\na single view or with a post-processing of single-view poses. MVTOP models the\nmulti-view geometry via lines of sight that emanate from the respective camera\ncenters. While the method assumes the camera interior and relative orientations\nare known for a particular scene, they can vary for each inference. This makes\nthe method versatile. The use of the lines of sight enables MVTOP to correctly\npredict the correct pose with the merged multi-view information. To show the\nmodel's capabilities, we provide a synthetic data set that can only be solved\nwith such holistic multi-view approaches since the poses in the dataset cannot\nbe solved with just one view. Our method outperforms single-view and all\nexisting multi-view approaches on our dataset and achieves competitive results\non the YCB-V dataset. To the best of our knowledge, no holistic multi-view\nmethod exists that can resolve such pose ambiguities reliably. Our model is\nend-to-end trainable and does not require any additional data, e.g., depth.", "AI": {"tldr": "MVTOP是一种基于Transformer的多视角刚体姿态估计方法，通过早期特征融合和视线建模解决单视角或后处理无法解决的姿态模糊问题，并在挑战性数据集上表现优异。", "motivation": "现有的单视角或基于单视角后处理的方法无法解决某些姿态模糊问题，需要一种能够可靠解决此类模糊的整体多视角方法。", "method": "MVTOP是一种基于Transformer的方法，通过早期融合视点特定特征来解决姿态模糊。它通过从相机中心发出的视线来建模多视角几何，并假设相机内参和相对方向已知（但每次推理可变）。该模型是端到端可训练的，不需要额外数据（如深度）。作者还提供了一个新的合成数据集，其中姿态无法通过单视角解决，以展示模型能力。", "result": "MVTOP能够解决单视角或单视角姿态后处理无法解决的姿态模糊。在作者提供的合成数据集上，该方法优于单视角和所有现有多视角方法，并在YCB-V数据集上取得了有竞争力的结果。", "conclusion": "MVTOP是一种新颖、通用且可靠的整体多视角姿态估计方法，能有效解决复杂的姿态模糊问题，并超越了现有技术，为多视角姿态估计领域树立了新标准。"}}
{"id": "2508.03313", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03313", "abs": "https://arxiv.org/abs/2508.03313", "authors": ["Libo Zhang", "Xinyu Yi", "Feng Xu"], "title": "BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices", "comment": "9 pages, 10 figures", "summary": "In recent years, tracking human motion using IMUs from everyday devices such\nas smartphones and smartwatches has gained increasing popularity. However, due\nto the sparsity of sensor measurements and the lack of datasets capturing human\nmotion over uneven terrain, existing methods often struggle with pose\nestimation accuracy and are typically limited to recovering movements on flat\nterrain only. To this end, we present BaroPoser, the first method that combines\nIMU and barometric data recorded by a smartphone and a smartwatch to estimate\nhuman pose and global translation in real time. By leveraging barometric\nreadings, we estimate sensor height changes, which provide valuable cues for\nboth improving the accuracy of human pose estimation and predicting global\ntranslation on non-flat terrain. Furthermore, we propose a local thigh\ncoordinate frame to disentangle local and global motion input for better pose\nrepresentation learning. We evaluate our method on both public benchmark\ndatasets and real-world recordings. Quantitative and qualitative results\ndemonstrate that our approach outperforms the state-of-the-art (SOTA) methods\nthat use IMUs only with the same hardware configuration.", "AI": {"tldr": "BaroPoser是首个结合智能手机和智能手表IMU与气压数据，实时估计人体姿态和非平坦地形全局位移的方法，显著优于现有仅使用IMU的方法。", "motivation": "现有使用IMU的运动追踪方法存在传感器测量稀疏、缺乏非平坦地形数据集的问题，导致姿态估计精度不足且通常仅限于平坦地形。因此，需要一种能有效处理非平坦地形运动追踪的方法。", "method": "BaroPoser结合了智能手机和智能手表记录的IMU和气压数据。通过气压数据估计传感器高度变化，为姿态估计和非平坦地形全局位移预测提供线索。此外，引入局部大腿坐标系以解耦局部和全局运动输入，优化姿态表示学习。", "result": "在公共基准数据集和真实世界记录上的定量和定性评估表明，BaroPoser在相同硬件配置下，性能优于现有仅使用IMU的最先进方法。", "conclusion": "BaroPoser是第一个利用IMU和气压数据实现实时人体姿态和非平坦地形全局位移估计的方法，通过整合气压信息和创新的局部坐标系，显著提升了运动追踪的准确性和适用性。"}}
{"id": "2508.03244", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03244", "abs": "https://arxiv.org/abs/2508.03244", "authors": ["Chuanzhi Xu", "Haoxian Zhou", "Langyi Chen", "Yuk Ying Chung", "Qiang Qu"], "title": "Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution", "comment": null, "summary": "Event cameras offer unparalleled advantages such as high temporal resolution,\nlow latency, and high dynamic range. However, their limited spatial resolution\nposes challenges for fine-grained perception tasks. In this work, we propose an\nultra-lightweight, stream-based event-to-event super-resolution method based on\nSpiking Neural Networks (SNNs), designed for real-time deployment on\nresource-constrained devices. To further reduce model size, we introduce a\nnovel Dual-Forward Polarity-Split Event Encoding strategy that decouples\npositive and negative events into separate forward paths through a shared SNN.\nFurthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss\n(LearnSTPLoss) that adaptively balances temporal, spatial, and polarity\nconsistency using learnable uncertainty-based weights. Experimental results\ndemonstrate that our method achieves competitive super-resolution performance\non multiple datasets while significantly reducing model size and inference\ntime. The lightweight design enables embedding the module into event cameras or\nusing it as an efficient front-end preprocessing for downstream vision tasks.", "AI": {"tldr": "本文提出一种基于脉冲神经网络（SNNs）的超轻量级、流式事件到事件超分辨率方法，专为资源受限设备实时部署设计。", "motivation": "事件相机具有高时间分辨率、低延迟和高动态范围的优势，但其有限的空间分辨率限制了精细感知任务的应用。", "method": "该方法采用超轻量级、流式事件到事件超分辨率，基于脉冲神经网络（SNNs）。引入了新颖的双向极性分离事件编码策略，将正负事件分离到共享SNN的不同前向路径中以减小模型尺寸。此外，提出了一种可学习的时空极性感知损失（LearnSTPLoss），通过可学习的不确定性权重自适应平衡时间、空间和极性一致性。", "result": "实验结果表明，该方法在多个数据集上实现了有竞争力的超分辨率性能，同时显著减小了模型尺寸和推理时间。", "conclusion": "该轻量化设计使得模块能够嵌入到事件相机中，或作为下游视觉任务的高效前端预处理模块使用。"}}
{"id": "2508.03402", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03402", "abs": "https://arxiv.org/abs/2508.03402", "authors": ["Pingchuan Ma", "Xiaopei Yang", "Yusong Li", "Ming Gui", "Felix Krause", "Johannes Schusterbauer", "Björn Ommer"], "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models", "comment": "ICCV 2025, Project Page: https://compvis.github.io/SCFlow/", "summary": "Explicitly disentangling style and content in vision models remains\nchallenging due to their semantic overlap and the subjectivity of human\nperception. Existing methods propose separation through generative or\ndiscriminative objectives, but they still face the inherent ambiguity of\ndisentangling intertwined concepts. Instead, we ask: Can we bypass explicit\ndisentanglement by learning to merge style and content invertibly, allowing\nseparation to emerge naturally? We propose SCFlow, a flow-matching framework\nthat learns bidirectional mappings between entangled and disentangled\nrepresentations. Our approach is built upon three key insights: 1) Training\nsolely to merge style and content, a well-defined task, enables invertible\ndisentanglement without explicit supervision; 2) flow matching bridges on\narbitrary distributions, avoiding the restrictive Gaussian priors of diffusion\nmodels and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51\nstyles $\\times$ 10,000 content samples) was curated to simulate disentanglement\nthrough systematic style-content pairing. Beyond controllable generation tasks,\nwe demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot\nsettings and achieves competitive performance, highlighting that\ndisentanglement naturally emerges from the invertible merging process.", "AI": {"tldr": "SCFlow提出了一种流匹配框架，通过学习风格和内容的可逆合并来自然实现解缠，避免了传统显式解缠的挑战。", "motivation": "由于语义重叠和人类感知的主观性，在视觉模型中显式解缠风格和内容仍然具有挑战性。现有方法面临概念交织的固有模糊性。本文旨在探索是否可以通过学习可逆地合并风格和内容来绕过显式解缠，从而使分离自然地出现。", "method": "本文提出了SCFlow，一个流匹配框架，用于学习缠绕和解缠表示之间的双向映射。其核心思想包括：1) 仅训练合并风格和内容（一个明确定义的任务）即可实现无需显式监督的可逆解缠；2) 流匹配可以在任意分布上进行，避免了扩散模型和归一化流的限制性高斯先验；3) 构建了一个包含51万个样本（51种风格 × 1万个内容样本）的合成数据集，通过系统性的风格-内容配对来模拟解缠。", "result": "SCFlow在可控生成任务之外，还展示了其在零样本设置下对ImageNet-1k和WikiArt的泛化能力，并取得了有竞争力的性能，这突出表明解缠自然地从可逆合并过程中涌现出来。", "conclusion": "通过学习风格和内容的可逆合并，可以自然地实现解缠，这为解决风格与内容解缠的难题提供了一种新颖有效的方法。"}}
{"id": "2508.03252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03252", "abs": "https://arxiv.org/abs/2508.03252", "authors": ["Wentao Qu", "Guofeng Mei", "Jing Wang", "Yujiao Wu", "Xiaoshui Huang", "Liang Xiao"], "title": "Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion", "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust\n3D object detection tasks. Existing methods often rely on the score matching\nfrom 3D boxes or pre-trained diffusion priors. However, they typically require\nmulti-step iterations in inference, which limits efficiency. To address this,\nwe propose a \\textbf{R}obust single-stage fully \\textbf{S}parse 3D object\n\\textbf{D}etection \\textbf{Net}work with a Detachable Latent Framework (DLF) of\nDDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in\nlatent feature spaces through lightweight denoising networks like multi-level\ndenoising autoencoders (DAEs). This enables RSDNet to effectively understand\nscene distributions under multi-level perturbations, achieving robust and\nreliable detection. Meanwhile, we reformulate the noising and denoising\nmechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise\nsamples and targets, enhancing RSDNet robustness to multiple perturbations.\nFurthermore, a semantic-geometric conditional guidance is introduced to\nperceive the object boundaries and shapes, alleviating the center feature\nmissing problem in sparse representations, enabling RSDNet to perform in a\nfully sparse detection pipeline. Moreover, the detachable denoising network\ndesign of DLF enables RSDNet to perform single-step detection in inference,\nfurther enhancing detection efficiency. Extensive experiments on public\nbenchmarks show that RSDNet can outperform existing methods, achieving\nstate-of-the-art detection.", "AI": {"tldr": "RSDNet是一个鲁棒的单阶段全稀疏3D目标检测网络，它利用可分离的潜空间去噪扩散模型（DDPMs），通过轻量级去噪网络和语义几何条件引导，实现了高效且最先进的检测性能。", "motivation": "现有基于DDPM的3D目标检测方法通常依赖于多步迭代推理，效率低下，这限制了它们在实际应用中的潜力。", "method": "本文提出了RSDNet，一个带有可分离潜空间框架（DLF）的DDPMs。具体方法包括：1) 在潜特征空间中通过多级去噪自编码器（DAEs）学习去噪过程；2) 重新设计DDPM的加噪和去噪机制，以构建多类型、多级噪声样本和目标，增强对多扰动的鲁棒性；3) 引入语义几何条件引导，解决稀疏表示中的中心特征缺失问题，实现全稀疏检测；4) DLF的可分离去噪网络设计使得单步推理成为可能，提升检测效率。", "result": "在公共基准测试上的大量实验表明，RSDNet性能优于现有方法，达到了最先进的检测水平。", "conclusion": "RSDNet成功地将DDPM的鲁棒性与单阶段、全稀疏和高效的特性结合起来，为3D目标检测提供了一个高性能的解决方案。"}}
{"id": "2508.03404", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03404", "abs": "https://arxiv.org/abs/2508.03404", "authors": ["Xinlei Yu", "Zhangquan Chen", "Yudong Zhang", "Shilin Lu", "Ruolin Shen", "Jiangning Zhang", "Xiaobin Hu", "Yanwei Fu", "Shuicheng Yan"], "title": "Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling", "comment": null, "summary": "Existing vision-language models (VLMs), whether generalists or specialists,\nremain constrained by their parameter scale, lack robust self-correction\ncapabilities, and underperform in tasks involving long visual contexts and\ncomplex reasoning, resulting in suboptimal performance on document-based tasks.\nTo address this, we propose MACT, a Multi-Agent Collaboration framework with\nTest-Time scaling, tailored for visual document understanding and visual\nquestion answering (VQA). It comprises four distinct small-scale agents, i.e.,\nplanning, execution, judgment, and answer agents, with clearly defined roles\nand effective collaboration. Notably, the judgment agent exclusively verifies\ncorrectness and redirects to prior agents for revisions, outperforming\nconventional correction strategies. To further expand the capability boundaries\nof the framework, we propose mixed reward modeling that balances agent-specific\nabilities and global collaboration, as well as agent-wise hybrid test-time\nscaling, which customizes different scaling strategies for each agent based on\ntheir functions. Evaluated on benchmarks spanning both document-based and\nnon-document-based settings, our MACT shows superior performance with a smaller\nparameter scale without sacrificing the ability of general and mathematical\ntasks. Especially, it stands out in benchmarks involving long visual contexts\nand complicated reasoning. The three variants of MACT consistently hold the top\nthree positions in average scores, leading in 13 of the 15 benchmarks. Code\nwill be available at: https://github.com/YU-deep/MACT.git.", "AI": {"tldr": "MACT是一个多智能体协作框架，通过规划、执行、判断和回答智能体，并结合测试时缩放和混合奖励模型，显著提升了视觉文档理解和视觉问答性能，尤其在长视觉上下文和复杂推理任务中表现出色，且参数规模更小。", "motivation": "现有视觉语言模型（VLM）受参数规模限制，缺乏鲁棒的自我纠正能力，且在涉及长视觉上下文和复杂推理的任务（特别是文档任务）中表现不佳。", "method": "提出MACT框架，包含四个小型智能体：规划、执行、判断和回答。判断智能体专门负责验证和重定向以进行修正。此外，引入混合奖励建模以平衡智能体特定能力和全局协作，并采用智能体级混合测试时缩放策略，根据各智能体功能定制缩放方案。", "result": "MACT在文档和非文档基准测试中均表现出卓越性能，参数规模更小，且不牺牲通用和数学任务能力。在涉及长视觉上下文和复杂推理的基准测试中尤为突出。MACT的三个变体在平均得分中稳居前三，并在15个基准测试中的13个中领先。", "conclusion": "MACT通过其独特的多智能体协作框架、强大的自我纠正机制和灵活的测试时缩放策略，有效克服了现有VLM的局限性，在视觉文档理解和视觉问答任务上取得了显著的性能提升，尤其是在处理复杂和长上下文视觉信息时表现优异。"}}
{"id": "2508.03256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03256", "abs": "https://arxiv.org/abs/2508.03256", "authors": ["Gang Dai", "Yifan Zhang", "Yutao Qin", "Qiangya Guo", "Shuangping Huang", "Shuicheng Yan"], "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation", "comment": "To appear in ICCV2025", "summary": "Existing handwritten text generation methods primarily focus on isolated\nwords. However, realistic handwritten text demands attention not only to\nindividual words but also to the relationships between them, such as vertical\nalignment and horizontal spacing. Therefore, generating entire text lines\nemerges as a more promising and comprehensive task. However, this task poses\nsignificant challenges, including the accurate modeling of complex style\npatterns encompassing both intra- and inter-word relationships, and maintaining\ncontent accuracy across numerous characters. To address these challenges, we\npropose DiffBrush, a novel diffusion-based model for handwritten text-line\ngeneration. Unlike existing methods, DiffBrush excels in both style imitation\nand content accuracy through two key strategies: (1) content-decoupled style\nlearning, which disentangles style from content to better capture intra-word\nand inter-word style patterns by using column- and row-wise masking; and (2)\nmulti-scale content learning, which employs line and word discriminators to\nensure global coherence and local accuracy of textual content. Extensive\nexperiments show that DiffBrush excels in generating high-quality text lines,\nparticularly in style reproduction and content preservation. Code is available\nat https://github.com/dailenson/DiffBrush.", "AI": {"tldr": "DiffBrush是一种新颖的基于扩散模型，用于生成手写文本行，它通过内容解耦的风格学习和多尺度内容学习，在风格模仿和内容准确性方面表现出色。", "motivation": "现有手写文本生成方法主要关注孤立的单词，但真实手写文本不仅需要考虑单个单词，还需要关注它们之间的关系（如垂直对齐和水平间距）。生成整个文本行更具挑战性，需要准确建模包含词内和词间关系的复杂风格模式，并保持多字符的内容准确性。", "method": "本文提出了DiffBrush，一个基于扩散模型的手写文本行生成模型。它采用两种关键策略：1) 内容解耦的风格学习，通过列向和行向掩码将风格与内容分离，以更好地捕捉词内和词间风格模式；2) 多尺度内容学习，利用行和词判别器确保文本内容的全局连贯性和局部准确性。", "result": "广泛的实验表明，DiffBrush在生成高质量文本行方面表现出色，特别是在风格再现和内容保持方面。", "conclusion": "DiffBrush模型有效解决了手写文本行生成中的复杂风格模式建模和内容准确性维护的挑战，在风格模仿和内容准确性方面均取得了显著成效。"}}
{"id": "2508.03411", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03411", "abs": "https://arxiv.org/abs/2508.03411", "authors": ["Diana-Nicoleta Grigore", "Neelu Madan", "Andreas Mogelmose", "Thomas B. Moeslund", "Radu Tudor Ionescu"], "title": "SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation", "comment": null, "summary": "Unsupervised video segmentation is a challenging computer vision task,\nespecially due to the lack of supervisory signals coupled with the complexity\nof visual scenes. To overcome this challenge, state-of-the-art models based on\nslot attention often have to rely on large and computationally expensive neural\narchitectures. To this end, we propose a simple knowledge distillation\nframework that effectively transfers object-centric representations to a\nlightweight student. The proposed framework, called SlotMatch, aligns\ncorresponding teacher and student slots via the cosine similarity, requiring no\nadditional distillation objectives or auxiliary supervision. The simplicity of\nSlotMatch is confirmed via theoretical and empirical evidence, both indicating\nthat integrating additional losses is redundant. We conduct experiments on two\ndatasets to compare the state-of-the-art teacher model, SlotContrast, with our\ndistilled student. The results show that our student based on SlotMatch matches\nand even outperforms its teacher, while using 3.6x less parameters and running\n1.9x faster. Moreover, our student surpasses previous unsupervised video\nsegmentation models.", "AI": {"tldr": "本文提出了一种名为SlotMatch的简单知识蒸馏框架，能将无监督视频分割中基于槽位注意力的大型模型（教师）的物体中心表示有效迁移到轻量级学生模型，在性能匹配甚至超越教师的同时显著减少参数量并提高运行速度。", "motivation": "无监督视频分割是一个具有挑战性的任务，缺乏监督信号且视觉场景复杂。现有的最先进模型（如基于槽位注意力的模型）往往需要庞大且计算昂贵的神经网络架构。", "method": "本文提出了一个名为SlotMatch的知识蒸馏框架。该框架通过余弦相似度对齐教师模型和学生模型中对应的槽位，无需额外的蒸馏目标或辅助监督。理论和经验证据表明，集成额外损失是多余的。", "result": "实验结果显示，基于SlotMatch的学生模型在性能上与最先进的教师模型SlotContrast匹配甚至超越，同时使用的参数量减少了3.6倍，运行速度提升了1.9倍。此外，该学生模型也超越了以往的无监督视频分割模型。", "conclusion": "SlotMatch框架证明了通过简单的知识蒸馏，可以有效地将物体中心表示从大型模型迁移到轻量级模型，从而在无监督视频分割任务中实现更高的效率和竞争力，而无需额外的复杂性。"}}
{"id": "2508.03266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03266", "abs": "https://arxiv.org/abs/2508.03266", "authors": ["Huaihai Lyu", "Chaofan Chen", "Yuheng Ji", "Changsheng Xu"], "title": "EgoPrompt: Prompt Pool Learning for Egocentric Action Recognition", "comment": null, "summary": "Driven by the increasing demand for applications in augmented and virtual\nreality, egocentric action recognition has emerged as a prominent research\narea. It is typically divided into two subtasks: recognizing the performed\nbehavior (i.e., verb component) and identifying the objects being acted upon\n(i.e., noun component) from the first-person perspective. However, most\nexisting approaches treat these two components as independent classification\ntasks, focusing on extracting component-specific knowledge while overlooking\ntheir inherent semantic and contextual relationships, leading to fragmented\nrepresentations and sub-optimal generalization capability. To address these\nchallenges, we propose a prompt learning-based framework, EgoPrompt, to conduct\nthe egocentric action recognition task. Building on the existing prompting\nstrategy to capture the component-specific knowledge, we construct a Unified\nPrompt Pool space to establish interaction between the two types of component\nrepresentations. Specifically, the component representations (from verbs and\nnouns) are first decomposed into fine-grained patterns with the prompt pair\nform. Then, these pattern-level representations are fused through an\nattention-based mechanism to facilitate cross-component interaction. To ensure\nthe prompt pool is informative, we further introduce a novel training\nobjective, Diverse Pool Criteria. This objective realizes our goals from two\nperspectives: Prompt Selection Frequency Regularization and Prompt Knowledge\nOrthogonalization. Extensive experiments are conducted on the Ego4D,\nEPIC-Kitchens, and EGTEA datasets. The results consistently show that EgoPrompt\nachieves state-of-the-art performance across within-dataset, cross-dataset, and\nbase-to-novel generalization benchmarks.", "AI": {"tldr": "针对第一视角动作识别中动词和名词组件独立处理的问题，本文提出EgoPrompt框架，通过提示学习和统一提示池空间，融合动词和名词表示，实现组件间交互，显著提升了泛化能力。", "motivation": "增强现实和虚拟现实应用对第一视角动作识别的需求日益增长。现有方法将动作识别的动词（行为）和名词（对象）组件视为独立的分类任务，忽略了它们固有的语义和上下文关系，导致表示碎片化和泛化能力不足。", "method": "本文提出了基于提示学习的EgoPrompt框架。该框架在现有提示策略基础上，构建了一个统一提示池空间，以建立动词和名词两种组件表示之间的交互。具体而言，组件表示首先被分解为提示对形式的细粒度模式，然后通过基于注意力的机制融合这些模式级表示，以促进跨组件交互。为确保提示池的信息量，进一步引入了新颖的训练目标——多样化池标准（Diverse Pool Criteria），该目标从提示选择频率正则化和提示知识正交化两个角度实现。", "result": "在Ego4D、EPIC-Kitchens和EGTEA数据集上进行了广泛实验。结果一致表明，EgoPrompt在数据集内、跨数据集以及从基础到新颖的泛化基准上都取得了最先进的性能。", "conclusion": "EgoPrompt通过统一处理第一视角动作识别中的动词和名词组件，并建立它们之间的语义和上下文关系，有效解决了现有方法的局限性。其提出的提示学习框架和多样化提示池机制，显著提升了模型的泛化能力，达到了当前最优水平。"}}
{"id": "2508.03415", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.03415", "abs": "https://arxiv.org/abs/2508.03415", "authors": ["Shivangi Nigam", "Adarsh Prasad Behera", "Shekhar Verma", "P. Nagabhushan"], "title": "Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN", "comment": "This paper is currently under review for publication in an IEEE\n  Transactions. If accepted, the copyright will be transferred to IEEE", "summary": "This paper presents Fd-CycleGAN, an image-to-image (I2I) translation\nframework that enhances latent representation learning to approximate real data\ndistributions. Building upon the foundation of CycleGAN, our approach\nintegrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to\ncapture fine-grained local pixel semantics while preserving structural\ncoherence from the source domain. We employ distribution-based loss metrics,\nincluding KL/JS divergence and log-based similarity measures, to explicitly\nquantify the alignment between real and generated image distributions in both\nspatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we\nconduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a\nsynthetically augmented Strike-off dataset. Compared to baseline CycleGAN and\nother state-of-the-art methods, our approach demonstrates superior perceptual\nquality, faster convergence, and improved mode diversity, particularly in\nlow-data regimes. By effectively capturing local and global distribution\ncharacteristics, Fd-CycleGAN achieves more visually coherent and semantically\nconsistent translations. Our results suggest that frequency-guided latent\nlearning significantly improves generalization in image translation tasks, with\npromising applications in document restoration, artistic style transfer, and\nmedical image synthesis. We also provide comparative insights with\ndiffusion-based generative models, highlighting the advantages of our\nlightweight adversarial approach in terms of training efficiency and\nqualitative output.", "AI": {"tldr": "Fd-CycleGAN是一个改进的图像到图像翻译框架，通过引入局部邻域编码和频率感知监督，并使用分布损失，实现了更优的感知质量、更快的收敛和更好的模式多样性，尤其在数据量较少时表现出色。", "motivation": "现有图像到图像翻译方法在潜在表示学习和逼近真实数据分布方面存在不足，导致生成图像的局部语义和结构一致性欠佳。研究旨在提高生成图像的感知质量、收敛速度和模式多样性。", "method": "Fd-CycleGAN在CycleGAN基础上，整合了局部邻域编码（LNE）和频率感知监督，以捕获细粒度的局部像素语义并保持结构连贯性。采用基于分布的损失度量（如KL/JS散度和基于对数的相似度）来量化真实和生成图像在空间和频率域的分布对齐。同时，与扩散模型进行了比较，以突出其轻量级对抗方法的优势。", "result": "Fd-CycleGAN在Horse2Zebra、Monet2Photo和合成Strike-off数据集上进行了实验，结果表明其在感知质量、收敛速度和模式多样性方面优于基线CycleGAN和其他SOTA方法，尤其在低数据量情况下表现更佳。实现了视觉上更连贯、语义上更一致的翻译。", "conclusion": "频率引导的潜在学习显著改善了图像翻译任务的泛化能力，在文档修复、艺术风格迁移和医学图像合成等领域具有广阔应用前景。Fd-CycleGAN作为一种轻量级对抗方法，在训练效率和定性输出方面优于扩散生成模型。"}}
{"id": "2508.03277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03277", "abs": "https://arxiv.org/abs/2508.03277", "authors": ["Hang Guo", "Qing Zhang", "Zixuan Gao", "Siyuan Yang", "Shulin Peng", "Xiang Tao", "Ting Yu", "Yan Wang", "Qingli Li"], "title": "Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification", "comment": "Accepted by ACMMM'25", "summary": "Accurate prediction of placental diseases via whole slide images (WSIs) is\ncritical for preventing severe maternal and fetal complications. However, WSI\nanalysis presents significant computational challenges due to the massive data\nvolume. Existing WSI classification methods encounter critical limitations: (1)\ninadequate patch selection strategies that either compromise performance or\nfail to sufficiently reduce computational demands, and (2) the loss of global\nhistological context resulting from patch-level processing approaches. To\naddress these challenges, we propose an Efficient multimodal framework for\nPatient-level placental disease Diagnosis, named EmmPD. Our approach introduces\na two-stage patch selection module that combines parameter-free and learnable\ncompression strategies, optimally balancing computational efficiency with\ncritical feature preservation. Additionally, we develop a hybrid multimodal\nfusion module that leverages adaptive graph learning to enhance pathological\nfeature representation and incorporates textual medical reports to enrich\nglobal contextual understanding. Extensive experiments conducted on both a\nself-constructed patient-level Placental dataset and two public datasets\ndemonstrating that our method achieves state-of-the-art diagnostic performance.\nThe code is available at https://github.com/ECNU-MultiDimLab/EmmPD.", "AI": {"tldr": "针对胎盘疾病WSI分析中存在的计算挑战和全局上下文丢失问题，本文提出了EmmPD框架，通过两阶段补丁选择和混合多模态融合（结合自适应图学习和文本报告）实现了最先进的诊断性能。", "motivation": "现有WSI分类方法在胎盘疾病诊断中存在局限性：1) 补丁选择策略不足，导致性能或计算效率受损；2) 补丁级处理导致全局组织学上下文丢失。", "method": "本文提出了EmmPD框架：1) 两阶段补丁选择模块，结合无参数和可学习压缩策略，平衡计算效率与关键特征保留；2) 混合多模态融合模块，利用自适应图学习增强病理特征表示，并整合文本医学报告以丰富全局上下文理解。", "result": "在自建的患者级胎盘数据集和两个公共数据集上进行了广泛实验，结果表明该方法实现了最先进的诊断性能。", "conclusion": "EmmPD框架通过创新的补丁选择和多模态融合策略，有效解决了WSI分析中的计算挑战和上下文丢失问题，在胎盘疾病诊断中展现出卓越的性能。"}}
{"id": "2508.03426", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03426", "abs": "https://arxiv.org/abs/2508.03426", "authors": ["Futian Wang", "Yuhan Qiao", "Xiao Wang", "Fuling Wang", "Yuxiang Zhang", "Dengdi Sun"], "title": "R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation", "comment": null, "summary": "X-ray medical report generation is one of the important applications of\nartificial intelligence in healthcare. With the support of large foundation\nmodels, the quality of medical report generation has significantly improved.\nHowever, challenges such as hallucination and weak disease diagnostic\ncapability still persist. In this paper, we first construct a large-scale\nmulti-modal medical knowledge graph (termed M3KG) based on the ground truth\nmedical report using the GPT-4o. It contains 2477 entities, 3 kinds of\nrelations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert\nPlus dataset. Then, we sample it to obtain multi-granularity semantic graphs\nand use an R-GCN encoder for feature extraction. For the input X-ray image, we\nadopt the Swin-Transformer to extract the vision features and interact with the\nknowledge using cross-attention. The vision tokens are fed into a Q-former and\nretrieved the disease-aware vision tokens using another cross-attention.\nFinally, we adopt the large language model to map the semantic knowledge graph,\ninput X-ray image, and disease-aware vision tokens into language descriptions.\nExtensive experiments on multiple datasets fully validated the effectiveness of\nour proposed knowledge graph and X-ray report generation framework. The source\ncode of this paper will be released on\nhttps://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "本文构建了一个大规模多模态医学知识图谱（M3KG），并将其与视觉特征结合，通过大语言模型生成X射线医学报告，以解决现有方法中幻觉和诊断能力弱的问题。", "motivation": "尽管大基础模型显著提升了医学报告生成质量，但在X射线医学报告生成中，仍然存在幻觉（hallucination）和疾病诊断能力弱的挑战。", "method": "首先，利用GPT-4o从真实医学报告构建了一个大规模多模态医学知识图谱（M3KG），包含实体、关系、三元组和疾病感知视觉token。接着，对M3KG进行采样以获得多粒度语义图，并使用R-GCN编码器提取特征。对于输入的X射线图像，采用Swin-Transformer提取视觉特征，并通过交叉注意力与知识交互。视觉token通过Q-former和另一个交叉注意力检索疾病感知视觉token。最后，使用大语言模型将语义知识图谱、输入X射线图像和疾病感知视觉token映射为语言描述。", "result": "在多个数据集上进行的广泛实验充分验证了所提出的知识图谱和X射线报告生成框架的有效性。", "conclusion": "本文提出的基于知识图谱和多模态交互的X射线报告生成框架，有效提升了报告质量，解决了现有模型中幻觉和诊断能力不足的问题。"}}
{"id": "2508.03300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03300", "abs": "https://arxiv.org/abs/2508.03300", "authors": ["Jun Luo", "Zijing Zhao", "Yang Liu"], "title": "Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation", "comment": "Accepted to IROS 2025", "summary": "Deep learning-based semantic segmentation models achieve impressive results\nyet remain limited in handling distribution shifts between training and test\ndata. In this paper, we present SDGPA (Synthetic Data Generation and\nProgressive Adaptation), a novel method that tackles zero-shot domain adaptive\nsemantic segmentation, in which no target images are available, but only a text\ndescription of the target domain's style is provided. To compensate for the\nlack of target domain training data, we utilize a pretrained off-the-shelf\ntext-to-image diffusion model, which generates training images by transferring\nsource domain images to target style. Directly editing source domain images\nintroduces noise that harms segmentation because the layout of source images\ncannot be precisely maintained. To address inaccurate layouts in synthetic\ndata, we propose a method that crops the source image, edits small patches\nindividually, and then merges them back together, which helps improve spatial\nprecision. Recognizing the large domain gap, SDGPA constructs an augmented\nintermediate domain, leveraging easier adaptation subtasks to enable more\nstable model adaptation to the target domain. Additionally, to mitigate the\nimpact of noise in synthetic data, we design a progressive adaptation strategy,\nensuring robust learning throughout the training process. Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance in zero-shot\nsemantic segmentation. The code is available at\nhttps://github.com/ROUJINN/SDGPA", "AI": {"tldr": "SDGPA提出了一种零样本域适应语义分割方法，通过文本到图像扩散模型生成合成数据并采用渐进式适应策略来应对目标域无图像的挑战。", "motivation": "深度学习语义分割模型在训练和测试数据之间存在分布偏移时表现受限。在零样本域适应场景下，目标域没有可用的图像，仅提供文本描述，这进一步增加了挑战。", "method": "1. 利用预训练的文本到图像扩散模型，将源域图像转换为目标风格以生成训练图像。2. 针对合成数据中布局不准确的问题，提出裁剪源图像、单独编辑小块并重新合并的方法，以提高空间精度。3. 构建一个增强的中间域，通过简化适应子任务来稳定模型向目标域的适应。4. 设计渐进式适应策略，以减轻合成数据中噪声的影响，确保训练过程中的鲁棒学习。", "result": "该方法在零样本语义分割中取得了最先进的性能。", "conclusion": "SDGPA通过创新的合成数据生成和渐进式适应策略，有效解决了零样本域适应语义分割的难题，即使在目标域仅有文本描述的情况下也能实现高性能表现。"}}
{"id": "2508.03437", "categories": ["cs.CV", "cs.AI", "62M10", "I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2508.03437", "abs": "https://arxiv.org/abs/2508.03437", "authors": ["Hongjun Liu", "Chao Yao", "Yalan Zhang", "Xiaokun wang", "Xiaojuan Ban"], "title": "Spatial Imputation Drives Cross-Domain Alignment for EEG Classification", "comment": "ACMMM 2025 poster", "summary": "Electroencephalogram (EEG) signal classification faces significant challenges\ndue to data distribution shifts caused by heterogeneous electrode\nconfigurations, acquisition protocols, and hardware discrepancies across\ndomains. This paper introduces IMAC, a novel channel-dependent mask and\nimputation self-supervised framework that formulates the alignment of\ncross-domain EEG data shifts as a spatial time series imputation task. To\naddress heterogeneous electrode configurations in cross-domain scenarios, IMAC\nfirst standardizes different electrode layouts using a 3D-to-2D positional\nunification mapping strategy, establishing unified spatial representations.\nUnlike previous mask-based self-supervised representation learning methods,\nIMAC introduces spatio-temporal signal alignment. This involves constructing a\nchannel-dependent mask and reconstruction task framed as a low-to-high\nresolution EEG spatial imputation problem. Consequently, this approach\nsimulates cross-domain variations such as channel omissions and temporal\ninstabilities, thus enabling the model to leverage the proposed imputer for\nrobust signal alignment during inference. Furthermore, IMAC incorporates a\ndisentangled structure that separately models the temporal and spatial\ninformation of the EEG signals separately, reducing computational complexity\nwhile enhancing flexibility and adaptability. Comprehensive evaluations across\n10 publicly available EEG datasets demonstrate IMAC's superior performance,\nachieving state-of-the-art classification accuracy in both cross-subject and\ncross-center validation scenarios. Notably, IMAC shows strong robustness under\nboth simulated and real-world distribution shifts, surpassing baseline methods\nby up to $35$\\% in integrity scores while maintaining consistent classification\naccuracy.", "AI": {"tldr": "IMAC是一种新颖的基于通道依赖掩码和插补的自监督框架，通过将跨域EEG数据对齐转化为空间时间序列插补任务，有效解决了EEG分类中因异构电极配置、采集协议和硬件差异导致的数据分布偏移问题。", "motivation": "EEG信号分类面临挑战，原因在于不同域（如异构电极配置、采集协议、硬件差异）导致的数据分布偏移。现有方法难以有效处理这些跨域差异。", "method": "IMAC框架首先采用3D到2D的位置统一映射策略，标准化不同电极布局以建立统一空间表示。然后，它引入通道依赖掩码和重建任务，将其构建为低分辨率到高分辨率的EEG空间插补问题，模拟跨域变化（如通道遗漏和时间不稳定性）。此外，IMAC采用解耦结构，独立建模EEG信号的时间和空间信息，以降低计算复杂性并增强灵活性和适应性。", "result": "在10个公开EEG数据集上的综合评估表明，IMAC在跨受试者和跨中心验证场景中均实现了最先进的分类精度。值得注意的是，IMAC在模拟和真实世界分布偏移下均表现出强大的鲁棒性，完整性分数比基线方法高出35%，同时保持了稳定的分类精度。", "conclusion": "IMAC框架通过创新的空间时间序列插补方法，有效解决了跨域EEG数据对齐的挑战，显著提升了EEG分类的准确性和鲁棒性，在多种分布偏移条件下表现出色。"}}
{"id": "2508.03317", "categories": ["cs.CV", "68T07", "I.4.8"], "pdf": "https://arxiv.org/pdf/2508.03317", "abs": "https://arxiv.org/abs/2508.03317", "authors": ["Mahdi Golizadeh", "Nassibeh Golizadeh", "Mohammad Ali Keyvanrad", "Hossein Shirazi"], "title": "Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review", "comment": "20 pages, 11 figures, This paper was submitted to IEEE Transactions\n  on Neural Networks and Learning Systems", "summary": "Object detection has achieved remarkable accuracy through deep learning, yet\nthese improvements often come with increased computational cost, limiting\ndeployment on resource-constrained devices. Knowledge Distillation (KD)\nprovides an effective solution by enabling compact student models to learn from\nlarger teacher models. However, adapting KD to object detection poses unique\nchallenges due to its dual objectives-classification and localization-as well\nas foreground-background imbalance and multi-scale feature representation. This\nreview introduces a novel architecture-centric taxonomy for KD methods,\ndistinguishing between CNN-based detectors (covering backbone-level,\nneck-level, head-level, and RPN/RoI-level distillation) and Transformer-based\ndetectors (including query-level, feature-level, and logit-level distillation).\nWe further evaluate representative methods using the MS COCO and PASCAL VOC\ndatasets with mAP@0.5 as performance metric, providing a comparative analysis\nof their effectiveness. The proposed taxonomy and analysis aim to clarify the\nevolving landscape of KD in object detection, highlight current challenges, and\nguide future research toward efficient and scalable detection systems.", "AI": {"tldr": "该综述提出了一种以架构为中心的知识蒸馏（KD）分类法，用于目标检测，涵盖CNN和Transformer模型，并评估了代表性方法，旨在指导高效可扩展检测系统的未来研究。", "motivation": "深度学习使目标检测精度显著提升，但计算成本也随之增加，限制了其在资源受限设备上的部署。知识蒸馏（KD）是有效解决方案，但将其应用于目标检测面临独特挑战，如分类与定位双重目标、前景-背景不平衡以及多尺度特征表示。", "method": "引入了一种新颖的、以架构为中心的KD方法分类法：针对CNN-based检测器，分为骨干级、颈部级、头部级和RPN/RoI级蒸馏；针对Transformer-based检测器，包括查询级、特征级和logit级蒸馏。此外，使用MS COCO和PASCAL VOC数据集，以mAP@0.5为性能指标，评估了代表性方法并进行了比较分析。", "result": "通过提出的分类法和评估，对各种代表性KD方法的有效性进行了比较分析，旨在阐明目标检测中KD的发展现状。", "conclusion": "所提出的分类法和分析旨在澄清目标检测中KD的演变格局，突出当前挑战，并指导未来研究，以实现高效和可扩展的检测系统。"}}
{"id": "2508.03480", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03480", "abs": "https://arxiv.org/abs/2508.03480", "authors": ["Junjie Cao", "Kaizhou Li", "Xinchun Yu", "Hongxiang Li", "Xiaoping Zhang"], "title": "VideoGuard: Protecting Video Content from Unauthorized Editing", "comment": "ai security, 10pages, 5 figures", "summary": "With the rapid development of generative technology, current generative\nmodels can generate high-fidelity digital content and edit it in a controlled\nmanner. However, there is a risk that malicious individuals might misuse these\ncapabilities for misleading activities. Although existing research has\nattempted to shield photographic images from being manipulated by generative\nmodels, there remains a significant disparity in the protection offered to\nvideo content editing. To bridge the gap, we propose a protection method named\nVideoGuard, which can effectively protect videos from unauthorized malicious\nediting. This protection is achieved through the subtle introduction of nearly\nunnoticeable perturbations that interfere with the functioning of the intended\ngenerative diffusion models. Due to the redundancy between video frames, and\ninter-frame attention mechanism in video diffusion models, simply applying\nimage-based protection methods separately to every video frame can not shield\nvideo from unauthorized editing. To tackle the above challenge, we adopt joint\nframe optimization, treating all video frames as an optimization entity.\nFurthermore, we extract video motion information and fuse it into optimization\nobjectives. Thus, these alterations can effectively force the models to produce\noutputs that are implausible and inconsistent. We provide a pipeline to\noptimize this perturbation. Finally, we use both objective metrics and\nsubjective metrics to demonstrate the efficacy of our method, and the results\nshow that the protection performance of VideoGuard is superior to all the\nbaseline methods.", "AI": {"tldr": "本文提出VideoGuard方法，通过引入微小扰动来有效保护视频内容免受未经授权的生成式模型恶意编辑。", "motivation": "生成技术快速发展，可生成高保真数字内容并进行受控编辑，但存在被恶意滥用的风险。现有研究主要关注图片保护，视频内容编辑的保护存在显著不足。", "method": "提出VideoGuard方法，通过引入几乎不可察觉的微小扰动来干扰生成扩散模型的功能。为解决视频帧间冗余和帧间注意力机制问题，该方法采用联合帧优化（将所有视频帧视为一个优化实体），并提取视频运动信息融入优化目标，迫使模型产生不合理和不一致的输出。文中提供了扰动优化流程。", "result": "通过客观和主观指标评估，结果表明VideoGuard方法的保护性能优于所有基线方法。", "conclusion": "VideoGuard方法能有效保护视频免受未经授权的恶意编辑，通过联合帧优化和融入视频运动信息，实现了优越的保护效果。"}}
{"id": "2508.03320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03320", "abs": "https://arxiv.org/abs/2508.03320", "authors": ["Peiyu Wang", "Yi Peng", "Yimeng Gan", "Liang Hu", "Tianyidan Xie", "Xiaokun Wang", "Yichen Wei", "Chuanxin Tang", "Bo Zhu", "Changshi Li", "Hongyang Wei", "Eric Li", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation", "comment": null, "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.", "AI": {"tldr": "Skywork UniPic是一个15亿参数的自回归模型，在一个单一架构中统一了图像理解、文本到图像生成和图像编辑，无需任务特定适配器，并在消费级硬件上实现了最先进的性能。", "motivation": "现有统一多模态系统通常需要任务特定适配器或模块间连接器，且高保真多模态集成往往需要过高的资源。研究动机是开发紧凑且能在普通硬件上实现最先进性能的多模态系统。", "method": ["采用解耦编码策略：使用掩码自回归编码器进行合成，SigLIP2编码器进行理解，两者均输入共享的自回归解码器。", "设计了渐进式、分辨率感知的训练计划，从256x256扩展到1024x1024，并动态解冻参数以平衡容量和稳定性。", "构建了精心策划的亿级规模数据集，并辅以任务特定的奖励模型来优化生成和编辑目标。"], "result": ["GenEval得分达到0.86，超越了大多数现有统一模型。", "DPG-Bench复杂生成任务创下85.5的新纪录。", "图像编辑方面，GEditBench-EN得分5.83，ImgEdit-Bench得分3.49。", "能在低于15 GB的GPU内存（如RTX 4090）下生成1024x1024的图像。"], "conclusion": "Skywork UniPic证明了高保真多模态集成无需巨大的资源投入，为可部署的高保真多模态AI建立了一个实用的范例。"}}
{"id": "2508.03483", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03483", "abs": "https://arxiv.org/abs/2508.03483", "authors": ["Dasol Choi Jihwan Lee", "Minjae Lee", "Minsuk Kahng"], "title": "When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models", "comment": null, "summary": "While prior research on text-to-image generation has predominantly focused on\nbiases in human depictions, we investigate a more subtle yet pervasive\nphenomenon: demographic bias in generated objects (e.g., cars). We introduce\nSODA (Stereotyped Object Diagnostic Audit), a novel framework for\nsystematically measuring such biases. Our approach compares visual attributes\nof objects generated with demographic cues (e.g., \"for young people'') to those\nfrom neutral prompts, across 2,700 images produced by three state-of-the-art\nmodels (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories.\nThrough a comprehensive analysis, we uncover strong associations between\nspecific demographic groups and visual attributes, such as recurring color\npatterns prompted by gender or ethnicity cues. These patterns reflect and\nreinforce not only well-known stereotypes but also more subtle and unintuitive\nbiases. We also observe that some models generate less diverse outputs, which\nin turn amplifies the visual disparities compared to neutral prompts. Our\nproposed auditing framework offers a practical approach for testing, revealing\nhow stereotypes still remain embedded in today's generative models. We see this\nas an essential step toward more systematic and responsible AI development.", "AI": {"tldr": "该研究引入SODA框架，系统测量文本到图像生成模型中物体（而非人类）的隐含人口统计学偏见，发现模型在生成物体时会根据人口学提示（如年龄、性别、种族）产生并强化刻板印象，呼吁更负责任的AI开发。", "motivation": "先前的研究主要关注文本到图像生成模型在人类描绘上的偏见，但研究者认为在生成的物体中也存在一种更微妙但普遍的人口统计学偏见，亟需系统性测量和揭示。", "method": "引入了SODA（Stereotyped Object Diagnostic Audit）框架，通过比较在带有特定人口统计学提示（如“为年轻人设计的”）和中性提示下生成的物体的视觉属性。研究使用了2700张由GPT Image-1、Imagen 4和Stable Diffusion三款主流模型生成的图像，涵盖五种物体类别，进行综合分析。", "result": "研究发现特定人口统计学群体与视觉属性之间存在强烈的关联，例如由性别或种族提示引起的重复颜色模式。这些模式不仅反映并强化了众所周知的刻板印象，也揭示了更微妙和不直观的偏见。此外，部分模型生成的输出多样性较低，这进一步放大了与中性提示相比的视觉差异。", "conclusion": "所提出的SODA审计框架为测试和揭示生成模型中仍然存在的刻板印象提供了一种实用方法。这被认为是迈向更系统化和负责任的AI开发的关键一步。"}}
{"id": "2508.03334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03334", "abs": "https://arxiv.org/abs/2508.03334", "authors": ["Xunzhi Xiang", "Yabo Chen", "Guiyu Zhang", "Zhongyu Wang", "Zhe Gao", "Quanming Xiang", "Gonghu Shang", "Junqi Liu", "Haibin Huang", "Yang Gao", "Chi Zhang", "Qi Fan", "Xuelong Li"], "title": "Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation", "comment": null, "summary": "Current autoregressive diffusion models excel at video generation but are\ngenerally limited to short temporal durations. Our theoretical analysis\nindicates that the autoregressive modeling typically suffers from temporal\ndrift caused by error accumulation and hinders parallelization in long video\nsynthesis. To address these limitations, we propose a novel\nplanning-then-populating framework centered on Macro-from-Micro Planning (MMPL)\nfor long video generation. MMPL sketches a global storyline for the entire\nvideo through two hierarchical stages: Micro Planning and Macro Planning.\nSpecifically, Micro Planning predicts a sparse set of future keyframes within\neach short video segment, offering motion and appearance priors to guide\nhigh-quality video segment generation. Macro Planning extends the in-segment\nkeyframes planning across the entire video through an autoregressive chain of\nmicro plans, ensuring long-term consistency across video segments.\nSubsequently, MMPL-based Content Populating generates all intermediate frames\nin parallel across segments, enabling efficient parallelization of\nautoregressive generation. The parallelization is further optimized by Adaptive\nWorkload Scheduling for balanced GPU execution and accelerated autoregressive\nvideo generation. Extensive experiments confirm that our method outperforms\nexisting long video generation models in quality and stability. Generated\nvideos and comparison results are in our project page.", "AI": {"tldr": "本文提出了一种名为MMPL（Macro-from-Micro Planning）的“规划-填充”框架，用于生成长视频，解决了现有自回归扩散模型在长视频生成中存在的时序漂移和并行化限制。", "motivation": "当前的自回归扩散模型在视频生成方面表现出色，但通常仅限于短时程视频。理论分析表明，自回归建模常因误差累积导致时序漂移，并阻碍长视频合成中的并行化。", "method": "本文提出了一种以MMPL为核心的“规划-填充”框架。MMPL通过两个层级阶段勾勒出整个视频的全局故事情节：微观规划（Micro Planning）在每个短视频段内预测稀疏的未来关键帧，提供运动和外观先验以指导高质量视频段生成；宏观规划（Macro Planning）通过微观计划的自回归链将段内关键帧规划扩展到整个视频，确保视频段之间的长期一致性。随后，基于MMPL的内容填充（Content Populating）在不同视频段之间并行生成所有中间帧，实现自回归生成的高效并行化。并行化通过自适应工作负载调度（Adaptive Workload Scheduling）进一步优化，以平衡GPU执行并加速自回归视频生成。", "result": "广泛的实验证实，该方法在质量和稳定性方面优于现有长视频生成模型。", "conclusion": "MMPL框架通过分层规划和并行内容生成，有效解决了自回归模型在长视频生成中的局限性，实现了高质量和高稳定性的长视频生成。"}}
{"id": "2508.03538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03538", "abs": "https://arxiv.org/abs/2508.03538", "authors": ["Inamullah", "Imran Razzak", "Shoaib Jameel"], "title": "Retinal Lipidomics Associations as Candidate Biomarkers for Cardiovascular Health", "comment": null, "summary": "Retinal microvascular imaging is increasingly recognised as a non invasive\nmethod for evaluating systemic vascular and metabolic health. However, the\nassociation between lipidomics and retinal vasculature remains inadequate. This\nstudy investigates the relationships between serum lipid subclasses, free fatty\nacids (FA), diacylglycerols (DAG), triacylglycerols (TAG), and cholesteryl\nesters (CE), and retinal microvascular characteristics in a large\npopulation-based cohort. Using Spearman correlation analysis, we examined the\ninterconnection between lipid subclasses and ten retinal microvascular traits,\napplying the Benjamini-Hochberg false discovery rate (BH-FDR) to adjust for\nstatistical significance.\n  Results indicated that FA were linked to retinal vessel twistiness, while CE\ncorrelated with the average widths of arteries and veins. Conversely, DAG and\nTAG showed negative correlations with the width and complexity of arterioles\nand venules. These findings suggest that retinal vascular architecture reflects\ndistinct circulating lipid profiles, supporting its role as a non-invasive\nmarker of systemic metabolic health. This study is the first to integrate deep\nlearning (DL)derived retinal traits with lipidomic subclasses in a healthy\ncohort, thereby providing insights into microvascular structural changes\nindependent of disease status or treatment effects.", "AI": {"tldr": "本研究首次在一个健康人群队列中，结合深度学习衍生的视网膜血管特征和脂质组学亚类，揭示了血清脂质亚类（如游离脂肪酸、胆固醇酯、甘油二酯和甘油三酯）与视网膜微血管特征之间的关联。", "motivation": "视网膜微血管成像作为评估全身血管和代谢健康的非侵入性方法日益受到认可，然而，脂质组学与视网膜血管之间的关联性研究不足。", "method": "研究在一个大型基于人群的队列中进行，使用Spearman相关分析检查脂质亚类与十种视网膜微血管特征之间的相互关系，并应用Benjamini-Hochberg假发现率（BH-FDR）进行统计显著性调整。视网膜特征通过深度学习（DL）技术获得。", "result": "结果显示，游离脂肪酸（FA）与视网膜血管的弯曲度相关；胆固醇酯（CE）与动脉和静脉的平均宽度相关。相反，甘油二酯（DAG）和甘油三酯（TAG）与小动脉和小静脉的宽度和复杂性呈负相关。", "conclusion": "这些发现表明视网膜血管结构反映了不同的循环脂质谱，支持其作为全身代谢健康非侵入性标志物的作用。本研究首次在一个健康队列中整合了深度学习衍生的视网膜特征与脂质组学亚类，为独立于疾病状态或治疗效果的微血管结构变化提供了见解。"}}
{"id": "2508.03336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03336", "abs": "https://arxiv.org/abs/2508.03336", "authors": ["Tongshun Zhang", "Pingping Liu", "Zixuan Zhong", "Zijian Zhang", "Qiuzhan Zhou"], "title": "Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration", "comment": null, "summary": "Recovering fine-grained details in extremely dark images remains challenging\ndue to severe structural information loss and noise corruption. Existing\nenhancement methods often fail to preserve intricate details and sharp edges,\nlimiting their effectiveness in downstream applications like text and edge\ndetection. To address these deficiencies, we propose an efficient dual-stage\napproach centered on detail recovery for dark images. In the first stage, we\nintroduce a Residual Fourier-Guided Module (RFGM) that effectively restores\nglobal illumination in the frequency domain. RFGM captures inter-stage and\ninter-channel dependencies through residual connections, providing robust\npriors for high-fidelity frequency processing while mitigating error\naccumulation risks from unreliable priors. The second stage employs\ncomplementary Mamba modules specifically designed for textural structure\nrefinement: (1) Patch Mamba operates on channel-concatenated non-downsampled\npatches, meticulously modeling pixel-level correlations to enhance fine-grained\ndetails without resolution loss. (2) Grad Mamba explicitly focuses on\nhigh-gradient regions, alleviating state decay in state space models and\nprioritizing reconstruction of sharp edges and boundaries. Extensive\nexperiments on multiple benchmark datasets and downstream applications\ndemonstrate that our method significantly improves detail recovery performance\nwhile maintaining efficiency. Crucially, the proposed modules are lightweight\nand can be seamlessly integrated into existing Fourier-based frameworks with\nminimal computational overhead. Code is available at\nhttps://github.com/bywlzts/RFGM.", "AI": {"tldr": "本文提出一种高效的双阶段方法，用于恢复极暗图像中的精细细节。第一阶段利用残差傅里叶引导模块恢复全局光照；第二阶段通过Mamba模块（Patch Mamba和Grad Mamba）精炼纹理结构和锐利边缘。", "motivation": "在极暗图像中恢复精细细节具有挑战性，因为存在严重的结构信息丢失和噪声污染。现有增强方法往往无法保留复杂的细节和锐利的边缘，从而限制了它们在文本和边缘检测等下游应用中的有效性。", "method": "该方法采用高效的双阶段方法：\n1.  **第一阶段**：引入残差傅里叶引导模块（RFGM），在频域有效恢复全局光照。RFGM通过残差连接捕获阶段间和通道间依赖，提供鲁棒的先验信息，同时减轻误差累积。\n2.  **第二阶段**：采用互补的Mamba模块进行纹理结构精炼。Patch Mamba在非下采样补丁上操作，建模像素级关联以增强精细细节。Grad Mamba专注于高梯度区域，重建锐利边缘和边界，并缓解状态空间模型中的状态衰减。", "result": "在多个基准数据集和下游应用上的广泛实验表明，该方法显著提高了细节恢复性能，同时保持了效率。所提出的模块轻量级，可以无缝集成到现有基于傅里叶的框架中，计算开销极小。", "conclusion": "本文提出的双阶段方法，结合RFGM和创新的Mamba模块，成功解决了极暗图像细节恢复的挑战，显著提升了图像质量和下游应用表现，且具有高效和轻量级的特点。"}}
{"id": "2508.03596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03596", "abs": "https://arxiv.org/abs/2508.03596", "authors": ["Wuyang Li", "Wentao Pan", "Xiaoyuan Liu", "Zhendong Luo", "Chenxin Li", "Hengyu Liu", "Din Ping Tsai", "Mu Ku Chen", "Yixuan Yuan"], "title": "MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy", "comment": "ICCV 2025 (Highlight); Project Page:\n  https://cuhk-aim-group.github.io/MetaScope/", "summary": "Miniaturized endoscopy has advanced accurate visual perception within the\nhuman body. Prevailing research remains limited to conventional cameras\nemploying convex lenses, where the physical constraints with millimetre-scale\nthickness impose serious impediments on the micro-level clinical. Recently,\nwith the emergence of meta-optics, ultra-micro imaging based on metalenses\n(micron-scale) has garnered great attention, serving as a promising solution.\nHowever, due to the physical difference of metalens, there is a large gap in\ndata acquisition and algorithm research. In light of this, we aim to bridge\nthis unexplored gap, advancing the novel metalens endoscopy. First, we\nestablish datasets for metalens endoscopy and conduct preliminary optical\nsimulation, identifying two derived optical issues that physically adhere to\nstrong optical priors. Second, we propose MetaScope, a novel optics-driven\nneural network tailored for metalens endoscopy driven by physical optics.\nMetaScope comprises two novel designs: Optics-informed Intensity Adjustment\n(OIA), rectifying intensity decay by learning optical embeddings, and\nOptics-informed Chromatic Correction (OCC), mitigating chromatic aberration by\nlearning spatial deformations informed by learned Point Spread Function (PSF)\ndistributions. To enhance joint learning, we further deploy a gradient-guided\ndistillation to transfer knowledge from the foundational model adaptively.\nExtensive experiments demonstrate that MetaScope not only outperforms\nstate-of-the-art methods in both metalens segmentation and restoration but also\nachieves impressive generalized ability in real biomedical scenes.", "AI": {"tldr": "针对微型内窥镜中金属透镜成像的挑战，本文提出MetaScope神经网络，通过光学信息调整亮度和校正色差，显著提升了分割和复原性能。", "motivation": "现有微型内窥镜受限于传统凸透镜的物理尺寸（毫米级），难以实现微米级临床应用。虽然金属透镜（微米级）提供了有前景的解决方案，但其物理特性导致数据获取和算法研究存在巨大空白，亟需填补。", "method": "首先，建立了金属透镜内窥镜数据集并进行初步光学模拟，识别出两个强光学先验相关的问题。其次，提出了MetaScope，一个由物理光学驱动的新型神经网络。MetaScope包含两个新颖设计：光学信息强度调整（OIA）用于学习光学嵌入以校正强度衰减；光学信息色差校正（OCC）用于通过学习点扩散函数（PSF）分布指导的空间形变来减轻色差。此外，还部署了梯度引导蒸馏以自适应地从基础模型中转移知识，以增强联合学习。", "result": "大量实验表明，MetaScope不仅在金属透镜图像分割和复原方面均优于现有最先进方法，而且在真实生物医学场景中展现出出色的泛化能力。", "conclusion": "本研究成功填补了金属透镜内窥镜成像的算法空白，MetaScope为微米级内窥镜提供了高性能的图像处理解决方案，推动了微型内窥镜技术的发展。"}}
{"id": "2508.03337", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03337", "abs": "https://arxiv.org/abs/2508.03337", "authors": ["Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Ziyang Chen", "Weiyu Guo", "Hui Xiong"], "title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration", "comment": "Corresponding authors: Weiyu Guo, Hui Xiong", "summary": "The practical application of Multimodal Large Language Models (MLLMs) to\nVideo Question Answering (Video-QA) is severely hindered by the high token cost\nof processing numerous video frames. While increasing the number of sampled\nframes is a common strategy, we observe a \"less is more\" phenomenon where\nexcessive frames can paradoxically degrade performance due to context dilution.\nConcurrently, state-of-the-art keyframe selection methods, while effective,\nstill yield significant temporal redundancy, which we term 'visual echoes'. To\naddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel\npost-processing method that intelligently prunes the selected keyframes. AFP\nemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 and\nCLIP feature space to identify and merge these echoes into single\nrepresentatives. To compensate for information loss, we then introduce a\nlightweight, text-based semantic graph that provides critical context with\nminimal token overhead. Conducting extensive experiments on the LongVideoBench\nand VideoMME benchmarks across multiple leading MLLMs, our full approach\ndemonstrates a drastic reduction in required frames by up to 86.9% and total\ninput tokens by up to 83.2%. Crucially, by providing a concise, high-quality\nset of frames, our method not only enhances efficiency but often improves\naccuracy over baselines that use more frames. The code will be released upon\npublication.", "AI": {"tldr": "针对视频问答（Video-QA）中多模态大语言模型（MLLMs）因帧数过多导致的高昂token成本和性能下降问题，本文提出自适应帧剪枝（AFP）方法，通过智能合并冗余帧并结合轻量级语义图，大幅减少输入帧数和token，同时提升效率和准确性。", "motivation": "多模态大语言模型在视频问答中的实际应用受限于处理大量视频帧导致的高昂token成本。研究发现，过度增加帧数反而会因上下文稀释而降低性能（“少即是多”现象）。此外，现有关键帧选择方法仍存在显著的时间冗余，即“视觉回声”。", "method": "提出自适应帧剪枝（AFP）作为关键帧的后处理方法。AFP在融合的ResNet-50和CLIP特征空间上应用自适应层次聚类算法，识别并合并视觉回声为单个代表帧。为弥补信息损失，引入一个轻量级的、基于文本的语义图，以最小的token开销提供关键上下文。", "result": "在LongVideoBench和VideoMME基准测试上，该方法使所需帧数最多减少86.9%，总输入token最多减少83.2%。通过提供简洁、高质量的帧集，该方法不仅提高了效率，而且在许多情况下比使用更多帧的基线模型取得了更高的准确性。", "conclusion": "自适应帧剪枝（AFP）及其配套的语义图机制有效解决了视频问答中MLLMs面临的高token成本和上下文稀释问题。通过智能地精简视频帧并提供必要的语义补充，该方法显著提升了MLLMs在视频问答任务上的效率和准确性。"}}
{"id": "2508.03625", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03625", "abs": "https://arxiv.org/abs/2508.03625", "authors": ["Daniel DeAlcala", "Aythami Morales", "Julian Fierrez", "Ruben Tolosana"], "title": "AttZoom: Attention Zoom for Better Visual Features", "comment": "Accepted at ICCVw HiCV", "summary": "We present Attention Zoom, a modular and model-agnostic spatial attention\nmechanism designed to improve feature extraction in convolutional neural\nnetworks (CNNs). Unlike traditional attention approaches that require\narchitecture-specific integration, our method introduces a standalone layer\nthat spatially emphasizes high-importance regions in the input. We evaluated\nAttention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet,\nshowing consistent improvements in Top-1 and Top-5 classification accuracy.\nVisual analyses using Grad-CAM and spatial warping reveal that our method\nencourages fine-grained and diverse attention patterns. Our results confirm the\neffectiveness and generality of the proposed layer for improving CCNs with\nminimal architectural overhead.", "AI": {"tldr": "Attention Zoom是一种模块化、模型无关的空间注意力机制，通过强调输入中的重要区域，在多种CNN骨干网络上持续提升分类精度，且架构开销极小。", "motivation": "传统的注意力机制需要与特定网络架构深度集成，限制了其通用性。本研究旨在开发一种独立、通用的空间注意力层，以改进CNN的特征提取。", "method": "提出了一种名为Attention Zoom的独立层，该层通过空间方式强调输入中的高重要性区域。在CIFAR-100和TinyImageNet数据集上，使用多个CNN骨干网络进行了评估。同时，采用Grad-CAM和空间扭曲进行视觉分析。", "result": "在Top-1和Top-5分类准确率上均显示出持续的提升。视觉分析表明，该方法能促进细粒度和多样化的注意力模式。", "conclusion": "Attention Zoom层有效且通用，能以最小的架构开销显著改善卷积神经网络的性能。"}}
{"id": "2508.03338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03338", "abs": "https://arxiv.org/abs/2508.03338", "authors": ["Tongshun Zhang", "Pingping Liu", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement", "comment": null, "summary": "Images captured in nighttime scenes suffer from severely reduced visibility,\nhindering effective content perception. Current low-light image enhancement\n(LLIE) methods face significant challenges: data-driven end-to-end mapping\nnetworks lack interpretability or rely on unreliable prior guidance, struggling\nunder extremely dark conditions, while physics-based methods depend on\nsimplified assumptions that often fail in complex real-world scenarios. To\naddress these limitations, we propose CIVQLLIE, a novel framework that\nleverages the power of discrete representation learning through causal\nreasoning. We achieve this through Vector Quantization (VQ), which maps\ncontinuous image features to a discrete codebook of visual tokens learned from\nlarge-scale high-quality images. This codebook serves as a reliable prior,\nencoding standardized brightness and color patterns that are independent of\ndegradation. However, direct application of VQ to low-light images fails due to\ndistribution shifts between degraded inputs and the learned codebook.\nTherefore, we propose a multi-level causal intervention approach to\nsystematically correct these shifts. First, during encoding, our Pixel-level\nCausal Intervention (PCI) module intervenes to align low-level features with\nthe brightness and color distributions expected by the codebook. Second, a\nFeature-aware Causal Intervention (FCI) mechanism with Low-frequency Selective\nAttention Gating (LSAG) identifies and enhances channels most affected by\nillumination degradation, facilitating accurate codebook token matching while\nenhancing the encoder's generalization performance through flexible\nfeature-level intervention. Finally, during decoding, the High-frequency Detail\nReconstruction Module (HDRM) leverages structural information preserved in the\nmatched codebook representations to reconstruct fine details using deformable\nconvolution techniques.", "AI": {"tldr": "本文提出了一种名为CIVQLLIE的新型低光照图像增强（LLIE）框架，通过向量量化（VQ）进行离散表示学习，并结合多级因果干预来解决现有方法在极暗条件下的局限性及可解释性问题。", "motivation": "现有低光照图像增强方法存在显著挑战：数据驱动的端到端映射网络缺乏可解释性或依赖不可靠先验，在极端黑暗条件下表现不佳；基于物理的方法则依赖简化假设，在复杂真实场景中常常失效。", "method": "本文提出CIVQLLIE框架，利用向量量化（VQ）将连续图像特征映射到从高质量图像学习到的离散视觉令牌码本，该码本作为可靠先验。为解决低光照输入与码本之间的分布偏移，提出多级因果干预方法：1) 编码阶段的像素级因果干预（PCI）模块校正低级特征；2) 结合低频选择性注意力门控（LSAG）的特征感知因果干预（FCI）机制识别并增强受照度退化影响最严重的通道；3) 解码阶段的高频细节重建模块（HDRM）利用可变形卷积技术从匹配的码本表示中重建精细细节。", "result": "通过将连续图像特征转换为离散码本表示，并引入多级因果干预机制，本文提出的方法能够系统地校正低光照图像与学习码本之间的分布偏移，从而实现准确的码本令牌匹配，并有效重建高频细节，增强了编码器的泛化性能，克服了传统LLIE方法在极端黑暗和复杂场景下的局限性。", "conclusion": "CIVQLLIE框架通过结合向量量化学习到的可靠离散先验和多级因果干预策略，有效解决了低光照图像增强中存在的可见性差、可解释性不足以及对复杂场景适应性差的问题，为极端低光照条件下的图像增强提供了一种新颖且鲁棒的解决方案。"}}
{"id": "2508.03343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03343", "abs": "https://arxiv.org/abs/2508.03343", "authors": ["Junlong Ren", "Gangjian Zhang", "Honghao Fu", "Pengcheng Wu", "Hao Wang"], "title": "WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval", "comment": null, "summary": "Text-Motion Retrieval (TMR) aims to retrieve 3D motion sequences semantically\nrelevant to text descriptions. However, matching 3D motions with text remains\nhighly challenging, primarily due to the intricate structure of human body and\nits spatial-temporal dynamics. Existing approaches often overlook these\ncomplexities, relying on general encoding methods that fail to distinguish\ndifferent body parts and their dynamics, limiting precise semantic alignment.\nTo address this, we propose WaMo, a novel wavelet-based multi-frequency feature\nextraction framework. It fully captures part-specific and time-varying motion\ndetails across multiple resolutions on body joints, extracting discriminative\nmotion features to achieve fine-grained alignment with texts. WaMo has three\nkey components: (1) Trajectory Wavelet Decomposition decomposes motion signals\ninto frequency components that preserve both local kinematic details and global\nmotion semantics. (2) Trajectory Wavelet Reconstruction uses learnable inverse\nwavelet transforms to reconstruct original joint trajectories from extracted\nfeatures, ensuring the preservation of essential spatial-temporal information.\n(3) Disordered Motion Sequence Prediction reorders shuffled motion sequences to\nimprove the learning of inherent temporal coherence, enhancing motion-text\nalignment. Extensive experiments demonstrate WaMo's superiority, achieving\n17.0\\% and 18.2\\% improvements in $Rsum$ on HumanML3D and KIT-ML datasets,\nrespectively, outperforming existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "WaMo是一种基于小波的多频特征提取框架，用于解决文本-动作检索（TMR）中3D动作与文本的精细对齐挑战，通过捕获身体部位特异性和时变运动细节，显著提升了检索性能。", "motivation": "现有文本-动作检索（TMR）方法在匹配3D动作与文本时面临挑战，主要原因是人体结构的复杂性和其时空动态性，现有方法常忽略这些复杂性，无法区分不同身体部位及其动态，导致语义对齐不精确。", "method": "本文提出了WaMo框架，包含三个关键组件：1) 轨迹小波分解：将运动信号分解为频率分量，同时保留局部运动学细节和全局运动语义。2) 轨迹小波重建：使用可学习的逆小波变换从提取的特征中重建原始关节轨迹，确保保留关键时空信息。3) 乱序运动序列预测：通过重新排序打乱的运动序列，改善对内在时间连贯性的学习，从而增强运动与文本的对齐。", "result": "WaMo在HumanML3D和KIT-ML数据集上，Rsum指标分别取得了17.0%和18.2%的提升，优于现有最先进（SOTA）方法。", "conclusion": "WaMo通过有效捕获身体部位特异性和时变运动细节，解决了文本-动作检索中的挑战，实现了运动与文本的精细对齐，并显著提升了检索性能。"}}
{"id": "2508.03356", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03356", "abs": "https://arxiv.org/abs/2508.03356", "authors": ["Matteo Caligiuri", "Francesco Barbato", "Donald Shenaj", "Umberto Michieli", "Pietro Zanuttigh"], "title": "FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models", "comment": "7 pages (main document) + 12 pages (appendix), 3 figures (main) + 12\n  figures (appendix), 5 tables (main) + 6 tables (appendix), submitted to AAAI\n  2026", "summary": "Federated Learning (FL) is an established paradigm for training deep learning\nmodels on decentralized data. However, as the size of the models grows,\nconventional FL approaches often require significant computational resources on\nclient devices, which may not be feasible. We introduce FedPromo, a novel\nframework that enables efficient adaptation of large-scale foundation models\nstored on a central server to new domains encountered only by remote clients.\nInstead of directly training the large model on client devices, FedPromo\noptimizes lightweight proxy models via FL, significantly reducing computational\noverhead while maintaining privacy. Our method follows a two-stage process:\nfirst, server-side knowledge distillation aligns the representations of a\nlarge-scale foundation model (e.g., a transformer) with those of a compact\ncounterpart (e.g., a CNN). Then, the compact model encoder is deployed to\nclient devices, where trainable classifiers are learned locally. These\nclassifiers are subsequently aggregated and seamlessly transferred back to the\nfoundation model, facilitating personalized adaptation without requiring direct\naccess to user data. Through novel regularization strategies, our framework\nenables decentralized multi-domain learning, balancing performance, privacy,\nand resource efficiency. Extensive experiments on five image classification\nbenchmarks demonstrate that FedPromo outperforms existing methods while\nassuming limited-resource clients.", "AI": {"tldr": "FedPromo是一种新颖的联邦学习框架，通过在客户端优化轻量级代理模型并利用服务器端知识蒸馏，实现了在资源受限客户端上高效适应大型基础模型，同时保护隐私。", "motivation": "传统的联邦学习方法在客户端设备上训练大型深度学习模型时，需要大量的计算资源，这对于资源受限的客户端来说是不可行的。", "method": "FedPromo采用两阶段方法：1. 服务器端知识蒸馏：将大型基础模型（如Transformer）的表示与紧凑型模型（如CNN）对齐。2. 客户端联邦学习：将紧凑型模型编码器部署到客户端，本地学习可训练分类器，然后聚合并无缝传回基础模型，实现个性化适应。通过新颖的正则化策略，实现分散式多领域学习。", "result": "在五个图像分类基准测试中，FedPromo在假设客户端资源有限的情况下，表现优于现有方法。", "conclusion": "FedPromo框架在性能、隐私和资源效率之间取得了平衡，为在资源受限的联邦学习环境中适应大型基础模型提供了一种高效、私密且多领域适用的解决方案。"}}
{"id": "2508.03373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03373", "abs": "https://arxiv.org/abs/2508.03373", "authors": ["Ni Tang", "Xiaotong Luo", "Zihan Cheng", "Liangtai Zhou", "Dongxiao Zhang", "Yanyun Qu"], "title": "Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration", "comment": null, "summary": "Diffusion models have revealed powerful potential in all-in-one image\nrestoration (AiOIR), which is talented in generating abundant texture details.\nThe existing AiOIR methods either retrain a diffusion model or fine-tune the\npretrained diffusion model with extra conditional guidance. However, they often\nsuffer from high inference costs and limited adaptability to diverse\ndegradation types. In this paper, we propose an efficient AiOIR method,\nDiffusion Once and Done (DOD), which aims to achieve superior restoration\nperformance with only one-step sampling of Stable Diffusion (SD) models.\nSpecifically, multi-degradation feature modulation is first introduced to\ncapture different degradation prompts with a pretrained diffusion model. Then,\nparameter-efficient conditional low-rank adaptation integrates the prompts to\nenable the fine-tuning of the SD model for adapting to different degradation\ntypes. Besides, a high-fidelity detail enhancement module is integrated into\nthe decoder of SD to improve structural and textural details. Experiments\ndemonstrate that our method outperforms existing diffusion-based restoration\napproaches in both visual quality and inference efficiency.", "AI": {"tldr": "本文提出了一种名为DOD（Diffusion Once and Done）的高效全能图像恢复（AiOIR）方法，通过一步Stable Diffusion采样实现卓越的恢复性能，解决了现有扩散模型恢复方法推理成本高和适应性差的问题。", "motivation": "现有的基于扩散模型的全能图像恢复（AiOIR）方法通常面临高昂的推理成本和对多样退化类型适应性有限的问题。", "method": "该方法首先引入多退化特征调制来捕获不同的退化提示，然后通过参数高效的条件低秩适应（LoRA）将这些提示整合，以微调SD模型适应不同退化类型。此外，还在SD解码器中集成了一个高保真细节增强模块，以改善结构和纹理细节。最终实现一步采样。", "result": "实验证明，该方法在视觉质量和推理效率方面均优于现有基于扩散的恢复方法。", "conclusion": "DOD方法通过一步采样实现了高效且高性能的图像恢复，有效解决了现有扩散模型恢复方法的局限性。"}}
{"id": "2508.03374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03374", "abs": "https://arxiv.org/abs/2508.03374", "authors": ["Keyi Li", "Alexander Jaus", "Jens Kleesiek", "Rainer Stiefelhagen"], "title": "GRASPing Anatomy to Improve Pathology Segmentation", "comment": "Accepted at 16th MICCAI Workshop on Machine Learning in Medical\n  Imaging (MLMI2025)", "summary": "Radiologists rely on anatomical understanding to accurately delineate\npathologies, yet most current deep learning approaches use pure pattern\nrecognition and ignore the anatomical context in which pathologies develop. To\nnarrow this gap, we introduce GRASP (Guided Representation Alignment for the\nSegmentation of Pathologies), a modular plug-and-play framework that enhances\npathology segmentation models by leveraging existing anatomy segmentation\nmodels through pseudolabel integration and feature alignment. Unlike previous\napproaches that obtain anatomical knowledge via auxiliary training, GRASP\nintegrates into standard pathology optimization regimes without retraining\nanatomical components. We evaluate GRASP on two PET/CT datasets, conduct\nsystematic ablation studies, and investigate the framework's inner workings. We\nfind that GRASP consistently achieves top rankings across multiple evaluation\nmetrics and diverse architectures. The framework's dual anatomy injection\nstrategy, combining anatomical pseudo-labels as input channels with\ntransformer-guided anatomical feature fusion, effectively incorporates\nanatomical context.", "AI": {"tldr": "GRASP是一个即插即用框架，通过伪标签集成和特征对齐，利用现有解剖结构分割模型增强病理分割，无需重新训练解剖组件，从而引入解剖学上下文。", "motivation": "大多数当前深度学习方法在病理分割中仅依赖模式识别，忽略了病理发生发展的解剖学上下文，而放射科医生在病理描绘中高度依赖解剖学理解。", "method": "GRASP（病理分割引导表示对齐）是一个模块化即插即用框架。它通过以下方式增强病理分割模型：1. 伪标签集成（将解剖学伪标签作为输入通道）；2. 特征对齐（通过Transformer引导的解剖学特征融合）。该框架集成到标准病理优化方案中，无需重新训练现有的解剖学组件。", "result": "GRASP在两个PET/CT数据集上，跨多种评估指标和不同架构，始终取得顶尖排名。其双重解剖学注入策略（结合解剖学伪标签作为输入通道和Transformer引导的解剖学特征融合）有效地融入了解剖学上下文。", "conclusion": "GRASP通过其独特的双重解剖学注入策略，成功将解剖学上下文整合到病理分割模型中，提升了模型的性能，弥补了深度学习与放射科医生解剖学理解之间的差距。"}}
{"id": "2508.03375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03375", "abs": "https://arxiv.org/abs/2508.03375", "authors": ["Jingjie Wang", "Shunli Zhang", "Xiang Wei", "Senmao Tian"], "title": "GaitAdapt: Continual Learning for Evolving Gait Recognition", "comment": null, "summary": "Current gait recognition methodologies generally necessitate retraining when\nencountering new datasets. Nevertheless, retrained models frequently encounter\ndifficulties in preserving knowledge from previous datasets, leading to a\nsignificant decline in performance on earlier test sets. To tackle these\nchallenges, we present a continual gait recognition task, termed GaitAdapt,\nwhich supports the progressive enhancement of gait recognition capabilities\nover time and is systematically categorized according to various evaluation\nscenarios. Additionally, we propose GaitAdapter, a non-replay continual\nlearning approach for gait recognition. This approach integrates the\nGaitPartition Adaptive Knowledge (GPAK) module, employing graph neural networks\nto aggregate common gait patterns from current data into a repository\nconstructed from graph vectors. Subsequently, this repository is used to\nimprove the discriminability of gait features in new tasks, thereby enhancing\nthe model's ability to effectively recognize gait patterns. We also introduce a\nEuclidean Distance Stability Method (EDSN) based on negative pairs, which\nensures that newly added gait samples from different classes maintain similar\nrelative spatial distributions across both previous and current gait tasks,\nthereby alleviating the impact of task changes on the distinguishability of\noriginal domain features. Extensive evaluations demonstrate that GaitAdapter\neffectively retains gait knowledge acquired from diverse tasks, exhibiting\nmarkedly superior discriminative capability compared to alternative methods.", "AI": {"tldr": "本文提出GaitAdapt持续步态识别任务，并开发了GaitAdapter非回放持续学习方法，通过知识聚合和距离稳定性来解决新旧数据集知识遗忘问题，有效提升模型识别能力。", "motivation": "现有步态识别方法在面对新数据集时需要重新训练，但重训模型难以保留旧知识，导致在先前测试集上的性能显著下降。这促使研究人员寻求一种能持续学习并避免知识遗忘的步态识别方案。", "method": "本文定义了GaitAdapt持续步态识别任务，并提出了GaitAdapter方法。该方法包含两个核心组件：1) GaitPartition Adaptive Knowledge (GPAK) 模块，利用图神经网络将当前数据中的通用步态模式聚合到图向量构成的知识库中，以增强新任务中步态特征的判别性。2) 基于负样本的欧氏距离稳定性方法 (EDSN)，确保来自不同类别的新增步态样本在先前和当前任务中保持相似的相对空间分布，从而减轻任务变化对原始域特征可区分性的影响。", "result": "广泛评估表明，GaitAdapter能有效保留从不同任务中获取的步态知识，与现有方法相比，表现出显著优越的判别能力。", "conclusion": "GaitAdapter为持续步态识别提供了一种有效的非回放学习解决方案，成功解决了知识遗忘问题，并显著提升了模型在多任务环境下的识别性能和知识保留能力。"}}
{"id": "2508.03388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03388", "abs": "https://arxiv.org/abs/2508.03388", "authors": ["Yizhe Xiong", "Zihan Zhou", "Yiwen Liang", "Hui Chen", "Zijia Lin", "Tianxiang Hao", "Fan Zhang", "Jungong Han", "Guiguang Ding"], "title": "Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation", "comment": "9 pages, 7 figures", "summary": "Test-Time Adaptation (TTA) has emerged as an effective solution for adapting\nVision Transformers (ViT) to distribution shifts without additional training\ndata. However, existing TTA methods often incur substantial computational\noverhead, limiting their applicability in resource-constrained real-world\nscenarios. To reduce inference cost, plug-and-play token aggregation methods\nmerge redundant tokens in ViTs to reduce total processed tokens. Albeit\nefficient, it suffers from significant performance degradation when directly\nintegrated with existing TTA methods. We formalize this problem as Efficient\nTest-Time Adaptation (ETTA), seeking to preserve the adaptation capability of\nTTA while reducing inference latency. In this paper, we first provide a\ntheoretical analysis from a novel mutual information perspective, showing that\ntoken aggregation inherently leads to information loss, which cannot be fully\nmitigated by conventional norm-tuning-based TTA methods. Guided by this\ninsight, we propose to \\textbf{N}eutralize Token \\textbf{A}ggregation\n\\textbf{v}ia \\textbf{I}nformation \\textbf{A}ugmentation (\\textbf{NAVIA}).\nSpecifically, we directly augment the [CLS] token embedding and incorporate\nadaptive biases into the [CLS] token in shallow layers of ViTs. We\ntheoretically demonstrate that these augmentations, when optimized via entropy\nminimization, recover the information lost due to token aggregation. Extensive\nexperiments across various out-of-distribution benchmarks demonstrate that\nNAVIA significantly outperforms state-of-the-art methods by over 2.5\\%, while\nachieving an inference latency reduction of more than 20\\%, effectively\naddressing the ETTA challenge.", "AI": {"tldr": "本文提出NAVIA，一种高效的测试时间适应（ETTA）方法，通过信息增强来弥补ViT中令牌聚合导致的信息损失，显著提升性能并降低推理延迟。", "motivation": "现有的ViT测试时间适应（TTA）方法计算开销大，不适用于资源受限场景。令牌聚合虽能降低推理成本，但与现有TTA结合时会造成显著的性能下降，主要原因是信息丢失。因此，需要一种在降低延迟的同时保持TTA适应能力的方法。", "method": "本文将问题形式化为高效测试时间适应（ETTA）。首先，从互信息角度理论分析令牌聚合导致信息损失，且传统TTA难以弥补。受此启发，提出NAVIA（通过信息增强中和令牌聚合）。具体地，NAVIA直接增强[CLS]令牌嵌入，并在ViT浅层为[CLS]令牌引入自适应偏差。理论证明，这些增强在通过熵最小化优化时，能恢复因令牌聚合而丢失的信息。", "result": "在各种分布外基准测试中，NAVIA的性能超越现有最先进方法超过2.5%，同时推理延迟降低超过20%。", "conclusion": "NAVIA有效解决了ETTA挑战，在保持TTA适应能力的同时显著降低了推理延迟，实现了性能与效率的平衡。"}}
{"id": "2508.03397", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.03397", "abs": "https://arxiv.org/abs/2508.03397", "authors": ["Xinzhu Li", "Juepeng Zheng", "Yikun Chen", "Xudong Mao", "Guanghui Yue", "Wei Zhou", "Chenlei Lv", "Ruomei Wang", "Fan Zhou", "Baoquan Zhao"], "title": "DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition", "comment": null, "summary": "Robust gait recognition requires highly discriminative representations, which\nare closely tied to input modalities. While binary silhouettes and skeletons\nhave dominated recent literature, these 2D representations fall short of\ncapturing sufficient cues that can be exploited to handle viewpoint variations,\nand capture finer and meaningful details of gait. In this paper, we introduce a\nnovel framework, termed DepthGait, that incorporates RGB-derived depth maps and\nsilhouettes for enhanced gait recognition. Specifically, apart from the 2D\nsilhouette representation of the human body, the proposed pipeline explicitly\nestimates depth maps from a given RGB image sequence and uses them as a new\nmodality to capture discriminative features inherent in human locomotion. In\naddition, a novel multi-scale and cross-level fusion scheme has also been\ndeveloped to bridge the modality gap between depth maps and silhouettes.\nExtensive experiments on standard benchmarks demonstrate that the proposed\nDepthGait achieves state-of-the-art performance compared to peer methods and\nattains an impressive mean rank-1 accuracy on the challenging datasets.", "AI": {"tldr": "提出了一种名为DepthGait的新框架，通过结合RGB图像导出的深度图和剪影来增强步态识别，并设计了多尺度跨层融合方案。", "motivation": "现有的2D表示（如二值剪影和骨架）在处理视角变化和捕获步态的精细细节方面存在不足，无法提供足够的判别性特征。", "method": "引入DepthGait框架，利用RGB图像序列显式估计深度图作为新的模态，并结合传统剪影表示。同时，开发了一种新颖的多尺度和跨层融合方案，以弥合深度图和剪影之间的模态差距。", "result": "在标准基准测试中，所提出的DepthGait方法与同类方法相比达到了最先进的性能，并在挑战性数据集上取得了令人印象深刻的平均Rank-1准确率。", "conclusion": "结合深度图和剪影，并通过专门设计的融合方案，能够显著提升步态识别的性能，有效应对视角变化并捕获更丰富的步态信息。"}}
{"id": "2508.03441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03441", "abs": "https://arxiv.org/abs/2508.03441", "authors": ["Ning Zhu", "Xiaochuan Ma", "Shaoting Zhang", "Guotai Wang"], "title": "MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis", "comment": "23 pages, 6 figures, 10 tables", "summary": "Cold-Start Active Learning (CSAL) aims to select informative samples for\nannotation without prior knowledge, which is important for improving annotation\nefficiency and model performance under a limited annotation budget in medical\nimage analysis. Most existing CSAL methods rely on Self-Supervised Learning\n(SSL) on the target dataset for feature extraction, which is inefficient and\nlimited by insufficient feature representation. Recently, pre-trained\nFoundation Models (FMs) have shown powerful feature extraction ability with a\npotential for better CSAL. However, this paradigm has been rarely investigated,\nwith a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we\npropose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical\nimage analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets\nunder different annotation budgets, covering classification and segmentation\ntasks from diverse medical modalities. It is also the first CSAL benchmark that\nevaluates both the feature extraction and sample selection stages. Our\nexperimental results reveal that: 1) Most FMs are effective feature extractors\nfor CSAL, with DINO family performing the best in segmentation; 2) The\nperformance differences of these FMs are large in segmentation tasks, while\nsmall for classification; 3) Different sample selection strategies should be\nconsidered in CSAL on different datasets, with Active Learning by Processing\nSurprisal (ALPS) performing the best in segmentation while RepDiv leading for\nclassification. The code is available at\nhttps://github.com/HiLab-git/MedCAL-Bench.", "AI": {"tldr": "该论文提出了MedCAL-Bench，首个针对医学图像分析中基于基础模型（FM）的冷启动主动学习（CSAL）的系统性基准，评估了多种FM和CSAL策略，并揭示了它们在不同任务和数据集上的性能表现。", "motivation": "现有CSAL方法多依赖于自监督学习（SSL）进行特征提取，效率低下且特征表示有限。尽管预训练的基础模型（FM）在特征提取方面表现出色，但其在CSAL中的应用鲜有研究，且缺乏相应的比较基准。", "method": "提出了MedCAL-Bench基准，系统性地评估了14个基础模型和7种CSAL策略，涵盖7个医学图像数据集（包括分类和分割任务，来自不同医学模态），并在不同的标注预算下进行测试。该基准首次同时评估了特征提取和样本选择阶段。", "result": "实验结果显示：1) 大多数基础模型是有效的CSAL特征提取器，其中DINO系列在分割任务中表现最佳；2) 这些基础模型在分割任务中的性能差异较大，但在分类任务中差异较小；3) 在不同数据集上，CSAL应考虑不同的样本选择策略，其中ALPS在分割任务中表现最佳，而RepDiv在分类任务中表现领先。", "conclusion": "基础模型在医学图像分析的冷启动主动学习中展现出强大的潜力。本研究通过MedCAL-Bench基准，为FM在CSAL任务中的特征提取和样本选择提供了深入的性能洞察，并为未来的研究提供了指导。"}}
{"id": "2508.03442", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03442", "abs": "https://arxiv.org/abs/2508.03442", "authors": ["Shangwen Zhu", "Qianyu Peng", "Yuting Hu", "Zhantao Yang", "Han Zhang", "Zhao Pu", "Ruili Feng", "Fan Cheng"], "title": "RAAG: Ratio Aware Adaptive Guidance", "comment": null, "summary": "Flow-based generative models have recently achieved remarkable progress in\nimage and video synthesis, with classifier-free guidance (CFG) becoming the\nstandard tool for high-fidelity, controllable generation. However, despite\ntheir practical success, little is known about how guidance interacts with\ndifferent stages of the sampling process-especially in the fast, low-step\nregimes typical of modern flow-based pipelines. In this work, we uncover and\nanalyze a fundamental instability: the earliest reverse steps are acutely\nsensitive to the guidance scale, owing to a pronounced spike in the relative\nstrength (RATIO) of conditional to unconditional predictions. Through rigorous\ntheoretical analysis and empirical validation, we show that this RATIO spike is\nintrinsic to the data distribution, independent of the model architecture, and\ncauses exponential error amplification when paired with strong guidance. To\naddress this, we propose a simple, theoretically grounded, RATIO-aware adaptive\nguidance schedule that automatically dampens the guidance scale at early steps\nbased on the evolving RATIO, using a closed-form exponential decay. Our method\nis lightweight, requires no additional inference overhead, and is compatible\nwith standard flow frameworks. Experiments across state-of-the-art image\n(SD3.5, Lumina) and video (WAN2.1) models demonstrate that our approach enables\nup to 3x faster sampling while maintaining or improving generation quality,\nrobustness, and semantic alignment. Extensive ablation studies further confirm\nthe generality and stability of our schedule across models, datasets, and\nhyperparameters. Our findings highlight the critical role of stepwise guidance\nadaptation in unlocking the full potential of fast flow-based generative\nmodels.", "AI": {"tldr": "本文揭示了流生成模型在早期采样步骤中存在的指导不稳定问题，该问题源于条件与非条件预测的相对强度（RATIO）峰值。作者提出了一种RATIO感知的自适应指导策略，通过在早期步骤中衰减指导尺度来解决此问题，从而在保持或提高生成质量的同时，实现高达3倍的采样速度提升。", "motivation": "尽管流生成模型在图像和视频合成中取得了显著进展，且分类器无关指导（CFG）已成为高保真、可控生成的标准工具，但人们对其在采样过程中，特别是在现代流管道典型的快速、低步长方案中，如何与不同阶段交互知之甚少。研究发现，在早期反向步骤中存在一个根本性不稳定性，即对指导尺度的敏感性。", "method": "本文揭示并分析了早期反向步骤中条件预测与非条件预测相对强度（RATIO）的显著峰值，指出这是数据分布固有的，与模型架构无关，并在与强指导结合时导致指数级误差放大。为解决此问题，提出了一种简单、理论上合理且RATIO感知的自适应指导策略，该策略根据不断演变的RATIO，通过封闭形式的指数衰减，自动抑制早期步骤的指导尺度。该方法轻量级，无需额外推理开销，并兼容标准流框架。", "result": "实验证明，所提出的方法在最先进的图像（SD3.5, Lumina）和视频（WAN2.1）模型上，实现了高达3倍的采样速度提升，同时保持或提高了生成质量、鲁棒性和语义对齐。广泛的消融研究进一步证实了该策略在不同模型、数据集和超参数下的通用性和稳定性。", "conclusion": "研究结果强调了分步指导自适应在充分发挥快速流生成模型潜力中的关键作用。"}}
{"id": "2508.03447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03447", "abs": "https://arxiv.org/abs/2508.03447", "authors": ["Qiyu Chen", "Zhen Qu", "Wei Luo", "Haiming Yao", "Yunkang Cao", "Yuxin Jiang", "Yinan Duan", "Huiyuan Luo", "Chengkan Lv", "Zhengtao Zhang"], "title": "CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection", "comment": "19 pages, 33 figures, 14 tables", "summary": "Recently, large pre-trained vision-language models have shown remarkable\nperformance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single\nauxiliary dataset, the model enables cross-category anomaly detection on\ndiverse datasets covering industrial defects and medical lesions. Compared to\nmanually designed prompts, prompt learning eliminates the need for expert\nknowledge and trial-and-error. However, it still faces the following\nchallenges: (i) static learnable tokens struggle to capture the continuous and\ndiverse patterns of normal and anomalous states, limiting generalization to\nunseen categories; (ii) fixed textual labels provide overly sparse category\ninformation, making the model prone to overfitting to a specific semantic\nsubspace. To address these issues, we propose Conditional Prompt Synthesis\n(CoPS), a novel framework that synthesizes dynamic prompts conditioned on\nvisual features to enhance ZSAD performance. Specifically, we extract\nrepresentative normal and anomaly prototypes from fine-grained patch features\nand explicitly inject them into prompts, enabling adaptive state modeling.\nGiven the sparsity of class labels, we leverage a variational autoencoder to\nmodel semantic image features and implicitly fuse varied class tokens into\nprompts. Additionally, integrated with our spatially-aware alignment mechanism,\nextensive experiments demonstrate that CoPS surpasses state-of-the-art methods\nby 2.5% AUROC in both classification and segmentation across 13 industrial and\nmedical datasets. Code will be available at https://github.com/cqylunlun/CoPS.", "AI": {"tldr": "本文提出了一种名为条件提示合成（CoPS）的新框架，通过合成动态提示并融入视觉特征，显著提升了零样本异常检测（ZSAD）的性能，解决了现有方法中静态提示和稀疏标签导致的泛化性差和过拟合问题。", "motivation": "现有零样本异常检测（ZSAD）中的提示学习面临挑战：(i) 静态可学习标记难以捕捉正常和异常状态的连续多样模式，限制了对未见类别的泛化能力；(ii) 固定文本标签提供过于稀疏的类别信息，使模型容易过拟合到特定语义子空间。", "method": "本文提出条件提示合成（CoPS）框架，通过以下方式合成动态提示：(i) 从细粒度补丁特征中提取代表性的正常和异常原型并明确注入提示，实现自适应状态建模；(ii) 利用变分自编码器（VAE）对语义图像特征进行建模，并将多样的类别标记隐式融合到提示中，以解决类别标签稀疏性问题；(iii) 整合空间感知对齐机制。", "result": "CoPS在13个工业和医学数据集的分类和分割任务中，AUROC指标均超越现有最先进方法2.5%。", "conclusion": "CoPS通过合成动态的、视觉特征条件化的提示，有效解决了零样本异常检测中提示学习的泛化性和过拟合问题，在多种数据集上实现了卓越的异常检测性能。"}}
{"id": "2508.03449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03449", "abs": "https://arxiv.org/abs/2508.03449", "authors": ["Xuan Dong", "Xiangyuan Sun", "Xia Wang", "Jian Song", "Ya Li", "Weixin Li"], "title": "Video Demoireing using Focused-Defocused Dual-Camera System", "comment": null, "summary": "Moire patterns, unwanted color artifacts in images and videos, arise from the\ninterference between spatially high-frequency scene contents and the spatial\ndiscrete sampling of digital cameras. Existing demoireing methods primarily\nrely on single-camera image/video processing, which faces two critical\nchallenges: 1) distinguishing moire patterns from visually similar real\ntextures, and 2) preserving tonal consistency and temporal coherence while\nremoving moire artifacts. To address these issues, we propose a dual-camera\nframework that captures synchronized videos of the same scene: one in focus\n(retaining high-quality textures but may exhibit moire patterns) and one\ndefocused (with significantly reduced moire patterns but blurred textures). We\nuse the defocused video to help distinguish moire patterns from real texture,\nso as to guide the demoireing of the focused video. We propose a frame-wise\ndemoireing pipeline, which begins with an optical flow based alignment step to\naddress any discrepancies in displacement and occlusion between the focused and\ndefocused frames. Then, we leverage the aligned defocused frame to guide the\ndemoireing of the focused frame using a multi-scale CNN and a multi-dimensional\ntraining loss. To maintain tonal and temporal consistency, our final step\ninvolves a joint bilateral filter to leverage the demoireing result from the\nCNN as the guide to filter the input focused frame to obtain the final output.\nExperimental results demonstrate that our proposed framework largely\noutperforms state-of-the-art image and video demoireing methods.", "AI": {"tldr": "本文提出一种双摄像头去摩尔纹框架，利用散焦视频辅助聚焦视频的去摩尔纹，以解决区分摩尔纹与真实纹理以及保持色调和时间一致性的挑战。", "motivation": "现有单摄像头去摩尔纹方法面临两大挑战：1) 难以区分摩尔纹与视觉上相似的真实纹理；2) 在去除摩尔纹伪影的同时难以保持色调一致性和时间连贯性。", "method": "研究者提出一个双摄像头框架，同步捕捉同一场景的视频：一个聚焦视频（保留高质量纹理但可能有摩尔纹）和一个散焦视频（摩尔纹显著减少但纹理模糊）。散焦视频用于帮助区分摩尔纹和真实纹理，以指导聚焦视频的去摩尔纹。去摩尔纹流程是逐帧进行的，首先通过光流对齐聚焦和散焦帧；然后，利用对齐后的散焦帧，通过多尺度CNN和多维度训练损失来指导聚焦帧的去摩尔纹；最后，使用联合双边滤波器，以CNN的去摩尔纹结果为指导，对输入聚焦帧进行滤波，以保持色调和时间一致性。", "result": "实验结果表明，所提出的框架在很大程度上优于现有的图像和视频去摩尔纹方法。", "conclusion": "双摄像头框架通过利用散焦视频的辅助，有效解决了去摩尔纹中区分摩尔纹和真实纹理的难题，并在保持色调和时间一致性方面表现出色，取得了领先的去摩尔纹效果。"}}
{"id": "2508.03457", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03457", "abs": "https://arxiv.org/abs/2508.03457", "authors": ["Haotian Wang", "Yuzhe Weng", "Jun Du", "Haoran Xu", "Xiaoyan Wu", "Shan He", "Bing Yin", "Cong Liu", "Jianqing Gao", "Qingfeng Liu"], "title": "READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation", "comment": "9 pages", "summary": "The introduction of diffusion models has brought significant advances to the\nfield of audio-driven talking head generation. However, the extremely slow\ninference speed severely limits the practical implementation of diffusion-based\ntalking head generation models. In this study, we propose READ, the first\nreal-time diffusion-transformer-based talking head generation framework. Our\napproach first learns a spatiotemporal highly compressed video latent space via\na temporal VAE, significantly reducing the token count to accelerate\ngeneration. To achieve better audio-visual alignment within this compressed\nlatent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to\ngenerate temporally compressed speech latent codes corresponding to the video\nlatent space. These latent representations are then modeled by a carefully\ndesigned Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient\ntalking head synthesis. Furthermore, to ensure temporal consistency and\naccelerated inference in extended generation, we propose a novel asynchronous\nnoise scheduler (ANS) for both the training and inference process of our\nframework. The ANS leverages asynchronous add-noise and asynchronous\nmotion-guided generation in the latent space, ensuring consistency in generated\nvideo clips. Experimental results demonstrate that READ outperforms\nstate-of-the-art methods by generating competitive talking head videos with\nsignificantly reduced runtime, achieving an optimal balance between quality and\nspeed while maintaining robust metric stability in long-time generation.", "AI": {"tldr": "本文提出了READ，首个基于扩散-Transformer的实时音视频驱动说话人头部生成框架，通过压缩潜在空间和异步噪声调度器显著提升了推理速度和生成质量。", "motivation": "现有的扩散模型在音视频驱动说话人头部生成方面取得了显著进展，但其极慢的推理速度严重限制了实际应用。", "method": "1. 通过时序VAE学习时空高度压缩的视频潜在空间，以减少token数量并加速生成。2. 提出预训练的语音自编码器（SpeechAE）生成与视频潜在空间对应的时序压缩语音潜在编码。3. 设计了音频到视频扩散Transformer（A2V-DiT）骨干网络来建模这些潜在表示。4. 提出了一种新颖的异步噪声调度器（ANS），用于训练和推理过程，通过异步加噪和异步运动引导生成确保视频片段的时序一致性并加速推理。", "result": "实验结果表明，READ在生成具有竞争力的说话人头部视频方面优于现有最先进的方法，显著缩短了运行时间，在质量和速度之间取得了最佳平衡，并能长时间生成并保持鲁棒的度量稳定性。", "conclusion": "READ成功实现了实时音视频驱动说话人头部生成，在保持高质量的同时显著提升了推理速度，为该领域的实际应用提供了可行方案。"}}
{"id": "2508.03458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03458", "abs": "https://arxiv.org/abs/2508.03458", "authors": ["Zilin Chen", "Shengnan Lu"], "title": "AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection", "comment": null, "summary": "Accurate detection of polyps is of critical importance for the early and\nintermediate stages of colorectal cancer diagnosis. Compared to static images,\ndynamic colonoscopy videos provide more comprehensive visual information, which\ncan facilitate the development of effective treatment plans. However, unlike\nfixed-camera recordings, colonoscopy videos often exhibit rapid camera\nmovement, introducing substantial background noise that disrupts the structural\nintegrity of the scene and increases the risk of false positives. To address\nthese challenges, we propose the Adaptive Video Polyp Detection Network\n(AVPDN), a robust framework for multi-scale polyp detection in colonoscopy\nvideos. AVPDN incorporates two key components: the Adaptive Feature Interaction\nand Augmentation (AFIA) module and the Scale-Aware Context Integration (SACI)\nmodule. The AFIA module adopts a triple-branch architecture to enhance feature\nrepresentation. It employs dense self-attention for global context modeling,\nsparse self-attention to mitigate the influence of low query-key similarity in\nfeature aggregation, and channel shuffle operations to facilitate inter-branch\ninformation exchange. In parallel, the SACI module is designed to strengthen\nmulti-scale feature integration. It utilizes dilated convolutions with varying\nreceptive fields to capture contextual information at multiple spatial scales,\nthereby improving the model's denoising capability. Experiments conducted on\nseveral challenging public benchmarks demonstrate the effectiveness and\ngeneralization ability of the proposed method, achieving competitive\nperformance in video-based polyp detection tasks.", "AI": {"tldr": "提出了一种名为AVPDN的自适应视频息肉检测网络，通过特征交互增强和尺度感知上下文集成模块，有效解决了结肠镜视频中快速相机移动导致的背景噪声和误报问题，提高了息肉检测的准确性和泛化能力。", "motivation": "结直肠癌的早期和中期诊断中，息肉的准确检测至关重要。动态结肠镜视频提供更全面的视觉信息，但快速的相机移动引入大量背景噪声，破坏场景结构，增加误报风险。现有方法难以有效处理这些挑战。", "method": "提出了自适应视频息肉检测网络（AVPDN），包含两个核心组件：自适应特征交互与增强（AFIA）模块和尺度感知上下文集成（SACI）模块。AFIA模块采用三分支架构，通过密集自注意力进行全局上下文建模，稀疏自注意力减轻低查询-键相似性影响，以及通道混洗操作促进分支间信息交换，以增强特征表示。SACI模块利用不同感受野的扩张卷积捕获多尺度上下文信息，增强模型去噪能力，从而加强多尺度特征集成。", "result": "在多个具有挑战性的公共基准数据集上进行的实验证明，所提出的方法具有有效性和泛化能力，在基于视频的息肉检测任务中取得了具有竞争力的性能。", "conclusion": "AVPDN是一个鲁棒的框架，能够有效应对结肠镜视频中快速相机移动带来的挑战，实现了多尺度息肉的准确检测，为结直肠癌的早期诊断提供了有力的技术支持。"}}
{"id": "2508.03469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03469", "abs": "https://arxiv.org/abs/2508.03469", "authors": ["Jiabing Yang", "Chenhang Cui", "Yiyang Zhou", "Yixiang Chen", "Peng Xia", "Ying Wei", "Tao Yu", "Yan Huang", "Liang Wang"], "title": "IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models", "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nsignificant progress across multiple domains. However, these models still face\nthe inherent challenge of integrating vision and language for collaborative\ninference, which often leads to \"hallucinations\", outputs that are not grounded\nin the corresponding images. Many efforts have been made to address these\nissues, but each comes with its own limitations, such as high computational\ncost or expensive dataset annotation. Recent research shows that LVLMs exhibit\na long-term bias where hallucinations increase as the sequence length grows,\nyet the underlying cause remains poorly understood. Building on extensive\nresearch into attention mechanisms in LVLMs, we analyze the relationship\nbetween this long-term bias and visual attention. In our research, we identify\na consistent phenomenon in current LVLMs: the model's attention to visual input\ndiminishes as the generated sequence grows, which we hypothesize to be a key\nfactor contributing to observed increasing hallucinations. Based on these\ninsights, we propose Image attention-guided Key-value merging cOllaborative\nDecoding (IKOD), a collaborative decoding strategy generating more\nimage-focused sequences. This method derives logits from shorter sequences with\nhigher image attention through key-value merging and combines them with those\nfrom the original decoding, effectively mitigating attention degradation and\nsuppressing hallucinations while not incurring too much inference cost.\nExtensive experiments on both hallucination and comprehensive benchmarks\ndemonstrate IKOD's superior effectiveness in mitigating hallucinations and\nimproving comprehensive capacities for LVLMs. Importantly, IKOD requires no\nadditional training or external tools, making it a lightweight and efficient\nframework applicable to various models.", "AI": {"tldr": "本文研究了大型视觉-语言模型（LVLMs）中幻觉（hallucinations）随序列长度增加而加剧的现象，发现这是由于模型对视觉输入的注意力随生成序列增长而减弱所致。作者提出了IKOD（Image attention-guided Key-value merging cOllaborative Decoding）方法，通过键值合并（key-value merging）和协同解码来维持视觉注意力，有效抑制幻觉且成本较低。", "motivation": "LVLMs在视觉和语言整合时常出现与图像不符的“幻觉”输出。现有解决幻觉的方法存在计算成本高或数据标注昂贵等局限性。研究发现LVLMs存在幻觉随序列长度增加而加剧的长期偏差，但其根本原因尚不清楚，这促使作者深入探究其与视觉注意力的关系。", "method": "通过对LVLMs中注意力机制的广泛研究，作者分析了长期偏差与视觉注意力的关系。他们发现，随着生成序列的增长，模型对视觉输入的注意力会持续减弱。基于此洞察，提出IKOD协同解码策略。该方法通过键值合并从具有更高图像注意力的短序列中获取逻辑值（logits），并将其与原始解码的逻辑值结合，从而有效缓解注意力衰减并抑制幻觉。", "result": "在幻觉和综合基准测试中，IKOD表现出卓越的有效性，能够显著减轻幻觉并提升LVLMs的综合能力。重要的是，IKOD无需额外训练或外部工具，是一个轻量且高效的框架，可适用于多种模型。", "conclusion": "IKOD通过解决LVLMs中视觉注意力随序列增长而衰减的问题，提供了一种有效且低成本的方法来抑制幻觉并提升模型性能，无需额外的训练或工具，使其成为一个有前景的通用框架。"}}
{"id": "2508.03485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03485", "abs": "https://arxiv.org/abs/2508.03485", "authors": ["Lianwei Yang", "Haokun Lin", "Tianchen Zhao", "Yichen Wu", "Hongyu Zhu", "Ruiqi Xie", "Zhenan Sun", "Yu Wang", "Qingyi Gu"], "title": "LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation", "comment": null, "summary": "Diffusion Transformers (DiTs) have achieved impressive performance in\ntext-to-image generation. However, their high computational cost and large\nparameter sizes pose significant challenges for usage in resource-constrained\nscenarios. Post-training quantization (PTQ) is a promising solution to reduce\nmemory usage and accelerate inference, but existing PTQ methods suffer from\nsevere performance degradation under extreme low-bit settings. We identify two\nkey obstacles to low-bit post-training quantization for DiT models: (1) model\nweights follow a Gaussian-like distribution with long tails, causing uniform\nquantization to poorly allocate intervals and leading to significant errors;\n(2) two types of activation outliers: (i) Mild Outliers with slightly elevated\nvalues, and (ii) Salient Outliers with large magnitudes concentrated in\nspecific channels, which disrupt activation quantization. To address these\nissues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We\nintroduce Twin-Log Quantization (TLQ), a log-based method that aligns well with\nthe weight distribution and reduces quantization errors. We also propose an\nAdaptive Rotation Scheme (ARS) that dynamically applies Hadamard or\noutlier-aware rotations based on activation fluctuation, effectively mitigating\nthe impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX\nunder various bit-width settings, and validate the performance on COCO, MJHQ,\nand sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while\npreserving image quality, outperforming existing PTQ baselines.", "AI": {"tldr": "针对Diffusion Transformers (DiTs)模型在极低比特下的后训练量化(PTQ)性能下降问题，本文提出了LRQ-DiT框架，通过双对数量化(TLQ)解决权重长尾分布，并引入自适应旋转方案(ARS)处理激活异常值，实现了DiT模型的低比特量化且保持图像质量，优于现有基线方法。", "motivation": "DiT模型在文本到图像生成中表现出色，但计算成本高、参数量大，不适用于资源受限场景。PTQ是降低内存和加速推理的有效方案，但在极低比特设置下现有PTQ方法性能严重下降。主要障碍是：1) 模型权重呈高斯状长尾分布，导致均匀量化误差大；2) 激活存在两种异常值（轻微和显著），干扰激活量化。", "method": "提出LRQ-DiT高效准确的PTQ框架。引入双对数量化(Twin-Log Quantization, TLQ)作为基于对数的权重处理方法，更好地匹配权重分布并减少量化误差。提出自适应旋转方案(Adaptive Rotation Scheme, ARS)，根据激活波动动态应用Hadamard或异常值感知旋转，有效缓解两种激活异常值的影响。", "result": "在PixArt和FLUX模型上，以及COCO、MJHQ和sDCI数据集上进行了评估。LRQ-DiT实现了DiT模型的低比特量化，同时保持了图像质量，性能优于现有的PTQ基线方法。", "conclusion": "LRQ-DiT是一个有效且准确的DiT模型PTQ框架，它通过创新的量化和异常值处理策略，成功解决了DiT模型在极低比特量化中的挑战，实现了性能与效率的平衡。"}}
{"id": "2508.03490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03490", "abs": "https://arxiv.org/abs/2508.03490", "authors": ["Yu Zhou", "Pelle Thielmann", "Ayush Chamoli", "Bruno Mirbach", "Didier Stricker", "Jason Rambach"], "title": "ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes", "comment": "12 pages, 4 figures. Accepted for presentation at EUSIPCO 2025,\n  September 8-12, 2025. List of accepted papers available at\n  http://cmsworkshops.com/EUSIPCO2025/papers/accepted_papers.php", "summary": "The construction industry represents a major sector in terms of resource\nconsumption. Recycled construction material has high reuse potential, but\nquality monitoring of the aggregates is typically still performed with manual\nmethods. Vision-based machine learning methods could offer a faster and more\nefficient solution to this problem, but existing segmentation methods are by\ndesign not directly applicable to images with hundreds of small particles. In\nthis paper, we propose ParticleSAM, an adaptation of the segmentation\nfoundation model to images with small and dense objects such as the ones often\nencountered in construction material particles. Moreover, we create a new dense\nmulti-particle dataset simulated from isolated particle images with the\nassistance of an automated data generation and labeling pipeline. This dataset\nserves as a benchmark for visual material quality control automation while our\nsegmentation approach has the potential to be valuable in application areas\nbeyond construction where small-particle segmentation is needed. Our\nexperimental results validate the advantages of our method by comparing to the\noriginal SAM method both in quantitative and qualitative experiments.", "AI": {"tldr": "该论文提出了ParticleSAM，一个针对建筑材料中密集小颗粒图像的分割基础模型SAM的改进版本，并创建了一个新的密集多颗粒数据集，用于自动化视觉材料质量控制。", "motivation": "建筑行业资源消耗巨大，回收材料潜力高，但骨料质量监测仍依赖人工方法。现有视觉分割方法不适用于含有数百个小颗粒的图像，因此需要更快、更高效的解决方案。", "method": "提出ParticleSAM，将分割基础模型SAM适应于小而密集的物体图像。同时，通过自动化数据生成和标注流程，从孤立颗粒图像中模拟创建了一个新的密集多颗粒数据集。", "result": "实验结果表明，与原始SAM方法相比，ParticleSAM在定量和定性实验中都展现出优势。", "conclusion": "ParticleSAM方法在建筑材料质量控制自动化方面具有潜在价值，并可应用于其他需要小颗粒分割的领域。新创建的数据集可作为视觉材料质量控制自动化的基准。"}}
{"id": "2508.03492", "categories": ["cs.CV", "65K05, 68T30", "I.4.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.03492", "abs": "https://arxiv.org/abs/2508.03492", "authors": ["Mohammadsadegh Khoshghiaferezaee", "Moritz Krauth", "Shima Shabani", "Michael Breuß"], "title": "Quality Versus Sparsity in Image Recovery by Dictionary Learning Using Iterative Shrinkage", "comment": "6 pages, 4 figures, 3 tables, IEEE-IPTA,2025", "summary": "Sparse dictionary learning (SDL) is a fundamental technique that is useful\nfor many image processing tasks. As an example we consider here image recovery,\nwhere SDL can be cast as a nonsmooth optimization problem. For this kind of\nproblems, iterative shrinkage methods represent a powerful class of algorithms\nthat are subject of ongoing research. Sparsity is an important property of the\nlearned solutions, as exactly the sparsity enables efficient further processing\nor storage. The sparsity implies that a recovered image is determined as a\ncombination of a number of dictionary elements that is as low as possible.\nTherefore, the question arises, to which degree sparsity should be enforced in\nSDL in order to not compromise recovery quality. In this paper we focus on the\nsparsity of solutions that can be obtained using a variety of optimization\nmethods. It turns out that there are different sparsity regimes depending on\nthe method in use. Furthermore, we illustrate that high sparsity does in\ngeneral not compromise recovery quality, even if the recovered image is quite\ndifferent from the learning database.", "AI": {"tldr": "该研究探讨了稀疏字典学习（SDL）在图像恢复中的应用，重点分析了不同优化方法下解的稀疏度，并指出高稀疏度通常不会损害恢复质量。", "motivation": "稀疏字典学习（SDL）是图像处理（如图像恢复）中的基础技术，可被视为非光滑优化问题。迭代收缩方法是解决这类问题的有力算法。稀疏性对于高效处理或存储至关重要，但问题在于SDL中应强制执行何种程度的稀疏性才不会损害恢复质量。", "method": "本文关注使用多种优化方法获得的解的稀疏性，并分析了不同方法如何影响稀疏性。", "result": "研究发现，存在不同的稀疏度方案，这取决于所使用的优化方法。此外，高稀疏度通常不会损害恢复质量，即使恢复的图像与学习数据库差异很大。", "conclusion": "在稀疏字典学习中，可以通过多种优化方法实现不同的稀疏度，并且高稀疏度通常不会影响图像恢复的质量。"}}
{"id": "2508.03494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03494", "abs": "https://arxiv.org/abs/2508.03494", "authors": ["Shreyank N Gowda", "Xiaobo Jin", "Christian Wagner"], "title": "Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval", "comment": null, "summary": "In cross-modal retrieval tasks, such as image-to-report and report-to-image\nretrieval, accurately aligning medical images with relevant text reports is\nessential but challenging due to the inherent ambiguity and variability in\nmedical data. Existing models often struggle to capture the nuanced,\nmulti-level semantic relationships in radiology data, leading to unreliable\nretrieval results. To address these issues, we propose the Prototype-Enhanced\nConfidence Modeling (PECM) framework, which introduces multi-level prototypes\nfor each modality to better capture semantic variability and enhance retrieval\nrobustness. PECM employs a dual-stream confidence estimation that leverages\nprototype similarity distributions and an adaptive weighting mechanism to\ncontrol the impact of high-uncertainty data on retrieval rankings. Applied to\nradiology image-report datasets, our method achieves significant improvements\nin retrieval precision and consistency, effectively handling data ambiguity and\nadvancing reliability in complex clinical scenarios. We report results on\nmultiple different datasets and tasks including fully supervised and zero-shot\nretrieval obtaining performance gains of up to 10.17%, establishing in new\nstate-of-the-art.", "AI": {"tldr": "该论文提出了原型增强置信度建模（PECM）框架，用于解决医学图像与报告的跨模态检索中数据歧义性和变异性问题，通过引入多级原型和双流置信度估计，显著提高了检索精度和一致性。", "motivation": "在医学跨模态检索任务中，准确对齐医学图像和文本报告至关重要，但由于医学数据固有的歧义性和变异性，现有模型难以捕捉放射学数据中细致的多级语义关系，导致检索结果不可靠。", "method": "本文提出了原型增强置信度建模（PECM）框架。该框架为每种模态引入了多级原型，以更好地捕捉语义变异性并增强检索鲁棒性。PECM采用双流置信度估计，利用原型相似性分布和自适应加权机制来控制高不确定性数据对检索排名的影响。", "result": "将所提出的方法应用于放射学图像-报告数据集，PECM在检索精度和一致性方面取得了显著改进，有效处理了数据歧义性，并提升了复杂临床场景中的可靠性。在多个不同数据集和任务（包括全监督和零样本检索）上，性能提升高达10.17%，建立了新的最先进水平。", "conclusion": "PECM框架通过有效处理医学数据的歧义性和变异性，显著提高了跨模态检索的精度和可靠性，在复杂临床场景中表现出色，并达到了新的最先进性能。"}}
{"id": "2508.03497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03497", "abs": "https://arxiv.org/abs/2508.03497", "authors": ["Deqiang Yin", "Junyi Guo", "Huanda Lu", "Fangyu Wu", "Dongming Lu"], "title": "EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation", "comment": null, "summary": "Instruction-based garment editing enables precise image modifications via\nnatural language, with broad applications in fashion design and customization.\nUnlike general editing tasks, it requires understanding garment-specific\nsemantics and attribute dependencies. However, progress is limited by the\nscarcity of high-quality instruction-image pairs, as manual annotation is\ncostly and hard to scale. While MLLMs have shown promise in automated data\nsynthesis, their application to garment editing is constrained by imprecise\ninstruction modeling and a lack of fashion-specific supervisory signals. To\naddress these challenges, we present an automated pipeline for constructing a\ngarment editing dataset. We first define six editing instruction categories\naligned with real-world fashion workflows to guide the generation of balanced\nand diverse instruction-image triplets. Second, we introduce Fashion Edit\nScore, a semantic-aware evaluation metric that captures semantic dependencies\nbetween garment attributes and provides reliable supervision during\nconstruction. Using this pipeline, we construct a total of 52,257 candidate\ntriplets and retain 20,596 high-quality triplets to build EditGarment, the\nfirst instruction-based dataset tailored to standalone garment editing. The\nproject page is https://yindq99.github.io/EditGarment-project/.", "AI": {"tldr": "该论文提出了一个自动化流程，用于构建高质量的指令式服装编辑数据集（EditGarment），解决了现有数据稀缺和多模态大模型（MLLMs）在时尚领域应用受限的问题。", "motivation": "指令式服装编辑在时尚设计和定制中有广泛应用，但其发展受限于高质量指令-图像对的稀缺性。手动标注成本高昂且难以扩展，而现有MLLMs在服装编辑数据合成方面存在指令建模不精确和缺乏时尚特定监督信号的问题。", "method": "首先，定义了六种与实际时尚工作流程对齐的编辑指令类别，以指导平衡和多样化的指令-图像三元组生成。其次，引入了“时尚编辑得分”（Fashion Edit Score），这是一个语义感知的评估指标，用于捕捉服装属性之间的语义依赖关系，并在数据构建过程中提供可靠的监督。", "result": "利用该自动化流程，共构建了52,257个候选三元组，并从中保留了20,596个高质量三元组，从而构建了EditGarment数据集。EditGarment是第一个专为独立服装编辑量身定制的指令式数据集。", "conclusion": "该论文通过提出的自动化流程，成功构建了高质量、指令式的服装编辑数据集EditGarment，有效解决了服装编辑领域数据稀缺的挑战，并为未来的研究提供了可靠的数据基础。"}}
{"id": "2508.03511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03511", "abs": "https://arxiv.org/abs/2508.03511", "authors": ["Yazhou Zhu", "Haofeng Zhang"], "title": "MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation", "comment": "Accepted by MICCAI 2025", "summary": "Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential\nsolution for segmenting medical images with limited annotation using knowledge\nfrom other domains. The significant performance of current CD-FSMIS models\nrelies on the heavily training procedure over other source medical domains,\nwhich degrades the universality and ease of model deployment. With the\ndevelopment of large visual models of natural images, we propose a\ntraining-free CD-FSMIS model that introduces the Multi-center Adaptive\nUncertainty-aware Prompting (MAUP) strategy for adapting the foundation model\nSegment Anything Model (SAM), which is trained with natural images, into the\nCD-FSMIS task. To be specific, MAUP consists of three key innovations: (1)\nK-means clustering based multi-center prompts generation for comprehensive\nspatial coverage, (2) uncertainty-aware prompts selection that focuses on the\nchallenging regions, and (3) adaptive prompt optimization that can dynamically\nadjust according to the target region complexity. With the pre-trained DINOv2\nfeature encoder, MAUP achieves precise segmentation results across three\nmedical datasets without any additional training compared with several\nconventional CD-FSMIS models and training-free FSMIS model. The source code is\navailable at: https://github.com/YazhouZhu19/MAUP.", "AI": {"tldr": "本文提出了一种名为MAUP的免训练跨域少样本医学图像分割（CD-FSMIS）模型，通过多中心自适应不确定性感知提示策略，将自然图像基础模型SAM应用于医学图像分割，并在DINOv2特征编码器辅助下实现了精确分割。", "motivation": "现有CD-FSMIS模型依赖于大量源域训练，降低了模型的通用性和部署便捷性。随着大型视觉模型在自然图像领域的进展，研究者希望探索一种无需额外训练即可适应CD-FSMIS任务的解决方案。", "method": "提出Multi-center Adaptive Uncertainty-aware Prompting (MAUP) 策略来适应基础模型SAM。MAUP包含三个关键创新点：1) 基于K-means聚类的多中心提示生成，实现全面的空间覆盖；2) 不确定性感知提示选择，聚焦于挑战区域；3) 自适应提示优化，根据目标区域复杂性动态调整。模型利用预训练的DINOv2特征编码器。", "result": "MAUP在三个医学数据集上实现了精确的分割结果，且无需任何额外训练，性能优于多个传统CD-FSMIS模型和免训练FSMIS模型。", "conclusion": "MAUP提供了一种创新的免训练方法，通过有效适应自然图像基础模型SAM，解决了CD-FSMIS中模型通用性和部署的挑战，为医学图像分割提供了高效且便捷的解决方案。"}}
{"id": "2508.03516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03516", "abs": "https://arxiv.org/abs/2508.03516", "authors": ["Shiben Liu", "Mingyue Xu", "Huijie Fan", "Qiang Wang", "Yandong Tang", "Zhi Han"], "title": "Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification", "comment": "9 papges, 6 figures", "summary": "Lifelong person re-identification (LReID) encounters a key challenge:\nbalancing the preservation of old knowledge with adaptation to new information.\nExisting LReID methods typically employ knowledge distillation to enforce\nrepresentation alignment. However, these approaches ignore two crucial aspects:\nspecific distribution awareness and cross-domain unified knowledge learning,\nboth of which are essential for addressing this challenge. To overcome these\nlimitations, we propose a novel distribution-aware knowledge unification and\nassociation (DKUA) framework where domain-style modeling is performed for each\ninstance to propagate domain-specific representations, enhancing\nanti-forgetting and generalization capacity. Specifically, we design a\ndistribution-aware model to transfer instance-level representations of the\ncurrent domain into the domain-specific representations with the different\ndomain styles, preserving learned knowledge without storing old samples. Next,\nwe propose adaptive knowledge consolidation (AKC) to dynamically generate the\nunified representation as a cross-domain representation center. To further\nmitigate forgetting, we develop a unified knowledge association (UKA)\nmechanism, which explores the unified representation as a bridge to explicitly\nmodel inter-domain associations, reducing inter-domain gaps. Finally,\ndistribution-based knowledge transfer (DKT) is proposed to prevent the current\ndomain distribution from deviating from the cross-domain distribution center,\nimproving adaptation capacity. Experimental results show our DKUA outperforms\nthe existing methods by 7.6%/5.3% average mAP/R@1 improvement on\nanti-forgetting and generalization capacity, respectively. Our code will be\npublicly released.", "AI": {"tldr": "本文提出了一种新颖的分布感知知识统一与关联（DKUA）框架，用于终身行人重识别（LReID），通过域风格建模、自适应知识整合、统一知识关联和基于分布的知识迁移，有效平衡旧知识的保留与新信息的适应，显著提升了抗遗忘和泛化能力。", "motivation": "现有终身行人重识别（LReID）方法通常采用知识蒸馏来对齐表示，但忽略了特定分布感知和跨域统一知识学习这两个关键方面，导致在平衡旧知识保留和新信息适应方面面临挑战。", "method": "本文提出了分布感知知识统一与关联（DKUA）框架：\n1.  **分布感知模型**：对每个实例进行域风格建模，将当前域的实例级表示转换为具有不同域风格的域特定表示，以保留学习到的知识。\n2.  **自适应知识整合（AKC）**：动态生成统一表示作为跨域表示中心。\n3.  **统一知识关联（UKA）**：利用统一表示作为桥梁，显式建模域间关联，减少域间差距，进一步缓解遗忘。\n4.  **基于分布的知识迁移（DKT）**：防止当前域分布偏离跨域分布中心，提高适应能力。", "result": "实验结果表明，DKUA框架在抗遗忘和泛化能力方面，平均mAP和R@1分别比现有方法提高了7.6%和5.3%。", "conclusion": "所提出的DKUA框架通过其独特的分布感知和知识统一机制，有效解决了终身行人重识别中知识遗忘和泛化能力的挑战，显著优于现有方法，证明了其在平衡新旧知识方面的有效性。"}}
{"id": "2508.03524", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03524", "abs": "https://arxiv.org/abs/2508.03524", "authors": ["Stefan Brandstätter", "Maximilian Köller", "Philipp Seeböck", "Alissa Blessing", "Felicitas Oberndorfer", "Svitlana Pochepnia", "Helmut Prosch", "Georg Langs"], "title": "Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models", "comment": null, "summary": "In histopathology, tissue samples are often larger than a standard microscope\nslide, making stitching of multiple fragments necessary to process entire\nstructures such as tumors. Automated stitching is a prerequisite for scaling\nanalysis, but is challenging due to possible tissue loss during preparation,\ninhomogeneous morphological distortion, staining inconsistencies, missing\nregions due to misalignment on the slide, or frayed tissue edges. This limits\nstate-of-the-art stitching methods using boundary shape matching algorithms to\nreconstruct artificial whole mount slides (WMS). Here, we introduce\nSemanticStitcher using latent feature representations derived from a visual\nhistopathology foundation model to identify neighboring areas in different\nfragments. Robust pose estimation based on a large number of semantic matching\ncandidates derives a mosaic of multiple fragments to form the WMS. Experiments\non three different histopathology datasets demonstrate that SemanticStitcher\nyields robust WMS mosaicing and consistently outperforms the state of the art\nin correct boundary matches.", "AI": {"tldr": "针对组织病理学中大尺寸样本的拼接挑战，本文提出了SemanticStitcher，它利用视觉病理学基础模型的潜在特征表示进行语义匹配和鲁棒的姿态估计，从而实现全切片图像（WMS）的自动拼接，并优于现有技术。", "motivation": "组织病理学中的组织样本通常大于标准显微镜载玻片，需要拼接多个碎片才能处理肿瘤等完整结构。自动化拼接是规模化分析的先决条件，但现有方法面临组织丢失、形态畸变、染色不一致、区域缺失或组织边缘磨损等挑战，限制了基于边界形状匹配的传统拼接方法重建人工全切片图像的能力。", "method": "本文引入了SemanticStitcher。它利用从视觉病理学基础模型中提取的潜在特征表示来识别不同碎片中的相邻区域。通过大量语义匹配候选点进行鲁棒的姿态估计，从而将多个碎片拼接成全切片图像（WMS）。", "result": "在三个不同的组织病理学数据集上的实验表明，SemanticStitcher能够生成鲁棒的全切片图像拼接，并且在正确的边界匹配方面始终优于现有技术。", "conclusion": "SemanticStitcher通过利用来自视觉病理学基础模型的语义特征，提供了一种鲁棒且性能卓越的自动化全切片图像重建解决方案，有效克服了传统边界匹配方法的局限性，提升了组织病理学分析的效率和准确性。"}}
{"id": "2508.03535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03535", "abs": "https://arxiv.org/abs/2508.03535", "authors": ["Kaishen Yuan", "Yuting Zhang", "Shang Gao", "Yijie Zhu", "Wenshuo Chen", "Yutao Yue"], "title": "CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation", "comment": "10 pages, 9 figures", "summary": "Emotional Image Content Generation (EICG) aims to generate semantically clear\nand emotionally faithful images based on given emotion categories, with broad\napplication prospects. While recent text-to-image diffusion models excel at\ngenerating concrete concepts, they struggle with the complexity of abstract\nemotions. There have also emerged methods specifically designed for EICG, but\nthey excessively rely on word-level attribute labels for guidance, which suffer\nfrom semantic incoherence, ambiguity, and limited scalability. To address these\nchallenges, we propose CoEmoGen, a novel pipeline notable for its semantic\ncoherence and high scalability. Specifically, leveraging multimodal large\nlanguage models (MLLMs), we construct high-quality captions focused on\nemotion-triggering content for context-rich semantic guidance. Furthermore,\ninspired by psychological insights, we design a Hierarchical Low-Rank\nAdaptation (HiLoRA) module to cohesively model both polarity-shared low-level\nfeatures and emotion-specific high-level semantics. Extensive experiments\ndemonstrate CoEmoGen's superiority in emotional faithfulness and semantic\ncoherence from quantitative, qualitative, and user study perspectives. To\nintuitively showcase scalability, we curate EmoArt, a large-scale dataset of\nemotionally evocative artistic images, providing endless inspiration for\nemotion-driven artistic creation. The dataset and code are available at\nhttps://github.com/yuankaishen2001/CoEmoGen.", "AI": {"tldr": "本文提出CoEmoGen，一个新颖的情感图像内容生成（EICG）框架，通过利用多模态大语言模型（MLLMs）提供高质量语义指导和设计分层低秩适应（HiLoRA）模块，解决了现有方法在生成抽象情感图像时存在的语义不连贯、模糊和可扩展性差的问题，并构建了大规模情感艺术图像数据集EmoArt。", "motivation": "现有文本到图像扩散模型在生成具象概念方面表现出色，但在处理抽象情感时面临挑战。专门为EICG设计的方法过度依赖词级属性标签，导致语义不连贯、模糊且可扩展性有限。", "method": "CoEmoGen主要采用两种方法：1. 利用多模态大语言模型（MLLMs）构建高质量、专注于情感触发内容的图像描述，以提供丰富的上下文语义指导。2. 借鉴心理学洞察，设计分层低秩适应（HiLoRA）模块，协同建模情感极性共享的低级特征和情感特有的高级语义。", "result": "CoEmoGen在情感忠实度和语义连贯性方面表现出卓越的性能，并通过定量、定性和用户研究得到了广泛验证。此外，本文还策划并发布了大规模情感艺术图像数据集EmoArt，以直观展示其可扩展性。", "conclusion": "CoEmoGen通过创新的语义指导和特征建模方法，显著提升了情感图像内容生成的语义连贯性和可扩展性，有效克服了现有模型的局限性，并为情感驱动的艺术创作提供了宝贵资源。"}}
{"id": "2508.03539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03539", "abs": "https://arxiv.org/abs/2508.03539", "authors": ["Long Qian", "Bingke Zhu", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "title": "Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection", "comment": null, "summary": "Despite substantial progress in anomaly synthesis methods, existing\ndiffusion-based and coarse inpainting pipelines commonly suffer from structural\ndeficiencies such as micro-structural discontinuities, limited semantic\ncontrollability, and inefficient generation. To overcome these limitations, we\nintroduce ARAS, a language-conditioned, auto-regressive anomaly synthesis\napproach that precisely injects local, text-specified defects into normal\nimages via token-anchored latent editing. Leveraging a hard-gated\nauto-regressive operator and a training-free, context-preserving masked\nsampling kernel, ARAS significantly enhances defect realism, preserves\nfine-grained material textures, and provides continuous semantic control over\nsynthesized anomalies. Integrated within our Quality-Aware Re-weighted Anomaly\nDetection (QARAD) framework, we further propose a dynamic weighting strategy\nthat emphasizes high-quality synthetic samples by computing an image-text\nsimilarity score with a dual-encoder model. Extensive experiments across three\nbenchmark datasets-MVTec AD, VisA, and BTAD, demonstrate that our QARAD\noutperforms SOTA methods in both image- and pixel-level anomaly detection\ntasks, achieving improved accuracy, robustness, and a 5 times synthesis speedup\ncompared to diffusion-based alternatives. Our complete code and synthesized\ndataset will be publicly available.", "AI": {"tldr": "ARAS是一种语言条件自回归异常合成方法，通过潜在编辑精确注入局部缺陷，解决了现有方法的结构缺陷和效率问题。结合QARAD框架，它通过动态加权强调高质量合成样本，显著提升了异常检测性能和合成速度。", "motivation": "现有的扩散模型和粗糙修复管道在异常合成方面存在结构缺陷，如微结构不连续、语义可控性有限和生成效率低下。", "method": "本文提出了ARAS（语言条件自回归异常合成方法），通过基于token的潜在编辑将文本指定的局部缺陷精确注入正常图像。ARAS利用硬门控自回归操作符和免训练的上下文保留掩码采样核。此外，在QARAD（质量感知重加权异常检测）框架中，提出了一种动态加权策略，通过双编码器模型计算图像-文本相似度分数，以强调高质量的合成样本。", "result": "ARAS显著增强了缺陷的真实感，保留了精细的材料纹理，并提供了对合成异常的连续语义控制。QARAD在MVTec AD、VisA和BTAD三个基准数据集上，在图像级和像素级异常检测任务中均优于SOTA方法，实现了更高的准确性、鲁棒性，并且合成速度比基于扩散的方法快5倍。", "conclusion": "ARAS和QARAD框架通过创新的异常合成和质量感知加权策略，有效克服了现有方法的局限性，大幅提升了异常检测的性能和效率，为异常合成和检测领域带来了显著进步。"}}
{"id": "2508.03542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03542", "abs": "https://arxiv.org/abs/2508.03542", "authors": ["Dmitrii Korzh", "Dmitrii Tarasov", "Artyom Iudin", "Elvir Karimov", "Matvey Skripkin", "Nikita Kuzmin", "Andrey Kuznetsov", "Oleg Y. Rogov", "Ivan Oseledets"], "title": "Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences", "comment": null, "summary": "Conversion of spoken mathematical expressions is a challenging task that\ninvolves transcribing speech into a strictly structured symbolic representation\nwhile addressing the ambiguity inherent in the pronunciation of equations.\nAlthough significant progress has been achieved in automatic speech recognition\n(ASR) and language models (LM), the problem of converting spoken mathematics\ninto LaTeX remains underexplored. This task directly applies to educational and\nresearch domains, such as lecture transcription or note creation. Based on ASR\npost-correction, prior work requires 2 transcriptions, focuses only on isolated\nequations, has a limited test set, and provides neither training data nor\nmultilingual coverage. To address these issues, we present the first fully\nopen-source large-scale dataset, comprising over 66,000 human-annotated audio\nsamples of mathematical equations and sentences in both English and Russian,\ndrawn from diverse scientific domains. In addition to the ASR post-correction\nmodels and few-shot prompting, we apply audio language models, demonstrating\ncomparable character error rate (CER) results on the MathSpeech benchmark (28%\nvs. 30%) for the equations conversion. In contrast, on the proposed\nS2L-equations benchmark, our models outperform the MathSpeech model by a\nsubstantial margin of more than 40 percentage points, even after accounting for\nLaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for\nmathematical sentence recognition (S2L-sentences) and achieve an equation CER\nof 40%. This work lays the groundwork for future advances in multimodal AI,\nwith a particular focus on mathematical content recognition.", "AI": {"tldr": "该研究提出了首个大规模、开源、多语言（英俄）的数学语音转LaTeX数据集，并应用音频语言模型，显著提升了数学公式和句子识别的准确率，为多模态AI在数学内容识别方面奠定了基础。", "motivation": "将口头数学表达式转换为结构化的符号表示（如LaTeX）是一项具有挑战性的任务，涉及语音转录和歧义处理。尽管ASR和LM取得了进展，但口头数学转LaTeX问题仍未被充分探索。现有工作存在局限性，如需要两次转录、仅关注孤立方程、测试集有限且缺乏训练数据和多语言覆盖。", "method": "构建了一个包含超过66,000个人工标注的数学公式和句子音频样本（英语和俄语）的大规模开放数据集。除了ASR后校正模型和少样本提示外，还应用了音频语言模型进行转换。", "result": "在MathSpeech基准测试上，公式转换的字符错误率（CER）与现有模型相当（28% vs 30%）。在提出的S2L-equations基准测试上，模型表现显著优于MathSpeech模型，领先超过40个百分点（27% vs 64%）。首次建立了数学句子识别的基准（S2L-sentences），并实现了40%的公式CER。", "conclusion": "这项工作为未来多模态AI，特别是数学内容识别领域的发展奠定了基础。"}}
{"id": "2508.03545", "categories": ["cs.CV", "q-bio.QM", "62P10", "I.4.8"], "pdf": "https://arxiv.org/pdf/2508.03545", "abs": "https://arxiv.org/abs/2508.03545", "authors": ["Stephanie Wohlfahrt", "Christoph Praschl", "Horst Leitner", "Wolfram Jantsch", "Julia Konic", "Silvio Schueler", "Andreas Stöckl", "David C. Schedl"], "title": "Advancing Wildlife Monitoring: Drone-Based Sampling for Roe Deer Density Estimation", "comment": "6 pages, 1 figure, 1 table, International Wildlife Congress 2025", "summary": "We use unmanned aerial drones to estimate wildlife density in southeastern\nAustria and compare these estimates to camera trap data. Traditional methods\nlike capture-recapture, distance sampling, or camera traps are well-established\nbut labour-intensive or spatially constrained. Using thermal (IR) and RGB\nimagery, drones enable efficient, non-intrusive animal counting. Our surveys\nwere conducted during the leafless period on single days in October and\nNovember 2024 in three areas of a sub-Illyrian hill and terrace landscape.\nFlight transects were based on predefined launch points using a 350 m grid and\nan algorithm that defined the direction of systematically randomized transects.\nThis setup allowed surveying large areas in one day using multiple drones,\nminimizing double counts. Flight altitude was set at 60 m to avoid disturbing\nroe deer (Capreolus capreolus) while ensuring detection. Animals were manually\nannotated in the recorded imagery and extrapolated to densities per square\nkilometer. We applied three extrapolation methods with increasing complexity:\nnaive area-based extrapolation, bootstrapping, and zero-inflated negative\nbinomial modelling. For comparison, a Random Encounter Model (REM) estimate was\ncalculated using camera trap data from the flight period. The drone-based\nmethods yielded similar results, generally showing higher densities than REM,\nexcept in one area in October. We hypothesize that drone-based density reflects\ndaytime activity in open and forested areas, while REM estimates average\nactivity over longer periods within forested zones. Although both approaches\nestimate density, they offer different perspectives on wildlife presence. Our\nresults show that drones offer a promising, scalable method for wildlife\ndensity estimation.", "AI": {"tldr": "本研究利用无人机结合热成像和RGB图像估算野生动物密度，并与相机陷阱数据进行比较，发现无人机是一种有前景且可扩展的野生动物密度估算方法。", "motivation": "传统的野生动物密度估算方法（如捕获-再捕获、距离采样或相机陷阱）劳动强度大或受空间限制。无人机提供了一种高效、非侵入性的动物计数方法。", "method": "研究在奥地利东南部无叶期（2024年10月和11月）的三个区域进行。使用无人机搭载热成像（IR）和RGB图像传感器，在预设的350米网格和算法定义的系统随机样带上飞行，飞行高度60米。记录的图像中动物被手动标注，并采用三种外推方法（朴素面积外推、自助法、零膨胀负二项式模型）估算每平方公里的密度。同时，使用飞行期间的相机陷阱数据计算随机相遇模型（REM）估算值进行比较。", "result": "无人机估算方法得出相似结果，且通常比随机相遇模型（REM）估算值更高，但10月份的一个区域除外。研究推测无人机估算的密度反映了白天在开放和森林区域的活动，而REM估算的是森林区域在更长时间内的平均活动。", "conclusion": "尽管无人机和相机陷阱提供了不同的野生动物存在视角，但研究结果表明无人机是一种有前景、可扩展的野生动物密度估算方法。"}}
{"id": "2508.03564", "categories": ["cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2508.03564", "abs": "https://arxiv.org/abs/2508.03564", "authors": ["Annemarie McCarthy"], "title": "A Scalable Machine Learning Pipeline for Building Footprint Detection in Historical Maps", "comment": "15 pages, 11 figures", "summary": "Historical maps offer a valuable lens through which to study past landscapes\nand settlement patterns. While prior research has leveraged machine learning\nbased techniques to extract building footprints from historical maps, such\napproaches have largely focused on urban areas and tend to be computationally\nintensive. This presents a challenge for research questions requiring analysis\nacross extensive rural regions, such as verifying historical census data or\nlocating abandoned settlements. In this paper, this limitation is addressed by\nproposing a scalable and efficient pipeline tailored to rural maps with sparse\nbuilding distributions. The method described employs a hierarchical machine\nlearning based approach: convolutional neural network (CNN) classifiers are\nfirst used to progressively filter out map sections unlikely to contain\nbuildings, significantly reducing the area requiring detailed analysis. The\nremaining high probability sections are then processed using CNN segmentation\nalgorithms to extract building features. The pipeline is validated using test\nsections from the Ordnance Survey Ireland historical 25 inch map series and 6\ninch map series, demonstrating both high performance and improved efficiency\ncompared to conventional segmentation-only approaches. Application of the\ntechnique to both map series, covering the same geographic region, highlights\nits potential for historical and archaeological discovery. Notably, the\npipeline identified a settlement of approximately 22 buildings in Tully, Co.\nGalway, present in the 6 inch map, produced in 1839, but absent from the 25\ninch map, produced in 1899, suggesting it may have been abandoned during the\nGreat Famine period.", "AI": {"tldr": "该研究提出了一种可扩展且高效的机器学习管道，用于从历史农村地图中提取稀疏分布的建筑物，通过分层CNN方法显著提高了效率，并在爱尔兰地图上验证了其性能，成功识别了一个可能在大饥荒期间被废弃的定居点。", "motivation": "现有从历史地图中提取建筑物足迹的机器学习方法主要集中在城市区域，且计算密集，难以应用于需要分析广阔农村地区（建筑物稀疏）的研究问题，例如验证历史人口普查数据或定位废弃定居点。", "method": "本文提出了一种针对农村地图稀疏建筑物分布的、可扩展且高效的管道。该方法采用分层机器学习方法：首先使用卷积神经网络（CNN）分类器逐步过滤掉不太可能包含建筑物的地图区域，显著减少需要详细分析的面积；然后，使用CNN分割算法处理剩余的高概率区域以提取建筑物特征。", "result": "该管道在爱尔兰测绘局历史25英寸和6英寸系列地图的测试部分上进行了验证，结果表明其性能高，且与传统仅分割方法相比效率更高。将该技术应用于覆盖相同地理区域的两种地图系列，发现了一个约22栋建筑的定居点（位于戈尔韦郡图利），该定居点在1839年制作的6英寸地图中存在，但在1899年制作的25英寸地图中已消失。", "conclusion": "该管道在历史和考古发现方面具有巨大潜力。通过识别出在不同时期地图上消失的定居点，推测其可能在爱尔兰大饥荒期间被废弃，从而为历史研究提供了新的线索。"}}
{"id": "2508.03566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03566", "abs": "https://arxiv.org/abs/2508.03566", "authors": ["Xinyu Xiong", "Zihuang Wu", "Lei Zhang", "Lei Lu", "Ming Li", "Guanbin Li"], "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks", "comment": "Technical Report", "summary": "Recent studies have highlighted the potential of adapting the Segment\nAnything Model (SAM) for various downstream tasks. However, constructing a more\npowerful and generalizable encoder to further enhance performance remains an\nopen challenge. In this work, we propose SAM2-UNeXT, an advanced framework that\nbuilds upon the core principles of SAM2-UNet while extending the\nrepresentational capacity of SAM2 through the integration of an auxiliary\nDINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue\nlayer, our approach enables more accurate segmentation with a simple\narchitecture, relaxing the need for complex decoder designs. Extensive\nexperiments conducted on four benchmarks, including dichotomous image\nsegmentation, camouflaged object detection, marine animal segmentation, and\nremote sensing saliency detection, demonstrate the superior performance of our\nproposed method. The code is available at\nhttps://github.com/WZH0120/SAM2-UNeXT.", "AI": {"tldr": "该论文提出了SAM2-UNeXT框架，通过集成DINOv2编码器和采用双分辨率策略，显著增强了SAM的编码器能力，并在多种下游分割任务中取得了优越性能。", "motivation": "尽管Segment Anything Model (SAM) 在下游任务中展现出巨大潜力，但构建一个更强大、更具泛化能力的编码器以进一步提升性能仍然是一个开放的挑战。", "method": "本文提出了SAM2-UNeXT，一个基于SAM2-UNet的高级框架。它通过集成辅助的DINOv2编码器来扩展SAM2的表示能力，并结合双分辨率策略和密集连接层，以简单的架构实现更精确的分割，从而减少了对复杂解码器设计的需求。", "result": "在二值图像分割、伪装物体检测、海洋动物分割和遥感显著性检测四个基准测试上进行的广泛实验表明，所提出的方法展现出卓越的性能。", "conclusion": "通过增强编码器能力和简化架构设计，SAM2-UNeXT在多种图像分割任务中实现了性能提升和更好的泛化能力，验证了其有效性和优越性。"}}
{"id": "2508.03578", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03578", "abs": "https://arxiv.org/abs/2508.03578", "authors": ["Jonas Leo Mueller", "Lukas Engel", "Eva Dorschky", "Daniel Krauss", "Ingrid Ullmann", "Martin Vossiek", "Bjoern M. Eskofier"], "title": "RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data", "comment": null, "summary": "Radar-based human pose estimation (HPE) provides a privacy-preserving,\nillumination-invariant sensing modality but is challenged by noisy,\nmultipath-affected measurements. We introduce RadProPoser, a probabilistic\nencoder-decoder architecture that processes complex-valued radar tensors from a\ncompact 3-transmitter, 4-receiver MIMO radar. By incorporating variational\ninference into keypoint regression, RadProPoser jointly predicts 26\nthree-dimensional joint locations alongside heteroscedastic aleatoric\nuncertainties and can be recalibrated to predict total uncertainty. We explore\ndifferent probabilistic formulations using both Gaussian and Laplace\ndistributions for latent priors and likelihoods. On our newly released dataset\nwith optical motion-capture ground truth, RadProPoser achieves an overall mean\nper-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree\naspect angle. The learned uncertainties exhibit strong alignment with actual\npose errors and can be calibrated to produce reliable prediction intervals,\nwith our best configuration achieving an expected calibration error of 0.021.\nAs an additional demonstration, sampling from these latent distributions\nenables effective data augmentation for downstream activity classification,\nresulting in an F1 score of 0.870. To our knowledge, this is the first\nend-to-end radar tensor-based HPE system to explicitly model and quantify\nper-joint uncertainty from raw radar tensor data, establishing a foundation for\nexplainable and reliable human motion analysis in radar applications.", "AI": {"tldr": "RadProPoser是一种基于MIMO雷达的概率编码器-解码器架构，用于人体姿态估计，它能同时预测3D关节位置和异方差不确定性，并在新数据集上表现出色，且其不确定性可用于数据增强。", "motivation": "雷达人体姿态估计具有隐私保护和光照不变性优点，但面临测量噪声和多径效应的挑战。现有系统缺乏对每关节不确定性的显式建模和量化。", "method": "引入RadProPoser，一个概率编码器-解码器架构，处理来自3发4收MIMO雷达的复数值雷达张量。通过将变分推断整合到关键点回归中，联合预测26个3D关节位置和异方差不确定性。探索了使用高斯和拉普拉斯分布作为潜在先验和似然的不同概率公式。", "result": "在新发布的数据集上，RadProPoser的平均每关节位置误差（MPJPE）为6.425厘米，在45度视角下为5.678厘米。学习到的不确定性与实际姿态误差高度一致，并能被校准以产生可靠的预测区间，最佳配置的预期校准误差为0.021。通过潜在分布采样进行数据增强，在下游活动分类任务中F1分达到0.870。", "conclusion": "RadProPoser是首个端到端基于雷达张量的人体姿态估计系统，能够从原始雷达张量数据中显式建模和量化每关节不确定性，为雷达应用中可解释和可靠的人体运动分析奠定了基础。"}}
{"id": "2508.03598", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03598", "abs": "https://arxiv.org/abs/2508.03598", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "title": "DyCAF-Net: Dynamic Class-Aware Fusion Network", "comment": "Accepted to IEEE DSAA 2025 (10 pages, 5 figures)", "summary": "Recent advancements in object detection rely on modular architectures with\nmulti-scale fusion and attention mechanisms. However, static fusion heuristics\nand class-agnostic attention limit performance in dynamic scenes with\nocclusions, clutter, and class imbalance. We introduce Dynamic Class-Aware\nFusion Network (DyCAF-Net) that addresses these challenges through three\ninnovations: (1) an input-conditioned equilibrium-based neck that iteratively\nrefines multi-scale features via implicit fixed-point modeling, (2) a dual\ndynamic attention mechanism that adaptively recalibrates channel and spatial\nresponses using input- and class-dependent cues, and (3) class-aware feature\nadaptation that modulates features to prioritize discriminative regions for\nrare classes. Through comprehensive ablation studies with YOLOv8 and related\narchitectures, alongside benchmarking against nine state-of-the-art baselines,\nDyCAF-Net achieves significant improvements in precision, mAP@50, and mAP@50-95\nacross 13 diverse benchmarks, including occlusion-heavy and long-tailed\ndatasets. The framework maintains computational efficiency ($\\sim$11.1M\nparameters) and competitive inference speeds, while its adaptability to scale\nvariance, semantic overlaps, and class imbalance positions it as a robust\nsolution for real-world detection tasks in medical imaging, surveillance, and\nautonomous systems.", "AI": {"tldr": "提出DyCAF-Net，通过动态类感知融合和注意力机制，解决现有目标检测在复杂场景中的局限性，显著提升性能并保持效率。", "motivation": "现有目标检测方法依赖的静态融合启发式和类别无关注意力机制，在动态场景（如遮挡、杂乱、类别不平衡）中性能受限。", "method": "提出DyCAF-Net，包含三项创新：1) 基于输入条件的平衡颈部，通过隐式不动点建模迭代优化多尺度特征；2) 双动态注意力机制，利用输入和类别依赖线索自适应调整通道和空间响应；3) 类感知特征自适应，调制特征以优先处理稀有类别的判别区域。", "result": "DyCAF-Net在13个多样化基准（包括遮挡严重和长尾数据集）上，相对于YOLOv8及9个SOTA基线，在精度、mAP@50和mAP@50-95方面取得了显著提升。同时保持计算效率（约11.1M参数）和有竞争力的推理速度。", "conclusion": "DyCAF-Net能适应尺度变化、语义重叠和类别不平衡，是一个鲁棒的解决方案，适用于医学影像、监控和自动驾驶系统等现实世界检测任务。"}}
{"id": "2508.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03609", "abs": "https://arxiv.org/abs/2508.03609", "authors": ["Rodrigo Verschae", "Ignacio Bugueno-Cordova"], "title": "evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition", "comment": null, "summary": "Event-based cameras are bio-inspired vision sensors that asynchronously\ncapture per-pixel intensity changes with microsecond latency, high temporal\nresolution, and high dynamic range, providing valuable information about the\nspatio-temporal dynamics of the scene. In the present work, we propose\nevTransFER, a transfer learning-based framework and architecture for face\nexpression recognition using event-based cameras. The main contribution is a\nfeature extractor designed to encode the spatio-temporal dynamics of faces,\nbuilt by training an adversarial generative method on a different problem\n(facial reconstruction) and then transferring the trained encoder weights to\nthe face expression recognition system. We show that this proposed transfer\nlearning method greatly improves the ability to recognize facial expressions\ncompared to training a network from scratch. In addition, we propose an\narchitecture that incorporates an LSTM to capture longer-term facial expression\ndynamics, and we introduce a new event-based representation, referred to as\nTIE, both of which further improve the results. We evaluate the proposed\nframework on the event-based facial expression database e-CK+ and compare it to\nstate-of-the-art methods. The results show that the proposed framework\nevTransFER achieves a 93.6\\% recognition rate on the e-CK+ database,\nsignificantly improving the accuracy (25.9\\% points or more) when compared to\nstate-of-the-art performance for similar problems.", "AI": {"tldr": "evTransFER是一个基于事件相机的面部表情识别框架，通过对抗生成方法进行迁移学习，显著提高了识别准确率。", "motivation": "事件相机具有低延迟、高时间分辨率和高动态范围的特点，能有效捕捉场景的时空动态，为面部表情识别提供了有价值的信息。", "method": "本文提出了evTransFER框架，其核心是一个特征提取器，通过在面部重建任务上训练对抗生成模型，然后将训练好的编码器权重迁移到面部表情识别系统。此外，还引入了LSTM来捕捉长期面部表情动态，并提出了一种新的事件表示TIE，以进一步提高性能。", "result": "evTransFER在事件基面部表情数据库e-CK+上实现了93.6%的识别率，与现有最先进方法相比，准确率显著提高了25.9%或更多。", "conclusion": "所提出的迁移学习方法（evTransFER），结合LSTM和TIE事件表示，大大提高了使用事件相机进行面部表情识别的能力和准确性。"}}
{"id": "2508.03618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03618", "abs": "https://arxiv.org/abs/2508.03618", "authors": ["Nassim Ali Ousalah", "Peyman Rostami", "Anis Kacem", "Enjie Ghorbel", "Emmanuel Koumandakis", "Djamila Aouada"], "title": "FPG-NAS: FLOPs-Aware Gated Differentiable Neural Architecture Search for Efficient 6DoF Pose Estimation", "comment": "Accepted to the 27th IEEE International Workshop on Multimedia Signal\n  Processing (MMSP) 2025", "summary": "We introduce FPG-NAS, a FLOPs-aware Gated Differentiable Neural Architecture\nSearch framework for efficient 6DoF object pose estimation. Estimating 3D\nrotation and translation from a single image has been widely investigated yet\nremains computationally demanding, limiting applicability in\nresource-constrained scenarios. FPG-NAS addresses this by proposing a\nspecialized differentiable NAS approach for 6DoF pose estimation, featuring a\ntask-specific search space and a differentiable gating mechanism that enables\ndiscrete multi-candidate operator selection, thus improving architectural\ndiversity. Additionally, a FLOPs regularization term ensures a balanced\ntrade-off between accuracy and efficiency. The framework explores a vast search\nspace of approximately 10\\textsuperscript{92} possible architectures.\nExperiments on the LINEMOD and SPEED+ datasets demonstrate that FPG-NAS-derived\nmodels outperform previous methods under strict FLOPs constraints. To the best\nof our knowledge, FPG-NAS is the first differentiable NAS framework\nspecifically designed for 6DoF object pose estimation.", "AI": {"tldr": "FPG-NAS是一种FLOPs感知的门控可微分神经架构搜索框架，用于高效的6DoF物体姿态估计。", "motivation": "6DoF物体姿态估计计算量大，限制了其在资源受限场景中的应用。", "method": "FPG-NAS提出了一种专门针对6DoF姿态估计的可微分NAS方法，具有任务特定的搜索空间和可微分门控机制，实现离散多候选操作符选择，增强架构多样性。此外，引入FLOPs正则化项以平衡精度和效率。该框架探索了约10^92种可能的架构。", "result": "在LINEMOD和SPEED+数据集上的实验表明，FPG-NAS派生模型在严格的FLOPs限制下优于现有方法。", "conclusion": "FPG-NAS是首个专门为6DoF物体姿态估计设计的可微分NAS框架。"}}
{"id": "2508.03643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03643", "abs": "https://arxiv.org/abs/2508.03643", "authors": ["Xiangyu Sun", "Haoyi jiang", "Liu Liu", "Seungtae Nam", "Gyeongjin Kang", "Xinjie wang", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang", "Eunbyung Park"], "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images", "comment": "The code is available at https://github.com/HorizonRobotics/Uni3R", "summary": "Reconstructing and semantically interpreting 3D scenes from sparse 2D views\nremains a fundamental challenge in computer vision. Conventional methods often\ndecouple semantic understanding from reconstruction or necessitate costly\nper-scene optimization, thereby restricting their scalability and\ngeneralizability. In this paper, we introduce Uni3R, a novel feed-forward\nframework that jointly reconstructs a unified 3D scene representation enriched\nwith open-vocabulary semantics, directly from unposed multi-view images. Our\napproach leverages a Cross-View Transformer to robustly integrate information\nacross arbitrary multi-view inputs, which then regresses a set of 3D Gaussian\nprimitives endowed with semantic feature fields. This unified representation\nfacilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic\nsegmentation, and depth prediction, all within a single, feed-forward pass.\nExtensive experiments demonstrate that Uni3R establishes a new state-of-the-art\nacross multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on\nScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D\nscene reconstruction and understanding. The code is available at\nhttps://github.com/HorizonRobotics/Uni3R.", "AI": {"tldr": "Uni3R是一个前馈框架，能直接从无姿态多视角图像中联合重建统一的3D场景表示，并赋予开放词汇语义，实现高保真新视角合成、语义分割和深度预测。", "motivation": "传统方法将语义理解与重建分离，或需要昂贵的逐场景优化，限制了可扩展性和泛化性。", "method": "Uni3R采用跨视图Transformer整合多视角信息，然后回归一组带有语义特征场的3D高斯基元，实现单次前馈通过即可完成新视角合成、开放词汇3D语义分割和深度预测。", "result": "Uni3R在多个基准测试中达到了新的最先进水平，包括RE10K上的25.07 PSNR和ScanNet上的55.84 mIoU。", "conclusion": "Uni3R为可泛化、统一的3D场景重建和理解提供了一种新范式。"}}
{"id": "2508.03694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03694", "abs": "https://arxiv.org/abs/2508.03694", "authors": ["Jianxiong Gao", "Zhaoxi Chen", "Xian Liu", "Jianfeng Feng", "Chenyang Si", "Yanwei Fu", "Yu Qiao", "Ziwei Liu"], "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation", "comment": "Project page: https://vchitect.github.io/LongVie-project/", "summary": "Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.", "AI": {"tldr": "LongVie是一个端到端自回归框架，通过统一噪声初始化、全局控制信号归一化、多模态控制和退化感知训练，解决了可控超长视频生成中的时间不一致性和视觉退化问题，并引入了新的基准测试。", "motivation": "现有方法在生成超长视频时存在时间不一致性和视觉退化问题，难以扩展。主要挑战源于：独立的噪声初始化、独立的控制信号归一化以及单模态指导的局限性。", "method": "本文提出了LongVie框架，核心设计包括：1) 统一的噪声初始化策略以保持片段间生成一致性；2) 全局控制信号归一化以确保整个视频在控制空间中的对齐；3) 集成密集（如深度图）和稀疏（如关键点）控制信号的多模态控制框架；4) 退化感知训练策略，自适应平衡模态贡献以保持视觉质量。此外，还引入了LongVGenBench，一个包含100个高分辨率、超过一分钟视频的综合基准测试。", "result": "LongVie在长距离可控性、一致性和质量方面均实现了最先进的性能。", "conclusion": "LongVie通过其创新的设计有效解决了可控超长视频生成中的关键挑战，并在性能上超越了现有方法，为该领域树立了新标杆。"}}
{"id": "2508.03695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03695", "abs": "https://arxiv.org/abs/2508.03695", "authors": ["Pulkit Kumar", "Shuaiyi Huang", "Matthew Walmer", "Sai Saketh Rambhatla", "Abhinav Shrivastava"], "title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition", "comment": "Accepted at ICCV 2025; First two authors contributed equally", "summary": "Video understanding requires effective modeling of both motion and appearance\ninformation, particularly for few-shot action recognition. While recent\nadvances in point tracking have been shown to improve few-shot action\nrecognition, two fundamental challenges persist: selecting informative points\nto track and effectively modeling their motion patterns. We present Trokens, a\nnovel approach that transforms trajectory points into semantic-aware relational\ntokens for action recognition. First, we introduce a semantic-aware sampling\nstrategy to adaptively distribute tracking points based on object scale and\nsemantic relevance. Second, we develop a motion modeling framework that\ncaptures both intra-trajectory dynamics through the Histogram of Oriented\nDisplacements (HoD) and inter-trajectory relationships to model complex action\npatterns. Our approach effectively combines these trajectory tokens with\nsemantic features to enhance appearance features with motion information,\nachieving state-of-the-art performance across six diverse few-shot action\nrecognition benchmarks: Something-Something-V2 (both full and small splits),\nKinetics, UCF101, HMDB51, and FineGym. For project page see\nhttps://trokens-iccv25.github.io", "AI": {"tldr": "Trokens通过将轨迹点转换为语义感知的关系令牌，并结合自适应采样和全面的运动建模，显著提升了少样本动作识别的性能，达到了当前最佳水平。", "motivation": "少样本动作识别需要有效建模运动和外观信息。尽管点跟踪技术有所进展，但选择信息丰富的跟踪点以及有效建模其运动模式仍是两大挑战。", "method": "提出了Trokens方法。首先，引入了一种语义感知采样策略，根据物体尺度和语义相关性自适应地分布跟踪点。其次，开发了一个运动建模框架，通过定向位移直方图（HoD）捕获轨迹内动态，并通过轨迹间关系建模复杂的动作模式。该方法将这些轨迹令牌与语义特征结合，用运动信息增强外观特征。", "result": "在六个不同的少样本动作识别基准测试（Something-Something-V2、Kinetics、UCF101、HMDB51和FineGym）上均实现了最先进的性能。", "conclusion": "Trokens通过有效结合语义感知点采样和全面的运动建模，成功地将丰富的运动信息融入外观特征，显著提升了少样本动作识别的能力，达到了新的性能标杆。"}}
