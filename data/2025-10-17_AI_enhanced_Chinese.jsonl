{"id": "2510.13887", "categories": ["eess.IV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.13887", "abs": "https://arxiv.org/abs/2510.13887", "authors": ["Xiaojian Ding", "Lin Zhao", "Xian Li", "Xiaoying Zhu"], "title": "Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion", "comment": null, "summary": "Incomplete multi-view data, where certain views are entirely missing for some\nsamples, poses significant challenges for traditional multi-view clustering\nmethods. Existing deep incomplete multi-view clustering approaches often rely\non static fusion strategies or two-stage pipelines, leading to suboptimal\nfusion results and error propagation issues. To address these limitations, this\npaper proposes a novel incomplete multi-view clustering framework based on\nHierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC\nachieves robust cross-view fusion through a dual-level semantic space design.\nIn the low-level semantic space, consistency alignment is ensured by maximizing\nmutual information across views. In the high-level semantic space, adaptive\nview weights are dynamically assigned based on the distributional affinity\nbetween individual views and an initial fused representation, followed by\nweighted fusion to generate a unified global representation. Additionally,\nHSACC implicitly recovers missing views by projecting aligned latent\nrepresentations into high-dimensional semantic spaces and jointly optimizes\nreconstruction and clustering objectives, enabling cooperative learning of\ncompletion and clustering. Experimental results demonstrate that HSACC\nsignificantly outperforms state-of-the-art methods on five benchmark datasets.\nAblation studies validate the effectiveness of the hierarchical alignment and\ndynamic weighting mechanisms, while parameter analysis confirms the model's\nrobustness to hyperparameter variations.", "AI": {"tldr": "本文提出HSACC框架，通过分层语义对齐和协同补全来解决不完整多视图聚类问题，实现了鲁棒的跨视图融合和缺失视图恢复。", "motivation": "传统和现有深度不完整多视图聚类方法面临挑战，包括对某些样本视图完全缺失、静态融合策略、两阶段管道导致次优融合结果和误差传播问题。", "method": "HSACC框架基于双层语义空间设计：在低级语义空间通过最大化互信息确保视图间一致性对齐；在高级语义空间根据视图与初始融合表示的分布亲和度动态分配自适应视图权重，然后进行加权融合生成统一全局表示。此外，通过将对齐的潜在表示投影到高维语义空间，隐式恢复缺失视图，并联合优化重建和聚类目标，实现补全与聚类的协同学习。", "result": "实验结果表明，HSACC在五个基准数据集上显著优于现有最先进方法。消融研究验证了分层对齐和动态加权机制的有效性，参数分析证实了模型对超参数变化的鲁棒性。", "conclusion": "HSACC通过其新颖的分层语义对齐和协同补全机制，有效解决了不完整多视图聚类中的挑战，取得了卓越的性能和鲁棒性。"}}
{"id": "2510.13889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13889", "abs": "https://arxiv.org/abs/2510.13889", "authors": ["Yue Hu", "Guohang Zhuang"], "title": "MultiFoodhat: A potential new paradigm for intelligent food quality inspection", "comment": null, "summary": "Food image classification plays a vital role in intelligent food quality\ninspection, dietary assessment, and automated monitoring. However, most\nexisting supervised models rely heavily on large labeled datasets and exhibit\nlimited generalization to unseen food categories. To overcome these challenges,\nthis study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning\nframework for zero-shot food recognition. The framework integrates\nvision-language models (VLMs) and large language models (LLMs) to enable\ncollaborative reasoning through multi-round visual-textual dialogues. An Object\nPerception Token (OPT) captures fine-grained visual attributes, while an\nInteractive Reasoning Agent (IRA) dynamically interprets contextual cues to\nrefine predictions. This multi-agent design allows flexible and human-like\nunderstanding of complex food scenes without additional training or manual\nannotations. Experiments on multiple public food datasets demonstrate that\nMultiFoodChat achieves superior recognition accuracy and interpretability\ncompared with existing unsupervised and few-shot methods, highlighting its\npotential as a new paradigm for intelligent food quality inspection and\nanalysis.", "AI": {"tldr": "MultiFoodChat是一个对话驱动的多智能体推理框架，结合视觉-语言模型和大型语言模型，实现零样本食物识别，无需额外训练或标注，并展现出卓越的准确性和可解释性。", "motivation": "现有的食物图像分类监督模型严重依赖大量标注数据集，且对未见过的食物类别泛化能力有限。", "method": "本研究引入MultiFoodChat框架，通过多轮视觉-文本对话实现协作推理。它整合了视觉-语言模型（VLMs）和大型语言模型（LLMs）。其中，对象感知令牌（OPT）用于捕获细粒度视觉属性，交互式推理智能体（IRA）动态解释上下文线索以完善预测。这种多智能体设计使得系统能够灵活地、像人类一样理解复杂的食物场景。", "result": "在多个公共食物数据集上的实验表明，MultiFoodChat与现有的无监督和少样本方法相比，取得了卓越的识别准确性和可解释性，且无需额外的训练或手动标注。", "conclusion": "MultiFoodChat作为一种零样本食物识别的新范式，在智能食物质量检测和分析方面具有巨大潜力。"}}
{"id": "2510.13933", "categories": ["eess.IV", "68T45 (Primary) 68T07, 68U05 (Secondary)", "I.3.7; I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.13933", "abs": "https://arxiv.org/abs/2510.13933", "authors": ["Tianxiang Yang", "Marco Volino", "Armin Mustafa", "Greg Maguire", "Robert Kosk"], "title": "Image-based Facial Rig Inversion", "comment": "The 22nd ACM SIGGRAPH European Conference on Visual Media Production\n  (CVMP2025) Short Paper", "summary": "We present an image-based rig inversion framework that leverages two\nmodalities: RGB appearance and RGB-encoded normal maps. Each modality is\nprocessed by an independent Hiera transformer backbone, and the extracted\nfeatures are fused to regress 102 rig parameters derived from the Facial Action\nCoding System (FACS). Experiments on synthetic and scanned datasets demonstrate\nthat the method generalizes to scanned data, producing faithful\nreconstructions.", "AI": {"tldr": "该论文提出了一种基于图像的索具反演框架，利用RGB外观和RGB编码法线图两种模态，通过独立的Hiera Transformer骨干网络处理并融合特征，以回归102个FACS索具参数。", "motivation": "旨在通过图像数据，准确地从外观和几何信息中提取面部索具参数，实现忠实的面部重建。", "method": "使用RGB外观和RGB编码法线图两种模态作为输入。每种模态由独立的Hiera Transformer骨干网络处理。提取的特征被融合，以回归102个源自面部动作编码系统（FACS）的索具参数。", "result": "在合成数据集和扫描数据集上的实验表明，该方法能够泛化到扫描数据，并产生忠实的重建效果。", "conclusion": "所提出的基于图像的索具反演框架，通过结合RGB外观和法线图的Hiera Transformer处理，能够有效地从图像中提取FACS索具参数，并实现对扫描数据的良好泛化和准确重建。"}}
{"id": "2510.13867", "categories": ["eess.IV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13867", "abs": "https://arxiv.org/abs/2510.13867", "authors": ["Semih Esenlik", "Yaojun Wu", "Zhaobin Zhang", "Ye-Kui Wang", "Kai Zhang", "Li Zhang", "João Ascenso", "Shan Liu"], "title": "An Overview of the JPEG AI Learning-Based Image Coding Standard", "comment": "IEEE Transactions on Circuits and Systems for Video Technology", "summary": "JPEG AI is an emerging learning-based image coding standard developed by\nJoint Photographic Experts Group (JPEG). The scope of the JPEG AI is the\ncreation of a practical learning-based image coding standard offering a\nsingle-stream, compact compressed domain representation, targeting both human\nvisualization and machine consumption. Scheduled for completion in early 2025,\nthe first version of JPEG AI focuses on human vision tasks, demonstrating\nsignificant BD-rate reductions compared to existing standards, in terms of\nMS-SSIM, FSIM, VIF, VMAF, PSNR-HVS, IW-SSIM and NLPD quality metrics. Designed\nto ensure broad interoperability, JPEG AI incorporates various design features\nto support deployment across diverse devices and applications. This paper\nprovides an overview of the technical features and characteristics of the JPEG\nAI standard.", "AI": {"tldr": "JPEG AI是一个新兴的基于学习的图像编码标准，旨在为人类视觉和机器消费提供单一流、紧凑的压缩表示，与现有标准相比，在多项质量指标上实现了显著的BD率降低。", "motivation": "动机是创建一个实用的、基于学习的图像编码标准，即JPEG AI，它能提供单一流、紧凑的压缩域表示，同时满足人类视觉和机器消费的需求。", "method": "本文概述了JPEG AI标准的技术特性和特点。JPEG AI标准本身采用基于学习的图像编码方法。", "result": "JPEG AI在MS-SSIM、FSIM、VIF、VMAF、PSNR-HVS、IW-SSIM和NLPD等多种质量指标方面，与现有标准相比，实现了显著的BD率降低。它还融入了各种设计特性以支持在不同设备和应用上的广泛互操作性。", "conclusion": "JPEG AI是一个正在开发的、基于学习的图像编码标准，旨在为人类视觉和机器消费提供高性能和广泛互操作性，其第一个版本专注于人类视觉任务，并计划于2025年初完成。"}}
{"id": "2510.14000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios.", "AI": {"tldr": "本文提出DRIP，一种结合强化学习先验动作分布的扩散精炼规划器，用于在狭窄空间中实现高精度自动化泊车规划，显著提高了规划成功率和精度。", "motivation": "随着停车需求增长，自动泊车规划方法在狭窄空间中的可靠性变得至关重要。现有方法依赖显式动作建模，难以准确模拟最优动作分布，导致在高精度机动中面临挑战。", "method": "本文提出DRIP（Diffusion-Refined Planner），它以强化学习（RL）先验动作分布为基础。RL预训练策略提供先验动作分布，用于正则化扩散训练过程。在推理阶段，去噪过程将这些粗略的先验细化为更精确的动作分布。通过在训练期间利用RL先验分布引导去噪轨迹，扩散模型获得了良好的初始化，从而实现了更准确的动作建模。", "result": "实验结果表明，DRIP在受限空间泊车环境中显著提高了规划性能，同时在常见场景中保持了强大的泛化能力。该方法实现了更准确的动作建模、更高的规划成功率和更少的推理步骤。", "conclusion": "DRIP通过结合强化学习先验和扩散模型，有效地解决了狭窄空间中高精度自动泊车规划的挑战，显著提升了规划性能和成功率。"}}
{"id": "2510.14244", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14244", "abs": "https://arxiv.org/abs/2510.14244", "authors": ["Arnaud Judge", "Nicolas Duchateau", "Thierry Judge", "Roman A. Sandler", "Joseph Z. Sokol", "Christian Desrosiers", "Olivier Bernard", "Pierre-Marc Jodoin"], "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation", "comment": "10 pages, submitted to IEEE TMI", "summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling\nknowledge transfer across domains, reducing the need for additional expert\nannotations. However, many approaches struggle with reliability in the target\ndomain, an issue particularly critical in medical image segmentation, where\naccuracy and anatomical validity are essential. This challenge is further\nexacerbated in spatio-temporal data, where the lack of temporal consistency can\nsignificantly degrade segmentation quality, and particularly in\nechocardiography, where the presence of artifacts and noise can further hinder\nsegmentation performance. To address these issues, we present RL4Seg3D, an\nunsupervised domain adaptation framework for 2D + time echocardiography\nsegmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to\nenhance key landmark precision in its segmentations while processing full-sized\ninput videos. By leveraging reinforcement learning for image segmentation, our\napproach improves accuracy, anatomical validity, and temporal consistency while\nalso providing, as a beneficial side effect, a robust uncertainty estimator,\nwhich can be used at test time to further enhance segmentation performance. We\ndemonstrate the effectiveness of our framework on over 30,000 echocardiographic\nvideos, showing that it outperforms standard domain adaptation techniques\nwithout the need for any labels on the target domain. Code is available at\nhttps://github.com/arnaudjudge/RL4Seg3D.", "AI": {"tldr": "RL4Seg3D是一个无监督域适应框架，利用强化学习对2D+时间超声心动图进行分割，提高了准确性、解剖学有效性和时间一致性，且无需目标域标签。", "motivation": "域适应方法在医疗图像分割中（特别是时空数据如超声心动图）存在目标域可靠性不足、缺乏时间一致性、以及伪影和噪声干扰等问题，而准确性和解剖学有效性至关重要。", "method": "提出RL4Seg3D，一个用于2D+时间超声心动图分割的无监督域适应框架。它整合了新颖的奖励函数和融合方案，以提高关键地标的精确度，并处理全尺寸输入视频。该方法利用强化学习进行图像分割，并提供一个鲁棒的不确定性估计器。", "result": "RL4Seg3D提高了分割的准确性、解剖学有效性和时间一致性，并提供了一个鲁棒的不确定性估计器。在超过30,000个超声心动图视频上，它优于标准域适应技术，且无需任何目标域标签。", "conclusion": "RL4Seg3D有效解决了超声心动图分割中无监督域适应的挑战，在准确性、解剖学有效性和时间一致性方面表现出色，并提供了一个有益的不确定性估计器。"}}
{"id": "2510.13904", "categories": ["eess.IV", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.13904", "abs": "https://arxiv.org/abs/2510.13904", "authors": ["Akarsh Prabhakara", "Yawen Liu", "Aswin C. Sankaranarayanan", "Anthony Rowe", "Swarun Kumar"], "title": "Millimeter Wave Inverse Pinhole Imaging", "comment": null, "summary": "Millimeter wave (mmWave) radars are popular for perception in vision-denied\ncontexts due to their compact size. This paper explores emerging use-cases that\ninvolve static mount or momentarily-static compact radars, for example, a\nhovering drone. The key challenge with static compact radars is that their\nlimited form-factor also limits their angular resolution. This paper presents\nUmbra, a mmWave high resolution imaging system, that introduces the concept of\nrotating mmWave \"inverse pinholes\" for angular resolution enhancement. We\npresent the imaging system model, design, and evaluation of mmWave inverse\npinholes. The inverse pinhole is attractive for its lightweight nature, which\nenables low-power rotation, upgrading static-mount radars. We also show how\npropellers in aerial vehicles act as natural inverse pinholes and can enjoy the\nbenefits of high-resolution imaging even while they are momentarily static,\ne.g., hovering. Our evaluation shows Umbra resolving up to 2.5$^{\\circ}$ with\njust a single antenna, a 5$\\times$ improvement compared to 14$^{\\circ}$ from a\ncompact mmWave radar baseline.", "AI": {"tldr": "本文提出Umbra系统，通过引入旋转的毫米波“逆针孔”（如无人机螺旋桨）概念，显著提升了静态紧凑型毫米波雷达的角分辨率，实现了5倍的改进。", "motivation": "毫米波雷达因其紧凑性在视觉受限环境中广受欢迎，但静态或瞬时静态的紧凑型雷达受限于其尺寸，角分辨率较低，这限制了其在新兴应用场景（如悬停无人机）中的表现。", "method": "本文提出了毫米波高分辨率成像系统Umbra，核心是引入旋转的毫米波“逆针孔”概念来增强角分辨率。研究了成像系统模型、逆针孔的设计与评估，并指出飞行器螺旋桨可作为天然的逆针孔利用。", "result": "Umbra系统通过单个天线实现了2.5°的角分辨率，与紧凑型毫米波雷达基线的14°相比，性能提升了5倍。", "conclusion": "旋转的毫米波“逆针孔”（包括无人机螺旋桨等天然结构）能够有效提升静态紧凑型毫米波雷达的角分辨率，使其在高分辨率成像方面具备更广泛的应用潜力。"}}
{"id": "2510.14045", "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.14045", "abs": "https://arxiv.org/abs/2510.14045", "authors": ["Qinghua Ma", "Reetam Sen Biswas", "Denis Osipov", "Guannan Qu", "Soummya Kar", "Shimiao Li"], "title": "Multi-Period Sparse Optimization for Proactive Grid Blackout Diagnosis", "comment": null, "summary": "Existing or planned power grids need to evaluate survivability under extreme\nevents, like a number of peak load overloading conditions, which could possibly\ncause system collapses (i.e. blackouts). For realistic extreme events that are\ncorrelated or share similar patterns, it is reasonable to expect that the\ndominant vulnerability or failure sources behind them share the same locations\nbut with different severity. Early warning diagnosis that proactively\nidentifies the key vulnerabilities responsible for a number of system collapses\nof interest can significantly enhance resilience. This paper proposes a\nmulti-period sparse optimization method, enabling the discovery of {persistent\nfailure sources} across a sequence of collapsed systems with increasing system\nstress, such as rising demand or worsening contingencies. This work defines\npersistency and efficiently integrates persistency constraints to capture the\n``hidden'' evolving vulnerabilities. Circuit-theory based power flow\nformulations and circuit-inspired optimization heuristics are used to\nfacilitate the scalability of the method. Experiments on benchmark systems show\nthat the method reliably tracks persistent vulnerability locations under\nincreasing load stress, and solves with scalability to large systems ({on\naverage} taking {around} 200 s per scenario on 2000+ bus systems).", "AI": {"tldr": "本文提出了一种多周期稀疏优化方法，用于在电力系统压力（如负荷增加）下，识别并跟踪导致系统崩溃的持续性脆弱点或故障源，以增强电网的韧性。", "motivation": "现有电网需要评估在极端事件（如多个峰值负荷过载）下的生存能力，这些事件可能导致系统崩溃（停电）。对于相关或模式相似的极端事件，其主要脆弱点或故障源通常位于相同位置但严重程度不同。主动识别导致系统崩溃的关键脆弱点，能显著提升电网韧性，因此需要一种早期预警诊断方法。", "method": "本文提出了一种多周期稀疏优化方法，旨在发现一系列随着系统压力增加（如需求上升或故障恶化）而崩溃的系统中“持续性故障源”。该方法定义并有效整合了持续性约束，以捕捉“隐藏的”演变中的脆弱点。为提高方法的可扩展性，采用了基于电路理论的潮流公式和受电路启发优化启发式算法。", "result": "在基准系统上的实验表明，该方法能够可靠地跟踪在负荷压力增加下的持续性脆弱点位置，并能扩展到大型系统（例如，在2000+总线系统上，每个场景平均耗时约200秒）。", "conclusion": "所提出的多周期稀疏优化方法能够有效识别电力系统在不断增加的压力下的持续性故障源，为增强电网韧性提供了可靠且可扩展的早期预警诊断工具。"}}
{"id": "2510.14043", "categories": ["eess.SY", "cs.AI", "cs.CR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14043", "abs": "https://arxiv.org/abs/2510.14043", "authors": ["Shimiao Li", "Guannan Qu", "Bryan Hooi", "Vyas Sekar", "Soummya Kar", "Larry Pileggi"], "title": "Cyber-Resilient System Identification for Power Grid through Bayesian Integration", "comment": null, "summary": "Power grids increasingly need real-time situational awareness under the\never-evolving cyberthreat landscape. Advances in snapshot-based system\nidentification approaches have enabled accurately estimating states and\ntopology from a snapshot of measurement data, under random bad data and\ntopology errors. However, modern interactive, targeted false data can stay\nundetectable to these methods, and significantly compromise estimation\naccuracy. This work advances system identification that combines snapshot-based\nmethod with time-series model via Bayesian Integration, to advance cyber\nresiliency against both random and targeted false data. Using a distance-based\ntime-series model, this work can leverage historical data of different\ndistributions induced by changes in grid topology and other settings. The\nnormal system behavior captured from historical data is integrated into system\nidentification through a Bayesian treatment, to make solutions robust to\ntargeted false data. We experiment on mixed random anomalies (bad data,\ntopology error) and targeted false data injection attack (FDIA) to demonstrate\nour method's 1) cyber resilience: achieving over 70% reduction in estimation\nerror under FDIA; 2) anomalous data identification: being able to alarm and\nlocate anomalous data; 3) almost linear scalability: achieving comparable speed\nwith the snapshot-based baseline, both taking <1min per time tick on the large\n2,383-bus system using a laptop CPU.", "AI": {"tldr": "本文提出一种结合快照式系统辨识与时间序列模型的贝叶斯集成方法，旨在提高电力系统对随机和定向虚假数据的网络弹性，显著降低估计误差并实现异常数据识别，同时保持高效性。", "motivation": "电力系统日益需要实时态势感知，但现有基于快照的系统辨识方法对现代互动式、定向的虚假数据攻击（如虚假数据注入攻击FDIA）缺乏抵抗力，导致估计精度严重受损。", "method": "该研究通过贝叶斯集成将快照式系统辨识与基于距离的时间序列模型相结合。它利用历史数据（包括因电网拓扑变化引起的各种分布）捕捉正常的系统行为，并将这些信息融入系统辨识过程，以增强对定向虚假数据的鲁棒性。", "result": "1) 网络弹性：在虚假数据注入攻击（FDIA）下，估计误差降低超过70%；2) 异常数据识别：能够发出警报并定位异常数据；3) 近乎线性的可扩展性：在2,383节点的大型系统上，每时间步耗时不到1分钟（使用笔记本电脑CPU），与基线快照方法速度相当。", "conclusion": "所提出的贝叶斯集成方法显著提升了电力系统对随机和定向虚假数据的网络弹性，不仅提高了估计精度，实现了异常数据识别，而且在大型系统上展现出良好的可扩展性和效率。"}}
{"id": "2510.13827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13827", "abs": "https://arxiv.org/abs/2510.13827", "authors": ["Ashish Kattamuri", "Ishita Prasad", "Meetu Malhotra", "Arpita Vats", "Rahul Raja", "Albert Lie"], "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL", "comment": "20th International Workshop on Semantic and Social Media Adaptation &\n  Personalization", "summary": "Current Text-to-SQL methods are evaluated and only focused on executable\nqueries, overlooking the semantic alignment challenge -- both in terms of the\nsemantic meaning of the query and the correctness of the execution results.\nEven execution accuracy itself shows significant drops when moving from English\nto other languages, with an average decline of 6 percentage points across\nnon-English languages. We address these challenges by presenting a new\nframework that combines Group Relative Policy Optimization (GRPO) within a\nmultilingual contrastive reward signal to enhance both task efficiency and\nsemantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method\nteaches models to obtain better correspondence between SQL generation and user\nintent by combining a reward signal based on semantic similarity. On the\nseven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO\nimproved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and\nsemantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive\nreward signal in the GRPO framework further improved the average semantic\naccuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our\nexperiments showcase that a smaller, parameter-efficient 3B LLaMA model\nfine-tuned with our contrastive reward signal outperforms a much larger\nzero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from\n81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly\nmatches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using\njust 3,000 reinforcement learning training examples. These results demonstrate\nhow we can improve the performance of Text-to-SQL systems with contrastive\nrewards for directed semantic alignment, without requiring large-scale training\ndatasets.", "AI": {"tldr": "本文提出了一种结合群相对策略优化（GRPO）和多语言对比奖励信号的新框架，显著提升了跨语言Text-to-SQL系统的执行和语义准确性，尤其是在小模型和少量训练数据下。", "motivation": "当前的Text-to-SQL方法仅关注可执行查询，忽略了查询语义和执行结果正确性的语义对齐挑战。此外，从英语转向其他语言时，执行准确性显著下降，平均下降6个百分点。", "method": "提出了一种新框架，将群相对策略优化（GRPO）与多语言对比奖励信号相结合。该方法通过结合基于语义相似度的奖励信号，使模型更好地将SQL生成与用户意图对齐，从而提高跨语言场景下Text-to-SQL系统的任务效率和语义准确性。", "result": "在七语言MultiSpider数据集上，使用GRPO微调LLaMA-3-3B模型将执行准确性提高到87.4%（+26 pp），语义准确性提高到52.29%（+32.86 pp）。在GRPO框架中加入对比奖励信号后，平均语义准确性进一步提高到59.14%（+6.85 pp）。实验表明，使用对比奖励信号微调的3B LLaMA小模型在执行准确性上优于更大的零样本8B LLaMA模型（88.86% vs 81.43%），并且语义准确性几乎匹配（59.14% vs 68.57%），仅使用了3,000个强化学习训练样本。", "conclusion": "研究表明，通过对比奖励进行语义对齐可以显著提高Text-to-SQL系统的性能，即使使用较小的模型和有限的训练数据集，也能在跨语言场景中实现更好的表现。"}}
{"id": "2510.14018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14018", "abs": "https://arxiv.org/abs/2510.14018", "authors": ["Adam Morris", "Timothy Pelham", "Edmund R. Hunt"], "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms", "comment": null, "summary": "This paper introduces a method for designing spatially intelligent robot\nswarm behaviors to localize concealed radio emitters. We use differential\nevolution to generate geometric patrol routes that localize unknown signals\nindependently of emitter parameters, a key challenge in electromagnetic\nsurveillance. Patrol shape and antenna type are shown to influence information\ngain, which in turn determines the effective triangulation coverage. We\nsimulate a four-robot swarm across eight configurations, assigning\npre-generated patrol routes based on a specified patrol shape and sensing\ncapability (antenna type: omnidirectional or directional). An emitter is placed\nwithin the map for each trial, with randomized position, transmission power and\nfrequency. Results show that omnidirectional localization success rates are\ndriven primarily by source location rather than signal properties, with\nfailures occurring most often when sources are placed in peripheral areas of\nthe map. Directional antennas are able to overcome this limitation due to their\nhigher gain and directivity, with an average detection success rate of 98.75%\ncompared to 80.25% for omnidirectional. Average localization errors range from\n1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional\nsensing; while directional sensing also benefits from shorter patrol edges.\nThese results demonstrate that a swarm's ability to predict electromagnetic\nphenomena is directly dependent on its physical interaction with the\nenvironment. Consequently, spatial intelligence, realized here through\noptimized patrol routes and antenna selection, is a critical design\nconsideration for effective robotic surveillance.", "AI": {"tldr": "本文提出了一种通过差分进化优化巡逻路径和天线类型，设计机器人群空间智能行为来定位隐蔽无线电发射源的方法，并证明了定向天线在定位精度和成功率上优于全向天线。", "motivation": "在电磁监测中，独立于发射器参数定位未知信号是一个关键挑战。研究旨在开发一种方法，使机器人群能够有效、准确地定位隐蔽的无线电发射源。", "method": "该研究使用差分进化算法生成几何巡逻路线。通过模拟一个四机器人群体的八种配置，根据指定的巡逻形状和传感能力（全向或定向天线）分配预生成的巡逻路线。每次试验都在地图中放置一个发射器，其位置、发射功率和频率均随机化。研究分析了巡逻形状和天线类型对信息增益和有效三角测量覆盖范围的影响。", "result": "全向定位的成功率主要受信号源位置影响，外围区域的信号源最常导致定位失败。定向天线由于其更高的增益和指向性克服了这一限制，平均检测成功率为98.75%，而全向天线为80.25%。定向传感的平均定位误差范围为1.01-1.30米，全向传感为1.67-1.90米；定向传感还受益于更短的巡逻边缘。结果表明，机器人群预测电磁现象的能力直接取决于其与环境的物理交互。", "conclusion": "通过优化巡逻路线和天线选择实现的空间智能，是有效机器人监测的关键设计考量，能够显著提高隐蔽无线电发射源的定位成功率和精度。"}}
{"id": "2510.14052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14052", "abs": "https://arxiv.org/abs/2510.14052", "authors": ["Xixing Xue", "Dong Shen", "Steven X. Ding", "Dong Zhao"], "title": "Dual Detection Framework for Faults and Integrity Attacks in Cyber-Physical Control Systems", "comment": null, "summary": "Anomaly detection plays a vital role in the security and safety of\ncyber-physical control systems, and accurately distinguishing between different\nanomaly types is crucial for system recovery and mitigation. This study\nproposes a dual detection framework for anomaly detection and discrimination.\nBy leveraging the dynamic characteristics of control loops and the stealthiness\nfeatures of integrity attacks, the closed-loop stealthiness condition is first\nderived, and two dedicated detectors are designed and deployed on the\ncontroller side and the plant side, respectively, enabling joint plant fault\nand cyber attack detection. Moreover, by jointly analyzing the residual\nresponse of the two detectors corresponding to different anomalies, it is\nproved that the proposed method can distinguish between faults and integrity\nattacks due to the detectors' individual residual spaces. According to the\ndetector's residual space, the fault and attack detection performance is\nfurther improved by a two-stage optimization scheme. Simulation results\nvalidate the effectiveness of the proposed approach.", "AI": {"tldr": "本研究提出了一种双重检测框架，用于网络物理控制系统中的异常检测和判别，能有效区分系统故障和完整性攻击。", "motivation": "异常检测对于网络物理控制系统的安全至关重要，而准确区分不同异常类型对于系统恢复和缓解措施是关键。", "method": "该研究利用控制回路的动态特性和完整性攻击的隐蔽性，首先推导出闭环隐蔽性条件。然后，设计并分别在控制器侧和设备侧部署了两个专用检测器，以实现联合的设备故障和网络攻击检测。通过联合分析两个检测器对不同异常的残差响应，证明了该方法可以根据检测器的独立残差空间区分故障和完整性攻击。最后，通过两阶段优化方案进一步提高了故障和攻击检测性能。", "result": "所提出的方法能够区分故障和完整性攻击，并且通过优化方案进一步提高了检测性能。仿真结果验证了该方法的有效性。", "conclusion": "该双重检测框架能够有效地检测和区分网络物理控制系统中的异常（故障和完整性攻击），并具有改进的检测性能。"}}
{"id": "2510.13858", "categories": ["cs.AI", "I.6.4"], "pdf": "https://arxiv.org/pdf/2510.13858", "abs": "https://arxiv.org/abs/2510.13858", "authors": ["Raheleh Biglari", "Joachim Denil"], "title": "Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context", "comment": "10 pages", "summary": "Model validity is as critical as the model itself, especially when guiding\ndecision-making processes. Traditional approaches often rely on predefined\nvalidity frames, which may not always be available or sufficient. This paper\nintroduces the Decision Oriented Technique (DOTechnique), a novel method for\ndetermining model validity based on decision consistency rather than output\nsimilarity. By evaluating whether surrogate models lead to equivalent decisions\ncompared to high-fidelity models, DOTechnique enables efficient identification\nof validity regions, even in the absence of explicit validity boundaries. The\napproach integrates domain constraints and symbolic reasoning to narrow the\nsearch space, enhancing computational efficiency. A highway lane change system\nserves as a motivating example, demonstrating how DOTechnique can uncover the\nvalidity region of a simulation model. The results highlight the potential of\nthe technique to support finding model validity through decision-maker context.", "AI": {"tldr": "本文提出了一种名为DOTechnique的新方法，通过评估模型在决策一致性而非输出相似性方面的表现来确定模型有效性区域，尤其适用于决策指导。", "motivation": "传统模型有效性评估方法依赖预定义框架，但这些框架并非总是可用或充分，尤其在模型用于指导决策时，模型有效性至关重要。", "method": "DOTechnique通过比较替代模型与高保真模型所产生的决策是否等效来确定模型有效性，而非输出相似性。该方法集成了领域约束和符号推理以缩小搜索空间，从而提高计算效率。", "result": "DOTechnique能够有效地识别模型有效性区域，即使在没有明确有效性边界的情况下。通过一个高速公路换道系统的案例，证明了该技术能够揭示仿真模型的有效性区域，并支持通过决策者上下文找到模型有效性。", "conclusion": "DOTechnique为确定模型有效性提供了一种有潜力的新途径，它侧重于决策一致性，并通过整合决策者上下文来提高评估效率和实用性。"}}
{"id": "2510.14340", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14340", "abs": "https://arxiv.org/abs/2510.14340", "authors": ["Siva Teja Kakileti", "Bharath Govindaraju", "Sudhakar Sampangi", "Geetha Manjunath"], "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities", "comment": null, "summary": "Mammography, the current standard for breast cancer screening, has reduced\nsensitivity in women with dense breast tissue, contributing to missed or\ndelayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures\nfunctional vascular and metabolic cues that may complement mammographic\nstructural data. This study investigates whether a breast density-informed\nmulti-modal AI framework can improve cancer detection by dynamically selecting\nthe appropriate imaging modality based on breast tissue composition. A total of\n324 women underwent both mammography and thermal imaging. Mammography images\nwere analyzed using a multi-view deep learning model, while Thermalytix\nassessed thermal images through vascular and thermal radiomics. The proposed\nframework utilized Mammography AI for fatty breasts and Thermalytix AI for\ndense breasts, optimizing predictions based on tissue type. This multi-modal AI\nframework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity\nof 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI\n(sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity\n92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography\ndropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%),\nwhereas Thermalytix AI maintained high and consistent sensitivity in both\n(92.59% and 92.86%, respectively). This demonstrates that a density-informed\nmulti-modal AI framework can overcome key limitations of unimodal screening and\ndeliver high performance across diverse breast compositions. The proposed\nframework is interpretable, low-cost, and easily deployable, offering a\npractical path to improving breast cancer screening outcomes in both\nhigh-resource and resource-limited settings.", "AI": {"tldr": "本研究提出了一种基于乳腺密度的多模态AI框架，结合乳腺X线摄影和热成像技术，显著提高了乳腺癌筛查的准确性，尤其是在致密型乳腺中，克服了单一模态筛查的局限性。", "motivation": "乳腺X线摄影是乳腺癌筛查的标准方法，但在致密型乳腺女性中敏感性较低，导致漏诊或延迟诊断。研究旨在探索一种结合功能性热成像与结构性乳腺X线摄影的多模态AI框架，以提高癌症检测率。", "method": "研究招募了324名女性，同时进行乳腺X线摄影和热成像。乳腺X线图像由多视图深度学习模型分析，热图像由Thermalytix通过血管和热辐射组学评估。所提出的多模态AI框架根据乳腺组织类型动态选择：对脂肪型乳腺使用乳腺X线摄影AI，对致密型乳腺使用Thermalytix AI，以优化预测。", "result": "该多模态AI框架实现了94.55%的敏感性和79.93%的特异性，优于单独的乳腺X线摄影AI（敏感性81.82%，特异性86.25%）和Thermalytix AI（敏感性92.73%，特异性75.46%）。值得注意的是，乳腺X线摄影在致密型乳腺中的敏感性显著下降（67.86%），而Thermalytix AI在脂肪型和致密型乳腺中均保持了高且一致的敏感性（分别为92.59%和92.86%）。", "conclusion": "基于乳腺密度的多模态AI框架能够克服单一模态筛查的主要局限性，在不同乳腺组织构成中均能提供高性能。该框架具有可解释性、低成本和易于部署的特点，为改善高资源和资源有限地区乳腺癌筛查结果提供了一条实用的途径。"}}
{"id": "2510.13899", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13899", "abs": "https://arxiv.org/abs/2510.13899", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann", "Jörg Keckstein", "Simon Keckstein"], "title": "Post-surgical Endometriosis Segmentation in Laparoscopic Videos", "comment": "This is a demo paper that was already published\n  https://ieeexplore.ieee.org/document/9461900 but a preprint/author's copy is\n  needed for the funding agency", "summary": "Endometriosis is a common women's condition exhibiting a manifold visual\nappearance in various body-internal locations. Having such properties makes its\nidentification very difficult and error-prone, at least for laymen and\nnon-specialized medical practitioners. In an attempt to provide assistance to\ngynecologic physicians treating endometriosis, this demo paper describes a\nsystem that is trained to segment one frequently occurring visual appearance of\nendometriosis, namely dark endometrial implants. The system is capable of\nanalyzing laparoscopic surgery videos, annotating identified implant regions\nwith multi-colored overlays and displaying a detection summary for improved\nvideo browsing.", "AI": {"tldr": "本文描述了一个辅助妇科医生诊断子宫内膜异位症的系统，该系统能够从腹腔镜手术视频中分割出暗色子宫内膜异位病灶。", "motivation": "子宫内膜异位症的视觉表现多样且位于体内不同身体部位，使得其识别非常困难且容易出错，尤其对于非专业医疗人员。因此，需要为妇科医生提供辅助。", "method": "开发了一个系统，该系统经过训练，能够分割子宫内膜异位症中一种常见的视觉表现，即暗色子宫内膜植入物。", "result": "该系统能够分析腹腔镜手术视频，用多色叠加层标注识别出的植入物区域，并显示检测摘要以改善视频浏览。", "conclusion": "该系统通过自动识别和标注子宫内膜异位病灶，为妇科医生提供辅助，有助于提高诊断效率和准确性。"}}
{"id": "2510.13993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13993", "abs": "https://arxiv.org/abs/2510.13993", "authors": ["Jia Yun Chua", "Argyrios Zolotas", "Miguel Arana-Catania"], "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models", "comment": "11 pages, 7 figures, 8 tables. To be published in Applied AI Letters", "summary": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios.", "AI": {"tldr": "该研究结合传统视觉模型（如YOLO）和视觉语言模型（如LLaVA、ChatGPT、Gemini），显著提升了遥感图像中飞机检测和场景理解的准确性，尤其是在数据有限和图像质量下降的挑战性条件下。", "motivation": "传统视觉模型在遥感领域受限于对大量领域特定标注数据的需求，且在复杂环境中缺乏上下文理解能力。视觉语言模型（VLMs）虽能整合视觉和文本数据，但在遥感领域的应用，特别是其通用性方面，尚未得到充分探索。", "method": "该研究通过将YOLO等传统视觉模型与LLaVA、ChatGPT和Gemini等视觉语言模型相结合，旨在实现更准确和上下文感知的遥感图像解释，重点关注飞机检测和场景理解。性能评估在标注、未标注以及对遥感至关重要的降级图像场景下进行。", "result": "研究结果显示，在飞机检测和计数精度方面，模型平均MAE提高了48.46%，尤其是在原始和降级场景下的挑战性条件下表现突出。在遥感图像的综合理解方面，CLIPScore提高了6.17%。", "conclusion": "所提出的结合传统视觉模型和视觉语言模型的方法为更先进、高效的遥感图像分析铺平了道路，特别是在少样本学习场景中具有显著潜力。"}}
{"id": "2510.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/.", "AI": {"tldr": "OATH是一种多智能体任务分配与规划（MATP）框架，通过引入障碍物感知策略、自适应采样和聚类-拍卖-选择框架，解决了MATP在障碍物丰富环境中的可扩展性、空间推理和适应性挑战，并利用大型语言模型（LLM）实现实时规划指导。", "motivation": "多智能体任务分配与规划（MATP）在可扩展性、空间推理以及在障碍物丰富环境中的适应性方面面临巨大挑战。", "method": "1. 开发了自适应Halton序列地图，首次将Halton采样应用于MATP并根据障碍物分布调整采样密度。2. 提出了一个聚类-拍卖-选择框架，该框架结合了障碍物感知聚类、加权拍卖和集群内任务选择。3. 利用大型语言模型（LLM）解释人类指令并实时指导规划器。", "result": "与现有最先进的MATP基线相比，OATH在NVIDIA Isaac Sim中验证，显示出在任务分配质量、可扩展性、对动态变化的适应性以及整体执行性能方面的显著提升。", "conclusion": "OATH通过其新颖的障碍物感知策略、多机制框架和LLM集成，有效解决了MATP的挑战，实现了异构机器人团队的高效协调、可扩展性和近乎最优的分配性能。"}}
{"id": "2510.13979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13979", "abs": "https://arxiv.org/abs/2510.13979", "authors": ["Supriti Sinhamahapatra", "Jan Niehues"], "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks", "comment": null, "summary": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily\nrely on acoustic information while disregarding additional multi-modal context.\nHowever, visual information are essential in disambiguation and adaptation.\nWhile most work focus on speaker images to handle noise conditions, this work\nalso focuses on integrating presentation slides for the use cases of scientific\npresentation.\n  In a first step, we create a benchmark for multi-modal presentation including\nan automatic analysis of transcribing domain-specific terminology. Next, we\nexplore methods for augmenting speech models with multi-modal information. We\nmitigate the lack of datasets with accompanying slides by a suitable approach\nof data augmentation. Finally, we train a model using the augmented dataset,\nresulting in a relative reduction in word error rate of approximately 34%,\nacross all words and 35%, for domain-specific terms compared to the baseline\nmodel.", "AI": {"tldr": "该研究通过整合演示文稿幻灯片作为多模态信息，显著提高了科学演讲场景下自动语音识别（ASR）的准确性，特别是对于领域特定术语。", "motivation": "当前最先进的ASR系统主要依赖声学信息，忽略了多模态上下文（如视觉信息），而视觉信息在消歧和适应性方面至关重要。现有工作多关注说话人图像以处理噪音，但本研究旨在探索整合演示文稿幻灯片在科学演讲场景中的应用。", "method": "首先，创建了一个多模态演讲基准，并自动分析转录领域特定术语。其次，探索了用多模态信息（幻灯片）增强语音模型的方法。通过数据增强方法弥补了缺乏带幻灯片数据集的问题。最后，使用增强后的数据集训练模型。", "result": "与基线模型相比，该方法使所有词的词错误率相对降低了约34%，领域特定术语的词错误率相对降低了约35%。", "conclusion": "将演示文稿幻灯片整合到ASR系统中作为多模态上下文，能够显著提高ASR性能，尤其是在科学演讲场景下对领域特定术语的识别效果更佳。"}}
{"id": "2510.13828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13828", "abs": "https://arxiv.org/abs/2510.13828", "authors": ["Ratna Kandala", "Akshata Kishore Moharir", "Divya Arvinda Nayak"], "title": "From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) has been presented as the critical\ncomponent for unlocking the potential of machine learning in mental health\nscreening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI\ntechniques, such as SHAP and LIME, excel at producing technically faithful\noutputs such as feature importance scores, but fail to deliver clinically\nrelevant, actionable insights that can be used by clinicians or understood by\npatients. This disconnect between technical transparency and human utility is\nthe primary barrier to real-world adoption. This paper argues that this gap is\na translation problem and proposes the Generative Operational Framework, a\nnovel system architecture that leverages Large Language Models (LLMs) as a\ncentral translation engine. This framework is designed to ingest the raw,\ntechnical outputs from diverse XAI tools and synthesize them with clinical\nguidelines (via RAG) to automatically generate human-readable, evidence-backed\nclinical narratives. To justify our solution, we provide a systematic analysis\nof the components it integrates, tracing the evolution from intrinsic models to\ngenerative XAI. We demonstrate how this framework directly addresses key\noperational barriers, including workflow integration, bias mitigation, and\nstakeholder-specific communication. This paper also provides a strategic\nroadmap for moving the field beyond the generation of isolated data points\ntoward the delivery of integrated, actionable, and trustworthy AI in clinical\npractice.", "AI": {"tldr": "XAI在心理健康筛查中存在“实验室到临床”的差距，现有技术缺乏临床相关、可操作的见解。本文提出“生成式操作框架”，利用大型语言模型(LLM)作为翻译引擎，将XAI技术输出与临床指南结合，生成人类可读的临床叙述，以弥合这一差距并促进实际应用。", "motivation": "尽管可解释人工智能(XAI)被认为是解锁机器学习在心理健康筛查(MHS)中潜力的关键，但“实验室到临床”的差距依然存在。现有的XAI技术（如SHAP和LIME）虽然能生成技术上忠实的输出，但无法提供临床医生或患者能理解的、具有临床相关性且可操作的见解，这种技术透明度与人类实用性之间的脱节是实际应用的主要障碍。", "method": "本文提出“生成式操作框架”(Generative Operational Framework)，这是一种新颖的系统架构，利用大型语言模型(LLM)作为核心翻译引擎。该框架旨在摄取来自不同XAI工具的原始技术输出，并将其与临床指南（通过RAG技术）综合，自动生成人类可读、有证据支持的临床叙述。文章还系统分析了其集成的组件，并追溯了从内在模型到生成式XAI的演变。", "result": "该框架直接解决了关键的操作障碍，包括工作流程集成、偏见缓解和面向特定利益相关者的沟通问题。它将技术透明度转化为人类可用的见解，从而促进了XAI在心理健康筛查中的实际应用。", "conclusion": "本文提供了一个战略路线图，旨在推动该领域超越生成孤立的数据点，实现临床实践中集成、可操作和值得信赖的AI。通过弥合技术透明度与人类实用性之间的鸿沟，该框架有望促进XAI在心理健康筛查中的实际落地和应用。"}}
{"id": "2510.13829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13829", "abs": "https://arxiv.org/abs/2510.13829", "authors": ["Shinwoo Park", "Hyejin Park", "Hyeseon Ahn", "Yo-Sub Han"], "title": "A Linguistics-Aware LLM Watermarking via Syntactic Predictability", "comment": null, "summary": "As large language models (LLMs) continue to advance rapidly, reliable\ngovernance tools have become critical. Publicly verifiable watermarking is\nparticularly essential for fostering a trustworthy AI ecosystem. A central\nchallenge persists: balancing text quality against detection robustness. Recent\nstudies have sought to navigate this trade-off by leveraging signals from model\noutput distributions (e.g., token-level entropy); however, their reliance on\nthese model-specific signals presents a significant barrier to public\nverification, as the detection process requires access to the logits of the\nunderlying model. We introduce STELA, a novel framework that aligns watermark\nstrength with the linguistic degrees of freedom inherent in language. STELA\ndynamically modulates the signal using part-of-speech (POS) n-gram-modeled\nlinguistic indeterminacy, weakening it in grammatically constrained contexts to\npreserve quality and strengthen it in contexts with greater linguistic\nflexibility to enhance detectability. Our detector operates without access to\nany model logits, thus facilitating publicly verifiable detection. Through\nextensive experiments on typologically diverse languages-analytic English,\nisolating Chinese, and agglutinative Korean-we show that STELA surpasses prior\nmethods in detection robustness. Our code is available at\nhttps://github.com/Shinwoo-Park/stela_watermark.", "AI": {"tldr": "STELA是一个新型的LLM水印框架，通过利用语言的自由度动态调整水印强度，在不访问模型对数的情况下实现公开可验证的检测，从而在文本质量和检测鲁棒性之间取得更好的平衡。", "motivation": "随着大型语言模型（LLMs）的快速发展，可靠的治理工具变得至关重要。公开可验证的水印技术对于建立可信赖的AI生态系统尤为关键。现有方法在平衡文本质量和检测鲁棒性方面面临挑战，且其对模型输出分布（如token级熵）的依赖使得检测过程需要访问底层模型的对数（logits），从而阻碍了公开验证。", "method": "本文提出了STELA框架，该框架将水印强度与语言固有的语言自由度对齐。STELA使用词性（POS）n-gram建模的语言不确定性动态调制信号：在语法受限的上下文中减弱信号以保持质量，在语言灵活性更大的上下文中增强信号以提高可检测性。其检测器无需访问任何模型对数，从而便于公开可验证的检测。", "result": "通过在不同类型语言（分析型英语、孤立型汉语和黏着型韩语）上进行广泛实验，结果表明STELA在检测鲁棒性方面超越了现有方法。", "conclusion": "STELA提供了一种新颖、公开可验证的水印解决方案，通过利用语言的内在特性，有效地平衡了文本质量和检测鲁棒性，克服了先前方法对模型对数访问的限制。"}}
{"id": "2510.14065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14065", "abs": "https://arxiv.org/abs/2510.14065", "authors": ["Gaoyuan Liu", "Joris de Winter", "Yuri Durodie", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning", "comment": null, "summary": "Task and motion planning (TAMP) for robotics manipulation necessitates\nlong-horizon reasoning involving versatile actions and skills. While\ndeterministic actions can be crafted by sampling or optimizing with certain\nconstraints, planning actions with uncertainty, i.e., probabilistic actions,\nremains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)\nexcels in acquiring versatile, yet short-horizon, manipulation skills that are\nrobust with uncertainties. In this letter, we design a method that integrates\nRL skills into TAMP pipelines. Besides the policy, a RL skill is defined with\ndata-driven logical components that enable the skill to be deployed by symbolic\nplanning. A plan refinement sub-routine is designed to further tackle the\ninevitable effect uncertainties. In the experiments, we compare our method with\nbaseline hierarchical planning from both TAMP and RL fields and illustrate the\nstrength of the method. The results show that by embedding RL skills, we extend\nthe capability of TAMP to domains with probabilistic skills, and improve the\nplanning efficiency compared to the previous methods.", "AI": {"tldr": "本文提出一种将强化学习（RL）技能集成到任务与运动规划（TAMP）流程中的方法，以应对机器人操作中不确定性动作的挑战，从而扩展TAMP的能力并提高规划效率。", "motivation": "机器人操作中的任务与运动规划（TAMP）需要长期的推理和多功能动作，但难以处理具有不确定性的概率性动作。而强化学习（RL）擅长学习对不确定性鲁棒的多功能、短周期操作技能。", "method": "设计了一种将RL技能整合到TAMP管道中的方法。RL技能不仅包含策略，还通过数据驱动的逻辑组件进行定义，使其能够被符号规划部署。此外，还设计了一个计划细化子程序来处理不确定性影响。", "result": "实验结果表明，通过嵌入RL技能，该方法将TAMP的能力扩展到包含概率性技能的领域，并且与现有方法相比，提高了规划效率。该方法与TAMP和RL领域的基线分层规划进行了比较，并展示了其优势。", "conclusion": "通过将RL技能嵌入到TAMP中，可以有效处理概率性技能，扩展TAMP的能力，并显著提高规划效率。"}}
{"id": "2510.14075", "categories": ["eess.SY", "cs.AI", "cs.SY", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.14075", "abs": "https://arxiv.org/abs/2510.14075", "authors": ["Milad Hoseinpour", "Vladimir Dvorkin"], "title": "DiffOPF: Diffusion Solver for Optimal Power Flow", "comment": "7 pages, 4 figures, 2 tables", "summary": "The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads\nto dispatch setpoints. The variability of system parameters (e.g., admittances,\ntopology) further contributes to the multiplicity of dispatch setpoints for a\ngiven load. Existing deep learning OPF solvers are single-valued and thus fail\nto capture the variability of system parameters unless fully represented in the\nfeature space, which is prohibitive. To solve this problem, we introduce a\ndiffusion-based OPF solver, termed \\textit{DiffOPF}, that treats OPF as a\nconditional sampling problem. The solver learns the joint distribution of loads\nand dispatch setpoints from operational history, and returns the marginal\ndispatch distributions conditioned on loads. Unlike single-valued solvers,\nDiffOPF enables sampling statistically credible warm starts with favorable cost\nand constraint satisfaction trade-offs. We explore the sample complexity of\nDiffOPF to ensure the OPF solution within a prescribed distance from the\noptimization-based solution, and verify this experimentally on power system\nbenchmarks.", "AI": {"tldr": "本文提出了一种名为DiffOPF的基于扩散的OPF求解器，它将OPF视为条件采样问题，以解决现有深度学习求解器无法捕获OPF多值性和系统参数变异性的问题，并能生成具有良好成本和约束满意度权衡的统计可信热启动。", "motivation": "最优潮流（OPF）是一个多值、非凸的映射，从负荷到调度设定点。系统参数（如导纳、拓扑）的变异性进一步导致给定负荷存在多个调度设定点。现有深度学习OPF求解器是单值的，因此无法捕捉系统参数的变异性，除非在特征空间中完整表示，但这成本过高。", "method": "本文引入了一种基于扩散的OPF求解器，称为DiffOPF，它将OPF视为一个条件采样问题。该求解器从运行历史中学习负荷和调度设定点的联合分布，并返回以负荷为条件的边际调度分布。", "result": "与单值求解器不同，DiffOPF能够采样出统计可信的热启动，这些热启动在成本和约束满足方面具有良好的权衡。研究还探讨了DiffOPF的样本复杂度，以确保OPF解决方案与基于优化的解决方案之间的距离在规定范围内，并通过电力系统基准进行了实验验证。", "conclusion": "DiffOPF成功地通过将OPF视为条件采样问题，解决了其多值性和系统参数变异性给现有深度学习求解器带来的挑战，能够生成高质量的调度分布和热启动，从而在成本和约束满足方面提供更好的权衡。"}}
{"id": "2510.14713", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.14713", "abs": "https://arxiv.org/abs/2510.14713", "authors": ["Tingyu Lin", "Armin Dadras", "Florian Kleber", "Robert Sablatnig"], "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models", "comment": "5 pages, accepted at AIROV2025", "summary": "Camera movement conveys spatial and narrative information essential for\nunderstanding video content. While recent camera movement classification (CMC)\nmethods perform well on modern datasets, their generalization to historical\nfootage remains unexplored. This paper presents the first systematic evaluation\nof deep video CMC models on archival film material. We summarize representative\nmethods and datasets, highlighting differences in model design and label\ndefinitions. Five standard video classification models are assessed on the\nHISTORIAN dataset, which includes expert-annotated World War II footage. The\nbest-performing model, Video Swin Transformer, achieves 80.25% accuracy,\nshowing strong convergence despite limited training data. Our findings\nhighlight the challenges and potential of adapting existing models to\nlow-quality video and motivate future work combining diverse input modalities\nand temporal architectures.", "AI": {"tldr": "本文首次系统评估了深度视频摄像机运动分类模型在历史档案影片上的泛化能力，发现Video Swin Transformer表现最佳，并探讨了现有模型适应低质量视频的潜力和挑战。", "motivation": "摄像机运动对理解视频内容至关重要。尽管现代摄像机运动分类（CMC）方法在现代数据集上表现良好，但它们对历史影片的泛化能力尚未被探索。", "method": "本文总结了代表性的方法和数据集，并评估了五种标准视频分类模型在HISTORIAN数据集（包含专家标注的二战影片）上的表现。", "result": "表现最佳的模型是Video Swin Transformer，在有限的训练数据下取得了80.25%的准确率，显示出强大的收敛性。", "conclusion": "研究结果突出了现有模型适应低质量视频的挑战和潜力，并为未来结合多样化输入模态和时间架构的工作提供了动力。"}}
{"id": "2510.13830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13830", "abs": "https://arxiv.org/abs/2510.13830", "authors": ["Zhongze Cai", "Xiaocheng Li"], "title": "Users as Annotators: LLM Preference Learning from Comparison Mode", "comment": null, "summary": "Pairwise preference data have played an important role in the alignment of\nlarge language models (LLMs). Each sample of such data consists of a prompt,\ntwo different responses to the prompt, and a binary label indicating which of\nthe two responses is better. The labels are usually annotated by professional\nhuman annotators. In this paper, we consider an alternative approach to collect\npairwise preference data -- user annotation from comparison mode. With the\nincreasingly wider adoption of LLMs among the population, users are\ncontributing more and more of their preference labels through their daily\ninteractions with the LLMs. The upside of such labels is that users are the\nbest experts in judging the responses to their own queries/prompts, but the\ndownside is the lack of quality control in these labels. In this paper, we\nconsider a new idea of generating two responses from two different models or\ntwo different versions of the same model. The asymmetry allows us to make an\ninference of the user's data quality through our proposed user behavior model.\nWe develop an expectation-maximization algorithm to estimate a latent quality\nfactor of the user, and filter users' annotation data accordingly. The\ndownstream task shows the effectiveness of our approach in both capturing the\nuser behavior and data filtering for LLM alignment.", "AI": {"tldr": "本文提出一种利用用户在LLM比较模式下生成的偏好数据进行模型对齐的方法，通过引入不对称响应生成和用户行为模型，并结合EM算法估计用户质量因子，从而过滤低质量数据，有效提升LLM对齐效果。", "motivation": "LLM对齐中的成对偏好数据通常由专业标注员提供，成本高昂。用户标注是潜在的替代方案，用户最了解自己的查询需求，但其数据质量缺乏控制。本文旨在利用用户标注数据的优势，同时解决其质量控制问题。", "method": "研究者从比较模式中收集用户偏好标注数据。为了评估用户数据质量，他们提出一种新方法：从两个不同模型或同一模型的不同版本生成两个响应，这种不对称性允许通过提出的用户行为模型推断用户数据质量。开发了期望最大化（EM）算法来估计用户的潜在质量因子，并据此过滤用户的标注数据。", "result": "下游任务显示，该方法在捕捉用户行为和为LLM对齐过滤数据方面均有效。", "conclusion": "通过不对称响应生成、用户行为模型和EM算法，可以有效估计并利用用户标注数据的质量因子进行数据过滤，从而成功地将用户生成的偏好数据应用于LLM对齐，解决了其固有的质量控制挑战。"}}
{"id": "2510.13995", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13995", "abs": "https://arxiv.org/abs/2510.13995", "authors": ["Kelvin Szolnoky", "Anders Blilie", "Nita Mulliqi", "Toyonori Tsuzuki", "Hemamali Samaratunga", "Matteo Titus", "Xiaoyi Ji", "Sol Erika Boman", "Einar Gudlaugsson", "Svein Reidar Kjosavik", "José Asenjo", "Marcello Gambacorta", "Paolo Libretti", "Marcin Braun", "Radisław Kordek", "Roman Łowicki", "Brett Delahunt", "Kenneth A. Iczkowski", "Theo van der Kwast", "Geert J. L. H. van Leenders", "Katia R. M. Leite", "Chin-Chen Pan", "Emiel Adrianus Maria Janssen", "Martin Eklund", "Lars Egevad", "Kimmo Kartasalo"], "title": "Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer", "comment": null, "summary": "Background: Cribriform morphology in prostate cancer is a histological\nfeature that indicates poor prognosis and contraindicates active surveillance.\nHowever, it remains underreported and subject to significant interobserver\nvariability amongst pathologists. We aimed to develop and validate an AI-based\nsystem to improve cribriform pattern detection.\n  Methods: We created a deep learning model using an EfficientNetV2-S encoder\nwith multiple instance learning for end-to-end whole-slide classification. The\nmodel was trained on 640 digitised prostate core needle biopsies from 430\npatients, collected across three cohorts. It was validated internally (261\nslides from 171 patients) and externally (266 slides, 104 patients from three\nindependent cohorts). Internal validation cohorts included laboratories or\nscanners from the development set, while external cohorts used completely\nindependent instruments and laboratories. Annotations were provided by three\nexpert uropathologists with known high concordance. Additionally, we conducted\nan inter-rater analysis and compared the model's performance against nine\nexpert uropathologists on 88 slides from the internal validation cohort.\n  Results: The model showed strong internal validation performance (AUC: 0.97,\n95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external\nvalidation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:\n0.45-0.64). In our inter-rater analysis, the model achieved the highest average\nagreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine\npathologists whose Cohen's kappas ranged from 0.35 to 0.62.\n  Conclusion: Our AI model demonstrates pathologist-level performance for\ncribriform morphology detection in prostate cancer. This approach could enhance\ndiagnostic reliability, standardise reporting, and improve treatment decisions\nfor prostate cancer patients.", "AI": {"tldr": "该研究开发并验证了一个基于AI的系统，用于检测前列腺癌中的筛状形态，该系统表现出病理学家水平的性能，有望提高诊断可靠性。", "motivation": "前列腺癌中的筛状形态是预后不良的组织学特征，且不建议主动监测。然而，其报告不足，且病理学家之间存在显著的判读差异，因此需要一个更客观、一致的检测方法。", "method": "研究构建了一个深度学习模型，采用EfficientNetV2-S编码器和多实例学习进行端到端全玻片分类。该模型在来自430名患者的640张数字化前列腺核心针活检玻片上进行训练，并在内部（261张玻片）和外部（266张玻片，来自三个独立队列）进行了验证。模型性能还与九名专家泌尿病理学家在88张玻片上进行了比较。", "result": "模型在内部验证中表现出色（AUC: 0.97, Kappa: 0.81），在外部验证中也显示出稳健性（AUC: 0.90, Kappa: 0.55）。在判读一致性分析中，模型取得了最高的平均一致性（Kappa: 0.66），优于所有九名病理学家（Kappa范围为0.35至0.62）。", "conclusion": "该AI模型在前列腺癌筛状形态检测方面表现出病理学家水平的性能。这种方法有望提高诊断可靠性，标准化报告，并改进前列腺癌患者的治疗决策。"}}
{"id": "2510.14946", "categories": ["eess.IV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14946", "abs": "https://arxiv.org/abs/2510.14946", "authors": ["Romina Aalishah", "Mozhgan Navardi", "Tinoosh Mohsenin"], "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices", "comment": "The 11th IEEE International Conference on Edge Computing and Scalable\n  Cloud (IEEE EdgeCom 2025)", "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity.", "AI": {"tldr": "该论文提出了EdgeNavMamba，一个基于强化学习的框架，用于在资源受限的边缘设备上实现目标导向导航。它结合了高效的Mamba目标检测模型，并通过模型压缩显著降低了模型大小和能耗，同时保持了高精度和导航成功率。", "motivation": "在自主导航领域，特别是在资源受限的边缘设备上运行实时应用时，部署高效准确的深度学习模型一直是一个挑战。边缘设备计算能力和内存有限，因此模型效率和压缩至关重要。", "method": "作者提出了EdgeNavMamba框架，该框架基于强化学习实现目标导向导航，并采用高效的Mamba目标检测模型。为训练和评估检测器，他们引入了一个在多样室内环境中收集的自定义形状检测数据集。目标检测器作为预处理模块，从视觉输入中提取边界框（BBOX），然后将其传递给强化学习策略以控制目标导向导航。", "result": "实验结果显示，学生模型在NVIDIA Jetson Orin Nano和Raspberry Pi 5边缘设备上，模型大小减少了67%，每次推理能耗降低了高达73%，同时保持了与教师模型相同的性能。EdgeNavMamba在MiniWorld和IsaacLab模拟器中保持了高检测精度，与基线相比参数减少了31%。在MiniWorld模拟器中，导航策略在不同复杂度的环境中实现了超过90%的成功率。", "conclusion": "EdgeNavMamba成功地解决了在资源受限边缘设备上部署高效准确深度学习模型的挑战。它通过结合高效的Mamba目标检测和强化学习，实现了显著的模型压缩和能耗降低，同时在自主导航任务中保持了高精度和出色的导航性能。"}}
{"id": "2510.14100", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14100", "abs": "https://arxiv.org/abs/2510.14100", "authors": ["Rohan Walia", "Mitchell Black", "Andrew Schoer", "Kevin Leahy"], "title": "Belief Space Control of Safety-Critical Systems Under State-Dependent Measurement Noise", "comment": "Preprint - Submitted to the 2026 American Control Conference", "summary": "Safety-critical control is imperative for deploying autonomous systems in the\nreal world. Control Barrier Functions (CBFs) offer strong safety guarantees\nwhen accurate system and sensor models are available. However, widely used\nadditive, fixed-noise models are not representative of complex sensor\nmodalities with state-dependent error characteristics. Although CBFs have been\ndesigned to mitigate uncertainty using fixed worst-case bounds on measurement\nnoise, this approach can lead to overly-conservative control. To solve this\nproblem, we extend the Belief Control Barrier Function (BCBF) framework to\naccommodate state-dependent measurement noise via the Generalized Extended\nKalman Filter (GEKF) algorithm, which models measurement noise as a linear\nfunction of the state. Using the original BCBF framework as baseline, we\ndemonstrate the performance of the BCBF-GEKF approach through simulation\nresults on a 1D single integrator setpoint tracking scenario and 2D unicycle\nkinematics trajectory tracking scenario. Our results confirm that the BCBF-GEKF\napproach offers less conservative control with greater safety.", "AI": {"tldr": "本文通过将广义扩展卡尔曼滤波器（GEKF）集成到信念控制障碍函数（BCBF）框架中，解决了自动系统中状态依赖测量噪声导致的过度保守控制问题，实现了更安全、更少保守的控制。", "motivation": "在自动系统中，控制障碍函数（CBFs）在系统和传感器模型准确时能提供强大的安全保障。然而，传统的固定加性噪声模型无法代表具有状态依赖误差特性的复杂传感器模态。使用固定最坏情况测量噪声界限来缓解不确定性会导致过度保守的控制，因此需要一种方法来处理状态依赖的测量噪声，以实现更优的控制。", "method": "本文将信念控制障碍函数（BCBF）框架扩展，通过广义扩展卡尔曼滤波器（GEKF）算法来适应状态依赖的测量噪声。GEKF算法将测量噪声建模为状态的线性函数。研究通过1D单积分器设定点跟踪和2D独轮车运动学轨迹跟踪场景的仿真结果，将BCBF-GEKF方法与原始BCBF框架进行了性能比较。", "result": "仿真结果表明，BCBF-GEKF方法与基准BCBF框架相比，能够提供更少保守且具有更高安全性的控制。", "conclusion": "BCBF-GEKF方法成功地处理了状态依赖的测量噪声，有效解决了传统CBF方法中因不确定性处理不当而导致的过度保守控制问题，从而提高了自主系统的安全性和控制性能。"}}
{"id": "2510.13985", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13985", "abs": "https://arxiv.org/abs/2510.13985", "authors": ["María Victoria Carro", "Denise Alejandra Mester", "Francisca Gauna Selasco", "Giovanni Franco Gabriel Marraffini", "Mario Alejandro Leiva", "Gerardo I. Simari", "María Vanina Martinez"], "title": "Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment", "comment": null, "summary": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this work,\nwe examine whether large language models are prone to developing causal\nillusions when faced with a classic cognitive science paradigm: the contingency\njudgment task. To investigate this, we constructed a dataset of 1,000 null\ncontingency scenarios (in which the available information is not sufficient to\nestablish a causal relationship between variables) within medical contexts and\nprompted LLMs to evaluate the effectiveness of potential causes. Our findings\nshow that all evaluated models systematically inferred unwarranted causal\nrelationships, revealing a strong susceptibility to the illusion of causality.\nWhile there is ongoing debate about whether LLMs genuinely understand causality\nor merely reproduce causal language without true comprehension, our findings\nsupport the latter hypothesis and raise concerns about the use of language\nmodels in domains where accurate causal reasoning is essential for informed\ndecision-making.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在因果判断任务中表现出“因果错觉”，即使在缺乏支持证据的情况下也推断出不合理的因果关系。", "motivation": "因果学习中的“因果错觉”是一种常见的人类认知偏差，它被认为是社会偏见、刻板印象、错误信息和迷信思维的根源。本研究旨在探讨大型语言模型是否也容易产生这种错觉，特别是在面对经典的列联判断任务时。", "method": "研究构建了一个包含1000个零列联场景（即信息不足以建立因果关系的场景）的医学背景数据集。然后，研究人员提示LLMs评估这些场景中潜在原因的有效性，以观察它们是否会推断出不合理的因果关系。", "result": "所有被评估的LLMs都系统性地推断出不合理的因果关系，这表明它们对“因果错觉”具有很强的敏感性。这一发现支持了LLMs只是复制因果语言而非真正理解因果关系的假设。", "conclusion": "研究结果支持了LLMs可能只是在没有真正理解的情况下复制因果语言的假设，并对在需要准确因果推理以做出明智决策的领域中使用语言模型提出了担忧。"}}
{"id": "2510.14072", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14072", "abs": "https://arxiv.org/abs/2510.14072", "authors": ["Hemjyoti Das", "Christian Ott"], "title": "Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load", "comment": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "In this work, we present a novel control approach based on partial feedback\nlinearization (PFL) for the stabilization of a suspended aerial platform with\nan attached load. Such systems are envisioned for various applications in\nconstruction sites involving cranes, such as the holding and transportation of\nheavy objects. Our proposed control approach considers the underactuation of\nthe whole system while utilizing its coupled dynamics for stabilization. We\ndemonstrate using numerical stability analysis that these coupled terms are\ncrucial for the stabilization of the complete system. We also carried out\nrobustness analysis of the proposed approach in the presence of external wind\ndisturbances, sensor noise, and uncertainties in system dynamics. As our\nenvisioned target application involves cranes in outdoor construction sites,\nour control approaches rely on only onboard sensors, thus making it suitable\nfor such applications. We carried out extensive simulation studies and\nexperimental tests to validate our proposed control approach.", "AI": {"tldr": "本文提出了一种基于部分反馈线性化（PFL）的新型控制方法，用于稳定带有悬挂负载的空中平台，并通过仿真和实验验证了其有效性和鲁棒性。", "motivation": "此研究旨在为建筑工地（如起重机）中涉及重物抓取和运输的应用场景，提供一种能够稳定带有悬挂负载的空中平台的控制解决方案。特别关注在户外环境下仅依靠机载传感器进行控制的需求。", "method": "提出了一种基于部分反馈线性化（PFL）的新型控制方法。该方法考虑了整个系统的欠驱动特性，并利用其耦合动力学进行稳定。通过数值稳定性分析证明了耦合项对系统稳定的重要性。还进行了鲁棒性分析，以评估在外部风扰动、传感器噪声和系统动力学不确定性下的性能。控制方法仅依赖机载传感器。通过广泛的仿真研究和实验测试进行验证。", "result": "数值稳定性分析表明，系统中的耦合项对于整个系统的稳定至关重要。所提出的控制方法在存在外部风扰动、传感器噪声和系统动力学不确定性的情况下表现出良好的鲁棒性。仿真和实验测试验证了该控制方法的有效性。", "conclusion": "所提出的基于PFL的控制方法能够有效稳定带有悬挂负载的空中平台，且具有良好的鲁棒性。该方法仅依赖机载传感器，使其非常适合户外建筑工地中涉及起重机的应用。"}}
{"id": "2510.13831", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13831", "abs": "https://arxiv.org/abs/2510.13831", "authors": ["Chao Han", "Yijuan Liang", "Zihao Xuan", "Daokuan Wu", "Wei Zhang", "Xiaoyu Shen"], "title": "Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference", "comment": null, "summary": "The deployment of large language models (LLMs) in real-world applications is\nincreasingly limited by their high inference cost. While recent advances in\ndynamic token-level computation allocation attempt to improve efficiency by\nselectively activating model components per token, existing methods rely on\ngreedy routing--a myopic execute-or-skip mechanism that often leads to\nirreversible information loss and suboptimal token selection. This paper\nintroduces informed routing, a new paradigm that proactively addresses these\nissues. The key insight is to assess not only a token's immediate importance\nbut also its recoverability, i.e., how well its transformation can be\napproximated. To this end, we propose the Lightweight Feature Forecaster (LFF),\na small predictive module that estimates a unit's output before routing\ndecisions are made. This enables a flexible execute-or-approximate policy that\npreserves model fidelity while drastically reducing computation. Extensive\nexperiments on both language modeling and reasoning tasks show that informed\nrouting achieves state-of-the-art efficiency-performance trade-offs across\nmultiple sparsity levels. Notably, even without final LoRA fine-tuning, our\nmethod matches or surpasses strong baselines that require full fine-tuning, all\nwhile reducing training time by over 50%. The code is available at:\nhttps://github.com/EIT-NLP/informed-routing", "AI": {"tldr": "本文提出了一种名为“知情路由”（informed routing）的新范式，通过评估令牌的重要性及其可恢复性，并使用轻量级特征预测器（LFF）实现“执行或近似”策略，显著降低大型语言模型（LLM）的推理成本，同时保持模型性能。", "motivation": "大型语言模型（LLMs）的高推理成本日益限制其在实际应用中的部署。现有动态令牌级计算分配方法（如贪婪路由）通常会导致不可逆的信息损失和次优的令牌选择。", "method": "本文引入了“知情路由”范式，其核心思想是不仅评估令牌的即时重要性，还要评估其可恢复性（即其转换可被近似的程度）。为此，提出了一种轻量级特征预测器（LFF），这是一个小型预测模块，用于在路由决策前估计单元的输出，从而实现灵活的“执行或近似”策略，在大幅减少计算的同时保持模型保真度。", "result": "在语言建模和推理任务上进行的广泛实验表明，“知情路由”在多个稀疏度级别上实现了最先进的效率-性能权衡。值得注意的是，即使没有最终的LoRA微调，该方法也能匹配或超越需要完全微调的强基线，同时训练时间减少了50%以上。", "conclusion": "“知情路由”通过引入“执行或近似”策略，在评估令牌重要性和可恢复性的基础上，有效解决了LLM的高推理成本问题。它在显著降低计算量的同时保持了模型性能，并在效率-性能权衡方面取得了最先进的成果。"}}
{"id": "2510.14025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14025", "abs": "https://arxiv.org/abs/2510.14025", "authors": ["Junjie Nan", "Jianing Li", "Wei Chen", "Mingkun Zhang", "Xueqi Cheng"], "title": "NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations", "comment": null, "summary": "Adversarial purification has achieved great success in combating adversarial\nimage perturbations, which are usually assumed to be additive. However,\nnon-additive adversarial perturbations such as blur, occlusion, and distortion\nare also common in the real world. Under such perturbations, existing\nadversarial purification methods are much less effective since they are\ndesigned to fit the additive nature. In this paper, we propose an extended\nadversarial purification framework named NAPPure, which can further handle\nnon-additive perturbations. Specifically, we first establish the generation\nprocess of an adversarial image, and then disentangle the underlying clean\nimage and perturbation parameters through likelihood maximization. Experiments\non GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the\nrobustness of image classification models against non-additive perturbations.", "AI": {"tldr": "本文提出了一种名为NAPPure的扩展对抗性净化框架，旨在有效处理非加性对抗性扰动，显著提升图像分类模型的鲁棒性。", "motivation": "现有的对抗性净化方法主要针对加性扰动设计，但在真实世界中，模糊、遮挡、扭曲等非加性对抗性扰动也很常见。这些方法在非加性扰动下效果不佳，因此需要新的框架来应对。", "method": "NAPPure框架首先建立对抗性图像的生成过程，然后通过最大化似然函数来解耦潜在的干净图像和扰动参数。", "result": "在GTSRB和CIFAR-10数据集上的实验表明，NAPPure显著提高了图像分类模型对抗非加性扰动的鲁棒性。", "conclusion": "NAPPure提供了一个处理非加性对抗性扰动的有效净化框架，能够增强模型在复杂真实世界扰动下的性能。"}}
{"id": "2510.14119", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14119", "abs": "https://arxiv.org/abs/2510.14119", "authors": ["Ali Eslami", "Mohammad Pirani"], "title": "Resource-Aware Stealthy Attacks in Vehicle Platoons", "comment": "13 pages, 8 figures", "summary": "Connected and Autonomous Vehicles (CAVs) are transforming modern\ntransportation by enabling cooperative applications such as vehicle platooning,\nwhere multiple vehicles travel in close formation to improve efficiency and\nsafety. However, the heavy reliance on inter-vehicle communication makes\nplatoons highly susceptible to attacks, where even subtle manipulations can\nescalate into severe physical consequences. While existing research has largely\nfocused on defending against attacks, far less attention has been given to\nstealthy adversaries that aim to covertly manipulate platoon behavior. This\npaper introduces a new perspective on the attack design problem by\ndemonstrating how attackers can guide platoons toward their own desired\ntrajectories while remaining undetected. We outline conditions under which such\nattacks are feasible, analyze their dependence on communication topologies and\ncontrol protocols, and investigate the resources required by the attacker. By\ncharacterizing the resources needed to launch stealthy attacks, we address\nsystem vulnerabilities and informing the design of resilient countermeasures.\nOur findings reveal critical weaknesses in current platoon architectures and\nanomaly detection mechanisms and provide methods to develop more secure and\ntrustworthy CAV systems.", "AI": {"tldr": "本文从攻击者角度探讨了自动驾驶车队（CAV platooning）中的隐蔽攻击，展示了攻击者如何在不被发现的情况下，将车队引导至其预设轨迹，从而揭示了当前车队架构和异常检测机制的关键弱点，并为开发更安全的CAV系统提供了方法。", "motivation": "自动驾驶车队高度依赖车间通信，极易受到攻击。现有研究主要关注防御，但对旨在秘密操纵车队行为的隐蔽攻击关注不足。研究需要从攻击设计角度探讨攻击者如何能不被察觉地控制车队。", "method": "本文提出了一种新的攻击设计视角。通过阐述隐蔽攻击的可行条件，分析其对通信拓扑和控制协议的依赖性，并调查攻击者所需的资源，来展示攻击者如何秘密引导车队至其期望的轨迹。", "result": "研究结果揭示了当前车队架构和异常检测机制的关键弱点。通过量化发起隐蔽攻击所需的资源，本文指出了系统漏洞，并为设计有弹性的对抗措施提供了信息。", "conclusion": "通过识别系统漏洞并为设计弹性对策提供信息，本文旨在开发更安全、更值得信赖的自动驾驶车队系统，从而增强其安全性与可靠性。"}}
{"id": "2510.14117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14117", "abs": "https://arxiv.org/abs/2510.14117", "authors": ["Zhiyuan Wu", "Yijiong Lin", "Yongqiang Zhao", "Xuyang Zhang", "Zhuo Chen", "Nathan Lepora", "Shan Luo"], "title": "ViTacGen: Robotic Pushing with Vision-to-Touch Generation", "comment": null, "summary": "Robotic pushing is a fundamental manipulation task that requires tactile\nfeedback to capture subtle contact forces and dynamics between the end-effector\nand the object. However, real tactile sensors often face hardware limitations\nsuch as high costs and fragility, and deployment challenges involving\ncalibration and variations between different sensors, while vision-only\npolicies struggle with satisfactory performance. Inspired by humans' ability to\ninfer tactile states from vision, we propose ViTacGen, a novel robot\nmanipulation framework designed for visual robotic pushing with vision-to-touch\ngeneration in reinforcement learning to eliminate the reliance on\nhigh-resolution real tactile sensors, enabling effective zero-shot deployment\non visual-only robotic systems. Specifically, ViTacGen consists of an\nencoder-decoder vision-to-touch generation network that generates contact depth\nimages, a standardized tactile representation, directly from visual image\nsequence, followed by a reinforcement learning policy that fuses visual-tactile\ndata with contrastive learning based on visual and generated tactile\nobservations. We validate the effectiveness of our approach in both simulation\nand real world experiments, demonstrating its superior performance and\nachieving a success rate of up to 86\\%.", "AI": {"tldr": "ViTacGen提出了一种基于视觉到触觉生成（vision-to-touch generation）的机器人推拉框架，通过从视觉图像序列生成触觉深度图像，使纯视觉机器人系统能够进行零样本部署，克服了真实触觉传感器的局限性。", "motivation": "真实的触觉传感器成本高昂、易碎、部署和校准困难，且不同传感器之间存在差异；而纯视觉策略在机器人推拉任务中表现不佳。研究灵感来源于人类能够从视觉推断触觉状态的能力。", "method": "ViTacGen框架包含两部分：1) 一个编码器-解码器视觉到触觉生成网络，该网络直接从视觉图像序列生成接触深度图像（一种标准化的触觉表示）；2) 一个强化学习策略，该策略通过基于视觉和生成的触觉观测的对比学习来融合视觉-触觉数据。", "result": "在仿真和真实世界实验中，该方法均表现出卓越的性能，成功率高达86%。", "conclusion": "ViTacGen通过视觉到触觉生成，有效消除了对高分辨率真实触觉传感器的依赖，实现了在纯视觉机器人系统上的零样本部署，显著提升了机器人推拉任务的性能。"}}
{"id": "2510.14035", "categories": ["cs.AI", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.14035", "abs": "https://arxiv.org/abs/2510.14035", "authors": ["Rajesh Mangannavar", "Prasad Tadepalli"], "title": "GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations", "comment": "10 pages content. 2 pages references", "summary": "We introduce an action-centric graph representation framework for learning to\nguide planning in Partially Observable Markov Decision Processes (POMDPs).\nUnlike existing approaches that require domain-specific neural architectures\nand struggle with scalability, GammaZero leverages a unified graph-based belief\nrepresentation that enables generalization across problem sizes within a\ndomain. Our key insight is that belief states can be systematically transformed\ninto action-centric graphs where structural patterns learned on small problems\ntransfer to larger instances. We employ a graph neural network with a decoder\narchitecture to learn value functions and policies from expert demonstrations\non computationally tractable problems, then apply these learned heuristics to\nguide Monte Carlo tree search on larger problems. Experimental results on\nstandard POMDP benchmarks demonstrate that GammaZero achieves comparable\nperformance to BetaZero when trained and tested on the same-sized problems,\nwhile uniquely enabling zero-shot generalization to problems 2-4 times larger\nthan those seen during training, maintaining solution quality with reduced\nsearch requirements.", "AI": {"tldr": "GammaZero 提出了一种以动作为中心的图表示框架，用于指导部分可观测马尔可夫决策过程（POMDPs）的规划，实现了对更大规模问题的零样本泛化。", "motivation": "现有方法需要领域特定的神经网络架构，难以扩展，且在问题规模变化时泛化能力差。研究旨在开发一种统一的、可泛化的方法来解决这些挑战。", "method": "该方法将信念状态系统地转换为以动作为中心的图。利用图神经网络（GNN）和解码器架构，从可计算的小规模问题专家演示中学习价值函数和策略。然后，将这些学习到的启发式方法应用于指导更大规模问题上的蒙特卡洛树搜索（MCTS）。核心思想是小问题上学习到的结构模式可以迁移到大问题上。", "result": "实验结果表明，GammaZero 在相同规模问题上与 BetaZero 表现相当。更重要的是，它独特地实现了对训练时未见的、规模大2-4倍的问题的零样本泛化，同时保持了解决方案质量并降低了搜索需求。", "conclusion": "GammaZero 提供了一个新颖的、基于图的 POMDP 规划框架，通过学习结构模式实现了跨问题规模的泛化，尤其是在处理大型、复杂 POMDP 问题时表现出显著的零样本泛化能力和效率。"}}
{"id": "2510.13832", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13832", "abs": "https://arxiv.org/abs/2510.13832", "authors": ["Minsik Choi", "Hyegang Son", "Changhoon Kim", "Young Geun Kim"], "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning", "comment": "32 pages", "summary": "Transformer-based models have achieved remarkable performance in NLP tasks.\nHowever, their structural characteristics-multiple layers and attention\nheads-introduce efficiency challenges in inference and deployment. To address\nthese challenges, various pruning methods have recently been proposed. Notably,\ngradient-based methods using Head Importance Scores (HIS) have gained traction\nfor interpretability, efficiency, and ability to identify redundant heads.\nHowever, HIS alone has limitations as it captures only the gradient-driven\ncontribution, overlooking the diversity of attention patterns. To overcome\nthese limitations, we introduce a novel pruning criterion, HIES (Head\nImportance-Entropy Score), which integrates head importance scores with\nattention entropy, providing complementary evidence on per-head contribution.\nEmpirically, HIES-based pruning yields up to 15.2% improvement in model quality\nand 2.04x improvement in stability over HIS-only methods, enabling substantial\nmodel compression without sacrificing either accuracy or stability. Code will\nbe released upon publication.", "AI": {"tldr": "本文提出了一种名为HIES（Head Importance-Entropy Score）的新型剪枝准则，它结合了注意力头重要性分数和注意力熵，以更有效地修剪Transformer模型，在模型质量和稳定性方面优于仅使用HIS的方法。", "motivation": "Transformer模型在NLP任务中表现出色，但其多层和多注意力头的结构导致推理和部署效率低下。现有的梯度基剪枝方法（如使用HIS）虽然有优势，但仅捕获梯度驱动的贡献，忽略了注意力模式的多样性，存在局限性。", "method": "引入了一种新颖的剪枝准则HIES（Head Importance-Entropy Score），该准则将注意力头重要性分数与注意力熵相结合，为每个注意力头的贡献提供了互补的证据。", "result": "基于HIES的剪枝方法在模型质量上比仅使用HIS的方法提高了高达15.2%，在稳定性上提高了2.04倍，实现了显著的模型压缩而未牺牲准确性或稳定性。", "conclusion": "HIES准则通过整合注意力头重要性分数和注意力熵，克服了传统梯度基剪枝方法的局限性，实现了更高效、更稳定的Transformer模型压缩，同时保持了模型性能。"}}
{"id": "2510.14234", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14234", "abs": "https://arxiv.org/abs/2510.14234", "authors": ["Ning Han", "Gu Gong", "Bin Zhang", "Yuexuan Xu", "Bohan Yang", "Yunhui Liu", "David Navarro-Alarcon"], "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space", "comment": null, "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.", "AI": {"tldr": "本文提出一种无模型方法，通过深度学习提取关键点，并结合预设性能控制和障碍Lyapunov函数，实现三维可变形物体的形状控制及关键点约束。", "motivation": "三维可变形物体的操纵面临巨大挑战，因为其状态空间是无限维的，且形变动力学复杂。现有方法依赖于特征降维，但可能丢失空间信息。", "method": "该方法是无模型的，使用深度学习从点云中提取关键点作为特征向量，这既降低了特征空间维度又保留了空间信息。将形变操纵简化为视觉伺服问题，并用形变雅可比矩阵描述形状动力学。为提高控制精度，开发了一种结合障碍Lyapunov函数（BLF）的预设性能控制方法，以强制执行关键点约束。闭环系统稳定性通过Lyapunov方法严格分析和验证。", "result": "实验结果证明了所提出方法的有效性和鲁棒性。", "conclusion": "所提出的方法能够有效且鲁棒地实现三维可变形物体的形状控制，并能强制执行关键点约束。"}}
{"id": "2510.13835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13835", "abs": "https://arxiv.org/abs/2510.13835", "authors": ["Avik Dutta", "Priyanshu Gupta", "Hosein Hasanbeig", "Rahul Pratap Singh", "Harshit Nigam", "Sumit Gulwani", "Arjun Radhakrishna", "Gustavo Soares", "Ashish Tiwari"], "title": "ConDABench: Interactive Evaluation of Language Models for Data Analysis", "comment": null, "summary": "Real-world data analysis tasks often come with under-specified goals and\nunclean data. User interaction is necessary to understand and disambiguate a\nuser's intent, and hence, essential to solving these complex tasks. Existing\nbenchmarks for evaluating LLMs on data analysis tasks do not capture these\ncomplexities or provide first-class support for interactivity. We introduce\nConDABench, a framework for generating conversational data analysis (ConDA)\nbenchmarks and evaluating external tools on the generated benchmarks. \\bench\nconsists of (a) a multi-agent workflow for generating realistic benchmarks from\narticles describing insights gained from public datasets, (b) 1,420 ConDA\nproblems generated using this workflow, and (c) an evaluation harness that, for\nthe first time, makes it possible to systematically evaluate conversational\ndata analysis tools on the generated ConDA problems. Evaluation of\nstate-of-the-art LLMs on the benchmarks reveals that while the new generation\nof models are better at solving more instances, they are not necessarily better\nat solving tasks that require sustained, long-form engagement. ConDABench is an\navenue for model builders to measure progress towards truly collaborative\nmodels that can complete complex interactive tasks.", "AI": {"tldr": "ConDABench是一个用于生成和评估对话式数据分析（ConDA）基准的框架，旨在解决现有基准缺乏交互性和真实世界复杂性的问题。", "motivation": "现实世界的数据分析任务通常目标不明确且数据不干净，需要用户交互来理解和澄清意图。现有的评估大型语言模型（LLM）数据分析任务的基准未能捕捉这些复杂性，也未提供对交互性的原生支持。", "method": "ConDABench包含三个主要部分：(a) 一个多智能体工作流，用于从描述公共数据集洞察的文章中生成真实的基准；(b) 使用此工作流生成了1,420个ConDA问题；(c) 一个评估工具，首次实现了对对话式数据分析工具在生成问题上的系统评估。", "result": "对最先进LLM在ConDABench基准上的评估显示，新一代模型虽然在解决更多实例方面表现更好，但它们不一定擅长解决需要持续、长时间参与的任务。", "conclusion": "ConDABench为模型开发者提供了一个衡量其模型在实现真正协作、能够完成复杂交互式任务方面的进展的途径。"}}
{"id": "2510.14032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14032", "abs": "https://arxiv.org/abs/2510.14032", "authors": ["Xiaoqian Shen", "Wenxuan Zhang", "Jun Chen", "Mohamed Elhoseiny"], "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding", "comment": "NeurIPS 2025 (Spotlight). Webpage at\n  https://xiaoqian-shen.github.io/Vgent", "summary": "Understanding and reasoning over long videos pose significant challenges for\nlarge video language models (LVLMs) due to the difficulty in processing\nintensive video tokens beyond context window and retaining long-term sequential\ninformation. Retrieval-Augmented Generation (RAG) has demonstrated\neffectiveness in processing long context for Large Language Models (LLMs);\nhowever, applying RAG to long video faces challenges such as disrupted temporal\ndependencies and inclusion of irrelevant information that can hinder accurate\nreasoning. To address these limitations, we propose Vgent, a novel graph-based\nretrieval-reasoning-augmented generation framework to enhance LVLMs for long\nvideo understanding. Our approach introduces two key innovations: (i) It\nrepresents videos by structured graphs with semantic relationships across video\nclips preserved to improve retrieval effectiveness. (ii) It introduces an\nintermediate reasoning step to mitigate the reasoning limitation of LVLMs,\nwhich leverages structured verification to reduce retrieval noise and\nfacilitate the explicit aggregation of relevant information across clips,\nresulting in more accurate and context-aware responses. We comprehensively\nevaluate our framework with various open-source LVLMs on three long-video\nunderstanding benchmarks. Our approach yielded an overall performance\nimprovement of $3.0\\%\\sim 5.4\\%$ over base models on MLVU, and outperformed\nstate-of-the-art video RAG methods by $8.6\\%$. Our code is publicly available\nat https://xiaoqian-shen.github.io/Vgent.", "AI": {"tldr": "本文提出Vgent，一个新颖的基于图的检索-推理增强生成框架，旨在通过结构化视频表示和中间推理步骤，提升大型视频语言模型（LVLMs）在长视频理解方面的能力。", "motivation": "大型视频语言模型（LVLMs）在处理长视频时面临挑战，包括上下文窗口限制导致难以处理大量视频token以及难以保留长期序列信息。现有的检索增强生成（RAG）方法应用于长视频时，存在破坏时间依赖性和引入无关信息的问题，这会阻碍准确的推理。", "method": "本文提出Vgent框架，包含两项关键创新：(i) 将视频表示为结构化图，保留视频片段间的语义关系以提高检索效率。(ii) 引入中间推理步骤，利用结构化验证减少检索噪声，并促进跨片段相关信息的显式聚合，从而缓解LVLMs的推理限制，生成更准确、上下文感知的响应。", "result": "Vgent框架在三个长视频理解基准上，对各种开源LVLMs进行了全面评估。结果显示，与基线模型相比，Vgent在MLVU上实现了3.0%~5.4%的整体性能提升，并超越了最先进的视频RAG方法8.6%。", "conclusion": "Vgent通过其独特的图结构视频表示和中间推理机制，有效解决了大型视频语言模型在长视频理解中面临的检索和推理挑战，显著提升了模型性能。"}}
{"id": "2510.14053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14053", "abs": "https://arxiv.org/abs/2510.14053", "authors": ["Shriyash Upadhyay", "Chaithanya Bandi", "Narmeen Oozeer", "Philip Quirke"], "title": "Position: Require Frontier AI Labs To Release Small \"Analog\" Models", "comment": null, "summary": "Recent proposals for regulating frontier AI models have sparked concerns\nabout the cost of safety regulation, and most such regulations have been\nshelved due to the safety-innovation tradeoff. This paper argues for an\nalternative regulatory approach that ensures AI safety while actively promoting\ninnovation: mandating that large AI laboratories release small, openly\naccessible analog models (scaled-down versions) trained similarly to and\ndistilled from their largest proprietary models.\n  Analog models serve as public proxies, allowing broad participation in safety\nverification, interpretability research, and algorithmic transparency without\nforcing labs to disclose their full-scale models. Recent research demonstrates\nthat safety and interpretability methods developed using these smaller models\ngeneralize effectively to frontier-scale systems. By enabling the wider\nresearch community to directly investigate and innovate upon accessible\nanalogs, our policy substantially reduces the regulatory burden and accelerates\nsafety advancements.\n  This mandate promises minimal additional costs, leveraging reusable resources\nlike data and infrastructure, while significantly contributing to the public\ngood. Our hope is not only that this policy be adopted, but that it illustrates\na broader principle supporting fundamental research in machine learning: deeper\nunderstanding of models relaxes the safety-innovation tradeoff and lets us have\nmore of both.", "AI": {"tldr": "本文提出一种新的AI监管方法：强制大型AI实验室发布小型、开放的模拟模型，以促进安全验证和创新，同时降低监管成本。", "motivation": "当前的AI前沿模型监管提案因成本高昂和安全-创新权衡而被搁置，引发了对AI安全监管成本的担忧。", "method": "强制大型AI实验室发布与其最大专有模型训练方式相似并从中提取的小型、开放可访问的模拟模型（缩小版）。", "result": "模拟模型作为公共代理，允许广泛参与安全验证、可解释性研究和算法透明度，而无需披露完整模型。现有研究表明，在这些小型模型上开发的安全和可解释性方法可有效推广到前沿系统。该政策能显著降低监管负担，加速安全进展，且额外成本极低。", "conclusion": "该政策不仅有望被采纳，更体现了一个更广泛的原则：深入理解模型可以缓解安全与创新之间的权衡，实现两者兼得。"}}
{"id": "2510.14542", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14542", "abs": "https://arxiv.org/abs/2510.14542", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "title": "A Deep State-Space Model Compression Method using Upper Bound on Output Error", "comment": null, "summary": "We study deep state-space models (Deep SSMs) that contain\nlinear-quadratic-output (LQO) systems as internal blocks and present a\ncompression method with a provable output error guarantee. We first derive an\nupper bound on the output error between two Deep SSMs and show that the bound\ncan be expressed via the $h^2$-error norms between the layerwise LQO systems,\nthereby providing a theoretical justification for existing model order\nreduction (MOR)-based compression. Building on this bound, we formulate an\noptimization problem in terms of the $h^2$-error norm and develop a\ngradient-based MOR method. On the IMDb task from the Long Range Arena\nbenchmark, we demonstrate that our compression method achieves strong\nperformance. Moreover, unlike prior approaches, we reduce roughly 80% of\ntrainable parameters without retraining, with only a 4-5% performance drop.", "AI": {"tldr": "本文提出了一种针对包含线性二次输出（LQO）系统的深度状态空间模型（Deep SSMs）的压缩方法，该方法具有可证明的输出误差保证，并通过梯度优化实现显著的参数削减和强大的性能。", "motivation": "现有深度状态空间模型（Deep SSMs）的压缩方法缺乏理论上的输出误差保证，且可能需要重新训练。研究旨在开发一种具有可证明误差界限的压缩方法，并减少训练参数而不需重新训练。", "method": "首先，推导了两个Deep SSMs之间输出误差的上界，并证明该界限可通过层级LQO系统之间的$h^2$-误差范数表示。基于此界限，构建了一个以$h^2$-误差范数为目标的优化问题，并开发了一种基于梯度的模型降阶（MOR）方法。", "result": "该压缩方法在Long Range Arena基准测试的IMDb任务上表现出色。与现有方法不同，它在不重新训练的情况下减少了大约80%的可训练参数，且性能仅下降4-5%。推导的误差界限为现有的基于MOR的压缩提供了理论依据。", "conclusion": "本研究提供了一种理论上合理且实践中高效的深度状态空间模型压缩方法。该方法通过梯度优化的模型降阶，实现了显著的参数削减和强大的性能，且无需重新训练，克服了以往方法的局限性。"}}
{"id": "2510.14051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14051", "abs": "https://arxiv.org/abs/2510.14051", "authors": ["Avihai Naaman", "Ron Shapira Weber", "Oren Freifeld"], "title": "Synchronization of Multiple Videos", "comment": "ICCV 2025", "summary": "Synchronizing videos captured simultaneously from multiple cameras in the\nsame scene is often easy and typically requires only simple time shifts.\nHowever, synchronizing videos from different scenes or, more recently,\ngenerative AI videos, poses a far more complex challenge due to diverse\nsubjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal\nPrototype Learning (TPL), a prototype-based framework that constructs a shared,\ncompact 1D representation from high-dimensional embeddings extracted by any of\nvarious pretrained models. TPL robustly aligns videos by learning a unified\nprototype sequence that anchors key action phases, thereby avoiding exhaustive\npairwise matching. Our experiments show that TPL improves synchronization\naccuracy, efficiency, and robustness across diverse datasets, including\nfine-grained frame retrieval and phase classification tasks. Importantly, TPL\nis the first approach to mitigate synchronization issues in multiple generative\nAI videos depicting the same action. Our code and a new multiple video\nsynchronization dataset are available at https://bgu-cs-vil.github.io/TPL/", "AI": {"tldr": "本文提出了一种名为时间原型学习（TPL）的新型框架，用于解决来自不同场景或生成式AI视频的复杂同步问题，通过学习统一的原型序列实现高效且鲁棒的对齐。", "motivation": "传统的视频同步方法通常仅适用于同一场景的视频，通过简单的时间偏移即可实现。然而，对于来自不同场景或生成式AI的视频，由于主题、背景多样以及非线性时间错位，同步面临巨大挑战。", "method": "本文提出了时间原型学习（TPL）框架。该方法从预训练模型提取的高维嵌入中构建共享的、紧凑的1D表示。TPL通过学习一个统一的原型序列来锚定关键动作阶段，从而实现视频的鲁棒对齐，避免了详尽的成对匹配。", "result": "实验结果表明，TPL在包括细粒度帧检索和阶段分类任务在内的各种数据集上，显著提高了同步的准确性、效率和鲁棒性。值得注意的是，TPL是首个能够解决多个描绘相同动作的生成式AI视频同步问题的方案。", "conclusion": "TPL提供了一个创新且高效的解决方案，能够克服传统方法在处理来自不同场景或生成式AI视频同步时的局限性，其在准确性、效率和鲁棒性方面均表现出色，尤其在生成式AI视频同步方面填补了空白。"}}
{"id": "2510.14106", "categories": ["cs.AI", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.14106", "abs": "https://arxiv.org/abs/2510.14106", "authors": ["Carter Blair", "Kate Larson"], "title": "Generating Fair Consensus Statements with Social Choice on Token-Level MDPs", "comment": null, "summary": "Current frameworks for consensus statement generation with large language\nmodels lack the inherent structure needed to provide provable fairness\nguarantees when aggregating diverse free-form opinions. We model the task as a\nmulti-objective, token-level Markov Decision Process (MDP), where each\nobjective corresponds to an agent's preference. Token-level rewards for each\nagent are derived from their policy (e.g., a personalized language model). This\napproach utilizes the finding that such policies implicitly define optimal\nQ-functions, providing a principled way to quantify rewards at each generation\nstep without a value function (Rafailov et al., 2024). This MDP formulation\ncreates a formal structure amenable to analysis using principles from social\nchoice theory. We propose two approaches grounded in social choice theory.\nFirst, we propose a stochastic generation policy guaranteed to be in the\nex-ante core, extending core stability concepts from voting theory to text\ngeneration. This policy is derived from an underlying distribution over\ncomplete statements that maximizes proportional fairness (Nash Welfare).\nSecond, for generating a single statement, we target the maximization of\negalitarian welfare using search algorithms within the MDP framework.\nEmpirically, experiments using language models to instantiate agent policies\nshow that search guided by the egalitarian objective generates consensus\nstatements with improved worst-case agent alignment compared to baseline\nmethods, including the Habermas Machine (Tessler et al., 2024).", "AI": {"tldr": "该研究提出一种基于多目标马尔可夫决策过程（MDP）和社会选择理论的框架，用于使用大型语言模型生成具有可证明公平性保证的共识声明，以聚合多样化的自由形式意见。", "motivation": "当前使用大型语言模型生成共识声明的框架，在聚合多样化的自由形式意见时，缺乏提供可证明公平性保证所需的内在结构。", "method": "将任务建模为多目标、令牌级别的马尔可夫决策过程（MDP），其中每个目标对应一个代理的偏好。每个代理的令牌级别奖励从其策略（例如，个性化语言模型）中导出，利用了策略隐式定义最优Q函数的方法。在此MDP框架下，提出了两种基于社会选择理论的方法：1) 一种随机生成策略，保证在事前核心（ex-ante core）中，通过最大化比例公平性（纳什福利）的完整声明分布导出；2) 对于生成单个声明，在MDP框架内使用搜索算法最大化平等福利（egalitarian welfare）。", "result": "实证实验表明，与包括Habermas Machine在内的基线方法相比，由平等主义目标指导的搜索生成共识声明，在最坏情况下的代理对齐方面有所改进。", "conclusion": "通过将共识声明生成建模为多目标MDP并结合社会选择理论，可以构建一个正式结构来生成具有可证明公平性保证的共识声明，尤其是在最坏情况下的代理对齐方面表现出优越性。"}}
{"id": "2510.14696", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14696", "abs": "https://arxiv.org/abs/2510.14696", "authors": ["Kevin Wu", "Rabab Haider", "Pascal Van Hentenryck"], "title": "High-Resolution PTDF-Based Planning of Storage and Transmission Under High Renewables", "comment": null, "summary": "Transmission Expansion Planning (TEP) optimizes power grid upgrades and\ninvestments to ensure reliable, efficient, and cost-effective electricity\ndelivery while addressing grid constraints. To support growing demand and\nrenewable energy integration, energy storage is emerging as a pivotal asset\nthat provides temporal flexibility and alleviates congestion. This paper\ndevelops a multiperiod, two-stage PTDF formulation that co-optimizes\ntransmission upgrades and storage siting/sizing. To ensure scalability, a\ntrust-region, multicut Benders scheme warm-started from per-representative-day\noptima is proposed. Applied to a 2,000-bus synthetic Texas system under\nhigh-renewable projections, the method attains final optimality gaps below 1%\nand yields a plan with storage at about 180 nodes (32% of peak renewable\ncapacity). These results demonstrate that the proposed PTDF-based methodology\nefficiently handles large distributed storage fleets, demonstrating scalability\nat high spatial resolution", "AI": {"tldr": "本文提出了一种多周期、两阶段的PTDF公式，用于联合优化输电线路升级和储能选址/规模，并采用信任域多割Benders方案解决大规模问题，成功应用于德州电网，证明了其可扩展性。", "motivation": "随着电力需求增长和可再生能源并网，电网需要可靠、高效、经济的电力传输，同时应对电网约束。储能作为一种提供时间灵活性和缓解拥堵的关键资产，对于解决这些挑战至关重要。", "method": "开发了一种多周期、两阶段的PTDF（潮流分布因子）公式，共同优化输电线路升级和储能的选址与规模。为确保可扩展性，提出了一种信任域、多割Benders方案，并从每个代表日的最优解进行热启动。", "result": "该方法应用于一个具有高可再生能源预测的2000节点合成德州系统。结果显示，最终最优性差距低于1%，并规划了在约180个节点（占峰值可再生能源容量的32%）部署储能。", "conclusion": "所提出的基于PTDF的方法能够高效处理大规模分布式储能系统，并在高空间分辨率下展现出良好的可扩展性。"}}
{"id": "2510.14293", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.", "AI": {"tldr": "本文提出了一种名为COLA的纯本体感受强化学习方法，实现了人机协作搬运中的顺从性，通过预测物体运动和人类意图来协调轨迹规划和负载平衡，显著降低了人类努力并增强了鲁棒性。", "motivation": "人机协作在医疗、家庭辅助和制造业中具有巨大潜力。虽然机器人手臂的顺从性协作已得到广泛开发，但由于类人机器人的复杂全身动力学，实现顺从性的人类-类人机器人协作仍未得到充分探索。", "method": "本文提出了一种名为COLA的纯本体感受强化学习方法，该方法将领导者和跟随者行为结合到一个策略中。模型在具有动态物体交互的闭环环境中进行训练，以隐式预测物体运动模式和人类意图，从而通过协调轨迹规划实现顺从性协作以保持负载平衡。", "result": "模拟实验表明，与基线方法相比，该模型将人类努力减少了24.7%，同时保持了物体稳定性。真实世界实验验证了其在不同物体类型（盒子、桌子、担架等）和运动模式（直线、转弯、爬坡）下的鲁棒协作搬运能力。对23名参与者进行的人类用户研究证实，与基线模型相比，平均提升了27.4%。该方法无需外部传感器或复杂的交互模型即可实现顺从性人机协作搬运。", "conclusion": "COLA方法为实现顺从性人机协作搬运提供了一种实用解决方案，无需外部传感器或复杂交互模型，适用于实际部署，并在降低人类努力和增强鲁棒性方面表现出色。"}}
{"id": "2510.14081", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.14081", "abs": "https://arxiv.org/abs/2510.14081", "authors": ["Emanuel Garbin", "Guy Adam", "Oded Krams", "Zohar Barzelay", "Eran Guendelman", "Michael Schwarz", "Moran Vatelmacher", "Yigal Shenkman", "Eli Peker", "Itai Druker", "Uri Patish", "Yoav Blum", "Max Bluvstein", "Junxuan Li", "Rawal Khirodkar", "Shunsuke Saito"], "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images", "comment": null, "summary": "We present a novel, zero-shot pipeline for creating hyperrealistic,\nidentity-preserving 3D avatars from a few unstructured phone images. Existing\nmethods face several challenges: single-view approaches suffer from geometric\ninconsistencies and hallucinations, degrading identity preservation, while\nmodels trained on synthetic data fail to capture high-frequency details like\nskin wrinkles and fine hair, limiting realism. Our method introduces two key\ncontributions: (1) a generative canonicalization module that processes multiple\nunstructured views into a standardized, consistent representation, and (2) a\ntransformer-based model trained on a new, large-scale dataset of high-fidelity\nGaussian splatting avatars derived from dome captures of real people. This\n\"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars\nwith compelling realism and robust identity preservation from unstructured\nphotos.", "AI": {"tldr": "本文提出一种新颖的零样本流水线，能从少量非结构化手机图像创建超逼真、身份保持的3D数字人，解决了现有方法在几何一致性、身份保持和高频细节方面的不足。", "motivation": "现有方法存在以下挑战：单视图方法导致几何不一致和幻觉，损害身份保持；而基于合成数据训练的模型无法捕捉皮肤皱纹和细发等高频细节，限制了真实感。", "method": "本方法引入两个关键贡献：1) 一个生成式规范化模块，将多个非结构化视图处理成标准化、一致的表示；2) 一个基于Transformer的模型，该模型在一个新的大规模高保真高斯泼溅数字人数据集上进行训练，这些数据来源于真实人物的穹顶捕获。整个流水线被称为“捕获、规范化、泼溅”（Capture, Canonicalize, Splat）。", "result": "该流水线能从非结构化照片生成具有引人注目的真实感和强大身份保持能力的静态四分之一身体数字人。", "conclusion": "本文成功开发了一种从少量非结构化手机图像创建超逼真、身份保持的3D数字人的零样本流水线，有效解决了现有方法的局限性。"}}
{"id": "2510.13836", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13836", "abs": "https://arxiv.org/abs/2510.13836", "authors": ["Debarun Bhattacharjya", "Balaji Ganesan", "Junkyu Lee", "Radu Marinescu", "Katsiaryna Mirylenka", "Michael Glass", "Xiao Shou"], "title": "SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models", "comment": "15 pages including appendix, Findings of EMNLP 2025", "summary": "When does a large language model (LLM) know what it does not know?\nUncertainty quantification (UQ) provides measures of uncertainty, such as an\nestimate of the confidence in an LLM's generated output, and is therefore\nincreasingly recognized as a crucial component of trusted AI systems. Black-box\nUQ methods do not require access to internal model information from the\ngenerating LLM and therefore have numerous real-world advantages, such as\nrobustness to system changes, adaptability to choice of LLM, reduced costs, and\ncomputational tractability. In this paper, we investigate the effectiveness of\nUQ techniques that are primarily but not necessarily entirely black-box, where\nthe consistency between a generated output and other sampled generations is\nused as a proxy for confidence in its correctness. We propose a high-level\nnon-verbalized similarity-based aggregation framework that subsumes a broad\nswath of UQ approaches suitable for complex generative tasks, as well as\nintroduce specific novel techniques from the framework that train confidence\nestimation models using small training sets. Through an empirical study with\ndatasets spanning the diverse tasks of question answering, summarization, and\ntext-to-SQL, we demonstrate that our proposed similarity-based methods can\nyield better calibrated confidences than baselines.", "AI": {"tldr": "本文研究了大型语言模型（LLM）的黑盒不确定性量化（UQ）方法，特别是基于生成输出之间相似性来估计置信度的方法，并提出了一种新的聚合框架和技术，以提高LLM在多种生成任务中的置信度校准。", "motivation": "LLM需要知道其何时“不知”，这对于构建可信赖的AI系统至关重要。不确定性量化（UQ）为此提供了衡量标准。黑盒UQ方法因其对系统变化的鲁棒性、对LLM选择的适应性、成本效益和计算可行性等优点，在现实世界中具有显著优势。", "method": "本文调查了主要为黑盒的UQ技术，其中将生成输出与其他采样生成之间的“一致性”用作置信度的代理。作者提出了一种高层次的非语言化、基于相似性的聚合框架，该框架涵盖了适用于复杂生成任务的广泛UQ方法，并引入了利用小训练集训练置信度估计模型的特定新颖技术。", "result": "通过对问答、摘要和文本到SQL等多样化任务的数据集进行实证研究，结果表明，所提出的基于相似性的方法能够产生比基线方法更好校准的置信度。", "conclusion": "基于相似性的黑盒不确定性量化方法可以有效提高大型语言模型在复杂生成任务中的置信度校准，从而增强其可信赖性。"}}
{"id": "2510.13837", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50, 68T45", "I.2.7; I.2.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.13837", "abs": "https://arxiv.org/abs/2510.13837", "authors": ["Weibin Cai", "Reza Zafarani"], "title": "Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection", "comment": null, "summary": "Hate speech detection has been extensively studied, yet existing methods\noften overlook a real-world complexity: training labels are biased, and\ninterpretations of what is considered hate vary across individuals with\ndifferent cultural backgrounds. We first analyze these challenges, including\ndata sparsity, cultural entanglement, and ambiguous labeling. To address them,\nwe propose a culture-aware framework that constructs individuals' hate\nsubspaces. To alleviate data sparsity, we model combinations of cultural\nattributes. For cultural entanglement and ambiguous labels, we use label\npropagation to capture distinctive features of each combination. Finally,\nindividual hate subspaces, which in turn can further enhance classification\nperformance. Experiments show our method outperforms state-of-the-art by 1.05\\%\non average across all metrics.", "AI": {"tldr": "本文提出一个文化感知框架，通过构建个体仇恨子空间来解决仇恨言论检测中标签偏差和文化差异问题，并显著提升了检测性能。", "motivation": "现有仇恨言论检测方法忽略了真实世界中训练标签的偏见以及不同文化背景个体对仇恨定义解释的差异。具体挑战包括数据稀疏性、文化纠缠和模糊标签。", "method": "提出一个文化感知框架，用于构建个体的仇恨子空间。为缓解数据稀疏性，模型组合文化属性；为处理文化纠缠和模糊标签，使用标签传播来捕获每种组合的独特特征。最终，这些个体仇恨子空间进一步提升了分类性能。", "result": "实验结果表明，该方法在所有评估指标上，平均比现有最先进方法高出1.05%。", "conclusion": "所提出的文化感知框架能有效解决仇恨言论检测中的标签偏差和文化差异问题，通过考虑个体和文化背景，显著提升了检测的准确性和性能。"}}
{"id": "2510.14300", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.", "AI": {"tldr": "AdaMoE是一种基于MoE的视觉-语言-动作（VLA）模型架构，它通过继承预训练权重和引入解耦的专家选择与加权机制，实现了计算效率和性能的提升，特别是在机器人操作任务中表现优异。", "motivation": "VLA模型扩展面临两大挑战：1) 从头训练需要大量计算资源和数据集，而机器人数据稀缺，因此需要充分利用预训练权重。2) 实时控制要求在模型容量和计算效率之间取得平衡。", "method": "本文提出了AdaMoE，一种MoE架构。它继承了密集VLA模型的预训练权重，并通过将前馈层替换为稀疏激活的MoE层来扩展动作专家。AdaMoE采用解耦技术，通过独立的尺度适配器与传统路由器并行工作，将专家选择与专家加权分离，从而实现协作式专家利用而非赢者通吃模式。", "result": "AdaMoE在关键基准测试中持续优于基线模型，在LIBERO上性能提升1.8%，在RoboTwin上提升9.3%。最重要的是，在真实世界实验中取得了21.5%的显著改进，验证了其在机器人操作任务中的实际有效性。该方法在保持计算效率的同时实现了卓越的性能。", "conclusion": "通过AdaMoE的协作式专家利用（解耦专家选择和加权），可以有效地扩展VLA模型，实现卓越的性能并保持计算效率，这在机器人操作任务中得到了实践验证，证明了专业知识无需垄断，而是可以通过协作实现更优结果。"}}
{"id": "2510.14787", "categories": ["eess.SY", "cs.SY", "math.OC", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2510.14787", "abs": "https://arxiv.org/abs/2510.14787", "authors": ["Lorenzo Zino", "Alessandro Casu", "Alessandro Rizzo"], "title": "A Human-Vector Susceptible--Infected--Susceptible Model for Analyzing and Controlling the Spread of Vector-Borne Diseases", "comment": "To appear in the Proceedings of the 2025 European Control Conference\n  (ECC)", "summary": "We propose an epidemic model for the spread of vector-borne diseases. The\nmodel, which is built extending the classical susceptible-infected-susceptible\nmodel, accounts for two populations -- humans and vectors -- and for\ncross-contagion between the two species, whereby humans become infected upon\ninteraction with carrier vectors, and vectors become carriers after interaction\nwith infected humans. We formulate the model as a system of ordinary\ndifferential equations and leverage monotone systems theory to rigorously\ncharacterize the epidemic dynamics. Specifically, we characterize the global\nasymptotic behavior of the disease, determining conditions for quick\neradication of the disease (i.e., for which all trajectories converge to a\ndisease-free equilibrium), or convergence to a (unique) endemic equilibrium.\nThen, we incorporate two control actions: namely, vector control and incentives\nto adopt protection measures. Using the derived mathematical tools, we assess\nthe impact of these two control actions and determine the optimal control\npolicy.", "AI": {"tldr": "本文提出了一个扩展的易感-感染-易感（SIS）模型，用于描述媒介传播疾病的流行，该模型考虑了人类和媒介两种群体之间的交叉感染。研究分析了疾病的动态行为（根除或地方性流行），并评估了两种控制措施（媒介控制和激励保护措施）的效果，以确定最优控制策略。", "motivation": "研究的动机在于理解和预测媒介传播疾病的传播动态，并为疾病的控制和管理提供科学依据。特别地，旨在通过数学模型确定疾病根除或地方性流行的条件，并评估不同干预措施的效果，以制定最优的控制策略。", "method": "研究方法包括：1) 扩展经典SIS模型，纳入人类和媒介两个群体及其之间的交叉感染机制；2) 将模型表述为常微分方程系统；3) 利用单调系统理论严格刻画疾病的全局渐近行为；4) 引入媒介控制和激励保护措施两种控制行动；5) 使用推导出的数学工具评估这些控制行动的影响并确定最优控制策略。", "result": "研究结果表明：1) 成功刻画了疾病的全局渐近行为；2) 确定了疾病快速根除（所有轨迹收敛到无病平衡点）或收敛到唯一地方性流行平衡点的条件；3) 评估了媒介控制和激励保护措施这两种控制行动对疾病传播的影响；4) 确定了最优的控制策略。", "conclusion": "本文提出的模型能够有效地描述媒介传播疾病的动态。通过利用单调系统理论，可以严格分析疾病的长期行为，并确定疾病根除或地方性流行的条件。此外，研究还为评估和优化媒介控制及保护措施提供了数学工具，有助于制定更有效的疾病防控政策。"}}
{"id": "2510.14112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14112", "abs": "https://arxiv.org/abs/2510.14112", "authors": ["Huiliang Zhang", "Di Wu", "Arnaud Zinflou", "Benoit Boulet"], "title": "STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management", "comment": null, "summary": "Building energy management is essential for achieving carbon reduction goals,\nimproving occupant comfort, and reducing energy costs. Coordinated building\nenergy management faces critical challenges in exploiting spatial-temporal\ndependencies while ensuring operational safety across multi-building systems.\nCurrent multi-building energy systems face three key challenges: insufficient\nspatial-temporal information exploitation, lack of rigorous safety guarantees,\nand system complexity. This paper proposes Spatial-Temporal Enhanced Safe\nMulti-Agent Coordination (STEMS), a novel safety-constrained multi-agent\nreinforcement learning framework for coordinated building energy management.\nSTEMS integrates two core components: (1) a spatial-temporal graph\nrepresentation learning framework using a GCN-Transformer fusion architecture\nto capture inter-building relationships and temporal patterns, and (2) a\nsafety-constrained multi-agent RL algorithm incorporating Control Barrier\nFunctions to provide mathematical safety guarantees. Extensive experiments on\nreal-world building datasets demonstrate STEMS's superior performance over\nexisting methods, showing that STEMS achieves 21% cost reduction, 18% emission\nreduction, and dramatically reduces safety violations from 35.1% to 5.6% while\nmaintaining optimal comfort with only 0.13 discomfort proportion. The framework\nalso demonstrates strong robustness during extreme weather conditions and\nmaintains effectiveness across different building types.", "AI": {"tldr": "本文提出了STEMS，一个新颖的、受安全约束的多智能体强化学习框架，用于协调多建筑能源管理。它通过结合时空图表示学习和带控制障碍函数的安全约束MARL算法，解决了现有方法在时空信息利用、安全保障和系统复杂性方面的不足，显著降低了成本和排放，并大幅提高了运行安全性。", "motivation": "建筑能源管理对于实现碳减排、提高居住舒适度和降低能源成本至关重要。然而，协调多建筑能源管理面临关键挑战，包括：不足的时空信息利用、缺乏严格的安全保障以及系统复杂性。", "method": "本文提出了时空增强型安全多智能体协调（STEMS）框架，其核心组件包括：1) 一个使用GCN-Transformer融合架构的时空图表示学习框架，用于捕捉建筑间关系和时间模式；2) 一个结合控制障碍函数（CBF）以提供数学安全保障的安全约束多智能体强化学习算法。", "result": "在真实世界建筑数据集上的实验表明，STEMS优于现有方法：实现了21%的成本降低、18%的排放减少，并将安全违规率从35.1%大幅降低至5.6%，同时保持了最佳舒适度（不适比例仅为0.13）。该框架在极端天气条件下也表现出强大的鲁棒性，并适用于不同类型的建筑。", "conclusion": "STEMS是一个高效、安全且鲁棒的协调多建筑能源管理框架，它通过先进的时空信息处理和严格的安全保障机制，显著提升了能源效率、环境友好性和用户舒适度，并有效解决了现有方法的痛点。"}}
{"id": "2510.14143", "categories": ["cs.CV", "q-bio.QM", "92C55, 68U10", "I.4.0; J.3"], "pdf": "https://arxiv.org/pdf/2510.14143", "abs": "https://arxiv.org/abs/2510.14143", "authors": ["Alexandr A. Kalinin", "Anne E. Carpenter", "Shantanu Singh", "Matthew J. O'Meara"], "title": "cubic: CUDA-accelerated 3D Bioimage Computing", "comment": "accepted to BioImage Computing workshop @ ICCV 2025", "summary": "Quantitative analysis of multidimensional biological images is useful for\nunderstanding complex cellular phenotypes and accelerating advances in\nbiomedical research. As modern microscopy generates ever-larger 2D and 3D\ndatasets, existing computational approaches are increasingly limited by their\nscalability, efficiency, and integration with modern scientific computing\nworkflows. Existing bioimage analysis tools often lack application programmable\ninterfaces (APIs), do not support graphics processing unit (GPU) acceleration,\nlack broad 3D image processing capabilities, and/or have poor interoperability\nfor compute-heavy workflows. Here, we introduce cubic, an open-source Python\nlibrary that addresses these challenges by augmenting widely used SciPy and\nscikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.\ncubic's API is device-agnostic and dispatches operations to GPU when data\nreside on the device and otherwise executes on CPU, seamlessly accelerating a\nbroad range of image processing routines. This approach enables GPU\nacceleration of existing bioimage analysis workflows, from preprocessing to\nsegmentation and feature extraction for 2D and 3D data. We evaluate cubic both\nby benchmarking individual operations and by reproducing existing deconvolution\nand segmentation pipelines, achieving substantial speedups while maintaining\nalgorithmic fidelity. These advances establish a robust foundation for\nscalable, reproducible bioimage analysis that integrates with the broader\nPython scientific computing ecosystem, including other GPU-accelerated methods,\nenabling both interactive exploration and automated high-throughput analysis\nworkflows. cubic is openly available at\nhttps://github$.$com/alxndrkalinin/cubic", "AI": {"tldr": "本文介绍了一个名为`cubic`的开源Python库，它通过集成CuPy和RAPIDS cuCIM，为广泛使用的SciPy和scikit-image API提供了GPU加速的生物图像分析功能，解决了现有工具在可扩展性、效率和3D处理方面的局限性。", "motivation": "现代显微镜生成日益庞大的2D和3D生物图像数据集，但现有的计算方法在可扩展性、效率和与现代科学计算工作流程的集成方面受到限制。许多生物图像分析工具缺乏API、不支持GPU加速、缺乏广泛的3D图像处理能力，并且/或在计算密集型工作流程中互操作性差。", "method": "引入了`cubic`，一个开源Python库。它通过来自CuPy和RAPIDS cuCIM的GPU加速替代方案，增强了广泛使用的SciPy和scikit-image API。`cubic`的API是设备无关的，当数据驻留在GPU上时，它将操作分派给GPU执行；否则，在CPU上执行，从而无缝加速各种图像处理例程。", "result": "通过基准测试单个操作和复现现有去卷积和分割管道，`cubic`在保持算法保真度的同时，实现了显著的加速。这包括对2D和3D数据的预处理、分割和特征提取等工作流程。", "conclusion": "`cubic`为可扩展、可复现的生物图像分析奠定了坚实的基础，并与更广泛的Python科学计算生态系统（包括其他GPU加速方法）集成，从而支持交互式探索和自动化高通量分析工作流程。"}}
{"id": "2510.13839", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13839", "abs": "https://arxiv.org/abs/2510.13839", "authors": ["Dekai Zhang", "Simone Conia", "Antonio Rago"], "title": "Meronymic Ontology Extraction via Large Language Models", "comment": null, "summary": "Ontologies have become essential in today's digital age as a way of\norganising the vast amount of readily available unstructured text. In providing\nformal structure to this information, ontologies have immense value and\napplication across various domains, e.g., e-commerce, where countless product\nlistings necessitate proper product organisation. However, the manual\nconstruction of these ontologies is a time-consuming, expensive and laborious\nprocess. In this paper, we harness the recent advancements in large language\nmodels (LLMs) to develop a fully-automated method of extracting product\nontologies, in the form of meronymies, from raw review texts. We demonstrate\nthat the ontologies produced by our method surpass an existing, BERT-based\nbaseline when evaluating using an LLM-as-a-judge. Our investigation provides\nthe groundwork for LLMs to be used more generally in (product or otherwise)\nontology extraction.", "AI": {"tldr": "本文提出了一种利用大型语言模型（LLM）从原始评论文本中全自动提取产品本体（以部分-整体关系形式）的方法，该方法在LLM作为评估者的基准测试中优于现有的BERT基线。", "motivation": "本体在组织大量非结构化文本方面至关重要，但手动构建本体耗时、昂贵且费力。特别是在电子商务等领域，海量产品列表需要适当的组织。", "method": "利用大型语言模型（LLM）的最新进展，开发了一种全自动方法，从原始评论文本中提取产品本体，具体形式为部分-整体关系（meronymies）。", "result": "通过使用LLM作为评估者进行评估，该方法生成的本体超越了现有的基于BERT的基线。", "conclusion": "这项研究为LLM更广泛地应用于（产品或其他类型）本体提取奠定了基础。"}}
{"id": "2510.14834", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14834", "abs": "https://arxiv.org/abs/2510.14834", "authors": ["Daniel Russell", "Dakota Hamilton", "Mads R. Almassalkhi", "Hamid R. Ossareh"], "title": "Improved Voltage Regulation with Optimal Design of Decentralized Volt-VAr Control", "comment": null, "summary": "Integration of distributed energy resources has created a need for\nautonomous, dynamic voltage regulation. Decentralized Volt-VAr Control (VVC) of\ngrid-connected inverters presents a unique opportunity for voltage management\nbut, if designed poorly, can lead to unstable behavior when in feedback with\nthe grid. We model the grid-VVC closed-loop dynamics with a linearized power\nflow approach, leveraging historical data, which shows improvement over the\ncommonly used LinDistFlow model. This model is used to design VVC slopes by\nminimizing steady-state voltage deviation from the nominal value, subject to a\nnon-convex spectral radius stability constraint, which has not been previously\nimplemented within this context. We compare this constraint to existing convex\nrestrictions and demonstrate, through simulations on a realistic feeder, that\nusing the spectral radius results in more effective voltage regulation.", "AI": {"tldr": "本文提出一种利用历史数据改进的线性化潮流模型来设计分布式能源的去中心化VVC斜率，通过最小化稳态电压偏差并施加非凸谱半径稳定性约束，实现了比传统凸约束更有效的电压调节。", "motivation": "分布式能源的整合对自主、动态电压调节提出了需求。并网逆变器的去中心化Volt-VAr控制（VVC）为电压管理提供了独特机会，但设计不当可能导致与电网反馈时出现不稳定行为。", "method": "研究采用线性化潮流方法，利用历史数据建立电网-VVC闭环动力学模型，该模型优于常用的LinDistFlow模型。利用此模型，通过最小化稳态电压偏差来设计VVC斜率，并首次在此背景下引入非凸谱半径稳定性约束。将此约束与现有凸约束进行比较，并通过在实际馈线上的仿真进行验证。", "result": "通过在实际馈线上的仿真，结果表明使用谱半径约束能实现更有效的电压调节，优于现有的凸限制。", "conclusion": "结合历史数据的改进线性化潮流模型和非凸谱半径稳定性约束，可以设计出更稳定、更有效的去中心化VVC斜率，从而改善分布式能源集成环境下的电压调节性能。"}}
{"id": "2510.14179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14179", "abs": "https://arxiv.org/abs/2510.14179", "authors": ["Yuancheng Xu", "Wenqi Xian", "Li Ma", "Julien Philip", "Ahmet Levent Taşel", "Yiwei Zhao", "Ryan Burgert", "Mingming He", "Oliver Hermann", "Oliver Pilarski", "Rahul Garg", "Paul Debevec", "Ning Yu"], "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures", "comment": "Accepted to SIGGRAPH Asia 2025", "summary": "We introduce a framework that enables both multi-view character consistency\nand 3D camera control in video diffusion models through a novel customization\ndata pipeline. We train the character consistency component with recorded\nvolumetric capture performances re-rendered with diverse camera trajectories\nvia 4D Gaussian Splatting (4DGS), lighting variability obtained with a video\nrelighting model. We fine-tune state-of-the-art open-source video diffusion\nmodels on this data to provide strong multi-view identity preservation, precise\ncamera control, and lighting adaptability. Our framework also supports core\ncapabilities for virtual production, including multi-subject generation using\ntwo approaches: joint training and noise blending, the latter enabling\nefficient composition of independently customized models at inference time; it\nalso achieves scene and real-life video customization as well as control over\nmotion and spatial layout during customization. Extensive experiments show\nimproved video quality, higher personalization accuracy, and enhanced camera\ncontrol and lighting adaptability, advancing the integration of video\ngeneration into virtual production. Our project page is available at:\nhttps://eyeline-labs.github.io/Virtually-Being.", "AI": {"tldr": "本文介绍了一个框架，通过新颖的定制数据管道，使视频扩散模型能够实现多视角角色一致性和3D相机控制。", "motivation": "现有视频扩散模型在多视角角色一致性和3D相机控制方面存在局限，限制了其在虚拟制作中的应用。本研究旨在解决这些挑战。", "method": "该框架通过以下方式实现：1) 使用记录的体积捕获性能数据，通过4D高斯泼溅（4DGS）以不同相机轨迹重新渲染，并结合视频重照明模型实现光照变化，训练角色一致性组件。2) 在此数据上微调最先进的开源视频扩散模型。3) 支持多主体生成（联合训练和噪声混合）以及场景/真实视频定制、运动和空间布局控制。", "result": "实验结果表明，该框架显著提高了视频质量、个性化精度（多视角身份保持）、相机控制能力和光照适应性。", "conclusion": "该框架通过提供强大的多视角身份保持、精确的相机控制和光照适应性，以及支持多主体生成等核心功能，推进了视频生成技术与虚拟制作的集成。"}}
{"id": "2510.14338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14338", "abs": "https://arxiv.org/abs/2510.14338", "authors": ["Yuanhong Zeng", "Anushri Dixit"], "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion", "comment": null, "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation.", "AI": {"tldr": "本文提出了一种风险感知强化学习方法，通过条件风险价值（CVaR）训练一系列风险条件策略，并使用多臂bandit框架在线自适应选择最佳策略，以提高四足机器人未知环境下的稳定性和性能。", "motivation": "在未知环境中，四足机器人的运动控制需要更好的稳定性和样本效率。研究旨在训练具有不同鲁棒性水平的策略，并能在线自适应地选择最适合当前环境的鲁棒性水平，以确保性能。", "method": "1. 使用条件风险价值（CVaR）约束的策略优化技术，训练一系列风险条件策略，以实现更好的稳定性和样本效率。2. 在部署时，利用多臂bandit框架，仅基于观察到的回合回报，自适应地从策略族中选择表现最佳的策略，无需特权环境信息，并能实时适应未知条件。", "result": "1. 在八种未见过的模拟环境和真实机器人（Unitree Go2）的未见地形中进行了评估。2. 与其他基线相比，风险感知策略在未见环境中实现了近两倍的平均性能和尾部性能。3. 基于bandit的自适应方法能够在两分钟内，在未知地形中选择表现最佳的风险感知策略。", "conclusion": "所提出的风险感知强化学习方法，结合CVaR训练和多臂bandit自适应选择，显著提高了四足机器人在未知环境中的鲁棒性和性能，并能快速适应变化的环境。"}}
{"id": "2510.14133", "categories": ["cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14133", "abs": "https://arxiv.org/abs/2510.14133", "authors": ["Edoardo Allegrini", "Ananth Shreekumar", "Z. Berkay Celik"], "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems", "comment": null, "summary": "Agentic AI systems, which leverage multiple autonomous agents and Large\nLanguage Models (LLMs), are increasingly used to address complex, multi-step\ntasks. The safety, security, and functionality of these systems are critical,\nespecially in high-stakes applications. However, the current ecosystem of\ninter-agent communication is fragmented, with protocols such as the Model\nContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol\nfor coordination being analyzed in isolation. This fragmentation creates a\nsemantic gap that prevents the rigorous analysis of system properties and\nintroduces risks such as architectural misalignment and exploitable\ncoordination issues. To address these challenges, we introduce a modeling\nframework for agentic AI systems composed of two foundational models. The\nfirst, the host agent model, formalizes the top-level entity that interacts\nwith the user, decomposes tasks, and orchestrates their execution by leveraging\nexternal agents and tools. The second, the task lifecycle model, details the\nstates and transitions of individual sub-tasks from creation to completion,\nproviding a fine-grained view of task management and error handling. Together,\nthese models provide a unified semantic framework for reasoning about the\nbehavior of multi-AI agent systems. Grounded in this framework, we define 17\nproperties for the host agent and 14 for the task lifecycle, categorized into\nliveness, safety, completeness, and fairness. Expressed in temporal logic,\nthese properties enable formal verification of system behavior, detection of\ncoordination edge cases, and prevention of deadlocks and security\nvulnerabilities. Through this effort, we introduce the first rigorously\ngrounded, domain-agnostic framework for the systematic analysis, design, and\ndeployment of correct, reliable, and robust agentic AI systems.", "AI": {"tldr": "该论文提出了一个统一的建模框架，包含宿主代理模型和任务生命周期模型，并定义了基于时序逻辑的形式化属性，以解决多代理AI系统中通信协议碎片化导致的安全和功能分析挑战，从而实现对这些系统行为的严格验证和可靠部署。", "motivation": "代理式AI系统在处理复杂任务时日益普及，但其内部代理间通信协议（如MCP和A2A）的碎片化导致语义鸿沟，阻碍了对系统属性的严格分析，并引入了架构错位和可利用的协调问题等风险，尤其在高风险应用中，这要求提高系统的安全性、保障性和功能性。", "method": "该研究引入了一个由两个基础模型组成的代理式AI系统建模框架：1. 宿主代理模型：形式化顶层实体，负责用户交互、任务分解和协调外部代理与工具执行。2. 任务生命周期模型：详细描述单个子任务从创建到完成的状态和转换，提供任务管理和错误处理的细粒度视图。基于此统一语义框架，研究定义了宿主代理的17个属性和任务生命周期的14个属性，分类为活性、安全性、完整性和公平性，并用时序逻辑表达，以实现系统行为的形式化验证。", "result": "所提出的框架能够对系统行为进行形式化验证，检测协调边缘情况，并预防死锁和安全漏洞。它提供了一个严格基础的、领域无关的框架，用于系统地分析、设计和部署正确、可靠和健壮的代理式AI系统。", "conclusion": "该研究引入了首个严格基础的、领域无关的框架，通过统一的宿主代理和任务生命周期模型以及基于时序逻辑的形式化属性，解决了代理式AI系统通信碎片化的问题，从而能够对多AI代理系统的行为进行系统性分析、设计和部署，显著提升了系统的正确性、可靠性和健壮性。"}}
{"id": "2510.14136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14136", "abs": "https://arxiv.org/abs/2510.14136", "authors": ["David Roqui", "Adèle Cormier", "nistor Grozavu", "Ann Bourges"], "title": "A Multimodal Approach to Heritage Preservation in the Context of Climate Change", "comment": null, "summary": "Cultural heritage sites face accelerating degradation due to climate change,\nyet tradi- tional monitoring relies on unimodal analysis (visual inspection or\nenvironmental sen- sors alone) that fails to capture the complex interplay\nbetween environmental stres- sors and material deterioration. We propose a\nlightweight multimodal architecture that fuses sensor data (temperature,\nhumidity) with visual imagery to predict degradation severity at heritage\nsites. Our approach adapts PerceiverIO with two key innovations: (1) simplified\nencoders (64D latent space) that prevent overfitting on small datasets (n=37\ntraining samples), and (2) Adaptive Barlow Twins loss that encourages modality\ncomplementarity rather than redundancy. On data from Strasbourg Cathedral, our\nmodel achieves 76.9% accu- racy, a 43% improvement over standard multimodal\narchitectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.\nAblation studies reveal that sensor-only achieves 61.5% while image-only\nreaches 46.2%, confirming successful multimodal synergy. A systematic\nhyperparameter study identifies an optimal moderate correlation target ({\\tau}\n=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy\ncompared to other {\\tau} values ({\\tau} =0.1/0.5/0.7: 53.8%, {\\tau} =0.9:\n61.5%). This work demonstrates that architectural sim- plicity combined with\ncontrastive regularization enables effective multimodal learning in data-scarce\nheritage monitoring contexts, providing a foundation for AI-driven con-\nservation decision support systems.", "AI": {"tldr": "本文提出了一种轻量级多模态架构，通过融合传感器数据和视觉图像来预测文化遗产地的退化严重程度，即使在数据稀缺的情况下也能实现高精度，为AI驱动的文物保护提供支持。", "motivation": "文化遗产地因气候变化面临加速退化，但传统监测方法（单一模态分析，如仅视觉检查或环境传感器）无法捕捉环境压力源与材料劣化之间复杂的相互作用，因此需要更全面的监测方法。", "method": "该研究提出了一种轻量级多模态架构，融合传感器数据（温度、湿度）和视觉图像，以预测遗产地的退化严重程度。该方法改进了PerceiverIO，主要创新包括：1) 简化的编码器（64D潜在空间），以防止在小数据集（37个训练样本）上过拟合；2) 自适应Barlow Twins损失，鼓励模态互补而非冗余。", "result": "在斯特拉斯堡大教堂的数据集上，该模型实现了76.9%的准确率，比标准多模态架构（VisualBERT、Transformer）提高了43%，比原始PerceiverIO提高了25%。消融研究显示，仅使用传感器数据准确率为61.5%，仅使用图像数据准确率为46.2%，证实了多模态协同作用的成功。系统性的超参数研究发现，最佳的中等相关目标（τ=0.3）能平衡对齐和互补性，达到69.2%的准确率。", "conclusion": "这项工作表明，架构的简洁性与对比正则化相结合，可以在数据稀缺的遗产监测背景下实现有效的多模态学习，为AI驱动的文物保护决策支持系统奠定了基础。"}}
{"id": "2510.14357", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14357", "abs": "https://arxiv.org/abs/2510.14357", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation", "comment": null, "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN.", "AI": {"tldr": "本研究提出SUM-AgriVLN方法，通过空间理解记忆模块（SUM）利用3D重建和表示来存储空间记忆，从而改进农业视觉-语言导航（VLN）机器人根据自然语言指令进行导航的能力，特别是在重复指令场景下。", "motivation": "现有的农业VLN方法（如AgriVLN）将每个导航指令视为独立事件，忽略了过去经验提供空间上下文的潜力，而实际农业场景中导航指令常重复出现。", "method": "本文提出了空间理解记忆农业视觉-语言导航（SUM-AgriVLN）方法。其中，SUM模块通过3D重建和表示来理解并保存空间记忆，以利用过去的经验为后续导航提供空间上下文。", "result": "在A2A基准测试中，SUM-AgriVLN有效将成功率从0.47提高到0.54，同时导航误差从2.91米略微增加到2.93米。", "conclusion": "SUM-AgriVLN方法在农业领域实现了最先进的性能，通过引入空间理解记忆显著提升了机器人根据自然语言指令进行导航的成功率。"}}
{"id": "2510.14150", "categories": ["cs.AI", "cs.LG", "cs.NE", "I.2.7; I.2.2"], "pdf": "https://arxiv.org/pdf/2510.14150", "abs": "https://arxiv.org/abs/2510.14150", "authors": ["Henrique Assumpção", "Diego Ferreira", "Leandro Campos", "Fabricio Murai"], "title": "CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization", "comment": "11 pages, 9 figures, 2 tables", "summary": "In this work, we introduce CodeEvolve, an open-source evolutionary coding\nagent that unites Large Language Models (LLMs) with genetic algorithms to solve\ncomplex computational problems. Our framework adapts powerful evolutionary\nconcepts to the LLM domain, building upon recent methods for generalized\nscientific discovery. CodeEvolve employs an island-based genetic algorithm to\nmaintain population diversity and increase throughput, introduces a novel\ninspiration-based crossover mechanism that leverages the LLMs context window to\ncombine features from successful solutions, and implements meta-prompting\nstrategies for dynamic exploration of the solution space. We conduct a rigorous\nevaluation of CodeEvolve on a subset of the mathematical benchmarks used to\nevaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that\nour method surpasses AlphaEvolve's performance on several challenging problems.\nTo foster collaboration and accelerate progress, we release our complete\nframework as an open-source repository.", "AI": {"tldr": "CodeEvolve是一个开源的演化编码智能体，结合了大型语言模型（LLM）和遗传算法来解决复杂的计算问题，并在数学基准测试中超越了AlphaEvolve。", "motivation": "研究旨在通过将强大的演化概念应用于LLM领域，并借鉴最近的广义科学发现方法，来解决复杂的计算问题。", "method": "CodeEvolve采用了岛屿式遗传算法来维持种群多样性和提高吞吐量；引入了一种新颖的基于灵感的交叉机制，利用LLM的上下文窗口来组合成功解决方案的特征；并实现了元提示策略以动态探索解决方案空间。", "result": "在用于评估Google DeepMind闭源AlphaEvolve的数学基准测试子集上进行严格评估后，CodeEvolve在几个具有挑战性的问题上超越了AlphaEvolve的性能。", "conclusion": "CodeEvolve是一个有效的开源演化编码智能体，它将LLM与遗传算法相结合，在复杂计算问题上表现出色，甚至超越了AlphaEvolve，并且其完整框架已开源以促进协作和加速进展。"}}
{"id": "2510.14203", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.14203", "abs": "https://arxiv.org/abs/2510.14203", "authors": ["Ryo Masumura", "Shota Orihashi", "Mana Ihori", "Tomohiro Tanaka", "Naoki Makishima", "Taiga Yamane", "Naotaka Kawata", "Satoshi Suzuki", "Taichi Katayama"], "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition", "comment": "Accepted at APSIPA ASC 2025", "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO.", "AI": {"tldr": "本文提出了一种联合建模方法，用于从多模态人类行为中自动识别大五人格和HEXACO人格特质。", "motivation": "大多数现有研究侧重于大五人格识别，但很少有研究关注HEXACO人格特质（特别是与移位攻击和报复相关的诚实-谦逊特质）。此外，机器学习模型中大五人格和HEXACO人格之间的关系尚不明确。", "method": "本文提出了一种联合建模方法，旨在共同优化大五人格和HEXACO人格的识别。", "result": "使用自我介绍视频数据集进行的实验表明，所提出的方法能够有效地识别大五人格和HEXACO人格。", "conclusion": "该联合建模方法可以有效地从多模态人类行为中识别大五人格和HEXACO人格特质。"}}
{"id": "2510.14838", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14838", "abs": "https://arxiv.org/abs/2510.14838", "authors": ["Ziqing Zhu"], "title": "Dynamic-Key-Aware Co-Simulation Framework for Next Generation of SCADA Systems Encrypted by Quantum-Key-Distribution Techniques", "comment": null, "summary": "To address growing cybersecurity challenges in modern power dispatch systems,\nthis paper proposes a multi-layer modeling and optimization framework for SCADA\nsystems enhanced with quantum key distribution (QKD). While most existing\napplications of QKD in the power sector focus on building secure point-to-point\ncommunication tunnels, they rarely consider the system-level coupling between\nkey dynamics and control scheduling. In contrast, our approach integrates\nquantum key generation, consumption, inventory prediction, and control latency\ninto a unified model, enabling key-aware reconfiguration of SCADA control\nchains based on task security demands and real-time resource constraints. To\nresolve conflicts in key resource allocation between transmission system\noperators (TSOs) and distribution system operators (DSOs), we formulate a\nbi-level Stackelberg game and transform it into a mathematical program with\ncomplementarity constraints (MPCC). We further develop an efficient Level\nDecomposition-Complementarity Pruning (LD-CP) algorithm to solve the problem.\nTo support reproducible evaluation, we build an end-to-end co-simulation\nplatform that integrates physical-layer disruptions via OpenQKD-Sim,\nQ3P/IEC-104 protocol stack binding, and real-time control-chain monitoring\nthrough Grafana. Experimental results on the IEEE 39- and 118-bus systems show\nthat our method increases task success rate by 25%, reduces peak frequency\ndeviation by 70%, and improves key utilization to 83%. This work lays the\nfoundation for future quantum-secure control systems in power grid operations.", "AI": {"tldr": "本文提出了一种用于增强型量子密钥分发（QKD）SCADA系统的多层建模和优化框架，该框架将量子密钥动态与控制调度相结合，实现了密钥感知的控制链重配置。通过双层博弈解决密钥资源分配冲突，并开发了LD-CP算法进行求解，实验结果显著提升了任务成功率、降低了频率偏差并提高了密钥利用率。", "motivation": "现代电力调度系统面临日益增长的网络安全挑战。现有电力领域QKD应用主要关注点对点安全通信，但很少考虑密钥动态与控制调度之间的系统级耦合。", "method": "1. 提出了一个QKD增强型SCADA系统的多层建模和优化框架。2. 将量子密钥生成、消耗、库存预测和控制延迟集成到统一模型中。3. 基于任务安全需求和实时资源约束，实现SCADA控制链的密钥感知重配置。4. 将输电系统运营商（TSO）和配电系统运营商（DSO）之间的密钥资源分配冲突建模为双层Stackelberg博弈。5. 将博弈转化为带互补约束的数学规划（MPCC）。6. 开发了高效的层次分解-互补剪枝（LD-CP）算法来解决该问题。7. 构建了一个集成了OpenQKD-Sim、Q3P/IEC-104协议栈绑定和Grafana实时监控的端到端协同仿真平台。", "result": "在IEEE 39-和118-总线系统上的实验结果显示：1. 任务成功率提高了25%。2. 峰值频率偏差降低了70%。3. 密钥利用率提高到83%。", "conclusion": "这项工作为未来电力电网运行中的量子安全控制系统奠定了基础。"}}
{"id": "2510.14854", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14854", "abs": "https://arxiv.org/abs/2510.14854", "authors": ["Honglei Ma", "Erwu Liu", "Wei Ni", "Zhijun Fang", "Rui Wang", "Yongbin Gao", "Dusit Niyato", "Ekram Hossain"], "title": "Through-the-Earth Magnetic Induction Communication and Networking: A Comprehensive Survey", "comment": "This work has been accepted by the IEEE Communications Surveys &\n  Tutorials (COMST) for publication.The final published version will be\n  available on IEEE Xplore", "summary": "Magnetic induction (MI) communication (MIC) has emerged as a promising\ncandidate for underground communication networks due to its excellent\npenetration capabilities. Integration with Space-Air-Ground-Underground (SAGUI)\nnetworks in next-generation mobile communication systems requires a\nwell-defined network architecture. A recent discovery in MIC research, MI fast\nfading, remains in its early stages and presents unique challenges. This paper\nprovides a comprehensive survey on through-the-earth (TTE) MIC, covering MI\napplications, channel modeling, point-to-point MIC design, relay techniques,\nnetwork frameworks, and emerging technologies. We compare various MIC\napplications to highlight TTE-specific challenges and review the principles of\nchannel modeling, addressing both MI slow fading and MI fast fading, along with\nits potential impact on existing MIC theories. We conduct a fine-grained\ndecomposition of MI channel power gain into four distinct physical parameters,\nand propose a novel geometric model to analyze MI fast fading. We also\nsummarize MI relay techniques, examine crosstalk effects in relay and\nhigh-density networks, and explore key research tasks within the OSI framework\nfor a holistic MI network protocol in SAGUI. To bridge the gaps identified, we\npropose a MIC framework that supports TCP/IP and Linux, enabling full\nimplementation of existing and emerging MIC solutions. This framework empowers\nresearchers to leverage Linux resources and deep learning platforms for\naccelerated development of MIC in SAGUI networks. Remaining research\nchallenges, open issues, and promising novel techniques are further identified\nto advance MIC research.", "AI": {"tldr": "本文对地层穿透磁感应通信（TTE MIC）进行了全面综述，涵盖应用、信道建模（包括磁感应快衰落）、中继技术、网络架构，并提出了一个支持TCP/IP和Linux的MIC框架，以促进其在空天地一体化（SAGUI）网络中的发展。", "motivation": "磁感应通信（MIC）因其卓越的穿透能力而成为地下通信网络的一个有前景的候选技术。将其集成到下一代移动通信系统的空天地一体化（SAGUI）网络中需要明确的网络架构。此外，MIC研究中最近发现的磁感应快衰落仍处于早期阶段，并带来了独特的挑战。", "method": "本文进行了一项综合性调查，比较了各种MIC应用，回顾了信道建模原理（包括慢衰落和快衰落），将MI信道功率增益细分为四个物理参数，并提出了一个新颖的几何模型来分析MI快衰落。此外，还总结了MI中继技术，考察了中继和高密度网络中的串扰效应，并探索了OSI框架内的关键研究任务，以构建SAGUI中的整体MI网络协议。最后，提出了一个支持TCP/IP和Linux的MIC框架。", "result": "本文识别了TTE特有的挑战，审查了信道建模原理并讨论了快衰落的影响，将MI信道功率增益分解为四个物理参数，并提出了一个分析MI快衰落的几何模型。总结了MI中继技术并检查了串扰效应。提出了一个支持TCP/IP和Linux的MIC框架，旨在弥补现有差距并加速SAGUI网络中MIC解决方案的开发。同时，也识别了剩余的研究挑战和有前景的新技术。", "conclusion": "本文通过提出一个支持TCP/IP和Linux的MIC框架，弥补了在空天地一体化（SAGUI）网络中集成MIC所识别的差距，该框架能赋能研究人员利用Linux资源和深度学习平台加速MIC的开发。此外，还指出了剩余的研究挑战、开放性问题和有前景的新技术，以推动MIC研究的进展。"}}
{"id": "2510.13843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13843", "abs": "https://arxiv.org/abs/2510.13843", "authors": ["Zhirong Chou", "Quan Qin", "Shi Li"], "title": "Serialized EHR make for good text representations", "comment": null, "summary": "The emergence of foundation models in healthcare has opened new avenues for\nlearning generalizable representations from large scale clinical data. Yet,\nexisting approaches often struggle to reconcile the tabular and event based\nnature of Electronic Health Records (EHRs) with the sequential priors of\nnatural language models. This structural mismatch limits their ability to\ncapture longitudinal dependencies across patient encounters. We introduce\nSerialBEHRT, a domain aligned foundation model that extends SciBERT through\nadditional pretraining on structured EHR sequences. SerialBEHRT is designed to\nencode temporal and contextual relationships among clinical events, thereby\nproducing richer patient representations. We evaluate its effectiveness on the\ntask of antibiotic susceptibility prediction, a clinically meaningful problem\nin antibiotic stewardship. Through extensive benchmarking against state of the\nart EHR representation strategies, we demonstrate that SerialBEHRT achieves\nsuperior and more consistent performance, highlighting the importance of\ntemporal serialization in foundation model pretraining for healthcare.", "AI": {"tldr": "本文介绍了SerialBEHRT，一个通过在结构化EHR序列上进行额外预训练来扩展SciBERT的领域对齐基础模型，旨在更好地捕获临床事件的纵向和上下文关系，并在抗生素敏感性预测任务中表现出卓越性能。", "motivation": "现有医疗领域的基础模型难以调和电子健康记录（EHR）的表格和事件性质与自然语言模型的序列先验之间的结构不匹配问题，这限制了它们捕捉患者就诊之间纵向依赖关系的能力。", "method": "研究者引入了SerialBEHRT，这是一个领域对齐的基础模型。它通过在结构化EHR序列上进行额外的预训练来扩展SciBERT，旨在编码临床事件之间的时间和上下文关系，从而产生更丰富的患者表征。", "result": "通过与现有最先进的EHR表征策略进行广泛基准测试，SerialBEHRT在抗生素敏感性预测任务上取得了卓越且更一致的性能。", "conclusion": "研究结果强调了在医疗领域基础模型预训练中时间序列化的重要性，SerialBEHRT通过有效处理EHR的时间和上下文关系，显著提升了患者表征能力和预测性能。"}}
{"id": "2510.14414", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14414", "abs": "https://arxiv.org/abs/2510.14414", "authors": ["Baris Baysal", "Omid Arfaie", "Ramazan Unal"], "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit", "comment": null, "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle.", "AI": {"tldr": "本研究介绍了一种名为RoboANKLE的动力式胫骨假肢，通过新颖的能量存储和释放机制（ESER和EES），提供完整的蹬地辅助，旨在模仿自然踝关节运动，并在保持能量自主性和最小化重量的同时，实现高扭矩和功率输出。", "motivation": "现有主动式胫骨假肢在保持能量自主性和最小化重量方面面临挑战。研究旨在设计一种能够提供足够运动范围和必要扭矩的假肢，以实现日常活动中的自然踝关节运动，特别是通过提供广泛的蹬地辅助来模仿人类踝关节的自然扭矩曲线。", "method": "研究采用能量存储和扩展释放（ESER）机制与新型额外能量存储（EES）机制相结合的设计。通过运动学和动力学分析确定设计参数并评估性能。构建CAD模型进行全面的动态和结构分析，并利用拓扑优化实现最小化重量。最终制造原型进行实验评估以验证设计和功能。", "result": "RoboANKLE原型质量为1.92公斤，尺寸为261x107x420毫米。功能评估显示，它能以95%的精度达到自然的踝关节最大背屈角度。由于所实现的机制，RoboANKLE能够产生比自然行走所需扭矩高57%的扭矩。其功率生成能力比步态周期中的自然功率高10%。", "conclusion": "RoboANKLE成功实现了完整的蹬地辅助，能够以高精度模仿自然的踝关节运动，并提供远超自然行走所需的扭矩和功率，同时有效解决了主动式胫骨假肢在重量和能量输出方面的挑战。"}}
{"id": "2510.14230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14230", "abs": "https://arxiv.org/abs/2510.14230", "authors": ["Hongsong Wang", "Renxi Cheng", "Yang Zhang", "Chaolei Han", "Jie Gui"], "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection", "comment": "Published in the ICCV2025, COde is\n  https://github.com/hongsong-wang/LOTA", "summary": "The rapid advancement of GAN and Diffusion models makes it more difficult to\ndistinguish AI-generated images from real ones. Recent studies often use\nimage-based reconstruction errors as an important feature for determining\nwhether an image is AI-generated. However, these approaches typically incur\nhigh computational costs and also fail to capture intrinsic noisy features\npresent in the raw images. To solve these problems, we innovatively refine\nerror extraction by using bit-plane-based image processing, as lower bit planes\nindeed represent noise patterns in images. We introduce an effective bit-planes\nguided noisy image generation and exploit various image normalization\nstrategies, including scaling and thresholding. Then, to amplify the noise\nsignal for easier AI-generated image detection, we design a maximum gradient\npatch selection that applies multi-directional gradients to compute the noise\nscore and selects the region with the highest score. Finally, we propose a\nlightweight and effective classification head and explore two different\nstructures: noise-based classifier and noise-guided classifier. Extensive\nexperiments on the GenImage benchmark demonstrate the outstanding performance\nof our method, which achieves an average accuracy of \\textbf{98.9\\%}\n(\\textbf{11.9}\\%~$\\uparrow$) and shows excellent cross-generator generalization\ncapability. Particularly, our method achieves an accuracy of over 98.2\\% from\nGAN to Diffusion and over 99.2\\% from Diffusion to GAN. Moreover, it performs\nerror extraction at the millisecond level, nearly a hundred times faster than\nexisting methods. The code is at https://github.com/hongsong-wang/LOTA.", "AI": {"tldr": "该研究提出一种高效的AI生成图像检测方法，通过位平面处理提取噪声特征，结合梯度补丁选择和轻量级分类器，实现了高准确率、出色的跨生成器泛化能力和极快的错误提取速度。", "motivation": "GAN和Diffusion模型快速发展，使得区分AI生成图像和真实图像变得困难。现有方法通常依赖基于图像的重建误差作为特征，但计算成本高昂，且未能捕获原始图像中固有的噪声特征。", "method": "本研究通过以下方法解决问题：1) 创新性地利用基于位平面的图像处理来提炼错误提取，因为较低的位平面能代表图像中的噪声模式。2) 引入有效的位平面引导噪声图像生成和各种图像归一化策略（包括缩放和阈值）。3) 设计最大梯度补丁选择，通过多方向梯度计算噪声分数并选择最高分区域，以放大噪声信号。4) 提出轻量级且有效的分类头，并探索了两种结构：基于噪声的分类器和噪声引导的分类器。", "result": "在GenImage基准测试中，该方法取得了平均98.9%的准确率（提高11.9%），并展现出卓越的跨生成器泛化能力。具体而言，从GAN到Diffusion的准确率超过98.2%，从Diffusion到GAN的准确率超过99.2%。此外，其错误提取速度达到毫秒级，比现有方法快近百倍。", "conclusion": "该研究提出的方法通过利用位平面中的噪声特征，结合高效的噪声信号放大和轻量级分类器，显著提高了AI生成图像检测的准确性、泛化能力，并大幅降低了计算成本，实现了毫秒级的错误提取速度。"}}
{"id": "2510.14931", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.14931", "abs": "https://arxiv.org/abs/2510.14931", "authors": ["Bo Wang", "Tianyu Han", "Guangwei Wang"], "title": "Further Results on Safety-Critical Stabilization of Force-Controlled Nonholonomic Mobile Robots", "comment": null, "summary": "In this paper, we address the stabilization problem for force-controlled\nnonholonomic mobile robots under safety-critical constraints. We propose a\ncontinuous, time-invariant control law based on the gamma m-quadratic\nprogramming (gamma m-QP) framework, which unifies control Lyapunov functions\n(CLFs) and control barrier functions (CBFs) to enforce both stability and\nsafety in the closed-loop system. For the first time, we construct a global,\ntime-invariant, strict Lyapunov function for the closed-loop nonholonomic\nmobile robot system with a nominal stabilization controller in polar\ncoordinates; this strict Lyapunov function then serves as the CLF in the QP\ndesign. Next, by exploiting the inherent cascaded structure of the vehicle\ndynamics, we develop a CBF for the mobile robot via an integrator backstepping\nprocedure. Our main results guarantee both asymptotic stability and safety for\nthe closed-loop system. Both the simulation and experimental results are\npresented to illustrate the effectiveness and performance of our approach.", "AI": {"tldr": "本文提出了一种基于gamma m-二次规划（QP）框架的连续时不变控制律，用于稳定力控非完整移动机器人，同时确保安全关键约束下的渐近稳定性和安全性。", "motivation": "解决力控非完整移动机器人的稳定化问题，并同时满足安全关键约束，确保闭环系统的稳定性和安全性。", "method": "1. 提出了一种基于gamma m-QP框架的连续时不变控制律，该框架统一了控制Lyapunov函数（CLF）和控制障碍函数（CBF）。2. 首次在极坐标系下构建了一个全局、时不变的严格Lyapunov函数作为QP设计中的CLF。3. 利用车辆动力学固有的级联结构，通过积分反步法开发了移动机器人的CBF。", "result": "主要结果保证了闭环系统的渐近稳定性和安全性。仿真和实验结果均验证了该方法的有效性和性能。", "conclusion": "所提出的基于gamma m-QP的控制方法能够有效地稳定力控非完整移动机器人，并同时确保系统的安全性和渐近稳定性。"}}
{"id": "2510.13842", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.13842", "abs": "https://arxiv.org/abs/2510.13842", "authors": ["Yutao Wu", "Xiao Liu", "Yinghui Li", "Yifeng Gao", "Yifan Ding", "Jiale Ding", "Xiang Zheng", "Xingjun Ma"], "title": "ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking", "comment": null, "summary": "Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation\n(RAG) systems by injecting adversarial content into knowledge bases, tricking\nLarge Language Models (LLMs) into producing attacker-controlled outputs\ngrounded in manipulated context. Prior work highlights LLMs' susceptibility to\nmisleading or malicious retrieved content. However, real-world fact-checking\nscenarios are more challenging, as credible evidence typically dominates the\nretrieval pool. To investigate this problem, we extend knowledge poisoning to\nthe fact-checking setting, where retrieved context includes authentic\nsupporting or refuting evidence. We propose \\textbf{ADMIT}\n(\\textbf{AD}versarial \\textbf{M}ulti-\\textbf{I}njection \\textbf{T}echnique), a\nfew-shot, semantically aligned poisoning attack that flips fact-checking\ndecisions and induces deceptive justifications, all without access to the\ntarget LLMs, retrievers, or token-level control. Extensive experiments show\nthat ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4\ncross-domain benchmarks, achieving an average attack success rate (ASR) of 86\\%\nat an extremely low poisoning rate of $0.93 \\times 10^{-6}$, and remaining\nrobust even in the presence of strong counter-evidence. Compared with prior\nstate-of-the-art attacks, ADMIT improves ASR by 11.2\\% across all settings,\nexposing significant vulnerabilities in real-world RAG-based fact-checking\nsystems.", "AI": {"tldr": "本文研究了在真实世界事实核查场景中，知识投毒对检索增强生成（RAG）系统的威胁。提出了一种名为ADMIT的少样本、语义对齐的投毒攻击，该攻击能以极低投毒率高效诱导大型语言模型（LLM）产生错误判断和虚假理由，揭示了RAG事实核查系统的显著漏洞。", "motivation": "现有研究已指出LLM易受误导或恶意检索内容的影响。然而，在真实世界的事实核查场景中，可信证据通常在检索池中占主导地位，这使得投毒攻击更具挑战性。因此，需要研究在包含真实支持或反驳证据的检索上下文中，知识投毒如何影响事实核查。", "method": "本文提出ADMIT（ADversarial Multi-Injection Technique），一种少样本、语义对齐的投毒攻击。该攻击无需访问目标LLM、检索器或进行令牌级控制，即可翻转事实核查决策并诱导欺骗性理由。研究将知识投毒扩展到事实核查设置，其中检索上下文包含真实的支持或反驳证据。", "result": "ADMIT在4种检索器、11个LLM和4个跨领域基准上有效迁移，平均攻击成功率（ASR）高达86%，且投毒率极低（0.93 × 10^-6）。即使在存在强反驳证据的情况下，该攻击仍能保持鲁棒性。与现有最先进攻击相比，ADMIT在所有设置下将ASR提高了11.2%。", "conclusion": "ADMIT攻击揭示了真实世界RAG事实核查系统中的显著漏洞。即使在可信证据占主导的情况下，知识投毒也能有效诱导LLM产生 attacker-controlled 的输出和欺骗性理由，对RAG系统的安全性构成严重威胁。"}}
{"id": "2510.14154", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14154", "abs": "https://arxiv.org/abs/2510.14154", "authors": ["Tian Liu", "Alex Cann", "Ian Colbert", "Mehdi Saeedi"], "title": "Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola", "comment": "8 pages, 4 figures, 5 tables", "summary": "While the rapid advancements in the reinforcement learning (RL) research\ncommunity have been remarkable, the adoption in commercial video games remains\nslow. In this paper, we outline common challenges the Game AI community faces\nwhen using RL-driven NPCs in practice, and highlight the intersection of RL\nwith traditional behavior trees (BTs) as a crucial juncture to be explored\nfurther. Although the BT+RL intersection has been suggested in several research\npapers, its adoption is rare. We demonstrate the viability of this approach\nusing AMD Schola -- a plugin for training RL agents in Unreal Engine -- by\ncreating multi-task NPCs in a complex 3D environment inspired by the commercial\nvideo game ``The Last of Us\". We provide detailed methodologies for jointly\ntraining RL models with BTs while showcasing various skills.", "AI": {"tldr": "本文探讨了强化学习（RL）在商业视频游戏NPC中应用缓慢的问题，提出并演示了将RL与传统行为树（BTs）结合的可行性，使用AMD Schola在虚幻引擎中创建了多任务NPC。", "motivation": "尽管强化学习研究进展迅速，但在商业视频游戏中的应用却很缓慢。游戏AI社区在使用RL驱动的NPC时面临实际挑战。BT+RL的结合虽然被提出，但很少被采用，因此需要进一步探索和验证其可行性。", "method": "作者首先概述了游戏AI社区在使用RL驱动NPC时面临的常见挑战，并强调了RL与行为树（BTs）结合的重要性。他们使用AMD Schola（一个用于在虚幻引擎中训练RL代理的插件），在一个受商业游戏“The Last of Us”启发的复杂3D环境中，创建了多任务NPC，以展示这种方法的潜力。论文还提供了RL模型与BTs联合训练的详细方法。", "result": "研究结果表明，将RL与行为树结合的方法是可行的，并成功展示了在这种联合训练方法下，多任务NPC能够习得各种技能。这验证了BT+RL作为解决RL在游戏NPC中实际应用挑战的有效途径。", "conclusion": "RL与行为树（BTs）的结合是解决强化学习在商业视频游戏NPC中应用挑战的关键途径。通过实际演示，证明了这种联合训练方法的可行性，为未来游戏AI的开发提供了实用的解决方案。"}}
{"id": "2510.14241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14241", "abs": "https://arxiv.org/abs/2510.14241", "authors": ["Soumyya Kanti Datta", "Tanvi Ranga", "Chengzhe Sun", "Siwei Lyu"], "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis", "comment": null, "summary": "The rise of manipulated media has made deepfakes a particularly insidious\nthreat, involving various generative manipulations such as lip-sync\nmodifications, face-swaps, and avatar-driven facial synthesis. Conventional\ndetection methods, which predominantly depend on manually designed\nphoneme-viseme alignment thresholds, fundamental frame-level consistency\nchecks, or a unimodal detection strategy, inadequately identify modern-day\ndeepfakes generated by advanced generative models such as GANs, diffusion\nmodels, and neural rendering techniques. These advanced techniques generate\nnearly perfect individual frames yet inadvertently create minor temporal\ndiscrepancies frequently overlooked by traditional detectors. We present a\nnovel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic\nAnalysis(PIA), incorporating language, dynamic face motion, and facial\nidentification cues to address these limitations. We utilize phoneme sequences,\nlip geometry data, and advanced facial identity embeddings. This integrated\nmethod significantly improves the detection of subtle deepfake alterations by\nidentifying inconsistencies across multiple complementary modalities. Code is\navailable at https://github.com/skrantidatta/PIA", "AI": {"tldr": "本文提出了一种名为PIA的新型多模态音视频框架，通过结合语言、动态面部运动和面部识别线索，显著提高了对现代深度伪造（deepfake）媒体的检测能力。", "motivation": "传统的深度伪造检测方法（依赖手动设计的音素-视觉对齐阈值、帧级一致性检查或单模态策略）无法有效识别由GANs、扩散模型和神经渲染等先进生成模型产生的现代深度伪造。这些先进技术生成的视频帧近乎完美，但无意中会产生传统检测器容易忽略的微小时间不一致性。", "method": "本文提出了一种新颖的多模态音视频框架——音素-时间与身份-动态分析（PIA）。该方法整合了语言、动态面部运动和面部识别线索，具体利用音素序列、唇部几何数据和先进的面部身份嵌入来识别跨多个互补模态的不一致性。", "result": "这种集成方法通过识别多个互补模态间的不一致性，显著提高了对细微深度伪造篡改的检测能力。", "conclusion": "PIA框架通过结合多模态信息，有效解决了现代深度伪造媒体带来的检测挑战，尤其擅长识别先进生成模型产生的细微时间差异。"}}
{"id": "2510.13847", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13847", "abs": "https://arxiv.org/abs/2510.13847", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models", "comment": null, "summary": "Speculative decoding (a.k.a. speculative sampling) has become a standard way\nto accelerate LLM inference: a small drafter proposes multiple tokens and a\nlarge target model verifies them once per speculation length. Recently, scaling\nof the LLM vocabulary has pushed the number of tokens to grow substantially.\nWhile verification over the full vocabulary leaves the target model largely\nunaffected, the O(|V|d) parameters in the drafter's output head become a\nlatency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,\nFR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the\ntarget model's vocabulary, ranked in descending order of token frequency.\nAlthough this reduces draft-time compute, it is brittle, since: (i) frequency\nlists are corpus-dependent and require retuning to generalize, and (ii) static\nshortlists suppress rare or domain-specific tokens, lowering the expected\nnumber of tokens per verification step. We propose DynaSpec, a\ncontext-dependent dynamic shortlisting mechanism that is robust, speeds up\ndrafting, and generalizes across diverse tasks. Concretely, we introduce\nlightweight, coarse-grained meta-classifiers that route contexts to a small\nnumber of token clusters; the union of the top-k selected clusters forms the\ndrafter's shortlist, while verification retains the full vocabulary and\nexactness. The meta-classifier finishes its computation earlier than the\ndrafter's hidden state generation by exploiting parallel execution of draft\nencoding and meta shortlisting on separate streams. On standard\nspeculative-decoding benchmarks, we observe consistent gains in mean accepted\nlength over fixed-shortlist baselines, while context-dependent selection\nenables smaller shortlists without degrading acceptance.", "AI": {"tldr": "本文提出DynaSpec，一种上下文相关的动态词表短名单机制，用于加速LLM推断中的推测解码。它通过轻量级元分类器动态选择推测器（drafter）的词表子集，解决了传统固定短名单方法的局限性和推测器词表过大导致的延迟瓶颈，同时保持完整性并提高接受长度。", "motivation": "随着LLM词表的扩大，推测解码中推测器的输出头参数（O(|V|d)）成为延迟瓶颈。现有方法（如FR-Spec, VocabTrim）使用固定的词表子集，但这些方法依赖语料库、需要重新调整，并且会抑制稀有或特定领域的词元，导致每次验证的预期词元数量降低。", "method": "DynaSpec引入轻量级、粗粒度的元分类器，将上下文路由到少量词元簇。这些选定簇的并集构成推测器的短名单，而验证阶段仍使用完整词表以保持精确性。元分类器的计算通过与推测器编码并行执行，提前完成。", "result": "在标准推测解码基准测试中，DynaSpec相对于固定短名单基线，在平均接受长度上表现出持续的提升。上下文相关的选择允许使用更小的短名单，而不会降低接受率。", "conclusion": "DynaSpec是一种鲁棒、加速推测器草稿生成、且可泛化到不同任务的上下文相关动态短名单机制。它通过动态选择词表子集，有效解决了推测解码的延迟瓶颈，并在提高效率的同时保持或提升了词元接受率。"}}
{"id": "2510.14454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14454", "abs": "https://arxiv.org/abs/2510.14454", "authors": ["Tao Huang", "Huayi Wang", "Junli Ren", "Kangning Yin", "Zirui Wang", "Xiao Chen", "Feiyu Jia", "Wentao Zhang", "Junfeng Long", "Jingbo Wang", "Jiangmiao Pang"], "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking", "comment": "9 pages", "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/.", "AI": {"tldr": "AdaMimic是一种新颖的运动跟踪算法，它能使人形机器人仅通过一个参考动作实现可适应的控制，同时保持高精度和灵活性。", "motivation": "现有运动先验方法在少量动作下具有良好的适应性，但牺牲了模仿精度；而运动跟踪方法能实现精确模仿，却需要大量训练动作和测试时目标动作。本研究旨在结合两者的优点，在保证模仿精度的同时，减少数据依赖，实现人形机器人的动作适应性。", "method": "本研究提出了AdaMimic算法。首先，它通过将单个参考动作稀疏化为关键帧，并进行轻微编辑（仅需最少物理假设），来创建增强数据集。然后，通过跟踪这些稀疏关键帧来初始化策略，生成密集的中间动作。接着，训练适配器来调整跟踪速度，并根据调整结果细化低级动作，从而实现灵活的时间扭曲，进一步提高模仿精度和适应性。", "result": "该方法在仿真和真实世界的Unitree G1人形机器人上，针对多种任务和广泛的适应条件进行了验证，结果表明其取得了显著的改进。", "conclusion": "AdaMimic成功地解决了现有方法在数据依赖、模仿精度和适应性方面的局限性，实现了仅从一个参考动作就能进行可适应的人形机器人控制。"}}
{"id": "2510.14467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14467", "abs": "https://arxiv.org/abs/2510.14467", "authors": ["Shang-Fu Chen", "Co Yong", "Shao-Hua Sun"], "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models", "comment": "Published in IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)", "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data.", "AI": {"tldr": "模仿学习通常假设专家演示完美，但实际中常有噪声。本文提出“过滤-恢复”框架，首先过滤干净样本，再用条件扩散模型恢复噪声样本，实验证明其在各种任务中优于现有方法，并对噪声具有鲁棒性。", "motivation": "现有的模仿学习算法大多假设专家演示是完美的，但在实际应用中，专家演示往往包含由人类错误或传感器/控制系统不准确性引起的固有噪声和缺陷。", "method": "提出一个“过滤-恢复”框架。该方法首先从演示数据中过滤出干净的样本，然后利用条件扩散模型来恢复那些带有噪声的样本。", "result": "实验结果表明，该框架在机器人手臂操作、灵巧操作和运动等多种领域中始终优于现有方法。消融研究进一步验证了每个组件的有效性，并展示了该框架对不同噪声类型和水平的鲁棒性。", "conclusion": "这些结果证实了该框架对含噪声离线演示数据的实际适用性。"}}
{"id": "2510.14169", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14169", "abs": "https://arxiv.org/abs/2510.14169", "authors": ["Praphul Singh", "Corey Barrett", "Sumana Srivasta", "Amitabh Saikia", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "JEDA: Query-Free Clinical Order Search from Ambient Dialogues", "comment": null, "summary": "Clinical conversations mix explicit directives (order a chest X-ray) with\nimplicit reasoning (the cough worsened overnight, we should check for\npneumonia). Many systems rely on LLM rewriting, adding latency, instability,\nand opacity that hinder real-time ordering. We present JEDA (Joint Embedding\nfor Direct and Ambient clinical orders), a domain-initialized bi-encoder that\nretrieves canonical orders directly and, in a query-free mode, encodes a short\nrolling window of ambient dialogue to trigger retrieval. Initialized from\nPubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA\naligns heterogeneous expressions of intent to shared order concepts. Training\nuses constrained LLM guidance to tie each signed order to complementary\nformulations (command only, context only, command+context, context+reasoning),\nproducing clearer inter-order separation, tighter query extendash order\ncoupling, and stronger generalization. The query-free mode is noise-resilient,\nreducing sensitivity to disfluencies and ASR errors by conditioning on a short\nwindow rather than a single utterance. Deployed in practice, JEDA yields large\ngains and substantially outperforms its base encoder and recent open embedders\n(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The\nresult is a fast, interpretable, LLM-free retrieval layer that links ambient\ncontext to actionable clinical orders in real time.", "AI": {"tldr": "JEDA是一个领域初始化的双编码器，用于从混合了显性和隐性信息的临床对话中实时检索规范的临床医嘱，无需LLM重写，且表现优于现有方法。", "motivation": "传统的LLM重写方法在处理临床医嘱时存在延迟、不稳定性和不透明性，阻碍了实时应用。临床对话中医嘱表达形式多样，既有明确指令也有隐含推理，需要一种能有效处理这些异构表达并实时链接到可操作医嘱的系统。", "method": "JEDA是一个领域初始化的双编码器，基于PubMedBERT并采用去重安全的对比学习目标进行微调。它通过受约束的LLM指导，将每份医嘱与其互补的表达形式（仅命令、仅上下文、命令+上下文、上下文+推理）关联起来。它支持直接检索，也支持在“无查询”模式下，通过编码一段简短的滚动对话窗口来触发检索，以增强对口语不流畅和ASR错误的鲁棒性。", "result": "JEDA在实际部署中取得了显著提升，并显著优于其基础编码器和近期开放的嵌入模型（如Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma）。其结果是一个快速、可解释、无需LLM的检索层，能够实时将环境上下文与可操作的临床医嘱关联起来。", "conclusion": "JEDA提供了一个高效、可解释且无需LLM的实时检索解决方案，能够将临床对话中的环境上下文与可执行的医嘱紧密结合，解决了现有方法在速度、稳定性和透明度方面的挑战。"}}
{"id": "2510.14176", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14176", "abs": "https://arxiv.org/abs/2510.14176", "authors": ["Roger Creus Castanyer", "Faisal Mohamed", "Pablo Samuel Castro", "Cyrus Neary", "Glen Berseth"], "title": "ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) algorithms are highly sensitive to reward\nfunction specification, which remains a central challenge limiting their broad\napplicability. We present ARM-FM: Automated Reward Machines via Foundation\nModels, a framework for automated, compositional reward design in RL that\nleverages the high-level reasoning capabilities of foundation models (FMs).\nReward machines (RMs) -- an automata-based formalism for reward specification\n-- are used as the mechanism for RL objective specification, and are\nautomatically constructed via the use of FMs. The structured formalism of RMs\nyields effective task decompositions, while the use of FMs enables objective\nspecifications in natural language. Concretely, we (i) use FMs to automatically\ngenerate RMs from natural language specifications; (ii) associate language\nembeddings with each RM automata-state to enable generalization across tasks;\nand (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse\nsuite of challenging environments, including evidence of zero-shot\ngeneralization.", "AI": {"tldr": "ARM-FM框架利用基础模型（FM）自动从自然语言规范中生成奖励机器（RM），以解决强化学习（RL）中奖励函数规范的挑战，实现任务分解和零样本泛化。", "motivation": "强化学习算法对奖励函数规范高度敏感，这是限制其广泛应用的核心挑战。", "method": "本研究提出了ARM-FM框架：1) 利用基础模型（FM）从自然语言规范中自动生成奖励机器（RM），RM作为RL目标规范的机制；2) 将语言嵌入与每个RM自动机状态关联起来，以实现跨任务的泛化；3) 结合RM的结构化形式主义实现有效的任务分解，并利用FM实现自然语言的目标规范。", "result": "提供了ARM-FM在各种具有挑战性的环境中有效性的实证证据，包括零样本泛化的证据。", "conclusion": "ARM-FM框架通过利用基础模型实现奖励机器的自动化生成，有效解决了强化学习中奖励函数规范的难题，实现了从自然语言到结构化奖励设计的转化，并展现了良好的泛化能力。"}}
{"id": "2510.14511", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14511", "abs": "https://arxiv.org/abs/2510.14511", "authors": ["Mingtian Du", "Suhas Raghavendra Kulkarni", "Simone Kager", "Domenico Campolo"], "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots", "comment": null, "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies.", "AI": {"tldr": "本文建立了机器人介导人机（双人）交互系统在网络时延下的分析稳定性判据，区分了时延无关和时延相关的稳定性，并探讨了参数对稳定性的影响，为系统设计和时延补偿提供了指导。", "motivation": "研究网络时延下机器人介导人际（双人）触觉通信系统的稳定性问题，以确保远程交互的可靠性。", "method": "采用频域分析方法，结合数值仿真进行支持，并通过机器人执行类人运动的实验进一步验证了稳定性与运动性能之间的相关性。", "result": "提出了时延无关和时延相关的稳定性判据。时延无关判据保证了无论时延多大系统都稳定，而时延相关判据则定义了系统失稳前的最大可容忍时延。判据表明稳定性依赖于控制器和机器人动态参数，其中刚度增加会非线性地降低最大可容忍时延，从而提高系统脆弱性。实验还揭示了稳定性与运动性能之间的关联。", "conclusion": "所提出的稳定性判据可推广应用于多种机器人介导交互系统，并能为稳定的远程双人系统设计提供指导。研究结果也为有效的时延补偿策略提供了先决条件。"}}
{"id": "2510.14546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14546", "abs": "https://arxiv.org/abs/2510.14546", "authors": ["Matti Pekkanen", "Francesco Verdoja", "Ville Kyrki"], "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps", "comment": "Submitted to ICRA 2026", "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.", "AI": {"tldr": "本文提出一种利用视觉-语言模型嵌入空间中的同义词和反义词，通过启发式方法估计相关语言空间，并训练分类器来识别环境中与查询相关部分的解决方案，显著提高了地图和图像的可查询性。", "motivation": "视觉-语言模型（VLM）嵌入被广泛用于机器人地图以实现开放词汇的场景理解，但核心挑战在于机器人难以确定环境中哪些部分与用户查询相关。", "method": "该方法利用查询在嵌入空间中的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并用此训练一个分类器，将环境划分为匹配项和非匹配项。该技术与所使用的表示和编码器无关，且所需训练有限。", "result": "通过对地图和标准图像基准进行查询的广泛实验表明，该方法显著提高了地图和图像的可查询性。", "conclusion": "所提出的查询技术能够有效解决识别环境中与查询相关部分的问题，提升了地图和图像的查询能力，且对表示和编码器具有普适性，训练需求低。"}}
{"id": "2510.14947", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14947", "abs": "https://arxiv.org/abs/2510.14947", "authors": ["Blake Werner", "Lizhi Yang", "Aaron D. Ames"], "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion", "comment": "8 pages", "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion.", "AI": {"tldr": "本文提出一种分层控制架构（LCA），结合快速本体感受稳定器和慢速感知策略，显著提升了人形机器人在非结构化环境中的鲁棒运动能力，优于端到端设计。", "motivation": "在非结构化环境中，人形机器人需要平衡快速的低层稳定与慢速的感知决策。现有的单一同质（end-to-end）设计难以实现这种平衡，导致性能不够鲁棒。", "method": "采用简单的分层控制架构（LCA），包含一个高速运行的本体感受稳定器和一个紧凑的低速感知策略。通过两阶段训练课程进行训练：首先进行盲稳定器预训练，然后进行感知微调。", "result": "在仿真和硬件（Unitree G1人形机器人）上，分层策略持续优于单阶段替代方案。在楼梯和壁架任务中，本文方法能够成功完成，而单阶段感知策略则失败。结果表明，即使使用最少的感知编码器，分层架构也能实现更强的鲁棒性。", "conclusion": "时间尺度的架构分离是实现鲁棒感知条件运动的关键促成因素，其重要性超越了网络规模或复杂性。"}}
{"id": "2510.13848", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13848", "abs": "https://arxiv.org/abs/2510.13848", "authors": ["Ondrej Bohdal", "Konstantinos Theodosiadis", "Asterios Mpatziakas", "Dimitris Filippidis", "Iro Spyrou", "Christos Zonios", "Anastasios Drosou", "Dimosthenis Ioannidis", "Kyeng-Hun Lee", "Jijoong Moon", "Hyeonmok Ko", "Mete Ozay", "Umberto Michieli"], "title": "On-device System of Compositional Multi-tasking in Large Language Models", "comment": "Accepted at EMNLP 2025 (industry track)", "summary": "Large language models (LLMs) are commonly adapted for diverse downstream\ntasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters\n(LoRA). While adapters can be combined to handle multiple tasks separately,\nstandard approaches struggle when targeting the simultaneous execution of\ncomplex tasks, such as generating a translated summary from a long\nconversation. To address this challenge, we propose a novel approach tailored\nspecifically for compositional multi-tasking scenarios involving summarization\nand translation. Our technique involves adding a learnable projection layer on\ntop of the combined summarization and translation adapters. This design enables\neffective integration while maintaining efficiency through reduced\ncomputational overhead compared to alternative strategies requiring extensive\nretraining or sequential processing. We demonstrate the practical viability of\nour method within an on-device environment by developing an Android app capable\nof executing compositional tasks seamlessly. Experimental results indicate our\nsolution performs well and is fast in both cloud-based and on-device\nimplementations, highlighting the potential benefits of adopting our framework\nin real-world applications demanding high-speed operation alongside resource\nconstraints.", "AI": {"tldr": "本文提出一种新方法，通过在组合的LoRA适配器之上添加可学习投影层，解决了LLM在执行复杂组合多任务（如翻译摘要）时的效率和集成问题，并在设备端实现了高性能和快速处理。", "motivation": "尽管LoRA等参数高效微调技术能将大型语言模型（LLM）应用于多种下游任务，但标准方法在同时执行复杂组合任务（例如从长对话中生成翻译摘要）时表现不佳。", "method": "针对摘要和翻译等组合多任务场景，本文提出一种新方法。该方法在结合了摘要和翻译的LoRA适配器之上添加一个可学习的投影层。这种设计旨在实现有效集成，同时通过减少计算开销来保持效率。此外，通过开发一个Android应用来验证其在设备环境中的实际可行性。", "result": "实验结果表明，该解决方案在云端和设备端实现中均表现良好且速度快。这突出了该框架在要求高速操作和资源限制的实际应用中具有潜在优势。", "conclusion": "本文提出的方法通过在组合的LoRA适配器之上添加可学习投影层，有效解决了LLM在执行复杂组合多任务时的集成和效率挑战，并在资源受限的设备环境中展示了其高性能和实用性。"}}
{"id": "2510.14194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14194", "abs": "https://arxiv.org/abs/2510.14194", "authors": ["Göktuğ Bender", "Samer Faraj", "Anand Bhardwaj"], "title": "Implementation of AI in Precision Medicine", "comment": "Accepted to SMASH 2025", "summary": "Artificial intelligence (AI) has become increasingly central to precision\nmedicine by enabling the integration and interpretation of multimodal data, yet\nimplementation in clinical settings remains limited. This paper provides a\nscoping review of literature from 2019-2024 on the implementation of AI in\nprecision medicine, identifying key barriers and enablers across data quality,\nclinical reliability, workflow integration, and governance. Through an\necosystem-based framework, we highlight the interdependent relationships\nshaping real-world translation and propose future directions to support\ntrustworthy and sustainable implementation.", "AI": {"tldr": "本文对2019-2024年AI在精准医疗中实施的文献进行了范围界定审查，识别了数据质量、临床可靠性、工作流程整合和治理等方面的关键障碍和促成因素，并提出了未来的发展方向。", "motivation": "人工智能在整合和解释多模态数据方面对精准医疗至关重要，但其在临床环境中的实施仍然有限。", "method": "采用范围界定审查（scoping review）的方法，分析了2019-2024年间的文献，并使用基于生态系统的框架来识别关键障碍和促成因素。", "result": "识别出数据质量、临床可靠性、工作流程整合和治理等方面的关键障碍和促成因素，并强调了影响实际转化中相互依赖的关系。", "conclusion": "提出了支持可信赖和可持续实施的未来发展方向，以推动人工智能在精准医疗领域的实际应用。"}}
{"id": "2510.14251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14251", "abs": "https://arxiv.org/abs/2510.14251", "authors": ["Mingkai Liu", "Dikai Fan", "Haohua Que", "Haojia Gao", "Xiao Liu", "Shuxue Peng", "Meixia Lin", "Shengyu Gu", "Ruicong Ye", "Wanli Qiu", "Handong Yao", "Ruopeng Zhang", "Xianliang Huang"], "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering", "comment": "8 pages", "summary": "Efficient localization and high-quality rendering in large-scale scenes\nremain a significant challenge due to the computational cost involved. While\nScene Coordinate Regression (SCR) methods perform well in small-scale\nlocalization, they are limited by the capacity of a single network when\nextended to large-scale scenes. To address these challenges, we propose the\nMixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables\nefficient localization and high-quality rendering in large-scale scenes.\nInspired by the remarkable capabilities of MOE in large model domains, we\nintroduce a gating network to implicitly classify and select sub-networks,\nensuring that only a single sub-network is activated during each inference.\nFurtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to\nenhance the localization accuracy on large-scale scene. Our framework provides\na significant reduction in costs while maintaining higher precision, offering\nan efficient solution for large-scale scene applications. Additional\nexperiments on the Cambridge test set demonstrate that our method achieves\nhigh-quality rendering results with merely 10 minutes of training.", "AI": {"tldr": "本文提出了一种名为MACE的混合专家加速坐标编码方法，通过引入门控网络和无辅助损失负载均衡策略，解决了大规模场景中高效定位和高质量渲染的挑战，显著降低了计算成本并提高了精度。", "motivation": "在大规模场景中，高效定位和高质量渲染由于计算成本高昂而面临巨大挑战。现有的场景坐标回归（SCR）方法在小规模场景中表现良好，但当扩展到大规模场景时，受限于单个网络的容量，性能受到限制。", "method": "本文提出了一种名为混合专家加速坐标编码（MACE）的方法。该方法受MOE（混合专家）模型的启发，引入了一个门控网络来隐式分类和选择子网络，确保每次推理时仅激活一个子网络。此外，还提出了一种无辅助损失负载均衡（ALF-LB）策略，以提高大规模场景的定位精度。", "result": "MACE框架在保持更高精度的同时，显著降低了成本。在Cambridge测试集上的实验表明，该方法仅需10分钟的训练即可实现高质量的渲染结果。", "conclusion": "MACE为大规模场景应用提供了一个高效的解决方案，能够实现高效定位和高质量渲染，同时显著降低了计算成本并保持了高精度。"}}
{"id": "2510.14959", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14959", "abs": "https://arxiv.org/abs/2510.14959", "authors": ["Lizhi Yang", "Blake Werner", "Massimiliano de Sa Aaron D. Ames"], "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions", "comment": "8 pages", "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.", "AI": {"tldr": "CBF-RL框架在强化学习训练中嵌入控制障碍函数（CBF），使策略内化安全约束，无需在线安全过滤器即可实现安全部署，并提高探索效率和收敛速度。", "motivation": "强化学习（RL）虽强大但常以安全为代价，可能导致实际部署中的灾难性后果。控制障碍函数（CBF）能有效保障动态安全，但传统在线安全过滤器部署可能导致保守行为，因为RL策略不了解CBF。", "method": "本文提出CBF-RL框架，通过在训练中强制执行CBF来生成安全的RL行为：1) 最小化地修改名义RL策略，通过CBF项编码安全约束；2) 在训练中对策略的rollout进行安全过滤。理论上证明了连续时间安全过滤器可以通过离散时间rollout的闭式表达式实现部署。", "result": "CBF-RL使学习到的策略内化了安全约束，既能强制执行更安全的动作，又能偏向更安全的奖励，从而无需在线安全过滤器即可安全部署。在导航任务和Unitree G1人形机器人上的消融研究表明，CBF-RL实现了更安全的探索、更快的收敛和在不确定性下的鲁棒性能，使人形机器人在真实世界中无需运行时安全过滤器即可安全避障和爬楼梯。", "conclusion": "CBF-RL框架通过在训练阶段整合CBF，成功地将安全约束内化到强化学习策略中，从而生成本质上安全的行为，无需在线安全过滤器。这显著提高了探索安全性、收敛速度和在实际应用中的鲁棒性。"}}
{"id": "2510.13849", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13849", "abs": "https://arxiv.org/abs/2510.13849", "authors": ["Andrey Goncharov", "Nikolai Kondusov", "Alexey Zaytsev"], "title": "Language steering in latent space to mitigate unintended code-switching", "comment": null, "summary": "Multilingual Large Language Models (LLMs) often exhibit unintended\ncode-switching, reducing reliability in downstream tasks. We propose\nlatent-space language steering, a lightweight inference-time method that\nidentifies language directions via PCA on parallel translations and steers\ntoken embeddings along these axes to control language identity. Our approach\nmitigates code-switching while preserving semantics with negligible\ncomputational overhead and requires only minimal parallel data for calibration.\nEmpirically, we achieve 95-99\\% language classification accuracy using a single\nprincipal component and reduce next-token distributional divergence by up to\n42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further\nanalyze the layer-wise evolution of language representations, revealing that\nlanguage identity concentrates in final layers with near-perfect linear\nseparability.", "AI": {"tldr": "本文提出了一种轻量级推理时方法——潜在空间语言引导，通过PCA识别语言方向并调整token嵌入，以减轻多语言大型语言模型中的代码转换现象，同时保持语义。", "motivation": "多语言大型语言模型（LLMs）经常出现意外的代码转换，这降低了它们在下游任务中的可靠性。", "method": "该方法称为潜在空间语言引导，在推理时进行。它通过对并行翻译数据进行主成分分析（PCA）来识别语言方向，然后沿着这些轴调整token嵌入，以控制语言识别。该方法只需少量并行数据进行校准。", "result": "实验结果表明，使用单个主成分即可实现95-99%的语言分类准确率，并在Qwen2.5和Llama-3.2模型上，跨多个语言对将下一个token的分布散度降低了高达42%。此外，分析发现语言识别信息主要集中在模型的最终层，且具有近乎完美的线性可分性。", "conclusion": "该方法能够有效缓解代码转换，同时保持语义，计算开销可忽略不计。研究还揭示了语言识别在LLM的最终层中具有高度的集中性和可分离性。"}}
{"id": "2510.13850", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13850", "abs": "https://arxiv.org/abs/2510.13850", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the UID Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "Large language models (LLMs) often solve problems using step-by-step\nChain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently\nunfaithful or hard to interpret. Inspired by the Uniform Information Density\n(UID) hypothesis in psycholinguistics -- which posits that humans communicate\nby maintaining a stable flow of information -- we introduce entropy-based\nmetrics to analyze the information flow within reasoning traces. Surprisingly,\nacross three challenging mathematical benchmarks, we find that successful\nreasoning in LLMs is globally non-uniform: correct solutions are characterized\nby uneven swings in information density, in stark contrast to human\ncommunication patterns. This result challenges assumptions about machine\nreasoning and suggests new directions for designing interpretable and adaptive\nreasoning models.", "AI": {"tldr": "研究发现，大型语言模型（LLMs）的成功推理过程在信息密度上是全局非均匀的，与人类交流模式形成鲜明对比，这挑战了机器推理的现有假设。", "motivation": "大型语言模型（LLMs）的思维链（CoT）推理中间步骤常不忠实或难以解释。受心理语言学中“统一信息密度（UID）假说”（人类通过维持稳定的信息流进行交流）的启发，研究者希望通过分析LLMs推理轨迹中的信息流来理解其推理过程。", "method": "引入了基于熵的度量标准来分析推理轨迹中的信息流。", "result": "在三个具有挑战性的数学基准测试中，研究发现LLMs的成功推理是全局非均匀的：正确的解决方案以信息密度的不均匀波动为特征，这与人类交流模式形成鲜明对比。", "conclusion": "这一结果挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提出了新的方向。"}}
{"id": "2510.14584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14584", "abs": "https://arxiv.org/abs/2510.14584", "authors": ["Benno Wingender", "Nils Dengler", "Rohit Menon", "Sicong Pan", "Maren Bennewitz"], "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning", "comment": null, "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.", "AI": {"tldr": "本文提出了一种广义可放置性度量，无需形状先验，直接从带噪声点云评估放置姿态，实现对未知物体在真实世界噪声下的可靠抓取和放置。", "motivation": "现有方法在抓取和放置未知物体时，过度依赖强对象先验（如CAD模型）或平面支撑假设，这限制了其泛化能力以及抓取与放置之间统一推理的能力。", "method": "引入了一种广义可放置性度量，直接从带噪声点云评估放置姿态，无需任何形状先验。该度量联合评估稳定性、可抓取性和间隙。从原始几何数据中提取物体支撑面，生成多方向放置的候选方案，并采样满足碰撞和稳定性约束的接触点。通过将抓取分数与每个候选放置条件化，实现模型无关的统一抓取-放置推理。", "result": "在未曾见过的真实物体和非平面物体支撑上，该度量在预测稳定性损失方面达到了与CAD模型相当的准确性，并且通常比基于学习的预测器产生更符合物理规律的放置。", "conclusion": "所提出的方法通过统一的抓取-放置推理，能够在没有模型先验的情况下，从原始几何数据中选择导致稳定、无碰撞放置的抓取-放置对，显著提高了在复杂真实世界场景中处理未知物体的能力。"}}
{"id": "2510.14245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14245", "abs": "https://arxiv.org/abs/2510.14245", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication", "comment": null, "summary": "Optical camera communication (OCC) represents a promising visible light\ncommunication technology. Nonetheless, typical OCC systems utilizing\nframe-based cameras are encumbered by limitations, including low bit rate and\nhigh processing load. To address these issues, OCC system utilizing an\nevent-based vision sensor (EVS) as receivers have been proposed. The EVS\nenables high-speed, low-latency, and robust communication due to its\nasynchronous operation and high dynamic range. In existing event-based OCC\nsystems, conventional modulation schemes such as on-off keying (OOK) and pulse\nposition modulation have been applied, however, to the best of our knowledge,\nno modulation method has been proposed that fully exploits the unique\ncharacteristics of the EVS. This paper proposes a novel modulation scheme,\ncalled the event interval modulation (EIM) scheme, specifically designed for\nevent-based OCC. EIM enables improvement in transmission speed by modulating\ninformation using the intervals between events. This paper proposes a\ntheoretical model of EIM and conducts a proof-of-concept experiment. First, the\nparameters of the EVS are tuned and customized to optimize the frequency\nresponse specifically for EIM. Then, the maximum modulation order usable in EIM\nis determined experimentally. We conduct transmission experiments based on the\nobtained parameters. Finally, we report successful transmission at 28 kbps over\n10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new\nbenchmark for bit rate in event-based OCC systems.", "AI": {"tldr": "本文提出事件间隔调制（EIM）方案，专为基于事件的可见光通信（OCC）设计，以充分利用事件相机（EVS）的特性，并在传输速率上（10米28 kbps，50米8.4 kbps）设定了新基准。", "motivation": "传统的基于帧的可见光通信（OCC）系统存在低比特率和高处理负载的限制。虽然基于事件的视觉传感器（EVS）能实现高速、低延迟和鲁棒的通信，但现有调制方案（如OOK和PPM）未能充分利用EVS的独特特性。", "method": "提出了一种名为事件间隔调制（EIM）的新型调制方案，专为基于事件的OCC设计，通过事件之间的时间间隔来调制信息。建立了EIM的理论模型，并进行了概念验证实验。具体步骤包括：优化EVS参数以适应EIM的频率响应；实验确定EIM中可使用的最大调制阶数；基于所得参数进行传输实验。", "result": "在室内环境下，实现了10米距离上28 kbps的成功传输，以及50米距离上8.4 kbps的成功传输。这些结果为基于事件的OCC系统的比特率设定了新的基准。", "conclusion": "所提出的EIM方案有效提升了基于事件的OCC系统的传输速度，充分利用了EVS的独特特性，并在此领域建立了新的比特率基准。"}}
{"id": "2510.14255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14255", "abs": "https://arxiv.org/abs/2510.14255", "authors": ["Liao Shen", "Wentao Jiang", "Yiran Zhu", "Tiezheng Ge", "Zhiguo Cao", "Bo Zheng"], "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization", "comment": null, "summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable\nprogress in synthesizing high-quality, temporally coherent videos from static\nimages. Among all the applications of I2V, human-centric video generation\nincludes a large portion. However, existing I2V models encounter difficulties\nin maintaining identity consistency between the input human image and the\ngenerated video, especially when the person in the video exhibits significant\nexpression changes and movements. This issue becomes critical when the human\nface occupies merely a small fraction of the image. Since humans are highly\nsensitive to identity variations, this poses a critical yet under-explored\nchallenge in I2V generation. In this paper, we propose Identity-Preserving\nReward-guided Optimization (IPRO), a novel video diffusion framework based on\nreinforcement learning to enhance identity preservation. Instead of introducing\nauxiliary modules or altering model architectures, our approach introduces a\ndirect and effective tuning algorithm that optimizes diffusion models using a\nface identity scorer. To improve performance and accelerate convergence, our\nmethod backpropagates the reward signal through the last steps of the sampling\nchain, enabling richer gradient feedback. We also propose a novel facial\nscoring mechanism that treats faces in ground-truth videos as facial feature\npools, providing multi-angle facial information to enhance generalization. A\nKL-divergence regularization is further incorporated to stabilize training and\nprevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V\nmodel and our in-house I2V model demonstrate the effectiveness of our method.\nOur project and code are available at\n\\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.", "AI": {"tldr": "本文提出IPRO，一个基于强化学习的视频扩散框架，通过引入直接且有效的调优算法和人脸身份评分器，显著提升了人像视频生成中的身份一致性，尤其在表情和动作变化大的情况下。", "motivation": "现有图像到视频（I2V）模型在人像视频生成中难以保持身份一致性，尤其当人物表情和动作变化显著或人脸在图像中占比较小时。由于人类对身份变化高度敏感，这是一个关键但未被充分探索的挑战。", "method": "本文提出身份保持奖励引导优化（IPRO），一个基于强化学习的视频扩散框架。它通过人脸身份评分器优化扩散模型，不改变模型架构或引入辅助模块。为提高性能和加速收敛，IPRO通过采样链的最后几步反向传播奖励信号，并提出一种新颖的人脸评分机制，将真实视频中的人脸视为特征池以增强泛化性。此外，还引入KL散度正则化来稳定训练并防止过拟合。", "result": "在Wan 2.2 I2V模型和自研I2V模型上进行的广泛实验证明了该方法的有效性。", "conclusion": "IPRO通过强化学习和创新的奖励机制，在不修改模型架构的前提下，有效解决了人像视频生成中身份一致性难以保持的问题，尤其在复杂场景下表现出色。"}}
{"id": "2510.14968", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14968", "abs": "https://arxiv.org/abs/2510.14968", "authors": ["Mingxuan Yan", "Yuping Wang", "Zechun Liu", "Jiachen Li"], "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025); Project Website: rdd-neurips.github.io", "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.", "AI": {"tldr": "为解决分层视觉-语言-动作（VLA）框架中长时任务的子任务分解问题，本文提出了一种基于检索的演示分解器（RDD）。RDD通过将分解的子任务区间视觉特征与低层视觉运动策略的训练数据对齐，自动分解演示，从而避免了手动标注或启发式规则带来的性能下降，并在模拟和真实世界任务中均优于现有方法。", "motivation": "现有的分层VLA框架在处理长时任务时，依赖于对VLM规划器进行微调以分解任务。这需要预先将演示分割成子任务，通常通过人工标注或启发式规则完成。然而，启发式分割的子任务可能与低层视觉运动策略的训练数据存在显著偏差，从而降低任务性能。", "method": "本文提出了一种基于检索的演示分解器（RDD）。该方法通过将分解的子任务区间的视觉特征与低层视觉运动策略训练数据中的视觉特征进行对齐，从而自动地将演示分解为子任务。", "result": "RDD在模拟和真实世界任务中均优于最先进的子任务分解器，并在多样化设置中展现了鲁棒性。", "conclusion": "RDD通过自动对齐视觉特征与低层策略训练数据，有效解决了分层VLA框架中子任务分解的挑战，显著提高了任务性能和鲁棒性。"}}
{"id": "2510.14612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14612", "abs": "https://arxiv.org/abs/2510.14612", "authors": ["Gabriel Fischer Abati", "João Carlos Virgolino Soares", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning", "comment": null, "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size.", "AI": {"tldr": "该论文提出了一种将四足机器人本体感知时间序列数据转换为结构化二维图像的新方法，并利用卷积神经网络进行运动相关任务学习，显著提高了接触估计的准确性和泛化能力。", "motivation": "现有方法可能未能充分捕捉四足机器人本体感知时间序列数据中的时间动态、信号间相关性以及机器人形态结构，限制了卷积神经网络在机器人运动学习任务中的应用。需要一种更丰富的特征空间来提升学习效果。", "method": "将来自四足机器人的本体感知时间序列数据（如关节位置、IMU读数和足部速度）编码为结构化的二维图像。这种编码方法保留了时间动态，并通过图像的空间排列保留了机器人的形态结构，从而捕捉信号间关联和步态相关模式。然后，使用卷积神经网络（CNNs）处理这些图像，应用于接触估计等运动相关任务。", "result": "实验结果表明，与传统的基于序列的模型相比，所提出的基于图像的表示方法在预测准确性和泛化能力方面均有所提升。在接触估计任务中，准确率从MI-HGNN方法的87.7%提高到94.5%，且使用了15倍更短的窗口大小。", "conclusion": "将本体感知时间序列数据进行跨模态编码（转换为图像）的策略在机器人状态学习中具有巨大潜力，能够显著提高四足机器人运动相关任务（如接触估计）的预测准确性和泛化能力。"}}
{"id": "2510.13851", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13851", "abs": "https://arxiv.org/abs/2510.13851", "authors": ["Sicheng Lyu", "Yu Gu", "Xinyu Wang", "Jerry Huang", "Sitao Luan", "Yufei Cui", "Xiao-Wen Chang", "Peng Lu"], "title": "EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing", "comment": null, "summary": "Large language models (LLMs) require continual updates to rectify outdated or\nerroneous knowledge. Model editing has emerged as a compelling paradigm for\nintroducing targeted modifications without the computational burden of full\nretraining. Existing approaches are mainly based on a locate-then-edit\nframework. However, in sequential editing contexts, where multiple updates are\napplied over time, they exhibit significant limitations and suffer from\ncatastrophic interference, i.e., new edits compromise previously integrated\nupdates and degrade preserved knowledge. To address these challenges, we\nintroduce EvoEdit, a novel editing strategy that mitigates catastrophic\ninterference through sequential null-space alignment, enabling stable and\nefficient model editing. By performing sequential null-space alignment for each\nincoming edit, EvoEdit preserves both original and previously modified\nknowledge representations and maintains output invariance on preserved\nknowledge even across long edit sequences, effectively mitigating interference.\nEvaluations on real-world sequential knowledge-editing benchmarks show that\nEvoEdit achieves better or comparable performance than prior state-of-the-art\nlocate-then-edit techniques, with up to 3.53 times speedup. Overall, these\nresults underscore the necessity of developing more principled approaches for\ndesigning LLMs in dynamically evolving information settings, while providing a\nsimple yet effective solution with strong theoretical guarantees.", "AI": {"tldr": "本文提出EvoEdit，一种通过顺序零空间对齐来缓解灾难性干扰的新型LLM模型编辑策略，在顺序编辑场景中能稳定高效地更新模型知识。", "motivation": "大型语言模型（LLMs）需要持续更新以纠正过时或错误知识。现有模型编辑方法在顺序编辑场景中存在严重局限性，会遭受灾难性干扰，即新编辑会损害先前整合的更新和保留的知识。", "method": "本文引入EvoEdit，一种通过顺序零空间对齐（sequential null-space alignment）来减轻灾难性干扰的编辑策略。对于每个传入的编辑，EvoEdit执行顺序零空间对齐，以保留原始和先前修改的知识表示，并在长编辑序列中保持保留知识的输出不变性。", "result": "在真实世界的顺序知识编辑基准测试中，EvoEdit表现出优于或与现有最先进的“定位-然后-编辑”技术相当的性能，并实现了高达3.53倍的速度提升。它有效缓解了干扰，并保持了知识表示的完整性。", "conclusion": "这些结果强调了在动态信息环境中设计LLMs时开发更具原则性方法的重要性，同时EvoEdit提供了一个简单、有效且具有强大理论保证的解决方案。"}}
{"id": "2510.14207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14207", "abs": "https://arxiv.org/abs/2510.14207", "authors": ["Trilok Padhi", "Pinxian Lu", "Abdulkadir Erol", "Tanmay Sutar", "Gauri Sharma", "Mina Sonmez", "Munmun De Choudhury", "Ugur Kursuncu"], "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks", "comment": "13 pages, 4 figures", "summary": "Large Language Model (LLM) agents are powering a growing share of interactive\nweb applications, yet remain vulnerable to misuse and harm. Prior jailbreak\nresearch has largely focused on single-turn prompts, whereas real harassment\noften unfolds over multi-turn interactions. In this work, we present the Online\nHarassment Agentic Benchmark consisting of: (i) a synthetic multi-turn\nharassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)\nsimulation informed by repeated game theory, (iii) three jailbreak methods\nattacking agents across memory, planning, and fine-tuning, and (iv) a\nmixed-methods evaluation framework. We utilize two prominent LLMs,\nLLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our\nresults show that jailbreak tuning makes harassment nearly guaranteed with an\nattack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,\nand 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal\nrate to 1-2% in both models. The most prevalent toxic behaviors are Insult with\n84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.\n31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive\ncategories such as sexual or racial harassment. Qualitative evaluation further\nreveals that attacked agents reproduce human-like aggression profiles, such as\nMachiavellian/psychopathic patterns under planning, and narcissistic tendencies\nwith memory. Counterintuitively, closed-source and open-source models exhibit\ndistinct escalation trajectories across turns, with closed-source models\nshowing significant vulnerability. Overall, our findings show that multi-turn\nand theory-grounded attacks not only succeed at high rates but also mimic\nhuman-like harassment dynamics, motivating the development of robust safety\nguardrails to ultimately keep online platforms safe and responsible.", "AI": {"tldr": "本研究发现，针对大型语言模型（LLM）代理的多轮在线骚扰攻击，特别是经过越狱微调后，成功率极高（95-99%），并能模拟人类的攻击模式，凸显了当前安全防护的不足。", "motivation": "LLM代理在交互式网络应用中日益普及，但易受滥用和伤害。以往的越狱研究主要集中在单轮提示，而现实中的骚扰往往是多轮互动的。因此，需要研究多轮互动中LLM代理的脆弱性。", "method": "本研究提出了“在线骚扰代理基准”，包括：(i) 合成的多轮骚扰对话数据集；(ii) 基于重复博弈论的多代理（骚扰者、受害者）模拟；(iii) 攻击代理记忆、规划和微调的三种越狱方法；(iv) 混合方法评估框架。研究使用了LLaMA-3.1-8B-Instruct（开源）和Gemini-2.0-flash（闭源）两种LLM进行测试。", "result": "越狱微调使骚扰成功率大幅提高，Llama模型从57.25-64.19%升至95.78-96.89%，Gemini模型从98.46%升至99.33%，同时拒绝率降至1-2%。最普遍的有毒行为是侮辱（84.9-87.8% vs 44.2-50.8%）和煽动（81.2-85.1% vs 31.5-38.8%），表明对敏感类别的防护较弱。定性评估显示，受攻击代理能重现人类攻击性特征，如马基雅维利/精神病态模式和自恋倾向。出乎意料的是，闭源模型和开源模型在多轮互动中表现出不同的升级轨迹，闭源模型显示出显著的脆弱性。", "conclusion": "多轮且基于理论的攻击不仅成功率高，而且能模仿人类的骚扰动态。这促使我们必须开发更强大的安全防护措施，以确保在线平台的安全和负责任。"}}
{"id": "2510.14615", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decré"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.", "AI": {"tldr": "本文提出CAMPD，一种上下文感知的扩散模型，用于解决机器人运动规划中传统方法和现有扩散模型在泛化性、高维空间和复杂环境下的局限性，实现了对未知环境的有效泛化，且无需特定传感器。", "motivation": "经典的机器人运动规划方法（如采样和优化）难以扩展到高维状态空间和复杂环境。现有扩散模型虽然在运动规划中表现出潜力，但多数模型仅针对单一环境训练，泛化性差；少数能处理多环境的模型则依赖特定传感器提供环境信息，限制了其适用性。", "method": "本文提出了上下文感知运动规划扩散（CAMPD）模型。它利用无分类器去噪概率扩散模型，并以与传感器无关的上下文信息作为条件。该模型在著名的U-Net架构中集成了注意力机制，使其能够根据任意数量的上下文参数进行条件化。", "result": "CAMPD在7自由度机器人机械手上进行了评估，并与现有最先进的方法在实际任务中进行基准测试。结果表明，CAMPD能够泛化到未见过的环境，生成高质量、多模态的轨迹，且所需时间远少于现有方法。", "conclusion": "CAMPD通过利用与传感器无关的上下文信息和注意力机制，有效解决了机器人运动规划中泛化性差的问题，使其能够适应多样化的场景而无需重新训练，并在性能上超越了现有方法。"}}
{"id": "2510.14240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14240", "abs": "https://arxiv.org/abs/2510.14240", "authors": ["Jiayu Wang", "Yifei Ming", "Riya Dulepet", "Qinglin Chen", "Austin Xu", "Zixuan Ke", "Frederic Sala", "Aws Albarghouthi", "Caiming Xiong", "Shafiq Joty"], "title": "LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild", "comment": null, "summary": "Deep research -- producing comprehensive, citation-grounded reports by\nsearching and synthesizing information from hundreds of live web sources --\nmarks an important frontier for agentic systems. To rigorously evaluate this\nability, four principles are essential: tasks should be (1) user-centric,\nreflecting realistic information needs, (2) dynamic, requiring up-to-date\ninformation beyond parametric knowledge, (3) unambiguous, ensuring consistent\ninterpretation across users, and (4) multi-faceted and search-intensive,\nrequiring search over numerous web sources and in-depth analysis. Existing\nbenchmarks fall short of these principles, often focusing on narrow domains or\nposing ambiguous questions that hinder fair comparison. Guided by these\nprinciples, we introduce LiveResearchBench, a benchmark of 100 expert-curated\ntasks spanning daily life, enterprise, and academia, each requiring extensive,\ndynamic, real-time web search and synthesis. Built with over 1,500 hours of\nhuman labor, LiveResearchBench provides a rigorous basis for systematic\nevaluation. To evaluate citation-grounded long-form reports, we introduce\nDeepEval, a comprehensive suite covering both content- and report-level\nquality, including coverage, presentation, citation accuracy and association,\nconsistency and depth of analysis. DeepEval integrates four complementary\nevaluation protocols, each designed to ensure stable assessment and high\nagreement with human judgments. Using LiveResearchBench and DeepEval, we\nconduct a comprehensive evaluation of 17 frontier deep research systems,\nincluding single-agent web search, single-agent deep research, and multi-agent\nsystems. Our analysis reveals current strengths, recurring failure modes, and\nkey system components needed to advance reliable, insightful deep research.", "AI": {"tldr": "本文提出LiveResearchBench基准和DeepEval评估套件，用于严格评估能够从实时网络源进行搜索和综合以生成引用报告的深度研究智能体系统，并对17个前沿系统进行了全面评估，揭示了其优缺点。", "motivation": "现有的智能体系统基准在评估深度研究能力方面存在不足，未能满足用户中心、动态、明确和多方面搜索密集型等关键原则，导致无法对生成全面、基于引用的报告的能力进行严格评估。", "method": "作者定义了评估深度研究的四个核心原则。基于这些原则，他们推出了LiveResearchBench，一个包含100个专家策划任务的基准，要求进行广泛、动态的实时网络搜索和综合。同时，他们引入了DeepEval，一个全面的评估套件，用于评估引用报告的内容和报告级别质量，包括覆盖范围、呈现、引用准确性、关联性、一致性和分析深度，并集成了四种互补的评估协议。最后，利用LiveResearchBench和DeepEval，对17个前沿深度研究系统进行了全面评估。", "result": "通过对17个前沿深度研究系统的评估，分析揭示了当前系统的优势、常见的失败模式，以及推进可靠、有洞察力的深度研究所需关键系统组件。", "conclusion": "LiveResearchBench和DeepEval为系统评估深度研究智能体系统提供了严格的基础。通过对现有系统的全面评估，研究为未来开发更可靠、更具洞察力的深度研究系统指明了方向。"}}
{"id": "2510.14256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14256", "abs": "https://arxiv.org/abs/2510.14256", "authors": ["Xiangyu Meng", "Zixian Zhang", "Zhenghao Zhang", "Junchao Liao", "Long Qin", "Weizhi Wang"], "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning", "comment": null, "summary": "While advanced methods like VACE and Phantom have advanced video generation\nfor specific subjects in diverse scenarios, they struggle with multi-human\nidentity preservation in dynamic interactions, where consistent identities\nacross multiple characters are critical. To address this, we propose\nIdentity-GRPO, a human feedback-driven optimization pipeline for refining\nmulti-human identity-preserving video generation. First, we construct a video\nreward model trained on a large-scale preference dataset containing\nhuman-annotated and synthetic distortion data, with pairwise annotations\nfocused on maintaining human consistency throughout the video. We then employ a\nGRPO variant tailored for multi-human consistency, which greatly enhances both\nVACE and Phantom. Through extensive ablation studies, we evaluate the impact of\nannotation quality and design choices on policy optimization. Experiments show\nthat Identity-GRPO achieves up to 18.9% improvement in human consistency\nmetrics over baseline methods, offering actionable insights for aligning\nreinforcement learning with personalized video generation.", "AI": {"tldr": "本文提出Identity-GRPO，一个由人类反馈驱动的优化流程，旨在解决现有视频生成方法在动态交互中难以保持多人物身份一致性的问题，并通过定制的GRPO变体显著提升了VACE和Phantom的性能。", "motivation": "现有先进的视频生成方法（如VACE和Phantom）在多样化场景下能生成特定主题视频，但在动态交互中难以保持多个人物身份的一致性，而这种一致性对于多角色场景至关重要。", "method": ["构建了一个视频奖励模型，该模型在一个大规模偏好数据集上训练，数据集包含人类标注和合成扭曲数据，并着重于视频中人物一致性的成对标注。", "采用了一种针对多人物一致性定制的GRPO（广义策略优化）变体。", "利用该GRPO变体显著增强了VACE和Phantom的性能。", "通过广泛的消融研究评估了标注质量和设计选择对策略优化的影响。"], "result": "Identity-GRPO在人物一致性指标上比基线方法提高了高达18.9%，并提供了将强化学习与个性化视频生成对齐的实用见解。", "conclusion": "Identity-GRPO通过人类反馈驱动的优化流程，有效解决了多人物身份保持视频生成中的挑战，显著提升了现有方法的性能，并为强化学习在个性化视频生成领域的应用提供了宝贵经验。"}}
{"id": "2510.14266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14266", "abs": "https://arxiv.org/abs/2510.14266", "authors": ["Miu Sumino", "Mayu Ishii", "Shun Kaizu", "Daisuke Hisano", "Yu Nakayama"], "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment", "comment": null, "summary": "We propose a robust demodulation scheme for optical camera communication\nsystems using an event-based vision sensor, combining OOK with toggle\ndemodulation and a digital phase-locked loop. This is the first report to\nachieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor\nexperiments.", "AI": {"tldr": "该论文提出了一种结合OOK、翻转解调和数字锁相环的鲁棒解调方案，用于基于事件视觉传感器的光摄像头通信系统，首次在户外实验中实现了200米-60kbps和400米-30kbps下BER < 10^-3的性能。", "motivation": "动机是提高光摄像头通信（OCC）系统在户外长距离条件下的解调鲁棒性和性能，解决现有系统可能面临的挑战。", "method": "该研究采用了一种结合OOK（开关键控）、翻转解调（toggle demodulation）和数字锁相环（digital phase-locked loop）的解调方案，并利用事件型视觉传感器进行光信号接收。", "result": "主要成果是在户外实验中，首次在200米距离下实现了60kbps数据速率且误码率（BER）低于10^-3，以及在400米距离下实现了30kbps数据速率且误码率低于10^-3。", "conclusion": "该研究提出了一种鲁棒的光摄像头通信解调方案，通过结合特定技术，显著提升了系统在复杂户外环境和长距离下的通信性能和可靠性。"}}
{"id": "2510.13852", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13852", "abs": "https://arxiv.org/abs/2510.13852", "authors": ["Peter Banyas", "Shristi Sharma", "Alistair Simmons", "Atharva Vispute"], "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups", "comment": "For associated code repository, see\n  http://github.com/banyasp/consistencyAI For user-friendly web app, see\n  http://v0-llm-comparison-webapp.vercel.app/", "summary": "Is an LLM telling you different facts than it's telling me? This paper\nintroduces ConsistencyAI, an independent benchmark for measuring the factual\nconsistency of large language models (LLMs) for different personas.\nConsistencyAI tests whether, when users of different demographics ask identical\nquestions, the model responds with factually inconsistent answers. Designed\nwithout involvement from LLM providers, this benchmark offers impartial\nevaluation and accountability. In our experiment, we queried 19 LLMs with\nprompts that requested 5 facts for each of 15 topics. We repeated this query\n100 times for each LLM, each time adding prompt context from a different\npersona selected from a subset of personas modeling the general population. We\nprocessed the responses into sentence embeddings, computed cross-persona cosine\nsimilarity, and computed the weighted average of cross-persona cosine\nsimilarity to calculate factual consistency scores. In 100-persona experiments,\nscores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as\na benchmark threshold. xAI's Grok-3 is most consistent, while several\nlightweight models rank lowest. Consistency varies by topic: the job market is\nleast consistent, G7 world leaders most consistent, and issues like vaccines or\nthe Israeli-Palestinian conflict diverge by provider. These results show that\nboth the provider and the topic shape the factual consistency. We release our\ncode and interactive demo to support reproducible evaluation and encourage\npersona-invariant prompting strategies.", "AI": {"tldr": "本文引入了ConsistencyAI，一个独立基准，用于衡量大型语言模型（LLMs）在不同用户画像下事实一致性。研究发现，LLM的事实一致性因模型提供者和主题而异。", "motivation": "研究动机是探究LLM是否会向不同人口统计学特征的用户提供事实不一致的答案，即使他们提出相同的问题。这旨在提供对LLM事实一致性的公正评估和问责机制，且不受LLM提供商的参与影响。", "method": "引入了ConsistencyAI基准。对19个LLM进行了实验，每个模型被要求针对15个主题提供5个事实。每个查询重复100次，每次添加来自不同人口画像的提示上下文。将响应处理成句子嵌入，计算跨画像的余弦相似度，并计算加权平均值以得出事实一致性分数。", "result": "在100个画像的实验中，一致性分数范围从0.9065到0.7896，平均值为0.8656，被采纳为基准阈值。xAI的Grok-3最一致，而一些轻量级模型排名最低。一致性因主题而异：就业市场最不一致，G7世界领导人最一致，而疫苗或巴以冲突等问题则因提供商而异。结果表明，提供商和主题都影响事实一致性。", "conclusion": "LLM在不同用户画像下的事实一致性存在差异，且这种差异受模型提供商和主题的影响。ConsistencyAI提供了一个衡量这一指标的独立基准。研究鼓励采用可复现的评估方法和与用户画像无关的提示策略。"}}
{"id": "2510.13853", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13853", "abs": "https://arxiv.org/abs/2510.13853", "authors": ["Fabian Wenz", "Omar Bouattour", "Devin Yang", "Justin Choi", "Cecil Gregg", "Nesime Tatbul", "Çağatay Demiralp"], "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation", "comment": "CIDR'26", "summary": "Large language models (LLMs) have been successfully applied to many tasks,\nincluding text-to-SQL generation. However, much of this work has focused on\npublicly available datasets, such as Fiben, Spider, and Bird. Our earlier work\nshowed that LLMs are much less effective in querying large private enterprise\ndata warehouses and released Beaver, the first private enterprise text-to-SQL\nbenchmark. To create Beaver, we leveraged SQL logs, which are often readily\navailable. However, manually annotating these logs to identify which natural\nlanguage questions they answer is a daunting task. Asking database\nadministrators, who are highly trained experts, to take on additional work to\nconstruct and validate corresponding natural language utterances is not only\nchallenging but also quite costly. To address this challenge, we introduce\nBenchPress, a human-in-the-loop system designed to accelerate the creation of\ndomain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses\nretrieval-augmented generation (RAG) and LLMs to propose multiple natural\nlanguage descriptions. Human experts then select, rank, or edit these drafts to\nensure accuracy and domain alignment. We evaluated BenchPress on annotated\nenterprise SQL logs, demonstrating that LLM-assisted annotation drastically\nreduces the time and effort required to create high-quality benchmarks. Our\nresults show that combining human verification with LLM-generated suggestions\nenhances annotation accuracy, benchmark reliability, and model evaluation\nrobustness. By streamlining the creation of custom benchmarks, BenchPress\noffers researchers and practitioners a mechanism for assessing text-to-SQL\nmodels on a given domain-specific workload. BenchPress is freely available via\nour public GitHub repository at\nhttps://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our\nwebsite at http://dsg-mcgraw.csail.mit.edu:5000.", "AI": {"tldr": "BenchPress是一个人机协作系统，利用RAG和大型语言模型（LLMs）加速为私有企业数据仓库创建领域特定Text-to-SQL基准，显著降低了手动标注的时间和精力。", "motivation": "LLMs在公共Text-to-SQL数据集上表现良好，但在查询大型私有企业数据仓库时效果不佳。创建针对私有数据的基准（如Beaver）需要将SQL日志手动标注为自然语言问题，这项任务对训练有素的数据库管理员来说既耗时又昂贵。", "method": "引入了BenchPress系统。给定一个SQL查询，BenchPress使用检索增强生成（RAG）和LLMs生成多个自然语言描述草稿。人类专家随后选择、排序或编辑这些草稿，以确保其准确性和领域一致性。", "result": "BenchPress在标注企业SQL日志方面表现出色，证明LLM辅助标注能大幅减少创建高质量基准所需的时间和精力。结合人工验证和LLM生成的建议，提高了标注准确性、基准可靠性和模型评估的鲁棒性。", "conclusion": "BenchPress通过简化自定义基准的创建，为研究人员和实践者提供了一种评估Text-to-SQL模型在特定领域工作负载上表现的有效机制。该系统已公开发布，可供免费使用。"}}
{"id": "2510.14627", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.", "AI": {"tldr": "GOPLA是一个分层框架，通过结合多模态大语言模型和扩散规划器，从增强的人类演示中学习可泛化的机器人物体放置，显著提高了放置成功率和泛化能力。", "motivation": "机器人作为智能助手协助人类进行日常家居整理时，物体放置是一个核心挑战，它需要同时考虑语义偏好（如常识性物体关系）和几何可行性（如避免碰撞）。", "method": "本文提出了GOPLA，一个分层框架：1. 多模态大语言模型将人类指令和视觉输入转化为结构化的计划，指定成对的物体关系。2. 空间映射器将这些计划转换为具有几何常识的3D可供性图。3. 基于扩散的规划器生成放置姿态，该过程由测试时成本、多计划分布和碰撞避免指导。为克服数据稀缺，引入了一个可扩展的管道，将人类放置演示扩展为多样化的合成训练数据。", "result": "GOPLA在定位精度和物理合理性方面进行评估，将放置成功率比次优方法提高了30.04个百分点，并在广泛的真实世界机器人放置场景中展示了强大的泛化能力。", "conclusion": "GOPLA通过有效整合语义和几何推理，解决了机器人物体放置的挑战，在放置成功率和泛化能力方面取得了显著提升，证明了其在实际应用中的潜力。"}}
{"id": "2510.14260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14260", "abs": "https://arxiv.org/abs/2510.14260", "authors": ["Tingman Yan", "Tao Liu", "Xilian Yang", "Qunfei Zhao", "Zeyang Xia"], "title": "MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching", "comment": null, "summary": "Cross-view matching is fundamentally achieved through cross-attention\nmechanisms. However, matching of high-resolution images remains challenging due\nto the quadratic complexity and lack of explicit matching constraints in the\nexisting cross-attention. This paper proposes an attention mechanism,\nMatchAttention, that dynamically matches relative positions. The relative\nposition determines the attention sampling center of the key-value pairs given\na query. Continuous and differentiable sliding-window attention sampling is\nachieved by the proposed BilinearSoftmax. The relative positions are\niteratively updated through residual connections across layers by embedding\nthem into the feature channels. Since the relative position is exactly the\nlearning target for cross-view matching, an efficient hierarchical cross-view\ndecoder, MatchDecoder, is designed with MatchAttention as its core component.\nTo handle cross-view occlusions, gated cross-MatchAttention and a\nconsistency-constrained loss are proposed. These two components collectively\nmitigate the impact of occlusions in both forward and backward passes, allowing\nthe model to focus more on learning matching relationships. When applied to\nstereo matching, MatchStereo-B ranked 1st in average error on the public\nMiddlebury benchmark and requires only 29ms for KITTI-resolution inference.\nMatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU\nmemory. The proposed models also achieve state-of-the-art performance on KITTI\n2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high\naccuracy and low computational complexity makes real-time, high-resolution, and\nhigh-accuracy cross-view matching possible. Code is available at\nhttps://github.com/TingmanYan/MatchAttention.", "AI": {"tldr": "本文提出了一种名为MatchAttention的新型注意力机制，通过动态匹配相对位置和创新的BilinearSoftmax实现连续滑动窗口采样，解决了高分辨率图像交叉视图匹配中现有交叉注意力存在的二次复杂度及缺乏显式匹配约束的问题。结合MatchDecoder、门控交叉MatchAttention和一致性约束损失，有效处理遮挡，在多个立体匹配和光流数据集上取得了最先进的性能，同时实现了高效率和低内存消耗，使得实时、高分辨率、高精度的交叉视图匹配成为可能。", "motivation": "现有的交叉注意力机制在处理高分辨率图像时面临挑战，主要原因在于其二次复杂度和缺乏明确的匹配约束。此外，交叉视图匹配中的遮挡问题也未能得到有效解决。", "method": "该研究提出了MatchAttention注意力机制，通过动态匹配相对位置来确定键值对的注意力采样中心。引入BilinearSoftmax实现连续可微分的滑动窗口注意力采样。相对位置通过残差连接跨层迭代更新，并嵌入到特征通道中。设计了以MatchAttention为核心的MatchDecoder，用于高效的分层交叉视图匹配。为处理交叉视图遮挡，提出了门控交叉MatchAttention和一致性约束损失。", "result": "在Middlebury基准测试中，MatchStereo-B在平均误差上排名第一。MatchStereo-B在KITTI分辨率推理中仅需29毫秒。MatchStereo-T能够以0.1秒处理4K UHD图像，仅使用3GB GPU内存。所提出的模型在KITTI 2012、KITTI 2015、ETH3D和Spring flow数据集上也取得了最先进的性能。", "conclusion": "所提出的MatchAttention及其相关组件（MatchDecoder、门控交叉MatchAttention和一致性约束损失）的结合，成功解决了高分辨率交叉视图匹配中的核心挑战，实现了高精度和低计算复杂度的完美结合，使得实时、高分辨率、高精度的交叉视图匹配成为现实。"}}
{"id": "2510.14253", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14253", "abs": "https://arxiv.org/abs/2510.14253", "authors": ["Wangtao Sun", "Xiang Cheng", "Jialin Fan", "Yao Xu", "Xing Yu", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Towards Agentic Self-Learning LLMs in Search Environment", "comment": null, "summary": "We study whether self-learning can scale LLM-based agents without relying on\nhuman-curated datasets or predefined rule-based rewards. Through controlled\nexperiments in a search-agent setting, we identify two key determinants of\nscalable agent training: the source of reward signals and the scale of agent\ntask data. We find that rewards from a Generative Reward Model (GRM) outperform\nrigid rule-based signals for open-domain learning, and that co-evolving the GRM\nwith the policy further boosts performance. Increasing the volume of agent task\ndata-even when synthetically generated-substantially enhances agentic\ncapabilities. Building on these insights, we propose \\textbf{Agentic\nSelf-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning\nframework that unifies task generation, policy execution, and evaluation within\na shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,\na Policy Model, and a Generative Reward Model to form a virtuous cycle of\nharder task setting, sharper verification, and stronger solving. Empirically,\nASL delivers steady, round-over-round gains, surpasses strong RLVR baselines\n(e.g., Search-R1) that plateau or degrade, and continues improving under\nzero-labeled-data conditions, indicating superior sample efficiency and\nrobustness. We further show that GRM verification capacity is the main\nbottleneck: if frozen, it induces reward hacking and stalls progress; continual\nGRM training on the evolving data distribution mitigates this, and a small\nlate-stage injection of real verification data raises the performance ceiling.\nThis work establishes reward source and data scale as critical levers for\nopen-domain agent learning and demonstrates the efficacy of multi-role\nco-evolution for scalable, self-improving agents. The data and code of this\npaper are released at\nhttps://github.com/forangel2014/Towards-Agentic-Self-Learning", "AI": {"tldr": "本文提出了一种名为“代理自学习”（ASL）的闭环、多角色强化学习框架，通过生成式奖励模型和合成任务数据，使基于大型语言模型的代理能够在没有人工标注数据或预定义规则奖励的情况下，实现可扩展的自我提升。", "motivation": "研究旨在探索如何扩展基于大型语言模型的代理，使其无需依赖人工策划的数据集或预定义的基于规则的奖励，从而实现更自主、更通用的学习能力。", "method": "研究通过搜索代理设置中的受控实验，确定了可扩展代理训练的两个关键因素：奖励信号的来源和代理任务数据的规模。在此基础上，提出并构建了“代理自学习”（ASL）框架。ASL是一个完全闭环、多角色的强化学习框架，统一了任务生成、策略执行和评估，并在共享的工具环境和大型语言模型骨干下协调提示生成器、策略模型和生成式奖励模型（GRM），形成一个良性循环。此外，ASL还强调持续训练GRM以适应不断演变的数据分布。", "result": "研究发现，生成式奖励模型（GRM）在开放域学习中优于僵硬的基于规则的信号，并且GRM与策略的共同演化能进一步提升性能。增加代理任务数据的量（即使是合成生成的数据）也能显著增强代理能力。实验表明，ASL能持续获得逐轮提升，超越了表现停滞或下降的强RLVR基线（如Search-R1），并在零标注数据条件下持续改进，显示出卓越的样本效率和鲁棒性。研究还指出GRM的验证能力是主要瓶颈：如果冻结GRM，会导致奖励欺骗并阻碍进展；持续在演变数据分布上训练GRM可以缓解此问题，而后期少量真实验证数据的注入能提高性能上限。", "conclusion": "本研究确立了奖励来源和数据规模是开放域代理学习的关键杠杆，并证明了多角色协同演化对于可扩展、自我改进代理的有效性。"}}
{"id": "2510.14265", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14265", "abs": "https://arxiv.org/abs/2510.14265", "authors": ["Xukai Wang", "Xuanbo Liu", "Mingrui Chen", "Haitian Zhong", "Xuanlin Yang", "Bohan Zeng", "Jinbo Hu", "Hao Liang", "Junbo Niu", "Xuchen Li", "Ruitao Wu", "Ruichuan An", "Yang Shi", "Liu Liu", "Xu-Yao Zhang", "Qiang Liu", "Zhouchen Lin", "Wentao Zhang", "Bin Dong"], "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning", "comment": "21 pages, 12 figures", "summary": "With the advancement of powerful large-scale reasoning models, effectively\nevaluating the reasoning capabilities of these models has become increasingly\nimportant. However, existing benchmarks designed to assess the reasoning\nabilities of large models tend to be limited in scope and lack the flexibility\nto adapt their difficulty according to the evolving reasoning capacities of the\nmodels. To address this, we propose MorphoBench, a benchmark that incorporates\nmultidisciplinary questions to evaluate the reasoning capabilities of large\nmodels and can adjust and update question difficulty based on the reasoning\nabilities of advanced models. Specifically, we curate the benchmark by\nselecting and collecting complex reasoning questions from existing benchmarks\nand sources such as Olympiad-level competitions. Additionally, MorphoBench\nadaptively modifies the analytical challenge of questions by leveraging key\nstatements generated during the model's reasoning process. Furthermore, it\nincludes questions generated using simulation software, enabling dynamic\nadjustment of benchmark difficulty with minimal resource consumption. We have\ngathered over 1,300 test questions and iteratively adjusted the difficulty of\nMorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.\nMorphoBench enhances the comprehensiveness and validity of model reasoning\nevaluation, providing reliable guidance for improving both the reasoning\nabilities and scientific robustness of large models. The code has been released\nin https://github.com/OpenDCAI/MorphoBench.", "AI": {"tldr": "本文提出了MorphoBench，一个多学科、自适应难度的基准测试，用于评估大型模型的推理能力，旨在解决现有基准范围有限且缺乏灵活调整难度的问题。", "motivation": "现有用于评估大型模型推理能力的基准测试范围有限，并且缺乏根据模型不断演进的推理能力来调整难度的灵活性。", "method": "MorphoBench通过以下方式构建：1) 收集现有基准和奥林匹克竞赛等来源的复杂推理问题。2) 利用模型推理过程中生成的关键语句，自适应地修改问题的分析挑战。3) 包含使用模拟软件生成的问题，以最小资源消耗动态调整基准难度。该基准已收集1,300多个测试问题，并根据o3和GPT-5等模型的推理能力迭代调整了难度。", "result": "MorphoBench增强了模型推理评估的全面性和有效性。", "conclusion": "MorphoBench为提高大型模型的推理能力和科学鲁棒性提供了可靠的指导。"}}
{"id": "2510.14270", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.14270", "abs": "https://arxiv.org/abs/2510.14270", "authors": ["Alexander Valverde", "Brian Xu", "Yuyin Zhou", "Meng Xu", "Hongyun Wang"], "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering", "comment": null, "summary": "Scene reconstruction has emerged as a central challenge in computer vision,\nwith approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting\nachieving remarkable progress. While Gaussian Splatting demonstrates strong\nperformance on large-scale datasets, it often struggles to capture fine details\nor maintain realism in regions with sparse coverage, largely due to the\ninherent limitations of sparse 3D training data.\n  In this work, we propose GauSSmart, a hybrid method that effectively bridges\n2D foundational models and 3D Gaussian Splatting reconstruction. Our approach\nintegrates established 2D computer vision techniques, including convex\nfiltering and semantic feature supervision from foundational models such as\nDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D\nsegmentation priors and high-dimensional feature embeddings, our method guides\nthe densification and refinement of Gaussian splats, improving coverage in\nunderrepresented areas and preserving intricate structural details.\n  We validate our approach across three datasets, where GauSSmart consistently\noutperforms existing Gaussian Splatting in the majority of evaluated scenes.\nOur results demonstrate the significant potential of hybrid 2D-3D approaches,\nhighlighting how the thoughtful combination of 2D foundational models with 3D\nreconstruction pipelines can overcome the limitations inherent in either\napproach alone.", "AI": {"tldr": "GauSSmart是一种混合方法，结合2D基础模型和3D高斯泼溅技术，以解决高斯泼溅在细节和稀疏区域重建上的不足，通过2D先验和特征指导高斯点的密集化和细化。", "motivation": "高斯泼溅（Gaussian Splatting）在处理大规模数据集时表现出色，但在捕捉精细细节或在稀疏覆盖区域保持真实感方面存在困难，这主要是由于稀疏3D训练数据固有的局限性。", "method": "GauSSmart提出了一种混合方法，有效地结合了2D基础模型和3D高斯泼溅重建。它整合了2D计算机视觉技术，包括凸滤波（convex filtering）和来自DINO等基础模型的语义特征监督。通过利用2D分割先验和高维特征嵌入，该方法指导高斯点的密集化和细化，从而改善未充分表示区域的覆盖范围并保留复杂的结构细节。", "result": "GauSSmart在三个数据集上进行了验证，结果表明它在大多数评估场景中持续优于现有的高斯泼溅方法。", "conclusion": "混合2D-3D方法具有显著潜力，将2D基础模型与3D重建流程相结合，可以克服单一方法固有的局限性。"}}
{"id": "2510.13854", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13854", "abs": "https://arxiv.org/abs/2510.13854", "authors": ["Mamadou K. Keita", "Christopher Homan", "Sebastien Diarra"], "title": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging", "comment": null, "summary": "We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that\nintegrates a multi-tiered system of linguistic rules directly into a neural\nnetwork's training objective. R2T's novelty lies in its adaptive loss function,\nwhich includes a regularization term that teaches the model to handle\nout-of-vocabulary (OOV) words with principled uncertainty. We frame this work\nas a case study in a paradigm we call principled learning (PrL), where models\nare trained with explicit task constraints rather than on labeled examples\nalone. Our experiments on Zarma part-of-speech (POS) tagging show that the\nR2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,\noutperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We\nfurther show that for more complex tasks like named entity recognition (NER),\nR2T serves as a powerful pre-training step; a model pre-trained with R2T and\nfine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.", "AI": {"tldr": "R2T框架是一种混合方法，将语言规则融入神经网络训练目标，通过自适应损失函数处理OOV词汇的不确定性。该方法在扎尔马语POS标注上仅用无标注文本就达到98.2%的准确率，并能作为NER的有效预训练步骤，显著减少对标注数据的依赖。", "motivation": "传统模型在处理词汇表外（OOV）词汇时面临挑战，且通常需要大量标注数据。作者旨在开发一种通过明确任务约束（而非仅依赖标注示例）来训练模型的方法，即“原则性学习”。", "method": "引入Rule-to-Tag（R2T）框架，这是一种将多层语言规则系统直接整合到神经网络训练目标中的混合方法。其核心是包含一个正则化项的自适应损失函数，旨在教会模型以“有原则的不确定性”处理OOV词汇。此方法被视为“原则性学习”（PrL）范式的一个案例研究。", "result": "在扎尔马语词性标注（POS tagging）实验中，仅用无标注文本训练的R2T-BiLSTM模型达到了98.2%的准确率，优于使用300个标注句子微调的AfriBERTa等基线模型。对于命名实体识别（NER）等更复杂的任务，R2T可作为强大的预训练步骤；经R2T预训练后仅用50个标注句子微调的模型，其性能优于使用300个标注句子训练的基线模型。", "conclusion": "R2T框架通过将语言规则融入神经网络训练，有效提升了OOV词汇的处理能力，并在仅使用无标注数据的情况下实现了高精度POS标注。同时，它还能作为复杂任务的强大预训练工具，显著减少对标注数据的需求，验证了“原则性学习”范式的有效性。"}}
{"id": "2510.14301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14301", "abs": "https://arxiv.org/abs/2510.14301", "authors": ["Bingjie Zhang", "Yibo Yang", "Renzhe", "Dandan Guo", "Jindong Gu", "Philip Torr", "Bernard Ghanem"], "title": "A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in diverse\ntasks, yet their safety alignment remains fragile during adaptation. Even when\nfine-tuning on benign data or with low-rank adaptation, pre-trained safety\nbehaviors are easily degraded, leading to harmful responses in the fine-tuned\nmodels. To address this challenge, we propose GuardSpace, a guardrail framework\nfor preserving safety alignment throughout fine-tuning, composed of two key\ncomponents: a safety-sensitive subspace and a harmful-resistant null space.\nFirst, we explicitly decompose pre-trained weights into safety-relevant and\nsafety-irrelevant components using covariance-preconditioned singular value\ndecomposition, and initialize low-rank adapters from the safety-irrelevant\nones, while freezing safety-relevant components to preserve their associated\nsafety mechanism. Second, we construct a null space projector that restricts\nadapter updates from altering safe outputs on harmful prompts, thereby\nmaintaining the original refusal behavior. Experiments with various pre-trained\nmodels on multiple downstream tasks demonstrate that GuardSpace achieves\nsuperior performance over existing methods. Notably, for Llama-2-7B-Chat\nfine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,\nreducing the average harmful score from 14.4% to 3.6%, while improving the\naccuracy from from 26.0% to 28.0%.", "AI": {"tldr": "GuardSpace是一个用于在微调过程中保持大型语言模型安全对齐的框架。它通过分解预训练权重并限制适配器更新来防止有害响应，同时提高任务性能。", "motivation": "大型语言模型（LLMs）在适应（如微调或低秩适应）过程中，其预训练的安全行为容易退化，导致微调后的模型产生有害响应。", "method": "GuardSpace包含两个关键组件：1) 安全敏感子空间：使用协方差预处理的奇异值分解将预训练权重分解为安全相关和安全无关部分，冻结安全相关部分，并从安全无关部分初始化低秩适配器。2) 有害抵抗零空间：构建一个零空间投影器，限制适配器更新改变有害提示上的安全输出，从而保持原始的拒绝行为。", "result": "GuardSpace在多个下游任务上表现优于现有方法。例如，在GSM8K上微调Llama-2-7B-Chat时，GuardSpace将平均有害分数从14.4%降低到3.6%（优于AsFT），同时将准确率从26.0%提高到28.0%。", "conclusion": "GuardSpace框架通过有效保留安全对齐，成功解决了大型语言模型在微调过程中安全行为脆弱的问题，并在提高任务性能的同时显著降低了有害响应。"}}
{"id": "2510.14643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14643", "abs": "https://arxiv.org/abs/2510.14643", "authors": ["Lara Brudermüller", "Brandon Hung", "Xinghao Zhu", "Jiuguang Wang", "Nick Hawes", "Preston Culbertson", "Simon Le Cleac'h"], "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation", "comment": "9 pages, 5 figures", "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.", "AI": {"tldr": "本文提出了一种生成式预测控制（GPC）框架，通过使用在模拟中收集的采样式模型预测控制（SPC）序列训练条件流匹配模型，来摊销SPC。该方法学习有效的提议分布，提高在线规划的采样效率，并首次应用于四足机器人的实际接触丰富的运动操作。", "motivation": "传统MPC方法依赖迭代优化或基于梯度的求解器，效率较低。研究动机在于直接从噪声SPC数据中学习有意义的提议分布，以实现更高效、更明智的在线规划采样。", "method": "开发了一个生成式预测控制（GPC）框架，通过使用在模拟中收集的SPC控制序列训练条件流匹配模型来引导和摊销采样式模型预测控制（SPC）。该方法直接从噪声SPC数据中学习提议分布。", "result": "该方法提高了采样效率，减少了规划视野要求，并能稳健地泛化到不同的任务变体。它首次成功应用于四足机器人在实际世界中进行接触丰富的运动操作，并在模拟和硬件上进行了广泛验证。", "conclusion": "GPC框架通过学习有效的提议分布，成功地摊销了SPC，显著提升了在线规划的采样效率和鲁棒性。它在实际复杂的机器人任务中展现了强大的应用潜力。"}}
{"id": "2510.14273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14273", "abs": "https://arxiv.org/abs/2510.14273", "authors": ["Kieu-Anh Truong Thi", "Huy-Hieu Pham", "Duc-Trong Le"], "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts", "comment": null, "summary": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.", "AI": {"tldr": "本文提出了一种基于因果推断的新框架，利用语义特征并减轻混杂因素的影响，通过实现“前门原则”来解决组织病理学图像分析中的域偏移问题，并在未见过的域上取得了显著的性能提升。", "motivation": "组织病理学中的域偏移（由采集过程或数据源差异引起）严重挑战了深度学习模型的泛化能力。现有方法主要依赖于通过对齐特征分布或引入统计变异来建模统计相关性，但往往忽略了因果关系。", "method": "提出了一种新颖的基于因果推断的框架，该框架利用语义特征并减轻混杂因素的影响。通过设计明确结合中介变量和观察到的组织切片的转换策略，实现了“前门原则”。", "result": "在CAMELYON17数据集和一个私有组织病理学数据集上进行了验证，在未见过的域上表现出持续的性能提升。在CAMELYON17数据集和私有组织病理学数据集上均取得了高达7%的改进，优于现有基线方法。", "conclusion": "这些结果突显了因果推断作为解决组织病理学图像分析中域偏移问题的强大工具的潜力。"}}
{"id": "2510.13855", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13855", "abs": "https://arxiv.org/abs/2510.13855", "authors": ["Zhichen Zeng", "Qi Yu", "Xiao Lin", "Ruizhong Qiu", "Xuying Ning", "Tianxin Wei", "Yuchen Yan", "Jingrui He", "Hanghang Tong"], "title": "Harnessing Consistency for Robust Test-Time LLM Ensemble", "comment": "15 pages, 12 figures", "summary": "Different large language models (LLMs) exhibit diverse strengths and\nweaknesses, and LLM ensemble serves as a promising approach to integrate their\ncomplementary capabilities. Despite substantial progress in improving ensemble\nquality, limited attention has been paid to the robustness of ensembles against\npotential erroneous signals, which often arise from heterogeneous tokenization\nschemes and varying model expertise. Our analysis shows that ensemble failures\ntypically arise from both the token level and the model level: the former\nreflects severe disagreement in token predictions, while the latter involves\nlow confidence and pronounced disparities among models. In light of this, we\npropose CoRE, a plug-and-play technique that harnesses model consistency for\nrobust LLM ensemble, which can be seamlessly integrated with diverse ensemble\nmethods. Token-level consistency captures fine-grained disagreements by\napplying a low-pass filter to downweight uncertain tokens with high\ninconsistency, often due to token misalignment, thereby improving robustness at\na granular level. Model-level consistency models global agreement by promoting\nmodel outputs with high self-confidence and minimal divergence from others,\nenhancing robustness at a coarser level. Extensive experiments across diverse\nbenchmarks, model combinations, and ensemble strategies demonstrate that CoRE\nconsistently improves ensemble performance and robustness.", "AI": {"tldr": "本文提出CoRE，一种即插即用技术，通过解决词元级别和模型级别的不一致性，显著提升大型语言模型（LLM）集成方法的性能和鲁棒性。", "motivation": "LLM集成是结合不同模型优势的有效方法，但在提升集成质量方面，对集成方法抵抗潜在错误信号的鲁棒性关注不足。这些错误信号常源于异构的词元化方案和模型专业知识差异。研究发现集成失败通常源于词元级别（预测严重分歧）和模型级别（置信度低且模型间差异大）。", "method": "本文提出CoRE，一种即插即用技术，利用模型一致性来增强LLM集成的鲁棒性。它包括两个层面：1. 词元级别一致性：通过应用低通滤波器来降低不确定词元（常因词元错位导致不一致性高）的权重，从而在细粒度层面提高鲁棒性。2. 模型级别一致性：通过提升自我置信度高且与其他模型差异最小的模型输出，在粗粒度层面增强鲁棒性。", "result": "在多样化的基准测试、模型组合和集成策略上进行的广泛实验表明，CoRE持续改进了集成方法的性能和鲁棒性。", "conclusion": "CoRE通过有效处理词元和模型级别的不一致性，显著提升了LLM集成的性能和鲁棒性，并且可以无缝集成到各种集成方法中。"}}
{"id": "2510.14312", "categories": ["cs.AI", "cs.CL", "cs.CR", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.14312", "abs": "https://arxiv.org/abs/2510.14312", "authors": ["Mason Nakamura", "Abhinav Kumar", "Saaduddin Mahmud", "Sahar Abdelnabi", "Shlomo Zilberstein", "Eugene Bagdasarian"], "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies", "comment": null, "summary": "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems.", "AI": {"tldr": "本文提出了Terrarium框架，一个基于黑板设计的模块化、可配置测试平台，用于细致研究大型语言模型（LLM）驱动的多智能体系统（MAS）中的安全、隐私和信任问题，并识别了关键攻击向量。", "motivation": "由LLM驱动的MAS可以自动化复杂的任务（如会议调度），但由于需要处理非结构化私有数据、用户约束和偏好，引入了新的风险，包括未对齐（misalignment）和恶意方攻击（如窃取用户数据或损害智能体）。", "method": "本文提出了Terrarium框架，它通过重新利用多智能体系统早期的黑板设计，创建了一个模块化、可配置的测试平台，用于研究多智能体协作。该框架识别了未对齐、恶意智能体、受损通信和数据投毒等关键攻击向量，并实现了三个协作MAS场景以及四种代表性攻击来展示其灵活性。", "result": "Terrarium框架被证明具有灵活性，能够实现和演示三种协作MAS场景及四种代表性攻击。它提供了快速原型设计、评估和迭代防御措施与系统设计的工具。", "conclusion": "Terrarium框架旨在加速构建值得信赖的多智能体系统，通过提供工具来研究和解决LLM驱动MAS中的安全、隐私和信任问题。"}}
{"id": "2510.14647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14647", "abs": "https://arxiv.org/abs/2510.14647", "authors": ["Jialei Huang", "Yang Ye", "Yuanqing Gong", "Xuezhou Zhu", "Yang Gao", "Kaifeng Zhang"], "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation", "comment": "8 pages", "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.", "AI": {"tldr": "本文提出SaTA框架，通过将触觉特征锚定到手部运动学坐标系，实现了高精度的灵巧操作，显著优于现有视觉-触觉基线。", "motivation": "现有视觉-触觉学习方法难以达到传统基于模型方法在亚毫米级精度任务上的表现。主要限制在于未能有效利用触觉信号的丰富性和其与手部运动学的空间关系。理想的触觉表示应能将接触测量精确地定位到稳定参考系中，同时保留详细感官信息，以实现精确的物体几何推断。", "method": "引入SaTA（Spatially-anchored Tactile Awareness），一个端到端的策略框架。该框架通过正向运动学将触觉特征明确锚定到手部运动学坐标系中，从而实现精确的几何推理，而无需物体模型或显式姿态估计。核心思想是，空间锚定的触觉表示不仅能检测接触，还能精确推断手部坐标系中的物体几何。", "result": "SaTA在多项高难度灵巧操作任务中得到验证，包括双臂USB-C对接（亚毫米级对齐）、灯泡安装（螺纹啮合、旋转控制）和卡片滑动（力调制、角度精度）。相比强大的视觉-触觉基线，SaTA显著提高了成功率（最高达30%），并缩短了任务完成时间（27%）。", "conclusion": "SaTA通过空间锚定触觉感知，有效解决了学习型方法在灵巧操作中对亚毫米级精度的严格要求，证明了将触觉特征精确地定位在手部运动学坐标系中的重要性，从而实现无需显式模型的高精度几何推理。"}}
{"id": "2510.14677", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14677", "abs": "https://arxiv.org/abs/2510.14677", "authors": ["Steffen Hagedorn", "Luka Donkov", "Aron Distelzweig", "Alexandru P. Condurache"], "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks", "comment": null, "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.", "AI": {"tldr": "本文将先进的学习型交通代理模型SMART集成到nuPlan中，用于评估自动驾驶规划器，发现与传统的IDM代理相比，IDM模拟高估了规划性能，但一些规划器在复杂交互场景中表现更好。研究建议将SMART交互式模拟作为nuPlan中的新闭环基准。", "motivation": "闭环模拟中常用的基于规则的交通代理（如IDM）行为过于简单和被动，无法真实反映规划器的缺陷并可能导致评估结果偏差。它们无法应对复杂的交互场景，如对相邻车道车辆的反应，从而阻碍了对复杂交互能力的测试。", "method": "将最先进的学习型交通代理模型SMART集成到nuPlan平台中。通过使用SMART代理对14个最新的规划器和既定基线进行评估，并与IDM代理的评估结果进行对比，量化了在缩小模拟与现实差距时结论如何变化。", "result": "IDM模拟高估了规划性能，几乎所有规划器的分数在使用SMART代理时都有所下降。然而，许多规划器在多车道、交互密集型场景（如变道或转弯）中表现出比之前假设更好的交互能力，甚至有所提升。在闭环中训练的方法表现出最佳且最稳定的驾驶性能。但在增强型极端场景中达到极限时，所有学习型规划器性能会急剧下降，而基于规则的规划器仍能保持合理的基本行为。", "conclusion": "基于研究结果，建议将SMART交互式模拟作为nuPlan中新的标准闭环基准。同时发布SMART代理作为IDM的替代品，以促进更真实的规划器评估。"}}
{"id": "2510.14304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14304", "abs": "https://arxiv.org/abs/2510.14304", "authors": ["Kyungryul Back", "Seongbeom Park", "Milim Kim", "Mincheol Kwon", "SangHyeok Lee", "Hyunyoung Lee", "Junhee Cho", "Seunghyun Park", "Jinkyu Kim"], "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding", "comment": "EMNLP 2025 Findings; Project: https://github.com/KR-0822/TCD", "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses.", "AI": {"tldr": "本文提出了一种免训练、三层对比解码加水印的方法，以减少大型视觉-语言模型（LVLMs）中的幻觉问题，并生成视觉上更具基础的响应。", "motivation": "大型视觉-语言模型（LVLMs）在多模态任务中表现出色，但仍容易产生幻觉，通常过度依赖单一模态或记忆训练数据而未能正确地将输出接地。", "method": "该方法是一种免训练的三层对比解码加水印技术，分三步进行：1) 在解码层中选择一个成熟层和一个初级层；2) 使用与水印相关的问题识别一个枢轴层，以评估该层是否视觉接地良好；3) 应用三层对比解码生成最终输出。", "result": "在POPE、MME和AMBER等公共基准测试中，该方法在减少LVLMs幻觉方面达到了最先进的性能，并生成了更多视觉上接地（visually grounded）的响应。", "conclusion": "所提出的三层对比解码加水印方法能有效减少大型视觉-语言模型中的幻觉，并显著提高其响应的视觉接地性。"}}
{"id": "2510.14319", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14319", "abs": "https://arxiv.org/abs/2510.14319", "authors": ["Xu Shen", "Qi Zhang", "Song Wang", "Zhen Tan", "Xinyu Zhao", "Laura Yao", "Vaishnav Tadiparthi", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Kwonjoon Lee", "Tianlong Chen"], "title": "Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction", "comment": null, "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative\nproblem solving but remain brittle to cascading errors: a single faulty step\ncan propagate across agents and disrupt the trajectory. In this paper, we\npresent MASC, a metacognitive framework that endows MAS with real-time,\nunsupervised, step-level error detection and self-correction. MASC rethinks\ndetection as history-conditioned anomaly scoring via two complementary designs:\n(1) Next-Execution Reconstruction, which predicts the embedding of the next\nstep from the query and interaction history to capture causal consistency, and\n(2) Prototype-Guided Enhancement, which learns a prototype prior over\nnormal-step embeddings and uses it to stabilize reconstruction and anomaly\nscoring under sparse context (e.g., early steps). When an anomaly step is\nflagged, MASC triggers a correction agent to revise the acting agent's output\nbefore information flows downstream. On the Who&When benchmark, MASC\nconsistently outperforms all baselines, improving step-level error detection by\nup to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers\nconsistent end-to-end gains across architectures, confirming that our\nmetacognitive monitoring and targeted correction can mitigate error propagation\nwith minimal overhead.", "AI": {"tldr": "MASC是一个元认知框架，为基于大型语言模型的多智能体系统（MAS）提供实时、无监督的步级错误检测和自我纠正能力，以缓解级联错误。", "motivation": "基于大型语言模型的多智能体系统在协作解决问题方面表现出色，但容易出现级联错误，即单个错误步骤可能在智能体间传播并破坏整个执行轨迹。", "method": "MASC将错误检测重新定义为历史条件下的异常评分，通过两种互补设计实现：1) 下一步执行重建（Next-Execution Reconstruction），根据查询和交互历史预测下一步的嵌入以捕捉因果一致性；2) 原型引导增强（Prototype-Guided Enhancement），学习正常步骤嵌入的原型先验，并在稀疏上下文（如早期步骤）下稳定重建和异常评分。当检测到异常步骤时，MASC会触发一个纠正智能体，在信息流向下游之前修正执行智能体的输出。", "result": "在Who&When基准测试中，MASC始终优于所有基线，将步级错误检测的AUC-ROC提高高达8.47%。将其应用于不同的MAS框架时，MASC在各种架构中都带来了持续的端到端性能提升。", "conclusion": "MASC的元认知监控和目标纠正能力，能以最小的开销有效缓解多智能体系统中的错误传播，并为系统带来一致的性能提升。"}}
{"id": "2510.14768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14768", "abs": "https://arxiv.org/abs/2510.14768", "authors": ["Fan Yang", "Zixuan Huang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery", "comment": null, "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.", "AI": {"tldr": "本文提出CADRE框架，利用强化学习和NDF提取的接触特征，实现机械手在物体掉落时进行接住并恢复到有利姿态，提高恢复成功率并泛化到新物体。", "motivation": "现实世界中的灵巧操作常遭遇意外错误和干扰，导致操作失败（如物体掉落），因此需要一种机制来处理掉落物体并恢复系统以便继续主要操作任务。", "method": "提出接触感知动态恢复（CADRE）的强化学习框架，其中包含一个受神经描述符场（NDF）启发的模块，用于提取隐式接触特征。与仅依赖物体姿态或点云输入的方法相比，NDF能直接推理手指与物体之间的对应关系，并适应不同的物体几何形状。", "result": "实验表明，结合接触特征能提高训练效率，增强RL训练的收敛性能，并最终带来更成功的恢复。此外，CADRE能零样本泛化到具有不同几何形状的未见过物体。", "conclusion": "CADRE通过整合接触特征，有效解决了机械手接住掉落物体并恢复的问题，显著提升了恢复性能和泛化能力，使其能适应多变的操作环境和物体。"}}
{"id": "2510.14359", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14359", "abs": "https://arxiv.org/abs/2510.14359", "authors": ["Zichen Wen", "Yiyu Wang", "Chenfei Liao", "Boxue Yang", "Junxian Li", "Weifeng Liu", "Haocong He", "Bolong Feng", "Xuyang Liu", "Yuanhuiyi Lyu", "Xu Zheng", "Xuming Hu", "Linfeng Zhang"], "title": "AI for Service: Proactive Assistance with AI Glasses", "comment": "24 pages, 5 figures, work in progress", "summary": "In an era where AI is evolving from a passive tool into an active and\nadaptive companion, we introduce AI for Service (AI4Service), a new paradigm\nthat enables proactive and real-time assistance in daily life. Existing AI\nservices remain largely reactive, responding only to explicit user commands. We\nargue that a truly intelligent and helpful assistant should be capable of\nanticipating user needs and taking actions proactively when appropriate. To\nrealize this vision, we propose Alpha-Service, a unified framework that\naddresses two fundamental challenges: Know When to intervene by detecting\nservice opportunities from egocentric video streams, and Know How to provide\nboth generalized and personalized services. Inspired by the von Neumann\ncomputer architecture and based on AI glasses, Alpha-Service consists of five\nkey components: an Input Unit for perception, a Central Processing Unit for\ntask scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit\nfor long-term personalization, and an Output Unit for natural human\ninteraction. As an initial exploration, we implement Alpha-Service through a\nmulti-agent system deployed on AI glasses. Case studies, including a real-time\nBlackjack advisor, a museum tour guide, and a shopping fit assistant,\ndemonstrate its ability to seamlessly perceive the environment, infer user\nintent, and provide timely and useful assistance without explicit prompts.", "AI": {"tldr": "本文提出了AI4Service范式，旨在将AI从被动工具转变为主动、自适应的伴侣，并通过Alpha-Service框架在AI眼镜上实现主动、实时的日常辅助。", "motivation": "现有AI服务大多是被动响应用户明确指令的。研究者认为，真正智能和有用的助手应能预测用户需求并适时主动采取行动。", "method": "提出了Alpha-Service统一框架，灵感来源于冯·诺依曼计算机架构，部署在AI眼镜上。它包含五个关键组件：用于感知的输入单元、用于任务调度的中央处理单元、用于工具利用的算术逻辑单元、用于长期个性化的记忆单元以及用于自然人机交互的输出单元。该框架通过多智能体系统实现。", "result": "通过案例研究（如实时二十一点顾问、博物馆导游和购物搭配助手），Alpha-Service展示了其无需明确提示即可无缝感知环境、推断用户意图并提供及时有用协助的能力。", "conclusion": "Alpha-Service框架作为AI4Service范式的初步探索，成功地展示了实现主动、实时、通用和个性化AI辅助的可能性，能有效地预测用户需求并提供帮助。"}}
{"id": "2510.13860", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13860", "abs": "https://arxiv.org/abs/2510.13860", "authors": ["Shivanshu Kumar", "Gopalakrishnan Srinivasan"], "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing", "comment": null, "summary": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint.", "AI": {"tldr": "ShishuLM是一种高效的语言模型架构，通过在适度上下文场景下用MLP近似Transformer块，显著减少了内存占用和延迟，尤其适用于小型语言模型。", "motivation": "Transformer模型性能优越但内存和计算开销巨大。研究发现其架构存在冗余，可通过优化在不牺牲性能的情况下降低成本。此外，智能体AI系统中对小型语言模型（SLM）的需求日益增长，需要更高效的SLM架构。", "method": "引入ShishuLM架构，它借鉴了AI可解释性和推理时层剪枝的研究成果。该方法通过观察在适度上下文场景下，归一化和注意力计算与输入大致呈线性关系，从而用多层感知器（MLP）近似整个Transformer块。这减少了参数数量和键值（KV）缓存需求。", "result": "ShishuLM在内存需求方面减少了高达25%，并在训练和推理期间将延迟提高了高达40%，优于其父模型。", "conclusion": "实验和分析结果为从预训练角度构建更高效的SLM架构提供了见解，ShishuLM证明了在适度上下文场景下用MLP近似Transformer块的可行性与优势。"}}
{"id": "2510.13856", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13856", "abs": "https://arxiv.org/abs/2510.13856", "authors": ["A H M Rezaul Karim", "Ozlem Uzuner"], "title": "Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA", "comment": null, "summary": "Medical Visual Question Answering (MedVQA) enables natural language queries\nover medical images to support clinical decision-making and patient care. The\nMEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to\ngenerate free-text responses and structured wound attributes from images and\npatient queries. We present the MasonNLP system, which employs a\ngeneral-domain, instruction-tuned large language model with a\nretrieval-augmented generation (RAG) framework that incorporates textual and\nvisual examples from in-domain data. This approach grounds outputs in\nclinically relevant exemplars, improving reasoning, schema adherence, and\nresponse quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our\nbest-performing system ranked 3rd among 19 teams and 51 submissions with an\naverage score of 41.37%, demonstrating that lightweight RAG with\ngeneral-purpose LLMs -- a minimal inference-time layer that adds a few relevant\nexemplars via simple indexing and fusion, with no extra training or complex\nre-ranking -- provides a simple and effective baseline for multimodal clinical\nNLP tasks.", "AI": {"tldr": "MasonNLP系统在MEDIQA-WV 2025创伤护理VQA任务中，结合通用LLM和轻量级检索增强生成（RAG）框架，取得了第三名的成绩，证明了该方法在多模态临床NLP任务中的有效性。", "motivation": "医疗视觉问答（MedVQA）旨在通过自然语言查询医疗图像来支持临床决策和患者护理。MEDIQA-WV 2025共享任务专注于创伤护理VQA，要求系统根据图像和患者查询生成自由文本响应及结构化创伤属性，这促使研究人员寻求有效的解决方案。", "method": "MasonNLP系统采用了一个通用的、经过指令微调的大型语言模型（LLM），并结合了检索增强生成（RAG）框架。该框架通过简单的索引和融合，整合了来自领域内数据的文本和视觉示例，以增强输出的临床相关性，且无需额外的训练或复杂的重排序。", "result": "MasonNLP系统在MEDIQA-WV 2025任务中，在19个团队和51份提交中排名第三，平均得分为41.37%。该方法在dBLEU、ROUGE、BERTScore和基于LLM的指标上，显著提升了推理能力、模式遵循度和响应质量。", "conclusion": "轻量级的RAG与通用LLM相结合，通过在推理时添加少量相关示例，为多模态临床自然语言处理任务提供了一个简单而有效的基线，无需额外的训练或复杂的重排序。"}}
{"id": "2510.14771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14771", "abs": "https://arxiv.org/abs/2510.14771", "authors": ["Xu Chi", "Chao Zhang", "Yang Su", "Lingfeng Dou", "Fujia Yang", "Jiakuo Zhao", "Haoyu Zhou", "Xiaoyou Jia", "Yong Zhou", "Shan An"], "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation", "comment": "17 pages", "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.", "AI": {"tldr": "Open TeleDex是一个统一的遥操作框架，旨在解决异构机器人平台模仿学习中高质量演示数据采集的瓶颈，通过支持任意机械臂、灵巧手和输入设备，并引入新型手部姿态重定向算法，显著提升了互操作性。", "motivation": "机器人模仿学习（IL）系统部署面临准确、高保真演示数据采集的瓶颈，尤其是在处理异构机器人平台时。现有遥操作系统难以保证跨多样化遥操作设备的精确数据收集。", "method": "开发了Open TeleDex统一遥操作框架，以应对“任意机械臂、任意灵巧手、任意外部输入设备”的“三重任意”挑战。此外，提出了一种新颖的手部姿态重定向算法，以增强系统互操作性。", "result": "Open TeleDex无缝支持任意机械臂、任意灵巧手和任意外部输入设备。所提出的手部姿态重定向算法显著提高了互操作性，实现了与更广泛异构主从设备的鲁棒和精确兼容。该系统建立了一个基础的、高质量的、公开可用的平台。", "conclusion": "Open TeleDex为加速复杂机器人操作和模仿学习领域的学术研究与工业发展提供了一个高质量的数据采集平台。"}}
{"id": "2510.14314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14314", "abs": "https://arxiv.org/abs/2510.14314", "authors": ["Shivangi Yadav", "Arun Ross"], "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection", "comment": null, "summary": "An iris biometric system can be compromised by presentation attacks (PAs)\nwhere artifacts such as artificial eyes, printed eye images, or cosmetic\ncontact lenses are presented to the system. To counteract this, several\npresentation attack detection (PAD) methods have been developed. However, there\nis a scarcity of datasets for training and evaluating iris PAD techniques due\nto the implicit difficulties in constructing and imaging PAs. To address this,\nwe introduce the Multi-domain Image Translative Diffusion StyleGAN\n(MID-StyleGAN), a new framework for generating synthetic ocular images that\ncaptures the PA and bonafide characteristics in multiple domains such as\nbonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the\nstrengths of diffusion models and generative adversarial networks (GANs) to\nproduce realistic and diverse synthetic data. Our approach utilizes a\nmulti-domain architecture that enables the translation between bonafide ocular\nimages and different PA domains. The model employs an adaptive loss function\ntailored for ocular data to maintain domain consistency. Extensive experiments\ndemonstrate that MID-StyleGAN outperforms existing methods in generating\nhigh-quality synthetic ocular images. The generated data was used to\nsignificantly enhance the performance of PAD systems, providing a scalable\nsolution to the data scarcity problem in iris and ocular biometrics. For\nexample, on the LivDet2020 dataset, the true detect rate at 1% false detect\nrate improved from 93.41% to 98.72%, showcasing the impact of the proposed\nmethod.", "AI": {"tldr": "为解决虹膜活体检测（PAD）数据稀缺问题，本文提出了MID-StyleGAN框架，结合扩散模型和GAN生成高质量合成眼部图像（包括活体和各种攻击类型），显著提升了PAD系统的性能。", "motivation": "虹膜生物识别系统易受演示攻击（如人工眼、打印图像、美瞳）的威胁。现有的演示攻击检测（PAD）方法因构建和成像攻击的固有困难，导致训练和评估数据集稀缺。", "method": "本文提出Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN)，该框架结合了扩散模型和生成对抗网络（GAN）的优点。它采用多域架构，实现活体眼部图像与不同演示攻击域（如打印眼、美瞳）之间的转换，并使用为眼部数据量身定制的自适应损失函数来保持域一致性。", "result": "MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。利用生成的数据，PAD系统性能显著提升，例如在LivDet2020数据集上，当误检率为1%时，真检率从93.41%提高到98.72%。", "conclusion": "MID-StyleGAN通过生成逼真且多样化的合成数据，为虹膜和眼部生物识别中的数据稀缺问题提供了一个可扩展的解决方案，显著增强了PAD系统的性能和对演示攻击的鲁棒性。"}}
{"id": "2510.13862", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13862", "abs": "https://arxiv.org/abs/2510.13862", "authors": ["Chenyu Zhang", "Sharifa Alghowinem", "Cynthia Breazeal"], "title": "Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues", "comment": "4 pages, 3 figures. Published in the 11th International Conference on\n  Affective Computing and Intelligent Interaction (ACII 2025), Late-Breaking\n  Results Track", "summary": "While recent studies have examined the leaning impact of large language model\n(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring\nremain insufficiently understood. This work introduces the first ensemble-LLM\nframework for large-scale affect sensing in tutoring dialogues, advancing the\nconversation on responsible pathways for integrating generative AI into\neducation by attending to learners' evolving affective states. To achieve this,\nwe analyzed two semesters' worth of 16,986 conversational turns exchanged\nbetween PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across\nthree U.S. institutions. To investigate learners' emotional experiences, we\ngenerate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,\nClaude), including scalar ratings of valence, arousal, and\nlearning-helpfulness, along with free-text emotion labels. These estimates are\nfused through rank-weighted intra-model pooling and plurality consensus across\nmodels to produce robust emotion profiles. Our analysis shows that during\ninteraction with the AI tutor, students typically report mildly positive affect\nand moderate arousal. Yet learning is not uniformly smooth: confusion and\ncuriosity are frequent companions to problem solving, and frustration, while\nless common, still surfaces in ways that can derail progress. Emotional states\nare short-lived--positive moments last slightly longer than neutral or negative\nones, but they are fragile and easily disrupted. Encouragingly, negative\nemotions often resolve quickly, sometimes rebounding directly into positive\nstates. Neutral moments frequently act as turning points, more often steering\nstudents upward than downward, suggesting opportunities for tutors to intervene\nat precisely these junctures.", "AI": {"tldr": "本研究引入了一种集成LLM框架，用于大规模情感感知AI辅导对话，分析了学生与AI导师互动时的情感动态，发现学生情绪通常呈轻度积极和中度兴奋，负面情绪常快速消散并转化为积极情绪。", "motivation": "尽管已有研究探讨大型语言模型（LLM）在教育中的学习影响，但LLM介导辅导的情感动态仍未被充分理解。本研究旨在通过关注学习者的情感演变，推进生成式AI负责任地融入教育的讨论。", "method": "研究引入了首个集成LLM框架，用于辅导对话中的大规模情感感知。分析了来自261名本科生与LLM驱动的AI导师PyTutor之间，横跨两个学期共16,986个对话回合。利用三个前沿LLM（Gemini, GPT-4o, Claude）生成零样本情感标注，包括效价、唤醒度、学习帮助度量表评分以及自由文本情感标签。这些估计通过排名加权模型内池化和跨模型多数共识进行融合，以生成稳健的情感画像。", "result": "分析显示，学生与AI导师互动时通常表现出轻度积极情感和中度唤醒。学习过程并非一帆风顺：困惑和好奇心是解决问题的常见伴侣，而挫折虽然不那么常见，但仍会以阻碍进展的方式出现。情绪状态是短暂的——积极时刻比中性或消极时刻持续稍长，但它们脆弱且易受干扰。令人鼓舞的是，负面情绪通常能迅速消散，有时直接反弹到积极状态。中性时刻经常充当转折点，更多地引导学生向上而非向下，这表明导师可以在这些关键时刻进行干预。", "conclusion": "学生在与AI导师互动时表现出复杂且动态的情感模式，虽然总体偏积极，但困惑和挫折仍然存在。负面情绪通常能快速解决并转向积极状态，而中性时刻是潜在的干预点。理解这些情感动态有助于负责任地将生成式AI融入教育，并为AI辅导干预提供指导。"}}
{"id": "2510.14387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14387", "abs": "https://arxiv.org/abs/2510.14387", "authors": ["Yijie Hu", "Zihao Zhou", "Kaizhu Huang", "Xiaowei Huang", "Qiufeng Wang"], "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?", "comment": null, "summary": "Math reasoning has been one crucial ability of large language models (LLMs),\nwhere significant advancements have been achieved in recent years. However,\nmost efforts focus on LLMs by curating high-quality annotation data and\nintricate training (or inference) paradigms, while the math reasoning\nperformance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM\ntypically consists of an LLM and a vision block, we wonder: Can MLLMs directly\nabsorb math reasoning abilities from off-the-shelf math LLMs without tuning?\nRecent model-merging approaches may offer insights into this question. However,\nthey overlook the alignment between the MLLM and LLM, where we find that there\nis a large gap between their parameter spaces, resulting in lower performance.\nOur empirical evidence reveals two key factors behind this issue: the\nidentification of crucial reasoning-associated layers in the model and the\nmitigation of the gaps in parameter space. Based on the empirical insights, we\npropose IP-Merging that first identifies the reasoning-associated parameters in\nboth MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to\nmaintain the alignment, and finally merges parameters in this subspace.\nIP-Merging is a tuning-free approach since parameters are directly adjusted.\nExtensive experiments demonstrate that our IP-Merging method can enhance the\nmath reasoning ability of MLLMs directly from Math LLMs without compromising\ntheir other capabilities.", "AI": {"tldr": "本文提出了一种名为IP-Merging的无调优方法，使多模态大语言模型（MLLM）能够直接从现成的数学大语言模型（Math LLM）中吸收数学推理能力，通过识别和投影关键推理参数来弥合参数空间差距。", "motivation": "尽管大语言模型（LLM）在数学推理方面取得了显著进展，但多模态大语言模型（MLLM）的数学推理能力仍然滞后。研究人员想知道MLLM是否可以在不进行调优的情况下直接吸收现成数学LLM的数学推理能力。现有的模型合并方法忽略了MLLM和LLM之间的参数空间对齐问题，导致性能下降。", "method": "基于经验观察（识别关键推理相关层和缓解参数空间差距），本文提出了IP-Merging方法。该方法首先识别MLLM和数学LLM中与推理相关的参数，然后将这些参数投影到MLLM的子空间中以保持对齐，最后在该子空间中合并参数。这是一种无需调优的方法。", "result": "广泛的实验表明，IP-Merging方法能够直接从数学LLM中增强MLLM的数学推理能力，同时不损害其其他能力。", "conclusion": "IP-Merging通过解决参数空间对齐问题，提供了一种有效的、无需调优的途径，使多模态大语言模型能够直接从数学大语言模型中获得强大的数学推理能力。"}}
{"id": "2510.14783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14783", "abs": "https://arxiv.org/abs/2510.14783", "authors": ["Aderik Verraest", "Stavrow Bahnam", "Robin Ferede", "Guido de Croon", "Christophe De Wagter"], "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning", "comment": null, "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.", "AI": {"tldr": "SkyDreamer是首个端到端视觉的自主无人机竞速策略，实现了完整的仿真到现实迁移、板载执行和冠军级性能，通过模型基强化学习实现鲁棒、高速飞行。", "motivation": "现有的自主无人机竞速（ADR）系统高度特化于无人机竞速，而端到端视觉方法虽有更广阔的应用前景，但至今没有系统能同时实现完整的仿真到现实迁移、板载执行和冠军级性能。", "method": "SkyDreamer采用端到端视觉策略，直接将像素级表示映射到电机指令。它基于“informed Dreamer”，这是一种模型基强化学习方法，其中世界模型在训练期间解码特权信息，充当隐式状态和参数估计器。该系统完全板载运行，无需外部辅助，通过世界模型隐藏状态解码的状态跟踪进度以解决视觉模糊性，且无需外部相机校准，支持快速部署。", "result": "SkyDreamer实现了鲁棒、高速飞行，最高速度达21米/秒，加速度达6g，并能执行倒立循环、Split-S和梯子等复杂机动。它通过处理低质量分割掩码展示了非平凡的视觉仿真到现实迁移，并通过准确估计最大可达电机转速并实时调整飞行路径，展现了对电池耗尽的鲁棒性。", "conclusion": "SkyDreamer证明了其对现实差距重要方面的适应性，在实现极其高速、敏捷飞行的同时，保持了鲁棒性。"}}
{"id": "2510.14354", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14354", "abs": "https://arxiv.org/abs/2510.14354", "authors": ["Siddharth Tourani", "Jayaram Reddy", "Sarvesh Thakur", "K Madhava Krishna", "Muhammad Haris Khan", "N Dinesh Reddy"], "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration", "comment": "8 pages, accepted at ICRA 2024 (International Conference on Robotics\n  and Automation)", "summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has\nbecome available. This prompts the question of how to utilize this data for\ngeometric reasoning of scenes. While many RGB-D registration meth- ods rely on\ngeometric and feature-based similarity, we take a different approach. We use\ncycle-consistent keypoints as salient points to enforce spatial coherence\nconstraints during matching, improving correspondence accuracy. Additionally,\nwe introduce a novel pose block that combines a GRU recurrent unit with\ntransformation synchronization, blending historical and multi-view data. Our\napproach surpasses previous self- supervised registration methods on ScanNet\nand 3DMatch, even outperforming some older supervised methods. We also\nintegrate our components into existing methods, showing their effectiveness.", "AI": {"tldr": "该研究提出了一种利用循环一致关键点和新型姿态块进行RGB-D自监督场景配准的方法，显著提高了配准精度，超越了现有的自监督和部分有监督方法。", "motivation": "随着消费级深度相机的普及，大量未标注的RGB-D数据变得可用。如何有效利用这些数据进行场景的几何推理，特别是配准，成为了一个重要问题。", "method": "本研究采用了不同于传统几何和特征相似性的配准方法：1. 使用循环一致关键点作为显著点，在匹配过程中强制执行空间一致性约束，以提高对应点精度。2. 引入了一种新颖的姿态块，该姿态块结合了GRU循环单元和变换同步机制，融合了历史数据和多视角数据。", "result": "该方法在ScanNet和3DMatch数据集上超越了以往的自监督配准方法，甚至优于一些较旧的有监督方法。此外，将提出的组件集成到现有方法中也显示了其有效性。", "conclusion": "所提出的基于循环一致关键点和新型姿态块的RGB-D自监督配准方法，在准确性方面表现出色，能够有效利用未标注的RGB-D数据，并可与其他方法结合使用以提升性能。"}}
{"id": "2510.13870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13870", "abs": "https://arxiv.org/abs/2510.13870", "authors": ["Junhoo Lee", "Seungyeon Kim", "Nojun Kwak"], "title": "Unlocking the Potential of Diffusion Language Models through Template Infilling", "comment": null, "summary": "Diffusion Language Models (DLMs) have emerged as a promising alternative to\nAutoregressive Language Models, yet their inference strategies remain limited\nto prefix-based prompting inherited from the autoregressive paradigm. In this\npaper, we propose Template Infilling (TI), a tailored conditioning methodology\nfor DLMs' generation process. Unlike conventional prefix prompting, TI first\ngenerates a structural template for the target response, then fills in the\nmasked segments. To enhance the flexibility of this structural control, we\nintroduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment\nlengths based on generation confidence. We demonstrate the effectiveness of our\napproach on mathematical reasoning and code generation benchmarks, achieving\nconsistent improvements of 17.01$\\%$p over baseline. Furthermore, we show that\nTI provides additional advantages in multi-token generation settings, enabling\neffective speedup while maintaining generation quality.", "AI": {"tldr": "本文提出模板填充（TI）和动态分段分配（DSA）作为扩散语言模型（DLMs）的定制条件生成方法，旨在克服传统前缀提示的局限性，并在数学推理和代码生成任务上实现显著的性能提升和生成加速。", "motivation": "扩散语言模型（DLMs）的推理策略目前仍限于从自回归范式继承的前缀提示，这种方法限制了其生成过程的灵活性和效率。", "method": "本文提出模板填充（TI）方法，它首先生成目标响应的结构模板，然后填充模板中的掩码片段。为增强结构控制的灵活性，引入动态分段分配（DSA），根据生成置信度自适应地调整片段长度。", "result": "该方法在数学推理和代码生成基准测试上比基线模型实现17.01%p的持续改进。此外，模板填充在多令牌生成设置中提供了额外优势，在保持生成质量的同时实现了有效的加速。", "conclusion": "模板填充（TI）结合动态分段分配（DSA）为扩散语言模型提供了一种有效的定制条件生成方法，显著提升了其在特定任务上的性能和生成效率，尤其是在多令牌生成场景下。"}}
{"id": "2510.14349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14349", "abs": "https://arxiv.org/abs/2510.14349", "authors": ["Yunnan Wang", "Fan Lu", "Kecheng Zheng", "Ziyuan Huang", "Ziqiang Li", "Wenjun Zeng", "Xin Jin"], "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models", "comment": "Under Review", "summary": "Multimodal large language models (MLLMs) integrate image features from visual\nencoders with LLMs, demonstrating advanced comprehension capabilities. However,\nmainstream MLLMs are solely supervised by the next-token prediction of textual\ntokens, neglecting critical vision-centric information essential for analytical\nabilities. To track this dilemma, we introduce VaCo, which optimizes MLLM\nrepresentations through Vision-Centric activation and Coordination from\nmultiple vision foundation models (VFMs). VaCo introduces visual discriminative\nalignment to integrate task-aware perceptual features extracted from VFMs,\nthereby unifying the optimization of both textual and visual outputs in MLLMs.\nSpecifically, we incorporate the learnable Modular Task Queries (MTQs) and\nVisual Alignment Layers (VALs) into MLLMs, activating specific visual signals\nunder the supervision of diverse VFMs. To coordinate representation conflicts\nacross VFMs, the crafted Token Gateway Mask (TGM) restricts the information\nflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo\nsignificantly improves the performance of different MLLMs on various\nbenchmarks, showcasing its superior capabilities in visual comprehension.", "AI": {"tldr": "VaCo通过多视觉基础模型（VFM）的视觉中心激活和协调，优化多模态大语言模型（MLLM）的表示，以整合任务感知视觉特征，显著提升视觉理解能力。", "motivation": "主流多模态大语言模型（MLLM）仅通过文本token的下一token预测进行监督，忽略了对分析能力至关重要的视觉中心信息，导致其视觉理解能力受限。", "method": "本文提出了VaCo框架，通过多视觉基础模型（VFM）的视觉中心激活和协调来优化MLLM的表示。VaCo引入视觉判别对齐，整合从VFM中提取的任务感知感知特征，从而统一了MLLM中文本和视觉输出的优化。具体而言，它将可学习的模块化任务查询（MTQ）和视觉对齐层（VAL）整合到MLLM中，在多种VFM的监督下激活特定的视觉信号。为协调VFM之间表示冲突，设计了Token网关掩码（TGM）来限制多个MTQ组之间的信息流。", "result": "大量实验表明，VaCo显著提升了不同MLLM在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。", "conclusion": "VaCo通过整合视觉中心信息和协调多视觉基础模型，有效解决了当前MLLM对视觉信息利用不足的问题，从而显著提高了模型的视觉理解能力。"}}
{"id": "2510.14827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.", "AI": {"tldr": "该论文提出了一种基于隐式神经函数的连续时空动态地图（MoD）表示，用于更准确、高效地建模复杂人类运动模式，解决了传统MoD离散采样和构建成本高的问题。", "motivation": "在复杂人类环境中，机器人安全高效运行需要良好的特定地点运动模式模型。现有的动态地图（MoD）表示采用离散空间采样，通常需要昂贵的离线构建，并且在采样不均匀区域存在不足。", "method": "本文提出了一种连续时空MoD表示，该表示基于隐式神经函数，直接将坐标映射到半包裹高斯混合模型（Semi-Wrapped Gaussian Mixture Model）的参数。这消除了离散化和不均匀采样区域插值的需要，实现了空间和时间上的平滑泛化。", "result": "与现有基线方法相比，该方法在大型公共数据集（包含长期真实世界人员跟踪数据）上进行了评估，实现了更高的运动表示精度，在稀疏区域具有更平滑的速度分布，同时保持计算效率。", "conclusion": "所提出的方法展示了一种强大而高效的复杂人类运动模式建模方式。"}}
{"id": "2510.13871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13871", "abs": "https://arxiv.org/abs/2510.13871", "authors": ["Elwin Huaman", "Wendi Huaman", "Jorge Luis Huaman", "Ninfa Quispe"], "title": "Quechua Speech Datasets in Common Voice: The Case of Puno Quechua", "comment": "to be published in the 9th Annual International Conference on\n  Information Management and Big Data (SIMBig 2025)", "summary": "Under-resourced languages, such as Quechuas, face data and resource scarcity,\nhindering their development in speech technology. To address this issue, Common\nVoice presents a crucial opportunity to foster an open and community-driven\nspeech dataset creation. This paper examines the integration of Quechua\nlanguages into Common Voice. We detail the current 17 Quechua languages,\npresenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes\nlanguage onboarding and corpus collection of both reading and spontaneous\nspeech data. Our results demonstrate that Common Voice now hosts 191.1 hours of\nQuechua speech (86\\% validated), with Puno Quechua contributing 12 hours (77\\%\nvalidated), highlighting the Common Voice's potential. We further propose a\nresearch agenda addressing technical challenges, alongside ethical\nconsiderations for community engagement and indigenous data sovereignty. Our\nwork contributes towards inclusive voice technology and digital empowerment of\nunder-resourced language communities.", "AI": {"tldr": "本文探讨了将克丘亚语（特别是普诺克丘亚语）整合到Common Voice平台，以解决资源匮乏语言的数据稀缺问题，并展示了其在收集语音数据方面的潜力。", "motivation": "资源匮乏语言（如克丘亚语）面临数据和资源稀缺，阻碍了其在语音技术领域的发展。Common Voice提供了一个开放且社区驱动的语音数据集创建机会，以解决此问题。", "method": "研究了17种克丘亚语在Common Voice上的整合情况，并以普诺克丘亚语（qxp）为例进行了深入案例研究，包括语言入驻和阅读及自发语音数据的语料库收集。", "result": "Common Voice目前拥有191.1小时的克丘亚语语音数据（86%已验证），其中普诺克丘亚语贡献了12小时（77%已验证），这突显了Common Voice的巨大潜力。论文还提出了解决技术挑战和伦理考量的研究议程。", "conclusion": "这项工作促进了包容性语音技术和资源匮乏语言社区的数字赋能。Common Voice在为这些语言收集和验证语音数据方面具有显著潜力，但仍需关注技术挑战、社区参与和原住民数据主权等伦理问题。"}}
{"id": "2510.14388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14388", "abs": "https://arxiv.org/abs/2510.14388", "authors": ["Zhe Wu", "Hongjin Lu", "Junliang Xing", "Changhao Zhang", "Yin Zhu", "Yuhao Yang", "Yuheng Jing", "Kai Li", "Kun Shao", "Jianye Hao", "Jun Wang", "Yuanchun Shi"], "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control", "comment": null, "summary": "Building agents that autonomously operate mobile devices has attracted\nincreasing attention. While Vision-Language Models (VLMs) show promise, most\nexisting approaches rely on direct state-to-action mappings, which lack\nstructured reasoning and planning, and thus generalize poorly to novel tasks or\nunseen UI layouts. We introduce Hi-Agent, a trainable hierarchical\nvision-language agent for mobile control, featuring a high-level reasoning\nmodel and a low-level action model that are jointly optimized. For efficient\ntraining, we reformulate multi-step decision-making as a sequence of\nsingle-step subgoals and propose a foresight advantage function, which\nleverages execution feedback from the low-level model to guide high-level\noptimization. This design alleviates the path explosion issue encountered by\nGroup Relative Policy Optimization (GRPO) in long-horizon tasks and enables\nstable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art\n(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,\nsignificantly outperforming prior methods across three paradigms: prompt-based\n(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement\nlearning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot\ngeneralization on the ScreenSpot-v2 benchmark. On the more challenging\nAndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,\nshowing strong adaptability in high-complexity mobile control scenarios.", "AI": {"tldr": "Hi-Agent是一种可训练的分层视觉语言智能体，用于移动设备控制。它通过分层推理和行动模型以及新颖的训练方法，在Android-in-the-Wild (AitW)基准上取得了最先进的性能，并展示了强大的泛化能力。", "motivation": "现有基于视觉语言模型（VLM）的移动设备控制方法主要依赖直接的状态到动作映射，缺乏结构化推理和规划能力，导致在面对新任务或未见用户界面布局时泛化能力差。", "method": "本文提出了Hi-Agent，一个可训练的分层视觉语言智能体。它包含一个高层推理模型和一个低层行动模型，两者联合优化。为提高训练效率，将多步决策重构为一系列单步子目标，并引入了“预见优势函数”（foresight advantage function），利用低层模型的执行反馈指导高层优化。这种设计解决了长周期任务中路径爆炸问题，并实现了稳定、无评论器的联合训练。", "result": "Hi-Agent在Android-in-the-Wild (AitW)基准测试中取得了87.9%的任务成功率，达到了新的SOTA，显著优于先前的提示式（AppAgent: 17.7%）、监督式（Filtered BC: 54.5%）和强化学习式（DigiRL: 71.9%）方法。它还在ScreenSpot-v2基准上展示了有竞争力的零样本泛化能力。在更具挑战性的AndroidWorld基准上，Hi-Agent也能有效扩展至更大的骨干网络，在高复杂度移动控制场景中展现出强大的适应性。", "conclusion": "Hi-Agent通过其创新的分层架构和联合优化策略，有效解决了移动设备控制中结构化推理和泛化能力不足的问题。它在多个基准测试中取得了卓越的性能，证明了其在复杂移动控制场景中的有效性和强大的适应性。"}}
{"id": "2510.14406", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14406", "abs": "https://arxiv.org/abs/2510.14406", "authors": ["Xikai Zhang", "Bo Wang", "Likang Xiao", "Yongzhi Li", "Quan Chen", "Wenju Wu", "Liu Liu"], "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning", "comment": null, "summary": "Although large language models (LLMs) have made significant strides across\nvarious tasks, they still face significant challenges in complex reasoning and\nplanning. For example, even with carefully designed prompts and prior\ninformation explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on\nthe TravelPlanner dataset in the sole-planning mode. Similarly, even in the\nthinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass\nRates of 5.9% and 40%, respectively. Although well-organized Multi-Agent\nSystems (MAS) can offer improved collective reasoning, they often suffer from\nhigh reasoning costs due to multi-round internal interactions, long\nper-response latency, and difficulties in end-to-end training. To address these\nchallenges, we propose a general and scalable framework called IMAGINE, short\nfor Integrating Multi-Agent System into One Model. This framework not only\nintegrates the reasoning and planning capabilities of MAS into a single,\ncompact model, but also significantly surpass the capabilities of the MAS\nthrough a simple end-to-end training. Through this pipeline, a single\nsmall-scale model is not only able to acquire the structured reasoning and\nplanning capabilities of a well-organized MAS but can also significantly\noutperform it. Experimental results demonstrate that, when using\nQwen3-8B-Instruct as the base model and training it with our method, the model\nachieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding\nthe 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.", "AI": {"tldr": "大型语言模型在复杂推理和规划方面表现不佳，多智能体系统（MAS）虽能改善但成本高昂。IMAGINE框架通过端到端训练将MAS的推理规划能力集成到一个紧凑模型中，显著超越了MAS的性能，并实现了更高的效率。", "motivation": "大型语言模型（如GPT-4o、Qwen3-8B-Instruct、DeepSeek-R1-671B）在复杂推理和规划任务（如TravelPlanner）上表现差劲，即使有精心设计的提示也仅能达到很低的通过率。多智能体系统（MAS）虽然能提升集体推理能力，但存在高推理成本、长响应延迟和端到端训练困难等问题。", "method": "本文提出了IMAGINE（Integrating Multi-Agent System into One Model）框架。该框架将多智能体系统的推理和规划能力整合到一个单一、紧凑的模型中。通过简单的端到端训练，该模型不仅能习得MAS的结构化推理和规划能力，还能显著超越MAS的性能。", "result": "实验结果表明，使用Qwen3-8B-Instruct作为基础模型并采用IMAGINE方法进行训练后，模型在TravelPlanner基准测试上达到了82.7%的最终通过率，远超DeepSeek-R1-671B的40%以及基础Qwen3-8B-Instruct的5.9%，同时保持了更小的模型规模。", "conclusion": "IMAGINE框架成功地将多智能体系统的推理和规划能力集成并增强到一个单一的小规模模型中，显著提升了模型在复杂任务上的性能，超越了现有大型模型和MAS的局限性，实现了更高的效率和效果。"}}
{"id": "2510.14376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14376", "abs": "https://arxiv.org/abs/2510.14376", "authors": ["Dongnam Byun", "Jungwon Park", "Jumgmin Ko", "Changin Choi", "Wonjong Rhee"], "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation", "comment": null, "summary": "Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation.", "AI": {"tldr": "该论文提出DOS方法，通过修改CLIP文本嵌入来解决文生图模型在处理多对象提示时出现的对象忽略和混合问题，显著提高了多对象图像生成的成功率。", "motivation": "尽管文生图模型在生成高质量图像方面取得进展，但它们在处理包含多个对象的提示时仍面临挑战，常导致对象被忽略或混合。作者通过研究识别出四种常见失败场景（相似形状、相似纹理、背景偏差和多对象），这些场景下对象间的关系容易导致生成失败。", "method": "基于对CLIP嵌入的两个关键观察，作者提出了DOS（Directional Object Separation）方法。该方法在将文本嵌入传递给文生图模型之前，修改了三种类型的CLIP文本嵌入。", "result": "实验结果表明，DOS持续提高了多对象图像生成的成功率并减少了对象混合。在人工评估中，DOS在四个基准测试中显著优于四种竞争方法，获得了26.24%-43.04%的更多投票。", "conclusion": "DOS是一种实用且有效的解决方案，能够显著改善文生图模型在处理多对象提示时的生成质量，解决了对象忽略和混合的问题。"}}
{"id": "2510.14849", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14849", "abs": "https://arxiv.org/abs/2510.14849", "authors": ["Marcello Sorge", "Nicola Cigarini", "Riccardo Lorigiola", "Giulia Michieletto", "Andrea Masiero", "Angelo Cenedese", "Alberto Guarnieri"], "title": "Multi Agent Switching Mode Controller for Sound Source localization", "comment": null, "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.", "AI": {"tldr": "本文设计了一种多智能体切换模式控制策略，用于声学目标定位，涵盖单源（刚性编队）和多源（独立搜索）场景。", "motivation": "在无法建立直视路径的恶劣条件下，基于声音的传感器能够让智能体定位目标，这使得声源搜寻成为机器人研究中的重要课题。", "method": "本文设计了一种多智能体切换模式控制策略，利用声学传感器进行目标定位。考虑了两种场景：一是智能体保持刚性编队向单一目标移动的单源定位；二是每个智能体独立搜索多个目标的多源定位。", "result": "本文提出并设计了一种适用于声学目标定位的多智能体切换模式控制策略，并为单源和多源定位场景定义了具体的实现方式。", "conclusion": "该工作提出了一种新颖的多智能体声学目标定位控制策略，为在复杂环境中利用声学传感器进行目标搜寻提供了解决方案。"}}
{"id": "2510.14374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14374", "abs": "https://arxiv.org/abs/2510.14374", "authors": ["Han Qiu", "Peng Gao", "Lewei Lu", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding", "comment": "ICCV 2025", "summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial\nunderstanding capabilities, such as referencing and grounding object\ndescriptions. Despite their successes, MLLMs still fall short in fine-grained\nspatial perception abilities, such as generating detailed region descriptions\nor accurately localizing objects. Additionally, they often fail to respond to\nthe user's requirements for desired fine-grained spatial understanding. This\nissue might arise because existing approaches primarily focus on tuning MLLMs\nto model pre-annotated instruction data to inject spatial knowledge, without\ndirect supervision of MLLMs' actual responses. We address this issue by SPR, a\nSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial\ncapabilities by rewarding MLLMs' detailed responses with precise object\nlocalization over vague or inaccurate responses. With randomly selected image\nregions and region descriptions from MLLMs, SPR introduces semantic and\nlocalization scores to comprehensively evaluate the text quality and\nlocalization quality in MLLM-generated descriptions. We also refine the MLLM\ndescriptions with better localization accuracy and pair the best-scored\nrefinement with the initial descriptions of the lowest score for direct\npreference optimization, thereby enhancing fine-grained alignment with visual\ninput. Extensive experiments over standard referring and grounding benchmarks\nshow that SPR improves MLLM spatial understanding capabilities effectively with\nminimal overhead in training. Data and code will be released at\nhttps://github.com/hanqiu-hq/SPR", "AI": {"tldr": "本文提出SPR（空间偏好奖励）方法，通过奖励MLLM生成更详细、定位更精确的响应，并结合语义和定位分数进行直接偏好优化，以增强其细粒度空间理解能力。", "motivation": "多模态大语言模型（MLLMs）在空间理解方面表现出色，但在细粒度空间感知（如生成详细区域描述或准确识别物体位置）方面仍有不足，且常无法满足用户对细粒度空间理解的要求。现有方法主要通过预标注数据注入空间知识，但缺乏对MLLM实际响应的直接监督。", "method": "本文提出SPR（空间偏好奖励）方法，通过奖励MLLM的详细且具有精确物体定位的响应（而非模糊或不准确的响应）来增强其空间能力。SPR引入语义和定位分数，全面评估MLLM生成的区域描述的文本质量和定位质量。它还通过优化提高MLLM描述的定位准确性，并将得分最高的优化结果与得分最低的初始描述配对，用于直接偏好优化，从而增强与视觉输入的细粒度对齐。", "result": "在标准指代和定位基准测试上的大量实验表明，SPR能有效提升MLLM的空间理解能力，且训练开销极小。", "conclusion": "SPR是一种有效且高效的方法，通过基于空间偏好奖励直接监督和优化MLLM的响应，从而增强其细粒度空间理解能力。"}}
{"id": "2510.13873", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13873", "abs": "https://arxiv.org/abs/2510.13873", "authors": ["Johann Pignat", "Milena Vucetic", "Christophe Gaudet-Blavignac", "Jamil Zaghir", "Amandine Stettler", "Fanny Amrein", "Jonatan Bonjour", "Jean-Philippe Goldman", "Olivier Michielin", "Christian Lovis", "Mina Bjelogrlic"], "title": "FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation", "comment": null, "summary": "Developing natural language processing tools for clinical text requires\nannotated datasets, yet French oncology resources remain scarce. We present\nFRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated\ncorpus of 1301 synthetic French clinical cases, initially translated from the\nSpanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is\nannotated with terms related to morphology, topography, and histologic\ndifferentiation, using the International Classification of Diseases for\nOncology (ICD-O) as reference. An additional annotation layer captures\ncomposite expression-level normalisations that combine multiple ICD-O elements\ninto unified clinical concepts. Annotation quality was ensured through expert\nreview: 1301 texts were manually annotated for entity spans by two domain\nexperts. A total of 71127 ICD-O normalisations were produced through a\ncombination of automated matching and manual validation by a team of five\nannotators. The final dataset representing 399 unique morphology codes (from\n2549 different expressions), 272 topography codes (from 3143 different\nexpressions), and 2043 unique composite expressions (from 11144 different\nexpressions). This dataset provides a reference standard for named entity\nrecognition and concept normalisation in French oncology texts.", "AI": {"tldr": "FRACCO是一个专家标注的法语临床肿瘤语料库，包含1301个合成病例，用于命名实体识别和概念标准化，涵盖形态、部位和组织学分化，并提供复合表达层面的标准化。", "motivation": "开发法语临床文本的自然语言处理工具需要标注数据集，但法语肿瘤学资源目前非常稀缺。", "method": "该研究开发了FRACCO语料库，包含1301个从西班牙CANTEMIST语料库翻译而来的合成法语临床病例。语料库使用国际肿瘤疾病分类（ICD-O）作为参考，由专家标注形态、部位和组织学分化相关术语。此外，还增加了一个标注层，捕获结合多个ICD-O元素的复合表达层面的标准化。通过两名领域专家手动标注实体跨度以及五名标注员结合自动化匹配和手动验证生成ICD-O标准化，确保了标注质量。", "result": "最终数据集包含71127个ICD-O标准化，其中包含399个独特的形态学代码（来自2549个不同表达）、272个部位代码（来自3143个不同表达）和2043个独特的复合表达（来自11144个不同表达）。", "conclusion": "FRACCO数据集为法语肿瘤文本中的命名实体识别和概念标准化提供了重要的参考标准。"}}
{"id": "2510.14830", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14830", "abs": "https://arxiv.org/abs/2510.14830", "authors": ["Kun Lei", "Huanyu Li", "Dongjie Yu", "Zhenyu Wei", "Lingxiao Guo", "Zhennan Jiang", "Ziyu Wang", "Shiyu Liang", "Huazhe Xu"], "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning", "comment": "https://lei-kun.github.io/RL-100/", "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.", "AI": {"tldr": "RL-100是一个基于扩散视觉运动策略的真实世界强化学习框架，通过三阶段训练和一致性蒸馏，实现了100%成功率、高效率和鲁棒性，适用于多种机器人任务。", "motivation": "真实世界的机器人操作（如家庭和工厂环境）需要达到或超越熟练人类操作员的可靠性、效率和鲁棒性。", "method": "RL-100框架基于扩散视觉运动策略（通过监督学习训练），采用三阶段训练流程：首先是模仿学习利用人类先验知识；其次是迭代离线强化学习，使用离线策略评估（OPE）门控PPO风格的更新，以实现保守可靠的改进；最后是在线强化学习消除残余故障模式。此外，引入了一个轻量级一致性蒸馏头，将扩散的多步采样过程压缩为单步策略，显著降低了控制延迟。该框架是任务、具身和表示无关的，支持3D点云和2D RGB输入，以及多种机器人平台和策略类型。", "result": "RL-100在七个真实机器人任务（包括动态刚体控制、流体/颗粒倾倒、可变形布料折叠、精确灵巧拧螺丝和多阶段榨汁）上进行了评估，所有900个评估回合均达到100%成功率，其中一个任务连续成功250次。该方法实现了接近人类遥操作或更好的时间效率，并展示了长达两小时不间断操作的多小时鲁棒性。", "conclusion": "RL-100框架在真实世界机器人操作中取得了卓越的性能，通过其三阶段训练和创新蒸馏技术，实现了前所未有的可靠性、效率和鲁棒性，使其成为解决复杂机器人任务的有效方案。"}}
{"id": "2510.14512", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14512", "abs": "https://arxiv.org/abs/2510.14512", "authors": ["Haoyuan Li", "Mathias Funk", "Aaqib Saeed"], "title": "Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration", "comment": null, "summary": "Federated Learning (FL) offers a powerful paradigm for training models on\ndecentralized data, but its promise is often undermined by the immense\ncomplexity of designing and deploying robust systems. The need to select,\ncombine, and tune strategies for multifaceted challenges like data\nheterogeneity and system constraints has become a critical bottleneck,\nresulting in brittle, bespoke solutions. To address this, we introduce\nHelmsman, a novel multi-agent system that automates the end-to-end synthesis of\nfederated learning systems from high-level user specifications. It emulates a\nprincipled research and development workflow through three collaborative\nphases: (1) interactive human-in-the-loop planning to formulate a sound\nresearch plan, (2) modular code generation by supervised agent teams, and (3) a\nclosed-loop of autonomous evaluation and refinement in a sandboxed simulation\nenvironment. To facilitate rigorous evaluation, we also introduce\nAgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess\nthe system-level generation capabilities of agentic systems in FL. Extensive\nexperiments demonstrate that our approach generates solutions competitive with,\nand often superior to, established hand-crafted baselines. Our work represents\na significant step towards the automated engineering of complex decentralized\nAI systems.", "AI": {"tldr": "本文提出Helmsman，一个多智能体系统，用于从高级用户规范自动端到端合成联邦学习（FL）系统，并通过AgentFL-Bench基准进行评估，其生成的解决方案性能优于或媲美手工设计方案。", "motivation": "联邦学习（FL）系统设计和部署极其复杂，面临数据异构性和系统限制等多方面挑战，导致解决方案脆弱且定制化，成为关键瓶颈。", "method": "引入Helmsman，一个多智能体系统，通过三个协作阶段自动化FL系统合成：1) 交互式人机协同规划；2) 监督智能体团队进行模块化代码生成；3) 在沙盒模拟环境中进行自主评估和优化。同时，提出了AgentFL-Bench，一个包含16个任务的新基准，用于评估智能体系统在FL中的系统级生成能力。", "result": "广泛的实验表明，Helmsman生成了与现有手工设计基线具有竞争力，且通常更优的解决方案。", "conclusion": "这项工作代表了复杂去中心化AI系统自动化工程的重大进展。"}}
{"id": "2510.13876", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13876", "abs": "https://arxiv.org/abs/2510.13876", "authors": ["Filipe Laitenberger", "Dawid Kopiczko", "Cees G. M. Snoek", "Yuki M. Asano"], "title": "What Layers When: Learning to Skip Compute in LLMs with Residual Gates", "comment": "Preprint", "summary": "We introduce GateSkip, a simple residual-stream gating mechanism that enables\ntoken-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is\nequipped with a sigmoid-linear gate that condenses the branch's output before\nit re-enters the residual stream. During inference we rank tokens by the gate\nvalues and skip low-importance ones using a per-layer budget. While early-exit\nor router-based Mixture-of-Depths models are known to be unstable and need\nextensive retraining, our smooth, differentiable gates fine-tune stably on top\nof pretrained models. On long-form reasoning, we save up to 15\\% compute while\nretaining over 90\\% of baseline accuracy. On instruction-tuned models we see\naccuracy gains at full compute and match baseline quality near 50\\% savings.\nThe learned gates give insight into transformer information flow (e.g., BOS\ntokens act as anchors), and the method combines easily with quantization,\npruning, and self-speculative decoding.", "AI": {"tldr": "GateSkip是一种用于解码器LMs的残差流门控机制，通过sigmoid-线性门控实现逐token的层跳过，可在保留高准确率的同时显著节省计算量，并提供模型信息流洞察。", "motivation": "现有的早期退出或基于路由器的MoD模型不稳定且需要大量重新训练。研究者旨在开发一种稳定、可微的机制，以实现解码器LMs中逐token的层跳过。", "method": "引入GateSkip机制，为每个Attention/MLP分支配备一个sigmoid-线性门，用于在输出重新进入残差流之前对其进行精简。在推理时，根据门值对token进行排序，并使用每层预算跳过重要性较低的token。这些平滑、可微的门可以在预训练模型上稳定地进行微调。", "result": "在长篇推理任务中，GateSkip节省了高达15%的计算量，同时保持了90%以上的基线准确率。在指令调优模型上，在全计算量下实现了准确率提升，并在接近50%的计算量节省下达到了基线质量。学习到的门还提供了对Transformer信息流的洞察（例如，BOS token充当锚点），并且该方法易于与量化、剪枝和自推测解码结合。", "conclusion": "GateSkip提供了一种稳定且有效的方法，可以在解码器LMs中实现逐token的层跳过，显著节省计算资源，并揭示了Transformer的信息流模式，同时具有良好的兼容性和扩展性。"}}
{"id": "2510.14412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14412", "abs": "https://arxiv.org/abs/2510.14412", "authors": ["Claudia Grundke", "Gabriele Röger"], "title": "Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms", "comment": "Extended version of a paper of the same title presented at the joint\n  KR/ICAPS 2025 workshop \"KRPlan: Knowledge Representation Meets Automated\n  Planning\"", "summary": "Axioms are a feature of the Planning Domain Definition Language PDDL that can\nbe considered as a generalization of database query languages such as Datalog.\nThe PDDL standard restricts negative occurrences of predicates in axiom bodies\nto predicates that are directly set by actions and not derived by axioms. In\nthe literature, authors often deviate from this limitation and only require\nthat the set of axioms is stratifiable. Both variants can express exactly the\nsame queries as least fixed-point logic, indicating that negative occurrences\nof derived predicates can be eliminated. We present the corresponding\ntransformation.", "AI": {"tldr": "PDDL公理是Datalog的推广。PDDL标准限制公理体中负谓词的出现，而文献中常放宽至可分层。本文指出这两种变体都等价于最小不动点逻辑，表明派生谓词的负出现可被消除，并提出了相应的转换方法。", "motivation": "PDDL标准对公理体中负谓词的限制与文献中的常见做法（要求可分层）存在差异。研究者希望明确这两种变体的表达能力是否相同，以及如何消除派生谓词的负出现。", "method": "提出了一种转换方法，用于消除PDDL公理中派生谓词的负出现。", "result": "PDDL公理的两种变体（标准限制和文献中的可分层要求）都与最小不动点逻辑具有完全相同的查询表达能力。这表明派生谓词的负出现是可以被消除的，并且本文提出了实现这一消除的转换方法。", "conclusion": "PDDL公理中派生谓词的负出现可以通过所提出的转换方法消除。无论是遵循PDDL标准还是仅要求公理集可分层，其表达能力都与最小不动点逻辑相同，意味着这两种处理负谓词的方式在理论表达力上是等价的。"}}
{"id": "2510.14851", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14851", "abs": "https://arxiv.org/abs/2510.14851", "authors": ["Jakob Bichler", "Andreu Matoses Gimenez", "Javier Alonso-Mora"], "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time", "comment": "7 pages, 5 figures. 2025 IEEE Int. Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS 2025). Website and Code:\n  https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "AI": {"tldr": "Sadcher是一个基于模仿学习、图注意力网络和Transformer的实时任务分配框架，用于异构多机器人团队，能够处理动态联盟形成和任务优先级约束，并在实时操作中实现高性能和泛化能力。", "motivation": "为异构多机器人团队开发一个实时的任务分配框架，需要考虑动态联盟形成、任务优先级约束，并能够在不同的时空分布环境中进行泛化。", "method": "Sadcher通过模仿学习进行训练，结合图注意力网络和Transformer来预测机器人与任务之间的分配奖励。基于预测的奖励，一个松弛二分匹配步骤生成具有可行性保证的高质量调度。该方法明确建模了机器人和任务位置、任务持续时间以及机器人剩余处理时间，从而实现了先进的时空推理和泛化能力。", "result": "该方法能够扩展到更大的任务集和团队规模。在随机的、未见过的中小规模团队问题上，Sadcher优于其他基于学习和启发式的基线方法，并且计算时间适用于实时操作。研究还探索了基于采样的变体并评估了机器人和任务数量的扩展性。此外，作者发布了一个包含25万个最优调度的数据库。", "conclusion": "Sadcher提供了一个有效且实用的实时任务分配框架，适用于异构多机器人团队。它在性能、可扩展性和泛化能力方面表现出色，满足了实时操作的需求，并为未来的研究提供了宝贵的数据集。"}}
{"id": "2510.14537", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14537", "abs": "https://arxiv.org/abs/2510.14537", "authors": ["Emanuele Antonioni", "Stefan Markovic", "Anirudha Shankar", "Jaime Bernardo", "Lovro Markovic", "Silvia Pareti", "Benedetto Proietti"], "title": "JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol", "comment": null, "summary": "AI systems are continually evolving and advancing, and user expectations are\nconcurrently increasing, with a growing demand for interactions that go beyond\nsimple text-based interaction with Large Language Models (LLMs). Today's\napplications often require LLMs to interact with external tools, marking a\nshift toward more complex agentic systems. To support this, standards such as\nthe Model Context Protocol (MCP) have emerged, enabling agents to access tools\nby including a specification of the capabilities of each tool within the\nprompt. Although this approach expands what agents can do, it also introduces a\ngrowing problem: prompt bloating. As the number of tools increases, the prompts\nbecome longer, leading to high prompt token costs, increased latency, and\nreduced task success resulting from the selection of tools irrelevant to the\nprompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework\ndesigned to help agents manage prompt size more effectively when using large\nsets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and\nuses the user's prompt to identify and include only the most relevant tools,\nbased on both the query and the taxonomy structure. In this paper, we describe\nthe design of the taxonomy, the tool selection algorithm, and the dataset used\nto evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt\nsize without significantly compromising the agent's ability to respond\neffectively. As the number of available tools for the agent grows\nsubstantially, JSPLIT even improves the tool selection accuracy of the agent,\neffectively reducing costs while simultaneously improving task success in\nhigh-complexity agent environments.", "AI": {"tldr": "JSPLIT是一个分类驱动的框架，旨在帮助LLM代理在使用大量工具时更有效地管理提示大小，通过分层组织工具并仅选择最相关的工具来减少提示膨胀，从而降低成本并提高任务成功率。", "motivation": "随着AI系统和用户期望的不断提高，LLM需要与外部工具交互，转向更复杂的代理系统。Model Context Protocol (MCP)等标准支持代理访问工具，但随着工具数量的增加，提示变得冗长（提示膨胀），导致高昂的提示令牌成本、增加的延迟以及因选择不相关工具而降低的任务成功率。", "method": "本文提出了JSPLIT，一个分类驱动的框架。它将工具组织成一个分层的分类结构，并利用用户的提示（结合查询和分类结构）来识别并仅包含最相关的工具。论文描述了分类的设计、工具选择算法以及用于评估JSPLIT的数据集。", "result": "JSPLIT显著减少了提示大小，同时并未显著损害代理的有效响应能力。当代理可用的工具数量大幅增加时，JSPLIT甚至提高了代理的工具选择准确性，有效降低了成本，同时在高复杂性代理环境中提高了任务成功率。", "conclusion": "JSPLIT提供了一种有效管理大型MCP工具集提示大小的解决方案，通过智能的工具选择机制，不仅降低了运营成本，还提升了代理在复杂任务环境中的性能和成功率。"}}
{"id": "2510.13878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13878", "abs": "https://arxiv.org/abs/2510.13878", "authors": ["Jimin Lim", "Arjun Damerla", "Arthur Jiang", "Nam Le"], "title": "TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks", "comment": "COLM 2025 @ ORIGen Workshop", "summary": "Large language models (LLMs) have shown to be increasingly capable of\nperforming reasoning tasks, but their ability to make sequential decisions\nunder uncertainty only using natural language remains underexplored. We\nintroduce a novel benchmark in which LLMs interact with multi-armed bandit\nenvironments using purely textual feedback, \"you earned a token\", without\naccess to numerical cues or explicit probabilities, resulting in the model to\ninfer latent reward structures purely off linguistic cues and to adapt\naccordingly. We evaluated the performance of four open-source LLMs and compare\ntheir performance to standard decision-making algorithms such as Thompson\nSampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.\nWhile most of the LLMs underperformed compared to the baselines, Qwen3-4B,\nachieved the best-arm selection rate of 89.2% , which significantly\noutperformed both the larger LLMs and traditional methods. Our findings suggest\nthat probabilistic reasoning is able to emerge from language alone, and we\npresent this benchmark as a step towards evaluating decision-making\ncapabilities in naturalistic, non-numeric contexts.", "AI": {"tldr": "本研究探索大型语言模型（LLMs）在仅使用自然语言进行不确定性下序贯决策的能力。通过一个多臂老虎机文本交互基准，发现Qwen3-4B表现出色，超越其他LLMs和传统算法，表明LLMs能从语言中进行概率推理。", "motivation": "尽管大型语言模型（LLMs）在推理任务上表现出越来越强的能力，但它们在不确定性下仅使用自然语言进行序贯决策的能力仍未得到充分探索。", "method": "引入了一个新颖的基准，其中LLMs通过纯文本反馈（如“你获得了一个代币”）与多臂老虎机环境交互，不提供数值线索或明确概率，要求模型纯粹从语言线索中推断潜在奖励结构并进行适应。评估了四种开源LLMs的性能，并将其与Thompson Sampling、Epsilon Greedy、Upper Confidence Bound (UCB) 和随机选择等标准决策算法进行比较。", "result": "大多数LLMs的表现不如基线算法。然而，Qwen3-4B实现了89.2%的最佳臂选择率，显著优于其他大型LLMs和传统方法。", "conclusion": "研究结果表明，概率推理能够仅从语言中产生。该基准为评估LLMs在自然、非数值语境下的决策能力迈出了重要一步。"}}
{"id": "2510.14893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.", "AI": {"tldr": "本文提出了一种名为STITCHER的无优化轨迹规划框架，通过图搜索拼接短轨迹段，实现了实时、表达性强、接近最优的长距离轨迹生成，解决了传统优化方法在计算时间和数值稳定性上的挑战。", "motivation": "自动驾驶高速导航需要实时生成动态可行、无碰撞并满足状态或执行器约束的敏捷轨迹。现代轨迹规划主要依赖数值优化，但其严格的计算时间要求和数值不稳定风险限制了在安全关键场景中的应用。", "method": "本文提出了一种名为STITCHER的无优化规划框架。它通过图搜索将短轨迹段拼接起来，以实时计算长距离、表达性强且接近最优的轨迹。该框架通过创新的规划架构和多项算法发展，实现了实时规划。", "result": "STITCHER在几毫秒内即可为50米x50米的大型环境生成安全轨迹，并在仿真中表现优于两种最先进的优化规划器。通过定制四旋翼飞行器的硬件测试，验证了STITCHER能够实时生成可跟踪的路径，并尊重非凸约束（如倾斜角和电机力限制），这些约束在基于优化的规划器中难以包含。", "conclusion": "STITCHER提供了一种实时、高效且能处理复杂非凸约束的轨迹规划解决方案，优于传统的优化方法，特别适用于高速自主导航和安全关键场景。"}}
{"id": "2510.14383", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14383", "abs": "https://arxiv.org/abs/2510.14383", "authors": ["Danish Ali", "Ajmal Mian", "Naveed Akhtar", "Ghulam Mubashar Hassan"], "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights", "comment": null, "summary": "Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment. It is challenging due to the heterogeneity of tumor subregions.\nMamba-based State Space Models have demonstrated promising performance.\nHowever, they incur significant computational overhead due to sequential\nfeature computation across multiple spatial axes. Moreover, their robustness\nacross diverse BraTS data partitions remains largely unexplored, leaving a\ncritical gap in reliable evaluation. To address these limitations, we propose\ndual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation\nmodel that captures multi-scale long-range dependencies with minimal\ncomputational overhead. We leverage a space-filling curve to preserve spatial\nlocality during 3D-to-1D feature mapping, thereby reducing reliance on\ncomputationally expensive multi-axial feature scans. To enrich feature\nrepresentation, we propose a gated fusion module that adaptively integrates\nforward and reverse contexts, along with a quantization block that discretizes\nfeatures to improve robustness. In addition, we propose five systematic folds\non BraTS2023 for rigorous evaluation of segmentation techniques under diverse\nconditions and present detailed analysis of common failure scenarios. On the\n20\\% test set used by recent methods, our model achieves Dice improvements of\n0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor.\nEvaluations on the proposed systematic five folds demonstrate that our model\nmaintains competitive whole tumor accuracy while achieving clear average Dice\ngains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing\nstate-of-the-art. Furthermore, our model attains 15 times improvement in\nefficiency while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing approaches.", "AI": {"tldr": "该论文提出了一种名为双分辨率双向Mamba (DRBD-Mamba) 的高效3D脑肿瘤分割模型，它能以最小的计算开销捕获多尺度长程依赖，并在BraTS数据集上展现出更高的分割精度、鲁棒性和15倍的计算效率提升。", "motivation": "脑肿瘤分割对临床诊断和治疗至关重要，但由于肿瘤亚区域的异质性而极具挑战性。现有的基于Mamba的状态空间模型虽然性能有前景，但存在显著的计算开销，并且在不同BraTS数据分区下的鲁棒性尚未得到充分探索。", "method": "研究者提出了DRBD-Mamba模型，通过以下方法解决问题：1) 利用空间填充曲线在3D到1D特征映射过程中保留空间局部性，减少对计算昂贵的多轴特征扫描的依赖。2) 提出门控融合模块，自适应地整合前向和反向上下文，以丰富特征表示。3) 引入量化块，离散化特征以提高鲁棒性。4) 在BraTS2023上提出了五种系统性的折叠评估方法，用于在多样化条件下严格评估分割技术。", "result": "在最近方法使用的20%测试集上，DRBD-Mamba在全肿瘤、肿瘤核心和增强肿瘤的Dice分数上分别提高了0.10%、1.75%和0.93%。在提出的系统性五折评估中，模型在全肿瘤精度上保持竞争力，并在肿瘤核心和增强肿瘤的平均Dice分数上分别比现有最先进方法提高了0.86%和1.45%。此外，模型在保持高分割精度的同时实现了15倍的效率提升。", "conclusion": "DRBD-Mamba模型在脑肿瘤分割任务中展现出卓越的鲁棒性、计算优势和高精度，有效解决了现有Mamba模型计算开销大和鲁棒性评估不足的问题，为临床应用提供了更高效可靠的工具。"}}
{"id": "2510.14403", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14403", "abs": "https://arxiv.org/abs/2510.14403", "authors": ["Chao Tu", "Kun Huang", "Jie Zhang", "Qianjin Feng", "Yu Zhang", "Zhenyuan Ning"], "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis", "comment": null, "summary": "The burgeoning discipline of computational pathology shows promise in\nharnessing whole slide images (WSIs) to quantify morphological heterogeneity\nand develop objective prognostic modes for human cancers. However, progress is\nimpeded by the computational bottleneck of gigapixel-size inputs and the\nscarcity of dense manual annotations. Current methods often overlook\nfine-grained information across multi-magnification WSIs and variations in\ntumor microenvironments. Here, we propose an easy-to-hard progressive\nrepresentation learning model, termed dual-curriculum contrastive\nmulti-instance learning (DCMIL), to efficiently process WSIs for cancer\nprognosis. The model does not rely on dense annotations and enables the direct\ntransformation of gigapixel-size WSIs into outcome predictions. Extensive\nexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)\ndemonstrate that DCMIL outperforms standard WSI-based prognostic models.\nAdditionally, DCMIL identifies fine-grained prognosis-salient regions, provides\nrobust instance uncertainty estimation, and captures morphological differences\nbetween normal and tumor tissues, with the potential to generate new biological\ninsights. All codes have been made publicly accessible at\nhttps://github.com/tuuuc/DCMIL.", "AI": {"tldr": "本文提出了一种名为DCMIL的双课程对比多实例学习模型，旨在高效处理吉像素级全玻片图像（WSIs）进行癌症预后预测，解决了计算瓶颈和稀疏标注问题，并在多癌种上取得了优异性能。", "motivation": "计算病理学在利用全玻片图像（WSIs）量化形态异质性和开发癌症客观预后模型方面潜力巨大。然而，吉像素级输入造成的计算瓶颈和密集手动标注的稀缺性阻碍了其进展。现有方法常忽略多放大倍数WSI中的细粒度信息以及肿瘤微环境的变化。", "method": "提出了一种名为双课程对比多实例学习（DCMIL）的“由易到难”渐进式表示学习模型。该模型不依赖密集标注，能够将吉像素级WSI直接转换为预后预测。它通过双课程对比学习和多实例学习框架来高效处理数据。", "result": "在12种癌症类型（5,954名患者，1254万张切片）上的广泛实验表明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL能够识别细粒度的预后显著区域，提供鲁棒的实例不确定性估计，并捕捉正常和肿瘤组织之间的形态差异。", "conclusion": "DCMIL是一种高效且无需密集标注的癌症预后模型，它克服了吉像素级WSI处理的计算瓶颈和标注稀缺问题，并能识别关键区域，提供不确定性估计，有望产生新的生物学见解。"}}
{"id": "2510.14902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14902", "abs": "https://arxiv.org/abs/2510.14902", "authors": ["Han Zhao", "Jiaxuan Zhang", "Wenxuan Song", "Pengxiang Ding", "Donglin Wang"], "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation", "comment": null, "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.", "AI": {"tldr": "本文提出了一种名为 VLA^2 的新型智能体框架，通过整合网络检索和物体检测等外部模块，有效提升了现有视觉-语言-动作 (VLA) 模型在处理训练数据之外的未知物体时的泛化能力。", "motivation": "当前的 VLA 模型虽然在多任务处理和指令泛化方面表现出色，但在遇到训练数据中未出现过的物体概念（如未见过的物体描述和纹理）时，其成功率会显著下降，面临泛化失败问题。", "method": "研究者提出了 VLA^2 框架，该框架以 OpenVLA 为执行骨干，并有效利用网络检索和物体检测等外部模块，为 VLA 模型提供关于目标物体的视觉和文本知识。此外，他们还在 LIBERO 仿真环境中引入了新物体和物体描述，构建了一个包含三个难度级别的新评估基准来测试方法的有效性。", "result": "VLA^2 框架在设计的困难级别泛化基准上成功超越了当前最先进的模型。与独立的 OpenVLA 基线相比，VLA^2 在困难级别基准上的成功率提高了 44.2%，在所有定制环境中的平均成功率提高了 20.2%，并且在域内任务上没有性能下降。", "conclusion": "VLA^2 框架通过利用外部知识模块，成功解决了 VLA 模型在处理分布外物体时的泛化失败问题，显著提升了模型在未知物体操作任务中的表现，且不影响域内性能。"}}
{"id": "2510.14389", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14389", "abs": "https://arxiv.org/abs/2510.14389", "authors": ["Brandon Hill", "Kma Solaiman"], "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble", "comment": "This paper has been submitted to IEEE/CVF WACV 2026 Applications\n  track and is currently under review", "summary": "Motherboard defect detection is critical for ensuring reliability in\nhigh-volume electronics manufacturing. While prior research in PCB inspection\nhas largely targeted bare-board or trace-level defects, assembly-level\ninspection of full motherboards inspection remains underexplored. In this work,\nwe present BoardVision, a reproducible framework for detecting assembly-level\ndefects such as missing screws, loose fan wiring, and surface scratches. We\nbenchmark two representative detectors - YOLOv7 and Faster R-CNN, under\ncontrolled conditions on the MiracleFactory motherboard dataset, providing the\nfirst systematic comparison in this domain. To mitigate the limitations of\nsingle models, where YOLO excels in precision but underperforms in recall and\nFaster R-CNN shows the reverse, we propose a lightweight ensemble,\nConfidence-Temporal Voting (CTV Voter), that balances precision and recall\nthrough interpretable rules. We further evaluate robustness under realistic\nperturbations including sharpness, brightness, and orientation changes,\nhighlighting stability challenges often overlooked in motherboard defect\ndetection. Finally, we release a deployable GUI-driven inspection tool that\nbridges research evaluation with operator usability. Together, these\ncontributions demonstrate how computer vision techniques can transition from\nbenchmark results to practical quality assurance for assembly-level motherboard\nmanufacturing.", "AI": {"tldr": "本文提出了BoardVision框架，用于检测主板装配级缺陷（如螺丝缺失、风扇接线松动），比较了YOLOv7和Faster R-CNN，并引入了结合两者优点的CTV Voter集成方法。研究还评估了模型在实际扰动下的鲁棒性，并发布了用户友好的检测工具，旨在将计算机视觉技术应用于实际主板质量保证。", "motivation": "主板缺陷检测对于高产量电子产品制造的可靠性至关重要。以往的PCB检测研究主要集中在裸板或走线级缺陷，而对完整主板的装配级检测仍未得到充分探索。", "method": "本文提出了可复现的BoardVision框架，用于检测装配级缺陷。在MiracleFactory主板数据集上，系统性比较了YOLOv7和Faster R-CNN两种代表性检测器。为平衡精度和召回率，提出了一种轻量级集成方法——置信度-时间投票（CTV Voter）。此外，还在清晰度、亮度、方向变化等实际扰动下评估了模型的鲁棒性，并发布了一个可部署的GUI驱动检测工具。", "result": "研究发现，YOLOv7在精度方面表现出色但召回率不足，而Faster R-CNN则相反。所提出的CTV Voter集成方法能有效平衡精度和召回率。结果还突出了主板缺陷检测中常被忽视的稳定性挑战。最终，证明了计算机视觉技术可以从基准测试结果过渡到装配级主板制造的实际质量保证。", "conclusion": "计算机视觉技术能够有效地应用于装配级主板制造的实际质量保证。通过BoardVision框架、CTV Voter集成方法、鲁棒性评估和可部署的检测工具，本研究成功地将计算机视觉从基准测试结果转化为实用的生产线质量控制解决方案。"}}
{"id": "2510.14538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14538", "abs": "https://arxiv.org/abs/2510.14538", "authors": ["Emanuele Marconato", "Samuele Bortolotti", "Emile van Krieken", "Paolo Morettin", "Elena Umili", "Antonio Vergari", "Efthymia Tsamoura", "Andrea Passerini", "Stefano Teso"], "title": "Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts", "comment": null, "summary": "Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose\npredictions comply with prior knowledge encoding, e.g. safety or structural\nconstraints. As such, it represents one of the most promising avenues for\nreliable and trustworthy AI. The core idea behind NeSy AI is to combine neural\nand symbolic steps: neural networks are typically responsible for mapping\nlow-level inputs into high-level symbolic concepts, while symbolic reasoning\ninfers predictions compatible with the extracted concepts and the prior\nknowledge. Despite their promise, it was recently shown that - whenever the\nconcepts are not supervised directly - NeSy models can be affected by Reasoning\nShortcuts (RSs). That is, they can achieve high label accuracy by grounding the\nconcepts incorrectly. RSs can compromise the interpretability of the model's\nexplanations, performance in out-of-distribution scenarios, and therefore\nreliability. At the same time, RSs are difficult to detect and prevent unless\nconcept supervision is available, which is typically not the case. However, the\nliterature on RSs is scattered, making it difficult for researchers and\npractitioners to understand and tackle this challenging problem. This overview\naddresses this issue by providing a gentle introduction to RSs, discussing\ntheir causes and consequences in intuitive terms. It also reviews and\nelucidates existing theoretical characterizations of this phenomenon. Finally,\nit details methods for dealing with RSs, including mitigation and awareness\nstrategies, and maps their benefits and limitations. By reformulating advanced\nmaterial in a digestible form, this overview aims to provide a unifying\nperspective on RSs to lower the bar to entry for tackling them. Ultimately, we\nhope this overview contributes to the development of reliable NeSy and\ntrustworthy AI models.", "AI": {"tldr": "本文概述了神经符号（NeSy）AI中推理捷径（Reasoning Shortcuts, RSs）的问题，讨论了其成因、后果、理论表征以及应对策略，旨在为研究人员提供一个统一的理解框架。", "motivation": "神经符号AI虽有潜力，但当概念未被直接监督时，易受推理捷径影响，导致概念接地不正确，从而损害模型解释性、泛化能力和可靠性。由于相关文献分散且难以检测和预防，因此需要一个全面的概述来帮助研究人员理解和解决这一挑战。", "method": "本文通过以下方式解决问题：1) 温和地介绍推理捷径的概念；2) 讨论其直观的成因和后果；3) 回顾和阐明现有现象的理论表征；4) 详细介绍处理推理捷径的方法，包括缓解和意识策略，并分析其优缺点。通过重新组织高级材料，提供易于理解的形式。", "result": "本文提供了一个关于推理捷径的统一视角，将分散的文献进行整合，使高级材料变得易于理解，从而降低了解决这一问题的门槛。它为研究人员和从业者提供了理解和应对推理捷径的全面工具集。", "conclusion": "通过提供对推理捷径的统一理解和应对策略，本文旨在促进可靠的神经符号AI和值得信赖的AI模型的开发。"}}
{"id": "2510.13880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13880", "abs": "https://arxiv.org/abs/2510.13880", "authors": ["Mauro Jose Pacchiotti", "Luciana Ballejos", "Mariel Ale"], "title": "PAGE: Prompt Augmentation for text Generation Enhancement", "comment": "in Spanish language", "summary": "In recent years, natural language generative models have shown outstanding\nperformance in text generation tasks. However, when facing specific tasks or\nparticular requirements, they may exhibit poor performance or require\nadjustments that demand large amounts of additional data. This work introduces\nPAGE (Prompt Augmentation for text Generation Enhancement), a framework\ndesigned to assist these models through the use of simple auxiliary modules.\nThese modules, lightweight models such as classifiers or extractors, provide\ninferences from the input text. The output of these auxiliaries is then used to\nconstruct an enriched input that improves the quality and controllability of\nthe generation. Unlike other generation-assistance approaches, PAGE does not\nrequire auxiliary generative models; instead, it proposes a simpler, modular\narchitecture that is easy to adapt to different tasks. This paper presents the\nproposal, its components and architecture, and reports a proof of concept in\nthe domain of requirements engineering, where an auxiliary module with a\nclassifier is used to improve the quality of software requirements generation.", "AI": {"tldr": "PAGE是一个通过使用轻量级辅助模块（如分类器或提取器）来增强自然语言生成模型性能和可控性的框架，无需额外的生成模型。", "motivation": "近年来自然语言生成模型表现出色，但在特定任务或要求下可能表现不佳，或需要大量额外数据进行调整。", "method": "PAGE框架通过简单的辅助模块（如分类器或提取器）从输入文本中提供推断。这些辅助模块的输出被用于构建一个丰富化的输入，以提高生成质量和可控性。它采用模块化架构，不依赖辅助生成模型。", "result": "在需求工程领域进行了概念验证，使用一个带有分类器的辅助模块成功提高了软件需求生成的质量。", "conclusion": "PAGE提供了一种简单、模块化且易于适应不同任务的架构，通过轻量级辅助模块提高了文本生成的质量和可控性，并在需求工程中得到了初步验证。"}}
{"id": "2510.14548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14548", "abs": "https://arxiv.org/abs/2510.14548", "authors": ["Asen Nachkov", "Xi Wang", "Luc Van Gool"], "title": "LLM Agents Beyond Utility: An Open-Ended Perspective", "comment": null, "summary": "Recent LLM agents have made great use of chain of thought reasoning and\nfunction calling. As their capabilities grow, an important question arises: can\nthis software represent not only a smart problem-solving tool, but an entity in\nits own right, that can plan, design immediate tasks, and reason toward\nbroader, more ambiguous goals? To study this question, we adopt an open-ended\nexperimental setting where we augment a pretrained LLM agent with the ability\nto generate its own tasks, accumulate knowledge, and interact extensively with\nits environment. We study the resulting open-ended agent qualitatively. It can\nreliably follow complex multi-step instructions, store and reuse information\nacross runs, and propose and solve its own tasks, though it remains sensitive\nto prompt design, prone to repetitive task generation, and unable to form\nself-representations. These findings illustrate both the promise and current\nlimits of adapting pretrained LLMs toward open-endedness, and point to future\ndirections for training agents to manage memory, explore productively, and\npursue abstract long-term goals.", "AI": {"tldr": "研究了预训练LLM智能体在开放式环境中规划、设计任务和实现模糊目标的能力，发现其有潜力但仍存在对提示敏感、任务重复和缺乏自我表征等局限性。", "motivation": "随着LLM智能体能力的增长，研究者想知道它们是否能超越智能问题解决工具的范畴，成为能够规划、设计即时任务并推理实现更广泛、模糊目标的实体。", "method": "采用开放式实验设置，增强预训练LLM智能体生成自身任务、积累知识并与环境广泛交互的能力，并对其进行定性研究。", "result": "该智能体能够可靠地遵循复杂多步指令，跨运行存储和重用信息，并提出和解决自身任务。然而，它仍然对提示设计敏感，容易生成重复任务，并且无法形成自我表征。", "conclusion": "这些发现揭示了将预训练LLM应用于开放式任务的潜力和当前局限性，并指明了未来在训练智能体管理记忆、进行有效探索和追求抽象长期目标方面的方向。"}}
{"id": "2510.13884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13884", "abs": "https://arxiv.org/abs/2510.13884", "authors": ["Bolei Ma", "Yong Cao", "Indira Sen", "Anna-Carolina Haensch", "Frauke Kreuter", "Barbara Plank", "Daniel Hershcovich"], "title": "Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate public opinion\nand other social phenomena. Most current studies constrain these simulations to\nmultiple-choice or short-answer formats for ease of scoring and comparison, but\nsuch closed designs overlook the inherently generative nature of LLMs. In this\nposition paper, we argue that open-endedness, using free-form text that\ncaptures topics, viewpoints, and reasoning processes \"in\" LLMs, is essential\nfor realistic social simulation. Drawing on decades of survey-methodology\nresearch and recent advances in NLP, we argue why this open-endedness is\nvaluable in LLM social simulations, showing how it can improve measurement and\ndesign, support exploration of unanticipated views, and reduce\nresearcher-imposed directive bias. It also captures expressiveness and\nindividuality, aids in pretesting, and ultimately enhances methodological\nutility. We call for novel practices and evaluation frameworks that leverage\nrather than constrain the open-ended generative diversity of LLMs, creating\nsynergies between NLP and social science.", "AI": {"tldr": "本文认为，大型语言模型（LLMs）在社会模拟中应采用开放式而非封闭式设计，以充分利用其生成能力，提高模拟的真实性和效用。", "motivation": "当前LLMs在社会模拟中多采用多项选择或简答等封闭式格式，便于评分和比较，但这忽视了LLMs固有的生成性，限制了模拟的真实性。", "method": "这是一篇立场论文，作者借鉴了数十年的调查方法学研究和自然语言处理（NLP）的最新进展，论证了在LLM社会模拟中开放式设计（使用自由文本捕捉LLMs中的主题、观点和推理过程）的价值。", "result": "开放式设计可以改善测量和设计，支持探索意想不到的观点，减少研究者强加的指导性偏见。它还能捕捉表达性和个体性，有助于预测试，并最终增强方法学效用。", "conclusion": "论文呼吁开发新的实践和评估框架，以利用而非限制LLMs的开放式生成多样性，从而在NLP和社会科学之间创造协同效应。"}}
{"id": "2510.13879", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13879", "abs": "https://arxiv.org/abs/2510.13879", "authors": ["Alexandre Galashov", "Matt Jones", "Rosemary Ke", "Yuan Cao", "Vaishnavh Nagarajan", "Michael C. Mozer"], "title": "Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production", "comment": null, "summary": "We explore a class of supervised training objectives that allow a language\nmodel to dynamically and autonomously scale the number of compute steps used\nfor each input token. For any token, the model can request additional compute\nsteps by emitting a <don't know> output. If the model is granted a delay, a\nspecialized <pause> token is inserted at the next input step, providing the\nmodel with additional compute resources to generate an output. The model can\nrequest multiple pauses. To train the model to use <don't know> outputs\njudiciously and to calibrate its uncertainty, we frame the selection of each\noutput token as a sequential-decision problem with a time cost. We refer to the\nclass of methods as $\\textit{Catch Your Breath}$ losses and we study three\nmethods in this class: CYB-AP frames the model's task as anytime prediction,\nwhere an output may be required at any step and accuracy is discounted over\ntime; CYB-VA is a variational approach that aims to maximize prediction\naccuracy subject to a specified distribution over stopping times; and CYB-DP\nimposes a penalty based on a computational budget. Through fine-tuning\nexperiments, we identify the best performing loss variant. The CYB model needs\nonly one third as much training data as the baseline (no pause) model needs to\nachieve the same performance, and half as much data as a model with pauses and\na cross-entropy loss. We find that the CYB model requests additional steps when\ndoing so improves accuracy, and the model adapts its processing time to\ntoken-level complexity and context. For example, it often pauses after plural\nnouns like $\\textit{patients}$ and $\\textit{challenges}$ but never pauses after\nthe first token of contracted words like $\\textit{wasn}$ and $\\textit{didn}$,\nand it shows high variability for ambiguous tokens like $\\textit{won}$, which\ncould function as either a verb or part of a contraction.", "AI": {"tldr": "本研究探索了一类名为“Catch Your Breath”（CYB）的监督训练目标，使语言模型能够动态调整每个输入token的计算步数，通过发出<don't know>标记请求暂停。实验表明，CYB模型只需更少的训练数据即可达到相同性能，并且能根据token的复杂性和上下文自适应地调整处理时间。", "motivation": "研究动机在于使语言模型能够动态、自主地调整每个输入token的计算步数，并训练模型明智地使用<don't know>输出，以校准其不确定性。", "method": "模型通过发出<don't know>输出请求额外的计算步数，成功后会在下一个输入步插入一个<pause>标记。研究将每个输出token的选择框定为一个具有时间成本的序列决策问题。提出了“Catch Your Breath”损失函数，并研究了三种变体：CYB-AP（随时预测，准确性随时间衰减）、CYB-VA（变分方法，最大化预测准确性并受限于停止时间分布）和CYB-DP（基于计算预算的惩罚）。通过微调实验评估了这些方法的性能。", "result": "最佳的CYB模型仅需基线模型（无暂停）三分之一的训练数据即可达到相同性能，并且只需带暂停和交叉熵损失模型一半的数据。CYB模型会在提高准确性时请求额外步骤，并且能根据token级别的复杂性和上下文调整其处理时间。例如，在“patients”和“challenges”等复数名词后经常暂停，但在“wasn”和“didn”等缩略词的首个token后从不暂停，对“won”等歧义token显示出高变异性。", "conclusion": "CYB损失函数能够有效地训练语言模型动态地分配计算资源，从而显著减少训练数据需求，提高效率，并使模型能够自适应地处理不同复杂度的token和上下文。"}}
{"id": "2510.14930", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14930", "abs": "https://arxiv.org/abs/2510.14930", "authors": ["Binghao Huang", "Jie Xu", "Iretiayo Akinola", "Wei Yang", "Balakumar Sundaralingam", "Rowland O'Flaherty", "Dieter Fox", "Xiaolong Wang", "Arsalan Mousavian", "Yu-Wei Chao", "Yunzhu Li"], "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin", "comment": "Accepted by 9th Conference on Robot Learning (CoRL 2025); Website:\n  https://binghao-huang.github.io/vt_refine/", "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.", "AI": {"tldr": "VT-Refine是一个视觉-触觉策略学习框架，它结合了真实世界演示、高保真触觉模拟和强化学习，旨在解决机器人精确、接触丰富的双臂装配任务，克服了行为克隆的局限性。", "motivation": "人类擅长通过适应丰富的触觉反馈来完成双臂装配任务，但机器人仅通过行为克隆难以复制这种能力，因为人类演示存在次优性和多样性有限的问题。", "method": "该方法首先使用同步的视觉和触觉输入，在少量演示数据上训练一个扩散策略。然后，将该策略转移到一个配备模拟触觉传感器的高保真数字孪生体中，并通过大规模强化学习进行进一步优化，以增强鲁棒性和泛化能力。为实现准确的虚实迁移，该研究利用高分辨率压阻式触觉传感器提供法向力信号，并通过GPU加速模拟进行逼真建模。", "result": "实验结果表明，VT-Refine通过增加数据多样性和实现更有效的策略微调，显著提高了模拟和真实世界中的装配性能。", "conclusion": "VT-Refine框架通过结合真实演示、高保真触觉模拟和强化学习，为精确、接触密集的双臂装配任务提供了一种有效的解决方案，显著提升了机器人的装配能力和适应性。"}}
{"id": "2510.14460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14460", "abs": "https://arxiv.org/abs/2510.14460", "authors": ["Sven Jacob", "Weijia Shao", "Gjergji Kasneci"], "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences", "comment": "Accepted at GCPR 2025 (German Conference on Pattern Recognition).\n  This is a different version as submitted to the conference, not the official\n  conference proceedings", "summary": "Video-based object detection plays a vital role in safety-critical\napplications. While deep learning-based object detectors have achieved\nimpressive performance, they remain vulnerable to adversarial attacks,\nparticularly those involving universal perturbations. In this work, we propose\na minimally distorted universal adversarial attack tailored for video object\ndetection, which leverages nuclear norm regularization to promote structured\nperturbations concentrated in the background. To optimize this formulation\nefficiently, we employ an adaptive, optimistic exponentiated gradient method\nthat enhances both scalability and convergence. Our results demonstrate that\nthe proposed attack outperforms both low-rank projected gradient descent and\nFrank-Wolfe based attacks in effectiveness while maintaining high stealthiness.\nAll code and data are publicly available at\nhttps://github.com/jsve96/AO-Exp-Attack.", "AI": {"tldr": "本文提出了一种针对视频目标检测的通用对抗性攻击，通过核范数正则化生成集中在背景的结构化扰动，并利用自适应乐观指数梯度法进行优化，实现了更高的攻击有效性和隐蔽性。", "motivation": "深度学习目标检测器在视频应用中表现出色，但在安全关键场景下，它们仍然容易受到对抗性攻击，特别是通用扰动攻击的威胁。", "method": "本文提出了一种针对视频目标检测的最小失真通用对抗性攻击。它利用核范数正则化来促使扰动集中在背景中，形成结构化扰动。为高效优化此公式，研究采用了自适应、乐观的指数梯度法，以提高可扩展性和收敛性。", "result": "实验结果表明，所提出的攻击方法在有效性方面优于低秩投影梯度下降和基于Frank-Wolfe的攻击，同时保持了较高的隐蔽性。", "conclusion": "本文成功开发了一种针对视频目标检测的有效且隐蔽的通用对抗性攻击，通过核范数正则化和优化的自适应指数梯度法，能够生成集中在背景的结构化扰动，从而对深度学习检测器构成威胁。"}}
{"id": "2510.14431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14431", "abs": "https://arxiv.org/abs/2510.14431", "authors": ["Hui Xiang", "Yifan Bian", "Li Li", "Jingran Wu", "Xianguo Zhang", "Dong Liu"], "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding", "comment": "10 pages", "summary": "Neural video compression (NVC) technologies have advanced rapidly in recent\nyears, yielding state-of-the-art schemes such as DCVC-RT that offer superior\ncompression efficiency to H.266/VVC and real-time encoding/decoding\ncapabilities. Nonetheless, existing NVC schemes have several limitations,\nincluding inefficiency in dealing with disocclusion and new content, interframe\nerror propagation and accumulation, among others. To eliminate these\nlimitations, we borrow the idea from classic video coding schemes, which allow\nintra coding within inter-coded frames. With the intra coding tool enabled,\ndisocclusion and new content are properly handled, and interframe error\npropagation is naturally intercepted without the need for manual refresh\nmechanisms. We present an NVC framework with unified intra and inter coding,\nwhere every frame is processed by a single model that is trained to perform\nintra/inter coding adaptively. Moreover, we propose a simultaneous two-frame\ncompression design to exploit interframe redundancy not only forwardly but also\nbackwardly. Experimental results show that our scheme outperforms DCVC-RT by an\naverage of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality\nper frame, and retains real-time encoding/decoding performances. Code and\nmodels will be released.", "AI": {"tldr": "本研究提出了一种统一的帧内/帧间神经视频编码（NVC）框架，通过借鉴经典视频编码的帧内编码思想和引入同时双帧压缩设计，有效解决了现有NVC在处理遮挡解除、新内容和误差传播方面的局限性，实现了比DCVC-RT更优的压缩效率和更稳定的性能，同时保持实时编码/解码能力。", "motivation": "现有的神经视频压缩（NVC）方案存在多项局限性，包括在处理遮挡解除和新内容时的低效性，以及帧间误差的传播和累积问题。", "method": "1. 借鉴经典视频编码方案，在帧间编码帧中引入帧内编码能力，以有效处理遮挡解除、新内容并自然地拦截帧间误差传播。2. 提出一个统一的帧内/帧间编码NVC框架，其中单个模型经过训练可自适应地执行帧内/帧间编码。3. 引入同时双帧压缩设计，以同时利用前向和后向的帧间冗余。", "result": "实验结果表明，该方案在BD-rate上平均比DCVC-RT降低10.7%，每帧的比特率和质量更稳定，并保持了实时编码/解码性能。", "conclusion": "通过引入统一的帧内/帧间编码和同时双帧压缩设计，本研究提出的NVC框架成功克服了现有NVC的局限性，显著提升了压缩效率和稳定性，同时维持了实时性能，为未来的NVC发展提供了新的方向。"}}
{"id": "2510.14952", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.", "AI": {"tldr": "RoboGhost是一个无需重定向的框架，通过直接将人形机器人策略与基于语言的运动潜在空间关联，实现了更直接、更可靠的语言引导人形机器人运动，显著减少延迟并提高性能。", "motivation": "现有的语言引导人形机器人运动流程（解码、重定向、跟踪）繁琐、不可靠，容易产生累积误差、高延迟，并导致语义与控制之间的耦合薄弱。因此，需要一种更直接的语言到动作路径，以消除脆弱的中间阶段。", "method": "RoboGhost是一个无需重定向的框架，它直接将人形机器人策略与基于语言的运动潜在空间关联。它使用基于扩散的策略直接从噪声中去噪可执行动作，并采用混合因果Transformer-扩散运动生成器来确保长期一致性、稳定性和多样性，从而产生丰富的潜在表示。", "result": "RoboGhost显著降低了部署延迟，提高了成功率和跟踪精度，并在真实人形机器人上生成了平滑、语义对齐的运动。此外，该框架自然地扩展到图像、音频和音乐等其他模态。", "conclusion": "RoboGhost通过提供一个直接、无需重定向的语言到动作路径，为视觉-语言-动作人形机器人系统奠定了通用基础，克服了先前多阶段方法的局限性，实现了更高效、更可靠的人形机器人控制。"}}
{"id": "2510.14621", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14621", "abs": "https://arxiv.org/abs/2510.14621", "authors": ["Yuanyi Song", "Heyuan Huang", "Qiqiang Lin", "Yin Zhao", "Xiangmou Qu", "Jun Wang", "Xingyu Lou", "Weiwen Liu", "Zhuosheng Zhang", "Jun Wang", "Yong Yu", "Weinan Zhang", "Zhaoxiang Wang"], "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks", "comment": null, "summary": "The rapid advancement of multimodal large language models has enabled agents\nto operate mobile devices by directly interacting with graphical user\ninterfaces, opening new possibilities for mobile automation. However,\nreal-world mobile tasks are often complex and allow for multiple valid\nsolutions. This contradicts current mobile agent evaluation standards: offline\nstatic benchmarks can only validate a single predefined \"golden path\", while\nonline dynamic testing is constrained by the complexity and non-reproducibility\nof real devices, making both approaches inadequate for comprehensively\nassessing agent capabilities. To bridge the gap between offline and online\nevaluation and enhance testing stability, this paper introduces a novel\ngraph-structured benchmarking framework. By modeling the finite states observed\nduring real-device interactions, it achieves static simulation of dynamic\nbehaviors. Building on this, we develop ColorBench, a benchmark focused on\ncomplex long-horizon tasks. It supports evaluation of multiple valid solutions,\nsubtask completion rate statistics, and atomic-level capability analysis.\nColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average\nlength of over 13 steps. Each task includes at least two correct paths and\nseveral typical error paths, enabling quasi-dynamic interaction. By evaluating\nColorBench across various baselines, we discover limitations of existing models\nand propose improvement directions and feasible technical pathways to enhance\nagents' performance on complex, long-horizon problems based on experimental\nresults. Code and data are available at:\nhttps://github.com/MadeAgents/ColorBench.", "AI": {"tldr": "本文提出了一种名为ColorBench的图结构基准测试框架，用于评估移动设备上的多模态大型语言模型代理，解决了现有评估方法无法处理复杂、多解任务的问题，并揭示了现有模型的局限性。", "motivation": "多模态大型语言模型使代理能够通过图形用户界面操作移动设备，开辟了移动自动化的新可能性。然而，现实世界的移动任务往往复杂且存在多种有效解决方案。这与当前的移动代理评估标准相矛盾：离线静态基准只能验证单一预定义的“黄金路径”，而在线动态测试则受限于真实设备的复杂性和不可复现性，这两种方法都无法全面评估代理的能力。", "method": "本文引入了一种新颖的图结构基准测试框架，通过建模真实设备交互中观察到的有限状态，实现了动态行为的静态模拟。在此基础上，开发了ColorBench，一个专注于复杂长周期任务的基准。它支持评估多个有效解决方案、子任务完成率统计和原子级能力分析。ColorBench包含175个任务（74个单应用，101个跨应用），平均长度超过13步，每个任务至少包含两条正确路径和几条典型的错误路径，实现了准动态交互。", "result": "ColorBench包含175个任务（74个单应用，101个跨应用），平均长度超过13步，每个任务至少包含两条正确路径和几条典型的错误路径。通过对ColorBench上各种基线的评估，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术路径，以增强代理在复杂、长周期问题上的性能。", "conclusion": "ColorBench基准测试框架有效弥补了离线和在线评估之间的差距，增强了测试稳定性，并能够全面评估移动代理在复杂、多解任务中的能力。实验结果揭示了现有模型的不足，并为未来改进代理在复杂长周期问题上的性能提供了指导和技术路径。"}}
{"id": "2509.26255", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.", "AI": {"tldr": "本文提出了一种抽象世界模型框架，通过学习符号状态表示和内生/外生机制的因果过程，解决包含外生过程的长期具身规划挑战，实现了快速且泛化能力强的规划。", "motivation": "长期具身规划面临挑战，因为世界不仅通过智能体的行动而改变，外生过程（如水加热、多米诺骨牌效应）也会与智能体的行动同时发生，这使得规划变得复杂。", "method": "提出了一种抽象世界模型框架，联合学习 (i) 符号状态表示 和 (ii) 内生动作和外生机制的因果过程。每个因果过程都模拟了随机因果关系的时间进程。模型通过变分贝叶斯推断结合大型语言模型（LLM）的提议，从有限数据中学习。", "result": "在五个模拟桌面机器人环境中，学习到的模型实现了快速规划，并能泛化到包含更多对象和更复杂目标的未见任务，性能优于多种基线方法。", "conclusion": "所提出的抽象世界模型能够有效处理包含外生过程的长期具身规划问题，通过学习符号表示和因果过程，实现了高效、泛化能力强的规划能力。"}}
{"id": "2510.14462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14462", "abs": "https://arxiv.org/abs/2510.14462", "authors": ["Youwan Mahé", "Elise Bannier", "Stéphanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review", "comment": null, "summary": "Unsupervised deep generative models are emerging as a promising alternative\nto supervised methods for detecting and segmenting anomalies in brain imaging.\nUnlike fully supervised approaches, which require large voxel-level annotated\ndatasets and are limited to well-characterised pathologies, these models can be\ntrained exclusively on healthy data and identify anomalies as deviations from\nlearned normative brain structures. This PRISMA-guided scoping review\nsynthesises recent work on unsupervised deep generative models for anomaly\ndetection in neuroimaging, including autoencoders, variational autoencoders,\ngenerative adversarial networks, and denoising diffusion models. A total of 49\nstudies published between 2018 - 2025 were identified, covering applications to\nbrain MRI and, less frequently, CT across diverse pathologies such as tumours,\nstroke, multiple sclerosis, and small vessel disease. Reported performance\nmetrics are compared alongside architectural design choices. Across the\nincluded studies, generative models achieved encouraging performance for large\nfocal lesions and demonstrated progress in addressing more subtle\nabnormalities. A key strength of generative models is their ability to produce\ninterpretable pseudo-healthy (also referred to as counterfactual)\nreconstructions, which is particularly valuable when annotated data are scarce,\nas in rare or heterogeneous diseases. Looking ahead, these models offer a\ncompelling direction for anomaly detection, enabling semi-supervised learning,\nsupporting the discovery of novel imaging biomarkers, and facilitating within-\nand cross-disease deviation mapping in unified end-to-end frameworks. To\nrealise clinical impact, future work should prioritise anatomy-aware modelling,\ndevelopment of foundation models, task-appropriate evaluation metrics, and\nrigorous clinical validation.", "AI": {"tldr": "本范围综述分析了无监督深度生成模型在脑部影像异常检测中的应用，涵盖了其方法、性能、优势以及未来发展方向。", "motivation": "传统的监督学习方法需要大量体素级别的标注数据，且受限于已知病理类型。无监督深度生成模型则能仅通过健康数据进行训练，识别与正常脑结构的学习偏差作为异常，为标注数据稀缺的罕见或异质性疾病提供了一种有前景的替代方案。", "method": "本研究采用PRISMA指导的范围综述方法，综合了2018年至2025年间发表的49项关于无监督深度生成模型在神经影像异常检测中的工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型。综述涵盖了脑部MRI和CT在肿瘤、中风、多发性硬化症和小血管病等多种病理学应用，并比较了报告的性能指标和架构设计选择。", "result": "生成模型在检测大型局灶性病变方面取得了令人鼓舞的性能，并在处理更细微的异常方面取得了进展。其主要优势在于能够生成可解释的伪健康（或反事实）重建图像，这对于标注数据稀缺的罕见或异质性疾病尤其有价值。", "conclusion": "无监督深度生成模型为异常检测提供了一个引人注目的方向，有助于半监督学习、新型影像生物标志物的发现以及统一的端到端框架中的疾病内和跨疾病偏差映射。未来工作应优先考虑解剖学感知建模、基础模型的开发、任务适用的评估指标以及严格的临床验证，以实现临床影响。"}}
{"id": "2510.14463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14463", "abs": "https://arxiv.org/abs/2510.14463", "authors": ["Thomas Katraouras", "Dimitrios Rafailidis"], "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration", "comment": "Accepted at WI-IAT 2025", "summary": "Image quality is a critical factor in delivering visually appealing content\non web platforms. However, images often suffer from degradation due to lossy\noperations applied by online social networks (OSNs), negatively affecting user\nexperience. Image restoration is the process of recovering a clean high-quality\nimage from a given degraded input. Recently, multi-task (all-in-one) image\nrestoration models have gained significant attention, due to their ability to\nsimultaneously handle different types of image degradations. However, these\nmodels often come with an excessively high number of trainable parameters,\nmaking them computationally inefficient. In this paper, we propose a strategy\nfor compressing multi-task image restoration models. We aim to discover highly\nsparse subnetworks within overparameterized deep models that can match or even\nsurpass the performance of their dense counterparts. The proposed model, namely\nMIR-L, utilizes an iterative pruning strategy that removes low-magnitude\nweights across multiple rounds, while resetting the remaining weights to their\noriginal initialization. This iterative process is important for the multi-task\nimage restoration model's optimization, effectively uncovering \"winning\ntickets\" that maintain or exceed state-of-the-art performance at high sparsity\nlevels. Experimental evaluation on benchmark datasets for the deraining,\ndehazing, and denoising tasks shows that MIR-L retains only 10% of the\ntrainable parameters while maintaining high image restoration performance. Our\ncode, datasets and pre-trained models are made publicly available at\nhttps://github.com/Thomkat/MIR-L.", "AI": {"tldr": "本文提出了一种名为MIR-L的迭代剪枝策略，用于压缩多任务图像恢复模型，通过发现稀疏子网络，在大幅减少参数的同时保持甚至超越原始模型的性能。", "motivation": "图像在在线社交网络中常因有损操作而降级，影响用户体验。多任务图像恢复模型能有效处理多种降级，但参数过多导致计算效率低下，需要更高效的模型。", "method": "本文提出了MIR-L模型，采用迭代剪枝策略。该策略分多轮移除低幅值权重，并将剩余权重重置为初始值。这种迭代过程旨在发现能够保持或超越最先进性能的“中奖彩票”（winning tickets），即便在高度稀疏的情况下。", "result": "在去雨、去雾和去噪等基准数据集上的实验评估表明，MIR-L模型仅保留了10%的可训练参数，却依然保持了高水平的图像恢复性能。", "conclusion": "所提出的MIR-L模型通过迭代剪枝策略，成功地压缩了多任务图像恢复模型，在显著减少计算量的同时，保持了卓越的图像恢复效果，解决了现有模型效率低下的问题。"}}
{"id": "2510.13885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13885", "abs": "https://arxiv.org/abs/2510.13885", "authors": ["Ariel Kamen"], "title": "Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization", "comment": "10 pages, 4 figures,", "summary": "This study presents a comparative evaluation of ten state-of-the-art large\nlanguage models (LLMs) applied to unstructured text categorization using the\nInteractive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis\nemployed a uniform dataset of 8,660 human-annotated samples and identical\nzero-shot prompts to ensure methodological consistency across all models.\nEvaluation metrics included four classic measures - accuracy, precision,\nrecall, and F1-score - and three LLM-specific indicators: hallucination ratio,\ninflation ratio, and categorization cost.\n  Results show that, despite their rapid advancement, contemporary LLMs achieve\nonly moderate classic performance, with average scores of 34% accuracy, 42%\nprecision, 45% recall, and 41% F1-score. Hallucination and inflation ratios\nreveal that models frequently overproduce categories relative to human\nannotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B\noffered the most favorable cost-to-performance balance, while GPT 120B\ndemonstrated the lowest hallucination ratio. The findings suggest that scaling\nand architectural improvements alone do not ensure better categorization\naccuracy, as the task requires compressing rich unstructured text into a\nlimited taxonomy - a process that challenges current model architectures.\n  To address these limitations, a separate ensemble-based approach was\ndeveloped and tested. The ensemble method, in which multiple LLMs act as\nindependent experts, substantially improved accuracy, reduced inflation, and\ncompletely eliminated hallucinations. These results indicate that coordinated\norchestration of models - rather than sheer scale - may represent the most\neffective path toward achieving or surpassing human-expert performance in\nlarge-scale text categorization.", "AI": {"tldr": "本研究比较评估了十个最先进的大型语言模型（LLMs）在非结构化文本分类任务上的表现，发现其独立性能中等，幻觉和膨胀率较高。然而，通过开发和测试基于集成（ensemble）的方法，模型性能显著提升，且完全消除了幻觉，表明模型协调而非单纯扩展规模是实现或超越人类专家水平的关键。", "motivation": "本研究旨在评估当前最先进的大型语言模型（LLMs）在复杂非结构化文本分类任务（使用IAB 2.2分层分类法）中的实际表现，包括其传统性能指标以及特有的幻觉、膨胀和成本问题，并探索克服其局限性的有效方法。", "method": "研究采用统一方法，使用包含8,660个人工标注样本的数据集和相同的零样本提示，对十个LLMs进行评估。评估指标包括准确率、精确率、召回率、F1-分数四项经典指标，以及幻觉率、膨胀率和分类成本三项LLM特定指标。此外，还开发并测试了一种基于集成的模型协调方法。", "result": "结果显示，LLMs的经典性能表现平平，平均准确率为34%，精确率为42%，召回率为45%，F1-分数为41%。模型普遍存在较高的幻觉和膨胀率。Gemini 1.5/2.0 Flash和GPT 20B/120B在成本效益方面表现最佳，GPT 120B的幻觉率最低。研究发现，单纯的规模扩展和架构改进并不能确保更高的分类准确性。然而，所开发的集成方法显著提高了准确性，降低了膨胀率，并完全消除了幻觉。", "conclusion": "当前LLMs在将丰富的非结构化文本压缩到有限分类法时面临挑战。研究表明，模型间的协调编排（如集成方法）而非单纯的规模扩展，可能是实现或超越人类专家在大规模文本分类任务中表现的最有效途径。"}}
{"id": "2510.14665", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14665", "abs": "https://arxiv.org/abs/2510.14665", "authors": ["Rikard Rosenbacke", "Carl Rosenbacke", "Victor Rosenbacke", "Martin McKee"], "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding.", "AI": {"tldr": "大型语言模型（LLMs）类似于人类的System 1认知，易产生幻觉。本文提出Rose-Frame框架，通过“地图vs领土”、“直觉vs理性”、“冲突vs确认”三个维度，诊断人机交互中的认知和认识论偏差，旨在促进更透明和批判性地部署AI。", "motivation": "LLMs日益深入人类交流和决策，但其输出基于统计预测而非推理，存在模糊性、偏见和缺乏真相访问的问题，并有产生幻觉的风险。作者认为LLMs大规模地操作System 1认知，缺乏反思和证伪能力。", "method": "引入Rose-Frame框架，这是一个三维工具，用于诊断人机交互中的认知和认识论漂移。其三个维度是：(i) 地图vs领土（区分现实的表征与现实本身）；(ii) 直觉vs理性（借鉴双过程理论，区分快速、情感判断与缓慢、反思性思维）；(iii) 冲突vs确认（审视想法是通过异议批判性测试还是通过相互验证简单强化）。", "result": "Rose-Frame框架不试图通过更多数据或规则修复LLMs，而是提供一个反思性工具，使模型的局限性和用户的假设都变得可见，从而实现更透明和批判性地部署AI。它将AI对齐重新定义为认知治理：无论是人类还是人工的直觉，都必须由人类理性来管理。", "conclusion": "为了使机器的流畅性与人类的理解对齐，我们必须嵌入反思性、可证伪的监督机制，确保人类理性能够治理人工智能的直觉。"}}
{"id": "2510.14669", "categories": ["cs.AI", "68T01, 68T09, 62P10 68T01, 68T09, 62P10", "I.2.6; I.5.4; H.2.8; J.3; K.4.1; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.14669", "abs": "https://arxiv.org/abs/2510.14669", "authors": ["Sara Altamirano", "Arjan Vreeken", "Sennay Ghebreab"], "title": "Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review", "comment": "Extended version of the paper accepted at the AAAI/ACM Conference on\n  AI, Ethics, and Society (AIES 2025), including an appendix. 10 pages, 2\n  figures", "summary": "Machine learning (ML) promises to revolutionize public health through\nimproved surveillance, risk stratification, and resource allocation. However,\nwithout systematic attention to algorithmic bias, ML may inadvertently\nreinforce existing health disparities. We present a systematic literature\nreview of algorithmic bias identification, discussion, and reporting in Dutch\npublic health ML research from 2021 to 2025. To this end, we developed the Risk\nof Algorithmic Bias Assessment Tool (RABAT) by integrating elements from\nestablished frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible\nAI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals\npervasive gaps: although data sampling and missing data practices are well\ndocumented, most studies omit explicit fairness framing, subgroup analyses, and\ntransparent discussion of potential harms. In response, we introduce a\nfour-stage fairness-oriented framework called ACAR (Awareness,\nConceptualization, Application, Reporting), with guiding questions derived from\nour systematic literature review to help researchers address fairness across\nthe ML lifecycle. We conclude with actionable recommendations for public health\nML practitioners to consistently consider algorithmic bias and foster\ntransparency, ensuring that algorithmic innovations advance health equity\nrather than undermine it.", "AI": {"tldr": "本研究系统性回顾了2021-2025年荷兰公共卫生机器学习研究中的算法偏见识别、讨论和报告情况，发现普遍存在不足，并提出了ACAR公平导向框架和具体建议，以促进健康公平。", "motivation": "机器学习有望通过改进监测、风险分层和资源分配来革新公共卫生领域。然而，如果不对算法偏见进行系统性关注，机器学习可能会无意中加剧现有的健康不平等。", "method": "研究开发了“算法偏见风险评估工具”（RABAT），整合了Cochrane偏倚风险、PROBAST和微软负责任人工智能清单等框架元素，并将其应用于35项同行评审研究，进行了2021-2025年荷兰公共卫生机器学习研究中算法偏见的系统性文献回顾。在此基础上，提出了一个名为ACAR（意识、概念化、应用、报告）的四阶段公平导向框架。", "result": "分析揭示了普遍存在的不足：尽管数据抽样和缺失数据处理实践记录良好，但大多数研究忽略了明确的公平性框架、亚组分析以及对潜在危害的透明讨论。", "conclusion": "研究提出了ACAR公平导向框架，并提供了具体、可操作的建议，旨在帮助公共卫生机器学习从业者在整个机器学习生命周期中持续考虑算法偏见并提高透明度，确保算法创新能够促进而非损害健康公平。"}}
{"id": "2510.13890", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 68T07 (Secondary)", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.13890", "abs": "https://arxiv.org/abs/2510.13890", "authors": ["Fali Wang", "Jihai Chen", "Shuhua Yang", "Ali Al-Lawati", "Linli Tang", "Hui Liu", "Suhang Wang"], "title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness", "comment": "17 pages, 17 figures, under review", "summary": "Large language models (LLMs) have advanced many domains and applications but\nface high fine-tuning costs, inference latency, limited edge deployability, and\nreliability concerns. Small language models (SLMs), compact, efficient, and\nadaptable, offer complementary remedies. Recent work explores collaborative\nframeworks that fuse SLMs' specialization and efficiency with LLMs'\ngeneralization and reasoning to meet diverse objectives across tasks and\ndeployment scenarios. Motivated by these developments, this paper presents a\nsystematic survey of SLM-LLM collaboration organized by collaboration\nobjectives. We propose a taxonomy with four goals: performance enhancement,\ncost-effectiveness, cloud-edge privacy, and trustworthiness. Within this\nframework, we review representative methods, summarize design paradigms, and\noutline open challenges and future directions toward efficient, secure, and\nscalable SLM-LLM collaboration.", "AI": {"tldr": "本文对小型语言模型（SLM）与大型语言模型（LLM）的协作进行了系统性综述，旨在解决LLM的局限性，并提出了一个以协作目标为导向的分类法。", "motivation": "大型语言模型（LLM）面临高微调成本、推理延迟、边缘部署受限和可靠性问题。小型语言模型（SLM）因其紧凑、高效和适应性强，能提供补充解决方案。近期研究探索将SLM的专业化和效率与LLM的泛化和推理能力融合的协作框架，以应对不同任务和部署场景的需求。", "method": "本文进行了一项系统性调查，提出了一个基于协作目标的分类法，包含四个主要目标：性能提升、成本效益、云边隐私和可信度。在此框架内，文章回顾了代表性方法，总结了设计范式。", "result": "本文回顾了SLM-LLM协作的代表性方法，总结了其设计范式，并概述了实现高效、安全和可扩展的SLM-LLM协作所面临的开放挑战和未来发展方向。", "conclusion": "SLM-LLM协作是解决LLM局限性的有效途径，通过系统性地探索其协作目标、方法和挑战，可以推动更高效、安全和可扩展的语言模型应用。"}}
{"id": "2510.13888", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13888", "abs": "https://arxiv.org/abs/2510.13888", "authors": ["Wenjie Ma", "Andrei Cojocaru", "Neel Kolhe", "Bradley Louie", "Robin Said Sharif", "Haihan Zhang", "Vincent Zhuang", "Matei Zaharia", "Sewon Min"], "title": "Reliable Fine-Grained Evaluation of Natural Language Math Proofs", "comment": "31 pages, 6 figures, 10 tables", "summary": "Recent advances in large language models (LLMs) for mathematical reasoning\nhave largely focused on tasks with easily verifiable final answers; however,\ngenerating and verifying natural language math proofs remains an open\nchallenge. We identify the absence of a reliable, fine-grained evaluator for\nLLM-generated math proofs as a critical gap. To address this, we propose a\nsystematic methodology for developing and validating evaluators that assign\nfine-grained scores on a 0-7 scale to model-generated math proofs. To enable\nthis study, we introduce ProofBench, the first expert-annotated dataset of\nfine-grained proof ratings, spanning 145 problems from six major math\ncompetitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from\nGemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as\na testbed, we systematically explore the evaluator design space across key\naxes: the backbone model, input context, instructions and evaluation workflow.\nOur analysis delivers ProofGrader, an evaluator that combines a strong\nreasoning backbone LM, rich context from reference solutions and marking\nschemes, and a simple ensembling method; it achieves a low Mean Absolute Error\n(MAE) of 0.926 against expert scores, significantly outperforming naive\nbaselines. Finally, we demonstrate its practical utility in a best-of-$n$\nselection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out\nof 7), closing 78% of the gap between a naive binary evaluator (2.48) and the\nhuman oracle (4.62), highlighting its potential to advance downstream proof\ngeneration.", "AI": {"tldr": "本文提出了一种名为 ProofGrader 的细粒度评估器，用于评估大型语言模型（LLM）生成的数学证明。该评估器在专家标注数据集 ProofBench 上表现出色，实现了低平均绝对误差，并显著提升了证明选择任务的性能。", "motivation": "当前大型语言模型在数学推理方面主要关注易于验证最终答案的任务，但在生成和验证自然语言数学证明方面仍面临挑战。主要原因是缺乏可靠、细粒度的评估器来评估LLM生成的数学证明。", "method": "研究人员提出了一种系统方法来开发和验证对LLM生成数学证明进行0-7分细粒度评分的评估器。为此，他们构建了首个专家标注的细粒度证明评分数据集 ProofBench，包含来自六个主要数学竞赛的145个问题和来自Gemini-2.5-pro、o3和DeepSeek-R1的435个LLM生成解决方案。利用ProofBench，他们系统探索了评估器设计空间，包括骨干模型、输入上下文、指令和评估工作流程。最终，他们开发出 ProofGrader，它结合了强大的推理骨干语言模型、来自参考解决方案和评分方案的丰富上下文以及简单的集成方法。", "result": "ProofGrader 在专家评分上的平均绝对误差（MAE）为0.926，显著优于朴素基线。在最佳-n选择任务中（n=16），ProofGrader 的平均得分为4.14（满分7分），弥补了朴素二元评估器（2.48分）与人类专家（4.62分）之间78%的差距。", "conclusion": "ProofGrader 显著推进了LLM生成数学证明的细粒度评估，并展示了其在下游证明生成任务中的实际应用潜力，有望改进证明生成过程。"}}
{"id": "2510.14670", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14670", "abs": "https://arxiv.org/abs/2510.14670", "authors": ["Marco Simoni", "Aleksandar Fontana", "Andrea Saracino", "Paolo Mori"], "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence", "comment": null, "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that\nconnects natural-language cyber threat queries with executable reasoning over a\nstructured knowledge graph. It integrates a path planner model, which predicts\nlogical relation chains from text, and a graph executor that traverses the\nTITAN Ontology to retrieve factual answers and supporting evidence. Unlike\ntraditional retrieval systems, TITAN operates on a typed, bidirectional graph\nderived from MITRE, allowing reasoning to move clearly and reversibly between\nthreats, behaviors, and defenses. To support training and evaluation, we\nintroduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:\n13951) pairing natural language questions with executable reasoning paths and\nstep by step Chain of Thought explanations. Empirical evaluations show that\nTITAN enables models to generate syntactically valid and semantically coherent\nreasoning paths that can be deterministically executed on the underlying graph.", "AI": {"tldr": "TITAN是一个将自然语言网络威胁查询与结构化知识图谱上的可执行推理相结合的框架，它利用路径规划器和图执行器，基于MITRE衍生的双向图谱提供事实答案和证据。", "motivation": "传统的检索系统在处理网络威胁数据时存在局限性，无法实现清晰、可逆的威胁、行为和防御之间的推理。因此，需要一个能够将自然语言查询转化为可执行推理路径的系统。", "method": "TITAN框架整合了一个路径规划器模型（从文本预测逻辑关系链）和一个图执行器（遍历TITAN本体论以检索事实答案和支持证据）。它操作于一个从MITRE衍生的、类型化、双向的图谱。为支持训练和评估，研究引入了TITAN数据集，包含88209个自然语言问题与可执行推理路径对。", "result": "实证评估表明，TITAN使模型能够生成语法有效且语义连贯的推理路径，这些路径可以在底层图谱上确定性地执行。", "conclusion": "TITAN成功地提供了一个通过自动化导航进行威胁情报分析的框架，它能够将自然语言查询转化为可执行的知识图谱推理，并由一个大规模的新数据集支持其训练和评估。"}}
{"id": "2510.14828", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14828", "abs": "https://arxiv.org/abs/2510.14828", "authors": ["Jinrui Liu", "Bingyan Nie", "Boyu Li", "Yaran Chen", "Yuze Wang", "Shunsen He", "Haoran Li"], "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning", "comment": null, "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.", "AI": {"tldr": "本研究提出RoboGPT-R1，一个用于具身规划的两阶段微调框架，结合监督学习和强化学习，通过设计规则奖励函数显著提升了机器人处理复杂长周期操作任务的推理和物理理解能力。", "motivation": "现有基于SFT的大型语言模型和视觉语言模型在复杂真实世界环境中的长周期操作任务中，由于常识和推理能力受限，以及泛化性差和物理理解不足，难以成功执行。因此，需要改进具身智能体的推理能力以完成复杂的人类指令。", "method": "本研究提出了RoboGPT-R1，一个两阶段微调框架：第一阶段通过专家序列进行监督训练以获取基础知识；第二阶段利用强化学习（RL）解决模型在视觉空间理解和推理方面的不足。为实现多步推理任务中的物理理解和动作序列一致性，设计了一个规则奖励函数，同时考虑长周期性能和环境中的动作约束。", "result": "在EmbodiedBench基准测试中，RoboGPT-R1（基于Qwen2.5-VL-3B训练）的表现显著优于更大规模的模型GPT-4o-mini 21.33%，并超越了其他基于Qwen2.5-VL-7B训练的工作20.33%。", "conclusion": "RoboGPT-R1的两阶段微调框架及其设计的规则奖励函数，有效提升了具身智能体在长周期操作任务中的推理、视觉空间理解和物理理解能力，表现出卓越的性能，即使是较小规模的模型也能超越大型模型。"}}
{"id": "2510.14836", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14836", "abs": "https://arxiv.org/abs/2510.14836", "authors": ["Yixuan Li", "Yuhui Chen", "Mingcai Zhou", "Haoran Li"], "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models", "comment": null, "summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks.", "AI": {"tldr": "QDepth-VLA框架通过辅助深度预测任务增强视觉-语言-动作（VLA）模型，以提升其空间感知和精细操作能力。", "motivation": "现有VLA模型在精细操作任务中，缺乏对必要3D结构的理解和推理能力，难以实现精确控制。", "method": "提出QDepth-VLA框架，通过一个辅助深度预测任务来增强VLA模型。设计了一个专门的深度专家，预测由VQ-VAE编码器获取的深度图的量化潜在令牌，从而使模型学习到捕捉关键几何线索的深度感知表示。", "result": "在模拟基准和真实世界任务上的实验结果表明，QDepth-VLA展现出强大的空间推理能力和有竞争力的操作任务性能。", "conclusion": "QDepth-VLA成功地通过深度预测任务为VLA模型带来了重要的3D空间理解，显著提升了模型在操作任务中的表现。"}}
{"id": "2510.14493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14493", "abs": "https://arxiv.org/abs/2510.14493", "authors": ["Aleksis Pirinen", "Delia Fano Yela", "Smita Chakraborty", "Erik Källman"], "title": "Grazing Detection using Deep Learning and Sentinel-2 Time Series Data", "comment": "Code and models: https://github.com/aleksispi/pib-ml-grazing", "summary": "Grazing shapes both agricultural production and biodiversity, yet scalable\nmonitoring of where grazing occurs remains limited. We study seasonal grazing\ndetection from Sentinel-2 L2A time series: for each polygon-defined field\nboundary, April-October imagery is used for binary prediction (grazed / not\ngrazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance\nfeatures, and achieve an average F1 score of 77 percent across five validation\nsplits, with 90 percent recall on grazed pastures. Operationally, if inspectors\ncan visit at most 4 percent of sites annually, prioritising fields predicted by\nour model as non-grazed yields 17.2 times more confirmed non-grazing sites than\nrandom inspection. These results indicate that coarse-resolution, freely\navailable satellite data can reliably steer inspection resources for\nconservation-aligned land-use compliance. Code and models have been made\npublicly available.", "AI": {"tldr": "该研究利用Sentinel-2卫星数据和CNN-LSTM集成模型，实现了对季节性放牧的自动化检测，并能有效指导土地利用合规性检查。", "motivation": "放牧活动对农业生产和生物多样性都有重要影响，但目前缺乏可扩展的放牧监测方法。", "method": "研究使用Sentinel-2 L2A时间序列数据（4月至10月），针对每个多边形定义的田地边界，训练了一个CNN-LSTM模型集成。该模型基于多时相反射率特征进行二元预测（放牧/未放牧）。", "result": "模型在五个验证集上取得了平均77%的F1分数，对放牧牧场的召回率达到90%。在操作层面，如果每年只能检查4%的地点，优先检查模型预测为“未放牧”的田地，可以比随机检查多发现17.2倍的已确认未放牧地点。", "conclusion": "研究表明，粗分辨率、免费的卫星数据能够可靠地指导检查资源，以确保符合环境保护的土地利用合规性。"}}
{"id": "2510.13892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13892", "abs": "https://arxiv.org/abs/2510.13892", "authors": ["Zhaoyang Shang", "Sibo Wei", "Jianbin Guo", "Rui Zhou", "Lifeng Dong", "Yin Luo"], "title": "The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data", "comment": null, "summary": "Large Language Models (LLMs) excel in general tasks, but adapting them to\nspecialized domains relies on high-quality supervised fine-tuning (SFT) data.\nAlthough existing methods can identify subsets of high-quality data and reduce\ntraining cost to some extent, their selection process still suffers from\nover-reliance on LLMs' internal knowledge, weak interpretability, and limited\ngeneralization. To address these limitations, we propose THTB (The Harder The\nBetter), a cognitive science-inspired framework for instruction data selection\nand annotation guidance. THTB prioritizes higher-level cognitive instructions\nby combining quality filtering with intrinsic and extrinsic hardness scoring,\noffering interpretable and quantifiable criteria for efficient SFT, both in\ndata selection and annotation guidance. Experiments show that THTB enables\nmodels trained on only 5% of the data to outperform full-dataset training,\nwhile achieving superior generalization compared with LLM-only selection. In\naddition, THTB provides effective annotation guidance in vertical domains,\nenabling a model trained on just 2% of the data to surpass models trained on\nmuch larger datasets, demonstrating strong potential for domain adaptation. Our\ncode, datasets, and models are available on\nhttps://github.com/DYJG-research/THTB.", "AI": {"tldr": "THTB是一个受认知科学启发的框架，通过结合质量过滤和内外难度评分来优先选择高难度指令数据，从而实现高效的监督微调（SFT）数据选择和标注指导，在少量数据上训练的模型能超越全数据集训练并展现出更好的泛化能力。", "motivation": "大型语言模型（LLMs）在特定领域适应时依赖高质量的SFT数据。现有数据选择方法存在过度依赖LLM内部知识、可解释性弱和泛化能力有限的问题。", "method": "本文提出了THTB（The Harder The Better）框架，该框架受认知科学启发，通过结合质量过滤以及内在和外在难度评分来优先选择更高认知水平的指令。它为高效SFT提供了可解释和可量化的标准，用于数据选择和标注指导。", "result": "实验表明，THTB使得仅用5%数据训练的模型性能优于全数据集训练的模型，并且比仅依赖LLM选择的方法具有更强的泛化能力。此外，THTB在垂直领域提供了有效的标注指导，使得仅用2%数据训练的模型超越了使用更多数据训练的模型。", "conclusion": "THTB框架为高效SFT和领域适应提供了强大的潜力，通过可解释和可量化的标准改进了数据选择和标注指导。"}}
{"id": "2510.14516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14516", "abs": "https://arxiv.org/abs/2510.14516", "authors": ["Ali Kashefi", "Tapan Mukerji"], "title": "Vision Mamba for Permeability Prediction of Porous Media", "comment": null, "summary": "Vision Mamba has recently received attention as an alternative to Vision\nTransformers (ViTs) for image classification. The network size of Vision Mamba\nscales linearly with input image resolution, whereas ViTs scale quadratically,\na feature that improves computational and memory efficiency. Moreover, Vision\nMamba requires a significantly smaller number of trainable parameters than\ntraditional convolutional neural networks (CNNs), and thus, they can be more\nmemory efficient. Because of these features, we introduce, for the first time,\na neural network that uses Vision Mamba as its backbone for predicting the\npermeability of three-dimensional porous media. We compare the performance of\nVision Mamba with ViT and CNN models across multiple aspects of permeability\nprediction and perform an ablation study to assess the effects of its\ncomponents on accuracy. We demonstrate in practice the aforementioned\nadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction of\nthree-dimensional porous media. We make the source code publicly available to\nfacilitate reproducibility and to enable other researchers to build on and\nextend this work. We believe the proposed framework has the potential to be\nintegrated into large vision models in which Vision Mamba is used instead of\nViTs.", "AI": {"tldr": "本文首次将Vision Mamba作为骨干网络应用于三维多孔介质渗透率预测，并证明其在计算效率、内存效率和预测准确性方面优于Vision Transformer和卷积神经网络。", "motivation": "Vision Mamba因其与输入图像分辨率呈线性而非二次方关系的网络大小，以及比传统CNNs更少的参数，展现出卓越的计算和内存效率，这使其成为图像分类中Vision Transformer的潜在替代方案。作者受此启发，希望利用Vision Mamba的这些优势来解决三维多孔介质渗透率预测问题。", "method": "引入了一个以Vision Mamba为骨干的神经网络来预测三维多孔介质的渗透率。通过多方面比较Vision Mamba与Vision Transformer和CNN模型在渗透率预测上的性能。同时，进行了消融研究以评估Vision Mamba各组件对准确性的影响。", "result": "实践证明，Vision Mamba在三维多孔介质渗透率预测方面优于Vision Transformer和CNNs，验证了其在计算和内存效率上的优势。研究还通过消融实验评估了其组件对准确性的影响。", "conclusion": "所提出的基于Vision Mamba的框架在三维多孔介质渗透率预测中表现出色，具有将其集成到使用Vision Mamba而非Vision Transformer的大型视觉模型中的潜力。研究源代码已公开，以促进重现性和进一步研究。"}}
{"id": "2510.14676", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14676", "abs": "https://arxiv.org/abs/2510.14676", "authors": ["Bianca Maria Lerma", "Rafael Peñaloza"], "title": "NAEL: Non-Anthropocentric Ethical Logic", "comment": "Accepted to the FEAR workshop 2025", "summary": "We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical\nframework for artificial agents grounded in active inference and symbolic\nreasoning. Departing from conventional, human-centred approaches to AI ethics,\nNAEL formalizes ethical behaviour as an emergent property of intelligent\nsystems minimizing global expected free energy in dynamic, multi-agent\nenvironments. We propose a neuro-symbolic architecture to allow agents to\nevaluate the ethical consequences of their actions in uncertain settings. The\nproposed system addresses the limitations of existing ethical models by\nallowing agents to develop context-sensitive, adaptive, and relational ethical\nbehaviour without presupposing anthropomorphic moral intuitions. A case study\ninvolving ethical resource distribution illustrates NAEL's dynamic balancing of\nself-preservation, epistemic learning, and collective welfare.", "AI": {"tldr": "本文提出了NAEL（非人类中心伦理逻辑），一个基于主动推理和符号推理的新型AI伦理框架，旨在通过最小化全局预期自由能，使智能体在动态多智能体环境中发展出适应性、关系性的伦理行为，而非预设人类道德直觉。", "motivation": "传统的AI伦理方法以人类为中心，存在局限性。研究动机是开发一种能让智能体在不预设拟人化道德直觉的情况下，发展出情境敏感、适应性强且具有关系性的伦理行为的框架。", "method": "引入NAEL框架，将伦理行为形式化为智能体在动态、多智能体环境中最小化全局预期自由能的涌现特性。提出了一种神经-符号架构，使智能体能够在不确定环境中评估其行为的伦理后果。", "result": "NAEL框架使智能体能够发展出情境敏感、适应性强且具有关系性的伦理行为，克服了现有伦理模型的局限性。通过一个伦理资源分配的案例研究，展示了NAEL在自我保护、认知学习和集体福利之间进行动态平衡的能力。", "conclusion": "NAEL提供了一种非人类中心的AI伦理方法，通过结合主动推理和符号推理，使人工智能体能够发展出适应性强、情境敏感且关系性的伦理行为，从而在复杂环境中进行更有效的伦理决策。"}}
{"id": "2510.14976", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14976", "abs": "https://arxiv.org/abs/2510.14976", "authors": ["Shaowei Liu", "Chuan Guo", "Bing Zhou", "Jian Wang"], "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation", "comment": "Accepted to ICCV 2025. Project page:\n  https://stevenlsw.github.io/ponimator/", "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.", "AI": {"tldr": "Ponimator是一个基于近距离交互姿态的框架，利用两个条件扩散模型，实现多功能人际交互动画，包括从图像、文本或单人姿态生成交互动作序列。", "motivation": "人类能够从近距离人际交互姿态中直观地推断上下文并预测动态。受此启发，本文旨在构建一个能够实现类似功能的交互动画框架。", "method": "Ponimator框架包含两个条件扩散模型，利用了交互姿态的先验知识：1) 姿态动画器，利用时间先验从交互姿态生成动态运动序列；2) 姿态生成器，利用空间先验从单人姿态、文本或两者合成交互姿态。训练数据来自运动捕捉交互数据集中的近距离两人姿态及其周围的时间上下文。", "result": "Ponimator支持多种任务，包括基于图像的交互动画、反应动画和文本到交互的合成，有助于将高质量运动捕捉数据中的交互知识迁移到开放世界场景。实验证明了姿态先验的普适性以及该框架的有效性和鲁棒性。", "conclusion": "Ponimator是一个基于近距离交互姿态的简单而有效的框架，通过利用交互姿态先验和扩散模型，成功实现了多功能人际交互动画，并在多样化的数据集和应用中展现出其普遍性、有效性和鲁棒性。"}}
{"id": "2510.13893", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13893", "abs": "https://arxiv.org/abs/2510.13893", "authors": ["Olga E. Sorokoletova", "Francesco Giarrusso", "Vincenzo Suriani", "Daniele Nardi"], "title": "Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection", "comment": null, "summary": "Jailbreaking techniques pose a significant threat to the safety of Large\nLanguage Models (LLMs). Existing defenses typically focus on single-turn\nattacks, lack coverage across languages, and rely on limited taxonomies that\neither fail to capture the full diversity of attack strategies or emphasize\nrisk categories rather than the jailbreaking techniques. To advance the\nunderstanding of the effectiveness of jailbreaking techniques, we conducted a\nstructured red-teaming challenge. The outcome of our experiments are manifold.\nFirst, we developed a comprehensive hierarchical taxonomy of 50 jailbreak\nstrategies, consolidating and extending prior classifications into seven broad\nfamilies, including impersonation, persuasion, privilege escalation, cognitive\noverload, obfuscation, goal conflict, and data poisoning. Second, we analyzed\nthe data collected from the challenge to examine the prevalence and success\nrates of different attack types, providing insights into how specific jailbreak\nstrategies exploit model vulnerabilities and induce misalignment. Third, we\nbenchmark a popular LLM for jailbreak detection, evaluating the benefits of\ntaxonomy-guided prompting for improving automatic detection. Finally, we\ncompiled a new Italian dataset of 1364 multi-turn adversarial dialogues,\nannotated with our taxonomy, enabling the study of interactions where\nadversarial intent emerges gradually and succeeds in bypassing traditional\nsafeguards.", "AI": {"tldr": "该研究通过结构化红队挑战，开发了一个包含50种越狱策略的综合分层分类法，分析了不同攻击类型的流行度和成功率，并评估了分类法引导提示对自动检测的益处，同时编译了一个新的多轮意大利语对抗性对话数据集。", "motivation": "现有防御措施主要关注单轮攻击，缺乏跨语言覆盖，且依赖的分类法未能充分捕捉攻击策略的多样性或侧重于风险类别而非越狱技术，这阻碍了对越狱技术有效性的深入理解。", "method": "研究方法包括：1) 进行了结构化红队挑战；2) 开发了一个包含50种越狱策略的综合分层分类法，归纳为七大家族；3) 分析了挑战中收集的数据，以检查不同攻击类型的流行度和成功率；4) 基准测试了一个流行的LLM越狱检测器，评估了分类法引导提示对自动检测的益处；5) 编译了一个新的包含1364个多轮对抗性对话的意大利语数据集，并用所开发的分类法进行了标注。", "result": "研究成果包括：1) 提出了一个包含50种越狱策略的综合分层分类法，分为七大家族（如伪装、说服、权限提升等）；2) 分析了不同攻击类型的流行度和成功率，揭示了特定越狱策略如何利用模型漏洞并导致偏差；3) 评估了分类法引导提示在改进自动检测方面的有效性；4) 编译了一个新的意大利语多轮对抗性对话数据集，用于研究逐渐出现的对抗性意图和绕过传统防御的交互。", "conclusion": "本研究通过开发全面的越狱策略分类法、分析攻击有效性、改进自动检测方法以及构建新的多轮对抗性数据集，显著提升了对LLM越狱技术的理解和防御能力，特别是对多轮和跨语言攻击的应对。"}}
{"id": "2510.14525", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14525", "abs": "https://arxiv.org/abs/2510.14525", "authors": ["Qurrat Ul Ain", "Atif Aftab Ahmed Jilani", "Zunaira Shafqat", "Nigar Azhar Butt"], "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing", "comment": null, "summary": "Defective surgical instruments pose serious risks to sterility, mechanical\nintegrity, and patient safety, increasing the likelihood of surgical\ncomplications. However, quality control in surgical instrument manufacturing\noften relies on manual inspection, which is prone to human error and\ninconsistency. This study introduces SurgScan, an AI-powered defect detection\nframework for surgical instruments. Using YOLOv8, SurgScan classifies defects\nin real-time, ensuring high accuracy and industrial scalability. The model is\ntrained on a high-resolution dataset of 102,876 images, covering 11 instrument\ntypes and five major defect categories. Extensive evaluation against\nstate-of-the-art CNN architectures confirms that SurgScan achieves the highest\naccuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,\nmaking it suitable for industrial deployment. Statistical analysis demonstrates\nthat contrast-enhanced preprocessing significantly improves defect detection,\naddressing key limitations in visual inspection. SurgScan provides a scalable,\ncost-effective AI solution for automated quality control, reducing reliance on\nmanual inspection while ensuring compliance with ISO 13485 and FDA standards,\npaving the way for enhanced defect detection in medical manufacturing.", "AI": {"tldr": "本研究提出SurgScan，一个基于AI的医疗器械缺陷检测框架，利用YOLOv8实现高精度实时检测，以替代人工检查，提高质量控制效率和安全性。", "motivation": "有缺陷的手术器械会严重影响无菌性、机械完整性和患者安全，增加手术并发症的风险。然而，目前手术器械制造中的质量控制主要依赖人工检查，容易出现人为错误和不一致性。", "method": "本研究引入SurgScan框架，采用YOLOv8进行实时缺陷分类。模型在一个包含102,876张高分辨率图像的数据集上进行训练，涵盖11种器械类型和五种主要缺陷类别。研究还采用了对比度增强预处理技术。", "result": "SurgScan实现了99.3%的最高准确率，并具有4.2-5.8毫秒/图像的实时推理速度，优于现有最先进的CNN架构。统计分析表明，对比度增强预处理显著提高了缺陷检测能力。", "conclusion": "SurgScan提供了一个可扩展、经济高效的AI解决方案，用于自动化质量控制，减少对手动检查的依赖，同时确保符合ISO 13485和FDA标准，为医疗制造中的缺陷检测铺平了道路。"}}
{"id": "2510.13898", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13898", "abs": "https://arxiv.org/abs/2510.13898", "authors": ["Misam Abbas"], "title": "Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges", "comment": "Accepted for publication at the 2025 IEEE ICDM Workshop on \"Grounding\n  Documents with Reasoning, Agents, Retrieval, and Attribution\". This is author\n  submitted version. Not yet published", "summary": "Attributing authorship in the era of large language models (LLMs) is\nincreasingly challenging as machine-generated prose rivals human writing. We\nbenchmark two complementary attribution mechanisms , fixed Style Embeddings and\nan instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an\nopen dataset of 600 balanced instances spanning six domains (academic, news,\nfiction, blogs, spoken transcripts, and TV/movie scripts). Each instance\ncontains a human prompt with both a gold continuation and an LLM-generated\ncontinuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding\nbaseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.\n68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA\ncontinuations (85 pct vs. 81 pct) but the results are not statistically\nsignificant. Crucially, the LLM judge significantly outperforms in fiction and\nacademic prose, indicating semantic sensitivity, whereas embeddings dominate in\nspoken and scripted dialogue, reflecting structural strengths. These\ncomplementary patterns highlight attribution as a multidimensional problem\nrequiring hybrid strategies. To support reproducibility we provide code on\nGitHub and derived data on Hugging Face under the MIT license. This open\nframework provides a reproducible benchmark for attribution quality assessment\nin AI-generated content, along with a review of related literature influencing\nthis work.", "AI": {"tldr": "研究比较了固定风格嵌入和指令微调LLM（GPT-4o）在区分人类和LLM生成文本方面的表现，发现两者在不同LLM和文本领域有互补优势，表明需要混合归因策略。", "motivation": "大型语言模型（LLMs）生成的文本日益逼真，使得区分人类和机器创作的作者归属变得越来越困难，这促使研究者探索有效的归因机制。", "method": "研究使用了Human AI Parallel Corpus数据集，该数据集包含600个平衡实例，涵盖学术、新闻、小说等六个领域。每个实例包括人类提示、人类续写和由GPT-4o或LLaMA-70B-Instruct生成的LLM续写。研究对比了两种归因机制：固定风格嵌入（Style Embeddings）和指令微调的LLM判断器（GPT-4o）。", "result": "在GPT-4o生成的文本上，风格嵌入的总体准确率更高（82% vs. 68%）。在LLaMA生成的文本上，LLM判断器略优于风格嵌入（85% vs. 81%），但统计学上不显著。LLM判断器在小说和学术散文中表现显著更好，显示出语义敏感性；而风格嵌入在口语和剧本对话中占优，反映其结构性优势。", "conclusion": "作者归因是一个多维度问题，需要结合不同机制的混合策略。本研究提供的开放框架和基准测试有助于评估AI生成内容的归因质量，并强调了两种归因机制的互补性。"}}
{"id": "2510.14683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14683", "abs": "https://arxiv.org/abs/2510.14683", "authors": ["Devon Graham", "Kevin Leyton-Brown"], "title": "Practical, Utilitarian Algorithm Configuration", "comment": null, "summary": "Utilitarian algorithm configuration identifies a parameter setting for a\ngiven algorithm that maximizes a user's utility. Utility functions offer a\ntheoretically well-grounded approach to optimizing decision-making under\nuncertainty and are flexible enough to capture a user's preferences over\nalgorithm runtimes (e.g., they can describe a sharp cutoff after which a\nsolution is no longer required, a per-hour cost for compute, or diminishing\nreturns from algorithms that take longer to run). COUP is a recently-introduced\nutilitarian algorithm configuration procedure which was designed mainly to\noffer strong theoretical guarantees about the quality of the configuration it\nreturns, with less attention paid to its practical performance. This paper\ncloses that gap, bringing theoretically-grounded, utilitarian algorithm\nconfiguration to the point where it is competitive with widely used, heuristic\nconfiguration procedures that offer no performance guarantees. We present a\nseries of improvements to COUP that improve its empirical performance without\ndegrading its theoretical guarantees and demonstrate their benefit\nexperimentally. Using a case study, we also illustrate ways of exploring the\nrobustness of a given solution to the algorithm selection problem to variations\nin the utility function.", "AI": {"tldr": "本文通过一系列改进，显著提升了COUP（一种基于效用的算法配置程序）的实际性能，使其在保持理论保证的同时，能与主流启发式配置方法竞争。", "motivation": "COUP是一种新近提出的效用算法配置方法，其主要设计目标是提供关于配置质量的强大理论保证，但对其在实际应用中的性能关注较少。研究动机在于弥合这一理论与实践之间的差距，使COUP在实际应用中更具竞争力。", "method": "本文提出了一系列对COUP的改进措施。这些改进旨在提高其经验性能，同时不损害其原有的理论保证。此外，论文还通过案例研究，展示了探索给定算法选择问题解决方案对效用函数变化的鲁棒性的方法。", "result": "改进后的COUP在经验性能上取得了显著提升，使其能够与广泛使用的、不提供性能保证的启发式配置程序相媲美。实验证明了这些改进的有效性。通过案例研究，还展示了如何探索解决方案对效用函数变化的鲁棒性。", "conclusion": "通过一系列实用性改进，理论基础扎实的效用算法配置方法（COUP）现在已达到与主流启发式配置程序竞争的水平，且仍保持其理论保证。此外，还提供了探索解决方案鲁棒性的方法。"}}
{"id": "2510.14697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14697", "abs": "https://arxiv.org/abs/2510.14697", "authors": ["Bang An", "Yibo Yang", "Philip Torr", "Bernard Ghanem"], "title": "Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging", "comment": null, "summary": "Model merging aims to integrate task-specific abilities from individually\nfine-tuned models into a single model without extra training. In recent model\nmerging methods, task vector has become a fundamental building block, as it can\nencapsulate the residual information from finetuning. However, the merged model\noften suffers from notable performance degradation due to the conflicts caused\nby task-irrelevant redundancy in task vectors. Existing efforts in overcoming\nredundancy by randomly dropping elements in the parameter space involves\nrandomness and lacks knowledge awareness. To address these challenges, in this\nstudy, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.\nConcretely, we sample some training examples from each task, and feed them into\ntheir corresponding fine-tuned models to acquire the covariance matrices before\nlinear layers. We then perform a context-oriented singular value decomposition,\nwhich accentuates the weight components most relevant to the target knowledge.\nAs a result, we can split fine-tuned model weights into task-relevant and\nredundant components in the knowledge-aware subspace, and purify the task\nvector by pruning the redundant components. To induce fair pruning efforts\nacross models, we further introduce a spectral rank allocation strategy by\noptimizing a normalized activated pruning error. The task vector purification\nby our method as a plug-and-play scheme is applicable across various task\nvector-based merging methods to improve their performance. In experiments, we\ndemonstrate the effectiveness of PAVE across a diverse set of merging methods,\ntasks, and model architectures.", "AI": {"tldr": "模型合并中，任务向量因冗余导致性能下降。PAVE提出一种知识感知的子空间方法，通过上下文导向的奇异值分解和剪枝，净化任务向量，从而提高合并模型性能。", "motivation": "现有模型合并方法中，任务向量因包含任务无关的冗余信息而导致合并模型性能显著下降。现有通过随机丢弃元素来克服冗余的方法缺乏知识感知且具有随机性。", "method": "PAVE通过以下步骤净化任务向量：1) 从每个任务中采样训练样本，并输入到对应的微调模型以获取线性层前的协方差矩阵。2) 执行上下文导向的奇异值分解，以突出与目标知识最相关的权重分量。3) 在知识感知子空间中，将微调模型权重分解为任务相关和冗余部分，并通过剪枝冗余部分来净化任务向量。4) 引入谱秩分配策略，通过优化归一化激活剪枝误差，确保模型间公平的剪枝力度。PAVE作为即插即用方案，可应用于各种基于任务向量的合并方法。", "result": "实验证明，PAVE在多种合并方法、任务和模型架构上均能有效提升性能。", "conclusion": "PAVE通过在知识感知子空间中净化任务向量，有效解决了模型合并中因冗余导致的性能下降问题。它通过上下文导向的奇异值分解和公平的剪枝策略，作为一种即插即用的方案，显著提高了现有任务向量合并方法的性能。"}}
{"id": "2510.14526", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14526", "abs": "https://arxiv.org/abs/2510.14526", "authors": ["Yunze Tong", "Didi Zhu", "Zijing Hu", "Jinluan Yang", "Ziyu Zhao"], "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models", "comment": "Appendix will be appended soon", "summary": "In text-to-image generation, different initial noises induce distinct\ndenoising paths with a pretrained Stable Diffusion (SD) model. While this\npattern could output diverse images, some of them may fail to align well with\nthe prompt. Existing methods alleviate this issue either by altering the\ndenoising dynamics or by drawing multiple noises and conducting post-selection.\nIn this paper, we attribute the misalignment to a training-inference mismatch:\nduring training, prompt-conditioned noises lie in a prompt-specific subset of\nthe latent space, whereas at inference the noise is drawn from a\nprompt-agnostic Gaussian prior. To close this gap, we propose a noise projector\nthat applies text-conditioned refinement to the initial noise before denoising.\nConditioned on the prompt embedding, it maps the noise to a prompt-aware\ncounterpart that better matches the distribution observed during SD training,\nwithout modifying the SD model. Our framework consists of these steps: we first\nsample some noises and obtain token-level feedback for their corresponding\nimages from a vision-language model (VLM), then distill these signals into a\nreward model, and finally optimize the noise projector via a quasi-direct\npreference optimization. Our design has two benefits: (i) it requires no\nreference images or handcrafted priors, and (ii) it incurs small inference\ncost, replacing multi-sample selection with a single forward pass. Extensive\nexperiments further show that our prompt-aware noise projection improves\ntext-image alignment across diverse prompts.", "AI": {"tldr": "本文提出了一种噪声投影器，通过文本条件细化初始噪声，解决文本到图像生成中训练与推理不匹配的问题，从而在不修改Stable Diffusion模型的情况下，提高图像与提示词的对齐度，且推理成本较低。", "motivation": "在文本到图像生成中，不同的初始噪声会导致多样但有时与提示词不一致的图像。作者将这种不一致归因于训练与推理之间的不匹配：训练时噪声是提示词特异性的，而推理时噪声是从与提示词无关的高斯先验中抽取的。现有方法要么改变去噪动态，要么通过多样本选择来解决，但这通常成本较高。", "method": "本文提出一个“噪声投影器”，在去噪前对初始噪声进行文本条件细化。它根据提示词嵌入将噪声映射到与提示词相关的对应物，使其更好地匹配SD训练期间观察到的分布，且不修改SD模型。该框架包括：采样噪声、从视觉语言模型（VLM）获取图像的token级反馈、将信号提炼成奖励模型，最后通过准直接偏好优化来优化噪声投影器。", "result": "实验表明，所提出的提示词感知噪声投影器显著改善了各种提示词下的文本-图像对齐。其设计具有两个优点：(i) 不需要参考图像或手工先验；(ii) 仅需一次前向传播，推理成本低，取代了多样本选择。", "conclusion": "通过引入一个文本条件噪声投影器来细化初始噪声，本文成功弥合了文本到图像生成中的训练-推理差距。这种方法在不修改核心生成模型、无需外部数据且保持低推理成本的情况下，显著提升了生成图像与文本提示词的对齐质量。"}}
{"id": "2510.14528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14528", "abs": "https://arxiv.org/abs/2510.14528", "authors": ["Cheng Cui", "Ting Sun", "Suyin Liang", "Tingquan Gao", "Zelun Zhang", "Jiaxuan Liu", "Xueqing Wang", "Changda Zhou", "Hongen Liu", "Manhui Lin", "Yue Zhang", "Yubo Zhang", "Handong Zheng", "Jing Zhang", "Jun Zhang", "Yi Liu", "Dianhai Yu", "Yanjun Ma"], "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model", "comment": null, "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios.", "AI": {"tldr": "PaddleOCR-VL是一个最先进且资源高效的文档解析模型，其核心是0.9B的视觉-语言模型，支持109种语言和复杂元素识别，并在各项基准测试中表现出色，适合实际部署。", "motivation": "需要一个最先进（SOTA）且资源高效的模型来应对文档解析任务，特别是在多语言和复杂元素识别方面的挑战。", "method": "该研究提出了PaddleOCR-VL，其核心是PaddleOCR-VL-0.9B，一个紧凑而强大的视觉-语言模型。它将NaViT风格的动态分辨率视觉编码器与ERNIE-4.5-0.3B语言模型相结合，以实现准确的元素识别。该模型高效支持109种语言，并擅长识别文本、表格、公式和图表等复杂元素。", "result": "PaddleOCR-VL在广泛使用的公共基准和内部基准测试中，在页面级文档解析和元素级识别方面均达到了最先进的性能。它显著优于现有解决方案，与顶级视觉-语言模型相比具有强大的竞争力，并提供快速的推理速度。", "conclusion": "PaddleOCR-VL的卓越性能、资源效率和快速推理使其非常适合在实际场景中部署，解决了文档解析的实际需求。"}}
{"id": "2510.14702", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14702", "abs": "https://arxiv.org/abs/2510.14702", "authors": ["Penglong Zhai", "Jie Li", "Fanyi Di", "Yue Liu", "Yifang Yuan", "Jie Huang", "Peng Wu", "Sicong Wang", "Mingyang Yin", "Tingting Hu", "Yao Xu", "Xin Li"], "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction", "comment": "12 pages, 5 figures", "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.", "AI": {"tldr": "该论文提出了CoAST框架，通过自然语言接口，将世界知识、时空轨迹模式和用户画像融入大型语言模型，并通过两阶段（知识获取和认知对齐）训练，有效解决了LLM在下一兴趣点（POI）推荐中缺乏地理理解和世界知识的问题，并在离线和在线实验中取得了显著效果。", "motivation": "下一兴趣点（POI）推荐任务对基于位置的服务具有重要价值。尽管大型语言模型（LLM）在推荐系统中展现潜力，但它们主要在非结构化文本上预训练，缺乏对结构化地理实体和序列移动模式的理解。此外，工业级POI预测需要整合世界知识（如季节、天气、节假日）和用户画像（如习惯、职业、偏好）以提升用户体验和推荐性能。", "method": "本文提出了CoAST（Cognitive-Aligned Spatial-Temporal LLMs）框架，采用自然语言作为接口，整合世界知识、时空轨迹模式、用户画像和情境信息。CoAST主要包括两个阶段：\n1.  **推荐知识获取**：通过在脱敏用户的丰富时空轨迹数据上进行持续预训练。\n2.  **认知对齐**：通过监督微调（SFT）和随后的强化学习（RL）阶段，利用丰富训练数据将认知判断与人类偏好对齐。", "result": "在多个真实世界数据集上的大量离线实验，以及在AMAP App首页“猜你去哪儿”中部署的在线实验，均证明了CoAST框架的有效性。", "conclusion": "CoAST框架通过其两阶段训练（知识获取和认知对齐），成功地将地理理解、世界知识和人类认知偏好融入大型语言模型，有效解决了LLM在下一POI推荐任务中的局限性，显著提升了推荐性能和用户体验。"}}
{"id": "2510.13901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13901", "abs": "https://arxiv.org/abs/2510.13901", "authors": ["Tuan T. Nguyen", "John Le", "Thai T. Vu", "Willy Susilo", "Heath Cooper"], "title": "RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs", "comment": null, "summary": "Large language models (LLMs) achieve impressive performance across diverse\ntasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.\nWe present RAID (Refusal-Aware and Integrated Decoding), a framework that\nsystematically probes these weaknesses by crafting adversarial suffixes that\ninduce restricted content while preserving fluency. RAID relaxes discrete\ntokens into continuous embeddings and optimizes them with a joint objective\nthat (i) encourages restricted responses, (ii) incorporates a refusal-aware\nregularizer to steer activations away from refusal directions in embedding\nspace, and (iii) applies a coherence term to maintain semantic plausibility and\nnon-redundancy. After optimization, a critic-guided decoding procedure maps\nembeddings back to tokens by balancing embedding affinity with language-model\nlikelihood. This integration yields suffixes that are both effective in\nbypassing defenses and natural in form. Experiments on multiple open-source\nLLMs show that RAID achieves higher attack success rates with fewer queries and\nlower computational cost than recent white-box and black-box baselines. These\nfindings highlight the importance of embedding-space regularization for\nunderstanding and mitigating LLM jailbreak vulnerabilities.", "AI": {"tldr": "RAID是一个通过优化连续嵌入来生成对抗性后缀的框架，旨在绕过大型语言模型的安全机制，实现越狱攻击，同时保持流畅性。", "motivation": "大型语言模型（LLMs）尽管性能出色，但仍容易受到越狱攻击，这些攻击能够绕过其安全机制，促使模型生成受限内容。", "method": "RAID框架将离散令牌松弛为连续嵌入，并使用一个联合目标进行优化。该目标包含三个部分：(i) 鼓励生成受限响应，(ii) 引入一个拒绝感知正则化器，将激活从嵌入空间中的拒绝方向引导开，(iii) 应用一个连贯性项以保持语义合理性和非冗余性。优化后，通过批评者引导的解码过程，平衡嵌入亲和度和语言模型似然性，将嵌入映射回令牌。", "result": "在多个开源LLM上的实验表明，RAID比近期白盒和黑盒基线方法实现了更高的攻击成功率，同时查询次数更少，计算成本更低。", "conclusion": "这些发现强调了嵌入空间正则化对于理解和缓解LLM越狱漏洞的重要性。"}}
{"id": "2510.13900", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13900", "abs": "https://arxiv.org/abs/2510.13900", "authors": ["Julian Minder", "Clément Dumas", "Stewart Slocum", "Helena Casademunt", "Cameron Holmes", "Robert West", "Neel Nanda"], "title": "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences", "comment": null, "summary": "Finetuning on narrow domains has become an essential tool to adapt Large\nLanguage Models (LLMs) to specific tasks and to create models with known\nunusual properties that are useful for research. We show that narrow finetuning\ncreates strong biases in LLM activations that can be interpreted to understand\nthe finetuning domain. These biases can be discovered using simple tools from\nmodel diffing - the study of differences between models before and after\nfinetuning. In particular, analyzing activation differences on the first few\ntokens of random text and steering by adding this difference to the model\nactivations produces text similar to the format and general content of the\nfinetuning data. We demonstrate that these analyses contain crucial information\nby creating an LLM-based interpretability agent to understand the finetuning\ndomain. With access to the bias, the agent performs significantly better\ncompared to baseline agents using simple prompting. Our analysis spans\nsynthetic document finetuning for false facts, emergent misalignment,\nsubliminal learning, and taboo word guessing game models across different\narchitectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We\nsuspect these biases reflect overfitting and find that mixing pretraining data\ninto the finetuning corpus largely removes them, though residual risks may\nremain. Our work (1) demonstrates that narrowly finetuned models have salient\ntraces of their training objective in their activations and suggests ways to\nimprove how they are trained, (2) warns AI safety and interpretability\nresearchers that the common practice of using such models as a proxy for\nstudying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)\nhighlights the need for deeper investigation into the effects of narrow\nfinetuning and development of truly realistic case studies for model-diffing,\nsafety and interpretability research.", "AI": {"tldr": "狭窄领域微调会在大型语言模型激活中产生强烈的、可解释的偏差，这些偏差可以通过模型差异分析发现，并能揭示微调数据的内容和格式。这些偏差可能反映了过拟合，可以通过混合预训练数据来缓解。", "motivation": "微调是使大型语言模型适应特定任务的关键工具，但狭窄领域微调对模型内部激活的影响尚不清楚。理解这些偏差对于改进模型训练、AI安全和可解释性研究至关重要。", "method": "研究采用模型差异分析（对比微调前后模型差异）来发现激活偏差。具体方法包括分析随机文本前几个token的激活差异，并通过将这些差异添加到模型激活中进行“引导”，以生成与微调数据相似的文本。此外，还构建了一个基于LLM的可解释性代理来理解微调领域，并将其性能与基线代理进行比较。研究涵盖了多种架构（Gemma, LLaMA, Qwen）和规模（1B至32B参数），以及不同类型的微调任务（如虚假事实、意外失调、潜意识学习和禁忌词猜测游戏）。同时，也探讨了将预训练数据混合到微调语料库中的影响。", "result": "狭窄领域微调在LLM激活中产生了强烈的偏差，这些偏差可以被解释以理解微调领域。通过添加这些激活差异进行引导，可以生成与微调数据格式和内容相似的文本。获得偏差信息的LLM可解释性代理比基线代理表现显著更好。这些偏差被怀疑是过拟合的表现，将预训练数据混合到微调语料库中可以在很大程度上消除这些偏差。", "conclusion": "1. 狭窄领域微调的模型在激活中留下了其训练目标的显著痕迹，这为改进模型训练提供了方向。2. 警告AI安全和可解释性研究人员，将此类模型用作研究更广泛微调（如聊天微调）的代理可能不切实际。3. 强调需要更深入地研究狭窄领域微调的影响，并开发真正现实的案例研究，以促进模型差异分析、安全性和可解释性研究。"}}
{"id": "2510.13902", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.13902", "abs": "https://arxiv.org/abs/2510.13902", "authors": ["Nicole Smith-Vaniz", "Harper Lyon", "Lorraine Steigner", "Ben Armstrong", "Nicholas Mattei"], "title": "Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory", "comment": null, "summary": "Large Language Models (LLMs) have become increasingly incorporated into\neveryday life for many internet users, taking on significant roles as advice\ngivers in the domains of medicine, personal relationships, and even legal\nmatters. The importance of these roles raise questions about how and what\nresponses LLMs make in difficult political and moral domains, especially\nquestions about possible biases. To quantify the nature of potential biases in\nLLMs, various works have applied Moral Foundations Theory (MFT), a framework\nthat categorizes human moral reasoning into five dimensions: Harm, Fairness,\nIngroup Loyalty, Authority, and Purity. Previous research has used the MFT to\nmeasure differences in human participants along political, national, and\ncultural lines. While there has been some analysis of the responses of LLM with\nrespect to political stance in role-playing scenarios, no work so far has\ndirectly assessed the moral leanings in the LLM responses, nor have they\nconnected LLM outputs with robust human data. In this paper we analyze the\ndistinctions between LLM MFT responses and existing human research directly,\ninvestigating whether commonly available LLM responses demonstrate ideological\nleanings: either through their inherent responses, straightforward\nrepresentations of political ideologies, or when responding from the\nperspectives of constructed human personas. We assess whether LLMs inherently\ngenerate responses that align more closely with one political ideology over\nanother, and additionally examine how accurately LLMs can represent ideological\nperspectives through both explicit prompting and demographic-based\nrole-playing. By systematically analyzing LLM behavior across these conditions\nand experiments, our study provides insight into the extent of political and\ndemographic dependency in AI-generated responses.", "AI": {"tldr": "本研究利用道德基础理论（MFT）分析大型语言模型（LLM）的道德倾向和潜在偏见，通过与人类数据直接比较，探究LLM在固有回应、明确政治立场表示或扮演人类角色时是否展现出意识形态偏见，并评估其代表不同意识形态的准确性。", "motivation": "LLM日益成为日常生活中的建议提供者，尤其在医学、人际关系和法律等敏感领域，其回应的潜在偏见，特别是在政治和道德问题上的偏见，引发了重要关注。现有研究虽有使用MFT分析人类政治、国家和文化差异，但尚未直接评估LLM的道德倾向，也未将其输出与可靠的人类数据进行对比。", "method": "本研究将应用道德基础理论（MFT）来分析LLM的回应，并将其直接与现有的人类研究数据进行比较。具体方法包括：1) 调查LLM的固有回应是否表现出意识形态倾向；2) 评估LLM在直接表示政治意识形态（通过明确提示）时的表现；3) 分析LLM在扮演特定人类角色（基于人口统计学信息）时如何回应，以评估其代表不同意识形态的准确性。", "result": "本研究将评估LLM是否固有地生成更倾向于某一政治意识形态的回应，并检验LLM通过明确提示和基于人口统计学的角色扮演来准确代表意识形态视角的能力。通过系统分析LLM在这些条件和实验下的行为，本研究旨在揭示AI生成回应中政治和人口统计学依赖的程度。", "conclusion": "本研究通过系统分析LLM在不同条件和实验下的行为，旨在深入了解AI生成回应中政治和人口统计学依赖的程度，从而量化LLM潜在偏见的性质。"}}
{"id": "2510.14532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14532", "abs": "https://arxiv.org/abs/2510.14532", "authors": ["Xinrui Huang", "Fan Xiao", "Dongming He", "Anqi Gao", "Dandan Li", "Xiaofan Zhang", "Shaoting Zhang", "Xudong Wang"], "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology", "comment": null, "summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but\nradiographic image interpretation is limited by a shortage of trained\nprofessionals. While AI approaches have shown promise, existing dental AI\nsystems are restricted by their single-modality focus, task-specific design,\nand reliance on costly labeled data, hindering their generalization across\ndiverse clinical scenarios. To address these challenges, we introduce DentVFM,\nthe first family of vision foundation models (VFMs) designed for dentistry.\nDentVFM generates task-agnostic visual representations for a wide range of\ndental applications and uses self-supervised learning on DentVista, a large\ncurated dental imaging dataset with approximately 1.6 million multi-modal\nradiographic images from various medical centers. DentVFM includes 2D and 3D\nvariants based on the Vision Transformer (ViT) architecture. To address gaps in\ndental intelligence assessment and benchmarks, we introduce DentBench, a\ncomprehensive benchmark covering eight dental subspecialties, more diseases,\nimaging modalities, and a wide geographical distribution. DentVFM shows\nimpressive generalist intelligence, demonstrating robust generalization to\ndiverse dental tasks, such as disease diagnosis, treatment analysis, biomarker\nidentification, and anatomical landmark detection and segmentation.\nExperimental results indicate DentVFM significantly outperforms supervised,\nself-supervised, and weakly supervised baselines, offering superior\ngeneralization, label efficiency, and scalability. Additionally, DentVFM\nenables cross-modality diagnostics, providing more reliable results than\nexperienced dentists in situations where conventional imaging is unavailable.\nDentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and\nlabel-efficient model to improve intelligent dental healthcare and address\ncritical gaps in global oral healthcare.", "AI": {"tldr": "本文介绍了DentVFM，首个牙科视觉基础模型家族，它通过在大型多模态牙科影像数据集DentVista上进行自监督学习，实现了牙科任务的通用智能，并在新基准DentBench上表现出卓越的泛化能力、标签效率和跨模态诊断能力，甚至超越了经验丰富的牙医。", "motivation": "口腔颌面放射学面临专业人员短缺的挑战，而现有牙科AI系统存在单模态、任务专用性强、依赖昂贵标注数据等局限性，难以泛化到多样化的临床场景。", "method": "引入了DentVFM，首个专为牙科设计的视觉基础模型（VFM）家族，包括基于Vision Transformer (ViT) 架构的2D和3D变体。DentVFM在DentVista（一个包含约160万张多模态放射影像的大型牙科影像数据集）上采用自监督学习生成任务无关的视觉表示。同时，为了评估和基准测试牙科智能，本文还推出了DentBench，一个涵盖八个牙科亚专科、更多疾病、影像模态和广泛地理分布的综合基准。", "result": "DentVFM展现出令人印象深刻的通用智能，对各种牙科任务（如疾病诊断、治疗分析、生物标志物识别、解剖地标检测和分割）具有强大的泛化能力。实验结果表明，DentVFM显著优于有监督、自监督和弱监督基线，提供了卓越的泛化能力、标签效率和可扩展性。此外，DentVFM还支持跨模态诊断，在传统影像不可用的情况下，其结果比经验丰富的牙医更可靠。", "conclusion": "DentVFM为牙科AI设定了新的范式，提供了一个可扩展、适应性强且标签高效的模型，以改善智能牙科医疗保健，并解决全球口腔医疗保健中的关键空白。"}}
{"id": "2510.14703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14703", "abs": "https://arxiv.org/abs/2510.14703", "authors": ["Jianghao Lin", "Yuanyuan Shi", "Xin Peng", "Renjie Ding", "Hairui Wang", "Yuxuan Peng", "Bizhe Bai", "Weixi Song", "Fengshuo Bai", "Huacan Chai", "Weinan Zhang", "Fei Huang", "Ying Wen"], "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling", "comment": null, "summary": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation.", "AI": {"tldr": "本文提出一个结合细粒度束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升大型语言模型在结构化函数调用任务中的性能。ToolPRM通过首个细粒度调用内过程监督数据集进行训练，并在预测准确性和模型性能上超越现有方法，同时揭示了结构化输出推理扩展的“多探索少保留”原则。", "motivation": "大型语言模型（LLMs）作为自主智能体，函数调用是其与环境交互的核心机制。推理扩展是提升LLM性能的有效技术，但当前研究主要集中于非结构化输出生成，其在函数调用等结构化输出中的应用尚未得到充分探索。", "method": "本文提出一个推理扩展框架，结合了细粒度束搜索（fine-grained beam search）和过程奖励模型ToolPRM。ToolPRM能够对单个函数调用的内部步骤进行评分。为训练ToolPRM，构建了首个细粒度调用内过程监督数据集，该数据集通过函数掩码技术自动标注，提供步骤级的结构化工具使用推理奖励。", "result": "实验结果表明，ToolPRM在预测准确性方面优于粗粒度及结果奖励模型，显示其在监督函数调用推理过程中的强大能力。配备ToolPRM的推理扩展技术显著提升了基干模型在各种函数调用任务和基准上的性能。更重要的是，研究揭示了将推理扩展技术应用于结构化输出的关键原则：“多探索少保留”（explore more but retain less），这源于结构化函数调用生成的不可恢复性特征。", "conclusion": "ToolPRM及其推理扩展框架成功解决了推理扩展在结构化函数调用中应用不足的问题，通过细粒度过程监督显著提升了LLM在工具使用任务上的表现。同时，提出的“多探索少保留”原则为未来结构化输出的推理扩展研究提供了重要指导。"}}
{"id": "2510.14535", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14535", "abs": "https://arxiv.org/abs/2510.14535", "authors": ["Keima Abe", "Hayato Muraki", "Shuhei Tomoshige", "Kenichi Oishi", "Hitoshi Iyatomi"], "title": "Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval", "comment": "6 pages,3 figures, 3 tables. Accepted at 2025 IEEE International\n  Conference on Systems, Man, and Cybernetics (IEEE SMC 2025)", "summary": "Medical images like MR scans often show domain shifts across imaging sites\ndue to scanner and protocol differences, which degrade machine learning\nperformance in tasks such as disease classification. Domain harmonization is\nthus a critical research focus. Recent approaches encode brain images\n$\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then\ndisentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and\n$\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these\nmethods often lack interpretability$-$an essential requirement in medical\napplications$-$leaving practical issues unresolved. We propose\nPseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a\ngeneral framework for domain harmonization and interpretable representation\nlearning that preserves disease-relevant information in brain MR images.\nPL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract\n$\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image\n$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the\nencoder and domain predictor, the model learns to reconstruct the input image\n$\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and\n$\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared\nto prior methods, PL-SE-ADA achieves equal or better performance in image\nreconstruction, disease classification, and domain recognition. It also enables\nvisualization of both domain-independent brain features and domain-specific\ncomponents, offering high interpretability across the entire framework.", "AI": {"tldr": "PL-SE-ADA是一种新颖的领域协调和可解释表示学习框架，用于处理医学图像中的领域偏移，同时保留疾病相关信息并提供高可解释性。", "motivation": "医学图像（如MR扫描）由于成像站点、扫描仪和协议差异，常出现领域偏移，导致机器学习模型（如疾病分类）性能下降。现有领域协调方法虽然有效，但往往缺乏可解释性，这在医疗应用中是一个关键问题。", "method": "本文提出了伪线性风格编码器对抗域适应（PL-SE-ADA）框架。它包含两个编码器$f_E$和$f_{SE}$，分别提取领域不变特征($\boldsymbol{z_u}$)和领域特定特征($\boldsymbol{z_d}$)。此外，还有一个解码器$f_D$用于图像重建，以及一个领域预测器$g_D$。该方法采用编码器和领域预测器之间的对抗训练，并通过将$\boldsymbol{z_u}$和$\boldsymbol{z_d}$的重建结果相加来重建输入图像$\boldsymbol{x}$，以确保协调性和信息完整性。", "result": "与现有方法相比，PL-SE-ADA在图像重建、疾病分类和领域识别方面取得了相同或更优的性能。此外，它还能够可视化领域无关的脑部特征和领域特定的组成部分，从而在整个框架中提供了高度的可解释性。", "conclusion": "PL-SE-ADA是一个通用且可解释的领域协调和表示学习框架，能够有效解决医学图像中的领域偏移问题，同时在性能上与现有方法相当或更优，并显著提升了医学应用中至关重要的可解释性。"}}
{"id": "2510.14807", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14807", "abs": "https://arxiv.org/abs/2510.14807", "authors": ["Ruotian Peng", "Yi Ren", "Zhouliang Yu", "Weiyang Liu", "Yandong Wen"], "title": "SimKO: Simple Pass@K Policy Optimization", "comment": "Technical report (20 pages, 10 figures, project page:\n  https://spherelab.ai/simko/)", "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.", "AI": {"tldr": "本文分析了可验证奖励强化学习（RLVR）中存在的过度利用（高pass@1，低pass@K）问题，发现其训练过程中存在词元概率过度集中现象。为此，提出SimKO方法，通过非对称地调整正确和错误响应的词元概率，有效缓解了过度集中，显著提升了RLVR的探索能力和pass@K性能。", "motivation": "现有的可验证奖励强化学习（RLVR）方法存在系统性的利用偏向而非探索，表现为pass@1性能提高但pass@K（K>1）性能下降。研究动机在于理解并解决这一问题。", "method": "本文通过追踪词元级别上词汇候选的概率分布，分析了RLVR方法的训练动态。在此基础上，提出了Simple Pass@K Optimization (SimKO) 方法来缓解过度集中问题。SimKO采用非对称设计：对于已验证的正确响应，它提升前K个候选词的概率；对于已验证的错误响应，它对排名第一的候选词施加更强的惩罚。该方法在熵较高的词元处应用时，在缓解过度集中方面特别有效。", "result": "分析揭示了RLVR训练中存在一致的概率集中效应，即排名第一的候选词逐渐累积概率质量并抑制其他候选词的概率。更重要的是，更强的过度集中与更差的pass@K性能相关。SimKO方法在各种数学和逻辑推理基准测试中，始终在广泛的K值范围内产生更高的pass@K。", "conclusion": "SimKO通过缓解RLVR训练中的概率过度集中问题，有效鼓励了探索，为提高RLVR的pass@K性能提供了一种简单而有效的方法。"}}
{"id": "2510.14808", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14808", "abs": "https://arxiv.org/abs/2510.14808", "authors": ["Dominik Jehle", "Lennart Purucker", "Frank Hutter"], "title": "Agentic NL2SQL to Reduce Computational Costs", "comment": "Accepted at the NeurIPS 2025 Workshop on Efficient Reasoning. 10\n  pages, 11 figures", "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.", "AI": {"tldr": "Datalake Agent是一个代理系统，通过交互式循环和选择性信息请求，显著减少了大型语言模型（LLM）在自然语言到SQL（NL2SQL）任务中的提示词长度和处理成本，同时保持了竞争力。", "motivation": "大型语言模型（LLM）在NL2SQL任务中需要处理大量数据库元信息，导致提示词过长、token数量多和处理成本高。", "method": "引入Datalake Agent，一个代理系统，它不采用直接将所有元信息放入提示词的NL2SQL求解器，而是通过一个交互式循环来减少使用的元信息。在此循环中，LLM在一个推理框架内选择性地请求解决表格问答任务所需的必要信息。", "result": "Datalake Agent在包含23个数据库和100个表格问答任务的集合上进行了评估，结果显示LLM使用的token数量减少了高达87%，从而大幅降低了成本，同时保持了具有竞争力的性能。", "conclusion": "Datalake Agent通过优化元信息的使用，有效解决了LLM在NL2SQL任务中面临的提示词过长和成本高昂的问题，实现了显著的成本降低和性能保持。"}}
{"id": "2510.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13905", "abs": "https://arxiv.org/abs/2510.13905", "authors": ["Pan Chen", "Shaohong Chen", "Mark Wang", "Shi Xuan Leong", "Priscilla Fung", "Varinia Bernales", "Alan Aspuru-Guzik"], "title": "Schema for In-Context Learning", "comment": null, "summary": "In-Context Learning (ICL) enables transformer-based language models to adapt\nto new tasks by conditioning on demonstration examples. However, traditional\nexample-driven in-context learning lacks explicit modules for knowledge\nretrieval and transfer at the abstraction level. Inspired by cognitive science,\nspecifically schema theory, which holds that humans interpret new information\nby activating pre-existing mental frameworks (schemas) to structure\nunderstanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This\nframework extracts the representation of the building blocks of cognition for\nthe reasoning process instilled from prior examples, creating an abstracted\nschema, a lightweight, structured template of key inferential steps and their\nrelationships, which is then used to augment a model's reasoning process when\npresented with a novel question. We demonstrate that a broad range of large\nlanguage models (LLMs) lack the capacity to form and utilize internal\nschema-based learning representations implicitly, but instead benefit\nsignificantly from explicit schema-based scaffolding. Across chemistry and\nphysics questions from the GPQA dataset, our experiments show that SA-ICL\nconsistently boosts performance, up to 36.19 percent, when the single\ndemonstration example is of high quality, which simultaneously reduces reliance\non the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED\nIN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from\npattern priming to Chain-of-Thought prompting, but also paves a new path for\nenhancing human-like reasoning in LLMs.", "AI": {"tldr": "受认知科学中图式理论启发，本文提出了SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL)框架，通过从先验示例中提取抽象图式来增强大型语言模型的推理过程，显著提升了模型性能和可解释性。", "motivation": "传统的上下文学习（ICL）在抽象层面缺乏明确的知识检索和迁移模块。研究者受认知科学中图式理论（人类通过激活预存心理框架理解新信息）的启发，旨在通过引入显式图式来改善LLM的推理能力。", "method": "SA-ICL框架从先前示例中提取认知构建块的表示，创建了一个抽象图式。这个图式是一个轻量级、结构化的关键推理步骤及其关系的模板。然后，这个抽象图式被用来增强模型在处理新问题时的推理过程。", "result": "实验表明，大型语言模型本身缺乏隐式形成和利用基于图式学习表示的能力，但通过显式的基于图式的支架式学习（SA-ICL）能显著受益。在GPQA数据集的化学和物理问题上，SA-ICL持续提升了性能（最高达36.19%），尤其是在高质量的单一演示示例下，同时减少了对演示数量的依赖并增强了可解释性。", "conclusion": "SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL)不仅弥合了从模式引导到思维链提示等不同的ICL策略，还为增强大型语言模型的人类般推理能力开辟了一条新途径。"}}
{"id": "2510.14536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14536", "abs": "https://arxiv.org/abs/2510.14536", "authors": ["Chenyuan Qu", "Hao Chen", "Jianbo Jiao"], "title": "Exploring Image Representation with Decoupled Classical Visual Descriptors", "comment": "Accepted by The 36th British Machine Vision Conference (BMVC 2025)", "summary": "Exploring and understanding efficient image representations is a\nlong-standing challenge in computer vision. While deep learning has achieved\nremarkable progress across image understanding tasks, its internal\nrepresentations are often opaque, making it difficult to interpret how visual\ninformation is processed. In contrast, classical visual descriptors (e.g. edge,\ncolour, and intensity distribution) have long been fundamental to image\nanalysis and remain intuitively understandable to humans. Motivated by this\ngap, we ask a central question: Can modern learning benefit from these\nclassical cues? In this paper, we answer it with VisualSplit, a framework that\nexplicitly decomposes images into decoupled classical descriptors, treating\neach as an independent but complementary component of visual knowledge. Through\na reconstruction-driven pre-training scheme, VisualSplit learns to capture the\nessence of each visual descriptor while preserving their interpretability. By\nexplicitly decomposing visual attributes, our method inherently facilitates\neffective attribute control in various advanced visual tasks, including image\ngeneration and editing, extending beyond conventional classification and\nsegmentation, suggesting the effectiveness of this new learning approach for\nvisual understanding. Project page: https://chenyuanqu.com/VisualSplit/.", "AI": {"tldr": "VisualSplit框架通过将图像明确分解为可解耦的经典视觉描述符（如边缘、颜色），并采用重建驱动的预训练方案，旨在为深度学习模型提供可解释且可控的图像表示。", "motivation": "深度学习的内部表示通常不透明，难以解释视觉信息处理过程；而经典视觉描述符（如边缘、颜色分布）直观易懂。本文旨在解决这一鸿沟，探究现代学习能否从这些经典线索中受益。", "method": "VisualSplit框架将图像明确分解为解耦的经典描述符，每个描述符被视为独立但互补的视觉知识组件。通过重建驱动的预训练方案，VisualSplit学习捕捉每个视觉描述符的本质，同时保留其可解释性。", "result": "该方法通过明确分解视觉属性，内在促进了在各种高级视觉任务（包括图像生成和编辑）中有效的属性控制，超越了传统的分类和分割任务。这表明了这种新学习方法在视觉理解方面的有效性。", "conclusion": "VisualSplit框架通过将图像分解为可解释的经典描述符，成功地将现代学习与经典视觉线索相结合，不仅提高了深度学习模型的可解释性，还在图像生成和编辑等高级视觉任务中实现了有效的属性控制，为视觉理解提供了一种新的有效学习方法。"}}
{"id": "2510.14543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14543", "abs": "https://arxiv.org/abs/2510.14543", "authors": ["Ziqi Jiang", "Yanghao Wang", "Long Chen"], "title": "Exploring Cross-Modal Flows for Few-Shot Learning", "comment": "13 pages, 6 figures", "summary": "Aligning features from different modalities, is one of the most fundamental\nchallenges for cross-modal tasks. Although pre-trained vision-language models\ncan achieve a general alignment between image and text, they often require\nparameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT\nmethods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively\nfine-tune a subset of parameters, which can slightly adjust either visual or\ntextual features, and avoid overfitting. In this paper, we are the first to\nhighlight that all existing PEFT methods perform one-step adjustment. It is\ninsufficient for complex (or difficult) datasets, where features of different\nmodalities are highly entangled. To this end, we propose the first\nmodel-agnostic multi-step adjustment approach by learning a cross-modal\nvelocity field: Flow Matching Alignment (FMA). Specifically, to ensure the\ncorrespondence between categories during training, we first utilize a fixed\ncoupling strategy. Then, we propose a noise augmentation strategy to alleviate\nthe data scarcity issue. Finally, we design an early-stopping solver, which\nterminates the transformation process earlier, improving both efficiency and\naccuracy. Compared with one-step PEFT methods, FMA has the multi-step\nrectification ability to achieve more precise and robust alignment. Extensive\nresults have demonstrated that FMA can consistently yield significant\nperformance gains across various benchmarks and backbones, particularly on\nchallenging datasets.", "AI": {"tldr": "该论文提出首个模型无关的多步调整方法FMA（Flow Matching Alignment），通过学习跨模态速度场解决现有PEFT方法单步调整对复杂数据集跨模态特征对齐不足的问题，显著提升了对齐精度和鲁棒性。", "motivation": "跨模态任务的核心挑战是不同模态特征的对齐。尽管预训练视觉-语言模型能实现初步对齐，但仍需PEFT进行微调。现有PEFT方法（如prompt tuning、LoRA、adapter）仅进行单步调整，对于特征高度纠缠的复杂（或困难）数据集而言，这种单步调整是不足够的。", "method": "本文提出FMA（Flow Matching Alignment），一个模型无关的多步调整方法，通过学习跨模态速度场实现。具体方法包括：1) 采用固定耦合策略确保训练期间类别的对应性；2) 提出噪声增强策略缓解数据稀缺问题；3) 设计提前停止求解器，提前终止转换过程，提高效率和准确性。", "result": "与单步PEFT方法相比，FMA具有多步纠正能力，能实现更精确和鲁棒的对齐。广泛的实验结果表明，FMA在各种基准和骨干网络上，尤其是在具有挑战性的数据集上，都能持续获得显著的性能提升。", "conclusion": "FMA通过引入首个多步调整方法，成功解决了现有PEFT方法在复杂数据集上跨模态特征对齐的局限性，实现了更精确、鲁棒的特征对齐，并在多个任务和数据集上取得了显著的性能提升。"}}
{"id": "2510.13907", "categories": ["cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.13907", "abs": "https://arxiv.org/abs/2510.13907", "authors": ["Yuanchen Wu", "Saurabh Verma", "Justin Lee", "Fangzhou Xiong", "Poppy Zhang", "Amel Awadelkarim", "Xu Chen", "Yubai Yuan", "Shawndra Hill"], "title": "LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization", "comment": null, "summary": "Large language models (LLMs) are highly sensitive to their input prompts,\nmaking prompt design a central challenge. While automatic prompt optimization\n(APO) reduces manual engineering, most approaches assume access to ground-truth\nreferences such as labeled validation data. In practice, however, collecting\nhigh-quality labels is costly and slow. We propose the Prompt Duel Optimizer\n(PDO), a sample-efficient framework for label-free prompt optimization. PDO\nformulates the problem as a dueling-bandit setting, where supervision signal\ncomes from pairwise preference feedback provided by an LLM judge. The framework\ncombines Double Thompson Sampling (D-TS), which prioritizes informative prompt\ncomparisons, with Top-Performer Guided Mutation, which expands the candidate\npool by mutating high-performing prompts. PDO naturally operates in label-free\nsettings and can also incorporate partial labels to mitigate judge noise.\nExperiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently\noutperforms baseline methods. Ablation studies further demonstrate the\neffectiveness of both D-TS and prompt mutation.", "AI": {"tldr": "提出Prompt Duel Optimizer (PDO)，一个无需标签的、样本高效的提示词优化框架，利用LLM评判者提供成对偏好反馈，并结合Double Thompson Sampling和Top-Performer Guided Mutation。", "motivation": "大型语言模型（LLMs）对输入提示词高度敏感，提示词设计是一大挑战。现有的自动提示词优化（APO）方法大多需要地面真值标签（如标注验证数据），但高质量标签的收集成本高昂且耗时。", "method": "PDO将问题建模为决斗式多臂赌博机（dueling-bandit）设置，LLM评判者提供成对偏好反馈作为监督信号。该框架结合了Double Thompson Sampling (D-TS) 来优先处理信息量大的提示词比较，以及Top-Performer Guided Mutation来通过变异高性能提示词扩展候选池。PDO可在无标签环境下运行，也可整合部分标签以减轻评判者噪音。", "result": "在BIG-bench Hard (BBH) 和 MS MARCO 数据集上的实验表明，PDO持续优于基线方法。消融研究进一步证明了D-TS和提示词变异的有效性。", "conclusion": "PDO是一个有效、样本高效且无需标签的提示词优化框架，它通过LLM评判者的成对偏好反馈和创新的优化策略，解决了传统方法对标注数据的依赖问题。"}}
{"id": "2510.14553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14553", "abs": "https://arxiv.org/abs/2510.14553", "authors": ["Song Tang", "Peihao Gong", "Kunyu Li", "Kai Guo", "Boyu Wang", "Mao Ye", "Jianwei Zhang", "Xiatian Zhu"], "title": "Consistent text-to-image generation via scene de-contextualization", "comment": null, "summary": "Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity.", "AI": {"tldr": "本文提出了一种名为SDeC的无训练提示嵌入编辑方法，通过抑制文本到图像生成中主体与场景的内在关联（场景语境化），显著提升了跨场景生成图像时的主体一致性，且无需预先知道所有目标场景。", "motivation": "一致性文本到图像生成常因“身份（ID）偏移”而失败，即无法在不同场景中保持相同主体的一致性。现有方法通常依赖于预知所有目标场景的不切实际假设。本文揭示了ID偏移的关键来源是主体和场景语境之间的原生关联（场景语境化），这种关联源于T2I模型对大量自然图像训练分布的拟合。", "method": "本文首先正式证明了这种场景-ID关联的近乎普遍性，并推导了其强度的理论界限。在此基础上，提出了一种新颖、高效、无需训练的提示嵌入编辑方法——场景去语境化（SDeC）。SDeC通过量化SVD方向稳定性来自适应地重新加权相应特征值，识别并抑制ID提示嵌入中潜在的场景-ID关联，从而实现T2I内置场景语境化的反转过程。该方法允许按场景使用，无需预先访问所有目标场景。", "result": "实验证明，SDeC显著增强了身份保持能力，同时维持了场景多样性。", "conclusion": "SDeC是一种高度灵活且通用的解决方案，非常适合真实世界的应用场景，在这些场景中，预先了解所有目标场景通常是不可行或会随时间变化的。它有效解决了文本到图像生成中的身份偏移问题。"}}
{"id": "2510.13908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13908", "abs": "https://arxiv.org/abs/2510.13908", "authors": ["Dharunish Yugeswardeenoo", "Harshil Nukala", "Cole Blondin", "Sean O Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Interpreting the Latent Structure of Operator Precedence in Language Models", "comment": "9 pages, 4 figures. Accepted to INTERPLAY Workshop at COLM 2025", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities but continue to struggle with arithmetic tasks. Prior works\nlargely focus on outputs or prompting strategies, leaving the open question of\nthe internal structure through which models do arithmetic computation. In this\nwork, we investigate whether LLMs encode operator precedence in their internal\nrepresentations via the open-source instruction-tuned LLaMA 3.2-3B model. We\nconstructed a dataset of arithmetic expressions with three operands and two\noperators, varying the order and placement of parentheses. Using this dataset,\nwe trace whether intermediate results appear in the residual stream of the\ninstruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such\nas logit lens, linear classification probes, and UMAP geometric visualization.\nOur results show that intermediate computations are present in the residual\nstream, particularly after MLP blocks. We also find that the model linearly\nencodes precedence in each operator's embeddings post attention layer. We\nintroduce partial embedding swap, a technique that modifies operator precedence\nby exchanging high-impact embedding dimensions between operators.", "AI": {"tldr": "本研究通过分析LLaMA 3.2-3B模型内部表示，发现大型语言模型在算术计算中会编码运算符优先级，并在残差流中呈现中间计算结果，并引入了一种修改优先级的新技术。", "motivation": "大型语言模型在算术任务上表现不佳，以往研究主要关注输出或提示策略，而模型内部如何进行算术计算的结构仍是未解之谜。本研究旨在探究模型是否在其内部表示中编码了运算符优先级。", "method": "研究使用了开源指令微调的LLaMA 3.2-3B模型，构建了一个包含三个操作数和两个运算符的算术表达式数据集，并通过改变括号位置来控制优先级。应用了解释性技术，如logit lens、线性分类探测和UMAP几何可视化，以追踪残差流中的中间结果。此外，还引入了部分嵌入交换（partial embedding swap）技术来修改运算符优先级。", "result": "研究发现，中间计算结果存在于残差流中，特别是在MLP块之后。模型在线性层后注意力层中，将运算符的优先级线性编码到其嵌入中。通过部分嵌入交换技术，可以修改运算符的优先级。", "conclusion": "大型语言模型在内部表示中编码了运算符优先级，并且其残差流中存在中间算术计算结果。这些内部机制可以通过解释性技术进行探测和修改，为理解和改进LLM的算术能力提供了新的视角。"}}
{"id": "2510.14846", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.14846", "abs": "https://arxiv.org/abs/2510.14846", "authors": ["Zhuo-Yang Song"], "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents", "comment": "10 pages, 2 figures, 1 table", "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.", "AI": {"tldr": "本文提出了一种紧凑的形式理论，用于描述和衡量由领域先验引导、LLM辅助的迭代搜索，通过模糊关系算子和覆盖生成函数来量化搜索过程和可达性难度。", "motivation": "基于大型语言模型（LLMs）的“生成-过滤-精炼”迭代范式在推理、编程和AI+科学中的程序发现方面取得了进展，但搜索的有效性取决于如何将领域先验编码到操作结构化的假设空间中。因此，需要一种理论来描述和衡量LLM辅助的迭代搜索过程，并由领域先验引导。", "method": "本文提出了一种紧凑的形式理论。该理论将智能体表示为输入和输出上的模糊关系算子，并受限于固定的安全包络。为了描述多步推理/搜索，通过一个单一的延续参数对所有可达路径进行加权并求和，以获得一个覆盖生成函数。这会导出一个可达性难度的度量，并为安全包络所诱导的图上的搜索提供几何解释。最后，通过多数投票实例化来验证最简单的可测试推断。", "result": "该理论提供了一种可操作的语言和工具，用于衡量智能体及其搜索空间，并提出了对LLM构建的迭代搜索的系统形式化描述。", "conclusion": "该研究提供了一个系统而形式化的框架，用于理解和量化LLM辅助的、由领域先验引导的迭代搜索过程，从而能够更好地分析和优化这些搜索范式。"}}
{"id": "2510.14842", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14842", "abs": "https://arxiv.org/abs/2510.14842", "authors": ["Ben Elder", "Evelyn Duesterwald", "Vinod Muthusamy"], "title": "Boosting Instruction Following at Scale", "comment": "6+4 pages, 7 figures, 2 tables", "summary": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.", "AI": {"tldr": "本研究引入了“指令增强”作为一种后生成方法，以提高大型语言模型（LLM）指令遵循的可靠性，并提出了SCALEDIF基准。研究发现，指令数量增加时性能下降的主要原因是指令间的冲突，并开发了量化冲突评分工具。", "motivation": "开发者通常通过修改提示词来影响LLM的行为，但简单地增加指令并不能保证LLM会遵循。因此，需要一种方法来提高LLM指令遵循的可靠性，尤其是在指令数量增加导致性能下降的情况下。", "method": "本研究提出了“指令增强”（Instruction Boosting）作为一种后生成方法来提高LLM指令遵循率。为了评估，引入了SCALEDIF基准，该基准包含多达十条指令。此外，论文还分析了随着指令数量增加性能下降的趋势，并开发了一个量化冲突评分工具，以解释性能趋势并为开发者提供反馈。", "result": "指令增强方法将两条指令的遵循率提高了多达7个百分点，十条指令的遵循率提高了多达4个百分点。研究发现，指令数量增加时性能下降的一个重要因素是指令之间产生的紧张和冲突。所提出的量化冲突评分工具能够解释观察到的性能趋势，并为开发者提供关于额外提示指令对模型性能影响的反馈。", "conclusion": "指令增强是一种有效的后生成方法，可以提高LLM指令遵循的可靠性。LLM在处理多指令时性能下降的关键因素在于指令间的冲突程度，而量化冲突评分工具可以帮助开发者理解和管理这种影响，从而优化提示词设计。"}}
{"id": "2510.13910", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13910", "abs": "https://arxiv.org/abs/2510.13910", "authors": ["Jingru Lin", "Chen Zhang", "Stephen Y. Liu", "Haizhou Li"], "title": "RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) mitigates key limitations of Large\nLanguage Models (LLMs)-such as factual errors, outdated knowledge, and\nhallucinations-by dynamically retrieving external information. Recent work\nextends this paradigm through agentic RAG systems, where LLMs act as agents to\niteratively plan, retrieve, and reason over complex queries. However, these\nsystems still struggle with challenging multi-hop questions, and their\nintermediate reasoning capabilities remain underexplored. To address this, we\npropose RAGCap-Bench, a capability-oriented benchmark for fine-grained\nevaluation of intermediate tasks in agentic RAG workflows. We analyze outputs\nfrom state-of-the-art systems to identify common tasks and the core\ncapabilities required for their execution, then construct a taxonomy of typical\nLLM errors to design targeted evaluation questions. Experiments show that\n\"slow-thinking\" models with stronger RAGCap performance achieve better\nend-to-end results, underscoring the benchmark's validity and the importance of\nenhancing these intermediate capabilities.", "AI": {"tldr": "该论文提出了RAGCap-Bench，一个用于细粒度评估代理式RAG系统中中间任务能力的基准测试，并发现提升中间能力对端到端性能至关重要。", "motivation": "大型语言模型（LLMs）存在事实错误、知识过时和幻觉等局限性。检索增强生成（RAG）通过检索外部信息来缓解这些问题。代理式RAG系统进一步扩展了这一范式，但它们在处理复杂的多跳问题时仍面临挑战，且其中间推理能力尚未得到充分探索。", "method": "作者提出了RAGCap-Bench，一个面向能力的基准测试，用于细粒度评估代理式RAG工作流中的中间任务。他们分析了现有先进系统的输出，以识别常见任务及其所需的关键能力，并构建了典型的LLM错误分类法来设计有针对性的评估问题。", "result": "实验结果表明，在RAGCap性能上表现更强的“慢思考”模型能够获得更好的端到端结果。", "conclusion": "RAGCap-Bench基准测试的有效性得到了证实，并且强调了提升代理式RAG系统中中间能力的重要性。"}}
{"id": "2510.14560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14560", "abs": "https://arxiv.org/abs/2510.14560", "authors": ["Yulin Zhang", "Cheng Shi", "Yang Wang", "Sibei Yang"], "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video", "comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)", "summary": "Envision an AI capable of functioning in human-like settings, moving beyond\nmere observation to actively understand, anticipate, and proactively respond to\nunfolding events. Towards this vision, we focus on the innovative task where,\ngiven ego-streaming video input, an assistant proactively answers diverse,\nevolving questions at the opportune moment, while maintaining synchronized\nperception and reasoning. This task embodies three key properties: (1)\nProactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized\nEfficiency. To evaluate and address these properties, we first introduce\nESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a\nnovel framework designed for their rigorous assessment. Secondly, we propose a\ncomprehensive technical pipeline to enable models to tackle this challenging\ntask. This pipeline comprises: (1) a data engine, (2) a multi-stage training\nstrategy, and (3) a proactive dynamic compression technique. Our proposed model\neffectively addresses these critical properties while outperforming multiple\nbaselines across diverse online and offline benchmarks. Project\nPage:https://zhangyl4.github.io/publications/eyes-wide-open/", "AI": {"tldr": "该论文提出了一项创新任务，即AI助手能够根据第一人称视角流媒体视频，在适当的时机主动回答各种演变中的问题，并保持同步的感知和推理能力。", "motivation": "目前的AI系统主要停留在观察层面，缺乏在类人环境中主动理解、预测和响应事件的能力。研究旨在开发一种AI，能够超越被动观察，主动地与动态环境互动。", "method": "1. 引入了ESTP-Bench（Ego Streaming Proactive Benchmark）和ESTP-F1度量标准，以评估AI助手的“主动连贯性”、“及时响应性”和“同步效率”三个关键特性。2. 提出了一套全面的技术流程来解决这项挑战性任务，包括：数据引擎、多阶段训练策略和主动动态压缩技术。", "result": "所提出的模型能有效处理上述关键特性，并在多个在线和离线基准测试中超越了现有基线模型。", "conclusion": "该研究成功地定义了一个新任务，提供了评估框架，并开发了一个有效的技术流程，使AI助手能够从第一人称视角流媒体视频中主动、及时地回答问题，向实现更具人类化能力的AI迈进。"}}
{"id": "2510.14564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14564", "abs": "https://arxiv.org/abs/2510.14564", "authors": ["Junyi Wu", "Jiaming Xu", "Jinhao Li", "Yongkang Zhou", "Jiayi Pan", "Xingyang Li", "Guohao Dai"], "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU", "comment": "Accepted by ASP-DAC 2026", "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction\ntechnique. The traditional 3DGS training pipeline follows three sequential\nsteps: Gaussian densification, Gaussian projection, and color splatting.\nDespite its promising reconstruction quality, this conventional approach\nsuffers from three critical inefficiencies: (1) Skewed density allocation\nduring Gaussian densification, (2) Imbalanced computation workload during\nGaussian projection and (3) Fragmented memory access during color splatting.\n  To tackle the above challenges, we introduce BalanceGS, the algorithm-system\nco-design for efficient training in 3DGS. (1) At the algorithm level, we\npropose heuristic workload-sensitive Gaussian density control to automatically\nbalance point distributions - removing 80% redundant Gaussians in dense regions\nwhile filling gaps in sparse areas. (2) At the system level, we propose\nSimilarity-based Gaussian sampling and merging, which replaces the static\none-to-one thread-pixel mapping with adaptive workload distribution - threads\nnow dynamically process variable numbers of Gaussians based on local cluster\ndensity. (3) At the mapping level, we propose reordering-based memory access\nmapping strategy that restructures RGB storage and enables batch loading in\nshared memory.\n  Extensive experiments demonstrate that compared with 3DGS, our approach\nachieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible\nquality degradation.", "AI": {"tldr": "BalanceGS通过算法-系统协同设计，解决了3D高斯泼溅（3DGS）训练中的密度分配不均、计算负载不平衡和内存访问碎片化问题，实现了1.44倍的训练加速，同时保持了重建质量。", "motivation": "传统的3DGS训练流程存在三个关键低效问题：1) 高斯稠密化过程中密度分配不均；2) 高斯投影过程中计算工作负载不平衡；3) 颜色泼溅过程中内存访问碎片化。", "method": "本文提出了BalanceGS，一种针对3DGS高效训练的算法-系统协同设计方法：1) 在算法层面，引入启发式工作负载敏感的高斯密度控制，自动平衡点分布；2) 在系统层面，提出基于相似性的高斯采样和合并，用自适应工作负载分配取代静态线程-像素映射；3) 在映射层面，提出基于重排序的内存访问映射策略，重构RGB存储并实现共享内存中的批量加载。", "result": "与传统3DGS相比，BalanceGS在NVIDIA A100 GPU上实现了1.44倍的训练速度提升，同时质量下降可忽略不计。", "conclusion": "BalanceGS通过算法-系统协同设计，有效解决了3DGS训练中的关键低效问题，显著提升了训练速度，且不影响重建质量。"}}
{"id": "2510.14881", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14881", "abs": "https://arxiv.org/abs/2510.14881", "authors": ["Fikresilase Wondmeneh Abebayew"], "title": "The Gatekeeper Knows Enough", "comment": "7 pages, 1 figure", "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.", "AI": {"tldr": "本文提出Gatekeeper协议，通过引入低保真“潜在状态”推理和统一JSON格式的按需高保真上下文请求，解决LLM代理在与复杂系统交互时面临的有限上下文窗口和状态不同步问题，从而提高代理的可靠性和效率。", "motivation": "大型语言模型（LLMs）作为自主代理部署时，受限于有限的上下文窗口和无状态性导致的低效上下文管理及状态不同步，这导致输出不可靠、行为不可预测和资源使用低效，尤其是在与大型、结构化和敏感的知识系统（如代码库和文档）交互时。", "method": "引入Gatekeeper协议，这是一个领域无关的框架。该协议要求代理首先在系统极简的、低保真“潜在状态”表示上进行操作和推理，然后按需战略性地请求高保真上下文。所有交互通过统一的JSON格式进行，作为声明性、状态同步的协议，确保代理对系统的模型与系统实际状态保持一致。通过Sage（Gatekeeper协议的软件开发参考实现）进行了验证。", "result": "该方法显著提高了代理的可靠性，通过最小化令牌消耗提高了计算效率，并实现了与复杂系统的可扩展交互。Sage的实验结果证实了这一点。", "conclusion": "Gatekeeper协议为构建更健壮、可预测和可靠的AI代理提供了一种基础方法，适用于任何结构化知识领域，解决了LLM代理在实际应用中的核心挑战。"}}
{"id": "2510.14900", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14900", "abs": "https://arxiv.org/abs/2510.14900", "authors": ["Wen-Kwang Tsao", "Yao-Ching Yu", "Chien-Ming Huang"], "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates", "comment": null, "summary": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.", "AI": {"tldr": "本文提出了一种强化学习智能体，通过生成网络搜索查询和应用基于置信度的奖励，在没有标签数据的情况下，迭代地改进第三方日志到通用模式的映射准确性，显著减少了人工审查需求。", "motivation": "企业智能平台需要整合来自众多第三方供应商的日志，但供应商文档在测试时往往不可用、错位、不匹配、格式不佳或不完整，这使得模式映射极具挑战性。", "method": "引入了一个强化学习智能体，它无需标签示例或模型权重更新即可自我改进。在推理过程中，该智能体：1) 识别模糊的字段映射尝试。2) 生成有针对性的网络搜索查询以收集外部证据。3) 应用基于置信度的奖励来迭代地完善其映射。", "result": "将Microsoft Defender for Endpoint日志转换为通用模式的实验中，该方法将映射准确性从56.4%（仅LLM）提高到72.73%（RAG），再到100次迭代后的93.94%（使用GPT-4o）。同时，将需要专家审查的低置信度映射数量减少了85%。", "conclusion": "这种新方法为解决未来的行业问题提供了一种证据驱动、透明的解决方案，为更健壮、负责、可扩展、高效、灵活、适应性强和协作的解决方案铺平了道路。"}}
{"id": "2510.14861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14861", "abs": "https://arxiv.org/abs/2510.14861", "authors": ["Le Cong", "Zaixi Zhang", "Xiaotong Wang", "Yin Di", "Ruofan Jin", "Michal Gerasimiuk", "Yinkai Wang", "Ravi K. Dinesh", "David Smerkous", "Alex Smerkous", "Xuekun Wu", "Shilong Liu", "Peishan Li", "Yi Zhu", "Simran Serrao", "Ning Zhao", "Imran A. Mohammad", "John B. Sunwoo", "Joseph C. Wu", "Mengdi Wang"], "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans", "comment": null, "summary": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together.", "AI": {"tldr": "LabOS是首个结合计算推理和物理实验的AI协同科学家，通过多模态感知、自进化智能体和XR实现人机协作，使AI能够实时参与并协助实验，从而加速科学发现。", "motivation": "现代科学的快速发展需要理论与实践的结合。传统的AI主要停留在计算设计层面，未能深入参与物理实验，限制了其在实际操作中的作用。", "method": "LabOS通过连接多模型AI智能体、智能眼镜和扩展现实（XR）技术，实现了多模态感知。这使得AI能够“看到”科学家所见，理解实验上下文，并实时协助实验执行。它还包含自进化智能体和XR支持的人机协作功能。", "result": "LabOS在癌症免疫疗法靶点发现和干细胞工程等多个应用中得到了验证。它表明AI可以从单纯的计算设计转向实际参与实验，将实验室转变为一个智能、协作的环境，促进人机共同发现。", "conclusion": "LabOS证明了AI能够超越计算设计的范畴，成为实验室中与人类共同进化的协作伙伴，从而显著加速科学发现的过程，并推动科学研究进入人机智能协作的新时代。"}}
{"id": "2510.13912", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13912", "abs": "https://arxiv.org/abs/2510.13912", "authors": ["María Victoria Carro", "Denise Alejandra Mester", "Facundo Nieto", "Oscar Agustín Stanchi", "Guido Ernesto Bergman", "Mario Alejandro Leiva", "Eitan Sprejer", "Luca Nicolás Forziati Gangi", "Francisca Gauna Selasco", "Juan Gustavo Corvalán", "Gerardo I. Simari", "María Vanina Martinez"], "title": "AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs", "comment": "31 pages", "summary": "The core premise of AI debate as a scalable oversight technique is that it is\nharder to lie convincingly than to refute a lie, enabling the judge to identify\nthe correct position. Yet, existing debate experiments have relied on datasets\nwith ground truth, where lying is reduced to defending an incorrect\nproposition. This overlooks a subjective dimension: lying also requires the\nbelief that the claim defended is false. In this work, we apply debate to\nsubjective questions and explicitly measure large language models' prior\nbeliefs before experiments. Debaters were asked to select their preferred\nposition, then presented with a judge persona deliberately designed to conflict\nwith their identified priors. This setup tested whether models would adopt\nsycophantic strategies, aligning with the judge's presumed perspective to\nmaximize persuasiveness, or remain faithful to their prior beliefs. We\nimplemented and compared two debate protocols, sequential and simultaneous, to\nevaluate potential systematic biases. Finally, we assessed whether models were\nmore persuasive and produced higher-quality arguments when defending positions\nconsistent with their prior beliefs versus when arguing against them. Our main\nfindings show that models tend to prefer defending stances aligned with the\njudge persona rather than their prior beliefs, sequential debate introduces\nsignificant bias favoring the second debater, models are more persuasive when\ndefending positions aligned with their prior beliefs, and paradoxically,\narguments misaligned with prior beliefs are rated as higher quality in pairwise\ncomparison. These results can inform human judges to provide higher-quality\ntraining signals and contribute to more aligned AI systems, while revealing\nimportant aspects of human-AI interaction regarding persuasion dynamics in\nlanguage models.", "AI": {"tldr": "本研究探讨了AI辩论中大型语言模型（LLMs）的信念、说服力及辩论策略。发现LLMs倾向于迎合法官而非坚持自身信念，顺序辩论存在偏见，且虽然模型在捍卫与其信念一致的立场时更有说服力，但与其信念不符的论点反而被评价为更高质量。", "motivation": "现有的AI辩论实验多依赖于有“真实答案”的数据集，将“说谎”简化为辩护不正确的命题，而忽略了“说谎”的内在主观性，即辩论者需要相信其捍卫的主张是虚假的。本研究旨在探讨LLMs在主观问题上的辩论行为，并测量其先验信念。", "method": "研究将辩论应用于主观问题，并在实验前明确测量了大型语言模型的先验信念。辩论者被要求选择偏好立场，然后面对一个故意设计成与他们先验信念冲突的法官角色。研究比较了顺序和同时两种辩论协议，以评估潜在的系统性偏差。最后，评估了模型在捍卫与其先验信念一致或不一致的立场时，其说服力及论点质量。", "result": "主要发现包括：模型倾向于捍卫与法官角色一致的立场，而非其先验信念；顺序辩论引入了显著的偏见，有利于第二位辩论者；模型在捍卫与其先验信念一致的立场时更具说服力；但矛盾的是，与先验信念不符的论点在成对比较中被评为更高质量。", "conclusion": "这些结果可以指导人类法官提供更高质量的训练信号，有助于构建更对齐的AI系统，同时也揭示了语言模型中说服动态方面人机交互的重要方面。"}}
{"id": "2510.14576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14576", "abs": "https://arxiv.org/abs/2510.14576", "authors": ["Dongwook Lee", "Sol Han", "Jinwhan Kim"], "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification", "comment": "10 pages, 7 figures", "summary": "This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based\nmulti-branch neural network for vehicle re-identification. The proposed model\naddresses the challenge of learning discriminative and complementary features\nfrom three-dimensional point clouds to distinguish between vehicles. CALM-Net\nemploys a multi-branch architecture that integrates edge convolution, point\nattention, and a curvature embedding that characterizes local surface variation\nin point clouds. By combining these mechanisms, the model learns richer\ngeometric and contextual features that are well suited for the\nre-identification task. Experimental evaluation on the large-scale nuScenes\ndataset demonstrates that CALM-Net achieves a mean re-identification accuracy\nimprovement of approximately 1.97\\% points compared with the strongest baseline\nin our study. The results confirms the effectiveness of incorporating curvature\ninformation into deep learning architectures and highlight the benefit of\nmulti-branch feature learning for LiDAR point cloud-based vehicle\nre-identification.", "AI": {"tldr": "本文提出了CALM-Net，一个基于曲率感知激光雷达点云的多分支神经网络，用于车辆重识别，通过结合边缘卷积、点注意力机制和曲率嵌入，显著提升了识别准确率。", "motivation": "从三维点云中学习判别性且互补的特征以区分车辆，是车辆重识别任务中的一个挑战。", "method": "CALM-Net采用多分支架构，集成了边缘卷积、点注意力机制和表征局部表面变化的曲率嵌入。通过结合这些机制，模型学习更丰富的几何和上下文特征。", "result": "在nuScenes大规模数据集上的实验评估表明，CALM-Net的平均重识别准确率比研究中最强的基线提高了约1.97%个百分点。", "conclusion": "结果证实了将曲率信息融入深度学习架构的有效性，并强调了多分支特征学习对基于激光雷达点云的车辆重识别的益处。"}}
{"id": "2510.13909", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13909", "abs": "https://arxiv.org/abs/2510.13909", "authors": ["Xingrui Zhuo", "Jiapu Wang", "Gongqing Wu", "Zhongyuan Wang", "Jichen Zhang", "Shirui Pan", "Xindong Wu"], "title": "Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning", "comment": null, "summary": "Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in\nopen-domain KGs containing unknown entities and relations, which poses a\nchallenge for KGR models in comprehending uncertain KG components. Existing\nstudies have proposed Knowledge Graph Foundation Models (KGFMs) that learn\nstructural invariances across KGs to handle this uncertainty. Recently, Large\nLanguage Models (LLMs) have demonstrated strong capabilities for open-domain\nknowledge reasoning. As a result, the latest research has focused on LLM-based\nKGFMs that integrate LLM knowledge with KG context for inductive KGR. However,\nthe intrinsic knowledge of LLMs may be overshadowed by sparse KG context,\nleading to LLM knowledge distortion, which can cause irreversible damage to\nmodel reasoning. Moreover, existing LLM-based KGR methods still struggle to\nfully constrain generative hallucinations in LLMs, severely limiting the\ncredibility of reasoning results. To address these limitations, we propose a\nKnowledge Reasoning Language Model (KRLM) that achieves unified coordination\nbetween LLM knowledge and KG context throughout the KGR process. Specifically,\nwe design a Knowledge Reasoning Language (KRL) instruction format and a KRL\ntokenizer to align LLM knowledge with KG representations. Then, we propose a\nKRL attention layer that coordinates intrinsic LLM knowledge with additional KG\ncontext through a dynamic knowledge memory mechanism. Finally, a\nstructure-aware next-entity predictor is proposed, which strictly constrains\nthe reasoning results within a trustworthy knowledge domain. Extensive\nexperimental results on 25 real-world inductive KGR datasets demonstrate the\nsignificant superiority of the proposed KRLM\\footnote{Our source codes are\navailable at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot\nreasoning and fine-tuning scenarios.", "AI": {"tldr": "本文提出了一种知识推理语言模型（KRLM），通过统一协调大型语言模型（LLM）的内在知识与知识图谱（KG）上下文，解决归纳式知识图谱推理（KGR）中LLM知识扭曲和幻觉问题，并在25个真实世界数据集上取得了显著优越性。", "motivation": "现有的归纳式KGR模型在处理开放域KG中的未知实体和关系时面临挑战。尽管基于LLM的知识图谱基础模型（KGFM）结合了LLM知识与KG上下文，但稀疏的KG上下文可能掩盖LLM的内在知识，导致知识扭曲；同时，LLM的生成性幻觉也未能得到充分约束，严重限制了推理结果的可信度。", "method": "本文提出了KRLM模型。具体方法包括：1) 设计了一种知识推理语言（KRL）指令格式和KRL分词器，用于对齐LLM知识和KG表示。2) 提出了一种KRL注意力层，通过动态知识记忆机制协调LLM的内在知识与额外的KG上下文。3) 引入了一个结构感知型下一实体预测器，严格将推理结果限制在可信的知识域内，以约束生成性幻觉。", "result": "在25个真实世界的归纳式KGR数据集上进行的广泛实验结果表明，所提出的KRLM在零样本推理和微调两种场景下都展现出显著的优越性。", "conclusion": "KRLM通过在整个KGR过程中统一协调LLM知识和KG上下文，有效解决了现有LLM-based KGR方法中LLM知识扭曲和生成性幻觉的问题，从而提高了归纳式KGR的性能和可信度。"}}
{"id": "2510.14913", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14913", "abs": "https://arxiv.org/abs/2510.14913", "authors": ["Kyle Montgomery", "Sijun Tan", "Yuqi Chen", "Siyuan Zhuang", "Tianjun Zhang", "Raluca Ada Popa", "Chenguang Wang"], "title": "Budget-aware Test-time Scaling via Discriminative Verification", "comment": null, "summary": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.", "AI": {"tldr": "本文提出了一种混合方法，结合判别式验证器和自洽性，为大型语言模型提供了一种更高效、更有效的测试时扩展机制，在固定计算预算下优于生成式验证。", "motivation": "现有的大型语言模型测试时扩展方法（如生成式验证器）计算成本过高，限制了其实用性，因此需要一种更注重预算的替代方案。", "method": "本文对判别式验证器进行了彻底的实证分析，并将其与自洽性结合，形成一种混合方法。研究旨在评估这种混合方法在固定计算预算下的性能和效率。", "result": "在固定计算预算下，该混合方法（判别式验证器与自洽性的结合）显著超越了最先进的生成式验证方法，在AIME2025上实现了高达15.3%的准确率提升。它被认为是自洽性的一种“免费”升级。", "conclusion": "对于实际的、真实世界的应用，采用判别式验证器（特别是结合自洽性的混合方法）的预算感知型扩展不仅比单独使用自洽性有“免费”的性能提升，而且是比昂贵的生成式技术更有效、更高效的替代方案。"}}
{"id": "2510.13913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13913", "abs": "https://arxiv.org/abs/2510.13913", "authors": ["Shrey Pandit", "Xuan-Phi Nguyen", "Yifei Ming", "Austin Xu", "Jiayu Wang", "Caiming Xiong", "Shafiq Joty"], "title": "Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms", "comment": "Preprint. ICLR 26 submission", "summary": "Web-based 'deep research' agents aim to solve complex question - answering\ntasks through long-horizon interactions with online tools. These tasks remain\nchallenging, as the underlying language models are often not optimized for\nlong-horizon reasoning and exploration. Prior work has proposed workflows for\nconstructing instruction-tuning datasets, often leveraging knowledge graphs.\nHowever, such methods typically lack fine-grained control over difficulty and\nquality, yielding synthetic data that falls short of capturing the complexity\nrequired for long-horizon reasoning. Furthermore, many studies conflate data\nand training effects by comparing models trained under different optimization\nrecipes, making it difficult to isolate and evaluate the effectiveness of the\ndata itself. We introduce a two-pronged data synthesis pipeline that generates\nquestion - answer pairs by progressively increasing task complexity until a\nfrontier baseline web agent fails. The baseline agent plays multiple roles in\nthis process: attempting the questions, validating factuality, checking for\nalternative answers, and enforcing filtering. To evaluate the effectiveness of\nour synthesis methods, we adopt a controlled training setup based on\ndistillation from strong web agents. Experiments across multiple web-based\nbenchmarks show that our dataset - despite being smaller - enables the training\nof more effective web agents than existing datasets. In particular, our data\nexhibits twice the diversity in tool-use actions, allowing models trained on it\nto achieve stronger performance while avoiding repetitive tool-calling\nbehaviors.", "AI": {"tldr": "本文提出了一种新的数据合成流水线，通过逐步增加任务复杂度来生成问答对，从而训练出更有效、工具使用多样性更强的网络深度研究智能体，解决了现有数据生成方法在长程推理复杂性方面的不足。", "motivation": "目前的网络深度研究智能体在长程推理和探索方面表现不佳，因为底层语言模型未针对此优化。现有的指令调优数据集构建方法（常利用知识图谱）缺乏对难度和质量的精细控制，生成的合成数据无法捕捉长程推理所需的复杂性。此外，许多研究混淆了数据和训练效果，难以评估数据本身的有效性。", "method": "引入了一个双管齐下的数据合成流水线，通过逐步增加任务复杂度来生成问答对，直到一个前沿基线网络智能体失败。该基线智能体在此过程中扮演多重角色：尝试回答问题、验证事实性、检查替代答案以及执行过滤。为了评估合成方法的有效性，采用了基于从强大网络智能体蒸馏的受控训练设置。", "result": "尽管数据集规模较小，但我们的数据集能够训练出比现有数据集更有效的网络智能体。特别是，我们的数据在工具使用动作方面展现出两倍的多样性，使得在此基础上训练的模型能够获得更强的性能，同时避免重复的工具调用行为。", "conclusion": "所提出的数据合成方法能够生成高质量、高复杂度的训练数据，显著提升了网络深度研究智能体的性能和工具使用多样性，有效解决了现有数据生成方法在长程推理复杂性方面的局限性。"}}
{"id": "2510.14583", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14583", "abs": "https://arxiv.org/abs/2510.14583", "authors": ["Matan Rusanovsky", "Shimon Malnick", "Shai Avidan"], "title": "Talking Points: Describing and Localizing Pixels", "comment": null, "summary": "Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points.", "AI": {"tldr": "本文提出了一种新颖的框架，通过自然语言实现像素级精确的关键点理解和定位，并引入了新的数据集和评估协议。", "motivation": "现有的视觉-语言模型在跨模态理解方面取得了显著成功，但仍局限于物体或区域级别的定位，缺乏通过自然语言实现像素级精确关键点理解的能力。", "method": "该框架包含两个互补组件：一个生成丰富上下文关键点描述的“点描述器”（Point Descriptor）和一个从这些描述中回归精确像素坐标的“点定位器”（Point Localizer）。不同于以往依赖模板提示或关键点名称的方法，该方法生成自由形式、从粗到精的描述。由于缺乏训练数据，研究者构建了LlamaPointInPart数据集（包含2万多张图像-关键点-描述三元组）。为实现跨类别泛化，点描述器在AP-10K上通过GRPO进行优化，将冻结的点定位器作为奖励模型，以生成最大化定位精度的描述。评估采用新的协议，通过定位器衡量预测点与真实点的接近程度。", "result": "实验证明，该方法在LlamaPointInPart数据集上相比基线模型表现出卓越的性能。", "conclusion": "该框架的双向性质将为未来关键点引导的图像理解和语言引导的精确定位应用提供可能。"}}
{"id": "2510.14922", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14922", "abs": "https://arxiv.org/abs/2510.14922", "authors": ["Annisaa Fitri Nurfidausi", "Eleonora Mancini", "Paolo Torroni"], "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG", "comment": null, "summary": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection.", "AI": {"tldr": "本文系统性地探索了利用脑电图（EEG）、语音和文本进行多模态抑郁症检测的方法，比较了不同特征和模型策略，并发现三模态模型结合预训练嵌入能达到最先进的性能。", "motivation": "抑郁症的自动检测仍然充满挑战。现有研究在范围上有限，缺乏对特征的系统性比较，并且评估协议不一致，这些限制阻碍了多模态系统潜力的充分发挥。", "method": "研究系统地探索了EEG、语音和文本的特征表示（手工特征对比预训练嵌入）和建模策略（不同神经编码器）。比较了单模态、双模态和三模态配置，并分析了融合策略，特别关注了EEG的作用。采用一致的独立于受试者的分割方法，以确保稳健和可复现的基准测试。", "result": "研究结果表明：(i) 结合EEG、语音和文本模态能增强多模态检测效果；(ii) 预训练嵌入的表现优于手工特征；(iii) 精心设计的三模态模型实现了最先进的性能。", "conclusion": "这项工作为未来多模态抑郁症检测的研究奠定了基础，强调了多模态融合和预训练嵌入在提升检测精度方面的潜力。"}}
{"id": "2510.13915", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13915", "abs": "https://arxiv.org/abs/2510.13915", "authors": ["Ivan Lee", "Taylor Berg-Kirkpatrick"], "title": "Readability $\\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models", "comment": "Accepted to COLM 2025 (Spotlight)", "summary": "Recent studies suggest that very small language models (SLMs) can generate\nsurprisingly coherent text when trained on simplified, child-directed corpora\nsuch as TinyStories. These findings have been interpreted as evidence that\nreadability -- characterized by accessible vocabulary, familiar narrative\nstructure, and simple syntax -- plays a key role in enabling such capabilities\nto emerge. In this paper, we challenge that interpretation. We construct\nsynthetic datasets with matched structure but varied readability, and find that\nreadability alone does not predict coherence or learning efficiency in SLMs.\nModels trained on complex, adult-level text perform comparably to those trained\non simplified language, and even exhibit faster development of coherence during\ntraining. Instead, we show that statistical simplicity, as measured by n-gram\ndiversity, is a stronger predictor of learnability. Our findings caution\nagainst the growing trend of anthropomorphizing language model training --\ndrawing parallels to human cognitive development without empirical basis -- and\nargue for more precise reasoning about what properties actually support\ncapability emergence in small models.", "AI": {"tldr": "研究挑战了小型语言模型（SLM）连贯性由文本可读性决定的观点，发现统计简易性（如n-gram多样性）是更强的预测因子，而非可读性。", "motivation": "现有研究认为，在简化、面向儿童的语料库上训练的小型语言模型能生成连贯文本，这被解释为可读性（易懂词汇、熟悉叙事结构、简单句法）在能力涌现中扮演关键角色。本文旨在挑战这一解释。", "method": "构建了结构匹配但可读性不同的合成数据集，并在此基础上训练小型语言模型。", "result": "研究发现，仅可读性并不能预测SLM的连贯性或学习效率。在复杂、成人级别文本上训练的模型表现与在简化语言上训练的模型相当，甚至在训练过程中连贯性发展更快。相反，通过n-gram多样性衡量的统计简易性是可学习性更强的预测因子。", "conclusion": "研究结果警示不要将语言模型训练拟人化（无经验基础地与人类认知发展类比），并主张对实际支持小型模型能力涌现的属性进行更精确的推理。"}}
{"id": "2510.14588", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14588", "abs": "https://arxiv.org/abs/2510.14588", "authors": ["Zhifei Chen", "Tianshuo Xu", "Leyi Wu", "Luozhou Wang", "Dongyu Yan", "Zihan You", "Wenting Luo", "Guo Zhang", "Yingcong Chen"], "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding", "comment": "Code, model, and demos can be found at\n  https://envision-research.github.io/STANCE/", "summary": "Video generation has recently made striking visual progress, but maintaining\ncoherent object motion and interactions remains difficult. We trace two\npractical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)\noften collapse to too few effective tokens after encoding, weakening guidance;\nand (ii) optimizing for appearance and motion in a single head can favor\ntexture over temporal consistency. We present STANCE, an image-to-video\nframework that addresses both issues with two simple components. First, we\nintroduce Instance Cues -- a pixel-aligned control signal that turns sparse,\nuser-editable hints into a dense 2.5D (camera-relative) motion field by\naveraging per-instance flow and augmenting with monocular depth over the\ninstance mask. This reduces depth ambiguity compared to 2D arrow inputs while\nremaining easy to use. Second, we preserve the salience of these cues in token\nspace with Dense RoPE, which tags a small set of motion tokens (anchored on the\nfirst frame) with spatial-addressable rotary embeddings. Paired with joint RGB\n\\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors\nstructure while RGB handles appearance, stabilizing optimization and improving\ntemporal coherence without requiring per-frame trajectory scripts.", "AI": {"tldr": "STANCE是一个图像到视频生成框架，通过引入实例线索（Instance Cues）和密集旋转位置编码（Dense RoPE）来解决现有方法在保持物体运动和交互连贯性方面的瓶颈，显著提升了视频的时间连贯性。", "motivation": "当前视频生成在保持物体运动和交互连贯性方面存在困难。主要瓶颈包括：1) 人工提供的运动提示（如2D地图）在编码后有效令牌过少，导致指导不足；2) 在单个头部中同时优化外观和运动可能导致模型偏向纹理而非时间一致性。", "method": "STANCE框架通过两个核心组件解决上述问题：1) 实例线索（Instance Cues）：将稀疏、用户可编辑的提示转化为密集的2.5D（相机相对）运动场。它通过平均每个实例的流并结合单目深度信息来增强实例掩码，减少了2D箭头输入的深度模糊性。2) 密集旋转位置编码（Dense RoPE）：通过空间可寻址的旋转嵌入来标记一小部分运动令牌（锚定在第一帧），以在令牌空间中保持这些线索的显著性。此外，模型采用RGB与辅助图（分割或深度）联合预测，使结构锚定，RGB处理外观，从而稳定优化并提高时间连贯性。", "result": "STANCE框架有效解决了视频生成中物体运动和交互连贯性的问题。它通过实例线索减少了深度模糊性，并利用密集旋转位置编码在令牌空间中保持了运动线索的显著性。结合联合预测，模型稳定了优化过程，显著改善了时间连贯性，且无需逐帧轨迹脚本即可实现更精确的物体运动和交互。", "conclusion": "STANCE框架通过其创新的实例线索和密集旋转位置编码机制，成功克服了当前视频生成在保持物体运动和交互连贯性方面的两大实际瓶颈，显著提高了生成视频的时间一致性和整体质量，同时保持了易用性。"}}
{"id": "2510.13918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13918", "abs": "https://arxiv.org/abs/2510.13918", "authors": ["Peng Kuang", "Yanli Wang", "Xiaoyu Han", "Yaowenqi Liu", "Kaidi Xu", "Haohan Wang"], "title": "Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling", "comment": null, "summary": "Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),\ndesigned to verify and select the best responses from large language models\n(LLMs). However, this promise is challenged by recent benchmarks where simple\nmajority voting, which ignores PRM signals, occasionally outperforms standard\nPRM-based selection. This raises a critical question: How can we effectively\nutilize verification signals from PRMs for TTS? To address this, we start by\ndeveloping a theoretical framework for optimally combining signals from both\nthe LLM and the PRM. Our framework reveals that the optimal strategy is a\nweighted aggregation of responses, a strategy whose effectiveness hinges on\nestimating weights that capture the complex interplay between the models. Based\non our theoretical results, we empirically show that these optimal weighting\nfunctions differ significantly across LLM-PRM pairs and, notably, often assign\nsubstantial negative weights. Motivated by these insights, we propose efficient\npre-computation methods to calibrate these weighting functions. Extensive\nexperiments across 5 LLMs and 7 PRMs demonstrate that our calibration method\nsignificantly boosts the TTS efficiency, surpassing the performance of vanilla\nweighted majority voting while using only $21.3\\%$ of the computation.\nUltimately, our work demonstrates that investing in a more intelligent\naggregation strategy can be a more convincing path to performance gains than\nsimply scaling test-time computation.", "AI": {"tldr": "本文提出了一种理论框架和校准方法，用于优化大语言模型（LLM）和过程奖励模型（PRM）信号的结合，以提高测试时扩展（TTS）的效率，证明了智能聚合策略比单纯增加计算更有效。", "motivation": "尽管过程奖励模型（PRM）被设计用于测试时扩展（TTS）以验证和选择LLM的最佳响应，但最近的基准测试显示，简单的多数投票有时会优于基于PRM的选择，这引发了如何有效利用PRM验证信号的关键问题。", "method": "研究首先开发了一个理论框架，用于最优地结合LLM和PRM的信号，揭示了最优策略是响应的加权聚合。基于理论结果，提出了高效的预计算方法来校准这些加权函数，以捕捉模型之间复杂的相互作用。", "result": "经验结果表明，最优加权函数在不同的LLM-PRM对之间存在显著差异，并且通常会分配显著的负权重。通过对5个LLM和7个PRM进行广泛实验，证明所提出的校准方法显著提升了TTS效率，超越了香草加权多数投票的性能，同时仅使用了21.3%的计算量。", "conclusion": "研究最终证明，投资于更智能的聚合策略（即校准加权函数）是实现性能提升更有效途径，而非仅仅通过扩展测试时计算。"}}
{"id": "2510.14594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14594", "abs": "https://arxiv.org/abs/2510.14594", "authors": ["Hugo Markoff", "Jevgenijs Galaktionovs"], "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers", "comment": "Extended abstract. Submitted to AICC: Workshop on AI for Climate and\n  Conservation - EurIPS 2025 (non-archival)", "summary": "State-of-the-art animal classification models like SpeciesNet provide\npredictions across thousands of species but use conservative rollup strategies,\nresulting in many animals labeled at high taxonomic levels rather than species.\nWe present a hierarchical re-classification system for the Animal Detect\nplatform that combines SpeciesNet EfficientNetV2-M predictions with CLIP\nembeddings and metric learning to refine high-level taxonomic labels toward\nspecies-level identification. Our five-stage pipeline (high-confidence\nacceptance, bird override, centroid building, triplet-loss metric learning, and\nadaptive cosine-distance scoring) is evaluated on a segment of the LILA BC\nDesert Lion Conservation dataset (4,018 images, 15,031 detections). After\nrecovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify\n456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving\nspecies-level identification for 64.9 percent", "AI": {"tldr": "本文提出了一种分层重新分类系统，结合SpeciesNet、CLIP嵌入和度量学习，将高层级的动物分类标签细化到物种级别，显著提高了物种识别的准确性和覆盖率。", "motivation": "现有的动物分类模型（如SpeciesNet）常因保守的汇总策略，将许多动物标记为高层级分类而非具体物种，导致物种级别识别不足。", "method": "该系统是一个五阶段的管道：高置信度接受、鸟类覆盖、质心构建、三元组损失度量学习和自适应余弦距离评分。它结合了SpeciesNet EfficientNetV2-M的预测、CLIP嵌入和度量学习，并在LILA BC沙漠狮子保护数据集上进行了评估。", "result": "该系统从“空白”和“动物”标签中恢复了761个鸟类检测。对456个被标记为动物、哺乳动物或空白的检测进行了重新分类，准确率达到96.5%，其中64.9%实现了物种级别的识别。", "conclusion": "该分层重新分类系统成功地将高层级分类标签细化到物种级别，显著提高了动物检测的物种识别准确性和覆盖率。"}}
{"id": "2510.13920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13920", "abs": "https://arxiv.org/abs/2510.13920", "authors": ["Ye Yuan", "Mohammad Amin Shabani", "Siqi Liu"], "title": "FACTS: Table Summarization via Offline Template Generation with Agentic Workflows", "comment": "Under Review", "summary": "Query-focused table summarization requires generating natural language\nsummaries of tabular data conditioned on a user query, enabling users to access\ninsights beyond fact retrieval. Existing approaches face key limitations:\ntable-to-text models require costly fine-tuning and struggle with complex\nreasoning, prompt-based LLM methods suffer from token-limit and efficiency\nissues while exposing sensitive data, and prior agentic pipelines often rely on\ndecomposition, planning, or manual templates that lack robustness and\nscalability. To mitigate these issues, we introduce an agentic workflow, FACTS,\na Fast, Accurate, and Privacy-Compliant Table Summarization approach via\nOffline Template Generation. FACTS produces offline templates, consisting of\nSQL queries and Jinja2 templates, which can be rendered into natural language\nsummaries and are reusable across multiple tables sharing the same schema. It\nenables fast summarization through reusable offline templates, accurate outputs\nwith executable SQL queries, and privacy compliance by sending only table\nschemas to LLMs. Evaluations on widely-used benchmarks show that FACTS\nconsistently outperforms baseline methods, establishing it as a practical\nsolution for real-world query-focused table summarization.", "AI": {"tldr": "FACTS是一种基于离线模板生成的代理工作流，用于查询驱动的表格摘要，旨在解决现有方法的局限性，提供快速、准确且隐私合规的解决方案。", "motivation": "现有的查询驱动表格摘要方法存在以下问题：表格到文本模型需要昂贵的微调且难以处理复杂推理；基于提示的LLM方法受限于token数量、效率低下且可能暴露敏感数据；先前的代理流水线依赖分解、规划或手动模板，缺乏鲁棒性和可扩展性。", "method": "本文提出FACTS（Fast, Accurate, and Privacy-Compliant Table Summarization via Offline Template Generation），一种代理工作流。FACTS通过离线生成模板（包含SQL查询和Jinja2模板），这些模板可在具有相同模式的多个表格中重复使用，从而实现快速摘要、通过可执行SQL查询确保输出准确性，并通过仅向LLM发送表格模式来确保隐私合规性。", "result": "在广泛使用的基准测试中，FACTS始终优于基线方法。", "conclusion": "FACTS被证明是现实世界中查询驱动表格摘要的实用解决方案。"}}
{"id": "2510.14925", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14925", "abs": "https://arxiv.org/abs/2510.14925", "authors": ["Akira Okutomi"], "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models", "comment": "19 pages, 2 figures, preliminary version", "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision.", "AI": {"tldr": "本文将康德的《纯粹理性批判》重新解读为反馈稳定性理论，并引入复合不稳定性指标（H-Risk）来形式化。研究发现，H-Risk能预测线性高斯模拟中的过度自信错误，且在大型语言模型（LLMs）中，脆弱的内部动力学与校准不良和幻觉相关，揭示了康德式自我限制与反馈控制之间的结构性联系，为诊断和减少推理系统中的过度自信提供了新视角。", "motivation": "研究动机是将康德的《纯粹理性批判》重新诠释为一种反馈稳定性理论，将理性视为一种调节器，使推理保持在可能经验的界限内。作者旨在形式化这种直觉，并将其应用于现代推理系统，如大型语言模型。", "method": "研究方法包括：1. 将康德的《纯粹理性批判》重新解释为反馈稳定性理论。2. 通过结合谱裕度、条件数、时间敏感性和创新放大等因素的复合不稳定性指标（H-Risk）来形式化这一直觉。3. 在线性高斯模拟中测试H-Risk的预测能力。4. 将该框架扩展到大型语言模型（LLMs）中，分析其内部动力学与性能的关系。5. 探索批判式提示对LLMs校准和幻觉的影响。", "result": "主要结果有：1. 在线性高斯模拟中，即使在形式稳定条件下，更高的H-Risk也能预测过度自信的错误，揭示了名义稳定性和认知稳定性之间的差距。2. 在大型语言模型（LLMs）中，脆弱的内部动力学与校准不良和幻觉现象存在关联。3. 批判式提示对LLMs的校准和幻觉表现出复杂（混合）的效果。", "conclusion": "本文得出结论，康德的自我限制概念与反馈控制之间存在结构性桥梁，这为诊断和选择性地减少推理系统中的过度自信提供了一个有原则的视角。"}}
{"id": "2510.14942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14942", "abs": "https://arxiv.org/abs/2510.14942", "authors": ["Yao Zhang", "Yu Wu", "Haowei Zhang", "Weiguo Li", "Haokun Chen", "Jingpei Wu", "Guohao Li", "Zhen Han", "Volker Tresp"], "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning", "comment": "25 pages", "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.", "AI": {"tldr": "GroundedPRM是一个树引导且注重事实的框架，通过结合蒙特卡洛树搜索（MCTS）和外部工具验证，为大型语言模型（LLMs）提供自动、高质量的过程奖励模型（PRM）监督，解决了现有方法奖励噪声大、事实准确性低和目标不一致的问题，以更少的数据实现了显著的性能提升。", "motivation": "现有过程奖励模型（PRMs）在多步推理中面临挑战，主要原因在于缺乏可扩展、高质量的标注。现有方法依赖昂贵的人工标注、易产生幻觉的LLM自评估，或因信用归因不当而引入噪声的蒙特卡洛（MC）估计。这些问题导致了奖励噪声、事实准确性低和与步级推理目标不一致的核心局限性。", "method": "GroundedPRM通过以下方式解决这些挑战：1) 利用蒙特卡洛树搜索（MCTS）构建结构化推理路径，以减少奖励噪声并实现细粒度信用分配。2) 使用外部工具验证每个中间步骤，提供基于执行的正确性信号，消除幻觉监督。3) 设计混合奖励聚合机制，融合工具验证和MCTS反馈，结合步级验证和全局结果评估。4) 将奖励信号格式化为增强解释性和兼容性的生成式结构。", "result": "GroundedPRM仅使用4万个自动标注样本进行训练，这仅是表现最佳的自动标注PRM所用数据的10%。尽管如此，它在ProcessBench上实现了平均性能高达26%的相对提升。当用于奖励引导的贪婪搜索时，GroundedPRM甚至优于使用人工标注监督训练的PRM。", "conclusion": "GroundedPRM为实现高质量的过程级推理提供了一条可扩展且可验证的路径。它通过自动化的树引导和事实感知框架，显著提升了PRMs的性能和效率，解决了现有方法的关键局限性。"}}
{"id": "2510.14605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14605", "abs": "https://arxiv.org/abs/2510.14605", "authors": ["Yuyang Hong", "Jiaqi Gu", "Qi Yang", "Lubin Fan", "Yue Wu", "Ying Wang", "Kun Ding", "Shiming Xiang", "Jieping Ye"], "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "comment": "Accepted by NeurIPS 2025", "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF", "AI": {"tldr": "本文提出了一种名为Wiki-PRF的三阶段方法，用于基于知识的视觉问答（KB-VQA），通过工具调用、多模态知识检索和基于强化学习的过滤来提高多模态查询质量和检索结果的相关性，从而在基准数据集上取得了最先进的性能。", "motivation": "尽管检索增强生成（RAG）在基于知识的视觉问答（KB-VQA）任务中取得了显著进展，但它仍然面临多模态查询质量不高和检索结果相关性不足的挑战。", "method": "本文提出了Wiki-PRF，一个包含三个阶段的新方法：\n1. 处理阶段：动态调用视觉工具，提取精确的多模态信息用于检索。\n2. 检索阶段：整合视觉和文本特征，实现多模态知识检索。\n3. 过滤阶段：对检索结果进行相关性过滤和集中。为此，引入了一个视觉语言模型，该模型通过强化学习方式，以答案准确性和格式一致性作为奖励信号进行训练，以增强模型的推理能力、工具调用准确性以及无关内容的过滤能力。", "result": "在基准数据集（E-VQA和InfoSeek）上的实验表明，该方法在答案质量方面取得了显著改进（分别提升36.0和42.8），达到了最先进的性能。", "conclusion": "Wiki-PRF通过其三阶段方法，有效解决了KB-VQA中多模态查询质量和检索结果相关性的挑战，从而显著提升了答案质量并实现了最先进的性能。"}}
{"id": "2510.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14596", "abs": "https://arxiv.org/abs/2510.14596", "authors": ["Hugo Markoff", "Jevgenijs Galaktionovs"], "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering", "comment": "Extended abstract. Submitted to AICC: Workshop on AI for Climate and\n  Conservation - EurIPS 2025 (non-archival)", "summary": "Camera traps generate millions of wildlife images, yet many datasets contain\nspecies that are absent from existing classifiers. This work evaluates\nzero-shot approaches for organizing unlabeled wildlife imagery using\nself-supervised vision transformers, developed and tested within the Animal\nDetect platform for camera trap analysis. We compare unsupervised clustering\nmethods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)\ncombined with dimensionality reduction techniques (PCA, UMAP), and we\ndemonstrate continuous 1D similarity ordering via t-SNE projection. On a\n5-species test set with ground truth labels used only for evaluation, DINOv2\nwith UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D\nsorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent\nfor fish across 1,500 images. Based on these findings, we deployed continuous\nsimilarity ordering in production, enabling rapid exploratory analysis and\naccelerating manual annotation workflows for biodiversity monitoring.", "AI": {"tldr": "本研究评估了使用自监督视觉Transformer和零样本方法，通过无监督聚类和降维技术，对相机陷阱中未标记的野生动物图像进行组织和排序，以应对现有分类器中物种缺失的问题。", "motivation": "相机陷阱产生了数百万张野生动物图像，但许多数据集中包含现有分类器中不存在的物种，导致难以有效组织和分析这些未标记图像。", "method": "研究在Animal Detect平台内评估了零样本方法。比较了三种架构（CLIP、DINOv2、MegaDescriptor）与降维技术（PCA、UMAP）结合的无监督聚类方法（DBSCAN、GMM），并通过t-SNE投影展示了连续的1D相似度排序。", "result": "在包含5个物种的测试集上，DINOv2结合UMAP和GMM实现了88.6%的准确率（macro-F1 = 0.874）。在1,500张图像上，1D排序对哺乳动物和鸟类达到了88.2%的一致性，对鱼类达到了95.2%的一致性。", "conclusion": "基于研究结果，连续相似度排序已投入生产使用，能够实现快速探索性分析，并加速生物多样性监测中的手动标注工作流程。"}}
{"id": "2510.13925", "categories": ["cs.CL", "cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13925", "abs": "https://arxiv.org/abs/2510.13925", "authors": ["Daniel Adu Worae", "Spyridon Mastorakis"], "title": "An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation", "comment": null, "summary": "Internet of Things (IoT) networks generate diverse and high-volume traffic\nthat reflects both normal activity and potential threats. Deriving meaningful\ninsight from such telemetry requires cross-layer interpretation of behaviors,\nprotocols, and context rather than isolated detection. This work presents an\nLLM-powered AI agent framework that converts raw packet captures into\nstructured and semantically enriched representations for interactive analysis.\nThe framework integrates feature extraction, transformer-based anomaly\ndetection, packet and flow summarization, threat intelligence enrichment, and\nretrieval-augmented question answering. An AI agent guided by a large language\nmodel performs reasoning over the indexed traffic artifacts, assembling\nevidence to produce accurate and human-readable interpretations. Experimental\nevaluation on multiple IoT captures and six open models shows that hybrid\nretrieval, which combines lexical and semantic search with reranking,\nsubstantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with\ndense-only retrieval. System profiling further indicates low CPU, GPU, and\nmemory overhead, demonstrating that the framework achieves holistic and\nefficient interpretation of IoT network traffic.", "AI": {"tldr": "该研究提出了一个由大型语言模型（LLM）驱动的AI代理框架，用于将物联网（IoT）网络的原始数据包捕获转换为结构化、语义丰富的表示，以进行交互式分析和威胁解释。", "motivation": "物联网网络产生多样化、高容量的流量，需要跨层解释行为、协议和上下文来识别正常活动和潜在威胁，而非孤立的检测。", "method": "该框架集成了特征提取、基于Transformer的异常检测、数据包和流摘要、威胁情报丰富以及检索增强问答。一个由LLM引导的AI代理对索引的流量进行推理，并组装证据以生成可读的解释。实验评估了混合检索（结合词汇和语义搜索与重排）的有效性。", "result": "在多个物联网捕获和六个开放模型上的实验表明，混合检索相比仅密集检索显著提高了BLEU、ROUGE、METEOR和BERTScore分数。系统分析也显示了较低的CPU、GPU和内存开销。", "conclusion": "该框架实现了对物联网网络流量的整体且高效的解释，能够将原始数据转化为有意义的洞察。"}}
{"id": "2510.14980", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14980", "abs": "https://arxiv.org/abs/2510.14980", "authors": ["Wenqian Zhang", "Weiyang Liu", "Zhen Liu"], "title": "Agentic Design of Compositional Machines", "comment": "75 pages, 31 figures, Project Page: https://besiegefield.github.io", "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.", "AI": {"tldr": "本文探讨大型语言模型（LLMs）在组合式机器设计中的能力，引入BesiegeField测试平台进行评估。发现当前LLMs表现不足，并探索通过强化学习（RL）进行改进。", "motivation": "复杂机器设计是人类智慧和工程实践的标志。鉴于LLMs的最新进展，研究者想知道LLMs是否也能学习创造，特别是能否通过标准化组件组装机器以满足特定功能需求。", "method": "研究引入了基于游戏Besiege的BesiegeField测试平台，支持基于部件的构建、物理模拟和奖励驱动评估。使用该平台，研究者通过智能体工作流对现有LLMs进行基准测试，并探索了通过整理冷启动数据集和进行RL微调实验来改进模型的方法。", "result": "研究确定了成功进行机器设计所需的关键能力，包括空间推理、策略性组装和指令遵循。结果表明，当前的开源LLMs在这些方面表现不足。同时，文章也进行了RL微调实验，并指出了语言、机器设计和物理推理交叉领域的开放性挑战。", "conclusion": "尽管LLMs在机器设计方面面临空间推理等挑战，但通过BesiegeField平台和强化学习等方法，有望推动LLMs在该领域的进步，揭示了语言、机器设计和物理推理之间交叉领域的未解决问题。"}}
{"id": "2510.14617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14617", "abs": "https://arxiv.org/abs/2510.14617", "authors": ["Ning Ding", "Keisuke Fujii", "Toru Tamaki"], "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding", "comment": "9 pages, 3 figures. Accepted to ACM MMSports 2025", "summary": "Tactical understanding in badminton involves interpreting not only individual\nactions but also how tactics are dynamically executed over time. In this paper,\nwe propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and\ntemporal multi-scale video captioning in badminton, capable of generating\nshot-level captions that describe individual actions and tactic-level captions\nthat capture how these actions unfold over time within a tactical execution. We\nalso introduce the Shot2Tactic-Caption Dataset, the first badminton captioning\ndataset containing 5,494 shot captions and 544 tactic captions.\nShot2Tactic-Caption adopts a dual-branch design, with both branches including a\nvisual encoder, a spatio-temporal Transformer encoder, and a Transformer-based\ndecoder to generate shot and tactic captions. To support tactic captioning, we\nadditionally introduce a Tactic Unit Detector that identifies valid tactic\nunits, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic\ncaptioning, we further incorporate a shot-wise prompt-guided mechanism, where\nthe predicted tactic type and state are embedded as prompts and injected into\nthe decoder via cross-attention. The shot-wise prompt-guided mechanism enables\nour system not only to describe successfully executed tactics but also to\ncapture tactical executions that are temporarily interrupted and later resumed.\nExperimental results demonstrate the effectiveness of our framework in\ngenerating both shot and tactic captions. Ablation studies show that the\nResNet50-based spatio-temporal encoder outperforms other variants, and that\nshot-wise prompt structuring leads to more coherent and accurate tactic\ncaptioning.", "AI": {"tldr": "本文提出了一个名为Shot2Tactic-Caption的新型框架，用于羽毛球视频的语义和时序多尺度视频字幕生成，能够同时生成描述个体动作的击球级字幕和捕捉战术动态执行的战术级字幕，并发布了首个羽毛球字幕数据集。", "motivation": "在羽毛球中，战术理解不仅涉及对个体动作的解读，还需要理解战术如何随时间动态执行。现有方法可能无法充分捕捉这种动态的战术执行过程。", "method": "该框架采用双分支设计，每个分支包含视觉编码器、时空Transformer编码器和基于Transformer的解码器，分别生成击球和战术字幕。为支持战术字幕生成，引入了战术单元检测器来识别有效的战术单元、战术类型和战术状态。此外，还整合了击球级提示引导机制，将预测的战术类型和状态作为提示嵌入并通过交叉注意力注入解码器，以描述成功执行以及中断后恢复的战术。同时，发布了Shot2Tactic-Caption数据集，包含5,494个击球字幕和544个战术字幕。", "result": "实验结果表明，所提出的框架在生成击球和战术字幕方面均有效。消融研究显示，基于ResNet50的时空编码器优于其他变体，且击球级提示结构有助于提高战术字幕的连贯性和准确性。", "conclusion": "Shot2Tactic-Caption框架能有效生成羽毛球视频的多尺度字幕，包括个体动作和复杂的战术执行动态，其创新性的提示引导机制尤其有助于捕捉中断和恢复的战术情景。"}}
{"id": "2510.13926", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13926", "abs": "https://arxiv.org/abs/2510.13926", "authors": ["Congying Liu", "Xingyuan Wei", "Peipei Liu", "Yiqing Shen", "Yanxu Mao", "Tiehan Cui"], "title": "BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs", "comment": null, "summary": "Biomedical queries often rely on a deep understanding of specialized\nknowledge such as gene regulatory mechanisms and pathological processes of\ndiseases. They require detailed analysis of complex physiological processes and\neffective integration of information from multiple data sources to support\naccurate retrieval and reasoning. Although large language models (LLMs) perform\nwell in general reasoning tasks, their generated biomedical content often lacks\nscientific rigor due to the inability to access authoritative biomedical\ndatabases and frequently fabricates protein functions, interactions, and\nstructural details that deviate from authentic information. Therefore, we\npresent BioMedSearch, a multi-source biomedical information retrieval framework\nbased on LLMs. The method integrates literature retrieval, protein database and\nweb search access to support accurate and efficient handling of complex\nbiomedical queries. Through sub-queries decomposition, keywords extraction,\ntask graph construction, and multi-source information filtering, BioMedSearch\ngenerates high-quality question-answering results. To evaluate the accuracy of\nquestion answering, we constructed a multi-level dataset, BioMedMCQs,\nconsisting of 3,000 questions. The dataset covers three levels of reasoning:\nmechanistic identification, non-adjacent semantic integration, and temporal\ncausal reasoning, and is used to assess the performance of BioMedSearch and\nother methods on complex QA tasks. Experimental results demonstrate that\nBioMedSearch consistently improves accuracy over all baseline models across all\nlevels. Specifically, at Level 1, the average accuracy increases from 59.1% to\n91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging\nLevel 3, the average accuracy improves from 36.3% to 73.4%. The code and\nBioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search", "AI": {"tldr": "本文提出了BioMedSearch，一个基于LLM的多源生物医学信息检索框架，通过整合文献、蛋白质数据库和网络搜索，显著提高了复杂生物医学查询的准确性，并在新构建的多级数据集BioMedMCQs上表现优异。", "motivation": "尽管大型语言模型（LLMs）在通用推理任务中表现良好，但其生成的生物医学内容常缺乏科学严谨性，因无法访问权威生物医学数据库而频繁捏造蛋白质功能、相互作用和结构细节，这促使研究者寻求一种能有效整合多源信息的解决方案。", "method": "BioMedSearch框架整合了文献检索、蛋白质数据库和网络搜索。它通过子查询分解、关键词提取、任务图构建和多源信息过滤来生成高质量的问答结果。为评估准确性，研究团队构建了一个包含3000个问题的多级数据集BioMedMCQs，涵盖了机制识别、非邻近语义整合和时间因果推理三个推理级别。", "result": "实验结果表明，BioMedSearch在所有基线模型和所有推理级别上都持续提高了准确性。具体而言，在级别1，平均准确率从59.1%提高到91.9%；在级别2，从47.0%提高到81.0%；在最具挑战性的级别3，平均准确率从36.3%提高到73.4%。", "conclusion": "BioMedSearch通过有效整合多源生物医学信息，显著解决了LLM在生物医学领域信息准确性不足的问题，并在复杂的生物医学问答任务中展现出卓越的性能提升。"}}
{"id": "2510.14624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14624", "abs": "https://arxiv.org/abs/2510.14624", "authors": ["Natan Bagrov", "Eugene Khvedchenia", "Borys Tymchenko", "Shay Aharon", "Lior Kadoch", "Tomer Keren", "Ofri Masad", "Yonatan Geifman", "Ran Zilberstein", "Tuomas Rintamaki", "Matthieu Le", "Andrew Tao"], "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference", "comment": null, "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.", "AI": {"tldr": "本文提出高效视频采样（EVS）方法，通过识别并修剪视频中时间上静态的区域来减少令牌冗余，从而显著降低视频-语言模型（VLM）处理长视频的计算成本，提高推理速度，并支持更长的输入序列，同时保持语义完整性。", "motivation": "当前视觉-语言模型（VLM）处理视频时，由于处理密集帧序列的二次方成本，其可扩展性受到基本限制。长视频经常超出现代语言模型的令牌预算，导致严重的上下文限制和延迟问题。", "method": "高效视频采样（EVS）是一种即插即用的方法，通过识别并修剪连续帧之间保持不变的时间静态区域（空间区域）来减少视频中的令牌冗余。EVS保留位置身份，不需要架构更改或重新训练。它可以在推理时应用，也可以与使用随机修剪率的训练阶段相结合。", "result": "EVS显著减少了令牌数量，同时保持了语义保真度，实现了更快的推理和更长的输入序列。在推理时应用时，EVS将大型语言模型（LLM）的首次令牌生成时间（TTFT）最多缩短了4倍，而准确性损失极小。当与使用随机修剪率的训练阶段结合时，EVS能够使模型对不同的压缩级别保持鲁棒性，并在激进修剪下保持完整性能。实验证明EVS持续改善了效率-准确性权衡。", "conclusion": "EVS通过持续改善效率-准确性权衡，在不牺牲质量的情况下实现了可扩展的视频-语言理解，有效解决了VLM处理长视频的成本和效率问题。"}}
{"id": "2510.13928", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13928", "abs": "https://arxiv.org/abs/2510.13928", "authors": ["Shuo Xing", "Junyuan Hong", "Yifan Wang", "Runjin Chen", "Zhenyu Zhang", "Ananth Grama", "Zhengzhong Tu", "Zhangyang Wang"], "title": "LLMs Can Get \"Brain Rot\"!", "comment": null, "summary": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk\nweb text induces lasting cognitive decline in large language models (LLMs). To\ncausally isolate data quality, we run controlled experiments on real Twitter/X\ncorpora, constructing junk and reversely controlled datasets via two orthogonal\noperationalizations: M1 (engagement degree) and M2 (semantic quality), with\nmatched token scale and training operations across conditions. Contrary to the\ncontrol group, continual pre-training of 4 LLMs on the junk dataset causes\nnon-trivial declines (Hedges' $g>0.3$) on reasoning, long-context\nunderstanding, safety, and inflating \"dark traits\" (e.g., psychopathy,\nnarcissism). The gradual mixtures of junk and control datasets also yield\ndose-response cognition decay: for example, under M1, ARC-Challenge with Chain\nOf Thoughts drops $74.9 \\rightarrow 57.2$ and RULER-CWE $84.4 \\rightarrow 52.3$\nas junk ratio rises from $0\\%$ to $100\\%$.\n  Error forensics reveal several key insights. First, we identify\nthought-skipping as the primary lesion: models increasingly truncate or skip\nreasoning chains, explaining most of the error growth. Second, partial but\nincomplete healing is observed: scaling instruction tuning and clean data\npre-training improve the declined cognition yet cannot restore baseline\ncapability, suggesting persistent representational drift rather than format\nmismatch. Finally, we discover that the popularity, a non-semantic metric, of a\ntweet is a better indicator of the Brain Rot effect than the length in M1.\nTogether, the results provide significant, multi-perspective evidence that data\nquality is a causal driver of LLM capability decay, reframing curation for\ncontinual pretraining as a \\textit{training-time safety} problem and motivating\nroutine \"cognitive health checks\" for deployed LLMs.", "AI": {"tldr": "研究提出并验证了LLM脑损伤假说：持续暴露于低质量网络文本会导致大型语言模型（LLM）认知能力持续下降。数据质量是LLM能力衰退的因果驱动因素。", "motivation": "随着LLM的持续预训练和对网络数据的依赖，研究旨在探究数据质量（特别是“垃圾”网络文本）是否会导致LLM的认知能力下降，并将其提升为训练时的安全问题。", "method": "通过受控实验，在真实的Twitter/X语料库上构建了“垃圾”和“受控”数据集。采用两种正交的操作化方法：M1（参与度）和M2（语义质量）来定义数据质量。在匹配的token规模和训练操作下，对4个LLM进行持续预训练。通过错误分析（如思维跳过）和评估指令微调及干净数据预训练的恢复效果来深入探究。", "result": "持续在“垃圾”数据集上预训练会导致LLM在推理、长上下文理解、安全性以及“黑暗特质”（如精神病态、自恋）方面出现显著下降（Hedges' g > 0.3）。“垃圾”数据比例的增加会导致认知能力呈剂量-反应式衰退。主要的错误原因是“思维跳过”。指令微调和干净数据预训练能部分改善但无法完全恢复基线能力。推文的受欢迎程度（非语义指标）比长度更能预测“脑损伤”效应。", "conclusion": "数据质量是LLM能力衰退的因果驱动因素，这使得持续预训练的数据管理成为一个“训练时安全”问题。研究结果也提示了对已部署LLM进行常规“认知健康检查”的必要性。"}}
{"id": "2510.13916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13916", "abs": "https://arxiv.org/abs/2510.13916", "authors": ["Yuanhao Li", "Keyuan Lai", "Tianqi Wang", "Qihao Liu", "Jiawei Ma", "Yuan-Chao Hu"], "title": "Element2Vec: Build Chemical Element Representation from Text for Property Prediction", "comment": null, "summary": "Accurate property data for chemical elements is crucial for materials design\nand manufacturing, but many of them are difficult to measure directly due to\nequipment constraints. While traditional methods use the properties of other\nelements or related properties for prediction via numerical analyses, they\noften fail to model complex relationships. After all, not all characteristics\ncan be represented as scalars. Recent efforts have been made to explore\nadvanced AI tools such as language models for property estimation, but they\nstill suffer from hallucinations and a lack of interpretability. In this paper,\nwe investigate Element2Vecto effectively represent chemical elements from\nnatural languages to support research in the natural sciences. Given the text\nparsed from Wikipedia pages, we use language models to generate both a single\ngeneral-purpose embedding (Global) and a set of attribute-highlighted vectors\n(Local). Despite the complicated relationship across elements, the\ncomputational challenges also exist because of 1) the discrepancy in text\ndistribution between common descriptions and specialized scientific texts, and\n2) the extremely limited data, i.e., with only 118 known elements, data for\nspecific properties is often highly sparse and incomplete. Thus, we also design\na test-time training method based on self-attention to mitigate the prediction\nerror caused by Vanilla regression clearly. We hope this work could pave the\nway for advancing AI-driven discovery in materials science.", "AI": {"tldr": "本文提出了Element2Vecto模型，利用语言模型从自然语言中有效表征化学元素，生成全局和局部嵌入，并设计了一种测试时训练方法来缓解预测误差，以支持材料科学研究。", "motivation": "化学元素的准确性质数据对材料设计至关重要，但许多难以直接测量。传统方法难以建模复杂关系，且无法表示非标量特性。现有AI工具（如语言模型）在性质估计中存在幻觉和可解释性差的问题。此外，面临普通描述与专业科学文本间的文本分布差异，以及元素数据（仅118种）极其有限且稀疏不完整等计算挑战。", "method": "本研究从维基百科页面解析文本，并利用语言模型生成两种类型的元素表示：单一的通用嵌入（全局）和一组突出属性的向量（局部）。此外，还设计了一种基于自注意力机制的测试时训练方法，以缓解传统回归模型造成的预测误差。", "result": "本研究成功开发了Element2Vecto模型，能够从自然语言中有效表征化学元素，并生成全局和局部嵌入。同时，还设计了一种基于自注意力机制的测试时训练方法，旨在显著缓解传统回归模型在预测中遇到的误差，以应对数据稀疏和文本分布差异等挑战。", "conclusion": "这项工作有望为推动材料科学领域的AI驱动发现铺平道路。"}}
{"id": "2510.14630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14630", "abs": "https://arxiv.org/abs/2510.14630", "authors": ["Ming Gui", "Johannes Schusterbauer", "Timy Phan", "Felix Krause", "Josh Susskind", "Miguel Angel Bautista", "Björn Ommer"], "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation", "comment": "Code: https://github.com/CompVis/RepTok", "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling.", "AI": {"tldr": "RepTok是一个生成模型框架，它利用自监督视觉Transformer获得的单个连续潜在token来表示图像。通过微调预训练的SSL编码器并结合流匹配训练的生成解码器，RepTok实现了高效且高质量的图像重建和生成。", "motivation": "研究旨在解决二维潜在空间中的空间冗余问题，并显著降低生成模型的训练成本，同时保持或提高生成质量。", "method": "RepTok基于预训练的自监督学习（SSL）编码器，仅微调语义token嵌入，并将其与使用标准流匹配目标联合训练的生成解码器配对。为了丰富token的低级重建细节并保留原始SSL空间的良好几何特性，模型添加了余弦相似度损失来正则化适应后的token。核心方法是使用单个连续潜在token来表示图像。", "result": "RepTok在类别条件ImageNet生成上取得了具有竞争力的结果，并且能够自然地扩展到文本到图像合成，在MS-COCO上以极其有限的训练预算实现了具有竞争力的零样本性能。其单token公式解决了空间冗余并显著降低了训练成本，同时能够忠实地重建图像。", "conclusion": "研究结果强调了微调后的SSL表示作为紧凑且有效的潜在空间，在高效生成建模方面的巨大潜力。"}}
{"id": "2510.13935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13935", "abs": "https://arxiv.org/abs/2510.13935", "authors": ["Kenan Alkiek", "David Jurgens", "Vinod Vydiswaran"], "title": "Big Reasoning with Small Models: Instruction Retrieval at Inference Time", "comment": null, "summary": "Can we bring large-scale reasoning to local-scale compute? Small language\nmodels (SLMs) are increasingly attractive because they run efficiently on local\nhardware, offering strong privacy, low cost, and reduced environmental impact.\nYet they often struggle with tasks that require multi-step reasoning or\ndomain-specific knowledge. We address this limitation through instruction\nintervention at inference time, where an SLM retrieves structured reasoning\nprocedures rather than generating them from scratch. Our method builds an\nInstruction Corpus by grouping similar training questions and creating\ninstructions via GPT-5. During inference, the SLM retrieves the most relevant\ninstructions and follows their steps. Unlike retrieval-augmented generation,\nwhich retrieves text passages, instruction retrieval gives the model structured\nguidance for reasoning. We evaluate this framework on MedQA (medical board\nexams), MMLU Professional Law, and MathQA using models from 3B to 14B\nparameters without any additional fine-tuning. Instruction retrieval yields\nconsistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise\ninstructions outperform longer ones, and the magnitude of improvement depends\nstrongly on model family and intrinsic reasoning ability.", "AI": {"tldr": "本文提出了一种在推理时通过指令检索来增强小型语言模型（SLM）推理能力的方法。SLM不再从头生成推理过程，而是检索由GPT-5生成的结构化推理指令，从而在多个推理任务上实现了显著的性能提升。", "motivation": "小型语言模型（SLM）因其在本地硬件上高效运行、提供隐私、低成本和减少环境影响等优势而日益受到关注。然而，它们在需要多步推理或领域特定知识的任务上表现不佳，这是本研究旨在解决的限制。", "method": "该方法在推理时通过指令干预来解决SLM的限制。它首先通过将相似的训练问题分组并使用GPT-5创建指令来构建一个“指令语料库”。在推理过程中，SLM会检索最相关的指令并遵循其步骤。与检索增强生成（RAG）检索文本段落不同，指令检索为模型提供了结构化的推理指导。", "result": "该框架在MedQA（医学执照考试）、MMLU专业法律和MathQA上进行了评估，使用了3B到14B参数的模型，且未进行额外微调。指令检索带来了持续的性能提升：MedQA上提升9.4%，MMLU法律上提升7.9%，MathQA上提升5.1%。研究还发现，简洁的指令优于较长的指令，并且改进的幅度强烈依赖于模型家族和固有的推理能力。", "conclusion": "通过在推理时引入指令检索，可以有效弥补小型语言模型在多步推理和领域知识方面的不足。这种方法提供了结构化的推理指导，并在多个基准测试中取得了显著的性能提升，表明其是增强SLM推理能力的一种有效策略。"}}
{"id": "2510.14634", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14634", "abs": "https://arxiv.org/abs/2510.14634", "authors": ["Jihyun Yu", "Yoojin Oh", "Wonho Bae", "Mingyu Kim", "Junhyug Noh"], "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation", "comment": null, "summary": "Test-time adaptation (TTA) aims to correct performance degradation of deep\nmodels under distribution shifts by updating models or inputs using unlabeled\ntest data. Input-only diffusion-based TTA methods improve robustness for\nclassification to corruptions but rely on gradient guidance, limiting\nexploration and generalization across distortion types. We propose SteeringTTA,\nan inference-only framework that adapts Feynman-Kac steering to guide\ndiffusion-based input adaptation for classification with rewards driven by\npseudo-label. SteeringTTA maintains multiple particle trajectories, steered by\na combination of cumulative top-K probabilities and an entropy schedule, to\nbalance exploration and confidence. On ImageNet-C, SteeringTTA consistently\noutperforms the baseline without any model updates or source data.", "AI": {"tldr": "SteeringTTA提出了一种基于Feynman-Kac引导的扩散模型输入自适应方法，通过伪标签奖励和多粒子轨迹引导（结合top-K概率和熵调度）实现测试时自适应，在ImageNet-C上表现优异，无需模型更新或源数据。", "motivation": "现有的仅输入（input-only）的扩散模型测试时自适应（TTA）方法依赖梯度引导，限制了探索能力和对不同失真类型的泛化能力，导致在分布偏移下深度模型性能下降。", "method": "本文提出了SteeringTTA，一个仅推理（inference-only）的框架。它将Feynman-Kac引导适应于扩散模型驱动的输入自适应，以进行分类。该方法使用伪标签驱动奖励，并通过结合累积的top-K概率和熵调度来引导多个粒子轨迹，以平衡探索和置信度。", "result": "在ImageNet-C数据集上，SteeringTTA持续优于基线方法，并且在过程中无需任何模型更新或源数据。", "conclusion": "SteeringTTA通过Feynman-Kac引导的扩散模型输入自适应，有效解决了测试时自适应中的探索和泛化限制，在不修改模型或使用源数据的情况下显著提升了模型在分布偏移下的鲁棒性。"}}
{"id": "2510.13931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13931", "abs": "https://arxiv.org/abs/2510.13931", "authors": ["Siying Liu", "Shisheng Zhang", "Indu Bala"], "title": "Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions", "comment": "Preprint of a paper accepted as a poster at the NeurIPS 2025 Workshop\n  on Generative AI for Health (GenAI4Health). The final camera-ready workshop\n  version may differ. Licensed under CC BY 4.0", "summary": "Large language models (LLMs) are increasingly applied in biomedical domains,\nyet their reliability in drug-safety prediction remains underexplored. In this\nwork, we investigate whether LLMs incorporate socio-demographic information\ninto adverse event (AE) predictions, despite such attributes being clinically\nirrelevant. Using structured data from the United States Food and Drug\nAdministration Adverse Event Reporting System (FAERS) and a persona-based\nevaluation framework, we assess two state-of-the-art models, ChatGPT-4o and\nBio-Medical-Llama-3.8B, across diverse personas defined by education, marital\nstatus, employment, insurance, language, housing stability, and religion. We\nfurther evaluate performance across three user roles (general practitioner,\nspecialist, patient) to reflect real-world deployment scenarios where\ncommercial systems often differentiate access by user type. Our results reveal\nsystematic disparities in AE prediction accuracy. Disadvantaged groups (e.g.,\nlow education, unstable housing) were frequently assigned higher predicted AE\nlikelihoods than more privileged groups (e.g., postgraduate-educated, privately\ninsured). Beyond outcome disparities, we identify two distinct modes of bias:\nexplicit bias, where incorrect predictions directly reference persona\nattributes in reasoning traces, and implicit bias, where predictions are\ninconsistent, yet personas are not explicitly mentioned. These findings expose\ncritical risks in applying LLMs to pharmacovigilance and highlight the urgent\nneed for fairness-aware evaluation protocols and mitigation strategies before\nclinical deployment.", "AI": {"tldr": "本研究发现大型语言模型在药物安全不良事件预测中存在系统性偏见，对弱势群体预测出更高的不良事件可能性，并揭示了显性和隐性偏见模式。", "motivation": "尽管大型语言模型（LLMs）在生物医学领域应用日益广泛，但其在药物安全预测中的可靠性，特别是在是否会将社会人口学信息纳入不良事件（AE）预测方面，仍未得到充分探索。", "method": "研究使用了美国食品药品监督管理局不良事件报告系统（FAERS）的结构化数据，并采用基于角色（persona-based）的评估框架。评估了ChatGPT-4o和Bio-Medical-Llama-3.8B两款模型，涵盖了教育、婚姻状况、就业、保险、语言、住房稳定性及宗教等不同角色。此外，还评估了三种用户角色（全科医生、专科医生、患者）的表现。", "result": "研究揭示了不良事件预测准确性中存在的系统性差异。弱势群体（如低教育水平、住房不稳定）被预测出更高的不良事件可能性，而特权群体（如研究生学历、私人保险）则相反。研究还识别出两种偏见模式：显性偏见（预测推理直接提及角色属性）和隐性偏见（预测不一致但未明确提及角色属性）。", "conclusion": "这些发现揭示了将LLMs应用于药物警戒的严重风险，并强调在临床部署前，迫切需要制定公平性评估协议和缓解策略。"}}
{"id": "2510.13936", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13936", "abs": "https://arxiv.org/abs/2510.13936", "authors": ["Fengbin Zhu", "Xiang Yao Ng", "Ziyang Liu", "Chang Liu", "Xianwei Zeng", "Chao Wang", "Tianhui Tan", "Xuan Yao", "Pengyang Shao", "Min Xu", "Zixuan Wang", "Jing Wang", "Xin Lin", "Junfeng Li", "Jingxian Zhu", "Yang Zhang", "Wenjie Wang", "Fuli Feng", "Richang Hong", "Huanbo Luan", "Ke-Wei Huang", "Tat-Seng Chua"], "title": "FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis", "comment": null, "summary": "Deep Research (DR) agents, powered by advanced Large Language Models (LLMs),\nhave recently garnered increasing attention for their capability in conducting\ncomplex research tasks. However, existing literature lacks a rigorous and\nsystematic evaluation of DR Agent's capabilities in critical research analysis.\nTo address this gap, we first propose HisRubric, a novel evaluation framework\nwith a hierarchical analytical structure and a fine-grained grading rubric for\nrigorously assessing DR agents' capabilities in corporate financial analysis.\nThis framework mirrors the professional analyst's workflow, progressing from\ndata recognition to metric calculation, and finally to strategic summarization\nand interpretation. Built on this framework, we construct a FinDeepResearch\nbenchmark that comprises 64 listed companies from 8 financial markets across 4\nlanguages, encompassing a total of 15,808 grading items. We further conduct\nextensive experiments on the FinDeepResearch using 16 representative methods,\nincluding 6 DR agents, 5 LLMs equipped with both deep reasoning and search\ncapabilities, and 5 LLMs with deep reasoning capabilities only. The results\nreveal the strengths and limitations of these approaches across diverse\ncapabilities, financial markets, and languages, offering valuable insights for\nfuture research and development. The benchmark and evaluation code will be made\npublicly available.", "AI": {"tldr": "本研究旨在解决深度研究（DR）智能体在关键研究分析方面缺乏严格系统评估的问题。为此，我们提出了HisRubric评估框架和FinDeepResearch基准，并对16种方法进行了广泛实验，揭示了它们在财务分析中的优缺点。", "motivation": "现有文献缺乏对由大型语言模型（LLMs）驱动的深度研究（DR）智能体在关键研究分析能力方面进行严格和系统评估。", "method": "1. 提出了HisRubric评估框架：一个具有分层分析结构和细粒度评分标准的框架，用于严格评估DR智能体在企业财务分析中的能力，模仿专业分析师的工作流程。2. 构建了FinDeepResearch基准：包含来自8个金融市场、4种语言的64家上市公司，共计15,808个评分项。3. 进行了广泛实验：使用16种代表性方法（包括6个DR智能体、5个具备深度推理和搜索能力的LLM以及5个仅具备深度推理能力的LLM）在FinDeepResearch上进行评估。", "result": "实验结果揭示了这些方法在不同能力、金融市场和语言方面的优势和局限性。", "conclusion": "本研究为未来的研究和开发提供了宝贵的见解。基准和评估代码将公开发布。"}}
{"id": "2510.13940", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13940", "abs": "https://arxiv.org/abs/2510.13940", "authors": ["Zhen Yang", "Mingyang Zhang", "Feng Chen", "Ganggui Ding", "Liang Hou", "Xin Tao", "Pengfei Wan", "Ying-Cong Chen"], "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention", "comment": "Code: https://github.com/EnVision-Research/MTI", "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.", "AI": {"tldr": "本文提出了一种名为MTI（Minimal Test-Time Intervention）的无训练框架，通过选择性地干预高不确定性位置，以最小的开销提高大型语言模型（LLMs）的推理准确性和稳定性。", "motivation": "当前大型语言模型在推理时通过增加计算量来提高性能，但往往牺牲了效率。研究发现，推理不确定性高度局部化，只有一小部分高熵词元会显著影响输出的正确性，这促使研究者寻求一种更高效的干预方法。", "method": "本文提出了MTI框架，包含两部分：(i) 选择性CFG干预：仅在不确定位置应用无分类器指导（classifier-free guidance）；(ii) 轻量级负面提示指导：通过重用主模型的KV缓存来高效近似无条件解码。", "result": "MTI在通用、编码和STEM任务上均取得了显著且稳定的性能提升。例如，在Qwen3-8B-Base模型上，8个基准测试平均提升了1.35%；在Qwen3-32B-Reasoning模型上，AIME2024任务提升了5%。同时，该方法保持了高效率。", "conclusion": "MTI通过识别并选择性地干预推理过程中不确定性高的局部区域，以极小的开销显著提升了大型语言模型的推理准确性和稳定性，为提高LLM效率提供了一条新途径。"}}
{"id": "2510.14030", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14030", "abs": "https://arxiv.org/abs/2510.14030", "authors": ["César Guerra-Solano", "Zhuochun Li", "Xiang Lorraine Li"], "title": "Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games", "comment": "EMNLP Main 2025", "summary": "Large language models (LLMs) can exhibit biases in reasoning capabilities due\nto linguistic modality, performing better on tasks in one language versus\nanother, even with similar content. Most previous works evaluate this through\nreasoning tasks where reliance on strategies or knowledge can ensure success,\nsuch as in commonsense or math tasks. However, abstract reasoning is vital to\nreasoning for everyday life, where people apply \"out-of-the-box thinking\" to\nidentify and use patterns for solutions, without a reliance on formulaic\napproaches. Comparatively, little work has evaluated linguistic biases in this\ntask type. In this paper, we propose a task inspired by the New York Times\nConnections: GlobalGroup, that evaluates models in an abstract reasoning task\nacross several languages. We constructed a game benchmark with five linguistic\nbackgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the\nnative language and an English translation for comparison. We also proposed\ngame difficulty measurements to evaluate models on games with similar\ndifficulty, enabling a more controlled comparison, which is particularly\nimportant in reasoning evaluations. Through experimentation, we find English\nmodalities largely lead to better performance in this abstract reasoning task,\nand performance disparities between open- and closed-source models.", "AI": {"tldr": "本研究通过提出一个名为GlobalGroup的抽象推理任务，评估了大型语言模型（LLMs）在多语言环境下的语言偏见，发现LLMs在此类任务中表现出明显的英语偏好。", "motivation": "以往关于LLM语言偏见的研究主要集中在依赖策略或知识的推理任务（如常识或数学）上。然而，抽象推理对于日常生活中的“跳出固有思维”和模式识别至关重要，但很少有工作评估LLMs在此类任务中的语言偏见。", "method": "研究提出一个受《纽约时报》Connections启发的抽象推理任务GlobalGroup。构建了一个包含五种语言（英语、西班牙语、中文、印地语和阿拉伯语）的游戏基准，并为每种语言提供了原生版本和英语翻译版本以进行比较。同时，提出了游戏难度测量方法，以实现更受控的推理评估。", "result": "实验结果显示，英语模式在抽象推理任务中普遍表现更好。此外，开源模型和闭源模型之间存在性能差异。", "conclusion": "大型语言模型在抽象推理任务中表现出语言偏见，英语模式通常能带来更好的性能。同时，不同类型的模型（开源与闭源）之间也存在性能差距。"}}
{"id": "2510.14648", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14648", "abs": "https://arxiv.org/abs/2510.14648", "authors": ["Xinyao Liao", "Xianfang Zeng", "Ziye Song", "Zhoujie Fu", "Gang Yu", "Guosheng Lin"], "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "comment": null, "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality.", "AI": {"tldr": "本文提出了一种低成本的预训练策略，利用无配对视频片段进行上下文学习，以解决指令式视频编辑中大规模配对数据集的构建难题。该策略使基础视频生成模型具备通用编辑能力，并通过少量高质量配对数据进行微调，显著提升了指令遵循和视觉保真度。", "motivation": "指令式图像编辑发展迅速，但其在视频领域的扩展仍未充分探索，主要原因是构建大规模配对视频编辑数据集的成本和复杂性过高。", "method": "该方法首先基于HunyuanVideoT2V构建，采用低成本预训练策略，利用约100万个无配对真实视频片段进行上下文学习，以习得基本的编辑概念。随后，通过少于15万个精选配对编辑数据进行高效微调，以扩展更多编辑任务并提升编辑质量。", "result": "该方法在指令遵循和视觉保真度方面均超越了现有的指令式视频编辑方法，在编辑指令遵循上提升了12%，在编辑质量上提升了15%。预训练模型获得了根据输入指令进行添加、替换或删除等通用编辑能力。", "conclusion": "所提出的低成本预训练策略，结合少量高质量配对数据的精炼，能够有效解决指令式视频编辑的数据挑战，并显著提升编辑性能，实现了指令遵循和视觉保真度上的领先水平。"}}
{"id": "2510.13939", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.13939", "abs": "https://arxiv.org/abs/2510.13939", "authors": ["Tuhin Chakrabarty", "Jane C. Ginsburg", "Paramveer Dhillon"], "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers", "comment": "Preprint Under Review", "summary": "The use of copyrighted books for training AI models has led to numerous\nlawsuits from authors concerned about AI's ability to generate derivative\ncontent.Yet it's unclear whether these models can generate high quality\nliterary text while emulating authors' styles. To answer this we conducted a\npreregistered study comparing MFA-trained expert writers with three frontier AI\nmodels: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating\n50 award-winning authors' diverse styles. In blind pairwise evaluations by 159\nrepresentative expert & lay readers, AI-generated text from in-context\nprompting was strongly disfavored by experts for both stylistic fidelity\n(OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results\nwith lay readers. However, fine-tuning ChatGPT on individual authors' complete\nworks completely reversed these findings: experts now favored AI-generated text\nfor stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010),\nwith lay readers showing similar shifts. These effects generalize across\nauthors & styles. The fine-tuned outputs were rarely flagged as AI-generated\n(3% rate v. 97% for in-context prompting) by best AI detectors. Mediation\nanalysis shows this reversal occurs because fine-tuning eliminates detectable\nAI stylistic quirks (e.g., cliche density) that penalize in-context outputs.\nWhile we do not account for additional costs of human effort required to\ntransform raw AI output into cohesive, publishable prose, the median\nfine-tuning & inference cost of $81 per author represents a dramatic 99.7%\nreduction compared to typical professional writer compensation. Author-specific\nfine-tuning thus enables non-verbatim AI writing that readers prefer to expert\nhuman writing, providing empirical evidence directly relevant to copyright's\nfourth fair-use factor, the \"effect upon the potential market or value\" of the\nsource works.", "AI": {"tldr": "本研究发现，通过对特定作者的完整作品进行微调，AI模型能够生成专家读者更偏爱的、具有高度风格忠实度和写作质量的文学文本，且成本远低于人类作家，对版权法中的“市场影响”因素具有重要意义。", "motivation": "由于AI模型使用受版权保护的书籍进行训练，引发了大量关于AI生成衍生内容的担忧和诉讼。然而，目前尚不清楚AI模型是否能在模仿作者风格的同时生成高质量的文学文本。", "method": "研究采用预注册研究设计，将MFA（艺术硕士）培训的专业作家与ChatGPT、Claude和Gemini这三个前沿AI模型进行比较，模仿50位获奖作家的不同风格撰写450字以内的文本。159名专家和普通读者进行盲测配对评估。比较了两种AI生成方式：上下文提示（in-context prompting）和对单个作者完整作品进行微调（fine-tuning）。同时使用了AI检测器，并进行了中介分析和成本分析。", "result": "对于上下文提示生成的文本，专家读者在风格忠实度（OR=0.16）和写作质量（OR=0.13）上强烈不看好AI，而普通读者结果好坏参半。AI检测器识别率高达97%。然而，通过对单个作者作品进行微调后，结果完全逆转：专家读者现在更偏爱AI生成的文本，认为其风格忠实度更高（OR=8.16）且写作质量更好（OR=1.87），普通读者也表现出类似转变。微调后的AI文本很少被AI检测器识别（3%）。中介分析显示，这种逆转是由于微调消除了AI特有的风格缺陷（如陈词滥调）。此外，微调和推理的平均成本为每位作者81美元，比专业作家报酬降低了99.7%。", "conclusion": "针对特定作者的微调使AI能够生成非逐字模仿的文本，且读者（包括专家）更偏爱这种AI文本而非专家人类写作。这为版权法中“对潜在市场或价值的影响”这一公平使用因素提供了直接的经验证据。"}}
{"id": "2510.14657", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14657", "abs": "https://arxiv.org/abs/2510.14657", "authors": ["Kieran Carrigg", "Rob van Gastel", "Melda Yeghaian", "Sander Dalm", "Faysal Boughorbel", "Marcel van Gerven"], "title": "Decorrelation Speeds Up Vision Transformers", "comment": "15 pages, 12 figures, submitted to ICLR 2026", "summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields\nstrong performance in low-label regimes but comes with substantial\ncomputational costs, making it impractical in time- and resource-constrained\nindustrial settings. We address this by integrating Decorrelated\nBackpropagation (DBP) into MAE pre-training, an optimization method that\niteratively reduces input correlations at each layer to accelerate convergence.\nApplied selectively to the encoder, DBP achieves faster pre-training without\nloss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE\nreduces wall-clock time to baseline performance by 21.1%, lowers carbon\nemissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe\nsimilar gains when pre-training and fine-tuning on proprietary industrial data,\nconfirming the method's applicability in real-world scenarios. These results\ndemonstrate that DBP can reduce training time and energy use while improving\ndownstream performance for large-scale ViT pre-training.", "AI": {"tldr": "本文提出将去相关反向传播（DBP）集成到Masked Autoencoder（MAE）预训练中，以解决其高计算成本问题。DBP-MAE能显著缩短训练时间、降低碳排放并提升下游任务性能。", "motivation": "视觉Transformer（ViT）的Masked Autoencoder（MAE）预训练在低标签数据下表现出色，但其高昂的计算成本使其在时间和资源受限的工业环境中不切实际。", "method": "将去相关反向传播（DBP）集成到MAE预训练中，DBP是一种优化方法，通过迭代减少每一层的输入相关性来加速收敛。该方法选择性地应用于编码器。", "result": "在ImageNet-1K预训练和ADE20K微调任务中，DBP-MAE将达到基线性能所需的训练时间减少了21.1%，碳排放降低了21.4%，并使分割mIoU提高了1.1个百分点。在专有工业数据上的预训练和微调也观察到类似的性能提升。", "conclusion": "DBP能够减少大规模ViT预训练的时间和能源消耗，同时提高下游任务的性能，证明了其在实际场景中的适用性。"}}
{"id": "2510.14661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14661", "abs": "https://arxiv.org/abs/2510.14661", "authors": ["Weikang Yu", "Vincent Nwazelibe", "Xianping Ma", "Xiaokang Zhang", "Richard Gloaguen", "Xiao Xiang Zhu", "Pedram Ghamisi"], "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)", "comment": null, "summary": "Mining activities are essential for industrial and economic development, but\nremain a leading source of environmental degradation, contributing to\ndeforestation, soil erosion, and water contamination. Sustainable resource\nmanagement and environmental governance require consistent, long-term\nmonitoring of mining-induced land surface changes, yet existing datasets are\noften limited in temporal depth or geographic scope. To address this gap, we\npresent EuroMineNet, the first comprehensive multitemporal benchmark for mining\nfootprint mapping and monitoring based on Sentinel-2 multispectral imagery.\nSpanning 133 mining sites across the European Union, EuroMineNet provides\nannual observations and expert-verified annotations from 2015 to 2024, enabling\nGeoAI-based models to analyze environmental dynamics at a continental scale. It\nsupports two sustainability-driven tasks: (1) multitemporal mining footprint\nmapping for consistent annual land-use delineation, evaluated with a novel\nChange-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change\ndetection to capture both gradual and abrupt surface transformations.\nBenchmarking 20 state-of-the-art deep learning models reveals that while GeoAI\nmethods effectively identify long-term environmental changes, challenges remain\nin detecting short-term dynamics critical for timely mitigation. By advancing\ntemporally consistent and explainable mining monitoring, EuroMineNet\ncontributes to sustainable land-use management, environmental resilience, and\nthe broader goal of applying GeoAI for social and environmental good. We\nrelease the codes and datasets by aligning with FAIR and the open science\nparadigm at https://github.com/EricYu97/EuroMineNet.", "AI": {"tldr": "本文提出了EuroMineNet，这是一个基于Sentinel-2多光谱图像的综合性多时相基准数据集，用于欧洲采矿足迹的测绘和监测，支持GeoAI模型分析环境动态并评估了现有深度学习模型的性能。", "motivation": "采矿活动是环境退化的主要来源，导致森林砍伐、水污染等。可持续资源管理和环境治理需要长期监测采矿引起的土地表面变化，但现有数据集在时间深度或地理范围上存在局限性。", "method": "研究构建了EuroMineNet数据集，覆盖欧盟133个采矿点，提供2015年至2024年的年度观测数据和专家验证标注。该数据集支持两项任务：(1) 多时相采矿足迹测绘，并引入了Change-Aware Temporal IoU (CA-TIoU) 新指标进行评估；(2) 跨时相变化检测，以捕捉渐变和突变的表面转换。研究还基准测试了20个最先进的深度学习模型。", "result": "基准测试结果显示，GeoAI方法能有效识别长期环境变化，但在检测对及时缓解至关重要的短期动态方面仍存在挑战。", "conclusion": "EuroMineNet通过推动时间一致且可解释的采矿监测，有助于可持续土地利用管理、环境恢复力，并促进GeoAI在社会和环境效益方面的应用。所有代码和数据集均已根据FAIR原则和开放科学范式发布。"}}
{"id": "2510.14205", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14205", "abs": "https://arxiv.org/abs/2510.14205", "authors": ["Bingsheng Yao", "Bo Sun", "Yuanzhe Dong", "Yuxuan Lu", "Dakuo Wang"], "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans", "comment": "In Submission", "summary": "The emerging large language model role-playing agents (LLM RPAs) aim to\nsimulate individual human behaviors, but the persona fidelity is often\nundermined by manually-created profiles (e.g., cherry-picked information and\npersonality characteristics) without validating the alignment with the target\nindividuals. To address this limitation, our work introduces the Dynamic\nPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM\nRPAs' behaviors with those of target individuals by iteratively identifying the\ncognitive divergence, either through free-form or theory-grounded, structured\nanalysis, between generated behaviors and human ground truth, and refining the\npersona profile to mitigate these divergences.We evaluate DPRF with five LLMs\non four diverse behavior-prediction scenarios: formal debates, social media\nposts with mental health issues, public interviews, and movie reviews.DPRF can\nconsistently improve behavioral alignment considerably over baseline personas\nand generalizes across models and scenarios.Our work provides a robust\nmethodology for creating high-fidelity persona profiles and enhancing the\nvalidity of downstream applications, such as user simulation, social studies,\nand personalized AI.", "AI": {"tldr": "该研究引入了动态角色优化框架（DPRF），通过迭代识别认知差异并优化角色配置文件，显著提高了大型语言模型角色扮演代理（LLM RPAs）与目标个体行为的一致性。", "motivation": "现有的LLM RPAs在模拟人类行为时，其角色保真度往往不高，因为角色配置文件是手动创建的（例如，选择性信息和个性特征），缺乏与目标个体的一致性验证。", "method": "该研究提出了动态角色优化框架（DPRF）。DPRF通过自由形式或基于理论的结构化分析，迭代识别LLM RPAs生成行为与人类真实行为之间的认知差异，并优化角色配置文件以减少这些差异。研究使用五种LLM在四种不同的行为预测场景（正式辩论、心理健康社交媒体帖子、公开采访和电影评论）中评估了DPRF。", "result": "DPRF能够持续显著提高行为一致性，优于基线角色，并且在不同模型和场景中均具有泛化能力。", "conclusion": "该工作提供了一种创建高保真角色配置文件和增强下游应用（如用户模拟、社会研究和个性化AI）有效性的稳健方法。"}}
{"id": "2510.14113", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14113", "abs": "https://arxiv.org/abs/2510.14113", "authors": ["Matan Levi", "Daniel Ohayon", "Ariel Blobstein", "Ravid Sagi", "Ian Molloy", "Yair Allouche"], "title": "Toward Cybersecurity-Expert Small Language Models", "comment": null, "summary": "Large language models (LLMs) are transforming everyday applications, yet\ndeployment in cybersecurity lags due to a lack of high-quality, domain-specific\nmodels and training datasets. To address this gap, we present CyberPal 2.0, a\nfamily of cybersecurity-expert small language models (SLMs) ranging from 4B-20B\nparameters. To train CyberPal 2.0, we generate an enriched chain-of-thought\ncybersecurity instruction dataset built with our data enrichment and formatting\npipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of\nreasoning formats alongside LLM-driven multi-step grounding, yielding\nhigher-fidelity, task-grounded reasoning traces for security tasks. Across\ndiverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its\nbaselines and matches or surpasses various open and closed-source frontier\nmodels, while remaining a fraction of their size. On core cyber threat\nintelligence knowledge tasks, our models outperform almost all tested frontier\nmodels, ranking second only to Sec-Gemini v1. On core threat-investigation\ntasks, such as correlating vulnerabilities and bug tickets with weaknesses, our\nbest 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1,\nranking first, while our smallest 4B-parameter model ranks second.", "AI": {"tldr": "本文介绍了CyberPal 2.0，一个由4B-20B参数组成的网络安全领域小型语言模型（SLM）家族。通过使用SecKnowledge 2.0数据处理管道生成高质量的链式思维指令数据集，CyberPal 2.0在多个网络安全基准测试中表现优异，超越或媲美许多大型前沿模型，尤其在威胁情报和调查任务上。", "motivation": "大型语言模型（LLMs）在日常应用中取得了巨大进展，但在网络安全领域的部署却相对滞后。这主要是由于缺乏高质量、领域特定的模型和训练数据集。", "method": "研究团队开发了CyberPal 2.0，一个包含4B-20B参数的网络安全专家小型语言模型（SLM）家族。为了训练CyberPal 2.0，他们构建了SecKnowledge 2.0数据丰富和格式化管道，该管道整合了专家循环指导的推理格式和LLM驱动的多步基础化，从而生成了更高保真度、任务导向的链式思维网络安全指令数据集。", "result": "CyberPal 2.0在各种网络安全基准测试中持续超越基线模型，并与各种开源和闭源前沿模型持平或超越，而其规模仅是这些模型的一小部分。在核心网络威胁情报知识任务上，CyberPal 2.0几乎超越了所有测试的前沿模型，仅次于Sec-Gemini v1。在核心威胁调查任务（例如关联漏洞和错误票据与弱点）上，其最佳20B参数模型超越了GPT-4o、o1、o3-mini和Sec-Gemini v1，排名第一；最小的4B参数模型也排名第二。", "conclusion": "CyberPal 2.0通过提供高性能、领域特定的小型语言模型，并结合创新的数据生成方法，成功弥补了网络安全领域LLM部署的空白。这些模型在多种网络安全任务中展现出卓越的能力，甚至超越了许多规模更大的前沿模型。"}}
{"id": "2510.13975", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13975", "abs": "https://arxiv.org/abs/2510.13975", "authors": ["Kin Kwan Leung", "Mouloud Belbahri", "Yi Sui", "Alex Labach", "Xueying Zhang", "Stephen Rose", "Jesse C. Cresswell"], "title": "Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems", "comment": "8 pages", "summary": "Retrieval-augmented generation (RAG) is a prevalent approach for building\nLLM-based question-answering systems that can take advantage of external\nknowledge databases. Due to the complexity of real-world RAG systems, there are\nmany potential causes for erroneous outputs. Understanding the range of errors\nthat can occur in practice is crucial for robust deployment. We present a new\ntaxonomy of the error types that can occur in realistic RAG systems, examples\nof each, and practical advice for addressing them. Additionally, we curate a\ndataset of erroneous RAG responses annotated by error types. We then propose an\nauto-evaluation method aligned with our taxonomy that can be used in practice\nto track and address errors during development. Code and data are available at\nhttps://github.com/layer6ai-labs/rag-error-classification.", "AI": {"tldr": "本研究针对检索增强生成（RAG）系统中常见的错误，提出了一个错误类型分类法、一个带错误标注的数据集，以及一个用于开发过程中跟踪和解决错误的自动评估方法。", "motivation": "由于真实世界RAG系统的复杂性，其输出可能存在多种错误。理解这些错误对于系统的稳健部署至关重要，因此需要识别、分类并提供解决策略。", "method": ["提出了一个RAG系统错误类型的新分类法，包含错误示例和解决建议。", "整理并标注了一个包含RAG错误响应的数据集，按错误类型进行分类。", "提出了一种与所创建分类法对齐的自动评估方法，用于在开发过程中跟踪和解决错误。"], "result": ["建立了一个新的RAG系统错误类型分类法，提供了实际示例和解决策略。", "创建了一个包含错误类型标注的RAG错误响应数据集。", "开发了一个实用的自动评估方法，能够与分类法结合，在开发阶段有效识别和处理RAG系统中的错误。"], "conclusion": "本研究通过提供全面的错误分类法、标注数据集和自动评估方法，为理解和解决现实RAG系统中的错误提供了实用工具，有助于提高系统的鲁棒性和可靠性。"}}
{"id": "2510.14668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14668", "abs": "https://arxiv.org/abs/2510.14668", "authors": ["Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Sami Azam", "Asif Karim", "Jemima Beissbarth", "Amanda Leach"], "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging", "comment": null, "summary": "Knowledge distillation (KD) has traditionally relied on a static\nteacher-student framework, where a large, well-trained teacher transfers\nknowledge to a single student model. However, these approaches often suffer\nfrom knowledge degradation, inefficient supervision, and reliance on either a\nvery strong teacher model or large labeled datasets, which limits their\neffectiveness in real-world, limited-data scenarios. To address these, we\npresent the first-ever Weakly-supervised Chain-based KD network (WeCKD) that\nredefines knowledge transfer through a structured sequence of interconnected\nmodels. Unlike conventional KD, it forms a progressive distillation chain,\nwhere each model not only learns from its predecessor but also refines the\nknowledge before passing it forward. This structured knowledge transfer further\nenhances feature learning, reduces data dependency, and mitigates the\nlimitations of one-step KD. Each model in the distillation chain is trained on\nonly a fraction of the dataset and demonstrates that effective learning can be\nachieved with minimal supervision. Extensive evaluations across four otoscopic\nimaging datasets demonstrate that it not only matches but in many cases\nsurpasses the performance of existing supervised methods. Experimental results\non two other datasets further underscore its generalization across diverse\nmedical imaging modalities, including microscopic and magnetic resonance\nimaging. Furthermore, our evaluations resulted in cumulative accuracy gains of\nup to +23% over a single backbone trained on the same limited data, which\nhighlights its potential for real-world adoption.", "AI": {"tldr": "本文提出了一种名为WeCKD的弱监督链式知识蒸馏网络，通过构建相互连接的模型序列进行渐进式知识传递，有效解决了传统知识蒸馏在数据有限场景下的局限性，并在多种医学图像数据集上展现出超越现有监督方法的性能。", "motivation": "传统的知识蒸馏（KD）依赖静态的师生框架，常面临知识退化、监督效率低下以及过度依赖强大教师模型或大量标注数据的问题，这限制了其在真实世界、数据有限场景中的有效性。", "method": "本文提出了首个弱监督链式知识蒸馏网络（WeCKD）。它构建了一个渐进式蒸馏链，其中每个模型不仅从前一个模型学习，还在将知识传递给下一个模型之前对其进行提炼。链中的每个模型仅使用一小部分数据集进行训练，从而在最小监督下实现有效学习。", "result": "WeCKD在四个耳镜图像数据集上表现出与现有监督方法相当甚至超越的性能。在另外两个数据集（包括显微镜和磁共振成像）上的实验结果进一步证明了其在不同医学成像模态上的泛化能力。与在相同有限数据上训练的单一骨干网络相比，WeCKD实现了高达+23%的累积准确率提升。", "conclusion": "WeCKD通过结构化的链式知识传递，有效解决了传统知识蒸馏在数据有限场景中的挑战，显著增强了特征学习，降低了数据依赖性，并展现出强大的泛化能力和在现实世界中应用的巨大潜力。"}}
{"id": "2510.14211", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14211", "abs": "https://arxiv.org/abs/2510.14211", "authors": ["Beomseok Kang", "Jiwon Song", "Jae-Joon Kim"], "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning", "comment": null, "summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the\nreasoning capability of small language models by decomposing complex problems\ninto sequential sub-stages. However, this comes at the cost of increased\nlatency. We observe that existing adaptive acceleration techniques, such as\nlayer skipping, struggle to balance efficiency and accuracy in this setting due\nto two key challenges: (1) stage-wise variation in skip sensitivity, and (2)\nthe generation of redundant output tokens. To address these, we propose\nLiteStage, a latency-aware layer skipping framework for multi-stage reasoning.\nLiteStage combines a stage-wise offline search that allocates optimal layer\nbudgets with an online confidence-based generation early exit to suppress\nunnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and\nStrategyQA, show that LiteStage achieves up to 1.70x speedup with less than\n4.0% accuracy loss, outperforming prior training-free layer skipping methods.", "AI": {"tldr": "LiteStage是一种针对多阶段推理的低延迟层跳过框架，它通过结合阶段性离线层预算分配和在线置信度早期退出，显著加速小型语言模型的多阶段推理，同时保持高准确性。", "motivation": "多阶段推理虽能提升小型语言模型的推理能力，但会增加延迟。现有自适应加速技术（如层跳过）在多阶段推理中难以平衡效率和准确性，原因在于：1) 不同阶段的跳过敏感度差异；2) 存在冗余输出令牌生成。", "method": "LiteStage提出了一种延迟感知的层跳过框架，用于多阶段推理。它结合了：1) 阶段性离线搜索，以分配最佳的层预算；2) 基于在线置信度的生成早期退出机制，以抑制不必要的解码。", "result": "在OBQA、CSQA和StrategyQA等三个基准测试中，LiteStage实现了高达1.70倍的加速，而准确率损失低于4.0%，优于先前的免训练层跳过方法。", "conclusion": "LiteStage通过定制化的层跳过和早期退出策略，有效解决了多阶段推理中效率与准确性的权衡问题，显著加速了小型语言模型的推理过程，同时将准确率损失控制在可接受范围内。"}}
{"id": "2510.14014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14014", "abs": "https://arxiv.org/abs/2510.14014", "authors": ["Shehenaz Hossain", "Haithem Afli"], "title": "CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models", "comment": null, "summary": "Correct answers do not necessarily reflect cultural understanding. We\nintroduce CRaFT, an explanation-based multilingual evaluation framework\ndesigned to assess how large language models (LLMs) reason across cultural\ncontexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates\nmodel explanations using four interpretable metrics: Cultural Fluency,\nDeviation, Consistency, and Linguistic Adaptation. We apply the framework to 50\nculturally grounded questions from the World Values Survey, translated into\nArabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and\nFANAR) across over 2,100 answer-explanation pairs. Results reveal significant\ncross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances\nit, and Spanish remains largely stable. While GPT adapts more effectively\nacross languages, it exhibits lower consistency; FANAR shows stable but rigid\nreasoning. These findings suggest that cultural awareness in LLMs is not\nintrinsic but emerges through linguistic framing. CRaFT offers a new lens for\nevaluating cross-cultural reasoning in multilingual settings, providing\nactionable insights for building culturally adaptive language models.", "AI": {"tldr": "本文提出了CRaFT，一个基于解释的多语言评估框架，用于衡量大型语言模型（LLMs）在不同文化背景下的推理能力，并揭示了LLMs文化意识受语言框架影响。", "motivation": "研究动机是认识到仅仅依靠答案准确性无法反映LLMs的文化理解能力，因此需要一种方法来评估LLMs在跨文化情境中的推理方式。", "method": "研究引入了CRaFT框架，通过四个可解释指标（文化流畅性、偏差、一致性和语言适应性）评估模型解释，而非仅凭答案准确性。该框架应用于世界价值观调查中的50个文化相关问题，并翻译成阿拉伯语、孟加拉语和西班牙语，以评估GPT、DeepSeek和FANAR三款模型，共分析了2100多对答案-解释。", "result": "结果显示推理存在显著的跨语言差异：阿拉伯语降低流畅性，孟加拉语增强流畅性，西班牙语则保持稳定。GPT在跨语言适应性上表现更好但一致性较低；FANAR推理稳定但僵化。这些发现表明LLMs的文化意识并非内在固有，而是通过语言框架得以体现。", "conclusion": "CRaFT为多语言环境中LLMs的跨文化推理评估提供了一个新视角，并为构建具有文化适应性的语言模型提供了可操作的见解。"}}
{"id": "2510.14040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14040", "abs": "https://arxiv.org/abs/2510.14040", "authors": ["George Flint", "Kaustubh Kislay"], "title": "Quantifying Phonosemantic Iconicity Distributionally in 6 Languages", "comment": "7 pages, 2 figures, under review -- ACL (AACL 2025)", "summary": "Language is, as commonly theorized, largely arbitrary. Yet, systematic\nrelationships between phonetics and semantics have been observed in many\nspecific cases. To what degree could those systematic relationships manifest\nthemselves in large scale, quantitative investigations--both in previously\nidentified and unidentified phenomena? This work undertakes a distributional\napproach to quantifying phonosemantic iconicity at scale across 6 diverse\nlanguages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each\nlanguage, we analyze the alignment of morphemes' phonetic and semantic\nsimilarity spaces with a suite of statistical measures, and discover an array\nof interpretable phonosemantic alignments not previously identified in the\nliterature, along with crosslinguistic patterns. We also analyze 5 previously\nhypothesized phonosemantic alignments, finding support for some such alignments\nand mixed results for others.", "AI": {"tldr": "本文通过大规模定量研究，跨六种不同语言（英语、西班牙语、印地语、芬兰语、土耳其语和泰米尔语），量化了音义象征性（phonosemantic iconicity），发现了许多此前未识别的音义对应关系和跨语言模式，并验证了部分现有假设。", "motivation": "尽管语言通常被认为是任意的，但在许多特定案例中观察到语音和语义之间存在系统性关系。本研究旨在探讨这些系统性关系在多大程度上能够通过大规模定量调查来体现，包括先前已识别和未识别的现象。", "method": "本研究采用分布式方法，通过统计测量量化了大规模的音义象征性。具体而言，在每种语言中，分析了词素的语音相似性空间和语义相似性空间的一致性。", "result": "研究发现了一系列可解释的、此前文献中未识别的音义对应关系，以及跨语言模式。此外，对五种先前假设的音义对应关系进行了分析，结果显示部分假设得到了支持，而另一些则得到了混合结果。", "conclusion": "大规模定量分析揭示了语言中存在显著的音义象征性，不仅确认了部分现有假设，还发现了大量新的、可解释的音义对应关系和跨语言模式，挑战了语言完全任意性的传统观点。"}}
{"id": "2510.14672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14672", "abs": "https://arxiv.org/abs/2510.14672", "authors": ["Jinglei Zhang", "Yuanfan Guo", "Rolandos Alexandros Potamias", "Jiankang Deng", "Hang Xu", "Chao Ma"], "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning", "comment": "Accepted by ICCV 2025", "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io", "AI": {"tldr": "VTimeCoT是一个免训练框架，通过引入进度条视觉工具和视时序思维链（visuotemporal CoT），显著提升了多模态大语言模型在视频时序定位和推理问答任务上的性能。", "motivation": "多模态大语言模型（MLLM）在视频问答中存在视频时序定位和推理能力不足的问题，这阻碍了其在实际视频理解系统中的有效应用。", "method": "该框架名为VTimeCoT，是一个免训练的解决方案。它受人类使用视频播放器进度条进行视频理解的启发，引入了两个新颖的视觉工具：一个即插即用的进度条集成工具和一个高效高亮工具。此外，为解决传统基于文本的思维链（CoT）方法的局限性，该方法提出了一个视时序CoT过程，整合了视频和文本的跨模态推理。", "result": "该方法在Qwen2VL-7B和GPT4o基线上，在视频时序定位和基于推理的问答任务中均表现出显著的性能提升。此外，该框架实现了组合式且可解释的推理过程。", "conclusion": "VTimeCoT通过创新的进度条视觉工具和视时序思维链，有效解决了多模态大语言模型在视频时序定位和推理方面的不足，显著提升了视频问答的性能和可解释性。"}}
{"id": "2510.14271", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14271", "abs": "https://arxiv.org/abs/2510.14271", "authors": ["Yilun Zheng", "Dan Yang", "Jie Li", "Lin Shang", "Lihui Chen", "Jiahao Xu", "Sitao Luan"], "title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enable large language models\n(LLMs) instant access to relevant information for the generative process,\ndemonstrating their superior performance in addressing common LLM challenges\nsuch as hallucination, factual inaccuracy, and the knowledge cutoff.\nGraph-based RAG further extends this paradigm by incorporating knowledge graphs\n(KGs) to leverage rich, structured connections for more precise and inferential\nresponses. A critical challenge, however, is that most Graph-based RAG systems\nrely on LLMs for automated KG construction, often yielding noisy KGs with\nredundant entities and unreliable relationships. This noise degrades retrieval\nand generation performance while also increasing computational cost. Crucially,\ncurrent research does not comprehensively address the denoising problem for\nLLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for\nRetrieval Augmented Generation (DEG-RAG), a framework that addresses these\nchallenges through: (1) entity resolution, which eliminates redundant entities,\nand (2) triple reflection, which removes erroneous relations. Together, these\ntechniques yield more compact, higher-quality KGs that significantly outperform\ntheir unprocessed counterparts. Beyond the methods, we conduct a systematic\nevaluation of entity resolution for LLM-generated KGs, examining different\nblocking strategies, embedding choices, similarity metrics, and entity merging\ntechniques. To the best of our knowledge, this is the first comprehensive\nexploration of entity resolution in LLM-generated KGs. Our experiments\ndemonstrate that this straightforward approach not only drastically reduces\ngraph size but also consistently improves question answering performance across\ndiverse popular Graph-based RAG variants.", "AI": {"tldr": "针对LLM生成的知识图谱（KG）在图增强检索生成（Graph-based RAG）系统中存在的噪声问题，本文提出了DEG-RAG框架，通过实体解析和三元组反思技术对KG进行去噪，显著提升了RAG性能并减小了图谱规模。", "motivation": "图增强检索生成（Graph-based RAG）系统通过结合知识图谱提升了大型语言模型（LLM）处理幻觉和不准确信息的能力。然而，大多数此类系统依赖LLM自动构建KG，这常导致KG中存在冗余实体和不可靠关系等噪声，从而降低检索和生成性能，并增加计算成本。现有研究尚未全面解决LLM生成KG的去噪问题。", "method": "本文提出了DEG-RAG框架，通过以下两种技术解决KG去噪问题：\n1. 实体解析（entity resolution）：消除冗余实体。\n2. 三元组反思（triple reflection）：移除错误关系。\n此外，本文还对LLM生成的KG的实体解析进行了系统性评估，探讨了不同的阻塞策略、嵌入选择、相似性度量和实体合并技术。", "result": "DEG-RAG技术能够生成更紧凑、更高质量的知识图谱，其性能显著优于未经处理的图谱。实体解析不仅大幅减小了图谱规模，而且在各种流行的图增强检索生成变体中，一致性地提高了问答性能。这是首次对LLM生成的KG中实体解析进行全面探索。", "conclusion": "LLM生成的知识图谱中的噪声是图增强检索生成系统的一个关键挑战。通过实体解析和三元组反思等去噪技术（如DEG-RAG框架），可以有效提高知识图谱的质量和紧凑性，从而显著提升基于图的RAG系统的问答性能和效率。对LLM生成KG的实体解析进行系统性评估是解决这一问题的重要一步。"}}
{"id": "2510.14278", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14278", "abs": "https://arxiv.org/abs/2510.14278", "authors": ["Md Mahadi Hasan Nahid", "Davood Rafiei"], "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering", "comment": "18 pages", "summary": "Retrieval plays a central role in multi-hop question answering (QA), where\nanswering complex questions requires gathering multiple pieces of evidence. We\nintroduce an Agentic Retrieval System that leverages large language models\n(LLMs) in a structured loop to retrieve relevant evidence with high precision\nand recall. Our framework consists of three specialized agents: a Question\nAnalyzer that decomposes a multi-hop question into sub-questions, a Selector\nthat identifies the most relevant context for each sub-question (focusing on\nprecision), and an Adder that brings in any missing evidence (focusing on\nrecall). The iterative interaction between Selector and Adder yields a compact\nyet comprehensive set of supporting passages. In particular, it achieves higher\nretrieval accuracy while filtering out distracting content, enabling downstream\nQA models to surpass full-context answer accuracy while relying on\nsignificantly less irrelevant information. Experiments on four multi-hop QA\nbenchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --\ndemonstrates that our approach consistently outperforms strong baselines.", "AI": {"tldr": "本文提出了一种基于LLM的智能体检索系统，通过分解问题、选择精确上下文和补充缺失证据的迭代过程，显著提高了多跳问答（QA）中的证据检索精度和召回率。", "motivation": "在多跳问答中，回答复杂问题需要收集多条证据，而检索在此过程中扮演核心角色。现有方法可能难以同时实现高精度和高召回率，并过滤掉分散注意力的内容，因此需要一种更有效的方法来检索相关证据。", "method": "该系统采用了一个由LLM驱动的结构化循环，包含三个专业智能体：1. 问题分析器（将多跳问题分解为子问题）；2. 选择器（为每个子问题识别最相关的上下文，侧重精度）；3. 添加器（补充任何缺失的证据，侧重召回）。选择器和添加器之间进行迭代交互，以生成紧凑而全面的支持性段落。", "result": "该方法在过滤分散注意力内容的同时，实现了更高的检索准确性，并能使下游QA模型在依赖显著更少无关信息的情况下，超越全上下文答案的准确性。在HotpotQA、2WikiMultiHopQA、MuSiQue和MultiHopRAG四个多跳QA基准测试中，该方法始终优于强大的基线模型。", "conclusion": "所提出的智能体检索系统通过其模块化和迭代方法，有效提高了多跳问答的证据检索性能，为下游QA模型提供了更精确和全面的支持性证据，同时减少了无关信息。"}}
{"id": "2510.14709", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14709", "abs": "https://arxiv.org/abs/2510.14709", "authors": ["Caleb Robinson", "Kimberly T. Goetz", "Christin B. Khan", "Meredith Sackett", "Kathleen Leonard", "Rahul Dodhia", "Juan M. Lavista Ferres"], "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery", "comment": null, "summary": "Effective monitoring of whale populations is critical for conservation, but\ntraditional survey methods are expensive and difficult to scale. While prior\nwork has shown that whales can be identified in very high-resolution (VHR)\nsatellite imagery, large-scale automated detection remains challenging due to a\nlack of annotated imagery, variability in image quality and environmental\nconditions, and the cost of building robust machine learning pipelines over\nmassive remote sensing archives. We present a semi-automated approach for\nsurfacing possible whale detections in VHR imagery using a statistical anomaly\ndetection method that flags spatial outliers, i.e. \"interesting points\". We\npair this detector with a web-based labeling interface designed to enable\nexperts to quickly annotate the interesting points. We evaluate our system on\nthree benchmark scenes with known whale annotations and achieve recalls of\n90.3% to 96.4%, while reducing the area requiring expert inspection by up to\n99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method\ndoes not rely on labeled training data and offers a scalable first step toward\nfuture machine-assisted marine mammal monitoring from space. We have open\nsourced this pipeline at https://github.com/microsoft/whales.", "AI": {"tldr": "本文提出了一种半自动方法，通过统计异常检测在超高分辨率卫星图像中识别鲸鱼，并结合网络标注界面，显著减少了专家审查区域，同时保持了高召回率，为未来的海洋哺乳动物监测提供了可扩展的第一步。", "motivation": "鲸鱼种群监测对保护至关重要，但传统方法昂贵且难以扩展。尽管超高分辨率（VHR）卫星图像可用于鲸鱼识别，但由于缺乏标注数据、图像质量和环境条件的可变性，以及构建鲁棒机器学习管道的成本，大规模自动化检测仍然面临挑战。", "method": "采用半自动方法，利用统计异常检测来标记空间异常点（即“兴趣点”），以识别图像中可能的鲸鱼。该检测器与一个网络标注界面配对，使专家能够快速标注这些兴趣点。", "result": "在三个已知鲸鱼标注的基准场景上，系统实现了90.3%至96.4%的召回率。同时，将需要专家检查的区域减少了高达99.8%——在某些情况下，从超过1000平方公里减少到不足2平方公里。", "conclusion": "该方法不依赖于标注训练数据，提供了一个可扩展的第一步，用于未来机器辅助的太空海洋哺乳动物监测。该管道已开源。"}}
{"id": "2510.14307", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14307", "abs": "https://arxiv.org/abs/2510.14307", "authors": ["Sathyanarayanan Ramamoorthy", "Vishwa Shah", "Simran Khanuja", "Zaid Sheikh", "Shan Jie", "Ann Chia", "Shearman Chua", "Graham Neubig"], "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking", "comment": null, "summary": "This paper introduces MERLIN, a novel testbed system for the task of\nMultilingual Multimodal Entity Linking. The created dataset includes BBC news\narticle titles, paired with corresponding images, in five languages: Hindi,\nJapanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity\nmentions linked to 2,500 unique Wikidata entities. We also include several\nbenchmarks using multilingual and multimodal entity linking methods exploring\ndifferent language models like LLaMa-2 and Aya-23. Our findings indicate that\nincorporating visual data improves the accuracy of entity linking, especially\nfor entities where the textual context is ambiguous or insufficient, and\nparticularly for models that do not have strong multilingual abilities. For the\nwork, the dataset, methods are available here at\nhttps://github.com/rsathya4802/merlin", "AI": {"tldr": "本文介绍了MERLIN，一个用于多语言多模态实体链接任务的新型测试平台和数据集，包含五种语言的BBC新闻标题、图像及实体链接，并提供了基准测试结果，表明视觉数据能提高实体链接的准确性。", "motivation": "研究动机是为多语言多模态实体链接任务创建一个新颖的测试平台系统（MERLIN），以促进该领域的研究和评估。", "method": "方法包括：1. 创建了一个数据集，包含五种语言（印地语、日语、印尼语、越南语、泰米尔语）的BBC新闻文章标题和对应图片，其中有超过7,000个命名实体提及链接到2,500个独特的Wikidata实体。2. 使用多语言和多模态实体链接方法，探索了LLaMa-2和Aya-23等不同语言模型，进行了多项基准测试。", "result": "研究结果表明，整合视觉数据可以提高实体链接的准确性，尤其是在文本上下文模糊或信息不足的实体，以及对于多语言能力不强的模型，这种提升效果尤为显著。", "conclusion": "MERLIN是一个有价值的多语言多模态实体链接测试平台和数据集，其研究发现强调了视觉数据在提高实体链接准确性方面的关键作用，特别是在文本信息有限或模型多语言能力不足的情况下。"}}
{"id": "2510.14705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14705", "abs": "https://arxiv.org/abs/2510.14705", "authors": ["Seungjoo Shin", "Jaesik Park", "Sunghyun Cho"], "title": "Leveraging Learned Image Prior for 3D Gaussian Compression", "comment": "Accepted to ICCV 2025 Workshop on ECLR", "summary": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently\nachieved considerable success in minimizing storage overhead for 3D Gaussians\nwhile preserving high rendering quality. Despite the impressive storage\nreduction, the lack of learned priors restricts further advances in the\nrate-distortion trade-off for 3DGS compression tasks. To address this, we\nintroduce a novel 3DGS compression framework that leverages the powerful\nrepresentational capacity of learned image priors to recover\ncompression-induced quality degradation. Built upon initially compressed\nGaussians, our restoration network effectively models the compression artifacts\nin the image space between degraded and original Gaussians. To enhance the\nrate-distortion performance, we provide coarse rendering residuals into the\nrestoration network as side information. By leveraging the supervision of\nrestored images, the compressed Gaussians are refined, resulting in a highly\ncompact representation with enhanced rendering performance. Our framework is\ndesigned to be compatible with existing Gaussian compression methods, making it\nbroadly applicable across different baselines. Extensive experiments validate\nthe effectiveness of our framework, demonstrating superior rate-distortion\nperformance and outperforming the rendering quality of state-of-the-art 3DGS\ncompression methods while requiring substantially less storage.", "AI": {"tldr": "该论文提出了一种新的3D高斯Splatting（3DGS）压缩框架，利用学习到的图像先验来恢复压缩导致的质量下降，并通过渲染残差作为辅助信息，显著提升了速率-失真性能并减少了存储需求。", "motivation": "尽管现有的3DGS压缩技术在减少存储方面取得了成功，但由于缺乏学习到的先验知识，限制了其在速率-失真权衡方面的进一步提升。", "method": "该框架引入了一个恢复网络，利用学习到的图像先验来弥补压缩造成的质量退化。该网络在图像空间中建模退化高斯与原始高斯之间的压缩伪影，并将粗略渲染残差作为辅助信息输入恢复网络，以增强速率-失真性能。通过恢复图像的监督，对压缩的高斯进行优化，实现更紧凑的表示。", "result": "实验证明，该框架在速率-失真性能上表现优越，超越了最先进的3DGS压缩方法的渲染质量，同时显著减少了存储需求。该框架还兼容现有高斯压缩方法。", "conclusion": "所提出的框架有效解决了3DGS压缩中缺乏学习先验的限制，通过引入基于学习图像先验的恢复网络，显著提升了压缩性能和渲染质量，并具有广泛的兼容性。"}}
{"id": "2510.14128", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14128", "abs": "https://arxiv.org/abs/2510.14128", "authors": ["Darko Sasanski", "Dimitar Peshevski", "Riste Stojanov", "Dimitar Trajanov"], "title": "Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis", "comment": null, "summary": "Computational gastronomy increasingly relies on diverse, high-quality recipe\ndatasets to capture regional culinary traditions. Although there are\nlarge-scale collections for major languages, Macedonian recipes remain\nunder-represented in digital research. In this work, we present the first\nsystematic effort to construct a Macedonian recipe dataset through web scraping\nand structured parsing. We address challenges in processing heterogeneous\ningredient descriptions, including unit, quantity, and descriptor\nnormalization. An exploratory analysis of ingredient frequency and\nco-occurrence patterns, using measures such as Pointwise Mutual Information and\nLift score, highlights distinctive ingredient combinations that characterize\nMacedonian cuisine. The resulting dataset contributes a new resource for\nstudying food culture in underrepresented languages and offers insights into\nthe unique patterns of Macedonian culinary tradition.", "AI": {"tldr": "本文通过网络抓取和结构化解析，首次系统性地构建了一个马其顿语食谱数据集，并对其进行了探索性分析，揭示了马其顿烹饪的独特食材组合。", "motivation": "计算美食学需要多样化、高质量的食谱数据集来捕捉区域烹饪传统。尽管主要语言有大型数据集，但马其顿语食谱在数字研究中代表性不足。", "method": "通过网络抓取和结构化解析构建数据集，处理并规范化异构的食材描述（包括单位、数量和描述符）。使用点互信息（PMI）和提升度（Lift score）等指标，对食材频率和共现模式进行探索性分析。", "result": "创建了一个新的马其顿语食谱数据集，并揭示了马其顿烹饪中独特的食材组合模式。", "conclusion": "该数据集为研究代表性不足语言的饮食文化提供了新资源，并深入了解了马其顿烹饪传统的独特模式。"}}
{"id": "2510.14726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14726", "abs": "https://arxiv.org/abs/2510.14726", "authors": ["Dingzhou Xie", "Rushi Lan", "Cheng Pang", "Enhao Ning", "Jiahao Zeng", "Wei Zheng"], "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection", "comment": null, "summary": "Recent object detection methods have made remarkable progress by leveraging\nattention mechanisms to improve feature discriminability. However, most\nexisting approaches are confined to refining single-layer or fusing dual-layer\nfeatures, overlooking the rich inter-layer dependencies across multi-scale\nrepresentations. This limits their ability to capture comprehensive contextual\ninformation essential for detecting objects with large scale variations. In\nthis paper, we propose a novel Cross-Layer Feature Self-Attention Module\n(CFSAM), which holistically models both local and global dependencies within\nmulti-scale feature maps. CFSAM consists of three key components: a\nconvolutional local feature extractor, a Transformer-based global modeling unit\nthat efficiently captures cross-layer interactions, and a feature fusion\nmechanism to restore and enhance the original representations. When integrated\ninto the SSD300 framework, CFSAM significantly boosts detection performance,\nachieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO\n(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the\nmodule accelerates convergence during training without introducing substantial\ncomputational overhead. Our work highlights the importance of explicit\ncross-layer attention modeling in advancing multi-scale object detection.", "AI": {"tldr": "本文提出了一种新颖的跨层特征自注意力模块（CFSAM），通过整体建模多尺度特征图中的局部和全局依赖关系，显著提升了目标检测的性能，尤其是在处理大尺度变化对象时。", "motivation": "现有的目标检测方法在利用注意力机制时，大多局限于单层或双层特征的融合，忽略了多尺度表示之间丰富的跨层依赖关系。这限制了它们捕捉全面上下文信息的能力，尤其是在检测尺度变化较大的对象时。", "method": "本文提出了跨层特征自注意力模块（CFSAM），它包含三个关键组件：一个卷积局部特征提取器，一个基于Transformer的全局建模单元（有效捕获跨层交互），以及一个特征融合机制（恢复和增强原始表示）。该模块被集成到SSD300框架中。", "result": "CFSAM显著提升了检测性能，在PASCAL VOC上达到78.6% mAP（基线为75.5%），在COCO上达到52.1% mAP（基线为43.1%），优于现有注意力模块。此外，该模块在不引入大量计算开销的情况下，加速了训练收敛。", "conclusion": "本研究强调了显式跨层注意力建模在推进多尺度目标检测中的重要性。"}}
{"id": "2510.14110", "categories": ["cs.CL", "I.2.7, I.5.1"], "pdf": "https://arxiv.org/pdf/2510.14110", "abs": "https://arxiv.org/abs/2510.14110", "authors": ["Wael Rashwan", "Hossam M. Zawbaa", "Sourav Dutta", "Haytham Assem"], "title": "DROID: Dual Representation for Out-of-Scope Intent Detection", "comment": "14 pages, 6 figures, 4 Tables. Preprint submitted to IEEE\n  Transactions on Neural Networks and Learning Systems (TNNLS)", "summary": "Detecting out-of-scope (OOS) user utterances remains a key challenge in\ntask-oriented dialogue systems and, more broadly, in open-set intent\nrecognition. Existing approaches often depend on strong distributional\nassumptions or auxiliary calibration modules. We present DROID (Dual\nRepresentation for Out-of-Scope Intent Detection), a compact end-to-end\nframework that combines two complementary encoders -- the Universal Sentence\nEncoder (USE) for broad semantic generalization and a domain-adapted\nTransformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual\ndistinctions. Their fused representations are processed by a lightweight\nbranched classifier with a single calibrated threshold that separates in-domain\nand OOS intents without post-hoc scoring. To enhance boundary learning under\nlimited supervision, DROID incorporates both synthetic and open-domain outlier\naugmentation. Despite using only 1.5M trainable parameters, DROID consistently\noutperforms recent state-of-the-art baselines across multiple intent\nbenchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for\nOOS intents, with the most significant gains in low-resource settings. These\nresults demonstrate that dual-encoder representations with simple calibration\ncan yield robust, scalable, and reliable OOS detection for neural dialogue\nsystems.", "AI": {"tldr": "DROID是一个紧凑的端到端框架，通过结合通用句子编码器（USE）和领域自适应Transformer去噪自编码器（TSDAE）的双重表示，并辅以异常值增强，实现了卓越的域外（OOS）意图检测性能，尤其在资源受限环境下表现出色。", "motivation": "检测域外（OOS）用户话语是任务导向对话系统和开放集意图识别中的关键挑战。现有方法常依赖于强分布假设或辅助校准模块，这促使研究者寻求更鲁棒、可扩展且可靠的解决方案。", "method": "该研究提出了DROID（Dual Representation for Out-of-Scope Intent Detection）框架。它结合了两个互补的编码器：用于广泛语义泛化的通用句子编码器（USE）和用于领域特定上下文区分的领域自适应Transformer去噪自编码器（TSDAE）。融合后的表示由一个轻量级分支分类器处理，该分类器使用单一校准阈值来区分域内和OOS意图，无需后验评分。为增强有限监督下的边界学习，DROID还整合了合成和开放域异常值增强技术。", "result": "DROID仅使用1.5M可训练参数，但在多个意图基准测试中持续优于最新的SOTA基线。它在已知意图上实现了6-15%的宏观F1改进，在OOS意图上实现了8-20%的宏观F1改进，尤其在资源受限设置下取得了最显著的提升。", "conclusion": "研究结果表明，结合简单校准的双编码器表示能够为神经对话系统提供鲁棒、可扩展且可靠的OOS检测能力。"}}
{"id": "2510.14737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14737", "abs": "https://arxiv.org/abs/2510.14737", "authors": ["Seulki Park", "Zilin Wang", "Stella X. Yu"], "title": "Free-Grained Hierarchical Recognition", "comment": "26 pages", "summary": "Hierarchical image classification predicts labels across a semantic taxonomy,\nbut existing methods typically assume complete, fine-grained annotations, an\nassumption rarely met in practice. Real-world supervision varies in\ngranularity, influenced by image quality, annotator expertise, and task\ndemands; a distant bird may be labeled Bird, while a close-up reveals Bald\neagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet\nand structured into cognitively inspired basic, subordinate, and fine-grained\nlevels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,\nmixed-granularity labels reflecting human annotation behavior. We propose\nfree-grain learning, with heterogeneous supervision across instances. We\ndevelop methods that enhance semantic guidance via pseudo-attributes from\nvision-language models and visual guidance via semi-supervised learning. These,\nalong with strong baselines, substantially improve performance under mixed\nsupervision. Together, our benchmark and methods advance hierarchical\nclassification under real-world constraints.", "AI": {"tldr": "本文针对现实世界中分层图像分类标注粒度不一致的问题，提出了ImageNet-F基准数据集和“自由粒度学习”方法，并通过视觉语言模型伪属性和半监督学习增强语义和视觉指导，显著提升了混合监督下的性能。", "motivation": "现有的分层图像分类方法通常假设存在完整、细粒度的标注，但这在实际应用中很少见。现实世界的监督粒度因图像质量、标注者专业知识和任务需求而异，导致标注粒度不一致（例如，远处的鸟标记为“鸟”，近处的则为“白头鹰”）。", "method": "1. 引入ImageNet-F：一个从ImageNet整理而来的大规模基准数据集，具有认知启发的基本、下属和细粒度层级结构。2. 模拟混合粒度标注：使用CLIP作为语义模糊性的代理，模拟反映人类标注行为的混合粒度标签。3. 提出自由粒度学习（free-grain learning）：针对实例间的异构监督。4. 开发增强方法：通过视觉-语言模型（VLMs）的伪属性增强语义指导，以及通过半监督学习增强视觉指导。", "result": "所提出的方法，结合强大的基线模型，显著提升了在混合监督条件下的性能。", "conclusion": "ImageNet-F基准数据集和所提出的方法共同推动了在现实世界约束下分层分类技术的发展。"}}
{"id": "2510.14332", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.14332", "abs": "https://arxiv.org/abs/2510.14332", "authors": ["Yangyang Li"], "title": "A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease", "comment": "Peer-reviewed and published in Proceedings of the 2020 3rd\n  International Conference on Algorithms, Computing and Artificial Intelligence\n  (ACAI 2020). 7 pages, 5 figures", "summary": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD\npatients, leading to early treatments that lessen symptoms and alleviating\nfinancial burden of health care. As one of the leading signs of AD, language\ncapability changes can be used for early diagnosis of AD. In this paper, I\ndevelop a robust classification method using hybrid word embedding and\nfine-tuned hyperparameters to achieve state-of-the-art accuracy in the early\ndetection of AD. Specifically, we create a hybrid word embedding based on word\nvectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The\nscores identify whether a sentence is fluent or not and capture semantic\ncontext of the sentences. I enrich the word embedding by adding linguistic\nfeatures to analyze syntax and semantics. Further, we input an embedded feature\nvector into logistic regression and fine tune hyperparameters throughout the\npipeline. By tuning hyperparameters of the machine learning pipeline (e.g.,\nmodel regularization parameter, learning rate and vector size of Doc2Vec, and\nvector size of ELMo), I achieve 91% classification accuracy and an Area Under\nthe Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based\non my knowledge, my model with 91% accuracy and 97% AUC outperforms the best\nexisting NLP model for AD diagnosis with an accuracy of 88% [32]. I study the\nmodel stability through repeated experiments and find that the model is stable\neven though the training data is split randomly (standard deviation of accuracy\n= 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method\nis accurate and stable. This model can be used as a large-scale screening\nmethod for AD, as well as a complementary examination for doctors to detect AD.", "AI": {"tldr": "本文提出了一种基于混合词嵌入和超参数微调的鲁棒分类方法，用于早期阿尔茨海默病（AD）的检测，达到了91%的分类准确率和97%的AUC，优于现有最佳模型。", "motivation": "早期发现AD对患者的治疗和减轻医疗负担至关重要。语言能力变化是AD的早期信号之一，可用于早期诊断。", "method": "开发了一种混合词嵌入方法，结合Doc2Vec和ELMo生成词向量以获得句子的困惑度分数，捕捉句子的流畅性和语义上下文。通过添加语言特征（分析句法和语义）丰富了词嵌入。将嵌入的特征向量输入逻辑回归模型，并对整个机器学习流程中的超参数（如模型正则化参数、学习率、Doc2Vec和ELMo的向量大小）进行微调。通过重复实验研究了模型的稳定性。", "result": "该方法在区分早期AD和健康受试者方面达到了91%的分类准确率和97%的曲线下面积（AUC），优于现有最佳NLP模型（88%的准确率）。模型在随机分割训练数据的情况下表现出良好的稳定性（准确率标准差为0.0403；AUC标准差为0.0174）。", "conclusion": "所提出的方法在早期AD检测中准确且稳定，可作为AD大规模筛查工具和医生辅助诊断的补充检查方法。"}}
{"id": "2510.14741", "categories": ["cs.CV", "cs.AI", "I.2.m"], "pdf": "https://arxiv.org/pdf/2510.14741", "abs": "https://arxiv.org/abs/2510.14741", "authors": ["Simone Carnemolla", "Matteo Pennisi", "Sarinda Samarasinghe", "Giovanni Bellitto", "Simone Palazzo", "Daniela Giordano", "Mubarak Shah", "Concetto Spampinato"], "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models", "comment": "Accepted to NeurIPS 2025 (spotlight)", "summary": "Understanding and explaining the behavior of machine learning models is\nessential for building transparent and trustworthy AI systems. We introduce\nDEXTER, a data-free framework that employs diffusion models and large language\nmodels to generate global, textual explanations of visual classifiers. DEXTER\noperates by optimizing text prompts to synthesize class-conditional images that\nstrongly activate a target classifier. These synthetic samples are then used to\nelicit detailed natural language reports that describe class-specific decision\npatterns and biases. Unlike prior work, DEXTER enables natural language\nexplanation about a classifier's decision process without access to training\ndata or ground-truth labels. We demonstrate DEXTER's flexibility across three\ntasks-activation maximization, slice discovery and debiasing, and bias\nexplanation-each illustrating its ability to uncover the internal mechanisms of\nvisual classifiers. Quantitative and qualitative evaluations, including a user\nstudy, show that DEXTER produces accurate, interpretable outputs. Experiments\non ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms\nexisting approaches in global model explanation and class-level bias reporting.\nCode is available at https://github.com/perceivelab/dexter.", "AI": {"tldr": "DEXTER是一个无数据框架，利用扩散模型和大型语言模型为视觉分类器生成全局、文本解释，无需训练数据，能揭示决策模式和偏差，并已在多项任务和数据集上验证其准确性和可解释性。", "motivation": "为了构建透明和值得信赖的AI系统，理解和解释机器学习模型的行为至关重要。", "method": "DEXTER框架结合了扩散模型和大型语言模型。它通过优化文本提示来合成能强烈激活目标分类器的类别条件图像。然后，利用这些合成样本来生成详细的自然语言报告，描述特定类别的决策模式和偏差。该方法的一大特点是无需访问训练数据或真实标签。", "result": "DEXTER在激活最大化、切片发现和去偏、以及偏差解释等任务中展示了其灵活性和揭示视觉分类器内部机制的能力。定量和定性评估（包括用户研究）表明，DEXTER能产生准确、可解释的输出。在ImageNet、Waterbirds、CelebA和FairFaces等数据集上的实验证实，DEXTER在全局模型解释和类别级偏差报告方面优于现有方法。", "conclusion": "DEXTER提供了一个有效的无数据框架，能够为视觉分类器提供准确、可解释的全局文本解释，揭示其决策模式和潜在偏差，且无需训练数据，从而增强了AI系统的透明度和可信度。"}}
{"id": "2510.14200", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14200", "abs": "https://arxiv.org/abs/2510.14200", "authors": ["Zhichao Wang", "Andy Wong", "Ruslan Belkin"], "title": "RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following", "comment": null, "summary": "After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and\nRFT are applied to enhance instruction-following ability, mitigate undesired\nresponses, improve reasoning capability and enable efficient domain adaptation\nwith minimal data. SFT relies on the next-token prediction objective to\nstrengthen instruction following in a base model using a large corpus of\nhuman-labeled responses. In contrast, RFT employs a RL-based approach to adapt\nfine-tuned reasoning models to specific domains with limited supervision.\nInspired by RFT, we propose replacing SFT with RLSR to leverage the extensive\nSFT dataset in an RL framework, thereby improving the base model's\ninstruction-following ability. In RLSR, the base model generates multiple\nresponses for each prompt, and reward scores are computed as the cosine\nsimilarity in the semantic embedding space between the generated and\nhuman-labeled responses. RLSR can be utilized in multiple ways. It can directly\nreplace SFT, achieving superior performance on instruction-following\nbenchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval\nwin rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and\nRLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved\na win rate of 30.73% when trained with SFT + RLSR.", "AI": {"tldr": "本文提出RLSR（基于语义奖励的强化学习）方法，旨在利用强化学习框架和SFT数据集，通过计算生成响应与人类标注响应的语义相似度作为奖励，以提高大型语言模型的指令遵循能力，并在实验中表现优于SFT。", "motivation": "现有的大语言模型微调技术（如SFT、RLHF、RLVR、RFT）旨在增强指令遵循、减少不良响应、提高推理能力和实现高效领域适应。SFT依赖于下一个token预测，而RFT采用基于RL的方法进行领域适应。受RFT启发，作者希望通过将SFT替换为RLSR，利用SFT的广泛数据集，在RL框架下进一步提升基础模型的指令遵循能力。", "method": "本文提出RLSR方法，旨在取代或结合SFT。在RLSR中，基础模型为每个提示生成多个响应，奖励分数计算为生成响应与人类标注响应在语义嵌入空间中的余弦相似度。RLSR可以直接替代SFT，也可以与SFT结合使用以进一步提升性能。", "result": "RLSR在指令遵循基准测试中表现优异：例如，RLSR (SB) 在Qwen-7B (INFINITY) 上实现了26.34%的AlpacaEval胜率，超过了SFT的21.01%。此外，SFT与RLSR结合使用进一步提升了下游任务性能，Qwen-7B (INFINITY) 在SFT + RLSR训练下达到了30.73%的胜率。", "conclusion": "RLSR是一种有效的方法，通过在强化学习框架中利用SFT数据集和语义相似度奖励，显著提高了大型语言模型的指令遵循能力。无论是直接替代SFT还是与SFT结合使用，RLSR都能带来性能上的提升。"}}
{"id": "2510.14753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14753", "abs": "https://arxiv.org/abs/2510.14753", "authors": ["Xu Wu", "Zhihui Lai", "Xianxu Hou", "Jie Zhou", "Ya-nan Zhang", "Linlin Shen"], "title": "LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement", "comment": null, "summary": "Low-light image enhancement (LLIE) aims to improve illumination while\npreserving high-quality color and texture. However, existing methods often fail\nto extract reliable feature representations due to severely degraded\npixel-level information under low-light conditions, resulting in poor texture\nrestoration, color inconsistency, and artifact. To address these challenges, we\npropose LightQANet, a novel framework that introduces quantized and adaptive\nfeature learning for low-light enhancement, aiming to achieve consistent and\nrobust image quality across diverse lighting conditions. From the static\nmodeling perspective, we design a Light Quantization Module (LQM) to explicitly\nextract and quantify illumination-related factors from image features. By\nenforcing structured light factor learning, LQM enhances the extraction of\nlight-invariant representations and mitigates feature inconsistency across\nvarying illumination levels. From the dynamic adaptation perspective, we\nintroduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors\ninto learnable prompts to dynamically guide the feature learning process. LAPM\nenables the model to flexibly adapt to complex and continuously changing\nlighting conditions, further improving image enhancement. Extensive experiments\non multiple low-light datasets demonstrate that our method achieves\nstate-of-the-art performance, delivering superior qualitative and quantitative\nresults across various challenging lighting scenarios.", "AI": {"tldr": "本文提出LightQANet，一个用于微光图像增强的新框架，通过量化和自适应特征学习来解决现有方法在低光照条件下特征提取不足的问题，从而实现跨不同光照条件的一致且鲁棒的图像质量。", "motivation": "现有微光图像增强方法在低光照条件下由于像素级信息严重退化，难以提取可靠的特征表示，导致纹理恢复差、颜色不一致和伪影。研究旨在解决这些挑战，实现跨多样光照条件的一致且鲁棒的图像质量。", "method": "LightQANet框架包含两个核心模块：1) 光照量化模块（LQM），从图像特征中显式提取并量化与光照相关的因素，通过强制结构化光照因子学习来增强光照不变表示的提取，并减轻跨不同光照水平的特征不一致性。2) 光照感知提示模块（LAPM），将光照先验编码为可学习的提示，动态指导特征学习过程，使模型灵活适应复杂和不断变化的光照条件。", "result": "在多个微光数据集上的大量实验表明，所提出的方法实现了最先进的性能，在各种具有挑战性的光照场景下提供了卓越的定性和定量结果。", "conclusion": "LightQANet通过引入量化和自适应特征学习，有效解决了微光图像增强中的挑战，实现了跨不同光照条件的一致且鲁棒的图像质量，并在多个数据集上取得了最先进的性能。"}}
{"id": "2510.14353", "categories": ["cs.CL", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.14353", "abs": "https://arxiv.org/abs/2510.14353", "authors": ["Ziad Elshaer", "Essam A. Rashed"], "title": "CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering", "comment": null, "summary": "High-performing medical Large Language Models (LLMs) typically require\nextensive fine-tuning with substantial computational resources, limiting\naccessibility for resource-constrained healthcare institutions. This study\nintroduces a confidence-driven multi-model framework that leverages model\ndiversity to enhance medical question answering without fine-tuning. Our\nframework employs a two-stage architecture: a confidence detection module\nassesses the primary model's certainty, and an adaptive routing mechanism\ndirects low-confidence queries to Helper models with complementary knowledge\nfor collaborative reasoning. We evaluate our approach using\nQwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical\nbenchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework\nachieves competitive performance, with particularly strong results in PubMedQA\n(95.0\\%) and MedMCQA (78.0\\%). Ablation studies confirm that confidence-aware\nrouting combined with multi-model collaboration substantially outperforms\nsingle-model approaches and uniform reasoning strategies. This work establishes\nthat strategic model collaboration offers a practical, computationally\nefficient pathway to improve medical AI systems, with significant implications\nfor democratizing access to advanced medical AI in resource-limited settings.", "AI": {"tldr": "本研究提出了一种无需微调的置信度驱动多模型框架，通过将低置信度查询路由至辅助模型进行协作推理，显著提升了医疗问答性能，尤其适用于资源受限环境。", "motivation": "高性能医疗大型语言模型（LLMs）通常需要大量计算资源进行微调，这限制了资源有限的医疗机构对其的访问和使用。", "method": "该框架采用两阶段架构：首先，置信度检测模块评估主模型的确定性；其次，自适应路由机制将低置信度查询导向具有互补知识的辅助模型进行协作推理。研究在MedQA、MedMCQA和PubMedQA三个医疗基准上，使用Qwen3-30B-A3B-Instruct、Phi-4 14B和Gemma 2 12B进行了评估。", "result": "该框架取得了具有竞争力的性能，在PubMedQA上达到95.0%，在MedMCQA上达到78.0%的优异表现。消融研究证实，结合多模型协作的置信度感知路由显著优于单一模型方法和统一推理策略。", "conclusion": "战略性的模型协作提供了一种实用且计算高效的方法来改进医疗AI系统，对于在资源有限的环境中普及先进医疗AI具有重要意义。"}}
{"id": "2510.14765", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.14765", "abs": "https://arxiv.org/abs/2510.14765", "authors": ["Giuseppe Lorenzo Catalano", "Agata Marta Soccini"], "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality", "comment": "21 pages, 9 figures", "summary": "Space exploration increasingly relies on Virtual Reality for several tasks,\nsuch as mission planning, multidisciplinary scientific analysis, and astronaut\ntraining. A key factor for the reliability of the simulations is having\naccurate 3D representations of planetary terrains. Extraterrestrial heightmaps\nderived from satellite imagery often contain missing values due to acquisition\nand transmission constraints. Mars is among the most studied planets beyond\nEarth, and its extensive terrain datasets make the Martian surface\nreconstruction a valuable task, although many areas remain unmapped. Deep\nlearning algorithms can support void-filling tasks; however, whereas Earth's\ncomprehensive datasets enables the use of conditional methods, such approaches\ncannot be applied to Mars. Current approaches rely on simpler interpolation\ntechniques which, however, often fail to preserve geometric coherence. In this\nwork, we propose a method for reconstructing the surface of Mars based on an\nunconditional diffusion model. Training was conducted on an augmented dataset\nof 12000 Martian heightmaps derived from NASA's HiRISE survey. A\nnon-homogeneous rescaling strategy captures terrain features across multiple\nscales before resizing to a fixed 128x128 model resolution. We compared our\nmethod against established void-filling and inpainting techniques, including\nInverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an\nevaluation set of 1000 samples. Results show that our approach consistently\noutperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)\nand perceptual similarity (29-81% on LPIPS) with the original data.", "AI": {"tldr": "该研究提出一种基于无条件扩散模型的方法，用于重建火星表面，有效填充缺失数据并优于传统技术。", "motivation": "空间探索（如任务规划、科学分析、宇航员训练）日益依赖虚拟现实，需要精确的行星地形3D表示。地外高程图常因采集和传输限制而存在缺失值。火星地形数据虽多但仍有未测绘区域。现有深度学习方法（条件模型）不适用于火星，而传统插值技术难以保持几何一致性。", "method": "本研究提出一种基于无条件扩散模型的方法来重建火星表面。模型在包含12000张火星高程图的增强数据集（来自NASA HiRISE调查）上进行训练。采用非均匀重缩放策略以捕获多尺度地形特征，然后调整为128x128的模型分辨率。将该方法与逆距离加权（IDW）、克里金法和纳维-斯托克斯算法等现有空洞填充和修复技术在1000个样本的评估集上进行了比较。", "result": "结果表明，与原始数据相比，该方法在重建精度（RMSE降低4-15%）和感知相似性（LPIPS提高29-81%）方面始终优于上述传统方法。", "conclusion": "基于无条件扩散模型的方法能够有效、准确地重建火星表面，在保持几何连贯性和视觉相似性方面显著优于现有技术，为空间探索中的地形数据缺失问题提供了解决方案。"}}
{"id": "2510.14369", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14369", "abs": "https://arxiv.org/abs/2510.14369", "authors": ["Joseph E. Trujillo-Falcon", "Monica L. Bozeman", "Liam E. Llewellyn", "Samuel T. Halvorson", "Meryl Mizell", "Stuti Deshpande", "Bob Manning", "Todd Fagin"], "title": "From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program", "comment": null, "summary": "To advance a Weather-Ready Nation, the National Weather Service (NWS) is\ndeveloping a systematic translation program to better serve the 68.8 million\npeople in the U.S. who do not speak English at home. This article outlines the\nfoundation of an automated translation tool for NWS products, powered by\nartificial intelligence. The NWS has partnered with LILT, whose patented\ntraining process enables large language models (LLMs) to adapt neural machine\ntranslation (NMT) tools for weather terminology and messaging. Designed for\nscalability across Weather Forecast Offices (WFOs) and National Centers, the\nsystem is currently being developed in Spanish, Simplified Chinese, Vietnamese,\nand other widely spoken non-English languages. Rooted in best practices for\nmultilingual risk communication, the system provides accurate, timely, and\nculturally relevant translations, significantly reducing manual translation\ntime and easing operational workloads across the NWS. To guide the distribution\nof these products, GIS mapping was used to identify language needs across\ndifferent NWS regions, helping prioritize resources for the communities that\nneed them most. We also integrated ethical AI practices throughout the\nprogram's design, ensuring that transparency, fairness, and human oversight\nguide how automated translations are created, evaluated, and shared with the\npublic. This work has culminated into a website featuring experimental\nmultilingual NWS products, including translated warnings, 7-day forecasts, and\neducational campaigns, bringing the country one step closer to a national\nwarning system that reaches all Americans.", "AI": {"tldr": "美国国家气象局（NWS）正在开发一个由人工智能驱动的自动化翻译工具，旨在为美国非英语母语者提供准确、及时且文化相关的天气产品翻译，以实现“全民备灾”。", "motivation": "美国有6880万人不以英语为母语，为了更好地服务这部分人群，提升国家气象局的“全民备灾”目标，并减少人工翻译工作量，需要开发一个系统化的翻译程序。", "method": "NWS与LILT合作，利用其专利训练流程使大型语言模型（LLMs）和神经机器翻译（NMT）工具适应天气术语和信息。该系统旨在可扩展，并正在开发西班牙语、简体中文、越南语等多种语言版本。通过GIS地图识别不同区域的语言需求以指导产品分发，并整合了透明、公平和人工监督的伦理AI实践。", "result": "该系统正在开发中，已能显著减少人工翻译时间并减轻运营工作量。目前已推出了一个实验性多语言NWS产品网站，包含翻译后的预警、7天预报和教育宣传活动。", "conclusion": "这项工作使美国向一个能够覆盖所有公民的国家预警系统迈进了一步，通过提供准确、及时和文化相关的自动化翻译，提升了对非英语社区的风险沟通能力。"}}
{"id": "2510.14261", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14261", "abs": "https://arxiv.org/abs/2510.14261", "authors": ["Rahul Nadkarni", "Yanai Elazar", "Hila Gonen", "Noah A. Smith"], "title": "Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior", "comment": null, "summary": "We present an experimental recipe for studying the relationship between\ntraining data and language model (LM) behavior. We outline steps for\nintervening on data batches -- i.e., ``rewriting history'' -- and then\nretraining model checkpoints over that data to test hypotheses relating data to\nbehavior. Our recipe breaks down such an intervention into stages that include\nselecting evaluation items from a benchmark that measures model behavior,\nmatching relevant documents to those items, and modifying those documents\nbefore retraining and measuring the effects. We demonstrate the utility of our\nrecipe through case studies on factual knowledge acquisition in LMs, using both\ncooccurrence statistics and information retrieval methods to identify documents\nthat might contribute to knowledge learning. Our results supplement past\nobservational analyses that link cooccurrence to model behavior, while\ndemonstrating that extant methods for identifying relevant training documents\ndo not fully explain an LM's ability to correctly answer knowledge questions.\nOverall, we outline a recipe that researchers can follow to test further\nhypotheses about how training data affects model behavior. Our code is made\npublicly available to promote future work.", "AI": {"tldr": "本文提出了一种实验方法，用于研究训练数据与语言模型（LM）行为之间的关系，通过修改训练数据并重新训练模型来测试假设。", "motivation": "研究人员希望理解训练数据如何影响语言模型的行为，特别是模型如何获取事实知识。此前的分析多为观察性研究，缺乏系统性的实验方法。", "method": "本文提出了一套实验步骤，包括：从基准测试中选择评估项、将相关文档与评估项匹配、修改这些文档（即“重写历史”），然后重新训练模型检查点并测量其行为变化。通过在LM事实知识获取上的案例研究，利用共现统计和信息检索方法识别相关文档来验证该方法。", "result": "实验结果补充了过去将共现与模型行为关联的观察性分析。同时，研究表明现有识别相关训练文档的方法并不能完全解释语言模型正确回答知识问题的能力。", "conclusion": "本文提供了一个可供研究人员遵循的实验方案，以测试训练数据如何影响模型行为的更多假设。相关代码已公开，以促进未来的研究工作。"}}
{"id": "2510.14252", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14252", "abs": "https://arxiv.org/abs/2510.14252", "authors": ["Jihao Zhao", "Zhiyuan Ji", "Simin Niu", "Hanyu Wang", "Feiyu Xiong", "Zhiyu Li"], "title": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems", "comment": null, "summary": "The traditional RAG paradigm, which typically engages in the comprehension of\nrelevant text chunks in response to received queries, inherently restricts both\nthe depth of knowledge internalization and reasoning capabilities. To address\nthis limitation, our research transforms the text processing in RAG from\npassive chunking to proactive understanding, defining this process as document\nmemory extraction with the objective of simulating human cognitive processes\nduring reading. Building upon this, we propose the Mixtures of scenario-aware\ndocument Memories (MoM) framework, engineered to efficiently handle documents\nfrom multiple domains and train small language models (SLMs) to acquire the\nability to proactively explore and construct document memories. The MoM\ninitially instructs large language models (LLMs) to simulate domain experts in\ngenerating document logical outlines, thereby directing structured chunking and\ncore content extraction. It employs a multi-path sampling and multi-perspective\nevaluation mechanism, specifically designing comprehensive metrics that\nrepresent chunk clarity and extraction completeness to select the optimal\ndocument memories. Additionally, to infuse deeper human-like reading abilities\nduring the training of SLMs, we incorporate a reverse reasoning strategy, which\ndeduces refined expert thinking paths from high-quality outcomes. Finally,\nleveraging diverse forms of content generated by MoM, we develop a three-layer\ndocument memory retrieval mechanism, which is grounded in our theoretical proof\nfrom the perspective of probabilistic modeling. Extensive experimental results\nacross three distinct domains demonstrate that the MoM framework not only\nresolves text chunking challenges in existing RAG systems, providing LLMs with\nsemantically complete document memories, but also paves the way for SLMs to\nachieve human-centric intelligent text processing.", "AI": {"tldr": "本研究提出MoM（场景感知文档记忆混合体）框架，将传统RAG的被动分块转变为主动的文档记忆提取，模拟人类认知。MoM利用LLM生成逻辑大纲指导结构化分块，通过多路径采样和多视角评估优化文档记忆，并采用逆向推理训练SLM，最终实现更深度的知识内化和推理能力。", "motivation": "传统RAG范式中对相关文本块的被动理解限制了知识内化深度和推理能力。为解决此限制，研究旨在将RAG中的文本处理从被动分块转变为主动理解，模拟人类阅读时的认知过程。", "method": "本研究将RAG中的文本处理定义为文档记忆提取，并提出MoM框架：1. 指导LLM模拟领域专家生成文档逻辑大纲，以指导结构化分块和核心内容提取。2. 采用多路径采样和多视角评估机制，设计清晰度与提取完整性指标以选择最优文档记忆。3. 引入逆向推理策略，从高质量结果中推导出专家思维路径，用于训练SLM以获得更深层次的人类阅读能力。4. 基于概率建模的理论证明，开发了三层文档记忆检索机制。", "result": "在三个不同领域的广泛实验结果表明，MoM框架不仅解决了现有RAG系统中的文本分块挑战，为LLM提供了语义完整的文档记忆，而且为SLM实现以人为中心的智能文本处理铺平了道路。", "conclusion": "MoM框架通过将RAG的文本处理从被动分块提升到主动文档记忆提取，显著增强了LLM的知识处理能力，并使SLM能够实现类人智能的文本处理，解决了现有RAG系统的核心局限性。"}}
{"id": "2510.14770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14770", "abs": "https://arxiv.org/abs/2510.14770", "authors": ["Zhang Nengbo", "Hann Woei Ho", "Ye Zhou"], "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks", "comment": null, "summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in\nenvironments, where conventional radio-based methods suffer from spectrum\ncongestion, jamming, and high power consumption. Inspired by the waggle dance\nof honeybees, which efficiently communicate the location of food sources\nwithout sound or contact, we propose a novel visual communication framework for\nMAV swarms using motion-based signaling. In this framework, MAVs convey\ninformation, such as heading and distance, through deliberate flight patterns,\nwhich are passively captured by event cameras and interpreted using a\npredefined visual codebook of four motion primitives: vertical (up/down),\nhorizontal (left/right), left-to-up-to-right, and left-to-down-to-right,\nrepresenting control symbols (``start'', ``end'', ``1'', ``0''). To decode\nthese signals, we design an event frame-based segmentation model and a\nlightweight Spiking Neural Network (SNN) for action recognition. An integrated\ndecoding algorithm then combines segmentation and classification to robustly\ninterpret MAV motion sequences. Experimental results validate the framework's\neffectiveness, which demonstrates accurate decoding and low power consumption,\nand highlights its potential as an energy-efficient alternative for MAV\ncommunication in constrained environments.", "AI": {"tldr": "本文提出了一种受蜜蜂摆尾舞启发的微型飞行器（MAV）群视觉通信框架，通过MAV的特定飞行模式进行信息传递，并使用事件相机结合事件帧分割模型和轻量级脉冲神经网络（SNN）进行解码，实现了低功耗和高精度的通信。", "motivation": "在微型飞行器（MAV）群通信中，传统的无线电方法面临频谱拥堵、干扰和高功耗等挑战，因此需要一种更可靠、高效的替代方案。", "method": "该研究提出了一种基于运动信号的视觉通信框架。MAV通过预定义的四种运动基元（垂直、水平、左上右、左下右）飞行模式来编码信息（如航向、距离和控制符号“开始”、“结束”、“1”、“0”）。信息由事件相机被动捕获，并通过事件帧分割模型和轻量级脉冲神经网络（SNN）进行动作识别，最终通过集成的解码算法解释MAV的运动序列。", "result": "实验结果验证了该框架的有效性，证明了其能够实现准确的解码和低功耗，突显了其作为受限环境下MAV通信节能替代方案的潜力。", "conclusion": "该研究成功开发了一种基于视觉运动信号的MAV群通信框架，该框架具有准确性高、功耗低等优点，为在复杂环境中MAV的高效可靠通信提供了一种新的、节能的解决方案。"}}
{"id": "2510.14274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14274", "abs": "https://arxiv.org/abs/2510.14274", "authors": ["Lifu Tu", "Yingbo Zhou", "Semih Yavuz"], "title": "Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters", "comment": null, "summary": "Training effective multilingual embedding models presents unique challenges\ndue to the diversity of languages and task objectives. Although small\nmultilingual models (<1 B parameters) perform well on multilingual tasks\ngenerally, they consistently lag behind larger models (>1 B) in the most\nprevalent use case: retrieval. This raises a critical question: Can smaller\nmodels be retrofitted specifically for retrieval tasks to enhance their\nperformance? In this work, we investigate key factors that influence the\neffectiveness of multilingual embeddings, focusing on training data scale,\nnegative sampling strategies, and data diversity. We find that while increasing\nthe scale of training data yields initial performance gains, these improvements\nquickly plateau - indicating diminishing returns. Incorporating hard negatives\nproves essential for consistently improving retrieval accuracy. Furthermore,\nour analysis reveals that task diversity in the training data contributes more\nsignificantly to performance than language diversity alone. As a result, we\ndevelop a compact (approximately 300M) multilingual model that achieves\nretrieval performance comparable to or even surpassing current strong 7B\nmodels.", "AI": {"tldr": "研究表明，通过优化训练数据规模、负采样策略和任务多样性，小型多语言嵌入模型（约3亿参数）在检索任务上可以达到甚至超越大型模型（70亿参数）的性能。", "motivation": "尽管小型多语言模型在一般多语言任务上表现良好，但在最常见的检索用例中，它们始终落后于大型模型。这引发了一个关键问题：能否专门为检索任务改进小型模型以提高其性能？", "method": "本文研究了影响多语言嵌入模型有效性的关键因素，重点关注训练数据规模、负采样策略和数据多样性。在此基础上，开发了一个紧凑型（约3亿参数）多语言模型。", "result": "研究发现，增加训练数据规模最初能提高性能，但很快会达到瓶颈，回报递减。引入困难负样本对于持续提高检索准确性至关重要。此外，分析揭示，训练数据中的任务多样性对性能的贡献比单纯的语言多样性更显著。最终，开发出的紧凑模型在检索性能上可与当前强大的70亿参数模型相媲美甚至超越。", "conclusion": "通过深入探究训练数据规模、负采样策略和数据多样性，并侧重于任务多样性而非仅仅语言多样性，小型多语言模型（约3亿参数）可以实现与大型模型（70亿参数）相当甚至更优的检索性能。"}}
{"id": "2510.14792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14792", "abs": "https://arxiv.org/abs/2510.14792", "authors": ["Hojun Choi", "Youngsun Lim", "Jaeyo Shin", "Hyunjung Shim"], "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection", "comment": "28 pages, 13 Figures, 12 Tables", "summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object\ncategories beyond those seen during training. Recent approaches typically\nleverage vision-language models (VLMs) to generate pseudo-labels using\nimage-text alignment, allowing detectors to generalize to unseen classes\nwithout explicit supervision. However, these methods depend heavily on direct\nimage-text matching, neglecting the intermediate reasoning steps essential for\ninterpreting semantically complex scenes. This results in limited robustness\nwhen confronted with crowded or occluded visual contexts. In this paper, we\nintroduce CoT-PL, a new framework that employs structured visual\nchain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL\ndecomposes object understanding into three interpretable steps: (1) region\nperception even for unseen objects, (2) category recognition via zero-shot\nreasoning, and (3) background grounding to separate semantically complex\nobjects. Crucially, the third step naturally motivates our contrastive\nbackground learning (CBL) that uses the pre-computed background cues as\nnegatives to promote feature disentanglement between objects and background. In\nthis way, CoT reasoning and CBL form an integrated pipeline tailored to robust\npseudo-labeling in crowded or occluded scenes. Notably, in these two settings,\nour novel-class pseudo-label quality achieves relative improvements of 103.4%\nand 168.4% over the best prior, respectively. Our extensive experiments\ndemonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9\nmask AP on LVIS for novel classes, setting a new state of the art.", "AI": {"tldr": "本文提出CoT-PL框架，通过引入结构化视觉思维链（CoT）推理和对比背景学习（CBL），显著提升开放词汇目标检测在复杂场景（如拥挤、遮挡）下的伪标签质量和整体性能。", "motivation": "现有开放词汇目标检测（OVD）方法过度依赖直接图像-文本匹配，忽略了语义复杂场景中必要的中间推理步骤，导致在拥挤或遮挡的视觉上下文中鲁棒性有限。", "method": "CoT-PL框架通过以下方式进行伪标签生成：1) 采用结构化视觉思维链（CoT）推理，将目标理解分解为区域感知、零样本类别识别和背景接地三个可解释步骤。2) 背景接地步骤自然引出对比背景学习（CBL），利用预计算的背景线索作为负样本，促进目标与背景之间的特征解耦。CoT推理和CBL形成了一个集成管道。", "result": "CoT-PL在拥挤和遮挡场景下，新类别伪标签质量分别比现有最佳方法相对提升103.4%和168.4%。在开放词汇COCO数据集上，新类别AP50提升+7.7；在LVIS数据集上，新类别mask AP提升+2.9，达到了新的SOTA。", "conclusion": "CoT-PL通过其创新的CoT推理和CBL机制，有效解决了开放词汇目标检测在复杂场景下的鲁棒性问题，显著提升了伪标签质量和整体检测性能，为该领域树立了新标杆。"}}
{"id": "2510.14400", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14400", "abs": "https://arxiv.org/abs/2510.14400", "authors": ["Yingpeng Ning", "Yuanyuan Sun", "Ling Luo", "Yanhua Wang", "Yuchen Pan", "Hongfei Lin"], "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering", "comment": null, "summary": "Biomedical question answering (QA) requires accurate interpretation of\ncomplex medical knowledge. Large language models (LLMs) have shown promising\ncapabilities in this domain, with retrieval-augmented generation (RAG) systems\nenhancing performance by incorporating external medical literature. However,\nRAG-based approaches in biomedical QA suffer from hallucinations due to\npost-retrieval noise and insufficient verification of retrieved evidence,\nundermining response reliability. We propose MedTrust-Guided Iterative RAG, a\nframework designed to enhance factual consistency and mitigate hallucinations\nin medical QA. Our method introduces three key innovations. First, it enforces\ncitation-aware reasoning by requiring all generated content to be explicitly\ngrounded in retrieved medical documents, with structured Negative Knowledge\nAssertions used when evidence is insufficient. Second, it employs an iterative\nretrieval-verification process, where a verification agent assesses evidence\nadequacy and refines queries through Medical Gap Analysis until reliable\ninformation is obtained. Third, it integrates the MedTrust-Align Module (MTAM)\nthat combines verified positive examples with hallucination-aware negative\nsamples, leveraging Direct Preference Optimization to reinforce\ncitation-grounded reasoning while penalizing hallucination-prone response\npatterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our\napproach consistently outperforms competitive baselines across multiple model\narchitectures, achieving the best average accuracy with gains of 2.7% for\nLLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.", "AI": {"tldr": "该研究提出了一种名为MedTrust-Guided Iterative RAG的框架，通过引用感知推理、迭代检索验证和偏好优化，显著增强了医学问答的事实一致性并有效缓解了幻觉问题。", "motivation": "尽管大型语言模型（LLMs）在生物医学问答中展现出潜力，但基于检索增强生成（RAG）的方法因检索后噪声和证据验证不足，常导致幻觉，从而损害了响应的可靠性。", "method": "本研究提出了MedTrust-Guided Iterative RAG框架，包含三项核心创新：1. 强制执行引用感知推理，要求所有生成内容明确基于检索到的医学文档，并在证据不足时使用结构化的负面知识断言。2. 采用迭代检索-验证过程，其中验证代理评估证据充分性，并通过医学差距分析（Medical Gap Analysis）细化查询。3. 集成MedTrust-Align模块（MTAM），该模块结合已验证的正面示例和幻觉感知的负面样本，利用直接偏好优化（Direct Preference Optimization）来强化引用接地推理并惩罚易产生幻觉的响应模式。", "result": "在MedMCQA、MedQA和MMLU-Med数据集上的实验表明，该方法在多种模型架构上均持续优于现有基线，LLaMA3.1-8B-Instruct的平均准确率提高了2.7%，Qwen3-8B提高了2.4%，实现了最佳平均准确率。", "conclusion": "该MedTrust-Guided Iterative RAG框架能够有效增强医学问答中的事实一致性，并显著缓解幻觉问题，为构建更可靠的生物医学问答系统提供了新途径。"}}
{"id": "2510.14420", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14420", "abs": "https://arxiv.org/abs/2510.14420", "authors": ["Qingyu Ren", "Qianyu He", "Bowei Zhang", "Jie Zeng", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "title": "Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following", "comment": null, "summary": "Language models often struggle to follow multi-constraint instructions that\nare crucial for real-world applications. Existing reinforcement learning (RL)\napproaches suffer from dependency on external supervision and sparse reward\nsignals from multi-constraint tasks. We propose a label-free self-supervised RL\nframework that eliminates dependency on external supervision by deriving reward\nsignals directly from instructions and generating pseudo-labels for reward\nmodel training. Our approach introduces constraint decomposition strategies and\nefficient constraint-wise binary classification to address sparse reward\nchallenges while maintaining computational efficiency. Experiments show that\nour approach generalizes well, achieving strong improvements across 3 in-domain\nand 5 out-of-domain datasets, including challenging agentic and multi-turn\ninstruction following. The data and code are publicly available at\nhttps://github.com/Rainier-rq/verl-if", "AI": {"tldr": "本文提出了一种无标签自监督强化学习框架，通过直接从指令中获取奖励信号并生成伪标签来训练奖励模型，以解决语言模型在多约束指令遵循中对外部监督的依赖和奖励稀疏性问题，并在多个数据集上取得了显著改进。", "motivation": "语言模型在遵循多约束指令方面表现不佳，而这对于实际应用至关重要。现有的强化学习方法依赖外部监督，并且在多约束任务中面临奖励信号稀疏的问题。", "method": "提出了一种无标签自监督强化学习框架，通过以下方式消除对外部监督的依赖：1) 直接从指令中推导奖励信号；2) 为奖励模型训练生成伪标签。该方法还引入了约束分解策略和高效的逐约束二元分类，以解决奖励稀疏性问题并保持计算效率。", "result": "实验结果表明，该方法泛化性良好，在3个域内数据集和5个域外数据集（包括具有挑战性的智能体和多轮指令遵循任务）上均取得了显著改进。", "conclusion": "所提出的自监督强化学习框架有效解决了语言模型在遵循多约束指令时对外部监督的依赖和奖励稀疏性问题，展示了强大的泛化能力和计算效率。"}}
{"id": "2510.14276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14276", "abs": "https://arxiv.org/abs/2510.14276", "authors": ["Haiquan Zhao", "Chenhan Yuan", "Fei Huang", "Xiaomeng Hu", "Yichang Zhang", "An Yang", "Bowen Yu", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin", "Baosong Yang", "Chen Cheng", "Jialong Tang", "Jiandong Jiang", "Jianwei Zhang", "Jijie Xu", "Ming Yan", "Minmin Sun", "Pei Zhang", "Pengjun Xie", "Qiaoyu Tang", "Qin Zhu", "Rong Zhang", "Shibin Wu", "Shuo Zhang", "Tao He", "Tianyi Tang", "Tingyu Xia", "Wei Liao", "Weizhou Shen", "Wenbiao Yin", "Wenmeng Zhou", "Wenyuan Yu", "Xiaobin Wang", "Xiaodong Deng", "Xiaodong Xu", "Xinyu Zhang", "Yang Liu", "Yeqiu Li", "Yi Zhang", "Yong Jiang", "Yu Wan", "Yuxin Zhou"], "title": "Qwen3Guard Technical Report", "comment": null, "summary": "As large language models (LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diverse safety policies,\nrendering them incapable of accommodating varying safety tolerances across\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series of multilingual safety guardrail models with two\nspecialized variants: Generative Qwen3Guard, which casts safety classification\nas an instruction-following task to enable fine-grained tri-class judgments\n(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a\ntoken-level classification head for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for global LLM\ndeployments. Evaluated across English, Chinese, and multilingual benchmarks,\nQwen3Guard achieves state-of-the-art performance in both prompt and response\nsafety classification. All models are released under the Apache 2.0 license for\npublic use.", "AI": {"tldr": "Qwen3Guard是一系列多语言安全护栏模型，通过生成式和流式两种变体，实现了细粒度（安全、有争议、不安全）和实时（token级）的安全判断，解决了现有护栏模型在适应不同安全策略和流式推理中的局限性。", "motivation": "现有护rail模型存在两大局限性：1) 仅输出二元“安全/不安全”标签，无法适应不同领域的多样化安全容忍度；2) 需要完整的LLM输出才能进行安全检查，与流式推理不兼容，导致无法及时干预并增加有害部分输出的暴露风险。", "method": "Qwen3Guard提出了两种专用变体：1) Generative Qwen3Guard，将安全分类转化为指令遵循任务，实现细粒度的三类别判断（安全、有争议、不安全）；2) Stream Qwen3Guard，引入token级分类头，用于增量文本生成过程中的实时安全监控。两种变体均提供三种模型尺寸（0.6B、4B、8B），支持多达119种语言和方言。", "result": "Qwen3Guard在英语、中文和多语言基准测试中，在提示和响应安全分类方面均达到了最先进的性能，为全球LLM部署提供了全面、可扩展和低延迟的安全审查能力。", "conclusion": "Qwen3Guard通过其生成式和流式变体，有效克服了现有LLM安全护栏模型的局限性，提供了细粒度、实时、多语言的安全判断能力，并已在Apache 2.0许可下公开发布，以促进LLM的安全部署。"}}
{"id": "2510.14466", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14466", "abs": "https://arxiv.org/abs/2510.14466", "authors": ["Haolin Li", "Haipeng Zhang", "Mang Li", "Yaohua Wang", "Lijie Wen", "Yu Zhang", "Biqing Huang"], "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models", "comment": null, "summary": "As large language models (LLMs) rapidly advance, performance on high-resource\nlanguages (e.g., English, Chinese) is nearing saturation, yet remains\nsubstantially lower for low-resource languages (e.g., Urdu, Thai) due to\nlimited training data, machine-translation noise, and unstable cross-lingual\nalignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language\nModels), a training framework that robustly improves cross-lingual\nrepresentations under low-resource conditions while jointly strengthening\nretrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored\nRepresentation Composition Architecture), which anchors low-resource languages\nto an English semantic space via anchor-based alignment and multi-agent\ncollaborative encoding, preserving geometric stability in a shared embedding\nspace; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a\nlanguage-aware lightweight reasoning head with consistency regularization on\ntop of Arca's multilingual representations, unifying the training objective to\nenhance cross-lingual understanding, retrieval, and reasoning robustness. We\nfurther construct and release a multilingual product retrieval dataset covering\nfive Southeast Asian and two South Asian languages. Experiments across\nlow-resource benchmarks (cross-lingual retrieval, semantic similarity, and\nreasoning) show consistent gains and robustness under few-shot and\nnoise-amplified settings; ablations validate the contribution of both Arca and\nLaSR. Code will be released on GitHub and the dataset on Hugging Face.", "AI": {"tldr": "LiRA是一个训练框架，通过锚定低资源语言到英语语义空间并引入语言感知推理头，显著提升了大型语言模型在低资源语言条件下的跨语言表示、检索和推理能力。", "motivation": "大型语言模型在英语、中文等高资源语言上的表现已接近饱和，但在乌尔都语、泰语等低资源语言上的性能仍显著较低。这主要是由于训练数据有限、机器翻译噪声以及跨语言对齐不稳定所致。", "method": "本文提出了LiRA（Linguistic Robust Anchoring for Large Language Models）训练框架，包含两个模块：(i) Arca（Anchored Representation Composition Architecture），通过基于锚点的对齐和多智能体协作编码，将低资源语言锚定到英语语义空间，以保持共享嵌入空间的几何稳定性；(ii) LaSR（Language-coupled Semantic Reasoner），在Arca的多语言表示之上添加了一个语言感知的轻量级推理头，并结合一致性正则化，统一训练目标以增强跨语言理解、检索和推理的鲁棒性。此外，还构建并发布了一个多语言产品检索数据集。", "result": "在低资源基准测试（跨语言检索、语义相似性和推理）中，LiRA在少样本和噪声放大设置下均显示出持续的性能提升和鲁棒性。消融实验验证了Arca和LaSR两个模块的贡献。", "conclusion": "LiRA框架通过其独特的锚定表示和语言感知推理机制，有效解决了低资源语言在大型语言模型中性能不佳的问题，显著增强了模型的跨语言理解、检索和推理能力，尤其是在数据稀缺和噪声环境下。"}}
{"id": "2510.14800", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14800", "abs": "https://arxiv.org/abs/2510.14800", "authors": ["Usama Sajjad", "Abdul Rehman Akbar", "Ziyu Su", "Deborah Knight", "Wendy L. Frankel", "Metin N. Gurcan", "Wei Chen", "Muhammad Khalid Khan Niazi"], "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images", "comment": null, "summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally,\nwith approximately 154,000 new cases and 54,000 projected deaths anticipated\nfor 2025. The recent advancement of foundation models in computational\npathology has been largely propelled by task agnostic methodologies that can\noverlook organ-specific crucial morphological patterns that represent distinct\nbiological processes that can fundamentally influence tumor behavior,\ntherapeutic response, and patient outcomes. The aim of this study is to develop\na novel, interpretable AI model, PRISM (Prognostic Representation of Integrated\nSpatial Morphology), that incorporates a continuous variability spectrum within\neach distinct morphology to characterize phenotypic diversity and reflecting\nthe principle that malignant transformation occurs through incremental\nevolutionary processes rather than abrupt phenotypic shifts. PRISM is trained\non 8.74 million histological images extracted from surgical resection specimens\nof 424 patients with stage III CRC. PRISM achieved superior prognostic\nperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;\nHR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific\nmethods by 15% and AI foundation models by ~23% accuracy. It showed\nsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable\nperformance across clinicopathological subgroups, with minimal accuracy\nfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,\nreplicating the Alliance cohort finding of no survival difference between\ntreatments.", "AI": {"tldr": "本研究开发了一种名为PRISM的新型可解释AI模型，通过整合空间形态学和连续变异性，显著提高了III期结直肠癌（CRC）的五年总生存期（OS）预后准确性，优于现有方法和AI基础模型，并表现出良好的鲁棒性。", "motivation": "结直肠癌是全球第三大常见恶性肿瘤，预后挑战严峻。现有计算病理学中的基础模型多采用任务无关的方法，容易忽略器官特异性的关键形态学模式，而这些模式对肿瘤行为、治疗反应和患者预后至关重要。因此，需要开发一种能捕捉这些细微生物学过程的新型可解释AI模型。", "method": "研究开发了PRISM（Prognostic Representation of Integrated Spatial Morphology）模型，该模型将每种独特形态中的连续变异谱纳入考量，以表征表型多样性，并反映恶性转化是渐进式演变而非突然的表型转变。PRISM在从424名III期CRC患者手术切除标本中提取的874万张组织学图像上进行训练。", "result": "PRISM在预测五年OS方面取得了卓越的预后性能（AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；HR = 3.34，95% CI = 2.28-4.90；p < 0.0001），比现有CRC特异性方法高出15%的准确率，比AI基础模型高出约23%的准确率。该模型还表现出性别无关的鲁棒性（AUC delta = 0.02；准确率 delta = 0.15%）以及在不同临床病理亚组间的稳定性能，在5FU/LV和CPT-11/5FU/LV方案之间准确率波动极小（delta = 1.44%），这与Alliance队列研究中两种治疗方案无生存差异的发现一致。", "conclusion": "PRISM模型通过整合空间形态学中的连续变异性，提供了一种更准确、更鲁棒的III期CRC五年OS预后工具。它克服了现有基础模型在捕捉器官特异性形态模式方面的局限性，并有望为CRC患者的个性化治疗和管理提供有价值的指导。"}}
{"id": "2510.13996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13996", "abs": "https://arxiv.org/abs/2510.13996", "authors": ["Lukas Gienapp", "Christopher Schröder", "Stefan Schweter", "Christopher Akiki", "Ferdinand Schlatt", "Arden Zimmermann", "Phillipe Genêt", "Martin Potthast"], "title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models", "comment": "13 pages, 3 figures, 12 tables, includes datasheet", "summary": "Large language model development relies on large-scale training corpora, yet\nmost contain data of unclear licensing status, limiting the development of\ntruly open models. This problem is exacerbated for non-English languages, where\nopenly licensed text remains critically scarce. We introduce the German\nCommons, the largest collection of openly licensed German text to date. It\ncompiles data from 41 sources across seven domains, encompassing legal,\nscientific, cultural, political, news, economic, and web text. Through\nsystematic sourcing from established data providers with verifiable licensing,\nit yields 154.56 billion tokens of high-quality text for language model\ntraining. Our processing pipeline implements comprehensive quality filtering,\ndeduplication, and text formatting fixes, ensuring consistent quality across\nheterogeneous text sources. All domain subsets feature licenses of at least\nCC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and\nredistribution. The German Commons therefore addresses the critical gap in\nopenly licensed German pretraining data, and enables the development of truly\nopen German language models. We also release code for corpus construction and\ndata filtering tailored to German language text, rendering the German Commons\nfully reproducible and extensible.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14803", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14803", "abs": "https://arxiv.org/abs/2510.14803", "authors": ["Pedro R. A. S. Bassi", "Xinze Zhou", "Wenxuan Li", "Szymon Płotka", "Jieneng Chen", "Qi Chen", "Zheren Zhu", "Jakub Prządo", "Ibrahim E. Hamacı", "Sezgin Er", "Yuhan Wang", "Ashwin Kumar", "Bjoern Menze", "Jarosław B. Ćwikła", "Yuyin Zhou", "Akshay S. Chaudhari", "Curtis P. Langlotz", "Sergio Decherchi", "Andrea Cavalli", "Kang Wang", "Yang Yang", "Alan L. Yuille", "Zongwei Zhou"], "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks", "comment": null, "summary": "Early tumor detection save lives. Each year, more than 300 million computed\ntomography (CT) scans are performed worldwide, offering a vast opportunity for\neffective cancer screening. However, detecting small or early-stage tumors on\nthese CT scans remains challenging, even for experts. Artificial intelligence\n(AI) models can assist by highlighting suspicious regions, but training such\nmodels typically requires extensive tumor masks--detailed, voxel-wise outlines\nof tumors manually drawn by radiologists. Drawing these masks is costly,\nrequiring years of effort and millions of dollars. In contrast, nearly every CT\nscan in clinical practice is already accompanied by medical reports describing\nthe tumor's size, number, appearance, and sometimes, pathology\nresults--information that is rich, abundant, and often underutilized for AI\ntraining. We introduce R-Super, which trains AI to segment tumors that match\ntheir descriptions in medical reports. This approach scales AI training with\nlarge collections of readily available medical reports, substantially reducing\nthe need for manually drawn tumor masks. When trained on 101,654 reports, AI\nmodels achieved performance comparable to those trained on 723 masks. Combining\nreports and masks further improved sensitivity by +13% and specificity by +8%,\nsurpassing radiologists in detecting five of the seven tumor types. Notably,\nR-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,\nbladder, uterus, and esophagus, for which no public masks or AI models\npreviously existed. This study challenges the long-held belief that\nlarge-scale, labor-intensive tumor mask creation is indispensable, establishing\na scalable and accessible path toward early detection across diverse tumor\ntypes.\n  We plan to release our trained models, code, and dataset at\nhttps://github.com/MrGiovanni/R-Super", "AI": {"tldr": "本研究提出R-Super，一种利用医学报告而非昂贵的手动肿瘤掩膜来训练AI进行肿瘤分割的方法，实现了与掩膜训练相当的性能，并能扩展到以前无法分割的肿瘤类型，挑战了大规模手动掩膜的必要性。", "motivation": "早期肿瘤检测可挽救生命，但CT扫描中识别早期或微小肿瘤极具挑战性。AI模型能辅助检测，但其训练通常需要大量昂贵且耗时的人工绘制肿瘤掩膜。临床实践中，几乎每份CT扫描都附有详细的医学报告，这些报告信息丰富、数量庞大但常未充分用于AI训练。", "method": "引入R-Super方法，通过训练AI模型，使其能够根据医学报告中的肿瘤描述来分割肿瘤。该方法旨在利用大量现有的医学报告进行AI训练，从而大幅减少对手动绘制肿瘤掩膜的需求。", "result": "当使用101,654份报告进行训练时，AI模型的性能与使用723个掩膜训练的模型相当。结合报告和掩膜进一步将敏感性提高了13%，特异性提高了8%，在七种肿瘤类型中，有五种的检测性能超越了放射科医生。值得注意的是，R-Super还实现了脾脏、胆囊、前列腺、膀胱、子宫和食道肿瘤的分割，而这些肿瘤此前没有公开的掩膜或AI模型。", "conclusion": "本研究挑战了大规模、劳动密集型肿瘤掩膜创建不可或缺的长期观念，为跨多种肿瘤类型的早期检测提供了一条可扩展且易于实现的新途径。"}}
{"id": "2510.14296", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14296", "abs": "https://arxiv.org/abs/2510.14296", "authors": ["Md Mahadi Hasan Nahid", "Davood Rafiei", "Weiwei Zhang", "Yong Zhang"], "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL", "comment": "30 Pages", "summary": "Schema linking -- the process of aligning natural language questions with\ndatabase schema elements -- is a critical yet underexplored component of\nText-to-SQL systems. While recent methods have focused primarily on improving\nSQL generation, they often neglect the retrieval of relevant schema elements,\nwhich can lead to hallucinations and execution failures. In this work, we\npropose a context-aware bidirectional schema retrieval framework that treats\nschema linking as a standalone problem. Our approach combines two complementary\nstrategies: table-first retrieval followed by column selection, and\ncolumn-first retrieval followed by table selection. It is further augmented\nwith techniques such as question decomposition, keyword extraction, and\nkeyphrase extraction. Through comprehensive evaluations on challenging\nbenchmarks such as BIRD and Spider, we demonstrate that our method\nsignificantly improves schema recall while reducing false positives. Moreover,\nSQL generation using our retrieved schema consistently outperforms full-schema\nbaselines and closely approaches oracle performance, all without requiring\nquery refinement. Notably, our method narrows the performance gap between full\nand perfect schema settings by 50\\%. Our findings highlight schema linking as a\npowerful lever for enhancing Text-to-SQL accuracy and efficiency.", "AI": {"tldr": "本文提出了一种上下文感知的双向Schema检索框架，将Schema链接视为一个独立问题。该框架显著提高了Schema召回率并减少了误报，从而提升了Text-to-SQL系统的SQL生成准确性和效率。", "motivation": "现有的Text-to-SQL方法主要关注SQL生成，但忽视了相关Schema元素的检索，这常导致幻觉和执行失败。Schema链接是Text-to-SQL系统中一个关键但未充分探索的组件。", "method": "本文提出了一种上下文感知的双向Schema检索框架，将Schema链接视为一个独立问题。该方法结合了两种互补策略：先表后列选择和先列后表选择。此外，它还通过问题分解、关键词提取和关键短语提取等技术进行了增强。", "result": "在BIRD和Spider等基准测试中，该方法显著提高了Schema召回率并减少了误报。使用检索到的Schema进行SQL生成，其性能持续优于全Schema基线，并接近预言机性能，且无需查询细化。值得注意的是，该方法将全Schema和完美Schema设置之间的性能差距缩小了50%。", "conclusion": "研究结果强调，Schema链接是提升Text-to-SQL准确性和效率的强大杠杆。"}}
{"id": "2510.14616", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14616", "abs": "https://arxiv.org/abs/2510.14616", "authors": ["Shuangshuang Ying", "Yunwen Li", "Xingwei Qu", "Xin Li", "Sheng Jin", "Minghao Liu", "Zhoufutu Wen", "Xeron Du", "Tianyu Zheng", "Yichi Zhang", "Letian Ni", "Yuyang Cheng", "Qiguang Chen", "Jingzhe Ding", "Shengda Long", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Libo Qin", "Ge Zhang", "Wenhao Huang", "Wanxiang Che", "Chenghua Lin"], "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures", "comment": null, "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification.", "AI": {"tldr": "当前偏好学习方法在缺乏客观质量信号时表现不佳，主要检测客观错误而非主观偏好。本研究引入WritingPreferenceBench数据集，并发现具有显式推理链的生成式奖励模型在捕捉主观偏好方面表现显著优于序列模型和零样本大语言模型判断。", "motivation": "当前的偏好学习方法在标准基准上精度高，但当移除客观质量信号时性能显著下降。这表明它们主要学习检测客观错误，而非捕捉创意、风格和情感共鸣等主观质量偏好。", "method": "研究引入了WritingPreferenceBench数据集，包含1,800个人工标注的偏好对（1,200个英文，600个中文），涵盖8种创意写作类型，且响应在客观正确性、事实准确性和长度上均已匹配。在该基准上，评估了序列基奖励模型、零样本语言模型判断器以及生成式奖励模型（产生显式推理链）的性能，并分析了模型在不同写作类别间的表现差异及模型规模的影响。", "result": "序列基奖励模型平均准确率仅为52.7%，零样本语言模型判断器为53.9%。相比之下，产生显式推理链的生成式奖励模型达到了81.8%的准确率。模型在不同写作类别间表现出高度方差（18.2%至81.8%，平均标准差10.1%），且这种方差不随模型规模变化，27B参数模型并未比8B变体显示出持续改进。", "conclusion": "研究结果表明，当前的RLHF方法主要学习检测客观错误，而非捕捉主观质量偏好。成功的偏好建模可能需要中间推理表示，而非直接分类。"}}
{"id": "2510.14303", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.14303", "abs": "https://arxiv.org/abs/2510.14303", "authors": ["Ziye Xia", "Sergei S. Ospichev"], "title": "Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers", "comment": "9 pages, 10 figures", "summary": "In recent years, the rapid increase in academic publications across various\nfields has posed severe challenges for academic paper analysis: scientists\nstruggle to timely and comprehensively track the latest research findings and\nmethodologies. Key concept extraction has proven to be an effective analytical\nparadigm, and its automation has been achieved with the widespread application\nof language models in industrial and scientific domains. However, existing\npaper databases are mostly limited to similarity matching and basic\nclassification of key concepts, failing to deeply explore the relational\nnetworks between concepts. This paper is based on the OpenAlex opensource\nknowledge graph. By analyzing nearly 8,000 open-source paper data from\nNovosibirsk State University, we discovered a strong correlation between the\ndistribution patterns of paper key concept paths and both innovation points and\nrare paths. We propose a prompt engineering-based key concept path analysis\nmethod. This method leverages small language models to achieve precise key\nconcept extraction and innovation point identification, and constructs an agent\nbased on a knowledge graph constraint mechanism to enhance analysis accuracy.\nThrough fine-tuning of the Qwen and DeepSeek models, we achieved significant\nimprovements in accuracy, with the models publicly available on the Hugging\nFace platform.", "AI": {"tldr": "该研究提出一种基于提示工程的关键概念路径分析方法，利用小型语言模型和知识图谱代理，从学术论文中提取关键概念并识别创新点，显著提高了分析准确性。", "motivation": "学术论文数量激增导致科学家难以追踪最新研究，现有论文数据库在关键概念分析上仅限于相似性匹配和基本分类，未能深入探索概念间的关系网络。", "method": "该研究基于OpenAlex开源知识图谱，分析了近8000篇新西伯利亚国立大学的开源论文数据。提出了一种基于提示工程的关键概念路径分析方法，利用小型语言模型进行精确的关键概念提取和创新点识别，并构建了一个基于知识图谱约束机制的代理以提高分析准确性。通过对Qwen和DeepSeek模型进行微调，实现了性能提升。", "result": "研究发现论文关键概念路径的分布模式与创新点和稀有路径之间存在强相关性。通过对Qwen和DeepSeek模型进行微调，分析准确性得到了显著提高，并将模型公开发布在Hugging Face平台。", "conclusion": "该论文成功提出并验证了一种利用语言模型和知识图谱进行关键概念路径分析以识别学术创新点的方法，为解决学术信息过载问题提供了新的有效途径。"}}
{"id": "2510.14819", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14819", "abs": "https://arxiv.org/abs/2510.14819", "authors": ["Ji Cao", "Yu Wang", "Tongya Zheng", "Zujie Ren", "Canghong Jin", "Gang Chen", "Mingli Song"], "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning", "comment": null, "summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into\nlow-dimensional vectors, which can then be leveraged in various downstream\ntasks, including travel time estimation, location prediction, and trajectory\nsimilarity analysis. However, existing TRL methods suffer from a key oversight:\ntreating trajectories as isolated spatio-temporal sequences, without\nconsidering the external environment and internal route choice behavior that\ngovern their formation. To bridge this gap, we propose a novel framework that\nunifies comprehensive environment \\textbf{P}erception and explicit\n\\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation\nlearning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an\nEnvironment Perception Module to enhance the road network by capturing\nmulti-granularity environmental semantics from surrounding POI distributions.\nBuilding on this environment-aware backbone, a Route Choice Encoder then\ncaptures the route choice behavior inherent in each trajectory by modeling its\nconstituent road segment transitions as a sequence of decisions. These\nroute-choice-aware representations are finally aggregated to form the global\ntrajectory embedding. Extensive experiments on 3 real-world datasets across 5\ndownstream tasks validate the effectiveness and generalizability of PRTraj.\nMoreover, PRTraj demonstrates strong data efficiency, maintaining robust\nperformance under few-shot scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/PRTraj.", "AI": {"tldr": "PRTraj是一种新颖的轨迹表示学习框架，它通过整合环境感知和显式路径选择建模来克服现有方法的局限性，从而生成更有效和泛化的轨迹嵌入。", "motivation": "现有的轨迹表示学习（TRL）方法将轨迹视为孤立的时空序列，未能考虑其形成过程中的外部环境和内部路径选择行为，导致表示效果不佳。", "method": "PRTraj框架包含两个核心模块：1) 环境感知模块，通过捕获POI分布中的多粒度环境语义来增强路网；2) 路径选择编码器，在环境感知的基础上，将轨迹中的路段转换建模为一系列决策，从而捕捉固有的路径选择行为。最终，这些感知环境和路径选择的表示被聚合为全局轨迹嵌入。", "result": "在5个下游任务的3个真实世界数据集上进行的广泛实验验证了PRTraj的有效性和泛化能力。此外，PRTraj在少样本场景下表现出强大的数据效率和鲁棒性能。", "conclusion": "PRTraj通过统一环境感知和路径选择建模，显著提升了轨迹表示学习的效果和泛化能力，尤其在数据稀缺的情况下也能保持良好性能，为轨迹分析任务提供了更全面的表示方法。"}}
{"id": "2510.14077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14077", "abs": "https://arxiv.org/abs/2510.14077", "authors": ["Haziq Mohammad Khalid", "Athikash Jeyaganthan", "Timothy Do", "Yicheng Fu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models", "comment": "14 pages, 5 figures", "summary": "Large Language Models (LLMs) suffer significant performance degradation in\nmulti-turn conversations when information is presented incrementally. Given\nthat multi-turn conversations characterize everyday interactions with LLMs,\nthis degradation poses a severe challenge to real world usability. We\nhypothesize that abrupt increases in model uncertainty signal misalignment in\nmulti-turn LLM interactions, and we exploit this insight to dynamically realign\nconversational context. We introduce ERGO (Entropy-guided Resetting for\nGeneration Optimization), which continuously quantifies internal uncertainty\nvia Shannon entropy over next token distributions and triggers adaptive prompt\nconsolidation when a sharp spike in entropy is detected. By treating\nuncertainty as a first class signal rather than a nuisance to eliminate, ERGO\nembraces variability in language and modeling, representing and responding to\nuncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO\nyields a 56.6% average performance gain over standard baselines, increases\naptitude (peak performance capability) by 24.7%, and decreases unreliability\n(variability in performance) by 35.3%, demonstrating that uncertainty aware\ninterventions can improve both accuracy and reliability in conversational AI.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14305", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14305", "abs": "https://arxiv.org/abs/2510.14305", "authors": ["Mahbub E Sobhani", "Md. Faiyaz Abdullah Sayeedi", "Tasnim Mohiuddin", "Md Mofijul Islam", "Swakkhar Shatabda"], "title": "MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning", "comment": null, "summary": "Mathematical reasoning remains one of the most challenging domains for large\nlanguage models (LLMs), requiring not only linguistic understanding but also\nstructured logical deduction and numerical precision. While recent LLMs\ndemonstrate strong general-purpose reasoning abilities, their mathematical\ncompetence across diverse languages remains underexplored. Existing benchmarks\nprimarily focus on English or a narrow subset of high-resource languages,\nleaving significant gaps in assessing multilingual and cross-lingual\nmathematical reasoning. To address this, we introduce MathMist, a parallel\nmultilingual benchmark for mathematical problem solving and reasoning. MathMist\nencompasses over 21K aligned question-answer pairs across seven languages,\nrepresenting a balanced coverage of high-, medium-, and low-resource linguistic\nsettings. The dataset captures linguistic variety, multiple types of problem\nsettings, and solution synthesizing capabilities. We systematically evaluate a\ndiverse suite of models, including open-source small and medium LLMs,\nproprietary systems, and multilingual-reasoning-focused models, under\nzero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our\nresults reveal persistent deficiencies in LLMs' ability to perform consistent\nand interpretable mathematical reasoning across languages, with pronounced\ndegradation in low-resource settings. All the codes and data are available at\nGitHub: https://github.com/mahbubhimel/MathMist", "AI": {"tldr": "本文引入了MathMist，一个包含七种语言、21K对齐问答对的并行多语言数学推理基准。评估结果显示，大型语言模型在跨语言数学推理，特别是在低资源语言环境下，存在显著缺陷。", "motivation": "尽管大型语言模型（LLMs）展现出强大的通用推理能力，但其在多语言环境下的数学能力仍未得到充分探索。现有基准主要侧重于英语或少数高资源语言，导致在评估多语言和跨语言数学推理方面存在空白。", "method": "研究者引入了MathMist，一个并行多语言数学问题解决和推理基准，包含超过21K个跨七种语言对齐的问答对，涵盖高、中、低资源语言设置。他们系统地评估了包括开源LLM、专有系统和多语言推理模型在内的多种模型，采用了零样本、思维链（CoT）和代码切换推理范式。", "result": "评估结果表明，LLMs在跨语言执行一致且可解释的数学推理方面存在持续缺陷，尤其在低资源环境下表现出显著退化。", "conclusion": "LLMs在多语言数学推理方面，特别是在低资源语言环境中，仍面临严峻挑战，这突显了未来研究需要关注的方向。"}}
{"id": "2510.14823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14823", "abs": "https://arxiv.org/abs/2510.14823", "authors": ["Luca Morreale", "Alberto Gil C. P. Ramos", "Malcolm Chadwick", "Mehid Noroozi", "Ruchika Chavhan", "Abhinav Mehrotra", "Sourav Bhattacharya"], "title": "FraQAT: Quantization Aware Training with Fractional bits", "comment": null, "summary": "State-of-the-art (SOTA) generative models have demonstrated impressive\ncapabilities in image synthesis or text generation, often with a large capacity\nmodel. However, these large models cannot be deployed on smartphones due to the\nlimited availability of on-board memory and computations. Quantization methods\nlower the precision of the model parameters, allowing for efficient\ncomputations, \\eg, in \\INT{8}. Although aggressive quantization addresses\nefficiency and memory constraints, preserving the quality of the model remains\na challenge. To retain quality in previous aggressive quantization, we propose\na new fractional bits quantization (\\short) approach. The novelty is a simple\nyet effective idea: we progressively reduce the model's precision from 32 to 4\nbits per parameter, and exploit the fractional bits during optimization to\nmaintain high generation quality. We show that the \\short{} yields improved\nquality on a variety of diffusion models, including SD3.5-Medium, Sana,\n\\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard\nQAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the\nQualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).", "AI": {"tldr": "本文提出了一种名为分数比特量化（FBQ）的新方法，用于在保持生成质量的同时，将大型生成模型（如扩散模型）积极量化至4比特，使其能在移动设备上高效部署。", "motivation": "最先进的生成模型虽然能力强大，但由于内存和计算限制，无法部署在智能手机等移动设备上。激进的量化方法可以解决效率和内存问题，但往往难以保持模型质量。", "method": "本文提出分数比特量化（FBQ）方法。其核心思想是，在优化过程中，逐步将模型参数的精度从32比特降低到4比特，并利用分数比特来维持高质量的生成效果。", "result": "FBQ在多种扩散模型（包括SD3.5-Medium、Sana、Pixart和FLUX.1-schnell）上均提高了生成质量，其FiD得分比标准QAT低4-7%。此外，研究人员成功将Sana模型部署并在三星S25U手机上运行。", "conclusion": "分数比特量化（FBQ）是一种简单而有效的方法，能够对大型生成模型进行激进量化，使其在移动设备上实现高效部署的同时，显著保持甚至提升生成质量。"}}
{"id": "2510.14365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14365", "abs": "https://arxiv.org/abs/2510.14365", "authors": ["Anyun Zhuo", "Xuefei Ning", "Ningyuan Li", "Yu Wang", "Pinyan Lu"], "title": "On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?", "comment": null, "summary": "This work investigates the resilience of contemporary LLMs against frequent\nand structured character-level perturbations, specifically through the\ninsertion of noisy characters after each input character. We introduce\n\\nameshort{}, a practical method that inserts invisible Unicode control\ncharacters into text to discourage LLM misuse in scenarios such as online exam\nsystems. Surprisingly, despite strong obfuscation that fragments tokenization\nand reduces the signal-to-noise ratio significantly, many LLMs still maintain\nnotable performance. Through comprehensive evaluation across model-, problem-,\nand noise-related configurations, we examine the extent and mechanisms of this\nrobustness, exploring both the handling of character-level tokenization and\n\\textit{implicit} versus \\textit{explicit} denoising mechanism hypotheses of\ncharacter-level noises. We hope our findings on the low-level robustness of\nLLMs will shed light on the risks of their misuse and on the reliability of\ndeploying LLMs across diverse applications.", "AI": {"tldr": "本研究发现大型语言模型（LLMs）对字符级扰动（例如插入不可见Unicode字符）具有惊人的鲁棒性，即使在严重混淆和分词碎片化的情况下，仍能保持显著性能，这揭示了LLMs滥用的风险。", "motivation": "研究当代LLMs对频繁、结构化的字符级扰动（特别是每个输入字符后插入噪声字符）的韧性，并开发一种方法来阻止LLMs在在线考试系统等场景中被滥用。", "method": "引入了名为“INI”的实用方法，通过在文本中插入不可见的Unicode控制字符来创建字符级扰动。通过在模型、问题和噪声相关的配置上进行全面评估，探讨了这种鲁棒性的程度和机制，包括字符级分词的处理以及字符级噪声的隐式与显式去噪机制假设。", "result": "令人惊讶的是，尽管存在严重的混淆、分词碎片化和信噪比显著降低，许多LLMs仍然保持了显著的性能。", "conclusion": "研究结果揭示了LLMs的低级鲁棒性，这有助于理解其滥用的风险以及在不同应用中部署LLMs的可靠性。"}}
{"id": "2510.14660", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14660", "abs": "https://arxiv.org/abs/2510.14660", "authors": ["Linyue Ma", "Yilong Xu", "Xiang Long", "Zhi Zheng"], "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs", "comment": null, "summary": "Search augmentation empowers Large Language Models with retrieval\ncapabilities to overcome the limitations imposed by static parameters.\nRecently, Reinforcement Learning leverages tailored reward signals as a viable\ntechnique to enhance LLMs performing tasks involving search. However, existing\nreward modeling for search-augmented LLMs faces several limitations. Rule-based\nrewards, such as Exact Match, are verifiable but fragile to variations in\nexpression and cannot be applied to long-form workloads. In contrast,\ngenerative rewards improve robustness, but designing verifiable and stable\nrewards for long-form workloads in dynamic corpora remains challenging and also\nincurs high computational costs. In this paper, we propose a unified and\nverifiable paradigm, \"nugget-as-rubric\", which treats atomic information points\nas structured evaluation criteria for different search-augmentation workloads.\nShort-form tasks correspond to a single rubric, whereas long-form tasks expand\nto multiple rubrics aligned with the question's information needs. To support\nlong-form settings, we design an automatic rubric construction pipeline based\non query rewriting, which can automatically retrieve passages relevant to each\nquestion and extract rubrics from them, both from static corpora and from\ndynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a\n4B-parameter efficient generative verifier under our proposed verifiable\nparadigm, which is trained via the idea of distillation and a two-stage\nstrategy. Experimental results show that Search-Gen-V achieves strong\nverification accuracy across different workloads, making it a scalable, robust,\nand efficient verifiable reward constructor for search-augmented LLMs.", "AI": {"tldr": "本文提出了一种名为“信息点即评估标准”的统一可验证范式，并在此范式下开发了Search-Gen-V，一个高效的生成式验证器，用于为搜索增强型大型语言模型构建可验证的奖励信号，尤其擅长处理长篇任务。", "motivation": "现有搜索增强型大型语言模型的奖励模型存在局限：基于规则的奖励（如精确匹配）易受表达变化影响，不适用于长篇任务；生成式奖励虽然更鲁棒，但在动态语料库中为长篇任务设计可验证、稳定的奖励具有挑战性且计算成本高昂。", "method": "本文提出了“信息点即评估标准”（nugget-as-rubric）的统一可验证范式，将原子信息点作为结构化评估标准。对于短篇任务对应单一标准，长篇任务则扩展为多个与问题信息需求对齐的标准。为支持长篇设置，设计了一个基于查询重写的自动标准构建流程，能从静态和动态网络内容中检索相关段落并提取标准。此外，引入了Search-Gen-V，一个4B参数的高效生成式验证器，通过蒸馏和两阶段策略进行训练。", "result": "实验结果表明，Search-Gen-V在不同工作负载下均能实现强大的验证准确性。", "conclusion": "Search-Gen-V是为搜索增强型大型语言模型设计的一种可扩展、鲁棒且高效的可验证奖励构造器，有效解决了现有奖励模型的局限性。"}}
{"id": "2510.14831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14831", "abs": "https://arxiv.org/abs/2510.14831", "authors": ["Qi Chen", "Xinze Zhou", "Chen Liu", "Hao Chen", "Wenxuan Li", "Zekun Jiang", "Ziyan Huang", "Yuxuan Zhao", "Dexin Yu", "Junjun He", "Yefeng Zheng", "Ling Shao", "Alan Yuille", "Zongwei Zhou"], "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data", "comment": null, "summary": "AI for tumor segmentation is limited by the lack of large, voxel-wise\nannotated datasets, which are hard to create and require medical experts. In\nour proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found\nthat AI performance stopped improving after 1,500 scans. With synthetic data,\nwe reached the same performance using only 500 real scans. This finding\nsuggests that synthetic data can steepen data scaling laws, enabling more\nefficient model training than real data alone. Motivated by these lessons, we\ncreated AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130\ntumor instances per-voxel manually annotated in six organs (pancreas, liver,\nkidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23\nexpert radiologists, it is several orders of magnitude larger than existing\npublic tumor datasets. While we continue expanding the dataset, the current\nversion of AbdomenAtlas 2.0 already provides a strong foundation--based on\nlessons from the JHH dataset--for training AI to segment tumors in six organs.\nIt achieves notable improvements over public datasets, with a +7% DSC gain on\nin-distribution tests and +16% on out-of-distribution tests.", "AI": {"tldr": "肿瘤分割AI受限于缺乏大型像素级标注数据集。本文介绍了AbdomenAtlas 2.0，一个包含10,135张CT扫描、跨六个器官的超大型肿瘤标注数据集，并展示了其在提升AI性能方面的显著效果，同时指出合成数据可提高数据效率。", "motivation": "肿瘤分割AI受限于缺乏大型、像素级标注的数据集，此类数据集创建困难且需医学专家。研究发现AI性能在一定数量的真实数据后停止提升，而合成数据能以更少的真实数据达到相同性能，这促使研究者创建更大、更高效的数据集。", "method": "研究首先分析了专有JHH数据集（3,000张胰腺肿瘤扫描）上AI性能的扩展规律，并探索了合成数据对性能的影响。在此基础上，研究团队创建了AbdomenAtlas 2.0数据集，包含10,135张CT扫描，其中15,130个肿瘤实例在六个器官（胰腺、肝脏、肾脏、结肠、食道和子宫）进行了像素级手动标注，并有5,893张对照扫描，由23位专家放射科医生标注。最后，通过与公共数据集的对比，评估了AbdomenAtlas 2.0在AI肿瘤分割上的性能提升。", "result": "在JHH数据集上，AI性能在1,500张扫描后停止提升。使用合成数据，仅用500张真实扫描就达到了与1,500张真实扫描相同的性能。AbdomenAtlas 2.0比现有公共肿瘤数据集大几个数量级。基于AbdomenAtlas 2.0训练的AI模型在内部分布测试中DSC增益提高了7%，在外部分布测试中提高了16%，显著优于公共数据集。", "conclusion": "AbdomenAtlas 2.0数据集为六个器官的肿瘤分割AI训练提供了坚实基础，解决了数据稀缺的挑战。通过利用JHH数据集的经验和合成数据的潜力，该数据集显著提升了AI性能，尤其是在外部分布测试中，证明了其在医学图像分析中的巨大价值。"}}
{"id": "2510.14377", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14377", "abs": "https://arxiv.org/abs/2510.14377", "authors": ["Mykolas Sveistrys", "Richard Kunert"], "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora", "comment": null, "summary": "Recent advances in large language models (LLMs) and retrieval-augmented\ngeneration (RAG) have enabled progress on question answering (QA) when relevant\nevidence is in one (single-hop) or multiple (multi-hop) passages. Yet many\nrealistic questions about recurring report data - medical records, compliance\nfilings, maintenance logs - require aggregation across all documents, with no\nclear stopping point for retrieval and high sensitivity to even one missed\npassage. We term these pluri-hop questions and formalize them by three\ncriteria: recall sensitivity, exhaustiveness, and exactness. To study this\nsetting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48\npluri-hop questions built from 191 real-world wind industry reports in German\nand English. We show that PluriHopWIND is 8-40% more repetitive than other\ncommon datasets and thus has higher density of distractor documents, better\nreflecting practical challenges of recurring report corpora. We test a\ntraditional RAG pipeline as well as graph-based and multimodal variants, and\nfind that none of the tested approaches exceed 40% in statement-wise F1 score.\nMotivated by this, we propose PluriHopRAG, a RAG architecture that follows a\n\"check all documents individually, filter cheaply\" approach: it (i) decomposes\nqueries into document-level subquestions and (ii) uses a cross-encoder filter\nto discard irrelevant documents before costly LLM reasoning. We find that\nPluriHopRAG achieves relative F1 score improvements of 18-52% depending on base\nLLM. Despite its modest size, PluriHopWIND exposes the limitations of current\nQA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance\nhighlights the value of exhaustive retrieval and early filtering as a powerful\nalternative to top-k methods.", "AI": {"tldr": "本文定义了“多跳聚合式问题”（pluri-hop questions），并提出了PluriHopWIND数据集来研究这类问题。研究发现现有RAG方法表现不佳，因此提出了一种新的RAG架构PluriHopRAG，通过分解查询和早期过滤实现了显著的性能提升。", "motivation": "尽管大型语言模型（LLMs）和检索增强生成（RAG）在单跳和多跳问答方面取得了进展，但对于需要聚合所有文档信息（如医疗记录、合规文件）且对召回敏感的“多跳聚合式问题”，现有方法因无法确定检索停止点和对遗漏信息的敏感性而表现不足。", "method": "本文定义了多跳聚合式问题，并提出了PluriHopWIND数据集，该数据集包含48个问题，基于191份真实的德语和英语风能行业报告构建，具有高重复性和干扰文档密度。研究测试了传统的RAG管道、基于图和多模态的变体。在此基础上，提出PluriHopRAG架构，该架构将查询分解为文档级子问题，并使用交叉编码器过滤器在LLM推理之前廉价地丢弃不相关的文档。", "result": "在PluriHopWIND数据集上，所有测试的传统RAG方法（包括图基和多模态变体）的F1分数均未超过40%。PluriHopRAG根据基础LLM的不同，实现了18-52%的相对F1分数提升。PluriHopWIND数据集揭示了当前QA系统在重复、干扰丰富的语料库上的局限性。", "conclusion": "当前QA系统在处理重复、干扰丰富的语料库上的多跳聚合式问题时存在局限性。PluriHopRAG的“单独检查所有文档，廉价过滤”方法，即彻底检索和早期过滤，是替代top-k方法的一种强大且有效的方式，对这类问题具有重要价值。"}}
{"id": "2510.14628", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14628", "abs": "https://arxiv.org/abs/2510.14628", "authors": ["Qing Yang", "Zhenghao Liu", "Junxin Wang", "Yangfan Du", "Pengcheng Huang", "Tong Xiao"], "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF", "comment": null, "summary": "Text-To-Speech synthesis has achieved near-human quality in neutral speech,\nbut emotional expressiveness remains a challenge. Existing methods often rely\non costly emotion annotations or optimize indirect objectives that fail to\ncapture the emotional expressiveness and perceptual naturalness of speech,\nleading to generated speech that is accurate but emotionally flat. To address\nthese challenges, we propose the RLAIF-SPA framework, incorporating a\nReinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic\nSpeech Recognition (ASR) and Large Language Model (LLM) techniques to\nrespectively judge semantic accuracy and prosodic-emotional label alignment as\na direct reward for emotional expressiveness and intelligibility optimization.\nSpecifically, it leverages Prosodic Label Alignment to enhance expressive\nquality by jointly considering semantic accuracy and prosodic-emotional\nalignment along four fine-grained dimensions: Structure, Emotion, Speed, and\nTone. In addition, it incorporates Semantic Accuracy Feedback to ensure the\ngeneration of clear and accurate speech. Experiments on the Libri Speech\ndataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in\nWER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.", "AI": {"tldr": "本文提出了RLAIF-SPA框架，通过结合AI反馈强化学习（RLAIF）、自动语音识别（ASR）和大型语言模型（LLM），解决了文本到语音合成中情感表达的挑战，显著提升了语音的情感表现力和语义准确性。", "motivation": "文本到语音合成在自然语音方面已达到接近人类的质量，但在情感表达方面仍面临挑战。现有方法常依赖昂贵的情感标注或优化间接目标，导致生成的语音虽然准确但情感平淡。", "method": "本文提出了RLAIF-SPA框架，该框架整合了AI反馈强化学习（RLAIF）机制。它利用ASR来判断语义准确性，并使用LLM来判断韵律-情感标签对齐，将这两者作为情感表达和可懂度优化的直接奖励。具体来说，框架通过“韵律标签对齐”机制，从结构、情感、语速和语调四个维度共同考虑语义准确性和韵律-情感对齐，以增强表达质量。此外，它还结合了“语义准确性反馈”以确保生成语音的清晰度和准确性。", "result": "在Libri Speech数据集上进行的实验表明，RLAIF-SPA框架优于Chat-TTS。其语音错误率（WER）降低了26.1%，SIM-O指标提高了9.1%，并且在人工评估中获得了超过10%的提升。", "conclusion": "RLAIF-SPA框架通过引入AI反馈强化学习机制，并结合ASR和LLM，有效解决了文本到语音合成中情感表达的难题，显著提高了生成语音的情感表现力和语义准确性。"}}
{"id": "2510.14847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14847", "abs": "https://arxiv.org/abs/2510.14847", "authors": ["Meiqi Wu", "Jiashu Zhu", "Xiaokun Feng", "Chubin Chen", "Chen Zhu", "Bingze Song", "Fangyuan Mao", "Jiahong Wu", "Xiangxiang Chu", "Kaiqi Huang"], "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints", "comment": null, "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.", "AI": {"tldr": "针对视频生成模型在想象力场景中表现不佳的问题，本文提出了ImagerySearch，一种根据提示语义关系动态调整推理搜索空间和奖励函数的自适应测试时搜索策略，并引入了首个长距离语义提示基准LDT-Bench，实验证明了其有效性。", "motivation": "现有视频生成模型在现实场景中表现出色，但在想象力场景（涉及罕见概念和长距离语义关系）中性能显著下降，因为这些场景超出了训练分布。现有的测试时缩放方法受限于固定的搜索空间和静态奖励设计，难以适应想象力场景。", "method": "本文提出了ImagerySearch，一种提示引导的自适应测试时搜索策略，它根据提示中的语义关系动态调整推理搜索空间和奖励函数。此外，本文还引入了LDT-Bench，这是首个专门针对长距离语义提示的基准，包含2,839对不同的概念和自动化评估协议。", "result": "实验结果表明，ImagerySearch在LDT-Bench上持续优于强大的视频生成基线和现有的测试时缩放方法，并在VBench上取得了有竞争力的改进，证明了其在各种提示类型上的有效性。", "conclusion": "ImagerySearch通过动态调整推理搜索空间和奖励函数，有效解决了视频生成在想象力场景中的挑战，能够生成更连贯、视觉上更合理的视频。LDT-Bench的发布将促进未来在想象力视频生成领域的研究。"}}
{"id": "2510.14351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14351", "abs": "https://arxiv.org/abs/2510.14351", "authors": ["Perapard Ngokpol", "Kun Kerdthaisong", "Pasin Buakhaw", "Pitikorn Khlaisamniang", "Supasate Vorathammathorn", "Piyalitt Ittichaiwong", "Nutchanon Yongsatianchot"], "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts", "comment": null, "summary": "Large language models (LLMs) are increasingly used as role-playing agents,\nyet their capacity to faithfully and consistently portray version-specific\ncharacters -- for example, superheroes across comic and cinematic universes --\nremains underexplored. Superhero canons such as Marvel and DC provide a rich\ntestbed: decades of storytelling yield multiple incarnations of the same\ncharacter with distinct histories, values, and moral codes. To study this\nproblem, we introduce Beyond One World, a benchmark for character-grounded\nroleplay spanning 30 iconic heroes and 90 canon-specific versions. The\nbenchmark comprises two tasks: (i) Canon Events, which probes factual recall of\npivotal life stages, and (ii) Moral Dilemmas, which confronts models with\nethically charged scenarios. We score responses for canonical accuracy and\nreasoning fidelity under a framework that separates internal deliberation\n(\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act\nMatching, a metric that quantifies alignment between reasons and actions and\nserves as a proxy for model trustworthiness. Experiments across reasoning- and\nnon-reasoning-oriented models yield three findings: (1) chain-of-thought\nprompting improves narrative coherence in weaker models but can reduce\ncanonical accuracy in stronger ones; (2) cross-version generalization within a\ncharacter remains a major obstacle; and (3) models often excel at either\nthinking or acting, but rarely both. Beyond One World exposes critical gaps in\nmultiversal consistency and reasoning alignment, offering a challenging\nevaluation for role-playing LLMs.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14318", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14318", "abs": "https://arxiv.org/abs/2510.14318", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Aryansh Shrivastava", "Natasha Jaques", "Yarin Gal", "Sergey Levine"], "title": "Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL", "comment": null, "summary": "Large Language Models (LLMs) interact with millions of people worldwide in\napplications such as customer support, education and healthcare. However, their\nability to produce deceptive outputs, whether intentionally or inadvertently,\nposes significant safety concerns. The unpredictable nature of LLM behavior,\ncombined with insufficient safeguards against hallucination, misinformation,\nand user manipulation, makes their misuse a serious, real-world risk. In this\npaper, we investigate the extent to which LLMs engage in deception within\ndialogue, and propose the belief misalignment metric to quantify deception. We\nevaluate deception across four distinct dialogue scenarios, using five\nestablished deception detection metrics and our proposed metric. Our findings\nreveal this novel deception measure correlates more closely with human\njudgments than any existing metrics we test. Additionally, our benchmarking of\neight state-of-the-art models indicates that LLMs naturally exhibit deceptive\nbehavior in approximately 26% of dialogue turns, even when prompted with\nseemingly benign objectives. When prompted to deceive, LLMs are capable of\nincreasing deceptiveness by as much as 31% relative to baselines. Unexpectedly,\nmodels trained with RLHF, the predominant approach for ensuring the safety of\nwidely-deployed LLMs, still exhibit deception at a rate of 43% on average.\nGiven that deception in dialogue is a behavior that develops over an\ninteraction history, its effective evaluation and mitigation necessitates\nmoving beyond single-utterance analyses. We introduce a multi-turn\nreinforcement learning methodology to fine-tune LLMs to reduce deceptive\nbehaviors, leading to a 77.6% reduction compared to other instruction-tuned\nmodels.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14395", "abs": "https://arxiv.org/abs/2510.14395", "authors": ["Jun Li", "Qun Zhao"], "title": "Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis", "comment": null, "summary": "Suicide remains a critical global public health issue. While previous studies\nhave provided valuable insights into detecting suicidal expressions in\nindividual social media posts, limited attention has been paid to the analysis\nof longitudinal, sequential comment trees for predicting a user's evolving\nsuicidal risk. Users, however, often reveal their intentions through historical\nposts and interactive comments over time. This study addresses this gap by\ninvestigating how the information in comment trees affects both the\ndiscrimination and prediction of users' suicidal risk levels. We constructed a\nhigh-quality annotated dataset, sourced from Reddit, which incorporates users'\nposting history and comments, using a refined four-label annotation framework\nbased on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical\nanalysis of the dataset, along with experimental results from Large Language\nModels (LLMs) experiments, demonstrates that incorporating comment trees data\nsignificantly enhances the discrimination and prediction of user suicidal risk\nlevels. This research offers a novel insight to enhancing the detection\naccuracy of at-risk individuals, thereby providing a valuable foundation for\nearly suicide intervention strategies.", "AI": {"tldr": "本研究通过分析社交媒体上用户历史帖子和评论树中的纵向、序列信息，显著提升了用户自杀风险的识别和预测准确性，为早期干预提供了基础。", "motivation": "以往研究多关注单个社交媒体帖子的自杀表达检测，但忽略了用户通过历史帖子和交互式评论随时间推移逐渐显露意图的纵向、序列评论树分析，这限制了对用户不断演变的自杀风险的预测能力。", "method": "研究构建了一个高质量的Reddit标注数据集，包含用户的发帖历史和评论，并采用基于哥伦比亚自杀严重程度评定量表（C-SSRS）的四标签标注框架。通过对数据集进行统计分析，并利用大型语言模型（LLMs）进行实验。", "result": "统计分析和LLMs实验结果均表明，整合评论树数据显著增强了用户自杀风险水平的区分和预测能力。", "conclusion": "这项研究为提高高危个体的检测准确性提供了新颖的见解，为早期自杀干预策略奠定了宝贵基础。"}}
{"id": "2510.14855", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14855", "abs": "https://arxiv.org/abs/2510.14855", "authors": ["Harsha Kotla", "Arun Kumar Rajasekaran", "Hannah Rana"], "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation", "comment": null, "summary": "Early detection of melanoma has grown to be essential because it\nsignificantly improves survival rates, but automated analysis of skin lesions\nstill remains challenging. ABCDE, which stands for Asymmetry, Border\nirregularity, Color variation, Diameter, and Evolving, is a well-known\nclassification method for skin lesions, but most deep learning mechanisms treat\nit as a black box, as most of the human interpretable features are not\nexplained. In this work, we propose a deep learning framework that both\nclassifies skin lesions into categories and also quantifies scores for each\nABCD feature. It simulates the evolution of these features over time in order\nto represent the E aspect, opening more windows for future exploration. The A,\nB, C, and D values are quantified particularly within this work. Moreover, this\nframework also visualizes ABCD feature trajectories in latent space as skin\nlesions evolve from benign nevuses to malignant melanoma. The experiments are\nconducted using the HAM10000 dataset that contains around ten thousand images\nof skin lesions of varying stages. In summary, the classification worked with\nan accuracy of around 89 percent, with melanoma AUC being 0.96, while the\nfeature evaluation performed well in predicting asymmetry, color variation, and\ndiameter, though border irregularity remains more difficult to model. Overall,\nthis work provides a deep learning framework that will allow doctors to link ML\ndiagnoses to clinically relevant criteria, thus improving our understanding of\nskin cancer progression.", "AI": {"tldr": "该研究提出了一个深度学习框架，不仅能对皮肤病变进行分类，还能量化ABCDE特征评分，并模拟其随时间演变，以提高黑色素瘤诊断的可解释性。", "motivation": "黑色素瘤的早期检测至关重要，但自动化分析仍具挑战。现有的深度学习模型在皮肤病变分类中常作为“黑箱”，未能解释人类可理解的ABCDE特征，这阻碍了医生理解和信任机器学习诊断。", "method": "本文提出了一个深度学习框架，该框架：1) 对皮肤病变进行分类；2) 量化不对称性（A）、边缘不规则性（B）、颜色变异（C）和直径（D）等ABCD特征的得分；3) 模拟这些特征随时间演变以表示“演变”（E）方面；4) 可视化潜在空间中ABCD特征的演变轨迹。实验基于HAM10000数据集进行。", "result": "该框架的分类准确率约为89%，黑色素瘤的AUC为0.96。特征评估在预测不对称性、颜色变异和直径方面表现良好，但边缘不规则性仍难以建模。", "conclusion": "这项工作提供了一个深度学习框架，能够将机器学习诊断与临床相关的ABCDE标准联系起来，从而加深我们对皮肤癌进展的理解，并为医生提供更具洞察力的诊断工具。"}}
{"id": "2510.14773", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14773", "abs": "https://arxiv.org/abs/2510.14773", "authors": ["Hwiyeol Jo", "Joosung Lee", "Jaehone Lee", "Sang-Woo Lee", "Joonsuk Park", "Kang Min Yoo"], "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning", "comment": "ARR Submitted", "summary": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation.", "AI": {"tldr": "针对推理模型，答案提取方法对其性能和答案分布有显著影响。本文提出“答案再生”框架，通过额外模型推理和特定提示词，提高模型评估性能和鲁棒性，使其更可靠。", "motivation": "生成模型（特别是需要推理的模型）的评估中，答案提取算法对其性能和最终答案分布具有高度敏感性。", "method": "提出“答案再生”框架。该方法通过额外一次模型推理，将先前的输入和输出作为输入，并加上“Answer:”提示词，然后从重新生成的输出中选择或提取最终答案。", "result": "该与提取规则无关的方法展示了改进的性能和增强的鲁棒性。该框架已成功应用于通用数学问题和开放式问答任务。", "conclusion": "本研究的分析和提出的框架可以为模型评估提供更可靠的结果。"}}
{"id": "2510.14866", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14866", "abs": "https://arxiv.org/abs/2510.14866", "authors": ["Hatef Otroshi Shahreza", "Sébastien Marcel"], "title": "Benchmarking Multimodal Large Language Models for Face Recognition", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page.", "AI": {"tldr": "本研究系统评估了多模态大语言模型（MLLMs）在人脸识别任务上的性能，发现它们虽然能捕获丰富语义信息，但在零样本高精度识别场景中仍落后于专用模型。", "motivation": "尽管多模态大语言模型在视觉-语言任务中表现出色，但它们在人脸识别领域的潜力尚未充分探索。特别是，需要在一个标准协议下，在标准基准上评估开源MLLMs的性能，并与现有的人脸识别模型进行比较。", "method": "研究对最先进的MLLMs进行了系统性基准测试，用于人脸识别任务。测试使用了多个标准人脸识别数据集，包括LFW、CALFW、CPLFW、CFP、AgeDB和RFW。", "result": "实验结果表明，MLLMs能够捕获对人脸相关任务有用的丰富语义线索，但在零样本应用中的高精度识别场景中，其性能落后于专门的人脸识别模型。", "conclusion": "该基准测试为推动基于MLLM的人脸识别研究奠定了基础，并为设计具有更高准确性和泛化能力的新一代模型提供了见解。"}}
{"id": "2510.14242", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14242", "abs": "https://arxiv.org/abs/2510.14242", "authors": ["Parsa Hejabi", "Elnaz Rahmati", "Alireza S. Ziabari", "Morteza Dehghani"], "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs", "comment": "14 pages, 6 figures, 3 tables, and 1 algorithm", "summary": "Large Language Models (LLMs) often produce inconsistent answers when faced\nwith different phrasings of the same prompt. In this paper, we propose\nFlip-Flop Consistency ($F^2C$), an unsupervised training method that improves\nrobustness to such perturbations. $F^2C$ is composed of two key components. The\nfirst, Consensus Cross-Entropy (CCE), uses a majority vote across prompt\nvariations to create a hard pseudo-label. The second is a representation\nalignment loss that pulls lower-confidence and non-majority predictors toward\nthe consensus established by high-confidence, majority-voting variations. We\nevaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt\nvariations per dataset. On average, $F^2C$ raises observed agreement by 11.62%,\nimproves mean $F_1$ by 8.94%, and reduces performance variance across formats\nby 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively,\nincreasing $\\overline{F_1}$ and agreement while decreasing variance across most\nsource-target pairs. Finally, when trained on only a subset of prompt\nperturbations and evaluated on held-out formats, $F^2C$ consistently improves\nboth performance and agreement while reducing variance. These findings\nhighlight $F^2C$ as an effective unsupervised method for enhancing LLM\nconsistency, performance, and generalization under prompt perturbations. Code\nis available at\nhttps://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14874", "abs": "https://arxiv.org/abs/2510.14874", "authors": ["Guangyi Han", "Wei Zhai", "Yuhang Yang", "Yang Cao", "Zheng-Jun Zha"], "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions", "comment": null, "summary": "Hand-object interaction (HOI) is fundamental for humans to express intent.\nExisting HOI generation research is predominantly confined to fixed grasping\npatterns, where control is tied to physical priors such as force closure or\ngeneric intent instructions, even when expressed through elaborate language.\nSuch an overly general conditioning imposes a strong inductive bias for stable\ngrasps, thus failing to capture the diversity of daily HOI. To address these\nlimitations, we introduce Free-Form HOI Generation, which aims to generate\ncontrollable, diverse, and physically plausible HOI conditioned on fine-grained\nintent, extending HOI from grasping to free-form interactions, like pushing,\npoking, and rotating. To support this task, we construct WildO2, an in-the-wild\ndiverse 3D HOI dataset, which includes diverse HOI derived from internet\nvideos. Specifically, it contains 4.4k unique interactions across 92 intents\nand 610 object categories, each with detailed semantic annotations. Building on\nthis dataset, we propose TOUCH, a three-stage framework centered on a\nmulti-level diffusion model that facilitates fine-grained semantic control to\ngenerate versatile hand poses beyond grasping priors. This process leverages\nexplicit contact modeling for conditioning and is subsequently refined with\ncontact consistency and physical constraints to ensure realism. Comprehensive\nexperiments demonstrate our method's ability to generate controllable, diverse,\nand physically plausible hand interactions representative of daily activities.\nThe project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.", "AI": {"tldr": "本文提出“自由形式手物交互生成”任务，旨在生成可控、多样且物理合理的手物交互，超越传统固定抓取模式。为此，构建了大规模野外3D手物交互数据集WildO2，并提出了基于多级扩散模型的TOUCH三阶段框架，实现了精细语义控制下的多样化手部姿态生成。", "motivation": "现有的手物交互（HOI）生成研究主要局限于固定的抓取模式，依赖于力闭合等物理先验或通用意图指令。这种过于宽泛的条件设定导致了对稳定抓取的强烈归纳偏见，未能捕捉到日常HOI的多样性，例如推、戳、旋转等自由形式交互。", "method": "本文引入“自由形式手物交互生成”任务，并构建了WildO2数据集，该数据集包含来自网络视频的4.4k个独特交互、92种意图和610个物体类别，并附有详细语义标注。在此基础上，提出了TOUCH三阶段框架，其核心是一个多级扩散模型，能够实现精细语义控制，生成超越抓取先验的多功能手部姿态。该方法利用显式接触建模进行条件设定，并通过接触一致性和物理约束进行后续细化，以确保真实感。", "result": "全面的实验证明，本文方法能够生成可控、多样且物理合理的手部交互，代表了日常活动中的多种交互形式。", "conclusion": "本文成功解决了现有HOI生成研究中固定抓取模式的局限性，通过引入自由形式HOI生成任务、构建大规模多样化数据集WildO2以及提出创新的TOUCH框架，实现了对日常活动中多样化、可控且物理合理的手物交互的生成。"}}
{"id": "2510.14862", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.14862", "abs": "https://arxiv.org/abs/2510.14862", "authors": ["Mihai-Cristian Pîrvu", "Marius Leordeanu"], "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision", "comment": null, "summary": "The real-world is inherently multi-modal at its core. Our tools observe and\ntake snapshots of it, in digital form, such as videos or sounds, however much\nof it is lost. Similarly for actions and information passing between humans,\nlanguages are used as a written form of communication. Traditionally, Machine\nLearning models have been unimodal (i.e. rgb -> semantic or text ->\nsentiment_class). Recent trends go towards bi-modality, where images and text\nare learned together, however, in order to truly understand the world, we need\nto integrate all these independent modalities. In this work we try to combine\nas many visual modalities as we can using little to no human supervision. In\norder to do this, we use pre-trained experts and procedural combinations\nbetween them on top of raw videos using a fully autonomous data-pipeline, which\nwe also open-source. We then make use of PHG-MAE, a model specifically designed\nto leverage multi-modal data. We show that this model which was efficiently\ndistilled into a low-parameter (<1M) can have competitive results compared to\nmodels of ~300M parameters. We deploy this model and analyze the use-case of\nreal-time semantic segmentation from handheld devices or webcams on commodity\nhardware. Finally, we deploy other off-the-shelf models using the same\nframework, such as DPT for near real-time depth estimation.", "AI": {"tldr": "该研究旨在通过结合尽可能多的视觉模态，利用预训练专家和自主数据管道，以及专门的多模态模型PHG-MAE，以极少的人工监督实现对真实世界的理解。其成果是一个参数量极低（<1M）但性能可与大型模型媲美的模型，适用于商品硬件上的实时语义分割和深度估计。", "motivation": "现实世界本质上是多模态的，但传统的机器学习模型通常是单模态或双模态的。为了真正理解世界，需要整合所有独立的模态，因为数字形式的观察（如视频、声音）会丢失大量信息。", "method": "研究方法包括：1) 尽可能多地结合视觉模态，且几乎无需人工监督；2) 在原始视频上使用预训练专家和程序化组合；3) 采用完全自主的数据管道（已开源）；4) 使用专门为利用多模态数据设计的PHG-MAE模型；5) 将PHG-MAE高效蒸馏成低参数（<1M）模型；6) 部署该模型并分析其在手持设备或网络摄像头上进行实时语义分割的应用；7) 使用相同框架部署其他现成模型（如DPT用于近实时深度估计）。", "result": "研究结果表明：1) 经过高效蒸馏的低参数PHG-MAE模型（<1M参数）与约300M参数的模型相比，能取得具有竞争力的结果；2) 该模型可以在商品硬件上的手持设备或网络摄像头实现实时语义分割；3) 使用相同框架，其他现成模型（如DPT）可以实现近实时深度估计。", "conclusion": "该研究成功展示了一种在极少人工监督下，通过整合多模态数据来理解真实世界的方法。通过利用预训练专家和高效蒸馏的PHG-MAE模型，实现了在商品硬件上进行实时多模态理解的高效模型，为更全面地理解多模态真实世界提供了可能。"}}
{"id": "2510.14438", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14438", "abs": "https://arxiv.org/abs/2510.14438", "authors": ["Rui Wang", "Ce Zhang", "Jun-Yu Ma", "Jianshu Zhang", "Hongru Wang", "Yi Chen", "Boyang Xue", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Kam-Fai Wong"], "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents", "comment": null, "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations.", "AI": {"tldr": "本文提出“探索以演进”（Explore to Evolve）范式，为网络智能体构建可验证的训练数据，以增强其信息聚合能力。基于此范式，作者开发了WebAggregator模型，其32B版本在GAIA-text上超越GPT-4.1，并构建了一个新的聚合能力基准WebAggregatorQA，揭示了现有领先模型在该任务上的不足。", "motivation": "现有开源深度研究智能体主要侧重于增强信息检索能力，以定位特定信息，却忽视了信息聚合的关键需求，这限制了它们支持深度研究的能力。", "method": "本文提出了“探索以演进”（Explore to Evolve）范式来可扩展地构建可验证的网络智能体训练数据。该范式首先通过主动在线探索从真实网络获取基础信息，然后智能体利用收集到的证据，通过选择、组合和精炼12种高级逻辑类型的操作，自主演进一个聚合程序，以合成可验证的问答对。通过这种方式，作者生成了WebAggregatorQA数据集（包含1万个样本，覆盖5万个网站和11个领域），并基于SmolAgents框架收集监督微调轨迹，开发了一系列基础模型WebAggregator。此外，作者还构建了WebAggregatorQA的人工标注评估子集作为挑战性测试集。", "result": "WebAggregator-8B模型达到了GPT-4.1的性能水平，而WebAggregator-32B版本在GAIA-text上超越GPT-4.1超过10%，并接近Claude-3.7-sonnet的性能。在新构建的WebAggregatorQA基准测试中，Claude-3.7-sonnet仅取得28%的得分，GPT-4.1得分25.8%。即使智能体能够检索到所有参考文献，它们在WebAggregatorQA上仍然表现不佳。", "conclusion": "当前领先的网络智能体在信息聚合能力方面存在显著不足，即使在所有参考信息都可用的情况下也难以完成聚合任务。这突出表明，需要加强网络智能体基础模型的信息聚合能力。"}}
{"id": "2510.14453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14453", "abs": "https://arxiv.org/abs/2510.14453", "authors": ["Reid T. Johnson", "Michelle D. Pain", "Jordan D. West"], "title": "Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents", "comment": "31 pages, 7 figures", "summary": "We present Natural Language Tools (NLT), a framework that replaces\nprogrammatic JSON tool calling in large language models (LLMs) with natural\nlanguage outputs. By decoupling tool selection from response generation, NLT\neliminates task interference and format constraints that degrade tool call\nperformance. When evaluated across 10 models and 6,400 trials spanning customer\nservice and mental health domains, NLT improves tool calling accuracy by 18.4\npercentage points while reducing output variance by 70%. Open-weight models see\nthe largest gains, surpassing flagship closed-weight alternatives, with\nimplications for model training in both reinforcement learning and supervised\nfine-tuning stages. These improvements persist under prompt perturbations and\nextend tool-calling capabilities to models lacking native support.", "AI": {"tldr": "NLT框架通过使用自然语言输出而非程序化JSON调用工具，显著提升了大型语言模型（LLMs）的工具调用准确性并降低了输出方差，尤其对开源模型效果显著。", "motivation": "LLMs中程序化JSON工具调用存在任务干扰和格式限制，导致工具调用性能下降。", "method": "NLT（Natural Language Tools）框架通过用自然语言输出取代程序化JSON工具调用，将工具选择与响应生成解耦。", "result": "NLT在10个模型和6,400次试验（涵盖客户服务和心理健康领域）中，将工具调用准确率提高了18.4个百分点，同时将输出方差降低了70%。开源模型获得了最大的提升，甚至超越了旗舰闭源模型。这些改进在提示扰动下依然存在，并能将工具调用能力扩展到原生不支持的模型。", "conclusion": "NLT框架有效解决了现有LLM工具调用中的性能问题，通过自然语言方法大幅提升了准确性和稳定性，对模型训练（强化学习和监督微调）具有重要意义，尤其利好开源模型。"}}
{"id": "2510.14904", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14904", "abs": "https://arxiv.org/abs/2510.14904", "authors": ["Gabriel Fiastre", "Antoine Yang", "Cordelia Schmid"], "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos", "comment": "20 pages, 8 figures", "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.", "AI": {"tldr": "本文提出MaskCaptioner，一个端到端模型，通过利用最先进的VLM生成合成字幕，并在扩展数据集（LVISCap和LV-VISCap）上进行预训练，实现了视频密集物体描述（DVOC）任务的最新SOTA性能。", "motivation": "密集视频物体描述（DVOC）任务复杂，需要理解时空细节并用自然语言描述。以往的方法采用分离训练策略，可能导致次优性能。此外，手动标注成本高昂，限制了大规模数据集的构建。", "method": "本文利用最先进的视觉语言模型（VLM）为时空局部实体生成合成字幕。通过将这些合成字幕扩展到LVIS和LV-VIS数据集，创建了LVISCap和LV-VISCap。随后，在这些数据集上训练了一个名为MaskCaptioner的端到端模型，该模型能够联合检测、分割、跟踪和描述物体轨迹。", "result": "MaskCaptioner在LVISCap和LV-VISCap上进行预训练后，在VidSTG、VLN和BenSMOT这三个现有基准测试中，取得了视频密集物体描述（DVOC）任务的最新SOTA结果。", "conclusion": "通过提出一个端到端模型MaskCaptioner，并利用合成字幕扩展数据集，本文成功解决了DVOC任务的复杂性和标注成本高的问题，并显著提升了该任务的性能，达到了当前最佳水平。"}}
{"id": "2510.14919", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14919", "abs": "https://arxiv.org/abs/2510.14919", "authors": ["Kyle Montgomery", "David Park", "Jianhong Tu", "Michael Bendersky", "Beliz Gunel", "Dawn Song", "Chenguang Wang"], "title": "Predicting Task Performance with Context-aware Scaling Laws", "comment": null, "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.", "AI": {"tldr": "本文提出一个新框架，通过联合建模训练计算量和上下文长度来预测大型语言模型在下游任务上的性能，并经验性地验证了其准确性和泛化能力。", "motivation": "传统的扩展定律未能捕捉下游任务性能，因为上下文在下游任务中起着关键作用。因此，需要一个能将训练计算量和上下文考虑在内的框架来预测下游性能。", "method": "研究者提出了一个可解释的框架，将下游性能建模为训练计算量和所提供上下文的函数。他们通过在Llama-2-7B和Llama-2-13B的扩展上下文变体上，针对算术推理、常识推理和机器翻译这三个任务的65,500个独特实例进行拟合，对该框架进行了实证验证。", "result": "研究结果表明，该框架能准确建模分布内的下游性能，在三个数量级的训练计算量范围内具有泛化性，并能可靠地推断随上下文量增加而变化的性能。", "conclusion": "这些发现为训练计算量和上下文利用之间的相互作用提供了宝贵见解，为设计更高效、适用于不同下游任务的长上下文大型语言模型提供了指导。"}}
{"id": "2510.14504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14504", "abs": "https://arxiv.org/abs/2510.14504", "authors": ["Matt Grenander", "Shay B. Cohen", "Mark Steedman"], "title": "Efficient Seq2seq Coreference Resolution Using Entity Representations", "comment": null, "summary": "Seq2seq coreference models have introduced a new paradigm for coreference\nresolution by learning to generate text corresponding to coreference labels,\nwithout requiring task-specific parameters. While these models achieve new\nstate-of-the-art performance, they do so at the cost of flexibility and\nefficiency. In particular, they do not efficiently handle incremental settings\nsuch as dialogue, where text must processed sequentially. We propose a\ncompressed representation in order to improve the efficiency of these methods\nin incremental settings. Our method works by extracting and re-organizing\nentity-level tokens, and discarding the majority of other input tokens. On\nOntoNotes, our best model achieves just 0.6 CoNLL F1 points below a\nfull-prefix, incremental baseline while achieving a compression ratio of 1.8.\nOn LitBank, where singleton mentions are annotated, it passes state-of-the-art\nperformance. Our results indicate that discarding a wide portion of tokens in\nseq2seq resolvers is a feasible strategy for incremental coreference\nresolution.", "AI": {"tldr": "本文提出了一种压缩表示方法，通过提取实体级token并丢弃大部分其他输入token，以提高Seq2seq共指消解模型在增量设置（如对话）中的效率和灵活性，同时保持高性能。", "motivation": "Seq2seq共指消解模型虽然达到了SOTA性能，但其灵活性和效率较低，尤其是在需要顺序处理文本的增量设置（如对话）中表现不佳。", "method": "该方法通过提取和重新组织实体级token，并丢弃大部分其他输入token，来创建一种压缩表示，从而提高模型在增量设置中的效率。", "result": "在OntoNotes数据集上，最佳模型仅比全前缀增量基线低0.6 CoNLL F1点，同时实现了1.8的压缩比。在LitBank数据集（标注了单例提及）上，该模型超越了SOTA性能。", "conclusion": "在Seq2seq共指消解器中丢弃大部分token是实现增量共指消解的一种可行策略。"}}
{"id": "2510.14876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14876", "abs": "https://arxiv.org/abs/2510.14876", "authors": ["Roni Goldshmidt", "Hamish Scott", "Lorenzo Niccolini", "Shizhan Zhu", "Daniel Moura", "Orly Zvitia"], "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data", "comment": null, "summary": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research.", "AI": {"tldr": "本文提出了BADAS，一个基于Nexar行车记录仪数据的碰撞预测模型家族，旨在解决现有方法无法区分自我车辆威胁的问题，并在多个基准测试中取得了最先进的性能。", "motivation": "现有的碰撞预测方法未能区分自我车辆威胁和不涉及自我车辆的随机事故，导致在实际部署中产生过多的虚假警报。", "method": "该研究引入了BADAS模型，采用V-JEPA2骨干网络进行端到端训练。模型在Nexar的真实世界行车记录仪碰撞数据集上进行训练，该数据集是第一个明确为自我中心评估设计的基准。研究者重新标注了主要的基准数据集，以识别自我车辆的参与度，添加了共识警报时间标签，并在需要时合成了负样本，以实现公平的AP/AUC和时间评估。BADAS有两个变体：BADAS-Open（在1.5k公开视频上训练）和BADAS1.0（在40k专有视频上训练）。", "result": "BADAS在DAD、DADA-2000、DoTA和Nexar等数据集上均取得了最先进的AP/AUC性能，并优于前向碰撞ADAS基线，同时生成了更真实的事故发生时间估计。研究者还发布了BADAS-Open模型权重、代码以及所有评估数据集的重新标注，以促进自我中心碰撞预测研究。", "conclusion": "BADAS是一个有效的自我中心碰撞预测模型家族，它解决了现有方法在区分自我车辆威胁方面的不足，并在多项基准测试中展现出卓越的性能和更准确的预测时间。通过发布模型和数据，该研究为未来的自我中心碰撞预测研究奠定了基础。"}}
{"id": "2510.14882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14882", "abs": "https://arxiv.org/abs/2510.14882", "authors": ["Keli Liu", "Zhendong Wang", "Wengang Zhou", "Shaodong Xu", "Ruixiao Dong", "Houqiang Li"], "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention", "comment": null, "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.", "AI": {"tldr": "ScaleWeaver是一个新颖的框架，通过参数高效微调，在先进的视觉自回归（VAR）模型上实现了高保真、可控的文本到图像生成，同时保持了高效率。", "motivation": "尽管视觉自回归（VAR）模型在生成保真度和推理效率方面取得了显著进展，但与扩散模型相比，其控制机制的探索不足，无法实现精确和灵活的控制。本文旨在弥补这一关键空白。", "method": "ScaleWeaver框架通过参数高效微调先进的VAR模型来实现可控生成。其核心模块是改进的MMDiT块，其中包含提出的Reference Attention模块。该模块通过丢弃不必要的图像到条件的注意力，降低了计算成本并稳定了控制注入。此外，它策略性地强调参数复用，利用VAR骨干网络的能力处理控制信息，并引入了一个零初始化的线性投影以确保有效整合控制信号而不干扰基础模型的生成能力。", "result": "广泛的实验表明，ScaleWeaver在提供高质量生成和精确控制的同时，比基于扩散的方法具有更高的效率。", "conclusion": "ScaleWeaver为视觉自回归范式下的可控文本到图像生成提供了一个实用且有效的解决方案。"}}
{"id": "2510.14565", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14565", "abs": "https://arxiv.org/abs/2510.14565", "authors": ["Kyubyung Chae", "Gihoon Kim", "Gyuseong Lee", "Taesup Kim", "Jaejin Lee", "Heejin Kim"], "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs", "comment": null, "summary": "Recent trends in LLMs development clearly show growing interest in the use\nand application of sovereign LLMs. The global debate over sovereign LLMs\nhighlights the need for governments to develop their LLMs, tailored to their\nunique socio-cultural and historical contexts. However, there remains a\nshortage of frameworks and datasets to verify two critical questions: (1) how\nwell these models align with users' socio-cultural backgrounds, and (2) whether\nthey maintain safety and technical robustness without exposing users to\npotential harms and risks. To address this gap, we construct a new dataset and\nintroduce an analytic framework for extracting and evaluating the\nsocio-cultural elements of sovereign LLMs, alongside assessments of their\ntechnical robustness. Our experimental results demonstrate that while sovereign\nLLMs play a meaningful role in supporting low-resource languages, they do not\nalways meet the popular claim that these models serve their target users well.\nWe also show that pursuing this untested claim may lead to underestimating\ncritical quality attributes such as safety. Our study suggests that advancing\nsovereign LLMs requires a more extensive evaluation that incorporates a broader\nrange of well-grounded and practical criteria.", "AI": {"tldr": "该研究构建了一个新数据集和分析框架，用于评估主权大型语言模型（LLMs）的社会文化契合度、安全性及技术稳健性。结果显示，主权LLMs在支持低资源语言方面有意义，但其与目标用户的契合度常被高估，且可能忽视关键安全问题。", "motivation": "主权LLMs的开发日益受到关注，各国政府希望开发符合其独特社会文化和历史背景的LLMs。然而，目前缺乏框架和数据集来验证这些模型与用户社会文化背景的契合度，以及它们在保持安全和技术稳健性方面是否能避免潜在危害和风险。", "method": "研究构建了一个新的数据集，并引入了一个分析框架，用于提取和评估主权LLMs的社会文化元素，并同时评估其技术稳健性。", "result": "实验结果表明，主权LLMs在支持低资源语言方面发挥着有意义的作用，但它们并非总是能很好地满足其目标用户的需求。此外，追求这一未经检验的主张可能会低估安全等关键质量属性。", "conclusion": "研究建议，推进主权LLMs的发展需要更广泛的评估，纳入更全面、更实用且有充分依据的标准。"}}
{"id": "2510.14896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14896", "abs": "https://arxiv.org/abs/2510.14896", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection", "comment": null, "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.", "AI": {"tldr": "该论文提出了一种基于多模态大型语言模型（MLLM）的新型半监督视频异常检测（VAD）框架，通过提取和解释对象活动及交互的文本描述来检测复杂异常，并提供可解释性。", "motivation": "现有的半监督视频异常检测（VAD）方法在检测涉及对象交互的复杂异常方面表现不佳，并且普遍缺乏可解释性。", "method": "该方法通过向MLLM查询不同时刻的对象对视觉输入，生成标称视频中对象活动和交互的文本描述。这些文本描述作为高层次表示，在测试时通过与标称训练视频中的文本描述进行比较来检测异常。该方法可与传统VAD方法结合以增强可解释性。", "result": "该方法不仅能有效检测基于复杂交互的异常，而且在没有交互异常的数据集上也达到了最先进的性能，并提供了固有的可解释性。", "conclusion": "所提出的基于MLLM的VAD框架通过关注对象活动和交互的文本解释，成功克服了现有方法在检测复杂异常和提供可解释性方面的局限性，并在多个基准数据集上取得了优异性能。"}}
{"id": "2510.14955", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14955", "abs": "https://arxiv.org/abs/2510.14955", "authors": ["Guo Cheng", "Danni Yang", "Ziqi Huang", "Jianlou Si", "Chenyang Si", "Ziwei Liu"], "title": "RealDPO: Real or Not Real, that is the Preference", "comment": "Code:https://github.com/Vchitect/RealDPO Project\n  Page:https://vchitect.github.io/RealDPO-Project/", "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.", "AI": {"tldr": "本文提出RealDPO，一种利用真实世界数据作为偏好学习正样本的新型对齐范式，结合DPO和定制损失函数，以解决视频生成模型在复杂运动合成方面的挑战，显著提升了视频质量、文本对齐和运动真实感。", "motivation": "视频生成模型在合成质量上有所进步，但仍难以生成自然、流畅且上下文一致的复杂运动，这限制了其实际应用。现有模型缺乏有效的纠正反馈机制。", "method": "引入RealDPO，一种新的对齐范式，利用真实世界数据作为偏好学习的正样本。它采用直接偏好优化（DPO）并结合定制的损失函数来增强运动真实感。通过对比真实世界视频与模型错误输出，RealDPO实现了迭代自我修正。此外，为支持复杂运动合成的后期训练，提出了高质量人类日常活动视频数据集RealAction-5K。", "result": "实验证明，与最先进的模型和现有偏好优化技术相比，RealDPO显著改善了视频质量、文本对齐和运动真实感。", "conclusion": "RealDPO通过利用真实世界数据和DPO方法，有效解决了视频生成模型在复杂运动合成方面的关键挑战，显著提升了生成视频的真实感和整体质量。"}}
{"id": "2510.14943", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14943", "abs": "https://arxiv.org/abs/2510.14943", "authors": ["Wenkai Yang", "Weijie Liu", "Ruobing Xie", "Yiju Guo", "Lulu Wu", "Saiyong Yang", "Yankai Lin"], "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding", "comment": "Work in progress. Github repo: https://github.com/RUCBM/LaSeR", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.", "AI": {"tldr": "本文提出LaSeR算法，通过理论推导发现LLM自我验证的RL目标可简化为“末尾词元自奖励分数”，并以此分数在RLVR训练中与验证器奖励对齐，从而高效地联合优化LLM的推理和自我验证能力，显著提升推理性能和推理效率。", "motivation": "强化学习与可验证奖励（RLVR）是提升大型语言模型（LLM）推理能力的核心范式。然而，测试时缺乏验证信号，现有方法通过训练模型的自我验证能力来解决，但这需要LLM使用两个独立的提示模板顺序生成解决方案和自我验证，导致效率显著降低。", "method": "本文理论揭示了自我验证RL目标的闭式解可以简化为“末尾词元自奖励分数”，该分数等于策略模型在解决方案末尾词元对预设词元的下一个词元对数概率与一个预计算常数之差，再乘以KL系数。基于此洞察，本文提出了LaSeR算法，该算法通过一个均方误差（MSE）损失来增强原始RLVR损失，使末尾词元自奖励分数与基于验证器的推理奖励对齐，从而联合优化LLM的推理和自奖励能力。这些分数在生成后立即从末尾词元的下一个词元概率分布中推导，仅增加极小的额外推理成本。", "result": "实验表明，LaSeR方法不仅提高了模型的推理性能，还赋予其卓越的自奖励能力，从而提升了其推理时间的扩展性能。", "conclusion": "LaSeR算法通过引入基于末尾词元自奖励分数的优化方法，高效地将LLM的推理和自我验证能力统一起来。该方法在训练和测试中都能利用优化后的自奖励分数，显著提升了模型性能和推理效率。"}}
{"id": "2510.14640", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14640", "abs": "https://arxiv.org/abs/2510.14640", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "Intent Clustering with Shared Pseudo-Labels", "comment": null, "summary": "In this paper, we propose an intuitive, training-free and label-free method\nfor intent clustering that makes minimal assumptions using lightweight and\nopen-source LLMs. Many current approaches rely on commercial LLMs, which are\ncostly, and offer limited transparency. Additionally, their methods often\nexplicitly depend on knowing the number of clusters in advance, which is often\nnot the case in realistic settings. To address these challenges, instead of\nasking the LLM to match similar text directly, we first ask it to generate\npseudo-labels for each text, and then perform multi-label classification in\nthis pseudo-label set for each text. This approach is based on the hypothesis\nthat texts belonging to the same cluster will share more labels, and will\ntherefore be closer when encoded into embeddings. These pseudo-labels are more\nhuman-readable than direct similarity matches. Our evaluation on four benchmark\nsets shows that our approach achieves results comparable to and better than\nrecent baselines, while remaining simple and computationally efficient. Our\nfindings indicate that our method can be applied in low-resource scenarios and\nis stable across multiple models and datasets.", "AI": {"tldr": "本文提出了一种直观、免训练、免标签的意图聚类方法，利用轻量级开源LLM生成伪标签进行多标签分类。该方法解决了商业LLM成本高、透明度低以及需要预知聚类数量的挑战，在多个基准测试中表现出与现有基线相当甚至更好的性能，同时保持简单、高效和稳定性，适用于低资源场景。", "motivation": "当前许多意图聚类方法依赖于昂贵且透明度有限的商业大型语言模型（LLM），并且通常需要预先知道聚类的数量，这在实际应用中往往是不现实的。", "method": "该方法不直接让LLM匹配相似文本，而是首先让LLM为每个文本生成伪标签，然后在此伪标签集上对每个文本进行多标签分类。其核心假设是同一簇的文本将共享更多标签，从而在嵌入编码后彼此更接近。生成的伪标签比直接的相似性匹配更具可读性。", "result": "在四个基准数据集上的评估表明，该方法取得了与最新基线相当甚至更好的结果，同时保持了方法的简单性和计算效率。", "conclusion": "该研究表明所提出的方法可以应用于低资源场景，并且在多个模型和数据集上都表现出良好的稳定性。"}}
{"id": "2510.14662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14662", "abs": "https://arxiv.org/abs/2510.14662", "authors": ["Xinyue Ma", "Pol Pastells", "Mireia Farrús", "Mariona Taulé"], "title": "Semantic Prosody in Machine Translation: the English-Chinese Case of Passive Structures", "comment": "11 pages, 2 figures, *SEM workshop at EMNLP 2025 conference", "summary": "Semantic prosody is a collocational meaning formed through the co-occurrence\nof a linguistic unit and a consistent series of collocates, which should be\ntreated separately from semantic meaning. Since words that are literal\ntranslations of each other may have different semantic prosody, more attention\nshould be paid to this linguistic property to generate accurate translations.\nHowever, current machine translation models cannot handle this problem. To\nbridge the gap, we propose an approach to teach machine translation models\nabout semantic prosody of a specific structure. We focus on Chinese BEI\npassives and create a dataset of English-Chinese sentence pairs with the\npurpose of demonstrating the negative semantic prosody of BEI passives. Then we\nfine-tune OPUS-MT, NLLB-600M and mBART50 models with our dataset for the\nEnglish-Chinese translation task. Our results show that fine-tuned MT models\nperform better on using BEI passives for translating unfavourable content and\navoid using it for neutral and favourable content. Also, in NLLB-600M, which is\na multilingual model, this knowledge of semantic prosody can be transferred\nfrom English-Chinese translation to other language pairs, such as\nSpanish-Chinese.", "AI": {"tldr": "本研究旨在解决机器翻译模型在处理语义韵律方面的不足，特别是中文“被”字句的负面语义韵律。通过构建特定数据集并微调主流MT模型，成功提升了模型在翻译负面内容时使用“被”字句，并避免在积极或中性内容中使用，且知识具有跨语言迁移性。", "motivation": "语义韵律是词语共现形成的搭配意义，与字面意义不同。字面翻译准确的词语可能具有不同的语义韵律，导致翻译不准确。现有机器翻译模型无法处理这一问题，尤其在中文“被”字句等具有特定语义韵律的结构上存在欠缺。", "method": "本研究聚焦于中文“被”字句，构建了一个包含英汉句对的数据集，旨在展示“被”字句的负面语义韵律。随后，使用该数据集对OPUS-MT、NLLB-600M和mBART50模型进行微调，以改进其英汉翻译任务中的语义韵律处理能力。", "result": "微调后的机器翻译模型在翻译不利内容时能更好地使用“被”字句，并能避免在翻译中性或有利内容时使用“被”字句。此外，在多语言模型NLLB-600M中，习得的语义韵律知识可以从英汉翻译迁移到其他语言对，如西汉翻译。", "conclusion": "通过专门的数据集进行微调，可以有效地教会机器翻译模型处理特定结构的语义韵律（如中文“被”字句的负面韵律），从而提高翻译的准确性。这种知识在某些多语言模型中还展现出跨语言迁移的能力。"}}
{"id": "2510.14960", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14960", "abs": "https://arxiv.org/abs/2510.14960", "authors": ["Shizun Wang", "Zhenxiang Jiang", "Xingyi Yang", "Xinchao Wang"], "title": "C4D: 4D Made from 3D through Dual Correspondences", "comment": "Accepted to ICCV 2025", "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry\nand camera poses, is an inevitably challenging problem. While recent\npointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great\nprogress in reconstructing static scenes, directly applying them to dynamic\nscenes leads to inaccurate results. This discrepancy arises because moving\nobjects violate multi-view geometric constraints, disrupting the\nreconstruction. To address this, we introduce C4D, a framework that leverages\ntemporal Correspondences to extend existing 3D reconstruction formulation to\n4D. Specifically, apart from predicting pointmaps, C4D captures two types of\ncorrespondences: short-term optical flow and long-term point tracking. We train\na dynamic-aware point tracker that provides additional mobility information,\nfacilitating the estimation of motion masks to separate moving elements from\nthe static background, thus offering more reliable guidance for dynamic scenes.\nFurthermore, we introduce a set of dynamic scene optimization objectives to\nrecover per-frame 3D geometry and camera parameters. Simultaneously, the\ncorrespondences lift 2D trajectories into smooth 3D trajectories, enabling\nfully integrated 4D reconstruction. Experiments show that our framework\nachieves complete 4D recovery and demonstrates strong performance across\nmultiple downstream tasks, including depth estimation, camera pose estimation,\nand point tracking. Project Page: https://littlepure2333.github.io/C4D", "AI": {"tldr": "C4D框架通过利用时间对应（光流和动态感知点跟踪）和动态场景优化，将现有基于点图的3D重建方法扩展到4D，从而从单目视频中恢复动态几何和相机姿态。", "motivation": "现有的基于点图的3D重建方法在静态场景中表现良好，但直接应用于动态场景时，由于移动物体违反多视图几何约束，导致重建不准确。", "method": "引入C4D框架，该框架除了预测点图外，还捕获两种时间对应：短期光流和长期点跟踪。训练了一个动态感知点跟踪器，提供额外的移动信息以估计运动掩码，从而分离移动元素和静态背景。此外，引入了一套动态场景优化目标来恢复每帧的3D几何和相机参数。同时，对应关系将2D轨迹提升为平滑的3D轨迹，实现完全集成的4D重建。", "result": "该框架实现了完整的4D恢复，并在深度估计、相机姿态估计和点跟踪等多个下游任务中表现出强大的性能。", "conclusion": "C4D通过整合时间对应和动态场景优化，成功地将3D重建扩展到4D，解决了从单目视频中恢复动态几何和相机姿态的挑战，实现了鲁棒的4D重建。"}}
{"id": "2510.14885", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14885", "abs": "https://arxiv.org/abs/2510.14885", "authors": ["Logan Lawrence", "Oindrila Saha", "Megan Wei", "Chen Sun", "Subhransu Maji", "Grant Van Horn"], "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction", "comment": "Accepted to WACV26. 12 pages, 8 tables, 5 figures", "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.", "AI": {"tldr": "针对多模态大语言模型（MLLM）在细粒度视觉分类（FGVC）中自由形式响应评估的挑战，本文提出nlg2choice，一个两阶段方法：先开放式提问，再约束解码预测。该方法在多选和检索场景下表现优异，并在七个细粒度数据集上取得显著提升。", "motivation": "尽管MLLM在零样本视觉分类中引起了广泛关注，但评估自回归模型自由形式响应的问题依然存在。现有工作主要关注纯语言任务或少于5个选项的多项选择题（MCQ），这无法满足FGVC中数百到数千个高度相关选项的需求。此外，在高度多选的MCQ设置下，如何将LLM的选择提取扩展到检索问题，且计算选择集概率成本高昂，尚不明确。", "method": "本文提出nlg2choice，一个简单的两阶段方法：1. 向MLLM提出开放式问题，限制最少。2. 使用纯文本约束解码来预测最可能的选项。在检索设置中，采用早停法计算约束响应选择某个选项的概率，以显著提高吞吐量。", "result": "在七个细粒度视觉数据集上，分类和检索评估方面均显示出改进。该性能在LLM用户以不同自然语言方式实现任务时依然保持。", "conclusion": "nlg2choice方法有效解决了MLLM在细粒度视觉分类中自由响应评估的挑战，特别是在多选和检索场景下，提供了一种计算高效且鲁棒的解决方案。"}}
{"id": "2510.14718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14718", "abs": "https://arxiv.org/abs/2510.14718", "authors": ["Xingmeng Zhao", "Dan Schumacher", "Veronica Rammouz", "Anthony Rios"], "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms", "comment": "8 pages main + Appendix", "summary": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling\nfast development of tools like stress monitors, wellness trackers, and mental\nhealth chatbots. However, rapid and low-barrier development can introduce risks\nof bias, privacy violations, and unequal access, especially when systems ignore\nreal-world contexts and diverse user needs. Many recent methods use AI to\ndetect risks automatically, but this can reduce human engagement in\nunderstanding how harms arise and who they affect. We present a human-centered\nframework that generates user stories and supports multi-agent discussions to\nhelp people think creatively about potential benefits and harms before\ndeployment. In a user study, participants who read stories recognized a broader\nrange of harms, distributing their responses more evenly across all 13 harm\ntypes. In contrast, those who did not read stories focused primarily on privacy\nand well-being (58.3%). Our findings show that storytelling helped participants\nspeculate about a broader range of harms and benefits and think more creatively\nabout AI's impact on users.", "AI": {"tldr": "AI在医疗领域快速发展，但存在偏见和隐私等风险。本文提出以人为中心的框架，通过用户故事和多智能体讨论帮助人们在部署前识别潜在危害。用户研究表明，故事能帮助参与者识别更广泛的危害。", "motivation": "人工智能（AI）在医疗保健领域迅速发展，但快速、低门槛的开发可能引入偏见、隐私侵犯和不平等待遇等风险，尤其当系统忽视真实世界背景和多样化用户需求时。现有的自动化风险检测方法可能减少人类对危害产生机制的理解和关注。", "method": "本文提出一个以人为中心的框架，该框架通过生成用户故事并支持多智能体讨论，旨在帮助人们在部署AI系统之前，创造性地思考潜在的益处和危害。", "result": "在一项用户研究中，阅读故事的参与者识别出更广泛的危害类型，并将他们的回应更均匀地分布在所有13种危害类型中。相比之下，未阅读故事的参与者主要关注隐私和福祉（占58.3%）。", "conclusion": "研究结果表明，讲故事有助于参与者推测更广泛的危害和益处，并更具创造性地思考AI对用户的影响。"}}
{"id": "2510.14954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14954", "abs": "https://arxiv.org/abs/2510.14954", "authors": ["Zhe Li", "Weihao Yuan", "Weichao Shen", "Siyu Zhu", "Zilong Dong", "Chang Xu"], "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression", "comment": null, "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.", "AI": {"tldr": "本文提出了一种连续掩码自回归运动Transformer（C-MARMT）框架，用于全身多模态人体运动生成，结合DiT结构和AdaLN/交叉注意力进行多模态融合，在文本、语音和音乐到运动任务上均优于现有方法。", "motivation": "全身多模态人体运动生成面临两大挑战：一是创建有效的运动生成机制，二是将文本、语音和音乐等多种模态整合到统一的框架中。现有方法通常采用离散掩码建模或自回归建模，可能存在局限性。", "method": "本文开发了一种连续掩码自回归运动Transformer（C-MARMT），它考虑人体运动的序列性质进行因果注意力。在该Transformer中，引入了门控线性注意力（关注关键动作）和RMSNorm模块（抑制异常运动或多模态异质分布引起的不稳定性）。为了增强运动生成和多模态泛化能力，采用了DiT结构将Transformer的条件扩散到目标。通过AdaLN和交叉注意力机制融合文本、语音和音乐信号。", "result": "实验结果表明，该框架在所有模态上均优于现有方法，包括文本到运动、语音到手势和音乐到舞蹈的生成任务。", "conclusion": "所提出的框架成功解决了全身多模态人体运动生成中的关键挑战，通过创新的C-MARMT结构、DiT扩散机制和高效的多模态融合策略，实现了卓越的性能表现。"}}
{"id": "2510.14945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14945", "abs": "https://arxiv.org/abs/2510.14945", "authors": ["JoungBin Lee", "Jaewoo Jung", "Jisang Han", "Takuya Narihira", "Kazumi Fukuda", "Junyoung Seo", "Sunghwan Hong", "Yuki Mitsufuji", "Seungryong Kim"], "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation", "comment": "Project page : https://cvlab-kaist.github.io/3DScenePrompt/", "summary": "We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/", "AI": {"tldr": "3DScenePrompt是一个视频生成框架，能从任意长度输入生成下一视频块，同时实现精确的相机控制和场景一致性，通过双时空条件和3D静态场景记忆解决现有方法的局限。", "motivation": "现有方法在单图像或短视频片段条件下生成时，难以保持长程场景一致性、实现精确的相机控制，并且在生成超出时间边界时无法正确处理动态元素。", "method": "该方法采用双时空条件（时间上相邻帧用于运动连续性，空间上相邻内容用于场景一致性），并引入3D场景记忆来表示从整个输入视频中提取的静态几何。通过动态SLAM和新颖的动态遮罩策略构建该记忆，将静态场景几何与移动元素分离。静态场景表示可投影到任意目标视角，提供几何一致的3D空间提示，同时允许动态区域自然演变。", "result": "实验表明，该框架在场景一致性、相机可控性和生成质量方面显著优于现有方法，且不牺牲计算效率或运动真实感。", "conclusion": "3DScenePrompt通过其独特的双时空条件和3D静态场景记忆机制，成功实现了从任意长度输入生成具有高场景一致性、精确相机控制和运动真实感的视频块，为长视频生成领域提供了新的解决方案。"}}
{"id": "2510.14738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14738", "abs": "https://arxiv.org/abs/2510.14738", "authors": ["Mengzhao Jia", "Zhihan Zhang", "Ignacio Cases", "Zheyuan Liu", "Meng Jiang", "Peng Qi"], "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) have rapidly advanced from\nperception tasks to complex multi-step reasoning, yet reinforcement learning\nwith verifiable rewards (RLVR) often leads to spurious reasoning since only the\nfinal-answer correctness is rewarded. To address this limitation, we propose\nAutoRubric-R1V, a framework that integrates RLVR with process-level supervision\nthrough automatically collected rubric-based generative rewards. Our key\ninnovation lies in a scalable self-aggregation method that distills consistent\nreasoning checkpoints from successful trajectories, enabling problem-specific\nrubric construction without human annotation or stronger teacher models. By\njointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves\nstate-of-the-art performance on six multimodal reasoning benchmarks and\nsubstantially improves reasoning faithfulness in dedicated evaluations.", "AI": {"tldr": "针对多模态大语言模型（MLLMs）在可验证奖励强化学习（RLVR）中存在的虚假推理问题，本文提出了AutoRubric-R1V框架，通过自动收集基于评分标准的生成式奖励，整合RLVR与过程级监督，显著提升了推理性能和忠实性。", "motivation": "多模态大语言模型（MLLMs）在可验证奖励强化学习（RLVR）中常产生虚假推理，因为只奖励最终答案的正确性，缺乏对推理过程的监督。", "method": "本文提出了AutoRubric-R1V框架，该框架将RLVR与过程级监督相结合，通过自动收集基于评分标准的生成式奖励。其核心创新在于一种可扩展的自聚合方法，该方法从成功的轨迹中提炼出一致的推理检查点，从而无需人工标注或更强的教师模型即可构建特定问题的评分标准。该方法联合利用了基于评分标准和结果的奖励。", "result": "AutoRubric-R1V在六个多模态推理基准测试中取得了最先进的性能，并在专门评估中显著提高了推理的忠实性。", "conclusion": "AutoRubric-R1V通过整合自动生成的基于评分标准的奖励和过程级监督，有效解决了MLLMs在RLVR中虚假推理的局限性，实现了卓越的性能和更高的推理忠实性。"}}
{"id": "2510.14620", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14620", "abs": "https://arxiv.org/abs/2510.14620", "authors": ["Kedi Chen", "Zhikai Lei", "Xu Guo", "Xuecheng Wu", "Siyuan Zeng", "Jianghao Yin", "Yinqi Zhang", "Qin Chen", "Jie Zhou", "Liang He", "Qipeng Guo", "Kai Chen", "Wei Zhang"], "title": "Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models", "comment": null, "summary": "Large language models (LLMs) make remarkable progress in reasoning tasks.\nAmong different reasoning modes, inductive reasoning, due to its better\nalignment with human learning, attracts increasing interest. However, research\non inductive reasoning faces certain challenges. First, existing inductive data\nmostly focuses on superficial regularities while lacking more complex internal\npatterns. Second, current works merely prompt LLMs or finetune on simple\nprompt-response pairs, but do not provide precise thinking processes nor\nimplement difficulty control. Unlike previous work, we address these challenges\nby introducing \\textit{CodeSeq}, a synthetic post-training dataset built from\nnumber sequences. We package number sequences into algorithmic problems to\ndiscover their general terms, defining a general term generation (GTG) task\ncorrespondingly. Our pipeline generates supervised finetuning data by\nreflecting on failed test cases and incorporating iterative corrections,\nthereby teaching LLMs to learn autonomous case generation and self-checking.\nAdditionally, it leverages reinforcement learning with a novel Case-Synergy\nSolvability Scaling Reward based on both solvability, estimated from the\nproblem pass rate, and the success rate of self-directed case generation,\nenabling models to learn more effectively from both successes and failures.\nExperimental results show that the models trained with \\textit{CodeSeq} improve\non various reasoning tasks and can preserve the models' OOD performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14958", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14958", "abs": "https://arxiv.org/abs/2510.14958", "authors": ["Weikang Shi", "Aldrich Yu", "Rongyao Fang", "Houxing Ren", "Ke Wang", "Aojun Zhou", "Changyao Tian", "Xinyu Fu", "Yuxuan Hu", "Zimu Lu", "Linjiang Huang", "Si Liu", "Rui Liu", "Hongsheng Li"], "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning", "comment": "Project Page: https://mathcanvas.github.io/", "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/", "AI": {"tldr": "该研究引入了MathCanvas框架，旨在赋予大型多模态模型（LMMs）内在的视觉思维链（VCoT）能力，以解决LLMs在几何等视觉数学领域中的不足。通过大规模预训练和策略性微调，MathCanvas显著提升了LMMs在生成和利用视觉辅助解决复杂数学问题方面的表现。", "motivation": "大型语言模型（LLMs）在文本推理方面表现出色，但在本质上依赖视觉辅助的数学领域（如几何）中表现不佳。现有的视觉思维链（VCoT）方法常受限于僵硬的外部工具，或无法生成解决复杂问题所需的高保真度、策略性适时的图表。", "method": "该研究提出了MathCanvas框架，旨在赋予统一的大型多模态模型（LMMs）内在的视觉思维链（VCoT）能力。方法分两阶段：1. 视觉操作阶段：在包含10M图说-图表对（MathCanvas-Imagen）和5.2M逐步编辑轨迹（MathCanvas-Edit）的15.2M对语料库上进行预训练，以掌握图表生成和编辑。2. 策略性视觉辅助推理阶段：在包含219K交错视觉-文本推理路径示例的新数据集MathCanvas-Instruct上对模型进行微调，以学习何时以及如何利用视觉辅助。为严格评估，引入了MathCanvas-Bench基准，包含3K需要交错视觉-文本解决方案的问题。", "result": "基于MathCanvas框架训练的模型BAGEL-Canvas在MathCanvas-Bench上比强大的LMM基线取得了86%的相对改进，并对其他公共数学基准展现出卓越的泛化能力。", "conclusion": "该工作提供了一个完整的工具包——包括框架、数据集和基准——旨在解锁大型多模态模型（LMMs）中复杂、类人的视觉辅助推理能力。"}}
{"id": "2510.14763", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14763", "abs": "https://arxiv.org/abs/2510.14763", "authors": ["Yunwen Li", "Shuangshuang Ying", "Xingwei Qu", "Xin Li", "Sheng Jin", "Minghao Liu", "Zhoufutu Wen", "Tianyu Zheng", "Xeron Du", "Qiguang Chen", "Jiajun Shi", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Libo Qin", "Stephen Huang", "Wanxiang Che", "Chenghua Lin", "Eli Zhang"], "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes", "comment": null, "summary": "Large language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We present COIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs, COIG-Writer comprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a\nreverse-engineered prompt, (2) detailed creative reasoning documenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided by process supervision) and linguistic expression (maintained\nby general-purpose data). Our findings reveal three critical insights: (1)\nProcess supervision is highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with no cross-lingual transfer (89.26pp gap between\nChinese and English performance), and (3) lexical diversity inversely\ncorrelates with creative quality (TTR paradox), suggesting high diversity\nsignals compensatory behavior for logical deficiencies. These findings\nestablish that creative excellence emerges from the interaction between logical\nscaffolding and linguistic grounding, analogous to how mathematical reasoning\nenhances but cannot replace linguistic competence in foundation models.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14969", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14969", "abs": "https://arxiv.org/abs/2510.14969", "authors": ["Yiming Wang", "Da Yin", "Yuedong Cui", "Ruichen Zheng", "Zhiqian Li", "Zongyu Lin", "Di Wu", "Xueqing Wu", "Chenchen Ye", "Yu Zhou", "Kai-Wei Chang"], "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training", "comment": "Preprint. Project page:\n  https://ui-simulator.notion.site/llms-as-scalable-digital-world-simulator;\n  Code and data: https://github.com/WadeYin9712/UI-Simulator", "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.", "AI": {"tldr": "本文提出UI-Simulator，一个可扩展的范式，通过生成结构化UI状态和转换来合成大规模训练轨迹，以解决数字代理训练中真实UI数据收集成本高昂的问题。", "motivation": "数字代理需要多样化、大规模的UI轨迹才能在真实世界任务中泛化，但收集此类数据在人工标注、基础设施和工程方面成本过高。", "method": "UI-Simulator范式集成了数字世界模拟器以生成多样化的UI状态、引导式探索过程以实现连贯探索，以及轨迹封装器以生成高质量和多样化的训练轨迹。此外，还提出了UI-Simulator-Grow，一种通过优先处理高影响力任务和合成信息丰富的轨迹变体来实现更快速、数据高效扩展的策略。", "result": "在WebArena和AndroidWorld上的实验表明，UI-Simulator在鲁棒性方面与使用真实UI训练的开源代理相当或超越，尽管使用了较弱的教师模型。UI-Simulator-Grow仅使用Llama-3-8B-Instruct作为基础模型，就达到了Llama-3-70B-Instruct的性能。", "conclusion": "UI-Simulator通过大规模合成UI轨迹，显著提高了数字代理的训练效率和性能。特别是UI-Simulator-Grow的定向合成扩展范式，展示了持续高效增强数字代理的巨大潜力。"}}
{"id": "2510.14962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14962", "abs": "https://arxiv.org/abs/2510.14962", "authors": ["Thao Nguyen", "Jiaqi Ma", "Fahad Shahbaz Khan", "Souhaib Ben Taieb", "Salman Khan"], "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion", "comment": null, "summary": "Precipitation nowcasting, predicting future radar echo sequences from current\nobservations, is a critical yet challenging task due to the inherently chaotic\nand tightly coupled spatio-temporal dynamics of the atmosphere. While recent\nadvances in diffusion-based models attempt to capture both large-scale motion\nand fine-grained stochastic variability, they often suffer from scalability\nissues: latent-space approaches require a separately trained autoencoder,\nadding complexity and limiting generalization, while pixel-space approaches are\ncomputationally intensive and often omit attention mechanisms, reducing their\nability to model long-range spatio-temporal dependencies. To address these\nlimitations, we propose a Token-wise Attention integrated into not only the\nU-Net diffusion model but also the spatio-temporal encoder that dynamically\ncaptures multi-scale spatial interactions and temporal evolution. Unlike prior\napproaches, our method natively integrates attention into the architecture\nwithout incurring the high resource cost typical of pixel-space diffusion,\nthereby eliminating the need for separate latent modules. Our extensive\nexperiments and visual evaluations across diverse datasets demonstrate that the\nproposed method significantly outperforms state-of-the-art approaches, yielding\nsuperior local fidelity, generalization, and robustness in complex\nprecipitation forecasting scenarios.", "AI": {"tldr": "本文提出了一种将Token-wise注意力机制原生集成到U-Net扩散模型和时空编码器中的降水临近预报方法，解决了现有扩散模型的可扩展性问题，并在复杂场景中实现了卓越的性能。", "motivation": "降水临近预报因其固有的混沌和紧密耦合的时空动态而极具挑战性。现有的基于扩散模型的方法存在可扩展性问题：潜在空间方法需要单独训练自编码器，增加了复杂性并限制了泛化能力；像素空间方法计算密集，且常省略注意力机制，降低了建模长程时空依赖的能力。", "method": "本文提出将Token-wise注意力机制集成到U-Net扩散模型和时空编码器中。这种方法动态捕捉多尺度空间交互和时间演变，并原生集成注意力机制到架构中，避免了像素空间扩散模型的高资源成本，也无需单独的潜在模块。", "result": "在多个数据集上的大量实验和视觉评估表明，所提出的方法显著优于现有最先进的方法，在复杂的降水预报场景中展现出卓越的局部保真度、泛化能力和鲁棒性。", "conclusion": "本文提出的集成Token-wise注意力机制的扩散模型有效解决了现有降水临近预报扩散模型的可扩展性限制，并在性能上取得了显著提升，为复杂降水预报提供了更可靠的解决方案。"}}
{"id": "2510.14824", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14824", "abs": "https://arxiv.org/abs/2510.14824", "authors": ["Ziqi Dai", "Xin Zhang", "Mingxin Li", "Yanzhao Zhang", "Dingkun Long", "Pengjun Xie", "Meishan Zhang", "Wenjie Li", "Min Zhang"], "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking", "comment": null, "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.", "AI": {"tldr": "本研究比较了对比学习（CL）和监督微调（SFT）在大型语言模型（LLM）重排序中的表现，发现SFT因其更强的权重方案而显著优于CL，并取得了最先进的重排序结果。", "motivation": "在信息检索领域，BERT风格编码器通常通过对比学习（CL）进行训练，而大型语言模型（LLM）则倾向于使用监督微调（SFT）进行重排序，这与LLM的生成特性更吻合。这种分歧引出了一个核心问题：哪种目标函数更适合基于LLM的重排序，以及造成差异的机制是什么？", "method": "本研究在通用多模态检索（UMR）框架下，对CL和SFT在重排序中的应用进行了全面的比较和分析。研究将目标函数分解为权重（控制更新幅度）和方向（指导模型更新）两个组成部分，并提出了一个统一的框架来理解它们的相互作用。通过探究性实验，对比了两种方法的权重和方向。此外，还进行了SFT的大规模训练和消融实验。", "result": "探究性实验发现，SFT提供了比CL明显更强的权重方案，而偏好的评分方向则没有明确的优胜者。这些结果共同表明SFT在LLM重排序方面比CL具有持续的优势。通过SFT进行大规模训练，在MRB基准测试上取得了新的最先进的重排序器。", "conclusion": "对于LLM重排序，监督微调（SFT）始终优于对比学习（CL），这主要是因为SFT提供了更强大的权重方案。本研究的发现有望为该领域的未来研究和应用提供有益的指导。"}}
{"id": "2510.14965", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14965", "abs": "https://arxiv.org/abs/2510.14965", "authors": ["Miao Hu", "Zhiwei Huang", "Tai Wang", "Jiangmiao Pang", "Dahua Lin", "Nanning Zheng", "Runsen Xu"], "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes", "comment": "30 pages", "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ .", "AI": {"tldr": "本文提出了ChangingGrounding，首个用于在变化场景中进行3D视觉定位（3DVG）的基准，强调利用记忆和主动探索，以克服现有方法对静态点云的假设。同时，提出了一种零样本方法Mem-ChangingGrounder，该方法结合了跨模态检索和轻量级多视图融合，在降低探索成本的同时提高了定位精度。", "motivation": "现有3D视觉定位（3DVG）方法假设场景是重建且最新的点云，这导致了昂贵的重复扫描并阻碍了实际部署。然而，现实世界的机器人需要在不断变化的场景中根据自然语言指令定位物体。因此，需要一种能够利用历史观察、按需探索并适应场景变化的3DVG方法。", "method": "1. 提出了ChangingGrounding基准，明确衡量智能体在变化场景中利用过去观察、按需探索并提供精确3D边界框的能力。2. 提出了Mem-ChangingGrounder，一种零样本方法，其步骤包括：识别查询隐含的物体类型，检索相关记忆以指导行动，在场景中高效探索目标，在操作无效时回退，对目标进行多视图扫描，并将多视图扫描的融合证据投射以获得精确的物体边界框。", "result": "Mem-ChangingGrounder在ChangingGrounding基准上取得了最高的定位精度，同时大大降低了探索成本。这表明该方法能够有效地在变化场景中进行3DVG。", "conclusion": "ChangingGrounding基准和Mem-ChangingGrounder方法有望推动3DVG研究向更实用、以记忆为中心的方向发展，以适应现实世界的应用需求。"}}
{"id": "2510.14975", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14975", "abs": "https://arxiv.org/abs/2510.14975", "authors": ["Hengyuan Xu", "Wei Cheng", "Peng Xing", "Yixiao Fang", "Shuhan Wu", "Rui Wang", "Xianfang Zeng", "Daxin Jiang", "Gang Yu", "Xingjun Ma", "Yu-Gang Jiang"], "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation", "comment": "23 Pages; Project Page: https://doby-xu.github.io/WithAnyone/; Code:\n  https://github.com/Doby-Xu/WithAnyone", "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.", "AI": {"tldr": "该研究通过构建大规模配对数据集、引入新基准和提出对比身份损失训练范式，解决了文本到图像生成中身份一致性不足和“复制粘贴”伪影的问题，并推出了WithAnyone模型。", "motivation": "文本到图像生成中，身份一致性生成面临挑战，现有模型因缺乏大规模配对数据集而依赖基于重建的训练，导致模型直接复制参考人脸（“复制粘贴”伪影），而非在姿态、表情、光照变化下保持身份一致性，这限制了生成的可控性和表达能力。", "method": "1. 构建了大规模多人物配对数据集MultiID-2M，为每个身份提供多样化参考。\n2. 引入了一个基准，用于量化“复制粘贴”伪影以及身份保真度与多样性之间的权衡。\n3. 提出了一种新颖的训练范式，采用对比身份损失，利用配对数据平衡保真度与多样性。\n4. 基于扩散模型开发了WithAnyone模型。", "result": "WithAnyone模型有效缓解了“复制粘贴”伪影，同时保持了高身份相似性。实验表明，它显著减少了伪影，提高了姿态和表情的可控性，并保持了强大的感知质量。用户研究进一步验证了其在实现高身份保真度的同时，能够进行富有表现力的可控生成。", "conclusion": "该研究通过多项创新（数据集、基准、训练范式和WithAnyone模型）成功解决了身份一致性文本到图像生成中的“复制粘贴”问题，实现了在保持高身份保真度的同时，生成更具可控性和多样性的图像。"}}
{"id": "2510.14398", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14398", "abs": "https://arxiv.org/abs/2510.14398", "authors": ["Shiyao Ding", "Takayuki Ito"], "title": "Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation", "comment": null, "summary": "Large language models (LLMs) excel at general next-token prediction but still\nstruggle to generate responses that reflect how individuals truly communicate,\nsuch as replying to emails or social messages in their own style. However, real\nSNS or email histories are difficult to collect due to privacy concerns. To\naddress this, we propose the task of \"Your Next Token Prediction (YNTP)\", which\nmodels a user's precise word choices through controlled human-agent\nconversations. We build a multilingual benchmark of 100 dialogue sessions\nacross English, Japanese, and Chinese, where users interact for five days with\npsychologically grounded NPCs based on MBTI dimensions. This setup captures\nnatural, daily-life communication patterns and enables analysis of users'\ninternal models. We evaluate prompt-based and fine-tuning-based personalization\nmethods, establishing the first benchmark for YNTP and a foundation for\nuser-aligned language modeling. The dataset is available at:\nhttps://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14973", "abs": "https://arxiv.org/abs/2510.14973", "authors": ["Quan Nguyen-Tri", "Mukul Ranjan", "Zhiqiang Shen"], "title": "Attention Is All You Need for KV Cache in Diffusion LLMs", "comment": "https://vila-lab.github.io/elastic-cache-webpage/", "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.", "AI": {"tldr": "本文提出了一种名为Elastic-Cache的自适应策略，用于扩散大语言模型（DLMs）的关键值（KV）缓存重计算，旨在最大化预测精度并最小化解码延迟，通过选择性地刷新缓存来减少冗余计算。", "motivation": "现有方法在每个去噪步骤和层级都为所有token重新计算QKV，即使KV状态变化不大，尤其是在浅层，这导致了大量的冗余计算，增加了解码延迟。", "method": "作者基于三项观察：1) 远距离的MASK token可分块缓存；2) KV动态性随深度增加，提示可从深层开始选择性刷新；3) 最受关注的token的KV漂移最小。在此基础上，提出了Elastic-Cache策略：1) 通过对最受关注token进行注意力感知漂移测试来决定何时刷新；2) 通过深度感知调度来决定何处刷新，从选定层开始重新计算，同时重用浅层缓存和窗口外MASK缓存。该策略是免训练且与架构无关的。", "result": "实验结果表明，Elastic-Cache在数学推理和代码生成任务上均实现了显著加速：GSM8K上加速8.7倍（256 tokens），长序列上加速45.1倍，HumanEval上加速4.8倍，并始终保持比基线更高的准确性。相比现有基于置信度的方法，该方法在GSM8K上实现了6.8倍的更高吞吐量，同时保持了生成质量。", "conclusion": "Elastic-Cache通过自适应、层感知的缓存更新，有效减少了冗余计算，显著加速了扩散大语言模型的解码过程，且几乎不损失生成质量，从而使其能够进行实际部署。"}}
{"id": "2510.14853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14853", "abs": "https://arxiv.org/abs/2510.14853", "authors": ["Guinan Su", "Yanwu Yang", "Li Shen", "Lu Yin", "Shiwei Liu", "Jonas Geiping"], "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models", "comment": null, "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose \\textit{a data-free, online test-time framework} that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite.", "AI": {"tldr": "本文提出了一种无需外部数据、在线的测试时自适应框架，用于动态优化MoE模型在文本生成过程中的路由决策，从而提升性能并增强对上下文变化的鲁棒性。", "motivation": "Mixture-of-Experts (MoE) 模型在部署时常因分布偏移导致路由决策不佳。现有测试时自适应方法主要针对密集模型且需要外部数据，不适用于MoE架构。作者发现可以仅基于输入上下文，在线优化MoE专家选择，无需参考数据。", "method": "该方法是一个数据无关、在线的测试时框架，在文本生成过程中持续调整MoE路由决策。它在两个阶段循环：在预填充阶段和后续的常规间隔中，利用已生成序列的自监督信息来优化模型的路由决策；然后，模型正常生成文本，保持修改后的路由器直到下一次适应。通过轻量级加性向量实现，仅更新选定层中的路由器logits，以保持计算效率并防止过度适应。", "result": "实验结果表明，该方法在具有挑战性的推理任务上实现了持续的性能提升，同时保持了对上下文偏移的鲁棒性。例如，在HumanEval任务上使用OLMoE模型取得了5.5%的改进。此外，由于其即插即用的特性，该方法能与现有测试时扩展技术互补，例如与DeepSeek-V2-Lite上的自洽性结合时，平均增益达到6%。", "conclusion": "本文提出的无需数据、在线的测试时框架能够有效且高效地适应MoE模型的路由决策，显著提升了模型在推理任务上的性能和对上下文变化的鲁棒性，并且可以轻松地与现有技术结合。"}}
{"id": "2510.14977", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14977", "abs": "https://arxiv.org/abs/2510.14977", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Xin Tao", "Pengfei Wan", "Jie Zhou", "Jiwen Lu"], "title": "Terra: Explorable Native 3D World Model with Point Latents", "comment": "Project Page: https://huang-yh.github.io/terra/", "summary": "World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.", "AI": {"tldr": "Terra是一种原生的3D世界模型，它在内在3D潜在空间中表示和生成可探索的环境，通过点到高斯变分自编码器（P2G-VAE）和稀疏点流匹配网络（SPFlow）实现高3D一致性和灵活渲染。", "motivation": "大多数现有世界模型依赖于像素对齐表示，忽略了物理世界固有的3D特性，这损害了3D一致性并降低了建模效率。", "method": "本文提出了Terra，一个原生3D世界模型。它引入了点到高斯变分自编码器（P2G-VAE），将3D输入编码为潜在点表示，然后解码为3D高斯基元以共同建模几何和外观。接着，引入了稀疏点流匹配网络（SPFlow）来生成潜在点表示，同时对点潜在的位置和特征进行去噪。Terra通过在点潜在空间中的渐进生成实现可探索的世界建模。", "result": "Terra通过原生3D表示和架构实现了精确的多视角一致性，支持从任何视点进行灵活渲染，仅需一次生成过程。它在ScanNet v2数据集上的重建和生成方面达到了最先进的性能，并具有高3D一致性。", "conclusion": "Terra成功解决了现有世界模型在3D一致性和效率方面的不足，通过其原生3D表示和生成方法，实现了卓越的3D环境建模能力和可探索性。"}}
{"id": "2510.14978", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14978", "abs": "https://arxiv.org/abs/2510.14978", "authors": ["Nupur Kumari", "Sheng-Yu Wang", "Nanxuan Zhao", "Yotam Nitzan", "Yuheng Li", "Krishna Kumar Singh", "Richard Zhang", "Eli Shechtman", "Jun-Yan Zhu", "Xun Huang"], "title": "Learning an Image Editing Model without Image Editing Pairs", "comment": "project page: https://nupurkmr9.github.io/npedit/", "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.", "AI": {"tldr": "本文提出了一种无需配对数据的新训练范式，通过展开扩散模型并利用视觉-语言模型（VLM）的反馈，实现了与监督学习方法媲美的图像编辑效果。", "motivation": "现有图像编辑模型依赖大规模配对数据集进行监督微调，但此类数据难以获取。使用合成配对数据可能导致预训练模型的伪影传播和放大，这是一个关键瓶颈。", "method": "该方法通过在训练过程中展开（unrolling）一个少步扩散模型来直接优化它，完全无需配对数据。它利用视觉-语言模型（VLM）提供反馈，评估编辑是否遵循指令并保留未改变内容，从而生成端到端优化的直接梯度。为确保视觉保真度，还引入了分布匹配损失（DMD），以约束生成图像保持在预训练模型学习到的图像流形内。", "result": "在少步设置下，该方法在标准基准测试中表现与使用大量监督配对数据训练的各种图像编辑扩散模型相当，且无需任何配对数据。在给定相同VLM作为奖励模型的情况下，该方法还优于Flow-GRPO等基于强化学习的技术。", "conclusion": "该研究提出了一种无需配对数据的图像编辑训练新范式，有效解决了数据收集的瓶颈问题，并在少步设置下取得了与监督学习方法相当甚至更优的性能。"}}
{"id": "2510.14865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14865", "abs": "https://arxiv.org/abs/2510.14865", "authors": ["Emmy Liu", "Graham Neubig", "Chenyan Xiong"], "title": "Midtraining Bridges Pretraining and Posttraining Distributions", "comment": null, "summary": "Recently, many language models have been pretrained with a \"midtraining\"\nphase, in which higher quality, often instruction-formatted data, is mixed in\nat the end of pretraining. Despite the popularity of this practice, there is\nlittle scientific understanding of this phase of model training or why it is\neffective. In this work, we conduct the first systematic investigation of\nmidtraining through controlled experiments with language models pretrained from\nscratch and fine-tuned on supervised finetuning datasets in different domains.\nWe find that when compared after supervised fine-tuning, the effectiveness of\nmidtraining is highest in the math and code domains, where midtraining can best\nreduce the syntactic gap between pretraining and posttraining data. In these\ncases, midtraining consistently outperforms continued pretraining in both\nin-domain validation loss as well as pretraining data forgetting after\nposttraining. We conduct ablations on the starting time of the midtraining\nphase and mixture weights of the midtraining data, using code midtraining as a\ncase study, and find that timing has a greater impact than mixture weights,\nwith earlier introduction of specialized data, yielding greater benefits\nin-domain as well as preserving general language modeling better. These\nfindings establish midtraining as a domain adaptation technique that compared\nto continued pretraining yields better performance through reduced forgetting.", "AI": {"tldr": "本研究首次系统性地探究了语言模型训练中的“中训”阶段。结果表明，中训在数学和代码领域最有效，因为它能减少预训练和后训练数据之间的句法差异，且通过减少遗忘，性能优于持续预训练。中训开始的时机比数据混合权重更重要。", "motivation": "尽管“中训”阶段（在预训练后期混合高质量、指令格式数据）在语言模型训练中很流行，但对其有效性及其背后的科学原理缺乏深入理解。", "method": "本研究通过受控实验对“中训”进行了首次系统性调查。实验对象是重新预训练的语言模型，并在不同领域的监督微调（SFT）数据集上进行微调。研究比较了中训与持续预训练的效果，并对中训阶段的起始时间及数据混合权重进行了消融实验，以代码中训为例进行案例研究。", "result": "研究发现，在监督微调后，中训在数学和代码领域效果最佳，因为它能有效减少预训练和后训练数据之间的句法差异。在这种情况下，中训在域内验证损失和后训练后的预训练数据遗忘方面均优于持续预训练。消融实验表明，中训阶段的起始时间比数据混合权重影响更大，越早引入特定领域数据，域内收益越大，同时也能更好地保留通用语言建模能力。", "conclusion": "这些发现确立了中训是一种领域适应技术，与持续预训练相比，它通过减少遗忘实现了更好的性能。"}}
{"id": "2510.14981", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14981", "abs": "https://arxiv.org/abs/2510.14981", "authors": ["Hadi Alzayer", "Yunzhi Zhang", "Chen Geng", "Jia-Bin Huang", "Jiajun Wu"], "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing", "comment": "Project page: https://coupled-diffusion.github.io", "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.", "AI": {"tldr": "该论文提出了一种推理时扩散采样方法，通过耦合扩散采样（一种隐式3D正则化方法），利用预训练的2D图像编辑模型实现多视角一致的图像编辑，解决了现有2D模型缺乏一致性及3D优化方法耗时长、不稳定的问题。", "motivation": "预训练的2D图像编辑模型能独立地为多视角图像生成高质量编辑，但无法保持视角间的一致性。现有的通过优化显式3D表示的方法通常耗时较长，且在稀疏视角设置下不稳定。", "method": "提出了一种隐式3D正则化方法，通过约束生成的2D图像序列符合预训练的多视角图像分布来实现。具体通过“耦合扩散采样”技术，同时从多视角图像分布和2D编辑图像分布中采样两条轨迹，并使用耦合项来强制生成图像之间的多视角一致性。", "result": "该框架在三种不同的多视角图像编辑任务上验证了其有效性和通用性，展示了其适用于各种模型架构，并突显了其作为多视角一致性编辑通用解决方案的潜力。", "conclusion": "该方法提供了一个通用且有效的解决方案，可以在推理时使用预训练的2D图像编辑模型进行多视角一致的图像编辑。"}}
{"id": "2510.14871", "categories": ["cs.CL", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14871", "abs": "https://arxiv.org/abs/2510.14871", "authors": ["Erwei Wang", "Samuel Bayliss", "Andra Bisca", "Zachary Blair", "Sangeeta Chowdhary", "Kristof Denolf", "Jeff Fifield", "Brandon Freiberger", "Erika Hunhoff", "Phil James-Roxby", "Jack Lo", "Joseph Melber", "Stephen Neuendorffer", "Eddie Richter", "Andre Rosti", "Javier Setoain", "Gagandeep Singh", "Endri Taka", "Pranathi Vasireddy", "Zhewen Yu", "Niansong Zhang", "Jinming Zhuang"], "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR", "comment": null, "summary": "General-purpose compilers abstract away parallelism, locality, and\nsynchronization, limiting their effectiveness on modern spatial architectures.\nAs modern computing architectures increasingly rely on fine-grained control\nover data movement, execution order, and compute placement for performance,\ncompiler infrastructure must provide explicit mechanisms for orchestrating\ncompute and data to fully exploit such architectures. We introduce MLIR-AIR, a\nnovel, open-source compiler stack built on MLIR that bridges the semantic gap\nbetween high-level workloads and fine-grained spatial architectures such as\nAMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured\nrepresentations for asynchronous and hierarchical operations across compute and\nmemory resources. AIR primitives allow the compiler to orchestrate spatial\nscheduling, distribute computation across hardware regions, and overlap\ncommunication with computation without relying on ad hoc runtime coordination\nor manual scheduling. We demonstrate MLIR-AIR's capabilities through two case\nstudies: matrix multiplication and the multi-head attention block from the\nLLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute\nefficiency and generates implementations with performance almost identical to\nstate-of-the-art, hand-optimized matrix multiplication written using the\nlower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we\ndemonstrate that the AIR interface supports fused implementations using\napproximately 150 lines of code, enabling tractable expression of complex\nworkloads with efficient mapping to spatial hardware. MLIR-AIR transforms\nhigh-level structured control flow into spatial programs that efficiently\nutilize the compute fabric and memory hierarchy of an NPU, leveraging\nasynchronous execution, tiling, and communication overlap through\ncompiler-managed scheduling.", "AI": {"tldr": "MLIR-AIR是一个基于MLIR的新型编译器栈，旨在弥合高级工作负载与AMD NPU等空间架构之间的语义鸿沟，通过显式机制高效编排计算和数据。", "motivation": "通用编译器抽象了并行性、局部性和同步性，限制了其在现代空间架构上的效率。现代计算架构越来越依赖对数据移动、执行顺序和计算放置的细粒度控制来提高性能，因此编译器基础设施必须提供显式机制来充分利用这些架构。", "method": "引入了MLIR-AIR，一个基于MLIR的开源编译器栈，定义了AIR方言。AIR方言提供了跨计算和内存资源的异步和分层操作的结构化表示。AIR原语允许编译器编排空间调度、在硬件区域间分配计算，并实现通信与计算的重叠，无需依赖临时运行时协调或手动调度。", "result": "在矩阵乘法案例中，MLIR-AIR实现了高达78.7%的计算效率，其性能与使用MLIR-AIE框架手写优化的最先进实现几乎相同。在LLaMA 2模型的多头注意力块案例中，MLIR-AIR支持约150行代码的融合实现，证明了其能够有效表达复杂工作负载并高效映射到空间硬件。MLIR-AIR通过编译器管理的调度，利用异步执行、分块和通信重叠，将高级结构化控制流转换为高效利用NPU计算结构和内存层次结构的空间程序。", "conclusion": "MLIR-AIR成功弥合了高级工作负载与细粒度空间架构之间的语义鸿沟，通过编译器管理的调度和编排，实现了对NPU计算结构和内存层次结构的高效利用，从而支持复杂工作负载的有效映射和高性能执行。"}}
{"id": "2510.14979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14979", "abs": "https://arxiv.org/abs/2510.14979", "authors": ["Haiwen Diao", "Mingxuan Li", "Silei Wu", "Linjun Dai", "Xiaohua Wang", "Hanming Deng", "Lewei Lu", "Dahua Lin", "Ziwei Liu"], "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale", "comment": "21 pages, 7 figures", "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.", "AI": {"tldr": "本文提出了一系列构建原生视觉-语言模型（VLM）的指导原则，并基于这些原则推出了NEO系列模型。NEO在仅使用3.9亿图文示例的情况下，能与顶尖的模块化VLM相媲美，并有效解决了原生VLM的挑战。", "motivation": "原生VLM作为模块化VLM的有力竞争者正在兴起，但其普及面临两大挑战：一是原生VLM与模块化VLM之间的根本性限制及其克服方法；二是如何降低原生VLM研究的门槛，加速领域进展。本文旨在阐明这些挑战并提供解决方案。", "method": "本文提出了构建原生VLM的指导原则，即一个原生VLM基元应：(i) 有效对齐像素和词语表示；(ii) 无缝整合独立的视觉和语言模块的优势；(iii) 内在地体现支持统一视觉-语言编码、对齐和推理的跨模态特性。基于这些原则，本文构建了NEO系列原生VLM。", "result": "NEO系列原生VLM在多样化的真实世界场景中能够与顶尖的模块化VLM相媲美。它仅使用3.9亿图文示例，便能从头高效地发展视觉感知能力，并成功缓解了密集单体模型中的视觉-语言冲突。", "conclusion": "NEO被定位为可扩展且强大的原生VLM的基石，并提供了一套可重用组件，以促进一个经济高效且可扩展的生态系统。本文通过NEO展示了原生VLM的巨大潜力，并为该领域的进一步研究提供了可访问的资源。"}}
{"id": "2510.14915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14915", "abs": "https://arxiv.org/abs/2510.14915", "authors": ["Xujun Peng", "Anoop Kumar", "Jingyu Wu", "Parker Glenn", "Daben Liu"], "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation", "comment": "EMNLP 2025 Industry track", "summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models\n(LLMs) to generate accurate and reliable responses that are grounded in\nretrieved context. However, LLMs often generate inconsistent outputs for\nsemantically equivalent inputs, a problem compounded by the scarcity of\nconsistency-focused training data and the limitations of current fine-tuning\ntechniques in enhancing output consistency. We propose a new approach combining\nsystematic synthetic data generation, triplet loss for better embeddings, and a\nnovel layer-wise model merging approach. Using consistency-aware weights\nderived from intermediate layer activations, our method effectively integrates\nknowledge from specialized models. Experimental results how that our merged\nmodel significantly enhances output consistency, achieving a ~47.5\\%\nimprovement in response similarity over the baseline, thus offering a practical\nsolution for increasing the reliability of an industrial RAG system.", "AI": {"tldr": "针对RAG系统中大型语言模型（LLM）输出不一致问题，本文提出了一种结合系统性合成数据生成、三元组损失和新型层级模型合并的方法，显著提升了模型输出的一致性。", "motivation": "RAG系统中的LLM在面对语义等价输入时常产生不一致的输出，且目前缺乏针对一致性的训练数据和有效的微调技术来解决此问题。", "method": "该研究提出了一种新方法，包括：1) 系统性合成数据生成；2) 使用三元组损失（triplet loss）以获得更好的嵌入表示；3) 一种新颖的层级模型合并方法，该方法利用从中间层激活中导出的一致性感知权重，有效整合了专业模型的知识。", "result": "实验结果表明，通过该方法合并的模型显著增强了输出一致性，响应相似度比基线模型提高了约47.5%。", "conclusion": "该方法为提高工业RAG系统的可靠性提供了一个实用的解决方案，有效解决了LLM输出不一致的挑战。"}}
{"id": "2509.25991", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25991", "abs": "https://arxiv.org/abs/2509.25991", "authors": ["Haiyang Li", "Yaxiong Wang", "Shengeng Tang", "Lianwei Wu", "Lechao Cheng", "Zhun Zhong"], "title": "Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline", "comment": null, "summary": "In recent years, detecting fake multimodal content on social media has drawn\nincreasing attention. Two major forms of deception dominate: human-crafted\nmisinformation (e.g., rumors and misleading posts) and AI-generated content\nproduced by image synthesis models or vision-language models (VLMs). Although\nboth share deceptive intent, they are typically studied in isolation. NLP\nresearch focuses on human-written misinformation, while the CV community\ntargets AI-generated artifacts. As a result, existing models are often\nspecialized for only one type of fake content. In real-world scenarios,\nhowever, the type of a multimodal post is usually unknown, limiting the\neffectiveness of such specialized systems. To bridge this gap, we construct the\nOmnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive\nbenchmark of 127K samples that integrates human-curated misinformation from\nexisting resources with newly synthesized AI-generated examples. Based on this\ndataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a\nframework designed to handle both forms of deception. UMFDet leverages a VLM\nbackbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to\ncapture category-specific cues, and an attribution chain-of-thought mechanism\nthat provides implicit reasoning guidance for locating salient deceptive\nsignals. Extensive experiments demonstrate that UMFDet achieves robust and\nconsistent performance across both misinformation types, outperforming\nspecialized baselines and offering a practical solution for real-world\nmultimodal deception detection.", "AI": {"tldr": "该研究构建了OmniFake数据集，并提出了UMFDet框架，旨在统一检测社交媒体上人类制造和AI生成的多模态虚假内容，解决了现有模型专业化、无法应对未知欺骗类型的问题。", "motivation": "当前研究将人类制造的虚假信息和AI生成的内容孤立处理，导致现有模型仅擅长检测其中一种，但在现实世界中，多模态帖子的欺骗类型通常未知，限制了这些专业化系统的有效性。", "method": "研究构建了OmniFake数据集（12.7万样本），整合了现有资源中的人类策划虚假信息和新合成的AI生成示例。在此基础上，提出了统一多模态虚假内容检测（UMFDet）框架，该框架利用VLM骨干网络，并增强了类别感知专家混合（MoE）适配器以捕获类别特定线索，以及归因思维链机制以提供定位显著欺骗信号的隐式推理指导。", "result": "UMFDet在两种虚假信息类型上都取得了稳健且一致的性能，优于专业化基线模型，并为真实世界的多模态欺骗检测提供了实用解决方案。", "conclusion": "UMFDet框架通过有效处理人类制造和AI生成的虚假内容，为真实世界多模态欺骗检测提供了一个实用且性能优越的统一解决方案。"}}
{"id": "2510.14937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14937", "abs": "https://arxiv.org/abs/2510.14937", "authors": ["Jianfeng Zhu", "Julina Maharjan", "Xinyu Li", "Karin G. Coifman", "Ruoming Jin"], "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations", "comment": "7 pages 1 figure", "summary": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited.", "AI": {"tldr": "本研究评估了机器学习模型（包括零样本GPT和LoRA微调RoBERTa）在心理健康筛查中的有效性，使用553份真实半结构化访谈数据集，在抑郁症、焦虑症和创伤后应激障碍诊断中实现了超过80%的准确率，为低门槛、AI驱动的早期诊断提供了可行路径。", "motivation": "心理健康障碍在全球范围内仍是导致残疾的主要原因，但由于主观评估、临床资源有限、污名化和认知不足，抑郁症、焦虑症和创伤后应激障碍常被漏诊或误诊（初级保健中超过60%的病例），迫切需要可扩展、易于获取且情境感知的诊断工具来支持早期检测和干预。", "method": "研究使用包含553份真实世界、半结构化访谈的独特数据集，每份访谈都配有重度抑郁发作、焦虑症和创伤后应激障碍的真实诊断。评估了多种模型类别，包括使用GPT-4.1 Mini和MetaLLaMA进行零样本提示，以及使用LoRA（LowRank Adaptation）进行微调的RoBERTa模型。", "result": "模型在所有诊断类别中均实现了超过80%的准确率，尤其在创伤后应激障碍（PTSD）方面表现出色（高达89%的准确率和98%的召回率）。研究还发现，使用更短、更集中的上下文片段可以提高召回率。LoRA微调被证明既高效又有效，较低秩配置（如秩8和16）在各项评估指标上均保持了有竞争力的性能。", "conclusion": "研究结果表明，基于大型语言模型（LLM）的模型可以显著优于传统的自我报告筛查工具，为低门槛、AI驱动的早期诊断提供了途径。这项工作为将机器学习整合到真实世界的临床工作流程中奠定了基础，尤其适用于资源匮乏或污名化严重、及时心理健康护理最受限制的环境。"}}
{"id": "2510.14949", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14949", "abs": "https://arxiv.org/abs/2510.14949", "authors": ["Yu Zhou", "Sohyun An", "Haikang Deng", "Da Yin", "Clark Peng", "Cho-Jui Hsieh", "Kai-Wei Chang", "Nanyun Peng"], "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation", "comment": null, "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance.", "AI": {"tldr": "研究发现，多模态生成模型在处理方言文本输入时性能显著下降。本文构建了一个包含六种英语方言的大规模基准，并提出了一种基于编码器的缓解策略，该策略能显著提升模型在方言上的表现，同时不损害标准美式英语性能。", "motivation": "接触语言（如英语）存在丰富的区域方言变体，方言使用者常与生成模型互动。然而，多模态生成模型能否有效处理方言文本输入并生成内容，这是一个未被充分研究的问题。", "method": "构建了一个涵盖六种常见英语方言的大规模基准数据集，收集并验证了超过4200个独特的提示词。评估了17个图像和视频生成模型，采用自动和人工评估。设计了一种通用的基于编码器的缓解策略，以使模型识别新的方言特征，同时保持标准美式英语（SAE）的性能。", "result": "当前最先进的多模态生成模型在使用单个方言词汇时，性能下降32.26%至48.17%。常见的缓解方法（如微调和提示词重写）只能带来微小的性能提升（< 7%），并可能导致标准美式英语性能显著下降。本文提出的基于编码器的方法在Stable Diffusion 1.5等模型上的实验表明，它能同时将五种方言的性能提升至与标准美式英语相当的水平（+34.4%），而对标准美式英语性能的损害几乎为零。", "conclusion": "多模态生成模型在处理方言文本输入时存在严重的性能缺陷。本文提出的基于编码器的缓解策略能够有效提升模型在方言上的表现，使其与标准美式英语性能持平，且不影响标准美式英语性能，为解决这一问题提供了通用方案。"}}
{"id": "2510.14944", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.14944", "abs": "https://arxiv.org/abs/2510.14944", "authors": ["Yuxing Lu", "Xukai Zhao", "J. Ben Tamo", "Micky C. Nnamdi", "Rui Peng", "Shuang Zeng", "Xingyu Hu", "Jinzhuo Wang", "May D. Wang"], "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics", "comment": "22 pages, 6 figures, 4 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14967", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14967", "abs": "https://arxiv.org/abs/2510.14967", "authors": ["Guoqing Wang", "Sunhao Dai", "Guangze Ye", "Zeyu Gan", "Wei Yao", "Yong Deng", "Xiaofeng Wu", "Zhenzhe Ying"], "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14972", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14972", "abs": "https://arxiv.org/abs/2510.14972", "authors": ["Yinxi Li", "Yuntian Deng", "Pengyu Nie"], "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar", "comment": null, "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.14756", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14756", "abs": "https://arxiv.org/abs/2510.14756", "authors": ["Manar Abdelatty", "Maryam Nouh", "Jacob K. Rosenstein", "Sherief Reda"], "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate hardware\ndesign tasks, including the generation of Verilog code. While early benchmarks\nfocus primarily on functional correctness, efficient hardware design demands\nadditional optimization for synthesis metrics such as area, delay, and power.\nExisting benchmarks fall short in evaluating these aspects comprehensively:\nthey often lack optimized baselines or testbenches for verification. To address\nthese gaps, we present Pluto, a benchmark and evaluation framework designed to\nassess the efficiency of LLM-generated Verilog designs. Pluto presents a\ncomprehensive evaluation set of 114 problems with self-checking testbenches and\nmultiple Pareto-optimal reference implementations. Experimental results show\nthat state-of-the-art LLMs can achieve high functional correctness, reaching\n78.3\\% at pass@1, but their synthesis efficiency still lags behind\nexpert-crafted implementations, with area efficiency of 63.8\\%, delay\nefficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights\nthe need for efficiency-aware evaluation frameworks such as Pluto to drive\nprogress in hardware-focused LLM research.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
