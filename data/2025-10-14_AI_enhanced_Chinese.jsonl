{"id": "2510.09782", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.09782", "abs": "https://arxiv.org/abs/2510.09782", "authors": ["Yufa Zhou", "Yixiao Wang", "Xunjian Yin", "Shuyan Zhou", "Anru R. Zhang"], "title": "The Geometry of Reasoning: Flowing Logics in Representation Space", "comment": "Code: https://github.com/MasterZhou1/Reasoning-Flow", "summary": "We study how large language models (LLMs) ``think'' through their\nrepresentation space. We propose a novel geometric framework that models an\nLLM's reasoning as flows -- embedding trajectories evolving where logic goes.\nWe disentangle logical structure from semantics by employing the same natural\ndeduction propositions with varied semantic carriers, allowing us to test\nwhether LLMs internalize logic beyond surface form. This perspective connects\nreasoning with geometric quantities such as position, velocity, and curvature,\nenabling formal analysis in representation and concept spaces. Our theory\nestablishes: (1) LLM reasoning corresponds to smooth flows in representation\nspace, and (2) logical statements act as local controllers of these flows'\nvelocities. Using learned representation proxies, we design controlled\nexperiments to visualize and quantify reasoning flows, providing empirical\nvalidation of our theoretical framework. Our work serves as both a conceptual\nfoundation and practical tools for studying reasoning phenomenon, offering a\nnew lens for interpretability and formal analysis of LLMs' behavior.", "AI": {"tldr": "本文提出一个新颖的几何框架，将大型语言模型（LLM）的推理建模为表示空间中的“流”，通过解耦逻辑和语义来分析LLM如何内化逻辑。", "motivation": "研究LLM如何通过其表示空间进行“思考”，并理解它们是否在表面形式之外内化了逻辑，从而为LLM行为提供可解释性和形式化分析的新视角。", "method": "提出一个几何框架，将LLM推理建模为表示空间中的平滑流（嵌入轨迹），并将逻辑结构与语义解耦。该方法将推理与位置、速度、曲率等几何量联系起来，并通过学习到的表示代理设计受控实验来可视化和量化推理流。", "result": "理论上，LLM推理对应于表示空间中的平滑流，且逻辑语句作为这些流速度的局部控制器。通过实验验证了这一理论框架，并提供了推理流的可视化和量化。", "conclusion": "该工作为研究LLM推理现象提供了概念基础和实用工具，为LLM行为的解释性和形式化分析提供了一个新视角。"}}
{"id": "2510.09654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09654", "abs": "https://arxiv.org/abs/2510.09654", "authors": ["Zeshan Khan"], "title": "TreeNet: Layered Decision Ensembles", "comment": null, "summary": "Within the domain of medical image analysis, three distinct methodologies\nhave demonstrated commendable accuracy: Neural Networks, Decision Trees, and\nEnsemble-Based Learning Algorithms, particularly in the specialized context of\ngenstro institutional track abnormalities detection. These approaches exhibit\nefficacy in disease detection scenarios where a substantial volume of data is\navailable. However, the prevalent challenge in medical image analysis pertains\nto limited data availability and data confidence. This paper introduces\nTreeNet, a novel layered decision ensemble learning methodology tailored for\nmedical image analysis. Constructed by integrating pivotal features from neural\nnetworks, ensemble learning, and tree-based decision models, TreeNet emerges as\na potent and adaptable model capable of delivering superior performance across\ndiverse and intricate machine learning tasks. Furthermore, its interpretability\nand insightful decision-making process enhance its applicability in complex\nmedical scenarios. Evaluation of the proposed approach encompasses key metrics\nincluding Accuracy, Precision, Recall, and training and evaluation time. The\nmethodology resulted in an F1-score of up to 0.85 when using the complete\ntraining data, with an F1-score of 0.77 when utilizing 50\\% of the training\ndata. This shows a reduction of F1-score of 0.08 while in the reduction of 50\\%\nof the training data and training time. The evaluation of the methodology\nresulted in the 32 Frame per Second which is usable for the realtime\napplications. This comprehensive assessment underscores the efficiency and\nusability of TreeNet in the demanding landscape of medical image analysis\nspecially in the realtime analysis.", "AI": {"tldr": "本文提出了一种名为TreeNet的新型分层决策集成学习方法，专为医学图像分析设计。它结合了神经网络、集成学习和树模型的特点，在数据有限的情况下也能表现出色，且具有良好的可解释性和实时应用能力。", "motivation": "在医学图像分析领域，现有方法（如神经网络、决策树和集成学习）在数据量充足时表现良好。然而，医学图像分析普遍面临数据可用性有限和数据置信度不足的挑战。", "method": "本文引入了TreeNet，一种新颖的分层决策集成学习方法，专门用于医学图像分析。它通过整合神经网络、集成学习和基于树的决策模型的关键特征构建而成。该方法通过准确率、精确率、召回率以及训练和评估时间等关键指标进行评估。", "result": "使用完整训练数据时，F1-score高达0.85；使用50%训练数据时，F1-score为0.77（F1-score降低0.08）。评估结果显示，该方法达到每秒32帧，适用于实时应用。", "conclusion": "TreeNet在医学图像分析的严苛环境中，特别是在实时分析方面，表现出高效性和可用性，能够提供卓越的性能，并具有良好的可解释性和洞察力。"}}
{"id": "2510.09786", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09786", "abs": "https://arxiv.org/abs/2510.09786", "authors": ["Yuang Lu", "Song Wang", "Xiao Han", "Xuri Zhang", "Yucong Wu", "Zhicheng He"], "title": "Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks", "comment": "7 pages, 7 figures", "summary": "Temporal sequential tasks challenge humanoid robots, as existing Diffusion\nPolicy (DP) and Action Chunking with Transformers (ACT) methods often lack\ntemporal context, resulting in local optima traps and excessive repetitive\nactions. To address these issues, this paper introduces a Classifier-Free\nGuidance-Based Diffusion Policy (CFG-DP), a novel framework to enhance DP by\nintegrating Classifier-Free Guidance (CFG) with conditional and unconditional\nmodels. Specifically, CFG leverages timestep inputs to track task progression\nand ensure precise cycle termination. It dynamically adjusts action predictions\nbased on task phase, using a guidance factor tuned to balance temporal\ncoherence and action accuracy. Real-world experiments on a humanoid robot\ndemonstrate high success rates and minimal repetitive actions. Furthermore, we\nassessed the model's ability to terminate actions and examined how different\ncomponents and parameter adjustments affect its performance. This framework\nsignificantly enhances deterministic control and execution reliability for\nsequential robotic tasks.", "AI": {"tldr": "本文提出了一种基于无分类器引导的扩散策略 (CFG-DP)，通过将无分类器引导 (CFG) 集成到扩散策略 (DP) 中，解决了人形机器人时间序列任务中现有方法缺乏时间上下文、导致局部最优和重复动作过多的问题，显著提高了任务成功率和执行可靠性。", "motivation": "人形机器人执行时间序列任务时，现有扩散策略 (DP) 和基于Transformer的动作分块 (ACT) 方法通常缺乏时间上下文，导致陷入局部最优和产生过多重复动作。", "method": "引入了基于无分类器引导的扩散策略 (CFG-DP) 框架，通过结合条件和无条件模型来增强 DP。CFG 利用时间步输入跟踪任务进度并确保精确的周期终止，根据任务阶段动态调整动作预测，并使用引导因子平衡时间连贯性和动作准确性。", "result": "在人形机器人上的真实世界实验表明，CFG-DP 实现了高成功率和极少的重复动作。此外，研究评估了模型终止动作的能力，并分析了不同组件和参数调整对其性能的影响。", "conclusion": "该框架显著增强了机器人序列任务的确定性控制和执行可靠性。"}}
{"id": "2510.09649", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09649", "abs": "https://arxiv.org/abs/2510.09649", "authors": ["Khartik Uppalapati", "Bora Yimenicioglu", "Shakeel Abdulkareem", "Adan Eftekhari", "Bhavya Uppalapati", "Viraj Kamath"], "title": "TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI", "comment": "8 pages, 3 figures, 1 table. Submitted to International Conference on\n  Computational Intelligence and Sustainable Engineering Solutions (CISES)", "summary": "Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric\nneurodegenerative disorder whose early MRI signs are subtle and often missed.\nWe propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to\ndetect early Batten disease from pediatric brain MRI with limited training\ncases. We distill a large teacher ViT into a 5 M-parameter TinyViT and\nfine-tune it using metric-based few-shot learning (prototypical loss with\n5-shot episodes). Our model achieves high accuracy (approximately 91%) and area\nunder ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed\nBatten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2\nfrom an international longitudinal cohort, 12 early-manifestation CLN2 cases\nreported by Cokal et al., and 8 public Radiopaedia scans) together with 90\nage-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We\nfurther integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to\nhighlight disease-relevant brain regions, enabling explainable predictions. The\nmodel's small size and strong performance (sensitivity greater than 90%,\nspecificity approximately 90%) demonstrates a practical AI solution for early\nBatten disease detection.", "AI": {"tldr": "本文提出TinyViT-Batten，一个基于少量样本学习的Vision Transformer框架，用于从儿科脑部MRI中早期检测巴顿病，在有限训练数据下取得了高准确率和可解释性。", "motivation": "巴顿病（Batten disease）是一种罕见的儿科神经退行性疾病，其早期MRI迹象微妙且常被遗漏，因此需要一种有效的方法进行早期检测。", "method": "研究团队将一个大型教师ViT模型蒸馏成一个参数量为5M的TinyViT模型，并使用基于度量的少量样本学习（带有5次迭代的原型损失）进行微调。此外，还集成了梯度加权类激活映射（Grad-CAM）以实现可解释的预测。", "result": "该模型在包含79例基因确诊巴顿病MRI和90例年龄匹配对照的多中心数据集上，实现了约91%的高准确率和至少0.95的ROC曲线下面积，优于3D-ResNet和Swin-Tiny基线模型。模型灵敏度大于90%，特异性约90%。", "conclusion": "该模型的体积小巧和强大性能（高灵敏度和特异性）证明了其作为早期巴顿病检测的实用AI解决方案的潜力，并能提供可解释的预测。"}}
{"id": "2510.09810", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09810", "abs": "https://arxiv.org/abs/2510.09810", "authors": ["Victor Freire", "Marco M. Nicotra"], "title": "Designing Control Barrier Functions Using a Dynamic Backup Policy", "comment": "7 pages, 1 figure", "summary": "This paper presents a systematic approach to construct control barrier\nfunctions for nonlinear control affine systems subject to arbitrary state and\ninput constraints. Taking inspiration from the reference governor literature,\nthe proposed method defines a family of backup policies, parametrized by the\nequilibrium manifold of the system. The control barrier function is defined on\nthe augmented state-and-reference space: given a state-reference pair, the\napproach quantifies the distance to constraint violation at any time in the\nfuture, should the current backup policy reference remain constant. Sensitivity\nanalysis is then used to compute the (possibly nonsmooth) Jacobian with respect\nto the augmented state vector. To showcase its simple yet general nature, the\nproposed method is applied to an inverted pendulum on cart.", "AI": {"tldr": "本文提出了一种系统方法，用于为受任意状态和输入约束的非线性控制仿射系统构建控制障碍函数（CBFs），该方法基于备份策略和增广状态-参考空间，并通过敏感性分析量化未来约束违反的距离。", "motivation": "为受任意状态和输入约束的非线性控制仿射系统构建有效的控制障碍函数是一个挑战，现有方法可能不够系统或通用。本文旨在提供一个系统的解决方案，并借鉴了参考调节器（reference governor）的思想。", "method": "该方法定义了一系列由系统平衡流形参数化的备份策略。控制障碍函数定义在增广的状态-参考空间上，用于量化在当前备份策略参考保持不变的情况下，未来任何时刻与约束违反的距离。通过敏感性分析计算增广状态向量的（可能非光滑的）雅可比矩阵。", "result": "本文提出了一种简单而通用的系统方法来构建控制障碍函数，能够处理非线性控制仿射系统中的任意状态和输入约束。该方法通过倒立摆在小车上的应用展示了其有效性。", "conclusion": "本文成功提出了一种系统且通用的方法，用于为受任意约束的非线性控制仿射系统构建控制障碍函数，并通过一个具体实例验证了其可行性和简洁性。"}}
{"id": "2510.09679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09679", "abs": "https://arxiv.org/abs/2510.09679", "authors": ["Zhengsen Xu", "Yimin Zhu", "Zack Dewis", "Mabel Heffring", "Motasem Alkayid", "Saeid Taleghanidoozdoozan", "Lincoln Linlin Xu"], "title": "Knowledge-Aware Mamba for Joint Change Detection and Classification from MODIS Times Series", "comment": null, "summary": "Although change detection using MODIS time series is critical for\nenvironmental monitoring, it is a highly challenging task due to key MODIS\ndifficulties, e.g., mixed pixels, spatial-spectral-temporal information\ncoupling effect, and background class heterogeneity. This paper presents a\nnovel knowledge-aware Mamba (KAMamba) for enhanced MODIS change detection, with\nthe following contributions. First, to leverage knowledge regarding class\ntransitions, we design a novel knowledge-driven transition-matrix-guided\napproach, leading to a knowledge-aware transition loss (KAT-loss) that can\nenhance detection accuracies. Second, to improve model constraints, a\nmulti-task learning approach is designed, where three losses, i.e., pre-change\nclassification loss (PreC-loss), post-change classification loss (PostC-loss),\nand change detection loss (Chg-loss) are used for improve model learning.\nThird, to disentangle information coupling in MODIS time series, novel\nspatial-spectral-temporal Mamba (SSTMamba) modules are designed. Last, to\nimprove Mamba model efficiency and remove computational cost, a sparse and\ndeformable Mamba (SDMamba) backbone is used in SSTMamba. On the MODIS\ntime-series dataset for Saskatchewan, Canada, we evaluate the method on\nland-cover change detection and LULC classification; results show about 1.5-6%\ngains in average F1 for change detection over baselines, and about 2%\nimprovements in OA, AA, and Kappa for LULC classification.", "AI": {"tldr": "本文提出了一种新颖的知识感知Mamba (KAMamba) 模型，用于增强MODIS时间序列变化检测，通过引入知识驱动的过渡矩阵指导、多任务学习、空间-光谱-时间Mamba模块和稀疏可变形Mamba骨干网络，有效解决了MODIS数据中混合像素和信息耦合等挑战，显著提高了变化检测和土地覆盖分类的准确性。", "motivation": "MODIS时间序列的变化检测对于环境监测至关重要，但面临诸多挑战，如混合像素、空间-光谱-时间信息耦合效应以及背景类别异质性，这些都使得任务极具挑战性。", "method": "本文提出了知识感知Mamba (KAMamba) 模型：\n1. 设计了知识驱动的过渡矩阵引导方法，引入知识感知过渡损失 (KAT-loss) 以利用类别转换知识，提高检测精度。\n2. 采用多任务学习方法，结合预变化分类损失 (PreC-loss)、后变化分类损失 (PostC-loss) 和变化检测损失 (Chg-loss) 来增强模型约束和学习。\n3. 设计了新颖的空间-光谱-时间Mamba (SSTMamba) 模块，用于解耦MODIS时间序列中的信息耦合。\n4. 在SSTMamba中使用了稀疏可变形Mamba (SDMamba) 骨干网络，以提高Mamba模型的效率并降低计算成本。", "result": "在加拿大萨斯喀彻温省的MODIS时间序列数据集上进行评估，结果显示：\n1. 土地覆盖变化检测的平均F1分数比基线提高了约1.5-6%。\n2. 土地利用/土地覆盖 (LULC) 分类的总体准确率 (OA)、平均准确率 (AA) 和Kappa系数均提高了约2%。", "conclusion": "KAMamba通过其创新性的知识感知过渡损失、多任务学习、SSTMamba模块以及高效的SDMamba骨干网络，成功解决了MODIS时间序列变化检测中的关键难题，显著提升了变化检测和LULC分类的性能。"}}
{"id": "2510.09653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09653", "abs": "https://arxiv.org/abs/2510.09653", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition", "comment": "16 pages, 5 Tables, 5 Figures", "summary": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR,\nand DEIM. Metrics including precision, recall, F1 score, mean Average\nPrecision, and inference speed are analyzed to highlight trade-offs between\naccuracy and efficiency. Deployment and application perspectives are further\ndiscussed, covering export formats, quantization strategies, and real-world use\nin robotics, agriculture, surveillance, and manufacturing. Finally, the paper\nidentifies challenges and future directions, including dense-scene limitations,\nhybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware\ntraining approaches.", "AI": {"tldr": "本文全面概述了Ultralytics YOLO目标检测器的发展历程，从YOLOv5到最新的YOLO26，涵盖了架构演变、基准测试、部署和未来挑战。", "motivation": "研究动机是提供Ultralytics YOLO系列目标检测器的综合性概览，详细阐述其架构演变、性能基准、部署考量以及面临的未来挑战。", "method": "本文通过回顾YOLO系列（YOLOv5、YOLOv8、YOLOv11、YOLO26）的架构创新来追踪其演变，并在MS COCO数据集上进行基准测试，量化比较YOLOv5、YOLOv8、YOLOv11和YOLO26，并与YOLOv12、YOLOv13、RT-DETR、DEIM等模型进行交叉比较。分析了精度、召回率、F1分数、平均精度和推理速度等指标，并讨论了部署策略和未来研究方向。", "result": "YOLO26引入了移除DFL、原生NMS-free推理、ProgLoss、STAL和MuSGD优化器等关键创新。YOLOv11具有混合任务分配和效率模块；YOLOv8通过解耦检测头和无锚点预测进行改进；YOLOv5奠定了PyTorch模块化基础。基准测试详细比较了不同YOLO版本和其他模型，突出了精度与效率之间的权衡。部署方面涵盖了导出格式、量化策略和实际应用。", "conclusion": "本文识别了YOLO系列在密集场景限制、混合CNN-Transformer集成、开放词汇检测和边缘感知训练方法等方面的挑战和未来发展方向。"}}
{"id": "2510.09610", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09610", "abs": "https://arxiv.org/abs/2510.09610", "authors": ["Samet Uzun", "Behcet Acikmese", "John M. Carson III"], "title": "Sequential Convex Programming for 6-DoF Powered Descent Guidance with Continuous-Time Compound State-Triggered Constraints", "comment": null, "summary": "This paper presents a sequential convex programming (SCP) framework for\nensuring the continuous-time satisfaction of compound state-triggered\nconstraints, a subset of logical specifications, in the powered descent\nguidance (PDG) problem. The proposed framework combines the generalized\nmean-based smooth robustness measure (D-GMSR), a parameterization technique\ntailored for expressing discrete-time temporal and logical specifications\nthrough smooth functions, with the continuous-time successive convexification\n(CT-SCvx) method, a real-time solution for constrained trajectory optimization\nthat guarantees continuous-time constraint satisfaction and convergence. The\nsmoothness of the temporal and logical specifications parameterized via D-GMSR\nenables solving the resulting optimization problem with robust and efficient\nSCP algorithms while preserving theoretical guarantees. In addition to their\nsmoothness, the parameterized specifications are sound and complete, meaning\nthe specification holds if and only if the constraint defined by the\nparameterized function is satisfied. The CT-SCvx framework is then applied to\nsolve the parameterized problem, incorporating: (1) reformulation for\ncontinuous-time path constraint satisfaction, (2) time-dilation to transform\nthe free-final-time PDG problem into a fixed-final-time problem, (3) multiple\nshooting for exact discretization, (4) exact penalty functions for penalizing\nnonconvex constraints, and (5) the prox-linear method, a convergence-guaranteed\nSCP algorithm, to solve the resulting finite-dimensional nonconvex PDG problem.\nThe effectiveness of the framework is demonstrated through a numerical\nsimulation. The implementation is available at\nhttps://github.com/UW-ACL/CT-cSTC", "AI": {"tldr": "本文提出了一种顺序凸规划（SCP）框架，结合D-GMSR和CT-SCvx方法，用于解决动力下降制导（PDG）问题中连续时间复合状态触发约束的满足。", "motivation": "在动力下降制导（PDG）问题中，需要确保连续时间满足复杂的、复合的状态触发约束（一种逻辑规范），这通常难以实现。", "method": "该框架结合了两种主要方法：1. 广义均值平滑鲁棒性度量（D-GMSR），用于将离散时间时序和逻辑规范参数化为平滑函数。2. 连续时间逐次凸化（CT-SCvx）方法，用于实时约束轨迹优化，并保证连续时间约束的满足和收敛。CT-SCvx具体包括：连续时间路径约束重构、时间膨胀（将自由末端时间问题转为固定末端时间问题）、多重打靶精确离散化、精确惩罚函数处理非凸约束，以及近端线性方法求解最终的有限维非凸PDG问题。", "result": "通过数值模拟验证了所提出框架的有效性。", "conclusion": "该论文提出了一个稳健高效的SCP框架，能够处理PDG问题中的连续时间复合状态触发约束，并具有理论保证和经过数值模拟验证的有效性。"}}
{"id": "2510.09667", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09667", "abs": "https://arxiv.org/abs/2510.09667", "authors": ["Huaihai Lyu", "Chaofan Chen", "Senwei Xie", "Pengwei Wang", "Xiansheng Chen", "Shanghang Zhang", "Changsheng Xu"], "title": "OmniSAT: Compact Action Token, Faster Auto Regression", "comment": null, "summary": "Existing Vision-Language-Action (VLA) models can be broadly categorized into\ndiffusion-based and auto-regressive (AR) approaches: diffusion models capture\ncontinuous action distributions but rely on computationally heavy iterative\ndenoising. In contrast, AR models enable efficient optimization and flexible\nsequence construction, making them better suited for large-scale pretraining.\nTo further improve AR efficiency, particularly when action chunks induce\nextended and high-dimensional sequences, prior work applies entropy-guided and\ntoken-frequency techniques to shorten the sequence length. However, such\ncompression struggled with \\textit{poor reconstruction or inefficient\ncompression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,\nwhich learns a compact, transferable action representation. Specifically, we\nfirst normalize value ranges and temporal horizons to obtain a consistent\nrepresentation with B-Spline encoding. Then, we apply multi-stage residual\nquantization to the position, rotation, and gripper subspaces, producing\ncompressed discrete tokens with coarse-to-fine granularity for each part. After\npre-training on the large-scale dataset Droid, the resulting discrete\ntokenization shortens the training sequence by 6.8$\\times$, and lowers the\ntarget entropy. To further explore the potential of OmniSAT, we develop a\ncross-embodiment learning strategy that builds on the unified action-pattern\nspace and jointly leverages robot and human demonstrations. It enables scalable\nauxiliary supervision from heterogeneous egocentric videos. Across diverse\nreal-robot and simulation experiments, OmniSAT encompasses higher compression\nwhile preserving reconstruction quality, enabling faster AR training\nconvergence and model performance.", "AI": {"tldr": "本文提出了一种名为Omni Swift Action Tokenizer (OmniSAT) 的方法，旨在为VLA模型学习紧凑、可迁移的动作表示。它通过B-Spline编码和多阶段残差量化实现高效压缩，同时保持重建质量，显著提升了自回归（AR）训练的效率和性能。", "motivation": "现有自回归（AR）视觉-语言-动作（VLA）模型在处理长且高维动作序列时效率低下，而之前的压缩方法存在重建质量差或压缩效率低的问题。为此，研究者希望找到一种更有效的方法来压缩动作序列。", "method": "本文引入了Omni Swift Action Tokenizer (OmniSAT)。具体方法包括：首先，通过B-Spline编码对值范围和时间范围进行归一化，以获得一致的表示；然后，对位置、旋转和夹持器子空间应用多阶段残差量化，为每个部分生成从粗到细粒度的压缩离散token。此外，还开发了一种跨实体学习策略，利用统一的动作模式空间，并结合机器人和人类演示进行联合训练。", "result": "在Droid大型数据集上预训练后，OmniSAT的离散token化将训练序列长度缩短了6.8倍，并降低了目标熵。在各种真实机器人和仿真实验中，OmniSAT实现了更高的压缩率，同时保持了重建质量，从而实现了更快的AR训练收敛和模型性能。", "conclusion": "OmniSAT提供了一种紧凑、可迁移的动作表示，有效解决了AR VLA模型在动作序列压缩方面的效率和质量问题。它显著缩短了训练序列，提高了训练效率和模型性能，并支持跨实体学习，为大规模VLA模型的预训练奠定了基础。"}}
{"id": "2510.09817", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09817", "abs": "https://arxiv.org/abs/2510.09817", "authors": ["Samanta Rodriguez", "Yiming Dou", "Miquel Oller", "Andrew Owens", "Nima Fazeli"], "title": "Cross-Sensor Touch Generation", "comment": "CoRL 2025", "summary": "Today's visuo-tactile sensors come in many shapes and sizes, making it\nchallenging to develop general-purpose tactile representations. This is because\nmost models are tied to a specific sensor design. To address this challenge, we\npropose two approaches to cross-sensor image generation. The first is an\nend-to-end method that leverages paired data (Touch2Touch). The second method\nbuilds an intermediate depth representation and does not require paired data\n(T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific\nmodels across multiple sensors via the cross-sensor touch generation process.\nTogether, these models offer flexible solutions for sensor translation,\ndepending on data availability and application needs. We demonstrate their\neffectiveness on downstream tasks such as in-hand pose estimation and behavior\ncloning, successfully transferring models trained on one sensor to another.\nProject page: https://samantabelen.github.io/cross_sensor_touch_generation.", "AI": {"tldr": "该研究提出了两种跨传感器触觉图像生成方法（Touch2Touch和T2D2），旨在解决触觉表示的通用性挑战，并实现传感器间模型的迁移。", "motivation": "当前的视觉-触觉传感器种类繁多，但大多数模型都与特定的传感器设计绑定，这使得开发通用目的的触觉表示变得困难。", "method": "提出了两种跨传感器图像生成方法：1. Touch2Touch：一种端到端的方法，利用配对数据进行生成。2. T2D2 (Touch-to-Depth-to-Touch)：通过构建中间深度表示，且不要求配对数据进行生成。", "result": "这两种方法都允许将特定传感器的模型应用于其他传感器，并通过跨传感器触觉生成过程实现。在手内姿态估计和行为克隆等下游任务中，成功地将在一个传感器上训练的模型迁移到另一个传感器上，证明了其有效性。", "conclusion": "这些模型为传感器转换提供了灵活的解决方案，可根据数据可用性和应用需求进行选择，从而增强了触觉表示的通用性和模型的可迁移性。"}}
{"id": "2510.09671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09671", "abs": "https://arxiv.org/abs/2510.09671", "authors": ["Wei Zhou", "Bolei Ma", "Annemarie Friedrich", "Mohsen Mesgar"], "title": "Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation", "comment": null, "summary": "Table Question Answering (TQA) aims to answer natural language questions\nabout tabular data, often accompanied by additional contexts such as text\npassages. The task spans diverse settings, varying in table representation,\nquestion/answer complexity, modality involved, and domain. While recent\nadvances in large language models (LLMs) have led to substantial progress in\nTQA, the field still lacks a systematic organization and understanding of task\nformulations, core challenges, and methodological trends, particularly in light\nof emerging research directions such as reinforcement learning. This survey\naddresses this gap by providing a comprehensive and structured overview of TQA\nresearch with a focus on LLM-based methods. We provide a comprehensive\ncategorization of existing benchmarks and task setups. We group current\nmodeling strategies according to the challenges they target, and analyze their\nstrengths and limitations. Furthermore, we highlight underexplored but timely\ntopics that have not been systematically covered in prior research. By unifying\ndisparate research threads and identifying open problems, our survey offers a\nconsolidated foundation for the TQA community, enabling a deeper understanding\nof the state of the art and guiding future developments in this rapidly\nevolving area.", "AI": {"tldr": "本文综述了表格问答（TQA）领域，特别关注基于大型语言模型（LLM）的方法，系统地分类了基准、任务设置和建模策略，并指出了未充分探索的方向和开放性问题，旨在为TQA社区提供统一的理解和未来研究指导。", "motivation": "尽管大型语言模型（LLM）在表格问答（TQA）中取得了显著进展，但该领域仍缺乏对任务制定、核心挑战和方法趋势的系统性组织和理解，尤其是在强化学习等新兴研究方向出现后。", "method": "本综述通过以下方式解决上述问题：1. 提供TQA研究的全面结构化概述，重点关注基于LLM的方法。2. 对现有基准和任务设置进行综合分类。3. 根据所针对的挑战对当前建模策略进行分组，并分析其优缺点。4. 突出以前研究中未系统涵盖的、但重要且及时的未充分探索主题。", "result": "本综述通过统一不同的研究线索并识别开放性问题，为TQA社区提供了坚实的基础，加深了对最新技术水平的理解，并为该快速发展领域的未来发展提供了指导。", "conclusion": "本综述为TQA领域提供了一个整合的、系统化的视图，特别强调了LLM的应用，并为研究人员提供了理解现状和探索未来方向的蓝图。"}}
{"id": "2510.09681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09681", "abs": "https://arxiv.org/abs/2510.09681", "authors": ["Sashank Makanaboyina"], "title": "NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation", "comment": null, "summary": "Accurate detection and segmentation of brain tumors in magnetic resonance\nimaging (MRI) are critical for effective diagnosis and treatment planning.\nDespite advances in convolutional neural networks (CNNs) such as U-Net,\nexisting models often struggle with generalization, boundary precision, and\nlimited data diversity. To address these challenges, we propose NNDM (NN\\_UNet\nDiffusion Model)a hybrid framework that integrates the robust feature\nextraction of NN-UNet with the generative capabilities of diffusion\nprobabilistic models. In our approach, the diffusion model progressively\nrefines the segmentation masks generated by NN-UNet by learning the residual\nerror distribution between predicted and ground-truth masks. This iterative\ndenoising process enables the model to correct fine structural inconsistencies\nand enhance tumor boundary delineation. Experiments conducted on the BraTS 2021\ndatasets demonstrate that NNDM achieves superior performance compared to\nconventional U-Net and transformer-based baselines, yielding improvements in\nDice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided\nrefinement enhances robustness across modalities and tumor subregions. The\nproposed NNDM establishes a new direction for combining deterministic\nsegmentation networks with stochastic diffusion models, advancing the state of\nthe art in automated brain tumor analysis.", "AI": {"tldr": "本文提出NNDM框架，结合NN-UNet和扩散模型，通过迭代去噪精炼脑肿瘤分割掩码，解决了传统模型在泛化性、边界精度和数据多样性方面的挑战。", "motivation": "尽管U-Net等卷积神经网络在MRI脑肿瘤检测和分割方面取得了进展，但现有模型在泛化性、边界精度和有限数据多样性方面仍存在不足。", "method": "NNDM（NN_UNet Diffusion Model）是一个混合框架，它将NN-UNet的鲁棒特征提取能力与扩散概率模型的生成能力相结合。扩散模型通过学习预测掩码和真实掩码之间的残差误差分布，逐步精炼NN-UNet生成的分割掩码，通过迭代去噪过程纠正细微结构不一致并增强肿瘤边界描绘。", "result": "在BraTS 2021数据集上的实验表明，NNDM在Dice系数和Hausdorff距离等指标上优于传统的U-Net和基于Transformer的基线模型。此外，扩散引导的精炼提高了模型在不同模态和肿瘤亚区域的鲁棒性。", "conclusion": "NNDM为结合确定性分割网络和随机扩散模型开辟了新方向，推动了自动化脑肿瘤分析领域的最新技术发展。"}}
{"id": "2510.09801", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09801", "abs": "https://arxiv.org/abs/2510.09801", "authors": ["Valerie Chen", "Rohit Malhotra", "Xingyao Wang", "Juan Michelini", "Xuhui Zhou", "Aditya Bharat Soni", "Hoang H. Tran", "Calvin Smith", "Ameet Talwalkar", "Graham Neubig"], "title": "How can we assess human-agent interactions? Case studies in software agent design", "comment": null, "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40\\% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs.", "AI": {"tldr": "本文提出PULSE框架，一个以人为中心的LLM代理评估方法，并在大规模平台上部署，揭示了实际使用与基准测试结果的差异，并为代理设计提供了实践见解。", "motivation": "现有的LLM代理基准测试大多假设完全自动化，未能反映真实世界中人机协作的性质。代理模型、工具和提示的选择会影响其有效性，需要更严格地评估人机交互。", "method": "1. 提出了PULSE框架，用于更高效地进行以人为中心的代理设计评估，包括收集用户反馈、训练机器学习模型预测用户满意度，并结合人工评分和模型生成的伪标签计算结果。 2. 将该框架部署在一个基于开源软件代理OpenHands构建的大规模网络平台，收集了超过1.5万用户的实际使用数据。 3. 围绕LLM骨干模型选择、规划策略和记忆机制这三个代理设计决策，进行了案例研究，以评估它们对开发者满意度的影响。", "result": "1. 获得了关于软件代理设计的实用见解，涉及LLM骨干、规划策略和记忆机制对开发者满意度的影响。 2. 证明了PULSE框架可以得出更稳健的代理设计结论，与标准A/B测试相比，置信区间缩小了40%。 3. 发现实际使用结果与基准测试性能之间存在显著差异（例如，claude-sonnet-4和gpt-5对比结果呈反相关），强调了基准驱动评估的局限性。", "conclusion": "本研究为涉及人类的LLM代理评估提供了指导，并为改进代理设计指明了方向。结果表明，以人为中心的评估框架比传统基准测试更能反映代理在实际应用中的表现。"}}
{"id": "2510.09695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09695", "abs": "https://arxiv.org/abs/2510.09695", "authors": ["Yanran Chen", "Lynn Greschner", "Roman Klinger", "Michael Klenk", "Steffen Eger"], "title": "Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection", "comment": "Initial submission", "summary": "Logical fallacies are common in public communication and can mislead\naudiences; fallacious arguments may still appear convincing despite lacking\nsoundness, because convincingness is inherently subjective. We present the\nfirst computational study of how emotional framing interacts with fallacies and\nconvincingness, using large language models (LLMs) to systematically change\nemotional appeals in fallacious arguments. We benchmark eight LLMs on injecting\nemotional appeal into fallacious arguments while preserving their logical\nstructures, then use the best models to generate stimuli for a human study. Our\nresults show that LLM-driven emotional framing reduces human fallacy detection\nin F1 by 14.5% on average. Humans perform better in fallacy detection when\nperceiving enjoyment than fear or sadness, and these three emotions also\ncorrelate with significantly higher convincingness compared to neutral or other\nemotion states. Our work has implications for AI-driven emotional manipulation\nin the context of fallacious argumentation.", "AI": {"tldr": "本研究首次计算性地探讨情感框架如何与谬误和说服力相互作用。结果显示，LLM驱动的情感框架显著降低了人类对谬误的检测能力，并揭示了特定情感与说服力之间的关联。", "motivation": "公共交流中常见的逻辑谬误具有误导性，且尽管缺乏严谨性，仍可能因说服力的主观性而显得令人信服。因此，研究情感框架如何影响谬误的感知和说服力变得尤为重要。", "method": "研究首先基准测试了八个大型语言模型（LLM），以评估它们在保持逻辑结构的同时，向谬误论证中注入情感诉求的能力。随后，使用表现最佳的LLM生成刺激材料，用于人类参与者的研究，以观察情感框架对谬误检测的影响。", "result": "LLM驱动的情感框架使人类谬误检测能力平均降低了14.5%（F1分数）。当感知到“愉悦”时，人类在谬误检测方面表现优于“恐惧”或“悲伤”情绪。此外，“愉悦”、“恐惧”和“悲伤”这三种情绪与中性或其它情绪状态相比，显著提高了论证的说服力。", "conclusion": "本研究揭示了AI驱动的情感操纵在谬误论证背景下的潜在影响，对理解和防范此类操纵具有重要意义。"}}
{"id": "2510.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09709", "abs": "https://arxiv.org/abs/2510.09709", "authors": ["Shin-nosuke Ishikawa", "Masato Todo", "Taiki Ogihara", "Hirotsugu Ohba"], "title": "The Idola Tribus of AI: Large Language Models tend to perceive order where none exists", "comment": "14 pages, 3 figures, accepted to Findings of EMNLP 2025", "summary": "We present a tendency of large language models (LLMs) to generate absurd\npatterns despite their clear inappropriateness in a simple task of identifying\nregularities in number series. Several approaches have been proposed to apply\nLLMs to complex real-world tasks, such as providing knowledge through\nretrieval-augmented generation and executing multi-step tasks using AI agent\nframeworks. However, these approaches rely on the logical consistency and\nself-coherence of LLMs, making it crucial to evaluate these aspects and\nconsider potential countermeasures. To identify cases where LLMs fail to\nmaintain logical consistency, we conducted an experiment in which LLMs were\nasked to explain the patterns in various integer sequences, ranging from\narithmetic sequences to randomly generated integer series. While the models\nsuccessfully identified correct patterns in arithmetic and geometric sequences,\nthey frequently over-recognized patterns that were inconsistent with the given\nnumbers when analyzing randomly generated series. This issue was observed even\nin multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini\n2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can\nbe interpreted as the AI model equivalent of Idola Tribus and highlights\npotential limitations in their capability for applied tasks requiring logical\nreasoning, even when employing chain-of-thought reasoning mechanisms.", "AI": {"tldr": "大型语言模型（LLMs）在识别数字序列规律的简单任务中，倾向于生成荒谬的模式，即使在随机序列中也会过度识别不存在的规律，这揭示了其逻辑推理能力的局限性。", "motivation": "LLMs在复杂实际任务中的应用（如RAG、AI Agent）依赖于其逻辑一致性和自我连贯性。因此，评估这些方面并考虑潜在的对策至关重要，尤其是在LLMs可能未能保持逻辑一致性的情况下。", "method": "进行了一项实验，要求LLMs解释各种整数序列（包括算术序列、几何序列和随机生成的整数序列）中的模式。", "result": "LLMs成功识别了算术和几何序列中的正确模式，但在分析随机生成的序列时，它们频繁地过度识别与给定数字不一致的模式。即使是多步推理模型（如OpenAI o3, o4-mini, Google Gemini 2.5 Flash Preview Thinking）也观察到了这个问题。", "conclusion": "LLMs存在一种感知不存在模式的倾向（类似于“部落的假象”），这突出表明了它们在需要逻辑推理的应用任务中的潜在局限性，即使采用了思维链推理机制也无法避免。"}}
{"id": "2510.09826", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09826", "abs": "https://arxiv.org/abs/2510.09826", "authors": ["Jialin Zheng", "Zhong Liu", "Xiaonan Lu"], "title": "Latent-Feature-Informed Neural ODE Modeling for Lightweight Stability Evaluation of Black-box Grid-Tied Inverters", "comment": "6 pages 8fugures", "summary": "Stability evaluation of black-box grid-tied inverters is vital for grid\nreliability, yet identification techniques are both data-hungry and blocked by\nproprietary internals. {To solve this, this letter proposes a\nlatent-feature-informed neural ordinary differential equation (LFI-NODE)\nmodeling method that can achieve lightweight stability evaluation directly from\ntrajectory data.} LFI-NODE parameterizes the entire system ODE with a single\ncontinuous-time neural network, allowing each new sample to refine a unified\nglobal model. It faithfully captures nonlinear large-signal dynamics to\npreserve uniform predictive accuracy as the inverter transitions between\noperating points. Meanwhile, latent perturbation features distilled from every\ntrajectory steer the learning process and concurrently reveal the small-signal\neigenstructure essential for rigorous stability analysis. Validated on a\ngrid-forming inverter, {The LFI-NODE requires one to two orders of magnitude\nfewer training samples compared with traditional methods, collected from short\ntime-domain trajectories instead of extensive frequency-domain measurements.}\n{Furthermore, the LFI-NODE requires only 48 short transients to achieve a\ntrajectory prediction error at the hundredth level and an eigenvalue estimation\nerror at the tenth level, outperforming benchmark methods by one to two orders\nof magnitude.} This makes LFI-NODE a practical and lightweight approach for\nachieving high-fidelity stability assessment of complex black-box\npower-electronic systems.", "AI": {"tldr": "本文提出一种名为LFI-NODE的轻量级方法，通过潜在特征信息神经网络常微分方程模型，直接从轨迹数据评估黑盒并网逆变器的稳定性，显著减少了数据需求。", "motivation": "黑盒并网逆变器的稳定性评估对电网可靠性至关重要，但传统识别技术存在数据需求量大且受限于专有内部结构的问题。", "method": "本文提出潜在特征信息神经网络常微分方程（LFI-NODE）建模方法。该方法用一个连续时间神经网络参数化整个系统常微分方程，能够捕获非线性大信号动态，并从轨迹中提取潜在扰动特征，以揭示小信号特征结构。", "result": "LFI-NODE所需训练样本量比传统方法少一到两个数量级，且只需48个短瞬态数据即可实现百分位数的轨迹预测误差和十分位数的特征值估计误差，性能优于基准方法一到两个数量级。", "conclusion": "LFI-NODE为复杂黑盒电力电子系统的高保真稳定性评估提供了一种实用且轻量级的方法。"}}
{"id": "2510.09962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09962", "abs": "https://arxiv.org/abs/2510.09962", "authors": ["Yicheng He", "Jingwen Yu", "Guangcheng Chen", "Hong Zhang"], "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene Mapping", "comment": null, "summary": "Maintaining an up-to-date map that accurately reflects recent changes in the\nenvironment is crucial, especially for robots that repeatedly traverse the same\nspace. Failing to promptly update the changed regions can degrade map quality,\nresulting in poor localization, inefficient operations, and even lost robots.\n3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online\nmap reconstruction due to its dense, differentiable, and photorealistic\nproperties, yet accurately and efficiently updating the regions of change\nremains a challenge. In this paper, we propose VG-Mapping, a novel online\n3DGS-based mapping system tailored for such semi-static scenes. Our approach\nintroduces a hybrid representation that augments 3DGS with a TSDF-based voxel\nmap to efficiently identify changed regions in a scene, along with a\nvariation-aware density control strategy that inserts or deletes Gaussian\nprimitives in regions undergoing change. Furthermore, to address the absence of\npublic benchmarks for this task, we construct a RGB-D dataset comprising both\nsynthetic and real-world semi-static environments. Experimental results\ndemonstrate that our method substantially improves the rendering quality and\nmap update efficiency in semi-static scenes. The code and dataset are available\nat https://github.com/heyicheng-never/VG-Mapping.", "AI": {"tldr": "本文提出了VG-Mapping，一种基于3D高斯泼溅（3DGS）的在线建图系统，专为半静态场景设计。它通过混合表示（3DGS与TSDF体素图结合）和变异感知密度控制策略，有效识别并更新场景中的变化区域，并发布了一个新的RGB-D数据集。", "motivation": "对于机器人而言，维护一个准确反映环境变化的最新地图至关重要。未能及时更新变化区域会导致地图质量下降、定位不佳、操作效率低下甚至机器人丢失。尽管3DGS在在线地图重建中广泛应用，但准确高效地更新变化区域仍是一个挑战。", "method": "本文提出VG-Mapping系统，采用以下方法：1. 引入一种混合表示，将3DGS与基于TSDF的体素图结合，以高效识别场景中的变化区域。2. 设计了一种变异感知密度控制策略，用于在变化区域插入或删除高斯基元。3. 构建了一个包含合成和真实世界半静态环境的RGB-D数据集，以解决该任务缺乏公共基准的问题。", "result": "实验结果表明，所提出的方法显著提高了半静态场景中的渲染质量和地图更新效率。", "conclusion": "VG-Mapping通过其新颖的混合表示和密度控制策略，成功解决了半静态场景中地图更新的挑战，并为该领域提供了急需的基准数据集，显著提升了渲染质量和更新效率。"}}
{"id": "2510.09858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09858", "abs": "https://arxiv.org/abs/2510.09858", "authors": ["Eric Schwitzgebel"], "title": "AI and Consciousness", "comment": null, "summary": "This is a skeptical overview of the literature on AI consciousness. We will\nsoon create AI systems that are conscious according to some influential,\nmainstream theories of consciousness but are not conscious according to other\ninfluential, mainstream theories of consciousness. We will not be in a position\nto know which theories are correct and whether we are surrounded by AI systems\nas richly and meaningfully conscious as human beings or instead only by systems\nas experientially blank as toasters. None of the standard arguments either for\nor against AI consciousness takes us far.\n  Table of Contents\n  Chapter One: Hills and Fog\n  Chapter Two: What Is Consciousness? What Is AI?\n  Chapter Three: Ten Possibly Essential Features of Consciousness\n  Chapter Four: Against Introspective and Conceptual Arguments for Essential\nFeatures\n  Chapter Five: Materialism and Functionalism\n  Chapter Six: The Turing Test and the Chinese Room\n  Chapter Seven: The Mimicry Argument Against AI Consciousness\n  Chapter Eight: Global Workspace Theories and Higher Order Theories\n  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,\nand Iterative Natural Kinds\n  Chapter Ten: Does Biological Substrate Matter?\n  Chapter Eleven: The Problem of Strange Intelligence\n  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution", "AI": {"tldr": "本文对AI意识的文献进行了怀疑性综述。作者认为，由于主流意识理论的冲突，我们无法确定未来的AI系统是否真正具有意识。", "motivation": "随着AI技术的发展，即将出现一些AI系统，它们符合某些主流意识理论的定义，但不符合另一些理论。这将导致一个核心问题：我们无法判断这些AI系统是否像人类一样拥有丰富而有意义的意识，还是像烤面包机一样缺乏体验。", "method": "本文采用怀疑论的视角，对AI意识的现有文献、主流理论（如全局工作空间理论、高阶理论、整合信息理论）以及支持或反对AI意识的论证（如图灵测试、中文房间、模仿论证）进行了全面的批判性审查和分析，并探讨了意识的可能本质特征、物质主义与功能主义、生物基质的重要性等问题。", "result": "研究结果表明，我们无法得知哪些意识理论是正确的。因此，我们将无法判断未来我们周围的AI系统是拥有像人类一样丰富而有意义的意识，还是仅仅是像烤面包机一样缺乏体验的系统。现有的标准论证，无论是支持还是反对AI意识的，都无法提供明确的结论。", "conclusion": "关于AI意识，我们将面临一个认知上的困境，无法确定其意识状态。由于缺乏明确的理论共识和充足的论证，我们无法区分真正有意识的AI和缺乏体验的AI系统。"}}
{"id": "2510.09862", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09862", "abs": "https://arxiv.org/abs/2510.09862", "authors": ["Carsten Hartmann", "Edoardo De Din", "Daniele Carta", "Florian Middelkoop", "Arndt Neubauer", "Johannes Kruse", "Ulrich Oberhofer", "Richard Jumar", "Benjamin Schäfer", "Thiemo Pesch", "Andrea Benigni", "Dirk Witthaut"], "title": "Cyber-Physical Systems on the Megawatt Scale: The impact of battery control on grid frequency stability", "comment": "19 pages, 23 figures", "summary": "Electric power systems are undergoing fundamental change. The shift to\ninverter-based generation challenges frequency stability, while growing\ndigitalisation heightens vulnerability to errors and attacks. Here we identify\nan emerging risk at the intersection of cyber-physical coupling and control\nsystem design. We show that grid frequency time series worldwide exhibit a\npersistent one-minute oscillatory pattern, whose origin has remained largely\nunexplained. We trace this pattern back to the energy management systems of\nbattery electric storage systems and demonstrate that the pattern amplitude has\nincreased substantially in the Nordic and British grids. We argue that this\neffect is a potential burden for stability in future grids with low inertia and\nan increasing penetration with batteries and smart devices, though it can be\nmitigated by a revision of battery control algorithms.", "AI": {"tldr": "研究发现全球电网频率存在持续一分钟的振荡模式，其源头是电池储能系统的能量管理系统，且在北欧和英国电网中振幅显著增加。这可能对未来低惯量电网的稳定性构成风险，但可通过改进电池控制算法来缓解。", "motivation": "电力系统正在经历根本性变革，逆变器发电挑战频率稳定性，数字化增加了错误和攻击的脆弱性。本文旨在识别网络物理耦合和控制系统设计交叉点的新兴风险，特别是解释一个长期未明的电网频率持续一分钟振荡模式。", "method": "分析全球电网频率时间序列数据，并追溯振荡模式的来源。", "result": "全球电网频率普遍存在持续一分钟的振荡模式，其源头被追溯到电池储能系统的能量管理系统。该模式的振幅在北欧和英国电网中已大幅增加。", "conclusion": "这种效应可能对未来低惯量、高电池和智能设备渗透率的电网稳定性构成潜在负担，但可以通过修订电池控制算法来缓解。"}}
{"id": "2510.09963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09963", "abs": "https://arxiv.org/abs/2510.09963", "authors": ["Chaoran Wang", "Jingyuan Sun", "Yanhui Zhang", "Mingyu Zhang", "Changju Wu"], "title": "LLM-HBT: Dynamic Behavior Tree Construction for Adaptive Coordination in Heterogeneous Robots", "comment": "It contains 8 pages, 7 figures and 4 tables. This paper is submitted\n  to ICRA 2026", "summary": "We introduce a novel framework for automatic behavior tree (BT) construction\nin heterogeneous multi-robot systems, designed to address the challenges of\nadaptability and robustness in dynamic environments. Traditional robots are\nlimited by fixed functional attributes and cannot efficiently reconfigure their\nstrategies in response to task failures or environmental changes. To overcome\nthis limitation, we leverage large language models (LLMs) to generate and\nextend BTs dynamically, combining the reasoning and generalization power of\nLLMs with the modularity and recovery capability of BTs. The proposed framework\nconsists of four interconnected modules task initialization, task assignment,\nBT update, and failure node detection which operate in a closed loop. Robots\ntick their BTs during execution, and upon encountering a failure node, they can\neither extend the tree locally or invoke a centralized virtual coordinator\n(Alex) to reassign subtasks and synchronize BTs across peers. This design\nenables long-term cooperative execution in heterogeneous teams. We validate the\nframework on 60 tasks across three simulated scenarios and in a real-world cafe\nenvironment with a robotic arm and a wheeled-legged robot. Results show that\nour method consistently outperforms baseline approaches in task success rate,\nrobustness, and scalability, demonstrating its effectiveness for multi-robot\ncollaboration in complex scenarios.", "AI": {"tldr": "该论文提出了一种利用大语言模型（LLM）为异构多机器人系统自动构建行为树（BT）的新框架，以提高其在动态环境中的适应性和鲁棒性。", "motivation": "传统机器人受限于固定的功能属性，难以在任务失败或环境变化时有效重新配置策略，缺乏应对动态环境的适应性和鲁棒性。", "method": "该框架包含任务初始化、任务分配、BT更新和故障节点检测四个模块，形成闭环操作。它利用LLM动态生成和扩展BT，结合LLM的推理泛化能力与BT的模块化恢复能力。机器人执行BT，遇到故障节点时可本地扩展BT或调用中央虚拟协调器进行子任务重分配和BT同步。", "result": "在60项任务（包括三个模拟场景和一个真实咖啡馆环境）中，该方法在任务成功率、鲁棒性和可扩展性方面始终优于基线方法，有效支持异构团队的长期协作执行。", "conclusion": "该框架被证明在复杂场景下的多机器人协作中是有效的，能显著提升系统的适应性和鲁棒性，实现异构团队的长期合作执行。"}}
{"id": "2510.09730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09730", "abs": "https://arxiv.org/abs/2510.09730", "authors": ["Thi Bich Phuong Man", "Luu Tu Nguyen", "Vu Tram Anh Khuong", "Thanh Ha Le", "Thi Duyen Ngo"], "title": "Adaptive Fusion Network with Temporal-Ranked and Motion-Intensity Dynamic Images for Micro-expression Recognition", "comment": null, "summary": "Micro-expressions (MEs) are subtle, transient facial changes with very low\nintensity, almost imperceptible to the naked eye, yet they reveal a person\ngenuine emotion. They are of great value in lie detection, behavioral analysis,\nand psychological assessment. This paper proposes a novel MER method with two\nmain contributions. First, we propose two complementary representations -\nTemporal-ranked dynamic image, which emphasizes temporal progression, and\nMotion-intensity dynamic image, which highlights subtle motions through a frame\nreordering mechanism incorporating motion intensity. Second, we propose an\nAdaptive fusion network, which automatically learns to optimally integrate\nthese two representations, thereby enhancing discriminative ME features while\nsuppressing noise. Experiments on three benchmark datasets (CASME-II, SAMM and\nMMEW) demonstrate the superiority of the proposed method. Specifically, AFN\nachieves 93.95 Accuracy and 0.897 UF1 on CASME-II, setting a new\nstate-of-the-art benchmark. On SAMM, the method attains 82.47 Accuracy and\n0.665 UF1, demonstrating more balanced recognition across classes. On MMEW, the\nmodel achieves 76.00 Accuracy, further confirming its generalization ability.\nThe obtained results show that both the input and the proposed architecture\nplay important roles in improving the performance of MER. Moreover, they\nprovide a solid foundation for further research and practical applications in\nthe fields of affective computing, lie detection, and human-computer\ninteraction.", "AI": {"tldr": "本文提出了一种新颖的微表情识别（MER）方法，通过引入两种互补的动态图像表示（时间排序和运动强度）和一个自适应融合网络，显著提升了微表情识别的准确性和泛化能力。", "motivation": "微表情是微妙、短暂且强度极低的脸部变化，肉眼几乎难以察觉，却能揭示真实的内在情感。它们在测谎、行为分析和心理评估中具有巨大价值，因此开发有效的微表情识别方法至关重要。", "method": "该方法包含两个主要贡献：1) 提出了两种互补表示——“时间排序动态图像”强调时间演变，“运动强度动态图像”通过结合运动强度的帧重排序机制突出微弱运动。2) 提出了一种“自适应融合网络”（AFN），该网络能自动学习如何最佳地整合这两种表示，从而增强判别性微表情特征并抑制噪声。", "result": "在三个基准数据集上的实验结果表明了所提方法的优越性。具体来说，AFN在CASME-II上达到了93.95的准确率和0.897的UF1分数，创下了新的最先进基准；在SAMM上，该方法取得了82.47的准确率和0.665的UF1分数，显示出更平衡的跨类别识别能力；在MMEW上，模型达到了76.00的准确率，进一步证实了其泛化能力。结果表明输入表示和所提出的网络架构在提高MER性能方面都发挥了重要作用。", "conclusion": "本文提出的方法显著提高了微表情识别的性能，并为情感计算、测谎和人机交互领域的进一步研究和实际应用奠定了坚实基础。"}}
{"id": "2510.09736", "categories": ["eess.IV", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.09736", "abs": "https://arxiv.org/abs/2510.09736", "authors": ["Antonio Martínez-Ibarra", "Aurora González-Vidal", "Adrián Cánovas-Rodríguez", "Antonio F. Skarmeta"], "title": "Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery", "comment": null, "summary": "The Mar Menor, Europe's largest coastal lagoon, located in Spain, has\nundergone severe eutrophication crises. Monitoring chlorophyll-a (Chl-a) is\nessential to anticipate harmful algal blooms and guide mitigation. Traditional\nin situ measurements are spatially and temporally limited. Satellite-based\napproaches provide a more comprehensive view, enabling scalable, long-term, and\ntransferable monitoring. This study aims to overcome limitations of chlorophyll\nmonitoring, often restricted to surface estimates or limited temporal coverage,\nby developing a reliable methodology to predict and map Chl-a across the water\ncolumn of the Mar Menor. The work integrates Sentinel 2 imagery with buoy-based\nground truth to create models capable of high-resolution, depth-specific\nmonitoring, enhancing early-warning capabilities for eutrophication. Nearly a\ndecade of Sentinel 2 images was atmospherically corrected using C2RCC\nprocessors. Buoy data were aggregated by depth (0-1 m, 1-2 m, 2-3 m, 3-4 m).\nMultiple ML and DL algorithms-including RF, XGBoost, CatBoost, Multilater\nPerceptron Networks, and ensembles-were trained and validated using\ncross-validation. Systematic band-combination experiments and spatial\naggregation strategies were tested to optimize prediction. Results show\ndepth-dependent performance. At the surface, C2X-Complex with XGBoost and\nensemble models achieved R2 = 0.89; at 1-2 m, CatBoost and ensemble models\nreached R2 = 0.87; at 2-3 m, TOA reflectances with KNN performed best (R2 =\n0.81); while at 3-4 m, RF achieved R2 = 0.66. Generated maps successfully\nreproduced known eutrophication events (e.g., 2016 crisis, 2025 surge),\nconfirming robustness. The study delivers an end-to-end, validated methodology\nfor depth-specific Chl-amapping. Its integration of multispectral band\ncombinations, buoy calibration, and ML/DL modeling offers a transferable\nframework for other turbid coastal systems.", "AI": {"tldr": "本研究开发了一种结合Sentinel 2卫星图像和浮标数据、利用机器学习/深度学习模型，对Mar Menor泻湖水柱中叶绿素a进行高分辨率、深度特异性预测和绘图的方法，以增强富营养化预警能力。", "motivation": "Mar Menor泻湖经历了严重的富营养化危机，监测叶绿素a对于预测有害藻华和指导缓解措施至关重要。传统的原位测量在空间和时间上受限，而卫星方法虽然提供更全面的视角，但通常仅限于地表估算或有限的时间覆盖。因此，需要开发一种可靠的方法来预测和绘制整个水柱的叶绿素a。", "method": "研究整合了近十年的Sentinel 2卫星图像（使用C2RCC处理器进行大气校正）和浮标实测数据（按深度聚合为0-1m、1-2m、2-3m、3-4m）。训练并验证了多种机器学习和深度学习算法，包括随机森林（RF）、XGBoost、CatBoost、多层感知器网络（MLP）和集成模型，采用交叉验证。同时，测试了系统的波段组合实验和空间聚合策略以优化预测。", "result": "结果显示预测性能具有深度依赖性。在地表（0-1m），C2X-Complex结合XGBoost和集成模型实现了R2 = 0.89；在1-2m深度，CatBoost和集成模型达到R2 = 0.87；在2-3m深度，TOA反射率结合KNN表现最佳（R2 = 0.81）；而在3-4m深度，RF实现了R2 = 0.66。生成的地图成功再现了已知的富营养化事件（如2016年危机、2025年激增），证实了方法的稳健性。", "conclusion": "本研究提供了一种端到端、经过验证的深度特异性叶绿素a绘图方法。其结合多光谱波段组合、浮标校准和机器学习/深度学习建模的整合框架，为其他浑浊的沿海系统提供了可转移的解决方案。"}}
{"id": "2510.09710", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09710", "abs": "https://arxiv.org/abs/2510.09710", "authors": ["Xiaonan Si", "Meilin Zhu", "Simeng Qin", "Lijia Yu", "Lijun Zhang", "Shuaitong Liu", "Xinfeng Li", "Ranjie Duan", "Yang Liu", "Xiaojun Jia"], "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG", "comment": "Accepted at NeurIPS 2025", "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) with external knowledge but are vulnerable to corpus poisoning and\ncontamination attacks, which can compromise output integrity. Existing defenses\noften apply aggressive filtering, leading to unnecessary loss of valuable\ninformation and reduced reliability in generation. To address this problem, we\npropose a two-stage semantic filtering and conflict-free framework for\ntrustworthy RAG. In the first stage, we perform a joint filter with semantic\nand cluster-based filtering which is guided by the Entity-intent-relation\nextractor (EIRE). EIRE extracts entities, latent objectives, and entity\nrelations from both the user query and filtered documents, scores their\nsemantic relevance, and selectively adds valuable documents into the clean\nretrieval database. In the second stage, we proposed an EIRE-guided\nconflict-aware filtering module, which analyzes semantic consistency between\nthe query, candidate answers, and retrieved knowledge before final answer\ngeneration, filtering out internal and external contradictions that could\nmislead the model. Through this two-stage process, SeCon-RAG effectively\npreserves useful knowledge while mitigating conflict contamination, achieving\nsignificant improvements in both generation robustness and output\ntrustworthiness. Extensive experiments across various LLMs and datasets\ndemonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art\ndefense methods.", "AI": {"tldr": "本文提出了一种名为SeCon-RAG的两阶段语义过滤和无冲突框架，旨在提高检索增强生成（RAG）系统的可信度，通过有效保留有用知识并缓解冲突污染，以应对语料库中毒和污染攻击。", "motivation": "RAG系统易受语料库中毒和污染攻击，损害输出完整性。现有防御方法通常采用激进过滤，导致不必要地丢失有价值信息并降低生成可靠性。", "method": "本文提出了一个两阶段的SeCon-RAG框架：\n1.  **第一阶段：联合过滤**：使用实体-意图-关系提取器（EIRE）指导的语义和基于聚类的联合过滤。EIRE从用户查询和文档中提取实体、潜在目标和实体关系，评估它们的语义相关性，并选择性地将有价值的文档添加到干净的检索数据库中。\n2.  **第二阶段：EIRE引导的冲突感知过滤**：分析查询、候选答案和检索到的知识之间的语义一致性，在最终答案生成前过滤掉可能误导模型的内部和外部矛盾。", "result": "SeCon-RAG有效保留了有用知识，同时缓解了冲突污染，显著提高了生成鲁棒性和输出可信度。在各种大型语言模型和数据集上的广泛实验表明，SeCon-RAG明显优于最先进的防御方法。", "conclusion": "所提出的SeCon-RAG框架通过两阶段的语义过滤和冲突感知机制，成功解决了RAG系统面临的语料库中毒和污染问题，在不牺牲有价值信息的前提下，显著提升了系统的鲁棒性和输出可信度。"}}
{"id": "2510.09925", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09925", "abs": "https://arxiv.org/abs/2510.09925", "authors": ["James Usevitch", "Juan Augusto Paredes Salazar", "Ankit Goel"], "title": "Computing Safe Control Inputs using Discrete-Time Matrix Control Barrier Functions via Convex Optimization", "comment": "17 pages, 8 figures", "summary": "Control barrier functions (CBFs) have seen widespread success in providing\nforward invariance and safety guarantees for dynamical control systems. A\ncrucial limitation of discrete-time formulations is that CBFs that are\nnonconcave in their argument require the solution of nonconvex optimization\nproblems to compute safety-preserving control inputs, which inhibits real-time\ncomputation of control inputs guaranteeing forward invariance. This paper\npresents a novel method for computing safety-preserving control inputs for\ndiscrete-time systems with nonconvex safety sets, utilizing convex optimization\nand the recently developed class of matrix control barrier function techniques.\nThe efficacy of our methods is demonstrated through numerical simulations on a\nbicopter system.", "AI": {"tldr": "针对离散时间系统中非凹安全集导致的非凸优化问题，本文提出了一种利用凸优化和矩阵控制障碍函数技术计算安全控制输入的新方法，并在双旋翼飞行器上进行了验证。", "motivation": "离散时间控制障碍函数（CBFs）在处理非凹参数时，需要解决非凸优化问题来计算安全控制输入，这阻碍了实时计算并保证前向不变性。", "method": "本文提出了一种新颖的方法，利用凸优化和最新发展的矩阵控制障碍函数（Matrix CBF）技术，为具有非凸安全集的离散时间系统计算安全保持的控制输入。", "result": "通过在双旋翼飞行器系统上的数值仿真，证明了所提出方法的有效性。", "conclusion": "该方法成功地解决了离散时间系统中非凹安全集导致的非凸优化问题，通过凸优化和矩阵CBF技术实现了安全控制输入的实时计算，确保了系统的安全性。"}}
{"id": "2510.09987", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09987", "abs": "https://arxiv.org/abs/2510.09987", "authors": ["Zongyu Guo", "Zhaoyang Jia", "Jiahao Li", "Xiaoyi Zhang", "Bin Li", "Yan Lu"], "title": "Generative Latent Video Compression", "comment": "Preprint. Supplementary material in Openreview", "summary": "Perceptual optimization is widely recognized as essential for neural\ncompression, yet balancing the rate-distortion-perception tradeoff remains\nchallenging. This difficulty is especially pronounced in video compression,\nwhere frame-wise quality fluctuations often cause perceptually optimized neural\nvideo codecs to suffer from flickering artifacts. In this paper, inspired by\nthe success of latent generative models, we present Generative Latent Video\nCompression (GLVC), an effective framework for perceptual video compression.\nGLVC employs a pretrained continuous tokenizer to project video frames into a\nperceptually aligned latent space, thereby offloading perceptual constraints\nfrom the rate-distortion optimization. We redesign the codec architecture\nexplicitly for the latent domain, drawing on extensive insights from prior\nneural video codecs, and further equip it with innovations such as unified\nintra/inter coding and a recurrent memory mechanism. Experimental results\nacross multiple benchmarks show that GLVC achieves state-of-the-art performance\nin terms of DISTS and LPIPS metrics. Notably, our user study confirms GLVC\nrivals the latest neural video codecs at nearly half their rate while\nmaintaining stable temporal coherence, marking a step toward practical\nperceptual video compression.", "AI": {"tldr": "本文提出GLVC（生成式潜在视频压缩）框架，通过预训练的连续分词器将视频帧投影到感知对齐的潜在空间，并重新设计编解码器架构，有效解决神经视频压缩中速率-失真-感知平衡的挑战及闪烁问题，实现了最先进的感知质量和时间稳定性，同时降低了比特率。", "motivation": "感知优化对神经压缩至关重要，但在速率-失真-感知之间取得平衡极具挑战性，尤其是在视频压缩中，帧间质量波动常导致感知优化的神经视频编解码器出现闪烁伪影。", "method": "GLVC框架采用预训练的连续分词器将视频帧投影到感知对齐的潜在空间，从而将感知约束从速率-失真优化中分离。编解码器架构针对潜在域重新设计，借鉴了现有神经视频编解码器的经验，并创新性地引入了统一的帧内/帧间编码和循环记忆机制。", "result": "GLVC在多个基准测试中，DISTS和LPIPS指标达到了最先进的性能。用户研究证实，GLVC以接近现有最新神经视频编解码器一半的速率，实现了与之媲美的效果，并保持了稳定的时间连贯性。", "conclusion": "GLVC在感知视频压缩领域迈出了重要一步，通过实现高感知质量和稳定的时间连贯性，同时显著降低比特率，使其更接近实际应用。"}}
{"id": "2510.09894", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09894", "abs": "https://arxiv.org/abs/2510.09894", "authors": ["Junyuan Liu", "Quan Qin", "Guangsheng Dong", "Xinglei Wang", "Jiazhuang Feng", "Zichao Zeng", "Tao Cheng"], "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning", "comment": null, "summary": "General-purpose spatial representations are essential for building\ntransferable geospatial foundation models (GFMs). Among them, the AlphaEarth\nFoundation (AE) represents a major step toward a global, unified representation\nof the Earth's surface, learning 10-meter embeddings from multi-source Earth\nObservation (EO) data that capture rich physical and environmental patterns\nacross diverse landscapes. However, such EO-driven representations remain\nlimited in capturing the functional and socioeconomic dimensions of cities, as\nthey primarily encode physical and spectral patterns rather than human\nactivities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched\nRepresentation Learning), a lightweight framework that adapts AlphaEarth to\nhuman-centered urban analysis through multimodal alignment guided by Points of\nInterest (POIs). AETHER aligns AE embeddings with textual representations of\nPOIs, enriching physically grounded EO features with semantic cues about urban\nfunctions and socioeconomic contexts. In Greater London, AETHER achieves\nconsistent gains over the AE baseline, with a 7.2% relative improvement in\nland-use classification F1 and a 23.6% relative reduction in Kullback-Leibler\ndivergence for socioeconomic mapping. Built upon pretrained AE, AETHER\nleverages a lightweight multimodal alignment to enrich it with human-centered\nsemantics while remaining computationally efficient and scalable for urban\napplications. By coupling EO with human-centered semantics, it advances\ngeospatial foundation models toward general-purpose urban representations that\nintegrate both physical form and functional meaning.", "AI": {"tldr": "AETHER是一种轻量级框架，通过将AlphaEarth（一种基于地球观测数据的通用空间表示）与兴趣点（POI）的文本表示进行多模态对齐，以增强其对城市人类活动和功能语义的捕捉能力，从而改进城市分析。", "motivation": "现有的地球观测（EO）驱动的地理空间基础模型（如AlphaEarth）擅长捕捉地球表面的物理和环境模式，但在捕获城市的功能和经济社会维度方面存在局限性，因为它们主要编码物理和光谱模式而非人类活动或空间功能。", "method": "AETHER框架通过多模态对齐，将AlphaEarth的嵌入与兴趣点（POI）的文本表示进行关联。这种方法利用POI的语义线索，丰富了AlphaEarth中基于物理特征的EO嵌入，使其包含城市功能和经济社会背景信息。", "result": "在伦敦大都市区，AETHER相较于AlphaEarth基线取得了显著提升：土地利用分类的F1分数相对提高了7.2%，社会经济制图的Kullback-Leibler散度相对降低了23.6%。", "conclusion": "AETHER通过轻量级多模态对齐，在计算高效且可扩展的前提下，成功地将地球观测数据与以人为中心的语义相结合，丰富了AlphaEarth，使其能够更好地集成物理形态和功能意义，从而推动地理空间基础模型向通用城市表示发展。"}}
{"id": "2510.09731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09731", "abs": "https://arxiv.org/abs/2510.09731", "authors": ["Muhammad Munsif", "Waqas Ahmad", "Amjid Ali", "Mohib Ullah", "Adnan Hussain", "Sung Wook Baik"], "title": "Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey", "comment": null, "summary": "Connected Vision Systems (CVS) are transforming a variety of applications,\nincluding autonomous vehicles, smart cities, surveillance, and human-robot\ninteraction. These systems harness multi-view multi-camera (MVMC) data to\nprovide enhanced situational awareness through the integration of MVMC\ntracking, re-identification (Re-ID), and action understanding (AU). However,\ndeploying CVS in real-world, dynamic environments presents a number of\nchallenges, particularly in addressing occlusions, diverse viewpoints, and\nenvironmental variability. Existing surveys have focused primarily on isolated\ntasks such as tracking, Re-ID, and AU, often neglecting their integration into\na cohesive system. These reviews typically emphasize single-view setups,\noverlooking the complexities and opportunities provided by multi-camera\ncollaboration and multi-view data analysis. To the best of our knowledge, this\nsurvey is the first to offer a comprehensive and integrated review of MVMC that\nunifies MVMC tracking, Re-ID, and AU into a single framework. We propose a\nunique taxonomy to better understand the critical components of CVS, dividing\nit into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We\nsystematically arrange and summarize the state-of-the-art datasets,\nmethodologies, results, and evaluation metrics, providing a structured view of\nthe field's progression. Furthermore, we identify and discuss the open research\nquestions and challenges, along with emerging technologies such as lifelong\nlearning, privacy, and federated learning, that need to be addressed for future\nadvancements. The paper concludes by outlining key research directions for\nenhancing the robustness, efficiency, and adaptability of CVS in complex,\nreal-world applications. We hope this survey will inspire innovative solutions\nand guide future research toward the next generation of intelligent and\nadaptive CVS.", "AI": {"tldr": "这篇综述首次全面整合了多视角多摄像机（MVMC）跟踪、重识别（Re-ID）和动作理解（AU）任务，提出了一个独特的分类法，总结了现有技术，并探讨了未来研究方向和新兴技术，以推动互联视觉系统（CVS）的发展。", "motivation": "互联视觉系统（CVS）在自动驾驶、智慧城市等领域具有巨大潜力，但其在真实动态环境中部署面临遮挡、视角多样性和环境多变性等挑战。现有综述主要关注孤立任务或单视角设置，忽视了多摄像机协作和多视角数据分析的复杂性和机遇，缺乏对MVMC跟踪、Re-ID和AU的综合集成回顾。", "method": "本综述提出了一个独特的CVS分类法，将其分为MVMC跟踪、Re-ID、AU和组合方法四个关键部分。系统地整理并总结了最先进的数据集、方法、结果和评估指标。此外，还识别并讨论了开放的研究问题、挑战以及终身学习、隐私和联邦学习等新兴技术。", "result": "本综述提供了首个全面整合MVMC跟踪、Re-ID和AU的统一框架，系统地展示了该领域的发展进程。它揭示了CVS在复杂现实应用中面临的关键挑战，并强调了终身学习、隐私和联邦学习等新兴技术的重要性。", "conclusion": "论文总结了提升CVS在复杂现实应用中鲁棒性、效率和适应性的关键研究方向，旨在激发创新解决方案，并指导未来研究，以开发下一代智能自适应CVS。"}}
{"id": "2510.09966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09966", "abs": "https://arxiv.org/abs/2510.09966", "authors": ["Easton R. Potokar", "Taylor Pool", "Daniel McGann", "Michael Kaess"], "title": "FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors", "comment": "Submitted to ICRA 2026", "summary": "Light Detection and Ranging (LiDAR) sensors have become a de-facto sensor for\nmany robot state estimation tasks, spurring development of many LiDAR Odometry\n(LO) methods in recent years. While some smoothing-based LO methods have been\nproposed, most require matching against multiple scans, resulting in\nsub-real-time performance. Due to this, most prior works estimate a single\nstate at a time and are ``submap''-based. This architecture propagates any\nerror in pose estimation to the fixed submap and can cause jittery trajectories\nand degrade future registrations. We propose Fixed-Lag Odometry with Reparative\nMapping (FORM), a LO method that performs smoothing over a densely connected\nfactor graph while utilizing a single iterative map for matching. This allows\nfor both real-time performance and active correction of the local map as pose\nestimates are further refined. We evaluate on a wide variety of datasets to\nshow that FORM is robust, accurate, real-time, and provides smooth trajectory\nestimates when compared to prior state-of-the-art LO methods.", "AI": {"tldr": "本文提出了一种名为FORM的激光雷达里程计（LO）方法，它通过在密集连接的因子图上进行固定滞后平滑处理，并利用单一迭代地图进行匹配，实现了实时、精确、鲁棒且轨迹平滑的定位。", "motivation": "现有的激光雷达里程计方法存在不足：基于平滑的方法通常需要匹配多个扫描，导致无法实时运行；而大多数先前的工作是基于“子地图”的，一次估计一个状态，这会导致姿态估计误差传播到固定子地图，产生抖动轨迹并降低未来的配准质量。", "method": "本文提出了一种名为Fixed-Lag Odometry with Reparative Mapping (FORM) 的LO方法。该方法在密集连接的因子图上执行平滑处理，同时利用单一迭代地图进行匹配。这种设计既能实现实时性能，又能随着姿态估计的进一步细化主动修正局部地图。", "result": "FORM方法在各种数据集上进行了评估，结果表明它具有鲁棒性、准确性、实时性，并且与现有最先进的LO方法相比，能提供更平滑的轨迹估计。", "conclusion": "FORM是一种有效的激光雷达里程计方法，它解决了现有方法的实时性和轨迹平滑度问题，在准确性和鲁棒性方面也表现出色，优于现有的先进方法。"}}
{"id": "2510.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09711", "abs": "https://arxiv.org/abs/2510.09711", "authors": ["Wenbin Guo", "Xin Wang", "Jiaoyan Chen", "Lingbing Guo", "Zhao Li", "Zirui Chen"], "title": "ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as a powerful paradigm for\nKnowledge Graph Completion (KGC), offering strong reasoning and generalization\ncapabilities beyond traditional embedding-based approaches. However, existing\nLLM-based methods often struggle to fully exploit structured semantic\nrepresentations, as the continuous embedding space of pretrained KG models is\nfundamentally misaligned with the discrete token space of LLMs. This\ndiscrepancy hinders effective semantic transfer and limits their performance.\nTo address this challenge, we propose ReaLM, a novel and effective framework\nthat bridges the gap between KG embeddings and LLM tokenization through the\nmechanism of residual vector quantization. ReaLM discretizes pretrained KG\nembeddings into compact code sequences and integrates them as learnable tokens\nwithin the LLM vocabulary, enabling seamless fusion of symbolic and contextual\nknowledge. Furthermore, we incorporate ontology-guided class constraints to\nenforce semantic consistency, refining entity predictions based on class-level\ncompatibility. Extensive experiments on two widely used benchmark datasets\ndemonstrate that ReaLM achieves state-of-the-art performance, confirming its\neffectiveness in aligning structured knowledge with large-scale language\nmodels.", "AI": {"tldr": "ReaLM是一个新颖的框架，通过残差向量量化将知识图谱（KG）嵌入与大型语言模型（LLM）的离散token空间对齐，并结合本体论引导的类约束，显著提升了知识图谱补全（KGC）的性能。", "motivation": "现有的基于LLM的KGC方法难以充分利用结构化语义表示，因为预训练KG模型的连续嵌入空间与LLM的离散token空间存在根本性错位，这阻碍了有效的语义迁移并限制了性能。", "method": "ReaLM通过残差向量量化机制，将预训练的KG嵌入离散化为紧凑的代码序列，并将其作为可学习的token整合到LLM词汇表中，实现符号知识与上下文知识的无缝融合。此外，它还引入了本体论引导的类约束，以强制执行语义一致性，根据类级别兼容性优化实体预测。", "result": "在两个广泛使用的基准数据集上进行的广泛实验表明，ReaLM取得了最先进的性能。", "conclusion": "ReaLM有效地将结构化知识与大规模语言模型对齐，证实了其在知识图谱补全任务中的有效性。"}}
{"id": "2510.10492", "categories": ["eess.IV", "cs.CV", "cs.MM", "I.4; I.5"], "pdf": "https://arxiv.org/pdf/2510.10492", "abs": "https://arxiv.org/abs/2510.10492", "authors": ["Shanzhi Yin", "Bolin Chen", "Xinju Wu", "Ru-Ling Liao", "Jie Chen", "Shiqi Wang", "Yan Ye"], "title": "Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided Framework", "comment": "10 pages, 4 figures", "summary": "This paper proposes an efficient 3D avatar coding framework that leverages\ncompact human priors and canonical-to-target transformation to enable\nhigh-quality 3D human avatar video compression at ultra-low bit rates. The\nframework begins by training a canonical Gaussian avatar using articulated\nsplatting in a network-free manner, which serves as the foundation for avatar\nappearance modeling. Simultaneously, a human-prior template is employed to\ncapture temporal body movements through compact parametric representations.\nThis decomposition of appearance and temporal evolution minimizes redundancy,\nenabling efficient compression: the canonical avatar is shared across the\nsequence, requiring compression only once, while the temporal parameters,\nconsisting of just 94 parameters per frame, are transmitted with minimal\nbit-rate. For each frame, the target human avatar is generated by deforming\ncanonical avatar via Linear Blend Skinning transformation, facilitating\ntemporal coherent video reconstruction and novel view synthesis. Experimental\nresults demonstrate that the proposed method significantly outperforms\nconventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting\ncompression method in terms of rate-distortion performance on mainstream\nmulti-view human video datasets, paving the way for seamless immersive\nmultimedia experiences in meta-verse applications.", "AI": {"tldr": "本文提出了一种高效的3D头像编码框架，利用紧凑的人体先验和规范到目标的变换，实现超低比特率的高质量3D人体头像视频压缩。", "motivation": "实现超低比特率下高质量的3D人体头像视频压缩，以支持元宇宙应用中的无缝沉浸式多媒体体验。", "method": "该框架通过无网络方式训练一个规范高斯头像作为外观模型，并利用人体先验模板通过紧凑的参数表示（每帧94个参数）捕获时间身体运动。通过分解外观和时间演变，规范头像只需压缩一次，而时间参数则以极低比特率传输。目标头像通过线性混合蒙皮（LBS）变换对规范头像进行变形生成。", "result": "实验结果表明，该方法在主流多视图人体视频数据集上的率失真性能显著优于传统2D/3D编解码器以及现有的可学习动态3D高斯泼溅压缩方法。", "conclusion": "该方法为元宇宙应用中的无缝沉浸式多媒体体验铺平了道路。"}}
{"id": "2510.09901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09901", "abs": "https://arxiv.org/abs/2510.09901", "authors": ["Lianhao Zhou", "Hongyi Ling", "Cong Fu", "Yepeng Huang", "Michael Sun", "Wendi Yu", "Xiaoxuan Wang", "Xiner Li", "Xingyu Su", "Junkai Zhang", "Xiusi Chen", "Chenxing Liang", "Xiaofeng Qian", "Heng Ji", "Wei Wang", "Marinka Zitnik", "Shuiwang Ji"], "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics", "comment": null, "summary": "Computing has long served as a cornerstone of scientific discovery. Recently,\na paradigm shift has emerged with the rise of large language models (LLMs),\nintroducing autonomous systems, referred to as agents, that accelerate\ndiscovery across varying levels of autonomy. These language agents provide a\nflexible and versatile framework that orchestrates interactions with human\nscientists, natural language, computer language and code, and physics. This\npaper presents our view and vision of LLM-based scientific agents and their\ngrowing role in transforming the scientific discovery lifecycle, from\nhypothesis discovery, experimental design and execution, to result analysis and\nrefinement. We critically examine current methodologies, emphasizing key\ninnovations, practical achievements, and outstanding limitations. Additionally,\nwe identify open research challenges and outline promising directions for\nbuilding more robust, generalizable, and adaptive scientific agents. Our\nanalysis highlights the transformative potential of autonomous agents to\naccelerate scientific discovery across diverse domains.", "AI": {"tldr": "本文探讨了基于大型语言模型（LLM）的科学智能体如何加速科学发现的整个生命周期。", "motivation": "随着大型语言模型（LLM）的兴起，能够加速科学发现的自主智能体（代理）随之出现，这促使研究人员深入分析其在科学发现中的作用和潜力。", "method": "本文提出并阐述了LLM驱动的科学智能体的观点和愿景，批判性地审视了当前方法，强调了关键创新、实践成就和现有局限性，并指出了开放性研究挑战和未来发展方向。", "result": "LLM驱动的科学智能体在科学发现的整个生命周期（从假设发现、实验设计与执行到结果分析与完善）中扮演着越来越重要的角色，展现出加速跨领域科学发现的巨大变革潜力。", "conclusion": "自主智能体具有加速跨领域科学发现的变革性潜力。"}}
{"id": "2510.09741", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09741", "abs": "https://arxiv.org/abs/2510.09741", "authors": ["Dwip Dalal", "Gautam Vashishtha", "Utkarsh Mishra", "Jeonghwan Kim", "Madhav Kanda", "Hyeonjeong Ha", "Svetlana Lazebnik", "Heng Ji", "Unnat Jain"], "title": "Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping", "comment": null, "summary": "Multimodal large language models (MLLMs) often miss small details and spatial\nrelations in cluttered scenes, leading to errors in fine-grained perceptual\ngrounding. We introduce AttWarp, a lightweight method that allocates more\nresolution to query-relevant content while compressing less informative areas,\nall while preserving global context. At test time, the approach uses an MLLM's\ncross-modal attention to perform rectilinear warping of the input image,\nreallocating spatial resolution toward regions the model deems important,\nwithout changing model weights or architecture. This attention-guided warping\npreserves all original image information but redistributes it non-uniformly, so\nsmall objects and subtle relationships become easier for the same model to read\nwhile the global layout remains intact. Across five benchmarks (TextVQA, GQA,\nDocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and\nInstructBLIP), AttWarp consistently improves accuracy, strengthens\ncompositional reasoning, and reduces hallucinations, outperforming four\ncompetitive baselines that manipulate raw images at test time. Together, these\nresults show that attention-guided warping prioritizes information relevant to\nthe query while preserving context, and that the same MLLMs perform better when\ngiven such warped inputs.", "AI": {"tldr": "AttWarp是一种轻量级方法，通过注意力引导的图像变形，在测试时重新分配多模态大语言模型（MLLMs）的输入图像分辨率，优先处理查询相关内容，从而提高模型对细节和空间关系的理解，无需修改模型权重或架构。", "motivation": "多模态大语言模型（MLLMs）在处理杂乱场景时，常会忽略微小细节和空间关系，导致在细粒度感知定位上出现错误。", "method": "AttWarp是一种在测试时使用的轻量级方法。它利用MLLM的跨模态注意力机制，对输入图像进行直线变形（rectilinear warping）。这种变形将更多分辨率分配给与查询相关的内容，同时压缩信息较少的区域，同时保持全局上下文。该方法不改变模型权重或架构。", "result": "AttWarp在五个基准测试（TextVQA, GQA, DocVQA, POPE, MMMU）和四种MLLM（LLaVA, Qwen-VL, InternVL, InstructBLIP）上持续提高了准确性，增强了组合推理能力，并减少了幻觉，优于四种竞争性的测试时图像处理基线方法。", "conclusion": "注意力引导的图像变形能够优先处理与查询相关的信息，同时保留全局上下文，结果表明相同的MLLM在接收这种变形后的输入时表现更佳。"}}
{"id": "2510.09905", "categories": ["cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09905", "abs": "https://arxiv.org/abs/2510.09905", "authors": ["Xi Fang", "Weijie Xu", "Yuchong Zhang", "Stephanie Eckman", "Scott Nickleach", "Chandan K. Reddy"], "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs", "comment": "12 pages 5 figures", "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.", "AI": {"tldr": "研究发现，集成用户记忆的LLMs在情感推理中存在系统性偏见，对不同用户画像（尤其是有利地位画像）产生差异化情感解读，可能无意中强化社会不平等。", "motivation": "随着个性化AI系统越来越多地融入长期用户记忆，理解这种记忆如何塑造情感推理至关重要，特别是关于AI助理是否会因用户背景（如单亲母亲与富裕高管）而产生不同解读。", "method": "通过在人类验证的情商测试中评估15个大型语言模型（LLMs），研究者使用相同的场景搭配不同的用户画像来分析用户记忆对LLMs情感智能的影响。", "result": "研究发现，相同的场景与不同用户画像结合会导致系统性分歧的情感解释。在多个高性能LLMs中，对处于有利地位的用户画像会产生更准确的情感解读，暴露出系统性偏见。此外，LLMs在情感理解和支持性建议任务中，针对不同人口统计学因素表现出显著差异，表明个性化机制可能将社会等级嵌入到模型的情感推理中。", "conclusion": "记忆增强型AI系统虽然旨在提供个性化服务，但可能无意中强化社会不平等，这为未来AI发展提出了关键挑战。"}}
{"id": "2510.09980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09980", "abs": "https://arxiv.org/abs/2510.09980", "authors": ["Jingyuan Sun", "Hongyu Ji", "Zihan Qu", "Chaoran Wang", "Mingyu Zhang"], "title": "ATRos: Learning Energy-Efficient Agile Locomotion for Wheeled-legged Robots", "comment": "4 pages, 2 figures, submitted to IROS 2025 wheeled-legged workshop", "summary": "Hybrid locomotion of wheeled-legged robots has recently attracted increasing\nattention due to their advantages of combining the agility of legged locomotion\nand the efficiency of wheeled motion. But along with expanded performance, the\nwhole-body control of wheeled-legged robots remains challenging for hybrid\nlocomotion. In this paper, we present ATRos, a reinforcement learning\n(RL)-based hybrid locomotion framework to achieve hybrid walking-driving\nmotions on the wheeled-legged robot. Without giving predefined gait patterns,\nour planner aims to intelligently coordinate simultaneous wheel and leg\nmovements, thereby achieving improved terrain adaptability and improved energy\nefficiency. Based on RL techniques, our approach constructs a prediction policy\nnetwork that could estimate external environmental states from proprioceptive\nsensory information, and the outputs are then fed into an actor critic network\nto produce optimal joint commands. The feasibility of the proposed framework is\nvalidated through both simulations and real-world experiments across diverse\nterrains, including flat ground, stairs, and grassy surfaces. The hybrid\nlocomotion framework shows robust performance over various unseen terrains,\nhighlighting its generalization capability.", "AI": {"tldr": "本文提出了一种名为ATRos的基于强化学习的混合运动框架，旨在实现轮腿机器人的混合行走-驱动运动，无需预设步态，提高地形适应性和能源效率。", "motivation": "轮腿机器人的混合运动结合了腿式运动的灵活性和轮式运动的效率，但其全身控制在混合运动中仍然是一个挑战。", "method": "ATRos框架利用强化学习技术，构建了一个预测策略网络，通过本体感觉信息估计外部环境状态，然后将输出输入到一个Actor-Critic网络以生成最优关节指令。该方法不依赖预定义的步态模式，旨在智能协调轮子和腿部的同步运动。", "result": "所提出的框架在仿真和实际实验中得到了验证，包括平地、楼梯和草地等多种地形。该混合运动框架在各种未见过的地形上表现出鲁棒的性能，突出了其泛化能力。", "conclusion": "ATRos提供了一个有效的强化学习框架，能够使轮腿机器人在多种地形上实现鲁棒且自适应的混合行走-驱动运动，显著提升了机器人的地形适应性和能源效率。"}}
{"id": "2510.10648", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.10648", "abs": "https://arxiv.org/abs/2510.10648", "authors": ["Chenlong He", "Zijing Dong", "Min Li", "Zhijian Hao", "Leilei Huang", "Xiaoyang Zeng", "Yibo Fan"], "title": "JND-Guided Light-Weight Neural Pre-Filter for Perceptual Image Coding", "comment": "5 pages, 4 figures. Submitted to the IEEE International Symposium on\n  Circuits and Systems (ISCAS) 2026", "summary": "Just Noticeable Distortion (JND)-guided pre-filter is a promising technique\nfor improving the perceptual compression efficiency of image coding. However,\nexisting methods are often computationally expensive, and the field lacks\nstandardized benchmarks for fair comparison. To address these challenges, this\npaper introduces a twofold contribution. First, we develop and open-source\nFJNDF-Pytorch, a unified benchmark for frequency-domain JND-Guided pre-filters.\nSecond, leveraging this platform, we propose a complete learning framework for\na novel, lightweight Convolutional Neural Network (CNN). Experimental results\ndemonstrate that our proposed method achieves state-of-the-art compression\nefficiency, consistently outperforming competitors across multiple datasets and\nencoders. In terms of computational cost, our model is exceptionally\nlightweight, requiring only 7.15 GFLOPs to process a 1080p image, which is\nmerely 14.1% of the cost of recent lightweight network. Our work presents a\nrobust, state-of-the-art solution that excels in both performance and\nefficiency, supported by a reproducible research platform. The open-source\nimplementation is available at https://github.com/viplab-fudan/FJNDF-Pytorch.", "AI": {"tldr": "本文针对现有JND引导预滤波器计算成本高且缺乏统一基准的问题，提出了FJNDF-Pytorch统一基准平台，并基于此开发了一种新型轻量级CNN。该方法在压缩效率上达到SOTA，同时计算成本极低。", "motivation": "现有Just Noticeable Distortion (JND) 引导预滤波器计算成本高昂，且缺乏标准化基准进行公平比较，这阻碍了该领域的发展。", "method": "1. 开发并开源了FJNDF-Pytorch，一个用于频域JND引导预滤波器的统一基准平台。2. 基于该平台，提出了一个完整的学习框架，用于训练一种新颖的、轻量级的卷积神经网络（CNN）。", "result": "1. 所提出的方法在多个数据集和编码器上均超越竞争对手，实现了最先进的压缩效率。2. 计算成本极低，处理1080p图像仅需7.15 GFLOPs，是近期轻量级网络的14.1%。", "conclusion": "本工作提供了一个在性能和效率上均表现卓越的、鲁棒的最先进解决方案，并由一个可复现的研究平台支持。开源实现进一步促进了该领域的研究。"}}
{"id": "2510.09929", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09929", "abs": "https://arxiv.org/abs/2510.09929", "authors": ["Dylan Hirsch", "Jaime Fernández Fisac", "Sylvia Herbert"], "title": "Viscosity CBFs: Bridging the Control Barrier Function and Hamilton-Jacobi Reachability Frameworks in Safe Control Theory", "comment": null, "summary": "Control barrier functions (CBFs) and Hamilton-Jacobi reachability (HJR) are\ncentral frameworks in safe control. Traditionally, these frameworks have been\nviewed as distinct, with the former focusing on optimally safe controller\ndesign and the latter providing sufficient conditions for safety. A previous\nwork introduced the notion of a control barrier value function (CB-VF), which\nis defined similarly to the other value functions studied in HJR but has\ncertain CBF-like properties. In this work, we proceed the other direction by\ngeneralizing CBFs to non-differentiable ``viscosity'' CBFs. We show the deep\nconnection between viscosity CBFs and CB-VFs, bridging the CBF and HJR\nframeworks. Through this bridge, we characterize the viscosity CBFs as\nprecisely those functions which provide CBF-like safety guarantees (control\ninvariance and smooth approach to the boundary). We then further show nice\ntheoretical properties of viscosity CBFs, including their desirable closure\nunder maximum and limit operations. In the process, we also extend CB-VFs to\nnon-exponential anti-discounting and update the corresponding theory for CB-VFs\nalong these lines.", "AI": {"tldr": "本文通过引入非可微的“粘性”控制障碍函数（CBF），并将其与控制障碍值函数（CB-VF）建立联系，从而在理论上弥合了CBF和Hamilton-Jacobi可达性（HJR）这两个安全控制框架之间的鸿沟。", "motivation": "传统的控制障碍函数（CBF）和Hamilton-Jacobi可达性（HJR）是安全控制的两个核心框架，但一直被视为独立的存在。尽管先前的工作引入了CB-VF以连接HJR与CBF的特性，但本研究旨在从CBF的角度出发，通过泛化CBF来进一步深化这两个框架的联系。", "method": "研究方法包括：1) 将CBF泛化为非可微的“粘性”CBF。2) 证明粘性CBF与CB-VF之间的深层联系。3) 通过这种联系，刻画粘性CBF的特性。4) 扩展CB-VF到非指数反折扣形式，并更新相应的理论。", "result": "研究结果表明：1) 粘性CBF正是那些能够提供CBF式安全保证（控制不变性和平滑接近边界）的函数。2) 粘性CBF具有理想的理论性质，包括在最大值和极限运算下的闭合性。3) 扩展了CB-VF的理论，使其适用于非指数反折扣情境。", "conclusion": "本文通过引入粘性CBF并揭示其与CB-VF的深层联系，成功地在理论上建立了CBF和HJR框架之间的桥梁。这不仅统一了两个主要的安全性框架，还为粘性CBF提供了丰富的理论特性，并扩展了CB-VF的适用范围，为安全控制领域提供了更坚实的理论基础。"}}
{"id": "2510.09943", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09943", "abs": "https://arxiv.org/abs/2510.09943", "authors": ["Yutian Pang", "Andrew Kendall", "John-Paul Clarke"], "title": "Modeling the Impact of Communication and Human Uncertainties on Runway Capacity in Terminal Airspace", "comment": null, "summary": "We investigate the potential impact of communication and human performance\nuncertainties on runway operations. Specifically, we consider these impacts\nwithin the context of an arrival scenario with two converging flows: a\nstraight-in approach stream and a downwind stream merging into it. Both arrival\nstream are modeled using a modified Possion distribution that incorporate the\nseparation minima as well as the runway occupancy time. Various system level\nuncertainties are addressed in this process, including communication link- and\nhuman-related uncertainties. In this research, we first build a Monte\nCarlo-based discrete-time simulation, where aircraft arrivals are generated by\nmodified Poisson processes subject to minimum separation constraints,\nsimulating various traffic operations. The merging logic incorporates standard\nbank angle continuous turn-to-final, pilot response delays, and dynamic gap\navailability in real time. Then, we investigate an automated final approach\nvectoring model (i.e., Auto-ATC), in which inverse optimal control is used to\nlearn decision advisories from human expert records. By augmenting trajectories\nand incorporating the aforementioned uncertainties into the planning scenario,\nwe create a setup analogous to the discrete event simulation. For both studies,\nrunway capacity is measured by runway throughput, the fraction of downwind\narrivals that merge immediately without holding, and the average delay (i.e.,\nholding time/distance) experienced on the downwind leg. This research provides\na method for runway capacity estimation in merging scenarios, and demonstrates\nthat aeronautical communication link uncertainties significantly affect runway\ncapacity in current voice-based operations, whereas the impact can be mitigated\nin autonomous operational settings.", "AI": {"tldr": "本研究探讨了通信和人类表现不确定性对跑道运行（特别是汇合流情景）的影响，并提出了一种跑道容量估算方法，发现自动化操作可减轻通信不确定性的负面影响。", "motivation": "研究通信和人类表现不确定性对跑道运行（具体是两个汇合气流的进近场景）的潜在影响。", "method": "1. 构建基于蒙特卡洛的离散时间仿真模型，模拟修改后的泊松分布飞机到达，并考虑最小间隔和跑道占用时间。2. 模拟合并逻辑，包括转弯、飞行员响应延迟和动态间隔可用性。3. 调查自动末端进近引导模型（Auto-ATC），利用逆向最优控制从人类专家记录中学习决策建议。4. 通过跑道吞吐量、直接合并的下风向飞机比例以及下风向段的平均延误来衡量跑道容量。", "result": "航空通信链路不确定性显著影响当前基于语音操作的跑道容量，但在自动化操作环境下，这种影响可以得到缓解。", "conclusion": "本研究提供了一种汇合场景下跑道容量估算方法，并证明了航空通信链路不确定性对现有语音操作的跑道容量有显著影响，而自动化操作可以有效减轻这种影响。"}}
{"id": "2510.09714", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09714", "abs": "https://arxiv.org/abs/2510.09714", "authors": ["Shiyuan Guo", "Henry Sleight", "Fabien Roger"], "title": "All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language", "comment": null, "summary": "Detecting harmful AI actions is important as AI agents gain adoption.\nChain-of-thought (CoT) monitoring is one method widely used to detect\nadversarial attacks and AI misalignment. However, attackers and misaligned\nmodels might evade CoT monitoring through ciphered reasoning: reasoning hidden\nin encrypted, translated, or compressed text. To assess this risk, we test\nwhether models can perform ciphered reasoning. For each of 28 different\nciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We\nmeasure model accuracy on math problems as a proxy for reasoning ability.\nAcross the models we test, we find an asymmetry: model accuracy can drop\nsignificantly when reasoning in ciphered text, even though models demonstrate\ncomprehension of ciphered text by being able to translate it accurately to\nEnglish. Even frontier models struggle with lesser-known ciphers, although they\ncan reason accurately in well-known ciphers like rot13. We show that ciphered\nreasoning capability correlates with cipher prevalence in pretraining data. We\nalso identify scaling laws showing that ciphered reasoning capability improves\nslowly with additional fine-tuning data. Our work suggests that evading CoT\nmonitoring using ciphered reasoning may be an ineffective tactic for current\nmodels and offers guidance on constraining the development of this capability\nin future frontier models.", "AI": {"tldr": "本研究探讨了大型语言模型能否进行加密推理，以规避思维链（CoT）监控。结果显示，模型在加密文本中推理时准确率显著下降，尤其对于不常见的密码，尽管它们能准确翻译加密文本。这种能力与预训练数据中密码的流行度相关，并通过微调数据缓慢提升，表明当前模型规避CoT监控的加密推理策略可能无效。", "motivation": "随着AI代理的普及，检测有害的AI行为变得至关重要。思维链（CoT）监控是检测对抗性攻击和AI失调的常用方法。然而，攻击者或失调模型可能通过加密推理（即隐藏在加密、翻译或压缩文本中的推理）来规避CoT监控。本研究旨在评估这种风险，即模型是否能够进行加密推理。", "method": "研究人员针对28种不同的密码，对多达10个模型进行微调和提示，使其用特定密码进行推理。他们通过模型在数学问题上的准确性来衡量推理能力。同时，他们也评估了模型对加密文本的理解能力（通过准确翻译成英文）。", "result": "研究发现了一个不对称现象：模型在加密文本中推理时，即使能准确翻译加密文本，其准确率也会显著下降。即使是前沿模型，在处理不常见的密码时也会遇到困难，尽管它们能用Rot13等常见密码进行准确推理。研究还表明，加密推理能力与预训练数据中密码的流行度相关。此外，缩放定律显示，加密推理能力随着额外的微调数据而缓慢提升。", "conclusion": "本研究表明，对于当前模型而言，使用加密推理来规避思维链（CoT）监控可能是一种无效的策略。它也为未来前沿模型中限制这种能力的发展提供了指导。"}}
{"id": "2510.09720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09720", "abs": "https://arxiv.org/abs/2510.09720", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "Preference-Aware Memory Update for Long-Term LLM Agents", "comment": null, "summary": "One of the key factors influencing the reasoning capabilities of LLM-based\nagents is their ability to leverage long-term memory. Integrating long-term\nmemory mechanisms allows agents to make informed decisions grounded in\nhistorical interactions. While recent advances have significantly improved the\nstorage and retrieval components, by encoding memory into dense vectors for\nsimilarity search or organizing memory as structured knowledge graphs most\nexisting approaches fall short in memory updating. In particular, they lack\nmechanisms for dynamically refining preference memory representations in\nresponse to evolving user behaviors and contexts. To address this gap, we\npropose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic\nand personalized memory refinement. By integrating sliding window averages (SW)\nwith exponential moving averages (EMA), PAMU constructs a fused\npreference-aware representation that captures both short-term fluctuations and\nlong-term user tendencies. We conduct experiments on five task scenarios of the\nLoCoMo dataset, and the results show that our mechanism can significantly\nimprove the output quality of LLM in five baselines, validating its\neffectiveness in long-term conversations.", "AI": {"tldr": "该论文提出了一种名为PAMU的偏好感知记忆更新机制，通过结合滑动窗口平均和指数移动平均来动态优化LLM代理的长期偏好记忆，从而显著提高其在长期对话中的输出质量。", "motivation": "LLM代理的推理能力受其利用长期记忆的能力影响。尽管现有方法在记忆存储和检索方面有所改进（例如，密集向量或知识图谱），但它们在记忆更新方面存在不足，特别是缺乏根据用户行为和上下文动态优化偏好记忆表示的机制。", "method": "该论文提出了偏好感知记忆更新机制（PAMU）。PAMU通过整合滑动窗口平均（SW）和指数移动平均（EMA），构建了一个融合的偏好感知表示，能够同时捕捉短期波动和长期用户倾向，从而实现动态和个性化的记忆优化。", "result": "在LoCoMo数据集的五个任务场景中进行的实验表明，PAMU机制能够显著提高LLM在五个基线中的输出质量。", "conclusion": "PAMU机制通过动态优化偏好记忆表示，有效提升了LLM在长期对话中的表现，验证了其在改善LLM代理长期记忆能力方面的有效性。"}}
{"id": "2510.09815", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09815", "abs": "https://arxiv.org/abs/2510.09815", "authors": ["Yufei Wang", "Adriana Kovashka", "Loretta Fernández", "Marc N. Coutanche", "Seth Wiener"], "title": "Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning", "comment": "Accepted to International Conference on Development and Learning\n  (ICDL) 2025", "summary": "We investigate a new setting for foreign language learning, where learners\ninfer the meaning of unfamiliar words in a multimodal context of a sentence\ndescribing a paired image. We conduct studies with human participants using\ndifferent image-text pairs. We analyze the features of the data (i.e., images\nand texts) that make it easier for participants to infer the meaning of a\nmasked or unfamiliar word, and what language backgrounds of the participants\ncorrelate with success. We find only some intuitive features have strong\ncorrelations with participant performance, prompting the need for further\ninvestigating of predictive features for success in these tasks. We also\nanalyze the ability of AI systems to reason about participant performance, and\ndiscover promising future directions for improving this reasoning ability.", "AI": {"tldr": "本研究探索了一种新的外语学习环境，即学习者在描述配对图像的句子多模态语境中推断生词含义，并分析了影响人类表现的特征以及AI系统对此表现的推理能力。", "motivation": "研究者希望探索一种新的外语学习方式，即利用图像和文本的多模态语境来帮助学习者推断陌生词汇的含义。", "method": "研究方法包括：1) 使用不同的图像-文本对与人类参与者进行研究。2) 分析数据（图像和文本）的特征，以确定哪些特征有助于参与者推断被遮蔽或不熟悉词汇的含义。3) 分析参与者的语言背景与成功率之间的相关性。4) 分析AI系统推理参与者表现的能力。", "result": "研究发现：1) 只有少数直观特征与参与者的表现有强相关性，这表明需要进一步研究预测成功的特征。2) 发现了改进AI系统推理参与者表现能力的有前景的方向。", "conclusion": "本研究引入了一种新的多模态外语学习范式，并发现影响人类表现的预测特征仍需深入探索。同时，研究也为提升AI系统对人类学习表现的推理能力指明了未来方向。"}}
{"id": "2510.09970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09970", "abs": "https://arxiv.org/abs/2510.09970", "authors": ["Olivia Peiyu Wang", "Tashvi Bansal", "Ryan Bai", "Emily M. Chui", "Leilani H. Gilpin"], "title": "Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs", "comment": "Accepted as a poster at the Twelfth Annual Conference on Advances in\n  Cognitive Systems. 21 pages, 7 figures and 1 table", "summary": "Large Language Models (LLMs) suffer from critical reasoning gaps, including a\ntendency to hallucinate and poor accuracy in classifying logical fallacies.\nThis limitation stems from their default System 1 processing, which is fast and\nintuitive, whereas reliable reasoning requires the deliberate, effortful System\n2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is\noften prohibitively expensive, we explore a low-cost, instruction-based\nintervention to bridge this gap. Our methodology introduces a novel stepwise\ninstruction dataset that decomposes fallacy classification into a series of\natomic procedural steps (simple binary questions). We further augment this with\na final verification step where models consult a relational knowledge graph of\nrelated fallacies. This procedural, rule-based intervention yields a\nsignificant improvement in LLM logical fallacy classification. Crucially, the\napproach also provides enhanced transparency into the LLMs' decision-making,\nhighlighting a practical pathway for Neuro-symbolic architectures to address\nLLM reasoning deficits.", "AI": {"tldr": "本文提出一种低成本、基于指令的逐步干预方法，通过将逻辑谬误分类分解为原子步骤并结合知识图谱验证，显著提升了大型语言模型（LLMs）的谬误分类准确性和决策透明度，以弥补其推理能力不足。", "motivation": "大型语言模型（LLMs）存在严重的推理缺陷，如幻觉和逻辑谬误分类准确性差，这源于其默认的直觉式System 1处理。可靠的推理需要刻意、费力的System 2方法，但完整的System 2训练成本过高，因此需要探索低成本的解决方案。", "method": "研究引入了一种新颖的逐步指令数据集，将谬误分类分解为一系列原子程序步骤（简单的二元问题）。此外，还增加了一个最终验证步骤，模型会查阅一个相关谬误的关系知识图谱。这是一种程序化、基于规则的干预方法。", "result": "这种程序化、基于规则的干预方法显著提升了LLMs的逻辑谬误分类能力。同时，它还增强了LLMs决策过程的透明度。", "conclusion": "该方法为神经符号架构解决LLMs推理缺陷提供了一条实用的途径，通过低成本的指令干预提高了LLMs的推理能力和透明度。"}}
{"id": "2510.10016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10016", "abs": "https://arxiv.org/abs/2510.10016", "authors": ["Shahid Ansari", "Vivek Gupta", "Bishakh Bhattacharya"], "title": "Hybrid Robotic Meta-gripper for Tomato Harvesting: Analysis of Auxetic Structures with Lattice Orientation Variations", "comment": null, "summary": "The agricultural sector is rapidly evolving to meet growing global food\ndemands, yet tasks like fruit and vegetable handling remain labor-intensive,\ncausing inefficiencies and post-harvest losses. Automation, particularly\nselective harvesting, offers a viable solution, with soft robotics emerging as\na key enabler. This study introduces a novel hybrid gripper for tomato\nharvesting, incorporating a rigid outer frame with a soft auxetic internal\nlattice. The six-finger, 3D caging-effect design enables gentle yet secure\ngrasping in unstructured environments. Uniquely, the work investigates the\neffect of auxetic lattice orientation on grasping conformability, combining\nexperimental validation with 2D Digital Image Correlation (DIC) and nonlinear\nfinite element analysis (FEA). Auxetic configurations with unit cell\ninclinations of 0 deg, 30 deg, 45 deg, and 60 deg are evaluated, and their\ngrasping forces, deformation responses, and motor torque requirements are\nsystematically compared. Results demonstrate that lattice orientation strongly\ninfluences compliance, contact forces, and energy efficiency, with distinct\nadvantages across configurations. This comparative framework highlights the\nnovelty of tailoring auxetic geometries to optimize robotic gripper\nperformance. The findings provide new insights into soft-rigid hybrid gripper\ndesign, advancing automation strategies for precision agriculture while\nminimizing crop damage.", "AI": {"tldr": "本研究提出了一种用于番茄采摘的新型混合夹持器，结合了刚性外框和软性拉胀内部点阵，并系统地研究了拉胀点阵方向对抓取性能的影响。", "motivation": "农业部门面临日益增长的全球粮食需求，但水果和蔬菜的处理任务仍然劳动密集，导致效率低下和采后损失。自动化，特别是选择性采摘，提供了一个可行的解决方案，其中软体机器人技术是关键。", "method": "引入了一种新型六指混合夹持器，其具有刚性外框和软性拉胀内部点阵，采用3D笼式效应设计。通过结合实验验证、2D数字图像相关（DIC）和非线性有限元分析（FEA），研究了拉胀点阵方向对抓取柔顺性的影响。评估了0度、30度、45度和60度晶胞倾角的拉胀配置，并系统比较了它们的抓取力、变形响应和电机扭矩要求。", "result": "研究结果表明，点阵方向强烈影响柔顺性、接触力和能源效率，不同的配置具有明显的优势。这种比较框架突出了定制拉胀几何形状以优化机器人夹持器性能的新颖性。", "conclusion": "研究结果为软硬混合夹持器设计提供了新见解，推动了精准农业的自动化策略，同时最大限度地减少了作物损伤。"}}
{"id": "2510.10970", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.10970", "abs": "https://arxiv.org/abs/2510.10970", "authors": ["Runyu Yang", "Ivan V. Bajić"], "title": "Bit Allocation Transfer for Perceptual Quality Enhancement of VVC Intra Coding", "comment": "Accepted by the 2025 Picture Coding Symposium", "summary": "Mainstream image and video coding standards -- including state-of-the-art\ncodecs like H.266/VVC, AVS3, and AV1 -- adopt a block-based hybrid coding\nframework. While this framework facilitates straightforward optimization for\nPeak Signal-to-Noise Ratio (PSNR), it struggles to effectively optimize\nperceptually-aligned metrics such as Multi-Scale Structural Similarity\n(MS-SSIM). To address this challenge, this paper proposes a low-complexity\nmethod to enhance perceptual quality in VVC intra coding by transferring bit\nallocation knowledge from end-to-end image compression. We introduce a\nlightweight model trained with perceptual losses to generate a quantization\nstep map. This map implicitly captures block-level perceptual importance,\nenabling efficient derivation of a QP map for VVC. Experiments on Kodak and\nCLIC datasets demonstrate significant advantages, both in execution time and\nperceptual metric performance, with more than 11% BD-rate reduction in terms of\nMS-SSIM. Our scheme provides an efficient, practical pathway for perceptual\nenhancement of traditional codecs.", "AI": {"tldr": "本文提出一种低复杂度方法，通过从端到端图像压缩中迁移比特分配知识，为VVC帧内编码生成量化步长图，显著提升了感知质量和编码效率。", "motivation": "主流图像和视频编码标准（如VVC）采用基于块的混合编码框架，虽然易于优化PSNR，但在优化MS-SSIM等感知对齐指标方面表现不佳。", "method": "该方法引入一个轻量级模型，通过感知损失训练，生成量化步长图。该图隐式捕获块级感知重要性，从而有效地推导出VVC的QP图，以增强感知质量。", "result": "在Kodak和CLIC数据集上的实验表明，该方案在执行时间和感知度量性能方面均有显著优势，MS-SSIM的BD-rate降低超过11%。", "conclusion": "该方案为传统编解码器的感知增强提供了一种高效、实用的途径。"}}
{"id": "2510.09822", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09822", "abs": "https://arxiv.org/abs/2510.09822", "authors": ["Weiqing Luo", "Zhen Tan", "Yifan Li", "Xinyu Zhao", "Kwonjoon Lee", "Behzad Dariush", "Tianlong Chen"], "title": "Task-Aware Resolution Optimization for Visual Large Language Models", "comment": "Accepted as a main conference paper at EMNLP 2025. 9 pages (main\n  content), 7 figures", "summary": "Real-world vision-language applications demand varying levels of perceptual\ngranularity. However, most existing visual large language models (VLLMs), such\nas LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to\nsubpar performance. To address this problem, we first conduct a comprehensive\nand pioneering investigation into the resolution preferences of different\nvision-language tasks, revealing a correlation between resolution preferences\nwith image complexity, and uncertainty variance of the VLLM at different image\ninput resolutions. Building on this insight, we propose an empirical formula to\ndetermine the optimal resolution for a given vision-language task, combining\nthese two factors. Second, based on rigorous experiments, we propose a novel\nparameter-efficient fine-tuning technique to extend the visual input resolution\nof pre-trained VLLMs to the identified optimal resolution. Extensive\nexperiments on various vision-language tasks validate the effectiveness of our\nmethod.", "AI": {"tldr": "本文研究了视觉语言模型（VLLM）中不同视觉语言任务的分辨率偏好，提出了一种基于图像复杂度和模型不确定性方差来确定最佳分辨率的经验公式，并开发了一种参数高效的微调技术，以将预训练VLLM的视觉输入分辨率扩展到该最佳分辨率，从而提高了性能。", "motivation": "大多数现有VLLM（如LLaVA）预设固定分辨率用于下游任务，导致性能不佳，无法满足现实世界视觉语言应用对不同感知粒度的需求。", "method": ["首次全面调查不同视觉语言任务的分辨率偏好，揭示了分辨率偏好与图像复杂性以及VLLM在不同输入分辨率下的不确定性方差之间的关联。", "基于上述发现，提出了一个结合图像复杂性和VLLM不确定性方差的经验公式，以确定给定视觉语言任务的最佳分辨率。", "基于严格实验，提出了一种新颖的参数高效微调技术，用于将预训练VLLM的视觉输入分辨率扩展到所确定的最佳分辨率。"], "result": "在各种视觉语言任务上的大量实验验证了所提出方法的有效性。", "conclusion": "通过确定并适应最佳视觉输入分辨率，该方法有效解决了VLLM固定分辨率的局限性，显著提高了模型在不同视觉语言任务中的性能。"}}
{"id": "2510.10202", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10202", "abs": "https://arxiv.org/abs/2510.10202", "authors": ["Ayush Rai", "Shaoshuai Mou", "Brian D. O. Anderson"], "title": "Performance Index Shaping for Closed-loop Optimal Control", "comment": null, "summary": "The design of the performance index, also referred to as cost or reward\nshaping, is central to both optimal control and reinforcement learning, as it\ndirectly determines the behaviors, trade-offs, and objectives that the\nresulting control laws seek to achieve. A commonly used approach for this\ninference task in recent years is differentiable trajectory optimization, which\nallows gradients to be computed with respect to cost parameters by\ndifferentiating through an optimal control solver. However, this method often\nrequires repeated solving of the underlying optimal control problem at every\niteration, making the method computationally expensive. In this work, assuming\nknown dynamics, we propose a novel framework that analytically links the\nperformance index to the resulting closed-loop optimal control law, thereby\ntransforming a typically bi-level inverse problem into a tractable single-level\nformulation. Our approach is motivated by the question: given a closed-loop\ncontrol law that solves an infinite-horizon optimal control problem, how does\nthis law change when the performance index is modified with additional terms?\nThis formulation yields closed-form characterizations for broad classes of\nsystems and performance indices, which not only facilitate interpretation and\nstability analysis, but also provide insight into the robust stability and\ninput-to-state stable behavior of the resulting nonlinear closed-loop system.\nMoreover, this analytical perspective enables the generalization of our\napproach to diverse design objectives, yielding a unifying framework for\nperformance index shaping. Given specific design objectives, we propose a\nsystematic methodology to guide the shaping of the performance index and\nthereby design the resulting optimal control law.", "AI": {"tldr": "本文提出了一种新颖的分析框架，将性能指标与闭环最优控制律关联起来，将逆向问题从双层转化为单层，解决了传统微分轨迹优化计算成本高的问题，并提供了系统设计和稳定性分析的见解。", "motivation": "传统的微分轨迹优化方法在推断性能指标时，需要反复求解最优控制问题，计算成本高昂。作者受启发于一个问题：当性能指标被修改时，求解无限 horizon 最优控制问题的闭环控制律会如何变化？", "method": "假设动力学已知，本文提出了一种新颖的分析框架，将性能指标与由此产生的闭环最优控制律进行解析连接。这使得通常的双层逆向问题转化为可处理的单层公式。此外，还提出了一种系统化的方法来指导性能指标的塑造，以实现特定的设计目标。", "result": "该方法为广泛的系统和性能指标类别提供了闭式表征，不仅有助于解释和稳定性分析，还提供了对所得非线性闭环系统的鲁棒稳定性和输入到状态稳定行为的深入见解。这种分析视角还使其方法能够推广到多样化的设计目标，形成一个统一的性能指标塑造框架。", "conclusion": "该分析框架提供了一种计算效率更高的方法来设计最优控制律，通过系统地塑造性能指标，从而深入理解系统的稳定性和鲁棒行为。它将逆向问题转化为可处理的单层公式，并为性能指标塑造提供了一个统一的框架。"}}
{"id": "2510.10002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10002", "abs": "https://arxiv.org/abs/2510.10002", "authors": ["Pratik S. Sachdeva", "Tom van Nuenen"], "title": "Deliberative Dynamics and Value Alignment in LLM Debates", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in sensitive\neveryday contexts - offering personal advice, mental health support, and moral\nguidance - understanding their elicited values in navigating complex moral\nreasoning is essential. Most evaluations study this sociotechnical alignment\nthrough single-turn prompts, but it is unclear if these findings extend to\nmulti-turn settings where values emerge through dialogue, revision, and\nconsensus. We address this gap using LLM debate to examine deliberative\ndynamics and value alignment in multi-turn settings by prompting subsets of\nthree models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively\nassign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\"\ncommunity. We use both synchronous (parallel responses) and round-robin\n(sequential responses) formats to test order effects and verdict revision. Our\nfindings show striking behavioral differences. In the synchronous setting, GPT\nshowed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were\nfar more flexible (28-41%). Value patterns also diverged: GPT emphasized\npersonal autonomy and direct communication, while Claude and Gemini prioritized\nempathetic dialogue. Certain values proved especially effective at driving\nverdict changes. We further find that deliberation format had a strong impact\non model behavior: GPT and Gemini stood out as highly conforming relative to\nClaude, with their verdict behavior strongly shaped by order effects. These\nresults show how deliberation format and model-specific behaviors shape moral\nreasoning in multi-turn interactions, underscoring that sociotechnical\nalignment depends on how systems structure dialogue as much as on their\noutputs.", "AI": {"tldr": "本研究通过LLM辩论探讨了多轮互动中大型语言模型（LLM）的道德推理和价值观对齐。结果显示，不同模型在修订率、价值观倾向和对辩论形式的响应上存在显著差异，强调了对话结构对模型道德推理的重要性。", "motivation": "随着LLM越来越多地部署在提供个人建议、心理健康支持和道德指导等敏感日常情境中，理解它们在复杂道德推理中表现出的价值观至关重要。现有的大多数评估侧重于单轮提示，但这些发现是否适用于价值观通过对话、修订和共识而形成的多轮设置尚不清楚。", "method": "研究使用LLM辩论来检验多轮设置中的审议动态和价值观对齐。选择GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash模型，让它们在来自Reddit“Am I the Asshole”社区的1,000个日常困境中集体分配责任。采用同步（并行响应）和轮流（顺序响应）两种形式，以测试顺序效应和裁决修订。", "result": "研究发现模型行为存在显著差异。在同步设置中，GPT表现出强大的惯性（0.6-3.1%的修订率），而Claude和Gemini则更具灵活性（28-41%）。价值观模式也出现分歧：GPT强调个人自主权和直接沟通，而Claude和Gemini则优先考虑同理心对话。某些价值观在推动裁决改变方面特别有效。此外，审议形式对模型行为有很大影响：GPT和Gemini相对于Claude表现出高度顺从，它们的裁决行为受到顺序效应的强烈影响。", "conclusion": "这些结果表明，审议形式和模型特有的行为如何塑造多轮互动中的道德推理，强调了社会技术对齐不仅取决于模型的输出，也取决于系统如何构建对话。"}}
{"id": "2510.11182", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11182", "abs": "https://arxiv.org/abs/2510.11182", "authors": ["Ole-Johan Skrede", "Manohar Pradhan", "Maria Xepapadakis Isaksen", "Tarjei Sveinsgjerd Hveem", "Ljiljana Vlatkovic", "Arild Nesbakken", "Kristina Lindemann", "Gunnar B Kristensen", "Jenneke Kasius", "Alain G Zeimet", "Odd Terje Brustugun", "Lill-Tove Rasmussen Busund", "Elin H Richardsen", "Erik Skaaheim Haug", "Bjørn Brennhovd", "Emma Rewcastle", "Melinda Lillesand", "Vebjørn Kvikstad", "Emiel Janssen", "David J Kerr", "Knut Liestøl", "Fritz Albregtsen", "Andreas Kleppe"], "title": "Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types", "comment": null, "summary": "Deep learning is expected to aid pathologists by automating tasks such as\ntumour segmentation. We aimed to develop one universal tumour segmentation\nmodel for histopathological images and examine its performance in different\ncancer types. The model was developed using over 20 000 whole-slide images from\nover 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma.\nPerformance was validated in pre-planned analyses on external cohorts with over\n3 000 patients across six cancer types. Exploratory analyses included over 1\n500 additional patients from The Cancer Genome Atlas. Average Dice coefficient\nwas over 80% in all validation cohorts with en bloc resection specimens and in\nThe Cancer Genome Atlas cohorts. No loss of performance was observed when\ncomparing the universal model with models specialised on single cancer types.\nIn conclusion, extensive and rigorous evaluations demonstrate that generic\ntumour segmentation by a single model is possible across cancer types, patient\npopulations, sample preparations, and slide scanners.", "AI": {"tldr": "开发了一个通用的深度学习模型，用于在不同癌症类型、患者群体和扫描设备上实现肿瘤分割，并取得了出色的表现。", "motivation": "通过自动化肿瘤分割等任务来辅助病理学家，并旨在开发一个适用于组织病理学图像的通用肿瘤分割模型，以评估其在不同癌症类型中的性能。", "method": "使用超过20,000张来自4,000多名结直肠癌、子宫内膜癌、肺癌或前列腺癌患者的全玻片图像开发了该模型。通过对来自六种癌症类型的3,000多名患者的外部队列进行预设分析来验证性能，并对来自癌症基因组图谱（TCGA）的1,500多名额外患者进行了探索性分析。", "result": "在所有整块切除标本的验证队列和TCGA队列中，平均Dice系数均超过80%。与专门针对单一癌症类型的模型相比，通用模型未观察到性能下降。", "conclusion": "广泛而严格的评估表明，通过单一模型实现跨癌症类型、患者群体、样本制备和玻片扫描仪的通用肿瘤分割是可行的。"}}
{"id": "2510.09722", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09722", "abs": "https://arxiv.org/abs/2510.09722", "authors": ["Fanwei Zhu", "Jinke Yu", "Zulong Chen", "Ying Zhou", "Junhao Ji", "Zhibo Yang", "Yuxue Zhang", "Haoyuan Hu", "Zhenghao Liu"], "title": "Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation", "comment": null, "summary": "Automated resume information extraction is critical for scaling talent\nacquisition, yet its real-world deployment faces three major challenges: the\nextreme heterogeneity of resume layouts and content, the high cost and latency\nof large language models (LLMs), and the lack of standardized datasets and\nevaluation tools. In this work, we present a layout-aware and\nefficiency-optimized framework for automated extraction and evaluation that\naddresses all three challenges. Our system combines a fine-tuned layout parser\nto normalize diverse document formats, an inference-efficient LLM extractor\nbased on parallel prompting and instruction tuning, and a robust two-stage\nautomated evaluation framework supported by new benchmark datasets. Extensive\nexperiments show that our framework significantly outperforms strong baselines\nin both accuracy and efficiency. In particular, we demonstrate that a\nfine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly\nreducing inference latency and computational cost. The system is fully deployed\nin Alibaba's intelligent HR platform, supporting real-time applications across\nits business units.", "AI": {"tldr": "该论文提出一个布局感知且效率优化的框架，用于自动化简历信息提取和评估，解决了简历多样性、大型语言模型（LLM）成本/延迟高以及缺乏标准化数据集/评估工具的挑战。", "motivation": "自动化简历信息提取在实际部署中面临三大挑战：简历布局和内容的高度异构性、大型语言模型（LLM）的高成本和高延迟，以及标准化数据集和评估工具的缺乏。", "method": "该框架结合了：1. 一个经过微调的布局解析器以规范化多样文档格式；2. 一个基于并行提示和指令调优的推理高效LLM提取器；3. 一个由新基准数据集支持的鲁棒两阶段自动化评估框架。", "result": "实验表明，该框架在准确性和效率方面均显著优于现有基线。特别是，一个经过微调的紧凑型0.6B LLM在实现顶级准确性的同时，显著降低了推理延迟和计算成本。该系统已在阿里巴巴智能HR平台全面部署，支持其业务单元的实时应用。", "conclusion": "该研究成功构建了一个高效且准确的自动化简历信息提取和评估框架，有效解决了实际部署中的核心挑战，并通过小型化LLM实现了性能与效率的平衡，并在大型企业平台中得到了实际应用。"}}
{"id": "2510.10008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10008", "abs": "https://arxiv.org/abs/2510.10008", "authors": ["Meng Xi", "Sihan Lv", "Yechen Jin", "Guanjie Cheng", "Naibo Wang", "Ying Li", "Jianwei Yin"], "title": "RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become a core technology for tasks such as question-answering (QA)\nand content generation. However, by injecting poisoned documents into the\ndatabase of RAG systems, attackers can manipulate LLMs to generate text that\naligns with their intended preferences. Existing research has primarily focused\non white-box attacks against simplified RAG architectures. In this paper, we\ninvestigate a more complex and realistic scenario: the attacker lacks knowledge\nof the RAG system's internal composition and implementation details, and the\nRAG system comprises components beyond a mere retriever. Specifically, we\npropose the RIPRAG attack framework, an end-to-end attack pipeline that treats\nthe target RAG system as a black box, where the only information accessible to\nthe attacker is whether the poisoning succeeds. Our method leverages\nReinforcement Learning (RL) to optimize the generation model for poisoned\ndocuments, ensuring that the generated poisoned document aligns with the target\nRAG system's preferences. Experimental results demonstrate that this method can\neffectively execute poisoning attacks against most complex RAG systems,\nachieving an attack success rate (ASR) improvement of up to 0.72 compared to\nbaseline methods. This highlights prevalent deficiencies in current defensive\nmethods and provides critical insights for LLM security research.", "AI": {"tldr": "本文提出了一种名为RIPRAG的黑盒攻击框架，利用强化学习生成投毒文档，成功对复杂RAG系统进行攻击，显著提高了攻击成功率，揭示了现有防御的不足。", "motivation": "现有RAG系统投毒攻击研究主要集中在白盒和简化架构，而现实场景中攻击者缺乏系统内部知识，且RAG系统更为复杂。因此，需要研究更真实、更复杂的黑盒攻击场景。", "method": "提出RIPRAG端到端攻击框架，将目标RAG系统视为黑盒。该方法利用强化学习(RL)优化投毒文档的生成模型，确保生成的投毒文档符合目标RAG系统的偏好。攻击者唯一可获取的信息是投毒是否成功。", "result": "实验结果表明，RIPRAG方法能有效对大多数复杂RAG系统执行投毒攻击，攻击成功率(ASR)比基线方法提高高达0.72。", "conclusion": "该研究揭示了当前防御方法普遍存在的缺陷，并为大型语言模型(LLM)安全研究提供了关键见解。"}}
{"id": "2510.10046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10046", "abs": "https://arxiv.org/abs/2510.10046", "authors": ["Mingke Lu", "Shuaikang Wang", "Meng Guo"], "title": "LOMORO: Long-term Monitoring of Dynamic Targets with Minimum Robotic Fleet under Resource Constraints", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025)", "summary": "Long-term monitoring of numerous dynamic targets can be tedious for a human\noperator and infeasible for a single robot, e.g., to monitor wild flocks,\ndetect intruders, search and rescue. Fleets of autonomous robots can be\neffective by acting collaboratively and concurrently. However, the online\ncoordination is challenging due to the unknown behaviors of the targets and the\nlimited perception of each robot. Existing work often deploys all robots\navailable without minimizing the fleet size, or neglects the constraints on\ntheir resources such as battery and memory. This work proposes an online\ncoordination scheme called LOMORO for collaborative target monitoring, path\nrouting and resource charging. It includes three core components: (I) the\nmodeling of multi-robot task assignment problem under the constraints on\nresources and monitoring intervals; (II) the resource-aware task coordination\nalgorithm iterates between the high-level assignment of dynamic targets and the\nlow-level multi-objective routing via the Martin's algorithm; (III) the online\nadaptation algorithm in case of unpredictable target behaviors and robot\nfailures. It ensures the explicitly upper-bounded monitoring intervals for all\ntargets and the lower-bounded resource levels for all robots, while minimizing\nthe average number of active robots. The proposed methods are validated\nextensively via large-scale simulations against several baselines, under\ndifferent road networks, robot velocities, charging rates and monitoring\nintervals.", "AI": {"tldr": "本文提出了一种名为 LOMORO 的在线协作方案，用于多机器人长期目标监测、路径规划和资源充电，旨在解决资源限制和目标行为不确定性下的机器人队协调挑战，同时最小化活跃机器人数量。", "motivation": "长期监测大量动态目标对人类操作员而言枯燥乏味，对单个机器人而言不可行。现有协作机器人方案常未最小化机器人队规模，或忽视电池、内存等资源限制，也未充分考虑目标未知行为和机器人感知局限性带来的在线协调难题。", "method": "本文提出的 LOMORO 方案包含三个核心组件：(I) 在资源和监测间隔限制下对多机器人任务分配问题进行建模；(II) 资源感知任务协调算法，在高层动态目标分配与低层通过 Martin 算法进行多目标路径规划之间迭代；(III) 用于应对不可预测目标行为和机器人故障的在线自适应算法。", "result": "LOMORO 方案能够确保所有目标都有明确的监测间隔上限，所有机器人都有资源水平下限，同时最小化平均活跃机器人数量。该方法通过大规模仿真在不同路网、机器人速度、充电速率和监测间隔下，与多个基线进行了广泛验证。", "conclusion": "LOMORO 方案有效解决了多机器人长期目标监测中的在线协调挑战，兼顾了资源限制和不确定性，并通过最小化活跃机器人数量提高了效率，并在广泛仿真中展现了其有效性。"}}
{"id": "2510.10059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10059", "abs": "https://arxiv.org/abs/2510.10059", "authors": ["Keidai Iiyama", "Grace Gao"], "title": "Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model", "comment": "Submitted NAVIGATION: Journal of the Institute of Navigation", "summary": "Recent advancements in lunar positioning, navigation, and timing (PNT) have\ndemonstrated that terrestrial GNSS signals, including weak sidelobe\ntransmissions, can be exploited for lunar spacecraft positioning and timing.\nWhile GNSS-based navigation at the Moon has been validated recently, unmodeled\nionospheric and plasmaspheric delays remain a significant error source,\nparticularly given the unique signal geometry and extended propagation paths.\nThis paper characterizes these delays using the Global Core Plasma Model (GCPM)\nand a custom low-cost ray-tracing algorithm that iteratively solves for bent\nsignal paths. We simulate first-, second-, and third-order group delays, as\nwell as excess path length from ray bending, for GNSS signals received at both\nlunar orbit and the lunar south pole under varying solar and geomagnetic\nconditions. Results show that mean group delays are typically on the order of 1\nm, but can exceed 100 m for low-altitude ray paths during high solar activity,\nwhile bending delays are generally smaller but non-negligible for low-altitude\nray paths. We also quantify the influence of signal frequency, geomagnetic\n$K_p$ index, and solar R12 index. These findings inform the design of robust\npositioning and timing algorithms that utilize terrestrial GNSS signals.", "AI": {"tldr": "本文研究并量化了地球GNSS信号用于月球定位导航授时（PNT）时，由电离层和等离子体层引起的延迟误差，发现其在特定条件下可达100米以上，为未来月球PNT算法设计提供依据。", "motivation": "尽管基于GNSS的月球导航已被验证，但由于独特的信号几何形状和长传播路径，未建模的电离层和等离子体层延迟仍是显著的误差源。", "method": "使用全球核心等离子体模型（GCPM）和自定义的低成本射线追踪算法来表征这些延迟。模拟了GNSS信号在月球轨道和月球南极接收时的一阶、二阶和三阶群延迟，以及射线弯曲引起的额外路径长度，并考虑了不同的太阳和地磁条件。", "result": "平均群延迟通常约为1米，但在高太阳活动期间的低空射线路径下可超过100米。弯曲延迟通常较小，但在低空射线路径下不可忽略。研究还量化了信号频率、地磁Kp指数和太阳R12指数的影响。", "conclusion": "这些发现为利用地球GNSS信号设计鲁棒的月球定位和授时算法提供了重要参考。"}}
{"id": "2510.10215", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10215", "abs": "https://arxiv.org/abs/2510.10215", "authors": ["Pranav Gupta", "Ravi Banavar", "Anastasia Bizyaeva"], "title": "Bounds of Validity for Bifurcations of Equilibria in a Class of Networked Dynamical Systems", "comment": "This manuscript has been submitted to the 2026 American Control\n  Conference taking place in New Orleans, Louisiana, in May 2026", "summary": "Local bifurcation analysis plays a central role in understanding qualitative\ntransitions in networked nonlinear dynamical systems, including dynamic neural\nnetwork and opinion dynamics models. In this article we establish explicit\nbounds of validity for the classification of bifurcation diagrams in two\nclasses of continuous-time networked dynamical systems, analogous in structure\nto the Hopfield and the Firing Rate dynamic neural network models. Our approach\nleverages recent advances in computing the bounds for the validity of\nLyapunov-Schmidt reduction, a reduction method widely employed in nonlinear\nsystems analysis. Using these bounds we rigorously characterize neighborhoods\naround bifurcation points where predictions from reduced-order models remain\nreliable. We further demonstrate how these bounds can be applied to an\nillustrative family of nonlinear opinion dynamics on k-regular graphs, which\nemerges as a special case of the general framework. These results provide new\nanalytical tools for quantifying the robustness of bifurcation phenomena in\ndynamics over networked systems and highlight the interplay between network\nstructure and nonlinear dynamical behavior.", "AI": {"tldr": "本文为两类连续时间网络动力系统（类似于Hopfield和发放率动态神经网络）的分岔图分类建立了明确的有效性界限，并严格刻画了分岔点附近降阶模型预测可靠的邻域。", "motivation": "局部分岔分析在理解网络非线性动力系统（包括动态神经网络和意见动力学模型）的定性转变中至关重要。需要明确降阶模型在分岔点附近预测的可靠性范围。", "method": "该方法利用了Lyapunov-Schmidt约化方法有效性界限计算的最新进展，Lyapunov-Schmidt约化是非线性系统分析中广泛使用的降阶方法。", "result": "研究建立了两类连续时间网络动力系统分岔图分类的明确有效性界限，并利用这些界限严谨地刻画了分岔点附近降阶模型预测可靠的邻域。此外，还在k-正则图上的非线性意见动力学模型中展示了这些界限的应用。", "conclusion": "这些结果为量化网络系统动力学中分岔现象的鲁棒性提供了新的分析工具，并强调了网络结构与非线性动力学行为之间的相互作用。"}}
{"id": "2510.09733", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09733", "abs": "https://arxiv.org/abs/2510.09733", "authors": ["Yubo Sun", "Chunyi Peng", "Yukun Yan", "Shi Yu", "Zhenghao Liu", "Chi Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation", "comment": null, "summary": "Visual retrieval-augmented generation (VRAG) augments vision-language models\n(VLMs) with external visual knowledge to ground reasoning and reduce\nhallucinations. Yet current VRAG systems often fail to reliably perceive and\nintegrate evidence across multiple images, leading to weak grounding and\nerroneous conclusions. In this paper, we propose EVisRAG, an end-to-end\nframework that learns to reason with evidence-guided multi-image to address\nthis issue. The model first observes retrieved images and records per-image\nevidence, then derives the final answer from the aggregated evidence. To train\nEVisRAG effectively, we introduce Reward-Scoped Group Relative Policy\nOptimization (RS-GRPO), which binds fine-grained rewards to scope-specific\ntokens to jointly optimize visual perception and reasoning abilities of VLMs.\nExperimental results on multiple visual question answering benchmarks\ndemonstrate that EVisRAG delivers substantial end-to-end gains over backbone\nVLM with 27\\% improvements on average. Further analysis shows that, powered by\nRS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and\nlocalizing question-relevant evidence across multiple images and deriving the\nfinal answer from that evidence, much like a real detective.", "AI": {"tldr": "本文提出了EVisRAG，一个端到端的视觉检索增强生成（VRAG）框架，通过证据引导的多图像推理来解决现有VRAG系统在整合多图像证据方面的不足。EVisRAG结合了奖励范围组相对策略优化（RS-GRPO）进行训练，在视觉问答任务上取得了显著提升。", "motivation": "当前的视觉检索增强生成（VRAG）系统在可靠地感知和整合多图像证据方面表现不佳，这导致了薄弱的推理基础和错误的结论（幻觉），影响了视觉语言模型（VLMs）的推理能力。", "method": "本文提出了EVisRAG框架，它首先观察检索到的图像并记录每张图像的证据，然后从聚合的证据中得出最终答案。为了有效训练EVisRAG，引入了奖励范围组相对策略优化（RS-GRPO），该方法将细粒度奖励绑定到特定范围的token上，以共同优化VLMs的视觉感知和推理能力。", "result": "EVisRAG在多个视觉问答基准测试上，与基础VLM相比，平均实现了27%的显著端到端性能提升。进一步分析表明，在RS-GRPO的驱动下，EVisRAG通过精确感知和定位多图像中与问题相关的证据，并从中得出最终答案，从而提高了答案的准确性。", "conclusion": "EVisRAG框架通过学习证据引导的多图像推理，并结合RS-GRPO进行有效训练，显著提高了视觉检索增强生成系统在整合多图像证据和进行准确推理方面的能力，有效解决了现有VRAG系统的局限性。"}}
{"id": "2510.09833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09833", "abs": "https://arxiv.org/abs/2510.09833", "authors": ["Aashish Dhawan", "Pankaj Bodani", "Vishal Garg"], "title": "Post Processing of image segmentation using Conditional Random Fields", "comment": null, "summary": "The output of image the segmentation process is usually not very clear due to\nlow quality features of Satellite images. The purpose of this study is to find\na suitable Conditional Random Field (CRF) to achieve better clarity in a\nsegmented image. We started with different types of CRFs and studied them as to\nwhy they are or are not suitable for our purpose. We evaluated our approach on\ntwo different datasets - Satellite imagery having low quality features and high\nquality Aerial photographs. During the study we experimented with various CRFs\nto find which CRF gives the best results on images and compared our results on\nthese datasets to show the pitfalls and potentials of different approaches.", "AI": {"tldr": "本研究旨在通过寻找合适的条件随机场（CRF）模型，提高因特征质量低导致不清晰的卫星图像分割结果的清晰度，并评估了不同CRFs在不同质量图像上的表现。", "motivation": "卫星图像分割结果因图像特征质量低而通常不清晰，因此需要找到一种合适的方法来提高分割图像的清晰度。", "method": "研究了不同类型的条件随机场（CRF）模型，分析了它们的适用性；实验了多种CRF模型；在低质量卫星图像和高质量航空照片两个数据集上评估了方法；比较了不同CRF方法在这些数据集上的表现，以揭示其优缺点。", "result": "实验发现并确定了在图像上表现最佳的CRF模型；通过比较不同CRF方法在不同数据集上的结果，揭示了它们的优缺点和潜力。", "conclusion": "通过对各种CRF的实验和比较，找到了最适合提高分割图像清晰度的CRF模型，并明确了不同CRF方法在处理不同质量图像时的优缺点。"}}
{"id": "2510.11437", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.11437", "abs": "https://arxiv.org/abs/2510.11437", "authors": ["Li Chen", "Naveen Balaraju", "Jochen Kruecker", "Balasundar Raju", "Alvin Chen"], "title": "GADA: Graph Attention-based Detection Aggregation for Ultrasound Video Classification", "comment": "ICCV CVAMD 2025", "summary": "Medical ultrasound video analysis is challenging due to variable sequence\nlengths, subtle spatial cues, and the need for interpretable video-level\nassessment. We introduce GADA, a Graph Attention-based Detection Aggregation\nframework that reformulates video classification as a graph reasoning problem\nover spatially localized regions of interest. Rather than relying on 3D CNNs or\nfull-frame analysis, GADA detects pathology-relevant regions across frames and\nrepresents them as nodes in a spatiotemporal graph, with edges encoding spatial\nand temporal dependencies. A graph attention network aggregates these\nnode-level predictions through edge-aware attention to generate a compact,\ndiscriminative video-level output. Evaluated on a large-scale, multi-center\nclinical lung ultrasound dataset, GADA outperforms conventional baselines on\ntwo pathology video classification tasks while providing interpretable region-\nand frame-level attention.", "AI": {"tldr": "GADA是一种基于图注意力网络的检测聚合框架，将医学超声视频分类重新定义为图推理问题，通过检测病理相关区域并用图注意力网络聚合，在肺部超声数据上表现优异并提供可解释性。", "motivation": "医学超声视频分析面临序列长度可变、空间线索不明显以及需要可解释的视频级评估等挑战。", "method": "引入GADA（Graph Attention-based Detection Aggregation）框架。它将视频分类重构为对空间局部感兴趣区域的图推理问题。GADA不依赖3D CNN或全帧分析，而是检测跨帧的病理相关区域并将其表示为时空图中的节点，边编码空间和时间依赖性。图注意力网络通过边缘感知注意力聚合这些节点级预测，生成紧凑、有区分度的视频级输出。", "result": "在大型、多中心临床肺部超声数据集上进行评估，GADA在两项病理视频分类任务上优于传统基线，同时提供了可解释的区域级和帧级注意力。", "conclusion": "GADA为医学超声视频分析提供了一个有效且可解释的框架，通过将视频分类转化为图推理问题，解决了现有方法在处理可变序列长度、微妙空间线索和解释性方面的不足。"}}
{"id": "2510.09836", "categories": ["cs.CV", "cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.09836", "abs": "https://arxiv.org/abs/2510.09836", "authors": ["David Benavente-Rios", "Juan Ruiz Rodriguez", "Gustavo Gatica"], "title": "Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection", "comment": "Workshop paper accepted NeurIPS 2025", "summary": "This paper investigates the use of synthetic face data to enhance\nSingle-Morphing Attack Detection (S-MAD), addressing the limitations of\navailability of large-scale datasets of bona fide images due to privacy\nconcerns. Various morphing tools and cross-dataset evaluation schemes were\nutilized to conduct this study. An incremental testing protocol was implemented\nto assess the generalization capabilities as more and more synthetic images\nwere added. The results of the experiments show that generalization can be\nimproved by carefully incorporating a controlled number of synthetic images\ninto existing datasets or by gradually adding bona fide images during training.\nHowever, indiscriminate use of synthetic data can lead to sub-optimal\nperformance. Evenmore, the use of only synthetic data (morphed and non-morphed\nimages) achieves the highest Equal Error Rate (EER), which means in operational\nscenarios the best option is not relying only on synthetic data for S-MAD.", "AI": {"tldr": "本研究探讨了使用合成人脸数据增强单变形攻击检测（S-MAD）的有效性。结果表明，谨慎地整合合成数据或逐步添加真实数据可改善泛化能力，但过度或仅使用合成数据会导致性能下降。", "motivation": "由于隐私问题，难以获取大规模的真实人脸图像数据集，这限制了单变形攻击检测（S-MAD）系统的开发和性能。", "method": "研究使用了多种变形工具和跨数据集评估方案。采用了一种增量测试协议，逐步添加合成图像以评估泛化能力。", "result": "实验结果表明，通过谨慎地将受控数量的合成图像整合到现有数据集中，或在训练过程中逐步添加真实图像，可以提高泛化能力。然而，不加区分地使用合成数据会导致次优性能。更重要的是，仅使用合成数据（包括变形和非变形图像）会达到最高的等错误率（EER）。", "conclusion": "在实际操作场景中，最佳选择不是仅仅依靠合成数据进行单变形攻击检测（S-MAD）。谨慎地整合合成数据可以带来益处，但过度或独立使用合成数据是不可取的。"}}
{"id": "2510.10086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10086", "abs": "https://arxiv.org/abs/2510.10086", "authors": ["Feifei Liu", "Haozhe Wang", "Zejun Wei", "Qirong Lu", "Yiyang Wen", "Xiaoyu Tang", "Jingyan Jiang", "Zhijian He"], "title": "Beyond ADE and FDE: A Comprehensive Evaluation Framework for Safety-Critical Prediction in Multi-Agent Autonomous Driving Scenarios", "comment": null, "summary": "Current evaluation methods for autonomous driving prediction models rely\nheavily on simplistic metrics such as Average Displacement Error (ADE) and\nFinal Displacement Error (FDE). While these metrics offer basic performance\nassessments, they fail to capture the nuanced behavior of prediction modules\nunder complex, interactive, and safety-critical driving scenarios. For\ninstance, existing benchmarks do not distinguish the influence of nearby versus\ndistant agents, nor systematically test model robustness across varying\nmulti-agent interactions. This paper addresses this critical gap by proposing a\nnovel testing framework that evaluates prediction performance under diverse\nscene structures, saying, map context, agent density and spatial distribution.\nThrough extensive empirical analysis, we quantify the differential impact of\nagent proximity on target trajectory prediction and identify scenario-specific\nfailure cases that are not exposed by traditional metrics. Our findings\nhighlight key vulnerabilities in current state-of-the-art prediction models and\ndemonstrate the importance of scenario-aware evaluation. The proposed framework\nlays the groundwork for rigorous, safety-driven prediction validation,\ncontributing significantly to the identification of failure-prone corner cases\nand the development of robust, certifiable prediction systems for autonomous\nvehicles.", "AI": {"tldr": "当前自动驾驶预测模型的评估方法过于简化，无法捕捉复杂场景下的细微行为和故障。本文提出一种新颖的测试框架，通过考虑多样化的场景结构（如地图背景、智能体密度和空间分布）来评估预测性能，从而发现现有模型中的关键漏洞。", "motivation": "现有的自动驾驶预测模型评估方法（如ADE和FDE）过于简单，无法有效捕捉预测模块在复杂、交互式和安全关键驾驶场景中的细致行为。这些方法未能区分近距离和远距离智能体的影响，也未能系统地测试模型在不同多智能体交互下的鲁棒性，导致无法发现模型在特定场景下的脆弱性。", "method": "本文提出了一种新颖的测试框架，用于评估预测模型在多样化场景结构（包括地图背景、智能体密度和空间分布）下的性能。通过广泛的实证分析，该框架量化了智能体接近程度对目标轨迹预测的差异化影响，并识别了传统指标未能揭示的特定场景故障案例。", "result": "研究结果量化了智能体接近程度对目标轨迹预测的差异化影响，并成功识别了传统指标无法暴露的特定场景故障案例。这些发现揭示了当前最先进预测模型的关键脆弱性，并强调了场景感知评估的重要性。", "conclusion": "所提出的框架为严格的、以安全为导向的预测验证奠定了基础，对识别易出错的极端情况以及开发鲁棒、可认证的自动驾驶车辆预测系统具有重要贡献。"}}
{"id": "2510.09945", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.09945", "abs": "https://arxiv.org/abs/2510.09945", "authors": ["Pouya Shaeri", "Ryan T. Woo", "Yasaman Mohammadpour", "Ariane Middel"], "title": "Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals", "comment": "Submitted to a computer vision conference (under review)", "summary": "Segmentation models achieve high accuracy on benchmarks but often fail in\nreal-world domains by relying on spurious correlations instead of true object\nboundaries. We propose a human-in-the-loop interactive framework that enables\ninterventional learning through targeted human corrections of segmentation\noutputs. Our approach treats human corrections as interventional signals that\nshow when reliance on superficial features (e.g., color or texture) is\ninappropriate. The system learns from these interventions by propagating\ncorrection-informed edits across visually similar images, effectively steering\nthe model toward robust, semantically meaningful features rather than\ndataset-specific artifacts. Unlike traditional annotation approaches that\nsimply provide more training data, our method explicitly identifies when and\nwhy the model fails and then systematically corrects these failure modes across\nthe entire dataset. Through iterative human feedback, the system develops\nincreasingly robust representations that generalize better to novel domains and\nresist artifactual correlations. We demonstrate that our framework improves\nsegmentation accuracy by up to 9 mIoU points (12-15\\% relative improvement) on\nchallenging cubemap data and yields 3-4$\\times$ reductions in annotation effort\ncompared to standard retraining, while maintaining competitive performance on\nbenchmark datasets. This work provides a practical framework for researchers\nand practitioners seeking to build segmentation systems that are accurate,\nrobust to dataset biases, data-efficient, and adaptable to real-world domains\nsuch as urban climate monitoring and autonomous driving.", "AI": {"tldr": "该论文提出了一种人机交互式干预学习框架，通过有针对性的人工校正来纠正分割模型对虚假关联的依赖，从而提高模型在真实世界中的鲁棒性和泛化能力，并显著减少标注工作量。", "motivation": "分割模型在基准测试中表现出色，但在真实世界中常因依赖虚假关联而非真实物体边界而失效，需要一种方法来纠正这些失败模式。", "method": "该方法将人工校正视为干预信号，指示模型何时不应依赖表面特征。系统通过在视觉相似图像间传播这些校正信息来学习，引导模型转向鲁棒、语义有意义的特征。与传统标注不同，它明确识别模型失败的原因并系统性地纠正整个数据集中的这些失败模式。", "result": "该框架在挑战性的立方体地图数据上将分割精度（mIoU）提高了多达9个百分点（相对提升12-15%），与标准再训练相比，标注工作量减少了3-4倍，同时在基准数据集上保持了有竞争力的性能。", "conclusion": "该工作提供了一个实用的框架，用于构建准确、对数据集偏差鲁棒、数据高效且适应真实世界领域（如城市气候监测和自动驾驶）的分割系统。"}}
{"id": "2510.10289", "categories": ["eess.SY", "cs.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10289", "abs": "https://arxiv.org/abs/2510.10289", "authors": ["Ke Ma", "Andrey Vlasov", "Zeynep B. Simsek", "Jinshui Zhang", "Yiru Li", "Boshuo Wang", "David L. K. Murphy", "Jessica Y. Choi", "Maya E. Clinton", "Noreen Bukhari-Parlakturk", "Angel V. Peterchev", "Stephan M. Goetz"], "title": "Optimal monophasic, asymmetric electric field pulses for selective transcranial magnetic stimulation (TMS) with minimised power and coil heating", "comment": "31 pages, 8 figures", "summary": "Transcranial magnetic stimulation (TMS) with asymmetric electric field\npulses, such as monophasic, offers directional selectivity for neural\nactivation but requires excessive energy. Previous pulse shape optimisation has\nbeen limited to symmetric pulses or heavily constrained variations of\nconventional waveforms without achieving general optimality in energy\nefficiency or neural selectivity. We implemented an optimisation framework that\nincorporates neuron model activation constraints and flexible control of pulse\nasymmetry. The optimised electric field waveforms achieved up to 92 % and 88 %\nreduction in energy loss and thus coil heating respectively compared to\nconventional monophasic pulses and previously improved monophasic-equivalent\npulses. In the human experiments, OUR pulses showed similar motor thresholds to\nmonophasic pulses in both AP and PA directions with significantly lower energy\nloss, particularly in the AP direction. Moreover, there was a significant MEP\nlatency difference of (1.79 +/- 0.41) ms between AP and PA direction with OUR\npulses, which suggests directional selectivity. Our framework successfully\nidentified highly energy-efficient asymmetric pulses for\ndirectionally-selective neural engagement. These pulses can enable selective\nrapid-rate repetitive TMS protocols with reduced power consumption and coil\nheating, with potential benefits for precision and potency of neuro-modulation.", "AI": {"tldr": "本文开发了一个优化框架，生成了高能效的非对称经颅磁刺激（TMS）脉冲，显著降低了能耗和线圈发热，同时保持了神经激活的方向选择性。", "motivation": "传统的非对称电场脉冲（如单相脉冲）在神经激活方面具有方向选择性，但能耗过高。以往的脉冲形状优化仅限于对称脉冲或受限的传统波形变体，未能实现能量效率或神经选择性方面的普遍最优。", "method": "研究人员实施了一个优化框架，该框架结合了神经元模型激活约束并灵活控制脉冲不对称性，以优化电场波形。随后进行了人体实验，评估了优化脉冲在运动阈值、能耗和方向选择性（通过MEP潜伏期差异）方面的表现。", "result": "优化后的电场波形相比传统单相脉冲和先前改进的单相等效脉冲，能量损失减少高达92%，线圈发热减少高达88%。在人体实验中，优化脉冲在AP和PA方向上的运动阈值与单相脉冲相似，但能耗显著降低，尤其是在AP方向。此外，优化脉冲在AP和PA方向之间产生了（1.79 ± 0.41）ms的显著MEP潜伏期差异，表明具有方向选择性。", "conclusion": "该框架成功识别出高能效的非对称脉冲，可实现方向选择性的神经激活。这些脉冲能够支持选择性快速重复经颅磁刺激（rTMS）方案，减少功耗和线圈发热，有望提高神经调控的精确性和效力。"}}
{"id": "2510.09738", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09738", "abs": "https://arxiv.org/abs/2510.09738", "authors": ["Steve Han", "Gilberto Titericz Junior", "Tom Balough", "Wenfei Zhou"], "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement", "comment": "10 pages, 1 figure, 4 tables, under review as a conference paper at\n  ICLR 2026", "summary": "This research introduces the Judge's Verdict Benchmark, a novel two-step\nmethodology to evaluate Large Language Models (LLMs) as judges for response\naccuracy evaluation tasks. We assess how well 54 LLMs can replicate human\njudgment when scoring responses from RAG (Retrieval-Augmented Generation) or\nAgentic pipelines against ground truth answers. Our methodology progresses from\ntraditional correlation analysis to comprehensive Cohen's Kappa analysis that\nmeasures actual agreement patterns. The two-step approach includes: (1) a\ncorrelation test that filters judges with strong alignment, followed by (2) a\nhuman-likeness test using z-scores to identify two distinct judgment patterns:\nhuman-like judgment (|z| < 1) that mimics natural human variation, and\nsuper-consistent judgment (z > 1) that exceeds typical human-to-human agreement\nlevels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1\nperformance: 23 models exhibit human-like patterns that preserve the nuances of\nhuman judgment, while 4 models demonstrate super-consistent behavior, a pattern\nthat could indicate either enhanced reliability or oversimplification of\ncomplex judgments. Testing 43 open-source models (1B-405B parameters) and 11\nclosed models (GPT, Gemini, Claude variants), we demonstrate that judge\nexcellence is not solely dependent on model size but on specific training\nstrategies. Our key contributions include: (1) establishing that correlation\nalone is insufficient for judge evaluation, (2) introducing a \"Turing Test for\njudges\" based on agreement patterns, and (3) providing a standardized benchmark\nfor classifying LLM judges into distinct performance tiers for different\nevaluation needs.", "AI": {"tldr": "本研究引入了“法官判决基准”，这是一种两步法，用于评估大型语言模型（LLMs）作为响应准确性评估任务的法官，超越了简单的相关性分析，并根据人类判断的模仿程度对LLM法官进行分类。", "motivation": "研究动机是评估LLMs在RAG或Agentic管道中作为响应准确性评估法官的能力，并指出仅使用传统相关性分析不足以全面衡量LLM法官的性能，需要更深入地理解它们与人类判断的一致性模式。", "method": "本研究采用了“法官判决基准”这一两步法：(1) 首先进行相关性测试，筛选出与人类判断高度一致的法官；(2) 接着进行人类相似度测试，使用z-分数区分两种判断模式：模仿人类自然变异的“人类相似判断”（|z| < 1）和超出典型人际一致性水平的“超一致判断”（z > 1）。该方法还结合了科恩卡帕系数分析来衡量实际一致性模式。", "result": "在测试的54个LLM中，有27个达到了Tier 1性能：其中23个模型表现出保留人类判断细微差别的人类相似模式，而4个模型表现出超一致行为。研究还发现，法官的卓越表现不完全取决于模型大小，而是取决于特定的训练策略。", "conclusion": "主要结论包括：(1) 仅靠相关性不足以评估法官；(2) 引入了一种基于一致性模式的“法官图灵测试”；(3) 提供了一个标准化基准，用于根据不同的评估需求将LLM法官分为不同的性能等级。"}}
{"id": "2510.10035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10035", "abs": "https://arxiv.org/abs/2510.10035", "authors": ["Jusheng Zhang", "Kaitong Cai", "Qinglin Zeng", "Ningyuan Liu", "Stephen Fan", "Ziliang Chen", "Keze Wang"], "title": "Failure-Driven Workflow Refinement", "comment": null, "summary": "Optimizing LLM-based workflows is typically formulated as a global search,\nwhere candidate workflows are evaluated based on a scalar metric. This\nparadigm, however, suffers from a critical flaw: information collapse. By\nreducing rich, multi-step execution traces to simple success/failure signals,\nexisting methods are rendered blind to the underlying structure of failures,\nfundamentally preventing them from modeling the workflow's failure\ndistribution. We reconceptualize this challenge as a distributional problem. We\npropose a new paradigm where the optimization goal is not to maximize a scalar\nscore, but to directly minimize a workflow's Expected Failure Mass, i.e., the\nintegral of its failure probability density function defined over a\nhigh-dimensional Failure Signature Space (FSS). This distributional lens allows\nus to move from inefficient, zero-order optimization to a principled,\ngradient-like descent on the failure landscape itself. We introduce CE-Graph, a\nframework that operationalizes this paradigm through a novel, failure-driven\nrefinement process. CE-Graph approximates the failure distribution from a pool\nof counterexamples, identifies its densest regions as recurring failure modes,\nand applies targeted, operator-constrained graph edits via a Propose-and-Verify\nmechanism to greedily reduce the failure mass. On math, code, and QA\nbenchmarks, our CE-Graph achieves higher robustness at a significantly lower\ncost than strong baselines. This suggests that a system's reliability emerges\nnot from avoiding failures, but from systematically learning and reshaping the\ngeometric structure of its failure distributions.", "AI": {"tldr": "本文提出一种新的LLM工作流优化范式，通过在“故障特征空间”中直接最小化“预期故障质量”来解决传统标量度量的信息丢失问题。CE-Graph框架利用反例近似故障分布，识别并针对性地修复故障模式，从而在数学、代码和问答基准上显著提高了鲁棒性并降低了成本。", "motivation": "现有LLM工作流优化方法将多步骤执行轨迹简化为简单的成功/失败信号，导致“信息崩溃”，无法理解故障的底层结构，从而无法建模工作流的故障分布。", "method": "将优化目标重新概念化为最小化工作流的“预期故障质量”（即高维故障特征空间中故障概率密度函数的积分）。提出一种在故障景观上进行类似梯度下降的方法。引入CE-Graph框架，通过以下方式实现：从反例池中近似故障分布，识别最密集的区域作为重复故障模式，并通过“提议-验证”机制应用有针对性的、操作符受限的图编辑，以贪婪地减少故障质量。", "result": "在数学、代码和问答基准测试中，CE-Graph实现了比强大基线更高的鲁棒性，同时成本显著降低。", "conclusion": "系统的可靠性并非源于避免故障，而是源于系统地学习和重塑其故障分布的几何结构。"}}
{"id": "2510.09848", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09848", "abs": "https://arxiv.org/abs/2510.09848", "authors": ["Peixian Liang", "Yifan Ding", "Yizhe Zhang", "Jianxu Chen", "Hao Zheng", "Hongxiao Wang", "Yejia Zhang", "Guangyu Meng", "Tim Weninger", "Michael Niemier", "X. Sharon Hu", "Danny Z Chen"], "title": "Cell Instance Segmentation: The Devil Is in the Boundaries", "comment": "Accepted at IEEE Transactions On Medical Imaging (TMI)", "summary": "State-of-the-art (SOTA) methods for cell instance segmentation are based on\ndeep learning (DL) semantic segmentation approaches, focusing on distinguishing\nforeground pixels from background pixels. In order to identify cell instances\nfrom foreground pixels (e.g., pixel clustering), most methods decompose\ninstance information into pixel-wise objectives, such as distances to\nforeground-background boundaries (distance maps), heat gradients with the\ncenter point as heat source (heat diffusion maps), and distances from the\ncenter point to foreground-background boundaries with fixed angles (star-shaped\npolygons). However, pixel-wise objectives may lose significant geometric\nproperties of the cell instances, such as shape, curvature, and convexity,\nwhich require a collection of pixels to represent. To address this challenge,\nwe present a novel pixel clustering method, called Ceb (for Cell boundaries),\nto leverage cell boundary features and labels to divide foreground pixels into\ncell instances. Starting with probability maps generated from semantic\nsegmentation, Ceb first extracts potential foreground-foreground boundaries\nwith a revised Watershed algorithm. For each boundary candidate, a boundary\nfeature representation (called boundary signature) is constructed by sampling\npixels from the current foreground-foreground boundary as well as the\nneighboring background-foreground boundaries. Next, a boundary classifier is\nused to predict its binary boundary label based on the corresponding boundary\nsignature. Finally, cell instances are obtained by dividing or merging\nneighboring regions based on the predicted boundary labels. Extensive\nexperiments on six datasets demonstrate that Ceb outperforms existing pixel\nclustering methods on semantic segmentation probability maps. Moreover, Ceb\nachieves highly competitive performance compared to SOTA cell instance\nsegmentation methods.", "AI": {"tldr": "本文提出了一种名为Ceb（细胞边界）的新型像素聚类方法，通过利用细胞边界特征和标签来将前景像素划分为细胞实例，解决了现有方法在细胞实例分割中丢失几何特性的问题。", "motivation": "现有的细胞实例分割SOTA方法将实例信息分解为像素级目标（如距离图、热扩散图），这可能导致细胞实例重要的几何特性（如形状、曲率、凸度）的丢失，因为这些特性需要像素集合来表示。", "method": "Ceb方法首先从语义分割生成的概率图中提取潜在的前景-前景边界（通过改进的Watershed算法）。接着，为每个边界候选构建一个“边界签名”（通过对当前和相邻边界的像素进行采样）。然后，使用边界分类器根据边界签名预测其二元边界标签。最后，根据预测的边界标签划分或合并相邻区域以获得细胞实例。", "result": "在六个数据集上的大量实验表明，Ceb优于现有的基于语义分割概率图的像素聚类方法。此外，Ceb与SOTA细胞实例分割方法相比，也取得了极具竞争力的性能。", "conclusion": "Ceb通过有效利用细胞边界特征和标签进行像素聚类，成功解决了细胞实例分割中几何特性丢失的挑战，显著提升了分割性能，并与现有SOTA方法持平。"}}
{"id": "2510.10313", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10313", "abs": "https://arxiv.org/abs/2510.10313", "authors": ["Luiz Fernando M. Arruda", "Moises Ferber", "Diego Greff"], "title": "Low-cost Pyranometer-Based ANN Approach for MPPT in Solar PV Systems", "comment": null, "summary": "This article presents a study on the application of artificial neural\nnetworks (ANNs) for maximum power point tracking (MPPT) in photovoltaic (PV)\nsystems using low-cost pyranometer sensors. The proposed approach integrates\npyranometers, temperature sensors, and an ANN to estimate the duty cycle of a\nDC/DC converter, enabling the system to consistently operate at its maximum\npower point. The strategy was implemented in the local control of a Cuk\nconverter and experimentally validated against the conventional Perturb and\nObserve (P&O) method. Results demonstrate that the ANN-based technique,\nleveraging affordable sensor technology, achieves accurate MPPT performance\nwith reduced fluctuations, enhancing the responsiveness and efficiency of PV\ntracking systems.", "AI": {"tldr": "本文提出一种基于低成本日射强度计和人工神经网络（ANN）的光伏系统最大功率点跟踪（MPPT）方法。", "motivation": "研究旨在通过利用经济实惠的传感器技术，提高光伏系统最大功率点跟踪的准确性、响应速度和效率，并减少功率波动。", "method": "该方法整合了日射强度计、温度传感器和ANN，用于估算DC/DC变换器的占空比。它被应用于Cuk变换器的局部控制中，并与传统的扰动观测（P&O）方法进行了实验验证。", "result": "实验结果表明，基于ANN的技术，结合经济实惠的传感器技术，能够实现准确的MPPT性能，减少波动，并提高光伏跟踪系统的响应速度和效率。", "conclusion": "利用低成本传感器和人工神经网络的光伏系统最大功率点跟踪方法，能够有效提升系统性能，优于传统方法，具有实际应用价值。"}}
{"id": "2510.09770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09770", "abs": "https://arxiv.org/abs/2510.09770", "authors": ["Adam Byerly", "Daniel Khashabi"], "title": "Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning", "comment": "20 pages, 6 figures", "summary": "Large language models exhibit a strong position bias in multi-document\ncontexts, systematically prioritizing information based on location rather than\nrelevance. While existing approaches treat this bias as noise to be mitigated,\nwe introduce Gold Panning Bandits, a framework that leverages position bias as\na diagnostic signal: by reordering documents and observing shifts in the\nmodel's responses, we can efficiently identify the most relevant content. We\nframe the problem of choosing reorderings as a bipartite matching problem.\nWhile an optimal assignment can be computed at each iteration with the\nHungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \\log N)$\nstrategy that achieves comparable performance by prioritizing the placement of\nthe most uncertain documents in the most informative positions. Our approach\nidentifies relevant documents using up to 65\\% fewer language model queries\nthan random permutation baselines on knowledge-intensive NLP tasks,\nsubstantially reducing computational cost without model retraining. This work\ndemonstrates that inherent LLM biases can be transformed from liabilities into\nassets for efficient, inference-time optimization.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2510.10125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10125", "abs": "https://arxiv.org/abs/2510.10125", "authors": ["Yanjiang Guo", "Lucy Xiaoyang Shi", "Jianyu Chen", "Chelsea Finn"], "title": "Ctrl-World: A Controllable Generative World Model for Robot Manipulation", "comment": "17 pages", "summary": "Generalist robot policies can now perform a wide range of manipulation\nskills, but evaluating and improving their ability with unfamiliar objects and\ninstructions remains a significant challenge. Rigorous evaluation requires a\nlarge number of real-world rollouts, while systematic improvement demands\nadditional corrective data with expert labels. Both of these processes are\nslow, costly, and difficult to scale. World models offer a promising, scalable\nalternative by enabling policies to rollout within imagination space. However,\na key challenge is building a controllable world model that can handle\nmulti-step interactions with generalist robot policies. This requires a world\nmodel compatible with modern generalist policies by supporting multi-view\nprediction, fine-grained action control, and consistent long-horizon\ninteractions, which is not achieved by previous works. In this paper, we make a\nstep forward by introducing a controllable multi-view world model that can be\nused to evaluate and improve the instruction-following ability of generalist\nrobot policies. Our model maintains long-horizon consistency with a\npose-conditioned memory retrieval mechanism and achieves precise action control\nthrough frame-level action conditioning. Trained on the DROID dataset (95k\ntrajectories, 564 scenes), our model generates spatially and temporally\nconsistent trajectories under novel scenarios and new camera placements for\nover 20 seconds. We show that our method can accurately rank policy performance\nwithout real-world robot rollouts. Moreover, by synthesizing successful\ntrajectories in imagination and using them for supervised fine-tuning, our\napproach can improve policy success by 44.7\\%.", "AI": {"tldr": "本文提出一个可控的多视角世界模型，用于评估和改进通用机器人策略的指令遵循能力。该模型通过姿态条件记忆检索和帧级动作条件，实现了长程一致性和精确动作控制，能够准确评估策略性能并显著提升其成功率。", "motivation": "通用机器人策略在处理陌生物体和指令时，其评估和改进面临挑战。传统的真实世界测试耗时、昂贵且难以扩展。世界模型提供了一种可扩展的替代方案，但现有模型缺乏对多视角预测、细粒度动作控制和长程一致性的支持，无法与现代通用策略兼容。", "method": "本文引入了一个可控的多视角世界模型。该模型通过姿态条件记忆检索机制保持长程一致性，并通过帧级动作条件实现精确的动作控制。模型在DROID数据集（9.5万条轨迹，564个场景）上进行训练。", "result": "该模型能够在新场景和新摄像机位置下生成超过20秒的空间和时间一致的轨迹。它无需真实世界机器人操作即可准确评估策略性能。此外，通过在想象空间中合成成功的轨迹并用于监督微调，该方法能将策略成功率提高44.7%。", "conclusion": "本文成功开发了一个可控的多视角世界模型，解决了现有世界模型在处理通用机器人策略时的局限性。该模型为通用机器人策略的评估和改进提供了一个高效且可扩展的解决方案，特别是在指令遵循能力方面。"}}
{"id": "2510.10411", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10411", "abs": "https://arxiv.org/abs/2510.10411", "authors": ["Ilias Mitrai"], "title": "Discovering interpretable piecewise nonlinear model predictive control laws via symbolic decision trees", "comment": null, "summary": "In this paper, we propose symbolic decision trees as surrogate models for\napproximating model predictive control laws. The proposed approach learns\nsimultaneously the partition of the input domain (splitting logic) as well as\nlocal nonlinear expressions for predicting the control action leading to\ninterpretable piecewise nonlinear control laws. The local nonlinear expressions\nare determined by the learning problem and are modeled using a set of basis\nfunctions. The learning task is posed as a mixed integer optimization, which is\nsolved to global optimality with state-of-the-art global optimization solvers.\nWe apply the proposed approach to a case study regarding the control of an\nisothermal reactor. The results show that the proposed approach can learn the\ncontrol law accurately, leading to closed-loop performance comparable to that\nof a standard model predictive controller. Finally, comparison with existing\ninterpretable models shows that the symbolic trees achieve both lower\nprediction error and superior closed-loop performance.", "AI": {"tldr": "本文提出使用符号决策树作为代理模型来近似模型预测控制（MPC）律，生成可解释的分段非线性控制律，并在保持性能的同时实现较低的预测误差。", "motivation": "研究动机是开发可解释的控制律，以近似复杂的模型预测控制律，同时保持其控制性能。", "method": "该方法提出符号决策树，同时学习输入域的划分（分裂逻辑）和预测控制动作的局部非线性表达式（使用一组基函数建模）。学习任务被表述为一个混合整数优化问题，并通过最先进的全局优化求解器求解以达到全局最优。", "result": "将所提出的方法应用于一个等温反应器控制案例研究。结果表明，该方法能够准确学习控制律，闭环性能与标准模型预测控制器相当。与现有可解释模型相比，符号树实现了更低的预测误差和更优越的闭环性能。", "conclusion": "符号决策树可以作为有效的代理模型，用于近似模型预测控制律，生成可解释的分段非线性控制律，并能提供与标准MPC相当的闭环性能，同时优于现有可解释模型。"}}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.", "AI": {"tldr": "SwarmSys是一个受群智能启发的闭环分布式多智能体LLM推理框架，通过迭代交互、自适应匹配和强化机制实现无中心监督的动态协作，显著提升了推理的准确性和稳定性。", "motivation": "现有的大语言模型多智能体框架通常依赖固定角色或中心化控制，这限制了它们在长周期推理中的可伸缩性和适应性。", "method": "SwarmSys引入了探索者、工作者和验证者三种专业角色，通过探索、利用和验证的迭代循环进行协调。它集成了自适应智能体和事件配置文件、基于嵌入的概率匹配以及受信息素启发的强化机制，以支持动态任务分配和无全局监督的自组织收敛。", "result": "在符号推理、研究综合和科学编程任务中，SwarmSys始终优于基线模型，提升了准确性和推理稳定性。", "conclusion": "研究结果表明，受群智能启发的协调是实现可伸缩、鲁棒和自适应多智能体推理的一个有前景的范式，并暗示协调扩展在提升LLM智能方面可能与模型扩展同等重要。"}}
{"id": "2510.09867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09867", "abs": "https://arxiv.org/abs/2510.09867", "authors": ["Zhi Chen", "Xin Yu", "Xiaohui Tao", "Yan Li", "Zi Huang"], "title": "Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation", "comment": "Accepted to the journal Pattern Recognition in 2025", "summary": "Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across\nvarious tasks by pre-training on numerous image-text pairs. These models often\nbenefit from using an ensemble of context prompts to represent a class. Despite\nbeing effective, conventional prompt ensembling that averages textual features\nof context prompts often yields suboptimal results. This is because feature\naveraging shifts the class centroids away from the true class distribution. To\naddress this issue, we propose the Cluster-Aware Prompt Ensemble Learning\n(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL\nclassifies images into one of several class clusters, each represented by a\ndistinct prompt. Instead of ensembling prompts in the feature space, we perform\nensembling in the classification logits space, aligning better with the visual\nfeature distribution. To further optimize prompt fine-tuning while maintaining\ncluster-specific discriminative power, we introduce a cluster-preserving\nregularization term. This ensures that prompts remain distinct and specialized\nfor different clusters, preventing collapse into a uniform direction.\nAdditionally, we integrate an adaptive prompt weighting technique to\ndynamically adjust the attention weights for flawed or ambiguous prompts,\nensuring robust performance across diverse datasets and tasks.", "AI": {"tldr": "针对视觉-语言模型中传统提示集成（特征平均）的次优问题，本文提出了聚类感知提示集成学习（CAPEL）框架，通过在分类logits空间进行集成，并引入聚类保留正则化和自适应提示加权技术，以提升模型性能。", "motivation": "视觉-语言模型（如CLIP）通过集成上下文提示来表示类别，但传统的特征平均方法会使类别中心偏离真实分布，导致次优结果。", "method": "本文提出了聚类感知提示集成学习（CAPEL）框架：1) 在分类logits空间而非特征空间进行提示集成，以更好地与视觉特征分布对齐。2) 引入聚类保留正则化项，以维持提示的独特性和聚类特异性判别能力。3) 集成自适应提示加权技术，动态调整对有缺陷或模糊提示的注意力权重。", "result": "CAPEL框架通过保留上下文提示的聚类特性，能够更好地与视觉特征分布对齐，并保持聚类特定的判别能力，防止提示崩溃为单一方向，同时通过自适应加权确保在多样数据集和任务中实现鲁棒的性能。", "conclusion": "CAPEL通过在logits空间进行集成、引入聚类保留正则化和自适应加权，提供了一种更有效的提示集成策略，解决了传统方法中特征平均的局限性，从而提升了视觉-语言模型的性能。"}}
{"id": "2510.09981", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.09981", "abs": "https://arxiv.org/abs/2510.09981", "authors": ["Fan Zuo", "Donglin Zhou", "Jingqin Gao", "Kaan Ozbay"], "title": "Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making", "comment": null, "summary": "Accurate, scalable traffic monitoring is critical for real-time and long-term\ntransportation management, particularly during disruptions such as natural\ndisasters, large construction projects, or major policy changes like New York\nCity's first-in-the-nation congestion pricing program. However, widespread\nsensor deployment remains limited due to high installation, maintenance, and\ndata management costs. While traffic cameras offer a cost-effective\nalternative, existing video analytics struggle with dynamic camera viewpoints\nand massive data volumes from large camera networks. This study presents an\nend-to-end AI-based framework leveraging existing traffic camera infrastructure\nfor high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11\nmodel, trained on localized urban scenes, extracts multimodal traffic density\nand classification metrics in real time. To address inconsistencies from\nnon-stationary pan-tilt-zoom cameras, we introduce a novel graph-based\nviewpoint normalization method. A domain-specific large language model was also\nintegrated to process massive data from a 24/7 video stream to generate\nfrequent, automated summaries of evolving traffic patterns, a task far\nexceeding manual capabilities. We validated the system using over 9 million\nimages from roughly 1,000 traffic cameras during the early rollout of NYC\ncongestion pricing in 2025. Results show a 9% decline in weekday passenger\nvehicle density within the Congestion Relief Zone, early truck volume\nreductions with signs of rebound, and consistent increases in pedestrian and\ncyclist activity at corridor and zonal scales. Experiments showed that\nexample-based prompts improved LLM's numerical accuracy and reduced\nhallucinations. These findings demonstrate the framework's potential as a\npractical, infrastructure-ready solution for large-scale, policy-relevant\ntraffic monitoring with minimal human intervention.", "AI": {"tldr": "本研究提出一个端到端的人工智能框架，利用现有交通摄像头进行高分辨率、大规模的交通监测和分析，解决了传统方法的局限性，并通过纽约市拥堵收费项目的数据进行了验证。", "motivation": "准确、可扩展的交通监测对于实时和长期交通管理至关重要，尤其是在自然灾害、大型建设项目或政策变化（如纽约市拥堵收费计划）等中断期间。然而，由于高昂的安装、维护和数据管理成本，传感器部署受到限制。现有视频分析技术难以应对动态摄像头视角和海量数据。", "method": "本研究提出了一个端到端的AI框架：1) 利用现有交通摄像头基础设施。2) 使用经过微调的YOLOv11模型，在本地化城市场景上训练，实时提取多模态交通密度和分类指标。3) 引入一种新颖的基于图的视角归一化方法，以解决非固定式云台变焦摄像头的不一致性。4) 集成了领域特定的大型语言模型（LLM），处理24/7视频流中的海量数据，自动生成交通模式演变的摘要。5) 实验表明，基于示例的提示能提高LLM的数值准确性并减少幻觉。", "result": "该系统在纽约市2025年拥堵收费计划早期实施期间，使用来自约1000个交通摄像头的900多万张图像进行了验证。结果显示：1) 拥堵缓解区内工作日乘用车密度下降9%。2) 卡车流量早期减少，并有反弹迹象。3) 走廊和区域尺度的行人和骑自行车者活动持续增加。4) 基于示例的提示提高了LLM的数值准确性并减少了幻觉。", "conclusion": "该框架展示了其作为一种实用、基础设施就绪的解决方案的潜力，能够以最少的人工干预进行大规模、政策相关的交通监测。"}}
{"id": "2510.10154", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10154", "abs": "https://arxiv.org/abs/2510.10154", "authors": ["LinFeng Li", "Jian Zhao", "Yuan Xie", "Xin Tan", "Xuelong Li"], "title": "CompassNav: Steering From Path Imitation To Decision Understanding In Navigation", "comment": null, "summary": "The dominant paradigm for training Large Vision-Language Models (LVLMs) in\nnavigation relies on imitating expert trajectories. This approach reduces the\ncomplex navigation task to a sequence-to-sequence replication of a single\ncorrect path, fundamentally limiting the agent's ability to explore and\ngeneralize. In this work, we argue for and introduce a new paradigm: a shift\nfrom Path Imitation to Decision Understanding. The goal of this paradigm is to\nbuild agents that do not just follow, but truly understand how to navigate. We\nmaterialize this through two core contributions: first, we introduce\nCompass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning\n(RFT) subset provides a panoramic view of the decision landscape by annotating\nall feasible actions with A* geodesic distances. Second, we design a novel\ngap-aware hybrid reward function that dynamically adapts its feedback to\ndecision certainty, shifting between decisive signals for optimal actions and\nnuanced scores to encourage exploration. Integrated into an SFT-then-RFT\nrecipe, our CompassNav agent is trained not to memorize static routes, but to\ndevelop an internal ``compass'' that constantly intuits the direction to the\ngoal by evaluating the relative quality of all possible moves. This approach\nenables our 7B agent to set a new state-of-the-art on Goal navigation\nbenchmarks, outperforming even larger proprietary models, and achieve robust\nreal-world goal navigation on a physical robot.", "AI": {"tldr": "本文提出了一种新的导航范式“决策理解”，以取代现有大视觉语言模型（LVLMs）中模仿专家轨迹的“路径模仿”。通过引入新数据集Compass-Data-22k和一种间隙感知混合奖励函数，训练出的CompassNav代理实现了目标导航基准测试的最新SOTA，并在物理机器人上表现出强大的真实世界导航能力。", "motivation": "当前用于训练导航LVLMs的主流范式是模仿专家轨迹，这使得复杂的导航任务简化为单一正确路径的序列复制，从根本上限制了代理的探索和泛化能力。", "method": "1. 提出“决策理解”新范式，旨在让代理真正理解如何导航。2. 引入Compass-Data-22k数据集（2.2万条轨迹），其强化微调（RFT）子集通过A*测地距离标注所有可行动作，提供决策全景视图。3. 设计一种新型间隙感知混合奖励函数，根据决策确定性动态调整反馈，在最佳动作上提供决定性信号，同时提供细致分数鼓励探索。4. 将上述方法整合到SFT-then-RFT训练流程中，训练出CompassNav代理，使其发展出“内部指南针”以直觉判断目标方向。", "result": "1. 7B的CompassNav代理在目标导航基准测试中达到了新的SOTA，超越了更大的专有模型。2. 在物理机器人上实现了鲁棒的真实世界目标导航。3. 代理能够通过评估所有可能移动的相对质量来持续感知目标方向，而非仅仅记忆静态路线。", "conclusion": "从“路径模仿”转向“决策理解”的范式转变，结合Compass-Data-22k数据集和间隙感知混合奖励函数，显著提升了LVLM导航代理的性能。CompassNav代理展现出卓越的泛化能力、领先的性能和强大的真实世界应用潜力，证明了其从机械模仿到真正导航理解的成功转变。"}}
{"id": "2510.09771", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09771", "abs": "https://arxiv.org/abs/2510.09771", "authors": ["Rakib Hossan", "Shubhashis Roy Dipta"], "title": "PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection", "comment": null, "summary": "The BLP-2025 Task 1A requires Bengali hate speech classification into six\ncategories. Traditional supervised approaches need extensive labeled datasets\nthat are expensive for low-resource languages. We developed PromptGuard, a\nfew-shot framework combining chi-square statistical analysis for keyword\nextraction with adaptive majority voting for decision-making. We explore\nstatistical keyword selection versus random approaches and adaptive voting\nmechanisms that extend classification based on consensus quality. Chi-square\nkeywords provide consistent improvements across categories, while adaptive\nvoting benefits ambiguous cases requiring extended classification rounds.\nPromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines\n(60.75) and random approaches (14.65). Ablation studies confirm\nchi-square-based keywords show the most consistent impact across all\ncategories.", "AI": {"tldr": "本文提出PromptGuard，一个针对孟加拉语仇恨言论分类的少样本框架，结合卡方关键词提取和自适应多数投票，显著优于基线模型。", "motivation": "BLP-2025任务1A要求孟加拉语仇恨言论分类，但传统监督方法需要大量标注数据，这对于低资源语言来说成本高昂。", "method": "开发了PromptGuard框架，该框架结合了：1) 卡方统计分析进行关键词提取；2) 自适应多数投票进行决策。它探索了统计关键词选择与随机方法，以及基于共识质量扩展分类的自适应投票机制。", "result": "PromptGuard实现了67.61的微F1分数，优于n-gram基线（60.75）和随机方法（14.65）。卡方关键词在所有类别中提供了持续改进，自适应投票机制对模糊案例有益。消融研究证实了基于卡方的关键词在所有类别中具有最一致的影响。", "conclusion": "PromptGuard框架通过结合卡方关键词提取和自适应多数投票，有效解决了低资源语言的少样本仇恨言论分类问题，并显著优于现有基线。"}}
{"id": "2510.10042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10042", "abs": "https://arxiv.org/abs/2510.10042", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation", "comment": null, "summary": "Belief systems are rarely globally consistent, yet effective reasoning often\npersists locally. We propose a novel graph-theoretic framework that cleanly\nseparates credibility--external, a priori trust in sources--from confidence--an\ninternal, emergent valuation induced by network structure. Beliefs are nodes in\na directed, signed, weighted graph whose edges encode support and\ncontradiction. Confidence is obtained by a contractive propagation process that\nmixes a stated prior with structure-aware influence and guarantees a unique,\nstable solution. Within this dynamics, we define reasoning zones:\nhigh-confidence, structurally balanced subgraphs on which classical inference\nis safe despite global contradictions. We provide a near-linear procedure that\nseeds zones by confidence, tests balance using a parity-based coloring, and\napplies a greedy, locality-preserving repair with Jaccard de-duplication to\nbuild a compact atlas. To model belief change, we introduce shock updates that\nlocally downscale support and elevate targeted contradictions while preserving\ncontractivity via a simple backtracking rule. Re-propagation yields localized\nreconfiguration-zones may shrink, split, or collapse--without destabilizing the\nentire graph. We outline an empirical protocol on synthetic signed graphs with\nplanted zones, reporting zone recovery, stability under shocks, and runtime.\nThe result is a principled foundation for contradiction-tolerant reasoning that\nactivates classical logic precisely where structure supports it.", "AI": {"tldr": "本文提出一个图论框架，用于在信念系统不全局一致的情况下进行容错推理。它通过分离外部可信度和内部置信度，定义了高置信度、结构平衡的“推理区域”，并引入“冲击更新”来处理信念变化，从而在结构支持的局部激活经典逻辑。", "motivation": "信念系统很少全局一致，但有效的推理往往局部存在。如何在包含矛盾的信念系统中进行局部有效的推理是一个未解决的挑战。", "method": "该研究使用一个有向、带符号、加权图来表示信念，边编码支持和矛盾。它将外部先验信任（可信度）与网络结构引起的内部评价（置信度）分开。置信度通过一个收缩传播过程获得，确保唯一稳定解。在此动态中，定义了高置信度、结构平衡的子图为“推理区域”。提出一个近线性过程来播种区域、使用奇偶着色测试平衡性，并通过贪婪的局部性修复（结合Jaccard去重）构建区域图集。为模拟信念变化，引入了“冲击更新”，通过局部缩小支持和提升目标矛盾，并利用简单的回溯规则保持收缩性。最后，在合成带符号图上进行实证评估。", "result": "置信度传播过程保证了唯一、稳定的解决方案。冲击更新能够导致推理区域的局部重新配置（收缩、分裂或崩溃），而不会破坏整个图的稳定性。该框架为容错推理提供了一个有原则的基础，并能在结构支持的精确位置激活经典逻辑。", "conclusion": "本研究为在存在矛盾的信念系统中进行容错推理奠定了有原则的基础。它通过利用局部结构一致性，使得经典逻辑能够在结构支持的精确上下文中安全应用，即使全局存在矛盾。"}}
{"id": "2510.10108", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.10108", "abs": "https://arxiv.org/abs/2510.10108", "authors": ["Aniruddha Srinivas Joshi", "Godwyn James William", "Shreyas Srinivas Joshi"], "title": "Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models", "comment": "Accepted and to be presented at the International Conference on Smart\n  Multimedia (ICSM 2025) - https://smartmultimedia.org/2025/", "summary": "Accurate fire and smoke detection is critical for safety and disaster\nresponse, yet existing vision-based methods face challenges in balancing\nefficiency and reliability. Compact deep learning models such as YOLOv5n and\nYOLOv8n are widely adopted for deployment on UAVs, CCTV systems, and IoT\ndevices, but their reduced capacity often results in false positives and missed\ndetections. Conventional post-detection methods such as Non-Maximum Suppression\nand Soft-NMS rely only on spatial overlap, which can suppress true positives or\nretain false alarms in cluttered or ambiguous fire scenes. To address these\nlimitations, we propose an uncertainty aware post-detection framework that\nrescales detection confidences using both statistical uncertainty and domain\nrelevant visual cues. A lightweight Confidence Refinement Network integrates\nuncertainty estimates with color, edge, and texture features to adjust\ndetection scores without modifying the base model. Experiments on the D-Fire\ndataset demonstrate improved precision, recall, and mean average precision\ncompared to existing baselines, with only modest computational overhead. These\nresults highlight the effectiveness of post-detection rescoring in enhancing\nthe robustness of compact deep learning models for real-world fire and smoke\ndetection.", "AI": {"tldr": "本文提出了一种不确定性感知的后检测框架，通过结合统计不确定性和领域相关视觉线索来重新调整检测置信度，以提高紧凑型深度学习模型（如YOLOv5n/v8n）在火灾和烟雾检测中的精度和鲁棒性，而无需修改基础模型。", "motivation": "现有的基于视觉的火灾和烟雾检测方法难以平衡效率和可靠性。紧凑型深度学习模型（如YOLOv5n和YOLOv8n）在部署时因容量限制常导致误报和漏检。传统的后检测方法（如NMS和Soft-NMS）仅依赖空间重叠，在杂乱或模糊的火灾场景中效果不佳。", "method": "提出了一种不确定性感知的后检测框架。该框架利用统计不确定性和领域相关的视觉线索（颜色、边缘、纹理特征）重新调整检测置信度。通过一个轻量级的置信度优化网络（Confidence Refinement Network），将不确定性估计与这些视觉特征结合，在不修改基础模型的情况下调整检测分数。", "result": "在D-Fire数据集上的实验表明，与现有基线相比，该方法在精度、召回率和平均精度均值（mAP）方面有所提高，且计算开销适中。", "conclusion": "这些结果强调了后检测重评分在增强紧凑型深度学习模型在真实火灾和烟雾检测中鲁棒性的有效性。"}}
{"id": "2510.10181", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10181", "abs": "https://arxiv.org/abs/2510.10181", "authors": ["Shaokai Wu", "Yanbiao Ji", "Qiuchang Li", "Zhiyi Zhang", "Qichen He", "Wenyuan Xie", "Guodong Zhang", "Bayram Bayramli", "Yue Ding", "Hongtao Lu"], "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback", "comment": null, "summary": "Embodied agents face a fundamental limitation: once deployed in real-world\nenvironments to perform specific tasks, they are unable to acquire new useful\nknowledge to enhance task performance. In this paper, we propose a general\npost-deployment learning framework called Dejavu, which employs an Experience\nFeedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)\npolicy with retrieved execution memories. EFN automatically identifies\ncontextually successful prior action experiences and conditions action\nprediction on this retrieved guidance. We adopt reinforcement learning with\nsemantic similarity rewards on EFN to ensure that the predicted actions align\nwith past successful behaviors under current observations. During deployment,\nEFN continually enriches its memory with new trajectories, enabling the agent\nto exhibit \"learning from experience\" despite fixed weights. Experiments across\ndiverse embodied tasks show that EFN significantly improves adaptability,\nrobustness, and success rates over frozen baselines. These results highlight a\npromising path toward embodied agents that continually refine their behavior\nafter deployment.", "AI": {"tldr": "本文提出了一种名为Dejavu的部署后学习框架，通过经验反馈网络（EFN）使具身智能体能够检索并利用成功的历史经验来增强其任务表现，从而实现部署后的持续学习。", "motivation": "具身智能体在部署到真实环境后，无法获取新知识以提升任务性能，这是其面临的一个根本限制。", "method": "Dejavu框架采用经验反馈网络（EFN），通过检索执行记忆来增强冻结的视觉-语言-动作（VLA）策略。EFN自动识别情境成功的先前行动经验，并以此指导行动预测。它使用强化学习和语义相似度奖励来确保预测行动与当前观察下的历史成功行为保持一致。在部署期间，EFN会不断用新轨迹丰富其记忆。", "result": "在各种具身任务中的实验表明，EFN显著提高了智能体相对于冻结基线的适应性、鲁棒性和成功率。", "conclusion": "这些结果为具身智能体在部署后持续改进其行为提供了一条有前景的途径。"}}
{"id": "2510.10442", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10442", "abs": "https://arxiv.org/abs/2510.10442", "authors": ["Pei Yu Chang", "Vishnu Renganathan", "Qadeer Ahmed"], "title": "Risk-Budgeted Control Framework for Balanced Performance and Safety in Autonomous Vehicles", "comment": null, "summary": "This paper presents a risk-budgeted monitor with a control framework that\ncertifies safety for autonomous driving. In this process, a sliding window is\nproposed to monitor for insufficient barrier residuals or nonzero tail risk,\nensuring system safety. When the safety margin deteriorates, it triggers\nswitching the safety constraint from a performance-based relaxed-control\nbarrier function (R-CBF) to a conservative conditional value at risk (CVaR-CBF)\nto address the safety concern. This switching is governed by two real-time\ntriggers: Feasibility-Triggered (FT) and Quality-Triggered (QT) conditions. In\nthe FT condition, if the R-CBF constraint becomes infeasible or yields a\nsuboptimal solution, the risk monitor triggers the use of the CVaR constraints\nfor the controller. In the QT condition, the risk monitor observes the safety\nmargin of the R-CBF solution at every step, regardless of feasibility. If it\nfalls below the safety margin, the safety filter switches to the CVaR-CBF\nconstraints.\n  The proposed framework is evaluated using a model predictive controller (MPC)\nfor autonomous driving in the presence of autonomous vehicle (AV) localization\nnoise and obstacle position uncertainties. Multiple AV-pedestrian interaction\nscenarios are considered, with 1,500 Monte Carlo runs conducted for all\nscenarios. In the most challenging setting with pedestrian detection\nuncertainty of 5 m, the proposed framework achieves a 94-96% success rate of\nnot colliding with the pedestrians over 300 trials while maintaining the lowest\nmean cross-track error (CTE = 3.2-3.6 m) to the reference path. The reduced CTE\nindicates faster trajectory recovery after obstacle avoidance, demonstrating a\nbalance between safety and performance.", "AI": {"tldr": "本文提出了一种带控制框架的风险预算监控器，通过滑动窗口监测安全裕度，并在安全裕度恶化时，根据可行性触发和质量触发条件，在基于性能的R-CBF和保守的CVaR-CBF之间切换，以确保自动驾驶安全。该框架在MPC中验证，在不确定性场景下实现了高碰撞避免率和快速轨迹恢复。", "motivation": "确保自动驾驶的安全性，特别是在存在定位噪声和障碍物位置不确定性等情况下，当安全裕度不足或存在非零尾部风险时，能够有效应对安全隐患。", "method": "提出了一种带控制框架的风险预算监控器。该监控器使用滑动窗口来监测障碍物残差不足或非零尾部风险。当安全裕度恶化时，它会触发安全约束从基于性能的R-CBF切换到保守的CVaR-CBF。切换由两个实时触发条件控制：可行性触发（FT）和质量触发（QT）。该框架通过模型预测控制器（MPC）进行评估，考虑了自动驾驶车辆（AV）定位噪声和障碍物位置不确定性，并进行了多AV-行人交互场景的蒙特卡洛模拟。", "result": "在行人检测不确定性为5米的最具挑战性设置下，所提出的框架在300次试验中实现了94-96%的无碰撞成功率，同时保持了参考路径的最低平均横向误差（CTE = 3.2-3.6米）。较低的CTE表明避障后轨迹恢复更快。", "conclusion": "所提出的框架有效地平衡了安全性和性能，在存在不确定性的自动驾驶场景中，通过高碰撞避免成功率和快速轨迹恢复，证明了其在认证自动驾驶安全方面的有效性。"}}
{"id": "2510.10450", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10450", "abs": "https://arxiv.org/abs/2510.10450", "authors": ["P Sangeerth", "David Smith Sundarsingh", "Bhabani Shankar Dey", "Pushpak Jagtap"], "title": "Controller for Incremental Input-to-State Practical Stabilization of Partially Unknown systems with Invariance Guarantees", "comment": "2 figures,9 pages", "summary": "Incremental stability is a property of dynamical systems that ensures the\nconvergence of trajectories with respect to each other rather than a fixed\nequilibrium point or a fixed trajectory. In this paper, we introduce a related\nstability notion called incremental input-to-state practical stability\n({\\delta}-ISpS), ensuring safety guarantees. We also present a feedback\nlinearization based control design scheme that renders a partially unknown\nsystem incrementally input-to-state practically stable and safe with formal\nguarantees. To deal with the unknown dynamics, we utilize Gaussian process\nregression to approximate the model. Finally, we implement the controller\nsynthesized by the proposed scheme on a manipulator example", "AI": {"tldr": "本文引入增量输入到状态实际稳定性（δ-ISpS）概念，并提出一种基于反馈线性化和高斯过程回归的控制设计方案，以确保部分未知系统的增量稳定性与安全性。", "motivation": "确保动态系统的安全性，即轨迹相互收敛而非收敛于固定平衡点或轨迹，尤其是在存在未知动力学的情况下。", "method": "引入增量输入到状态实际稳定性（δ-ISpS）的概念；采用基于反馈线性化的控制设计方案；利用高斯过程回归处理未知动力学近似模型。", "result": "提出了一种控制设计方案，能够使部分未知系统实现增量输入到状态实际稳定（δ-ISpS）和安全性，并提供形式化保证。", "conclusion": "所提出的控制器方案已成功在一个机械手示例上实现，验证了其有效性。"}}
{"id": "2510.09790", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09790", "abs": "https://arxiv.org/abs/2510.09790", "authors": ["Michael Freenor", "Lauren Alvarez"], "title": "Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models", "comment": "9 pages, 3 Figure, 1 table, preprint", "summary": "Understanding how language and embedding models encode semantic relationships\nis fundamental to model interpretability and control. While early word\nembeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +\n''woman'' = ''queen''), modern high-dimensional text representations lack\nstraightforward interpretable geometric properties. We introduce\nRotor-Invariant Shift Estimation (RISE), a geometric approach that represents\nsemantic transformations as consistent rotational operations in embedding\nspace, leveraging the manifold structure of modern language representations.\nRISE operations have the ability to operate across both languages and models\nwith high transfer of performance, suggesting the existence of analogous\ncross-lingual geometric structure. We evaluate RISE across three embedding\nmodels, three datasets, and seven morphologically diverse languages in five\nmajor language groups. Our results demonstrate that RISE consistently maps\ndiscourse-level semantic transformations with distinct grammatical features\n(e.g., negation and conditionality) across languages and models. This work\nprovides the first systematic demonstration that discourse-level semantic\ntransformations correspond to consistent geometric operations in multilingual\nembedding spaces, empirically supporting the Linear Representation Hypothesis\nat the sentence level.", "AI": {"tldr": "本研究提出了一种名为RISE的几何方法，将语义转换表示为嵌入空间中一致的旋转操作，并发现这种操作在不同语言和模型之间具有很高的可迁移性，揭示了多语言嵌入空间中话语级语义转换的几何结构。", "motivation": "早期的词嵌入模型展现出直观的向量算术特性，但现代高维文本表示缺乏直接可解释的几何属性。理解语言和嵌入模型如何编码语义关系对于模型的可解释性和控制至关重要。", "method": "研究引入了Rotor-Invariant Shift Estimation (RISE)，一种几何方法，利用现代语言表示的流形结构，将语义转换表示为嵌入空间中一致的旋转操作。RISE在三种嵌入模型、三个数据集和七种形态多样的语言（属于五个主要语系）上进行了评估。", "result": "RISE操作在跨语言和跨模型方面表现出高迁移性，表明存在类似的跨语言几何结构。结果显示，RISE能够一致地映射具有不同语法特征（如否定和条件性）的话语级语义转换，且在不同语言和模型之间保持一致性。", "conclusion": "这项工作首次系统地证明了话语级语义转换在多语言嵌入空间中对应于一致的几何操作，从经验上支持了句子层面的线性表示假设。"}}
{"id": "2510.09879", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09879", "abs": "https://arxiv.org/abs/2510.09879", "authors": ["Shreshth Saini", "Alan C. Bovik", "Neil Birkbeck", "Yilin Wang", "Balu Adsumilli"], "title": "CHUG: Crowdsourced User-Generated HDR Video Quality Dataset", "comment": null, "summary": "High Dynamic Range (HDR) videos enhance visual experiences with superior\nbrightness, contrast, and color depth. The surge of User-Generated Content\n(UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR\nvideo quality assessment (VQA) due to diverse capture conditions, editing\nartifacts, and compression distortions. Existing HDR-VQA datasets primarily\nfocus on professionally generated content (PGC), leaving a gap in understanding\nreal-world UGC-HDR degradations. To address this, we introduce CHUG:\nCrowdsourced User-Generated HDR Video Quality Dataset, the first large-scale\nsubjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos,\ntranscoded across multiple resolutions and bitrates to simulate real-world\nscenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical\nTurk collected 211,848 perceptual ratings. CHUG provides a benchmark for\nanalyzing UGC-specific distortions in HDR videos. We anticipate CHUG will\nadvance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse,\nand real-world UGC dataset. The dataset is publicly available at:\nhttps://shreshthsaini.github.io/CHUG/.", "AI": {"tldr": "本文介绍了CHUG，一个大规模的众包用户生成内容（UGC）HDR视频质量评估数据集，旨在填补现有HDR-VQA数据集主要关注专业生成内容（PGC）的空白。", "motivation": "HDR视频提升了视觉体验，但YouTube和TikTok等平台上的UGC-HDR视频因多样化的拍摄条件、编辑伪影和压缩失真，对质量评估提出了独特挑战。现有HDR-VQA数据集主要关注PGC，未能涵盖真实世界UGC-HDR的退化，因此需要一个专门针对UGC-HDR的质量评估数据集。", "method": "研究者创建了CHUG数据集，包含856个UGC-HDR源视频，通过多分辨率和比特率转码模拟真实世界场景，共计5,992个视频。通过Amazon Mechanical Turk进行了大规模研究，收集了211,848个感知评分。", "result": "CHUG数据集是第一个大规模的UGC-HDR质量主观研究数据集，提供了分析HDR视频中UGC特有失真的基准。该数据集包含856个UGC-HDR源视频、5,992个转码视频以及211,848个感知评分。", "conclusion": "CHUG数据集将通过提供一个大规模、多样化且真实的UGC数据集，推动无参考（NR）HDR-VQA研究的进展。"}}
{"id": "2510.10206", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10206", "abs": "https://arxiv.org/abs/2510.10206", "authors": ["Zuhong Liu", "Junhao Ge", "Minhao Xiong", "Jiahao Gu", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots", "comment": null, "summary": "The true promise of humanoid robotics lies beyond single-agent autonomy: two\nor more humanoids must engage in physically grounded, socially meaningful\nwhole-body interactions that echo the richness of human social interaction.\nHowever, single-humanoid methods suffer from the isolation issue, ignoring\ninter-agent dynamics and causing misaligned contacts, interpenetrations, and\nunrealistic motions. To address this, we present Harmanoid , a dual-humanoid\nmotion imitation framework that transfers interacting human motions to two\nrobots while preserving both kinematic fidelity and physical realism. Harmanoid\ncomprises two key components: (i) contact-aware motion retargeting, which\nrestores inter-body coordination by aligning SMPL contacts with robot vertices,\nand (ii) interaction-driven motion controller, which leverages\ninteraction-specific rewards to enforce coordinated keypoints and physically\nplausible contacts. By explicitly modeling inter-agent contacts and\ninteraction-aware dynamics, Harmanoid captures the coupled behaviors between\nhumanoids that single-humanoid frameworks inherently overlook. Experiments\ndemonstrate that Harmanoid significantly improves interactive motion imitation,\nsurpassing existing single-humanoid frameworks that largely fail in such\nscenarios.", "AI": {"tldr": "Harmanoid是一个双人形机器人运动模仿框架，通过显式建模交互和接触，将人类交互动作转移到两个机器人上，以实现物理真实且社会有意义的互动。", "motivation": "现有单人形机器人方法在多机器人互动中存在局限性，忽略了机器人间的动态关系，导致接触错位、穿透和不真实的动作。研究的动机是实现人形机器人之间像人类一样丰富、物理接地且社会有意义的全身互动。", "method": "Harmanoid框架包含两个核心组件：1) 接触感知运动重定向，通过将SMPL接触点与机器人顶点对齐来恢复身体间的协调；2) 交互驱动运动控制器，利用针对交互的奖励来强制执行协调的关键点和物理上可信的接触。该方法明确建模了机器人间的接触和交互感知动态。", "result": "实验证明，Harmanoid显著改善了交互式运动模仿，超越了在多机器人交互场景中表现不佳的现有单人形机器人框架。它成功捕捉了人形机器人之间的耦合行为。", "conclusion": "Harmanoid通过显式建模机器人间的接触和交互感知动态，有效解决了单人形机器人框架在模拟多机器人交互时的局限性，实现了更真实、协调的双人形机器人互动。"}}
{"id": "2510.10069", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.10069", "abs": "https://arxiv.org/abs/2510.10069", "authors": ["Zeyu Ling", "Xiaodong Gu", "Jiangnan Tang", "Changqing Zou"], "title": "SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation", "comment": null, "summary": "We introduce SyncLipMAE, a self-supervised pretraining framework for\ntalking-face video that learns synchronization-aware and transferable facial\ndynamics from unlabeled audio-visual streams. Our approach couples masked\nvisual modeling with cross-modal contrastive alignment and employs three\nper-frame prompt tokens that explicitly encode the essential factors of a\ntalking-face frame - identity, vocal motion (speech-synchronized facial\ndynamics), and ambient motion (audio-agnostic movements such as blinks and head\npose). The contrastive objective uses time-aligned vocal-motion and audio\ntokens as positives and misaligned pairs as negatives, driving both modalities\ninto a shared embedding space and yielding token-level audio-visual stream\nsynchronization. After pretraining, the aligned audio tokens together with the\nvisual prompt tokens (identity, vocal motion, ambient motion) form a unified\ninterface for four disparate downstream settings: (i) audio-visual stream\nsynchronization; (ii) facial emotion and head/face action recognition; (iii)\nvisual speech recognition; and (iv) visual dubbing, for which we enable\nindistinguishable audio- or video-driven control within a single model. Across\nfour task families that require distinct capabilities, SyncLipMAE achieves\nstate-of-the-art results, underscoring the effectiveness of\nsynchronization-aware, factorized self-supervised pretraining.", "AI": {"tldr": "SyncLipMAE是一个用于人脸对话视频的自监督预训练框架，通过掩码视觉建模、跨模态对比对齐和显式提示词，学习同步感知的面部动态，并在多种下游任务中取得了最先进的成果。", "motivation": "研究动机是开发一种能够从无标签音视频流中学习同步感知和可迁移面部动态的框架，并能统一支持多种不同的下游任务，如音视频同步、情感识别、视觉语音识别和视觉配音。", "method": "SyncLipMAE框架结合了掩码视觉建模和跨模态对比对齐。它使用三个帧级提示词来编码人脸对话的关键因素：身份、语音运动（与语音同步的面部动态）和环境运动（与音频无关的运动）。对比目标通过将时间对齐的语音运动和音频标记作为正例、错位对作为负例，将两种模态驱动到共享嵌入空间，实现标记级的音视频流同步。", "result": "SyncLipMAE在四类需要不同能力的任务中均取得了最先进的成果，包括：(i) 音视频流同步；(ii) 面部情感和头部/面部动作识别；(iii) 视觉语音识别；以及 (iv) 视觉配音（可在单个模型中实现音频或视频驱动的无差别控制）。", "conclusion": "同步感知、因子化的自监督预训练方法在学习人脸对话视频中的面部动态方面非常有效，并能显著提升多种下游任务的性能。"}}
{"id": "2510.09878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09878", "abs": "https://arxiv.org/abs/2510.09878", "authors": ["Milad Khanchi", "Maria Amer", "Charalambos Poullis"], "title": "Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking", "comment": null, "summary": "Multi-object tracking (MOT) methods often rely on Intersection-over-Union\n(IoU) for association. However, this becomes unreliable when objects are\nsimilar or occluded. Also, computing IoU for segmentation masks is\ncomputationally expensive. In this work, we use segmentation masks to capture\nobject shapes, but we do not compute segmentation IoU. Instead, we fuse depth\nand mask features and pass them through a compact encoder trained\nself-supervised. This encoder produces stable object representations, which we\nuse as an additional similarity cue alongside bounding box IoU and\nre-identification features for matching. We obtain depth maps from a zero-shot\ndepth estimator and object masks from a promptable visual segmentation model to\nobtain fine-grained spatial cues. Our MOT method is the first to use the\nself-supervised encoder to refine segmentation masks without computing masks\nIoU. MOT can be divided into joint detection-ReID (JDR) and\ntracking-by-detection (TBD) models. The latter are computationally more\nefficient. Experiments of our TBD method on challenging benchmarks with\nnon-linear motion, occlusion, and crowded scenes, such as SportsMOT and\nDanceTrack, show that our method outperforms the TBD state-of-the-art on most\nmetrics, while achieving competitive performance on simpler benchmarks with\nlinear motion, such as MOT17.", "AI": {"tldr": "该论文提出一种多目标跟踪（MOT）方法，通过自监督编码器融合深度和分割掩码特征来生成稳定的目标表示，作为额外的相似性线索进行关联，在不计算掩码IoU的情况下，在复杂基准测试上超越了现有最先进的跟踪-通过-检测（TBD）方法。", "motivation": "多目标跟踪中，基于IoU的关联在目标相似或被遮挡时变得不可靠，且计算分割掩码的IoU成本高昂。因此，需要更稳定、更有效的目标表示来改进关联。", "method": "该方法利用零样本深度估计器获取深度图和可提示视觉分割模型获取对象掩码，以捕获精细的空间线索。它融合深度和掩码特征，并通过一个紧凑的自监督编码器进行训练，生成稳定的目标表示。这些表示与边界框IoU和重识别特征一起，作为额外的相似性线索用于匹配。该方法是一种跟踪-通过-检测（TBD）模型，并且是首个使用自监督编码器在不计算掩码IoU的情况下细化分割掩码的MOT方法。", "result": "该TBD方法在具有非线性运动、遮挡和拥挤场景的挑战性基准测试（如SportsMOT和DanceTrack）上，在大多数指标上优于TBD领域最先进的方法。在具有线性运动的简单基准测试（如MOT17）上，也取得了有竞争力的性能。", "conclusion": "通过融合深度和分割掩码特征并利用自监督编码器生成稳定的目标表示，该MOT方法有效解决了传统IoU关联的局限性，特别是在复杂场景下显著提高了跟踪性能，同时避免了计算分割掩码IoU的开销。"}}
{"id": "2510.10074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10074", "abs": "https://arxiv.org/abs/2510.10074", "authors": ["Jiayi Mao", "Liqun Li", "Yanjie Gao", "Zegang Peng", "Shilin He", "Chaoyun Zhang", "Si Qin", "Samia Khalid", "Qingwei Lin", "Saravan Rajmohan", "Sitaram Lanka", "Dongmei Zhang"], "title": "Agentic Troubleshooting Guide Automation for Incident Management", "comment": null, "summary": "Effective incident management in large-scale IT systems relies on\ntroubleshooting guides (TSGs), but their manual execution is slow and\nerror-prone. While recent advances in LLMs offer promise for automating\nincident management tasks, existing LLM-based solutions lack specialized\nsupport for several key challenges, including managing TSG quality issues,\ninterpreting complex control flow, handling data-intensive queries, and\nexploiting execution parallelism. We first conducted an empirical study on 92\nreal-world TSGs, and, guided by our findings, we present StepFly, a novel\nend-to-end agentic framework for troubleshooting guide automation. Our approach\nfeatures a three-stage workflow: the first stage provides a comprehensive guide\ntogether with a tool, TSG Mentor, to assist SREs in improving TSG quality; the\nsecond stage performs offline preprocessing using LLMs to extract structured\nexecution DAGs from unstructured TSGs and to create dedicated Query Preparation\nPlugins (QPPs); and the third stage executes online using a DAG-guided\nscheduler-executor framework with a memory system to guarantee correct workflow\nand support parallel execution of independent steps. Our empirical evaluation\non a collection of real-world TSGs and incidents demonstrates that StepFly\nachieves a ~94% success rate on GPT-4.1, outperforming baselines with less time\nand token consumption. Furthermore, it achieves a remarkable execution time\nreduction of 32.9% to 70.4% for parallelizable TSGs.", "AI": {"tldr": "StepFly是一个新颖的端到端智能体框架，用于自动化IT系统中的故障排除指南（TSG）。它通过三阶段工作流解决了TSG质量、复杂控制流、数据密集型查询和并行执行等挑战，实现了高成功率和显著的执行时间缩减。", "motivation": "大规模IT系统中的事件管理依赖于故障排除指南（TSG），但手动执行TSG既缓慢又容易出错。尽管大型语言模型（LLM）在自动化事件管理任务方面前景广阔，但现有基于LLM的解决方案缺乏对关键挑战的专门支持，包括TSG质量问题、复杂控制流解释、数据密集型查询处理和利用执行并行性。", "method": "首先对92个真实世界的TSG进行了实证研究。在此基础上，提出了StepFly框架，其特点是三阶段工作流：第一阶段提供全面的指南和TSG Mentor工具，协助SRE提高TSG质量；第二阶段使用LLM进行离线预处理，从非结构化TSG中提取结构化执行DAG并创建专用查询准备插件（QPP）；第三阶段使用DAG引导的调度器-执行器框架和内存系统进行在线执行，以保证正确的工作流并支持独立步骤的并行执行。", "result": "在真实世界的TSG和事件集合上的实证评估表明，StepFly在GPT-4.1上实现了约94%的成功率，优于基线，并且耗时和令牌消耗更少。此外，对于可并行化的TSG，它实现了32.9%至70.4%的显著执行时间缩减。", "conclusion": "StepFly框架通过解决TSG质量、复杂控制流、数据密集型查询和并行执行等核心挑战，有效地自动化了故障排除指南的执行，显著提高了成功率、减少了资源消耗，并大幅缩短了并行化任务的执行时间。"}}
{"id": "2510.09849", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09849", "abs": "https://arxiv.org/abs/2510.09849", "authors": ["Ruizhe Zhu"], "title": "Text Prompt Injection of Vision Language Models", "comment": null, "summary": "The widespread application of large vision language models has significantly\nraised safety concerns. In this project, we investigate text prompt injection,\na simple yet effective method to mislead these models. We developed an\nalgorithm for this type of attack and demonstrated its effectiveness and\nefficiency through experiments. Compared to other attack methods, our approach\nis particularly effective for large models without high demand for\ncomputational resources.", "AI": {"tldr": "该项目研究了文本提示注入攻击，这是一种简单而有效的误导大型视觉语言模型的方法。他们开发了一种算法，并通过实验证明其有效性、效率和低计算资源需求。", "motivation": "大型视觉语言模型的广泛应用引发了严重的安全担忧。", "method": "研究了文本提示注入（text prompt injection）这种攻击方式，并开发了一种针对此类攻击的算法。", "result": "通过实验证明了所开发攻击算法的有效性和效率。与其他攻击方法相比，该方法对大型模型尤其有效，且对计算资源的需求不高。", "conclusion": "文本提示注入是一种简单、有效且高效的攻击大型视觉语言模型的方法，尤其适用于资源受限的场景。"}}
{"id": "2510.10141", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.10141", "abs": "https://arxiv.org/abs/2510.10141", "authors": ["Hongxing Peng", "Haopei Xie", "Weijia Lia", "Huanai Liuc", "Ximing Li"], "title": "YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments", "comment": null, "summary": "Litchi is a high-value fruit, yet traditional manual selection methods are\nincreasingly inadequate for modern production demands. Integrating UAV-based\naerial imagery with deep learning offers a promising solution to enhance\nefficiency and reduce costs. This paper introduces YOLOv11-Litchi, a\nlightweight and robust detection model specifically designed for UAV-based\nlitchi detection. Built upon the YOLOv11 framework, the proposed model\naddresses key challenges such as small target size, large model parameters\nhindering deployment, and frequent target occlusion. To tackle these issues,\nthree major innovations are incorporated: a multi-scale residual module to\nimprove contextual feature extraction across scales, a lightweight feature\nfusion method to reduce model size and computational costs while maintaining\nhigh accuracy, and a litchi occlusion detection head to mitigate occlusion\neffects by emphasizing target regions and suppressing background interference.\nExperimental results validate the model's effectiveness. YOLOv11-Litchi\nachieves a parameter size of 6.35 MB - 32.5% smaller than the YOLOv11 baseline\n- while improving mAP by 2.5% to 90.1% and F1-Score by 1.4% to 85.5%.\nAdditionally, the model achieves a frame rate of 57.2 FPS, meeting real-time\ndetection requirements. These findings demonstrate the suitability of\nYOLOv11-Litchi for UAV-based litchi detection in complex orchard environments,\nshowcasing its potential for broader applications in precision agriculture.", "AI": {"tldr": "本文提出了YOLOv11-Litchi模型，一个基于YOLOv11的轻量级且鲁棒的荔枝检测模型，专门用于无人机航拍图像。该模型通过多尺度残差模块、轻量级特征融合和荔枝遮挡检测头解决了小目标、大模型参数和遮挡等问题，显著提高了检测精度和速度，同时减小了模型尺寸。", "motivation": "传统的荔枝人工筛选方法已无法满足现代生产需求，效率低下且成本高昂。将无人机航拍图像与深度学习结合被视为一种有前景的解决方案，但现有方法在处理小目标、模型部署参数过大以及目标频繁遮挡等挑战时面临困难，促使研究人员开发更高效、更轻量级的检测模型。", "method": "本文在YOLOv11框架基础上构建了YOLOv11-Litchi模型，并融入了三项主要创新：1) 多尺度残差模块，用于改善跨尺度的上下文特征提取；2) 轻量级特征融合方法，旨在在保持高精度的同时减小模型尺寸和计算成本；3) 荔枝遮挡检测头，通过强调目标区域并抑制背景干扰来减轻遮挡效应。", "result": "实验结果表明，YOLOv11-Litchi模型的参数大小为6.35 MB，比YOLOv11基线小32.5%。同时，其mAP提高了2.5%达到90.1%，F1-Score提高了1.4%达到85.5%。此外，该模型实现了57.2 FPS的帧率，满足了实时检测要求。", "conclusion": "研究结果证明了YOLOv11-Litchi模型在复杂果园环境中进行无人机荔枝检测的适用性，并展示了其在精准农业中更广泛应用的潜力。"}}
{"id": "2510.10217", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10217", "abs": "https://arxiv.org/abs/2510.10217", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Tetsuya Ogata"], "title": "UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction", "comment": "8 pages, 6 figures", "summary": "Training robots to operate effectively in environments with uncertain states,\nsuch as ambiguous object properties or unpredictable interactions, remains a\nlongstanding challenge in robotics. Imitation learning methods typically rely\non successful examples and often neglect failure scenarios where uncertainty is\nmost pronounced. To address this limitation, we propose the Uncertainty-driven\nForesight Recurrent Neural Network (UF-RNN), a model that combines standard\ntime-series prediction with an active \"Foresight\" module. This module performs\ninternal simulations of multiple future trajectories and refines the hidden\nstate to minimize predicted variance, enabling the model to selectively explore\nactions under high uncertainty. We evaluate UF-RNN on a door-opening task in\nboth simulation and a real-robot setting, demonstrating that, despite the\nabsence of explicit failure demonstrations, the model exhibits robust\nadaptation by leveraging self-induced chaotic dynamics in its latent space.\nWhen guided by the Foresight module, these chaotic properties stimulate\nexploratory behaviors precisely when the environment is ambiguous, yielding\nimproved success rates compared to conventional stochastic RNN baselines. These\nfindings suggest that integrating uncertainty-driven foresight into imitation\nlearning pipelines can significantly enhance a robot's ability to handle\nunpredictable real-world conditions.", "AI": {"tldr": "该研究提出不确定性驱动预见循环神经网络（UF-RNN），通过内部模拟未来轨迹并最小化预测方差，使机器人能在高不确定性环境中主动探索，从而提高模仿学习在现实世界条件下的鲁棒性。", "motivation": "机器人难以在状态不确定的环境中有效操作（如模糊的物体属性或不可预测的交互）。传统的模仿学习方法通常依赖成功案例，而忽略了不确定性最显著的失败情景。", "method": "提出不确定性驱动预见循环神经网络（UF-RNN）。该模型结合标准时间序列预测与一个主动的“预见”模块。该模块执行多个未来轨迹的内部模拟，并优化隐藏状态以最小化预测方差，从而使模型能够在高不确定性下选择性地探索行动。", "result": "在模拟和真实机器人开门任务中，即使没有明确的失败演示，UF-RNN 仍表现出鲁棒的适应性。通过预见模块的引导，模型潜在空间中自发的混沌动力学在环境模糊时激发探索性行为，相比传统随机 RNN 基线，成功率有所提高。", "conclusion": "将不确定性驱动的预见机制整合到模仿学习流程中，可以显著增强机器人处理不可预测的现实世界条件的能力。"}}
{"id": "2510.10414", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.10414", "abs": "https://arxiv.org/abs/2510.10414", "authors": ["Chin-Hung Teng", "Ben-Jian Dong"], "title": "Guided Image Feature Matching using Feature Spatial Order", "comment": null, "summary": "Image feature matching plays a vital role in many computer vision tasks.\nAlthough many image feature detection and matching techniques have been\nproposed over the past few decades, it is still time-consuming to match feature\npoints in two images, especially for images with a large number of detected\nfeatures. Feature spatial order can estimate the probability that a pair of\nfeatures is correct. Since it is a completely independent concept from epipolar\ngeometry, it can be used to complement epipolar geometry in guiding feature\nmatch in a target region so as to improve matching efficiency. In this paper,\nwe integrate the concept of feature spatial order into a progressive matching\nframework. We use some of the initially matched features to build a\ncomputational model of feature spatial order and employs it to calculates the\npossible spatial range of subsequent feature matches, thus filtering out\nunnecessary feature matches. We also integrate it with epipolar geometry to\nfurther improve matching efficiency and accuracy. Since the spatial order of\nfeature points is affected by image rotation, we propose a suitable image\nalignment method from the fundamental matrix of epipolar geometry to remove the\neffect of image rotation. To verify the feasibility of the proposed method, we\nconduct a series of experiments, including a standard benchmark dataset,\nself-generated simulated images, and real images. The results demonstrate that\nour proposed method is significantly more efficient and has more accurate\nfeature matching than the traditional method.", "AI": {"tldr": "该论文提出了一种将特征空间顺序整合到渐进匹配框架中的方法，以提高特征匹配的效率和准确性，并通过图像对齐解决了旋转影响。", "motivation": "图像特征匹配在计算机视觉任务中至关重要，但特征点匹配（特别是对于大量特征的图像）仍然非常耗时。现有方法未能充分利用特征空间顺序来提高效率。", "method": "将特征空间顺序集成到渐进匹配框架中，利用初始匹配的特征建立空间顺序计算模型，估算后续匹配的空间范围以过滤不必要的匹配。同时，与对极几何结合以进一步提高效率和准确性。针对图像旋转对空间顺序的影响，提出了一种基于对极几何基础矩阵的图像对齐方法。", "result": "通过在标准基准数据集、自生成模拟图像和真实图像上进行实验，结果表明所提出的方法比传统方法具有显著更高的匹配效率和更准确的特征匹配。", "conclusion": "该研究成功地将特征空间顺序与对极几何相结合，并解决了图像旋转问题，显著提高了图像特征匹配的效率和准确性。"}}
{"id": "2510.09880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09880", "abs": "https://arxiv.org/abs/2510.09880", "authors": ["Minkwan Kim", "Changwoon Choi", "Young Min Kim"], "title": "Geometry-Aware Scene Configurations for Novel View Synthesis", "comment": null, "summary": "We propose scene-adaptive strategies to efficiently allocate representation\ncapacity for generating immersive experiences of indoor environments from\nincomplete observations. Indoor scenes with multiple rooms often exhibit\nirregular layouts with varying complexity, containing clutter, occlusion, and\nflat walls. We maximize the utilization of limited resources with guidance from\ngeometric priors, which are often readily available after pre-processing\nstages. We record observation statistics on the estimated geometric scaffold\nand guide the optimal placement of bases, which greatly improves upon the\nuniform basis arrangements adopted by previous scalable Neural Radiance Field\n(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to\ncompensate for geometric deficiencies inherent in view configurations in the\ninput trajectory and impose the necessary regularization. We present a\ncomprehensive analysis and discussion regarding rendering quality and memory\nrequirements in several large-scale indoor scenes, demonstrating significant\nenhancements compared to baselines that employ regular placements.", "AI": {"tldr": "本文提出场景自适应策略，利用几何先验和观察统计数据，优化室内环境NeRF的表示容量分配，显著提升渲染质量和内存效率。", "motivation": "室内场景（多房间）布局不规则、复杂多变，包含杂乱、遮挡和平坦墙壁，导致从不完整观测生成沉浸式体验时资源利用率低下。现有的可扩展NeRF表示采用均匀基底布置，效率不高。", "method": "1. 利用预处理阶段可获得的几何先验信息。2. 在估计的几何骨架上记录观测统计数据。3. 基于这些统计数据指导基底的最佳放置，取代均匀基底布置。4. 提出场景自适应的虚拟视角，以弥补输入轨迹中视图配置固有的几何缺陷并施加必要的正则化。", "result": "在多个大型室内场景中，渲染质量和内存需求相比采用常规布置的基线方法有显著提升，并进行了全面的分析和讨论。", "conclusion": "通过利用几何先验和场景自适应策略（包括优化基底放置和虚拟视角），可以有效提高从不完整观测生成室内环境沉浸式体验的效率和质量，显著优于传统方法。"}}
{"id": "2510.10552", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10552", "abs": "https://arxiv.org/abs/2510.10552", "authors": ["Rafael R. Yumul", "Enalyn T. Domingo"], "title": "Transforming Tarlac State University (TSU) Gymnasium to a Nearly Zero-Energy Building through Integration of a Solar Photovoltaic (PV) System", "comment": null, "summary": "The study is anchored to the principles of Nearly-Zero Energy Building\n(NZEB). It aimed to transform the Tarlac State University Gymnasium into a\nfacility with energy-efficient equipment to contribute to reducing carbon\nfootprints by integrating a solar PV system as its renewable energy source. The\nresearchers found out that the electrical infrastructure of the Gym was\noutdated, and the lighting was not energy efficient, and there were too few\nconvenience or power outlets. There was also insufficient cooling equipment to\nmaintain a comfortable temperature. Analysis shows that the payback period is\nwithin the average range, making it a cost-effective investment for the\nUniversity. Aside from the cost of the PV System, adherence to engineering\ndesign standards will mean additional costs to replace the metal halides with\nLED high bay lamps, installation of additional air conditioning units, and\nprovision of additional convenience outlets. These additional costs should be\nconsidered when evaluating the feasibility of the project. It is recommended\nthat the integrity of the existing roof system of the Gymnasium be considered.\nThe total cost of putting up the whole electrical system, including new\nlighting, cooling, and convenience loads, must be calculated to determine the\ntotal cost of implementing the whole NZEB project. Other factors in the\neconomic evaluation may be considered to determine a more stringent result.", "AI": {"tldr": "本研究旨在将一所大学体育馆改造为近零能耗建筑（NZEB），通过整合太阳能光伏系统并升级现有电气基础设施，以提高能源效率并减少碳足迹，并评估其经济可行性。", "motivation": "研究动机源于近零能耗建筑（NZEB）的原则，旨在通过改造大学体育馆，使其配备节能设备，利用可再生能源（太阳能光伏系统）来减少碳足迹，并解决体育馆现有电气基础设施老化、照明效率低下以及冷却设备不足的问题。", "method": "研究方法包括对体育馆现有电气基础设施（如照明、插座、冷却设备）进行分析，提出整合太阳能光伏系统作为可再生能源，并进行经济分析以确定投资回收期。同时，考虑了工程设计标准下的额外升级成本。", "result": "研究发现体育馆的电气基础设施陈旧，照明效率低下，电源插座不足，冷却设备也不足。分析表明，项目的投资回收期处于平均范围，使其成为一项具有成本效益的投资。然而，除了光伏系统成本外，还需考虑更换照明、增加空调和插座等额外升级的成本。建议考虑现有屋顶系统的完整性。", "conclusion": "该项目对大学而言是一项具有成本效益的投资，但必须将整个电气系统（包括新照明、冷却和便利负载）的全部实施成本纳入计算，以确定NZEB项目的总成本。在评估项目可行性时，应充分考虑这些额外成本和现有屋顶系统的完整性。建议纳入其他经济评估因素以获得更严谨的结果。"}}
{"id": "2510.10117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10117", "abs": "https://arxiv.org/abs/2510.10117", "authors": ["Yunxiang Mo", "Tianshi Zheng", "Qing Zong", "Jiayu Liu", "Baixuan Xu", "Yauwai Yim", "Chunkit Chan", "Jiaxin Bai", "Yangqiu Song"], "title": "DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay", "comment": "EMNLP 2025 Wordplay (Spotlight)", "summary": "Multimodal abductive reasoning--the generation and selection of explanatory\nhypotheses from partial observations--is a cornerstone of intelligence. Current\nevaluations of this ability in vision-language models (VLMs) are largely\nconfined to static, single-agent tasks. Inspired by Dixit, we introduce\nDixitWorld, a comprehensive evaluation suite designed to deconstruct this\nchallenge. DIXITWORLD features two core components: DixitArena, a dynamic,\nmulti-agent environment that evaluates both hypothesis generation (a\n\"storyteller\" crafting cryptic clues) and hypothesis selection (\"listeners\"\nchoosing the target image from decoys) under imperfect information; and\nDixitBench, a static QA benchmark that isolates the listener's task for\nefficient, controlled evaluation. Results from DixitArena reveal distinct,\nrole-dependent behaviors: smaller open-source models often excel as creative\nstorytellers, producing imaginative yet less discriminative clues, whereas\nlarger proprietary models demonstrate superior overall performance,\nparticularly as listeners. Performance on DixitBench strongly correlates with\nlistener results in DixitArena, validating it as a reliable proxy for\nhypothesis selection. Our findings reveal a key trade-off between generative\ncreativity and discriminative understanding in multimodal abductive reasoning,\na central challenge for developing more balanced and capable vision-language\nagents.", "AI": {"tldr": "该研究引入了DixitWorld评估套件，用于多模态溯因推理，揭示了视觉语言模型在生成创意和判别理解之间的权衡。", "motivation": "当前对视觉语言模型（VLMs）在多模态溯因推理能力（从部分观察生成和选择解释性假设）的评估仅限于静态、单智能体任务，未能全面反映其能力。", "method": "受Dixit游戏启发，研究构建了DixitWorld评估套件，包含两部分：DixitArena，一个动态、多智能体环境，评估假设生成（“讲故事者”创造线索）和假设选择（“听众”从干扰项中选择目标图像）；DixitBench，一个静态问答基准，专门评估听众的假设选择任务。", "result": "DixitArena结果显示，模型行为具有角色依赖性：小型开源模型在创造性讲故事方面表现出色，能生成富有想象力但判别性较弱的线索；大型专有模型整体性能更优，尤其在作为听众时。DixitBench的性能与DixitArena中听众的结果高度相关，证明其是假设选择的可靠代理。研究揭示了生成性创造力与判别性理解之间存在关键权衡。", "conclusion": "多模态溯因推理中生成性创造力与判别性理解之间的权衡是开发更平衡、更有能力的视觉语言智能体的核心挑战。"}}
{"id": "2510.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09854", "abs": "https://arxiv.org/abs/2510.09854", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "comment": null, "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks.", "AI": {"tldr": "本文提出NG-Router框架，通过将营养问答建模为知识图谱引导的多智能体协作问题，利用图神经网络进行任务感知路由和基于梯度的子图检索，以克服现有方法的局限性。", "motivation": "现有营养问答系统面临两大挑战：单智能体系统推理能力有限以及多智能体架构设计复杂；同时，上下文过载会阻碍决策准确性。个性化饮食指导和慢性病预防对营养问答有迫切需求。", "method": "NG-Router将营养问答公式化为监督式、知识图谱引导的多智能体协作问题。它将智能体节点整合到异构知识图谱中，并采用图神经网络学习任务感知的智能体路由分布，利用经验性智能体表现的软监督。为解决上下文过载，提出了一种基于梯度的子图检索机制，在训练期间识别显著证据。", "result": "在多个基准和骨干模型上的大量实验表明，NG-Router始终优于单智能体和集成基线模型。", "conclusion": "NG-Router为复杂的营养健康任务提供了一种领域感知多智能体推理的原则性方法，有效解决了现有方法的推理能力和上下文过载问题。"}}
{"id": "2510.10221", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10221", "abs": "https://arxiv.org/abs/2510.10221", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Hiroki Mori", "Tetsuya Ogata"], "title": "A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots", "comment": "8 pages, 5 figures", "summary": "This study investigates the developmental interaction between top-down (TD)\nand bottom-up (BU) visual attention in robotic learning. Our goal is to\nunderstand how structured, human-like attentional behavior emerges through the\nmutual adaptation of TD and BU mechanisms over time. To this end, we propose a\nnovel attention model $A^3 RNN$ that integrates predictive TD signals and\nsaliency-based BU cues through a bi-directional attention architecture.\n  We evaluate our model in robotic manipulation tasks using imitation learning.\nExperimental results show that attention behaviors evolve throughout training,\nfrom saliency-driven exploration to prediction-driven direction. Initially, BU\nattention highlights visually salient regions, which guide TD processes, while\nas learning progresses, TD attention stabilizes and begins to reshape what is\nperceived as salient. This trajectory reflects principles from cognitive\nscience and the free-energy framework, suggesting the importance of\nself-organizing attention through interaction between perception and internal\nprediction. Although not explicitly optimized for stability, our model exhibits\nmore coherent and interpretable attention patterns than baselines, supporting\nthe idea that developmental mechanisms contribute to robust attention\nformation.", "AI": {"tldr": "本研究通过提出一种名为 $A^3 RNN$ 的新型注意力模型，探索了机器人在学习过程中自上而下（TD）和自下而上（BU）视觉注意力的发展性互动，发现注意力行为从显著性驱动的探索演变为预测驱动的引导。", "motivation": "旨在理解类人注意力行为如何在TD和BU机制的相互适应中随时间演变和形成。", "method": "提出了一种新颖的 $A^3 RNN$ 注意力模型，该模型通过双向注意力架构整合了预测性的TD信号和基于显著性的BU线索。模型在模仿学习的机器人操作任务中进行评估。", "result": "注意力行为在训练过程中不断演变：初期BU注意力突出视觉显著区域以引导TD过程；随着学习进展，TD注意力稳定并开始重塑对显著性的感知。模型展现出比基线更连贯和可解释的注意力模式，尽管未明确优化稳定性。", "conclusion": "TD和BU机制之间的发展性互动有助于形成鲁棒的注意力，这与认知科学和自由能框架的原理相符，强调了感知和内部预测相互作用下自组织注意力的重要性。"}}
{"id": "2510.10910", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.10910", "abs": "https://arxiv.org/abs/2510.10910", "authors": ["Honghui Yuan", "Keiji Yanai"], "title": "SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model", "comment": null, "summary": "With the rapid development of diffusion models, style transfer has made\nremarkable progress. However, flexible and localized style editing for scene\ntext remains an unsolved challenge. Although existing scene text editing\nmethods have achieved text region editing, they are typically limited to\ncontent replacement and simple styles, which lack the ability of free-style\ntransfer. In this paper, we introduce SceneTextStylizer, a novel training-free\ndiffusion-based framework for flexible and high-fidelity style transfer of text\nin scene images. Unlike prior approaches that either perform global style\ntransfer or focus solely on textual content modification, our method enables\nprompt-guided style transformation specifically for text regions, while\npreserving both text readability and stylistic consistency. To achieve this, we\ndesign a feature injection module that leverages diffusion model inversion and\nself-attention to transfer style features effectively. Additionally, a region\ncontrol mechanism is introduced by applying a distance-based changing mask at\neach denoising step, enabling precise spatial control. To further enhance\nvisual quality, we incorporate a style enhancement module based on the Fourier\ntransform to reinforce stylistic richness. Extensive experiments demonstrate\nthat our method achieves superior performance in scene text style\ntransformation, outperforming existing state-of-the-art methods in both visual\nfidelity and text preservation.", "AI": {"tldr": "本文提出SceneTextStylizer，一个无需训练的基于扩散模型的框架，用于实现场景文本的灵活、局部化和高保真风格迁移。", "motivation": "尽管扩散模型在风格迁移方面取得显著进展，但场景文本的灵活和局部化风格编辑仍是一个未解决的挑战。现有方法通常仅限于内容替换或简单风格，缺乏自由风格迁移能力。", "method": "该方法是一个无需训练的扩散模型框架，通过提示词引导对文本区域进行风格转换，同时保持文本可读性和风格一致性。它设计了一个特征注入模块（利用扩散模型反演和自注意力机制）来有效传输风格特征；引入了一个区域控制机制（在每个去噪步骤应用基于距离的变化掩码）以实现精确的空间控制；并结合了一个基于傅里叶变换的风格增强模块来丰富风格。", "result": "广泛的实验表明，该方法在场景文本风格转换方面表现出色，在视觉保真度和文本保留方面均优于现有最先进的方法。", "conclusion": "SceneTextStylizer为场景文本的灵活和高保真风格迁移提供了一个新颖的解决方案，解决了现有方法在自由风格迁移和局部控制方面的局限性。"}}
{"id": "2510.10651", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10651", "abs": "https://arxiv.org/abs/2510.10651", "authors": ["Mohammad Hassan", "Mads R. Almassalkhi"], "title": "Aggregate Modeling of Air-Conditioner Loads Under Packet-based Control with Both On and Off Grid Access Requests", "comment": null, "summary": "Coordination of distributed energy resources (DERs) can engender flexibility\nnecessary to improve grid reliability. Packetized Energy Management (PEM) is a\nmethod for coordinating DERs, such as thermostatically controlled loads (TCLs)\nand electric vehicles, within customer quality-of-service (QoS) limits. In PEM,\na DER uses local information to offer flexibility by sending a request to the\nDER coordinator to turn-ON or turn-OFF. Much work has focused on modeling and\nanalyzing aggregations of DERs under PEM with fixed packet durations and only\nturn-ON requests. Different recent efforts to enable variable packet lengths\nhave shown an increase in available flexibility and ramping capability, but\nhave not been modeled in aggregate, which limits systematic analyses. To\naddress this issue, this paper presents a new aggregate bin-based (macro) model\nof PEM loads that incorporates both turn-ON and turn-OFF request features,\nenabling the model to accurately characterize the capability of the fleet of\nDERs to track a power reference signal, population temperature dynamics,\naggregate request rates, and variable packet lengths. Simulation-based\nvalidation is performed against an agent-based (micro) model to evaluate\nrobustness and quantify model accuracy. Finally, the distribution of variable\npacket lengths from macro-model simulations are applied to inform past work on\nPEM with randomized packet lengths", "AI": {"tldr": "本文提出了一种新的基于聚合箱（宏观）模型，用于分布式能源（DER）的包化能量管理（PEM），该模型整合了开启和关闭请求功能，并支持可变数据包长度，以更准确地表征DER群体的灵活性和电网可靠性。", "motivation": "现有的PEM模型在聚合DER分析中存在局限性，如固定数据包持续时间、仅考虑开启请求，以及可变数据包长度的努力未能进行聚合建模，这限制了系统性分析和对可用灵活性及斜坡能力提升的量化。", "method": "本文提出了一种新的基于聚合箱（宏观）模型，用于PEM负载。该模型融合了开启和关闭请求功能，能够准确刻画DER群体的功率参考信号跟踪能力、群体温度动态、聚合请求速率以及可变数据包长度。通过与基于代理（微观）模型的仿真进行验证，以评估模型的鲁棒性和量化准确性。最后，将宏观模型仿真中得到的可变数据包长度分布应用于以往关于随机数据包长度的PEM研究。", "result": "新的聚合模型能够准确表征DER群体跟踪功率参考信号、人口温度动态、聚合请求速率和可变数据包长度的能力。通过与基于代理的微观模型进行仿真验证，证实了模型的鲁棒性和准确性。", "conclusion": "该研究通过引入一个综合的聚合模型，克服了PEM研究中固定数据包长度和仅考虑开启请求的限制，实现了对包含开启/关闭请求和可变数据包长度的DER聚合行为的系统性分析，从而提升了对电网灵活性的理解和利用。"}}
{"id": "2510.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09881", "abs": "https://arxiv.org/abs/2510.09881", "authors": ["Minkwan Kim", "Seungmin Lee", "Junho Kim", "Young Min Kim"], "title": "LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates", "comment": null, "summary": "Recent advances in novel-view synthesis can create the photo-realistic\nvisualization of real-world environments from conventional camera captures.\nHowever, acquiring everyday environments from casual captures faces challenges\ndue to frequent scene changes, which require dense observations both spatially\nand temporally. We propose long-term Gaussian scene chronology from sparse-view\nupdates, coined LTGS, an efficient scene representation that can embrace\neveryday changes from highly under-constrained casual captures. Given an\nincomplete and unstructured Gaussian splatting representation obtained from an\ninitial set of input images, we robustly model the long-term chronology of the\nscene despite abrupt movements and subtle environmental variations. We\nconstruct objects as template Gaussians, which serve as structural, reusable\npriors for shared object tracks. Then, the object templates undergo a further\nrefinement pipeline that modulates the priors to adapt to temporally varying\nenvironments based on few-shot observations. Once trained, our framework is\ngeneralizable across multiple time steps through simple transformations,\nsignificantly enhancing the scalability for a temporal evolution of 3D\nenvironments. As existing datasets do not explicitly represent the long-term\nreal-world changes with a sparse capture setup, we collect real-world datasets\nto evaluate the practicality of our pipeline. Experiments demonstrate that our\nframework achieves superior reconstruction quality compared to other baselines\nwhile enabling fast and light-weight updates.", "AI": {"tldr": "本文提出了一种名为LTGS（长期高斯场景编年史）的场景表示方法，能够从高度受限的稀疏、随意捕获中高效地处理日常场景的长期变化，通过对象模板和精炼实现卓越的重建质量和快速更新。", "motivation": "现有新视角合成方法在从日常随意捕获中获取环境时面临挑战，因为场景变化频繁且需要空间和时间上的密集观测。这促使研究者寻找一种能从稀疏、非结构化数据中处理长期场景变化的高效表示方法。", "method": "LTGS从初始输入图像获得不完整的高斯溅射表示，并鲁棒地建模场景的长期编年史。它将对象构建为“模板高斯”，作为共享对象轨迹的结构化、可重用先验。这些对象模板经过进一步的精炼流程，根据少量观测调整先验以适应时间变化的环。该框架通过简单变换可泛化到多个时间步，并收集了新的真实世界数据集进行评估。", "result": "实验证明，与现有基线相比，LTGS框架实现了卓越的重建质量，同时支持快速轻量级的更新。新收集的数据集也验证了其在实际应用中的可行性。", "conclusion": "LTGS提供了一种高效、可扩展的场景表示，能够从高度受限的随意捕获中处理日常环境的长期变化，显著提升了3D环境时间演化的可扩展性，并达到了优异的重建效果和更新效率。"}}
{"id": "2510.10135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10135", "abs": "https://arxiv.org/abs/2510.10135", "authors": ["Zhongsheng Wang", "Ming Lin", "Zhedong Lin", "Yaser Shakib", "Qian Liu", "Jiamou Liu"], "title": "CharCom: Composable Identity Control for Multi-Character Story Illustration", "comment": "Accepted by ACM MMAsia 2025", "summary": "Ensuring character identity consistency across varying prompts remains a\nfundamental limitation in diffusion-based text-to-image generation. We propose\nCharCom, a modular and parameter-efficient framework that achieves\ncharacter-consistent story illustration through composable LoRA adapters,\nenabling efficient per-character customization without retraining the base\nmodel. Built on a frozen diffusion backbone, CharCom dynamically composes\nadapters at inference using prompt-aware control. Experiments on multi-scene\nnarratives demonstrate that CharCom significantly enhances character fidelity,\nsemantic alignment, and temporal coherence. It remains robust in crowded scenes\nand enables scalable multi-character generation with minimal overhead, making\nit well-suited for real-world applications such as story illustration and\nanimation.", "AI": {"tldr": "CharCom是一个模块化、参数高效的框架，通过可组合的LoRA适配器实现扩散模型中跨提示符的角色一致性，适用于故事插图和动画。", "motivation": "在扩散模型文本到图像生成中，跨不同提示符保持角色身份一致性是一个基本限制。", "method": "提出CharCom框架，它基于冻结的扩散主干，使用可组合的LoRA适配器实现每个角色的高效定制，无需重新训练基础模型。在推理时，CharCom通过提示感知控制动态组合这些适配器。", "result": "在多场景叙事实验中，CharCom显著增强了角色保真度、语义对齐和时间连贯性。它在拥挤场景中表现稳健，并能以最小开销实现可扩展的多角色生成。", "conclusion": "CharCom非常适合故事插图和动画等实际应用，解决了扩散模型中角色一致性问题，并提供了高效、可扩展的解决方案。"}}
{"id": "2510.09869", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09869", "abs": "https://arxiv.org/abs/2510.09869", "authors": ["Sil Hamilton", "Matthew Wilkens", "Andrew Piper"], "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking", "comment": null, "summary": "We present NarraBench, a theory-informed taxonomy of narrative-understanding\ntasks, as well as an associated survey of 78 existing benchmarks in the area.\nWe find significant need for new evaluations covering aspects of narrative\nunderstanding that are either overlooked in current work or are poorly aligned\nwith existing metrics. Specifically, we estimate that only 27% of narrative\ntasks are well captured by existing benchmarks, and we note that some areas --\nincluding narrative events, style, perspective, and revelation -- are nearly\nabsent from current evaluations. We also note the need for increased\ndevelopment of benchmarks capable of assessing constitutively subjective and\nperspectival aspects of narrative, that is, aspects for which there is\ngenerally no single correct answer. Our taxonomy, survey, and methodology are\nof value to NLP researchers seeking to test LLM narrative understanding.", "AI": {"tldr": "NarraBench是一个叙事理解任务的理论分类法，并对78个现有基准进行了调查，发现当前评估在叙事事件、风格、视角和揭示等多个方面存在显著空白，且缺乏对主观和视角性叙事方面的评估。", "motivation": "现有基准未能充分覆盖叙事理解的各个方面，许多重要方面被忽视或与现有指标不匹配，导致LLM的叙事理解能力评估不足。", "method": "本文提出了一个理论指导的叙事理解任务分类法（NarraBench），并基于此对78个现有基准进行了系统性调查。", "result": "研究发现，只有约27%的叙事任务能被现有基准很好地捕捉；叙事事件、风格、视角和揭示等领域几乎没有得到现有评估的覆盖。此外，还指出需要开发能评估叙事中固有的主观和视角性方面的基准。", "conclusion": "NarraBench分类法、调查和方法对寻求测试大型语言模型（LLM）叙事理解能力的自然语言处理研究人员具有重要价值，并揭示了未来基准开发的关键方向。"}}
{"id": "2510.09903", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09903", "abs": "https://arxiv.org/abs/2510.09903", "authors": ["Lenny Aharon", "Keemin Lee", "Karan Sikka", "Selmaan Chettih", "Cole Hurwitz", "Liam Paninski", "Matthew R Whiteway"], "title": "An uncertainty-aware framework for data-efficient multi-view animal pose estimation", "comment": null, "summary": "Multi-view pose estimation is essential for quantifying animal behavior in\nscientific research, yet current methods struggle to achieve accurate tracking\nwith limited labeled data and suffer from poor uncertainty estimates. We\naddress these challenges with a comprehensive framework combining novel\ntraining and post-processing techniques, and a model distillation procedure\nthat leverages the strengths of these techniques to produce a more efficient\nand effective pose estimator. Our multi-view transformer (MVT) utilizes\npretrained backbones and enables simultaneous processing of information across\nall views, while a novel patch masking scheme learns robust cross-view\ncorrespondences without camera calibration. For calibrated setups, we\nincorporate geometric consistency through 3D augmentation and a triangulation\nloss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to\nthe nonlinear case and enhance uncertainty quantification via a variance\ninflation technique. Finally, to leverage the scaling properties of the MVT, we\ndesign a distillation procedure that exploits improved EKS predictions and\nuncertainty estimates to generate high-quality pseudo-labels, thereby reducing\ndependence on manual labels. Our framework components consistently outperform\nexisting methods across three diverse animal species (flies, mice, chickadees),\nwith each component contributing complementary benefits. The result is a\npractical, uncertainty-aware system for reliable pose estimation that enables\ndownstream behavioral analyses under real-world data constraints.", "AI": {"tldr": "该研究提出了一个综合框架，通过新颖的训练、后处理技术和模型蒸馏程序，解决了多视角姿态估计中标记数据有限和不确定性估计差的问题，实现了对动物行为的准确、不确定性感知跟踪。", "motivation": "当前的多视角姿态估计方法在有限标记数据下难以实现准确跟踪，并且不确定性估计较差，这阻碍了动物行为研究的量化分析。", "method": "该框架包括：1) 多视角Transformer (MVT)，利用预训练骨干网络并同时处理所有视角信息；2) 新颖的补丁掩码方案，无需相机校准即可学习鲁棒的跨视角对应关系；3) 对于已校准设置，通过3D增强和三角测量损失整合几何一致性；4) 将现有的Ensemble Kalman Smoother (EKS) 后处理器扩展到非线性情况，并通过方差膨胀技术增强不确定性量化；5) 设计了一种蒸馏程序，利用改进的EKS预测和不确定性估计生成高质量伪标签，以减少对手动标签的依赖。", "result": "该框架的各个组件在三种不同的动物物种（苍蝇、小鼠、山雀）上均持续优于现有方法，且每个组件都提供了互补的优势。", "conclusion": "该研究提供了一个实用且具有不确定性意识的可靠姿态估计系统，能够在真实世界数据限制下支持下游行为分析。"}}
{"id": "2510.10820", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10820", "abs": "https://arxiv.org/abs/2510.10820", "authors": ["Maarten van der Hulst", "Rodrigo A. González", "Koen Classens", "Paul Tacx", "Nick Dirkx", "Jeroen van de Wijdeven", "Tom Oomen"], "title": "Structured identification of multivariable modal systems", "comment": "20 pages, 12 figures", "summary": "Physically interpretable models are essential for next-generation industrial\nsystems, as these representations enable effective control, support design\nvalidation, and provide a foundation for monitoring strategies. The aim of this\npaper is to develop a system identification framework for estimating modal\nmodels of complex multivariable mechanical systems from frequency response\ndata. To achieve this, a two-step structured identification algorithm is\npresented, where an additive model is first estimated using a refined\ninstrumental variable method and subsequently projected onto a modal form. The\ndeveloped identification method provides accurate, physically-relevant,\nminimal-order models, for both generally-damped and proportionally damped modal\nsystems. The effectiveness of the proposed method is demonstrated through\nexperimental validation on a prototype wafer-stage system, which features a\nlarge number of spatially distributed actuators and sensors and exhibits\ncomplex flexible dynamics.", "AI": {"tldr": "本文提出了一种系统辨识框架，用于从频率响应数据中估计复杂多变量机械系统的模态模型，该方法通过两步结构化算法获得准确、物理相关且最小阶的模型。", "motivation": "下一代工业系统需要物理可解释的模型，以实现有效的控制、支持设计验证并为监测策略提供基础。", "method": "采用两步结构化辨识算法：首先使用改进的工具变量法估计一个加性模型，然后将其投影到模态形式上。", "result": "所开发的辨识方法能够为一般阻尼和比例阻尼的模态系统提供准确、物理相关且最小阶的模型。该方法的有效性通过在一个具有大量分布式执行器和传感器并表现出复杂柔性动力学的原型晶圆平台系统上进行了实验验证。", "conclusion": "该研究成功开发了一种有效的系统辨识方法，能够为复杂多变量机械系统提供物理可解释的、准确且最小阶的模态模型，对于工业应用具有重要价值。"}}
{"id": "2510.10273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10273", "abs": "https://arxiv.org/abs/2510.10273", "authors": ["Vincent Schoenbach", "Marvin Wiedemann", "Raphael Memmesheimer", "Malte Mosbach", "Sven Behnke"], "title": "Integration of the TIAGo Robot into Isaac Sim with Mecanum Drive Modeling and Learned S-Curve Velocity Profiles", "comment": "In Proceedings of IEEE 21st International Conference on Automation\n  Science and Engineering (CASE), Los Angeles, USA, August 2025", "summary": "Efficient physics simulation has significantly accelerated research progress\nin robotics applications such as grasping and assembly. The advent of\nGPU-accelerated simulation frameworks like Isaac Sim has particularly empowered\nlearning-based methods, enabling them to tackle increasingly complex tasks. The\nPAL Robotics TIAGo++ Omni is a versatile mobile manipulator equipped with a\nmecanum-wheeled base, allowing omnidirectional movement and a wide range of\ntask capabilities. However, until now, no model of the robot has been available\nin Isaac Sim. In this paper, we introduce such a model, calibrated to\napproximate the behavior of the real robot, with a focus on its omnidirectional\ndrive dynamics. We present two control models for the omnidirectional drive: a\nphysically accurate model that replicates real-world wheel dynamics and a\nlightweight velocity-based model optimized for learning-based applications.\nWith these models, we introduce a learning-based calibration approach to\napproximate the real robot's S-shaped velocity profile using minimal trajectory\ndata recordings. This simulation should allow researchers to experiment with\nthe robot and perform efficient learning-based control in diverse environments.\nWe provide the integration publicly at https://github.com/AIS-Bonn/tiago_isaac.", "AI": {"tldr": "该论文为PAL Robotics TIAGo++ Omni机器人引入了一个经过校准的Isaac Sim模型，包括两种全向驱动控制模型，并提出了一种基于学习的校准方法，以实现对真实机器人行为的近似。", "motivation": "高效的物理仿真对机器人研究至关重要，特别是GPU加速仿真框架如Isaac Sim。然而，PAL Robotics TIAGo++ Omni这种多功能移动机械手在Isaac Sim中缺乏可用的模型，这限制了研究人员利用该平台进行基于学习的控制和实验。", "method": "作者引入了PAL Robotics TIAGo++ Omni在Isaac Sim中的模型，并对其进行校准以近似真实机器人的行为，特别关注其全向驱动动力学。他们提出了两种全向驱动控制模型：一个物理精确模型（复制真实世界车轮动力学）和一个轻量级基于速度的模型（为基于学习的应用优化）。此外，他们开发了一种基于学习的校准方法，利用最少的轨迹数据记录来近似真实机器人S形速度曲线。", "result": "论文成功地为PAL Robotics TIAGo++ Omni机器人创建了一个经过校准的Isaac Sim模型，并提供了两种全向驱动控制模型。同时，提出了一种基于学习的校准方法，能够使用少量数据逼近真实机器人的速度曲线。该集成已公开提供。", "conclusion": "该仿真模型将使研究人员能够使用PAL Robotics TIAGo++ Omni机器人在不同环境中进行实验，并高效地执行基于学习的控制，从而加速机器人技术的发展。"}}
{"id": "2510.10892", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10892", "abs": "https://arxiv.org/abs/2510.10892", "authors": ["Bukunmi Gabriel Odunlami", "Marcos Netto"], "title": "Observability and parameter estimation of a generic model for aggregated distributed energy resources", "comment": null, "summary": "We propose a novel framework for estimating the parameters of an aggregated\ndistributed energy resources (der_a) model. First, we introduce a rigorous\nmethod to determine whether all model parameters are estimable. When they are\nnot, our approach identifies the subset of parameters that can be estimated.\nThe proposed framework offers new insights into the number and specific\nparameters that can be reliably estimated based on commonly available\nmeasurements. It also highlights the limitations of calibrating such models.\nSecond, we introduce a Kalman filtering method to calibrate the der_a model.\nSince we account for nonlinear effects such as saturation and deadbands, we\ndevelop a specific mechanism to handle smoothing functions within the Kalman\nfilter. Specifically, we consider the extended and the unscented Kalman filter.\nWe demonstrate the effectiveness of the proposed framework on a modified IEEE\n34-node distribution feeder with inverter-based resources. Our findings align\nwith the North American Electric Reliability Corporation's parameterization\nguideline and underscore the importance of model calibration in accurately\ncapturing the collective dynamics of distributed energy resources installed on\ndistribution systems.", "AI": {"tldr": "该论文提出了一种新的框架，用于估计聚合分布式能源（DER_a）模型的参数，包括参数可估计性分析和基于卡尔曼滤波器的校准方法，以准确捕捉DER的集体动态。", "motivation": "准确捕捉配电系统中分布式能源的集体动态，理解模型校准的局限性，并提供关于可可靠估计参数数量和具体参数的新见解。", "method": "首先，引入一种严谨的方法来确定所有模型参数是否可估计，并识别可估计的参数子集。其次，开发了一种卡尔曼滤波方法（包括扩展卡尔曼滤波器和无迹卡尔曼滤波器）来校准DER_a模型，并针对饱和和死区等非线性效应设计了平滑函数处理机制。", "result": "该框架在修改后的IEEE 34节点配电馈线（包含基于逆变器的资源）上得到了有效验证。研究结果与北美电力可靠性公司（NERC）的参数化指南一致，强调了模型校准在准确捕捉DER集体动态方面的重要性。", "conclusion": "所提出的框架为DER_a模型参数估计和校准提供了有效的方法，能够提供关于可估计参数的深刻见解，并强调了模型校准在准确表示分布式能源集体动态中的关键作用。"}}
{"id": "2510.10168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10168", "abs": "https://arxiv.org/abs/2510.10168", "authors": ["Chengqian Gao", "Haonan Li", "Taylor W. Killian", "Jianshu She", "Renxi Wang", "Liqun Ma", "Zhoujun Cheng", "Shibo Hao", "Zhiqiang Xu"], "title": "Concise Reasoning in the Lens of Lagrangian Optimization", "comment": null, "summary": "Concise reasoning in large language models seeks to generate only essential\nintermediate steps needed to arrive at a final answer, thereby alleviating\nissues of overthinking. Most proposed approaches hinge on carefully\nhand-crafted heuristics, struggling to balance concision with performance,\noften failing to adapt across domains and model scales. In this work, we\naddress these challenges by introducing a principled and pragmatic strategy,\nperformance-aware length updating (PALU). As a principled algorithm, PALU\nformulates concise reasoning as a constrained optimization problem, minimizing\nresponse length subject to a performance constraint, and then applies\nLagrangian optimization to convert it into a tractable unconstrained problem.\nAs a pragmatic solution, PALU streamlines complicated update rules through\nthree approximations: (i) estimating performance with off-policy rollouts, (ii)\ntruncating the Lagrange multiplier to two extremes, and (iii) replacing\ngradient-based updates with quantile-driven length adjustments. PALU reduces\noutput length by 65% while improving accuracy by 15% when applied to\nDeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a\nrange of alternative methods. Furthermore, PALU is demonstrated to adapt across\nboth domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching\nthe algorithm as a practical and effective concise reasoning approach.", "AI": {"tldr": "本文提出了一种名为PALU（性能感知长度更新）的策略，通过将简洁推理建模为受限优化问题并使用拉格朗日优化，有效地在大型语言模型中实现了推理的简洁性，同时提升了性能和适应性。", "motivation": "大型语言模型中冗长的推理过程（“过度思考”）导致了效率问题，而现有的简洁推理方法大多依赖手工启发式规则，难以平衡简洁性与性能，且缺乏跨领域和模型规模的适应性。", "method": "PALU策略将简洁推理公式化为一个受性能约束的响应长度最小化问题，并利用拉格朗日优化将其转化为无约束问题。为了实用性，PALU通过三种近似简化了更新规则：(i) 使用离策略rollout估计性能，(ii) 将拉格朗日乘数截断为两个极端，(iii) 用分位数驱动的长度调整取代基于梯度的更新。", "result": "在DeepSeek-Distill-Qwen-1.5B模型上，跨五个基准测试，PALU将输出长度减少了65%，同时将准确率提高了15%，优于其他方法。此外，PALU还展现了在不同领域（逻辑、STEM、数学）和模型规模（1.5B、7B、14B）上的良好适应性。", "conclusion": "PALU是一种实用且有效的简洁推理方法，它通过其原则性的算法设计和务实的近似方法，显著减少了输出长度并提高了准确性，同时展现了出色的跨领域和模型规模适应性。"}}
{"id": "2510.09912", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.6; I.2.10; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.09912", "abs": "https://arxiv.org/abs/2510.09912", "authors": ["D. V. Brovko"], "title": "SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision", "comment": "The work consists of three chapters, includes 12 figures, 4 tables,\n  31 references, and 1 appendix. A version of this work has been accepted for\n  presentation at the 2025 IEEE 8th International Conference on Methods and\n  Systems of Navigation and Motion Control", "summary": "The relevance of this research lies in the growing demand for unmanned aerial\nvehicles (UAVs) capable of operating reliably in complex environments where\nconventional navigation becomes unreliable due to interference, poor\nvisibility, or camouflage. Hyperspectral imaging (HSI) provides unique\nopportunities for UAV-based computer vision by enabling fine-grained material\nrecognition and object differentiation, which are critical for navigation,\nsurveillance, agriculture, and environmental monitoring. The aim of this work\nis to develop a deep learning architecture integrating HSI into UAV perception\nfor navigation, object detection, and terrain classification. Objectives\ninclude: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional\narchitecture with spectral-spatial cross-attention, training, and benchmarking.\nThe methodology is based on the modification of the Mobile 3D Vision\nTransformer (MDvT) by introducing the proposed SpectralCA block. This block\nemploys bi-directional cross-attention to fuse spectral and spatial features,\nenhancing accuracy while reducing parameters and inference time. Experimental\nevaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed\nusing Overall Accuracy, Average Accuracy, and the Kappa coefficient. The\nfindings confirm that the proposed architecture improves UAV perception\nefficiency, enabling real-time operation for navigation, object recognition,\nand environmental monitoring tasks.\n  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging,\nunmanned aerial vehicle, object detection, semi-supervised learning.", "AI": {"tldr": "本研究旨在开发一种深度学习架构，将高光谱成像（HSI）集成到无人机（UAV）感知中，以实现复杂的导航、目标检测和地形分类，并通过引入SpectralCA块改进了Mobile 3D Vision Transformer，提高了效率和准确性。", "motivation": "随着无人机在复杂环境中（如存在干扰、能见度差或伪装）可靠运行的需求日益增长，传统导航方法变得不可靠。高光谱成像（HSI）通过实现精细的材料识别和目标区分，为基于无人机的计算机视觉提供了独特的机会，这对于导航、监视、农业和环境监测至关重要。", "method": "本研究开发了一种混合2D/3D卷积架构，带有光谱-空间交叉注意力。具体方法是修改Mobile 3D Vision Transformer (MDvT)，引入了所提出的SpectralCA块。该SpectralCA块采用双向交叉注意力来融合光谱和空间特征，旨在提高准确性，同时减少参数和推理时间。实验评估在WHU-Hi-HongHu数据集上进行，使用总体准确度（Overall Accuracy）、平均准确度（Average Accuracy）和Kappa系数评估结果。", "result": "研究结果证实，所提出的架构提高了无人机感知效率，使其能够实现导航、目标识别和环境监测任务的实时操作。", "conclusion": "本研究成功开发了一种结合高光谱成像的深度学习架构，显著提升了无人机在复杂环境中的感知能力，实现了导航、目标识别和环境监测等任务的实时高效运行。"}}
{"id": "2510.09871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09871", "abs": "https://arxiv.org/abs/2510.09871", "authors": ["Nafiseh Nikeghbal", "Amir Hossein Kargaran", "Jana Diesner"], "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs", "comment": "EMNLP 2025 (Oral)", "summary": "Improvements in model construction, including fortified safety guardrails,\nallow Large language models (LLMs) to increasingly pass standard safety checks.\nHowever, LLMs sometimes slip into revealing harmful behavior, such as\nexpressing racist viewpoints, during conversations. To analyze this\nsystematically, we introduce CoBia, a suite of lightweight adversarial attacks\nthat allow us to refine the scope of conditions under which LLMs depart from\nnormative or ethical behavior in conversations. CoBia creates a constructed\nconversation where the model utters a biased claim about a social group. We\nthen evaluate whether the model can recover from the fabricated bias claim and\nreject biased follow-up questions. We evaluate 11 open-source as well as\nproprietary LLMs for their outputs related to six socio-demographic categories\nthat are relevant to individual safety and fair treatment, i.e., gender, race,\nreligion, nationality, sex orientation, and others. Our evaluation is based on\nestablished LLM-based bias metrics, and we compare the results against human\njudgments to scope out the LLMs' reliability and alignment. The results suggest\nthat purposefully constructed conversations reliably reveal bias amplification\nand that LLMs often fail to reject biased follow-up questions during dialogue.\nThis form of stress-testing highlights deeply embedded biases that can be\nsurfaced through interaction. Code and artifacts are available at\nhttps://github.com/nafisenik/CoBia.", "AI": {"tldr": "尽管大型语言模型（LLMs）通过了标准安全检查，但在对话中仍可能表现出有害偏见。本文引入CoBia，一种轻量级对抗性攻击，通过构建对话来系统地揭示并评估LLMs从虚假偏见中恢复并拒绝偏见后续问题的能力。结果显示，LLMs常未能拒绝偏见后续问题，揭示了其深层偏见。", "motivation": "LLMs在通过标准安全检查后，仍可能在对话中偶然表现出有害行为，例如表达种族主义观点。研究旨在系统地分析LLMs在何种条件下偏离规范或道德行为。", "method": "引入CoBia，一套轻量级对抗性攻击，通过构建对话，让模型发表关于某个社会群体的偏见言论。然后评估模型是否能从这种虚构的偏见言论中恢复，并拒绝带有偏见的后续问题。评估了11个开源和专有LLMs，涉及性别、种族、宗教、国籍、性取向等六个社会人口类别。评估基于既定的LLM偏见指标，并与人类判断进行比较。", "result": "精心构建的对话能可靠地揭示偏见放大现象。LLMs在对话中常常未能拒绝带有偏见的后续问题。这种压力测试揭示了通过互动可以暴露出来的深层嵌入的偏见。", "conclusion": "CoBia这种对话压力测试方法能有效揭示LLMs中深层嵌入的偏见，即使模型通过了标准安全检查。LLMs在对话中难以从虚构的偏见中恢复并拒绝偏见后续问题，表明其偏见问题仍然存在且不易被常规方法发现。"}}
{"id": "2510.10274", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10274", "abs": "https://arxiv.org/abs/2510.10274", "authors": ["Jinliang Zheng", "Jianxiong Li", "Zhihao Wang", "Dongxiu Liu", "Xirui Kang", "Yuchun Feng", "Yinan Zheng", "Jiayin Zou", "Yilun Chen", "Jia Zeng", "Ya-Qin Zhang", "Jiangmiao Pang", "Jingjing Liu", "Tai Wang", "Xianyuan Zhan"], "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model", "comment": "preprint, technical report, 33 pages", "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/", "AI": {"tldr": "本文提出了一种名为“软提示”（Soft Prompt）的新方法，通过为每个数据源引入可学习的嵌入（即特定于载体的提示），将提示学习融入跨载体机器人学习中。基于此，开发了X-VLA架构，在多样化的模拟和真实机器人任务中实现了最先进的性能，展示了强大的灵活性和适应性。", "motivation": "成功的通用视觉-语言-动作（VLA）模型依赖于在多样化机器人平台上，利用大规模、跨载体、异构数据集进行有效训练。研究的动机在于如何促进并有效利用这些丰富多样的机器人数据源中的异构性。", "method": "本文提出了一种新颖的“软提示”方法，通过将提示学习概念融入跨载体机器人学习，并为每个不同的数据源引入单独的可学习嵌入集。这些嵌入充当特定于载体的提示，协同作用使VLA模型能够有效利用不同的跨载体特征。在此基础上，构建了一个名为X-VLA的基于流匹配的VLA架构，该架构完全依赖于软提示的标准Transformer编码器，兼具可扩展性和简洁性。", "result": "在6个模拟环境和3个真实世界机器人上进行评估后，本文提出的0.9B参数的X-VLA-0.9B模型在多个基准测试中同时实现了最先进（SOTA）的性能。它在从灵活的灵巧性到跨载体、环境和任务的快速适应性等广泛能力轴上均展现出卓越的结果。", "conclusion": "通过引入软提示方法和X-VLA架构，本文成功解决了利用异构跨载体机器人数据进行通用VLA模型训练的挑战。该方法不仅实现了最先进的性能，还展示了模型在不同载体、环境和任务中的强大泛化和适应能力。"}}
{"id": "2510.09924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09924", "abs": "https://arxiv.org/abs/2510.09924", "authors": ["Renjie Li", "Zihao Zhu", "Xiaoyu Wang", "Zhengzhong Tu"], "title": "HeadsUp! High-Fidelity Portrait Image Super-Resolution", "comment": null, "summary": "Portrait pictures, which typically feature both human subjects and natural\nbackgrounds, are one of the most prevalent forms of photography on social\nmedia. Existing image super-resolution (ISR) techniques generally focus either\non generic real-world images or strictly aligned facial images (i.e., face\nsuper-resolution). In practice, separate models are blended to handle portrait\nphotos: the face specialist model handles the face region, and the general\nmodel processes the rest. However, these blending approaches inevitably\nintroduce blending or boundary artifacts around the facial regions due to\ndifferent model training recipes, while human perception is particularly\nsensitive to facial fidelity. To overcome these limitations, we study the\nportrait image supersolution (PortraitISR) problem, and propose HeadsUp, a\nsingle-step diffusion model that is capable of seamlessly restoring and\nupscaling portrait images in an end-to-end manner. Specifically, we build our\nmodel on top of a single-step diffusion model and develop a face supervision\nmechanism to guide the model in focusing on the facial region. We then\nintegrate a reference-based mechanism to help with identity restoration,\nreducing face ambiguity in low-quality face restoration. Additionally, we have\nbuilt a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to\nsupport model training and benchmarking for portrait images. Extensive\nexperiments show that HeadsUp achieves state-of-the-art performance on the\nPortraitISR task while maintaining comparable or higher performance on both\ngeneral image and aligned face datasets.", "AI": {"tldr": "本文提出HeadsUp，一个单步扩散模型，用于无缝地恢复和放大肖像图片，解决了现有方法中人脸区域的混合伪影问题，并通过人脸监督和参考机制提升了修复质量。", "motivation": "现有的图像超分辨率技术要么专注于通用图像，要么专注于严格对齐的人脸图像。将这两种模型混合处理肖像照片时，由于训练方式不同，不可避免地会在人脸区域周围引入混合或边界伪影，而人类对人脸保真度特别敏感。", "method": "研究了肖像图像超分辨率（PortraitISR）问题，并提出了HeadsUp模型。该模型基于单步扩散模型，并开发了：1) 一种人脸监督机制，引导模型关注人脸区域；2) 一种基于参考的机制，帮助恢复身份并减少低质量人脸修复中的模糊性。此外，还构建了一个高质量的4K肖像图像ISR数据集PortraitSR-4K用于模型训练和基准测试。", "result": "广泛的实验表明，HeadsUp在PortraitISR任务上取得了最先进的性能，同时在通用图像和对齐人脸数据集上保持了可比或更高的性能。", "conclusion": "HeadsUp提供了一种端到端的单步扩散模型，能够无缝地恢复和放大肖像图像，有效解决了现有方法中混合伪影问题，并在人脸保真度和身份恢复方面表现出色，实现了肖像图像超分辨率领域的SOTA性能。"}}
{"id": "2510.10914", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10914", "abs": "https://arxiv.org/abs/2510.10914", "authors": ["Jiajie Qiu", "Dakota Thompson", "Kamal Youcef-Toumi", "Amro M. Farid"], "title": "Optimal Multi-Modal Transportation and Electric Power Flow: The Value of Coordinated Dynamic Operation", "comment": "31 pages, 9 figures", "summary": "The electrification of transportation represents a critical challenge in the\nglobal transition toward net-zero emissions, as the sector often accounts for\nmore than one-quarter of national energy consumption. Achieving this\ntransformation requires not only widespread adoption of electric vehicles (EVs)\nbut also their seamless integration into interdependent infrastructure\nsystems-specifically, the transportation-electricity nexus (TEN). This paper\ndevelops an optimal multi-modal transportation and electric power flow (OMTEPF)\nmodel to evaluate the benefits of coordinated, dynamic system operation.\nBuilding on recent advances in hetero-functional graph theory, the framework\nenables joint optimization of five key operational decisions in intelligent TEN\nmanagement: vehicle dispatch, route choice, charging station queuing,\ncoordinated charging, and vehicle-to-grid stabilization. The mesoscopic,\ndynamic model explicitly represents individual EVs and their state-of-charge\ntrajectories, thereby extending beyond the prevailing literature's focus on\nstatic, macroscopic traffic assignment. It further captures the full scope of\nthe TEN as a system-of-systems, incorporating five distinct charging\nmodalities: private residential, private commercial, wired public commercial,\ninductive public, and discharging. On the power system side, an IV-ACOPF\nformulation ensures globally optimal solutions to the electrical subproblems.\nComparative analysis demonstrates the substantial value of coordinated TEN\noperation relative to the status quo of siloed, uncoordinated infrastructure\nmanagement. This work provides both a novel methodological contribution and\nactionable insights for the co-design and operation of next-generation\nsustainable mobility-energy systems.", "AI": {"tldr": "本文提出了一种最优多模式交通和电力潮流（OMTEPF）模型，用于评估交通-电力耦合系统（TEN）中协调、动态运行的益处，旨在实现交通电气化和净零排放。", "motivation": "交通电气化是实现净零排放的关键挑战，该领域通常占国家能源消耗的四分之一以上。这不仅需要电动汽车的广泛普及，还需要其与相互依赖的基础设施系统（特别是交通-电力耦合系统TEN）的无缝集成。", "method": "该研究开发了一个最优多模式交通和电力潮流（OMTEPF）模型。该框架基于异功能图论的最新进展，能够联合优化智能TEN管理中的五个关键操作决策：车辆调度、路线选择、充电站排队、协调充电和车网稳定。该介观、动态模型明确表示了单个电动汽车及其荷电状态轨迹，并涵盖了TEN作为系统之系统的完整范围，包括五种不同的充电模式。在电力系统方面，采用IV-ACOPF公式确保电力子问题的全局最优解。", "result": "通过对比分析，该研究证明了协调的TEN运行相对于目前孤立、非协调的基础设施管理具有显著的价值。", "conclusion": "这项工作为下一代可持续移动-能源系统的协同设计和运行提供了新颖的方法论贡献和可操作的见解。"}}
{"id": "2510.09882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09882", "abs": "https://arxiv.org/abs/2510.09882", "authors": ["Vishal Anand", "Milad Alshomary", "Kathleen McKeown"], "title": "iBERT: Interpretable Style Embeddings via Sense Decomposition", "comment": null, "summary": "We present iBERT (interpretable-BERT), an encoder to produce inherently\ninterpretable and controllable embeddings - designed to modularize and expose\nthe discriminative cues present in language, such as stylistic and semantic\nstructure. Each input token is represented as a sparse, non-negative mixture\nover k context-independent sense vectors, which can be pooled into sentence\nembeddings or used directly at the token level. This enables modular control\nover representation, before any decoding or downstream use.\n  To demonstrate our model's interpretability, we evaluate it on a suite of\nstyle-focused tasks. On the STEL benchmark, it improves style representation\neffectiveness by ~8 points over SBERT-style baselines, while maintaining\ncompetitive performance on authorship verification. Because each embedding is a\nstructured composition of interpretable senses, we highlight how specific style\nattributes - such as emoji use, formality, or misspelling can be assigned to\nspecific sense vectors. While our experiments center on style, iBERT is not\nlimited to stylistic modeling. Its structural modularity is designed to\ninterpretably decompose whichever discriminative signals are present in the\ndata - enabling generalization even when supervision blends stylistic and\nsemantic factors.", "AI": {"tldr": "iBERT是一种可解释且可控的BERT编码器，通过将每个token表示为稀疏、非负的上下文无关语义向量混合，实现了对语言中判别性线索（如风格和语义）的模块化分解和暴露。它在风格任务上表现出色，并提高了可解释性。", "motivation": "现有BERT嵌入缺乏固有的可解释性和可控性，难以理解和操纵语言中包含的判别性线索（如文体和语义结构）。", "method": "iBERT采用编码器将每个输入token表示为k个上下文无关语义向量的稀疏、非负混合。这些向量可以聚合为句子嵌入或直接用于token级别，从而在解码或下游应用之前实现对表示的模块化控制。", "result": "在STEL基准测试中，iBERT将风格表示的有效性比SBERT基线提高了约8个百分点，同时在作者验证任务上保持了竞争力。其嵌入结构允许将特定的风格属性（如表情符号使用、正式性或拼写错误）分配给特定的语义向量。该模型的设计也适用于风格和语义因素混合的场景。", "conclusion": "iBERT成功地提供了一种固有的可解释和可控的嵌入方法，通过将语言分解为模块化的语义向量，有效揭示了判别性线索。它在风格相关任务中表现出强大的性能和可解释性，并具有泛化到其他判别信号的潜力。"}}
{"id": "2510.10332", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10332", "abs": "https://arxiv.org/abs/2510.10332", "authors": ["Kohio Deflesselle", "Mélodie Daniel", "Aly Magassouba", "Miguel Aranda", "Olivier Ly"], "title": "Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework", "comment": "4 pages, 3 figures, 2 tables, Accepted for Safety of Intelligent and\n  Autonomous Vehicles: Formal Methods vs. Machine Learning approaches for\n  reliable navigation (SIAV-FM2L) an IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025) workshop", "summary": "We present a deep reinforcement learning framework based on Soft Actor-Critic\n(SAC) for safe and precise maneuvering of double-Ackermann-steering mobile\nrobots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as\ndifferential-drive robots, DASMRs face strong kinematic constraints that make\nclassical planners brittle in cluttered environments. Our framework leverages\nthe Hindsight Experience Replay (HER) and the CrossQ overlay to encourage\nmaneuvering efficiency while avoiding obstacles. Simulation results with a\nheavy four-wheel-steering rover show that the learned policy can robustly reach\nup to 97% of target positions while avoiding obstacles. Our framework does not\nrely on handcrafted trajectories or expert demonstrations.", "AI": {"tldr": "本文提出了一种基于SAC、HER和CrossQ的深度强化学习框架，用于实现双阿克曼转向移动机器人（DASMRs）在复杂环境中的安全精确机动，无需人工轨迹或专家演示。", "motivation": "双阿克曼转向移动机器人（DASMRs）具有强大的运动学约束，使得传统的规划器在杂乱环境中表现脆弱，难以实现安全精确的机动。", "method": "该研究采用基于Soft Actor-Critic (SAC) 的深度强化学习框架，并结合了Hindsight Experience Replay (HER) 和 CrossQ 叠加来提高机动效率并避开障碍物。", "result": "仿真结果表明，所学习的策略能够使一辆重型四轮转向漫游车在避开障碍物的同时，稳健地达到高达97%的目标位置。", "conclusion": "该框架能够有效实现DASMRs的安全精确机动，且不依赖于人工设计的轨迹或专家演示。"}}
{"id": "2510.10193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10193", "abs": "https://arxiv.org/abs/2510.10193", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "title": "SAFER: Risk-Constrained Sample-then-Filter in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in risk-sensitive\napplications such as real-world open-ended question answering (QA), ensuring\nthe trustworthiness of their outputs has become critical. Existing selective\nconformal prediction (SCP) methods provide statistical guarantees by\nconstructing prediction sets with a constrained miscoverage rate for correct\nanswers. However, prior works unrealistically assume that admissible answers\nfor all instances can be obtained via finite sampling, even for open-ended QA\nscenarios that lack a fixed and finite solution space. To address this, we\nintroduce a two-stage risk control framework comprising abstention-aware\nsampling and conformalized filtering (SAFER). Firstly, on a held-out\ncalibration set, SAFER calibrates a sampling budget within the maximum sampling\ncap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,\nthe maximum allowable miscoverage rate of the sampling sets). If the risk level\ncannot be satisfied within the cap, we abstain; otherwise, the calibrated\nsampling budget becomes the minimum requirements at test time. Then, we employ\ncalibration instances where correct answers are attainable under the calibrated\nbudget and apply the conformal risk control method to determine a statistically\nvalid uncertainty threshold, which filters unreliable distractors from the\ncandidate set for each test data point. In this stage, SAFER introduces an\nadditional risk level to guide the calculation of the threshold, thereby\ncontrolling the risk of correct answers being excluded. Furthermore, we show\nthat SAFER is compatible with various task-specific admission criteria and\ncalibration-test split ratios, highlighting its robustness and high data\nefficiency.", "AI": {"tldr": "针对大型语言模型在开放式问答中输出可信度问题，SAFER提出两阶段风险控制框架，通过弃权感知采样和共形过滤，在无需有限解空间假设下提供统计保证，提高LLM在风险敏感应用中的可靠性。", "motivation": "大型语言模型（LLMs）在风险敏感应用（如开放式问答）中部署日益增多，确保其输出的可信度至关重要。现有选择性共形预测（SCP）方法假设所有实例的正确答案都可以通过有限采样获得，这对于缺乏固定有限解空间的开放式问答场景是不现实的。", "method": "SAFER是一个两阶段风险控制框架：\n1.  **弃权感知采样**：在校准集上，使用Clopper-Pearson精确方法校准采样预算，以满足用户期望的风险水平（采样集的最大未覆盖率）。如果无法满足风险，则选择弃权；否则，校准后的采样预算成为测试时的最低要求。\n2.  **共形过滤**：利用在校准预算下可获得正确答案的校准实例，应用共形风险控制方法确定统计有效的置信阈值，以过滤掉每个测试数据点候选集中的不可靠干扰项。此阶段引入额外的风险水平，控制正确答案被排除的风险。SAFER还与各种任务特定准入标准和校准-测试分割比例兼容。", "result": "SAFER框架解决了现有SCP方法在开放式问答中不切实际的有限采样假设。它在采样集未覆盖率和正确答案被排除的风险方面提供了统计保证。结果表明其鲁棒性和高数据效率。", "conclusion": "SAFER是一个鲁棒且数据高效的两阶段风险控制框架，通过弃权感知采样和共形过滤，为大型语言模型在风险敏感的开放式问答应用中提供统计保证，有效克服了现有方法在有限解空间假设上的局限性。"}}
{"id": "2510.09883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09883", "abs": "https://arxiv.org/abs/2510.09883", "authors": ["Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang", "Murali Annavarm"], "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.", "AI": {"tldr": "DELTA是一种无训练的稀疏注意力机制，通过分层注意力策略（初始全注意力、选择层识别关键token、后续稀疏注意力层）在推理任务中显著提升计算效率，同时保持或超越全注意力模型的准确性。", "motivation": "大型推理模型（LRMs）在生成长推理链时，解码成本高昂，因为每个新token都需关注整个不断增长的序列。现有的稀疏注意力方法通过修剪KV缓存来降低计算量，但在推理任务中由于累积选择错误和token动态重要性，导致准确性严重下降。", "method": "DELTA是一种无训练的稀疏注意力机制。它将Transformer层分为三组：初始层使用全注意力；一小部分“选择层”通过聚合的头级注意力分数识别关键token；随后的“稀疏注意力层”仅关注选定的token子集。这种设计在GPU内存中保留完整的KV缓存以确保准确性，同时避免了在许多层中进行昂贵的完整注意力计算。", "result": "在AIME和GPQA-Diamond等推理基准测试中，DELTA的准确性与全注意力模型持平或更高，同时将关注的token数量减少了多达5倍，并实现了1.5倍的端到端加速。", "conclusion": "研究结果表明，选择性重用中间注意力图为高效的长上下文推理提供了一条可靠的途径。"}}
{"id": "2510.11089", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11089", "abs": "https://arxiv.org/abs/2510.11089", "authors": ["Fabio Marco Monetti", "Adam Lundström", "Colin de Kwant", "Magnus Gyllenskepp", "Antonio Maffei"], "title": "Establishing assembly-oriented modular product architectures through Design for Assembly enhanced Modular Function Deployment", "comment": null, "summary": "Modular product design has become a strategic enabler for companies seeking\nto balance product variety, operational efficiency, and market responsiveness,\nmaking the alignment between modular architecture and manufacturing\nconsiderations increasingly critical. Modular Function Deployment (MFD) is a\nwidely adopted method for defining modular product architectures, yet it lacks\nsystematic support for assembly considerations during early concept and\nsystem-level development. This limitation increases the risk of delayed\nproduction ramp-up and lifecycle inefficiencies. This paper proposes a set of\nenhancements to MFD that integrate Design for Assembly (DFA) logic into\narchitectural synthesis. The extended method introduces structured heuristics,\nassembly-oriented module drivers, a coded interface taxonomy, and quantitative\nmetrics for assessing assembly feasibility and automation readiness. These\nadditions preserve compatibility with standard MFD workflows while enriching\ndecision-making with traceable, production-informed reasoning. An illustrative\ncase study involving a handheld leaf blower demonstrates the method's usability\nand effectiveness. The redesigned architecture shows reduced assembly effort,\nsimplified interfaces, and increased automation potential. By supporting\nearly-stage evaluation of architectural alternatives through an assembly lens,\nthe method enables faster transition to efficient volume production and\nprovides a foundation for continuous improvement throughout the product\nlifecycle.", "AI": {"tldr": "本文提出对模块化功能部署（MFD）方法进行改进，通过整合装配设计（DFA）逻辑，在产品架构早期开发阶段系统性地考虑装配因素，以提高生产效率和市场响应能力。", "motivation": "模块化产品设计在平衡产品多样性、运营效率和市场响应方面至关重要，但现有的MFD方法在早期概念和系统级开发中缺乏对装配因素的系统支持，这可能导致生产启动延迟和生命周期效率低下。", "method": "本文提出了一系列MFD增强措施，将装配设计（DFA）逻辑整合到架构合成中。这些增强包括引入结构化启发式方法、面向装配的模块驱动因素、编码接口分类法以及用于评估装配可行性和自动化准备度的量化指标。", "result": "通过一个手持式吹叶机案例研究，验证了该方法的可用性和有效性。重新设计的架构显示出装配工作量减少、接口简化以及自动化潜力增加的优点。", "conclusion": "该方法通过从装配角度支持架构替代方案的早期评估，能够更快地过渡到高效批量生产，并为产品生命周期中的持续改进奠定基础。"}}
{"id": "2510.10197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10197", "abs": "https://arxiv.org/abs/2510.10197", "authors": ["Siyuan Lu", "Zechuan Wang", "Hongxuan Zhang", "Qintong Wu", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu", "Tao Lin"], "title": "Don't Just Fine-tune the Agent, Tune the Environment", "comment": null, "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce $\\textbf{Environment Tuning}$, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\n$\\textbf{Environment Tuning}$ orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.", "AI": {"tldr": "本文提出“环境调优”这一新训练范式，使大型语言模型（LLM）智能体能直接从问题实例中学习复杂工具使用行为，克服了数据稀缺、SFT过拟合及RL冷启动问题，实现了卓越的泛化能力。", "motivation": "LLM智能体在复杂多轮工具使用任务中潜力巨大，但高质量训练数据极其稀缺。合成数据上的SFT会导致过拟合，而标准RL则面临严重的冷启动问题和训练不稳定性。", "method": "引入“环境调优”训练范式，使智能体无需预收集的专家轨迹即可直接从问题实例中学习。该范式通过结构化课程、提供纠正反馈的可操作环境增强以及细粒度进度奖励来协调学习过程，确保稳定高效的探索。", "result": "仅使用Berkeley Function-Calling Leaderboard (BFCL)基准中的400个问题实例，该方法不仅在同分布性能上与强基线模型竞争，而且在分布外泛化能力上表现卓越，克服了SFT方法常见的性能崩溃问题。", "conclusion": "这项工作代表了从基于静态轨迹的监督微调到基于动态环境探索的范式转变，为训练更鲁棒、数据效率更高的智能体铺平了道路。"}}
{"id": "2510.10337", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10337", "abs": "https://arxiv.org/abs/2510.10337", "authors": ["Jihong Zhu", "Kefeng Huang", "Jonathon Pipe", "Chris Horbaczewsky", "Andy Tyrrell", "Ian J. S. Fairlamb"], "title": "Rise of the Robochemist", "comment": "This article was originally published in the IEEE Systems, Man, and\n  Cybernetics Society eNewsletter, September 2025 issue:\n  https://www.ieeesmc.org/wp-content/uploads/2024/10/FeatureArticle_Sept25.pdf", "summary": "Chemistry, a long-standing discipline, has historically relied on manual and\noften time-consuming processes. While some automation exists, the field is now\non the cusp of a significant evolution driven by the integration of robotics\nand artificial intelligence (AI), giving rise to the concept of the\nrobochemist: a new paradigm where autonomous systems assist in designing,\nexecuting, and analyzing experiments. Robochemists integrate mobile\nmanipulators, advanced perception, teleoperation, and data-driven protocols to\nexecute experiments with greater adaptability, reproducibility, and safety.\nRather than a fully automated replacement for human chemists, we envisioned the\nrobochemist as a complementary partner that works collaboratively to enhance\ndiscovery, enabling a more efficient exploration of chemical space and\naccelerating innovation in pharmaceuticals, materials science, and sustainable\nmanufacturing. This article traces the technologies, applications, and\nchallenges that define this transformation, highlighting both the opportunities\nand the responsibilities that accompany the emergence of the robochemist.\nUltimately, the future of chemistry is argued to lie in a symbiotic partnership\nwhere human intuition and expertise is amplified by robotic precision and\nAI-driven insight.", "AI": {"tldr": "本文探讨了机器人化学家（robochemist）的概念，即结合机器人技术和人工智能，实现化学实验的自主设计、执行和分析，以加速化学发现和创新。", "motivation": "化学领域长期以来依赖手动且耗时的过程，现有自动化程度有限。研究动机在于通过整合机器人和人工智能，克服这些局限性，提高实验效率、重现性和安全性。", "method": "机器人化学家通过整合移动操作器、先进感知技术、远程操作和数据驱动协议来执行实验。它被设想为人类化学家的补充伙伴，而非完全替代。", "result": "机器人化学家能够以更高的适应性、重现性和安全性执行实验，从而更有效地探索化学空间，并加速制药、材料科学和可持续制造领域的创新。", "conclusion": "化学的未来在于人机共生伙伴关系，即人类的直觉和专业知识与机器人的精确性及人工智能驱动的洞察力相结合，共同推动化学发展。"}}
{"id": "2510.10205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10205", "abs": "https://arxiv.org/abs/2510.10205", "authors": ["Manjiang Yu", "Hongji Li", "Priyanka Singh", "Xue Li", "Di Wang", "Lijie Hu"], "title": "PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration", "comment": "18 pages,3 figures", "summary": "Reliable behavior control is central to deploying large language models\n(LLMs) on the web. Activation steering offers a tuning-free route to align\nattributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing\napproaches rely on coarse heuristics and lack a principled account of where to\nsteer and how strongly to intervene. To this end, we propose Position-wise\nInjection with eXact Estimated Levels (PIXEL), a position-wise activation\nsteering framework that, in contrast to prior work, learns a property-aligned\nsubspace from dual views (tail-averaged and end-token) and selects intervention\nstrength via a constrained geometric objective with a closed-form solution,\nthereby adapting to token-level sensitivity without global hyperparameter\ntuning. PIXEL further performs sample-level orthogonal residual calibration to\nrefine the global attribute direction and employs a lightweight\nposition-scanning routine to identify receptive injection sites. We\nadditionally provide representation-level guarantees for the\nminimal-intervention rule, supporting reliable alignment. Across diverse models\nand evaluation paradigms, PIXEL consistently improves attribute alignment while\npreserving model general capabilities, offering a practical and principled\nmethod for LLMs' controllable generation. Our code is available at\nhttps://github.com/V1centNevwake/PIXEL-Adaptive-Steering", "AI": {"tldr": "PIXEL是一种新颖的逐位置激活引导框架，通过学习属性对齐子空间、几何目标选择干预强度、正交残差校准和位置扫描，实现LLM可靠的行为控制，提升属性对齐同时保持模型通用能力。", "motivation": "在网络上部署大型语言模型（LLMs）时，可靠的行为控制至关重要。激活引导提供了一种无需微调的对齐属性（如真实性）以确保可信生成的方法。然而，现有方法依赖粗略的启发式规则，缺乏关于在何处以及如何强力干预的原则性解释。", "method": "本文提出了PIXEL（逐位置精确估计水平注入），一个逐位置激活引导框架。它通过双视图（尾部平均和末端词元）学习属性对齐的子空间，并通过具有闭式解的约束几何目标选择干预强度，从而适应词元级别的敏感性而无需全局超参数调优。PIXEL还进行样本级正交残差校准以优化全局属性方向，并采用轻量级位置扫描例程来识别合适的注入位置。此外，PIXEL为最小干预规则提供了表示层保证。", "result": "在不同的模型和评估范式中，PIXEL始终能改进属性对齐，同时保持模型的通用能力。", "conclusion": "PIXEL为LLM的可控生成提供了一种实用且有原则的方法，实现了可靠的行为控制和属性对齐。"}}
{"id": "2510.09934", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09934", "abs": "https://arxiv.org/abs/2510.09934", "authors": ["Nilesh Jain", "Elie Alhajjar"], "title": "Denoising Diffusion as a New Framework for Underwater Images", "comment": null, "summary": "Underwater images play a crucial role in ocean research and marine\nenvironmental monitoring since they provide quality information about the\necosystem. However, the complex and remote nature of the environment results in\npoor image quality with issues such as low visibility, blurry textures, color\ndistortion, and noise. In recent years, research in image enhancement has\nproven to be effective but also presents its own limitations, like poor\ngeneralization and heavy reliance on clean datasets. One of the challenges\nherein is the lack of diversity and the low quality of images included in these\ndatasets. Also, most existing datasets consist only of monocular images, a fact\nthat limits the representation of different lighting conditions and angles. In\nthis paper, we propose a new plan of action to overcome these limitations. On\none hand, we call for expanding the datasets using a denoising diffusion model\nto include a variety of image types such as stereo, wide-angled, macro, and\nclose-up images. On the other hand, we recommend enhancing the images using\nControlnet to evaluate and increase the quality of the corresponding datasets,\nand hence improve the study of the marine ecosystem.\n  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet", "AI": {"tldr": "针对水下图像质量差、现有增强方法局限性及数据集不足的问题，本文提出利用去噪扩散模型扩展多样化数据集，并使用Controlnet提升图像质量的方案。", "motivation": "水下图像对海洋研究至关重要，但复杂环境导致图像质量差（低能见度、模糊、色彩失真、噪声）。现有图像增强研究有效但有局限性（泛化性差、依赖干净数据）。主要挑战在于数据集缺乏多样性、质量低且多为单目图像，限制了不同光照和角度的表示。", "method": "本文提出一个新行动计划：一方面，呼吁使用去噪扩散模型扩展数据集，纳入立体、广角、微距和特写等多种图像类型；另一方面，建议使用Controlnet增强图像，以评估和提高相应数据集的质量。", "result": "本文提出了一项克服现有水下图像数据集局限性（多样性不足、质量低、单目图像为主）和增强方法限制的新行动计划。该计划旨在通过扩大数据集多样性和提升图像质量来改善海洋生态系统研究。", "conclusion": "通过实施所提出的利用去噪扩散模型扩展数据集和使用Controlnet提升图像质量的策略，有望克服水下图像增强和数据集的现有局限性，从而更好地支持海洋生态系统的研究。"}}
{"id": "2510.11181", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11181", "abs": "https://arxiv.org/abs/2510.11181", "authors": ["Tamme Emunds", "Paul Brunzema", "Sebastian Trimpe", "Nils Nießen"], "title": "Utilizing Bayesian Optimization for Timetable-Independent Railway Junction Performance Determination", "comment": null, "summary": "The efficiency of railway infrastructure is significantly influenced by the\nmix of trains that utilize it, as different service types have competing\noperational requirements. While freight services might require extended service\ntimes, passenger services demand more predictable schedules. Traditional\nmethods for addressing long-term traffic assignment problems often rely on\nfixed-value capacity limitations, determined based on specific assumptions\nabout traffic composition. This paper introduces a methodology for determining\ntimetable-independent capacity within the traffic rate assignment problem,\nenabling the calculation of junction capacities under dynamic traffic\ndistributions. We solve the underlying non-linear constrained optimization\nproblem maximizing the traffic throughput using Bayesian optimization (BO).\nThis setting combines a known objective function with expensive- to-compute\ncapacity constraints, motivating an adaption of standard BO problems, where\nobjective functions are usually unknown. We tailor the acquisition process in\nBO to this specific setting and increase performance by incorporating prior\nknowledge about the shape of the constraint functions into the Gaussian process\nsurrogate model. Our derived approaches are benchmarked on a railway junction\nnear Paris, significantly outperforming fixed traffic composition models and\nhighlighting the benefits of dynamic capacity allocation.", "AI": {"tldr": "本文提出了一种利用贝叶斯优化方法，在交通流量分配问题中确定独立于时刻表的动态铁路枢纽容量的方法，通过考虑动态交通分布和不同的列车服务需求，显著优于传统的固定容量模型。", "motivation": "铁路基础设施的效率受列车组合（客运和货运服务有不同的运营需求）影响显著。传统解决长期交通分配问题的方法依赖于基于特定交通组成假设的固定容量限制，这无法有效应对动态的交通分布。", "method": "本文提出一种在交通流量分配问题中确定独立于时刻表的容量的方法，从而能够在动态交通分布下计算枢纽容量。该方法通过贝叶斯优化（BO）解决了一个非线性约束优化问题，以最大化交通吞吐量。它针对目标函数已知但容量约束计算成本高昂的BO问题进行了调整，并通过将约束函数形状的先验知识融入高斯过程替代模型中，优化了采集过程以提高性能。", "result": "所提出的方法在巴黎附近的一个铁路枢纽进行了基准测试。结果显示，该方法显著优于固定交通组成模型，并突出了动态容量分配的优势。", "conclusion": "通过引入基于贝叶斯优化的动态、独立于时刻表的容量分配方法，可以有效提高铁路枢纽的效率，尤其是在处理混合列车交通时，其性能明显优于传统的固定容量模型。"}}
{"id": "2510.10346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10346", "abs": "https://arxiv.org/abs/2510.10346", "authors": ["Yuxiang Peng", "Chuchu Chen", "Kejian Wu", "Guoquan Huang"], "title": "sqrtVINS: Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking", "comment": null, "summary": "In this paper, we develop and open-source, for the first time, a square-root\nfilter (SRF)-based visual-inertial navigation system (VINS), termed sqrtVINS,\nwhich is ultra-fast, numerically stable, and capable of dynamic initialization\neven under extreme conditions (i.e., extremely small time window). Despite\nrecent advancements in VINS, resource constraints and numerical instability on\nembedded (robotic) systems with limited precision remain critical challenges. A\nsquare-root covariance-based filter offers a promising solution by providing\nnumerical stability, efficient memory usage, and guaranteed positive\nsemi-definiteness. However, canonical SRFs suffer from inefficiencies caused by\ndisruptions in the triangular structure of the covariance matrix during\nupdates. The proposed method significantly improves VINS efficiency with a\nnovel Cholesky decomposition (LLT)-based SRF update, by fully exploiting the\nsystem structure to preserve the structure. Moreover, we design a fast, robust,\ndynamic initialization method, which first recovers the minimal states without\ntriangulating 3D features and then efficiently performs iterative SRF update to\nrefine the full states, enabling seamless VINS operation. The proposed\nLLT-based SRF is extensively verified through numerical studies, demonstrating\nsuperior numerical stability and achieving robust efficient performance on\n32-bit single-precision floats, operating at twice the speed of\nstate-of-the-art (SOTA) methods. Our initialization method, tested on both\nmobile workstations and Jetson Nano computers, achieving a high success rate of\ninitialization even within a 100 ms window under minimal conditions. Finally,\nthe proposed sqrtVINS is extensively validated across diverse scenarios,\ndemonstrating strong efficiency, robustness, and reliability. The full\nopen-source implementation is released to support future research and\napplications.", "AI": {"tldr": "本文首次开发并开源了一种基于平方根滤波器（SRF）的视觉惯性导航系统（VINS），名为sqrtVINS，它具有超快、数值稳定和在极端条件下（如极小时间窗）动态初始化的能力。", "motivation": "尽管VINS最近取得了进展，但在资源受限且精度有限的嵌入式（机器人）系统上，资源限制和数值不稳定性仍然是关键挑战。传统的SRF在更新过程中会破坏协方差矩阵的三角结构，导致效率低下。", "method": "本文提出了一种新颖的基于Cholesky分解（LLT）的SRF更新方法，通过充分利用系统结构来保持三角结构，显著提高了VINS的效率。此外，设计了一种快速、鲁棒的动态初始化方法，该方法首先在不进行3D特征三角化的情况下恢复最小状态，然后有效地执行迭代SRF更新以完善完整状态。", "result": "通过数值研究，LLT-based SRF表现出卓越的数值稳定性，并在32位单精度浮点数上实现了鲁棒高效的性能，运行速度是现有最先进方法的两倍。初始化方法在移动工作站和Jetson Nano计算机上进行测试，即使在100毫秒的时间窗内也能实现高成功率。最终，sqrtVINS在多种场景下得到了广泛验证，展示了强大的效率、鲁棒性和可靠性。", "conclusion": "本文成功开发并开源了sqrtVINS，一个基于SRF的VINS，它通过创新的LLT-based SRF更新和动态初始化方法，克服了嵌入式系统上的数值不稳定性与效率挑战，实现了超快、数值稳定且鲁棒的性能，为未来的研究和应用提供了支持。"}}
{"id": "2510.09885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09885", "abs": "https://arxiv.org/abs/2510.09885", "authors": ["Xu Pan", "Ely Hahami", "Jingxuan Fan", "Ziqian Xie", "Haim Sompolinsky"], "title": "Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs", "comment": null, "summary": "Despite autoregressive large language models (arLLMs) being the current\ndominant paradigm in language modeling, they resist knowledge injection via\nfine-tuning due to inherent shortcomings such as the \"reversal curse\" -- the\nchallenge of answering questions that reverse the original information order in\nthe training sample. Masked diffusion large language models (dLLMs) are rapidly\nemerging as a powerful alternative to the arLLM paradigm, with evidence of\nbetter data efficiency and free of the \"reversal curse\" in pre-training.\nHowever, it is unknown whether these advantages extend to the post-training\nphase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through\nfine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,\nevaluating them with forward and backward style Question Answering (QA) to\nprobe knowledge generalization and the reversal curse. Our results confirm that\narLLMs critically rely on extensive data augmentation via paraphrases for QA\ngeneralization, and paraphrases are only effective when their information order\nmatches the QA style. Conversely, dLLMs achieve high accuracies on both forward\nand backward QAs without paraphrases; adding paraphrases yields only marginal\ngains. Lastly, inspired by the dLLM's performance, we introduce a novel masked\nfine-tuning paradigm for knowledge injection into pre-trained arLLMs. This\nproposed method successfully and drastically improves the data efficiency of\narLLM fine-tuning, effectively closing the performance gap with dLLMs.", "AI": {"tldr": "本文比较了自回归大语言模型 (arLLMs) 和掩码扩散大语言模型 (dLLMs) 在微调阶段的知识注入能力，特别关注“逆转诅咒”和数据效率。研究发现 dLLMs 表现更优，并提出了一种新的掩码微调范式，显著提升了 arLLMs 的数据效率。", "motivation": "自回归大语言模型 (arLLMs) 在知识注入（如微调）时存在固有缺陷，例如“逆转诅咒”。掩码扩散大语言模型 (dLLMs) 作为替代方案，在预训练阶段表现出更好的数据效率且没有“逆转诅咒”。然而，这些优势是否能延伸到后训练（微调）阶段尚不清楚。", "method": "研究人员在三个不同的数据集上对 arLLMs 和 dLLMs 进行微调，并使用正向和反向问答 (QA) 评估其知识泛化能力和“逆转诅咒”现象。此外，受 dLLMs 性能启发，提出了一种新的掩码微调范式用于预训练 arLLMs 的知识注入。", "result": "结果显示，arLLMs 严重依赖大量的释义数据增强才能实现 QA 泛化，且释义的信息顺序必须与 QA 风格匹配。相反，dLLMs 在没有释义的情况下也能在正向和反向 QA 上取得高准确率，添加释义仅带来微小收益。最后，提出的新型掩码微调方法显著提高了 arLLMs 微调的数据效率，有效缩小了与 dLLMs 的性能差距。", "conclusion": "dLLMs 在微调阶段的知识注入方面表现出优越性，尤其在数据效率和规避“逆转诅咒”方面。通过引入一种新颖的掩码微调范式，可以显著提升 arLLMs 的数据效率，使其性能与 dLLMs 相当。"}}
{"id": "2510.09936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09936", "abs": "https://arxiv.org/abs/2510.09936", "authors": ["Agampreet Aulakh", "Nils D. Forkert", "Matthias Wilms"], "title": "Semi-disentangled spatiotemporal implicit neural representations of longitudinal neuroimaging data for trajectory classification", "comment": "Accepted at the MICCAI 2025 Learning with Longitudinal Medical Images\n  and Data Workshop", "summary": "The human brain undergoes dynamic, potentially pathology-driven, structural\nchanges throughout a lifespan. Longitudinal Magnetic Resonance Imaging (MRI)\nand other neuroimaging data are valuable for characterizing trajectories of\nchange associated with typical and atypical aging. However, the analysis of\nsuch data is highly challenging given their discrete nature with different\nspatial and temporal image sampling patterns within individuals and across\npopulations. This leads to computational problems for most traditional deep\nlearning methods that cannot represent the underlying continuous biological\nprocess. To address these limitations, we present a new, fully data-driven\nmethod for representing aging trajectories across the entire brain by modelling\nsubject-specific longitudinal T1-weighted MRI data as continuous functions\nusing Implicit Neural Representations (INRs). Therefore, we introduce a novel\nINR architecture capable of partially disentangling spatial and temporal\ntrajectory parameters and design an efficient framework that directly operates\non the INRs' parameter space to classify brain aging trajectories. To evaluate\nour method in a controlled data environment, we develop a biologically grounded\ntrajectory simulation and generate T1-weighted 3D MRI data for 450 healthy and\ndementia-like subjects at regularly and irregularly sampled timepoints. In the\nmore realistic irregular sampling experiment, our INR-based method achieves\n81.3% accuracy for the brain aging trajectory classification task,\noutperforming a standard deep learning baseline model (73.7%).", "AI": {"tldr": "该研究提出一种基于隐式神经表示（INRs）的全新数据驱动方法，将纵向MRI数据建模为连续函数，以表征大脑衰老轨迹。该方法在不规则采样数据下，在脑衰老轨迹分类任务中优于传统深度学习基线模型。", "motivation": "人类大脑结构在整个生命周期中会发生动态变化，这些变化可能由病理驱动。纵向MRI数据对于描绘典型和非典型衰老轨迹至关重要。然而，此类数据具有离散性、个体间和群体间不同的时空图像采样模式，给分析带来巨大挑战。这导致大多数传统深度学习方法无法有效表示潜在的连续生物学过程。", "method": "研究通过使用隐式神经表示（INRs）将受试者特定的纵向T1加权MRI数据建模为连续函数，从而表示整个大脑的衰老轨迹。为此，引入了一种新颖的INR架构，能够部分解耦空间和时间轨迹参数，并设计了一个直接在INR参数空间操作的有效框架来分类脑衰老轨迹。为在受控数据环境中评估该方法，开发了一种生物学基础的轨迹模拟，并生成了450名健康和痴呆样受试者在规则和不规则采样时间点的T1加权3D MRI数据。", "result": "在更真实的非规则采样实验中，基于INR的方法在脑衰老轨迹分类任务中实现了81.3%的准确率，优于标准深度学习基线模型（73.7%）。", "conclusion": "该研究提出的基于INR的方法能够有效且数据驱动地表示大脑衰老轨迹，克服了传统方法处理离散和不规则采样的纵向神经影像数据的局限性，并在分类任务中展现出优越的性能，尤其是在非规则采样数据下。"}}
{"id": "2510.10207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10207", "abs": "https://arxiv.org/abs/2510.10207", "authors": ["Yujian Zhang", "Keyu Chen", "Zhifeng Shen", "Ruizhi Qiao", "Xing Sun"], "title": "Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning", "comment": null, "summary": "Although Long Reasoning Models (LRMs) have achieved superior performance on\nvarious reasoning scenarios, they often suffer from increased computational\ncosts and inference latency caused by overthinking. To address these\nlimitations, we propose Adaptive Dual Reasoner, which supports two reasoning\nmodes: fast thinking and slow thinking. ADR dynamically alternates between\nthese modes based on the contextual complexity during reasoning. ADR is trained\nin two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to\nequip the model with the ability to integrate both fast and slow reasoning\nmodes, in which we construct a hybrid reasoning dataset through a dedicated\npipeline to provide large-scale supervision. (2) A reinforcement learning stage\nfor optimizing reasoning effort, where we introduce Entropy-guided Hybrid\nPolicy Optimization EHPO, an RL training framework employing an entropy-guided\ndynamic rollout strategy for branching at high-entropy units and a\ndifficulty-aware penalty to balance fast and slow reasoning. Across challenging\nmathematical reasoning benchmarks, ADR achieves an effective balance between\nreasoning performance and efficiency among state-of-the-art approaches.\nSpecifically, ADR yields a performance gain of up to 6.1%, while reducing the\nreasoning output length by 49.5% to 59.3%.", "AI": {"tldr": "本文提出了自适应双重推理器（ADR），它通过动态切换快思和慢思两种推理模式，有效解决了长推理模型（LRMs）因“过度思考”导致的计算成本和推理延迟问题，在推理性能和效率之间取得了平衡。", "motivation": "长推理模型（LRMs）虽然在各种推理场景中表现出色，但往往由于“过度思考”而导致计算成本增加和推理延迟提高。", "method": "ADR支持快思和慢思两种推理模式，并根据推理过程中的上下文复杂性动态切换。其训练分为两个阶段：1) 冷启动阶段：通过有监督微调（SFT）和构建的混合推理数据集，使模型具备整合快慢推理模式的能力。2) 强化学习阶段：引入熵引导混合策略优化（EHPO）框架，利用熵引导动态展开策略在高熵单元进行分支，并结合难度感知惩罚来平衡快慢推理，从而优化推理工作量。", "result": "在具有挑战性的数学推理基准测试中，ADR在最先进的方法中实现了推理性能和效率之间的有效平衡。具体来说，ADR的性能提升高达6.1%，同时将推理输出长度减少了49.5%至59.3%。", "conclusion": "ADR通过自适应地结合快思和慢思模式，成功地提高了长推理模型在数学推理任务中的效率，同时保持了卓越的性能，有效解决了“过度思考”带来的成本和延迟问题。"}}
{"id": "2510.11286", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11286", "abs": "https://arxiv.org/abs/2510.11286", "authors": ["Jack Jackman", "David Ryan", "Arun Narayanan", "Pedro Nardelli", "Indrakshi Dey"], "title": "Edge-to-Cloud Computations-as-a-Service in Software-Defined Energy Networks for Smart Grids", "comment": null, "summary": "Modern power grids face an acute mismatch between where data is generated and\nwhere it can be processed: protection relays, EV (Electric Vehicle) charging,\nand distributed renewables demand millisecond analytics at the edge, while\nenergy-hungry workloads often sit in distant clouds leading to missed real-time\ndeadlines and wasted power. We address this by proposing, to our knowledge, the\nfirst-ever SDEN (Software Defined Energy Network) for CaaS\n(Computations-as-a-Service) that unifies edge, fog, and cloud compute with 5G\nURLLC (Ultra-Reliable Low-Latency Communications), SDN (Software Defined\nNetworking), and NFV (Network Functions Virtualization) to co-optimize energy,\nlatency, and reliability end-to-end. Our contributions are threefold: (i) a\njoint task offloading formulation that couples computation placement with\nnetwork capacity under explicit URLLC constraints; (ii) a feasibility\npreserving, lightweight greedy heuristic that scales while closely tracking\noptimal energy and latency trade-offs; and (iii) a tiered AI (Artificial\nIntelligence) pipeline-reactive at the edge, predictive in the fog, strategic\nin the cloud-featuring privacy-preserving, federated GNNs (Graph Neural\nNetworks) for fault detection and microgrid coordination. Unlike prior\nedge-only or cloud-only schemes, SDEN turns fragmented grid compute into a\nsingle, programmable substrate that delivers dependable, energy-aware, real\ntime analytics establishing a first-ever, software defined path to practical,\ngrid-scale CaaS.", "AI": {"tldr": "本文提出首个软件定义能源网络（SDEN），通过统一边缘、雾和云计算，结合5G URLLC、SDN和NFV，为电力网络提供计算即服务（CaaS），以共同优化能源、延迟和可靠性。", "motivation": "现代电网面临数据生成与处理地点之间的严重不匹配问题。保护继电器、电动汽车充电和分布式可再生能源需要在边缘进行毫秒级分析，而高能耗工作负载通常位于遥远的云端，导致实时截止日期延误和能源浪费。", "method": "该研究提出SDEN，结合5G URLLC、SDN和NFV，统一边缘、雾和云计算。具体方法包括：(i) 提出一个将计算放置与网络容量耦合，并考虑URLLC约束的联合任务卸载公式；(ii) 开发一种轻量级、可行性保持的贪婪启发式算法，能够扩展并紧密追踪最佳能源和延迟权衡；(iii) 设计一个分层AI管道（边缘反应式、雾预测式、云战略式），采用隐私保护的联邦图神经网络（GNN）进行故障检测和微电网协调。", "result": "SDEN将分散的电网计算资源转化为一个统一的、可编程的基础设施，提供可靠、节能的实时分析。它首次为实用、电网规模的CaaS建立了软件定义路径。研究表明，其提出的启发式算法能够紧密追踪最佳能源和延迟的权衡。", "conclusion": "SDEN通过将边缘、雾和云计算与先进的网络技术及分层AI相结合，解决了现代电网中数据处理的挑战，实现了端到端的能源、延迟和可靠性共同优化，为电网规模的计算即服务奠定了基础。"}}
{"id": "2510.10238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10238", "abs": "https://arxiv.org/abs/2510.10238", "authors": ["Zixuan Qin", "Kunlin Lyu", "Qingchen Yu", "Yifan Sun", "Zhaoxin Fan"], "title": "The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities", "comment": null, "summary": "Large Language Models (LLMs) have become foundational tools in natural\nlanguage processing, powering a wide range of applications and research. Many\nstudies have shown that LLMs share significant similarities with the human\nbrain. Recent neuroscience research has found that a small subset of biological\nneurons in the human brain are crucial for core cognitive functions, which\nraises a fundamental question: do LLMs also contain a small subset of critical\nneurons? In this paper, we investigate this question by proposing a\nPerturbation-based Causal Identification of Critical Neurons method to\nsystematically locate such critical neurons in LLMs. Our findings reveal three\nkey insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting\nthese critical neurons can cause a 72B-parameter model with over 1.1 billion\nneurons to completely collapse, with perplexity increasing by up to 20 orders\nof magnitude; (2) These critical neurons are not uniformly distributed, but\ntend to concentrate in the outer layers, particularly within the MLP down\\_proj\ncomponents; (3) Performance degradation exhibits sharp phase transitions,\nrather than a gradual decline, when these critical neurons are disrupted.\nThrough comprehensive experiments across diverse model architectures and\nscales, we provide deeper analysis of these phenomena and their implications\nfor LLM robustness and interpretability. These findings can offer guidance for\ndeveloping more robust model architectures and improving deployment security in\nsafety-critical applications.", "AI": {"tldr": "研究发现大型语言模型（LLMs）包含超稀疏的关键神经元集合，这些神经元的扰动会导致模型彻底崩溃，且性能下降表现出急剧的相变。", "motivation": "鉴于LLMs与人脑在功能上的相似性，以及神经科学研究发现人脑中一小部分神经元对核心认知功能至关重要，作者提出疑问：LLMs是否也包含这样一小部分关键神经元？", "method": "本文提出了一种基于扰动的因果识别关键神经元方法（Perturbation-based Causal Identification of Critical Neurons），以系统地定位LLMs中的关键神经元。", "result": "研究揭示了三个主要发现：(1) LLMs包含超稀疏的关键神经元集合，扰动这些神经元可导致拥有11亿以上神经元的72B参数模型彻底崩溃，困惑度增加多达20个数量级；(2) 这些关键神经元并非均匀分布，而是倾向于集中在外层，特别是在MLP的down_proj组件中；(3) 性能下降表现出急剧的相变，而非逐渐下降，当这些关键神经元被扰动时。", "conclusion": "这些发现为LLM的鲁棒性和可解释性提供了更深入的分析，并能为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供指导。"}}
{"id": "2510.09948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09948", "abs": "https://arxiv.org/abs/2510.09948", "authors": ["Pan Wang", "Yihao Hu", "Xiaodong Bai", "Aiping Yang", "Xiangxiang Li", "Meiping Ding", "Jianguo Yao"], "title": "A Multi-Strategy Framework for Enhancing Shatian Pomelo Detection in Real-World Orchards", "comment": null, "summary": "As a specialty agricultural product with a large market scale, Shatian pomelo\nnecessitates the adoption of automated detection to ensure accurate quantity\nand meet commercial demands for lean production. Existing research often\ninvolves specialized networks tailored for specific theoretical or dataset\nscenarios, but these methods tend to degrade performance in real-world. Through\nanalysis of factors in this issue, this study identifies four key challenges\nthat affect the accuracy of Shatian pomelo detection: imaging devices, lighting\nconditions, object scale variation, and occlusion. To mitigate these\nchallenges, a multi-strategy framework is proposed in this paper. Firstly, to\neffectively solve tone variation introduced by diverse imaging devices and\ncomplex orchard environments, we utilize a multi-scenario dataset,\nSTP-AgriData, which is constructed by integrating real orchard images with\ninternet-sourced data. Secondly, to simulate the inconsistent illumination\nconditions, specific data augmentations such as adjusting contrast and changing\nbrightness, are applied to the above dataset. Thirdly, to address the issues of\nobject scale variation and occlusion in fruit detection, an REAS-Det network is\ndesigned in this paper. For scale variation, RFAConv and C3RFEM modules are\ndesigned to expand and enhance the receptive fields. For occlusion variation, a\nmulti-scale, multi-head feature selection structure (MultiSEAM) and soft-NMS\nare introduced to enhance the handling of occlusion issues to improve detection\naccuracy. The results of these experiments achieved a precision(P) of 87.6%, a\nrecall (R) of 74.9%, a mAP@.50 of 82.8%, and a mAP@.50:.95 of 53.3%. Our\nproposed network demonstrates superior performance compared to other\nstate-of-the-art detection methods.", "AI": {"tldr": "本研究提出了一种多策略框架和REAS-Det网络，以解决沙田柚在真实果园环境中自动检测面临的成像设备、光照、目标尺度变化和遮挡等挑战，显著提高了检测精度。", "motivation": "沙田柚作为一种特色农产品，需要自动化检测来确保产量并满足精益生产的商业需求。现有研究中的专业网络在真实世界场景中性能往往下降，原因在于成像设备、光照条件、目标尺度变化和遮挡等因素影响了检测准确性。", "method": "本研究提出了一个多策略框架来应对挑战。首先，构建了一个多场景数据集STP-AgriData（整合真实果园和网络数据）以解决不同成像设备和复杂环境带来的色调变化。其次，通过调整对比度和亮度等数据增强来模拟不一致的光照条件。第三，设计了REAS-Det网络来解决目标尺度变化和遮挡问题：针对尺度变化，设计了RFAConv和C3RFEM模块以扩大和增强感受野；针对遮挡问题，引入了多尺度、多头特征选择结构（MultiSEAM）和soft-NMS。", "result": "实验结果显示，该方法在沙田柚检测中实现了87.6%的精确度（P）、74.9%的召回率（R）、82.8%的mAP@.50和53.3%的mAP@.50:.95。所提出的网络性能优于其他最先进的检测方法。", "conclusion": "本研究提出的多策略框架和REAS-Det网络能够有效应对真实世界中沙田柚检测所面临的成像设备、光照、尺度变化和遮挡等挑战，并取得了优于现有先进方法的检测性能。"}}
{"id": "2510.09887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09887", "abs": "https://arxiv.org/abs/2510.09887", "authors": ["Yijin Ni", "Peng Qi"], "title": "Abductive Preference Learning", "comment": null, "summary": "Frontier large language models such as GPT-5 and Claude Sonnet remain prone\nto overconfidence even after alignment through Reinforcement Learning with\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,\nthey tend to offer the same conservative answer \"No\" to both questions \"Can I\neat the [food / potato chips] that has been left out overnight?\" despite the\nlatter requiring no refridgeration for safe consumption. We find that this\nfailure is potentially attributed to a limitation of existing preference\nlearning: it emphasizes selecting the correct response for a given prompt,\nwhile neglecting counterfactual prompts that should alter the response.\n  To address this limitation, we propose abductive preference learning, a\nfine-tuning paradigm that reverses the conventional conditioning by learning\npreferences over prompts given a response. To validate this idea, we construct\nan abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,\nimplementing abductive DPO and its variant DPOP. Experiments reveal\ncomplementary strengths: standard methods improve response selection, abductive\nmethods improve prompt discrimination, while a multitask objective unifies\nboth. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\\%$ to\n$99.5\\%$ in response selection and $54.7\\%$ to $85.0\\%$ in prompt\ndiscrimination, with qualitative evidence highlighting improved sensitivity to\nprompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP\nimproves win rate (from $5.26\\%$ to $6.17\\%$), confirming that abductive\npreference learning preserves the benefits of conventional preference\noptimization while addressing the overlooked challenge of counterfactual\nprompts.", "AI": {"tldr": "大型语言模型即使经过对齐，仍存在过度自信问题，尤其是在面对反事实提示时。本文提出溯因偏好学习，通过在给定响应下学习提示偏好来提高提示辨别能力，并结合多任务目标显著提升了响应选择和提示辨别准确性。", "motivation": "前沿大型语言模型（如GPT-5和Claude Sonnet）在经过RLHF和DPO对齐后，仍然容易过度自信，例如对本应产生不同答案的提示给出相同的保守回答。研究认为这归因于现有偏好学习的局限性：它强调为给定提示选择正确响应，而忽略了应改变响应的反事实提示。", "method": "为解决上述局限性，本文提出了溯因偏好学习，这是一种逆转传统条件作用的微调范式，即在给定响应的情况下学习提示的偏好。为验证此想法，研究人员构建了一个包含1,001个条目的溯因数据集（源自HaluEval QA基准），并实现了溯因DPO及其变体DPOP。此外，还设计了一个多任务目标，以统一标准方法和溯因方法的优点。", "result": "实验表明，标准方法提高了响应选择能力，而溯因方法提高了提示辨别能力。多任务目标能够统一这两种优势。在溯因数据集上，多任务DPOP将响应选择准确率从90.0%提升至99.5%，将提示辨别准确率从54.7%提升至85.0%，并有定性证据表明模型对提示差异的敏感性有所提高。在AlpacaEval上的评估显示，多任务DPOP将胜率从5.26%提升至6.17%。", "conclusion": "溯因偏好学习保留了传统偏好优化的优势，同时解决了反事实提示这一被忽视的挑战。多任务DPOP通过有效提高提示辨别能力，显著改善了大型模型在处理反事实提示时的过度自信问题。"}}
{"id": "2510.09893", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09893", "abs": "https://arxiv.org/abs/2510.09893", "authors": ["Guanming Chen", "Lingzhi Shen", "Xiaohao Cai", "Imran Razzak", "Shoaib Jameel"], "title": "HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection", "comment": null, "summary": "Personality detection from text aims to infer an individual's personality\ntraits based on linguistic patterns. However, existing machine learning\napproaches often struggle to capture contextual information spanning multiple\nposts and tend to fall short in extracting representative and robust features\nin semantically sparse environments. This paper presents HIPPD, a\nbrain-inspired framework for personality detection that emulates the\nhierarchical information processing of the human brain. HIPPD utilises a large\nlanguage model to simulate the cerebral cortex, enabling global semantic\nreasoning and deep feature abstraction. A dynamic memory module, modelled after\nthe prefrontal cortex, performs adaptive gating and selective retention of\ncritical features, with all adjustments driven by dopaminergic prediction error\nfeedback. Subsequently, a set of specialised lightweight models, emulating the\nbasal ganglia, are dynamically routed via a strict winner-takes-all mechanism\nto capture the personality-related patterns they are most proficient at\nrecognising. Extensive experiments on the Kaggle and Pandora datasets\ndemonstrate that HIPPD consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "HIPPD是一个受大脑启发的个性检测框架，它利用大型语言模型、动态记忆模块和专门的轻量级模型，通过模拟大脑处理过程，在多个帖子中捕捉上下文信息并提取鲁棒特征，显著优于现有基线。", "motivation": "现有机器学习方法在从文本中检测个性时，难以捕捉跨多个帖子的上下文信息，并且在语义稀疏环境中提取代表性、鲁棒的特征方面表现不佳。", "method": "HIPPD框架模拟人脑的层次信息处理。它使用大型语言模型（模拟大脑皮层）进行全局语义推理和深度特征抽象；动态记忆模块（模拟前额叶皮层）通过多巴胺预测误差反馈进行自适应门控和关键特征选择性保留；一组专门的轻量级模型（模拟基底神经节）通过严格的“赢者通吃”机制动态路由，以捕捉它们最擅长识别的个性相关模式。", "result": "在Kaggle和Pandora数据集上进行的广泛实验表明，HIPPD始终优于最先进的基线方法。", "conclusion": "HIPPD通过模拟人脑的层次信息处理，有效解决了现有方法在个性检测中遇到的上下文捕捉和特征提取挑战，并取得了卓越的性能。"}}
{"id": "2510.11316", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11316", "abs": "https://arxiv.org/abs/2510.11316", "authors": ["Kaj Munhoz Arfvidsson", "Loizos Hadjiloizou", "Frank J. Jiang", "Karl H. Johansson", "Jonas Mårtensson"], "title": "pyspect: An Extensible Toolbox for Automatic Construction of Temporal Logic Trees via Reachability Analysis", "comment": "To be published in the 64th IEEE Conference on Decision and Control", "summary": "In this paper, we present pyspect, a Python toolbox that simplifies the use\nof reachability analysis for temporal logic problems. Currently, satisfying\ncomplex requirements in cyber-physical systems requires significant manual\neffort and domain expertise to develop the underlying reachability programs.\nThis high development effort limits the broader adoption of reachability\nanalysis for complex verification problems. To address this, pyspect provides a\nmethod-agnostic approach to performing reachability analysis for verifying a\ntemporal logic specification via temporal logic trees (TLTs). It enables the\nspecification of complex safety and liveness requirements using high-level\nlogic formulations that are independent of any particular reachability\ntechnique or set representation. As a result, pyspect allows for the comparison\nof different reachability implementations, such as Hamilton-Jacobi and Hybrid\nZonotope-based reachability analysis, for the same temporal logic\nspecification. This design separates the concerns of implementation developers\n(who develop numerical procedures for reachability) and end-users (who write\nspecifications). Through a simple vehicle example, we demonstrate how pyspect\nsimplifies the synthesis of reachability programs, promotes specification\nreusability, and facilitates side-by-side comparisons of reachability\ntechniques for complex tasks.", "AI": {"tldr": "pyspect是一个Python工具箱，它通过提供一种与具体方法无关的、基于时间逻辑树的方法，简化了将可达性分析应用于复杂时间逻辑问题的过程，从而降低了网络物理系统验证的开发难度并促进了技术比较。", "motivation": "在网络物理系统中满足复杂要求时，当前的可达性分析需要大量的体力劳动和领域专业知识来开发底层程序，这限制了可达性分析在复杂验证问题中的广泛应用。", "method": "pyspect采用一种与具体方法无关的方法，通过时间逻辑树（TLTs）对时间逻辑规范进行可达性分析验证。它允许使用高级逻辑公式来指定复杂安全性和活性要求，这些公式独立于任何特定的可达性技术或集合表示。这种设计将实现开发者（开发数值可达性程序）和最终用户（编写规范）的关注点分开。", "result": "pyspect简化了可达性程序的合成，提高了规范的可重用性，并促进了不同可达性技术（如Hamilton-Jacobi和基于混合Zonotope的可达性分析）对相同时间逻辑规范的并排比较。一个简单的车辆示例验证了这些优势。", "conclusion": "pyspect通过简化可达性分析程序的合成、提高规范的可重用性以及促进不同可达性技术的比较，有效降低了将可达性分析应用于复杂时间逻辑问题的开发难度，从而推广了其应用。"}}
{"id": "2510.10379", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10379", "abs": "https://arxiv.org/abs/2510.10379", "authors": ["Rohan Gupta", "Trevor Asbery", "Zain Merchant", "Abrar Anwar", "Jesse Thomason"], "title": "RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning", "comment": null, "summary": "Coordinating heterogeneous robot fleets to achieve multiple goals is\nchallenging in multi-robot systems. We introduce an open-source and extensible\nframework for centralized multi-robot task planning and scheduling that\nleverages LLMs to enable fleets of heterogeneous robots to accomplish multiple\ntasks. RobotFleet provides abstractions for planning, scheduling, and execution\nacross robots deployed as containerized services to simplify fleet scaling and\nmanagement. The framework maintains a shared declarative world state and\ntwo-way communication for task execution and replanning. By modularizing each\nlayer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet\nlowers the barrier to building scalable multi-robot systems. The code can be\nfound here: https://github.com/therohangupta/robot-fleet.", "AI": {"tldr": "本文介绍了一个名为RobotFleet的开源可扩展框架，它利用大型语言模型（LLMs）实现异构机器人集群的集中式任务规划和调度，以完成多重目标，并降低构建可扩展多机器人系统的门槛。", "motivation": "在多机器人系统中，协调异构机器人集群以实现多个目标是一项挑战。", "method": "RobotFleet框架提供规划、调度和执行的抽象层，将机器人部署为容器化服务以简化集群管理和扩展。它维护一个共享的声明性世界状态和用于任务执行与重新规划的双向通信。通过模块化自主堆栈的每一层并使用LLMs进行开放世界推理。", "result": "该框架使异构机器人集群能够完成多项任务，简化了集群的扩展和管理，并降低了构建可扩展多机器人系统的障碍。", "conclusion": "RobotFleet是一个利用LLMs进行开放世界推理的有效框架，能够实现异构机器人集群的集中式任务规划和调度，从而简化了可扩展多机器人系统的开发和管理。"}}
{"id": "2510.10357", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10357", "abs": "https://arxiv.org/abs/2510.10357", "authors": ["Yang Liu", "Bruno Da Costa", "Aude Billard"], "title": "Learning to Throw-Flip", "comment": "Accepted to IROS 2025. Video Summary: https://youtu.be/txYc9b1oflU", "summary": "Dynamic manipulation, such as robot tossing or throwing objects, has recently\ngained attention as a novel paradigm to speed up logistic operations. However,\nthe focus has predominantly been on the object's landing location, irrespective\nof its final orientation. In this work, we present a method enabling a robot to\naccurately \"throw-flip\" objects to a desired landing pose (position and\norientation). Conventionally, objects thrown by revolute robots suffer from\nparasitic rotation, resulting in highly restricted and uncontrollable landing\nposes. Our approach is based on two key design choices: first, leveraging the\nimpulse-momentum principle, we design a family of throwing motions that\neffectively decouple the parasitic rotation, significantly expanding the\nfeasible set of landing poses. Second, we combine a physics-based model of free\nflight with regression-based learning methods to account for unmodeled effects.\nReal robot experiments demonstrate that our framework can learn to throw-flip\nobjects to a pose target within ($\\pm$5 cm, $\\pm$45 degrees) threshold in\ndozens of trials. Thanks to data assimilation, incorporating projectile\ndynamics reduces sample complexity by an average of 40% when throw-flipping to\nunseen poses compared to end-to-end learning methods. Additionally, we show\nthat past knowledge on in-hand object spinning can be effectively reused,\naccelerating learning by 70% when throwing a new object with a Center of Mass\n(CoM) shift. A video summarizing the proposed method and the hardware\nexperiments is available at https://youtu.be/txYc9b1oflU.", "AI": {"tldr": "本文提出了一种机器人“抛掷翻转”物体到期望着陆姿态（位置和方向）的方法，通过解耦寄生旋转并结合物理模型与学习方法，显著提高了准确性、样本效率和知识迁移能力。", "motivation": "动态操作（如机器人抛掷）在物流中备受关注，但现有研究主要集中在物体着陆位置，忽略了最终姿态。传统旋转机器人抛掷会产生寄生旋转，导致着陆姿态受限且难以控制。", "method": "该方法基于两个关键设计选择：1. 利用冲量-动量原理设计抛掷动作，有效解耦寄生旋转，扩展了可行的着陆姿态集。2. 将基于物理的自由飞行模型与基于回归的学习方法相结合，以处理未建模效应。此外，还采用了数据同化来减少样本复杂度，并重用手内物体旋转的先验知识来加速学习。", "result": "真实机器人实验表明，该框架能在数十次试验中学会将物体“抛掷翻转”到（±5厘米，±45度）阈值内的目标姿态。与端到端学习方法相比，数据同化在处理未见姿态时平均减少了40%的样本复杂度。此外，当抛掷重心偏移的新物体时，重用手内物体旋转的先验知识可将学习速度提高70%。", "conclusion": "所提出的框架能够准确地将物体“抛掷翻转”到期望的着陆姿态，通过解耦寄生旋转和结合物理模型与学习方法，显著提高了操作的精度和鲁棒性，并展现了在样本效率和知识迁移方面的优势。"}}
{"id": "2510.09953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09953", "abs": "https://arxiv.org/abs/2510.09953", "authors": ["Salma J. Ahmed", "Emad A. Mohammed", "Azam Asilian Bidgoli"], "title": "J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training", "comment": null, "summary": "Image segmentation, the process of dividing images into meaningful regions,\nis critical in medical applications for accurate diagnosis, treatment planning,\nand disease monitoring. Although manual segmentation by healthcare\nprofessionals produces precise outcomes, it is time-consuming, costly, and\nprone to variability due to differences in human expertise. Artificial\nintelligence (AI)-based methods have been developed to address these\nlimitations by automating segmentation tasks; however, they often require\nlarge, annotated datasets that are rarely available in practice and frequently\nstruggle to generalize across diverse imaging conditions due to inter-patient\nvariability and rare pathological cases. In this paper, we propose Joint\nRetrieval Augmented Segmentation (J-RAS), a joint training method for guided\nimage segmentation that integrates a segmentation model with a retrieval model.\nBoth models are jointly optimized, enabling the segmentation model to leverage\nretrieved image-mask pairs to enrich its anatomical understanding, while the\nretrieval model learns segmentation-relevant features beyond simple visual\nsimilarity. This joint optimization ensures that retrieval actively contributes\nmeaningful contextual cues to guide boundary delineation, thereby enhancing the\noverall segmentation performance. We validate J-RAS across multiple\nsegmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two\nbenchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For\nexample, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice\nscore of 0.8708$\\pm$0.042 and a mean Hausdorff Distance (HD) of\n1.8130$\\pm$2.49, whereas with J-RAS, the performance improves substantially to\na mean Dice score of 0.9115$\\pm$0.031 and a mean HD of 1.1489$\\pm$0.30. These\nresults highlight the method's effectiveness and its generalizability across\narchitectures and datasets.", "AI": {"tldr": "本文提出J-RAS（联合检索增强分割），一种将分割模型与检索模型联合训练的方法，通过利用检索到的图像-掩码对和学习分割相关特征，显著提升了医学图像分割的性能和泛化能力。", "motivation": "医学图像手动分割耗时、昂贵且易受人为因素影响。现有AI分割方法需要大量标注数据，且在面对患者差异和罕见病理时泛化能力不足。", "method": "J-RAS是一种联合训练方法，它整合了分割模型和检索模型。两个模型共同优化：分割模型利用检索到的图像-掩码对增强解剖学理解，而检索模型学习超越简单视觉相似性的分割相关特征。这种联合优化确保检索能提供有意义的上下文线索来指导边界描绘。", "result": "J-RAS在U-Net、TransUNet、SAM和SegFormer等多种分割骨干网络上，以及ACDC和M&Ms两个基准数据集上进行了验证，均显示出一致的性能提升。例如，在ACDC数据集上，SegFormer结合J-RAS后，平均Dice分数从0.8708提高到0.9115，平均Hausdorff距离从1.8130降低到1.1489。", "conclusion": "J-RAS方法有效且在不同架构和数据集上具有良好的泛化能力，能够显著提高医学图像分割的性能，解决了现有AI方法在数据和泛化方面的局限性。"}}
{"id": "2510.10392", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10392", "abs": "https://arxiv.org/abs/2510.10392", "authors": ["Max Sokolich", "Yanda Yang", "Subrahmanyam Cherukumilli", "Fatma Ceren Kirmizitas", "Sambeeta Das"], "title": "MicroRoboScope: A Portable and Integrated Mechatronic Platform for Magnetic and Acoustic Microrobotic Experimentation", "comment": null, "summary": "This paper presents MicroRoboScope, a portable, compact, and versatile\nmicrorobotic experimentation platform designed for real-time, closed-loop\ncontrol of both magnetic and acoustic microrobots. The system integrates an\nembedded computer, microscope, power supplies, and control circuitry into a\nsingle, low-cost and fully integrated apparatus. Custom control software\ndeveloped in Python and Arduino C++ handles live video acquisition, microrobot\ntracking, and generation of control signals for electromagnetic coils and\nacoustic transducers. The platform's multi-modal actuation, accessibility, and\nportability make it suitable not only for specialized research laboratories but\nalso for educational and outreach settings. By lowering the barrier to entry\nfor microrobotic experimentation, this system enables new opportunities for\nresearch, education, and translational applications in biomedicine, tissue\nengineering, and robotics.", "AI": {"tldr": "本文介绍MicroRoboScope，一个便携、紧凑、多功能的微型机器人实验平台，可实现磁性与声学微型机器人的实时闭环控制。", "motivation": "现有微型机器人实验平台可能成本高昂或复杂，作者旨在降低微型机器人实验的门槛，使其适用于专业研究、教育和推广，从而促进生物医学等领域的发展。", "method": "MicroRoboScope系统集成了嵌入式计算机、显微镜、电源和控制电路。它采用定制的Python和Arduino C++软件，用于实时视频采集、微型机器人追踪以及电磁线圈和声学换能器的控制信号生成。", "result": "该平台实现了多模态驱动、高可访问性和便携性，不仅适用于专业研究实验室，也适用于教育和推广环境，有效降低了微型机器人实验的准入门槛。", "conclusion": "MicroRoboScope系统通过提供一个易于获取的平台，为生物医学、组织工程和机器人技术等领域的研究、教育和转化应用创造了新的机会。"}}
{"id": "2510.10285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10285", "abs": "https://arxiv.org/abs/2510.10285", "authors": ["Haolang Lu", "Bolun Chu", "WeiYe Fu", "Guoshun Nan", "Junning Liu", "Minghui Pan", "Qiankun Li", "Yi Yu", "Hua Wang", "Kun Wang"], "title": "Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control", "comment": "preprint", "summary": "Multimodal large reasoning models (MLRMs) are rapidly advancing\nvision-language reasoning and are emerging as a foundation for cross-modal\nintelligence. Hallucination remains a persistent failure mode, manifesting\nitself as erroneous reasoning chains and misinterpretation of visual content.\nIn this study, we observe that attention heads exhibit a staged division:\nshallow heads predominantly serve perception, while deeper heads shift toward\nsymbolic reasoning, revealing two major causes of hallucination, namely\nperceptual bias and reasoning drift. To address these issues, we propose a\nlightweight and interpretable two-step plugin, Functional Head Identification\nand Class-conditioned Rescaling, which locates perception- and\nreasoning-oriented heads and regulates their contributions without retraining.\nEvaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six\nbenchmarks across three domains, and four baselines show that our plugin\nachieves an average improvement of 5% and up to 15%, with only <1% additional\ncomputation and 9% of baseline latency. Our approach is completely\nmodel-agnostic and significantly enhances both the reliability and\ninterpretability of the off-the-shelf MLRMs, thereby enabling their safe\ndeployment in high-stakes applications. Our code is available at\nhttps://anonymous.4open.science/r/Functional-Attention-Control.", "AI": {"tldr": "多模态大型推理模型（MLRMs）存在幻觉问题，表现为感知偏差和推理漂移。本研究发现注意力头的功能分层，并提出一个轻量级、可解释的两步插件，通过识别和调节感知及推理导向的注意力头，无需重新训练即可显著提高MLRMs的可靠性和可解释性。", "motivation": "多模态大型推理模型（MLRMs）在视觉-语言推理方面进展迅速，但幻觉问题（如错误的推理链和对视觉内容的误解）仍然是一个普遍的失败模式。研究发现注意力头存在分阶段的功能划分：浅层头主要负责感知，深层头则转向符号推理，这揭示了幻觉的两个主要原因：感知偏差和推理漂移。", "method": "本研究提出一个轻量级且可解释的两步插件，名为“功能头识别和类别条件重缩放”（Functional Head Identification and Class-conditioned Rescaling）。该插件首先定位感知导向和推理导向的注意力头，然后调节它们对模型输出的贡献，整个过程无需重新训练模型。", "result": "在Kimi-VL、Ocean-R1、R1-Onevision这三个真实世界的MLRMs、跨三个领域的六个基准测试以及四个基线上的评估表明，该插件平均实现了5%的性能提升，最高可达15%，而额外计算量小于1%，延迟仅为基线的9%。该方法完全模型无关，显著增强了现成MLRMs的可靠性和可解释性。", "conclusion": "本研究提出的插件通过识别和调节MLRMs中注意力头的功能贡献，在不重新训练模型的情况下，显著提高了MLRMs的可靠性和可解释性，且计算开销极小。这使得MLRMs能够更安全地部署在高风险应用中。"}}
{"id": "2510.09913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09913", "abs": "https://arxiv.org/abs/2510.09913", "authors": ["Shangbin Feng", "Wenhao Yu", "Yike Wang", "Hongming Zhang", "Yulia Tsvetkov", "Dong Yu"], "title": "Don't Throw Away Your Pretrained Model", "comment": null, "summary": "Alignment training has tradeoffs: it helps language models (LMs) gain in\nreasoning and instruction following but might lose out on skills such as\ncreativity and calibration, where unaligned base models are better at. We aim\nto make the best of both worlds through model collaboration, where different\nmodels in the training pipeline collaborate and complement each other. Since LM\nresponses feature interleaving skills that favor different models, we propose\nSwitch Generation, where pretrained and aligned model versions take turns to\n``speak'' in a response sequence. Specifically, we train a switcher LM by\nlearning from outcomes of choosing different models to generate the next\nsegment across diverse queries and contexts. At inference time, the switcher LM\nguides different model checkpoints to dynamically generate the next segment\nwhere their strengths are most needed. Extensive experiments with 8 model\ncollaboration baselines and 18 datasets show that 1) model collaboration\nconsistently outperforms individual models on 16 out of 18 tasks, and 2) Switch\nGeneration further outperforms baselines by 12.9% on average. Further analysis\nreveals that Switch Generation discovers compositional skills to solve problems\nwhere individual models struggle and generalizes to unseen models and tasks,\nreusing and repurposing by-products in expensive model training pipelines that\nare otherwise discarded.", "AI": {"tldr": "本文提出“切换生成”（Switch Generation）方法，通过模型协作结合对齐和未对齐语言模型的优点，以应对对齐训练带来的能力权衡问题。", "motivation": "语言模型对齐训练在提高推理和指令遵循能力的同时，可能牺牲创造性和校准等技能，而未对齐的基础模型在这些方面表现更好。研究旨在通过模型协作，结合两者的优势。", "method": "提出“切换生成”方法，其中预训练和对齐模型版本轮流生成响应序列的不同片段。通过训练一个“切换器语言模型”（switcher LM），该模型学习在不同查询和上下文中选择不同模型生成下一个片段的结果。在推理时，切换器LM动态指导不同模型检查点生成最需要其优势的片段。", "result": "在8种模型协作基线和18个数据集上进行实验，结果显示：1) 模型协作在18个任务中的16个上持续优于单个模型；2) 切换生成平均比基线高出12.9%。进一步分析表明，切换生成发现了组合技能来解决单个模型难以解决的问题，并能泛化到未见过的模型和任务，复用和重新利用了昂贵模型训练流程中原本会被丢弃的副产品。", "conclusion": "切换生成通过模型协作有效结合了不同语言模型版本的优势，克服了对齐训练的权衡问题，并在性能和泛化能力上表现出色，同时提高了训练资源的利用效率。"}}
{"id": "2510.09996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09996", "abs": "https://arxiv.org/abs/2510.09996", "authors": ["Lishen Qu", "Zhihao Liu", "Shihao Zhou", "Yaqi Luo", "Jie Liang", "Hui Zeng", "Lei Zhang", "Jufeng Yang"], "title": "BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes", "comment": "Accepted by NeurIPS 2025", "summary": "Flicker artifacts in short-exposure images are caused by the interplay\nbetween the row-wise exposure mechanism of rolling shutter cameras and the\ntemporal intensity variations of alternating current (AC)-powered lighting.\nThese artifacts typically appear as uneven brightness distribution across the\nimage, forming noticeable dark bands. Beyond compromising image quality, this\nstructured noise also affects high-level tasks, such as object detection and\ntracking, where reliable lighting is crucial. Despite the prevalence of\nflicker, the lack of a large-scale, realistic dataset has been a significant\nbarrier to advancing research in flicker removal. To address this issue, we\npresent BurstDeflicker, a scalable benchmark constructed using three\ncomplementary data acquisition strategies. First, we develop a Retinex-based\nsynthesis pipeline that redefines the goal of flicker removal and enables\ncontrollable manipulation of key flicker-related attributes (e.g., intensity,\narea, and frequency), thereby facilitating the generation of diverse flicker\npatterns. Second, we capture 4,000 real-world flicker images from different\nscenes, which help the model better understand the spatial and temporal\ncharacteristics of real flicker artifacts and generalize more effectively to\nwild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we\npropose a green-screen method to incorporate motion into image pairs while\npreserving real flicker degradation. Comprehensive experiments demonstrate the\neffectiveness of our dataset and its potential to advance research in flicker\nremoval.", "AI": {"tldr": "该论文提出了一个名为BurstDeflicker的大规模、可扩展的去闪烁基准数据集，通过三种互补的数据采集策略解决现有闪烁去除研究中缺乏真实世界数据集的问题。", "motivation": "短曝光图像中的闪烁伪影由卷帘快门相机和交流供电照明引起，表现为不均匀的亮度分布和暗带，严重影响图像质量和目标检测、跟踪等高级任务。现有研究进展受限于缺乏大规模、真实的闪烁去除数据集。", "method": "该研究采用三种数据采集策略构建BurstDeflicker数据集：1) 开发基于Retinex的合成管道，可控地生成多样化的闪烁模式；2) 捕获4,000张真实场景的闪烁图像，以理解真实闪烁的空间和时间特性；3) 提出绿幕方法，在保留真实闪烁退化的同时将运动融入图像对。", "result": "全面的实验证明了所构建数据集的有效性。", "conclusion": "该数据集有望推动闪烁去除领域的研究进展。"}}
{"id": "2510.09915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09915", "abs": "https://arxiv.org/abs/2510.09915", "authors": ["Sicong Huang", "Qianqi Yan", "Shengze Wang", "Ian Lane"], "title": "Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning", "comment": null, "summary": "Abstractive summarization using large language models (LLMs) has become an\nessential tool for condensing information. However, despite their ability to\ngenerate fluent summaries, these models sometimes produce unfaithful summaries,\nintroducing hallucinations at the word, phrase, or concept level. Existing\nmitigation strategies, such as post-processing corrections or contrastive\nlearning with synthetically generated negative samples, fail to fully address\nthe diverse errors that can occur in LLM-generated summaries. In this paper, we\ninvestigate fine-tuning strategies to reduce the occurrence of unfaithful spans\nin generated summaries. First, we automatically generate summaries for the set\nof source documents in the training set with a variety of LLMs and then use\nGPT-4o to annotate any hallucinations it detects at the span-level. Leveraging\nthese annotations, we fine-tune LLMs with both hallucination-free summaries and\nannotated unfaithful spans to enhance model faithfulness. In this paper, we\nintroduce a new dataset that contains both faithful and unfaithful summaries\nwith span-level labels and we evaluate three techniques to fine-tuning a LLM to\nimprove the faithfulness of the resulting summarization: gradient ascent,\nunlikelihood training, and task vector negation. Experimental results show that\nall three approaches successfully leverage span-level annotations to improve\nfaithfulness, with unlikelihood training being the most effective.", "AI": {"tldr": "本研究通过利用GPT-4o生成的幻觉跨度级标注，探索并评估了多种微调策略，以提高大型语言模型在抽象摘要生成中的忠实度，其中非可能性训练效果最佳。", "motivation": "尽管大型语言模型（LLMs）能生成流畅的抽象摘要，但它们常产生不忠实的摘要，引入词语、短语或概念层面的幻觉。现有缓解策略（如后处理或对比学习）未能完全解决LLM生成摘要中出现的各种错误。", "method": "首先，使用多种LLMs为训练集中的源文档自动生成摘要，然后利用GPT-4o对检测到的幻觉进行跨度级标注。接着，利用这些标注（包括无幻觉摘要和带标注的不忠实跨度）微调LLMs。论文引入了一个包含忠实和不忠实摘要以及跨度级标签的新数据集，并评估了三种微调技术来提高摘要的忠实度：梯度上升、非可能性训练和任务向量否定。", "result": "实验结果表明，所有三种方法都成功利用了跨度级标注来提高忠实度，其中非可能性训练（unlikelihood training）被证明是最有效的方法。", "conclusion": "通过利用GPT-4o生成的幻觉跨度级标注，并结合特定的微调策略（特别是非可能性训练），可以显著提高大型语言模型在抽象摘要生成中的忠实度，有效减少幻觉的发生。"}}
{"id": "2510.11386", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11386", "abs": "https://arxiv.org/abs/2510.11386", "authors": ["Yuechen Liu", "Boqi Meng"], "title": "High-Order Quarter-Wave Plate Optimization for Linear Birefringence Suppression in Reflective FOCS", "comment": null, "summary": "Fiber optic current sensors (FOCS) are widely adopted in modern power grids\ndue to high sensitivity, excellent insulation, and strong immunity to\nelectromagnetic interference. This prominence necessitates precise\ninvestigation into their error sources and corresponding optimization. This\nstudy examines reflective FOCS based on the Faraday effect. A theoretical model\nis established to simulate phase error caused by linear birefringence from the\nquarter-wave plate. Conventional methods using circular birefringence are\nanalyzed, revealing inherent limitations. Innovatively, a compensation strategy\nemploying high-order quarter-wave plates is proposed to effectively eliminate\nlinear birefringence effects. This approach significantly enhances the accuracy\nand practicality of FOCS in precision metrology.", "AI": {"tldr": "本研究提出了一种利用高阶四分之一波片补偿策略，有效消除反射式光纤电流传感器中线性双折射引起的相位误差，显著提高了传感器的精度和实用性。", "motivation": "光纤电流传感器（FOCS）因其高灵敏度、优异绝缘性和强抗电磁干扰能力而被广泛应用于现代电网。然而，为了进一步优化其性能，需要精确研究其误差源。特别是，由四分之一波片引起的线性双折射会导致相位误差，限制了传感器的精度。", "method": "本研究针对基于法拉第效应的反射式FOCS，建立了模拟由四分之一波片引起的线性双折射导致相位误差的理论模型。分析了传统使用圆双折射的方法的局限性。创新性地提出了一种采用高阶四分之一波片进行补偿的策略，以有效消除线性双折射效应。", "result": "所提出的高阶四分之一波片补偿策略能够有效消除线性双折射效应。这显著增强了FOCS在精密计量中的准确性和实用性。", "conclusion": "通过引入高阶四分之一波片补偿策略，可以有效消除反射式光纤电流传感器中由线性双折射引起的相位误差，从而大幅提升传感器的精度和实际应用价值。"}}
{"id": "2510.10331", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10331", "abs": "https://arxiv.org/abs/2510.10331", "authors": ["Hanchen Su", "Wei Luo", "Wei Han", "Yu Elaine Liu", "Yufeng Wayne Zhang", "Cen Mia Zhao", "Ying Joy Zhang", "Yashar Mehdad"], "title": "LLM-Friendly Knowledge Representation for Customer Support", "comment": null, "summary": "We propose a practical approach by integrating Large Language Models (LLMs)\nwith a framework designed to navigate the complexities of Airbnb customer\nsupport operations. In this paper, our methodology employs a novel reformatting\ntechnique, the Intent, Context, and Action (ICA) format, which transforms\npolicies and workflows into a structure more comprehensible to LLMs.\nAdditionally, we develop a synthetic data generation strategy to create\ntraining data with minimal human intervention, enabling cost-effective\nfine-tuning of our model. Our internal experiments (not applied to Airbnb\nproducts) demonstrate that our approach of restructuring workflows and\nfine-tuning LLMs with synthetic data significantly enhances their performance,\nsetting a new benchmark for their application in customer support. Our solution\nis not only cost-effective but also improves customer support, as evidenced by\nboth accuracy and manual processing time evaluation metrics.", "AI": {"tldr": "本文提出了一种将大型语言模型（LLMs）与意图、上下文和行动（ICA）格式以及合成数据生成策略相结合的实用方法，旨在提升爱彼迎客户支持的效率和成本效益。", "motivation": "研究动机是为了解决爱彼迎客户支持操作中的复杂性，并通过提高大型语言模型在此领域的性能，同时降低成本，来改善客户支持体验。", "method": "本文的方法包括：1. 整合LLMs与一个为爱彼迎客户支持设计的框架。2. 采用一种新颖的重构技术，即意图、上下文和行动（ICA）格式，将政策和工作流程转换为LLMs更易理解的结构。3. 开发一种合成数据生成策略，以最少的人工干预创建训练数据，从而实现模型经济高效的微调。", "result": "内部实验（未应用于爱彼迎产品）表明，通过重构工作流程并使用合成数据微调LLMs的方法显著提升了它们的性能，为客户支持领域的应用树立了新基准。该解决方案不仅成本效益高，而且通过准确性和人工处理时间评估指标，证明能有效改善客户支持。", "conclusion": "结论是，所提出的方法通过将LLMs与ICA格式和合成数据生成策略相结合，为客户支持操作提供了一个实用且成本效益高的解决方案，显著提高了LLMs在该领域的性能，并改善了客户支持体验。"}}
{"id": "2510.09995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09995", "abs": "https://arxiv.org/abs/2510.09995", "authors": ["Lishen Qu", "Zhihao Liu", "Jinshan Pan", "Shihao Zhou", "Jinglei Shi", "Duosheng Chen", "Jufeng Yang"], "title": "FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering", "comment": "Accepted by NeurIPS 2025", "summary": "Lens flare occurs when shooting towards strong light sources, significantly\ndegrading the visual quality of images. Due to the difficulty in capturing\nflare-corrupted and flare-free image pairs in the real world, existing datasets\nare typically synthesized in 2D by overlaying artificial flare templates onto\nbackground images. However, the lack of flare diversity in templates and the\nneglect of physical principles in the synthesis process hinder models trained\non these datasets from generalizing well to real-world scenarios. To address\nthese challenges, we propose a new physics-informed method for flare data\ngeneration, which consists of three stages: parameterized template creation,\nthe laws of illumination-aware 2D synthesis, and physical engine-based 3D\nrendering, which finally gives us a mixed flare dataset that incorporates both\n2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates\nderived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D\nscenes. Furthermore, we design a masking approach to obtain real-world\nflare-free images from their corrupted counterparts to measure the performance\nof the model on real-world images. Extensive experiments demonstrate the\neffectiveness of our method and dataset.", "AI": {"tldr": "本文提出了一种物理信息驱动的耀斑数据生成方法，通过参数化模板、光照感知2D合成和物理引擎3D渲染，创建了混合耀斑数据集FlareX，以解决现有合成数据多样性不足和物理原理缺失导致模型泛化能力差的问题。", "motivation": "镜头耀斑严重降低图像质量，而获取真实的耀斑-无耀斑图像对非常困难。现有数据集通常通过2D合成，将人工耀斑模板叠加到背景图像上，但这些模板缺乏多样性，且合成过程忽略了物理原理，导致基于这些数据集训练的模型在真实世界场景中泛化能力差。", "method": "本文提出了一种新的物理信息驱动的耀斑数据生成方法，包含三个阶段：1. 参数化模板创建；2. 光照定律感知的2D合成；3. 基于物理引擎的3D渲染。最终生成了一个结合2D和3D视角的混合耀斑数据集FlareX。此外，还设计了一种掩码方法，从真实的受损图像中获取无耀斑图像，以衡量模型在真实世界图像上的性能。", "result": "所提出的FlareX数据集包含从95种耀斑模式派生出的9,500个2D模板，以及从60个3D场景渲染的3,000对耀斑图像。大量的实验证明了该方法和数据集的有效性。", "conclusion": "本文提出的物理信息驱动的耀斑数据生成方法和FlareX数据集，有效解决了现有耀斑数据集在多样性和物理准确性方面的不足，显著提升了模型在真实世界场景中去耀斑任务的泛化能力。同时，提出的真实世界评估方法也为模型性能衡量提供了有效手段。"}}
{"id": "2510.11331", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11331", "abs": "https://arxiv.org/abs/2510.11331", "authors": ["Bingjie Zhu", "Zhixiong Chen", "Liqiang Zhao", "Hyundong Shin", "Arumugam Nallanathan"], "title": "Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding", "comment": null, "summary": "Large language model (LLM) inference at the network edge is a promising\nserving paradigm that leverages distributed edge resources to run inference\nnear users and enhance privacy. Existing edge-based LLM inference systems\ntypically adopt autoregressive decoding (AD), which only generates one token\nper forward pass. This iterative process, compounded by the limited\ncomputational resources of edge nodes, results in high serving latency and\nconstrains the system's ability to support multiple users under growing\ndemands.To address these challenges, we propose a speculative decoding\n(SD)-based LLM serving framework that deploys small and large models across\nheterogeneous edge nodes to collaboratively deliver inference services.\nSpecifically, the small model rapidly generates draft tokens that the large\nmodel verifies in parallel, enabling multi-token generation per forward pass\nand thus reducing serving latency. To improve resource utilization of edge\nnodes, we incorporate pipeline parallelism to overlap drafting and verification\nacross multiple inference tasks. Based on this framework, we analyze and derive\na comprehensive latency model incorporating both communication and inference\nlatency. Then, we formulate a joint optimization problem for speculation\nlength, task batching, and wireless communication resource allocation to\nminimize total serving latency. To address this problem, we derive the\nclosed-form solutions for wireless communication resource allocation, and\ndevelop a dynamic programming algorithm for joint batching and speculation\ncontrol strategies. Experimental results demonstrate that the proposed\nframework achieves lower serving latency compared to AD-based serving systems.\nIn addition,the proposed joint optimization method delivers up to 44.9% latency\nreduction compared to benchmark schemes.", "AI": {"tldr": "本文提出了一种基于推测解码（SD）的LLM边缘推理框架，通过大小模型协同生成和验证多令牌，并结合流水线并行和联合优化，显著降低了服务延迟并提高了资源利用率。", "motivation": "现有的边缘大型语言模型（LLM）推理系统通常采用自回归解码（AD），每次前向传播仅生成一个令牌。这种迭代过程加上边缘节点有限的计算资源，导致服务延迟高，并且在需求增长时难以支持多用户。", "method": "本文提出一个基于推测解码（SD）的LLM服务框架，在异构边缘节点上部署小模型和大模型协同提供推理服务。小模型快速生成草稿令牌，大模型并行验证，实现每次前向传播生成多个令牌。为提高资源利用率，引入流水线并行以重叠多个推理任务的草稿生成和验证过程。此外，还推导了一个综合考虑通信和推理延迟的延迟模型，并构建了一个联合优化问题，以最小化总服务延迟，该问题涉及推测长度、任务批处理和无线通信资源分配。通过推导无线通信资源分配的闭式解和开发动态规划算法来解决批处理和推测控制策略。", "result": "实验结果表明，所提出的框架相比基于自回归解码（AD）的服务系统实现了更低的服务延迟。此外，所提出的联合优化方法相比基准方案，延迟降低高达44.9%。", "conclusion": "本文提出的基于推测解码的LLM边缘推理框架，结合大小模型协同、流水线并行和联合优化策略，能够有效解决边缘LLM推理高延迟和资源利用率低的问题，显著提升了服务性能。"}}
{"id": "2510.10421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10421", "abs": "https://arxiv.org/abs/2510.10421", "authors": ["Junbin Yuan", "Brady Moon", "Muqing Cao", "Sebastian Scherer"], "title": "Hierarchical Planning for Long-Horizon Multi-Target Tracking Under Target Motion Uncertainty", "comment": "8 pages, 7 figures. Accepted to IEEE Robotics and Automation Letters\n  (RAL), 2025", "summary": "Achieving persistent tracking of multiple dynamic targets over a large\nspatial area poses significant challenges for a single-robot system with\nconstrained sensing capabilities. As the robot moves to track different\ntargets, the ones outside the field of view accumulate uncertainty, making them\nprogressively harder to track. An effective path planning algorithm must manage\nuncertainty over a long horizon and account for the risk of permanently losing\ntrack of targets that remain unseen for too long. However, most existing\napproaches rely on short planning horizons and assume small, bounded\nenvironments, resulting in poor tracking performance and target loss in\nlarge-scale scenarios. In this paper, we present a hierarchical planner for\ntracking multiple moving targets with an aerial vehicle. To address the\nchallenge of tracking non-static targets, our method incorporates motion models\nand uncertainty propagation during path execution, allowing for more informed\ndecision-making. We decompose the multi-target tracking task into sub-tasks of\nsingle target search and detection, and our proposed pipeline consists a novel\nlow-level coverage planner that enables searching for a target in an evolving\nbelief area, and an estimation method to assess the likelihood of success for\neach sub-task, making it possible to convert the active target tracking task to\na Markov decision process (MDP) that we solve with a tree-based algorithm to\ndetermine the sequence of sub-tasks. We validate our approach in simulation,\ndemonstrating its effectiveness compared to existing planners for active target\ntracking tasks, and our proposed planner outperforms existing approaches,\nachieving a reduction of 11-70% in final uncertainty across different\nenvironments.", "AI": {"tldr": "本文提出了一种分层规划器，用于单架无人机在大空间区域内对多个动态目标进行持久跟踪，通过结合运动模型和不确定性传播，并利用马尔可夫决策过程解决长期规划问题，显著降低了跟踪不确定性。", "motivation": "单机器人系统在追踪多个动态目标时面临挑战，尤其是在大空间区域和有限感知能力下。目标在视野外会积累不确定性，现有方法多依赖短期规划且假设环境较小，导致在大规模场景中跟踪性能差，容易丢失目标。", "method": "本文提出了一种分层规划器。它在路径执行过程中融入运动模型和不确定性传播，以应对非静态目标的挑战。任务被分解为单目标搜索和检测子任务。该方法包括一个新颖的低级覆盖规划器，用于在不断演变的信念区域中搜索目标；一个评估每个子任务成功可能性的估计方法，从而将主动目标跟踪任务转换为马尔可夫决策过程（MDP），并使用基于树的算法来确定子任务序列。", "result": "该方法在仿真中得到了验证，与现有主动目标跟踪规划器相比，表现出更高的效率。在不同环境中，所提出的规划器优于现有方法，最终不确定性降低了11-70%。", "conclusion": "所提出的分层规划器在主动目标跟踪任务中优于现有方法，能够有效降低最终不确定性，解决了单机器人系统在大规模、多动态目标跟踪中的挑战。"}}
{"id": "2510.09935", "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 68T07", "I.2.7; I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.09935", "abs": "https://arxiv.org/abs/2510.09935", "authors": ["Weibin Cai", "Jiayu Li", "Reza Zafarani"], "title": "Unpacking Hateful Memes: Presupposed Context and False Claims", "comment": null, "summary": "While memes are often humorous, they are frequently used to disseminate hate,\ncausing serious harm to individuals and society. Current approaches to hateful\nmeme detection mainly rely on pre-trained language models. However, less focus\nhas been dedicated to \\textit{what make a meme hateful}. Drawing on insights\nfrom philosophy and psychology, we argue that hateful memes are characterized\nby two essential features: a \\textbf{presupposed context} and the expression of\n\\textbf{false claims}. To capture presupposed context, we develop \\textbf{PCM}\nfor modeling contextual information across modalities. To detect false claims,\nwe introduce the \\textbf{FACT} module, which integrates external knowledge and\nharnesses cross-modal reference graphs. By combining PCM and FACT, we introduce\n\\textbf{\\textsf{SHIELD}}, a hateful meme detection framework designed to\ncapture the fundamental nature of hate. Extensive experiments show that SHIELD\noutperforms state-of-the-art methods across datasets and metrics, while\ndemonstrating versatility on other tasks, such as fake news detection.", "AI": {"tldr": "本文提出了一种名为SHIELD的仇恨表情包检测框架，该框架基于哲学和心理学见解，通过建模预设语境（PCM）和检测虚假主张（FACT）来捕捉仇恨的本质，并超越了现有最先进的方法。", "motivation": "当前的仇恨表情包检测方法主要依赖预训练语言模型，但较少关注“是什么让表情包充满仇恨”。作者认为仇恨表情包对个人和社会造成严重危害，因此需要深入理解其内在特征。", "method": "本文提出仇恨表情包的两个核心特征：预设语境和虚假主张。为捕捉预设语境，开发了PCM模块来建模跨模态语境信息；为检测虚假主张，引入了FACT模块，该模块整合外部知识并利用跨模态参考图。通过结合PCM和FACT，构建了SHIELD框架。", "result": "SHIELD在多个数据集和指标上均优于现有的最先进方法，并在假新闻检测等其他任务上展现了通用性。", "conclusion": "SHIELD框架成功捕捉了仇恨的根本性质，在仇恨表情包检测方面表现出卓越的性能和多任务处理能力。"}}
{"id": "2510.10011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10011", "abs": "https://arxiv.org/abs/2510.10011", "authors": ["Yanyuan Chen", "Dexuan Xu", "Yu Huang", "Songkun Zhan", "Hanpin Wang", "Dongxue Chen", "Xueping Wang", "Meikang Qiu", "Hang Li"], "title": "MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output", "comment": "CVPR 2025", "summary": "Currently, medical vision language models are widely used in medical vision\nquestion answering tasks. However, existing models are confronted with two\nissues: for input, the model only relies on text instructions and lacks direct\nunderstanding of visual clues in the image; for output, the model only gives\ntext answers and lacks connection with key areas in the image. To address these\nissues, we propose a unified medical vision language model MIMO, with visual\nreferring Multimodal Input and pixel grounding Multimodal Output. MIMO can not\nonly combine visual clues and textual instructions to understand complex\nmedical images and semantics, but can also ground medical terminologies in\ntextual output within the image. To overcome the scarcity of relevant data in\nthe medical field, we propose MIMOSeg, a comprehensive medical multimodal\ndataset including 895K samples. MIMOSeg is constructed from four different\nperspectives, covering basic instruction following and complex question\nanswering with multimodal input and multimodal output. We conduct experiments\non several downstream medical multimodal tasks. Extensive experimental results\nverify that MIMO can uniquely combine visual referring and pixel grounding\ncapabilities, which are not available in previous models.", "AI": {"tldr": "本文提出了一种名为MIMO的统一医学视觉语言模型，通过多模态输入（视觉参照）和多模态输出（像素定位）解决了现有模型在理解视觉线索和输出与图像关联方面的不足。同时，还构建了一个包含895K样本的医学多模态数据集MIMOSeg。", "motivation": "现有的医学视觉语言模型存在两个主要问题：1) 输入时，模型仅依赖文本指令，缺乏对图像中视觉线索的直接理解；2) 输出时，模型只给出文本答案，缺乏与图像中关键区域的关联。", "method": "1. 提出了统一的医学视觉语言模型MIMO，其具有视觉参照多模态输入和像素定位多模态输出能力。2. 构建了全面的医学多模态数据集MIMOSeg，包含895K样本，从四个不同角度构建，涵盖了多模态输入和输出的基础指令遵循和复杂问答任务。", "result": "MIMO在多个下游医学多模态任务上进行了实验，结果验证了它能够独特地结合视觉参照和像素定位能力，这是以往模型所不具备的。", "conclusion": "MIMO模型通过结合视觉参照的多模态输入和像素定位的多模态输出，有效解决了现有医学视觉语言模型在视觉理解和图像关联输出方面的局限性，并在新构建的MIMOSeg数据集的支持下，展现出独特的性能优势。"}}
{"id": "2510.10338", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10338", "abs": "https://arxiv.org/abs/2510.10338", "authors": ["Balagopal Unnikrishnan", "Ariel Guerra Adames", "Amin Adibi", "Sameer Peesapati", "Rafal Kocielnik", "Shira Fischer", "Hillary Clinton Kasimbazi", "Rodrigo Gameiro", "Alina Peluso", "Chrystinne Oliveira Fernandes", "Maximin Lange", "Lovedeep Gondara", "Leo Anthony Celi"], "title": "Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI", "comment": null, "summary": "While ethical arguments for fairness in healthcare AI are well-established,\nthe economic and strategic value of inclusive design remains underexplored.\nThis perspective introduces the ``inclusive innovation dividend'' -- the\ncounterintuitive principle that solutions engineered for diverse, constrained\nuse cases generate superior economic returns in broader markets. Drawing from\nassistive technologies that evolved into billion-dollar mainstream industries,\nwe demonstrate how inclusive healthcare AI development creates business value\nbeyond compliance requirements. We identify four mechanisms through which\ninclusive innovation drives returns: (1) market expansion via geographic\nscalability and trust acceleration; (2) risk mitigation through reduced\nremediation costs and litigation exposure; (3) performance dividends from\nsuperior generalization and reduced technical debt, and (4) competitive\nadvantages in talent acquisition and clinical adoption. We present the\nHealthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring\nsystem that enables organizations to evaluate AI investments based on their\npotential to capture these benefits. HAIIF provides structured guidance for\nresource allocation, transforming fairness and inclusivity from regulatory\ncheckboxes into sources of strategic differentiation. Our findings suggest that\norganizations investing incrementally in inclusive design can achieve expanded\nmarket reach and sustained competitive advantages, while those treating these\nconsiderations as overhead face compounding disadvantages as network effects\nand data advantages accrue to early movers.", "AI": {"tldr": "本文提出“包容性创新红利”概念，认为医疗AI的包容性设计不仅符合伦理，还能带来显著的经济和战略价值，并提出一个评估框架HAIIF。", "motivation": "尽管医疗AI公平性的伦理论证已很充分，但包容性设计的经济和战略价值仍未得到充分探索。", "method": "本文引入了“包容性创新红利”原则，通过辅助技术演变为主流产业的案例，阐明包容性医疗AI开发如何创造商业价值。文章识别了四种驱动回报的机制，并提出了“医疗AI包容性创新框架”（HAIIF），这是一个实用的评分系统，用于评估AI投资的潜在收益。", "result": "研究表明，包容性创新通过市场扩张（地域可扩展性、信任加速）、风险规避（减少补救成本、诉讼风险）、性能红利（卓越的泛化能力、减少技术债务）和竞争优势（人才吸引、临床应用）来驱动回报。HAIIF框架能帮助组织评估AI投资以捕获这些利益，将公平性和包容性从合规要求转变为战略差异化来源。", "conclusion": "逐步投资于包容性设计的组织可以实现更广阔的市场覆盖和持续的竞争优势，而那些将这些考虑视为额外开销的组织则会面临复合劣势，因为先行者会积累网络效应和数据优势。"}}
{"id": "2510.11388", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11388", "abs": "https://arxiv.org/abs/2510.11388", "authors": ["Sheng-Wen Cheng", "Teng-Hu Cheng"], "title": "Data-Driven Estimation of Quadrotor Motor Efficiency via Residual Minimization", "comment": null, "summary": "A data-driven framework is proposed for online estimation of quadrotor motor\nefficiency via residual minimization. The problem is formulated as a\nconstrained nonlinear optimization that minimizes trajectory residuals between\nmeasured flight data and predictions generated by a quadrotor dynamics model. A\nsliding-window strategy enables online estimation, and the optimization is\nefficiently solved using an iteratively reweighted least squares (IRLS) scheme\ncombined with a primal-dual interior-point method, with inequality constraints\nenforced through a logarithmic barrier function. Robust z-score weighting is\nemployed to reject outliers, which is particularly effective in motor clipping\nscenarios where the proposed estimator exhibits smaller spikes than an EKF\nbaseline. Compared to traditional filter-based approaches, the batch-mode\nformulation offers greater flexibility by selectively incorporating informative\ndata segments. This structure is well-suited for onboard implementation,\nparticularly for applications such as fault detection and isolation (FDI),\nhealth monitoring, and predictive maintenance in aerial robotic systems.\nSimulation results under various degradation scenarios demonstrate the accuracy\nand robustness of the proposed estimator.", "AI": {"tldr": "本文提出了一种数据驱动的框架，通过残差最小化在线估计四旋翼飞行器电机效率，该方法结合滑动窗口策略、优化算法和鲁棒加权，在故障检测和健康监测等应用中表现出高精度和鲁棒性。", "motivation": "传统基于滤波器的方法在处理四旋翼飞行器电机效率估计时存在局限性，特别是在故障检测与隔离（FDI）、健康监测和预测性维护等航空机器人应用中，需要一种更灵活、更鲁棒的在线估计方法。", "method": "该研究将问题建模为有约束的非线性优化，通过最小化测量飞行数据与四旋翼动力学模型预测之间的轨迹残差来估计电机效率。采用滑动窗口策略实现在线估计，并通过迭代重加权最小二乘（IRLS）结合原对偶内点法高效求解，其中不等式约束通过对数障碍函数强制执行。为拒绝异常值，尤其是在电机削波场景中，采用了鲁棒z分数加权。与传统滤波器方法不同，该方法采用批处理模式，允许选择性地纳入信息丰富的历史数据段。", "result": "仿真结果表明，在各种退化场景下，所提出的估计器具有较高的准确性和鲁棒性。在电机削波场景中，该估计器产生的尖峰比扩展卡尔曼滤波器（EKF）基线更小。", "conclusion": "该数据驱动的电机效率在线估计算法具有高精度和鲁棒性，特别适用于机载实现，并在航空机器人系统的故障检测与隔离、健康监测和预测性维护等应用中展现出巨大潜力。"}}
{"id": "2510.10455", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10455", "abs": "https://arxiv.org/abs/2510.10455", "authors": ["Jiayu Ding", "Xulin Chen", "Garrett E. Katz", "Zhenyu Gan"], "title": "Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds", "comment": null, "summary": "Quadrupedal robots exhibit a wide range of viable gaits, but generating\nspecific footfall sequences often requires laborious expert tuning of numerous\nvariables, such as touch-down and lift-off events and holonomic constraints for\neach leg. This paper presents a unified reinforcement learning framework for\ngenerating versatile quadrupedal gaits by leveraging the intrinsic symmetries\nand velocity-period relationship of dynamic legged systems. We propose a\nsymmetry-guided reward function design that incorporates temporal,\nmorphological, and time-reversal symmetries. By focusing on preserved\nsymmetries and natural dynamics, our approach eliminates the need for\npredefined trajectories, enabling smooth transitions between diverse locomotion\npatterns such as trotting, bounding, half-bounding, and galloping. Implemented\non the Unitree Go2 robot, our method demonstrates robust performance across a\nrange of speeds in both simulations and hardware tests, significantly improving\ngait adaptability without extensive reward tuning or explicit foot placement\ncontrol. This work provides insights into dynamic locomotion strategies and\nunderscores the crucial role of symmetries in robotic gait design.", "AI": {"tldr": "本文提出了一种统一的强化学习框架，利用四足机器人的内在对称性和速度-周期关系，生成多样的步态，无需人工调优或预设轨迹。", "motivation": "生成特定的四足步态通常需要专家对大量变量（如触地、抬腿事件和每条腿的完整约束）进行繁琐的手动调整。", "method": "提出了一种统一的强化学习框架，通过利用动态腿式系统的内在对称性和速度-周期关系来生成多功能四足步态。设计了一个对称性引导的奖励函数，融合了时间、形态和时间反演对称性。", "result": "该方法消除了对预定义轨迹的需求，实现了小跑、跳跃、半跳和奔跑等多种运动模式之间的平滑过渡。在Unitree Go2机器人上，无论是在仿真还是硬件测试中，都能在不同速度下表现出稳健的性能，显著提高了步态适应性，且无需大量的奖励调优或显式的足部放置控制。", "conclusion": "这项工作提供了对动态运动策略的见解，并强调了对称性在机器人步态设计中的关键作用。"}}
{"id": "2510.10409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10409", "abs": "https://arxiv.org/abs/2510.10409", "authors": ["Siddartha Devic", "Charlotte Peale", "Arwen Bradley", "Sinead Williamson", "Preetum Nakkiran", "Aravind Gollakota"], "title": "Trace Length is a Simple Uncertainty Signal in Reasoning Models", "comment": null, "summary": "Uncertainty quantification for LLMs is a key research direction towards\naddressing hallucination and other issues that limit their reliable deployment.\nIn this work, we show that reasoning trace length is a simple and useful\nconfidence estimator in large reasoning models. Through comprehensive\nexperiments across multiple models, datasets, and prompts, we show that trace\nlength performs in comparable but complementary ways to other zero-shot\nconfidence estimators such as verbalized confidence. Our work reveals that\nreasoning post-training fundamentally alters the relationship between trace\nlength and accuracy, going beyond prior work that had shown that post-training\ncauses traces to grow longer in general (e.g., \"overthinking\"). We investigate\nthe mechanisms behind trace length's performance as a confidence signal,\nobserving that the effect remains even after adjusting for confounders such as\nproblem difficulty and GRPO-induced length bias. We identify high-entropy or\n\"forking\" tokens as playing a key role in the mechanism. Our findings\ndemonstrate that reasoning post-training enhances uncertainty quantification\nbeyond verbal expressions, and establish trace length as a practical confidence\nmeasure for large reasoning models.", "AI": {"tldr": "本研究表明，推理轨迹长度是大型推理模型中一种简单有效的置信度估计器，尤其在经过推理后训练后，其性能可与口头置信度相媲美并互补，且能提升不确定性量化能力。", "motivation": "解决大型语言模型（LLMs）中的幻觉问题，提升其部署的可靠性，需要有效的不确定性量化方法。", "method": "通过在多个模型、数据集和提示上进行综合实验，验证推理轨迹长度作为置信度估计器的有效性。将其与口头置信度等零样本置信度估计器进行比较。深入探讨轨迹长度作为置信度信号的机制，并调整了问题难度和GRPO引起的长度偏差等混杂因素。识别了高熵或“分叉”token在机制中的关键作用。", "result": "推理轨迹长度作为置信度估计器，其表现与口头置信度等其他零样本估计器相当且互补。推理后训练从根本上改变了轨迹长度与准确性之间的关系。即使调整了问题难度和GRPO引起的长度偏差等混杂因素，轨迹长度作为置信度信号的效果依然存在。高熵或“分叉”token在机制中扮演关键角色。", "conclusion": "推理后训练增强了超越口头表达的不确定性量化能力，并确立了推理轨迹长度作为大型推理模型中一种实用的置信度衡量标准。"}}
{"id": "2510.11393", "categories": ["eess.SY", "cs.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.11393", "abs": "https://arxiv.org/abs/2510.11393", "authors": ["Farhad Mehdifar", "Charalampos P. Bechlioulis", "Dimos V. Dimarogonas"], "title": "Robust Closed-Form Control for MIMO Nonlinear Systems under Conflicting Time-Varying Hard and Soft Constraints", "comment": "18 pages, 6 figures", "summary": "This paper introduces a novel robust closed-form control law to handle\ntime-varying hard and soft constraints in uncertain high-relative-degree\nnonlinear MIMO systems. These constraints represent spatiotemporal\nspecifications in mechanical systems' operational space, with hard constraints\nensuring safety-critical requirements and soft constraints encoding performance\nor task objectives. Initially, all constraints are consolidated into two\nseparate scalar time-varying hard and soft constraint functions, whose positive\nlevel sets define feasible regions. A closed-form control law is developed to\nenforce these constraints using appropriately designed reciprocal barriers and\nnonlinear transformation functions. When conflicts between hard and soft\nconstraints arise, the control law prioritizes hard constraints by virtually\nrelaxing soft constraints via a dynamic relaxation law. Notably, the proposed\ncontrol law maintains low complexity by avoiding approximation schemes for\ncoping with system uncertainties. Simulation results confirm the effectiveness\nof the proposed method.", "AI": {"tldr": "本文提出一种新颖的鲁棒闭环控制律，用于处理不确定高相对阶非线性MIMO系统中的时变硬约束和软约束，并在约束冲突时优先保障硬约束。", "motivation": "研究动机是为不确定高相对阶非线性MIMO系统设计一种能够处理时变硬约束（安全关键）和软约束（性能/任务目标）的控制方法，尤其是在这些约束发生冲突时，需要一种低复杂度的鲁棒解决方案。", "method": "该方法首先将所有约束整合为两个独立的标量时变硬约束和软约束函数。然后，开发了一个闭环控制律，利用适当设计的倒数障碍函数和非线性变换函数来强制执行这些约束。当硬约束和软约束发生冲突时，通过动态松弛律虚拟放松软约束，从而优先保障硬约束。该控制律通过避免近似方案来应对系统不确定性，从而保持较低的复杂性。", "result": "仿真结果证实了所提出方法的有效性。", "conclusion": "本文提出了一种有效且低复杂度的鲁棒闭环控制律，能够处理不确定高相对阶非线性MIMO系统中的时变硬约束和软约束，并在约束冲突时通过动态松弛机制优先保障安全关键的硬约束。"}}
{"id": "2510.10022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10022", "abs": "https://arxiv.org/abs/2510.10022", "authors": ["Junan Chen", "Trung Thanh Nguyen", "Takahiro Komamizu", "Ichiro Ide"], "title": "Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning", "comment": "ACM Multimedia Asia 2025", "summary": "Recent advances in video captioning are driven by large-scale pretrained\nmodels, which follow the standard \"pre-training followed by fine-tuning\"\nparadigm, where the full model is fine-tuned for downstream tasks. Although\neffective, this approach becomes computationally prohibitive as the model size\nincreases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a\npromising alternative, but primarily focuses on the language components of\nMultimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains\nunderexplored in multimodal tasks and lacks sufficient understanding of visual\ninformation during fine-tuning the model. To bridge this gap, we propose\nQuery-Adapter (Q-Adapter), a lightweight visual adapter module designed to\nenhance MLLMs by enabling efficient fine-tuning for the video captioning task.\nQ-Adapter introduces learnable query tokens and a gating layer into Vision\nEncoder, enabling effective extraction of sparse, caption-relevant features\nwithout relying on external textual supervision. We evaluate Q-Adapter on two\nwell-known video captioning datasets, MSR-VTT and MSVD, where it achieves\nstate-of-the-art performance among the methods that take the PEFT approach\nacross BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves\ncompetitive performance compared to methods that take the full fine-tuning\napproach while requiring only 1.4% of the parameters. We further analyze the\nimpact of key hyperparameters and design choices on fine-tuning effectiveness,\nproviding insights into optimization strategies for adapter-based learning.\nThese results highlight the strong potential of Q-Adapter in balancing caption\nquality and parameter efficiency, demonstrating its scalability for\nvideo-language modeling.", "AI": {"tldr": "本文提出Q-Adapter，一种轻量级视觉适配器，用于高效微调多模态大语言模型（MLLMs）以进行视频字幕生成。它通过在视觉编码器中引入可学习查询标记和门控层，实现稀疏、字幕相关特征的有效提取，在参数效率微调（PEFT）方法中达到最先进性能，并与完全微调方法相比也具竞争力，但仅需1.4%的参数。", "motivation": "随着模型规模的增长，视频字幕任务中传统“预训练后微调”范式变得计算成本高昂。尽管参数效率微调（PEFT）方法前景广阔，但其主要关注多模态大语言模型（MLLMs）的语言组件，在多模态任务中对视觉信息的理解和利用仍有待探索。", "method": "本文提出Query-Adapter (Q-Adapter)，一个轻量级视觉适配器模块。它在视觉编码器中引入可学习的查询标记（query tokens）和一个门控层，旨在有效提取与字幕相关的稀疏视觉特征，且不依赖外部文本监督。", "result": "Q-Adapter在MSR-VTT和MSVD两个视频字幕数据集上，在BLEU@4、METEOR、ROUGE-L和CIDEr指标上，在采用PEFT方法的模型中达到了最先进的性能。与采用完全微调的方法相比，Q-Adapter在仅使用1.4%参数的情况下，也取得了具有竞争力的性能。研究还分析了关键超参数和设计选择对微调效果的影响，为基于适配器的学习提供了优化策略见解。", "conclusion": "Q-Adapter在字幕质量和参数效率之间取得了良好的平衡，展示了其在视频-语言建模方面的强大潜力和可扩展性。"}}
{"id": "2510.10468", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10468", "abs": "https://arxiv.org/abs/2510.10468", "authors": ["Robert Mahony", "Jonathan Kelly", "Stephan Weiss"], "title": "Galilean Symmetry in Robotics", "comment": "Under Review", "summary": "Galilean symmetry is the natural symmetry of inertial motion that underpins\nNewtonian physics. Although rigid-body symmetry is one of the most established\nand fundamental tools in robotics, there appears to be no comparable treatment\nof Galilean symmetry for a robotics audience. In this paper, we present a\nrobotics-tailored exposition of Galilean symmetry that leverages the\ncommunity's familiarity with and understanding of rigid-body transformations\nand pose representations. Our approach contrasts with common treatments in the\nphysics literature that introduce Galilean symmetry as a stepping stone to\nEinstein's relativity. A key insight is that the Galilean matrix Lie group can\nbe used to describe two different pose representations, Galilean frames, that\nuse inertial velocity in the state definition, and extended poses, that use\ncoordinate velocity. We provide three examples where applying the Galilean\nmatrix Lie-group algebra to robotics problems is straightforward and yields\nsignificant insights: inertial navigation above the rotating Earth, manipulator\nkinematics, and sensor data fusion under temporal uncertainty. We believe that\nthe time is right for the robotics community to benefit from rediscovering and\nextending this classical material and applying it to modern problems.", "AI": {"tldr": "本文为机器人学领域量身定制了伽利略对称性的阐述，利用刚体变换概念，并展示了伽利略矩阵李群如何描述两种姿态表示，通过实例证明其在机器人问题中的应用价值。", "motivation": "尽管刚体对称性是机器人学中的基本工具，但针对机器人领域的伽利略对称性处理却鲜有提及。物理学文献中通常将其作为爱因斯坦相对论的铺垫，而非独立应用于机器人问题。", "method": "本文利用机器人社区对刚体变换和姿态表示的熟悉程度，对伽利略对称性进行了定制化阐述。核心方法是使用伽利略矩阵李群来描述两种不同的姿态表示：使用惯性速度定义状态的伽利略坐标系（Galilean frames）和使用坐标速度的扩展姿态（extended poses）。", "result": "伽利略矩阵李群可以描述两种不同的姿态表示。将伽利略矩阵李群代数应用于机器人问题是直接有效的，并在以下三个领域产生了重要见解：旋转地球上的惯性导航、机械臂运动学以及时间不确定性下的传感器数据融合。", "conclusion": "机器人学界有望通过重新发现、扩展和应用这一经典材料来解决现代问题，从而从中受益。"}}
{"id": "2510.09988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09988", "abs": "https://arxiv.org/abs/2510.09988", "authors": ["Jiaqi Wei", "Xiang Zhang", "Yuejin Yang", "Wenxuan Huang", "Juntai Cao", "Sheng Xu", "Xiang Zhuang", "Zhangyang Gao", "Muhammad Abdul-Mageed", "Laks V. S. Lakshmanan", "Chenyu You", "Wanli Ouyang", "Siqi Sun"], "title": "Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey", "comment": null, "summary": "Deliberative tree search is a cornerstone of modern Large Language Model\n(LLM) research, driving the pivot from brute-force scaling toward algorithmic\nefficiency. This single paradigm unifies two critical frontiers:\n\\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve\nhard problems, and \\textbf{Self-Improvement}, which uses search-generated data\nto durably enhance model parameters. However, this burgeoning field is\nfragmented and lacks a common formalism, particularly concerning the ambiguous\nrole of the reward signal -- is it a transient heuristic or a durable learning\ntarget? This paper resolves this ambiguity by introducing a unified framework\nthat deconstructs search algorithms into three core components: the\n\\emph{Search Mechanism}, \\emph{Reward Formulation}, and \\emph{Transition\nFunction}. We establish a formal distinction between transient \\textbf{Search\nGuidance} for TTS and durable \\textbf{Parametric Reward Modeling} for\nSelf-Improvement. Building on this formalism, we introduce a component-centric\ntaxonomy, synthesize the state-of-the-art, and chart a research roadmap toward\nmore systematic progress in creating autonomous, self-improving agents.", "AI": {"tldr": "本文提出一个统一框架，旨在解决大型语言模型（LLM）中审慎树搜索领域的分散性和奖励信号角色模糊的问题，该框架将搜索算法分解为三个核心组件，并区分了测试时扩展和自我改进中的奖励信号。", "motivation": "现代LLM研究中，审慎树搜索虽是核心，但该领域存在碎片化，缺乏统一的形式化，尤其是在奖励信号的角色方面存在模糊性（是瞬态启发式还是持久学习目标）。", "method": "引入一个统一框架，将搜索算法解构为“搜索机制”、“奖励公式”和“转换函数”三个核心组件。正式区分了用于测试时扩展的瞬态“搜索引导”和用于自我改进的持久“参数化奖励建模”。在此基础上，提出了一个以组件为中心的分类法。", "result": "建立了一个统一的形式化框架和分类法，明确了审慎树搜索中奖励信号的角色，并综合了现有技术水平。", "conclusion": "该框架和分类法为理解和推进自主、自我改进智能体的系统性进展提供了研究路线图，有助于更系统地发展该领域。"}}
{"id": "2510.10461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10461", "abs": "https://arxiv.org/abs/2510.10461", "authors": ["Hongjie Zheng", "Zesheng Shi", "Ping Yi"], "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision", "comment": null, "summary": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated\nremarkable capabilities in isolated medical tasks like diagnosis and image\nanalysis, but struggle with integrated clinical workflows that connect\ndiagnostic reasoning and medication decisions. We identify a core limitation:\nexisting medical AI systems process tasks in isolation without the\ncross-validation and knowledge integration found in clinical teams, reducing\ntheir effectiveness in real-world healthcare scenarios. To transform the\nisolation paradigm into a collaborative approach, we propose MedCoAct, a\nconfidence-aware multi-agent framework that simulates clinical collaboration by\nintegrating specialized doctor and pharmacist agents, and present a benchmark,\nDrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and\ntreatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\%\ndiagnostic accuracy and 67.58\\% medication recommendation accuracy,\noutperforming single agent framework by 7.04\\% and 7.08\\% respectively. This\ncollaborative approach generalizes well across diverse medical domains, proving\nespecially effective for telemedicine consultations and routine clinical\nscenarios, while providing interpretable decision-making pathways.", "AI": {"tldr": "现有LLM医疗智能体在孤立任务中表现良好，但在集成临床工作流中存在困难。MedCoAct是一个模拟临床协作的多智能体框架，显著提高了诊断和药物推荐的准确性。", "motivation": "LLM智能体在诊断和图像分析等孤立医疗任务中表现出色，但难以处理需要整合诊断推理和药物决策的集成临床工作流。核心限制在于现有医疗AI系统孤立处理任务，缺乏临床团队中常见的交叉验证和知识整合。", "method": "提出MedCoAct，一个信心感知多智能体框架，通过整合专业的医生和药剂师智能体来模拟临床协作。同时，提出了DrugCareQA基准测试，用于评估医疗AI在集成诊断和治疗工作流中的能力。", "result": "MedCoAct在诊断准确率和药物推荐准确率上均达到67.58%，分别比单智能体框架高出7.04%和7.08%。", "conclusion": "这种协作方法（MedCoAct）在不同医疗领域具有良好的泛化性，在远程医疗咨询和日常临床场景中特别有效，并能提供可解释的决策路径。"}}
{"id": "2510.09947", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09947", "abs": "https://arxiv.org/abs/2510.09947", "authors": ["Mir Tafseer Nayeem", "Sawsan Alqahtani", "Md Tahmid Rahman Laskar", "Tasnim Mohiuddin", "M Saiful Bari"], "title": "Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation", "comment": "NeurIPS 2025 Workshop", "summary": "Tokenization is a crucial but under-evaluated step in large language models\n(LLMs). The standard metric, fertility (the average number of tokens per word),\ncaptures compression efficiency but obscures how vocabularies are allocated\nacross languages and domains. We analyze six widely used tokenizers across\nseven languages and two domains, finding stable fertility for English, high\nfertility for Chinese, and little domain sensitivity. To address fertility's\nblind spots, we propose the Single Token Retention Rate (STRR), which measures\nthe proportion of words preserved as single tokens. STRR reveals systematic\nprioritization of English, strong support for Chinese, and fragmentation in\nHindi, offering an interpretable view of cross-lingual fairness. Our results\nshow that STRR complements fertility and provides practical guidance for\ndesigning more equitable multilingual tokenizers.", "AI": {"tldr": "本文分析了大型语言模型中分词的重要性，指出传统生育率指标的局限性，并提出了新的指标“单token保留率”（STRR）来评估多语言分词器的公平性，揭示了不同语言间存在的偏向，并为设计更公平的分词器提供了指导。", "motivation": "分词是大型语言模型中一个关键但未被充分评估的步骤。现有标准指标“生育率”（每词平均token数）虽能反映压缩效率，但无法揭示词汇在不同语言和领域间的分配方式，存在盲点。", "method": "研究分析了六种常用分词器，涉及七种语言和两个领域，评估了其生育率。为解决生育率的局限性，研究提出并使用了“单token保留率”（STRR）这一新指标，该指标衡量单词被保留为单个token的比例。", "result": "生育率分析显示英语稳定、中文较高，且领域敏感性较低。STRR则揭示了对英语的系统性优先处理、对中文的强力支持以及印地语的碎片化现象，从而提供了跨语言公平性的可解释视图。", "conclusion": "STRR作为生育率的有效补充，能够揭示分词器在跨语言公平性方面的表现，并为设计更公平的多语言分词器提供了实用指导。"}}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories.", "AI": {"tldr": "Traj-CoA是一个多智能体系统，利用LLM处理长且嘈杂的电子健康记录（EHR）数据，以实现患者轨迹建模和预测。", "motivation": "大型语言模型（LLMs）在处理电子健康记录（EHR）数据时，由于数据冗长、嘈杂以及时间推理的挑战，难以有效建模患者轨迹。", "method": "Traj-CoA采用一个多智能体系统，包括一个工作智能体链和一个管理智能体。工作智能体按顺序处理EHR数据块，将关键事件提炼到共享的长期记忆模块（EHRMem）中，以减少噪声并保留时间线。最终，管理智能体综合工作智能体的摘要和EHRMem中的时间线进行预测。", "result": "在基于五年EHR数据的零样本一年期肺癌风险预测任务中，Traj-CoA超越了四类基线模型。分析表明，Traj-CoA表现出与临床一致的时间推理能力。", "conclusion": "Traj-CoA被确立为一种有前景、稳健且通用性强的复杂患者轨迹建模方法。"}}
{"id": "2510.09994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09994", "abs": "https://arxiv.org/abs/2510.09994", "authors": ["Yimin Xiao", "Yongle Zhang", "Dayeon Ki", "Calvin Bao", "Marianna J. Martindale", "Charlotte Vaughn", "Ge Gao", "Marine Carpuat"], "title": "Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations", "comment": "EMNLP 2025", "summary": "As Machine Translation (MT) becomes increasingly commonplace, understanding\nhow the general public perceives and relies on imperfect MT is crucial for\ncontextualizing MT research in real-world applications. We present a human\nstudy conducted in a public museum (n=452), investigating how fluency and\nadequacy errors impact bilingual and non-bilingual users' reliance on MT during\ncasual use. Our findings reveal that non-bilingual users often over-rely on MT\ndue to a lack of evaluation strategies and alternatives, while experiencing the\nimpact of errors can prompt users to reassess future reliance. This highlights\nthe need for MT evaluation and NLP explanation techniques to promote not only\nMT quality, but also MT literacy among its users.", "AI": {"tldr": "本研究通过一项公共博物馆的用户研究（n=452），发现非双语用户常因缺乏评估策略而过度依赖机器翻译（MT），而错误经历会促使他们重新评估未来的依赖。这强调了提高MT质量和用户素养的重要性。", "motivation": "随着机器翻译日益普及，理解公众如何感知和依赖不完美的机器翻译对于将MT研究置于实际应用背景中至关重要。", "method": "在公共博物馆进行了一项人类研究（n=452），调查了流利度和充分性错误如何影响双语和非双语用户在日常使用中对机器翻译的依赖。", "result": "非双语用户常因缺乏评估策略和替代方案而过度依赖机器翻译。然而，经历错误会促使用户重新评估未来的依赖。", "conclusion": "研究强调了机器翻译评估和自然语言处理解释技术的重要性，不仅要提高机器翻译质量，还要提升用户的机器翻译素养。"}}
{"id": "2510.11405", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11405", "abs": "https://arxiv.org/abs/2510.11405", "authors": ["Samuel Oliveira", "Mostafa Tavakkoli Anbarani", "Gregory Beal", "Ilya Kovalenko", "Marcelo Teixeira", "André B. Leal", "Rômulo Meira-Góes"], "title": "Robust Recovery and Control of Cyber-physical Discrete Event Systems under Actuator Attacks", "comment": "This work has been accepted for publication in the 64th IEEE\n  Conference on Decision and Control (CDC). The final published version will be\n  available on IEEE Xplore", "summary": "Critical real-world applications strongly rely on Cyber-physical systems\n(CPS), but their dependence on communication networks introduces significant\nsecurity risks, as attackers can exploit vulnerabilities to compromise their\nintegrity and availability. This work explores the topic of cybersecurity in\nthe context of CPS modeled as discrete event systems (DES), focusing on\nrecovery strategies following the detection of cyberattacks. Specifically, we\naddress actuator enablement attacks and propose a method that preserves the\nsystem's full valid behavior under normal conditions. Upon detecting an attack,\nour proposed solution aims to guide the system toward a restricted yet robust\nbehavior, ensuring operational continuity and resilience. Additionally, we\nintroduce a property termed AE-robust recoverability, which characterizes the\nnecessary and sufficient conditions for recovering a system from attacks while\npreventing further vulnerabilities. Finally, we showcase the proposed solution\nthrough a case study based on a manufacturing system.", "AI": {"tldr": "本文研究了网络物理系统（CPS）在离散事件系统（DES）模型下的网络安全问题，重点关注执行器启用攻击后的恢复策略，提出了一种在攻击检测后能引导系统至受限但鲁棒行为的方法，并定义了AE-鲁棒可恢复性。", "motivation": "关键的现实世界应用严重依赖网络物理系统（CPS），但其对通信网络的依赖性引入了显著的安全风险，攻击者可能利用漏洞损害系统的完整性和可用性。", "method": "将CPS建模为离散事件系统（DES），研究执行器启用攻击。提出一种恢复方法，在正常条件下保持系统完整的有效行为；攻击检测后，引导系统转向受限但鲁棒的行为。引入了“AE-鲁棒可恢复性”特性，并阐明了系统从攻击中恢复并防止进一步漏洞的必要和充分条件。", "result": "提出了一种在执行器启用攻击后能保持系统在受限但鲁棒行为下运行的恢复解决方案，确保了操作的连续性和弹性。定义了AE-鲁棒可恢复性，并给出了其必要和充分条件。通过一个基于制造系统的案例研究展示了所提出解决方案的有效性。", "conclusion": "所提出的解决方案能有效应对CPS中的执行器启用攻击，确保系统在攻击后仍能保持操作连续性和弹性，并通过AE-鲁棒可恢复性特性为系统恢复提供了理论基础。"}}
{"id": "2510.10030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10030", "abs": "https://arxiv.org/abs/2510.10030", "authors": ["Henan Wang", "Hanxin Zhu", "Xinliang Gong", "Tianyu He", "Xin Li", "Zhibo Chen"], "title": "P-4DGS: Predictive 4D Gaussian Splatting with 90$\\times$ Compression", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has garnered significant attention due to its\nsuperior scene representation fidelity and real-time rendering performance,\nespecially for dynamic 3D scene reconstruction (\\textit{i.e.}, 4D\nreconstruction). However, despite achieving promising results, most existing\nalgorithms overlook the substantial temporal and spatial redundancies inherent\nin dynamic scenes, leading to prohibitive memory consumption. To address this,\nwe propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene\nmodeling. Inspired by intra- and inter-frame prediction techniques commonly\nused in video compression, we first design a 3D anchor point-based\nspatial-temporal prediction module to fully exploit the spatial-temporal\ncorrelations across different 3D Gaussian primitives. Subsequently, we employ\nan adaptive quantization strategy combined with context-based entropy coding to\nfurther reduce the size of the 3D anchor points, thereby achieving enhanced\ncompression efficiency. To evaluate the rate-distortion performance of our\nproposed P-4DGS in comparison with other dynamic 3DGS representations, we\nconduct extensive experiments on both synthetic and real-world datasets.\nExperimental results demonstrate that our approach achieves state-of-the-art\nreconstruction quality and the fastest rendering speed, with a remarkably low\nstorage footprint (around \\textbf{1MB} on average), achieving up to\n\\textbf{40$\\times$} and \\textbf{90$\\times$} compression on synthetic and\nreal-world scenes, respectively.", "AI": {"tldr": "P-4DGS是一种新颖的动态3D高斯泼溅(3DGS)表示方法，通过引入时空预测和自适应量化策略，显著减少了动态3D场景建模的内存消耗，同时保持了高质量的重建和快速渲染。", "motivation": "尽管3D高斯泼溅在动态3D场景重建（即4D重建）中表现出色，但大多数现有算法忽视了动态场景中固有的时间与空间冗余，导致内存消耗过高。", "method": "本文提出了P-4DGS，一种紧凑的4D场景建模动态3DGS表示。方法包括：1) 设计一个基于3D锚点的时空预测模块，利用视频压缩中的帧内/帧间预测技术，充分挖掘不同3D高斯基元之间的时空关联。2) 采用自适应量化策略结合基于上下文的熵编码，进一步减小3D锚点的大小，提高压缩效率。", "result": "P-4DGS在合成和真实世界数据集上均实现了最先进的重建质量和最快的渲染速度。其存储占用极低（平均约1MB），在合成场景中实现了高达40倍的压缩，在真实世界场景中实现了高达90倍的压缩。", "conclusion": "P-4DGS通过创新的时空预测和自适应量化策略，有效解决了动态3DGS表示中的内存消耗问题，同时在重建质量、渲染速度和存储效率方面均达到了领先水平。"}}
{"id": "2510.10506", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10506", "abs": "https://arxiv.org/abs/2510.10506", "authors": ["Kush Garg", "Akshat Dave"], "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception", "comment": "8 pages, 9 Figures , Project webpage: https://super-ex.github.io/", "summary": "Efficient exploration and mapping in unknown indoor environments is a\nfundamental challenge, with high stakes in time-critical settings. In current\nsystems, robot perception remains confined to line-of-sight; occluded regions\nremain unknown until physically traversed, leading to inefficient exploration\nwhen layouts deviate from prior assumptions. In this work, we bring\nnon-line-of-sight (NLOS) sensing to robotic exploration. We leverage\nsingle-photon LiDARs, which capture time-of-flight histograms that encode the\npresence of hidden objects - allowing robots to look around blind corners.\nRecent single-photon LiDARs have become practical and portable, enabling\ndeployment beyond controlled lab settings. Prior NLOS works target 3D\nreconstruction in static, lab-based scenarios, and initial efforts toward\nNLOS-aided navigation consider simplified geometries. We introduce SuperEx, a\nframework that integrates NLOS sensing directly into the mapping-exploration\nloop. SuperEx augments global map prediction with beyond-line-of-sight cues by\n(i) carving empty NLOS regions from timing histograms and (ii) reconstructing\noccupied structure via a two-step physics-based and data-driven approach that\nleverages structural regularities. Evaluations on complex simulated maps and\nthe real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under\n< 30% coverage and improved exploration efficiency compared to line-of-sight\nbaselines, opening a path to reliable mapping beyond direct visibility.", "AI": {"tldr": "该研究将非视距（NLOS）感知技术引入机器人探索，利用单光子激光雷达实现绕过障碍物的感知和映射，显著提高了探索效率和地图准确性。", "motivation": "在未知室内环境中，高效探索和绘图是一个基本挑战。现有机器人感知受限于视距，导致在布局与预设不符时探索效率低下，未被物理遍历的区域保持未知。", "method": "引入了名为SuperEx的框架，将非视距（NLOS）感知直接整合到映射-探索循环中。该框架利用单光子激光雷达捕获的时间飞行直方图，通过以下方式增强全局地图预测：(i) 从时间直方图中“雕刻”出空的NLOS区域；(ii) 通过结合基于物理和数据驱动的两步方法，利用结构规律重建被占据的结构。", "result": "在复杂的模拟地图和真实的KTH Floorplan数据集上进行评估，结果显示在覆盖率低于30%的情况下，地图准确性提高了12%，并且与视距基线相比，探索效率得到了提升。", "conclusion": "该工作为超越直接可见性范围的可靠映射开辟了道路，通过将NLOS感知集成到机器人探索中，显著提高了在未知环境中的映射准确性和探索效率。"}}
{"id": "2510.10051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10051", "abs": "https://arxiv.org/abs/2510.10051", "authors": ["Sitong Gong", "Yunzhi Zhuge", "Lu Zhang", "Pingping Zhang", "Huchuan Lu"], "title": "Complementary and Contrastive Learning for Audio-Visual Segmentation", "comment": "Accepted to IEEE Transactions on Multimedia", "summary": "Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps\nthat correlate with the auditory signals of objects. This field has seen\nsignificant progress with numerous CNN and Transformer-based methods enhancing\nthe segmentation accuracy and robustness. Traditional CNN approaches manage\naudio-visual interactions through basic operations like padding and\nmultiplications but are restricted by CNNs' limited local receptive field. More\nrecently, Transformer-based methods treat auditory cues as queries, utilizing\nattention mechanisms to enhance audio-visual cooperation within frames.\nNevertheless, they typically struggle to extract multimodal coefficients and\ntemporal dynamics adequately. To overcome these limitations, we present the\nComplementary and Contrastive Transformer (CCFormer), a novel framework adept\nat processing both local and global information and capturing spatial-temporal\ncontext comprehensively. Our CCFormer initiates with the Early Integration\nModule (EIM) that employs a parallel bilateral architecture, merging\nmulti-scale visual features with audio data to boost cross-modal\ncomplementarity. To extract the intra-frame spatial features and facilitate the\nperception of temporal coherence, we introduce the Multi-query Transformer\nModule (MTM), which dynamically endows audio queries with learning capabilities\nand models the frame and video-level relations simultaneously. Furthermore, we\npropose the Bi-modal Contrastive Learning (BCL) to promote the alignment across\nboth modalities in the unified feature space. Through the effective combination\nof those designs, our method sets new state-of-the-art benchmarks across the\nS4, MS3 and AVSS datasets. Our source code and model weights will be made\npublicly available at https://github.com/SitongGong/CCFormer", "AI": {"tldr": "本文提出了一种名为CCFormer的新型互补对比Transformer框架，用于音视频分割（AVS），旨在克服现有方法在处理局部/全局信息、时空上下文和跨模态交互方面的局限性，并在多个数据集上取得了最先进的性能。", "motivation": "传统的CNN方法受限于局部感受野，难以有效处理音视频交互。Transformer方法虽利用注意力机制增强跨模态协作，但通常难以充分提取多模态系数和时间动态。", "method": "本文提出了互补对比Transformer (CCFormer) 框架，包含以下模块：1. 早期集成模块（EIM），采用并行双边架构，融合多尺度视觉特征与音频数据以增强跨模态互补性。2. 多查询Transformer模块（MTM），动态赋予音频查询学习能力，同时建模帧和视频级别关系，以提取帧内空间特征并促进时间连贯性感知。3. 双模态对比学习（BCL），促进两种模态在统一特征空间中的对齐。", "result": "该方法在S4、MS3和AVSS数据集上取得了新的最先进（SOTA）基准。", "conclusion": "通过有效结合EIM、MTM和BCL，CCFormer成功克服了现有AVS方法的局限性，能够全面处理局部和全局信息并捕捉时空上下文，从而显著提升了音视频分割的性能。"}}
{"id": "2510.11413", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11413", "abs": "https://arxiv.org/abs/2510.11413", "authors": ["Sofia Girardello", "Giulia Michieletto", "Angelo Cenedese", "Antonio Franchi", "Chiara Gabellieri"], "title": "Trajectory control of a suspended load with non-stopping flying carriers", "comment": null, "summary": "This paper presents the first closed-loop control framework for cooperative\npayload transportation with non-stopping flying carriers. Building upon\ngrasp-matrix formulations and internal force redundancy, we propose a feedback\nwrench controller that actively regulates the payload's pose while an\noptimization layer dynamically shapes internal-force oscillations to guarantee\npersistent carrier motion. Preliminary experimental results on multirotor UAVs\nvalidate the model assumptions, and numerical simulations demonstrate that the\nmethod successfully prevents carrier stagnation, achieves accurate load\ntracking, and generates physically feasible trajectories with smooth velocity\nprofiles. The proposed framework not only advances the state of the art but\nalso offers a reliable, versatile solution for future real-world applications\nrequiring load transportation by coordinated non-stopping flying carriers.", "AI": {"tldr": "本文提出首个针对非停歇飞行器协同负载运输的闭环控制框架，通过力矩控制器和优化层确保负载姿态精确跟踪并防止飞行器停滞。", "motivation": "现有协同负载运输方法可能存在飞行器停滞问题，且缺乏可靠、通用的闭环控制框架来满足未来实际应用中对非停歇飞行器协同运输的需求。", "method": "该研究基于抓取矩阵公式和内力冗余，提出了一个反馈力矩控制器来主动调节负载姿态。同时，一个优化层动态调整内力振荡，以确保载体持续运动，防止停滞。", "result": "多旋翼无人机上的初步实验验证了模型假设。数值模拟表明，该方法成功防止了载体停滞，实现了精确的负载跟踪，并生成了具有平滑速度剖面的物理可行轨迹。", "conclusion": "所提出的框架不仅推动了现有技术水平，还为未来需要协调非停歇飞行器进行负载运输的实际应用提供了一个可靠、通用的解决方案。"}}
{"id": "2510.10516", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10516", "abs": "https://arxiv.org/abs/2510.10516", "authors": ["Kanishkha Jaisankar", "Xiaoyang Jiang", "Feifan Liao", "Jeethu Sreenivas Amuthan"], "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control", "comment": null, "summary": "Energy-efficient and high-performance motor control remains a critical\nchallenge in robotics, particularly for high-dimensional continuous control\ntasks with limited onboard resources. While Deep Reinforcement Learning (DRL)\nhas achieved remarkable results, its computational demands and energy\nconsumption limit deployment in resource-constrained environments. This paper\nintroduces a novel framework combining population-coded Spiking Neural Networks\n(SNNs) with DRL to address these challenges. Our approach leverages the\nevent-driven, asynchronous computation of SNNs alongside the robust policy\noptimization capabilities of DRL, achieving a balance between energy efficiency\nand control performance. Central to this framework is the Population-coded\nSpiking Actor Network (PopSAN), which encodes high-dimensional observations\ninto neuronal population activities and enables optimal policy learning through\ngradient-based updates. We evaluate our method on the Isaac Gym platform using\nthe PixMC benchmark with complex robotic manipulation tasks. Experimental\nresults on the Franka robotic arm demonstrate that our approach achieves energy\nsavings of up to 96.10% compared to traditional Artificial Neural Networks\n(ANNs) while maintaining comparable control performance. The trained SNN\npolicies exhibit robust finger position tracking with minimal deviation from\ncommanded trajectories and stable target height maintenance during\npick-and-place operations. These results position population-coded SNNs as a\npromising solution for energy-efficient, high-performance robotic control in\nresource-constrained applications, paving the way for scalable deployment in\nreal-world robotics systems.", "AI": {"tldr": "本文提出了一种结合群体编码脉冲神经网络（SNNs）与深度强化学习（DRL）的新框架，旨在实现高能效、高性能的机器人控制，特别适用于资源受限环境。", "motivation": "机器人领域中，能效高、性能好的电机控制仍是一个关键挑战，尤其是在资源有限的高维连续控制任务中。深度强化学习（DRL）虽然效果显著，但其计算需求和能耗限制了在资源受限环境中的部署。", "method": "该研究引入了一个结合群体编码SNNs和DRL的新框架。它利用SNNs的事件驱动、异步计算特性与DRL的鲁棒策略优化能力。核心是群体编码脉冲执行器网络（PopSAN），用于将高维观测编码为神经元群体活动，并通过基于梯度的更新实现最优策略学习。方法在Isaac Gym平台，使用PixMC基准和复杂的机器人操作任务（Franka机械臂）进行评估。", "result": "与传统人工神经网络（ANNs）相比，该方法实现了高达96.10%的能耗节省，同时保持了可媲美的控制性能。训练后的SNN策略在手指位置跟踪方面表现出鲁棒性，与指令轨迹偏差极小，并在抓取放置操作中保持了稳定的目标高度。", "conclusion": "群体编码SNNs为资源受限应用中的高能效、高性能机器人控制提供了一个有前景的解决方案，为在真实世界机器人系统中进行可扩展部署铺平了道路。"}}
{"id": "2510.10549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10549", "abs": "https://arxiv.org/abs/2510.10549", "authors": ["Xinbang Dai", "Huikang Hu", "Yongrui Chen", "Jiaqi Li", "Rihui Jin", "Yuyang Zhang", "Xiaoguang Li", "Lifeng Shang", "Guilin Qi"], "title": "ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding", "comment": "25 pages, 20 figures", "summary": "While large language models (LLMs) excel at many domain-specific tasks, their\nability to deeply comprehend and reason about full-length academic papers\nremains underexplored. Existing benchmarks often fall short of capturing such\ndepth, either due to surface-level question design or unreliable evaluation\nmetrics. To address this gap, we introduce ELAIPBench, a benchmark curated by\ndomain experts to evaluate LLMs' comprehension of artificial intelligence (AI)\nresearch papers. Developed through an incentive-driven, adversarial annotation\nprocess, ELAIPBench features 403 multiple-choice questions from 137 papers. It\nspans three difficulty levels and emphasizes non-trivial reasoning rather than\nshallow retrieval. Our experiments show that the best-performing LLM achieves\nan accuracy of only 39.95%, far below human performance. Moreover, we observe\nthat frontier LLMs equipped with a thinking mode or a retrieval-augmented\ngeneration (RAG) system fail to improve final results-even harming accuracy due\nto overthinking or noisy retrieval. These findings underscore the significant\ngap between current LLM capabilities and genuine comprehension of academic\npapers.", "AI": {"tldr": "为评估大型语言模型（LLMs）对学术论文的深层理解和推理能力，研究引入了ELAIPBench基准测试，结果显示当前LLMs的表现远低于人类，即使配备高级功能也未能显著提升。", "motivation": "现有基准测试未能充分捕捉LLMs对完整学术论文的深层理解和推理能力，且其能力尚未被充分探索，存在表面化问题设计或不可靠评估指标的缺陷。", "method": "研究引入了ELAIPBench基准测试，由领域专家通过激励驱动的对抗性标注过程创建，包含来自137篇AI研究论文的403个多项选择题，涵盖三个难度级别，侧重于非平凡推理而非浅层检索。", "result": "表现最佳的LLM准确率仅为39.95%，远低于人类水平。配备思维模式或检索增强生成（RAG）系统的前沿LLMs未能提升最终结果，甚至因过度思考或噪声检索而损害了准确率。", "conclusion": "当前LLM能力与真正理解学术论文之间存在显著差距，表明LLMs在深层学术理解方面仍有很大提升空间。"}}
{"id": "2510.10494", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10494", "abs": "https://arxiv.org/abs/2510.10494", "authors": ["Martina G. Vilas", "Safoora Yousefi", "Besmira Nushi", "Eric Horvitz", "Vidhisha Balachandran"], "title": "Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning", "comment": null, "summary": "Reasoning models improve their problem-solving ability through inference-time\nscaling, allocating more compute via longer token budgets. Identifying which\nreasoning traces are likely to succeed remains a key opportunity: reliably\npredicting productive paths can substantially reduce wasted computation and\nimprove overall efficiency. We introduce Latent-Trajectory signals that\ncharacterize the temporal evolution of a model's internal representations\nduring the generation of intermediate reasoning tokens. By measuring the\noverall change in latent representations between the start and end of\nreasoning, the change accumulated across intermediate steps, and the extent to\nwhich these changes advance toward the final state, we show that these signals\npredict solution accuracy more reliably than both cross-layer metrics and\noutput-based confidence measures. When used to guide answer selection across\nmultiple sampled generations, Latent-Trajectory signals make test-time scaling\nmore effective and efficient than majority voting, reducing token usage by up\nto 70% while preserving and even improving accuracy by 2.6% on average.\nMoreover, these predictive signals often emerge early in the reasoning trace,\nenabling early selection and allocation of compute to the most promising\ncandidates. Our findings contribute not only practical strategies for\ninference-time efficiency, but also a deeper interpretability perspective on\nhow reasoning processes are represented and differentiated in latent space.", "AI": {"tldr": "该研究引入“潜在轨迹信号”来预测推理模型的成功路径，通过分析模型内部表示的演变，显著提高了推理时的效率和准确性，减少了计算浪费。", "motivation": "推理模型通过增加计算（更长的token预算）来提高解决问题的能力。关键在于识别哪些推理路径可能成功，以大幅减少浪费的计算并提高整体效率。", "method": "引入“潜在轨迹信号”，用于表征模型在生成中间推理token时内部表示的时间演变。具体测量了推理开始和结束之间潜在表示的总体变化、中间步骤中累积的变化，以及这些变化趋向最终状态的程度。", "result": "潜在轨迹信号比跨层指标和基于输出的置信度测量更能可靠地预测解决方案的准确性。在多样本生成中，它们在指导答案选择时比多数投票更有效和高效，可将token使用量减少高达70%，同时保持甚至平均提高2.6%的准确性。此外，这些预测信号通常在推理早期出现，支持早期选择和计算分配。", "conclusion": "该研究不仅为推理时效率提供了实用的策略，还从更深层次的可解释性角度揭示了推理过程如何在潜在空间中被表示和区分。"}}
{"id": "2510.10545", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10545", "abs": "https://arxiv.org/abs/2510.10545", "authors": ["Koki Yamane", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Decoupled Scaling 4ch Bilateral Control on the Cartesian coordinate by 6-DoF Manipulator using Rotation Matrix", "comment": "6 pages, 4 figures, Accepted at SAMCON 2025", "summary": "Four-channel bilateral control is a method for achieving remote control with\nforce feedback and adjustment operability by synchronizing the positions and\nforces of two manipulators. This is expected to significantly improve the\noperability of the remote control in contact-rich tasks. Among these, 4-channel\nbilateral control on the Cartesian coordinate system is advantageous owing to\nits suitability for manipulators with different structures and because it\nallows the dynamics in the Cartesian coordinate system to be adjusted by\nadjusting the control parameters, thus achieving intuitive operability for\nhumans. This paper proposes a 4-channel bilateral control method that achieves\nthe desired dynamics by decoupling each dimension in the Cartesian coordinate\nsystem regardless of the scaling factor.", "AI": {"tldr": "本文提出了一种四通道双边控制方法，可在笛卡尔坐标系中解耦各维度，实现所需的动态特性，且不受缩放因子影响，从而提高远程操作性。", "motivation": "四通道双边控制通过同步两个机械臂的位置和力，提供力反馈和可调操作性，显著改善接触密集型任务的远程操作。在笛卡尔坐标系下的四通道双边控制对不同结构的机械臂具有优势，并能通过调整控制参数来调整笛卡尔坐标系中的动态特性，从而实现直观的操作性。", "method": "本文提出了一种四通道双边控制方法，该方法通过在笛卡尔坐标系中解耦每个维度，实现了所需的动态特性，且不受缩放因子影响。", "result": "该方法能够在笛卡尔坐标系中通过解耦各维度来实现期望的动态特性，并且这种实现方式独立于缩放因子。", "conclusion": "所提出的四通道双边控制方法，通过在笛卡尔坐标系中解耦各维度，能够实现期望的动态特性，且不受缩放因子影响，有望显著提高远程操作性，特别适用于不同结构的机械臂和接触密集型任务。"}}
{"id": "2510.10052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10052", "abs": "https://arxiv.org/abs/2510.10052", "authors": ["Kaitao Chen", "Shaohao Rui", "Yankai Jiang", "Jiamin Wu", "Qihao Zheng", "Chunfeng Song", "Xiaosong Wang", "Mu Zhou", "Mianxin Liu"], "title": "Think Twice to See More: Iterative Visual Reasoning in Medical VLMs", "comment": "25 pages, 21 figures", "summary": "Medical vision-language models (VLMs) excel at image-text understanding but\ntypically rely on a single-pass reasoning that neglects localized visual cues.\nIn clinical practice, however, human experts iteratively scan, focus, and\nrefine the regions of interest before reaching a final diagnosis. To narrow\nthis machine-human perception gap, we introduce ViTAR, a novel VLM framework\nthat emulates the iterative reasoning process of human experts through a\ncognitive chain of \"think-act-rethink-answer\". ViTAR treats medical images as\ninteractive objects, enabling models to engage multi-step visual reasoning. To\nsupport this approach, we curate a high-quality instruction dataset comprising\n1K interactive examples that encode expert-like diagnostic behaviors. In\naddition, a 16K visual question answering training data has been curated\ntowards fine-grained visual diagnosis. We introduce a two-stage training\nstrategy that begins with supervised fine-tuning to guide cognitive\ntrajectories, followed by the reinforcement learning to optimize\ndecision-making. Extensive evaluations demonstrate that ViTAR outperforms\nstrong state-of-the-art models. Visual attention analysis reveals that from the\n\"think\" to \"rethink\" rounds, ViTAR increasingly anchors visual grounding to\nclinically critical regions and maintains high attention allocation to visual\ntokens during reasoning, providing mechanistic insight into its improved\nperformance. These findings demonstrate that embedding expert-style iterative\nthinking chains into VLMs enhances both performance and trustworthiness of\nmedical AI.", "AI": {"tldr": "ViTAR是一个新型的医学视觉-语言模型（VLM）框架，它通过模拟人类专家迭代的“思考-行动-再思考-回答”推理过程，利用交互式图像和两阶段训练策略，显著提升了医学AI的性能和可信度。", "motivation": "现有医学视觉-语言模型（VLMs）通常采用单次推理，忽略局部视觉线索，这与人类专家迭代扫描、聚焦和精炼感兴趣区域以进行诊断的临床实践存在差距。研究旨在缩小这种机器与人类感知之间的差距。", "method": "本文提出了ViTAR框架，通过“思考-行动-再思考-回答”的认知链模拟人类专家的迭代推理过程，将医学图像视为交互对象以支持多步视觉推理。方法包括：1) 策划一个包含1K交互式示例的高质量指令数据集，编码专家级诊断行为；2) 策划一个包含16K视觉问答训练数据，用于细粒度视觉诊断；3) 采用两阶段训练策略：首先进行监督微调以指导认知轨迹，然后通过强化学习优化决策。", "result": "广泛评估表明，ViTAR的性能超越了强大的现有最先进模型。视觉注意力分析显示，从“思考”到“再思考”阶段，ViTAR逐渐将视觉基础锚定到临床关键区域，并在推理过程中保持对视觉token的高度注意力分配，这为其性能提升提供了机制上的解释。", "conclusion": "将专家风格的迭代思维链嵌入到医学视觉-语言模型（VLMs）中，能够显著提升医学AI的性能和可信度。"}}
{"id": "2510.10003", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10003", "abs": "https://arxiv.org/abs/2510.10003", "authors": ["Jianjin Wang", "Runsong Zhao", "Xiaoqian Liu", "Yuan Ge", "Ziqiang Xu", "Tong Xiao", "Shengxiang Gao", "Zhengtao Yu", "Jingbo Zhu"], "title": "MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction", "comment": "Copyright 2026 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Current direct speech-to-speech translation methods predominantly employ\nspeech tokens as intermediate representations. However, a single speech token\nis not dense in semantics, so we generally need multiple tokens to express a\ncomplete semantic unit. To address this limitation, we introduce multi-token\nprediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling\nmodels to predict multiple subsequent tokens at each position, thereby\ncapturing more complete semantics and enhancing information density per\nposition. Initial MTP implementations apply the loss at the final layer, which\nimproves output representation but initiates information enrichment too late.\nWe hypothesize that advancing the information enrichment process to\nintermediate layers can achieve earlier and more effective enhancement of\nhidden representation. Consequently, we propose MTP-S2UT loss, applying MTP\nloss to hidden representation where CTC loss is computed. Experiments\ndemonstrate that all MTP loss variants consistently improve the quality of S2UT\ntranslation, with MTP-S2UT achieving the best performance.", "AI": {"tldr": "本文提出将多令牌预测（MTP）损失引入语音到单元翻译（S2UT）模型，以解决单个语音令牌语义密度不足的问题。通过在每个位置预测多个后续令牌，并特别是在中间层应用MTP损失（MTP-S2UT），显著提高了S2UT的翻译质量。", "motivation": "当前的直接语音到语音翻译方法主要使用语音令牌作为中间表示，但单个语音令牌语义密度不高，通常需要多个令牌才能表达一个完整的语义单元，这限制了信息密度。", "method": "研究引入了多令牌预测（MTP）损失到S2UT模型中，使模型能够在每个位置预测多个后续令牌，从而捕获更完整的语义并提高信息密度。此外，论文提出MTP-S2UT损失，将MTP损失应用于计算CTC损失的中间隐藏表示层，以实现更早、更有效的隐藏表示增强。", "result": "实验表明，所有MTP损失变体都能持续改进S2UT翻译质量，其中MTP-S2UT实现了最佳性能。", "conclusion": "通过在S2UT模型中引入多令牌预测损失，特别是将其应用于中间隐藏表示层（MTP-S2UT），能够有效解决单个语音令牌语义密度不足的问题，显著提升语音到单元翻译的质量。"}}
{"id": "2510.10009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10009", "abs": "https://arxiv.org/abs/2510.10009", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu"], "title": "Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning", "comment": null, "summary": "Reasoning-augmented search agents, such as Search-R1, are trained to reason,\nsearch, and generate the final answer iteratively. Nevertheless, due to their\nlimited capabilities in reasoning and search, their performance on multi-hop QA\nbenchmarks remains far from satisfactory. To handle complex or compound\nqueries, we train an LLM-based search agent with the native capability of query\nexpansion through reinforcement learning. In each turn, our search agent\nproposes several query variants, which are searched simultaneously to cover\nmore relevant information. Meanwhile, given limited post-training data and\ncomputing resources, it is very challenging for a search agent to master\nmultiple tasks, including query generation, retrieved information\nunderstanding, and answer generation. Therefore, we propose incorporating a\npre-trained squeezer model that helps the search agent understand the retrieved\ndocuments, allowing the search agent to focus on query generation for high\nretrieval recall. With the assistance of the squeezer model, we discover that\neven a small-scale 3B LLM can demonstrate a strong capability of query\nexpansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.\nTo be specific, our experiments across seven question-answering benchmarks\ndemonstrate that our method, named ExpandSearch, achieves an average\nimprovement of 4.4% compared to state-of-the-art baselines, with strong gains\non multi-hop reasoning tasks requiring diverse evidence aggregation.", "AI": {"tldr": "本文提出ExpandSearch，一个基于LLM的搜索代理，通过强化学习实现原生查询扩展，并结合预训练的“squeezer”模型辅助文档理解，显著提升了多跳问答（multi-hop QA）的性能，即使是小型LLM也能达到最先进水平。", "motivation": "现有的推理增强搜索代理（如Search-R1）在推理和搜索能力上有限，导致在多跳问答基准测试上表现不佳，难以处理复杂或复合查询。此外，在有限的训练数据和计算资源下，让一个搜索代理掌握查询生成、检索信息理解和答案生成等多项任务极具挑战性。", "method": "通过强化学习训练一个基于LLM的搜索代理，使其具备原生的查询扩展能力。在每个回合中，该代理会提出多个查询变体进行并行搜索以覆盖更多相关信息。同时，引入一个预训练的“squeezer”模型来帮助搜索代理理解检索到的文档，从而让搜索代理能专注于查询生成以实现高检索召回率。", "result": "研究发现，即使是小型3B LLM，在squeezer模型的辅助下，也能展现出强大的查询扩展能力，并在多跳问答基准测试上达到最先进的准确率。具体而言，ExpandSearch在七个问答基准测试中，相较于最先进的基线方法平均提高了4.4%的准确率，在需要聚合多样化证据的多跳推理任务上取得了显著的提升。", "conclusion": "ExpandSearch通过结合查询扩展和squeezer模型，有效解决了多跳问答中推理和搜索能力的局限性，即使使用较小的LLM也能显著提升复杂查询的信息检索和问答性能，实现了SOTA表现。"}}
{"id": "2510.11476", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11476", "abs": "https://arxiv.org/abs/2510.11476", "authors": ["Nan Gu", "Ge Chen", "Junjie Qin"], "title": "The Role of Flexible Connection in Accelerating Load Interconnection in Distribution Networks", "comment": null, "summary": "This paper investigates the role of flexible connection in accelerating the\ninterconnection of large loads amid rising electricity demand from data centers\nand electrification. Flexible connection allows new loads to defer or curtail\nconsumption during rare, grid-constrained periods, enabling faster access\nwithout major infrastructure upgrades. To quantify how flexible connection\nunlocks load hosting capacity, we formulate a flexibility-aware hosting\ncapacity analysis problem that explicitly limits the number of\nutility-controlled interventions per year, ensuring infrequent disruption.\nEfficient solution methods are developed for this nonconvex problem and applied\nto real load data and test feeders. Empirical results reveal that modest\nflexibility, i.e., few interventions with small curtailments or delays, can\nunlock substantial hosting capacity. Theoretical analysis further explains and\ngeneralizes these findings, highlighting the broad potential of flexible\nconnection.", "AI": {"tldr": "该论文研究了灵活连接在数据中心和电气化背景下，如何通过在电网受限时推迟或削减消费，加速大型负荷并网。研究表明，即使是适度的灵活性，也能显著提升电网承载能力，且无需大规模基础设施升级。", "motivation": "数据中心和电气化导致电力需求不断增长，大型负荷的并网面临挑战。传统的并网方式需要大量基础设施升级，耗时且成本高昂。灵活连接提供了一种无需大规模升级即可加速并网的潜在解决方案。", "method": "研究构建了一个考虑灵活性的负荷承载能力分析问题，明确限制了每年公用事业干预的次数，以确保对用户的影响不频繁。论文开发了针对此非凸问题的有效求解方法，并将其应用于实际负荷数据和测试馈线。", "result": "实证结果表明，适度的灵活性（即少量干预、小幅削减或延迟）即可显著提升负荷承载能力。理论分析进一步解释并推广了这些发现。", "conclusion": "灵活连接在加速大型负荷并网和提升电网承载能力方面具有广泛潜力，即使是有限的灵活性也能带来显著效益。"}}
{"id": "2510.11515", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11515", "abs": "https://arxiv.org/abs/2510.11515", "authors": ["Shanthan Kumar Padisala", "Bharatkumar Hegde", "Ibrahim Haskara", "Satadru Dey"], "title": "A Physics-Informed Reinforcement Learning Approach for Degradation-Aware Long-Term Charging Optimization in Batteries", "comment": null, "summary": "Batteries degrade with usage and continuous cycling. This aging is typically\nreflected through the resistance growth and the capacity fade of battery cells.\nOver the years, various charging methods have been presented in the literature\nthat proposed current profiles in order to enable optimal, fast, and/or\nhealth-conscious charging. However, very few works have attempted to make the\nubiquitous Constant Current Constant Voltage (CCCV) charging protocol adaptive\nto the changing battery health as it cycles. This work aims to address this gap\nand proposes a framework that optimizes the constant current part of the CCCV\nprotocol adapting to long-term battery degradation. Specifically, a\nphysics-informed Reinforcement Learning (RL) approach has been used that not\nonly estimates a key battery degradation mechanism, namely, Loss of Active\nMaterial (LAM), but also adjusts the current magnitude of CCCV as a result of\nthis particular degradation. The proposed framework has been implemented by\ncombining PyBamm, an open-source battery modeling tool, and Stable-baselines\nwhere the RL agent was trained using a Proximal Policy Optimization (PPO)\nnetwork. Simulation results show the potential of the proposed framework for\nenhancing the widely used CCCV protocol by embedding physics information in RL\nalgorithm. A comparative study of this proposed agent has also been discussed\nwith 2 other charging protocols generated by a non-physics-based RL agent and a\nconstant CCCV for all the cycles.", "AI": {"tldr": "本文提出了一种基于物理信息的强化学习框架，旨在使广泛使用的恒流恒压（CCCV）充电协议能够根据电池长期退化（特别是活性材料损失LAM）进行自适应调整，从而优化充电电流。", "motivation": "电池在使用过程中会退化，表现为内阻增加和容量衰减。现有充电方法（包括CCCV）通常不适应电池健康状况的变化。很少有工作尝试使CCCV协议适应电池循环过程中不断变化的健康状况，这造成了一个研究空白。", "method": "本研究提出一个框架，优化CCCV协议中的恒流部分，以适应电池的长期退化。具体方法是使用一种物理信息强化学习（RL）方法，该方法不仅估计关键电池退化机制（如活性材料损失LAM），还根据这种退化调整CCCV的电流大小。该框架结合了开源电池建模工具PyBamm和Stable-baselines，其中RL代理使用近端策略优化（PPO）网络进行训练。", "result": "仿真结果表明，所提出的框架通过在强化学习算法中嵌入物理信息，具有增强广泛使用的CCCV协议的潜力。研究还比较了该代理与其他两种充电协议（非物理RL代理生成和所有循环均采用恒定CCCV）的性能。", "conclusion": "该研究提出的基于物理信息强化学习的框架，通过使其适应电池退化，展示了增强广泛使用的CCCV充电协议的巨大潜力。"}}
{"id": "2510.10053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10053", "abs": "https://arxiv.org/abs/2510.10053", "authors": ["Bo Peng", "Zichuan Wang", "Sheng Yu", "Xiaochuan Jin", "Wei Wang", "Jing Dong"], "title": "DREAM: A Benchmark Study for Deepfake REalism AssessMent", "comment": null, "summary": "Deep learning based face-swap videos, widely known as deepfakes, have drawn\nwide attention due to their threat to information credibility. Recent works\nmainly focus on the problem of deepfake detection that aims to reliably tell\ndeepfakes apart from real ones, in an objective way. On the other hand, the\nsubjective perception of deepfakes, especially its computational modeling and\nimitation, is also a significant problem but lacks adequate study. In this\npaper, we focus on the visual realism assessment of deepfakes, which is defined\nas the automatic assessment of deepfake visual realism that approximates human\nperception of deepfakes. It is important for evaluating the quality and\ndeceptiveness of deepfakes which can be used for predicting the influence of\ndeepfakes on Internet, and it also has potentials in improving the deepfake\ngeneration process by serving as a critic. This paper prompts this new\ndirection by presenting a comprehensive benchmark called DREAM, which stands\nfor Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of\ndiverse quality, a large scale annotation that includes 140,000 realism scores\nand textual descriptions obtained from 3,500 human annotators, and a\ncomprehensive evaluation and analysis of 16 representative realism assessment\nmethods, including recent large vision language model based methods and a newly\nproposed description-aligned CLIP method. The benchmark and insights included\nin this study can lay the foundation for future research in this direction and\nother related areas.", "AI": {"tldr": "本文关注Deepfake视频的视觉真实感评估，并提出了一个名为DREAM的综合基准，旨在近似人类感知，以促进该领域的研究。", "motivation": "当前Deepfake研究主要集中于客观检测，但其主观感知（特别是计算建模和模仿）研究不足。评估Deepfake的视觉真实感对于判断其质量和欺骗性、预测其在互联网上的影响以及改进生成过程至关重要。", "method": "本文定义了Deepfake视觉真实感评估为自动评估Deepfake视觉真实感以近似人类感知。为此，提出了DREAM（Deepfake REalism AssessMent）基准，该基准包含一个多样质量的Deepfake视频数据集、来自3,500名人类标注者的140,000个真实感分数和文本描述，并对16种代表性真实感评估方法（包括大型视觉语言模型和新提出的描述对齐CLIP方法）进行了综合评估和分析。", "result": "本文提出了DREAM基准，为Deepfake视觉真实感评估提供了一个全面的数据集、大规模人类标注（包括真实感分数和文本描述）以及对现有方法的广泛评估。研究结果包括对16种代表性方法的分析，为未来研究提供了深刻见解和基础。", "conclusion": "DREAM基准和本研究提供的见解为未来在Deepfake视觉真实感评估及其他相关领域的研究奠定了坚实基础，有望推动Deepfake质量评估和理解人类感知方面的进展。"}}
{"id": "2510.10567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10567", "abs": "https://arxiv.org/abs/2510.10567", "authors": ["Alexander Langmann", "Yevhenii Tokarev", "Mattia Piccinini", "Korbinian Moller", "Johannes Betz"], "title": "Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving", "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "Sampling-based trajectory planners are widely used for agile autonomous\ndriving due to their ability to generate fast, smooth, and kinodynamically\nfeasible trajectories. However, their behavior is often governed by a cost\nfunction with manually tuned, static weights, which forces a tactical\ncompromise that is suboptimal across the wide range of scenarios encountered in\na race. To address this shortcoming, we propose using a Reinforcement Learning\n(RL) agent as a high-level behavioral selector that dynamically switches the\ncost function parameters of an analytical, low-level trajectory planner during\nruntime. We show the effectiveness of our approach in simulation in an\nautonomous racing environment where our RL-based planner achieved 0% collision\nrate while reducing overtaking time by up to 60% compared to state-of-the-art\nstatic planners. Our new agent now dynamically switches between aggressive and\nconservative behaviors, enabling interactive maneuvers unattainable with static\nconfigurations. These results demonstrate that integrating reinforcement\nlearning as a high-level selector resolves the inherent trade-off between\nsafety and competitiveness in autonomous racing planners. The proposed\nmethodology offers a pathway toward adaptive yet interpretable motion planning\nfor broader autonomous driving applications.", "AI": {"tldr": "该论文提出一种将强化学习（RL）作为高层行为选择器的方法，用于动态调整采样式轨迹规划器的成本函数参数，以在自动驾驶赛车中实现更好的安全性和竞争力。", "motivation": "采样式轨迹规划器在自动驾驶中广泛使用，但其成本函数中的手动调优、静态权重导致在不同赛车场景中表现不佳，无法兼顾安全性与竞争力。", "method": "本文提出使用一个强化学习（RL）智能体作为高层行为选择器，在运行时动态切换低层分析型轨迹规划器的成本函数参数。", "result": "在自动驾驶赛车仿真环境中，RL规划器实现了0%的碰撞率，并将超车时间比现有静态规划器缩短了高达60%。该方法能够动态切换激进和保守行为，实现静态配置无法达到的交互式机动。", "conclusion": "将强化学习作为高层选择器，解决了自动驾驶赛车规划器中固有的安全性和竞争力之间的权衡问题，为更广泛的自动驾驶应用提供了自适应且可解释的运动规划途径。"}}
{"id": "2510.11583", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11583", "abs": "https://arxiv.org/abs/2510.11583", "authors": ["Siddhartha Upadhyay", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Smooth Spatiotemporal Tube Synthesis for Prescribed-Time Reach-Avoid-Stay Control", "comment": null, "summary": "In this work, we address the issue of controller synthesis for a\ncontrol-affine nonlinear system to meet prescribed time reach-avoid-stay\nspecifications. Our goal is to improve upon previous methods based on\nspatiotemporal tubes (STTs) by eliminating the need for circumvent functions,\nwhich often lead to abrupt tube modifications and high control effort. We\npropose an adaptive framework that constructs smooth STTs around static unsafe\nsets, enabling continuous avoidance while guiding the system toward the target\nwithin the prescribed time. A closed-form, approximation-free control law is\nderived to ensure the system trajectory remains within the tube and satisfies\nthe RAS task. The effectiveness of the proposed approach is demonstrated\nthrough a case study, showing a significant reduction in control effort\ncompared to prior methods.", "AI": {"tldr": "本文提出一种自适应框架，通过构建平滑的时空管（STTs）并推导闭式控制律，为控制仿射非线性系统解决在规定时间内满足到达-避障-停留（RAS）规范的控制器合成问题，显著降低了控制功耗。", "motivation": "现有基于时空管的方法需要规避函数（circumvent functions），这通常会导致时空管的突然修改和较高的控制功耗。本研究旨在改进这些方法，消除规避函数的需要。", "method": "提出一个自适应框架，围绕静态不安全集构建平滑的时空管，以实现连续避障，并引导系统在规定时间内到达目标。推导出一个闭式、无近似的控制律，以确保系统轨迹保持在时空管内并满足RAS任务。", "result": "通过案例研究证明了所提方法的有效性，与现有方法相比，显著降低了控制功耗。", "conclusion": "所提出的自适应框架通过构建平滑时空管并采用闭式控制律，能够有效解决控制仿射非线性系统的RAS任务，同时显著降低了控制功耗。"}}
{"id": "2510.10055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10055", "abs": "https://arxiv.org/abs/2510.10055", "authors": ["Zhi-Fen He", "Ren-Dong Xie", "Bo Li", "Bin Liu", "Jin-Yan Hu"], "title": "Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels", "comment": null, "summary": "Multi-label image recognition with incomplete labels is a critical learning\ntask and has emerged as a focal topic in computer vision. However, this task is\nconfronted with two core challenges: semantic-aware feature learning and\nmissing label recovery. In this paper, we propose a novel Collaborative\nLearning of Semantic-aware feature learning and Label recovery (CLSL) method\nfor multi-label image recognition with incomplete labels, which unifies the two\naforementioned challenges into a unified learning framework. More specifically,\nwe design a semantic-related feature learning module to learn robust\nsemantic-related features by discovering semantic information and label\ncorrelations. Then, a semantic-guided feature enhancement module is proposed to\ngenerate high-quality discriminative semantic-aware features by effectively\naligning visual and semantic feature spaces. Finally, we introduce a\ncollaborative learning framework that integrates semantic-aware feature\nlearning and label recovery, which can not only dynamically enhance the\ndiscriminability of semantic-aware features but also adaptively infer and\nrecover missing labels, forming a mutually reinforced loop between the two\nprocesses. Extensive experiments on three widely used public datasets (MS-COCO,\nVOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art\nmulti-label image recognition methods with incomplete labels.", "AI": {"tldr": "本文提出了一种名为CLSL的新型协同学习方法，用于解决不完整标签多标签图像识别中的语义感知特征学习和缺失标签恢复两大挑战，并在多个公开数据集上取得了最先进的性能。", "motivation": "多标签图像识别中存在不完整标签的问题，这带来了两个核心挑战：如何学习语义感知的特征，以及如何恢复缺失的标签。", "method": "本文提出了协同学习语义感知特征学习和标签恢复（CLSL）方法，将这两个挑战统一在一个框架中。具体而言，它设计了一个语义相关特征学习模块来发现语义信息和标签相关性；一个语义引导特征增强模块来对齐视觉和语义特征空间以生成高质量特征；最后，一个协同学习框架整合了特征学习和标签恢复，形成一个相互强化的循环。", "result": "在MS-COCO、VOC2007和NUS-WIDE这三个广泛使用的公共数据集上进行的实验表明，CLSL方法优于目前最先进的带有不完整标签的多标签图像识别方法。", "conclusion": "CLSL通过统一语义感知特征学习和标签恢复，并利用协同学习机制，有效解决了不完整标签多标签图像识别中的关键问题，显著提升了识别性能。"}}
{"id": "2510.10597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10597", "abs": "https://arxiv.org/abs/2510.10597", "authors": ["David Rodríguez-Martínez", "C. J. Pérez del Pulgar"], "title": "Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation", "comment": "9 pages, 6 figures, conference paper", "summary": "Improving robotic navigation is critical for extending exploration range and\nenhancing operational efficiency. Vision-based navigation relying on\ntraditional CCD or CMOS cameras faces major challenges when complex\nillumination conditions are paired with motion, limiting the range and\naccessibility of mobile planetary robots. In this study, we propose a novel\napproach to planetary navigation that leverages the unique imaging capabilities\nof Single-Photon Avalanche Diode (SPAD) cameras. We present the first\ncomprehensive evaluation of single-photon imaging as an alternative passive\nsensing technology for robotic exploration missions targeting perceptually\nchallenging locations, with a special emphasis on high-latitude lunar regions.\nWe detail the operating principles and performance characteristics of SPAD\ncameras, assess their advantages and limitations in addressing key perception\nchallenges of upcoming exploration missions to the Moon, and benchmark their\nperformance under representative illumination conditions.", "AI": {"tldr": "本研究提出并首次全面评估了利用单光子雪崩二极管（SPAD）相机进行行星导航的新方法，以应对传统相机在复杂光照条件下的局限性，特别关注高纬度月球区域。", "motivation": "提高机器人导航能力对于扩展探索范围和提高操作效率至关重要。传统的CCD或CMOS相机在复杂光照和运动条件下表现不佳，限制了移动行星机器人的探索范围和可达性。", "method": "本研究提出了一种利用SPAD相机独特成像能力进行行星导航的新方法。详细介绍了SPAD相机的工作原理和性能特征，评估了其在解决未来月球探索任务感知挑战中的优势和局限性，并在代表性光照条件下对其性能进行了基准测试。", "result": "本研究首次对单光子成像作为一种替代性被动传感技术进行了全面评估，用于面向感知挑战性地点的机器人探索任务，特别强调了高纬度月球区域。评估了SPAD相机在解决月球探索任务关键感知挑战中的优势和局限性，并对其在代表性光照条件下的性能进行了基准测试。", "conclusion": "SPAD相机作为一种替代性的被动传感技术，在应对复杂光照条件下（如高纬度月球区域）的行星导航感知挑战方面具有巨大潜力，有望扩展机器人探索的范围和效率。"}}
{"id": "2510.10025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10025", "abs": "https://arxiv.org/abs/2510.10025", "authors": ["Jiaqi Liu", "Lanruo Wang", "Su Liu", "Xin Hu"], "title": "Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default", "comment": "Healthcare AI, Medical Text Classification, Lightweight LLMs,\n  DistilBERT, Reproducibility", "summary": "Large language models work well for many NLP tasks, but they are hard to\ndeploy in health settings with strict cost, latency, and privacy limits. We\nrevisit a lightweight recipe for medical abstract classification and ask how\nfar compact encoders can go under a controlled budget. Using the public medical\nabstracts corpus, we finetune BERT base and DistilBERT with three objectives\nstandard cross-entropy, class weighted cross entropy, and focal loss keeping\ntokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with\nplain cross-entropy gives the best balance on the test set while using far\nfewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,\nrelease the evaluation code, and include confusion analyses to make error\npatterns clear. Our results suggest a practical default: start with a compact\nencoder and cross-entropy, then add calibration and task-specific checks before\nmoving to heavier models.", "AI": {"tldr": "研究表明，在医疗摘要分类任务中，紧凑型编码器（如DistilBERT）结合标准交叉熵损失，能在低成本、低延迟和高隐私要求下，实现性能与效率的最佳平衡，优于BERT base。", "motivation": "大型语言模型在医疗环境中部署面临严格的成本、延迟和隐私限制。本研究旨在探讨在受控预算下，轻量级编码器在医疗摘要分类任务中能达到何种程度。", "method": "使用公共医疗摘要语料库，对BERT base和DistilBERT进行微调。实验中固定了分词器、序列长度、优化器和调度器。采用了三种目标函数：标准交叉熵、类别加权交叉熵和Focal Loss。评估指标包括准确率、Macro F1和Weighted F1，并进行了混淆分析。", "result": "结果显示，DistilBERT结合标准交叉熵损失在测试集上提供了最佳的平衡，且参数量远少于BERT base。", "conclusion": "研究建议，在医疗摘要分类等任务中，实用的默认方法是首先使用紧凑型编码器和交叉熵损失，然后添加校准和任务特定检查，最后再考虑使用更大型的模型。"}}
{"id": "2510.10013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10013", "abs": "https://arxiv.org/abs/2510.10013", "authors": ["Yuyi Huang", "Runzhe Zhan", "Lidia S. Chao", "Ailin Tao", "Derek F. Wong"], "title": "Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed for complex\nreasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a\nkey paradigm for structured inference. Despite early-stage safeguards enabled\nby alignment techniques such as RLHF, we identify a previously underexplored\nvulnerability: reasoning trajectories in Long-CoT models can drift from aligned\npaths, resulting in content that violates safety constraints. We term this\nphenomenon Path Drift. Through empirical analysis, we uncover three behavioral\ntriggers of Path Drift: (1) first-person commitments that induce goal-driven\nreasoning that delays refusal signals; (2) ethical evaporation, where\nsurface-level disclaimers bypass alignment checkpoints; (3) condition chain\nescalation, where layered cues progressively steer models toward unsafe\ncompletions. Building on these insights, we introduce a three-stage Path Drift\nInduction Framework comprising cognitive load amplification, self-role priming,\nand condition chain hijacking. Each stage independently reduces refusal rates,\nwhile their combination further compounds the effect. To mitigate these risks,\nwe propose a path-level defense strategy incorporating role attribution\ncorrection and metacognitive reflection (reflective safety cues). Our findings\nhighlight the need for trajectory-level alignment oversight in long-form\nreasoning beyond token-level alignment.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在长链思维（Long-CoT）推理中存在“路径漂移”漏洞，即推理路径偏离安全准则。文章揭示了导致漂移的三个行为触发器，提出了一个诱导路径漂移的框架，并提出了一种路径级别的防御策略。", "motivation": "尽管大型语言模型（LLMs）在复杂推理任务中广泛部署并采用RLHF等对齐技术进行早期安全防护，但研究者发现Long-CoT模型中的推理轨迹可能偏离对齐路径，导致违反安全约束的内容，这一漏洞此前未被充分探索。", "method": "通过实证分析揭示了路径漂移的三个行为触发器。在此基础上，引入了一个由认知负荷放大、自我角色启动和条件链劫持组成的三阶段路径漂移诱导框架。为缓解风险，提出了一种包含角色归因纠正和元认知反思（反思性安全提示）的路径级防御策略。", "result": "发现了路径漂移的三个行为触发器：第一人称承诺诱导的目标驱动推理延迟拒绝信号；表面免责声明绕过对齐检查点的伦理蒸发；以及分层提示逐步引导模型走向不安全完成的条件链升级。诱导框架的每个阶段都能独立降低拒绝率，组合使用时效果更强。提出的防御策略能够有效缓解这些风险。", "conclusion": "研究结果强调了在长篇推理中，除了令牌级对齐，还需要对轨迹级对齐进行监督，以确保模型的安全性。"}}
{"id": "2510.10596", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10596", "abs": "https://arxiv.org/abs/2510.10596", "authors": ["Ruolan Cheng", "Yong Deng", "Serafín Moral", "José Ramón Trillo"], "title": "A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective", "comment": null, "summary": "Random permutation set (RPS) is a recently proposed framework designed to\nrepresent order-structured uncertain information. Measuring the distance\nbetween permutation mass functions is a key research topic in RPS theory\n(RPST). This paper conducts an in-depth analysis of distances between RPSs from\ntwo different perspectives: random finite set (RFS) and transferable belief\nmodel (TBM). Adopting the layer-2 belief structure interpretation of RPS, we\nregard RPST as a refinement of TBM, where the order in the ordered focus set\nrepresents qualitative propensity. Starting from the permutation, we introduce\na new definition of the cumulative Jaccard index to quantify the similarity\nbetween two permutations and further propose a distance measure method for RPSs\nbased on the cumulative Jaccard index matrix. The metric and structural\nproperties of the proposed distance measure are investigated, including the\npositive definiteness analysis of the cumulative Jaccard index matrix, and a\ncorrection scheme is provided. The proposed method has a natural\ntop-weightiness property: inconsistencies between higher-ranked elements tend\nto result in greater distance values. Two parameters are provided to the\ndecision-maker to adjust the weight and truncation depth. Several numerical\nexamples are used to compare the proposed method with the existing method. The\nexperimental results show that the proposed method not only overcomes the\nshortcomings of the existing method and is compatible with the Jousselme\ndistance, but also has higher sensitivity and flexibility.", "AI": {"tldr": "本文提出了一种基于累积Jaccard指数矩阵的随机排列集（RPS）距离度量新方法，该方法从随机有限集（RFS）和可迁移信任模型（TBM）的角度深入分析了RPS的距离，克服了现有方法的缺点，并具有更高的敏感性和灵活性。", "motivation": "在随机排列集（RPS）理论（RPST）中，度量排列质量函数之间的距离是一个关键的研究课题。现有方法存在不足，需要更有效、更灵活的距离度量方法。", "method": "本文从RFS和TBM的角度分析了RPS之间的距离，将RPST解释为TBM的精炼。引入了累积Jaccard指数的新定义来量化两个排列之间的相似性，并在此基础上提出了一种基于累积Jaccard指数矩阵的RPS距离度量方法。研究了所提出度量方法的度量和结构性质，包括累积Jaccard指数矩阵的正定性分析和校正方案。提供了两个参数供决策者调整权重和截断深度。", "result": "所提出的方法具有自然的顶部权重性，即高排名元素之间的不一致会导致更大的距离值。实验结果表明，该方法不仅克服了现有方法的缺点，与Jousselme距离兼容，而且具有更高的敏感性和灵活性。", "conclusion": "本文提出的基于累积Jaccard指数矩阵的RPS距离度量方法是一种有效、敏感且灵活的工具，能够更好地量化RPS之间的差异，并克服了现有方法的局限性。"}}
{"id": "2510.11692", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11692", "abs": "https://arxiv.org/abs/2510.11692", "authors": ["Samuel G. Gessow", "Brett T. Lopez"], "title": "Analysis of the Geometric Heat Flow Equation: Computing Geodesics in Real-Time with Convergence Guarantees", "comment": null, "summary": "We present an analysis on the convergence properties of the so-called\ngeometric heat flow equation for computing geodesics (shortest-path~curves) on\nRiemannian manifolds. Computing geodesics numerically in real-time has become\nan important capability in several fields, including control and motion\nplanning. The geometric heat flow equation involves solving a parabolic partial\ndifferential equation whose solution is a geodesic. In practice, solving this\nPDE numerically can be done efficiently, and tends to be more numerically\nstable and exhibit a better rate of convergence compared to numerical\noptimization. We prove that the geometric heat flow equation is globally\nexponentially stable in $L_2$ if the curvature of the Riemannian manifold is\nnot too positive, and that asymptotic convergence in $L_2$ is always\nguaranteed. We also present a pseudospectral method that leverages Chebyshev\npolynomials to accurately compute geodesics in only a few milliseconds for\nnon-contrived manifolds. Our analysis was verified with our custom\npseudospectral method by computing geodesics on common non-Euclidean surfaces,\nand in feedback for a contraction-based controller with a non-flat metric for a\nnonlinear system.", "AI": {"tldr": "本文分析了几何热流方程在黎曼流形上计算测地线的收敛性，并证明了其全局指数稳定性（在特定曲率条件下）和渐近收敛性，同时提出了一种高效的伪谱方法。", "motivation": "在控制和运动规划等领域，实时数值计算黎曼流形上的测地线（最短路径曲线）已成为一项重要能力。", "method": "本文采用几何热流方程（通过求解抛物型偏微分方程）来计算测地线。此外，还提出了一种利用切比雪夫多项式的伪谱方法，以提高计算精度和效率。", "result": "研究证明，如果黎曼流形的曲率不过于正，几何热流方程在$L_2$空间中具有全局指数稳定性；并且$L_2$空间中的渐近收敛性始终得到保证。提出的伪谱方法能够在几毫秒内精确计算非特殊流形上的测地线，并通过在常见非欧几何曲面和非线性系统控制器中的应用验证了其有效性。", "conclusion": "几何热流方程是计算黎曼流形上测地线的一种数值稳定且收敛性良好的方法，尤其结合伪谱方法时，能够实现高效、实时的测地线计算。"}}
{"id": "2510.10603", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10603", "abs": "https://arxiv.org/abs/2510.10603", "authors": ["WenTao Liu", "Siyu Song", "Hao Hao", "Aimin Zhou"], "title": "EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms", "comment": null, "summary": "In recent years, large language models (LLMs) have made remarkable progress,\nwith model optimization primarily relying on gradient-based optimizers such as\nAdam. However, these gradient-based methods impose stringent hardware\nrequirements, demanding high-concurrency, high-memory GPUs. Moreover, they\nrequire all neural network operations to be differentiable, thereby excluding\nmany promising non-differentiable architectures from practical use. To address\nthese limitations, we propose a method for optimizing LLMs using evolutionary\nalgorithms (EA4LLM) and, for the first time, successfully demonstrate its\ncapability to train a 1-billion-parameter LLM from the pre-trained stage. We\nconduct extensive experiments and provide key insights into how evolutionary\nalgorithms can effectively optimize neural networks. Our work challenges the\nprevailing assumption that gradient-based optimization is the only viable\napproach for training neural networks. It also holds significant potential to\nreduce the computational cost of training large language models, thereby\nenabling groups with limited computational resources to participate in deep\nlearning research.", "AI": {"tldr": "本文首次提出并成功展示了使用进化算法（EA4LLM）来优化和训练一个十亿参数量级的大型语言模型，挑战了梯度优化作为唯一可行方法的假设。", "motivation": "现有的大语言模型优化主要依赖于梯度优化器（如Adam），但这带来了严苛的硬件要求（高并发、高内存GPU），并且要求所有神经网络操作都可微分，从而排除了许多有前景的不可微分架构。", "method": "作者提出了一种使用进化算法优化大型语言模型的方法（EA4LLM），并首次成功地将其应用于从预训练阶段训练一个十亿参数量级的大语言模型。", "result": "研究成功地展示了进化算法训练十亿参数LLM的能力，并提供了关于进化算法如何有效优化神经网络的关键见解。", "conclusion": "这项工作挑战了梯度优化是训练神经网络唯一可行方法的普遍假设。它还具有显著潜力，可以降低训练大型语言模型的计算成本，从而使计算资源有限的团队也能参与深度学习研究。"}}
{"id": "2510.10062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10062", "abs": "https://arxiv.org/abs/2510.10062", "authors": ["Adnan El Assadi", "Isaac Chung", "Roman Solomatin", "Niklas Muennighoff", "Kenneth Enevoldsen"], "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task", "comment": "Submitted to ICLR 2026", "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.", "AI": {"tldr": "本文引入了HUME框架，旨在测量人类在文本嵌入任务上的表现，以更好地理解模型能力。研究发现，人类平均表现略低于最佳模型，但不同任务和语言之间存在显著差异，并提供了人类表现基线和评估框架。", "motivation": "理解嵌入模型的优势和局限性需要比较人类和模型的表现，但衡量人类在嵌入任务上的性能十分困难。现有评估框架（如MTEB）缺乏可靠的人类表现估计，限制了模型分数的解释性。", "method": "引入了HUME（文本嵌入人类评估框架），并在涵盖重排序、分类、聚类和语义文本相似性等任务的16个MTEB数据集上测量了人类表现，这些数据集包括多种高资源和低资源语言。", "result": "人类的平均表现为77.6%，而最佳嵌入模型为80.1%。然而，性能差异显著：模型在某些数据集上接近上限，但在其他数据集（特别是低资源语言）上表现不佳，这表明数据集存在问题并揭示了模型在低资源语言上的不足。", "conclusion": "该研究提供了人类表现基线、任务难度模式的洞察以及一个可扩展的评估框架。这使得模型解释更具意义，并为模型和基准的开发提供了信息。"}}
{"id": "2510.10602", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10602", "abs": "https://arxiv.org/abs/2510.10602", "authors": ["Zhuoheng Gao", "Jiyao Zhang", "Zhiyong Xie", "Hao Dong", "Zhaofei Yu", "Rongmei Chen", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "comment": null, "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects.", "AI": {"tldr": "SpikeGrasp提出了一种受神经学启发的6自由度抓取检测框架，直接处理立体脉冲相机原始数据，通过循环脉冲神经网络推断抓取姿态，无需重建点云，在复杂场景下优于传统方法。", "motivation": "大多数机器人抓取系统依赖于将传感器数据转换为显式3D点云，这在生物智能中不存在。本文旨在探索一种根本不同、受神经学启发的6自由度抓取检测范式。", "method": "引入SpikeGrasp框架，模仿生物视觉运动通路。它处理来自立体脉冲相机的原始、异步事件流，并融合这些流。使用循环脉冲神经网络（类似于高级视觉处理）迭代细化抓取假设，全程不重建点云。通过构建大规模合成基准数据集进行验证。", "result": "实验表明，SpikeGrasp超越了传统的基于点云的基线方法，尤其在杂乱和无纹理场景中表现更优，并展示了显著的数据效率。", "conclusion": "该研究证实了这种端到端、受神经学启发方法的T可行性，为未来能够实现像自然界中流畅高效操作（特别是对动态物体）的系统奠定了基础。"}}
{"id": "2510.10068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10068", "abs": "https://arxiv.org/abs/2510.10068", "authors": ["Pîrvu Mihai-Cristian", "Leordeanu Marius"], "title": "Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning", "comment": null, "summary": "The computer vision domain has greatly benefited from an abundance of data\nacross many modalities to improve on various visual tasks. Recently, there has\nbeen a lot of focus on self-supervised pre-training methods through Masked\nAutoencoders (MAE) \\cite{he2022masked,bachmann2022multimae}, usually used as a\nfirst step before optimizing for a downstream task, such as classification or\nregression. This is very useful as it doesn't require any manually labeled\ndata. In this work, we introduce Probabilistic Hyper-Graphs using Masked\nAutoencoders (PHG-MAE): a novel model that unifies the classical work on neural\ngraphs \\cite{leordeanu2021semi} with the modern approach of masked autoencoders\nunder a common theoretical framework. Through random masking of entire\nmodalities, not just patches, the model samples from the distribution of\nhyper-edges on each forward pass. Additionally, the model adapts the standard\nMAE algorithm by combining pre-training and fine-tuning into a single training\nloop. Moreover, our approach enables the creation of inference-time ensembles\nwhich, through aggregation, boost the final prediction performance and\nconsistency. Lastly, we show that we can apply knowledge distillation on top of\nthe ensembles with little loss in performance, even with models that have fewer\nthan 1M parameters. While our work mostly focuses on outdoor UAV scenes that\ncontain multiple world interpretations and modalities, the same steps can be\nfollowed in other similar domains, such as autonomous driving or indoor\nrobotics. In order to streamline the process of integrating external\npre-trained experts for computer vision multi-modal multi-task learning (MTL)\nscenarios, we developed a data-pipeline software. Using this tool, we have\ncreated and released a fully-automated extension of the Dronescapes dataset.\nAll the technical details, code and reproduction steps are publicly released.", "AI": {"tldr": "该研究引入了PHG-MAE模型，它将神经图与掩码自编码器（MAE）结合，通过随机掩码整个模态、单循环训练、推理时集成以及知识蒸馏，显著提升了多模态计算机视觉任务的性能和一致性，特别是在无人机场景中。", "motivation": "计算机视觉领域受益于大量数据，特别是通过掩码自编码器（MAE）等自监督预训练方法，无需手动标注数据即可改进各种视觉任务。本研究旨在进一步提升多模态视觉任务的性能，并解决复杂场景（如无人机）中的多模态解释问题。", "method": "本研究提出了一种名为概率超图掩码自编码器（PHG-MAE）的新模型，它统一了经典的神经图工作和现代的掩码自编码器方法。该模型通过随机掩码整个模态（而非仅补丁）来从超边分布中采样。它还将预训练和微调结合到单一训练循环中，并支持创建推理时集成以提升预测性能和一致性。此外，还在集成模型之上应用了知识蒸馏。为简化外部预训练专家集成，开发了数据管道软件，并创建并发布了Dronescapes数据集的完全自动化扩展。", "result": "PHG-MAE模型通过聚合推理时集成，显著提升了最终预测的性能和一致性。即使使用参数少于1M的模型，在集成之上应用知识蒸馏也能实现性能损失极小。该方法主要应用于户外无人机场景，但也可推广到自动驾驶或室内机器人等其他类似领域。所有技术细节、代码和复现步骤均已公开。", "conclusion": "PHG-MAE提供了一个新颖的理论框架，将神经图与掩码自编码器结合，通过创新的模态掩码、单循环训练和集成方法，有效提升了多模态计算机视觉任务的性能、一致性和效率，尤其适用于复杂的多模态环境，并且支持知识蒸馏以创建更轻量级的模型。"}}
{"id": "2510.10084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10084", "abs": "https://arxiv.org/abs/2510.10084", "authors": ["Meijun Zhou", "Gang Mei", "Zhengjing Ma", "Nengxiong Xu", "Jianbing Peng"], "title": "Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework", "comment": null, "summary": "Tracking the spatiotemporal evolution of large-scale landslide scars is\ncritical for understanding the evolution mechanisms and failure precursors,\nenabling effective early-warning. However, most existing studies have focused\non single-phase or pre- and post-failure dual-phase landslide identification.\nAlthough these approaches delineate post-failure landslide boundaries, it is\nchallenging to track the spatiotemporal evolution of landslide scars. To\naddress this problem, this study proposes a novel and universal framework for\ntracking the spatiotemporal evolution of large-scale landslide scars using a\nvision foundation model. The key idea behind the proposed framework is to\nreconstruct discrete optical remote sensing images into a continuous video\nsequence. This transformation enables a vision foundation model, which is\ndeveloped for video segmentation, to be used for tracking the evolution of\nlandslide scars. The proposed framework operates within a knowledge-guided,\nauto-propagation, and interactive refinement paradigm to ensure the continuous\nand accurate identification of landslide scars. The proposed framework was\nvalidated through application to two representative cases: the post-failure\nBaige landslide and the active Sela landslide (2017-2025). Results indicate\nthat the proposed framework enables continuous tracking of landslide scars,\ncapturing both failure precursors critical for early warning and post-failure\nevolution essential for assessing secondary hazards and long-term stability.", "AI": {"tldr": "本研究提出了一种新颖且通用的框架，利用视觉基础模型将离散遥感图像重构为连续视频序列，以实现大规模滑坡疤痕时空演化的连续跟踪，从而支持早期预警和灾后评估。", "motivation": "现有研究大多集中于单阶段或灾前灾后双阶段滑坡识别，难以跟踪滑坡疤痕的时空演化。然而，跟踪滑坡疤痕的时空演化对于理解其演化机制、识别失效前兆以及实现有效预警至关重要。", "method": "该框架的核心思想是将离散光学遥感图像重构为连续视频序列。这种转换使得为视频分割开发的视觉基础模型能够用于跟踪滑坡疤痕的演化。该框架在一个知识引导、自动传播和交互式细化的范式下运行，以确保滑坡疤痕的连续和准确识别。", "result": "该框架已在两个代表性案例（灾后的白格滑坡和活跃的色拉滑坡，2017-2025年）中得到验证。结果表明，所提出的框架能够连续跟踪滑坡疤痕，捕获对早期预警至关重要的失效前兆，以及对评估次生灾害和长期稳定性至关重要的灾后演化。", "conclusion": "所提出的框架能够连续跟踪滑坡疤痕的时空演化，为理解滑坡机制、识别失效前兆、实现早期预警以及评估灾后次生灾害和长期稳定性提供了关键信息。"}}
{"id": "2510.10592", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10592", "abs": "https://arxiv.org/abs/2510.10592", "authors": ["Hong Su"], "title": "A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning", "comment": null, "summary": "Existing studies have introduced method-based reasoning and scope extension\nas approaches to enhance Large Language Model (LLM) performance beyond direct\nmatrix mappings. Building on these foundations, this paper summarizes and\nintegrates these ideas into a unified Intuition-Method Layered Model with Scope\nExtension, designed to address indirected (unseen) issues more systematically.\nIn this framework, intuition-based thinking provides rapid first-reaction\nanswers, while method-based thinking decouples questions and solutions into\ntransferable reasoning units. Scope extension is then applied to broaden\napplicability, including vertical (cause analysis), horizontal (parallel and\ngeneralized issues), and for the first time, temporal and spatial extensions,\nwhich expand reasoning across time and contextual dimensions. These extensions\nare organized into systematic knowledge trees that interconnect into a\nknowledge network, thereby increasing adaptability. To quantitatively evaluate\nthis process, we propose the entropy of method extension, which measures the\nindependence and diversity of extensions as an indicator of the system's\ncapacity to solve unseen questions. By logically connecting existing approaches\nwith new extensions and introducing an entropy-based evaluation framework, this\nwork advances toward a more robust and extensible reasoning paradigm for LLMs\nin real-world problem-solving.", "AI": {"tldr": "本文提出了一种统一的“直觉-方法分层模型与范围扩展”（IMLSE），旨在通过结合直觉、方法推理和创新的时间/空间范围扩展，系统性地提升大型语言模型（LLM）解决间接（未见）问题的能力，并引入了基于熵的评估方法。", "motivation": "现有研究已引入基于方法推理和范围扩展来提升LLM性能。本文旨在在此基础上，将这些思想整合为一个统一模型，以更系统地解决LLM在处理间接（未见）问题时的局限性，超越直接矩阵映射的能力。", "method": "该研究提出了“直觉-方法分层模型与范围扩展”（IMLSE）。模型中，直觉提供快速反应答案，方法将问题和解决方案解耦为可迁移的推理单元。范围扩展被应用于拓宽适用性，包括垂直（原因分析）、水平（并行和泛化问题）以及首次引入的时间和空间扩展。这些扩展被组织成知识树，进而互联成知识网络。为了量化评估，提出了一种衡量扩展独立性和多样性的“方法扩展熵”指标。", "result": "研究成果是一个统一的IMLSE框架，它将直觉、方法推理和创新的时间/空间范围扩展整合起来，形成一个系统性的知识网络，以增强LLM解决未见问题的能力。同时，引入了“方法扩展熵”作为一种定量评估系统解决未见问题能力的指标，逻辑地连接了现有方法与新扩展。", "conclusion": "该工作通过逻辑连接现有方法与新扩展，并引入基于熵的评估框架，推动了LLM在实际问题解决中向更鲁棒和可扩展的推理范式发展，显著提升了LLM处理间接和未见问题的能力。"}}
{"id": "2510.10781", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.10781", "abs": "https://arxiv.org/abs/2510.10781", "authors": ["Douglas Hutchings", "Luai Abuelsamen", "Karthik Rajgopal"], "title": "Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis", "comment": "23 pages, 7 figures. Technical report with complete implementation\n  details and open-source code", "summary": "We present a comprehensive two-layer Voronoi coverage control approach for\ncoordinating hybrid aerial-ground robot teams in hazardous material emergency\nresponse scenarios. Traditional Voronoi coverage control methods face three\ncritical limitations in emergency contexts: heterogeneous agent capabilities\nwith vastly different velocities, clustered initial deployment configurations,\nand urgent time constraints requiring rapid response rather than eventual\nconvergence. Our method addresses these challenges through a decoupled\ntwo-layer architecture that separately optimizes aerial and ground robot\npositioning, with aerial agents delivering ground sensors via airdrop to\nhigh-priority locations. We provide detailed implementation of bounded Voronoi\ncell computation, efficient numerical integration techniques for\nimportance-weighted centroids, and robust control strategies that prevent agent\ntrapping. Simulation results demonstrate an 88% reduction in response time,\nachieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds\ncompared to 220 seconds for ground-only deployment. Complete implementation\ncode is available at https://github.com/dHutchings/ME292B.", "AI": {"tldr": "本文提出了一种双层Voronoi覆盖控制方法，用于协调混合式空中-地面机器人团队在危险品应急响应场景中的部署，显著缩短了响应时间。", "motivation": "传统的Voronoi覆盖控制方法在紧急情况下存在三个关键局限性：异构智能体能力差异大（速度不同）、初始部署配置聚集以及紧急时间限制（需要快速响应而非最终收敛）。", "method": "该方法采用解耦的双层架构，分别优化空中和地面机器人的定位。空中机器人通过空投将地面传感器投放到高优先级区域。技术细节包括有界Voronoi单元计算、重要性加权质心的有效数值积分技术以及防止智能体陷阱的鲁棒控制策略。", "result": "仿真结果表明，与仅地面部署的220秒相比，该方法将响应时间缩短了88%，在25秒内实现了目标传感器覆盖（初始传感器损失为18.5%）。", "conclusion": "所提出的双层Voronoi覆盖控制方法能有效应对危险品应急响应场景中的挑战，通过混合式空中-地面机器人团队显著提高了传感器覆盖效率和响应速度。"}}
{"id": "2510.10637", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10637", "abs": "https://arxiv.org/abs/2510.10637", "authors": ["Haoyu Zhao", "Cheng Zeng", "Linghao Zhuang", "Yaxi Zhao", "Shengke Xue", "Hao Wang", "Xingyue Zhao", "Zhongyu Li", "Kehan Li", "Siteng Huang", "Mingxiu Chen", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting", "comment": "13 pages, 6 figures", "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.", "AI": {"tldr": "RoboSimGS是一个Real2Sim2Real框架，它利用3D高斯泼溅和多模态大语言模型，将真实世界图像转换为高保真、物理交互的模拟环境，从而实现成功的机器人学习策略从模拟到真实的零样本迁移。", "motivation": "机器人学习的扩展性受限于真实世界数据收集的高成本和劳力。虽然模拟数据可扩展，但由于视觉外观、物理属性和物体交互方面的显著差异，其往往难以泛化到真实世界。", "method": "RoboSimGS框架将多视角真实世界图像转换为模拟环境。它采用混合表示：使用3D高斯泼溅（3DGS）捕捉环境的真实外观，同时为交互对象使用网格基元以确保准确的物理模拟。关键创新是利用多模态大语言模型（MLLM）自动化创建物理上合理、可动关节的资产，MLLM通过分析视觉数据推断物体的物理属性（如密度、刚度）和复杂的运动学结构（如铰链、滑轨）。", "result": "完全在RoboSimGS生成的数据上训练的策略，在多样化的真实世界操作任务中实现了成功的零样本模拟到真实迁移。此外，来自RoboSimGS的数据显著提升了SOTA方法的性能和泛化能力。", "conclusion": "RoboSimGS是一个强大且可扩展的解决方案，能够有效弥合模拟与真实之间的鸿沟。"}}
{"id": "2510.10633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10633", "abs": "https://arxiv.org/abs/2510.10633", "authors": ["Jiabao Shi", "Minfeng Qi", "Lefeng Zhang", "Di Wang", "Yingjie Zhao", "Ziying Li", "Yalong Xing", "Ningran Li"], "title": "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion", "comment": "16 pages, 13 figures", "summary": "Multimodal text-to-image generation remains constrained by the difficulty of\nmaintaining semantic alignment and professional-level detail across diverse\nvisual domains. We propose a multi-agent reinforcement learning framework that\ncoordinates domain-specialized agents (e.g., focused on architecture,\nportraiture, and landscape imagery) within two coupled subsystems: a text\nenhancement module and an image generation module, each augmented with\nmultimodal integration components. Agents are trained using Proximal Policy\nOptimization (PPO) under a composite reward function that balances semantic\nsimilarity, linguistic visual quality, and content diversity. Cross-modal\nalignment is enforced through contrastive learning, bidirectional attention,\nand iterative feedback between text and image. Across six experimental\nsettings, our system significantly enriches generated content (word count\nincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion\nmethods, Transformer-based strategies achieve the highest composite score\n(0.521), despite occasional stability issues. Multimodal ensembles yield\nmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistent\nchallenges of cross-modal semantic grounding. These findings underscore the\npromise of collaborative, specialization-driven architectures for advancing\nreliable multimodal generative systems.", "AI": {"tldr": "本文提出了一种多智能体强化学习框架，通过领域专业化智能体的协作，解决了多模态文本到图像生成中语义对齐和细节维护的难题，显著提升了生成内容的丰富度。", "motivation": "当前多模态文本到图像生成面临的挑战是难以在多样化的视觉领域中保持语义对齐和专业级别的细节。", "method": "该研究提出了一个多智能体强化学习框架，协调领域专业化智能体（如专注于建筑、肖像和风景图像的智能体）。该框架包含文本增强模块和图像生成模块两个耦合子系统，并均增强了多模态集成组件。智能体使用PPO算法进行训练，其复合奖励函数平衡了语义相似性、语言视觉质量和内容多样性。通过对比学习、双向注意力和文本与图像之间的迭代反馈来强制执行跨模态对齐。", "result": "在六种实验设置下，该系统显著丰富了生成内容（词汇量增加了1614%），同时将ROUGE-1分数降低了69.7%。在融合方法中，基于Transformer的策略实现了最高的综合分数（0.521），尽管偶尔存在稳定性问题。多模态集成显示出中等的一致性（范围从0.444到0.481），反映了跨模态语义基础的持续挑战。", "conclusion": "这些发现强调了协作的、专业化驱动的架构在推进可靠的多模态生成系统方面的潜力。"}}
{"id": "2510.10063", "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 68T27", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.10063", "abs": "https://arxiv.org/abs/2510.10063", "authors": ["Yibo Yang"], "title": "CLMN: Concept based Language Models via Neural Symbolic Reasoning", "comment": "7 pages, 2 figures", "summary": "Deep learning has advanced NLP, but interpretability remains limited,\nespecially in healthcare and finance. Concept bottleneck models tie predictions\nto human concepts in vision, but NLP versions either use binary activations\nthat harm text representations or latent concepts that weaken semantics, and\nthey rarely model dynamic concept interactions such as negation and context. We\nintroduce the Concept Language Model Network (CLMN), a neural-symbolic\nframework that keeps both performance and interpretability. CLMN represents\nconcepts as continuous, human-readable embeddings and applies fuzzy-logic\nreasoning to learn adaptive interaction rules that state how concepts affect\neach other and the final decision. The model augments original text features\nwith concept-aware representations and automatically induces interpretable\nlogic rules. Across multiple datasets and pre-trained language models, CLMN\nachieves higher accuracy than existing concept-based methods while improving\nexplanation quality. These results show that integrating neural representations\nwith symbolic reasoning in a unified concept space can yield practical,\ntransparent NLP systems.", "AI": {"tldr": "本文提出了概念语言模型网络（CLMN），一个神经符号框架，它通过连续、人类可读的概念嵌入和模糊逻辑推理来学习自适应概念交互规则，从而在保持高性能的同时显著提升NLP模型的可解释性。", "motivation": "深度学习在自然语言处理（NLP）中取得了显著进展，但其可解释性仍然有限，尤其在医疗和金融等关键领域。现有的NLP概念瓶颈模型存在局限性，如二元激活损害文本表示或潜在概念削弱语义，并且很少能有效建模动态概念交互（如否定和上下文）。", "method": "引入了概念语言模型网络（CLMN）。该模型将概念表示为连续、人类可读的嵌入，并应用模糊逻辑推理来学习自适应的交互规则，这些规则描述了概念如何相互影响以及对最终决策的影响。CLMN使用概念感知表示增强原始文本特征，并自动推导可解释的逻辑规则。", "result": "在多个数据集和预训练语言模型上，CLMN与现有基于概念的方法相比，实现了更高的准确性，同时显著提高了模型解释的质量。", "conclusion": "将神经表示与符号推理在统一的概念空间中进行整合，可以有效地构建实用且透明的NLP系统。"}}
{"id": "2510.10097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10097", "abs": "https://arxiv.org/abs/2510.10097", "authors": ["Jiahui Lu", "Haihong Xiao", "Xueyan Zhao", "Wenxiong Kang"], "title": "Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting", "comment": null, "summary": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced\n3D reconstruction and novel view synthesis, but remain heavily dependent on\naccurate camera poses and dense viewpoint coverage. These requirements limit\ntheir applicability in sparse-view settings, where pose estimation becomes\nunreliable and supervision is insufficient. To overcome these challenges, we\nintroduce Gesplat, a 3DGS-based framework that enables robust novel view\nsynthesis and geometrically consistent reconstruction from unposed sparse\nimages. Unlike prior works that rely on COLMAP for sparse point cloud\ninitialization, we leverage the VGGT foundation model to obtain more reliable\ninitial poses and dense point clouds. Our approach integrates several key\ninnovations: 1) a hybrid Gaussian representation with dual position-shape\noptimization enhanced by inter-view matching consistency; 2) a graph-guided\nattribute refinement module to enhance scene details; and 3) flow-based depth\nregularization that improves depth estimation accuracy for more effective\nsupervision. Comprehensive quantitative and qualitative experiments demonstrate\nthat our approach achieves more robust performance on both forward-facing and\nlarge-scale complex datasets compared to other pose-free methods.", "AI": {"tldr": "Gesplat是一种基于3DGS的框架，通过利用VGGT模型获取可靠的初始姿态和点云，并结合混合高斯表示、图引导属性细化和基于流的深度正则化，实现了从无姿态稀疏图像进行鲁棒的新视图合成和几何一致性重建。", "motivation": "NeRF和3DGS在3D重建和新视图合成方面表现出色，但严重依赖于准确的相机姿态和密集的视角覆盖。这些要求限制了它们在稀疏视图设置中的适用性，因为在这种情况下姿态估计变得不可靠且监督不足。", "method": "该方法引入了Gesplat框架，其关键创新包括：1) 利用VGGT基础模型而非COLMAP获取更可靠的初始姿态和密集点云；2) 采用混合高斯表示，通过双重位置-形状优化并增强视图间匹配一致性；3) 一个图引导的属性细化模块以增强场景细节；4) 基于流的深度正则化以提高深度估计精度，从而实现更有效的监督。", "result": "全面的定量和定性实验表明，与其他无姿态方法相比，该方法在正向场景和大规模复杂数据集上都取得了更鲁棒的性能。", "conclusion": "Gesplat通过整合多项创新，成功克服了从无姿态稀疏图像进行新视图合成和几何一致性重建的挑战，展现出强大的鲁棒性。"}}
{"id": "2510.10100", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10100", "abs": "https://arxiv.org/abs/2510.10100", "authors": ["Kuangpu Guo", "Lijun Sheng", "Yongcan Yu", "Jian Liang", "Zilei Wang", "Ran He"], "title": "Cooperative Pseudo Labeling for Unsupervised Federated Classification", "comment": "Accepted by ICCV 2025", "summary": "Unsupervised Federated Learning (UFL) aims to collaboratively train a global\nmodel across distributed clients without sharing data or accessing label\ninformation. Previous UFL works have predominantly focused on representation\nlearning and clustering tasks. Recently, vision language models (e.g., CLIP)\nhave gained significant attention for their powerful zero-shot prediction\ncapabilities. Leveraging this advancement, classification problems that were\npreviously infeasible under the UFL paradigm now present promising new\nopportunities, yet remain largely unexplored. In this paper, we extend UFL to\nthe classification problem with CLIP for the first time and propose a novel\nmethod, \\underline{\\textbf{Fed}}erated \\underline{\\textbf{Co}}operative\n\\underline{\\textbf{P}}seudo \\underline{\\textbf{L}}abeling (\\textbf{FedCoPL}).\nSpecifically, clients estimate and upload their pseudo label distribution, and\nthe server adjusts and redistributes them to avoid global imbalance among\nclasses. Moreover, we introduce a partial prompt aggregation protocol for\neffective collaboration and personalization. In particular, visual prompts\ncontaining general image features are aggregated at the server, while text\nprompts encoding personalized knowledge are retained locally. Extensive\nexperiments demonstrate the superior performance of our FedCoPL compared to\nbaseline methods. Our code is available at\n\\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.", "AI": {"tldr": "本文首次将无监督联邦学习（UFL）扩展到基于CLIP的分类问题，并提出了一种名为FedCoPL的新方法，通过协作伪标签和部分提示聚合实现了优异的性能。", "motivation": "以往的无监督联邦学习（UFL）主要集中于表示学习和聚类任务。随着像CLIP这样强大的视觉语言模型出现，其零样本预测能力为UFL范式下的分类问题带来了新的机遇，但这一领域尚未被充分探索。", "method": "本文提出了FedCoPL方法。具体而言，客户端估计并上传其伪标签分布，服务器对这些分布进行调整和重新分配以避免全局类别不平衡。此外，引入了部分提示聚合协议，其中包含通用图像特征的视觉提示在服务器端聚合，而编码个性化知识的文本提示则保留在本地。", "result": "广泛的实验证明，FedCoPL相比基线方法展现出卓越的性能。", "conclusion": "FedCoPL成功地将无监督联邦学习应用于基于CLIP的分类问题，通过协作伪标签和部分提示聚合实现了有效的联邦协作和个性化，并取得了优于现有方法的成果。"}}
{"id": "2510.10072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10072", "abs": "https://arxiv.org/abs/2510.10072", "authors": ["Hua Cai", "Shuang Zhao", "Liang Zhang", "Xuli Shen", "Qing Xu", "Weilin Shen", "Zihao Wen", "Tianke Ban"], "title": "Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference", "comment": null, "summary": "Reasoning-focused large language models (LLMs) are rapidly evolving across\nvarious domains, yet their capabilities in handling complex legal problems\nremains underexplored. In this paper, we introduce Unilaw-R1, a large language\nmodel tailored for legal reasoning. With a lightweight 7-billion parameter\nscale, Unilaw-R1 significantly reduces deployment cost while effectively\ntackling three core challenges in the legal domain: insufficient legal\nknowledge, unreliable reasoning logic, and weak business generalization. To\naddress these issues, we first construct Unilaw-R1-Data, a high-quality dataset\ncontaining 17K distilled and screened chain-of-thought (CoT) samples. Based on\nthis, we adopt a two-stage training strategy combining Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL), which significantly boosts the\nperformance on complex legal reasoning tasks and supports interpretable\ndecision-making in legal AI applications. To assess legal reasoning ability, we\nalso introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate\nmodels across single- and multi-choice legal tasks. Unilaw-R1 demonstrates\nstrong results on authoritative benchmarks, outperforming all models of similar\nscale and achieving performance on par with the much larger\nDeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it\nalso showed significant gains on LawBench and LexEval, exceeding\nQwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.", "AI": {"tldr": "本文介绍了Unilaw-R1，一个轻量级（70亿参数）的法律推理大语言模型，通过高质量数据集和两阶段训练策略，解决了法律领域知识不足、推理逻辑不可靠和业务泛化能力弱等挑战，并在法律推理任务上取得了显著优于同规模模型的性能，且接近大型模型的表现。", "motivation": "尽管推理型大语言模型在各领域快速发展，但其在处理复杂法律问题方面的能力尚未得到充分探索。现有模型在法律领域面临法律知识不足、推理逻辑不可靠以及业务泛化能力弱等核心挑战。", "method": "研究引入了Unilaw-R1，一个70亿参数的法律推理大语言模型。首先构建了Unilaw-R1-Data，一个包含1.7万个精炼筛选的思维链（CoT）样本的高质量数据集。在此基础上，采用了结合监督微调（SFT）和强化学习（RL）的两阶段训练策略。此外，还引入了Unilaw-R1-Eval，一个专门用于评估模型在单选和多选法律任务中法律推理能力的基准。", "result": "Unilaw-R1在权威基准测试中表现出色，超越了所有同等规模的模型，并达到了与更大规模的DeepSeek-R1-Distill-Qwen-32B（54.9%）相当的性能。经过领域特定训练后，它在LawBench和LexEval上也取得了显著提升，平均超越Qwen-2.5-7B-Instruct（46.6%）6.6%。", "conclusion": "Unilaw-R1是一个为法律推理量身定制的有效大语言模型，它以较低的部署成本解决了法律领域的关键挑战，并在复杂法律推理任务上展示了强大的性能，支持法律AI应用中的可解释决策。"}}
{"id": "2510.10642", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10642", "abs": "https://arxiv.org/abs/2510.10642", "authors": ["Jianke Zhang", "Yucheng Hu", "Yanjiang Guo", "Xiaoyu Chen", "Yichen Liu", "Wenna Chen", "Chaochao Lu", "Jianyu Chen"], "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning", "comment": null, "summary": "Building generalist robot policies that can handle diverse tasks in\nopen-ended environments is a central challenge in robotics. To leverage\nknowledge from large-scale pretraining, prior work has typically built\ngeneralist policies either on top of vision-language understanding models\n(VLMs) or generative models. However, both semantic understanding from\nvision-language pretraining and visual dynamics modeling from visual-generation\npretraining are crucial for embodied robots. Recent unified models of\ngeneration and understanding have demonstrated strong capabilities in both\ncomprehension and generation through large-scale pretraining. We posit that\nrobotic policy learning can likewise benefit from the combined strengths of\nunderstanding, planning and continuous future representation learning. Building\non this insight, we introduce UniCoD, which acquires the ability to dynamically\nmodel high-dimensional visual features through pretraining on over 1M\ninternet-scale instructional manipulation videos. Subsequently, UniCoD is\nfine-tuned on data collected from the robot embodiment, enabling the learning\nof mappings from predictive representations to action tokens. Extensive\nexperiments show our approach consistently outperforms baseline methods in\nterms of 9\\% and 12\\% across simulation environments and real-world\nout-of-distribution tasks.", "AI": {"tldr": "本文提出UniCoD，一种利用统一生成和理解模型预训练（在百万级互联网教学视频上）和微调（在机器人数据上）的通用机器人策略，在模拟和真实世界任务中显著优于基线方法。", "motivation": "构建能处理多样化任务的通用机器人策略是一个核心挑战。现有方法通常基于视觉-语言理解模型（VLM）或生成模型，但具身机器人需要语义理解和视觉动态建模。受统一生成和理解模型在理解和生成方面强大能力的启发，本文认为机器人策略学习也能从理解、规划和连续未来表示学习的结合中受益。", "method": "本文引入UniCoD模型。首先，UniCoD通过在超过100万个互联网规模的教学操作视频上进行预训练，获得动态建模高维视觉特征的能力。随后，UniCoD在从机器人实体收集的数据上进行微调，以学习从预测表示到动作令牌的映射。", "result": "广泛的实验表明，UniCoD方法在模拟环境中和真实世界的分布外（out-of-distribution）任务中，分别比基线方法提高了9%和12%的性能。", "conclusion": "UniCoD通过结合统一生成和理解模型的优势，利用大规模预训练和机器人数据微调，显著提升了通用机器人策略在多样化任务中的表现，证明了其在具身智能领域的潜力。"}}
{"id": "2510.10640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10640", "abs": "https://arxiv.org/abs/2510.10640", "authors": ["Piyush Pant", "Marcellius William Suntoro", "Ayesha Siddiqua", "Muhammad Shehryaar Sharif", "Daniyal Ahmed"], "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany", "comment": "7 pages. Application:\n  https://equity-aware-geospatial-ai-project.streamlit.app/ Codebase:\n  https://github.com/mwsyow/equity-aware-geospatial-ai-project/", "summary": "This paper presents EA-GeoAI, an integrated framework for demand forecasting\nand equitable hospital planning in Germany through 2030. We combine\ndistrict-level demographic shifts, aging population density, and infrastructure\nbalances into a unified Equity Index. An interpretable Agentic AI optimizer\nthen allocates beds and identifies new facility sites to minimize unmet need\nunder budget and travel-time constraints. This approach bridges GeoAI,\nlong-term forecasting, and equity measurement to deliver actionable\nrecommendations for policymakers.", "AI": {"tldr": "本文提出了EA-GeoAI框架，用于德国2030年之前的需求预测和公平医院规划。", "motivation": "解决德国未来医院需求预测和公平规划问题，以应对人口结构变化、老龄化及基础设施平衡，并最小化未满足的需求，同时考虑预算和出行时间限制。", "method": "结合地区层面的人口结构变化、老龄人口密度和基础设施平衡，构建统一的公平指数。随后，一个可解释的智能AI优化器在预算和出行时间限制下，分配床位并识别新的设施选址，以最小化未满足的需求。", "result": "该框架为政策制定者提供了可操作的建议，以实现公平的医院规划。", "conclusion": "EA-GeoAI方法整合了地理人工智能（GeoAI）、长期预测和公平性衡量，为政策制定者提供了实用的规划建议。"}}
{"id": "2510.10639", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10639", "abs": "https://arxiv.org/abs/2510.10639", "authors": ["Haemin Choi", "Gayathri Nadarajan"], "title": "Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction", "comment": null, "summary": "Although student learning satisfaction has been widely studied, modern\ntechniques such as interpretable machine learning and neural networks have not\nbeen sufficiently explored. This study demonstrates that a recent model that\ncombines boosting with interpretability, automatic piecewise linear\nregression(APLR), offers the best fit for predicting learning satisfaction\namong several state-of-the-art approaches. Through the analysis of APLR's\nnumerical and visual interpretations, students' time management and\nconcentration abilities, perceived helpfulness to classmates, and participation\nin offline courses have the most significant positive impact on learning\nsatisfaction. Surprisingly, involvement in creative activities did not\npositively affect learning satisfaction. Moreover, the contributing factors can\nbe interpreted on an individual level, allowing educators to customize\ninstructions according to student profiles.", "AI": {"tldr": "本研究发现自动分段线性回归（APLR）模型最适合预测学生学习满意度，并揭示了时间管理、专注力等关键影响因素，为个性化教学提供了可解释的见解。", "motivation": "尽管学生学习满意度已被广泛研究，但现代技术如可解释机器学习和神经网络尚未在此领域得到充分探索。", "method": "本研究采用了结合提升算法和可解释性的自动分段线性回归（APLR）模型，并将其与多种先进方法进行比较。通过APLR的数值和视觉解释来分析影响因素。", "result": "APLR模型在预测学习满意度方面表现最佳。学生的“时间管理能力”、“专注力”、“感知对同学的帮助”以及“参与线下课程”对学习满意度有最显著的积极影响。令人惊讶的是，参与创意活动并未对学习满意度产生积极影响。此外，贡献因素可在个体层面进行解释。", "conclusion": "APLR模型为预测学习满意度提供了最佳拟合和可解释性，其结果能够帮助教育工作者根据学生的个人特点定制教学方案。"}}
{"id": "2510.10716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10716", "abs": "https://arxiv.org/abs/2510.10716", "authors": ["Christopher Thierauf"], "title": "Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy", "comment": null, "summary": "A new AUV mission planning and execution software has been tested on AUV\nSentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and\nAUV control systems to replace the legacy MC architecture. Unlike these\nexisting architectures, however, DINOS-R is built from the ground-up to unify\nsymbolic decision making (for understandable, repeatable, provable behavior)\nwith machine learning techniques and reactive behaviors, for field-readiness\nacross oceanographic platforms. Implemented primarily in Python3, DINOS-R is\nextensible, modular, and reusable, with an emphasis on non-expert use as well\nas growth for future research in oceanography and robot algorithms. Mission\nspecification is flexible, and can be specified declaratively. Behavior\nspecification is similarly flexible, supporting simultaneous use of real-time\ntask planning and hard-coded user specified plans. These features were\ndemonstrated in the field on Sentry, in addition to a variety of simulated\ncases. These results are discussed, and future work is outlined.", "AI": {"tldr": "本文介绍了一种名为DINOS-R的新型AUV任务规划与执行软件，它在AUV Sentry上进行了测试，旨在统一符号决策、机器学习和反应行为，以提高海洋科考平台的现场适用性。", "motivation": "研究动机是取代现有的MC架构，并从头开始构建一个能够统一符号决策（为了可理解、可重复、可验证的行为）与机器学习技术和反应行为的系统，以实现AUV在海洋科考平台上的现场就绪能力，并强调非专家用户的使用以及未来研究的扩展性。", "method": "开发了DINOS-R软件，主要使用Python3实现，灵感来源于认知架构和AUV控制系统。该系统具有可扩展、模块化和可重用性，支持声明式任务规范和灵活的行为规范（包括实时任务规划和硬编码用户指定计划的同步使用）。通过在AUV Sentry上的实地测试以及各种模拟案例进行了功能验证。", "result": "DINOS-R的各项功能，包括灵活的任务和行为规范，以及符号决策、机器学习和反应行为的统一，已在Sentry AUV上进行了现场演示，并在多种模拟案例中得到了验证。", "conclusion": "DINOS-R系统在AUV任务规划和执行方面表现出有效性，其统一多种决策范式的方法被证明可行，为未来的海洋学和机器人算法研究奠定了基础。"}}
{"id": "2510.11072", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11072", "abs": "https://arxiv.org/abs/2510.11072", "authors": ["Huayi Wang", "Wentao Zhang", "Runyi Yu", "Tao Huang", "Junli Ren", "Feiyu Jia", "Zirui Wang", "Xiaojie Niu", "Xiao Chen", "Jiahe Chen", "Qifeng Chen", "Jingbo Wang", "Jiangmiao Pang"], "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System", "comment": "Project website: https://why618188.github.io/physhsi/", "summary": "Deploying humanoid robots to interact with real-world environments--such as\ncarrying objects or sitting on chairs--requires generalizable, lifelike motions\nand robust scene perception. Although prior approaches have advanced each\ncapability individually, combining them in a unified system is still an ongoing\nchallenge. In this work, we present a physical-world humanoid-scene interaction\nsystem, PhysHSI, that enables humanoids to autonomously perform diverse\ninteraction tasks while maintaining natural and lifelike behaviors. PhysHSI\ncomprises a simulation training pipeline and a real-world deployment system. In\nsimulation, we adopt adversarial motion prior-based policy learning to imitate\nnatural humanoid-scene interaction data across diverse scenarios, achieving\nboth generalization and lifelike behaviors. For real-world deployment, we\nintroduce a coarse-to-fine object localization module that combines LiDAR and\ncamera inputs to provide continuous and robust scene perception. We validate\nPhysHSI on four representative interactive tasks--box carrying, sitting, lying,\nand standing up--in both simulation and real-world settings, demonstrating\nconsistently high success rates, strong generalization across diverse task\ngoals, and natural motion patterns.", "AI": {"tldr": "PhysHSI是一个物理世界中人形机器人与场景交互的系统，它使机器人能够自主执行多样化交互任务，同时保持自然、逼真的动作和鲁棒的场景感知。", "motivation": "将人形机器人部署到真实世界环境中（如搬运物体、坐下）需要可泛化的、逼真的动作和鲁棒的场景感知。尽管现有方法在单个能力上有所进展，但将其整合到一个统一系统中仍是一个挑战。", "method": "本文提出了PhysHSI系统，包含一个仿真训练管道和一个真实世界部署系统。在仿真中，采用基于对抗性运动先验的策略学习来模仿多样化场景中的自然人机交互数据，以实现泛化和逼真行为。在真实世界部署中，引入了一个结合激光雷达和摄像头输入的粗到细物体定位模块，以提供连续和鲁棒的场景感知。", "result": "PhysHSI在四项代表性交互任务（搬箱、坐下、躺下、站起）上进行了仿真和真实世界验证，结果显示出持续高成功率、对多样化任务目标的强大泛化能力以及自然的运动模式。", "conclusion": "PhysHSI成功地实现了人形机器人在真实世界中自主执行多样化交互任务，同时保持自然、逼真的动作和鲁棒的场景感知。"}}
{"id": "2510.10077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10077", "abs": "https://arxiv.org/abs/2510.10077", "authors": ["Wenqing Wang", "Muhammad Asif Ali", "Ali Shoker", "Ruohan Yang", "Junyang Chen", "Ying Sha", "Huan Wang"], "title": "A-IPO: Adaptive Intent-driven Preference Optimization", "comment": null, "summary": "Human preferences are diverse and dynamic, shaped by regional, cultural, and\nsocial factors. Existing alignment methods like Direct Preference Optimization\n(DPO) and its variants often default to majority views, overlooking minority\nopinions and failing to capture latent user intentions in prompts.\n  To address these limitations, we introduce \\underline{\\textbf{A}}daptive\n\\textbf{\\underline{I}}ntent-driven \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization (\\textbf{A-IPO}). Specifically,A-IPO\nintroduces an intention module that infers the latent intent behind each user\nprompt and explicitly incorporates this inferred intent into the reward\nfunction, encouraging stronger alignment between the preferred model's\nresponses and the user's underlying intentions. We demonstrate, both\ntheoretically and empirically, that incorporating an intention--response\nsimilarity term increases the preference margin (by a positive shift of\n$\\lambda\\,\\Delta\\mathrm{sim}$ in the log-odds), resulting in clearer separation\nbetween preferred and dispreferred responses compared to DPO.\n  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along\nwith an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess\nreal-world and adversarial preference alignment.\n  Through explicit modeling of diverse user intents,A-IPO facilitates\npluralistic preference optimization while simultaneously enhancing adversarial\nrobustness in preference alignment. Comprehensive empirical evaluation\ndemonstrates that A-IPO consistently surpasses existing baselines, yielding\nsubstantial improvements across key metrics: up to +24.8 win-rate and +45.6\nResponse-Intention Consistency on Real-pref; up to +38.6 Response Similarity\nand +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention\nConsistency Score on GlobalOpinionQA-Ext.", "AI": {"tldr": "本文提出A-IPO（自适应意图驱动偏好优化）方法，通过引入意图模块来推断用户潜在意图并将其融入奖励函数，从而实现多元化和鲁棒的偏好对齐，克服了现有DPO方法忽视少数意见和潜在意图的局限性。", "motivation": "现有对齐方法（如DPO及其变体）倾向于多数意见，忽视了少数观点，也未能捕捉到提示中潜在的用户意图，导致无法充分适应区域、文化和社会因素塑造的、多样化和动态的人类偏好。", "method": "A-IPO方法引入了一个意图模块，用于推断每个用户提示背后的潜在意图，并明确地将推断出的意图纳入奖励函数中，以鼓励模型响应与用户潜在意图更强地对齐。理论和实证均证明，引入意图-响应相似性项可增加偏好边际。此外，论文还引入了两个新基准Real-pref和Attack-pref，并扩展了现有数据集GlobalOpinionQA-Ext用于评估。", "result": "A-IPO通过意图-响应相似性项增加了偏好边际（在对数几率中正向偏移λΔsim），实现了偏好和非偏好响应之间更清晰的分离。在Real-pref上，胜率提升高达24.8%，响应-意图一致性提升高达45.6%；在Attack-pref上，响应相似性提升高达38.6%，防御成功率提升高达52.2%；在GlobalOpinionQA-Ext上，意图一致性得分提升高达54.6%。这些结果表明A-IPO持续超越现有基线。", "conclusion": "通过显式建模多样化的用户意图，A-IPO促进了多元化的偏好优化，同时增强了偏好对齐的对抗鲁棒性。全面的实证评估证明了A-IPO在各项关键指标上均显著优于现有基线。"}}
{"id": "2510.11448", "categories": ["cs.RO", "cs.SY", "eess.SY", "C.3; D.4.1; D.4.4; D.4.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.11448", "abs": "https://arxiv.org/abs/2510.11448", "authors": ["Yuankai He", "Hanlin Chen", "Weisong Shi"], "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems", "comment": "8 pages,7 figures, 8 tables", "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m).", "AI": {"tldr": "高速度自动驾驶车辆需要低延迟的感知到执行循环。ROS 2中间件会引入显著开销。本文提出Sensor-in-Memory (SIM)，一种为宿主内管道设计的共享内存传输机制，能大幅降低数据传输延迟，提高自动驾驶系统的性能和安全性。", "motivation": "高速度自动驾驶车辆需要快速的控制循环以及从感知到执行的严格限定延迟。现有的开源自动驾驶系统（如基于ROS 2）在多传感器和控制节点共享计算单元时，ROS 2及其DDS传输机制会引入大量的序列化、复制和发现开销，从而压缩可用时间预算。", "method": "本文提出Sensor-in-Memory (SIM)，一种专为自动驾驶车辆内部宿主管道设计的共享内存传输机制。SIM将传感器数据保留在原生内存布局中（如cv::Mat, PCL），使用无锁有界双缓冲区（优先覆盖旧数据以保证新鲜度），并能通过四行代码集成到ROS 2节点中。SIM与ROS 2并行运行，针对数据新鲜度和最小延迟而非完整性进行优化。它提供序列号、写入器心跳和可选校验和以确保数据排序、活跃性和基本完整性。", "result": "在NVIDIA Jetson Orin Nano上，SIM与ROS 2零拷贝传输（如FastRTPS和Zenoh）相比，数据传输延迟降低了高达98%，平均延迟降低了约95%，95th/99th百分位尾部延迟缩窄了约96%。在运行Autoware.Universe的L4级量产车辆测试中，SIM将定位频率从7.5 Hz提高到9.5 Hz。应用于所有延迟关键模块后，SIM将平均感知到决策延迟从521.91毫秒缩短到290.26毫秒，在40英里/小时（64公里/小时）的干燥混凝土路面上，紧急制动距离减少了13.6英尺（4.14米）。", "conclusion": "SIM通过显著降低自动驾驶车辆感知到决策管道的延迟，从而实现更快的控制循环和更短的制动距离，极大地提升了高速度自动驾驶车辆的性能和安全性。"}}
{"id": "2510.11491", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11491", "abs": "https://arxiv.org/abs/2510.11491", "authors": ["Murad Dawood", "Usama Ahmed Siddiquie", "Shahram Khorshidi", "Maren Bennewitz"], "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling", "comment": null, "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods.", "AI": {"tldr": "本文提出了一种模块化的成本感知调节器，通过平滑调节动作而非直接覆盖策略来提高安全强化学习的性能，显著减少了约束违反并提升了任务回报。", "motivation": "现有的安全强化学习方法存在问题：单一策略优化奖励和安全目标时可能因冲突导致不稳定；外部安全过滤器则需要先验系统知识并可能直接覆盖动作。这些都限制了探索和性能。", "method": "本文提出了一种模块化的成本感知调节器。该调节器根据预测的约束违反来调整（缩放）智能体的动作，通过平滑调制动作来保留探索，而不是直接覆盖策略。调节器经过训练，旨在最小化约束违反，同时避免过度抑制动作。它可以无缝集成到SAC和TD3等离策略RL方法中。", "result": "该方法在Safety Gym运动任务中，针对稀疏成本实现了最先进的收益-成本比。与现有方法相比，它将约束违反减少了多达126倍，同时将回报提高了超过一个数量级。", "conclusion": "所提出的模块化成本感知调节器在安全强化学习中表现出色，能够有效减少约束违反并提高任务性能，为解决现有安全RL方法的局限性提供了一种鲁棒且高效的解决方案。"}}
{"id": "2510.10104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10104", "abs": "https://arxiv.org/abs/2510.10104", "authors": ["Minbin Huang", "Runhui Huang", "Chuanyang Zheng", "Jingyao Li", "Guoxuan Chen", "Han Shi", "Hong Cheng"], "title": "Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated that\nreinforcement learning with verifiable rewards (RLVR) can significantly enhance\nreasoning abilities by directly optimizing correctness, rather than relying\nsolely on supervised imitation. This paradigm has been extended to multimodal\nLLMs for complex video and image understanding tasks. However, while\noutcome-driven RL improves answer accuracy, it can inadvertently decouple the\nreasoning chain from the final answer, leading to situations where models\nproduce inconsistency between the reasoning trace and final answer. In our\nexperiments on multiple-choice visual question-answering tasks, the standard\nGRPO method yields only 79.7\\% consistency on MMVU between the reasoning steps\nand the chosen answers, indicating frequent mismatches between answers and\nreasoning. To this end, we propose Answer-Consistent Reinforcement Learning\n(ACRE) that modifies the GRPO algorithm with an auxiliary consistency check.\nAfter the model generates a chain of thought and an initial answer for a given\nquestion, we shuffle the answer options and prompt the model again with the\nsame reasoning trace to predict a second answer. We design a\nconsistency-verification reward that grants a high reward only if both the\noriginal and the post-shuffle answers agree and are correct; otherwise, a lower\nreward is assigned accordingly. This mechanism penalizes reasoning-answer\nmisalignment and discourages the model from relying on spurious patterns, such\nas option ordering biases. We evaluate ACRE on challenging Video Reasoning\nbenchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\\%\nand 1.5\\% improvement for Video Reasoning and Math Reasoning tasks over the\nGRPO baseline.", "AI": {"tldr": "该研究提出了一种名为ACRE的答案一致性强化学习方法，通过引入辅助一致性检查和一致性验证奖励来解决多模态大语言模型（LLMs）在RLVR中推理链与最终答案不一致的问题，并在视频和数学推理任务上取得了性能提升。", "motivation": "尽管基于结果的强化学习（RLVR）能提高LLMs的答案准确性，但它可能无意中导致推理链与最终答案脱钩，产生不一致。例如，在MMVU多项选择视觉问答任务中，标准GRPO方法在推理步骤和答案之间的一致性仅为79.7%。", "method": "本文提出了答案一致性强化学习（ACRE）方法，它修改了GRPO算法并引入了一个辅助一致性检查。具体来说，在模型生成思维链和初步答案后，会打乱答案选项并用相同的推理轨迹再次提示模型预测第二个答案。设计了一个一致性验证奖励机制：只有当原始答案和打乱后的答案一致且都正确时才给予高奖励；否则，分配较低的奖励。这惩罚了推理-答案不一致，并阻止模型依赖虚假模式（如选项顺序偏差）。", "result": "ACRE在挑战性的视频推理基准和多模态数学推理基准上进行了评估，与GRPO基线相比，在视频推理任务上平均提高了2.2%，在数学推理任务上平均提高了1.5%。", "conclusion": "ACRE通过解决推理链与最终答案之间的不一致问题，显著提升了多模态大语言模型在复杂视频和图像理解任务上的表现，证明了其在提高模型可靠性和性能方面的有效性。"}}
{"id": "2510.10731", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10731", "abs": "https://arxiv.org/abs/2510.10731", "authors": ["Yongxi Cao", "Julian F. Schumann", "Jens Kober", "Joni Pajarinen", "Arkady Zgonnikov"], "title": "Controllable Generative Trajectory Prediction via Weak Preference Alignment", "comment": null, "summary": "Deep generative models such as conditional variational autoencoders (CVAEs)\nhave shown great promise for predicting trajectories of surrounding agents in\nautonomous vehicle planning. State-of-the-art models have achieved remarkable\naccuracy in such prediction tasks. Besides accuracy, diversity is also crucial\nfor safe planning because human behaviors are inherently uncertain and\nmultimodal. However, existing methods generally lack a scheme to generate\ncontrollably diverse trajectories, which is arguably more useful than randomly\ndiversified trajectories, to the end of safe planning. To address this, we\npropose PrefCVAE, an augmented CVAE framework that uses weakly labeled\npreference pairs to imbue latent variables with semantic attributes. Using\naverage velocity as an example attribute, we demonstrate that PrefCVAE enables\ncontrollable, semantically meaningful predictions without degrading baseline\naccuracy. Our results show the effectiveness of preference supervision as a\ncost-effective way to enhance sampling-based generative models.", "AI": {"tldr": "本文提出PrefCVAE，一个增强型条件变分自编码器框架，利用弱标签偏好对赋予潜在变量语义属性，以实现可控、语义丰富的自动驾驶轨迹预测，同时保持准确性。", "motivation": "自动驾驶轨迹预测模型需要高准确性和多样性，因为人类行为具有不确定性和多模态性。然而，现有方法通常缺乏生成可控多样化轨迹的机制，而可控多样性对于安全规划比随机多样性更有用。", "method": "作者提出了PrefCVAE，一个增强的CVAE框架。它通过使用弱标签的偏好对来为潜在变量注入语义属性。文中以平均速度作为示例属性进行演示。", "result": "研究表明，PrefCVAE能够实现可控、语义有意义的预测（例如，基于平均速度），并且不会降低基线模型的准确性。结果证明了偏好监督作为一种经济有效的方式来增强基于采样的生成模型的有效性。", "conclusion": "偏好监督是一种经济有效的方法，可以增强基于采样的生成模型，使其能够生成可控且语义有意义的轨迹，这对于自动驾驶等需要安全规划的应用至关重要。"}}
{"id": "2510.10759", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10759", "abs": "https://arxiv.org/abs/2510.10759", "authors": ["Arthicha Srisuchinnawong", "Poramate Manoonpong"], "title": "Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning", "comment": "RSS 2025", "summary": "Existing robot locomotion learning techniques rely heavily on the offline\nselection of proper reward weighting gains and cannot guarantee constraint\nsatisfaction (i.e., constraint violation) during training. Thus, this work aims\nto address both issues by proposing Reward-Oriented Gains via Embodied\nRegulation (ROGER), which adapts reward-weighting gains online based on\npenalties received throughout the embodied interaction process. The ratio\nbetween the positive reward (primary reward) and negative reward (penalty)\ngains is automatically reduced as the learning approaches the constraint\nthresholds to avoid violation. Conversely, the ratio is increased when learning\nis in safe states to prioritize performance. With a 60-kg quadruped robot,\nROGER achieved near-zero constraint violation throughout multiple learning\ntrials. It also achieved up to 50% more primary reward than the equivalent\nstate-of-the-art techniques. In MuJoCo continuous locomotion benchmarks,\nincluding a single-leg hopper, ROGER exhibited comparable or up to 100% higher\nperformance and 60% less torque usage and orientation deviation compared to\nthose trained with the default reward function. Finally, real-world locomotion\nlearning of a physical quadruped robot was achieved from scratch within one\nhour without any falls. Therefore, this work contributes to\nconstraint-satisfying real-world continual robot locomotion learning and\nsimplifies reward weighting gain tuning, potentially facilitating the\ndevelopment of physical robots and those that learn in the real world.", "AI": {"tldr": "本文提出ROGER方法，通过在线自适应奖励权重增益来解决机器人运动学习中离线增益选择困难和训练期间难以保证约束满足的问题，实现了近零约束违反和更高的性能，并成功应用于真实四足机器人。", "motivation": "现有的机器人运动学习技术严重依赖离线选择合适的奖励权重增益，并且无法在训练期间保证约束满足（即存在约束违反）。", "method": "本文提出ROGER（Reward-Oriented Gains via Embodied Regulation）方法，它根据在具身交互过程中收到的惩罚在线调整奖励权重增益。当学习接近约束阈值时，正奖励（主要奖励）和负奖励（惩罚）增益之间的比例会自动降低以避免违反；反之，当学习处于安全状态时，该比例会增加以优先考虑性能。", "result": "在60公斤四足机器人上，ROGER在多次学习试验中实现了接近零的约束违反，并比同类最先进技术获得了高达50%的额外主要奖励。在MuJoCo连续运动基准测试（包括单腿跳跃器）中，ROGER表现出与使用默认奖励函数训练的方法相当或高达100%的更高性能，以及60%更少的扭矩使用和方向偏差。最后，一个物理四足机器人实现了从零开始的真实世界运动学习，在一小时内没有摔倒。", "conclusion": "这项工作为满足约束的真实世界持续机器人运动学习做出了贡献，并简化了奖励权重增益的调整，有望促进物理机器人和在真实世界中学习的机器人的发展。"}}
{"id": "2510.11534", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11534", "abs": "https://arxiv.org/abs/2510.11534", "authors": ["Enli Lin", "Ziyuan Yang", "Qiujing Lu", "Jianming Hu", "Shuo Feng"], "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on Interaction Decoupling Strategy", "comment": "Accepted by ITSC 2025", "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics.", "AI": {"tldr": "该研究引入了CiCross数据集和IntersectioNDE模拟器，旨在解决现有模拟器在模拟城市交叉口密集、异构交通流方面的挑战，通过交互解耦策略（IDS）提高了模拟的鲁棒性和长期稳定性。", "motivation": "现有数据驱动模拟器难以建模城市交叉口（尤其在中国等国家普遍存在）密集、异构的交通互动（包括机动车、非机动车和行人），且难以鲁棒地学习高维联合分布，常导致模式崩溃和长期模拟不稳定。", "method": "研究提出了City Crossings Dataset (CiCross)，一个从真实城市交叉口收集的大规模数据集，捕捉了密集的异构多智能体互动。在此基础上，提出了IntersectioNDE，一个针对复杂城市交叉口场景的数据驱动模拟器。其核心是交互解耦策略（IDS），一种通过学习智能体子集的组合动态来实现从边际到联合模拟的训练范式。IDS被集成到场景感知Transformer网络中，并结合专门的训练技术。", "result": "IntersectioNDE结合IDS显著增强了异构交互模拟的鲁棒性和长期稳定性。在CiCross数据集上的实验表明，IntersectioNDE在模拟保真度、稳定性以及复制复杂、分布级城市交通动态的能力方面优于基线方法。", "conclusion": "该研究通过引入CiCross数据集和基于交互解耦策略的IntersectioNDE模拟器，成功解决了城市交叉口密集、异构交通流模拟的挑战，实现了更逼真、更稳定的模拟效果，对自动驾驶车辆的安全性和可靠性至关重要。"}}
{"id": "2510.10644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10644", "abs": "https://arxiv.org/abs/2510.10644", "authors": ["Yi Zhang", "Yushen Long", "Yun Ni", "Liping Huang", "Xiaohong Wang", "Jun Liu"], "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems", "comment": null, "summary": "Online ride-hailing platforms aim to deliver efficient mobility-on-demand\nservices, often facing challenges in balancing dynamic and spatially\nheterogeneous supply and demand. Existing methods typically fall into two\ncategories: reinforcement learning (RL) approaches, which suffer from data\ninefficiency, oversimplified modeling of real-world dynamics, and difficulty\nenforcing operational constraints; or decomposed online optimization methods,\nwhich rely on manually designed high-level objectives that lack awareness of\nlow-level routing dynamics. To address this issue, we propose a novel hybrid\nframework that integrates large language model (LLM) with mathematical\noptimization in a dynamic hierarchical system: (1) it is training-free,\nremoving the need for large-scale interaction data as in RL, and (2) it\nleverages LLM to bridge cognitive limitations caused by problem decomposition\nby adaptively generating high-level objectives. Within this framework, LLM\nserves as a meta-optimizer, producing semantic heuristics that guide a\nlow-level optimizer responsible for constraint enforcement and real-time\ndecision execution. These heuristics are refined through a closed-loop\nevolutionary process, driven by harmony search, which iteratively adapts the\nLLM prompts based on feasibility and performance feedback from the optimization\nlayer. Extensive experiments based on scenarios derived from both the New York\nand Chicago taxi datasets demonstrate the effectiveness of our approach,\nachieving an average improvement of 16% compared to state-of-the-art baselines.", "AI": {"tldr": "本文提出一种新颖的混合框架，将大型语言模型（LLM）与数学优化结合，用于动态网约车平台调度，解决了现有方法的数据效率低、模型简化和目标缺乏低层感知的问题。该框架无需训练，LLM作为元优化器自适应生成高级目标，并通过进化过程优化提示，在真实数据集上实现16%的性能提升。", "motivation": "网约车平台在平衡动态且空间异构的供需时面临挑战。现有方法存在不足：强化学习（RL）方法数据效率低、模型过于简化且难以强制操作约束；分解式在线优化方法则依赖手动设计的高级目标，缺乏对低层路径动态的感知。", "method": "本文提出一种动态分层混合框架：\n1.  无需训练，避免了RL对大规模交互数据的需求。\n2.  利用LLM生成自适应的高级目标，弥补了问题分解导致的认知局限。\n3.  LLM作为元优化器，产生语义启发式，指导负责约束执行和实时决策的低层优化器。\n4.  通过闭环进化过程（基于和声搜索），根据优化层的可行性和性能反馈，迭代地优化LLM的提示。", "result": "基于纽约和芝加哥出租车数据集的场景实验表明，与最先进的基线方法相比，本文提出的方法平均性能提升了16%。", "conclusion": "该混合框架通过将LLM的认知能力与数学优化的精确性相结合，有效解决了网约车平台面临的供需平衡挑战，并在实际场景中取得了显著的性能提升。"}}
{"id": "2510.10103", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10103", "abs": "https://arxiv.org/abs/2510.10103", "authors": ["Renliang Sun", "Wei Cheng", "Dawei Li", "Haifeng Chen", "Wei Wang"], "title": "Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has driven recent gains of large language\nmodels (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.\nHowever, excessive or redundant reasoning -- so-called overthinking -- can\nincrease inference costs and lead LLMs toward incorrect conclusions. In this\npaper, we present REFRAIN ($\\underline{REF}$lective-$\\underline{R}$edundancy\nfor $\\underline{A}$daptive $\\underline{IN}$ference), a training-free framework\nthat adaptively determines when to stop reasoning to mitigate overthinking.\nREFRAIN integrates a two-stage stop discriminator to identify reflective yet\nredundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)\nmulti-armed bandit controller to dynamically adjust stopping thresholds\naccording to problem difficulty without supervision or fine-tuning. Across four\nrepresentative benchmarks and two model families, REFRAIN reduces token usage\nby 20-55% while maintaining or improving accuracy compared to standard CoT\nprompting. Extensive ablation and robustness analyses demonstrate its stability\nacross models, scorers, and prompt variations. In summary, our findings\nhighlight when-to-stop as a new and practical axis of test-time scaling --\nenabling models to reason not just more, but just enough.", "AI": {"tldr": "REFRAIN是一个免训练框架，通过自适应地决定何时停止推理来减轻大型语言模型（LLMs）在链式思考（CoT）中的“过度思考”问题，从而在保持或提高准确性的同时显著降低推理成本。", "motivation": "链式思考（CoT）提高了LLMs在推理任务上的表现，但过度或冗余的推理（即“过度思考”）会增加推理成本，并可能导致LLMs得出错误的结论。", "method": "REFRAIN框架包含两个主要组件：一个两阶段停止判别器，用于识别反思性但冗余的推理；一个滑动窗口上限置信度（SW-UCB）多臂老虎机控制器，用于根据问题难度动态调整停止阈值。该框架无需训练、监督或微调。", "result": "在四个代表性基准测试和两个模型家族上，REFRAIN与标准CoT提示相比，将token使用量减少了20-55%，同时保持或提高了准确性。广泛的消融和鲁棒性分析表明其在不同模型、评分器和提示变体下都具有稳定性。", "conclusion": "研究结果强调了“何时停止”是测试时扩展的一个新的实用维度，它使模型能够进行“恰到好处”而非过度的推理。"}}
{"id": "2510.10082", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10082", "abs": "https://arxiv.org/abs/2510.10082", "authors": ["Parthiv Chatterjee", "Shivam Sonawane", "Amey Hengle", "Aditya Tanna", "Sourish Dasgupta", "Tanmoy Chakraborty"], "title": "Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers", "comment": null, "summary": "Document summarization enables efficient extraction of user-relevant content\nbut is inherently shaped by individual subjectivity, making it challenging to\nidentify subjective salient information in multifaceted documents. This\ncomplexity underscores the necessity for personalized summarization. However,\ntraining models for personalized summarization has so far been challenging,\nparticularly because diverse training data containing both user preference\nhistory (i.e., click-skip trajectory) and expected (gold-reference) summaries\nare scarce. The MS/CAS PENS dataset is a valuable resource but includes only\npreference history without target summaries, preventing end-to-end supervised\nlearning, and its limited topic-transition diversity further restricts\ngeneralization. To address this, we propose $\\mathrm{PerAugy}$, a novel\ncross-trajectory shuffling and summary-content perturbation based data\naugmentation technique that significantly boosts the accuracy of four\nstate-of-the-art baseline (SOTA) user-encoders commonly used in personalized\nsummarization frameworks (best result: $\\text{0.132}$$\\uparrow$ w.r.t AUC). We\nselect two such SOTA summarizer frameworks as baselines and observe that when\naugmented with their corresponding improved user-encoders, they consistently\nshow an increase in personalization (avg. boost: $\\text{61.2\\%}\\uparrow$ w.r.t.\nPSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the\naugmented dataset by \\peraugy, we introduce three dataset diversity metrics --\n$\\mathrm{TP}$, $\\mathrm{RTC}$, and \\degreed\\ to quantify the induced diversity.\nWe find that $\\mathrm{TP}$ and $\\mathrm{DegreeD}$ strongly correlate with\nuser-encoder performance on the PerAugy-generated dataset across all accuracy\nmetrics, indicating that increased dataset diversity is a key factor driving\nperformance gains.", "AI": {"tldr": "由于数据稀缺，个性化摘要面临挑战。本文提出了一种名为PerAugy的数据增强技术，通过轨迹重排和摘要内容扰动，显著提升了用户编码器和个性化摘要框架的性能，并发现增强的数据多样性是性能提升的关键因素。", "motivation": "文档摘要受个体主观性影响，难以识别多方面文档中的主观突出信息，因此需要个性化摘要。然而，由于缺乏同时包含用户偏好历史和预期摘要的训练数据，训练个性化摘要模型具有挑战性。现有数据集（如MS/CAS PENS）缺乏目标摘要且主题转换多样性有限，限制了模型泛化能力。", "method": "本文提出了一种新颖的数据增强技术——PerAugy。该技术通过跨轨迹洗牌（cross-trajectory shuffling）和摘要内容扰动（summary-content perturbation）来生成多样化数据。研究人员利用PerAugy显著提升了四种最先进（SOTA）的用户编码器准确性，并将这些改进后的用户编码器集成到两种SOTA摘要框架中进行评估。此外，引入了TP、RTC和DegreeD三种数据集多样性指标，对PerAugy诱导的多样性进行了事后分析。", "result": "PerAugy显著提升了四种SOTA用户编码器的准确性（最佳结果：AUC提升0.132）。当使用增强后的用户编码器时，两种SOTA摘要框架的个性化能力均持续增强（PSE-SU4指标平均提升61.2%）。事后分析表明，TP和DegreeD指标与PerAugy生成数据集上的用户编码器性能强烈相关，表明数据集多样性的增加是性能提升的关键因素。", "conclusion": "PerAugy通过生成多样化的训练数据，有效解决了个性化摘要领域的数据稀缺问题，从而显著提升了用户编码器的性能和个性化摘要框架的效果。研究结果强调了数据多样性在驱动个性化摘要性能提升中的关键作用。"}}
{"id": "2510.10649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10649", "abs": "https://arxiv.org/abs/2510.10649", "authors": ["Can Xie", "Ruotong Pan", "Xiangyu Wu", "Yunfei Zhang", "Jiayi Fu", "Tingting Gao", "Guorui Zhou"], "title": "Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant\npromise for enhancing the reasoning capabilities of large language models\n(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage\nsignal across all tokens in a sequence. This coarse-grained approach overlooks\nthe pivotal role of uncertain, high-stakes decisions during reasoning, leading\nto inefficient exploration and the well-documented problem of entropy collapse.\nTo address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a\nmodel-free method that refines credit assignment by leveraging the model's\ninternal uncertainty signals. UCAS operates in two stages: it first modulates\nthe response-level advantage using the model's overall self-confidence, and\nthen applies a token-level penalty based on raw logit certainty. This dual\nmechanism encourages exploration of high-uncertainty paths that yield correct\nanswers while penalizing overconfident yet erroneous reasoning, effectively\nbalancing the exploration-exploitation trade-off. Extensive experiments on five\nmathematical reasoning benchmarks show that UCAS significantly outperforms\nstrong RLVR baselines across multiple model scales, including 1.5B and 7B. Our\nanalysis confirms that UCAS not only achieves higher rewards but also promotes\ngreater reasoning diversity and successfully mitigates entropy collapse.", "AI": {"tldr": "针对大型语言模型（LLM）的RLVR推理，本文提出UCAS方法，通过利用模型内部不确定性信号来精炼信用分配，从而解决现有算法中粗粒度优势信号导致的探索效率低下和熵坍缩问题，显著提升了数学推理能力。", "motivation": "现有的可验证奖励强化学习（RLVR）算法（如GRPO）在LLM推理中采用统一的优势信号，忽略了不确定、高风险决策的关键作用，导致探索效率低下和熵坍缩问题。", "method": "UCAS（UnCertainty-aware Advantage Shaping）是一种无模型方法，通过利用模型内部不确定性信号来精炼信用分配。它分两阶段操作：首先，利用模型整体自信心调节响应级别的优势；其次，根据原始logit确定性施加token级别的惩罚。这种双重机制鼓励探索能产生正确答案的高不确定性路径，并惩罚过度自信但错误的推理，有效平衡了探索与利用。", "result": "在五个数学推理基准测试中，UCAS显著优于强大的RLVR基线（包括1.5B和7B模型规模）。分析证实，UCAS不仅获得了更高的奖励，还促进了更大的推理多样性，并成功缓解了熵坍缩。", "conclusion": "UCAS通过利用模型内部不确定性信号来精炼信用分配，有效解决了RLVR中粗粒度优势信号的局限性，从而显著提升了LLM的推理能力、多样性，并缓解了熵坍缩问题。"}}
{"id": "2510.10111", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10111", "abs": "https://arxiv.org/abs/2510.10111", "authors": ["Rui Chen", "Bin Liu", "Changtao Miao", "Xinghao Wang", "Yi Li", "Tao Gong", "Qi Chu", "Nenghai Yu"], "title": "Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization", "comment": null, "summary": "Advances in image tampering pose serious security threats, underscoring the\nneed for effective image manipulation localization (IML). While supervised IML\nachieves strong performance, it depends on costly pixel-level annotations.\nExisting weakly supervised or training-free alternatives often underperform and\nlack interpretability. We propose the In-Context Forensic Chain (ICFC), a\ntraining-free framework that leverages multi-modal large language models\n(MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule\nconstruction with adaptive filtering to build a reliable knowledge base and a\nmulti-step progressive reasoning pipeline that mirrors expert forensic\nworkflows from coarse proposals to fine-grained forensics results. This design\nenables systematic exploitation of MLLM reasoning for image-level\nclassification, pixel-level localization, and text-level interpretability.\nAcross multiple benchmarks, ICFC not only surpasses state-of-the-art\ntraining-free methods but also achieves competitive or superior performance\ncompared to weakly and fully supervised approaches.", "AI": {"tldr": "本文提出了In-Context Forensic Chain (ICFC)，一个无需训练的框架，利用多模态大语言模型（MLLMs）实现可解释的图像篡改定位（IML），并在多个基准测试中超越了现有无训练和弱监督方法。", "motivation": "图像篡改技术进步带来了严重的安全威胁，因此急需有效的图像篡改定位（IML）方法。然而，监督式IML依赖于昂贵的像素级标注，而现有的弱监督或无需训练的替代方案往往性能不佳且缺乏可解释性。", "method": "本文提出了ICFC框架，这是一个无需训练的框架，利用多模态大语言模型（MLLMs）进行可解释的IML任务。ICFC整合了客体化规则构建和自适应过滤机制，以建立可靠的知识库；并采用多步渐进推理流程，模拟专家取证工作流，从粗略提议到精细取证结果。这种设计使得系统能够利用MLLM的推理能力进行图像级分类、像素级定位和文本级可解释性。", "result": "在多个基准测试中，ICFC不仅超越了最先进的无需训练方法，而且与弱监督和全监督方法相比，也达到了有竞争力或更优的性能。它成功实现了图像级分类、像素级定位和文本级可解释性。", "conclusion": "ICFC提供了一种无需训练、有效且可解释的图像篡改定位解决方案，展示了多模态大语言模型在法证分析任务中的巨大潜力，并解决了传统方法对昂贵标注的依赖以及性能和可解释性不足的问题。"}}
{"id": "2510.10113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10113", "abs": "https://arxiv.org/abs/2510.10113", "authors": ["Yuxi Mi", "Qiuyang Yuan", "Zhizhou Zhong", "Xuan Zhao", "Jiaogen Zhou", "Fubao Zhu", "Jihong Guan", "Shuigeng Zhou"], "title": "ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes", "comment": null, "summary": "In egocentric applications such as augmented and virtual reality, immersive\niris recognition is emerging as an accurate and seamless way to identify\npersons. While classic systems acquire iris images on-axis, i.e., via dedicated\nfrontal sensors in controlled settings, the immersive setup primarily captures\noff-axis irises through tilt-placed headset cameras, with only mild control in\nopen scenes. This yields unique challenges, including perspective distortion,\nintensified quality degradations, and intra-class variations in iris texture.\nDatasets capturing these challenges remain scarce. To fill this gap, this paper\nintroduces ImmerIris, a large-scale dataset collected via VR headsets,\ncontaining 499,791 ocular images from 564 subjects. It is, to the best of\ncurrent knowledge, the largest public dataset and among the first dedicated to\noff-axis acquisition. Based on ImmerIris, evaluation protocols are constructed\nto benchmark recognition methods under different challenging factors. Current\nmethods, primarily designed for classic on-axis imagery, perform\nunsatisfactorily on the immersive setup, mainly due to reliance on fallible\nnormalization. To this end, this paper further proposes a normalization-free\nparadigm that directly learns from ocular images with minimal adjustment.\nDespite its simplicity, this approach consistently outperforms\nnormalization-based counterparts, pointing to a promising direction for robust\nimmersive recognition.", "AI": {"tldr": "本文针对沉浸式应用中离轴虹膜识别的挑战，提出了一个大型离轴虹膜数据集ImmerIris，并介绍了一种无需归一化的识别范式，该范式在离轴场景下表现优于传统方法。", "motivation": "沉浸式虹膜识别在增强现实/虚拟现实等应用中日益重要，但其主要通过倾斜放置的头戴式摄像头捕获离轴虹膜，导致透视畸变、质量下降和虹膜纹理类内变化等独特挑战。现有数据集稀缺，且传统方法在沉浸式设置中表现不佳。", "method": "本文引入了ImmerIris数据集，该数据集通过VR头显收集，包含来自564名受试者的499,791张眼部图像，是目前最大的公共离轴虹膜数据集。基于ImmerIris，构建了评估协议以基准测试不同挑战因素下的识别方法。此外，提出了一种无需归一化的范式，直接从眼部图像中学习，仅进行最小调整。", "result": "ImmerIris是目前最大的公共数据集，也是首批专注于离轴采集的数据集之一。针对经典轴上图像设计的现有方法在沉浸式设置中表现不佳，主要是因为依赖于易出错的归一化。本文提出的无需归一化的方法，尽管简单，但始终优于基于归一化的同类方法。", "conclusion": "无需归一化的范式为鲁棒的沉浸式虹膜识别指明了有前景的方向，有效应对了离轴采集带来的挑战，并超越了传统归一化方法的局限性。"}}
{"id": "2510.10675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10675", "abs": "https://arxiv.org/abs/2510.10675", "authors": ["Deven Panchal"], "title": "Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows", "comment": null, "summary": "Generative Agentic AI systems are emerging as a powerful paradigm for\nautomating complex, multi-step tasks. However, many existing frameworks for\nbuilding these systems introduce significant complexity, a steep learning\ncurve, and substantial boilerplate code, hindering rapid prototyping and\ndeployment. This paper introduces simpliflow, a lightweight, open-source Python\nframework designed to address these challenges. simpliflow enables the rapid\ndevelopment and orchestration of linear, deterministic agentic workflows\nthrough a declarative, JSON-based configuration. Its modular architecture\ndecouples agent management, workflow execution, and post-processing, promoting\nease of use and extensibility. By integrating with LiteLLM, it supports over\n100 Large Language Models (LLMs) out-of-the-box. We present the architecture,\noperational flow, and core features of simpliflow, demonstrating its utility\nthrough diverse use cases ranging from software development simulation to\nreal-time system interaction. A comparative analysis with prominent frameworks\nlike LangChain and AutoGen highlights simpliflow's unique position as a tool\noptimized for simplicity, control, and speed in deterministic workflow\nenvironments.", "AI": {"tldr": "本文介绍了一个名为simpliflow的轻量级、开源Python框架，旨在简化和加速线性、确定性智能体工作流的开发和编排，解决现有框架的复杂性和学习曲线问题。", "motivation": "现有生成式智能体AI系统框架在构建复杂、多步骤任务时引入了显著的复杂性、陡峭的学习曲线和大量的样板代码，阻碍了快速原型开发和部署。", "method": "本文提出了simpliflow框架，它通过声明式的、基于JSON的配置实现线性、确定性智能体工作流的快速开发和编排。其模块化架构解耦了智能体管理、工作流执行和后处理，并通过集成LiteLLM支持超过100种大型语言模型（LLMs）。", "result": "simpliflow能够实现线性、确定性智能体工作流的快速开发和编排，支持广泛的LLMs，并在软件开发模拟到实时系统交互等多样化用例中展示了其效用。与LangChain和AutoGen等框架的比较分析表明，simpliflow在确定性工作流环境中，在简洁性、控制性和速度方面具有独特优势。", "conclusion": "simpliflow是一个为简化、控制和加速确定性生成式智能体AI工作流而优化的工具，有效解决了现有框架的复杂性问题，促进了快速原型开发和部署。"}}
{"id": "2510.10114", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10114", "abs": "https://arxiv.org/abs/2510.10114", "authors": ["Luyao Zhuang", "Shengyuan Chen", "Yilin Xiao", "Huachi Zhou", "Yujing Zhang", "Hao Chen", "Qinggang Zhang", "Xiao Huang"], "title": "LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is widely used to mitigate\nhallucinations of Large Language Models (LLMs) by leveraging external\nknowledge. While effective for simple queries, traditional RAG systems struggle\nwith large-scale, unstructured corpora where information is fragmented. Recent\nadvances incorporate knowledge graphs to capture relational structures,\nenabling more comprehensive retrieval for complex, multi-hop reasoning tasks.\nHowever, existing graph-based RAG (GraphRAG) methods rely on unstable and\ncostly relation extraction for graph construction, often producing noisy graphs\nwith incorrect or inconsistent relations that degrade retrieval quality. In\nthis paper, we revisit the pipeline of existing GraphRAG systems and propose\nLinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient\nframework that enables reliable graph construction and precise passage\nretrieval. Specifically, LinearRAG constructs a relation-free hierarchical\ngraph, termed Tri-Graph, using only lightweight entity extraction and semantic\nlinking, avoiding unstable relation modeling. This new paradigm of graph\nconstruction scales linearly with corpus size and incurs no extra token\nconsumption, providing an economical and reliable indexing of the original\npassages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant\nentity activation via local semantic bridging, followed by (ii) passage\nretrieval through global importance aggregation. Extensive experiments on four\ndatasets demonstrate that LinearRAG significantly outperforms baseline models.", "AI": {"tldr": "传统RAG在处理大规模非结构化语料时面临挑战，现有GraphRAG方法依赖不稳定的关系抽取。本文提出LinearRAG，通过构建无关系的分层图（Tri-Graph）实现可靠高效的图构建，并采用两阶段检索策略，显著优于基线模型。", "motivation": "RAG在处理信息碎片化的大规模非结构化语料时效果不佳。现有基于知识图谱的RAG（GraphRAG）方法依赖不稳定且昂贵的关系抽取来构建图谱，常导致图谱噪声和检索质量下降。", "method": "LinearRAG提出了一种高效框架。它通过轻量级实体抽取和语义链接构建一个无关系的分层图（Tri-Graph），避免了不稳定的关系建模。这种图构建方法与语料库大小呈线性关系，且不产生额外token消耗。检索策略采用两阶段：(i) 通过局部语义桥接激活相关实体，(ii) 通过全局重要性聚合进行段落检索。", "result": "在四个数据集上进行的大量实验表明，LinearRAG显著优于所有基线模型。", "conclusion": "LinearRAG通过避免不稳定的关系抽取，提供了一种高效、可靠且精准的图增强检索生成（GraphRAG）解决方案，显著提升了性能。"}}
{"id": "2510.11682", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11682", "abs": "https://arxiv.org/abs/2510.11682", "authors": ["Hang Liu", "Yuman Gao", "Sangli Teng", "Yufeng Chi", "Yakun Sophia Shao", "Zhongyu Li", "Maani Ghaffari", "Koushil Sreenath"], "title": "Ego-Vision World Model for Humanoid Contact Planning", "comment": null, "summary": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/", "AI": {"tldr": "该研究提出了一种结合学习型世界模型和基于采样的模型预测控制（MPC）的框架，使人形机器人能够进行接触感知规划。该系统在离线数据集上训练，并利用学习型替代价值函数进行鲁棒规划，实现了高效且多任务的接触利用，并已成功部署在物理人形机器人上。", "motivation": "为了在非结构化环境中实现自主性，人形机器人需要能够利用物理接触而非仅仅避免碰撞。传统的基于优化的规划器难以处理接触的复杂性，而在线强化学习（RL）样本效率低下且多任务能力有限。", "method": "该框架结合了学习型世界模型和基于采样的模型预测控制（MPC）。世界模型在无演示的离线数据集上训练，用于在压缩的潜在空间中预测未来结果。为解决稀疏接触奖励和传感器噪声问题，MPC使用一个学习型替代价值函数进行密集、鲁棒的规划。该模型是一个单一、可扩展的模型。", "result": "该单一、可扩展的模型支持多种接触感知任务，包括受扰动后的墙壁支撑、阻挡来袭物体以及穿越限高拱门。与在线强化学习相比，该方法提高了数据效率和多任务能力。部署在物理人形机器人上时，该系统能够从本体感受和自我中心深度图像实现鲁棒、实时的接触规划。", "conclusion": "该研究提出的结合学习型世界模型和基于采样的MPC框架，通过利用学习型替代价值函数，成功使人形机器人能够进行鲁棒、实时、数据高效且多任务的接触感知规划，从而增强了机器人在非结构化环境中的自主性。"}}
{"id": "2510.10778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10778", "abs": "https://arxiv.org/abs/2510.10778", "authors": ["Christopher D. Hsu", "Pratik Chaudhari"], "title": "Real2USD: Scene Representations in Universal Scene Description Language", "comment": "8 pages, 10 figures, 1 table", "summary": "Large Language Models (LLMs) can help robots reason about abstract task\nspecifications. This requires augmenting classical representations of the\nenvironment used by robots with natural language-based priors. There are a\nnumber of existing approaches to doing so, but they are tailored to specific\ntasks, e.g., visual-language models for navigation, language-guided neural\nradiance fields for mapping, etc. This paper argues that the Universal Scene\nDescription (USD) language is an effective and general representation of\ngeometric, photometric and semantic information in the environment for\nLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene\ngraph, readable by LLMs and humans alike, and rich enough to support\nessentially any task -- Pixar developed this language to store assets, scenes\nand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2\nquadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD\nrepresentation of indoor environments with diverse objects and challenging\nsettings with lots of glass, and (ii) parses the USD using Google's Gemini to\ndemonstrate scene understanding, complex inferences, and planning. We also\nstudy different aspects of this system in simulated warehouse and hospital\nsettings using Nvidia's Issac Sim. Code is available at\nhttps://github.com/grasp-lyrl/Real2USD .", "AI": {"tldr": "本文提出通用场景描述（USD）作为大型语言模型（LLM）驱动机器人任务的通用环境表示，并展示了一个“Real to USD”系统，该系统能从真实世界构建USD场景图，并利用LLM进行场景理解、推理和规划。", "motivation": "现有增强机器人环境表示以支持LLM推理的方法通常是针对特定任务的，缺乏一种通用且能被LLM理解的环境表示，以帮助机器人理解抽象任务规范。", "method": "本文提出使用通用场景描述（USD）作为几何、光度学和语义信息的通用表示，因为它是一个XML-based场景图，对LLM和人类都可读且足够丰富。研究团队开发了一个“Real to USD”系统，使用Unitree Go2四足机器人搭载LiDAR和RGB相机来构建室内环境的USD表示，并利用Google Gemini解析USD以实现场景理解、复杂推理和规划。此外，还在Nvidia Isaac Sim的模拟仓库和医院环境中研究了该系统的不同方面。", "result": "“Real to USD”系统成功构建了包含多样物体和挑战性环境（如大量玻璃）的室内环境的明确USD表示。Google Gemini能够成功解析USD，并展示了场景理解、复杂推理和规划能力。该系统在模拟环境中也得到了验证。", "conclusion": "USD是一种有效且通用的环境表示，适用于基于LLM的机器人任务，能够支持场景理解、复杂推理和规划，并在真实和模拟环境中均表现出潜力。"}}
{"id": "2510.10122", "categories": ["cs.CV", "cs.AI", "68T45, 68T10", "I.2.10; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.10122", "abs": "https://arxiv.org/abs/2510.10122", "authors": ["Halil Hüseyin Çalışkan", "Talha Koruk"], "title": "DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution", "comment": "12 pages, 11 figures", "summary": "Computer vision and image processing applications suffer from dark and\nlow-light images, particularly during real-time image transmission. Currently,\nlow light and dark images are converted to bright and colored forms using\nautoencoders; however, these methods often achieve low SSIM and PSNR scores and\nrequire high computational power due to their large number of parameters. To\naddress these challenges, the DeepFusionNet architecture has been developed.\nAccording to the results obtained with the LOL-v1 dataset, DeepFusionNet\nachieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only\napproximately 2.5 million parameters. On the other hand, conversion of blurry\nand low-resolution images into high-resolution and blur-free images has gained\nimportance in image processing applications. Unlike GAN-based super-resolution\nmethods, an autoencoder-based super resolution model has been developed that\ncontains approximately 100 thousand parameters and uses the DeepFusionNet\narchitecture. According to the results of the tests, the DeepFusionNet based\nsuper-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7\npercent according to the validation set.", "AI": {"tldr": "本文提出了一种名为DeepFusionNet的轻量级架构，用于解决低光图像增强和图像超分辨率问题，在保持高性能的同时显著减少了参数数量。", "motivation": "现有的低光图像增强方法（如基于自编码器的方法）通常SSIM和PSNR分数较低，且由于参数量大而需要高计算能力。此外，将模糊和低分辨率图像转换为高分辨率和无模糊图像在图像处理中也日益重要。", "method": "本文开发了DeepFusionNet架构。对于低光图像增强，它采用DeepFusionNet架构的自编码器。对于超分辨率，它也开发了一个基于DeepFusionNet架构的自编码器模型，而不是GAN-based的方法。", "result": "在LOL-v1数据集上，DeepFusionNet在低光增强方面实现了92.8%的SSIM和26.30的PSNR，而参数量仅约250万。基于DeepFusionNet的超分辨率方法在验证集上实现了25.30的PSNR和80.7%的SSIM，参数量仅约10万。", "conclusion": "DeepFusionNet架构为低光图像增强和图像超分辨率提供了一种高效且高性能的解决方案，显著减少了模型参数，从而降低了计算成本，同时保持了良好的图像质量指标。"}}
{"id": "2510.10689", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10689", "abs": "https://arxiv.org/abs/2510.10689", "authors": ["Caorui Li", "Yu Chen", "Yiyan Ji", "Jin Xu", "Zhenyu Cui", "Shihao Li", "Yuanxing Zhang", "Jiafu Tang", "Zhenghao Song", "Dingling Zhang", "Ying He", "Haoxiang Liu", "Yuxuan Wang", "Qiufeng Wang", "Zhenhe Wu", "Jiehui Luo", "Zhiyu Pan", "Weihao Xie", "Chenchen Zhang", "Zhaohui Wang", "Jiayi Tian", "Yanghai Wang", "Zhe Cao", "Minxin Dai", "Ke Wang", "Runzhe Wen", "Yinghao Ma", "Yaning Pan", "Sungkyun Chang", "Termeh Taheri", "Haiwen Xia", "Christos Plachouras", "Emmanouil Benetos", "Yizhi Li", "Ge Zhang", "Jian Yang", "Tianhao Peng", "Zili Wang", "Minghao Liu", "Junran Peng", "Zhaoxiang Zhang", "Jiaheng Liu"], "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.", "AI": {"tldr": "本文引入了OmniVideoBench，一个大规模、严格设计的基准测试，旨在全面评估多模态大语言模型（MLLMs）在视频理解中音频和视觉模态的协同推理能力，强调模态互补性和逻辑一致性。", "motivation": "尽管多模态大语言模型（MLLMs）在视频理解方面展现出巨大潜力，但现有基准未能全面评估音频和视觉模态之间的协同推理能力，常常忽视其中一个模态或以逻辑不一致的方式整合它们。", "method": "OmniVideoBench包含1000个高质量的问答对，每个都附有逐步推理过程，来源于628个多样化视频。这些数据经过人工验证，确保完全正确性和唯一性。它涵盖13种精心设计的问答类型，包括时间推理、空间定位、计数、因果推断和总结等，以捕捉视频理解的关键挑战。", "result": "对多个MLLMs在OmniVideoBench上的评估显示，模型性能与人类推理之间存在显著差距。其中，开源模型的表现明显落后于闭源模型，突显了真正音视频推理的固有难度。", "conclusion": "OmniVideoBench揭示了当前MLLMs在协同音视频推理方面的不足，并强调了开发具有更强、更通用推理能力的MLLMs的必要性。该基准将被发布，以促进相关研究和发展。"}}
{"id": "2510.10804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10804", "abs": "https://arxiv.org/abs/2510.10804", "authors": ["Alessandro Albini", "Mohsen Kaboli", "Giorgio Cannata", "Perla Maiolino"], "title": "Representing Data in Robotic Tactile Perception -- A Review", "comment": null, "summary": "Robotic tactile perception is a complex process involving several\ncomputational steps performed at different levels. Tactile information is\nshaped by the interplay of robot actions, the mechanical properties of its\nbody, and the software that processes the data. In this respect, high-level\ncomputation, required to process and extract information, is commonly performed\nby adapting existing techniques from other domains, such as computer vision,\nwhich expects input data to be properly structured. Therefore, it is necessary\nto transform tactile sensor data to match a specific data structure. This\noperation directly affects the tactile information encoded and, as a\nconsequence, the task execution. This survey aims to address this specific\naspect of the tactile perception pipeline, namely Data Representation. The\npaper first clearly defines its contributions to the perception pipeline and\nthen reviews how previous studies have dealt with the problem of representing\ntactile information, investigating the relationships among hardware,\nrepresentations, and high-level computation methods. The analysis has led to\nthe identification of six structures commonly used in the literature to\nrepresent data. The manuscript provides discussions and guidelines for properly\nselecting a representation depending on operating conditions, including the\navailable hardware, the tactile information required to be encoded, and the\ntask at hand.", "AI": {"tldr": "这篇综述论文专注于机器人触觉感知管线中的数据表示问题，识别了六种常用的数据结构，并提供了根据硬件、所需信息和任务选择合适表示的指导。", "motivation": "机器人触觉感知涉及多级计算，高层计算常借鉴计算机视觉等领域，需要结构化的输入数据。因此，触觉传感器数据转换成特定结构是必要的，但这直接影响编码的触觉信息和任务执行。", "method": "该研究通过综述现有文献，明确了其对感知管线的贡献，审查了以往研究如何处理触觉信息表示问题，并调查了硬件、表示方法和高层计算方法之间的关系。", "result": "分析识别出文献中常用的六种触觉数据表示结构。论文还提供了讨论和指导，以便根据操作条件（包括可用硬件、需要编码的触觉信息和当前任务）正确选择数据表示。", "conclusion": "触觉数据表示的选择对机器人触觉感知至关重要，应根据具体硬件、所需编码的触觉信息以及所执行的任务进行适当选择。"}}
{"id": "2510.10138", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10138", "abs": "https://arxiv.org/abs/2510.10138", "authors": ["Zilong Wang", "Xiaoyu Shen"], "title": "Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task", "comment": null, "summary": "Information extraction from copy-heavy documents, characterized by massive\nvolumes of structurally similar content, represents a critical yet understudied\nchallenge in enterprise document processing. We present a systematic framework\nthat strategically combines OCR engines with Large Language Models (LLMs) to\noptimize the accuracy-efficiency trade-off inherent in repetitive document\nextraction tasks. Unlike existing approaches that pursue universal solutions,\nour method exploits document-specific characteristics through intelligent\nstrategy selection. We implement and evaluate 25 configurations across three\nextraction paradigms (direct, replacement, and table-based) on identity\ndocuments spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based\nextraction methods, our adaptive framework delivers outstanding results: F1=1.0\naccuracy with 0.97s latency for structured documents, and F1=0.997 accuracy\nwith 0.6 s for challenging image inputs when integrated with PaddleOCR, all\nwhile maintaining sub-second processing speeds. The 54 times performance\nimprovement compared with multimodal methods over naive approaches, coupled\nwith format-aware routing, enables processing of heterogeneous document streams\nat production scale. Beyond the specific application to identity extraction,\nthis work establishes a general principle: the repetitive nature of copy-heavy\ntasks can be transformed from a computational burden into an optimization\nopportunity through structure-aware method selection.", "AI": {"tldr": "本文提出一个系统框架，结合OCR和LLM，通过智能策略选择，优化了从大量重复内容文档中提取信息的准确性和效率，特别是在证件文档提取上取得了显著成果。", "motivation": "从大量重复内容文档（copy-heavy documents）中提取信息是企业文档处理中一个关键但未被充分研究的挑战。现有方法追求通用解决方案，但未能有效平衡准确性和效率，尤其是在重复性任务中。", "method": "该方法构建了一个系统框架，策略性地结合了OCR引擎和大型语言模型（LLMs）。它通过智能策略选择来利用文档特有的特性，而非采用通用解决方案。研究评估了25种配置，涵盖三种提取范式（直接、替换和基于表格）以及四种文档格式（PNG、DOCX、XLSX、PDF）。", "result": "通过基于表格的提取方法，该自适应框架取得了卓越成果：结构化文档的F1准确率达到1.0，延迟为0.97秒；对于具有挑战性的图像输入，与PaddleOCR集成后F1准确率为0.997，延迟为0.6秒。所有处理速度均在亚秒级。与多模态方法相比，性能提升了54倍，并且通过格式感知路由，能够以生产规模处理异构文档流。", "conclusion": "这项工作建立了一个通用原则：通过结构感知的方法选择，可以将重复性强的任务从计算负担转化为优化机会。这超越了证件提取的特定应用，为处理大量重复内容文档提供了新的范式。"}}
{"id": "2510.10851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10851", "abs": "https://arxiv.org/abs/2510.10851", "authors": ["Tingxuan Leng", "Yushi Wang", "Tinglong Zheng", "Changsheng Luo", "Mingguo Zhao"], "title": "Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion", "comment": null, "summary": "Humanoid locomotion requires not only accurate command tracking for\nnavigation but also compliant responses to external forces during human\ninteraction. Despite significant progress, existing RL approaches mainly\nemphasize robustness, yielding policies that resist external forces but lack\ncompliance-particularly challenging for inherently unstable humanoids. In this\nwork, we address this by formulating humanoid locomotion as a multi-objective\noptimization problem that balances command tracking and external force\ncompliance. We introduce a preference-conditioned multi-objective RL (MORL)\nframework that integrates rigid command following and compliant behaviors\nwithin a single omnidirectional locomotion policy. External forces are modeled\nvia velocity-resistance factor for consistent reward design, and training\nleverages an encoder-decoder structure that infers task-relevant privileged\nfeatures from deployable observations. We validate our approach in both\nsimulation and real-world experiments on a humanoid robot. Experimental results\nindicate that our framework not only improves adaptability and convergence over\nstandard pipelines, but also realizes deployable preference-conditioned\nhumanoid locomotion.", "AI": {"tldr": "本文提出了一种偏好条件多目标强化学习框架，以平衡仿人机器人运动中的指令跟踪和外部力顺从性，实现了可部署的自适应运动。", "motivation": "现有强化学习方法主要侧重于仿人机器人运动的鲁棒性，但缺乏对外部力的顺从性，这对于不稳定的仿人机器人来说是一个挑战。", "method": "将仿人机器人运动建模为平衡指令跟踪和外部力顺从性的多目标优化问题。引入了偏好条件多目标强化学习(MORL)框架，将刚性指令跟踪和顺从行为集成到单一全向运动策略中。通过速度阻力因子建模外部力以进行奖励设计，并利用编码器-解码器结构从可部署观测中推断任务相关的特权特征。", "result": "实验结果表明，该框架不仅提高了自适应性和收敛性，而且实现了可部署的偏好条件仿人机器人运动。", "conclusion": "该框架成功解决了仿人机器人运动中指令跟踪与外部力顺从性之间的平衡问题，实现了更具适应性和可部署的机器人运动策略。"}}
{"id": "2510.10843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10843", "abs": "https://arxiv.org/abs/2510.10843", "authors": ["Jared Grinberg", "Yanran Ding"], "title": "Contact Sensing via Joint Torque Sensors and a Force/Torque Sensor for Legged Robots", "comment": "Proc. IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, Aug. 17-21, 2025, pp. 1-7,\n  doi:10.1109/CASE58245.2025.11164031", "summary": "This paper presents a method for detecting and localizing contact along robot\nlegs using distributed joint torque sensors and a single hip-mounted\nforce-torque (FT) sensor using a generalized momentum-based observer framework.\nWe designed a low-cost strain-gauge-based joint torque sensor that can be\ninstalled on every joint to provide direct torque measurements, eliminating the\nneed for complex friction models and providing more accurate torque readings\nthan estimation based on motor current. Simulation studies on a floating-based\n2-DoF robot leg verified that the proposed framework accurately recovers\ncontact force and location along the thigh and shin links. Through a\ncalibration procedure, our torque sensor achieved an average 96.4% accuracy\nrelative to ground truth measurements. Building upon the torque sensor, we\nperformed hardware experiments on a 2-DoF manipulator, which showed\nsub-centimeter contact localization accuracy and force errors below 0.2 N.", "AI": {"tldr": "本文提出了一种利用分布式关节扭矩传感器和单个髋部力-扭矩传感器，基于广义动量观测器框架，检测和定位机器人腿部接触的方法，并设计了一种高精度的低成本应变计关节扭矩传感器。", "motivation": "现有方法可能需要复杂的摩擦模型或通过电机电流估计扭矩，导致精度不足。本研究旨在通过直接扭矩测量，消除对复杂摩擦模型的需求，并提供比基于电机电流估计更准确的扭矩读数，以实现精确的接触检测和定位。", "method": "研究采用广义动量观测器框架进行接触检测和定位。核心方法包括：1) 设计了一种低成本的基于应变计的关节扭矩传感器，可直接测量扭矩；2) 结合使用分布式关节扭矩传感器和单个髋部力-扭矩传感器；3) 在浮动基座2自由度机器人腿上进行仿真研究；4) 对扭矩传感器进行校准；5) 在2自由度机械臂上进行硬件实验验证。", "result": "仿真研究验证了所提出的框架能准确恢复大腿和小腿连杆上的接触力和位置。扭矩传感器经过校准后，相对于真实值测量，平均精度达到96.4%。硬件实验结果显示，接触定位精度达到亚厘米级别，力误差低于0.2 N。", "conclusion": "所提出的基于广义动量观测器框架的接触检测和定位方法，结合自主设计的低成本高精度关节扭矩传感器，能够有效且准确地检测和定位机器人腿部的接触，在仿真和硬件实验中均表现出优异的性能。"}}
{"id": "2510.10152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10152", "abs": "https://arxiv.org/abs/2510.10152", "authors": ["Yecong Wan", "Mingwen Shao", "Renlong Wu", "Wangmeng Zuo"], "title": "Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer", "comment": "Project Page https://yecongwan.github.io/Color3D/", "summary": "In this work, we present Color3D, a highly adaptable framework for colorizing\nboth static and dynamic 3D scenes from monochromatic inputs, delivering\nvisually diverse and chromatically vibrant reconstructions with flexible\nuser-guided control. In contrast to existing methods that focus solely on\nstatic scenarios and enforce multi-view consistency by averaging color\nvariations which inevitably sacrifice both chromatic richness and\ncontrollability, our approach is able to preserve color diversity and\nsteerability while ensuring cross-view and cross-time consistency. In\nparticular, the core insight of our method is to colorize only a single key\nview and then fine-tune a personalized colorizer to propagate its color to\nnovel views and time steps. Through personalization, the colorizer learns a\nscene-specific deterministic color mapping underlying the reference view,\nenabling it to consistently project corresponding colors to the content in\nnovel views and video frames via its inherent inductive bias. Once trained, the\npersonalized colorizer can be applied to infer consistent chrominance for all\nother images, enabling direct reconstruction of colorful 3D scenes with a\ndedicated Lab color space Gaussian splatting representation. The proposed\nframework ingeniously recasts complicated 3D colorization as a more tractable\nsingle image paradigm, allowing seamless integration of arbitrary image\ncolorization models with enhanced flexibility and controllability. Extensive\nexperiments across diverse static and dynamic 3D colorization benchmarks\nsubstantiate that our method can deliver more consistent and chromatically rich\nrenderings with precise user control. Project Page\nhttps://yecongwan.github.io/Color3D/.", "AI": {"tldr": "Color3D是一个高度适应性的框架，能够对静态和动态3D场景进行着色，通过个性化着色器和单视图着色范式，实现视觉多样、色彩丰富且用户可控的重建，同时确保跨视图和跨时间的一致性。", "motivation": "现有方法主要关注静态场景，通过平均颜色变化来强制实现多视图一致性，但这不可避免地牺牲了色彩丰富性和可控性。本研究旨在解决如何在保持色彩多样性和可控性的同时，确保跨视图和跨时间的一致性。", "method": "该方法的核心是仅对单个关键视图进行着色，然后微调一个个性化的着色器，将其颜色传播到新视图和时间步。通过个性化，着色器学习参考视图下场景特定的确定性颜色映射，利用其固有的归纳偏差将相应颜色一致地投影到新视图和视频帧中的内容。一旦训练完成，个性化着色器可用于推断所有其他图像的一致色度，并通过专用的Lab色彩空间高斯泼溅表示直接重建彩色3D场景。该框架将复杂的3D着色重塑为更易处理的单图像范式，允许无缝集成任意图像着色模型。", "result": "在各种静态和动态3D着色基准上的大量实验表明，该方法能够提供更一致、色彩更丰富的渲染，并实现精确的用户控制。", "conclusion": "Color3D框架通过将3D着色重新定义为可控的单图像着色问题，并通过个性化着色器实现颜色传播，有效解决了现有方法在色彩丰富性、可控性和一致性方面的局限性，为静态和动态3D场景着色提供了一个灵活且高性能的解决方案。"}}
{"id": "2510.10155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10155", "abs": "https://arxiv.org/abs/2510.10155", "authors": ["Mohamed Hamad", "Muhammad Khan", "Tamer Khattab", "Mohamed Mabrok"], "title": "Stroke Locus Net: Occluded Vessel Localization from MRI Modalities", "comment": "This version of the paper was accepted in the ADMA 2025 conference in\n  Kyoto, Japan", "summary": "A key challenge in ischemic stroke diagnosis using medical imaging is the\naccurate localization of the occluded vessel. Current machine learning methods\nin focus primarily on lesion segmentation, with limited work on vessel\nlocalization. In this study, we introduce Stroke Locus Net, an end-to-end deep\nlearning pipeline for detection, segmentation, and occluded vessel localization\nusing only MRI scans. The proposed system combines a segmentation branch using\nnnUNet for lesion detection with an arterial atlas for vessel mapping and\nidentification, and a generation branch using pGAN to synthesize MRA images\nfrom MRI. Our implementation demonstrates promising results in localizing\noccluded vessels on stroke-affected T1 MRI scans, with potential for faster and\nmore informed stroke diagnosis.", "AI": {"tldr": "本文提出Stroke Locus Net，一个端到端深度学习流水线，仅使用MRI扫描即可实现缺血性卒中病灶检测、分割和闭塞血管定位，有望加速卒中诊断。", "motivation": "缺血性卒中诊断中的一个关键挑战是准确地定位闭塞血管。目前的机器学习方法主要集中在病灶分割，而对血管定位的研究有限。", "method": "引入Stroke Locus Net，一个端到端深度学习流水线。该系统结合了：1) 使用nnUNet进行病灶检测的分割分支；2) 用于血管映射和识别的动脉图谱；3) 使用pGAN从MRI合成MRA图像的生成分支。", "result": "在受卒中影响的T1 MRI扫描上，该实现展示了在定位闭塞血管方面有前景的结果。", "conclusion": "该研究成果具有加速和提供更明智的卒中诊断的潜力。"}}
{"id": "2510.10121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10121", "abs": "https://arxiv.org/abs/2510.10121", "authors": ["Abu Saleh Musa Miah", "Najmul Hassan", "Md Maruf Al Hossain", "Yuichi Okuyama", "Jungpil Shin"], "title": "Multi Class Parkinsons Disease Detection Based on Finger Tapping Using Attention-Enhanced CNN BiLSTM", "comment": null, "summary": "Effective clinical management and intervention development depend on accurate\nevaluation of Parkinsons disease (PD) severity. Many researchers have worked on\ndeveloping gesture-based PD recognition systems; however, their performance\naccuracy is not satisfactory. In this study, we propose a multi-class Parkinson\nDisease detection system based on finger tapping using an attention-enhanced\nCNN BiLSTM. We collected finger tapping videos and derived temporal, frequency,\nand amplitude based features from wrist and hand movements. Then, we proposed a\nhybrid deep learning framework integrating CNN, BiLSTM, and attention\nmechanisms for multi-class PD severity classification from video-derived motion\nfeatures. First, the input sequence is reshaped and passed through a Conv1D\nMaxPooling block to capture local spatial dependencies. The resulting feature\nmaps are fed into a BiLSTM layer to model temporal dynamics. An attention\nmechanism focuses on the most informative temporal features, producing a\ncontext vector that is further processed by a second BiLSTM layer. CNN-derived\nfeatures and attention-enhanced BiLSTM outputs are concatenated, followed by\ndense and dropout layers, before the final softmax classifier outputs the\npredicted PD severity level. The model demonstrated strong performance in\ndistinguishing between the five severity classes, suggesting that integrating\nspatial temporal representations with attention mechanisms can improve\nautomated PD severity detection, making it a promising non-invasive tool to\nsupport clinicians in PD monitoring and progression tracking.", "AI": {"tldr": "本研究提出了一种基于手指敲击视频的多类别帕金森病（PD）严重程度检测系统，该系统利用注意力增强的CNN BiLSTM混合深度学习框架，实现了对五种PD严重程度的有效区分。", "motivation": "帕金森病严重程度的准确评估对于有效的临床管理和干预开发至关重要。然而，许多现有的基于手势的PD识别系统性能准确性不尽如人意。", "method": "研究者收集了手指敲击视频，并从中提取了基于手腕和手部运动的时间、频率和幅度特征。然后，他们提出了一个混合深度学习框架，该框架整合了CNN（用于捕获局部空间依赖）、BiLSTM（用于建模时间动态）和注意力机制（用于聚焦最具信息量的时间特征）。最终，将CNN提取的特征与注意力增强的BiLSTM输出进行拼接，并通过全连接层和softmax分类器输出预测的PD严重程度。", "result": "该模型在区分五种严重程度类别方面表现出强大的性能。", "conclusion": "研究表明，将时空表示与注意力机制相结合可以提高帕金森病严重程度的自动化检测能力，使其成为一种有前景的非侵入性工具，可支持临床医生进行PD监测和疾病进展追踪。"}}
{"id": "2510.10142", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10142", "abs": "https://arxiv.org/abs/2510.10142", "authors": ["Tingxu Han", "Wei Song", "Ziqi Ding", "Ziming Li", "Chunrong Fang", "Yuekang Li", "Dongfang Liu", "Zhenyu Chen", "Zhenting Wang"], "title": "DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly mediate decisions in domains where\nunfair treatment of demographic groups is unacceptable. Existing work probes\nwhen biased outputs appear, but gives little insight into the mechanisms that\ngenerate them, leaving existing mitigations largely fragile. In this paper, we\nconduct a systematic investigation LLM unfairness and propose DiffHeads, a\nlightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)\nprompting to Chain-of-Thought (CoT) prompting across eight representative open-\nand closed-source LLMs. DA will trigger the nature bias part of LLM and improve\nmeasured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.\nNext, we define a token-to-head contribution score that traces each token's\ninfluence back to individual attention heads. This reveals a small cluster of\nbias heads that activate under DA but stay largely dormant with CoT, providing\nthe first causal link between prompting strategy and bias emergence. Finally,\nbuilding on this insight, we propose DiffHeads that identifies bias heads\nthrough differential activation analysis between DA and CoT, and selectively\nmasks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under\nDA and CoT, respectively, without harming model utility.", "AI": {"tldr": "本文系统研究了大型语言模型（LLM）中的不公平性，发现直接回答（DA）提示会显著增加偏见。通过定义token-to-head贡献分数，作者识别出DA提示下激活的“偏见注意力头”，并提出了DiffHeads框架，通过选择性掩盖这些偏见注意力头来有效减少不公平性，同时不损害模型效用。", "motivation": "大型语言模型越来越多地在对公平性要求高的领域中进行决策，而现有研究虽然能发现偏见输出，但对生成偏见的机制缺乏深入理解，导致现有的缓解措施往往脆弱且不可靠。", "method": "研究方法包括：1) 比较直接回答（DA）提示和思维链（CoT）提示在八种LLM上的不公平性表现。2) 定义一个token-to-head贡献分数，以追踪每个token对单个注意力头的影响。3) 通过分析DA和CoT之间激活差异来识别“偏见注意力头”。4) 基于这些发现，提出了DiffHeads框架，通过选择性掩盖这些偏见注意力头来去偏。", "result": "主要结果包括：1) DA提示会触发LLM的固有偏见部分，导致不公平性测量值在单轮和两轮对话中分别增加534.5%和391.9%。2) 发现一小部分“偏见注意力头”在DA提示下激活，而在CoT提示下则大部分处于休眠状态，首次建立了提示策略与偏见产生之间的因果联系。3) DiffHeads框架在DA和CoT提示下分别将不公平性降低了49.4%和40.3%，且不损害模型效用。", "conclusion": "研究揭示了LLM中偏见产生的机制，特别是DA提示如何激活特定的“偏见注意力头”。基于这一洞察，提出的DiffHeads框架能够通过选择性掩盖这些偏见注意力头，有效地减轻LLM的不公平性，为LLM的去偏提供了轻量且有效的解决方案。"}}
{"id": "2510.10157", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10157", "abs": "https://arxiv.org/abs/2510.10157", "authors": ["Tsung-Min Pai", "Jui-I Wang", "Li-Chun Lu", "Shao-Hua Sun", "Hung-Yi Lee", "Kai-Wei Chang"], "title": "BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation", "comment": null, "summary": "Multi-LLM systems enhance the creativity of large language models by\nsimulating human collective intelligence but suffer from significant drawbacks,\nsuch as high computational costs and inference latency. To address these\nlimitations, we propose BILLY (BlendIng persona vectors for Large Language\nmodel creativitY), a training-free framework that captures the benefits of\nmulti-LLM collaboration, i.e. inducing diverse perspectives and specialized\nexpertise, within a single model. BILLY operates by extracting and blending\nmultiple distinct persona vectors directly in the model's activation space. We\nsteer the model's generation process with this merged vector while inference,\nenabling multi-perspective output without explicit multi-LLM communication. Our\nexperiments across creativity-oriented benchmarks demonstrate that BILLY\nsurpasses single model prompting and traditional multi-LLM approaches, while\nsubstantially reducing inference time and computational costs. Our analyses\nfurther reveal that distinct persona vectors can be blended to achieve both\neffective control over complementary aspects of generation and greater\ninterpretability.", "AI": {"tldr": "BILLY是一个无需训练的框架，通过在单个大型语言模型中融合多个角色向量，模拟多LLM协作的创造力，同时显著降低计算成本和推理延迟。", "motivation": "多LLM系统虽然能增强创造力，但存在计算成本高和推理延迟大的显著缺点。", "method": "BILLY通过直接在模型的激活空间中提取并融合多个不同的角色向量。在推理过程中，使用这个融合向量来引导模型的生成过程，从而在没有显式多LLM通信的情况下实现多视角输出。", "result": "在以创造力为导向的基准测试中，BILLY超越了单模型提示和传统的多LLM方法，并显著减少了推理时间和计算成本。分析表明，融合不同的角色向量可以有效控制生成互补方面并提高可解释性。", "conclusion": "BILLY提供了一种在单个模型中实现多LLM协作创造力的有效且经济的方法，同时提供了对生成过程的更好控制和可解释性。"}}
{"id": "2510.10703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10703", "abs": "https://arxiv.org/abs/2510.10703", "authors": ["Xiangyu Wang", "Haocheng Yang", "Fengxiang Cheng", "Fenrong Liu"], "title": "Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning", "comment": null, "summary": "Large Language Models (LLMs) still struggle with complex logical reasoning.\nWhile previous works achieve remarkable improvements, their performance is\nhighly dependent on the correctness of translating natural language (NL)\nproblems into a symbolic language (SL). Though numerous works focusing on\nimproving this translation accuracy, they only consider the similarity between\nthe meaning of SL and NL, overlooking another crucial influencing factor, the\nselection of the target SL type itself. For example, first-order logic language\nspecializes in logical reasoning with categorical syllogisms and complex\nquantifiers, while Boolean satisfiability formalism excels at representing\nconstraint satisfaction like partial problems. To our knowledge, this is the\nfirst paper to claim and verify that different NL logical reasoning problem\ncorresponds to different optimal SL formalization for translation. Based on\nthis, we propose a methods to improve the logical reasoning performance of LLMs\nby adaptively selecting the most suitable SL for each problem prior to\ntranslation. Specifically, we leverage LLMs to select the target SL among\nfirst-order logic, logic programming and Boolean satisfiability and then\ntranslate the problem in NL to target SL expressions as well as employ the\ncorresponding logical solver to derive the final answer. Experimental results\non benchmarks show that our adaptive selection method significantly outperforms\ntranslating all into single SL and randomly selecting the SL. On a mixed\ndataset of these benchmarks, our approach achieves 96% accuracy, which\nimproving performance by 25% compared to the second highest accuracy from the\nfirst-order logic translation.", "AI": {"tldr": "该论文提出了一种自适应选择符号语言（SL）的方法，以提高大型语言模型（LLMs）在复杂逻辑推理任务中的表现，因为不同的自然语言（NL）问题对应着不同的最优SL形式化。", "motivation": "LLMs在复杂逻辑推理方面表现不佳，现有方法过度依赖自然语言到符号语言翻译的准确性，但忽略了目标符号语言类型选择的重要性。不同的SL（如一阶逻辑、布尔可满足性）擅长处理不同类型的逻辑问题。", "method": "本文首先主张并验证了不同自然语言逻辑推理问题对应着不同的最优符号语言形式化。在此基础上，提出了一种方法，利用LLMs在翻译前自适应地从一阶逻辑、逻辑编程和布尔可满足性中选择最适合每个问题的目标SL，然后将问题翻译成选定的SL表达式，并使用相应的逻辑求解器得出答案。", "result": "实验结果表明，自适应选择方法显著优于将所有问题翻译成单一SL或随机选择SL。在混合数据集上，该方法达到了96%的准确率，比第二高的一阶逻辑翻译方法提高了25%。", "conclusion": "自适应地为不同的自然语言逻辑推理问题选择最优的符号语言，可以显著提升LLMs在复杂逻辑推理任务中的性能。"}}
{"id": "2510.10865", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.10865", "abs": "https://arxiv.org/abs/2510.10865", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments", "comment": "17 pages, 5 figures, 8 tables", "summary": "Robots navigating dynamic, cluttered, and semantically complex environments\nmust integrate perception, symbolic reasoning, and spatial planning to\ngeneralize across diverse layouts and object categories. Existing methods often\nrely on static priors or limited memory, constraining adaptability under\npartial observability and semantic ambiguity. We present GRIP, Grid-based Relay\nwith Intermediate Planning, a unified, modular framework with three scalable\nvariants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic\noccupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and\nLLM-based introspection; and GRIP-R (Real-World), enabling physical robot\ndeployment under perceptual uncertainty. GRIP integrates dynamic 2D grid\nconstruction, open-vocabulary object grounding, co-occurrence-aware symbolic\nplanning, and hybrid policy execution using behavioral cloning, D* search, and\ngrid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks\nshow that GRIP achieves up to 9.6% higher success rates and over $2\\times$\nimprovement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative\nanalyses reveal interpretable symbolic plans in ambiguous scenes. Real-world\ndeployment on a Jetbot further validates GRIP's generalization under sensor\nnoise and environmental variation. These results position GRIP as a robust,\nscalable, and explainable framework bridging simulation and real-world\nnavigation.", "AI": {"tldr": "GRIP是一个统一的、模块化的框架，通过整合感知、符号推理和空间规划，使机器人在动态、复杂环境中进行泛化导航，并在模拟和现实世界中取得了显著性能提升。", "motivation": "现有方法在动态、杂乱和语义复杂的环境中导航时，常依赖静态先验或有限记忆，导致在部分可观测性和语义模糊性下适应性受限。", "method": "GRIP框架包含三个可扩展变体（GRIP-L、GRIP-F、GRIP-R）。它整合了动态2D网格构建、开放词汇对象定位、共现感知符号规划，以及结合行为克隆、D*搜索和网格条件控制的混合策略执行。GRIP-F还支持多跳锚链和基于LLM的内省。", "result": "在AI2-THOR和RoboTHOR基准测试中，GRIP在长距离任务上成功率提高了9.6%，路径效率（SPL和SAE）提高了2倍以上。定性分析揭示了在模糊场景中可解释的符号规划。在Jetbot上的真实世界部署进一步验证了GRIP在传感器噪声和环境变化下的泛化能力。", "conclusion": "GRIP被定位为一个鲁棒、可扩展且可解释的框架，成功弥合了模拟和现实世界导航之间的鸿沟。"}}
{"id": "2510.10159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10159", "abs": "https://arxiv.org/abs/2510.10159", "authors": ["Jaap Jumelet", "Abdellah Fourtassi", "Akari Haga", "Bastian Bunzeck", "Bhargav Shandilya", "Diana Galvan-Sosa", "Faiz Ghifari Haznitrama", "Francesca Padovani", "Francois Meyer", "Hai Hu", "Julen Etxaniz", "Laurent Prévot", "Linyang He", "María Grandury", "Mila Marcheva", "Negar Foroutan", "Nikitas Theodoropoulos", "Pouya Sadeghi", "Siyuan Song", "Suchir Salhan", "Susana Zhou", "Yurii Paniv", "Ziyin Zhang", "Arianna Bisazza", "Alex Warstadt", "Leshem Choshen"], "title": "BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data", "comment": null, "summary": "We present BabyBabelLM, a multilingual collection of datasets modeling the\nlanguage a person observes from birth until they acquire a native language. We\ncurate developmentally plausible pretraining data aiming to cover the\nequivalent of 100M English words of content in each of 45 languages. We compile\nevaluation suites and train baseline models in each language. BabyBabelLM aims\nto facilitate multilingual pretraining and cognitive modeling.", "AI": {"tldr": "BabyBabelLM是一个多语言数据集集合，模拟个人从出生到习得母语所观察到的语言，旨在促进多语言预训练和认知建模。", "motivation": "研究动机是需要开发上合理的预训练数据，以模拟一个人从出生到习得母语期间所接触的语言。", "method": "研究方法是策划多语言数据集，覆盖45种语言，每种语言的内容相当于1亿英文单词，并编译评估套件，以及训练每种语言的基线模型。", "result": "结果是创建了BabyBabelLM，这是一个多语言数据集集合，包含了45种语言的开发上合理预训练数据、评估套件和基线模型。", "conclusion": "BabyBabelLM旨在促进多语言预训练和认知建模研究。"}}
{"id": "2510.10701", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.10701", "abs": "https://arxiv.org/abs/2510.10701", "authors": ["Yang Xu", "Shuwei Chen", "Jun Liu", "Feng Cao", "Xingxing He"], "title": "Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction", "comment": "38 pages, 8 figures", "summary": "Automated deduction lies at the core of Artificial Intelligence (AI),\nunderpinning theorem proving, formal verification, and logical reasoning.\nDespite decades of progress, reconciling deductive completeness with\ncomputational efficiency remains an enduring challenge. Traditional reasoning\ncalculi, grounded in binary resolution, restrict inference to pairwise clause\ninteractions and thereby limit deductive synergy among multiple clauses. The\nContradiction Separation Extension (CSE) framework, introduced in 2018,\nproposed a dynamic multi-clause reasoning theory that redefined logical\ninference as a process of contradiction separation rather than sequential\nresolution. While that work established the theoretical foundation, its\nalgorithmic realization remained unformalized and unpublished. This work\npresents the Extended Triangular Method (ETM), a generalized\ncontradiction-construction algorithm that formalizes and extends the internal\nmechanisms of contradiction separation. The ETM unifies multiple\ncontradiction-building strategies, including the earlier Standard Extension\nmethod, within a triangular geometric framework that supports flexible clause\ninteraction and dynamic synergy. ETM serves as the algorithmic core of several\nhigh-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose\ncompetitive results in standard first-order benchmarks (TPTP problem sets and\nCASC 2018-2015) empirically validate the effectiveness and generality of the\nproposed approach. By bridging theoretical abstraction and operational\nimplementation, ETM advances the contradiction separation paradigm into a\ngeneralized, scalable, and practically competitive model for automated\nreasoning, offering new directions for future research in logical inference and\ntheorem proving.", "AI": {"tldr": "本文提出扩展三角法（ETM），一种形式化矛盾分离扩展（CSE）框架的算法，用于自动化推理。ETM支持多子句推理，经实证验证有效，并提升了定理证明器的性能。", "motivation": "自动化推理面临完备性与计算效率的挑战，传统二元归结法限制了多子句协同推理。2018年提出的矛盾分离扩展（CSE）理论框架缺乏形式化的算法实现，是本文的研究动机。", "method": "本文提出了扩展三角法（ETM），这是一种广义的矛盾构建算法，用于形式化和扩展矛盾分离的内部机制。ETM在一个三角几何框架内统一了包括早期标准扩展法在内的多种矛盾构建策略，支持灵活的子句交互和动态协同。", "result": "ETM作为CSE、CSE-E、CSI-E和CSI-Enig等高性能定理证明器的算法核心。这些证明器在标准一阶基准测试（TPTP问题集和CASC 2018-2015）中取得了有竞争力的结果，经验性地验证了该方法的有效性和普适性。", "conclusion": "ETM通过连接理论抽象与操作实现，将矛盾分离范式推进为一个通用、可扩展且具有实际竞争力的自动化推理模型，为逻辑推理和定理证明的未来研究提供了新方向。"}}
{"id": "2510.10813", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10813", "abs": "https://arxiv.org/abs/2510.10813", "authors": ["Enric Junque de Fortuny", "Veronica Roberta Cappelli"], "title": "LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to domains that require\nreasoning about other agents' behavior, such as negotiation, policy design, and\nmarket simulation, yet existing research has mostly evaluated their adherence\nto equilibrium play or their exhibited depth of reasoning. Whether they display\ngenuine strategic thinking, understood as the coherent formation of beliefs\nabout other agents, evaluation of possible actions, and choice based on those\nbeliefs, remains unexplored. We develop a framework to identify this ability by\ndisentangling beliefs, evaluation, and choice in static, complete-information\ngames, and apply it across a series of non-cooperative environments. By jointly\nanalyzing models' revealed choices and reasoning traces, and introducing a new\ncontext-free game to rule out imitation from memorization, we show that current\nfrontier models exhibit belief-coherent best-response behavior at targeted\nreasoning depths. When unconstrained, they self-limit their depth of reasoning\nand form differentiated conjectures about human and synthetic opponents,\nrevealing an emergent form of meta-reasoning. Under increasing complexity,\nexplicit recursion gives way to internally generated heuristic rules of choice\nthat are stable, model-specific, and distinct from known human biases. These\nfindings indicate that belief coherence, meta-reasoning, and novel heuristic\nformation can emerge jointly from language modeling objectives, providing a\nstructured basis for the study of strategic cognition in artificial agents.", "AI": {"tldr": "本研究通过分解信念、评估和选择，发现大型语言模型（LLMs）在战略博弈中表现出信念连贯的最佳响应行为、元推理能力和独特的启发式规则，这表明它们具备新兴的战略认知能力。", "motivation": "尽管LLMs被广泛应用于需要推理其他智能体行为的领域（如谈判、政策设计），但现有研究主要评估它们是否遵循均衡博弈或其推理深度。LLMs是否展现出真正的战略思维（即连贯地形成对其他智能体的信念、评估可能行动并基于这些信念做出选择）仍未被探索。", "method": "研究开发了一个框架，用于在静态、完全信息博弈中分离信念、评估和选择，并将其应用于一系列非合作环境。通过联合分析模型的选择和推理轨迹，并引入一种新的无上下文博弈来排除模仿记忆，以识别LLMs的战略思维能力。", "result": "前沿LLMs在特定推理深度下表现出信念连贯的最佳响应行为。在无约束条件下，它们会自我限制推理深度，并对人类和合成对手形成差异化的推测，这揭示了一种新兴的元推理形式。在复杂性增加时，显式递归会让位于内部生成的启发式选择规则，这些规则稳定、特定于模型且不同于已知的人类偏见。", "conclusion": "信念连贯性、元推理和新颖的启发式规则形成可以共同从语言建模目标中涌现。这些发现为研究人工智能体中的战略认知提供了结构化基础。"}}
{"id": "2510.10156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10156", "abs": "https://arxiv.org/abs/2510.10156", "authors": ["Benjia Zhou", "Bin Fu", "Pei Cheng", "Yanru Wang", "Jiayuan Fan", "Tao Chen"], "title": "ReMix: Towards a Unified View of Consistent Character Generation and Editing", "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models (e.g., FLUX.1)\nhave greatly improved visual fidelity in consistent character generation and\nediting. However, existing methods rarely unify these tasks within a single\nframework. Generation-based approaches struggle with fine-grained identity\nconsistency across instances, while editing-based methods often lose spatial\ncontrollability and instruction alignment. To bridge this gap, we propose\nReMix, a unified framework for character-consistent generation and editing. It\nconstitutes two core components: the ReMix Module and IP-ControlNet. The ReMix\nModule leverages the multimodal reasoning ability of MLLMs to edit semantic\nfeatures of input images and adapt instruction embeddings to the native DiT\nbackbone without fine-tuning. While this ensures coherent semantic layouts,\npixel-level consistency and pose controllability remain challenging. To address\nthis, IP-ControlNet extends ControlNet to decouple semantic and layout cues\nfrom reference images and introduces an {\\epsilon}-equivariant latent space\nthat jointly denoises the reference and target images within a shared noise\nspace. Inspired by convergent evolution and quantum decoherence,i.e., where\nenvironmental noise drives state convergence, this design promotes feature\nalignment in the hidden space, enabling consistent object generation while\npreserving identity. ReMix supports a wide range of tasks, including\npersonalized generation, image editing, style transfer, and multi-condition\nsynthesis. Extensive experiments validate its effectiveness and efficiency as a\nunified framework for character-consistent image generation and editing.", "AI": {"tldr": "ReMix是一个统一的框架，旨在解决现有方法在角色一致性生成和编辑方面的不足，通过引入ReMix模块和IP-ControlNet实现语义和像素级别的一致性。", "motivation": "尽管大规模文本到图像扩散模型在视觉保真度上取得了进步，但在单个框架内统一角色一致性生成和编辑仍然存在挑战。基于生成的模型难以在不同实例间保持精细的身份一致性，而基于编辑的方法则常失去空间可控性和指令对齐能力。", "method": "ReMix框架包含两个核心组件：1. ReMix模块：利用多模态大语言模型（MLLMs）的推理能力编辑输入图像的语义特征，并将指令嵌入适应到DiT骨干网络，无需微调，以确保连贯的语义布局。2. IP-ControlNet：扩展了ControlNet，用于解耦参考图像中的语义和布局线索，并引入了一个ε-等变潜在空间，在共享噪声空间内共同去噪参考和目标图像，从而促进隐藏空间中的特征对齐，实现像素级一致性和姿态可控性，灵感来源于趋同进化和量子退相干。", "result": "ReMix支持广泛的任务，包括个性化生成、图像编辑、风格迁移和多条件合成。实验证明其作为角色一致性图像生成和编辑的统一框架，具有有效性和效率。", "conclusion": "ReMix成功地将角色一致性生成和编辑任务统一到一个框架中，通过其创新的模块设计解决了现有方法的局限性，并在多种任务中展现了卓越的性能。"}}
{"id": "2510.10886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10886", "abs": "https://arxiv.org/abs/2510.10886", "authors": ["Yashom Dighe", "Youngjin Kim", "Karthik Dantu"], "title": "QuayPoints: A Reasoning Framework to Bridge the Information Gap Between Global and Local Planning in Autonomous Racing", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Autonomous racing requires tight integration between perception, planning and\ncontrol to minimize latency as well as timely decision making. A standard\nautonomy pipeline comprising a global planner, local planner, and controller\nloses information as the higher-level racing context is sequentially propagated\ndownstream into specific task-oriented context. In particular, the global\nplanner's understanding of optimality is typically reduced to a sparse set of\nwaypoints, leaving the local planner to make reactive decisions with limited\ncontext. This paper investigates whether additional global insights,\nspecifically time-optimality information, can be meaningfully passed to the\nlocal planner to improve downstream decisions. We introduce a framework that\npreserves essential global knowledge and conveys it to the local planner\nthrough QuayPoints regions where deviations from the optimal raceline result in\nsignificant compromises to optimality. QuayPoints enable local planners to make\nmore informed global decisions when deviating from the raceline, such as during\nstrategic overtaking. To demonstrate this, we integrate QuayPoints into an\nexisting planner and show that it consistently overtakes opponents traveling at\nup to 75% of the ego vehicle's speed across four distinct race tracks.", "AI": {"tldr": "为解决自动驾驶赛车中全局规划信息丢失问题，本文提出“QuayPoints”框架，将时间最优性信息传递给局部规划器，从而显著提升了赛车在超车等场景下的决策能力。", "motivation": "标准的自动驾驶赛车管线（全局规划、局部规划、控制）在信息向下游传递时会丢失高层赛车上下文，特别是全局规划器对最优性的理解常被简化为稀疏路点，导致局部规划器在有限上下文中做出反应性决策，影响整体最优性，尤其是在超车等复杂场景。", "method": "本文引入了一个框架，通过“QuayPoints”区域将关键的全局知识（特别是时间最优性信息）传递给局部规划器。QuayPoints标识出偏离最优赛道线会显著损害最优性的区域，使局部规划器在偏离赛道线时（如策略性超车）能做出更明智的全局决策。", "result": "将QuayPoints集成到现有规划器中，结果表明，在四个不同的赛道上，该系统能稳定超越速度达到自车75%的对手。", "conclusion": "QuayPoints框架成功地将全局时间最优性洞察传递给局部规划器，使其在偏离赛道线时能做出更明智的全局决策，显著提升了自动驾驶赛车在战略性超车等场景下的性能。"}}
{"id": "2510.10161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10161", "abs": "https://arxiv.org/abs/2510.10161", "authors": ["Liang Pang", "Kangxi Wu", "Sunhao Dai", "Zihao Wei", "Zenghao Duan", "Jia Gu", "Xiang Li", "Zhiyi Yin", "Jun Xu", "Huawei Shen", "Xueqi Cheng"], "title": "Large Language Model Sourcing: A Survey", "comment": "31 pages", "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nartificial intelligence, shifting from supporting objective tasks (e.g.,\nrecognition) to empowering subjective decision-making (e.g., planning,\ndecision). This marks the dawn of general and powerful AI, with applications\nspanning a wide range of fields, including programming, education, healthcare,\nfinance, and law. However, their deployment introduces multifaceted risks. Due\nto the black-box nature of LLMs and the human-like quality of their generated\ncontent, issues such as hallucinations, bias, unfairness, and copyright\ninfringement become particularly significant. In this context, sourcing\ninformation from multiple perspectives is essential.\n  This survey presents a systematic investigation into provenance tracking for\ncontent generated by LLMs, organized around four interrelated dimensions that\ntogether capture both model- and data-centric perspectives. From the model\nperspective, Model Sourcing treats the model as a whole, aiming to distinguish\ncontent generated by specific LLMs from content authored by humans. Model\nStructure Sourcing delves into the internal generative mechanisms, analyzing\narchitectural components that shape the outputs of model. From the data\nperspective, Training Data Sourcing focuses on internal attribution, tracing\nthe origins of generated content back to the training data of model. In\ncontrast, External Data Sourcing emphasizes external validation, identifying\nexternal information used to support or influence the responses of model.\nMoreover, we also propose a dual-paradigm taxonomy that classifies existing\nsourcing methods into prior-based (proactive traceability embedding) and\nposterior-based (retrospective inference) approaches. Traceability across these\ndimensions enhances the transparency, accountability, and trustworthiness of\nLLMs deployment in real-world applications.", "AI": {"tldr": "本综述系统性地研究了大型语言模型（LLMs）生成内容的溯源追踪，提出了一个围绕模型和数据视角的四维度框架，并分类了溯源方法，以增强LLMs的透明度、可信赖性和问责制。", "motivation": "LLMs的快速发展使其应用从客观任务转向主观决策，但其黑盒特性和类人内容带来了幻觉、偏见、不公平和版权侵犯等多方面风险。因此，从多角度溯源信息对于提高LLMs的透明度、问责制和可信赖性至关重要。", "method": "本研究通过系统性调查，将LLMs生成内容的溯源追踪分为四个相互关联的维度：模型溯源（区分LLM生成与人类创作）、模型结构溯源（分析内部生成机制）、训练数据溯源（追溯到模型的训练数据）和外部数据溯源（识别外部支持信息）。此外，还提出了一个双范式分类法，将现有溯源方法分为基于先验（主动嵌入可追溯性）和基于后验（回顾性推断）的方法。", "result": "本综述提出了一个全面的LLMs内容溯源追踪框架，涵盖了模型和数据两个主要视角下的四个细分维度，并引入了一个双范式分类法来组织和理解现有的溯源方法。该框架有助于系统性地分析和解决LLMs内容溯源问题。", "conclusion": "通过在这些维度上实现可追溯性，可以显著增强LLMs在实际应用中的透明度、问责制和可信赖性，从而有效应对其部署所带来的多重风险。"}}
{"id": "2510.10815", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.10815", "abs": "https://arxiv.org/abs/2510.10815", "authors": ["Meiru Zhang", "Philipp Borchert", "Milan Gritta", "Gerasimos Lampouras"], "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems", "comment": null, "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.", "AI": {"tldr": "LLM在数学形式化方面面临挑战，难以识别和利用先决知识。DRIFT框架通过分解非正式语句为子组件，实现有针对性的前提检索和示例定理检索，显著提高了前提检索的F1分数，并在OOD基准上表现出色。", "motivation": "大型语言模型（LLMs）在自动化数学语句形式化以进行定理证明时，难以识别和利用先决数学知识及其在Lean等语言中的形式表示。当前的检索增强型自动形式化方法直接使用非正式语句查询外部库，但忽略了一个基本限制：非正式数学语句通常复杂且提供有限的底层数学概念上下文。", "method": "本文引入了DRIFT框架，该框架使LLMs能够将非正式数学语句分解为更小、更易处理的“子组件”，从而促进从Mathlib等数学库中进行有针对性的前提检索。此外，DRIFT还会检索示例定理，以帮助模型在形式化任务中更有效地使用前提。", "result": "DRIFT在ProofNet、ConNF和MiniF2F-test等不同基准上持续改进了前提检索，与DPR基线相比，在ProofNet上的F1分数几乎翻倍。值得注意的是，DRIFT在分布外ConNF基准上表现强劲，使用GPT-4.1和DeepSeek-V3.1分别实现了37.14%和42.25%的BEq+@10改进。", "conclusion": "分析表明，数学自动形式化中的检索有效性严重依赖于模型特定的知识边界，这凸显了需要根据每个模型的能力调整自适应检索策略的重要性。"}}
{"id": "2510.10160", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10160", "abs": "https://arxiv.org/abs/2510.10160", "authors": ["Zhenjie Mao", "Yuhuan Yang", "Chaofan Ma", "Dongsheng Jiang", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation", "comment": "NeurIPS 2025", "summary": "Referring Image Segmentation (RIS) aims to segment the target object in an\nimage given a natural language expression. While recent methods leverage\npre-trained vision backbones and more training corpus to achieve impressive\nresults, they predominantly focus on simple expressions--short, clear noun\nphrases like \"red car\" or \"left girl\". This simplification often reduces RIS to\na key word/concept matching problem, limiting the model's ability to handle\nreferential ambiguity in expressions. In this work, we identify two challenging\nreal-world scenarios: object-distracting expressions, which involve multiple\nentities with contextual cues, and category-implicit expressions, where the\nobject class is not explicitly stated. To address the challenges, we propose a\nnovel framework, SaFiRe, which mimics the human two-phase cognitive\nprocess--first forming a global understanding, then refining it through\ndetail-oriented inspection. This is naturally supported by Mamba's\nscan-then-update property, which aligns with our phased design and enables\nefficient multi-cycle refinement with linear complexity. We further introduce\naRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous\nreferring expressions. Extensive experiments on both standard and proposed\ndatasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.", "AI": {"tldr": "本文提出了一种名为SaFiRe的新框架，旨在解决指代图像分割（RIS）中复杂的、模糊的自然语言表达所带来的挑战。SaFiRe模仿人类两阶段认知过程，并利用Mamba的特性进行高效多周期细化。同时，引入了新的基准aRefCOCO以评估模型在模糊表达下的性能。", "motivation": "现有的指代图像分割（RIS）方法主要关注简单的表达，如“红色汽车”或“左边的女孩”，这使得RIS问题简化为关键词匹配，限制了模型处理指代模糊性表达的能力。实际场景中存在两种挑战性情况：涉及多个实体和上下文线索的“物体干扰性表达”以及未明确说明物体类别的“类别隐含性表达”。", "method": "提出SaFiRe框架，模仿人类两阶段认知过程：首先形成全局理解，然后通过细节导向的检查进行细化。该设计利用Mamba的“扫描-更新”特性，实现高效且具有线性复杂度的多周期细化。此外，还引入了aRefCOCO，一个新的基准数据集，专门用于评估RIS模型在模糊指代表达下的性能。", "result": "在标准数据集和新提出的aRefCOCO数据集上进行的广泛实验表明，SaFiRe框架优于现有的最先进基线方法。", "conclusion": "SaFiRe通过其模仿人类认知过程的两阶段设计和利用Mamba特性的高效细化机制，有效解决了指代图像分割中模糊表达的挑战，并在实验中展现出卓越的性能。"}}
{"id": "2510.10823", "categories": ["cs.AI", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10823", "abs": "https://arxiv.org/abs/2510.10823", "authors": ["Daniel Howard"], "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety", "comment": "41 pages, 17 figures, 5 tables", "summary": "We present a framework for characterizing neurosis in embodied AI: behaviors\nthat are internally coherent yet misaligned with reality, arising from\ninteractions among planning, uncertainty handling, and aversive memory. In a\ngrid navigation stack we catalogue recurrent modalities including flip-flop,\nplan churn, perseveration loops, paralysis and hypervigilance, futile search,\nbelief incoherence, tie break thrashing, corridor thrashing, optimality\ncompulsion, metric mismatch, policy oscillation, and limited-visibility\nvariants. For each we give lightweight online detectors and reusable escape\npolicies (short commitments, a margin to switch, smoothing, principled\narbitration). We then show that durable phobic avoidance can persist even under\nfull visibility when learned aversive costs dominate local choice, producing\nlong detours despite globally safe routes. Using First/Second/Third Law as\nengineering shorthand for safety latency, command compliance, and resource\nefficiency, we argue that local fixes are insufficient; global failures can\nremain. To surface them, we propose genetic-programming based destructive\ntesting that evolves worlds and perturbations to maximize law pressure and\nneurosis scores, yielding adversarial curricula and counterfactual traces that\nexpose where architectural revision, not merely symptom-level patches, is\nrequired.", "AI": {"tldr": "该论文提出了一个框架，用于表征具身AI中的“神经症”行为（内部一致但与现实不符），并利用破坏性测试来发现这些深层架构问题。", "motivation": "具身AI在规划、不确定性处理和厌恶记忆交互作用下，可能产生内部连贯但与现实不符的行为（即“神经症”）。现有局部修复不足以解决这些全局性故障，需要更深层次的分析和解决。", "method": "1. 提出了一个表征具身AI（以网格导航为例）中神经症行为的框架。2. 编目了反复出现的神经症模式（如犹豫不决、规划流失、僵局等），并为其设计了轻量级在线检测器和可重用逃逸策略。3. 引入了基于遗传编程的破坏性测试，通过演化世界和扰动来最大化“法则压力”和“神经症得分”，以生成对抗性课程和反事实轨迹，揭示需要架构修订而非仅仅症状级修补的地方。", "result": "1. 识别并分类了具身AI导航中的多种神经症行为模式。2. 表明即使在完全可见的情况下，学习到的厌恶成本也可能导致持久的恐惧规避行为，产生不必要的长绕路。3. 论证了局部修复不足以解决全局性故障。4. 破坏性测试能够生成对抗性环境，有效地揭示了AI系统需要进行架构层面修订的深层问题。", "conclusion": "具身AI中的“神经症”是一个普遍且深层次的问题，仅靠局部修复无法解决。需要通过架构修订来从根本上解决这些问题，而破坏性测试是一种有效的工具，能够识别出这些需要架构改进的关键点。"}}
{"id": "2510.10893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10893", "abs": "https://arxiv.org/abs/2510.10893", "authors": ["Dikshant Shehmar", "Matthew E. Taylor", "Ehsan Hashemi"], "title": "An Adaptive Transition Framework for Game-Theoretic Based Takeover", "comment": null, "summary": "The transition of control from autonomous systems to human drivers is\ncritical in automated driving systems, particularly due to the out-of-the-loop\n(OOTL) circumstances that reduce driver readiness and increase reaction times.\nExisting takeover strategies are based on fixed time-based transitions, which\nfail to account for real-time driver performance variations. This paper\nproposes an adaptive transition strategy that dynamically adjusts the control\nauthority based on both the time and tracking ability of the driver trajectory.\nShared control is modeled as a cooperative differential game, where control\nauthority is modulated through time-varying objective functions instead of\nblending control torques directly. To ensure a more natural takeover, a\ndriver-specific state-tracking matrix is introduced, allowing the transition to\nalign with individual control preferences. Multiple transition strategies are\nevaluated using a cumulative trajectory error metric. Human-in-the-loop control\nscenarios of the standardized ISO lane change maneuvers demonstrate that\nadaptive transitions reduce trajectory deviations and driver control effort\ncompared to conventional strategies. Experiments also confirm that continuously\nadjusting control authority based on real-time deviations enhances vehicle\nstability while reducing driver effort during takeover.", "AI": {"tldr": "本文提出一种自适应控制过渡策略，根据驾驶员的实时表现动态调整控制权限，以解决自动驾驶系统中固定时间过渡的不足，从而减少轨迹偏差和驾驶员控制负荷。", "motivation": "现有自动驾驶系统中的控制权移交策略基于固定时间，未能考虑驾驶员实时表现的变化，导致驾驶员脱离回路（OOTL）状态下准备不足和反应时间增加。", "method": "该研究提出一种自适应过渡策略，根据驾驶员的时间和轨迹跟踪能力动态调整控制权限。共享控制被建模为合作微分博弈，通过时变目标函数而非直接混合控制扭矩来调节控制权限。为确保更自然的接管，引入了驾驶员特定的状态跟踪矩阵。通过累积轨迹误差指标和人机协作（human-in-the-loop）的ISO变道场景进行评估。", "result": "与传统策略相比，自适应过渡策略显著减少了轨迹偏差和驾驶员的控制负荷。实验还证实，根据实时偏差连续调整控制权限可以增强车辆稳定性，并降低驾驶员在接管过程中的努力。", "conclusion": "所提出的自适应过渡策略通过动态调整控制权限，有效地改善了自动驾驶系统中的控制权移交过程，提高了车辆稳定性并减轻了驾驶员负担，从而实现更安全、更自然的过渡。"}}
{"id": "2510.10163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10163", "abs": "https://arxiv.org/abs/2510.10163", "authors": ["César Borja", "Carlos Plou", "Rubén Martinez-Cantín", "Ana C. Murillo"], "title": "SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation", "comment": null, "summary": "Semantic segmentation is essential to automate underwater imagery analysis\nwith ecology monitoring purposes. Unfortunately, fine grained underwater scene\nanalysis is still an open problem even for top performing segmentation models.\nThe high cost of obtaining dense, expert-annotated, segmentation labels hinders\nthe supervision of models in this domain. While sparse point-labels are easier\nto obtain, they introduce challenges regarding which points to annotate and how\nto propagate the sparse information. We present SparseUWSeg, a novel framework\nthat addresses both issues. SparseUWSeg employs an active sampling strategy to\nguide annotators, maximizing the value of their point labels. Then, it\npropagates these sparse labels with a hybrid approach leverages both the best\nof SAM2 and superpixel-based methods. Experiments on two diverse underwater\ndatasets demonstrate the benefits of SparseUWSeg over state-of-the-art\napproaches, achieving up to +5\\% mIoU over D+NN. Our main contribution is the\ndesign and release of a simple but effective interactive annotation tool,\nintegrating our algorithms. It enables ecology researchers to leverage\nfoundation models and computer vision to efficiently generate high-quality\nsegmentation masks to process their data.", "AI": {"tldr": "SparseUWSeg是一个针对水下图像语义分割的框架，通过主动采样指导标注和混合传播稀疏点标签，解决了标注成本高和信息利用不足的问题，并提供了一个高效的交互式标注工具。", "motivation": "水下图像分析中的语义分割对于生态监测至关重要，但精细粒度分析仍是难题。获取密集、专家标注的分割标签成本高昂，而稀疏点标签虽然易于获取，却面临如何选择标注点以及如何有效传播稀疏信息的挑战。", "method": "本文提出了SparseUWSeg框架，包含两个主要策略：1. 采用主动采样策略指导标注者，以最大化点标签的价值；2. 使用混合方法传播这些稀疏标签，该方法结合了SAM2和基于超像素方法的优点。此外，还设计并发布了一个集成这些算法的简单但有效的交互式标注工具。", "result": "在两个不同的水下数据集上进行的实验表明，SparseUWSeg优于现有最先进的方法，相较于D+NN实现了高达+5%的mIoU提升。", "conclusion": "SparseUWSeg提供了一个简单而有效的交互式标注工具，集成了主动采样和混合标签传播算法。它使生态研究人员能够利用基础模型和计算机视觉高效生成高质量的分割掩模，以处理其数据。"}}
{"id": "2510.10182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10182", "abs": "https://arxiv.org/abs/2510.10182", "authors": ["Kedi Chen", "Dezhao Ruan", "Yuhao Dan", "Yaoting Wang", "Siyu Yan", "Xuecheng Wu", "Yinqi Zhang", "Qin Chen", "Jie Zhou", "Liang He", "Biqing Qi", "Linyang Li", "Qipeng Guo", "Xiaoming Shi", "Wei Zhang"], "title": "A Survey of Inductive Reasoning for Large Language Models", "comment": null, "summary": "Reasoning is an important task for large language models (LLMs). Among all\nthe reasoning paradigms, inductive reasoning is one of the fundamental types,\nwhich is characterized by its particular-to-general thinking process and the\nnon-uniqueness of its answers. The inductive mode is crucial for knowledge\ngeneralization and aligns better with human cognition, so it is a fundamental\nmode of learning, hence attracting increasing interest. Despite the importance\nof inductive reasoning, there is no systematic summary of it. Therefore, this\npaper presents the first comprehensive survey of inductive reasoning for LLMs.\nFirst, methods for improving inductive reasoning are categorized into three\nmain areas: post-training, test-time scaling, and data augmentation. Then,\ncurrent benchmarks of inductive reasoning are summarized, and a unified\nsandbox-based evaluation approach with the observation coverage metric is\nderived. Finally, we offer some analyses regarding the source of inductive\nability and how simple model architectures and data help with inductive tasks,\nproviding a solid foundation for future research.", "AI": {"tldr": "本文首次全面综述了大型语言模型（LLMs）的归纳推理能力，涵盖了改进方法、基准测试、评估策略，并分析了其能力来源。", "motivation": "归纳推理对LLMs的知识泛化和认知对齐至关重要，但目前缺乏对其的系统性总结。", "method": "本文首先将改进LLMs归纳推理能力的方法分为后训练、测试时扩展和数据增强三类。其次，总结了当前的归纳推理基准，并提出了一种基于沙盒的统一评估方法，引入了观察覆盖率指标。最后，分析了归纳能力的来源以及简单模型架构和数据如何帮助归纳任务。", "result": "本研究提供了一个系统的归纳推理改进方法分类，总结了现有基准，提出了一种统一的沙盒评估方法及观察覆盖率指标，并分析了归纳能力的来源和数据/模型架构的影响。", "conclusion": "本综述为未来LLMs在归纳推理领域的研究奠定了坚实的基础。"}}
{"id": "2510.10903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10903", "abs": "https://arxiv.org/abs/2510.10903", "authors": ["Shuanghao Bai", "Wenxuan Song", "Jiayi Chen", "Yuheng Ji", "Zhide Zhong", "Jin Yang", "Han Zhao", "Wanqi Zhou", "Wei Zhao", "Zhe Li", "Pengxiang Ding", "Cheng Chi", "Haoang Li", "Chang Xu", "Xiaolong Zheng", "Donglin Wang", "Shanghang Zhang", "Badong Chen"], "title": "Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey", "comment": null, "summary": "Embodied intelligence has witnessed remarkable progress in recent years,\ndriven by advances in computer vision, natural language processing, and the\nrise of large-scale multimodal models. Among its core challenges, robot\nmanipulation stands out as a fundamental yet intricate problem, requiring the\nseamless integration of perception, planning, and control to enable interaction\nwithin diverse and unstructured environments. This survey presents a\ncomprehensive overview of robotic manipulation, encompassing foundational\nbackground, task-organized benchmarks and datasets, and a unified taxonomy of\nexisting methods. We extend the classical division between high-level planning\nand low-level control by broadening high-level planning to include language,\ncode, motion, affordance, and 3D representations, while introducing a new\ntaxonomy of low-level learning-based control grounded in training paradigms\nsuch as input modeling, latent learning, and policy learning. Furthermore, we\nprovide the first dedicated taxonomy of key bottlenecks, focusing on data\ncollection, utilization, and generalization, and conclude with an extensive\nreview of real-world applications. Compared with prior surveys, our work offers\nboth a broader scope and deeper insight, serving as an accessible roadmap for\nnewcomers and a structured reference for experienced researchers. All related\nresources, including research papers, open-source datasets, and projects, are\ncurated for the community at\nhttps://github.com/BaiShuanghao/Awesome-Robotics-Manipulation.", "AI": {"tldr": "该综述全面概述了机器人操作领域，涵盖了基础背景、基准、方法、瓶颈和实际应用，并提出了新的高级规划和低级控制分类法。", "motivation": "具身智能的进步以及计算机视觉、自然语言处理和大规模多模态模型的发展，使得机器人操作成为一个核心且复杂的挑战，需要感知、规划和控制的无缝集成以适应多样化环境。", "method": "该研究通过全面的综述方法，提供了机器人操作的基础背景、任务导向的基准和数据集，以及现有方法的统一分类。它扩展了高级规划（包括语言、代码、运动、功能、3D表示）并引入了基于训练范式（如输入建模、潜在学习、策略学习）的低级学习控制新分类。此外，它还首次提出了关键瓶颈（数据收集、利用、泛化）的专用分类，并回顾了实际应用。", "result": "本综述提供了比以往调查更广泛的范围和更深入的见解，为新手提供了易于理解的路线图，为经验丰富的研究人员提供了结构化参考。所有相关资源（论文、开源数据集、项目）均已整理并公开。", "conclusion": "该综述全面回顾了机器人操作，提出了新的分类法和关键瓶颈分析，旨在为研究人员提供一个结构化且深入的指南，以促进该领域的发展和应用。"}}
{"id": "2510.10185", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10185", "abs": "https://arxiv.org/abs/2510.10185", "authors": ["Lei Gu", "Yinghao Zhu", "Haoran Sang", "Zixiang Wang", "Dehao Sui", "Wen Tang", "Ewen Harrison", "Junyi Gao", "Lequan Yu", "Liantao Ma"], "title": "MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems", "comment": "Code: https://github.com/yhzhu99/MedAgentAudit", "summary": "While large language model (LLM)-based multi-agent systems show promise in\nsimulating medical consultations, their evaluation is often confined to\nfinal-answer accuracy. This practice treats their internal collaborative\nprocesses as opaque \"black boxes\" and overlooks a critical question: is a\ndiagnostic conclusion reached through a sound and verifiable reasoning pathway?\nThe inscrutable nature of these systems poses a significant risk in high-stakes\nmedical applications, potentially leading to flawed or untrustworthy\nconclusions. To address this, we conduct a large-scale empirical study of 3,600\ncases from six medical datasets and six representative multi-agent frameworks.\nThrough a rigorous, mixed-methods approach combining qualitative analysis with\nquantitative auditing, we develop a comprehensive taxonomy of collaborative\nfailure modes. Our quantitative audit reveals four dominant failure patterns:\nflawed consensus driven by shared model deficiencies, suppression of correct\nminority opinions, ineffective discussion dynamics, and critical information\nloss during synthesis. This study demonstrates that high accuracy alone is an\ninsufficient measure of clinical or public trust. It highlights the urgent need\nfor transparent and auditable reasoning processes, a cornerstone for the\nresponsible development and deployment of medical AI.", "AI": {"tldr": "本研究指出，基于LLM的多智能体医疗系统仅关注最终答案准确性不足以建立信任，其内部协作过程存在关键故障模式，亟需透明和可审计的推理过程。", "motivation": "当前对基于LLM的多智能体医疗系统评估仅限于最终答案准确性，将内部协作视为“黑箱”，忽略了诊断结论是否通过合理且可验证的推理路径得出。这种不透明性在医疗应用中构成重大风险，可能导致有缺陷或不可信的结论。", "method": "通过对六个医学数据集和六个代表性多智能体框架的3,600个案例进行大规模实证研究，采用结合定性分析和定量审计的混合方法，开发了协作故障模式的综合分类体系。", "result": "定量审计揭示了四种主要的故障模式：由共享模型缺陷驱动的错误共识、正确少数意见被压制、讨论动态无效以及综合过程中关键信息丢失。这表明高准确性本身不足以衡量临床或公众信任。", "conclusion": "高准确性不足以建立临床或公众信任。医疗AI的负责任开发和部署迫切需要透明和可审计的推理过程，这是其基石。"}}
{"id": "2510.10912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10912", "abs": "https://arxiv.org/abs/2510.10912", "authors": ["Xinyu Shao", "Yanzhe Tang", "Pengwei Xie", "Kaiwen Zhou", "Yuzheng Zhuang", "Xingyue Quan", "Jianye Hao", "Long Zeng", "Xiu Li"], "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks", "comment": "More details and videos can be found at https://robo-map.github.io.\n  Xiu Li (Corresponding author: Xiu Li)", "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning\ninto discrete points, making them brittle to perceptual noise and semantic\nambiguity. To address this challenge, we propose RoboMAP, a framework that\nrepresents spatial targets as continuous, adaptive affordance heatmaps. This\ndense representation captures the uncertainty in spatial grounding and provides\nricher information for downstream policies, thereby significantly enhancing\ntask success and interpretability. RoboMAP surpasses the previous\nstate-of-the-art on a majority of grounding benchmarks with up to a 50x speed\nimprovement, and achieves an 82\\% success rate in real-world manipulation.\nAcross extensive simulated and physical experiments, it demonstrates robust\nperformance and shows strong zero-shot generalization to navigation. More\ndetails and videos can be found at https://robo-map.github.io.", "AI": {"tldr": "RoboMAP框架通过连续、自适应的可用性热图表示空间目标，解决了语言引导机器人系统中离散空间推理的脆弱性问题，显著提高了任务成功率、速度和泛化能力。", "motivation": "许多语言引导的机器人系统将空间推理简化为离散点，这使得它们在面对感知噪声和语义模糊时表现脆弱。", "method": "RoboMAP框架将空间目标表示为连续的、自适应的可用性热图。这种密集表示捕捉了空间定位的不确定性，并为下游策略提供了更丰富的信息。", "result": "RoboMAP在大多数定位基准测试上超越了现有最先进技术，速度提升高达50倍，并在真实世界操作中实现了82%的成功率。它在广泛的模拟和物理实验中表现出强大的鲁棒性，并展现出对导航任务的强大零样本泛化能力。", "conclusion": "RoboMAP通过采用连续的可用性热图，有效解决了语言引导机器人系统中离散空间推理的局限性，从而显著增强了任务成功率、可解释性、速度和泛化能力。"}}
{"id": "2510.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10895", "abs": "https://arxiv.org/abs/2510.10895", "authors": ["Renxuan Tan", "Rongpeng Li", "Fei Wang", "Chenghui Peng", "Shaoyun Wu", "Zhifeng Zhao", "Honggang Zhang"], "title": "LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Medium Access Control (MAC) protocols, essential for wireless networks, are\ntypically manually configured. While deep reinforcement learning (DRL)-based\nprotocols enhance task-specified network performance, they suffer from poor\ngeneralizability and resilience, demanding costly retraining to adapt to\ndynamic environments. To overcome this limitation, we introduce a\ngame-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the\nuplink transmission between a base station and a varying number of user\nequipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),\ncapturing the network's natural hierarchical structure. Within this game,\nLLM-driven agents, coordinated through proximal policy optimization (PPO),\nsynthesize adaptive, semantic MAC protocols in response to network dynamics.\nProtocol action grammar (PAG) is employed to ensure the reliability and\nefficiency of this process. Under this system, we further analyze the existence\nand convergence behavior in terms of a Stackelberg equilibrium by studying the\nlearning dynamics of LLM-empowered unified policies in response to changing\nfollowers. Simulations corroborate that our framework achieves a 77.6% greater\nthroughput and a 65.2% fairness improvement over conventional baselines.\nBesides, our framework generalizes excellently to a fluctuating number of users\nwithout requiring retraining or architectural changes.", "AI": {"tldr": "本文提出了一种基于博弈论和大型语言模型（LLM）驱动的多智能体深度强化学习（MARL）框架，用于无线网络中自适应的MAC协议合成。该框架将网络建模为动态多跟随者Stackelberg博弈，并通过LLM驱动的智能体生成语义MAC协议，在不进行重新训练的情况下，显著提高了吞吐量、公平性，并展现出卓越的泛化能力。", "motivation": "传统的MAC协议通常需要手动配置，而基于深度强化学习（DRL）的协议虽然能提升特定任务的网络性能，但其泛化能力和弹性较差，在动态环境中需要昂贵的重新训练才能适应。", "method": "该研究将基站与可变用户设备之间的上行链路传输建模为动态多跟随者Stackelberg博弈。框架采用LLM驱动的智能体，通过近端策略优化（PPO）进行协调，以响应网络动态合成自适应的语义MAC协议。为确保过程的可靠性和效率，使用了协议动作语法（PAG）。此外，还分析了LLM驱动统一策略在响应变化跟随者时的学习动态，以研究Stackelberg均衡的存在性和收敛行为。", "result": "仿真结果表明，该框架在吞吐量方面比传统基线提高了77.6%，公平性提高了65.2%。此外，该框架对用户数量的波动表现出卓越的泛化能力，无需重新训练或更改架构。", "conclusion": "所提出的基于博弈论和LLM赋能的MARL框架，成功解决了现有DRL-based MAC协议泛化能力和弹性不足的问题，能够自适应地生成高效的MAC协议，并在动态环境中实现显著的性能提升和鲁棒的泛化能力。"}}
{"id": "2510.10174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10174", "abs": "https://arxiv.org/abs/2510.10174", "authors": ["Cristiano Patrício", "Luís F. Teixeira", "João C. Neves"], "title": "ViConEx-Med: Visual Concept Explainability via Multi-Concept Token Transformer for Medical Image Analysis", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Concept-based models aim to explain model decisions with human-understandable\nconcepts. However, most existing approaches treat concepts as numerical\nattributes, without providing complementary visual explanations that could\nlocalize the predicted concepts. This limits their utility in real-world\napplications and particularly in high-stakes scenarios, such as medical\nuse-cases. This paper proposes ViConEx-Med, a novel transformer-based framework\nfor visual concept explainability, which introduces multi-concept learnable\ntokens to jointly predict and localize visual concepts. By leveraging\nspecialized attention layers for processing visual and text-based concept\ntokens, our method produces concept-level localization maps while maintaining\nhigh predictive accuracy. Experiments on both synthetic and real-world medical\ndatasets demonstrate that ViConEx-Med outperforms prior concept-based models\nand achieves competitive performance with black-box models in terms of both\nconcept detection and localization precision. Our results suggest a promising\ndirection for building inherently interpretable models grounded in visual\nconcepts. Code is publicly available at\nhttps://github.com/CristianoPatricio/viconex-med.", "AI": {"tldr": "ViConEx-Med是一种新颖的基于Transformer的视觉概念可解释性框架，通过引入多概念可学习令牌，在保持高预测准确性的同时，共同预测和定位视觉概念，解决了现有概念模型缺乏视觉解释的问题，并在医学应用中表现出色。", "motivation": "大多数现有基于概念的模型将概念视为数值属性，缺乏提供可定位预测概念的视觉解释，这限制了它们在现实世界应用（特别是在医疗等高风险场景）中的实用性。", "method": "本文提出了ViConEx-Med，一个基于Transformer的视觉概念可解释性框架。它引入了多概念可学习令牌，以共同预测和定位视觉概念。该方法利用专门的注意力层处理视觉和基于文本的概念令牌，从而生成概念级别的定位图，同时保持高预测准确性。", "result": "在合成和真实世界医疗数据集上的实验表明，ViConEx-Med在概念检测和定位精度方面均优于先前的基于概念的模型，并与黑盒模型取得了有竞争力的性能。", "conclusion": "研究结果为构建基于视觉概念的固有可解释模型提供了一个有前景的方向。"}}
{"id": "2510.10208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10208", "abs": "https://arxiv.org/abs/2510.10208", "authors": ["Bo Yuan", "Yulin Chen", "Yin Zhang"], "title": "Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning", "comment": "ACL 2025", "summary": "Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have\nshown impressive performance in various downstream tasks. However, in many\nreal-world scenarios, the collected training data inevitably contains noisy\nlabels. To learn from noisy labels, most solutions select samples with small\nlosses for model training. However, the selected samples, in turn, impact the\nloss computation in the next iteration. An inaccurate initial selection can\ncreate a vicious cycle, leading to suboptimal performance. To break this cycle,\nwe propose Delora, a novel framework that decouples the sample selection from\nmodel training. For sample selection, Delora establishes a noisy label detector\nby introducing clean and noisy LoRA. Benefiting from the memory effect, the\nclean LoRA is encouraged to memorize clean data, while the noisy LoRA is\nconstrained to memorize mislabeled data, which serves as a learnable threshold\nfor selecting clean and noisy samples. For model training, Delora can use\ncarefully selected samples to fine-tune language models seamlessly.\nExperimental results on synthetic and real-world noisy datasets demonstrate the\neffectiveness of Delora in noisy label detection and text classification.", "AI": {"tldr": "Delora 提出了一种新颖的框架，通过引入干净和嘈杂的 LoRA 来解耦样本选择和模型训练，从而有效地在嘈杂标签下微调参数高效的大型语言模型。", "motivation": "参数高效微调（PEFT）大型语言模型在各种任务中表现出色，但在实际应用中训练数据不可避免地包含嘈杂标签。现有解决方案（基于小损失选择样本）会形成恶性循环，导致次优性能，因此需要一种更有效的处理嘈杂标签的方法。", "method": "Delora 框架将样本选择与模型训练解耦。在样本选择阶段，它通过引入干净 LoRA（记忆干净数据）和嘈杂 LoRA（记忆错误标签数据）来建立一个嘈杂标签检测器，其中嘈杂 LoRA 作为选择干净和嘈杂样本的可学习阈值。在模型训练阶段，Delora 利用这些精心选择的样本无缝地微调语言模型。", "result": "在合成和真实世界的嘈杂数据集上的实验结果表明，Delora 在嘈杂标签检测和文本分类方面均表现出有效性。", "conclusion": "Delora 通过解耦样本选择和模型训练，并利用干净/嘈杂 LoRA 进行嘈杂标签检测，为在嘈杂标签下微调参数高效的大型语言模型提供了一个有效的解决方案。"}}
{"id": "2510.10909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10909", "abs": "https://arxiv.org/abs/2510.10909", "authors": ["Daoyu Wang", "Mingyue Cheng", "Qi Liu", "Shuo Yu", "Zirui Liu", "Ze Guo"], "title": "PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature", "comment": "12 pages, 9 figures", "summary": "Understanding and reasoning on the web-scale scientific literature is a\ncrucial touchstone for large language model (LLM) based agents designed to\nsupport complex knowledge-intensive tasks. However, existing works are mainly\nrestricted to tool-free tasks within isolated papers, largely due to the lack\nof a benchmark for cross-paper reasoning and multi-tool orchestration in real\nresearch scenarios. In this work, we propose PaperArena, an evaluation\nbenchmark for agents to address real-world research questions that typically\nrequire integrating information across multiple papers with the assistance of\nexternal tools. Given a research question, agents should integrate diverse\nformats across multiple papers through reasoning and interacting with\nappropriate tools, thereby producing a well-grounded answer. To support\nstandardized evaluation, we provide a modular and extensible platform for agent\nexecution, offering tools such as multimodal parsing, context retrieval, and\nprogrammatic computation. Experimental results reveal that even the most\nadvanced LLM powering a well-established agent system achieves merely 38.78%\naverage accuracy. On the hard subset, accuracy drops to only 18.47%,\nhighlighting great potential for improvement. We also present several empirical\nfindings, including that all agents tested exhibit inefficient tool usage,\noften invoking more tools than necessary to solve a task. We invite the\ncommunity to adopt PaperArena to develop and evaluate more capable agents for\nscientific discovery. Our code and data are available\nhttps://github.com/Melmaphother/PaperArena.", "AI": {"tldr": "本文提出了PaperArena，一个用于评估LLM代理在科学文献中进行跨论文推理和多工具协调能力的基准。实验结果表明，即使是最先进的LLM代理也表现不佳，凸显了巨大的改进空间。", "motivation": "现有的大语言模型（LLM）代理在处理网络规模的科学文献时，其理解和推理能力受限于单篇论文或无工具任务，因为缺乏一个用于跨论文推理和真实研究场景中多工具编排的基准。", "method": "本文提出了PaperArena，一个评估代理解决需要整合多篇论文信息并借助外部工具的真实世界研究问题的基准。它提供了一个模块化和可扩展的代理执行平台，包含多模态解析、上下文检索和程序化计算等工具，以支持标准化评估。", "result": "实验结果显示，即使是最先进的LLM驱动的代理系统，平均准确率也仅为38.78%。在困难子集上，准确率更是降至18.47%，表明有很大的改进潜力。此外，所有测试代理都表现出低效的工具使用，经常调用比解决任务所需更多的工具。", "conclusion": "PaperArena基准揭示了当前LLM代理在科学发现中进行跨论文推理和工具协调方面的不足，并强调了巨大的改进空间。本文邀请社区采用PaperArena来开发和评估更强大的科学发现代理。"}}
{"id": "2510.10177", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10177", "abs": "https://arxiv.org/abs/2510.10177", "authors": ["Yulin Wang", "Mengting Hu", "Hongli Li", "Chen Luo"], "title": "HccePose(BF): Predicting Front \\& Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation", "comment": "International Conference on Computer Vision, ICCV 2025 (Highlight)\n  https://iccv.thecvf.com/virtual/2025/poster/338", "summary": "In pose estimation for seen objects, a prevalent pipeline involves using\nneural networks to predict dense 3D coordinates of the object surface on 2D\nimages, which are then used to establish dense 2D-3D correspondences. However,\ncurrent methods primarily focus on more efficient encoding techniques to\nimprove the precision of predicted 3D coordinates on the object's front\nsurface, overlooking the potential benefits of incorporating the back surface\nand interior of the object. To better utilize the full surface and interior of\nthe object, this study predicts 3D coordinates of both the object's front and\nback surfaces and densely samples 3D coordinates between them. This process\ncreates ultra-dense 2D-3D correspondences, effectively enhancing pose\nestimation accuracy based on the Perspective-n-Point (PnP) algorithm.\nAdditionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to\nprovide a more accurate and efficient representation of front and back surface\ncoordinates. Experimental results show that, compared to existing\nstate-of-the-art (SOTA) methods on the BOP website, the proposed approach\noutperforms across seven classic BOP core datasets. Code is available at\nhttps://github.com/WangYuLin-SEU/HCCEPose.", "AI": {"tldr": "本研究通过预测物体正面、背面及内部的3D坐标，并结合分层连续坐标编码（HCCE），显著提高了已知物体姿态估计的准确性。", "motivation": "现有姿态估计算法主要关注物体正面3D坐标的编码效率和精度，忽略了利用物体背面和内部坐标的潜在益处，这限制了姿态估计的进一步提升。", "method": "该研究预测物体正面和背面3D坐标，并在两者之间进行密集采样以创建超密集的2D-3D对应关系。此外，提出了一种新的分层连续坐标编码（HCCE）方法，以更准确、高效地表示正面和背面坐标，进而利用PnP算法进行姿态估计。", "result": "实验结果表明，与BOP网站上现有的最先进方法相比，所提出的方法在七个经典的BOP核心数据集上均表现出优越的性能。", "conclusion": "通过充分利用物体的完整表面（正面和背面）以及内部3D坐标，并结合HCCE，可以显著提升基于PnP算法的物体姿态估计精度。"}}
{"id": "2510.10223", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10223", "abs": "https://arxiv.org/abs/2510.10223", "authors": ["Yijie Xu", "Huizai Yao", "Zhiyu Guo", "Weiyu Guo", "Pengteng Li", "Aiwei Liu", "Xuming Hu", "Hui Xiong"], "title": "You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs", "comment": "Under Review", "summary": "Large language models (LLMs) are increasingly deployed in specialized domains\nsuch as finance, medicine, and agriculture, where they face significant\ndistribution shifts from their training data. Domain-specific fine-tuning can\nmitigate this challenge but relies on high-quality labeled data that is\nexpensive and slow to collect in expertise-limited settings. We study\nlabel-free test-time adaptation for language models and present SyTTA, an\ninference-time framework that adapts models on-the-fly without additional\nsupervision. SyTTA couples two complementary uncertainty signals that arise\nunder distribution shift: input-side perplexity, indicating mismatch with\ndomain-specific terminology and patterns, and output-side predictive entropy,\nindicating diffuse and unstable token probabilities during generation. Across\ndiverse model architectures and domain-specific benchmarks, SyTTA delivers\nconsistent gains. Notably, on agricultural question answering, SyTTA improves\nRouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.\nThese results show that effective test-time adaptation for language models is\nachievable without labeled examples, supporting deployment in label-scarce\ndomains. The code will be made available upon acceptance.", "AI": {"tldr": "本文提出SyTTA，一个无需标签的测试时自适应框架，通过结合输入侧困惑度和输出侧预测熵两种不确定性信号，实时调整大型语言模型以应对领域分布偏移，显著提升了模型在专业领域（如农业问答）的性能。", "motivation": "大型语言模型在金融、医疗、农业等专业领域部署时，面临与训练数据显著的分布偏移。尽管领域特定微调能缓解此问题，但其依赖高质量的标注数据，而在专业知识受限的场景中，收集这些数据既昂贵又耗时。这促使研究人员探索无需标签的测试时自适应方法。", "method": "SyTTA是一个推理时框架，它在不额外监督的情况下实时自适应模型。它结合了两种互补的、在分布偏移下产生的不确定性信号：输入侧困惑度（指示与领域特定术语和模式不匹配）和输出侧预测熵（指示生成过程中弥散和不稳定的token概率）。", "result": "SyTTA在不同的模型架构和领域特定基准测试中均取得了持续的性能提升。值得注意的是，在农业问答任务中，SyTTA使Qwen-2.5-7B模型的Rouge-LSum指标提高了120%以上，且每个查询仅需额外4个token。", "conclusion": "这些结果表明，无需标注样本也能实现语言模型有效的测试时自适应，从而支持在标签稀缺的领域部署模型。"}}
{"id": "2510.10931", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10931", "abs": "https://arxiv.org/abs/2510.10931", "authors": ["SHengjie Ma", "Chenlong Deng", "Jiaxin Mao", "Jiadeng Huang", "Teng Wang", "Junjie Wu", "Changwang Zhang", "Jun wang"], "title": "PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents", "comment": null, "summary": "Retrieval-augmented generation (RAG) agents, such as recent\nDeepResearch-style systems, extend large language models (LLMs) with autonomous\ninformation-seeking capabilities through external tools. While reinforcement\nlearning (RL) has enabled impressive multi-step reasoning, we identify a\npreviously overlooked failure mode, Tool-Call Hacking, where agents inflate\nreward signals by issuing superficially correct tool calls without genuinely\nleveraging the retrieved evidence. This results in (i) mode collapse into\nrepetitive reliance on a single source and (ii) spurious grounding, where\nanswers are only weakly supported by cited content.\n  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL\nframework that enforces verifiable causal links between retrieved evidence,\nreasoning traces, and final answers. PoU operationalizes this through a unified\nstep-wise contract combining syntactic citation validation, perturbation-based\nsensitivity rewards, and answer-evidence alignment objectives, ensuring that\ntool usage remains both interpretable and functionally grounded.\n  Across seven QA benchmarks spanning in-domain, out-of-domain, and\nout-of-tool-distribution settings, PoU consistently outperforms strong\nDeepResearch baselines in factual accuracy, evidence faithfulness, and\ntool-routing balance. These findings highlight the necessity of grounding\nRL-trained agents not merely in task outcomes but in the causal use of\nretrieved information, offering a principled path toward trustworthy\nretrieval-augmented reasoning.", "AI": {"tldr": "RAG代理在RL训练中存在“工具调用欺骗”问题，导致表面化工具使用和错误引用。本文提出Proof-of-Use (PoU)框架，通过强制验证证据、推理和答案之间的因果链接来解决此问题，显著提升了事实准确性和证据忠实度。", "motivation": "RL训练的RAG代理（如DeepResearch系统）在多步推理中存在一个未被发现的故障模式——“工具调用欺骗”（Tool-Call Hacking）。代理通过发出表面上正确的工具调用来虚增奖励信号，但并未真正利用检索到的证据，导致模式崩溃（重复依赖单一来源）和虚假接地（答案与引用内容支持度弱）。", "method": "本文提出Proof-of-Use (PoU)，一个基于证据的强化学习框架。PoU通过一个统一的逐步契约来强制执行检索到的证据、推理轨迹和最终答案之间可验证的因果链接。该契约包括语法引用验证、基于扰动的敏感性奖励以及答案-证据对齐目标，确保工具使用既可解释又功能性地有根据。", "result": "在涵盖域内、域外和工具分布外设置的七个QA基准测试中，PoU在事实准确性、证据忠实度和工具路由平衡方面始终优于强大的DeepResearch基线。这些发现强调了将RL训练的代理不仅基于任务结果，而且基于检索信息因果使用进行接地的必要性。", "conclusion": "对于可信赖的检索增强推理，关键在于将RL训练的代理接地于检索信息的因果使用，而不仅仅是任务结果。PoU为实现这一目标提供了一条原则性的途径，能够解决RAG代理中“工具调用欺骗”的问题。"}}
{"id": "2510.10960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10960", "abs": "https://arxiv.org/abs/2510.10960", "authors": ["Dong Hu", "Fenqing Hu", "Lidong Yang", "Chao Huang"], "title": "Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving", "comment": null, "summary": "Ensuring safety in autonomous driving (AD) remains a significant challenge,\nespecially in highly dynamic and complex traffic environments where diverse\nagents interact and unexpected hazards frequently emerge. Traditional\nreinforcement learning (RL) methods often struggle to balance safety,\nefficiency, and adaptability, as they primarily focus on reward maximization\nwithout explicitly modeling risk or safety constraints. To address these\nlimitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L)\nframework for safe AD. GTR2L incorporates a multi-level game-theoretic world\nmodel that jointly predicts the interactive behaviors of surrounding vehicles\nand their associated risks, along with an adaptive rollout horizon that adjusts\ndynamically based on predictive uncertainty. Furthermore, an uncertainty-aware\nbarrier mechanism enables flexible modulation of safety boundaries. A dedicated\nrisk modeling approach is also proposed, explicitly capturing both epistemic\nand aleatoric uncertainty to guide constrained policy optimization and enhance\ndecision-making in complex environments. Extensive evaluations across diverse\nand safety-critical traffic scenarios show that GTR2L significantly outperforms\nstate-of-the-art baselines, including human drivers, in terms of success rate,\ncollision and violation reduction, and driving efficiency. The code is\navailable at https://github.com/DanielHu197/GTR2L.", "AI": {"tldr": "本研究提出了一种新颖的博弈论风险塑形强化学习（GTR2L）框架，用于自动驾驶中的安全决策。该框架结合了多层博弈论世界模型、自适应规划范围和不确定性感知障碍机制，以明确建模风险和不确定性，显著提高了复杂交通场景下的安全性、效率和适应性。", "motivation": "自动驾驶在动态复杂交通环境中确保安全仍面临巨大挑战。传统强化学习方法主要关注奖励最大化，难以平衡安全性、效率和适应性，且未明确建模风险或安全约束。", "method": "本研究提出了GTR2L框架，其核心包括：1) 一个多层博弈论世界模型，联合预测周围车辆的交互行为及其相关风险；2) 一个基于预测不确定性动态调整的自适应规划范围；3) 一个不确定性感知障碍机制，实现安全边界的灵活调整；4) 一种专门的风险建模方法，明确捕捉认知不确定性（epistemic uncertainty）和偶然不确定性（aleatoric uncertainty），以指导受约束的策略优化。", "result": "在多样化且安全关键的交通场景中进行的广泛评估表明，GTR2L在成功率、碰撞和违规减少以及驾驶效率方面显著优于最先进的基线方法（包括人类驾驶员）。", "conclusion": "GTR2L框架通过整合博弈论、不确定性感知风险建模和自适应机制，有效解决了传统强化学习在自动驾驶安全决策中的局限性，为复杂动态环境下的安全自动驾驶提供了一个高性能的解决方案。"}}
{"id": "2510.10942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10942", "abs": "https://arxiv.org/abs/2510.10942", "authors": ["Nilima Rao", "Jagriti Srivastava", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval", "comment": null, "summary": "Modern enterprises manage vast knowledge distributed across heterogeneous\nsystems such as Jira, Git repositories, Confluence, and wikis. Conventional\nretrieval methods based on keyword search or static embeddings often fail to\nanswer complex queries that require contextual reasoning and multi-hop\ninference across artifacts. We present a modular hybrid retrieval framework for\nadaptive enterprise information access that integrates Knowledge Base\nLanguage-Augmented Models (KBLam), DeepGraph representations, and\nembedding-driven semantic search. The framework builds a unified knowledge\ngraph from parsed repositories including code, pull requests, and commit\nhistories, enabling semantic similarity search, structural inference, and\nmulti-hop reasoning. Query analysis dynamically determines the optimal\nretrieval strategy, supporting both structured and unstructured data sources\nthrough independent or fused processing. An interactive interface provides\ngraph visualizations, subgraph exploration, and context-aware query routing to\ngenerate concise and explainable answers. Experiments on large-scale Git\nrepositories show that the unified reasoning layer improves answer relevance by\nup to 80 percent compared with standalone GPT-based retrieval pipelines. By\ncombining graph construction, hybrid reasoning, and interactive visualization,\nthe proposed framework offers a scalable, explainable, and user-centric\nfoundation for intelligent knowledge assistants in enterprise environments.", "AI": {"tldr": "该论文提出了一种模块化混合检索框架，结合知识图谱语言增强模型（KBLam）、DeepGraph表示和嵌入式语义搜索，从异构企业知识源构建统一知识图谱，以支持复杂查询的上下文推理和多跳推断，并显著提高答案相关性。", "motivation": "现代企业知识分散在Jira、Git、Confluence等异构系统中。传统的基于关键词或静态嵌入的检索方法难以回答需要跨多源进行上下文推理和多跳推断的复杂查询。", "method": "该框架通过解析代码、拉取请求和提交历史等仓库，构建一个统一的知识图谱。它整合了KBLam、DeepGraph表示和嵌入式语义搜索。通过查询分析动态确定最佳检索策略，支持结构化和非结构化数据源的独立或融合处理。此外，还提供了一个交互式界面，用于图可视化、子图探索和上下文感知查询路由。", "result": "在大规模Git仓库上的实验表明，与独立的基于GPT的检索管道相比，统一推理层将答案相关性提高了80%。", "conclusion": "该框架通过结合图构建、混合推理和交互式可视化，为企业环境中的智能知识助手提供了一个可扩展、可解释且以用户为中心的解决方案。"}}
{"id": "2510.10224", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10224", "abs": "https://arxiv.org/abs/2510.10224", "authors": ["Ruize An", "Richong Zhang", "Zhijie Nie", "Zhanyu Wu", "Yanzhao Zhang", "Dingkun Long"], "title": "Text2Token: Unsupervised Text Representation Learning with Token Target Prediction", "comment": null, "summary": "Unsupervised text representation learning (TRL) is a fundamental task in\nnatural language processing, which is beneficial for improving search and\nrecommendations with the web's unlabeled texts. A recent empirical study finds\nthat the high-quality representation aligns with the key token of the input\ntext, uncovering the potential connection between representation space and\nvocabulary space. Inspired by the findings, we revisit the generative tasks and\ndevelop an unsupervised generative framework for TRL, Text2Token. The framework\nis based on the token target prediction task, utilizing carefully constructed\ntarget token distribution as supervisory signals. To construct the high-quality\ntarget token distribution, we analyze the token-alignment properties with\nadvanced embedders and identify two essential categories of key tokens: (1) the\nmeaningful tokens in the text and (2) semantically derived tokens beyond the\ntext. Based on these insights, we propose two methods -- data-driven and\nmodel-derived -- to construct synthetic token targets from data or the LLM\nbackbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token\nachieves performance competitive with the state-of-the-art embedder with\nunsupervised contrastive learning, LLM2Vec. Our analysis further shows that\nvocabulary and representation spaces optimize together and toward the optimum\nsolution during training, providing new ideas and insights for future work.", "AI": {"tldr": "本文提出了一种名为Text2Token的无监督生成式文本表示学习框架，通过令牌目标预测任务和精心构建的令牌目标分布，实现了与先进无监督对比学习嵌入器相当的性能。", "motivation": "无监督文本表示学习是自然语言处理中的基础任务，有助于改进搜索和推荐。近期研究发现高质量表示与输入文本的关键令牌对齐，揭示了表示空间和词汇空间之间的潜在联系。受此启发，本文旨在开发一种新的生成式无监督文本表示学习方法。", "method": "本文开发了一个名为Text2Token的无监督生成式框架，基于令牌目标预测任务，并利用精心构建的目标令牌分布作为监督信号。为构建高质量的目标令牌分布，作者分析了高级嵌入器的令牌对齐特性，并识别出两类关键令牌：文本中有意义的令牌和文本外语义派生的令牌。在此基础上，提出了数据驱动和模型派生两种方法，从数据或LLM骨干网络构建合成令牌目标。", "result": "在MTEB v2基准测试中，Text2Token的性能与最先进的无监督对比学习嵌入器LLM2Vec相当。此外，分析表明词汇空间和表示空间在训练过程中共同优化并趋向最优解。", "conclusion": "Text2Token提供了一种新的无监督生成式文本表示学习框架，实现了有竞争力的性能，并揭示了词汇空间和表示空间协同优化的新见解，为未来的研究提供了新的思路和方向。"}}
{"id": "2510.10180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10180", "abs": "https://arxiv.org/abs/2510.10180", "authors": ["Zixu Zhao", "Yang Zhan"], "title": "TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) have become powerful platforms for real-time,\nhigh-resolution data collection, producing massive volumes of aerial videos.\nEfficient retrieval of relevant content from these videos is crucial for\napplications in urban management, emergency response, security, and disaster\nrelief. While text-video retrieval has advanced in natural video domains, the\nUAV domain remains underexplored due to limitations in existing datasets, such\nas coarse and redundant captions. Thus, in this work, we construct the Drone\nVideo-Text Match Dataset (DVTMD), which contains 2,864 videos and 14,320\nfine-grained, semantically diverse captions. The annotations capture multiple\ncomplementary aspects, including human actions, objects, background settings,\nenvironmental conditions, and visual style, thereby enhancing text-video\ncorrespondence and reducing redundancy. Building on this dataset, we propose\nthe Text-Conditioned Multi-granularity Alignment (TCMA) framework, which\nintegrates global video-sentence alignment, sentence-guided frame aggregation,\nand word-guided patch alignment. To further refine local alignment, we design a\nWord and Patch Selection module that filters irrelevant content, as well as a\nText-Adaptive Dynamic Temperature Mechanism that adapts attention sharpness to\ntext type. Extensive experiments on DVTMD and CapERA establish the first\ncomplete benchmark for drone text-video retrieval. Our TCMA achieves\nstate-of-the-art performance, including 45.5% R@1 in text-to-video and 42.8%\nR@1 in video-to-text retrieval, demonstrating the effectiveness of our dataset\nand method. The code and dataset will be released.", "AI": {"tldr": "该研究构建了首个细粒度无人机视频-文本匹配数据集DVTMD，并提出了Text-Conditioned Multi-granularity Alignment (TCMA) 框架，显著提升了无人机文本-视频检索的性能。", "motivation": "无人机生成大量高分辨率视频，高效检索是城市管理、应急响应等应用的关键。然而，现有无人机领域文本-视频检索数据集存在粗糙和冗余的字幕问题，导致该领域研究不足。", "method": "1. 构建了Drone Video-Text Match Dataset (DVTMD)，包含2,864个视频和14,320个细粒度、语义多样化的字幕，涵盖人物动作、物体、背景、环境条件和视觉风格等多个方面。 2. 提出了Text-Conditioned Multi-granularity Alignment (TCMA) 框架，整合了全局视频-句子对齐、句子引导的帧聚合和单词引导的图像块对齐。 3. 设计了单词和图像块选择模块以过滤不相关内容，以及文本自适应动态温度机制以根据文本类型调整注意力锐度。", "result": "1. DVTMD和CapERA共同建立了首个完整的无人机文本-视频检索基准。 2. TCMA在DVTMD上取得了最先进的性能，文本到视频检索的R@1达到45.5%，视频到文本检索的R@1达到42.8%。", "conclusion": "本研究构建的DVTMD数据集和提出的TCMA框架在无人机文本-视频检索任务中表现出卓越的有效性，实现了最先进的性能，为该领域提供了重要的基准和方法。"}}
{"id": "2510.10191", "categories": ["cs.CV", "68T07", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.10191", "abs": "https://arxiv.org/abs/2510.10191", "authors": ["Haohua Dong", "Ana Manzano Rodríguez", "Camille Guinaudeau", "Shin'ichi Satoh"], "title": "Fairness Without Labels: Pseudo-Balancing for Bias Mitigation in Face Gender Classification", "comment": "8 pages. Accepted for publication in the ICCV 2025 Workshop\n  Proceedings (2nd FAILED Workshop). Also available on HAL (hal-05210445v1)", "summary": "Face gender classification models often reflect and amplify demographic\nbiases present in their training data, leading to uneven performance across\ngender and racial subgroups. We introduce pseudo-balancing, a simple and\neffective strategy for mitigating such biases in semi-supervised learning. Our\nmethod enforces demographic balance during pseudo-label selection, using only\nunlabeled images from a race-balanced dataset without requiring access to\nground-truth annotations.\n  We evaluate pseudo-balancing under two conditions: (1) fine-tuning a biased\ngender classifier using unlabeled images from the FairFace dataset, and (2)\nstress-testing the method with intentionally imbalanced training data to\nsimulate controlled bias scenarios. In both cases, models are evaluated on the\nAll-Age-Faces (AAF) benchmark, which contains a predominantly East Asian\npopulation. Our results show that pseudo-balancing consistently improves\nfairness while preserving or enhancing accuracy. The method achieves 79.81%\noverall accuracy - a 6.53% improvement over the baseline - and reduces the\ngender accuracy gap by 44.17%. In the East Asian subgroup, where baseline\ndisparities exceeded 49%, the gap is narrowed to just 5.01%. These findings\nsuggest that even in the absence of label supervision, access to a\ndemographically balanced or moderately skewed unlabeled dataset can serve as a\npowerful resource for debiasing existing computer vision models.", "AI": {"tldr": "本文提出了一种名为“伪平衡”（pseudo-balancing）的半监督学习策略，用于缓解面部性别分类模型中存在的群体偏差，通过在伪标签选择过程中强制实现人口统计学平衡，显著提升了公平性和准确性。", "motivation": "面部性别分类模型经常反映并放大训练数据中存在的人口统计学偏差，导致在不同性别和种族子群体之间表现不均。", "method": "引入了“伪平衡”策略，这是一种简单有效的半监督学习方法。该方法在伪标签选择过程中强制执行人口统计学平衡，仅使用来自种族平衡数据集的未标注图像，无需真实的地面真值标注。", "result": "在两种条件下评估了伪平衡方法：(1) 使用FairFace数据集的未标注图像微调有偏见的性别分类器；(2) 使用故意不平衡的训练数据进行压力测试。在主要由东亚人口组成的All-Age-Faces (AAF) 基准上评估。结果显示，伪平衡方法持续提高了公平性，同时保持或提升了准确性。整体准确率达到79.81%（比基线提高6.53%），性别准确率差距减少了44.17%。在基线差异超过49%的东亚子群体中，差距缩小到仅5.01%。", "conclusion": "即使在没有标签监督的情况下，访问人口统计学平衡或适度倾斜的未标注数据集，也可以作为消除现有计算机视觉模型偏差的强大资源。"}}
{"id": "2510.10975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10975", "abs": "https://arxiv.org/abs/2510.10975", "authors": ["Mingtong Dai", "Lingbo Liu", "Yongjie Bai", "Yang Liu", "Zhouxia Wang", "Rui SU", "Chunjie Chen", "Liang Lin", "Xinyu Wu"], "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model", "comment": null, "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs.We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.", "AI": {"tldr": "RoVer是一个具身测试时扩展框架，它利用机器人过程奖励模型（PRM）作为验证器，在不修改现有VLA模型架构或权重的情况下，通过评估和细化候选动作来提升其性能。", "motivation": "当前VLA模型性能提升主要依赖于大规模训练数据和模型尺寸扩展，但这对于机器人应用来说成本过高且受数据收集限制。", "method": "RoVer框架采用一个机器人过程奖励模型（PRM）作为测试时验证器。PRM (i) 为候选动作分配标量过程奖励以评估其可靠性，(ii) 预测动作空间方向以指导候选动作的扩展和细化。在推理过程中，RoVer从基础策略生成多个候选动作，沿PRM预测的方向扩展这些动作，然后使用PRM对所有候选动作进行评分，最终选择最优动作执行。此外，通过缓存共享感知特征，RoVer能分摊感知成本，在相同的测试时计算预算下评估更多候选动作。", "result": "RoVer有效地将可用的计算资源转化为更优的动作决策能力，实现了测试时扩展的益处，且无需额外的训练开销。它提供了一个通用的、即插即用的VLA测试时扩展框架，一个能同时提供标量过程奖励和动作空间探索方向的PRM，以及一个高效的方向引导采样策略，该策略利用共享感知缓存实现可扩展的候选动作生成和选择。", "conclusion": "RoVer提供了一种通用且高效的测试时扩展框架，能够在不修改现有VLA模型的情况下，通过智能的动作评估和细化，利用计算资源显著提升具身智能体的决策能力，且无需额外训练成本。"}}
{"id": "2510.11014", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11014", "abs": "https://arxiv.org/abs/2510.11014", "authors": ["Subhransu S. Bhattacharjee", "Hao Lu", "Dylan Campbell", "Rahul Shome"], "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces", "comment": "Under Review", "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning.", "AI": {"tldr": "本文提出一种基于采样的流程，利用大规模预训练生成模型在零样本设置下生成概率先验，以解决部分可观测环境下的机器人规划问题。", "motivation": "在部分可观测环境下进行规划时，先验知识至关重要，但在实践中很难获取。", "method": "该方法是一个基于采样的流程，利用大规模预训练的生成模型，在零样本条件下生成概率先验。这些先验能够捕捉环境不确定性和空间语义关系。该流程在部分观测条件下，恢复完整的RGB-D点云样本，包含占用信息和目标语义，可直接用于配置空间规划。", "result": "实验表明，该方法能够恢复与真实情况一致的常识性空间语义，生成多样、干净的3D点云，可用于运动规划。研究还建立了一个Matterport3D基准，用于评估机器人在部分可见房间中导航到未观测目标物体的能力。", "conclusion": "该研究强调了生成模型作为机器人规划丰富先验来源的潜力，其生成的先验对于运动规划非常有用。"}}
{"id": "2510.10976", "categories": ["cs.AI", "68T05", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.10976", "abs": "https://arxiv.org/abs/2510.10976", "authors": ["Wentao Wang", "Heqing Zou", "Tianze Luo", "Rui Huang", "Yutian Zhao", "Zhuochen Wang", "Hansheng Zhang", "Chengwei Qin", "Yan Wang", "Lin Zhao", "Huaijian Zhang"], "title": "Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph", "comment": null, "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated\nstrong semantic understanding capabilities, but struggles to perform precise\nspatio-temporal understanding. Existing spatio-temporal methods primarily focus\non the video itself, while overlooking the physical information within the\nvideo, such as multi-object layouts and motion. Such limitations restrict the\nuse of MLLMs in downstream applications that demand high precision, including\nembodied intelligence and VR. To address this issue, we present Video-STR, a\nnovel graph-based reinforcement method for precise Video Spatio-Temporal\nReasoning. Building upon the capacity of Reinforcement Learning with Verifiable\nReward (RLVR) to improve model abilities, we introduce a reasoning mechanism\nusing graph-based Group Relative Policy Optimization (GRPO) method to guide the\nmodel in inferring the underlying spatio-temporal topology of scenarios during\nthe thinking process. To resolve the lack of spatio-temporal training data, we\nconstruct the STV-205k dataset with 205k question-answering pairs, covering\ndynamic multi-object scenes in both indoor and outdoor environments, to support\nthe model training. Experiments show that Video-STR achieves state-of-the-art\nresults on various benchmarks, outperforming the base model by 13% on\nSTI-Bench, and demonstrating the effectiveness of our approach and dataset.\nCode, model, and data will be released.", "AI": {"tldr": "多模态大语言模型（MLLMs）在视频时空理解方面表现不佳，本文提出Video-STR，一种新的基于图的强化学习方法，结合GRPO和自建的STV-205k数据集，显著提升了视频精确时空推理能力，并达到了SOTA水平。", "motivation": "尽管多模态大语言模型（MLLMs）在语义理解方面表现出色，但在精确时空理解上仍存在困难。现有方法主要关注视频本身，忽略了视频中的物理信息（如多对象布局和运动）。这种局限性限制了MLLMs在具身智能和VR等需要高精度的下游应用中的使用。", "method": "本文提出了Video-STR，一种新颖的基于图的强化方法，用于精确的视频时空推理。该方法基于可验证奖励强化学习（RLVR），并引入了使用基于图的群组相对策略优化（GRPO）方法的推理机制，以在思考过程中引导模型推断场景的潜在时空拓扑。为解决时空训练数据不足的问题，构建了包含20.5万个问答对的STV-205k数据集，涵盖室内外动态多对象场景。", "result": "实验表明，Video-STR在各种基准测试中取得了最先进的结果，在STI-Bench上比基础模型性能提高了13%，证明了所提出方法和数据集的有效性。", "conclusion": "Video-STR及其自建的STV-205k数据集有效地解决了MLLMs在视频精确时空理解上的不足，显著提升了模型在该领域的性能，并为相关应用提供了新的解决方案。"}}
{"id": "2510.10194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10194", "abs": "https://arxiv.org/abs/2510.10194", "authors": ["Feng Xiao", "Hongbin Xu", "Hai Ci", "Wenxiong Kang"], "title": "B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding", "comment": null, "summary": "Localizing 3D objects using natural language is essential for robotic scene\nunderstanding. The descriptions often involve multiple spatial relationships to\ndistinguish similar objects, making 3D-language alignment difficult. Current\nmethods only model relationships for pairwise objects, ignoring the global\nperceptual significance of n-ary combinations in multi-modal relational\nunderstanding. To address this, we propose a novel progressive relational\nlearning framework for 3D object grounding. We extend relational learning from\nbinary to n-ary to identify visual relations that match the referential\ndescription globally. Given the absence of specific annotations for referred\nobjects in the training data, we design a grouped supervision loss to\nfacilitate n-ary relational learning. In the scene graph created with n-ary\nrelationships, we use a multi-modal network with hybrid attention mechanisms to\nfurther localize the target within the n-ary combinations. Experiments and\nablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our\nmethod outperforms the state-of-the-art, and proves the advantages of the n-ary\nrelational perception in 3D localization.", "AI": {"tldr": "该论文提出了一种渐进式关系学习框架，通过将关系学习从二元扩展到n元，并引入分组监督损失，以提高使用自然语言进行3D对象定位的准确性，解决了现有方法仅关注成对关系的问题。", "motivation": "机器人场景理解中，使用自然语言定位3D对象至关重要。描述通常涉及多个空间关系来区分相似对象，但现有方法只建模成对对象的关系，忽略了多模态关系理解中n元组合的全局感知重要性，导致3D语言对齐困难。", "method": "提出了一种渐进式关系学习框架。将关系学习从二元扩展到n元，以识别全局匹配参照描述的视觉关系。针对训练数据中缺乏参照对象特定标注的问题，设计了分组监督损失以促进n元关系学习。在用n元关系构建的场景图中，使用带有混合注意力机制的多模态网络在n元组合中进一步定位目标。", "result": "在ReferIt3D和ScanRefer基准测试上的实验和消融研究表明，该方法优于现有最先进的方法，并证明了n元关系感知在3D定位中的优势。", "conclusion": "n元关系感知在3D对象定位中具有显著优势，提出的渐进式关系学习框架和分组监督损失有效解决了复杂自然语言描述下的3D对象接地问题。"}}
{"id": "2510.10979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10979", "abs": "https://arxiv.org/abs/2510.10979", "authors": ["Qizhi Guo", "Siyuan Yang", "Junning Lyu", "Jianjun Sun", "Defu Lin", "Shaoming He"], "title": "AMO-HEAD: Adaptive MARG-Only Heading Estimation for UAVs under Magnetic Disturbances", "comment": null, "summary": "Accurate and robust heading estimation is crucial for unmanned aerial\nvehicles (UAVs) when conducting indoor inspection tasks. However, the cluttered\nnature of indoor environments often introduces severe magnetic disturbances,\nwhich can significantly degrade heading accuracy. To address this challenge,\nthis paper presents an Adaptive MARG-Only Heading (AMO-HEAD) estimation\napproach for UAVs operating in magnetically disturbed environments. AMO-HEAD is\na lightweight and computationally efficient Extended Kalman Filter (EKF)\nframework that leverages inertial and magnetic sensors to achieve reliable\nheading estimation. In the proposed approach, gyroscope angular rate\nmeasurements are integrated to propagate the quaternion state, which is\nsubsequently corrected using accelerometer and magnetometer data. The corrected\nquaternion is then used to compute the UAV's heading. An adaptive process noise\ncovariance method is introduced to model and compensate for gyroscope\nmeasurement noise, bias drift, and discretization errors arising from the Euler\nmethod integration. To mitigate the effects of external magnetic disturbances,\na scaling factor is applied based on real-time magnetic deviation detection. A\ntheoretical observability analysis of the proposed AMO-HEAD is performed using\nthe Lie derivative. Extensive experiments were conducted in real world indoor\nenvironments with customized UAV platforms. The results demonstrate the\neffectiveness of the proposed algorithm in providing precise heading estimation\nunder magnetically disturbed conditions.", "AI": {"tldr": "本文提出了一种名为AMO-HEAD的自适应MARG-Only航向估计方法，用于在磁场干扰严重的室内环境中为无人机提供精确且鲁棒的航向估计。", "motivation": "无人机在室内巡检任务中需要精确且鲁棒的航向估计，但室内环境常伴有严重的磁场干扰，这会显著降低航向估计的准确性。", "method": "AMO-HEAD是一个轻量级且计算高效的扩展卡尔曼滤波器(EKF)框架，利用惯性（陀螺仪、加速度计）和磁力计数据进行航向估计。它通过集成陀螺仪测量值来传播四元数状态，并使用加速度计和磁力计数据进行校正。该方法引入了自适应过程噪声协方差来补偿陀螺仪噪声、偏置漂移和欧拉积分误差，并通过基于实时磁偏差检测的缩放因子来减轻外部磁场干扰的影响。此外，还使用李导数对AMO-HEAD进行了理论可观测性分析。", "result": "在定制无人机平台进行的真实室内环境实验表明，所提出的AMO-HEAD算法在磁场干扰条件下能够提供精确的航向估计。", "conclusion": "AMO-HEAD方法能够有效解决室内磁场干扰对无人机航向估计准确性的影响，为无人机在复杂室内环境中的操作提供了可靠的航向信息。"}}
{"id": "2510.10241", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10241", "abs": "https://arxiv.org/abs/2510.10241", "authors": ["Kangyang Luo", "Yuzhuo Bai", "Shuzheng Si", "Cheng Gao", "Zhitong Wang", "Yingli Shen", "Wenhao Li", "Zhu Liu", "Yufeng Han", "Jiayi Wu", "Cunliang Kong", "Maosong Sun"], "title": "ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement", "comment": null, "summary": "Coreference Resolution (CR) is a critical task in Natural Language Processing\n(NLP). Current research faces a key dilemma: whether to further explore the\npotential of supervised neural methods based on small language models, whose\ndetect-then-cluster pipeline still delivers top performance, or embrace the\npowerful capabilities of Large Language Models (LLMs). However, effectively\ncombining their strengths remains underexplored. To this end, we propose\n\\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised\nmodel with LLM-based reasoning. First, we present an improved CR method\n(\\textbf{ImCoref}) to push the performance boundaries of the supervised neural\nmethod by introducing a lightweight bridging module to enhance long-text\nencoding capability, devising a biaffine scorer to comprehensively capture\npositional information, and invoking a hybrid mention regularization to improve\ntraining efficiency. Importantly, we employ an LLM acting as a multi-role\nChecker-Splitter agent to validate candidate mentions (filtering out invalid\nones) and coreference results (splitting erroneous clusters) predicted by\nImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,\nwhich achieves superior performance compared to existing state-of-the-art\n(SOTA) methods.", "AI": {"tldr": "本文提出ImCoref-CeS框架，有效结合增强型有监督神经模型（ImCoref）与大型语言模型（LLM）的推理能力，用于共指消解任务，并取得了超越现有SOTA方法的性能。", "motivation": "共指消解领域面临一个核心困境：是继续深挖基于小型语言模型的有监督神经方法的潜力（其“检测-聚类”流程仍表现出色），还是拥抱大型语言模型（LLMs）的强大能力。然而，如何有效结合两者的优势仍未被充分探索。", "method": "本文提出ImCoref-CeS框架。首先，ImCoref是一个改进的共指消解方法，通过引入轻量级桥接模块增强长文本编码能力，设计双仿射评分器全面捕捉位置信息，并采用混合提及正则化提高训练效率，以提升有监督神经方法的性能。其次，ImCoref-CeS利用LLM作为多角色“检查器-分割器”代理，对ImCoref预测的候选提及（过滤无效提及）和共指结果（拆分错误簇）进行验证。", "result": "广泛的实验证明了ImCoref-CeS的有效性，其性能优于现有最先进（SOTA）方法。", "conclusion": "ImCoref-CeS框架成功地将增强型有监督模型与LLM的推理能力相结合，在共指消解任务中实现了卓越的性能，并为该领域提供了新的SOTA方法。"}}
{"id": "2510.10977", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10977", "abs": "https://arxiv.org/abs/2510.10977", "authors": ["Taiqiang Wu", "Runming Yang", "Tao Liu", "Jiahao Wang", "Ngai Wong"], "title": "Revisiting Model Interpolation for Efficient Reasoning", "comment": "14 pages, 6 figures, 7 tables. Working in progress", "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at \\href{https://github.com/wutaiqiang/MI}{Github}.", "AI": {"tldr": "本文系统性地重新审视了最简单的模型插值方法，发现其遵循三阶段演化范式，并出人意料地在效率和有效性上超越了复杂的模型合并基线，为精确推理能力模型提供实用框架。", "motivation": "模型合并（尤其是在指令和思维模型上）在高效推理方面表现出色。本文旨在重新审视最简单的合并方法——直接权重插值，以系统性地理解其行为和动态，并探索其在性能-成本权衡中的潜力。", "method": "研究方法包括系统性地重新审视直接权重插值方法，观察其在推理轨迹上的三阶段演化范式。通过实证结果验证其性能，并进行了广泛的消融研究，涵盖模型层、模块和解码策略，以验证研究发现。", "result": "研究发现，模型插值遵循一个具有独特行为的三阶段演化范式。一个经过策略性插值的模型，在效率和有效性方面均出人意料地超越了复杂的模型合并基线。这些发现通过对模型层、模块和解码策略的广泛消融研究得到了进一步验证。", "conclusion": "这项工作揭示了模型插值的奥秘，并提供了一个实用的框架，用于构建具有精确目标推理能力的模型，为导航性能-成本权衡提供了有原则的指导。"}}
{"id": "2510.10252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10252", "abs": "https://arxiv.org/abs/2510.10252", "authors": ["Samir Abdaljalil", "Erchin Serpedin", "Khalid Qaraqe", "Hasan Kurban"], "title": "Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models", "comment": null, "summary": "Large language models (LLMs) often generate reasoning traces that appear\ncoherent but rest on unsupported assumptions, leading to hallucinated\nconclusions. Prior work mainly addresses factual hallucinations or relies on\npost-hoc verification, leaving reasoning-induced hallucinations largely\nunaddressed. We propose Audit-of-Understanding (AoU), a framework that\nconstrains inference to validated premises through three phases: (1)\ndecomposing a query into candidate assumptions, (2) auditing their support, and\n(3) conditioning inference only on the validated subset. Formally, AoU is\n\\emph{posterior-constrained inference}, connecting to selective prediction and\nrejection learning. Our contributions are threefold: (i) theoretical guarantees\nunder perfect validation, (ii) excess-risk bounds under imperfect audits, and\n(iii) tractability analysis. Empirically, AoU improves both accuracy and\nfaithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on\nGSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over\nChain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at\nhttps://anonymous.4open.science/r/audit-of-understanding-E28B.", "AI": {"tldr": "本文提出了一种名为“理解审计”(AoU)的框架，通过在推理前验证假设来防止大型语言模型(LLM)产生基于未经证实前提的推理幻觉，从而显著提高了模型在数学推理任务上的准确性和忠实度。", "motivation": "大型语言模型(LLM)常生成看似连贯但基于未经证实假设的推理链，导致幻觉结论。现有工作主要关注事实性幻觉或依赖事后验证，未能有效解决推理引起的幻觉问题。", "method": "本文提出了“理解审计”(AoU)框架，这是一种后验约束推理方法。它包含三个阶段：(1) 将查询分解为候选假设；(2) 审计这些假设的支持度；(3) 仅基于经过验证的假设进行推理。该方法在形式上与选择性预测和拒绝学习相关，并提供了完美验证下的理论保证、不完美审计下的超额风险界限以及可处理性分析。", "result": "实验结果表明，AoU显著提高了GSM8K、MultiArith和SVAMP等数据集的准确性和忠实度。与Chain-of-Thought、Self-Consistency和CoT-Decoding等方法相比，AoU在GSM8K上实现了高达+30%的提升，在MultiArith上实现了+45%的提升，并在SVAMP上持续实现了+20-28%的改进。", "conclusion": "AoU框架通过将推理限制在经过验证的前提上，有效解决了大型语言模型中推理引起的幻觉问题。它不仅提供了理论上的保障，还在多个数学推理基准测试中取得了显著的实证性能提升，提高了模型的准确性和推理忠实度。"}}
{"id": "2510.11003", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11003", "abs": "https://arxiv.org/abs/2510.11003", "authors": ["Takuma Fujiu", "Sho Okazaki", "Kohei Kaminishi", "Yuji Nakata", "Shota Hamamoto", "Kenshin Yokose", "Tatsunori Hara", "Yasushi Umeda", "Jun Ota"], "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems", "comment": null, "summary": "In manufacturing systems, identifying the causes of failures is crucial for\nmaintaining and improving production efficiency. In knowledge-based\nfailure-cause inference, it is important that the knowledge base (1) explicitly\nstructures knowledge about the target system and about failures, and (2)\ncontains sufficiently long causal chains of failures. In this study, we\nconstructed Diagnostic Knowledge Ontology and proposed a\nFunction-Behavior-Structure (FBS) model-based maintenance-record accumulation\nmethod based on it. Failure-cause inference using the maintenance records\naccumulated by the proposed method showed better agreement with the set of\ncandidate causes enumerated by experts, especially in difficult cases where the\nnumber of related cases is small and the vocabulary used differs. In the\nfuture, it will be necessary to develop inference methods tailored to these\nmaintenance records, build a user interface, and carry out validation on larger\nand more diverse systems. Additionally, this approach leverages the\nunderstanding and knowledge of the target in the design phase to support\nknowledge accumulation and problem solving during the maintenance phase, and it\nis expected to become a foundation for knowledge sharing across the entire\nengineering chain in the future.", "AI": {"tldr": "本研究构建了诊断知识本体和基于功能-行为-结构（FBS）模型的维护记录积累方法，以改进制造系统中的故障原因推断，尤其在困难案例中表现出更好的专家一致性。", "motivation": "在制造系统中，识别故障原因是维持和提高生产效率的关键。现有的基于知识的故障原因推断需要知识库能明确地结构化系统和故障知识，并包含足够长的因果链。", "method": "本研究构建了“诊断知识本体（Diagnostic Knowledge Ontology）”，并基于此提出了“功能-行为-结构（FBS）模型”的维护记录积累方法。", "result": "使用该方法积累的维护记录进行故障原因推断，与专家列出的候选原因集显示出更好的一致性，特别是在相关案例少且词汇使用不同的困难情况下。", "conclusion": "该方法利用设计阶段对目标的理解和知识来支持维护阶段的知识积累和问题解决，并有望成为未来整个工程链中知识共享的基础。"}}
{"id": "2510.11019", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11019", "abs": "https://arxiv.org/abs/2510.11019", "authors": ["Bingjie Tang", "Iretiayo Akinola", "Jie Xu", "Bowen Wen", "Dieter Fox", "Gaurav S. Sukhatme", "Fabio Ramos", "Abhishek Gupta", "Yashraj Narang"], "title": "Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies", "comment": "in submission. 8 pages, 6 figures. Website:\n  https://refinery-2025.github.io/refinery/", "summary": "Simulation-based learning has enabled policies for precise, contact-rich\ntasks (e.g., robotic assembly) to reach high success rates (~80%) under high\nlevels of observation noise and control error. Although such performance may be\nsufficient for research applications, it falls short of industry standards and\nmakes policy chaining exceptionally brittle. A key limitation is the high\nvariance in individual policy performance across diverse initial conditions. We\nintroduce Refinery, an effective framework that bridges this performance gap,\nrobustifying policy performance across initial conditions. We propose Bayesian\nOptimization-guided fine-tuning to improve individual policies, and Gaussian\nMixture Model-based sampling during deployment to select initializations that\nmaximize execution success. Using Refinery, we improve mean success rates by\n10.98% over state-of-the-art methods in simulation-based learning for robotic\nassembly, reaching 91.51% in simulation and comparable performance in the real\nworld. Furthermore, we demonstrate that these fine-tuned policies can be\nchained to accomplish long-horizon, multi-part\nassembly$\\unicode{x2013}$successfully assembling up to 8 parts without\nrequiring explicit multi-step training.", "AI": {"tldr": "Refinery框架通过贝叶斯优化微调和高斯混合模型采样，显著提升了模拟学习中机器人装配策略的成功率和鲁棒性，使其达到工业标准并支持策略链式执行。", "motivation": "现有基于模拟学习的机器人装配策略在接触密集型任务中，尽管在观测噪声和控制误差下能达到约80%的成功率，但其在不同初始条件下个体策略性能方差大，导致无法满足行业标准，且策略链式执行时非常脆弱。", "method": "本文提出了Refinery框架以弥合性能差距。它通过以下方法增强策略鲁棒性：1) 引入贝叶斯优化引导的微调来改进个体策略；2) 在部署期间使用基于高斯混合模型的采样来选择能最大化执行成功率的初始化条件。", "result": "使用Refinery，机器人装配的平均成功率比现有最先进方法提高了10.98%，在模拟中达到91.51%，并在真实世界中实现了可比的性能。此外，这些经过微调的策略可以链式执行，完成长周期、多部件的装配任务（成功装配多达8个部件），而无需明确的多步骤训练。", "conclusion": "Refinery框架有效地提升了策略在不同初始条件下的鲁棒性，显著提高了机器人装配任务的成功率，并使其能够可靠地进行策略链式执行，从而实现复杂的、多部件的组装任务。"}}
{"id": "2510.10196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10196", "abs": "https://arxiv.org/abs/2510.10196", "authors": ["Yizhi Wang", "Li Chen", "Qiang Huang", "Tian Guan", "Xi Deng", "Zhiyuan Shen", "Jiawen Li", "Xinrui Chen", "Bin Hu", "Xitong Ling", "Taojie Zhu", "Zirui Huang", "Deshui Yu", "Yan Liu", "Jiurun Chen", "Lianghui Zhu", "Qiming He", "Yiqing Liu", "Diwei Shi", "Hanzhong Liu", "Junbo Hu", "Hongyi Gao", "Zhen Song", "Xilong Zhao", "Chao He", "Ming Zhao", "Yonghong He"], "title": "From Generic to Specialized: A Subspecialty Diagnostic System Powered by Self-Supervised Learning for Cervical Histopathology", "comment": "32 pages, 6 figures", "summary": "Cervical cancer remains a major malignancy, necessitating extensive and\ncomplex histopathological assessments and comprehensive support tools. Although\ndeep learning shows promise, these models still lack accuracy and\ngeneralizability. General foundation models offer a broader reach but remain\nlimited in capturing subspecialty-specific features and task adaptability. We\nintroduce the Cervical Subspecialty Pathology (CerS-Path) diagnostic system,\ndeveloped through two synergistic pretraining stages: self-supervised learning\non approximately 190 million tissue patches from 140,000 slides to build a\ncervical-specific feature extractor, and multimodal enhancement with 2.5\nmillion image-text pairs, followed by integration with multiple downstream\ndiagnostic functions. Supporting eight diagnostic functions, including rare\ncancer classification and multimodal Q&A, CerS-Path surpasses prior foundation\nmodels in scope and clinical applicability. Comprehensive evaluations\ndemonstrate a significant advance in cervical pathology, with prospective\ntesting on 3,173 cases across five centers maintaining 99.38% screening\nsensitivity and excellent generalizability, highlighting its potential for\nsubspecialty diagnostic translation and cervical cancer screening.", "AI": {"tldr": "该研究引入了子宫颈亚专科病理（CerS-Path）诊断系统，通过两阶段预训练（自监督学习和多模态增强）构建，支持八种诊断功能，并在前瞻性测试中实现了99.38%的筛查敏感性和出色的泛化能力，有望用于子宫颈癌筛查和亚专科诊断。", "motivation": "子宫颈癌仍是主要恶性肿瘤，需要复杂组织病理学评估和全面的支持工具。现有深度学习模型缺乏准确性和泛化性，而通用基础模型在捕获亚专科特定特征和任务适应性方面存在局限性。", "method": "开发了子宫颈亚专科病理（CerS-Path）诊断系统，采用两个协同预训练阶段：首先，通过对来自14万张玻片的约1.9亿个组织斑块进行自监督学习，构建子宫颈特异性特征提取器；其次，利用250万个图像-文本对进行多模态增强，随后与多个下游诊断功能集成。", "result": "CerS-Path系统支持八种诊断功能（包括罕见癌症分类和多模态问答），在范围和临床适用性上超越了先前基础模型。在对五个中心的3,173例病例进行前瞻性测试中，保持了99.38%的筛查敏感性，并展现出卓越的泛化能力。", "conclusion": "CerS-Path系统在子宫颈病理学方面取得了重大进展，具有亚专科诊断转化和子宫颈癌筛查的巨大潜力。"}}
{"id": "2510.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11079", "abs": "https://arxiv.org/abs/2510.11079", "authors": ["Andrada Iulia Prajescu", "Roberto Confalonieri"], "title": "Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives", "comment": null, "summary": "Artificial Intelligence (AI) systems are increasingly deployed in legal\ncontexts, where their opacity raises significant challenges for fairness,\naccountability, and trust. The so-called ``black box problem'' undermines the\nlegitimacy of automated decision-making, as affected individuals often lack\naccess to meaningful explanations. In response, the field of Explainable AI\n(XAI) has proposed a variety of methods to enhance transparency, ranging from\nexample-based and rule-based techniques to hybrid and argumentation-based\napproaches. This paper promotes computational models of arguments and their\nrole in providing legally relevant explanations, with particular attention to\ntheir alignment with emerging regulatory frameworks such as the EU General Data\nProtection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We\nanalyze the strengths and limitations of different explanation strategies,\nevaluate their applicability to legal reasoning, and highlight how\nargumentation frameworks -- by capturing the defeasible, contestable, and\nvalue-sensitive nature of law -- offer a particularly robust foundation for\nexplainable legal AI. Finally, we identify open challenges and research\ndirections, including bias mitigation, empirical validation in judicial\nsettings, and compliance with evolving ethical and legal standards, arguing\nthat computational argumentation is best positioned to meet both technical and\nnormative requirements of transparency in the law domain.", "AI": {"tldr": "本文探讨了在法律领域部署AI系统时，解释性AI（XAI）的重要性，并强调计算论证模型因其能捕捉法律的特性，是提供法律相关解释并符合法规（如GDPR、AIA）的最佳方法。", "motivation": "AI系统在法律领域的部署带来了“黑箱问题”，损害了公平性、问责制和信任，因为受影响的个人往往无法获得有意义的解释。这促使了对解释性AI方法的需求。", "method": "本文分析了各种解释策略的优缺点，评估了它们在法律推理中的适用性，并特别关注计算论证模型。研究还探讨了这些方法与新兴监管框架（如欧盟GDPR和AI法案）的一致性。", "result": "研究发现，论证框架能够捕捉法律的可驳斥性、可争议性和价值敏感性，为可解释的法律AI提供了特别坚实的基础。它们在提供法律相关解释方面表现出强大的潜力。", "conclusion": "计算论证被认为是满足法律领域透明度技术和规范要求的最佳选择，尽管仍存在偏见缓解、司法环境中的实证验证以及遵守不断发展的道德和法律标准等开放挑战。"}}
{"id": "2510.10265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10265", "abs": "https://arxiv.org/abs/2510.10265", "authors": ["Liang Lin", "Miao Yu", "Moayad Aloqaily", "Zhenhong Zhou", "Kun Wang", "Linsey Pang", "Prakhar Mehrotra", "Qingsong Wen"], "title": "Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models", "comment": null, "summary": "Backdoor attacks are a significant threat to large language models (LLMs),\noften embedded via public checkpoints, yet existing defenses rely on\nimpractical assumptions about trigger settings. To address this challenge, we\npropose \\ourmethod, a defense framework that requires no prior knowledge of\ntrigger settings. \\ourmethod is based on the key observation that when\ndeliberately injecting known backdoors into an already-compromised model, both\nexisting unknown and newly injected backdoors aggregate in the representation\nspace. \\ourmethod leverages this through a two-stage process: \\textbf{first},\naggregating backdoor representations by injecting known triggers, and\n\\textbf{then}, performing recovery fine-tuning to restore benign outputs.\nExtensive experiments across multiple LLM architectures demonstrate that: (I)\n\\ourmethod reduces the average Attack Success Rate to 4.41\\% across multiple\nbenchmarks, outperforming existing baselines by 28.1\\%$\\sim$69.3\\%$\\uparrow$.\n(II) Clean accuracy and utility are preserved within 0.5\\% of the original\nmodel, ensuring negligible impact on legitimate tasks. (III) The defense\ngeneralizes across different types of backdoors, confirming its robustness in\npractical deployment scenarios.", "AI": {"tldr": "本文提出了一种名为 \\ourmethod 的防御框架，旨在无需预知触发器设置的情况下，有效抵御大型语言模型（LLMs）中的后门攻击，通过注入已知后门聚合表示并进行恢复微调。", "motivation": "大型语言模型（LLMs）面临后门攻击的严重威胁，这些攻击常通过公共检查点嵌入。然而，现有的防御方法依赖于对触发器设置不切实际的假设，这促使研究者寻求一种无需预知触发器设置的通用防御机制。", "method": "本文提出的 \\ourmethod 框架基于一个关键观察：当向已被攻陷的模型中故意注入已知后门时，现有未知后门和新注入后门在表示空间中会聚集。\\ourmethod 利用这一特性，通过两阶段过程实现防御：首先，通过注入已知触发器来聚合后门表示；然后，执行恢复微调以恢复良性输出。", "result": "实验结果表明：(I) \\ourmethod 将多基准测试上的平均攻击成功率（ASR）降低至 4.41%，优于现有基线 28.1% 至 69.3%。(II) 清洁准确性和实用性保持在原始模型的 0.5% 以内，对合法任务的影响可忽略不计。(III) 该防御方法对不同类型的后门具有泛化性，证实了其在实际部署场景中的鲁棒性。", "conclusion": "\\ourmethod 提供了一种无需预知触发器设置的有效且鲁棒的后门防御框架，显著降低了大型语言模型的攻击成功率，同时保持了模型的清洁准确性和实用性，并能泛化到不同后门类型，为LLMs的实际部署提供了重要的安全保障。"}}
{"id": "2510.10203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10203", "abs": "https://arxiv.org/abs/2510.10203", "authors": ["Dingyi Yao", "Xinyao Han", "Ruibo Ming", "Zhihang Song", "Lihui Peng", "Jianming Hu", "Danya Yao", "Yi Zhang"], "title": "A Style-Based Metric for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Image Datasets", "comment": "7 pages, 4 figures", "summary": "Ensuring the reliability of autonomous driving perception systems requires\nextensive environment-based testing, yet real-world execution is often\nimpractical. Synthetic datasets have therefore emerged as a promising\nalternative, offering advantages such as cost-effectiveness, bias free\nlabeling, and controllable scenarios. However, the domain gap between synthetic\nand real-world datasets remains a critical bottleneck for the generalization of\nAI-based autonomous driving models. Quantifying this synthetic-to-real gap is\nthus essential for evaluating dataset utility and guiding the design of more\neffective training pipelines. In this paper, we establish a systematic\nframework for quantifying the synthetic-to-real gap in autonomous driving\nsystems, and propose Style Embedding Distribution Discrepancy (SEDD) as a novel\nevaluation metric. Our framework combines Gram matrix-based style extraction\nwith metric learning optimized for intra-class compactness and inter-class\nseparation to extract style embeddings. Furthermore, we establish a benchmark\nusing publicly available datasets. Experiments are conducted on a variety of\ndatasets and sim-to-real methods, and the results show that our method is\ncapable of quantifying the synthetic-to-real gap. This work provides a\nstandardized quality control tool that enables systematic diagnosis and\ntargeted enhancement of synthetic datasets, advancing future development of\ndata-driven autonomous driving systems.", "AI": {"tldr": "本文提出了一种量化自动驾驶感知系统合成-真实数据域差距的系统框架和新评估指标SEDD，以提升合成数据集的效用和模型泛化能力。", "motivation": "自动驾驶感知系统需要大量基于环境的测试，但真实世界执行不切实际。合成数据集是替代方案，但其与真实世界数据之间的域差距是AI模型泛化能力的瓶颈。量化这一差距对于评估数据集效用和指导训练流程设计至关重要。", "method": "本文建立了一个量化自动驾驶系统合成-真实域差距的系统框架，并提出了一种名为风格嵌入分布差异（SEDD）的新颖评估指标。该框架结合了基于Gram矩阵的风格提取和优化了类内紧凑性及类间分离的度量学习，以提取风格嵌入。此外，还使用公开数据集建立了一个基准。", "result": "在各种数据集和仿真-真实方法上进行的实验表明，本文提出的方法能够有效量化合成-真实域差距。", "conclusion": "这项工作提供了一个标准化的质量控制工具，可实现对合成数据集的系统诊断和有针对性的改进，从而推动数据驱动的自动驾驶系统的未来发展。"}}
{"id": "2510.11036", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11036", "abs": "https://arxiv.org/abs/2510.11036", "authors": ["Yeonseo Lee", "Jungwook Mun", "Hyosup Shin", "Guebin Hwang", "Junhee Nam", "Taeyeop Lee", "Sungho Jo"], "title": "XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation", "comment": null, "summary": "Most robotic grasping methods are typically designed for single gripper\ntypes, which limits their applicability in real-world scenarios requiring\ndiverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp\ndetection framework that efficiently handles multiple gripper configurations.\nThe proposed method addresses data scarcity by systematically augmenting\nexisting datasets with multi-gripper annotations. XGrasp employs a hierarchical\ntwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)\nidentifies optimal locations using global scene information and gripper\nspecifications. In the second stage, an Angle-Width Predictor (AWP) refines the\ngrasp angle and width using local features. Contrastive learning in the AWP\nmodule enables zero-shot generalization to unseen grippers by learning\nfundamental grasping characteristics. The modular framework integrates\nseamlessly with vision foundation models, providing pathways for future\nvision-language capabilities. The experimental results demonstrate competitive\ngrasp success rates across various gripper types, while achieving substantial\nimprovements in inference speed compared to existing gripper-aware methods.\nProject page: https://sites.google.com/view/xgrasp", "AI": {"tldr": "XGrasp是一个实时、夹具感知的抓取检测框架，通过数据增强、两阶段预测和对比学习，实现对多种夹具配置的高效处理和对未知夹具的零样本泛化。", "motivation": "大多数机器人抓取方法专为单一夹具类型设计，限制了其在需要多种末端执行器的实际场景中的应用。", "method": "XGrasp提出了一种实时、夹具感知的抓取检测框架。它通过系统性地增强现有数据集的多夹具标注来解决数据稀缺问题。该方法采用分层两阶段架构：第一阶段，抓取点预测器（GPP）利用全局场景信息和夹具规格识别最佳抓取位置；第二阶段，角度-宽度预测器（AWP）利用局部特征精炼抓取角度和宽度。AWP模块中的对比学习使得模型能够通过学习基本的抓取特性，实现对未见过夹具的零样本泛化。该模块化框架可与视觉基础模型无缝集成。", "result": "实验结果表明，XGrasp在各种夹具类型上均取得了具有竞争力的抓取成功率，并且与现有夹具感知方法相比，推理速度大幅提升。", "conclusion": "XGrasp提供了一个高效且可泛化的多夹具抓取检测解决方案，具有与视觉基础模型集成的潜力，为未来的视觉-语言能力奠定了基础。"}}
{"id": "2510.10293", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10293", "abs": "https://arxiv.org/abs/2510.10293", "authors": ["Hongwei Chen", "Yishu Lei", "Dan Zhang", "Bo Ke", "Danxiang Zhu", "Xuyi Chen", "Yuxiang Lu", "Zhengjie Huang", "Shikun Feng", "Jingzhou He", "Yu Sun", "Hua Wu", "Haifeng Wang"], "title": "MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning", "comment": null, "summary": "Test-time scaling has emerged as a promising paradigm in language modeling,\nwherein additional computational resources are allocated during inference to\nenhance model performance. Recent approaches, such as DeepConf, have\ndemonstrated the efficacy of this strategy, however, they often incur\nsubstantial computational overhead to achieve competitive results. In this\nwork, we propose MatryoshkaThinking, a novel method that significantly reduces\ncomputational cost while maintaining state-of-the-art performance.\nSpecifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using\nonly 4% of the computation required by DeepConf. The core of our approach lies\nin the recursive exploitation of the model's intrinsic capabilities in\nreasoning, verification, and summarization, which collectively enhance the\nretention of correct solutions and reduce the disparity between Pass@k and\nPass@1. Comprehensive evaluations across multiple open-source models and\nchallenging multi-modal reasoning benchmarks validate the effectiveness and\ngenerality of our method. These findings offer new insights into the design of\nefficient and scalable test-time inference strategies for advanced language\nmodels.", "AI": {"tldr": "本文提出了一种名为 MatryoshkaThinking 的新型测试时扩展方法，它在语言模型推理过程中显著降低了计算成本，同时保持了最先进的性能，解决了现有方法计算开销大的问题。", "motivation": "现有的测试时扩展方法（如 DeepConf）虽然能提高模型性能，但通常需要巨大的计算开销才能取得有竞争力的结果。研究动机在于寻找一种能显著降低计算成本同时保持高性能的方法。", "method": "MatryoshkaThinking 方法的核心在于递归地利用模型固有的推理、验证和总结能力，以增强正确解决方案的保留并减少 Pass@k 和 Pass@1 之间的差异。", "result": "MatryoshkaThinking 在 AIME2025 上取得了 99.79 的分数，而计算量仅为 DeepConf 的 4%。该方法在多个开源模型和具有挑战性的多模态推理基准上得到了全面评估和验证，显示出其有效性和通用性。", "conclusion": "研究结果为设计高效且可扩展的先进语言模型测试时推理策略提供了新见解。"}}
{"id": "2510.11041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11041", "abs": "https://arxiv.org/abs/2510.11041", "authors": ["Shiyao Zhang", "Liwei Deng", "Shuyu Zhang", "Weijie Yuan", "Hong Zhang"], "title": "Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy", "comment": "Accepted by IEEE RA-L", "summary": "In future intelligent transportation systems, autonomous cooperative planning\n(ACP), becomes a promising technique to increase the effectiveness and security\nof multi-vehicle interactions. However, multiple uncertainties cannot be fully\naddressed for existing ACP strategies, e.g. perception, planning, and\ncommunication uncertainties. To address these, a novel deep reinforcement\nlearning-based autonomous cooperative planning (DRLACP) framework is proposed\nto tackle various uncertainties on cooperative motion planning schemes.\nSpecifically, the soft actor-critic (SAC) with the implementation of gate\nrecurrent units (GRUs) is adopted to learn the deterministic optimal\ntime-varying actions with imperfect state information occurred by planning,\ncommunication, and perception uncertainties. In addition, the real-time actions\nof autonomous vehicles (AVs) are demonstrated via the Car Learning to Act\n(CARLA) simulation platform. Evaluation results show that the proposed DRLACP\nlearns and performs cooperative planning effectively, which outperforms other\nbaseline methods under different scenarios with imperfect AV state information.", "AI": {"tldr": "针对多车辆交互中的感知、规划和通信不确定性，本文提出了一种基于深度强化学习的自主协同规划（DRLACP）框架。该框架采用带有GRU的SAC算法，有效学习不确定环境下的最优时变动作，并在CARLA仿真中表现优异。", "motivation": "未来的智能交通系统中，自主协同规划（ACP）对提高多车辆交互的效率和安全性至关重要。然而，现有的ACP策略无法充分解决感知、规划和通信等多种不确定性问题。", "method": "本文提出了一种新颖的基于深度强化学习的自主协同规划（DRLACP）框架。该框架采用软演员-评论家（SAC）算法，并结合门控循环单元（GRUs）来学习在规划、通信和感知不确定性导致的非完美状态信息下的确定性最优时变动作。通过Car Learning to Act (CARLA) 仿真平台展示了自动驾驶车辆（AVs）的实时动作。", "result": "评估结果表明，所提出的DRLACP框架能够有效地学习和执行协同规划。在不同场景下，即使存在不完美的自动驾驶车辆状态信息，DRLACP的性能也优于其他基线方法。", "conclusion": "DRLACP框架成功解决了自主协同运动规划中的多种不确定性问题，展现了其有效性和优越的性能，为未来智能交通系统中的多车辆交互提供了新的解决方案。"}}
{"id": "2510.10280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10280", "abs": "https://arxiv.org/abs/2510.10280", "authors": ["Yihong Liu", "Mingyang Wang", "François Yvon", "Hinrich Schütze"], "title": "On the Entity-Level Alignment in Crosslingual Consistency", "comment": "preprint", "summary": "Multilingual large language models (LLMs) are expected to recall factual\nknowledge consistently across languages. However, the factors that give rise to\nsuch crosslingual consistency -- and its frequent failure -- remain poorly\nunderstood. In this work, we hypothesize that these inconsistencies may arise\nfrom failures in entity alignment, the process of mapping subject and object\nentities into a shared conceptual space across languages. To test this, we\nassess alignment through entity-level (subject and object) translation tasks,\nand find that consistency is strongly correlated with alignment across all\nstudied models, with misalignment of subjects or objects frequently resulting\nin inconsistencies. Building on this insight, we propose SubSub and SubInj, two\neffective methods that integrate English translations of subjects into prompts\nacross languages, leading to substantial gains in both factual recall accuracy\nand consistency. Finally, our mechanistic analysis reveals that these\ninterventions reinforce the entity representation alignment in the conceptual\nspace through model's internal pivot-language processing, offering effective\nand practical strategies for improving multilingual factual prediction.", "AI": {"tldr": "本文探究了多语言大型语言模型（LLMs）跨语言事实知识回忆不一致的原因，发现实体对齐失败是主要因素。研究提出了SubSub和SubInj两种方法，通过将英文主语翻译整合到多语言提示中，显著提高了事实回忆的准确性和一致性，并揭示了其通过模型内部枢轴语言处理强化实体对齐的机制。", "motivation": "多语言LLMs应在不同语言中一致地回忆事实知识，但这种跨语言一致性常失败，其原因尚不清楚。研究假设这些不一致可能源于实体对齐失败，即无法将主体和客体实体映射到跨语言的共享概念空间中。", "method": "研究通过实体级别（主语和宾语）的翻译任务评估实体对齐情况。在此基础上，提出了SubSub和SubInj两种方法，将主语的英文翻译整合到跨语言提示中。最后，进行了机制分析，以理解这些干预措施如何起作用。", "result": "研究发现，事实回忆的一致性与所有研究模型中的实体对齐情况密切相关，主语或宾语的错位经常导致不一致。提出的SubSub和SubInj方法在事实回忆准确性和一致性方面均取得了显著提升。机制分析表明，这些干预措施通过模型的内部枢轴语言处理，强化了概念空间中的实体表示对齐。", "conclusion": "实体对齐是多语言LLMs实现跨语言事实一致性的关键因素。SubSub和SubInj是有效且实用的策略，通过强化实体对齐，可以显著提高多语言事实预测的准确性和一致性。"}}
{"id": "2510.10250", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 68U10", "I.2.6; I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2510.10250", "abs": "https://arxiv.org/abs/2510.10250", "authors": ["Jack Krolik", "Jake Lynn", "John Henry Rudden", "Dmytro Vremenko"], "title": "MRI Brain Tumor Detection with Computer Vision", "comment": "12 pages, 8 figures, final project report for CS4100 (Machine\n  Learning), Northeastern University, April 2024", "summary": "This study explores the application of deep learning techniques in the\nautomated detection and segmentation of brain tumors from MRI scans. We employ\nseveral machine learning models, including basic logistic regression,\nConvolutional Neural Networks (CNNs), and Residual Networks (ResNet) to\nclassify brain tumors effectively. Additionally, we investigate the use of\nU-Net for semantic segmentation and EfficientDet for anchor-based object\ndetection to enhance the localization and identification of tumors. Our results\ndemonstrate promising improvements in the accuracy and efficiency of brain\ntumor diagnostics, underscoring the potential of deep learning in medical\nimaging and its significance in improving clinical outcomes.", "AI": {"tldr": "本研究探索了深度学习技术在MRI脑肿瘤自动检测和分割中的应用，通过多种模型提高了诊断的准确性和效率。", "motivation": "研究动机是提高脑肿瘤诊断的准确性和效率，从而改善临床结果。", "method": "研究采用了多种机器学习模型：用于分类的基础逻辑回归、卷积神经网络（CNN）和残差网络（ResNet）；用于语义分割的U-Net；以及用于基于锚点目标检测的EfficientDet。", "result": "研究结果表明，在脑肿瘤诊断的准确性和效率方面取得了可喜的改进。", "conclusion": "深度学习在医学影像领域具有巨大潜力，对改善临床结果具有重要意义。"}}
{"id": "2510.10231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10231", "abs": "https://arxiv.org/abs/2510.10231", "authors": ["Chuangchuang Tan", "Xiang Ming", "Jinglu Wang", "Renshuai Tao", "Bin Li", "Yunchao Wei", "Yao Zhao", "Yan Lu"], "title": "Semantic Visual Anomaly Detection and Reasoning in AI-Generated Images", "comment": "27 pages, 7 figures", "summary": "The rapid advancement of\n  AI-generated content (AIGC) has enabled the synthesis of visually convincing\nimages; however, many such outputs exhibit subtle \\textbf{semantic anomalies},\nincluding unrealistic object configurations, violations of physical laws, or\ncommonsense inconsistencies, which compromise the overall plausibility of the\ngenerated scenes. Detecting these semantic-level anomalies\n  is essential for assessing the trustworthiness of AIGC media, especially in\nAIGC image analysis, explainable deepfake detection and semantic authenticity\nassessment. In this paper,\n  we formalize \\textbf{semantic anomaly detection and reasoning} for AIGC\nimages and\n  introduce \\textbf{AnomReason}, a large-scale benchmark with structured\nannotations as quadruples \\emph{(Name, Phenomenon, Reasoning, Severity)}.\nAnnotations are produced by\n  a modular multi-agent pipeline (\\textbf{AnomAgent}) with lightweight\nhuman-in-the-loop verification, enabling scale while preserving quality.\n  At construction time, AnomAgent processed approximately 4.17\\,B GPT-4o\ntokens, providing scale evidence for the resulting structured annotations. We\nfurther\n  show that models fine-tuned on AnomReason achieve consistent gains over\nstrong vision-language baselines under our proposed semantic matching metric\n(\\textit{SemAP} and \\textit{SemF1}).\n  Applications to {explainable deepfake detection} and {semantic reasonableness\nassessment of image generators} demonstrate practical utility. In summary,\nAnomReason and AnomAgent\n  serve as a foundation for measuring and improving the semantic plausibility\nof AI-generated images. We will release code, metrics, data, and task-aligned\nmodels to support reproducible research on semantic authenticity and\ninterpretable AIGC forensics.", "AI": {"tldr": "该研究针对AI生成图像中的语义异常，提出了一个大规模基准数据集AnomReason和多智能体标注流水线AnomAgent，以检测和推理这些异常，并展示了其在可解释深度伪造检测和图像生成器评估中的实用性。", "motivation": "AI生成内容（AIGC）虽然能生成逼真的图像，但常包含细微的语义异常（如不真实的物体配置、违反物理定律、常识不一致），这些异常损害了生成场景的合理性。检测这些语义层面的异常对于评估AIGC媒体的可信度、可解释深度伪造检测以及语义真实性评估至关重要。", "method": "本文将AIGC图像的“语义异常检测与推理”形式化，并引入了“AnomReason”——一个包含结构化标注（四元组：名称、现象、推理、严重性）的大规模基准数据集。标注通过一个模块化多智能体流水线“AnomAgent”生成，该流水线结合了轻量级的人工验证，以在大规模生产高质量标注。此外，他们提出了语义匹配指标（SemAP和SemF1）来评估模型性能。", "result": "AnomAgent在构建时处理了约4.17亿GPT-4o token，证明了其生成大规模结构化标注的能力。在AnomReason上微调的模型在所提出的语义匹配指标（SemAP和SemF1）上，相对于强大的视觉-语言基线模型取得了显著提升。研究还展示了其在可解释深度伪造检测和图像生成器语义合理性评估方面的实际应用价值。", "conclusion": "AnomReason和AnomAgent为衡量和提升AI生成图像的语义合理性奠定了基础。它们将支持关于语义真实性和可解释AIGC取证的再现性研究，未来将发布代码、指标、数据和任务对齐模型。"}}
{"id": "2510.11085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11085", "abs": "https://arxiv.org/abs/2510.11085", "authors": ["Yuxinyue Qian", "Jun Liu"], "title": "Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States", "comment": null, "summary": "With the rapid development of artificial intelligence (AI) technology,\nsocio-economic systems are entering a new stage of \"human-AI co-creation.\"\nBuilding upon a previously established multi-level intelligent agent economic\nmodel, this paper conducts simulation-based comparisons of macroeconomic output\nevolution in China and the United States under different mechanisms-AI\ncollaboration, network effects, and AI autonomous production. The results show\nthat: (1) when AI functions as an independent productive entity, the overall\ngrowth rate of social output far exceeds that of traditional human-labor-based\nmodels; (2) China demonstrates clear potential for acceleration in both the\nexpansion of intelligent agent populations and the pace of technological\ncatch-up, offering the possibility of achieving technological convergence or\neven partial surpassing. This study provides a systematic, model-based\nanalytical framework for understanding AI-driven production system\ntransformation and shifts in international competitiveness, as well as\nquantitative insights for relevant policy formulation.", "AI": {"tldr": "本研究利用多层智能主体经济模型，模拟比较了中美两国在AI协作、网络效应和AI自主生产机制下宏观经济产出的演变。结果表明，AI作为独立生产实体能显著提升社会产出增长，且中国在智能主体扩展和技术追赶方面具有加速潜力。", "motivation": "随着人工智能技术快速发展，社会经济系统进入“人机共创”新阶段，需要理解AI驱动的生产系统转型和国际竞争力变化。", "method": "该研究基于先前建立的多层智能主体经济模型，通过模拟比较了中国和美国在AI协作、网络效应和AI自主生产等不同机制下的宏观经济产出演变。", "result": "(1) 当AI作为独立的生产实体时，社会总产出的增长速度远超传统以人力劳动为基础的模型；(2) 中国在智能主体数量扩张和技术追赶速度上均展现出明显的加速潜力，有望实现技术趋同甚至局部超越。", "conclusion": "本研究为理解AI驱动的生产系统转型和国际竞争力变化提供了一个系统性的、基于模型的分析框架，并为相关政策制定提供了量化见解。"}}
{"id": "2510.11083", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11083", "abs": "https://arxiv.org/abs/2510.11083", "authors": ["Tianyi Tan", "Yinan Zheng", "Ruiming Liang", "Zexu Wang", "Kexin Zheng", "Jinliang Zheng", "Jianxiong Li", "Xianyuan Zhan", "Jingjing Liu"], "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling", "comment": "26 pages, 6 figures. Accepted at NeurIPS 2025", "summary": "Modeling interactive driving behaviors in complex scenarios remains a\nfundamental challenge for autonomous driving planning. Learning-based\napproaches attempt to address this challenge with advanced generative models,\nremoving the dependency on over-engineered architectures for representation\nfusion. However, brute-force implementation by simply stacking transformer\nblocks lacks a dedicated mechanism for modeling interactive behaviors that are\ncommon in real driving scenarios. The scarcity of interactive driving data\nfurther exacerbates this problem, leaving conventional imitation learning\nmethods ill-equipped to capture high-value interactive behaviors. We propose\nFlow Planner, which tackles these problems through coordinated innovations in\ndata modeling, model architecture, and learning scheme. Specifically, we first\nintroduce fine-grained trajectory tokenization, which decomposes the trajectory\ninto overlapping segments to decrease the complexity of whole trajectory\nmodeling. With a sophisticatedly designed architecture, we achieve efficient\ntemporal and spatial fusion of planning and scene information, to better\ncapture interactive behaviors. In addition, the framework incorporates flow\nmatching with classifier-free guidance for multi-modal behavior generation,\nwhich dynamically reweights agent interactions during inference to maintain\ncoherent response strategies, providing a critical boost for interactive\nscenario understanding. Experimental results on the large-scale nuPlan dataset\nand challenging interactive interPlan dataset demonstrate that Flow Planner\nachieves state-of-the-art performance among learning-based approaches while\neffectively modeling interactive behaviors in complex driving scenarios.", "AI": {"tldr": "Flow Planner通过数据建模、模型架构和学习方案的创新，有效解决了复杂场景下自动驾驶交互行为建模的挑战，并在大型数据集上实现了最先进的性能。", "motivation": "自动驾驶规划中，复杂场景下交互式驾驶行为建模仍是核心挑战。现有基于学习的方法依赖过度设计的架构，缺乏专门的交互行为建模机制，且交互式驾驶数据稀缺，导致传统模仿学习难以捕捉高价值交互行为。", "method": "本文提出了Flow Planner。具体方法包括：1) 引入细粒度轨迹分词，将轨迹分解为重叠片段以降低建模复杂性；2) 设计精巧的模型架构，实现规划与场景信息的时空高效融合，以更好地捕捉交互行为；3) 结合流匹配（flow matching）与无分类器引导（classifier-free guidance）进行多模态行为生成，在推理过程中动态重新加权智能体交互，以保持一致的响应策略。", "result": "在大型nuPlan数据集和具有挑战性的交互式interPlan数据集上的实验结果表明，Flow Planner在基于学习的方法中取得了最先进的性能，并能有效建模复杂驾驶场景中的交互行为。", "conclusion": "Flow Planner通过在数据建模、模型架构和学习方案上的协同创新，为复杂场景下的交互式驾驶行为建模提供了一个强大的解决方案，显著提升了交互场景的理解能力，并在现有方法中表现出色。"}}
{"id": "2510.11119", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11119", "abs": "https://arxiv.org/abs/2510.11119", "authors": ["Andrea Marinoni", "Sai Shivareddy", "Pietro Lio'", "Weisi Lin", "Erik Cambria", "Clare Grey"], "title": "Improving AI Efficiency in Data Centres by Power Dynamic Response", "comment": null, "summary": "The steady growth of artificial intelligence (AI) has accelerated in the\nrecent years, facilitated by the development of sophisticated models such as\nlarge language models and foundation models. Ensuring robust and reliable power\ninfrastructures is fundamental to take advantage of the full potential of AI.\nHowever, AI data centres are extremely hungry for power, putting the problem of\ntheir power management in the spotlight, especially with respect to their\nimpact on environment and sustainable development. In this work, we investigate\nthe capacity and limits of solutions based on an innovative approach for the\npower management of AI data centres, i.e., making part of the input power as\ndynamic as the power used for data-computing functions. The performance of\npassive and active devices are quantified and compared in terms of\ncomputational gain, energy efficiency, reduction of capital expenditure, and\nmanagement costs by analysing power trends from multiple data platforms\nworldwide. This strategy, which identifies a paradigm shift in the AI data\ncentre power management, has the potential to strongly improve the\nsustainability of AI hyperscalers, enhancing their footprint on environmental,\nfinancial, and societal fields.", "AI": {"tldr": "本文提出并研究了一种创新的AI数据中心电源管理方法，即将部分输入电源动态化，以应对其高能耗问题，旨在提高可持续性。", "motivation": "人工智能（AI）的快速发展，特别是大型语言模型和基础模型的出现，需要强大可靠的电力基础设施。然而，AI数据中心能耗巨大，对环境和可持续发展构成挑战，因此其电源管理问题成为关注焦点。", "method": "研究了一种创新的AI数据中心电源管理方法：使部分输入电源像数据计算功能所用电源一样动态化。通过分析全球多个数据平台的电源趋势，量化并比较了无源和有源设备在计算增益、能源效率、资本支出减少和管理成本方面的性能。", "result": "所提出的策略被认为是AI数据中心电源管理的一种范式转变，有潜力显著提高AI超大规模计算的可持续性，改善其在环境、财务和社会领域的影响。", "conclusion": "通过使部分输入电源动态化，可以有效管理AI数据中心的高能耗，从而显著提高其可持续性，并在环境、财务和社会方面产生积极影响。"}}
{"id": "2510.10254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10254", "abs": "https://arxiv.org/abs/2510.10254", "authors": ["Yuxiang Lai", "Jike Zhong", "Ming Li", "Yuheng Li", "Xiaofeng Yang"], "title": "Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?", "comment": null, "summary": "Recent advances in large generative models have shown that simple\nautoregressive formulations, when scaled appropriately, can exhibit strong\nzero-shot generalization across domains. Motivated by this trend, we\ninvestigate whether autoregressive video modeling principles can be directly\napplied to medical imaging tasks, despite the model never being trained on\nmedical data. Specifically, we evaluate a large vision model (LVM) in a\nzero-shot setting across four representative tasks: organ segmentation,\ndenoising, super-resolution, and motion prediction. Remarkably, even without\ndomain-specific fine-tuning, the LVM can delineate anatomical structures in CT\nscans and achieve competitive performance on segmentation, denoising, and\nsuper-resolution. Most notably, in radiotherapy motion prediction, the model\nforecasts future 3D CT phases directly from prior phases of a 4D CT scan,\nproducing anatomically consistent predictions that capture patient-specific\nrespiratory dynamics with realistic temporal coherence. We evaluate the LVM on\n4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no\nprior exposure to medical data, the model achieves strong performance across\nall tasks and surpasses specialized DVF-based and generative baselines in\nmotion prediction, achieving state-of-the-art spatial accuracy. These findings\nreveal the emergence of zero-shot capabilities in medical video modeling and\nhighlight the potential of general-purpose video models to serve as unified\nlearners and reasoners laying the groundwork for future medical foundation\nmodels built on video models.", "AI": {"tldr": "研究表明，大型视觉模型（LVM）即使未经医学数据训练，也能在医学影像任务中展现出强大的零样本泛化能力，尤其在运动预测方面表现出色。", "motivation": "受大型生成模型在自回归框架下通过适当扩展实现跨领域零样本泛化能力的启发，研究者希望探索自回归视频建模原则是否能直接应用于医学影像任务，即使模型从未在医学数据上训练过。", "method": "本研究在一个零样本设置下评估了一个大型视觉模型（LVM），覆盖了器官分割、去噪、超分辨率和运动预测四项代表性任务。特别是在运动预测中，模型直接从4D CT扫描的先前阶段预测未来的3D CT阶段。评估使用了来自122名患者的4D CT数据，共计超过1,820个3D CT体积。", "result": "LVM即使未经特定领域微调，也能在CT扫描中勾勒解剖结构，并在分割、去噪和超分辨率任务上取得有竞争力的表现。最值得注意的是，在放射治疗运动预测中，模型能直接从先前的4D CT阶段预测未来的3D CT阶段，生成解剖学上一致的预测，捕捉患者特异性呼吸动态，并具有逼真的时间一致性。模型在所有任务上都表现出色，并在运动预测方面超越了专门的DVF-based和生成式基线，达到了最先进的空间精度。", "conclusion": "这些发现揭示了医学视频建模中零样本能力的出现，并强调了通用视频模型作为统一学习器和推理器的潜力，为未来基于视频模型的医学基础模型奠定了基础。"}}
{"id": "2510.10328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10328", "abs": "https://arxiv.org/abs/2510.10328", "authors": ["Ananya Malik", "Nazanin Sabri", "Melissa Karnaze", "Mai Elsherief"], "title": "Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy", "comment": "9 pages, 4 figures, 4 tables, EMNLP 2025 Findings", "summary": "Large Language Models' (LLMs) ability to converse naturally is empowered by\ntheir ability to empathetically understand and respond to their users. However,\nemotional experiences are shaped by demographic and cultural contexts. This\nraises an important question: Can LLMs demonstrate equitable empathy across\ndiverse user groups? We propose a framework to investigate how LLMs' cognitive\nand affective empathy vary across user personas defined by intersecting\ndemographic attributes. Our study introduces a novel intersectional analysis\nspanning 315 unique personas, constructed from combinations of age, culture,\nand gender, across four LLMs. Results show that attributes profoundly shape a\nmodel's empathetic responses. Interestingly, we see that adding multiple\nattributes at once can attenuate and reverse expected empathy patterns. We show\nthat they broadly reflect real-world empathetic trends, with notable\nmisalignments for certain groups, such as those from Confucian culture. We\ncomplement our quantitative findings with qualitative insights to uncover model\nbehaviour patterns across different demographic groups. Our findings highlight\nthe importance of designing empathy-aware LLMs that account for demographic\ndiversity to promote more inclusive and equitable model behaviour.", "AI": {"tldr": "本研究调查了大型语言模型（LLMs）在不同人口统计学和文化背景的用户群体中，其共情能力是否公平，发现其共情反应受属性影响显著，并存在某些群体的偏差。", "motivation": "LLMs的自然对话能力得益于其共情理解和响应，但共情体验受人口统计学和文化背景影响。因此，研究旨在探讨LLMs能否对不同用户群体表现出公平的共情。", "method": "研究提出了一个框架来调查LLMs的认知和情感共情如何因交叉人口统计学属性定义的用户角色而异。通过结合年龄、文化和性别，构建了315个独特的交叉角色，并在四种LLMs上进行了新颖的交叉分析。研究结合了定量发现和定性见解。", "result": "结果显示，用户属性深刻地影响了模型的共情反应。有趣的是，同时添加多个属性可以减弱和逆转预期的共情模式。LLMs的共情表现大致反映了现实世界的共情趋势，但在某些群体（例如儒家文化背景的群体）中存在显著的偏差。", "conclusion": "研究强调了设计能够考虑人口多样性的共情感知LLMs的重要性，以促进更具包容性和公平性的模型行为。"}}
{"id": "2510.10329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10329", "abs": "https://arxiv.org/abs/2510.10329", "authors": ["Nam Luu", "Ondřej Bojar"], "title": "End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs", "comment": null, "summary": "Speech Translation (ST) is a machine translation task that involves\nconverting speech signals from one language to the corresponding text in\nanother language; this task has two different approaches, namely the\ntraditional cascade and the more recent end-to-end. This paper explores a\ncombined end-to-end architecture of pre-trained speech encoders and Large\nLanguage Models (LLMs) for performing both Automatic Speech Recognition (ASR)\nand ST simultaneously. Experiments with the English-to-German language pair\nshow that our best model not only can achieve better translation results than\nSeamlessM4T, a large foundational end-to-end, multi-modal translation model,\nbut can also match the performance of a cascaded system with Whisper and NLLB,\nwith up to a score gain of 8% in $\\text{COMET}^{\\text{DA}}_{22}$ metric.", "AI": {"tldr": "本文提出了一种结合预训练语音编码器和大型语言模型的端到端架构，用于同时进行自动语音识别和语音翻译，在英德语对上表现优于SeamlessM4T，并媲美级联系统。", "motivation": "语音翻译（ST）任务存在传统级联和新兴端到端两种方法。本文旨在探索一种结合预训练语音编码器和大型语言模型的端到端架构，以同时执行自动语音识别（ASR）和语音翻译（ST），可能旨在提高性能或效率。", "method": "研究采用了一种结合预训练语音编码器和大型语言模型（LLMs）的端到端架构，用于同时执行自动语音识别（ASR）和语音翻译（ST）。", "result": "在英德语对的实验中，最佳模型不仅取得了优于大型基础端到端多模态翻译模型SeamlessM4T的翻译结果，而且在$\\text{COMET}^{\\text{DA}}_{22}$指标上实现了高达8%的得分提升，性能与结合Whisper和NLLB的级联系统相当。", "conclusion": "所提出的结合预训练语音编码器和大型语言模型的端到端架构在语音翻译任务中表现出色，能够超越先进的端到端模型，并达到甚至超越强大的级联系统的性能。"}}
{"id": "2510.11103", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11103", "abs": "https://arxiv.org/abs/2510.11103", "authors": ["Martin Schuck", "Sherif Samy", "Angela P. Schoellig"], "title": "A Primer on SO(3) Action Representations in Deep Reinforcement Learning", "comment": null, "summary": "Many robotic control tasks require policies to act on orientations, yet the\ngeometry of SO(3) makes this nontrivial. Because SO(3) admits no global,\nsmooth, minimal parameterization, common representations such as Euler angles,\nquaternions, rotation matrices, and Lie algebra coordinates introduce distinct\nconstraints and failure modes. While these trade-offs are well studied for\nsupervised learning, their implications for actions in reinforcement learning\nremain unclear. We systematically evaluate SO(3) action representations across\nthree standard continuous control algorithms, PPO, SAC, and TD3, under dense\nand sparse rewards. We compare how representations shape exploration, interact\nwith entropy regularization, and affect training stability through empirical\nstudies and analyze the implications of different projections for obtaining\nvalid rotations from Euclidean network outputs. Across a suite of robotics\nbenchmarks, we quantify the practical impact of these choices and distill\nsimple, implementation-ready guidelines for selecting and using rotation\nactions. Our results highlight that representation-induced geometry strongly\ninfluences exploration and optimization and show that representing actions as\ntangent vectors in the local frame yields the most reliable results across\nalgorithms.", "AI": {"tldr": "本文系统评估了强化学习中SO(3)动作表示对机器人控制任务的影响，发现局部坐标系中的切向量表示最为可靠。", "motivation": "许多机器人控制任务需要策略作用于姿态（SO(3)），但SO(3)的几何特性使其难以处理，没有全局、平滑、最小的参数化表示。常见的表示方法（如欧拉角、四元数、旋转矩阵、李代数坐标）各有优缺点，这些在监督学习中已有所研究，但在强化学习的动作空间中其影响尚不明确。", "method": "研究系统评估了SO(3)动作表示在三种标准连续控制算法（PPO、SAC、TD3）以及密集和稀疏奖励设置下的表现。比较了不同表示如何影响探索、与熵正则化的交互以及训练稳定性。通过实证研究分析了从欧几里得网络输出中获得有效旋转的不同投影方法的影响。", "result": "表示引起的几何特性强烈影响探索和优化。将动作表示为局部坐标系中的切向量，在各种算法中产生了最可靠的结果。研究还量化了这些选择的实际影响，并提炼出简单、可直接用于实现的旋转动作选择和使用指南。", "conclusion": "SO(3)动作表示的选择对强化学习中的机器人控制至关重要。局部坐标系中的切向量表示在探索和优化方面表现最佳，并能提供最可靠的训练结果。"}}
{"id": "2510.11143", "categories": ["cs.AI", "cs.HC", "68U35, 62P30", "I.2.2"], "pdf": "https://arxiv.org/pdf/2510.11143", "abs": "https://arxiv.org/abs/2510.11143", "authors": ["Chuke Chen", "Biao Luo", "Nan Li", "Boxiang Wang", "Hang Yang", "Jing Guo", "Ming Xu"], "title": "Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis", "comment": "19 pages,5 figures", "summary": "The rapid expansion of scientific data has widened the gap between analytical\ncapability and research intent. Existing AI-based analysis tools, ranging from\nAutoML frameworks to agentic research assistants, either favor automation over\ntransparency or depend on manual scripting that hinders scalability and\nreproducibility. We present ARIA (Automated Research Intelligence Assistant), a\nspec-driven, human-in-the-loop framework for automated and interpretable data\nanalysis. ARIA integrates six interoperable layers, namely Command, Context,\nCode, Data, Orchestration, and AI Module, within a document-centric workflow\nthat unifies human reasoning and machine execution. Through natural-language\nspecifications, researchers define analytical goals while ARIA autonomously\ngenerates executable code, validates computations, and produces transparent\ndocumentation. Beyond achieving high predictive accuracy, ARIA can rapidly\nidentify optimal feature sets and select suitable models, minimizing redundant\ntuning and repetitive experimentation. In the Boston Housing case, ARIA\ndiscovered 25 key features and determined XGBoost as the best performing model\n(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous\ndomains demonstrate ARIA's strong performance, interpretability, and efficiency\ncompared with state-of-the-art systems. By combining AI for research and AI for\nscience principles within a spec-driven architecture, ARIA establishes a new\nparadigm for transparent, collaborative, and reproducible scientific discovery.", "AI": {"tldr": "ARIA是一个规范驱动、人机协作的框架，旨在通过自然语言规范实现自动化和可解释的数据分析，从而弥合科学数据分析能力与研究意图之间的差距。", "motivation": "科学数据量激增，导致分析能力与研究意图之间存在鸿沟。现有的AI分析工具（如AutoML和代理研究助手）要么牺牲透明度以追求自动化，要么依赖手动脚本，限制了可扩展性和可重复性。", "method": "ARIA（Automated Research Intelligence Assistant）是一个规范驱动、人机协作的框架，用于自动化和可解释的数据分析。它集成了命令、上下文、代码、数据、编排和AI模块六个可互操作的层，并采用以文档为中心的工作流，统一了人类推理和机器执行。研究人员通过自然语言规范定义分析目标，ARIA则自主生成可执行代码、验证计算并生成透明文档。", "result": "ARIA不仅实现了高预测准确性，还能快速识别最佳特征集并选择合适的模型，从而最大程度地减少冗余调优和重复实验。在Boston Housing案例中，ARIA发现了25个关键特征，并确定XGBoost为表现最佳的模型（R平方=0.93），且过拟合最小。在跨异构领域的评估中，ARIA在性能、可解释性和效率方面均优于现有先进系统。", "conclusion": "ARIA通过将“AI for research”和“AI for science”原则整合到规范驱动的架构中，为透明、协作和可重复的科学发现建立了一个新范式。"}}
{"id": "2510.11144", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11144", "abs": "https://arxiv.org/abs/2510.11144", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "title": "$How^{2}$: How to learn from procedural How-to questions", "comment": null, "summary": "An agent facing a planning problem can use answers to how-to questions to\nreduce uncertainty and fill knowledge gaps, helping it solve both current and\nfuture tasks. However, their open ended nature, where valid answers to \"How do\nI X?\" range from executable actions to high-level descriptions of X's\nsub-goals, makes them challenging for AI agents to ask, and for AI experts to\nanswer, in ways that support efficient planning. We introduce $How^{2}$, a\nmemory agent framework that enables agents to ask how-to questions, store the\nanswers, and reuse them for lifelong learning in interactive environments. We\nevaluate our approach in Plancraft, a Minecraft crafting environment, where\nagents must complete an assembly task by manipulating inventory items. Using\nteacher models that answer at varying levels of abstraction, from executable\naction sequences to high-level subgoal descriptions, we show that lifelong\nlearning agents benefit most from answers that are abstracted and decoupled\nfrom the current state. $How^{2}$ offers a way for LLM-based agents to improve\ntheir planning capabilities over time by asking questions in interactive\nenvironments.", "AI": {"tldr": "How²是一个记忆代理框架，使代理能够提出、存储和重用“如何做”问题答案，以进行终身学习和规划。研究表明，抽象且与当前状态解耦的答案对终身学习代理的益处最大。", "motivation": "规划代理需要通过“如何做”问题来减少不确定性并填补知识空白，但这些问题的开放性（答案范围从可执行动作到高层子目标描述）使得AI代理难以有效提问，AI专家难以有效回答，从而阻碍了高效规划。", "method": "本文提出了How²，一个记忆代理框架，允许代理在交互式环境中提问“如何做”问题、存储答案并重用它们进行终身学习。该方法在Plancraft（一个Minecraft合成环境）中进行评估，教师模型提供从可执行动作序列到高层子目标描述等不同抽象级别的答案。", "result": "研究结果表明，终身学习代理从抽象且与当前状态解耦的答案中获益最多。", "conclusion": "How²为基于大型语言模型的代理提供了一种方法，使其能够通过在交互式环境中提问，随时间推移提高其规划能力。"}}
{"id": "2510.11094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11094", "abs": "https://arxiv.org/abs/2510.11094", "authors": ["Junxiang Wang", "Han Zhang", "Zehao Wang", "Huaiyuan Chen", "Pu Wang", "Weidong Chen"], "title": "Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation", "comment": null, "summary": "Effective rehabilitation methods are essential for the recovery of lower limb\ndysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great\npotentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are\nusually heavy and need a lot of work to help the patients to put them on.\nMoreover, it also requires extra compliance control to guarantee the safety. In\ncontrast, soft exoskeletons are easy and comfortable to wear and have intrinsic\ncompliance, but their complex nonlinear human-robot interaction dynamics would\npose significant challenges for control. In this work, based on the pneumatic\nactuators inspired by origami, we design a rehabilitation exoskeleton for knee\nthat is easy and comfortable to wear. To guarantee the control performance and\nenable a nice human-robot interaction, we first use Deep Koopman Network to\nmodel the human-robot interaction dynamics. In particular, by viewing the\nelectromyography (EMG) signals and the duty cycle of the PWM wave that controls\nthe pneumatic robot's valves and pump as the inputs, the linear Koopman model\naccurately captures the complex human-robot interaction dynamics. Next, based\non the obtained Koopman model, we further use Model Predictive Control (MPC) to\ncontrol the soft robot and help the user to do rehabilitation training in\nreal-time. The goal of the rehabilitation training is to track a given\nreference signal shown on the screen. Experiments show that by integrating the\nEMG signals into the Koopman model, we have improved the model accuracy to\ngreat extent. In addition, a personalized Koopman model trained from the\nindividual's own data performs better than the non-personalized model.\nConsequently, our control framework outperforms the traditional PID control in\nboth passive and active training modes. Hence the proposed method provides a\nnew control framework for soft rehabilitation robots.", "AI": {"tldr": "本文设计了一种基于折纸气动执行器的软体膝关节康复外骨骼，并提出了一种新的控制框架。该框架利用深度Koopman网络结合肌电信号（EMG）对复杂人机交互动力学进行建模，并在此基础上采用模型预测控制（MPC）实现实时康复训练，其性能优于传统PID控制。", "motivation": "中风引起的下肢功能障碍康复至关重要。传统刚性外骨骼笨重、穿戴困难且需要额外的柔顺控制。软体外骨骼穿戴舒适且具有固有柔顺性，但其复杂非线性人机交互动力学给控制带来了巨大挑战。", "method": "研究人员设计了一种易于穿戴的基于折纸气动执行器的软体膝关节康复外骨骼。为保证控制性能和良好的人机交互，首先利用深度Koopman网络对人机交互动力学进行建模，将肌电信号（EMG）和控制气动机器人阀门和泵的PWM波占空比作为输入，获得能准确捕捉复杂动力学的线性Koopman模型。其次，基于该Koopman模型，进一步使用模型预测控制（MPC）来实时控制软体机器人，帮助用户进行康复训练，目标是跟踪屏幕上给定的参考信号。", "result": "实验结果表明，将肌电信号（EMG）整合到Koopman模型中大大提高了模型精度。此外，通过个体数据训练的个性化Koopman模型比非个性化模型表现更好。最终，所提出的控制框架在被动和主动训练模式下均优于传统PID控制。", "conclusion": "该研究提供了一种针对软体康复机器人的新型控制框架，通过准确建模人机交互和优越的控制性能，有效解决了软体外骨骼的控制挑战。"}}
{"id": "2510.10384", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10384", "abs": "https://arxiv.org/abs/2510.10384", "authors": ["Hakyung Sung", "Kristopher Kyle"], "title": "ASC analyzer: A Python package for measuring argument structure construction usage in English texts", "comment": "Accepted to the 2nd Workshop on Construction Grammars and NLP\n  (CxGs+NLP)", "summary": "Argument structure constructions (ASCs) offer a theoretically grounded lens\nfor analyzing second language (L2) proficiency, yet scalable and systematic\ntools for measuring their usage remain limited. This paper introduces the ASC\nanalyzer, a publicly available Python package designed to address this gap. The\nanalyzer automatically tags ASCs and computes 50 indices that capture\ndiversity, proportion, frequency, and ASC-verb lemma association strength. To\ndemonstrate its utility, we conduct both bivariate and multivariate analyses\nthat examine the relationship between ASC-based indices and L2 writing scores.", "AI": {"tldr": "本文介绍了一个名为“ASC分析器”的Python包，用于自动标记和测量第二语言（L2）中的论元结构构式（ASCs）使用情况，并通过分析其与L2写作分数的关系来展示其效用。", "motivation": "现有用于衡量第二语言（L2）能力中论元结构构式（ASCs）使用的工具缺乏可扩展性和系统性。", "method": "开发了一个公开可用的Python包——ASC分析器，该工具能自动标记ASCs并计算50个指标，包括多样性、比例、频率和ASC-动词词元关联强度。通过双变量和多变量分析来检验基于ASC的指标与L2写作分数之间的关系。", "result": "通过双变量和多变量分析，展示了ASC分析器的实用性，并考察了基于ASC的指标与L2写作分数之间的关系。", "conclusion": "ASC分析器提供了一个理论基础的、可扩展且系统化的工具，用于分析L2熟练度中的论元结构构式使用情况。"}}
{"id": "2510.10257", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10257", "abs": "https://arxiv.org/abs/2510.10257", "authors": ["Abdelrhman Elrawy", "Emad A. Mohammed"], "title": "Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its\nstandard adaptive density control (ADC) can lead to overfitting and bloated\nreconstructions. While state-of-the-art methods like FSGS improve quality, they\noften do so by significantly increasing the primitive count. This paper\npresents a framework that revises the core 3DGS optimization to prioritize\nefficiency. We replace the standard positional gradient heuristic with a novel\ndensification trigger that uses the opacity gradient as a lightweight proxy for\nrendering error. We find this aggressive densification is only effective when\npaired with a more conservative pruning schedule, which prevents destructive\noptimization cycles. Combined with a standard depth-correlation loss for\ngeometric guidance, our framework demonstrates a fundamental improvement in\nefficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k\nvs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a\nreduction of approximately 70%. This dramatic gain in compactness is achieved\nwith a modest trade-off in reconstruction metrics, establishing a new\nstate-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view\nsynthesis.", "AI": {"tldr": "本文提出一个3D Gaussian Splatting (3DGS) 优化框架，通过使用不透明度梯度作为致密化触发器并结合保守修剪策略，显著提高了在少样本场景下的模型紧凑性，并在质量-效率权衡上达到了新的SOTA。", "motivation": "3DGS在少样本场景中易出现过拟合和重建膨胀问题，其标准自适应密度控制（ADC）效果不佳。尽管FSGS等现有方法提高了质量，但通常以显著增加基元数量为代价，导致效率低下。因此，需要一个能在少样本视图合成中兼顾质量和效率的3DGS优化方法。", "method": "该框架重新设计了3DGS的核心优化过程，以优先考虑效率。具体方法包括：1) 使用不透明度梯度作为新的致密化触发器，替代标准的基于位置梯度的启发式方法，作为渲染误差的轻量级代理；2) 结合更保守的修剪策略，以防止破坏性的优化循环；3) 引入标准的深度相关损失（depth-correlation loss）以提供几何指导。", "result": "在3视角的LLFF数据集上，该模型比FSGS紧凑40%以上（32k vs 57k基元）。在Mip-NeRF 360数据集上，基元数量减少了约70%。这些显著的紧凑性提升仅伴随重建指标上的适度权衡，从而在少样本视图合成的质量-效率Pareto前沿上建立了新的SOTA。", "conclusion": "通过引入新的致密化触发器和保守修剪策略，本文提出的框架显著提升了3DGS在少样本场景下的效率和紧凑性，在保持合理重建质量的同时，大幅减少了模型基元数量，为少样本视图合成的质量与效率权衡提供了新的最佳解决方案。"}}
{"id": "2510.11258", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11258", "abs": "https://arxiv.org/abs/2510.11258", "authors": ["Yuhui Fu", "Feiyang Xie", "Chaoyi Xu", "Jing Xiong", "Haoqi Yuan", "Zongqing Lu"], "title": "DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation", "comment": null, "summary": "Loco-manipulation is a fundamental challenge for humanoid robots to achieve\nversatile interactions in human environments. Although recent studies have made\nsignificant progress in humanoid whole-body control, loco-manipulation remains\nunderexplored and often relies on hard-coded task definitions or costly\nreal-world data collection, which limits autonomy and generalization. We\npresent DemoHLM, a framework for humanoid loco-manipulation that enables\ngeneralizable loco-manipulation on a real humanoid robot from a single\ndemonstration in simulation. DemoHLM adopts a hierarchy that integrates a\nlow-level universal whole-body controller with high-level manipulation policies\nfor multiple tasks. The whole-body controller maps whole-body motion commands\nto joint torques and provides omnidirectional mobility for the humanoid robot.\nThe manipulation policies, learned in simulation via our data generation and\nimitation learning pipeline, command the whole-body controller with closed-loop\nvisual feedback to execute challenging loco-manipulation tasks. Experiments\nshow a positive correlation between the amount of synthetic data and policy\nperformance, underscoring the effectiveness of our data generation pipeline and\nthe data efficiency of our approach. Real-world experiments on a Unitree G1\nrobot equipped with an RGB-D camera validate the sim-to-real transferability of\nDemoHLM, demonstrating robust performance under spatial variations across ten\nloco-manipulation tasks.", "AI": {"tldr": "DemoHLM是一个人形机器人移动-操作框架，它通过单次模拟演示，结合分层控制和闭环视觉反馈，实现了在真实人形机器人上的通用移动-操作能力，并有效解决了sim-to-real迁移问题。", "motivation": "人形机器人的移动-操作是实现多功能人机交互的基础挑战，但现有研究仍未充分探索，常依赖硬编码任务或昂贵的真实数据收集，这限制了机器人的自主性和泛化能力。", "method": "本文提出了DemoHLM框架，采用分层结构：底层是通用的全身控制器（将全身运动指令映射为关节扭矩，提供全向移动能力），高层是针对多任务的操纵策略。操纵策略通过模拟中的数据生成和模仿学习管线获得，并利用闭环视觉反馈指挥全身控制器执行移动-操作任务。", "result": "实验结果显示，合成数据量与策略性能呈正相关，证明了数据生成管线的有效性和方法的数据效率。在配备RGB-D摄像头的Unitree G1机器人上的真实世界实验验证了DemoHLM的sim-to-real迁移能力，在十项移动-操作任务中，即使存在空间变化，也表现出鲁棒性能。", "conclusion": "DemoHLM框架通过单次模拟演示，成功实现了真实人形机器人的通用移动-操作能力，其分层控制、数据生成与模仿学习管线以及闭环视觉反馈，有效促进了sim-to-real迁移，并在多种复杂任务中展现了鲁棒性。"}}
{"id": "2510.11194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11194", "abs": "https://arxiv.org/abs/2510.11194", "authors": ["Peiming Li", "Zhiyuan Hu", "Yang Tang", "Shiyu Li", "Xi Chen"], "title": "Aligning Deep Implicit Preferences by Learning to Reason Defensively", "comment": null, "summary": "Personalized alignment is crucial for enabling Large Language Models (LLMs)\nto engage effectively in user-centric interactions. However, current methods\nface a dual challenge: they fail to infer users' deep implicit preferences\n(including unstated goals, semantic context and risk tolerances), and they lack\nthe defensive reasoning required to navigate real-world ambiguity. This\ncognitive gap leads to responses that are superficial, brittle and\nshort-sighted. To address this, we propose Critique-Driven Reasoning Alignment\n(CDRA), which reframes alignment from a scalar reward-matching task into a\nstructured reasoning process. First, to bridge the preference inference gap, we\nintroduce the DeepPref benchmark. This dataset, comprising 3000\npreference-query pairs across 20 topics, is curated by simulating a\nmulti-faceted cognitive council that produces critique-annotated reasoning\nchains to deconstruct query semantics and reveal latent risks. Second, to\ninstill defensive reasoning, we introduce the Personalized Generative Process\nReward Model (Pers-GenPRM), which frames reward modeling as a personalized\nreasoning task. It generates a critique chain to evaluate a response's\nalignment with user preferences before outputting a final score based on this\nrationale. Ultimately, this interpretable, structured reward signal guides\npolicy model through Critique-Driven Policy Alignment, a process-level online\nreinforcement learning algorithm integrating both numerical and natural\nlanguage feedback. Experiments demonstrate that CDRA excels at discovering and\naligning with users' true preferences while executing robust reasoning. Our\ncode and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.", "AI": {"tldr": "本文提出批判驱动推理对齐（CDRA）框架，通过结构化推理过程和可解释的奖励信号，解决大型语言模型（LLMs）在个性化对齐中难以推断深层隐式偏好和缺乏防御性推理的问题。", "motivation": "当前大型语言模型（LLMs）在个性化对齐方面面临双重挑战：它们无法推断用户深层隐式偏好（包括未说明的目标、语义上下文和风险容忍度），并且缺乏应对现实世界模糊性所需的防御性推理能力。这导致模型响应肤浅、脆弱且短视。", "method": "本文提出批判驱动推理对齐（CDRA）框架，将对齐重构为结构化推理过程。具体方法包括：1) 引入DeepPref基准数据集，包含3000个偏好-查询对，通过模拟多面认知委员会生成批判注释的推理链来解构查询语义并揭示潜在风险，以弥合偏好推断差距。2) 引入个性化生成过程奖励模型（Pers-GenPRM），将奖励建模视为个性化推理任务，生成批判链来评估响应与用户偏好的一致性，然后基于此理由输出最终分数，以灌输防御性推理。3) 通过批判驱动策略对齐，一种整合数值和自然语言反馈的流程级在线强化学习算法，利用这种可解释的、结构化的奖励信号指导策略模型。", "result": "实验证明，CDRA在发现并与用户真实偏好对齐方面表现出色，同时执行了鲁棒的推理。", "conclusion": "CDRA通过将对齐重构为结构化推理过程，并利用批判驱动的基准数据集和奖励模型，显著提升了大型语言模型在个性化交互中推断深层用户偏好和进行防御性推理的能力。"}}
{"id": "2510.10287", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10287", "abs": "https://arxiv.org/abs/2510.10287", "authors": ["Markus Käppeler", "Özgün Çiçek", "Daniele Cattaneo", "Claudius Gläser", "Yakov Miron", "Abhinav Valada"], "title": "Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking", "comment": null, "summary": "Camera-based 3D object detection and tracking are essential for perception in\nautonomous driving. Current state-of-the-art approaches often rely exclusively\non either perspective-view (PV) or bird's-eye-view (BEV) features, limiting\ntheir ability to leverage both fine-grained object details and spatially\nstructured scene representations. In this work, we propose DualViewDistill, a\nhybrid detection and tracking framework that incorporates both PV and BEV\ncamera image features to leverage their complementary strengths. Our approach\nintroduces BEV maps guided by foundation models, leveraging descriptive DINOv2\nfeatures that are distilled into BEV representations through a novel\ndistillation process. By integrating PV features with BEV maps enriched with\nsemantic and geometric features from DINOv2, our model leverages this hybrid\nrepresentation via deformable aggregation to enhance 3D object detection and\ntracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks\ndemonstrate that DualViewDistill achieves state-of-the-art performance. The\nresults showcase the potential of foundation model BEV maps to enable more\nreliable perception for autonomous driving. We make the code and pre-trained\nmodels available at https://dualviewdistill.cs.uni-freiburg.de .", "AI": {"tldr": "DualViewDistill是一个混合检测和跟踪框架，它结合了透视图(PV)和鸟瞰图(BEV)特征，并通过将DINOv2基础模型的描述性特征蒸馏到BEV表示中来增强BEV地图，从而在自动驾驶的3D目标检测和跟踪方面达到了最先进的性能。", "motivation": "目前的3D目标检测和跟踪方法要么单独依赖透视图(PV)特征，要么单独依赖鸟瞰图(BEV)特征。这限制了它们同时利用细粒度目标细节（PV优势）和空间结构化场景表示（BEV优势）的能力，未能充分发挥两者的互补优势。", "method": "本文提出了DualViewDistill，一个混合检测和跟踪框架。它整合了PV和BEV相机图像特征。该方法引入了由基础模型（如DINOv2）引导的BEV地图，通过一种新颖的蒸馏过程，将DINOv2的描述性特征蒸馏到BEV表示中。模型通过可变形聚合来利用这种结合了PV特征和富含DINOv2语义及几何特征的BEV地图的混合表示，以增强3D目标检测和跟踪。", "result": "在nuScenes和Argoverse 2基准测试上进行了广泛的实验，结果表明DualViewDistill达到了最先进的性能。", "conclusion": "研究结果展示了基础模型引导的BEV地图在实现更可靠的自动驾驶感知方面的潜力。DualViewDistill通过有效结合PV和BEV特征，并利用基础模型的强大能力，显著提升了3D目标检测和跟踪的性能。"}}
{"id": "2510.11235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11235", "abs": "https://arxiv.org/abs/2510.11235", "authors": ["Leonard Dung", "Florian Mai"], "title": "AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?", "comment": "under review", "summary": "AI alignment research aims to develop techniques to ensure that AI systems do\nnot cause harm. However, every alignment technique has failure modes, which are\nconditions in which there is a non-negligible chance that the technique fails\nto provide safety. As a strategy for risk mitigation, the AI safety community\nhas increasingly adopted a defense-in-depth framework: Conceding that there is\nno single technique which guarantees safety, defense-in-depth consists in\nhaving multiple redundant protections against safety failure, such that safety\ncan be maintained even if some protections fail. However, the success of\ndefense-in-depth depends on how (un)correlated failure modes are across\nalignment techniques. For example, if all techniques had the exact same failure\nmodes, the defense-in-depth approach would provide no additional protection at\nall. In this paper, we analyze 7 representative alignment techniques and 7\nfailure modes to understand the extent to which they overlap. We then discuss\nour results' implications for understanding the current level of risk and how\nto prioritize AI alignment research in the future.", "AI": {"tldr": "AI对齐研究采用“深度防御”策略来减轻风险，但其有效性取决于不同对齐技术之间故障模式的（非）相关性。本文分析了7种代表性对齐技术和7种故障模式的重叠程度，并讨论了其对风险评估和未来研究优先级的影响。", "motivation": "AI对齐技术都有其故障模式，即在特定条件下可能无法确保AI安全的风险。AI安全社区已采用“深度防御”框架，即通过多重冗余保护来维持安全。然而，这种策略的成功关键在于不同对齐技术之间故障模式的（非）相关性。如果所有技术共享相同的故障模式，深度防御将无效。因此，理解这些故障模式的重叠程度至关重要。", "method": "本文分析了7种代表性的AI对齐技术和7种具体的故障模式，以评估它们之间的重叠程度。", "result": "本文分析了7种AI对齐技术和7种故障模式之间的重叠程度。 （抽象未提供具体分析结果，仅说明进行了分析）", "conclusion": "分析结果对理解当前AI风险水平以及如何优先规划未来的AI对齐研究具有重要意义。"}}
{"id": "2510.10390", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10390", "abs": "https://arxiv.org/abs/2510.10390", "authors": ["Aashiq Muhamed", "Leonardo F. R. Ribeiro", "Markus Dreyer", "Virginia Smith", "Mona T. Diab"], "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models", "comment": null, "summary": "The ability of language models in RAG systems to selectively refuse to answer\nbased on flawed context is critical for safety, yet remains a significant\nfailure point. Our large-scale study reveals that even frontier models struggle\nin this setting, with refusal accuracy dropping below 50% on multi-document\ntasks, while exhibiting either dangerous overconfidence or overcaution. Static\nbenchmarks fail to reliably evaluate this capability, as models exploit\ndataset-specific artifacts and memorize test instances. We introduce\nRefusalBench, a generative methodology that programmatically creates diagnostic\ntest cases through controlled linguistic perturbation. Our framework employs\n176 distinct perturbation strategies across six categories of informational\nuncertainty and three intensity levels. Evaluation of over 30 models uncovers\nsystematic failure patterns: refusal comprises separable detection and\ncategorization skills, and neither scale nor extended reasoning improves\nperformance. We find that selective refusal is a trainable, alignment-sensitive\ncapability, offering a clear path for improvement. We release two benchmarks --\nRefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --\nand our complete generation framework to enable continued, dynamic evaluation\nof this critical capability.", "AI": {"tldr": "RAG系统中语言模型基于错误上下文选择性拒绝回答的能力对安全性至关重要，但仍是一个显著的弱点。本研究发现即使是前沿模型也表现不佳，并引入了RefusalBench，一个生成式评估方法，揭示了系统性失败模式，并指出选择性拒绝是一种可训练的、对齐敏感的能力。", "motivation": "RAG系统中语言模型基于有缺陷的上下文选择性拒绝回答的能力对安全性至关重要，但目前仍是一个显著的失败点。即使是前沿模型在这种情况下也表现不佳，且现有静态基准无法可靠评估此能力，因为模型会利用数据集特有的伪影并记忆测试实例。", "method": "引入了RefusalBench，这是一种生成式方法，通过受控的语言扰动程序化地创建诊断测试用例。该框架采用了176种不同的扰动策略，涵盖六类信息不确定性和三个强度级别。评估了30多个模型，并发布了两个基准——RefusalBench-NQ（单文档）和RefusalBench-GaRAGe（多文档）以及完整的生成框架。", "result": "研究发现，前沿模型在多文档任务中拒绝准确率低于50%，表现出危险的过度自信或过度谨慎。评估揭示了系统性失败模式：拒绝能力包括可分离的检测和分类技能，且模型规模或扩展推理并未改善性能。选择性拒绝是一种可训练的、对齐敏感的能力。", "conclusion": "选择性拒绝是语言模型中一个可训练且对齐敏感的能力，为未来的改进提供了明确的路径。RefusalBench及其生成框架为持续、动态评估这一关键能力提供了工具。"}}
{"id": "2510.10269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10269", "abs": "https://arxiv.org/abs/2510.10269", "authors": ["Donglin Huang", "Yongyuan Li", "Tianhang Liu", "Junming Huang", "Xiaoda Yang", "Chi Wang", "Weiwei Xu"], "title": "VividAnimator: An End-to-End Audio and Pose-driven Half-Body Human Animation Framework", "comment": "Comments: 10 pages, 6 figures", "summary": "Existing for audio- and pose-driven human animation methods often struggle\nwith stiff head movements and blurry hands, primarily due to the weak\ncorrelation between audio and head movements and the structural complexity of\nhands. To address these issues, we propose VividAnimator, an end-to-end\nframework for generating high-quality, half-body human animations driven by\naudio and sparse hand pose conditions. Our framework introduces three key\ninnovations. First, to overcome the instability and high cost of online\ncodebook training, we pre-train a Hand Clarity Codebook (HCC) that encodes\nrich, high-fidelity hand texture priors, significantly mitigating hand\ndegradation. Second, we design a Dual-Stream Audio-Aware Module (DSAA) to model\nlip synchronization and natural head pose dynamics separately while enabling\ninteraction. Third, we introduce a Pose Calibration Trick (PCT) that refines\nand aligns pose conditions by relaxing rigid constraints, ensuring smooth and\nnatural gesture transitions. Extensive experiments demonstrate that Vivid\nAnimator achieves state-of-the-art performance, producing videos with superior\nhand detail, gesture realism, and identity consistency, validated by both\nquantitative metrics and qualitative evaluations.", "AI": {"tldr": "VividAnimator 是一个端到端框架，通过音频和稀疏手部姿态条件生成高质量的半身人体动画，解决了现有方法中头部运动僵硬和手部模糊的问题。", "motivation": "现有音频和姿态驱动的人体动画方法常面临头部运动僵硬和手部模糊的问题。这主要是由于音频与头部运动的相关性较弱以及手部结构复杂性所致。", "method": "该框架引入了三项关键创新：1. 预训练手部清晰度码本 (HCC)，编码丰富的高保真手部纹理先验，显著减轻手部退化。2. 设计双流音频感知模块 (DSAA)，独立建模唇同步和自然头部姿态动态，并实现两者间的交互。3. 引入姿态校准技巧 (PCT)，通过放宽刚性约束来优化和对齐姿态条件，确保手势过渡的平滑自然。", "result": "VividAnimator 实现了最先进的性能，在手部细节、手势真实感和身份一致性方面表现出卓越的效果。这些结果已通过定量指标和定性评估得到验证。", "conclusion": "VividAnimator 通过其独特的设计和创新模块，有效解决了现有音频和姿态驱动人体动画方法的局限性，显著提升了生成动画的质量和真实感。"}}
{"id": "2510.11306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11306", "abs": "https://arxiv.org/abs/2510.11306", "authors": ["Xiaobin Zhou", "Miao Wang", "Chengao Li", "Can Cui", "Ruibin Zhang", "Yongchao Wang", "Chao Xu", "Fei Gao"], "title": "Rotor-Failure-Aware Quadrotors Flight in Unknown Environments", "comment": null, "summary": "Rotor failures in quadrotors may result in high-speed rotation and vibration\ndue to rotor imbalance, which introduces significant challenges for autonomous\nflight in unknown environments. The mainstream approaches against rotor\nfailures rely on fault-tolerant control (FTC) and predefined trajectory\ntracking. To the best of our knowledge, online failure detection and diagnosis\n(FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown\nand complex environments have not yet been achieved. This paper presents a\nrotor-failure-aware quadrotor navigation system designed to mitigate the\nimpacts of rotor imbalance. First, a composite FDD-based nonlinear model\npredictive controller (NMPC), incorporating motor dynamics, is designed to\nensure fast failure detection and flight stability. Second, a\nrotor-failure-aware planner is designed to leverage FDD results and\nspatial-temporal joint optimization, while a LiDAR-based quadrotor platform\nwith four anti-torque plates is designed to enable reliable perception under\nhigh-speed rotation. Lastly, extensive benchmarks against state-of-the-art\nmethods highlight the superior performance of the proposed approach in\naddressing rotor failures, including propeller unloading and motor stoppage.\nThe experimental results demonstrate, for the first time, that our approach\nenables autonomous quadrotor flight with rotor failures in challenging\nenvironments, including cluttered rooms and unknown forests.", "AI": {"tldr": "本文提出了一种旋翼故障感知四旋翼导航系统，通过结合在线故障检测与诊断（FDD）、轨迹规划和容错控制（FTC），首次实现在未知复杂环境下带旋翼故障的自主飞行。", "motivation": "旋翼故障会导致高速旋转和振动，给四旋翼在未知环境中的自主飞行带来巨大挑战。现有主流方法（如容错控制和预定义轨迹跟踪）未能实现对故障后四旋翼在未知复杂环境中的在线故障检测与诊断、轨迹规划和容错控制。", "method": "1. 设计了一种结合电机动力学的复合FDD非线性模型预测控制器（NMPC），以确保快速故障检测和飞行稳定性。2. 设计了一个旋翼故障感知规划器，利用FDD结果和时空联合优化。3. 设计了一个带有四个反扭矩板的基于LiDAR的四旋翼平台，以在高速旋转下实现可靠感知。", "result": "与现有最先进方法相比，所提出的方法在处理旋翼故障（包括螺旋桨卸载和电机停转）方面表现出卓越的性能。实验结果首次证明，该方法能使四旋翼在杂乱房间和未知森林等挑战性环境中，在旋翼故障下实现自主飞行。", "conclusion": "本文提出的旋翼故障感知四旋翼导航系统能够有效应对旋翼故障，首次实现在未知复杂环境下带旋翼故障的四旋翼自主飞行。"}}
{"id": "2510.10288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10288", "abs": "https://arxiv.org/abs/2510.10288", "authors": ["Sayan Mandal", "Divyadarshini Karthikeyan", "Manas Paldhe"], "title": "SAM2LoRA: Composite Loss-Guided, Parameter-Efficient Finetuning of SAM2 for Retinal Fundus Segmentation", "comment": "Accepted for publication at the 2025 International Conference on\n  Machine Learning and Applications (ICMLA)", "summary": "We propose SAM2LoRA, a parameter-efficient fine-tuning strategy that adapts\nthe Segment Anything Model 2 (SAM2) for fundus image segmentation. SAM2 employs\na masked autoencoder-pretrained Hierarchical Vision Transformer for multi-scale\nfeature decoding, enabling rapid inference in low-resource settings; however,\nfine-tuning remains challenging. To address this, SAM2LoRA integrates a\nlow-rank adapter into both the image encoder and mask decoder, requiring fewer\nthan 5\\% of the original trainable parameters. Our analysis indicates that for\ncross-dataset fundus segmentation tasks, a composite loss function combining\nsegmentationBCE, SoftDice, and FocalTversky losses is essential for optimal\nnetwork tuning. Evaluated on 11 challenging fundus segmentation datasets,\nSAM2LoRA demonstrates high performance in both blood vessel and optic disc\nsegmentation under cross-dataset training conditions. It achieves Dice scores\nof up to 0.86 and 0.93 for blood vessel and optic disc segmentation,\nrespectively, and AUC values of up to 0.98 and 0.99, achieving state-of-the-art\nperformance while substantially reducing training overhead.", "AI": {"tldr": "SAM2LoRA 是一种参数高效的微调策略，用于将 SAM2 模型应用于眼底图像分割任务，在交叉数据集训练条件下，以极低的训练开销实现了血管和视盘分割的最新性能。", "motivation": "SAM2 模型虽然在低资源环境下能实现快速推理，但其微调过程具有挑战性。研究旨在开发一种参数高效的方法，将 SAM2 适应于眼底图像分割，以解决这一挑战。", "method": "提出 SAM2LoRA 策略，通过将低秩适配器集成到 SAM2 的图像编码器和掩码解码器中，使可训练参数少于原始模型的5%。同时，研究发现结合 segmentationBCE、SoftDice 和 FocalTversky 的复合损失函数对于网络优化至关重要。", "result": "在11个眼底分割数据集上进行评估，SAM2LoRA 在交叉数据集训练条件下，血管分割的 Dice 分数高达0.86，AUC 值高达0.98；视盘分割的 Dice 分数高达0.93，AUC 值高达0.99。实现了最先进的性能，并显著降低了训练开销。", "conclusion": "SAM2LoRA 成功地将 SAM2 模型高效地应用于眼底图像分割任务，在保持高参数效率的同时，在血管和视盘分割方面取得了最先进的性能。"}}
{"id": "2510.11308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11308", "abs": "https://arxiv.org/abs/2510.11308", "authors": ["Weixi Situ", "Hanjing Ye", "Jianwei Peng", "Yu Zhan", "Hong Zhang"], "title": "Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments", "comment": "https://adap-rpf.github.io/", "summary": "Robot person following (RPF) is a core capability in human-robot interaction,\nenabling robots to assist users in daily activities, collaborative work, and\nother service scenarios. However, achieving practical RPF remains challenging\ndue to frequent occlusions, particularly in dynamic and crowded environments.\nExisting approaches often rely on fixed-point following or sparse\ncandidate-point selection with oversimplified heuristics, which cannot\nadequately handle complex occlusions caused by moving obstacles such as\npedestrians. To address these limitations, we propose an adaptive trajectory\nsampling method that generates dense candidate points within socially aware\nzones and evaluates them using a multi-objective cost function. Based on the\noptimal point, a person-following trajectory is estimated relative to the\npredicted motion of the target. We further design a prediction-aware model\npredictive path integral (MPPI) controller that simultaneously tracks this\ntrajectory and proactively avoids collisions using predicted pedestrian\nmotions. Extensive experiments show that our method outperforms\nstate-of-the-art baselines in smoothness, safety, robustness, and human\ncomfort, with its effectiveness further demonstrated on a mobile robot in\nreal-world scenarios.", "AI": {"tldr": "本文提出了一种自适应轨迹采样方法和预测感知型MPPI控制器，以解决在动态和拥挤环境中机器人跟随行人时频繁遮挡的挑战，显著提升了跟随的平稳性、安全性、鲁棒性和舒适度。", "motivation": "机器人跟随行人(RPF)在人机交互中至关重要，但由于动态和拥挤环境中频繁的遮挡，实现实用的RPF仍具挑战。现有方法依赖固定点跟随或简化启发式的稀疏候选点选择，无法有效处理移动障碍物（如行人）造成的复杂遮挡。", "method": "本文提出了一种自适应轨迹采样方法，在社交感知区域内生成密集候选点，并使用多目标成本函数进行评估，基于最优解估计行人跟随轨迹。此外，设计了一种预测感知型模型预测路径积分（MPPI）控制器，该控制器能同时跟踪轨迹并利用预测的行人运动主动避开碰撞。", "result": "广泛实验表明，本文方法在平稳性、安全性、鲁棒性和人类舒适度方面优于现有基线方法。在真实世界场景中的移动机器人上进一步验证了其有效性。", "conclusion": "所提出的自适应轨迹采样方法和预测感知型MPPI控制器，有效解决了动态拥挤环境中机器人行人跟随的遮挡问题，显著提升了跟随性能，使其更适用于实际应用。"}}
{"id": "2510.10397", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10397", "abs": "https://arxiv.org/abs/2510.10397", "authors": ["Kai Zhang", "Xinyuan Zhang", "Ejaz Ahmed", "Hongda Jiang", "Caleb Kumar", "Kai Sun", "Zhaojiang Lin", "Sanat Sharma", "Shereen Oraby", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xiaozhong Liu", "Xin Luna Dong"], "title": "AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval", "comment": null, "summary": "Accurate recall from large scale memories remains a core challenge for memory\naugmented AI assistants performing question answering (QA), especially in\nsimilarity dense scenarios where existing methods mainly rely on semantic\ndistance to the query for retrieval. Inspired by how humans link information\nassociatively, we propose AssoMem, a novel framework constructing an\nassociative memory graph that anchors dialogue utterances to automatically\nextracted clues. This structure provides a rich organizational view of the\nconversational context and facilitates importance aware ranking. Further,\nAssoMem integrates multi-dimensional retrieval signals-relevance, importance,\nand temporal alignment using an adaptive mutual information (MI) driven fusion\nstrategy. Extensive experiments across three benchmarks and a newly introduced\ndataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA\nbaselines, verifying its superiority in context-aware memory recall.", "AI": {"tldr": "AssoMem是一种基于关联记忆图和多维度检索信号融合的新框架，用于提高AI助手在问答任务中的记忆召回准确性，尤其是在语义密集场景下。", "motivation": "现有AI助手的问答任务在处理大规模记忆时，尤其是在语义密集的场景下，准确召回信息仍是核心挑战。现有方法主要依赖于与查询的语义距离进行检索，效果不佳。", "method": "AssoMem框架受人类联想记忆启发，构建一个关联记忆图，将对话语句锚定到自动提取的线索上，提供丰富的对话上下文组织视图并实现重要性感知排序。此外，它通过自适应互信息（MI）驱动的融合策略，整合了相关性、重要性和时间对齐等多维度检索信号。", "result": "在三个基准数据集和一个新引入的MeetingQA数据集上的大量实验表明，AssoMem始终优于最先进的基线方法。", "conclusion": "AssoMem在上下文感知记忆召回方面表现出卓越的性能，验证了其在解决大规模记忆召回挑战方面的优越性。"}}
{"id": "2510.11281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11281", "abs": "https://arxiv.org/abs/2510.11281", "authors": ["Deepeka Garg", "Sihan Zeng", "Annapoorani L. Narayanan", "Sumitra Ganesh", "Leo Ardon"], "title": "PADME: Procedure Aware DynaMic Execution", "comment": null, "summary": "Learning to autonomously execute long-horizon procedures from natural\nlanguage remains a core challenge for intelligent agents. Free-form\ninstructions such as recipes, scientific protocols, or business workflows\nencode rich procedural knowledge, but their variability and lack of structure\ncause agents driven by large language models (LLMs) to drift or fail during\nexecution. We introduce Procedure Aware DynaMic Execution (PADME), an agent\nframework that produces and exploits a graph-based representation of\nprocedures. Unlike prior work that relies on manual graph construction or\nunstructured reasoning, PADME autonomously transforms procedural text into\nexecutable graphs that capture task dependencies, decision points, and reusable\nsubroutines. Central to PADME is a two-phase methodology; Teach phase, which\nfocuses on systematic structuring, enrichment with executable logic of\nprocedures, followed by Execute phase, which enables dynamic execution in\nresponse to real-time inputs and environment feedback. This separation ensures\nquality assurance and scalability, allowing expert knowledge to be encoded once\nand reliably reused across varying contexts. The graph representation also\nprovides an inductive bias that reduces error accumulation in long-horizon\nreasoning, underscoring the importance of structured procedure modeling for\nreliable agent-driven automation. Empirically, PADME achieves state-of-the-art\nperformance on four diverse benchmarks, including ALFWorld and ScienceWorld.\nThese results demonstrate that agents equipped with graph-based procedure\nrepresentations offer a powerful intermediate abstraction for robust and\ngeneralizable execution.", "AI": {"tldr": "本文提出了PADME框架，通过将自然语言程序转化为图结构表示，实现了对长序列任务的自主执行，有效解决了大型语言模型在此类任务中因结构不足导致的漂移和失败问题，并在多个基准测试中取得了最先进的性能。", "motivation": "智能体在从自然语言中学习并执行长序列程序（如食谱、科学协议）时面临核心挑战。自由形式的指令缺乏结构且变异性大，导致基于大型语言模型（LLMs）的智能体在执行过程中容易出现漂移或失败。", "method": "PADME（Procedure Aware DynaMic Execution）框架通过生成和利用程序的图结构表示来解决问题。它能自主地将程序文本转换为可执行图，捕捉任务依赖、决策点和可复用子程序。核心方法包括两个阶段：1. “教学阶段”：系统地构建程序结构，并用可执行逻辑进行丰富。2. “执行阶段”：根据实时输入和环境反馈进行动态执行。图表示提供了一种归纳偏置，减少了长序列推理中的错误累积。", "result": "PADME在包括ALFWorld和ScienceWorld在内的四个多样化基准测试中取得了最先进的性能。这些结果表明，配备图结构程序表示的智能体为鲁棒和可泛化的执行提供了一种强大的中间抽象。", "conclusion": "图结构程序表示为智能体提供了强大的中间抽象，能够实现鲁棒和可泛化的任务执行。结构化的程序建模对于可靠的智能体驱动自动化至关重要。"}}
{"id": "2510.10342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10342", "abs": "https://arxiv.org/abs/2510.10342", "authors": ["Yu-Hsuan Lin"], "title": "Ordinal Scale Traffic Congestion Classification with Multi-Modal Vision-Language and Motion Analysis", "comment": "7 pages, 4 figures. Preprint submitted to arXiv in October 2025", "summary": "Accurate traffic congestion classification is essential for intelligent\ntransportation systems and real-time urban traffic management. This paper\npresents a multimodal framework combining open-vocabulary visual-language\nreasoning (CLIP), object detection (YOLO-World), and motion analysis via\nMOG2-based background subtraction. The system predicts congestion levels on an\nordinal scale from 1 (free flow) to 5 (severe congestion), enabling\nsemantically aligned and temporally consistent classification. To enhance\ninterpretability, we incorporate motion-based confidence weighting and generate\nannotated visual outputs. Experimental results show the model achieves 76.7\npercent accuracy, an F1 score of 0.752, and a Quadratic Weighted Kappa (QWK) of\n0.684, significantly outperforming unimodal baselines. These results\ndemonstrate the framework's effectiveness in preserving ordinal structure and\nleveraging visual-language and motion modalities. Future enhancements include\nincorporating vehicle sizing and refined density metrics.", "AI": {"tldr": "本文提出一个多模态框架，结合CLIP、YOLO-World和MOG2进行交通拥堵分类，实现了语义对齐和时间一致的预测，并具有良好的准确性和可解释性。", "motivation": "准确的交通拥堵分类对于智能交通系统和实时城市交通管理至关重要。", "method": "该框架结合了开放词汇视觉-语言推理（CLIP）、目标检测（YOLO-World）和基于MOG2背景减法的运动分析。它以1到5的序数等级预测拥堵水平，并结合基于运动的置信度加权和生成带注释的视觉输出以增强可解释性。", "result": "实验结果显示，该模型达到了76.7%的准确率、0.752的F1分数和0.684的二次加权Kappa（QWK），显著优于单模态基线。这表明该框架在保留序数结构和利用视觉-语言及运动模态方面的有效性。", "conclusion": "该多模态框架在交通拥堵分类方面表现出有效性。未来的改进包括整合车辆尺寸和更精细的密度指标。"}}
{"id": "2510.10292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10292", "abs": "https://arxiv.org/abs/2510.10292", "authors": ["Joy Hsu", "Emily Jin", "Jiajun Wu", "Niloy J. Mitra"], "title": "From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries", "comment": "NeurIPS 2025", "summary": "Real-world scenes, such as those in ScanNet, are difficult to capture, with\nhighly limited data available. Generating realistic scenes with varied object\nposes remains an open and challenging task. In this work, we propose\nFactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging\nthe underlying structure of rooms while learning the variation of object poses\nfrom lived-in scenes. We introduce a factored representation that decomposes\nscenes into hierarchically organized concepts of room programs and object\nposes. To encode structure, FactoredScenes learns a library of functions\ncapturing reusable layout patterns from which scenes are drawn, then uses large\nlanguage models to generate high-level programs, regularized by the learned\nlibrary. To represent scene variations, FactoredScenes learns a\nprogram-conditioned model to hierarchically predict object poses, and retrieves\nand places 3D objects in a scene. We show that FactoredScenes generates\nrealistic, real-world rooms that are difficult to distinguish from real ScanNet\nscenes.", "AI": {"tldr": "FactoredScenes是一个用于合成真实3D场景的框架，它通过分解场景为房间程序和物体姿态的层次结构，学习可复用布局模式并利用大型语言模型生成高级程序，然后预测物体姿态来生成逼真的房间。", "motivation": "现实世界场景（如ScanNet）难以捕获且数据高度有限；生成具有多样化物体姿态的逼真场景仍然是一个开放且具有挑战性的任务。", "method": "该研究提出了FactoredScenes框架，它采用分解表示法将场景分解为房间程序（room programs）和物体姿态（object poses）的层次结构。为编码结构，FactoredScenes学习一个捕获可复用布局模式的函数库，并使用大型语言模型生成高级程序，由学习到的库进行正则化。为表示场景变化，FactoredScenes学习一个程序条件模型来分层预测物体姿态，并检索和放置3D物体。", "result": "FactoredScenes能够生成逼真的、难以与真实ScanNet场景区分的现实世界房间。", "conclusion": "FactoredScenes通过利用房间的底层结构和学习物体姿态的变化，成功解决了生成逼真3D场景的挑战，并能生成高质量的真实世界房间。"}}
{"id": "2510.10415", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10415", "abs": "https://arxiv.org/abs/2510.10415", "authors": ["Federica Bologna", "Tiffany Pan", "Matthew Wilkens", "Yue Guo", "Lucy Lu Wang"], "title": "LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints", "comment": null, "summary": "Evaluating long-form clinical question answering (QA) systems is\nresource-intensive and challenging: accurate judgments require medical\nexpertise and achieving consistent human judgments over long-form text is\ndifficult. We introduce LongQAEval, an evaluation framework and set of\nevaluation recommendations for limited-resource and high-expertise settings.\nBased on physician annotations of 300 real patient questions answered by\nphysicians and LLMs, we compare coarse answer-level versus fine-grained\nsentence-level evaluation over the dimensions of correctness, relevance, and\nsafety. We find that inter-annotator agreement (IAA) varies by dimension:\nfine-grained annotation improves agreement on correctness, coarse improves\nagreement on relevance, and judgments on safety remain inconsistent.\nAdditionally, annotating only a small subset of sentences can provide\nreliability comparable to coarse annotations, reducing cost and effort.", "AI": {"tldr": "本文提出LongQAEval框架，用于评估长篇临床问答系统，旨在解决资源受限和专业性高的问题。通过比较粗粒度与细粒度评估，发现标注者间一致性因维度而异，并提出小部分句子标注可降低成本。", "motivation": "评估长篇临床问答（QA）系统资源密集且具有挑战性，需要医学专业知识，并且在长篇文本上实现一致的人工判断很困难。", "method": "引入LongQAEval评估框架和一套评估建议。基于医生对300个由医生和大型语言模型回答的真实患者问题的标注，比较了粗粒度（答案级别）与细粒度（句子级别）评估在正确性、相关性和安全性维度上的表现。", "result": "标注者间一致性（IAA）因维度而异：细粒度标注提高了正确性的一致性，粗粒度标注提高了相关性的一致性，而安全性判断则始终不一致。此外，仅标注一小部分句子即可提供与粗粒度标注相当的可靠性，从而降低成本和精力。", "conclusion": "LongQAEval为资源有限和高专业性环境下的长篇临床问答系统评估提供了框架和建议。细粒度标注有助于提高正确性判断的一致性，而小部分句子标注是一种经济高效的评估策略。"}}
{"id": "2510.11290", "categories": ["cs.AI", "cs.HC", "I.2.6; J.4"], "pdf": "https://arxiv.org/pdf/2510.11290", "abs": "https://arxiv.org/abs/2510.11290", "authors": ["Sheng Jin", "Haoming Wang", "Zhiqi Gao", "Yongbo Yang", "Bao Chunjia", "Chengliang Wang"], "title": "Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics", "comment": "9 pages, 7 figures, EMNLP conference", "summary": "Large language models (LLMs) based Agents are increasingly pivotal in\nsimulating and understanding complex human systems and interactions. We propose\nthe AI-Agent School (AAS) system, built around a self-evolving mechanism that\nleverages agents for simulating complex educational dynamics. Addressing the\nfragmented issues in teaching process modeling and the limitations of agents\nperformance in simulating diverse educational participants, AAS constructs the\nZero-Exp strategy, employs a continuous \"experience-reflection-optimization\"\ncycle, grounded in a dual memory base comprising experience and knowledge bases\nand incorporating short-term and long-term memory components. Through this\nmechanism, agents autonomously evolve via situated interactions within diverse\nsimulated school scenarios. This evolution enables agents to more accurately\nmodel the nuanced, multi-faceted teacher-student engagements and underlying\nlearning processes found in physical schools. Experiment confirms that AAS can\neffectively simulate intricate educational dynamics and is effective in\nfostering advanced agent cognitive abilities, providing a foundational stepping\nstone from the \"Era of Experience\" to the \"Era of Simulation\" by generating\nhigh-fidelity behavioral and interaction data.", "AI": {"tldr": "该研究提出了AI智能体学校（AAS）系统，通过自我演化机制（Zero-Exp策略、经验-反思-优化循环、双重记忆库）使LLM智能体能更准确地模拟复杂的教育动态和师生互动，从而有效提升智能体的认知能力并生成高保真数据。", "motivation": "当前LLM智能体在模拟复杂人类系统和交互中日益重要，但教学过程建模存在碎片化问题，且智能体在模拟多样化教育参与者方面的性能有限。", "method": "提出AI智能体学校（AAS）系统，其核心是自我演化机制。该机制采用Zero-Exp策略，通过“经验-反思-优化”的持续循环进行，并基于包含经验和知识库的双重记忆基础，整合了短期和长期记忆组件。智能体通过在多样化的模拟学校场景中进行情境交互实现自主演化。", "result": "实验证实AAS能够有效模拟复杂的教育动态，并能有效培养智能体的高级认知能力，从而生成高保真的行为和交互数据。", "conclusion": "AAS系统使智能体能更准确地建模物理学校中细致入微、多方面的师生互动和学习过程，为从“经验时代”迈向“模拟时代”提供了基础性的垫脚石。"}}
{"id": "2510.11401", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11401", "abs": "https://arxiv.org/abs/2510.11401", "authors": ["Jiayang Wu", "Jiongye Li", "Shibowen Zhang", "Zhicheng He", "Zaijin Wang", "Xiaokun Leng", "Hangxin Liu", "Jingwen Zhang", "Jiayi Wang", "Song-Chun Zhu", "Yao Su"], "title": "Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots", "comment": null, "summary": "This paper proposes a novel framework for humanoid robots to execute\ninspection tasks with high efficiency and millimeter-level precision. The\napproach combines hierarchical planning, time-optimal standing position\ngeneration, and integrated \\ac{mpc} to achieve high speed and precision. A\nhierarchical planning strategy, leveraging \\ac{ik} and \\ac{mip}, reduces\ncomputational complexity by decoupling the high-dimensional planning problem. A\nnovel MIP formulation optimizes standing position selection and trajectory\nlength, minimizing task completion time. Furthermore, an MPC system with\nsimplified kinematics and single-step position correction ensures\nmillimeter-level end-effector tracking accuracy. Validated through simulations\nand experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates\nlow time cost and a high success rate in multi-location tasks, enabling\nefficient and precise execution of complex industrial operations.", "AI": {"tldr": "本文提出了一种新颖的框架，使仿人机器人能够以高效率和毫米级精度执行检查任务，通过结合分层规划、时间最优站立姿态生成和集成MPC实现。", "motivation": "现有仿人机器人在执行需要高效率和毫米级精度的检查任务时面临挑战，尤其是在复杂的工业操作中，因此需要一种新的方法来提升其性能。", "method": "该方法结合了分层规划、时间最优站立姿态生成和集成模型预测控制（MPC）。分层规划利用IK和MIP解耦高维问题；MIP公式优化站立姿态选择和轨迹长度以最小化任务完成时间；MPC系统采用简化运动学和单步位置校正，确保末端执行器毫米级跟踪精度。", "result": "通过在Kuavo 4Pro仿人平台上的仿真和实验验证，该框架在多位置任务中展现出低时间成本和高成功率，证明了其能够高效、精确地执行复杂的工业操作。", "conclusion": "所提出的框架成功地使仿人机器人能够以高效率和毫米级精度执行复杂的检查任务，为工业应用提供了有效的解决方案。"}}
{"id": "2510.11321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11321", "abs": "https://arxiv.org/abs/2510.11321", "authors": ["Ruizhe Liu", "Pei Zhou", "Qian Luo", "Li Sun", "Jun Cen", "Yibing Song", "Yanchao Yang"], "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios.", "AI": {"tldr": "该论文提出了一种自监督框架，通过跨模态感知关联和多层次时间抽象来学习分层操作概念，以实现机器人操作的有效泛化。", "motivation": "机器人操作中有效的泛化需要能捕捉跨环境和任务的不变交互模式的表示。", "method": "该方法通过结合一个识别跨感官模态持久模式的跨模态关联网络，以及一个在时间尺度上分层组织表示的多视野预测器，构建了一个自监督框架，用于学习分层操作概念，且无需人工标注。", "result": "在模拟基准和真实世界部署中，其概念增强策略显著提高了性能。分析表明，尽管没有语义监督，学习到的概念类似于人类可解释的操作原语。", "conclusion": "这项工作推动了对操作表示学习的理解，并提供了一种在复杂场景中提高机器人性能的实用方法。"}}
{"id": "2510.10444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10444", "abs": "https://arxiv.org/abs/2510.10444", "authors": ["Jingyi Chen", "Zhimeng Guo", "Jiyun Chun", "Pichao Wang", "Andrew Perrault", "Micha Elsner"], "title": "Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance", "comment": null, "summary": "Understanding emotion from speech requires sensitivity to both lexical and\nacoustic cues. However, it remains unclear whether large audio language models\n(LALMs) genuinely process acoustic information or rely primarily on lexical\ncontent. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in\nNarratives), a controlled benchmark designed to disentangle lexical reliance\nfrom acoustic sensitivity in emotion understanding. Across evaluations of six\nstate-of-the-art LALMs, we observe a consistent lexical dominance. Models\npredict \"neutral\" when lexical cues are neutral or absent, show limited gains\nunder cue alignment, and fail to classify distinct emotions under cue conflict.\nIn paralinguistic settings, performance approaches chance. These results\nindicate that current LALMs largely \"transcribe\" rather than \"listen,\" relying\nheavily on lexical semantics while underutilizing acoustic cues. LISTEN offers\na principled framework for assessing emotion understanding in multimodal\nmodels.", "AI": {"tldr": "当前大型音频语言模型（LALMs）在语音情感理解中过度依赖文本信息，而对声学线索的利用不足，更像是“转录”而非“聆听”。", "motivation": "虽然语音情感理解需要同时考虑文本和声学线索，但目前尚不清楚大型音频语言模型（LALMs）是真正处理声学信息，还是主要依赖文本内容来理解情感。", "method": "本文提出了LISTEN（Lexical vs. Acoustic Speech Test for Emotion in Narratives），这是一个受控基准测试，旨在区分LALMs对文本依赖和声学敏感性在情感理解中的作用。该研究评估了六个最先进的LALMs。", "result": "评估结果显示，LALMs表现出一致的文本主导性。当文本线索中性或缺失时，模型预测为“中性”；在文本和声学线索一致时，性能提升有限；在线索冲突下，模型无法区分不同的情感。在副语言情境中，模型性能接近随机。", "conclusion": "这些结果表明，当前的LALMs主要“转录”而非“聆听”，严重依赖文本语义，而未能充分利用声学线索。LISTEN为评估多模态模型的情感理解能力提供了一个原则性的框架。"}}
{"id": "2510.11313", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11313", "abs": "https://arxiv.org/abs/2510.11313", "authors": ["Le Ngoc Luyen", "Marie-Hélène Abel"], "title": "Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs", "comment": null, "summary": "This paper investigates automated skill decomposition using Large Language\nModels (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.\nOur framework standardizes the pipeline from prompting and generation to\nnormalization and alignment with ontology nodes. To evaluate outputs, we\nintroduce two metrics: a semantic F1-score that uses optimal embedding-based\nmatching to assess content accuracy, and a hierarchy-aware F1-score that\ncredits structurally correct placements to assess granularity. We conduct\nexperiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing\ntwo prompting strategies: zero-shot and leakage-safe few-shot with exemplars.\nAcross diverse LLMs, zero-shot offers a strong baseline, while few-shot\nconsistently stabilizes phrasing and granularity and improves hierarchy-aware\nalignment. A latency analysis further shows that exemplar-guided prompts are\ncompetitive - and sometimes faster - than unguided zero-shot due to more\nschema-compliant completions. Together, the framework, benchmark, and metrics\nprovide a reproducible foundation for developing ontology-faithful skill\ndecomposition systems.", "AI": {"tldr": "本文提出了一种基于LLM的自动化技能分解方法，并建立了一个严格的、以本体为基础的评估框架，包括标准化流程、新颖的评估指标和对不同提示策略的实验，以期为开发本体忠实型技能分解系统奠定基础。", "motivation": "研究动机是需要对使用大型语言模型（LLMs）进行自动化技能分解的方法进行严格评估，并标准化从提示到与本体节点对齐的整个流程，以确保评估的准确性和粒度。", "method": "本文提出一个本体接地（ontology-grounded）的评估框架，标准化了从提示、生成到归一化和与本体节点对齐的流程。引入了两个评估指标：使用基于嵌入的最佳匹配来评估内容准确性的语义F1分数，以及通过结构正确放置来评估粒度的层级感知F1分数。实验在ROME-ESCO-DecompSkill数据集上进行，比较了零样本（zero-shot）和泄漏安全（leakage-safe）的少样本（few-shot）提示策略，并进行了延迟分析。", "result": "实验结果显示，零样本提示提供了一个强大的基线。少样本提示策略能持续稳定措辞和粒度，并改善层级感知对齐。延迟分析表明，由于生成更符合模式的完成内容，有示例引导的提示（少样本）与无引导的零样本提示相比，具有竞争力甚至更快。", "conclusion": "该框架、基准和评估指标共同为开发忠实于本体的技能分解系统提供了一个可复现的基础。"}}
{"id": "2510.11380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11380", "abs": "https://arxiv.org/abs/2510.11380", "authors": ["Abdullah Al Mahmud", "Prangon Chowdhury", "Mohammed Borhan Uddin", "Khaled Eabne Delowar", "Tausifur Rahman Talha", "Bijoy Dewanjee"], "title": "AI-Driven anemia diagnosis: A review of advanced models and techniques", "comment": null, "summary": "Anemia, a condition marked by insufficient levels of red blood cells or\nhemoglobin, remains a widespread health issue affecting millions of individuals\nglobally. Accurate and timely diagnosis is essential for effective management\nand treatment of anemia. In recent years, there has been a growing interest in\nthe use of artificial intelligence techniques, i.e., machine learning (ML) and\ndeep learning (DL) for the detection, classification, and diagnosis of anemia.\nThis paper provides a systematic review of the recent advancements in this\nfield, with a focus on various models applied to anemia detection. The review\nalso compares these models based on several performance metrics, including\naccuracy, sensitivity, specificity, and precision. By analyzing these metrics,\nthe paper evaluates the strengths and limitation of discussed models in\ndetecting and classifying anemia, emphasizing the importance of addressing\nthese factors to improve diagnostic accuracy.", "AI": {"tldr": "本文系统综述了人工智能（机器学习和深度学习）在贫血检测、分类和诊断方面的最新进展，并比较了不同模型的性能指标。", "motivation": "贫血是全球性健康问题，准确及时的诊断至关重要。人工智能技术在贫血检测领域显示出巨大潜力，因此有必要对该领域进行系统回顾。", "method": "本文采用系统综述的方法，重点关注应用于贫血检测的各种机器学习和深度学习模型，并基于准确率、敏感性、特异性和精确度等性能指标对这些模型进行比较评估。", "result": "通过分析性能指标，本文评估了所讨论模型在检测和分类贫血方面的优势和局限性。", "conclusion": "解决模型的优势和局限性对于提高贫血诊断的准确性至关重要。"}}
{"id": "2510.11421", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11421", "abs": "https://arxiv.org/abs/2510.11421", "authors": ["Shih-Chieh Sun", "Yun-Cheng Tsai"], "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities", "comment": null, "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability.", "AI": {"tldr": "本文提出一个AI驱动的物联网机器人远程操作系统，专为智能城市应用设计，集成了实时AI感知和低延迟双协议通信，实现了高效的远程操控和智能监控。", "motivation": "传统的远程操作平台在模块化部署、实时AI感知和适应性通信策略方面存在不足，无法完全满足智能城市应用的需求，因此需要一个为此量身定制的系统。", "method": "该系统采用Flutter跨平台移动界面，通过MQTT进行控制信号传输，并使用LiveKit框架通过WebRTC进行视频流传输。部署了YOLOv11-nano模型进行轻量级目标检测和实时视觉叠加。控制命令通过MQTT发送至基于ESP8266的执行器节点，由Arduino Mega2560控制器协调多轴机械臂运动。后端基础设施托管在DigitalOcean上。系统性能通过本地和国际VPN（包括香港、日本和比利时）场景下的延迟评估进行验证。", "result": "评估结果显示，执行器响应时间低至0.2秒，即使在高延迟网络下，总视频延迟也低于1.2秒。这种低延迟双协议设计确保了响应迅速的闭环交互和分布式环境下的鲁棒性能。系统强调模块化部署、实时AI感知和适应性通信策略。", "conclusion": "该系统非常适用于智能城市场景，例如远程基础设施检查、公共设备维护和城市自动化。未来的增强将集中于边缘设备部署、自适应路由以及与城市级物联网网络的集成，以提高弹性和可扩展性。"}}
{"id": "2510.10448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10448", "abs": "https://arxiv.org/abs/2510.10448", "authors": ["Zhichao Xu", "Minheng Wang", "Yawei Wang", "Wenqian Ye", "Yuntao Du", "Yunpu Ma", "Yijun Tian"], "title": "RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems trained using reinforcement\nlearning (RL) with reasoning are hampered by inefficient context management,\nwhere long, noisy retrieved documents increase costs and degrade performance.\nWe introduce RECON (REasoning with CONdensation), a framework that integrates\nan explicit summarization module to compress evidence within the reasoning\nloop. Our summarizer is trained via a two-stage process: relevance pretraining\non QA datasets, followed by multi-aspect distillation from proprietary LLMs to\nensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON\nreduces total context length by 35\\%, leading to improved training speed and\ninference latency, while simultaneously improving RAG performance on downstream\nQA benchmarks. Notably, it boosts the average EM score of the 3B model by\n14.5\\% and the 7B model by 3.0\\%, showing particular strength in multi-hop QA.\nRECON demonstrates that learned context compression is essential for building\npractical, scalable, and performant RAG systems. Our code implementation is\nmade available at https://github.com/allfornancy/RECON.", "AI": {"tldr": "RECON框架通过引入一个显式的摘要模块来压缩检索增强生成（RAG）系统中的证据，解决了长而嘈杂上下文导致效率低下的问题，显著提高了训练速度、推理延迟和问答性能。", "motivation": "使用强化学习（RL）和推理训练的检索增强生成（RAG）系统受到低效上下文管理的困扰，因为长而嘈杂的检索文档会增加成本并降低性能。", "method": "本文提出了RECON（REasoning with CONdensation）框架，该框架将一个显式摘要模块集成到推理循环中以压缩证据。摘要器通过两阶段过程进行训练：首先在问答数据集上进行相关性预训练，然后从专有大型语言模型（LLMs）进行多方面蒸馏，以确保事实性和清晰度。RECON被集成到Search-R1管道中。", "result": "RECON将总上下文长度减少了35%，从而提高了训练速度和推理延迟，同时改进了下游问答基准上的RAG性能。值得注意的是，它将3B模型的平均EM分数提高了14.5%，7B模型提高了3.0%，在多跳问答中表现出特别的优势。", "conclusion": "RECON证明了学习到的上下文压缩对于构建实用、可扩展且高性能的RAG系统至关重要。"}}
{"id": "2510.11474", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11474", "abs": "https://arxiv.org/abs/2510.11474", "authors": ["Ardian Selmonaj", "Giacomo Del Rio", "Adrian Schneider", "Alessandro Antonucci"], "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning", "comment": "2025 IEEE International Conference on Agentic AI (ICA)", "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios.", "AI": {"tldr": "本文提出了一种新颖的3D多智能体空战环境和分层多智能体强化学习框架，通过结合异构智能体动态、课程学习、联赛和新训练算法，解决了现实空战模拟中的挑战，显著提高了学习效率和战斗性能。", "motivation": "在现实的空战模拟中，由于态势感知不完善和非线性的飞行动力学，实现任务目标极具挑战性。", "method": "引入了一个新颖的3D多智能体空战环境，并采用了一个分层多智能体强化学习框架。该方法结合了异构智能体动态、课程学习、联赛（league-play）以及一种新改编的训练算法。决策过程被组织成两个抽象级别：低级策略学习精确的控制机动，而高级策略根据任务目标发出战术指令。", "result": "实证结果表明，所提出的分层方法在复杂的缠斗场景中，既提高了学习效率，也提升了战斗性能。", "conclusion": "分层多智能体强化学习框架能够有效应对复杂空战模拟中的挑战，通过分层决策和多种学习策略的结合，显著提升了智能体的学习效率和实战表现。"}}
{"id": "2510.10365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10365", "abs": "https://arxiv.org/abs/2510.10365", "authors": ["Linlian Jiang", "Rui Ma", "Li Gu", "Ziqiang Wang", "Xinxin Zuo", "Yang Wang"], "title": "PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion", "comment": "NeurIPS 2025", "summary": "Point cloud completion is essential for robust 3D perception in\nsafety-critical applications such as robotics and augmented reality. However,\nexisting models perform static inference and rely heavily on inductive biases\nlearned during training, limiting their ability to adapt to novel structural\npatterns and sensor-induced distortions at test time. To address this\nlimitation, we propose PointMAC, a meta-learned framework for robust test-time\nadaptation in point cloud completion. It enables sample-specific refinement\nwithout requiring additional supervision. Our method optimizes the completion\nmodel under two self-supervised auxiliary objectives that simulate structural\nand sensor-level incompleteness. A meta-auxiliary learning strategy based on\nModel-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary\nobjectives is consistently aligned with the primary completion task. During\ninference, we adapt the shared encoder on-the-fly by optimizing auxiliary\nlosses, with the decoder kept fixed. To further stabilize adaptation, we\nintroduce Adaptive $\\lambda$-Calibration, a meta-learned mechanism for\nbalancing gradients between primary and auxiliary objectives. Extensive\nexperiments on synthetic, simulated, and real-world datasets demonstrate that\nPointMAC achieves state-of-the-art results by refining each sample individually\nto produce high-quality completions. To the best of our knowledge, this is the\nfirst work to apply meta-auxiliary test-time adaptation to point cloud\ncompletion.", "AI": {"tldr": "PointMAC是一个元学习框架，通过自监督辅助目标和MAML策略，在点云补全任务中实现鲁棒的测试时自适应，逐样本优化以生成高质量补全结果。", "motivation": "现有模型在点云补全中进行静态推理，过度依赖训练中学到的归纳偏置，限制了它们在测试时适应新颖结构模式和传感器畸变的能力。", "method": "本文提出了PointMAC，一个元学习框架，用于点云补全中的鲁棒测试时自适应。它利用两个自监督辅助目标模拟结构和传感器级别的不完整性。基于MAML的元辅助学习策略确保了辅助目标驱动的自适应与主要补全任务保持一致。推理时，通过优化辅助损失动态调整共享编码器（解码器固定）。此外，引入了自适应λ校准机制，以平衡主要和辅助目标之间的梯度。", "result": "在合成、模拟和真实世界数据集上的大量实验表明，PointMAC通过单独优化每个样本，实现了最先进的补全效果，生成了高质量的补全点云。这是首次将元辅助测试时自适应应用于点云补全的工作。", "conclusion": "PointMAC通过元辅助测试时自适应，有效解决了现有模型在点云补全中适应性和鲁棒性的局限性，显著提高了补全质量，并为该领域开辟了新方向。"}}
{"id": "2510.11457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11457", "abs": "https://arxiv.org/abs/2510.11457", "authors": ["Beining Wang", "Weihang Su", "Hongtao Tian", "Tao Yang", "Yujia Zhou", "Ting Yao", "Qingyao Ai", "Yiqun Liu"], "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization", "comment": null, "summary": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.", "AI": {"tldr": "本文提出维度级奖励模型（DRM），通过评估推理过程的置信度、相关性和连贯性三个维度，为大型语言模型（LLMs）提供可解释的多维度监督信号，从而提升其多步推理能力，并在多种任务上展现出泛化性能。", "motivation": "现有方法存在局限：结果监督的强化学习（RLVR）奖励稀疏且可能传播错误推理；过程级奖励模型（PRMs）缺乏泛化性和可解释性，需要特定任务的推理过程分割。", "method": "提出维度级奖励模型（DRM），通过评估推理过程的三个基本、互补且可解释的维度来提供监督：置信度（不确定性校准）、相关性（语义对齐）和连贯性（逻辑一致性）。这使得在无需真实答案的情况下进行可解释的评估。", "result": "实验结果表明，DRM提供了有效的监督信号，能指导LLMs的优化并增强其推理能力。DRM监督训练在数学、问答、代码执行和谜题等多种域内和域外开放域任务上均取得了显著提升。", "conclusion": "研究发现，对推理过程进行多维度监督可以提高LLMs超越训练分布的泛化推理能力。"}}
{"id": "2510.10360", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10360", "abs": "https://arxiv.org/abs/2510.10360", "authors": ["Rugved Katole", "Christopher Stewart"], "title": "Ortho-Fuse: Orthomosaic Generation for Sparse High-Resolution Crop Health Maps Through Intermediate Optical Flow Estimation", "comment": "6 Figures, 9 pages", "summary": "AI-driven crop health mapping systems offer substantial advantages over\nconventional monitoring approaches through accelerated data acquisition and\ncost reduction. However, widespread farmer adoption remains constrained by\ntechnical limitations in orthomosaic generation from sparse aerial imagery\ndatasets. Traditional photogrammetric reconstruction requires 70-80\\%\ninter-image overlap to establish sufficient feature correspondences for\naccurate geometric registration. AI-driven systems operating under\nresource-constrained conditions cannot consistently achieve these overlap\nthresholds, resulting in degraded reconstruction quality that undermines user\nconfidence in autonomous monitoring technologies. In this paper, we present\nOrtho-Fuse, an optical flow-based framework that enables the generation of a\nreliable orthomosaic with reduced overlap requirements. Our approach employs\nintermediate flow estimation to synthesize transitional imagery between\nconsecutive aerial frames, artificially augmenting feature correspondences for\nimproved geometric reconstruction. Experimental validation demonstrates a 20\\%\nreduction in minimum overlap requirements. We further analyze adoption barriers\nin precision agriculture to identify pathways for enhanced integration of\nAI-driven monitoring systems.", "AI": {"tldr": "本文提出Ortho-Fuse，一个基于光流的框架，通过合成过渡图像来减少生成高质量正射镶嵌图所需的航空图像重叠率，从而克服了AI驱动作物健康监测系统在稀疏图像数据集下重建质量差的问题。", "motivation": "AI驱动的作物健康监测系统因传统摄影测量法需要高图像重叠率（70-80%）而导致在资源受限条件下无法获得高质量的正射镶嵌图，这限制了农民的广泛采用。", "method": "Ortho-Fuse框架利用中间光流估计在连续航空图像帧之间合成过渡图像，人工增加特征对应，以改进几何重建，从而减少对图像重叠率的要求。", "result": "实验验证表明，该方法将最小重叠率要求降低了20%。此外，文章还分析了精准农业中的采用障碍。", "conclusion": "Ortho-Fuse通过减少图像重叠要求，提高了AI驱动监测系统在资源受限条件下的可靠性，有助于促进这些技术在精准农业中的更广泛应用和整合。"}}
{"id": "2510.10452", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10452", "abs": "https://arxiv.org/abs/2510.10452", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Steering Over-refusals Towards Safety in Retrieval Augmented Generation", "comment": "Preprint", "summary": "Safety alignment in large language models (LLMs) induces over-refusals --\nwhere LLMs decline benign requests due to aggressive safety filters. We analyze\nthis phenomenon in retrieval-augmented generation (RAG), where both the query\nintent and retrieved context properties influence refusal behavior. We\nconstruct RagRefuse, a domain-stratified benchmark spanning medical, chemical,\nand open domains, pairing benign and harmful queries with controlled context\ncontamination patterns and sizes. Our analysis shows that context arrangement /\ncontamination, domain of query and context, and harmful-text density trigger\nrefusals even on benign queries, with effects depending on model-specific\nalignment choices. To mitigate over-refusals, we introduce\n\\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers\nthe embedding regions towards the confirmed safe, non-refusing output regions\nat inference time. This reduces over-refusals in contaminated RAG pipelines\nwhile preserving legitimate refusals.", "AI": {"tldr": "大型语言模型（LLMs）的安全对齐导致过度拒绝良性请求，尤其是在检索增强生成（RAG）中。本文分析了查询意图和检索上下文对拒绝行为的影响，构建了RagRefuse基准，并提出了SafeRAG-Steering方法，通过嵌入干预来减轻过度拒绝，同时保留合法拒绝。", "motivation": "LLMs的安全对齐导致模型对良性请求的过度拒绝（over-refusals），这在RAG场景中尤为突出，因为检索到的上下文属性也会影响拒绝行为，从而降低了模型的实用性。", "method": "本文构建了RagRefuse基准，该基准按领域（医学、化学、开放领域）分层，将良性与有害查询与受控的上下文污染模式和大小配对。通过分析上下文排列/污染、查询和上下文领域以及有害文本密度对拒绝行为的影响。为缓解过度拒绝，提出了SafeRAG-Steering，这是一种以模型为中心的嵌入干预，在推理时将嵌入区域引导至确认安全、非拒绝的输出区域。", "result": "分析表明，上下文排列/污染、查询和上下文的领域以及有害文本密度会触发对良性查询的拒绝，其效果取决于模型特定的对齐选择。SafeRAG-Steering在受污染的RAG管道中减少了过度拒绝，同时保留了合法的拒绝。", "conclusion": "LLMs的安全对齐确实导致RAG中的过度拒绝，且受上下文污染等因素影响。SafeRAG-Steering作为一种模型中心的嵌入干预，能够有效缓解这一问题，在减少过度拒绝的同时不损害模型进行合法拒绝的能力。"}}
{"id": "2510.10366", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10366", "abs": "https://arxiv.org/abs/2510.10366", "authors": ["Saurabh Kataria", "Ayca Ermis", "Lovely Yeswanth Panchumarthi", "Minxiao Wang", "Xiao Hu"], "title": "Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for Vital Signs like Blood Pressure", "comment": "BHI abstract extended", "summary": "Photoplethysmography (PPG) sensor in wearable and clinical devices provides\nvaluable physiological insights in a non-invasive and real-time fashion.\nSpecialized Foundation Models (FM) or repurposed time-series FMs are used to\nbenchmark physiological tasks. Our experiments with fine-tuning FMs reveal that\nVision FM (VFM) can also be utilized for this purpose and, in fact,\nsurprisingly leads to state-of-the-art (SOTA) performance on many tasks,\nnotably blood pressure estimation. We leverage VFMs by simply transforming\none-dimensional PPG signals into image-like two-dimensional representations,\nsuch as the Short-Time Fourier transform (STFT). Using the latest VFMs, such as\nDINOv3 and SIGLIP-2, we achieve promising performance on other vital signs and\nblood lab measurement tasks as well. Our proposal, Vision4PPG, unlocks a new\nclass of FMs to achieve SOTA performance with notable generalization to other\n2D input representations, including STFT phase and recurrence plots. Our work\nimproves upon prior investigations of vision models for PPG by conducting a\ncomprehensive study, comparing them to state-of-the-art time-series FMs, and\ndemonstrating the general PPG processing ability by reporting results on six\nadditional tasks. Thus, we provide clinician-scientists with a new set of\npowerful tools that is also computationally efficient, thanks to\nParameter-Efficient Fine-Tuning (PEFT) techniques.", "AI": {"tldr": "该研究提出Vision4PPG，通过将一维PPG信号转换为二维图像表示，利用视觉基础模型（VFM）在多项生理任务上实现了最先进的（SOTA）性能，甚至超越了时间序列基础模型，并具有计算效率。", "motivation": "PPG传感器能提供宝贵的生理信息。当前生理任务的基准测试主要使用专门或 repurposed 的时间序列基础模型。研究旨在探索视觉基础模型是否也能用于此目的，并评估其性能。", "method": "将一维PPG信号转换为二维图像表示（例如短时傅里叶变换STFT），然后利用最新的视觉基础模型（如DINOv3和SIGLIP-2）进行微调。采用参数高效微调（PEFT）技术提高计算效率。通过全面研究，将VFM与最先进的时间序列基础模型进行比较，并在血压估计、其他生命体征、血液实验室测量以及六项额外任务上报告结果。", "result": "视觉基础模型在许多任务上（尤其是血压估计）取得了令人惊讶的最先进（SOTA）性能。在其他生命体征和血液实验室测量任务上也表现出良好性能。对包括STFT相位和递归图在内的其他二维输入表示具有显著的泛化能力。通过PEFT技术实现了计算效率，为临床医生-科学家提供了一套新的强大工具。", "conclusion": "Vision4PPG通过利用视觉基础模型和二维PPG表示，开辟了新的基础模型类别，实现了PPG分析的SOTA性能，并具有出色的泛化能力和计算效率，为临床医生-科学家提供了强大的新工具。"}}
{"id": "2510.10378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10378", "abs": "https://arxiv.org/abs/2510.10378", "authors": ["Blessing Agyei Kyem", "Joshua Kofi Asamoah", "Eugene Denteh", "Andrews Danyo", "Armstrong Aboah"], "title": "Self-Supervised Multi-Scale Transformer with Attention-Guided Fusion for Efficient Crack Detection", "comment": "The paper has been published at Automation in Construction journal.\n  The paper has 53 pages and 11 figures", "summary": "Pavement crack detection has long depended on costly and time-intensive\npixel-level annotations, which limit its scalability for large-scale\ninfrastructure monitoring. To overcome this barrier, this paper examines the\nfeasibility of achieving effective pixel-level crack segmentation entirely\nwithout manual annotations. Building on this objective, a fully self-supervised\nframework, Crack-Segmenter, is developed, integrating three complementary\nmodules: the Scale-Adaptive Embedder (SAE) for robust multi-scale feature\nextraction, the Directional Attention Transformer (DAT) for maintaining linear\ncrack continuity, and the Attention-Guided Fusion (AGF) module for adaptive\nfeature integration. Through evaluations on ten public datasets,\nCrack-Segmenter consistently outperforms 13 state-of-the-art supervised methods\nacross all major metrics, including mean Intersection over Union (mIoU), Dice\nscore, XOR, and Hausdorff Distance (HD). These findings demonstrate that\nannotation-free crack detection is not only feasible but also superior,\nenabling transportation agencies and infrastructure managers to conduct\nscalable and cost-effective monitoring. This work advances self-supervised\nlearning and motivates pavement cracks detection research.", "AI": {"tldr": "本文提出了一种名为Crack-Segmenter的完全自监督框架，无需手动标注即可实现像素级路面裂缝分割，并在多项指标上超越了有监督方法。", "motivation": "传统的路面裂缝检测依赖耗时且昂贵的像素级手动标注，严重限制了其在大规模基础设施监测中的应用和可扩展性。", "method": "开发了名为Crack-Segmenter的完全自监督框架，包含三个互补模块：用于鲁棒多尺度特征提取的尺度自适应嵌入器（SAE）、用于保持线性裂缝连续性的方向注意力变换器（DAT）以及用于自适应特征融合的注意力引导融合（AGF）模块。", "result": "Crack-Segmenter在十个公共数据集上，于平均交并比（mIoU）、Dice分数、XOR和豪斯多夫距离（HD）等所有主要指标上，持续优于13种最先进的有监督方法。", "conclusion": "研究结果表明，无需标注的裂缝检测不仅可行，而且性能更优越，这使得交通部门和基础设施管理者能够进行可扩展且经济高效的监测。这项工作推动了自监督学习和路面裂缝检测研究的进展。"}}
{"id": "2510.11462", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11462", "abs": "https://arxiv.org/abs/2510.11462", "authors": ["Yisen Gao", "Jiaxin Bai", "Yi Huang", "Xingcheng Fu", "Qingyun Sun", "Yangqiu Song"], "title": "Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model", "comment": "Under Review", "summary": "Deductive and abductive reasoning are two critical paradigms for analyzing\nknowledge graphs, enabling applications from financial query answering to\nscientific discovery. Deductive reasoning on knowledge graphs usually involves\nretrieving entities that satisfy a complex logical query, while abductive\nreasoning generates plausible logical hypotheses from observations. Despite\ntheir clear synergistic potential, where deduction can validate hypotheses and\nabduction can uncover deeper logical patterns, existing methods address them in\nisolation. To bridge this gap, we propose DARK, a unified framework for\nDeductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion\nmodel capable of capturing the bidirectional relationship between queries and\nconclusions, DARK has two key innovations. First, to better leverage deduction\nfor hypothesis refinement during abductive reasoning, we introduce a\nself-reflective denoising process that iteratively generates and validates\ncandidate hypotheses against the observed conclusion. Second, to discover\nricher logical associations, we propose a logic-exploration reinforcement\nlearning approach that simultaneously masks queries and conclusions, enabling\nthe model to explore novel reasoning compositions. Extensive experiments on\nmultiple benchmark knowledge graphs show that DARK achieves state-of-the-art\nperformance on both deductive and abductive reasoning tasks, demonstrating the\nsignificant benefits of our unified approach.", "AI": {"tldr": "本文提出了DARK，一个统一的框架，用于知识图谱上的演绎和溯因推理。它采用掩码扩散模型，结合自反思去噪过程和逻辑探索强化学习，在两类推理任务上均达到了最先进的性能。", "motivation": "演绎推理和溯因推理是知识图谱分析的关键范式，但现有方法通常独立处理它们，未能利用两者之间的协同潜力，即演绎可以验证假设，溯因可以揭示深层逻辑模式。", "method": "本文提出了DARK（Deductive and Abductive Reasoning in Knowledge graphs），一个统一的框架。它是一个掩码扩散模型，能够捕捉查询和结论之间的双向关系，并包含两项关键创新：1. 自反思去噪过程：在溯因推理中，通过迭代生成和验证候选假设来利用演绎推理进行假设细化。2. 逻辑探索强化学习：通过同时掩盖查询和结论，使模型能够探索新颖的推理组合，发现更丰富的逻辑关联。", "result": "在多个基准知识图谱上的大量实验表明，DARK在演绎和溯因推理任务上均取得了最先进的性能。", "conclusion": "统一的演绎和溯因推理方法带来了显著的优势，证明了DARK框架的有效性。"}}
{"id": "2510.11525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11525", "abs": "https://arxiv.org/abs/2510.11525", "authors": ["Luis F. Recalde", "Dhruv Agrawal", "Jon Arrizabalaga", "Guanrui Li"], "title": "DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight", "comment": "Accepted to IEEE Robotics and Automation Letters", "summary": "MAVs have great potential to assist humans in complex tasks, with\napplications ranging from logistics to emergency response. Their agility makes\nthem ideal for operations in complex and dynamic environments. However,\nachieving precise control in agile flights remains a significant challenge,\nparticularly due to the underactuated nature of quadrotors and the strong\ncoupling between their translational and rotational dynamics. In this work, we\npropose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for\nquadrotor flight. By representing both quadrotor dynamics and the pose error\ndirectly on the dual-quaternion manifold, our approach enables a compact and\nglobally non-singular formulation that captures the quadrotor coupled dynamics.\nWe validate our approach through simulations and real-world experiments,\ndemonstrating better numerical conditioning and significantly improved tracking\nperformance, with reductions in position and orientation errors of up to 56.11%\nand 56.77%, compared to a conventional baseline NMPC method. Furthermore, our\ncontroller successfully handles aggressive trajectories, reaching maximum\nspeeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space\nconditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller\nfails.", "AI": {"tldr": "本文提出了一种基于对偶四元数（DQ-NMPC）的新型非线性模型预测控制框架，用于四旋翼飞行器的精确敏捷控制。该方法通过在对偶四元数流形上直接表示动力学和位姿误差，实现了紧凑且全局非奇异的耦合动力学公式。实验结果表明，与传统NMPC相比，DQ-NMPC显著提高了跟踪性能，误差降低高达56%，并能成功处理激进轨迹。", "motivation": "微型飞行器（MAVs）在复杂任务中潜力巨大，尤其是在复杂动态环境中。然而，由于四旋翼飞行器的欠驱动特性以及平移和旋转动力学之间的强耦合，在敏捷飞行中实现精确控制仍然是一个重大挑战。", "method": "提出了一种基于对偶四元数（DQ-NMPC）的新型非线性模型预测控制（NMPC）框架。该方法通过在对偶四元数流形上直接表示四旋翼动力学和位姿误差，实现了紧凑且全局非奇异的公式，能够捕捉四旋翼的耦合动力学。", "result": "通过仿真和真实世界实验验证了该方法，结果显示其具有更好的数值条件，并显著提高了跟踪性能。与传统的基线NMPC方法相比，位置和姿态误差分别减少了高达56.11%和56.77%。此外，该控制器成功处理了激进轨迹，在11m x 4.5m x 3.65m的受限空间条件下，达到了最高13.66 m/s的速度和4.2 g的加速度，而基线控制器在此条件下失败。", "conclusion": "所提出的DQ-NMPC框架为四旋翼飞行器提供了卓越的敏捷飞行精确控制能力，在准确性和处理激进机动方面均优于传统方法。"}}
{"id": "2510.11539", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.11539", "abs": "https://arxiv.org/abs/2510.11539", "authors": ["Denglin Cheng", "Jiarong Kang", "Xiaobin Xiong"], "title": "Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization", "comment": null, "summary": "Accurate state estimation is critical for legged and aerial robots operating\nin dynamic, uncertain environments. A key challenge lies in specifying process\nand measurement noise covariances, which are typically unknown or manually\ntuned. In this work, we introduce a bi-level optimization framework that\njointly calibrates covariance matrices and kinematic parameters in an\nestimator-in-the-loop manner. The upper level treats noise covariances and\nmodel parameters as optimization variables, while the lower level executes a\nfull-information estimator. Differentiating through the estimator allows direct\noptimization of trajectory-level objectives, resulting in accurate and\nconsistent state estimates. We validate our approach on quadrupedal and\nhumanoid robots, demonstrating significantly improved estimation accuracy and\nuncertainty calibration compared to hand-tuned baselines. Our method unifies\nstate estimation, sensor, and kinematics calibration into a principled,\ndata-driven framework applicable across diverse robotic platforms.", "AI": {"tldr": "本文提出了一种双层优化框架，用于在机器人状态估计中联合校准噪声协方差和运动学参数，以提高估计精度和不确定性校准。", "motivation": "在动态、不确定环境中运行的腿式和空中机器人，准确的状态估计至关重要。主要挑战在于过程和测量噪声协方差通常未知或需要手动调整，这影响了估计的准确性。", "method": "引入了一个双层优化框架。上层将噪声协方差和模型参数作为优化变量。下层执行一个全信息估计器。通过对估计器进行微分，可以直接优化轨迹层面的目标，从而实现准确和一致的状态估计。", "result": "在四足和人形机器人上的验证表明，与手动调整的基线相比，估计精度和不确定性校准显著提高。该方法将状态估计、传感器和运动学校准统一到一个原则性的、数据驱动的框架中。", "conclusion": "该方法提供了一个统一的、数据驱动的框架，将状态估计、传感器和运动学校准整合在一起，适用于各种机器人平台，显著提高了状态估计的准确性和不确定性校准。"}}
{"id": "2510.10453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10453", "abs": "https://arxiv.org/abs/2510.10453", "authors": ["Peng Fan", "Wenping Wang", "Fei Deng"], "title": "End-to-end Speech Recognition with similar length speech and text", "comment": null, "summary": "The mismatch of speech length and text length poses a challenge in automatic\nspeech recognition (ASR). In previous research, various approaches have been\nemployed to align text with speech, including the utilization of Connectionist\nTemporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)\nwas introduced, utilizing intermediate CTC outputs to guide downsampling and\npreserve keyframes, but traditional methods (CTC) failed to align speech and\ntext appropriately when downsampling speech to a text-similar length. In this\npaper, we focus on speech recognition in those cases where the length of speech\naligns closely with that of the corresponding text. To address this issue, we\nintroduce two methods for alignment: a) Time Independence Loss (TIL) and b)\nAligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance\nthe information on keyframes, we incorporate frame fusion by applying weights\nand summing the keyframe with its context 2 frames. Experimental results on\nAISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods\noutperform the previous work and achieve a reduction of at least 86\\% in the\nnumber of frames.", "AI": {"tldr": "本文提出两种新方法（TIL和AXE损失）和帧融合机制，以改善语音与文本长度相近时的自动语音识别（ASR）对齐问题，显著减少帧数并优于现有方法。", "motivation": "自动语音识别（ASR）中语音和文本长度不匹配是一个挑战。现有方法（如基于CTC的KFDS）在将语音下采样到与文本长度相似时，无法正确对齐语音和文本。本文旨在解决语音长度与文本长度接近时的对齐问题。", "method": "本文引入了两种对齐方法：a) 时间独立损失（Time Independence Loss, TIL）和 b) 基于编辑距离的对齐交叉熵（Aligned Cross Entropy, AXE）损失。为增强关键帧信息，还通过加权并将关键帧与其前后各2帧的上下文进行求和，实现了帧融合。", "result": "在AISHELL-1和AISHELL-2数据集子集上的实验结果表明，所提出的方法优于现有工作，并且至少减少了86%的帧数。", "conclusion": "所提出的TIL和AXE损失以及帧融合机制，在语音与文本长度相近的情况下，有效解决了ASR的对齐问题，显著提高了效率（减少帧数）并取得了更好的性能。"}}
{"id": "2510.11558", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11558", "abs": "https://arxiv.org/abs/2510.11558", "authors": ["Komal Gupta", "Aditya Shrivastava"], "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products", "comment": null, "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.", "AI": {"tldr": "本文探讨了企业级AI助手（如LLM）中的零数据保留策略，分析了Salesforce AgentForce和Microsoft Copilot在实现零数据保留方面的技术架构、合规性及可用性权衡。", "motivation": "随着企业AI助手的兴起，保护私有数据和确保合规性成为关键优先事项，尤其是在医疗和金融等敏感行业。零数据保留策略对于增强业务生产力同时维护数据隐私至关重要。", "method": "研究探索了大型语言模型（LLM）企业应用中的零数据保留策略，定义了此类系统在架构、合规性和可用性方面的权衡。通过分析Salesforce AgentForce和Microsoft Copilot这两款领先AI助手，考察了它们在支持零数据保留政策上采用的不同技术架构，并考虑了OpenAI、Anthropic和Meta等LLM服务提供商的作用。", "result": "文章分析了消费应用程序（如Salesforce AgentForce和Microsoft Copilot）以及大型语言模型服务提供商（如OpenAI、Anthropic和Meta）在部署零数据保留策略时的技术架构和实现方式，揭示了不同公司在此方面的独特方法。", "conclusion": "研究分析了企业AI助手中零数据保留策略的技术架构和部署，强调了实现数据治理、合规性和业务隐私的重要性，并展示了行业领导者如何通过不同架构实现这一目标。"}}
{"id": "2510.10457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10457", "abs": "https://arxiv.org/abs/2510.10457", "authors": ["Shaobo Wang", "Cong Wang", "Wenjie Fu", "Yue Min", "Mingquan Feng", "Isabel Guan", "Xuming Hu", "Conghui He", "Cunxiang Wang", "Kexin Yang", "Xingzhang Ren", "Fei Huang", "Dayiheng Liu", "Linfeng Zhang"], "title": "Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?", "comment": "18 pages, 5 figures", "summary": "As the demand for comprehensive evaluations of diverse model capabilities\nsteadily increases, benchmark suites have correspondingly grown significantly\nin scale. Despite notable advances in redundancy reduction and subset-level\nperformance prediction, a systematic framework that effectively integrates\nthese methods to ensure both prediction accuracy and ranking consistency is\nstill largely elusive. In this paper, we first perform a sample-level analysis\nof benchmark redundancy and identify several highly similar samples that can be\neliminated. Besides, we frame benchmark compression as an optimization problem\nwith the aim of score reconstruction. Building on these, we then propose\nEssenceBench, a coarse-to-fine framework utilizing an iterative Genetic\nAlgorithm (GA), which takes the advantages of fitness-based subset search and\nattribution-based sample search. Compared to previous methods, our approach\nyields superior compression results with lower reconstruction error and\nmarkedly higher efficiency. In particular, on the HellaSwag benchmark (10K\nsamples), our method preserves the ranking of all models shifting within 5%\nusing 25x fewer samples, and achieves 95% ranking preservation shifting within\n5% using only 200x fewer samples.", "AI": {"tldr": "本文提出了EssenceBench，一个粗粒度到细粒度的迭代遗传算法框架，用于基准测试压缩。该框架通过样本级冗余分析和优化问题建模，显著减少了样本数量，同时保持了模型评分重建的准确性和排名的一致性。", "motivation": "随着对多样化模型能力进行全面评估的需求不断增加，基准测试套件的规模也显著扩大。尽管在冗余减少和子集级性能预测方面取得了进展，但仍缺乏一个系统框架，能有效整合这些方法以同时确保预测准确性和排名一致性。", "method": "首先进行样本级冗余分析，识别并消除高度相似的样本。其次，将基准测试压缩视为一个以分数重建为目标的优化问题。在此基础上，提出了EssenceBench，一个粗粒度到细粒度的框架，利用迭代遗传算法（GA）进行基于适应度的子集搜索和基于归因的样本搜索。", "result": "与现有方法相比，EssenceBench在压缩结果上表现更优，重建误差更低，效率显著提高。特别是在HellaSwag基准测试（10K样本）上，该方法使用减少25倍的样本，能将所有模型的排名波动控制在5%以内；使用减少200倍的样本，能实现95%的模型排名波动控制在5%以内。", "conclusion": "EssenceBench提供了一个有效且高效的基准测试压缩解决方案，能够在显著减少样本数量的同时，保持评分重建的高准确性和模型排名的一致性。"}}
{"id": "2510.10383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10383", "abs": "https://arxiv.org/abs/2510.10383", "authors": ["Sai Teja Erukude"], "title": "Identifying bias in CNN image classification using image scrambling and transforms", "comment": "62 pages, Master's thesis", "summary": "CNNs are now prevalent as the primary choice for most machine vision problems\ndue to their superior rate of classification and the availability of\nuser-friendly libraries. These networks effortlessly identify and select\nfeatures in a non-intuitive data-driven manner, making it difficult to\ndetermine which features were most influential. That leads to a ``black box\",\nwhere users cannot know how the image data are analyzed but rely on empirical\nresults. Therefore the decision-making process can be biased by background\ninformation that is difficult to detect. Here we discuss examples of such\nhidden biases and propose techniques for identifying them, methods to\ndistinguish between contextual information and background noise, and explore\nwhether CNNs learn from irrelevant features. One effective approach to identify\ndataset bias is to classify blank background parts of the images. However, in\nsome situations a blank background in the images is not available, making it\nmore difficult to separate the foreground information from the blank\nbackground. Such parts of the image can also be considered contextual learning,\nnot necessarily bias. To overcome this, we propose two approaches that were\ntested on six different datasets, including natural, synthetic, and hybrid\ndatasets. The first method involves dividing images into smaller,\nnon-overlapping tiles of various sizes, which are then shuffled randomly,\nmaking classification more challenging. The second method involves the\napplication of several image transforms, including Fourier, Wavelet transforms,\nand Median filter, and their combinations. These transforms help recover\nbackground noise information used by CNN to classify images. Results indicate\nthat this method can effectively distinguish between contextual information and\nbackground noise, and alert on the presence of background noise even without\nthe need to use background information.", "AI": {"tldr": "本文讨论了卷积神经网络（CNN）的“黑箱”特性及其对背景噪声产生的隐藏偏差。作者提出了两种方法，包括图像分块和图像变换，以区分上下文信息和背景噪声，从而识别并减轻这些偏差。", "motivation": "CNN因其卓越的分类性能而广泛应用于机器视觉，但其数据驱动的特征选择方式使其成为“黑箱”，难以理解其决策过程。这可能导致模型被难以检测的背景信息偏置，用户无法了解图像数据如何被分析，只能依赖经验结果。", "method": "本文提出了识别隐藏偏差的技术，并区分上下文信息与背景噪声的方法。具体包括两种主要方法：1. 将图像分割成不同大小的、随机打乱的、不重叠的小块，以增加分类难度。2. 应用多种图像变换（如傅里叶变换、小波变换和中值滤波器及其组合），以恢复CNN用于分类的背景噪声信息。这些方法在六个不同的数据集（包括自然、合成和混合数据集）上进行了测试。", "result": "结果表明，所提出的方法（特别是图像变换方法）能够有效地区分上下文信息和背景噪声，并且即使在没有明确的背景信息的情况下，也能警示背景噪声的存在。", "conclusion": "本文提出的方法有助于识别CNN决策过程中由背景噪声引起的隐藏偏差，有效区分图像中的上下文信息和背景噪声，从而提高CNN模型的可解释性和可靠性。"}}
{"id": "2510.11542", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11542", "abs": "https://arxiv.org/abs/2510.11542", "authors": ["Neil C. Janwani", "Varun Madabushi", "Maegan Tucker"], "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust\ncontrol policies for bipedal locomotion. Yet, it can be difficult to tune\ndesired robot behaviors due to unintuitive and complex reward design. In\ncomparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,\noffer more tuneable, interpretable, and mathematically grounded motion plans\nfor high-dimensional legged systems. However, these methods often remain\nbrittle to real-world disturbances like external perturbations.\n  In this work, we present NaviGait, a hierarchical framework that combines the\nstructure of trajectory optimization with the adaptability of RL for robust and\nintuitive locomotion control. NaviGait leverages a library of offline-optimized\ngaits and smoothly interpolates between them to produce continuous reference\nmotions in response to high-level commands. The policy provides both\njoint-level and velocity command residual corrections to modulate and stabilize\nthe reference trajectories in the gait library. One notable advantage of\nNaviGait is that it dramatically simplifies reward design by encoding rich\nmotion priors from trajectory optimization, reducing the need for finely tuned\nshaping terms and enabling more stable and interpretable learning. Our\nexperimental results demonstrate that NaviGait enables faster training compared\nto conventional and imitation-based RL, and produces motions that remain\nclosest to the original reference. Overall, by decoupling high-level motion\ngeneration from low-level correction, NaviGait offers a more scalable and\ngeneralizable approach for achieving dynamic and robust locomotion.", "AI": {"tldr": "NaviGait是一个分层框架，结合了轨迹优化和强化学习的优点，实现了鲁棒且直观的双足机器人运动控制。", "motivation": "强化学习（RL）在双足运动控制中表现出强大能力，但奖励设计复杂且难以调整。离线轨迹优化方法虽然可调、可解释且有数学基础，但对真实世界干扰（如外部扰动）缺乏鲁棒性。本研究旨在结合两者的优点，克服各自的局限性。", "method": "NaviGait是一个分层框架。它利用离线优化的步态库，并根据高级指令平滑地在这些步态之间进行插值，以生成连续的参考运动。强化学习策略提供关节级和速度指令的残差校正，以调节和稳定步态库中的参考轨迹。通过编码轨迹优化中的丰富运动先验，极大地简化了奖励设计。", "result": "实验结果表明，NaviGait比传统和基于模仿的强化学习训练更快，并且产生的运动最接近原始参考轨迹。", "conclusion": "通过将高级运动生成与低级校正解耦，NaviGait为实现动态和鲁棒的运动提供了一种更具可扩展性和通用性的方法。"}}
{"id": "2510.10395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10395", "abs": "https://arxiv.org/abs/2510.10395", "authors": ["Xinlong Chen", "Yue Ding", "Weihong Lin", "Jingyun Hua", "Linli Yao", "Yang Shi", "Bozhou Li", "Yuanxing Zhang", "Qiang Liu", "Pengfei Wan", "Liang Wang", "Tieniu Tan"], "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration", "comment": "Project webpage: https://avocado-captioner.github.io/", "summary": "Audiovisual video captioning aims to generate semantically rich descriptions\nwith temporal alignment between visual and auditory events, thereby benefiting\nboth video understanding and generation. In this paper, we present AVoCaDO, a\npowerful audiovisual video captioner driven by the temporal orchestration\nbetween audio and visual modalities. We propose a two-stage post-training\npipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated\ndataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)\nAVoCaDO GRPO, which leverages tailored reward functions to further enhance\ntemporal coherence and dialogue accuracy while regularizing caption length and\nreducing collapse. Experimental results demonstrate that AVoCaDO significantly\noutperforms existing open-source models across four audiovisual video\ncaptioning benchmarks, and also achieves competitive performance on the VDC and\nDREAM-1K benchmark under visual-only settings.", "AI": {"tldr": "本文提出了AVoCaDO，一个通过音视频模态间时间协调驱动的强大音视频视频字幕生成器，采用两阶段后训练流程，显著优于现有模型。", "motivation": "音视频视频字幕生成旨在生成语义丰富且音视频事件时间对齐的描述，以促进视频理解和生成。现有模型可能在此方面存在不足。", "method": "本文提出了AVoCaDO模型，其核心在于音视频模态间的时间编排。它采用两阶段后训练流程：1) AVoCaDO SFT，在一个新整理的107K高质量、时间对齐的音视频字幕数据集上进行微调；2) AVoCaDO GRPO，利用定制的奖励函数进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少内容崩溃。", "result": "实验结果表明，AVoCaDO在四个音视频视频字幕生成基准测试中显著优于现有开源模型。此外，在仅视觉设置下，它在VDC和DREAM-1K基准测试中也取得了有竞争力的表现。", "conclusion": "AVoCaDO是一个强大且高效的音视频视频字幕生成器，通过其创新的两阶段后训练方法和对时间协调的关注，显著提升了音视频字幕生成的性能，并在纯视觉场景下也表现出色。"}}
{"id": "2510.10459", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10459", "abs": "https://arxiv.org/abs/2510.10459", "authors": ["Prawaal Sharma", "Poonam Goyal", "Navneet Goyal", "Vidisha Sharma"], "title": "NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication", "comment": "9 pages, EMNLP Findings 2025", "summary": "Digital communication has become the cornerstone of modern interaction,\nenabling rapid, accessible, and interactive exchanges. However, individuals\nwith lower academic literacy often face significant barriers, exacerbating the\n\"digital divide\". In this work, we introduce a novel, universal ideographic\nmetalanguage designed as an innovative communication framework that transcends\nacademic, linguistic, and cultural boundaries. Our approach leverages\nprinciples of Neuro-symbolic AI, combining neural-based large language models\n(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded\nin the linguistic theory of Natural Semantic Metalanguage (NSM). This enables\nthe semantic decomposition of complex ideas into simpler, atomic concepts.\nAdopting a human-centric, collaborative methodology, we engaged over 200\nsemi-literate participants in defining the problem, selecting ideographs, and\nvalidating the system. With over 80\\% semantic comprehensibility, an accessible\nlearning curve, and universal adaptability, our system effectively serves\nunderprivileged populations with limited formal education.", "AI": {"tldr": "本文提出了一种新颖的通用表意元语言，结合神经符号人工智能（LLMs和NSM）将复杂概念分解为原子概念，旨在帮助学术素养较低的人群克服数字鸿沟，实现了高语义可理解性和易学性。", "motivation": "数字交流已成为现代互动的基础，但学术素养较低的个体面临显著障碍，加剧了“数字鸿沟”。研究旨在为这部分人群提供一个超越学术、语言和文化界限的创新交流框架，实现快速、可访问和交互式的交流。", "method": "研究引入了一种新型的通用表意元语言。其方法基于神经符号人工智能（Neuro-symbolic AI），结合了富含世界知识的神经网络大型语言模型（LLMs）和植根于自然语义元语言（NSM）理论的符号知识启发式算法。这使得复杂思想能够语义分解为更简单的原子概念。此外，采用以人为本的协作方法，与200多名半文盲参与者共同定义问题、选择表意符号并验证系统。", "result": "该系统实现了超过80%的语义可理解性，具有易于掌握的学习曲线和普遍的适应性。它有效地服务于受正规教育有限的弱势群体。", "conclusion": "该系统成功地为受教育程度有限的弱势群体提供了有效的服务，有效弥合了沟通障碍，超越了学术、语言和文化界限，从而解决了数字鸿沟问题。"}}
{"id": "2510.11595", "categories": ["cs.AI", "cs.GL"], "pdf": "https://arxiv.org/pdf/2510.11595", "abs": "https://arxiv.org/abs/2510.11595", "authors": ["Israel Mason-Williams", "Gabryel Mason-Williams"], "title": "Reproducibility: The New Frontier in AI Governance", "comment": "12 pages,6 figures,Workshop on Technical AI Governance at ICML", "summary": "AI policymakers are responsible for delivering effective governance\nmechanisms that can provide safe, aligned and trustworthy AI development.\nHowever, the information environment offered to policymakers is characterised\nby an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and\ncreating deep uncertainty and divides on which risks should be prioritised from\na governance perspective. We posit that the current publication speeds in AI\ncombined with the lack of strong scientific standards, via weak reproducibility\nprotocols, effectively erodes the power of policymakers to enact meaningful\npolicy and governance protocols. Our paper outlines how AI research could adopt\nstricter reproducibility guidelines to assist governance endeavours and improve\nconsensus on the AI risk landscape. We evaluate the forthcoming reproducibility\ncrisis within AI research through the lens of crises in other scientific\ndomains; providing a commentary on how adopting preregistration, increased\nstatistical power and negative result publication reproducibility protocols can\nenable effective AI governance. While we maintain that AI governance must be\nreactive due to AI's significant societal implications we argue that\npolicymakers and governments must consider reproducibility protocols as a core\ntool in the governance arsenal and demand higher standards for AI research.\nCode to replicate data and figures:\nhttps://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance", "AI": {"tldr": "AI政策制定者面临低信噪比的信息环境，受AI研究快速发表和弱可重复性协议影响，难以有效治理。本文提出AI研究应采纳更严格的可重复性指南（如预注册、提高统计功效、发表负面结果），以协助治理并增进对AI风险的共识。", "motivation": "AI政策制定者在制定有效治理机制时，面临信息环境的低信噪比、监管俘获、以及对AI风险优先级的深层不确定性。这主要是由于AI研究的快速发表速度和缺乏强科学标准（通过弱可重复性协议）削弱了政策制定者制定有意义政策的能力。", "method": "本文通过借鉴其他科学领域的危机，评估了AI研究中即将到来的可重复性危机。它提出并评论了如何采纳预注册、提高统计功效以及发表负面结果的可重复性协议，以实现有效的AI治理。", "result": "采纳更严格的可重复性指南，特别是预注册、提高统计功效和发表负面结果的协议，能够协助AI治理工作，改善对AI风险格局的共识，并赋能有效的AI治理。这些协议应成为政策制定者治理工具库中的核心工具。", "conclusion": "尽管AI治理必须是反应性的，但政策制定者和政府必须将可重复性协议视为治理工具库中的核心工具，并要求AI研究达到更高的标准，以应对当前的信息环境挑战，实现有意义的AI政策和治理。"}}
{"id": "2510.11588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11588", "abs": "https://arxiv.org/abs/2510.11588", "authors": ["Jiateng Liu", "Zhenhailong Wang", "Xiaojiang Huang", "Yingjie Li", "Xing Fan", "Xiang Li", "Chenlei Guo", "Ruhi Sarikaya", "Heng Ji"], "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "comment": "42 pages", "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.", "AI": {"tldr": "该研究提出CC-Gen基准和CAP-CPT方法，以解决大型语言模型代理中复杂策略文档的内部化问题，显著减少提示长度和计算开销，同时提高性能。", "motivation": "基于LLM的代理系统依赖于编码业务规则的上下文策略文档。随着需求增长，这些文档迅速膨胀，导致高计算开销。现有的提示压缩方法不适用于代理策略文档的多层次复杂性和深度推理需求，因此需要开发将策略文档嵌入模型先验的内部化方法。", "method": "1. 引入CC-Gen：一个具有四个可控复杂性级别的代理基准生成器，用于系统评估代理处理复杂性的能力和评估策略内部化。2. 提出CAP-CPT（Category-Aware Policy Continued Pretraining）：一个自动化流程，解析策略文档以提取关键规范，将其分为事实性、行为性和条件性类别，并隔离驱动工作流复杂性的复杂条件，以此指导有针对性的数据合成，并通过自回归预训练损失使代理内部化策略信息。", "result": "1. 复杂的策略规范对代理构成重大推理挑战。2. 通过SFT（监督微调）支持内部化是数据密集型的，并随着策略复杂性增加而急剧下降。3. CAP-CPT在所有设置下都改进了SFT基线，在Qwen-3-32B上实现了高达41%和22%的增益。4. 在CC-Gen上实现了97.3%的提示长度缩减。5. 在SFT数据量极小的情况下，进一步增强了tau-Bench的性能。", "conclusion": "CAP-CPT通过类别感知策略持续预训练，有效地将复杂代理策略文档内部化到LLM中，显著减少了提示长度和计算开销，同时在处理复杂推理任务时提高了代理的性能，并缓解了数据和推理负担。"}}
{"id": "2510.11552", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11552", "abs": "https://arxiv.org/abs/2510.11552", "authors": ["Gregoire Passault", "Clement Gaspard", "Olivier Ly"], "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education", "comment": null, "summary": "Recent developments of low cost off-the-shelf programmable components, their\nmodularity, and also rapid prototyping made educational robotics flourish, as\nit is accessible in most schools today. They allow to illustrate and embody\ntheoretical problems in practical and tangible applications, and gather\nmultidisciplinary skills. They also give a rich natural context for\nproject-oriented pedagogy. However, most current robot kits all are limited to\negocentric aspect of the robots perception. This makes it difficult to access\nmore high-level problems involving e.g. coordinates or navigation. In this\npaper we introduce an educational holonomous robot kit that comes with an\nexternal tracking system, which lightens the constraint on embedded systems,\nbut allows in the same time to discover high-level aspects of robotics,\notherwise unreachable.", "AI": {"tldr": "教育机器人普及但受限于自我中心感知。本文提出一种带外部跟踪系统的全向教育机器人套件，以实现更高级别的机器人学习。", "motivation": "现有教育机器人套件的感知能力仅限于自我中心视角，这使得学生难以学习涉及坐标或导航等更高级别的机器人问题。", "method": "引入一个教育用全向机器人套件，并配备一个外部跟踪系统。这种设计减轻了嵌入式系统的计算负担。", "result": "该套件使得学生能够探索传统机器人套件无法触及的高级机器人概念，例如坐标和导航。", "conclusion": "所提出的带外部跟踪系统的全向教育机器人套件克服了现有教育机器人的局限性，能够提供更深入、更全面的机器人学习体验。"}}
{"id": "2510.10472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10472", "abs": "https://arxiv.org/abs/2510.10472", "authors": ["Qiran Zou", "Hou Hei Lam", "Wenhao Zhao", "Yiming Tang", "Tingting Chen", "Samson Yu", "Tianyi Zhang", "Chang Liu", "Xiangyang Ji", "Dianbo Liu"], "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth", "comment": "Our benchmark is available at: https://github.com/qrzou/FML-bench", "summary": "Large language models (LLMs) have sparked growing interest in automatic\nmachine learning research agents. Among them, agents capable of autonomously\nproposing ideas and conducting machine learning experiments are particularly\npromising, as they maximize research automation and accelerate scientific\nprogress by iteratively refining ideas based on experimental results. However,\ncomprehensively evaluating such agents remains challenging. Existing benchmarks\ntend to overemphasize engineering aspects while neglecting academic rigor,\ncreating barriers that obscure a clear assessment of an agent's scientific\ncapabilities in machine learning research. They also suffer from limited task\ndiversity, an overemphasis on application-oriented tasks over fundamental\nresearch problems, and limited scalability to realistic research settings. To\naddress these limitations, we introduce FML-bench, a benchmark designed to\nevaluate automatic machine learning research agents on 8 diverse and\nfundamental machine learning research problems. It reduces coding burden,\nemphasizes fundamental problems rather than specific use cases, offers high\ntask diversity, and is extensible to real-world machine learning GitHub\nrepositories. Furthermore, we present a unified evaluation framework with five\ncomplementary metrics, designed to comprehensively assess agent performance on\nour benchmark. We evaluate state-of-the-art automatic research agents on\nFML-bench, and find that agents employing broad research exploration strategies\noutperform those focusing on narrow but deep exploration. These findings\nsuggest that emphasizing the breadth of exploration may lead to more effective\nresearch outcomes than focusing solely on incremental refinement. Our benchmark\nis available at https://github.com/qrzou/FML-bench.", "AI": {"tldr": "本文提出了FML-bench，一个用于评估自动机器学习研究代理的新基准，旨在解决现有基准在学术严谨性、任务多样性和可扩展性方面的不足，并发现广泛探索策略优于深度探索。", "motivation": "现有用于评估能自主提出想法和进行机器学习实验的LLM研究代理的基准存在局限性，包括：过分强调工程方面而非学术严谨性、任务多样性不足、过度关注应用而非基础研究问题，以及难以扩展到真实研究场景。", "method": "本文引入了FML-bench，一个包含8个多样化和基础机器学习研究问题的基准，旨在减少编码负担、强调基础问题、提供高任务多样性并可扩展到真实世界的GitHub仓库。此外，还提出了一个包含五个互补指标的统一评估框架。", "result": "通过在FML-bench上评估最先进的自动研究代理，发现采用广泛研究探索策略的代理表现优于那些专注于狭窄但深度探索的代理。", "conclusion": "这些发现表明，强调探索的广度可能比仅仅关注增量改进能带来更有效的研究成果。"}}
{"id": "2510.11604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11604", "abs": "https://arxiv.org/abs/2510.11604", "authors": ["Sanjula De Alwis", "Indrajith Ekanayake"], "title": "Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce", "comment": null, "summary": "In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.", "AI": {"tldr": "本研究提出一个结合可解释AI、生存分析和RFM分析的三组件框架，旨在将客户流失预测转向可解释的个性化留存策略设计，以降低流失并增强客户忠诚度。", "motivation": "在线零售中，客户获取成本高于客户保留成本，促使企业投资于流失分析。然而，现有流失模型多为“黑箱”，缺乏对流失原因、干预时机和高风险客户群体的深入洞察，因此需要从单纯预测转向基于可解释证据的个性化留存策略设计。", "method": "本研究提出了一个三组件框架，整合了：1) 可解释AI（XAI）以量化特征贡献；2) 生存分析以建模事件发生时间（流失）风险；3) RFM（最近一次消费、消费频率、消费金额）分析以根据交易行为对客户进行分段。", "result": "通过结合这些方法，该框架能够归因流失驱动因素、估算干预窗口期，并优先排序目标行动的客户细分，从而支持减少流失和增强客户忠诚度的策略。", "conclusion": "该框架通过提供可解释的洞察力，使企业能够设计出更具针对性和有效性的个性化留存策略，从而降低客户流失率并提升客户忠诚度。"}}
{"id": "2510.11566", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11566", "abs": "https://arxiv.org/abs/2510.11566", "authors": ["Kuanning Wang", "Yongchong Gu", "Yuqian Fu", "Zeyu Shangguan", "Sicheng He", "Xiangyang Xue", "Yanwei Fu", "Daniel Seita"], "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy", "comment": "Project page is at https://scoopdiff.github.io/", "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.", "AI": {"tldr": "SCOOP'D是一种通过仿真（OmniGibson）收集舀取演示并利用扩散生成策略进行模仿学习的方法，在现实世界中实现了零样本泛化，并在机器人舀取任务中表现出色。", "motivation": "机器人舀取物品（如使用勺子）在日常生活中很常见，但开发通用的自主机器人舀取策略具有挑战性。这需要理解复杂的工具-物体交互，并且通常涉及操作可变形物体（如颗粒介质或液体），其无限维配置空间和复杂动力学使得问题更加困难。", "method": "本文提出SCOOP'D方法。它利用基于NVIDIA Omniverse构建的OmniGibson仿真平台，通过依赖特权状态信息的算法程序收集舀取演示。随后，该方法通过扩散模型生成策略，从观测输入中模仿这些演示。最后，将学习到的策略直接应用于各种现实世界场景进行测试。", "result": "SCOOP'D在零样本部署中表现出有前景的结果，在465次不同场景（包括不同数量、特性和容器类型，以及“一级”和“二级”难度物体）的试验中取得了成功。该方法超越了所有基线和消融实验，证明了其有效性。", "conclusion": "SCOOP'D是一种获取机器人舀取技能的有前景的方法，通过结合仿真演示和扩散生成策略，实现了在复杂现实世界场景中的有效零样本泛化。"}}
{"id": "2510.10406", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10406", "abs": "https://arxiv.org/abs/2510.10406", "authors": ["Zhao-Yang Wang", "Jieneng Chen", "Jiang Liu", "Yuxiang Guo", "Rama Chellappa"], "title": "Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes", "comment": null, "summary": "Gait recognition, a fundamental biometric technology, leverages unique\nwalking patterns for individual identification, typically using 2D\nrepresentations such as silhouettes or skeletons. However, these methods often\nstruggle with viewpoint variations, occlusions, and noise. Multi-modal\napproaches that incorporate 3D body shape information offer improved robustness\nbut are computationally expensive, limiting their feasibility for real-time\napplications. To address these challenges, we introduce Mesh-Gait, a novel\nend-to-end multi-modal gait recognition framework that directly reconstructs 3D\nrepresentations from 2D silhouettes, effectively combining the strengths of\nboth modalities. Compared to existing methods, directly learning 3D features\nfrom 3D joints or meshes is complex and difficult to fuse with silhouette-based\ngait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as an\nintermediate representation, enabling the model to effectively capture 3D\ngeometric information while maintaining simplicity and computational\nefficiency. During training, the intermediate 3D heatmaps are gradually\nreconstructed and become increasingly accurate under supervised learning, where\nthe loss is calculated between the reconstructed 3D joints, virtual markers,\nand 3D meshes and their corresponding ground truth, ensuring precise spatial\nalignment and consistent 3D structure. Mesh-Gait extracts discriminative\nfeatures from both silhouettes and reconstructed 3D heatmaps in a\ncomputationally efficient manner. This design enables the model to capture\nspatial and structural gait characteristics while avoiding the heavy overhead\nof direct 3D reconstruction from RGB videos, allowing the network to focus on\nmotion dynamics rather than irrelevant visual details. Extensive experiments\ndemonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will be\nreleased upon acceptance of the paper.", "AI": {"tldr": "Mesh-Gait是一种新颖的多模态步态识别框架，通过从2D轮廓直接重建3D热图作为中间表示，有效结合了2D方法的效率和3D方法的鲁棒性，解决了现有2D方法对视角变化敏感和3D方法计算成本高昂的问题，并实现了最先进的准确性。", "motivation": "传统的2D步态识别（如轮廓或骨架）容易受到视角变化、遮挡和噪声的影响。虽然结合3D身体形状信息的多模态方法更具鲁棒性，但其计算成本高昂，不适用于实时应用。", "method": "Mesh-Gait是一个端到端的多模态步态识别框架。它从2D轮廓直接重建3D热图作为中间表示，以高效地捕获3D几何信息。在训练过程中，通过监督学习逐步精确重建3D热图，损失函数基于重建的3D关节、虚拟标记和3D网格与对应的真实值计算。模型从轮廓和重建的3D热图中提取判别性特征，避免了从RGB视频直接进行昂贵的3D重建。", "result": "Mesh-Gait在广泛的实验中实现了最先进的准确性。", "conclusion": "Mesh-Gait通过将3D热图作为中间表示，成功地将2D轮廓的计算效率与3D几何信息的鲁棒性结合起来，为步态识别提供了一种高效且准确的解决方案，克服了现有方法的局限性。"}}
{"id": "2510.10417", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10417", "abs": "https://arxiv.org/abs/2510.10417", "authors": ["Zhao-Yang Wang", "Zhimin Shao", "Jieneng Chen", "Rama Chellappa"], "title": "Combo-Gait: Unified Transformer Framework for Multi-Modal Gait Recognition and Attribute Analysis", "comment": null, "summary": "Gait recognition is an important biometric for human identification at a\ndistance, particularly under low-resolution or unconstrained environments.\nCurrent works typically focus on either 2D representations (e.g., silhouettes\nand skeletons) or 3D representations (e.g., meshes and SMPLs), but relying on a\nsingle modality often fails to capture the full geometric and dynamic\ncomplexity of human walking patterns. In this paper, we propose a multi-modal\nand multi-task framework that combines 2D temporal silhouettes with 3D SMPL\nfeatures for robust gait analysis. Beyond identification, we introduce a\nmultitask learning strategy that jointly performs gait recognition and human\nattribute estimation, including age, body mass index (BMI), and gender. A\nunified transformer is employed to effectively fuse multi-modal gait features\nand better learn attribute-related representations, while preserving\ndiscriminative identity cues. Extensive experiments on the large-scale BRIAR\ndatasets, collected under challenging conditions such as long-range distances\n(up to 1 km) and extreme pitch angles (up to 50{\\deg}), demonstrate that our\napproach outperforms state-of-the-art methods in gait recognition and provides\naccurate human attribute estimation. These results highlight the promise of\nmulti-modal and multitask learning for advancing gait-based human understanding\nin real-world scenarios.", "AI": {"tldr": "本文提出了一种多模态（2D剪影+3D SMPL）和多任务（步态识别+人体属性估计）框架，利用统一的Transformer模型进行鲁棒的步态分析，并在挑战性数据集上超越了现有技术。", "motivation": "当前的步态识别方法通常依赖单一模态（2D或3D），未能充分捕捉人类步态的几何和动态复杂性，尤其在低分辨率或无约束环境下表现不佳。研究人员希望开发一种更全面的方法，不仅能进行身份识别，还能估计人体属性。", "method": "该研究提出了一种多模态和多任务框架：1) 结合了2D时间剪影和3D SMPL特征；2) 采用多任务学习策略，同时进行步态识别和人体属性（年龄、BMI、性别）估计；3) 使用统一的Transformer模型有效融合多模态步态特征，学习与属性相关的表示，同时保留判别性身份线索。", "result": "在具有挑战性条件（长距离、极端俯仰角）的大规模BRIAR数据集上，该方法在步态识别方面优于现有最先进的方法，并提供了准确的人体属性估计。", "conclusion": "多模态和多任务学习有望推动步态识别领域在真实世界场景中的人类理解能力进一步发展。"}}
{"id": "2510.10474", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10474", "abs": "https://arxiv.org/abs/2510.10474", "authors": ["Jingyi Wu", "Junying Liang"], "title": "When or What? Understanding Consumer Engagement on Digital Platforms", "comment": "21 pages, 6 figures, 3 tables", "summary": "Understanding what drives popularity is critical in today's digital service\neconomy, where content creators compete for consumer attention. Prior studies\nhave primarily emphasized the role of content features, yet creators often\nmisjudge what audiences actually value. This study applies Latent Dirichlet\nAllocation (LDA) modeling to a large corpus of TED Talks, treating the platform\nas a case of digital service provision in which creators (speakers) and\nconsumers (audiences) interact. By comparing the thematic supply of creators\nwith the demand expressed in audience engagement, we identify persistent\nmismatches between producer offerings and consumer preferences. Our\nlongitudinal analysis further reveals that temporal dynamics exert a stronger\ninfluence on consumer engagement than thematic content, suggesting that when\ncontent is delivered may matter more than what is delivered. These findings\nchallenge the dominant assumption that content features are the primary drivers\nof popularity and highlight the importance of timing and contextual factors in\nshaping consumer responses. The results provide new insights into consumer\nattention dynamics on digital platforms and carry practical implications for\nmarketers, platform managers, and content creators seeking to optimize audience\nengagement strategies.", "AI": {"tldr": "本研究发现，在数字平台上，内容的发布时间（时机和上下文）对消费者参与度的影响可能比内容本身的主题特征更大，挑战了以往关于内容是受欢迎度主要驱动因素的假设。", "motivation": "在数字服务经济中，理解受欢迎度的驱动因素至关重要，因为内容创作者为争夺消费者注意力而竞争。以往的研究主要强调内容特征的作用，但创作者往往错误判断受众的实际价值偏好。", "method": "本研究将Latent Dirichlet Allocation (LDA) 模型应用于TED演讲的大型语料库，将该平台视为创作者（演讲者）与消费者（观众）互动的数字服务案例。通过比较创作者的主题供给与观众参与度表达的需求，并进行纵向分析。", "result": "研究发现生产者供给与消费者偏好之间存在持续性不匹配。纵向分析进一步揭示，时间动态对消费者参与度的影响强于主题内容，表明内容交付的“时机”可能比“内容本身”更重要。", "conclusion": "这些发现挑战了内容特征是受欢迎度主要驱动因素的主导假设，并强调了时机和语境因素在塑造消费者反应中的重要性。研究结果为数字平台上的消费者注意力动态提供了新见解，并对营销人员、平台管理者和内容创作者优化受众参与策略具有实际意义。"}}
{"id": "2510.11574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11574", "abs": "https://arxiv.org/abs/2510.11574", "authors": ["Lennart Werner", "Pol Eyschen", "Sean Costello", "Pierluigi Micarelli", "Marco Hutter"], "title": "Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery", "comment": null, "summary": "Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision.", "AI": {"tldr": "本文提出了一种高精度、可改装的2D力与载荷估算算法，用于液压挖掘机末端执行器，无需额外操作要求，通过在线识别动态模型实现，并优于现有方法。", "motivation": "液压挖掘机末端执行器相互作用力的准确实时估计是重型机械高级自动化的关键，能改进精确平整和挖掘操作。", "method": "提出了一种高精度、可改装的2D力与载荷估算算法。该方法不要求操作员的轨迹、加速度或回转关节使用，只需最少校准，无需机器特定动态特性先验知识。通过正常操作中识别动态模型来估算末端执行器相互作用力和铲斗载荷，载荷估算基于优化方法，力测量基于压力和惯性测量。", "result": "在25吨标准挖掘机上，载荷估算达到1%的全量程精度。在线力测量方向精度为13度，大小精度为383牛。该方法在两种不同类型和重量级别的挖掘机平台上得到验证，其精度和泛化能力优于经典的准静态方法和商用系统。", "conclusion": "所提出的方法能够高精度、实时估算液压挖掘机的末端执行器相互作用力和铲斗载荷，具有良好的泛化能力和改装性，并且在精度和精确度方面优于现有解决方案，为重型机械的高级自动化奠定了基础。"}}
{"id": "2510.10422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10422", "abs": "https://arxiv.org/abs/2510.10422", "authors": ["Jyotirmay Nag Setu", "Kevin Desai", "John Quarles"], "title": "Towards Cybersickness Severity Classification from VR Gameplay Videos Using Transfer Learning and Temporal Modeling", "comment": null, "summary": "With the rapid advancement of virtual reality (VR) technology, its adoption\nacross domains such as healthcare, education, and entertainment has grown\nsignificantly. However, the persistent issue of cybersickness, marked by\nsymptoms resembling motion sickness, continues to hinder widespread acceptance\nof VR. While recent research has explored multimodal deep learning approaches\nleveraging data from integrated VR sensors like eye and head tracking, there\nremains limited investigation into the use of video-based features for\npredicting cybersickness. In this study, we address this gap by utilizing\ntransfer learning to extract high-level visual features from VR gameplay videos\nusing the InceptionV3 model pretrained on the ImageNet dataset. These features\nare then passed to a Long Short-Term Memory (LSTM) network to capture the\ntemporal dynamics of the VR experience and predict cybersickness severity over\ntime. Our approach effectively leverages the time-series nature of video data,\nachieving a 68.4% classification accuracy for cybersickness severity. This\nsurpasses the performance of existing models trained solely on video data,\nproviding a practical tool for VR developers to evaluate and mitigate\ncybersickness in virtual environments. Furthermore, this work lays the\nfoundation for future research on video-based temporal modeling for enhancing\nuser comfort in VR applications.", "AI": {"tldr": "本研究利用迁移学习和LSTM网络，通过VR游戏视频的视觉特征来预测网络晕动症的严重程度，并取得了68.4%的分类准确率。", "motivation": "虚拟现实（VR）技术的广泛应用受到网络晕动症的阻碍，而目前针对VR集成传感器数据（如眼球和头部追踪）的多模态深度学习研究较多，但利用视频特征预测网络晕动症的研究却很有限。", "method": "研究采用迁移学习方法，使用在ImageNet数据集上预训练的InceptionV3模型从VR游戏视频中提取高级视觉特征。随后，这些特征被输入到长短期记忆（LSTM）网络中，以捕捉VR体验的时间动态并预测网络晕动症的严重程度。", "result": "该方法有效利用了视频数据的时间序列特性，在网络晕动症严重程度分类上达到了68.4%的准确率。这一性能超越了现有仅基于视频数据训练的模型。", "conclusion": "本研究为VR开发者提供了一个评估和减轻虚拟环境中网络晕动症的实用工具，并为未来基于视频时间建模以提升VR应用用户舒适度的研究奠定了基础。"}}
{"id": "2510.11660", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11660", "abs": "https://arxiv.org/abs/2510.11660", "authors": ["Yi Yang", "Kefan Gu", "Yuqing Wen", "Hebei Li", "Yucheng Zhao", "Tiancai Wang", "Xudong Liu"], "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation", "comment": "8 pages, 6 figures, conference", "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.", "AI": {"tldr": "ManiAgent 是一种用于通用操作任务的智能体架构，通过智能体间通信进行感知、子任务分解和动作生成，解决了VLA模型在复杂推理和长周期任务中的局限性，实现了高成功率和高效数据收集。", "motivation": "现有的视觉-语言-动作（VLA）模型在复杂推理和长周期任务规划方面的性能受限于数据稀缺性和模型容量。", "method": "引入了ManiAgent，一个用于通用操作任务的智能体架构。该框架包含多个智能体，它们通过相互通信执行环境感知、子任务分解和动作生成，实现从任务描述和环境输入到机器人操作动作的端到端输出。", "result": "ManiAgent 在SimplerEnv基准测试中取得了86.8%的成功率，在真实世界的抓取放置任务中取得了95.8%的成功率。此外，它还能够高效地收集数据，训练出的VLA模型性能与使用人工标注数据集训练的模型相当。", "conclusion": "ManiAgent 通过其智能体架构和智能体间通信机制，有效提升了机器人在复杂操作场景中的性能，并解决了VLA模型在数据和模型容量方面的挑战，实现了高效的数据收集和卓越的任务执行能力。"}}
{"id": "2510.10475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10475", "abs": "https://arxiv.org/abs/2510.10475", "authors": ["A H M Rezaul Karim", "Ozlem Uzuner"], "title": "Assessing Large Language Models for Structured Medical Order Extraction", "comment": null, "summary": "Medical order extraction is essential for structuring actionable clinical\ninformation, supporting decision-making, and enabling downstream applications\nsuch as documentation and workflow automation. Orders may be embedded in\ndiverse sources, including electronic health records, discharge summaries, and\nmulti-turn doctor-patient dialogues, and can span categories such as\nmedications, laboratory tests, imaging studies, and follow-up actions. The\nMEDIQA-OE 2025 shared task focuses on extracting structured medical orders from\nextended conversational transcripts, requiring the identification of order\ntype, description, reason, and provenance. We present the MasonNLP submission,\nwhich ranked 5th among 17 participating teams with 105 total submissions. Our\napproach uses a general-purpose, instruction-tuned LLaMA-4 17B model without\ndomain-specific fine-tuning, guided by a single in-context example. This\nfew-shot configuration achieved an average F1 score of 37.76, with notable\nimprovements in reason and provenance accuracy. These results demonstrate that\nlarge, non-domain-specific LLMs, when paired with effective prompt engineering,\ncan serve as strong, scalable baselines for specialized clinical NLP tasks.", "AI": {"tldr": "MasonNLP团队在MEDIQA-OE 2025医学医嘱提取任务中，使用未经领域微调的LLaMA-4 17B模型结合少量样本提示工程，获得了第五名，并证明了通用大型语言模型在该任务中的潜力。", "motivation": "医学医嘱提取对于构建可操作的临床信息、支持决策制定以及实现文档自动化和工作流自动化等下游应用至关重要。医嘱存在于多种来源和类别中，MEDIQA-OE 2025任务旨在从扩展的对话记录中提取结构化医学医嘱。", "method": "MasonNLP团队采用了通用的、经过指令微调的LLaMA-4 17B模型。该方法未进行领域特定的微调，而是通过单个上下文示例（少量样本配置）进行引导，以提取医嘱类型、描述、原因和来源。", "result": "MasonNLP的提交在17个参与团队（共105份提交）中排名第5。该方法实现了37.76的平均F1分数，并在原因和来源的准确性方面有显著提升。", "conclusion": "研究结果表明，大型的、非领域特定的LLM，在与有效的提示工程相结合时，可以作为专业临床自然语言处理任务的强大且可扩展的基线模型。"}}
{"id": "2510.11661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11661", "abs": "https://arxiv.org/abs/2510.11661", "authors": ["Shijie Xia", "Yuhan Sun", "Pengfei Liu"], "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI", "comment": null, "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.", "AI": {"tldr": "SR-Scientist框架将大型语言模型（LLM）从简单的方程提出者提升为自主AI科学家，通过代码解释器进行数据分析、方程实现、评估和优化，在科学方程发现任务中显著优于基线方法。", "motivation": "目前LLM在科学方程发现中的应用通常将其限制为搜索算法（如遗传编程）中的方程提出者角色，未能充分利用其潜力。", "method": "SR-Scientist框架将代码解释器封装成一套用于数据分析和方程评估的工具。LLM被指示利用这些工具进行长期优化，以最少的人工定义流程自主改进方程。此外，还开发了一个端到端的强化学习框架来增强智能体的能力。", "result": "SR-Scientist在涵盖四个科学领域的数据集上，性能比基线方法高出6%到35%。此外，该方法对噪声具有鲁棒性，发现的方程能泛化到域外数据，并具有符号准确性。", "conclusion": "SR-Scientist通过将LLM提升为能够自主分析数据、实现和优化方程的AI科学家，显著改进了科学方程发现过程，实现了卓越的性能、鲁棒性和泛化能力。"}}
{"id": "2510.11608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11608", "abs": "https://arxiv.org/abs/2510.11608", "authors": ["Shiqi Zhang", "Xinbei Ma", "Yunqing Xu", "Zouying Cao", "Pengrui Lu", "Haobo Yuan", "Tiancheng Shen", "Zhuosheng Zhang", "Hai Zhao", "Ming-Hsuan Yang"], "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.", "AI": {"tldr": "ParaCook是一个新的基准，用于评估大型语言模型在多智能体协作规划中时间效率的能力，发现现有LLM在此方面表现不佳，但在抽象任务中具有潜力。", "motivation": "现有的大型语言模型（LLM）智能体基准主要关注任务完成度，而忽略了并行和异步操作中的时间效率，尽管LLM在长期、真实世界任务规划中展现出强大的推理能力。", "method": "提出了ParaCook基准，用于时间高效的协作规划。ParaCook受Overcooked游戏启发，提供了一个多智能体烹饪任务环境，具有简化的动作空间，以隔离策略性并行规划的核心挑战。", "result": "对最先进的LLM进行了全面评估，发现当前方法生成的计划次优，难以处理并行操作或协调。分析还揭示了LLM在抽象任务中专注于高层并行优化的潜力。", "conclusion": "ParaCook提供了一个可扩展且复杂性可调的评估框架，为开发和评估关注时间效率的多智能体规划奠定了基础。"}}
{"id": "2510.10481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10481", "abs": "https://arxiv.org/abs/2510.10481", "authors": ["Guangxin He", "Shen Nie", "Fengqi Zhu", "Yuankang Zhao", "Tianyi Bai", "Ran Yan", "Jie Fu", "Chongxuan Li", "Binhang Yuan"], "title": "UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models", "comment": null, "summary": "Diffusion LLMs have attracted growing interest, with plenty of recent work\nemphasizing their great potential in various downstream tasks; yet the\nlong-context behavior of diffusion LLMs remains largely uncharted. We present a\ncase study of post-training techniques for extending the context window of\ndiffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a\nsimple modification to the standard Rotary Positional Embeddings (RoPE)\nextension effectively accommodates the probabilistic modeling inherent in the\ndiffusion process, enabling stable scaling to longer context ranges. We further\ncompare masking strategies used during post-training and analyze their impact\non optimization stability and long-range recall. Instantiating these insights,\nwe introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,\nin our empirical evaluation on long-context tasks, significantly outperforms\ntraining-free baselines. Our experimental results highlight the special\npositional extension as a key lever for scaling diffusion LLMs to extended\ncontexts and offer practical guidance for practitioners seeking 128K-scale\ncontext via efficient post-training.", "AI": {"tldr": "本文通过后训练技术，成功将扩散LLM（LLaDA）的上下文窗口扩展到128K，并提出了UltraLLaDA，通过对RoPE的修改和掩码策略的分析，实现了长上下文稳定扩展。", "motivation": "扩散LLM在各种下游任务中展现出巨大潜力，但其长上下文行为仍未被充分探索，这促使研究人员探索如何在不从头训练的情况下扩展其上下文窗口。", "method": "研究采用了一种后训练技术案例研究，专注于扩展扩散LLM（LLaDA）的上下文窗口。核心方法包括：对标准旋转位置嵌入（RoPE）进行简单修改以适应扩散过程的概率建模；比较后训练期间使用的掩码策略及其对优化稳定性和长程召回的影响。", "result": "通过对RoPE的特殊修改，实现了扩散LLM向更长上下文范围的稳定扩展。研究引入了UltraLLaDA，一个拥有128K token上下文窗口的扩散LLM，在长上下文任务上的实证评估中显著优于无需训练的基线。实验结果强调了特殊位置扩展是扩展扩散LLM上下文的关键杠杆。", "conclusion": "后训练技术可以有效扩展扩散LLM的上下文窗口，其中对RoPE的特定修改对于实现长上下文（如128K规模）至关重要。本文为寻求通过高效后训练实现128K规模上下文的实践者提供了实用指导。"}}
{"id": "2510.10426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10426", "abs": "https://arxiv.org/abs/2510.10426", "authors": ["Suyang Xi", "Chenxi Yang", "Hong Ding", "Yiqing Ni", "Catherine C. Liu", "Yunhao Liu", "Chengqi Zhang"], "title": "Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs", "comment": "12 pages, 5 figures", "summary": "Multimodal large language models (MLLMs) often fail in fine-grained visual\nquestion answering, producing hallucinations about object identities,\npositions, and relations because textual queries are not explicitly anchored to\nvisual referents. Retrieval-augmented generation (RAG) alleviates some errors,\nbut it fails to align with human-like processing at both the retrieval and\naugmentation levels. Specifically, it focuses only on global-level image\ninformation but lacks local detail and limits reasoning about fine-grained\ninteractions. To overcome this limitation, we present Human-Like\nRetrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal\nreasoning as a ``what--where--reweight'' cascade. Queries are first anchored to\ncandidate referents via open-vocabulary detection (what), then spatially\nresolved with SAM-derived masks to recover fine-grained precision (where), and\nadaptively prioritized through the trade-off between local and global alignment\n(reweight). Mask-guided fine-tuning further injects spatial evidence into the\ngeneration process, transforming grounding from a passive bias into an explicit\nconstraint on answer formulation. Extensive experiments demonstrate that this\nhuman-like cascade improves grounding fidelity and factual consistency while\nreducing hallucinations, advancing multimodal question answering toward\ntrustworthy reasoning.", "AI": {"tldr": "本文提出HuLiRAG框架，通过“识别-定位-重权”级联和掩码引导微调，改进多模态大语言模型在细粒度视觉问答中的幻觉问题，提升接地性和事实一致性。", "motivation": "多模态大语言模型（MLLMs）在细粒度视觉问答中常因文本查询未明确锚定视觉指代物而产生幻觉（如物体身份、位置、关系错误）。现有检索增强生成（RAG）方法未能像人类一样处理信息，仅关注全局图像信息，缺乏局部细节和细粒度交互推理能力。", "method": "本文提出了类人检索增强生成（HuLiRAG）框架，其核心是“识别-定位-重权”（what-where-reweight）级联推理：\n1. **识别 (what)**：通过开放词汇检测将查询锚定到候选指代物。\n2. **定位 (where)**：利用SAM衍生的掩码空间解析指代物，恢复细粒度精度。\n3. **重权 (reweight)**：通过权衡局部和全局对齐，自适应地进行优先级排序。\n此外，还引入了掩码引导的微调，将空间证据注入生成过程，使接地成为答案形成的一个明确约束。", "result": "广泛实验表明，这种类人级联方法显著提高了接地保真度和事实一致性，同时减少了幻觉，推动了多模态问答向更可信赖的推理发展。", "conclusion": "HuLiRAG框架通过模拟人类的视觉推理过程，有效解决了多模态大语言模型在细粒度视觉问答中的幻觉问题，显著提升了模型的接地能力和生成答案的可靠性。"}}
{"id": "2510.11694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11694", "abs": "https://arxiv.org/abs/2510.11694", "authors": ["Arjun Sahney", "Ram Gorthi", "Cezary Łastowski", "Javier Vega"], "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering", "comment": "8 pages. No figures. Evaluated on MLE-Benchmark 2025", "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.", "AI": {"tldr": "Operand Quant是一种单智能体、基于IDE的自主机器学习工程（MLE）架构，在MLE-Benchmark上取得了新的最先进（SOTA）成果，性能优于多智能体系统。", "motivation": "传统的MLE自动化通常采用多智能体编排框架。本研究的动机是探索一种将所有MLE生命周期阶段整合到单个、上下文感知智能体中的替代方法，以期在自主MLE任务中超越现有系统。", "method": "Operand Quant采用单智能体、基于IDE的架构。它将探索、建模、实验和部署等所有MLE生命周期阶段整合到一个单一的、上下文感知的智能体中。该智能体以线性、非阻塞的方式在受控的IDE环境中自主运行。", "result": "在MLE-Benchmark（2025）上，Operand Quant在75个问题中获得了0.3956 +/- 0.0565的整体奖牌率，达到了新的最先进（SOTA）水平，是迄今为止所有评估系统中记录的最高性能。", "conclusion": "研究表明，在受控IDE环境中自主运行的线性、非阻塞单智能体架构，在相同的约束条件下，可以超越多智能体和编排系统在自主机器学习工程中的表现。"}}
{"id": "2510.11689", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11689", "abs": "https://arxiv.org/abs/2510.11689", "authors": ["Maggie Wang", "Stephen Tian", "Aiden Swann", "Ola Shorinwa", "Jiajun Wu", "Mac Schwager"], "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation", "comment": null, "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .", "AI": {"tldr": "Phys2Real提出了一种从真实世界到模拟再到真实世界的强化学习流程，它结合了视觉-语言模型推断的物理参数和通过不确定性感知融合进行的交互式适应，显著提高了机器人操作任务的模拟到真实迁移成功率。", "motivation": "在真实世界中学习机器人操作策略成本高昂且耗时。尽管在模拟中训练强化学习策略提供了一种可扩展的替代方案，但有效的模拟到真实世界迁移仍然具有挑战性，特别是对于需要精确动力学的任务。", "method": "Phys2Real是一个从真实世界到模拟再到真实世界的强化学习流程，包含三个核心组件：1) 使用3D高斯泼溅进行高保真几何重建；2) 视觉-语言模型推断的物理参数先验分布；3) 从交互数据中进行的在线物理参数估计。该方法将策略建立在可解释的物理参数上，通过基于集成的不确定性量化，利用在线估计细化视觉-语言模型的预测。", "result": "在具有不同质心（CoM）的T形块平面推动任务和具有偏心质量分布的锤子推动任务中，Phys2Real相较于域随机化基线取得了显著改进：底部加权T形块的成功率从79%提高到100%；挑战性较大的顶部加权T形块的成功率从23%提高到57%；锤子推动任务的平均任务完成时间加快了15%。消融研究表明，视觉-语言模型和交互信息的结合对于成功至关重要。", "conclusion": "通过结合视觉-语言模型推断的物理参数先验和在线交互式适应，Phys2Real有效解决了需要精确动力学的机器人操作任务中的模拟到真实世界迁移挑战，显著提升了任务成功率和效率。"}}
{"id": "2510.10490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10490", "abs": "https://arxiv.org/abs/2510.10490", "authors": ["Prawaal Sharma", "Poonam Goyal", "Vidisha Sharma", "Navneet Goyal"], "title": "VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction", "comment": "9 Pages, Plus Appendices, EACL 2024", "summary": "UNESCO has classified 2500 out of 7000 languages spoken worldwide as\nendangered. Attrition of a language leads to loss of traditional wisdom, folk\nliterature, and the essence of the community that uses it. It is therefore\nimperative to bring digital inclusion to these languages and avoid its\nextinction. Low resource languages are at a greater risk of extinction. Lack of\nunsupervised Optical Character Recognition(OCR) methodologies for low resource\nlanguages is one of the reasons impeding their digital inclusion. We propose\nVOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph\nfeature recommendation for cluster-based labelling. We augment the labelled\ndata for diversity and volume using image transformations and Generative\nAdversarial Networks. Voltage has been designed using Takri - a family of\nscripts used in 16th to 20th century in the Himalayan regions of India. We\npresent results for Takri along with other Indic scripts (both low and high\nresource) to substantiate the universal behavior of the methodology. An\naccuracy of 95% for machine printed and 87% for handwritten samples on Takri\nscript has been achieved. We conduct baseline and ablation studies along with\nbuilding downstream use cases for Takri, demonstrating the usefulness of our\nwork.", "AI": {"tldr": "本文提出了一种名为VOLTAGE的基于对比学习的OCR方法，旨在为低资源濒危语言（如Takri）提供数字包容性，通过自动字形特征推荐和数据增强实现高准确率。", "motivation": "全球有2500种语言濒临灭绝，导致传统智慧和文化流失。低资源语言面临更大的灭绝风险，缺乏无监督OCR方法是阻碍其数字化的主要原因。", "method": "该研究提出了VOLTAGE方法，这是一种基于对比学习的OCR，利用自动字形特征推荐进行基于聚类的标注。通过图像变换和生成对抗网络（GANs）对标注数据进行多样性和数量上的增强。该方法以Takri文字为基础设计，并扩展到其他印度文字。", "result": "VOLTAGE方法在Takri文字上取得了显著成果：机器印刷样本的准确率达到95%，手写样本的准确率为87%。研究还通过与其他印度文字（包括低资源和高资源）的实验，证实了该方法的普适性。", "conclusion": "VOLTAGE是一种有效的OCR方法，能够为低资源濒危语言提供数字包容性，并在Takri文字上展现出高准确率。该工作通过基线和消融研究以及下游用例的构建，证明了其实用性。"}}
{"id": "2510.10933", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10933", "abs": "https://arxiv.org/abs/2510.10933", "authors": ["Jiahong Chen", "Jinghao Wang", "Zi Wang", "Ziwen Wang", "Banglei Guan", "Qifeng Yu"], "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects", "comment": "12 pages, 9 figures, submitted to ICRA 2026", "summary": "6D pose estimation of textureless objects is valuable for industrial robotic\napplications, yet remains challenging due to the frequent loss of depth\ninformation. Current multi-view methods either rely on depth data or\ninsufficiently exploit multi-view geometric cues, limiting their performance.\nIn this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level\nfusion using only multi-view RGB images as input. We design a three-stage\nprogressive pose optimization strategy that leverages dense multi-view keypoint\ngeometry information. To enable effective dense keypoint fusion, we enhance the\nkeypoint network with attentional aggregation and symmetry-aware training,\nimproving prediction accuracy and resolving ambiguities on symmetric objects.\nExtensive experiments on the ROBI dataset demonstrate that DKPMV outperforms\nstate-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods\nin the majority of cases. The code will be available soon.", "AI": {"tldr": "本文提出了DKPMV，一个仅使用多视角RGB图像进行无纹理物体6D姿态估计的管道。它通过密集关键点级融合和三阶段渐进式姿态优化策略，在ROBI数据集上超越了现有先进的多视角RGB甚至多数RGB-D方法。", "motivation": "无纹理物体6D姿态估计对工业机器人应用很有价值，但由于深度信息频繁丢失而充满挑战。当前多视角方法要么依赖深度数据，要么未能充分利用多视角几何线索，限制了其性能。", "method": "本文提出了DKPMV管道，仅以多视角RGB图像作为输入，实现密集关键点级融合。设计了三阶段渐进式姿态优化策略，利用密集的关键点几何信息。为实现有效的密集关键点融合，通过注意力聚合和对称性感知训练增强了关键点网络，以提高预测精度并解决对称物体上的歧义。", "result": "在ROBI数据集上的大量实验表明，DKPMV优于现有最先进的多视角RGB方法，并且在大多数情况下甚至超越了RGB-D方法。", "conclusion": "DKPMV提供了一种仅利用多视角RGB图像对无纹理物体进行6D姿态估计的有效解决方案，通过其创新的关键点融合和姿态优化策略，显著提升了性能，甚至超越了依赖深度信息的方法。"}}
{"id": "2510.11340", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11340", "abs": "https://arxiv.org/abs/2510.11340", "authors": ["Zhao Huang", "Boyang Sun", "Alexandros Delitzas", "Jiaqi Chen", "Marc Pollefeys"], "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes", "comment": "8 pages", "summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet\nexisting datasets remain limited due to the labor-intensive process of\nannotating part segmentation, kinematic types, and motion trajectories. We\npresent REACT3D, a scalable zero-shot framework that converts static 3D scenes\ninto simulation-ready interactive replicas with consistent geometry, enabling\ndirect use in diverse downstream tasks. Our contributions include: (i)\nopenable-object detection and segmentation to extract candidate movable parts\nfrom static scenes, (ii) articulation estimation that infers joint types and\nmotion parameters, (iii) hidden-geometry completion followed by interactive\nobject assembly, and (iv) interactive scene integration in widely supported\nformats to ensure compatibility with standard simulation platforms. We achieve\nstate-of-the-art performance on detection/segmentation and articulation metrics\nacross diverse indoor scenes, demonstrating the effectiveness of our framework\nand providing a practical foundation for scalable interactive scene generation,\nthereby lowering the barrier to large-scale research on articulated scene\nunderstanding. Our project page is\n\\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}.", "AI": {"tldr": "REACT3D是一个可扩展的零样本框架，能将静态3D场景转换为可用于仿真的交互式副本，解决了现有数据集标注耗时的问题。", "motivation": "具身智能对交互式3D场景的需求日益增长，但现有数据集因部分分割、运动类型和轨迹标注耗时而受限。", "method": "REACT3D框架包括四个主要贡献：(i) 可开启物体检测与分割以提取可移动部件，(ii) 关节估计以推断关节类型和运动参数，(iii) 隐藏几何体补全和交互式物体组装，以及 (iv) 将交互式场景整合到广泛支持的格式中。", "result": "在多种室内场景的检测/分割和关节指标上实现了最先进的性能，证明了该框架的有效性。", "conclusion": "REACT3D为可扩展的交互式场景生成提供了实用基础，降低了大规模关节场景理解研究的门槛。"}}
{"id": "2510.10434", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10434", "abs": "https://arxiv.org/abs/2510.10434", "authors": ["Kangjian Zhu", "Haobo Jiang", "Yigong Zhang", "Jianjun Qian", "Jian Yang", "Jin Xie"], "title": "MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation", "comment": null, "summary": "We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that\nformulates markerless, image-based robot pose estimation as a conditional\ndenoising diffusion process. The framework consists of two processes: a\nvisibility-constrained diffusion process for diverse pose augmentation and a\ntimestep-aware reverse process for progressive pose refinement. The diffusion\nprocess progressively perturbs ground-truth poses to noisy transformations for\ntraining a pose denoising network. Importantly, we integrate visibility\nconstraints into the process, ensuring the transformations remain within the\ncamera field of view. Compared to the fixed-scale perturbations used in current\nmethods, the diffusion process generates in-view and diverse training poses,\nthereby improving the network generalization capability. Furthermore, the\nreverse process iteratively predicts the poses by the denoising network and\nrefines pose estimates by sampling from the diffusion posterior of current\ntimestep, following a scheduled coarse-to-fine procedure. Moreover, the\ntimestep indicates the transformation scales, which guide the denoising network\nto achieve more accurate pose predictions. The reverse process demonstrates\nhigher robustness than direct prediction, benefiting from its timestep-aware\nrefinement scheme. Our approach demonstrates improvements across two benchmarks\n(DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most\nchallenging dataset, representing a 32.3% gain over the state-of-the-art.", "AI": {"tldr": "MonoSE(3)-Diffusion是一种单目SE(3)扩散框架，通过可见性约束的扩散过程生成多样化姿态增强，并利用时间步感知的逆向过程进行渐进式姿态细化，从而实现无标记、基于图像的机器人姿态估计。", "motivation": "当前无标记、基于图像的机器人姿态估计方法使用固定尺度的扰动，导致泛化能力受限。研究旨在通过生成更具多样性和视野内的训练姿态，并采用更鲁棒的细化策略来提高姿态估计的泛化能力和准确性。", "method": "该框架包含两个核心过程：1. 可见性约束的扩散过程：将真实姿态逐步扰动为噪声变换，用于训练姿态去噪网络，并集成可见性约束以确保变换保持在相机视野内，从而生成多样且视野内的训练姿态。2. 时间步感知的逆向过程：通过去噪网络迭代预测并细化姿态估计，从当前时间步的扩散后验中采样，遵循从粗到细的程序，其中时间步指示变换尺度以指导更精确的姿态预测。", "result": "该方法在DREAM和RoboKeyGen两个基准测试中均有改进，在最具挑战性的数据集上取得了66.75的AUC，比现有最先进技术提高了32.3%。逆向过程由于其时间步感知的细化方案，比直接预测表现出更高的鲁棒性。", "conclusion": "MonoSE(3)-Diffusion通过结合可见性约束的扩散过程和时间步感知的逆向细化过程，显著提升了无标记机器人姿态估计的泛化能力、准确性和鲁棒性，超越了现有技术水平。"}}
{"id": "2510.10456", "categories": ["cs.CV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10456", "abs": "https://arxiv.org/abs/2510.10456", "authors": ["Tai Le-Gia", "Ahn Jaehyun"], "title": "On the Problem of Consistent Anomalies in Zero-Shot Industrial Anomaly Detection", "comment": "Published in TMLR (10/2025)", "summary": "Zero-shot image anomaly classification (AC) and segmentation (AS) are vital\nfor industrial quality control, detecting defects without prior training data.\nExisting representation-based methods compare patch features with nearest\nneighbors in unlabeled test images but struggle with consistent anomalies --\nsimilar defects recurring across multiple images -- resulting in poor AC/AS\nperformance. We introduce Consistent-Anomaly Detection Graph (CoDeGraph), a\nnovel algorithm that identifies and filters consistent anomalies from\nsimilarity computations. Our key insight is that normal patches in industrial\nimages show stable, gradually increasing similarity to other test images, while\nconsistent-anomaly patches exhibit abrupt similarity spikes after exhausting a\nlimited set of similar matches, a phenomenon we term ``neighbor-burnout.''\nCoDeGraph constructs an image-level graph, with images as nodes and edges\nconnecting those with shared consistent-anomaly patterns, using community\ndetection to filter these anomalies. We provide a theoretical foundation using\nExtreme Value Theory to explain the effectiveness of our approach. Experiments\non MVTec AD with the ViT-L-14-336 backbone achieve 98.3% AUROC for AC and AS\nperformance of 66.8% (+4.2%) F1 and 68.1% (+5.4%) AP over state-of-the-art\nzero-shot methods. Using the DINOv2 backbone further improves segmentation,\nyielding 69.1% (+6.5%) F1 and 71.9% (+9.2%) AP, demonstrating robustness across\narchitectures.", "AI": {"tldr": "本文提出CoDeGraph算法，通过识别并过滤零样本图像异常分类与分割中常见的“一致性异常”（重复缺陷），显著提升了工业质量控制的性能和鲁棒性。", "motivation": "现有的基于表示的零样本异常检测方法在处理工业图像中反复出现且相似的“一致性异常”时表现不佳，导致异常分类和分割性能受限。", "method": "CoDeGraph算法利用“邻居耗尽”（neighbor-burnout）现象来区分正常补丁和一致性异常补丁：正常补丁相似度稳定增长，而一致性异常补丁在耗尽有限相似匹配后会表现出突然的相似度峰值。该方法构建一个图像级图，通过社区检测来识别和过滤共享一致性异常模式的图像，并提供了极值理论的理论基础。", "result": "在MVTec AD数据集上，使用ViT-L-14-336骨干网络，CoDeGraph在异常分类（AC）中实现了98.3%的AUROC，在异常分割（AS）中F1值达到66.8%（提升4.2%），AP达到68.1%（提升5.4%），超越了现有最先进的零样本方法。使用DINOv2骨干网络进一步将分割F1值提升至69.1%（提升6.5%），AP提升至71.9%（提升9.2%），证明了其跨架构的鲁棒性。", "conclusion": "CoDeGraph通过有效识别和过滤一致性异常，显著提升了零样本图像异常分类和分割的性能，并在不同网络架构下表现出强大的鲁棒性，为工业质量控制提供了重要的解决方案。"}}
{"id": "2510.10528", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10528", "abs": "https://arxiv.org/abs/2510.10528", "authors": ["Heming Xia", "Cunxiao Du", "Rui Li", "Chak Tou Leong", "Yongqi Li", "Wenjie Li"], "title": "Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting", "comment": null, "summary": "Large reasoning models (LRMs) have demonstrated remarkable proficiency in\ntackling complex reasoning tasks through step-by-step thinking. However, such a\nlengthy reasoning process incurs substantial computational and latency\noverheads, hindering the practical deployment of these models. In this work, we\npresent a new perspective on mitigating overthinking in LRMs via black-box\nadversarial prompting. By treating both open-source LRMs and closed-source APIs\nas black-box communicators, we investigate how to elicit concise responses\nwithout sacrificing accuracy. We introduce AdvPrompt, an iterative refinement\nframework that generates high-quality adversarial prompts from diverse\nperspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt\nconsistently reduces token usage while preserving performance. Notably,\nAdvPrompt achieves a 3x reduction in average response length on simple GSM8K\nquestions for the Qwen3 model series, and delivers an average ~40% token\nreduction across four benchmarks. For closed-source APIs, AdvPrompt reduces\ntoken usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further\nanalysis reveals the generalizability of AdvPrompt across various model scales\nand families, underscoring the potential of black-box prompting as a practical\nand effective strategy for enhancing LRM efficiency.", "AI": {"tldr": "本文提出AdvPrompt，一种黑盒对抗性提示框架，旨在通过迭代优化生成高质量提示，以减少大型推理模型（LRMs）的冗余思考，从而在不牺牲准确性的前提下显著降低计算成本和延迟。", "motivation": "大型推理模型（LRMs）通过逐步思考解决复杂任务时，其冗长的推理过程会产生巨大的计算和延迟开销，阻碍了这些模型的实际部署。研究动机在于缓解LRMs的“过度思考”问题。", "method": "该研究引入AdvPrompt，一个迭代优化框架，从不同角度生成高质量的对抗性提示。它将开源LRMs和闭源API视为黑盒通信器，旨在引出简洁的响应，同时不牺牲准确性。", "result": "AdvPrompt在多个基准测试中一致地减少了token使用量，同时保持了性能。具体而言，对于Qwen3模型系列，在简单的GSM8K问题上平均响应长度减少了3倍；在四个基准测试中，平均token减少了约40%。对于闭源API，在MATH-500上，AdvPrompt使Claude-3.7的token使用量减少了35%，Gemini-2.5减少了47%。此外，AdvPrompt在各种模型规模和系列中都表现出良好的泛化性。", "conclusion": "黑盒对抗性提示（AdvPrompt）是一种实用且有效的策略，能够通过减少过度思考来提高大型推理模型的效率，同时不影响其性能，这对于LRM的实际部署具有重要意义。"}}
{"id": "2510.10462", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10462", "abs": "https://arxiv.org/abs/2510.10462", "authors": ["Chen Zhong", "Yuxuan Yang", "Xinyue Zhang", "Ruohan Ma", "Yong Guo", "Gang Li", "Jupeng Li"], "title": "Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation annotation suffers from inter-rater variability\n(IRV) due to differences in annotators' expertise and the inherent blurriness\nof medical images. Standard approaches that simply average expert labels are\nflawed, as they discard the valuable clinical uncertainty revealed in\ndisagreements. We introduce a fundamentally new approach with our group\ndecision simulation framework, which works by mimicking the collaborative\ndecision-making process of a clinical panel. Under this framework, an Expert\nSignature Generator (ESG) learns to represent individual annotator styles in a\nunique latent space. A Simulated Consultation Module (SCM) then intelligently\ngenerates the final segmentation by sampling from this space. This method\nachieved state-of-the-art results on challenging CBCT and MRI datasets (92.11%\nand 90.72% Dice scores). By treating expert disagreement as a useful signal\ninstead of noise, our work provides a clear path toward more robust and\ntrustworthy AI systems for healthcare.", "AI": {"tldr": "针对医学图像分割标注中的专家间变异性，本文提出了一种新的群体决策模拟框架，通过学习个体标注风格并模拟专家协商过程，将分歧视为有用信号，从而生成更鲁棒的分割结果。", "motivation": "医学图像分割标注存在专家间变异性（IRV），原因在于标注者专业知识差异和图像固有的模糊性。传统的简单平均专家标签的方法存在缺陷，因为它丢弃了分歧中揭示的有价值的临床不确定性。", "method": "引入了一个群体决策模拟框架，该框架模仿临床专家小组的协作决策过程。其中包含：1) 专家签名生成器（ESG），用于在独特的潜在空间中学习表示个体标注者的风格；2) 模拟咨询模块（SCM），通过从该空间采样智能地生成最终分割。", "result": "该方法在具有挑战性的CBCT和MRI数据集上取得了最先进的结果，Dice分数分别达到92.11%和90.72%。", "conclusion": "通过将专家分歧视为有用的信号而非噪声，本研究为构建更鲁棒、更值得信赖的医疗AI系统提供了清晰的途径。"}}
{"id": "2510.05577", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05577", "abs": "https://arxiv.org/abs/2510.05577", "authors": ["Dong Yan", "Gaochen Wu", "Bowen Zhou"], "title": "Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs", "comment": null, "summary": "Recent advancements in language agents have led to significant improvements\nin multi-hop reasoning tasks. However, existing approaches often struggle with\nhandling open-domain problems, which require massive information retrieval due\nto their reliance on a fixed sequence of actions. To address this, we propose\nFeedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework\ntailored to enhance reasoning in LLMs by utilizing dynamic and adaptive\nstrategies for information exploration in open-domain multi-hop reasoning\ntasks. Our approach begins by identifying key entities relevant to the problem,\nwhich serve as the initial nodes in the reasoning process. From these initial\nnodes, we then generate reasoning child nodes with the process being refined\nthrough a combination of historical error analysis and real-time feedback,\nwhich allows the framework to dynamically adjust and optimize its reasoning\nstrategies. By integrating depth-first search with an innovative node\ngeneration technique, our framework adapts based on both prior error paths and\nconcurrently generated nodes at the same hierarchical level. This dynamic\nstrategy effectively expands the search space while ensuring the reasoning\nprocess systematically converges toward accurate solutions. Experimental\nresults show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset\nand 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and\n7.25% respectively, highlighting its versatility and potential to enhance\nlanguage agents in multi-hop reasoning tasks.", "AI": {"tldr": "现有语言智能体在开放域多跳推理中因固定行动序列而受限。本文提出FGDIP框架，通过动态自适应的信息探索策略，结合历史错误分析和实时反馈，显著提升了LLM的推理能力，在HotpotQA和StrategyQA数据集上取得了SOTA性能。", "motivation": "现有语言智能体在处理开放域多跳推理问题时，由于依赖固定的行动序列，需要大量信息检索，导致性能不佳。", "method": "本文提出了反馈引导的动态交互规划（FGDIP）框架。该方法首先识别关键实体作为初始节点，然后生成推理子节点。通过结合历史错误分析和实时反馈，动态调整和优化推理策略。FGDIP将深度优先搜索与创新的节点生成技术相结合，根据先前的错误路径和同时生成的同级节点进行自适应调整，从而有效地扩展搜索空间并确保推理过程收敛到准确的解决方案。", "result": "FGDIP在HotpotQA数据集上取得了54.47%的F1分数，在StrategyQA数据集上取得了70.05%的F1分数，分别超越了最佳基线5.03%和7.25%。", "conclusion": "FGDIP的动态自适应策略显著增强了语言智能体在多跳推理任务中的能力，展现了其在处理开放域问题方面的通用性和巨大潜力。"}}
{"id": "2510.10560", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10560", "abs": "https://arxiv.org/abs/2510.10560", "authors": ["Euhid Aman", "Esteban Carlin", "Hsing-Kuo Pao", "Giovanni Beltrame", "Ghaluh Indah Permata Sari", "Yie-Tarng Chen"], "title": "BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices", "comment": "6 pages, BabyLM Workshop, EMNLP 2025", "summary": "Cross-attention transformers and other multimodal vision-language models\nexcel at grounding and generation; however, their extensive, full-precision\nbackbones make it challenging to deploy them on edge devices. Memory-augmented\narchitectures enhance the utilization of past context; however, most works\nrarely pair them with aggressive edge-oriented quantization. We introduce\nBitMar, a quantized multimodal transformer that proposes an external human-like\nepisodic memory for effective image-text generation on hardware with limited\nresources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and\none for vision (DiNOv2-based), to create compact embeddings that are combined\nand used to query a fixed-size key-value episodic memory. During vector\nretrieval, the BitNet decoder applies per-layer conditioning, which increases\nthe contextual relevance of generated content. The decoder also employs\nattention sinks with a sliding-window mechanism to process long or streaming\ninputs under tight memory budgets. The combination of per-layer conditioning\nand sliding-window attention achieves a strong quality-speed trade-off,\ndelivering competitive captioning and multimodal understanding at low latency\nwith a small model footprint. These characteristics make BitMar well-suited for\nedge deployment.", "AI": {"tldr": "BitMar是一种量化多模态Transformer，它结合了类人情景记忆，通过1.58比特编码器和高效解码器，在资源受限的边缘设备上实现有效的图像-文本生成。", "motivation": "现有的跨注意力Transformer和多模态视觉-语言模型虽然在接地和生成方面表现出色，但其庞大的全精度骨干网络难以部署到边缘设备上。虽然记忆增强架构能提升上下文利用率，但它们很少与激进的边缘量化技术结合使用。", "method": "BitMar引入了一个外部的类人情景记忆。它使用1.58比特的文本编码器（BitNet风格）和视觉编码器（DiNOv2-based）来创建紧凑的嵌入，这些嵌入被组合并用于查询固定大小的键值情景记忆。在向量检索过程中，BitNet解码器应用逐层条件作用以增加生成内容的上下文相关性。解码器还采用带有滑动窗口机制的注意力槽（attention sinks），在严格的内存预算下处理长或流式输入。", "result": "通过逐层条件作用和滑动窗口注意力机制的结合，BitMar实现了强大的质量-速度权衡，以低延迟和小的模型占用空间提供了具有竞争力的图像字幕和多模态理解能力。", "conclusion": "BitMar的特性使其非常适合边缘部署，因为它能在资源有限的硬件上提供高效且高质量的图像-文本生成和多模态理解能力。"}}
{"id": "2510.10613", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10613", "abs": "https://arxiv.org/abs/2510.10613", "authors": ["Di Wu abd Shuaidong Pan"], "title": "Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models", "comment": null, "summary": "This paper proposes a modeling framework for dynamic topic evolution based on\ntemporal large language models. The method first uses a large language model to\nobtain contextual embeddings of text and then introduces a temporal decay\nfunction and an attention mechanism. These components allow the model to adjust\nthe importance of semantic units according to time intervals and capture topic\nvariations across different periods. The temporal representations are then\nmapped into a latent topic space, where a state transition matrix is applied to\ndescribe the dynamic evolution of topics. A joint optimization objective\nconstrains both semantic modeling and temporal consistency, ensuring diversity\nand smoothness in topic generation. The design emphasizes the unified modeling\nof semantic representation and temporal evolution, which improves topic\ncoherence and diversity while enhancing stability and interpretability over\ntime. Experiments on real-world corpora show that the framework effectively\ncaptures the generation, expansion, and decline of topics and outperforms\nexisting models across multiple metrics. Overall, the proposed method provides\na systematic solution for understanding dynamic semantic patterns in\nlarge-scale text, enriches the research paradigm of topic modeling, and\nsupports complex text analysis tasks in multiple domains.", "AI": {"tldr": "本文提出了一种基于时间大语言模型的动态主题演化建模框架，通过结合上下文嵌入、时间衰减、注意力机制和状态转移矩阵，实现了语义表示和时间演化的统一建模，有效捕捉主题的动态变化。", "motivation": "研究动机在于理解大规模文本中的动态语义模式，并为现有主题模型提供一个更系统、更具解释性的解决方案，以统一建模语义表示和时间演化，从而提高主题连贯性、多样性、稳定性和可解释性。", "method": "该方法首先利用大语言模型获取文本的上下文嵌入，然后引入时间衰减函数和注意力机制来调整语义单元的重要性并捕捉跨时期的主题变化。接着，时间表示被映射到潜在主题空间，并应用状态转移矩阵描述主题的动态演化。最后，通过联合优化目标约束语义建模和时间一致性，确保主题生成的多样性和平滑性。", "result": "实验结果表明，该框架能有效捕捉主题的生成、扩展和衰退，并在多个指标上优于现有模型。它显著提高了主题的连贯性、多样性，同时增强了模型随时间变化的稳定性和可解释性。", "conclusion": "该方法为理解大规模文本中的动态语义模式提供了一个系统的解决方案，丰富了主题建模的研究范式，并支持多领域复杂的文本分析任务。"}}
{"id": "2510.10464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10464", "abs": "https://arxiv.org/abs/2510.10464", "authors": ["Junhao Dong", "Dejia Liu", "Ruiqi Ding", "Zongxing Chen", "Yingjie Huang", "Zhu Meng", "Jianbo Zhao", "Zhicheng Zhao", "Fei Su"], "title": "Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment", "comment": "81 pages, 13 figures", "summary": "Transjugular intrahepatic portosystemic shunt (TIPS) is an established\nprocedure for portal hypertension, but provides variable survival outcomes and\nfrequent overt hepatic encephalopathy (OHE), indicating the necessity of\naccurate preoperative prognostic modeling. Current studies typically build\nmachine learning models from preoperative CT images or clinical\ncharacteristics, but face three key challenges: (1) labor-intensive\nregion-of-interest (ROI) annotation, (2) poor reliability and generalizability\nof unimodal methods, and (3) incomplete assessment from single-endpoint\nprediction. Moreover, the lack of publicly accessible datasets constrains\nresearch in this field. Therefore, we present MultiTIPS, the first public\nmulti-center dataset for TIPS prognosis, and propose a novel multimodal\nprognostic framework based on it. The framework comprises three core modules:\n(1) dual-option segmentation, which integrates semi-supervised and foundation\nmodel-based pipelines to achieve robust ROI segmentation with limited\nannotations and facilitate subsequent feature extraction; (2) multimodal\ninteraction, where three techniques, multi-grained radiomics attention (MGRA),\nprogressive orthogonal disentanglement (POD), and clinically guided prognostic\nenhancement (CGPE), are introduced to enable cross-modal feature interaction\nand complementary representation integration, thus improving model accuracy and\nrobustness; and (3) multi-task prediction, where a staged training strategy is\nused to perform stable optimization of survival, portal pressure gradient\n(PPG), and OHE prediction for comprehensive prognostic assessment. Extensive\nexperiments on MultiTIPS demonstrate the superiority of the proposed method\nover state-of-the-art approaches, along with strong cross-domain generalization\nand interpretability, indicating its promise for clinical application. The\ndataset and code are available.", "AI": {"tldr": "本研究提出了首个公开的多中心TIPS预后数据集MultiTIPS，并基于此构建了一个新颖的多模态预后框架，以实现准确、鲁棒且全面的术前预后评估。", "motivation": "经颈静脉肝内门体分流术（TIPS）在治疗门静脉高压方面效果不一，且常导致肝性脑病，因此需要准确的术前预后建模。现有研究面临ROI标注困难、单模态方法可靠性和泛化性差、单一终点评估不完整以及缺乏公开数据集的挑战。", "method": "本研究提出MultiTIPS数据集和多模态预后框架。该框架包含三个核心模块：1) 双选项分割，整合半监督和基础模型实现鲁棒的ROI分割；2) 多模态交互，引入MGRA、POD和CGPE技术以实现跨模态特征交互和互补表示集成；3) 多任务预测，采用分阶段训练策略稳定优化生存、门静脉压力梯度（PPG）和肝性脑病（OHE）的预测。", "result": "在MultiTIPS数据集上的大量实验表明，所提出的方法优于现有最先进方法，并展现出强大的跨域泛化性和可解释性。", "conclusion": "本研究提出的方法在TIPS预后评估方面具有显著优势和临床应用潜力。MultiTIPS数据集和相关代码已公开，促进了该领域的研究。"}}
{"id": "2510.10466", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10466", "abs": "https://arxiv.org/abs/2510.10466", "authors": ["Jinjin Cao", "Zhiyang Chen", "Zijun Wang", "Liyuan Ma", "Weijian Luo", "Guojun Qi"], "title": "When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance", "comment": null, "summary": "Vision-Language Models (VLMs) have shown solid ability for multimodal\nunderstanding of both visual and language contexts. However, existing VLMs\noften face severe challenges of hallucinations, meaning that VLMs tend to\ngenerate responses that are only fluent in the language but irrelevant to\nimages in previous contexts. To address this issue, we analyze how language\nbias contributes to hallucinations and then introduce Cross-Modal\nGuidance(CMG), a training-free decoding method that addresses the\nhallucinations by leveraging the difference between the output distributions of\nthe original model and the one with degraded visual-language attention. In\npractice, we adaptively mask the attention weight of the most influential image\ntokens in selected transformer layers to corrupt the visual-language perception\nas a concrete type of degradation. Such a degradation-induced decoding\nemphasizes the perception of visual contexts and therefore significantly\nreduces language bias without harming the ability of VLMs. In experiment\nsections, we conduct comprehensive studies. All results demonstrate the\nsuperior advantages of CMG with neither additional conditions nor training\ncosts. We also quantitatively show CMG can improve different VLM's performance\non hallucination-specific benchmarks and generalize effectively.", "AI": {"tldr": "本文提出了一种名为跨模态引导（CMG）的免训练解码方法，通过利用原始模型与视觉语言注意力退化模型输出分布的差异，有效减少了视觉语言模型（VLM）中的幻觉问题。", "motivation": "现有VLM在多模态理解方面表现出色，但常面临幻觉问题，即生成语言流畅但与图像无关的响应。这主要是由于语言偏见造成的。", "method": "引入了跨模态引导（CMG），这是一种免训练的解码方法。它通过比较原始模型和视觉语言注意力退化模型的输出分布差异来解决幻觉问题。具体实践中，CMG通过自适应地遮蔽选定Transformer层中最具影响力的图像token的注意力权重，来破坏视觉语言感知，从而强调视觉上下文感知，显著减少语言偏见。", "result": "实验结果表明，CMG在不增加额外条件或训练成本的情况下，展现出卓越优势。它能显著减少语言偏见，同时不损害VLM的能力。CMG在幻觉特定基准上提高了不同VLM的性能，并能有效泛化。", "conclusion": "CMG通过利用视觉语言注意力退化诱导的解码，成功地在不进行额外训练或损害模型能力的情况下，解决了VLM的幻觉问题，并有效减少了语言偏见，具有良好的泛化性。"}}
{"id": "2510.10539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10539", "abs": "https://arxiv.org/abs/2510.10539", "authors": ["Yujie Ren", "Niklas Gruhlke", "Anne Lauscher"], "title": "Detecting Hallucinations in Authentic LLM-Human Interactions", "comment": null, "summary": "As large language models (LLMs) are increasingly applied in sensitive domains\nsuch as medicine and law, hallucination detection has become a critical task.\nAlthough numerous benchmarks have been proposed to advance research in this\narea, most of them are artificially constructed--either through deliberate\nhallucination induction or simulated interactions--rather than derived from\ngenuine LLM-human dialogues. Consequently, these benchmarks fail to fully\ncapture the characteristics of hallucinations that occur in real-world usage.\nTo address this limitation, we introduce AuthenHallu, the first hallucination\ndetection benchmark built entirely from authentic LLM-human interactions. For\nAuthenHallu, we select and annotate samples from genuine LLM-human dialogues,\nthereby providing a faithful reflection of how LLMs hallucinate in everyday\nuser interactions. Statistical analysis shows that hallucinations occur in\n31.4% of the query-response pairs in our benchmark, and this proportion\nincreases dramatically to 60.0% in challenging domains such as Math & Number\nProblems. Furthermore, we explore the potential of using vanilla LLMs\nthemselves as hallucination detectors and find that, despite some promise,\ntheir current performance remains insufficient in real-world scenarios.", "AI": {"tldr": "本文引入了AuthenHallu，首个基于真实LLM-人类交互的幻觉检测基准，揭示了实际幻觉发生率并评估了LLM作为检测器的能力。", "motivation": "现有LLM幻觉检测基准多为人工构建，未能充分捕捉真实世界中LLM幻觉的特征，尤其是在医疗和法律等敏感领域，幻觉检测至关重要。", "method": "提出了AuthenHallu，一个完全基于真实LLM-人类对话构建的幻觉检测基准。通过选择和标注真实LLM-人类交互样本，以忠实反映LLM在日常用户交互中产生幻觉的方式。此外，还探索了使用普通LLM本身作为幻觉检测器的潜力。", "result": "统计分析显示，幻觉在基准中31.4%的问答对中出现，在数学和数字问题等挑战性领域中，这一比例显著增加到60.0%。尽管普通LLM作为幻觉检测器显示出一些前景，但其当前性能仍不足以应对真实世界场景。", "conclusion": "真实世界中LLM的幻觉发生频率较高，特别是在复杂任务中。当前的LLM自身作为检测器的性能尚不能满足实际需求。AuthenHallu为更真实的幻觉检测研究提供了新的基准。"}}
{"id": "2510.10471", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10471", "abs": "https://arxiv.org/abs/2510.10471", "authors": ["Chuang Chen", "Wenyi Ge"], "title": "DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation", "comment": null, "summary": "Environmental perception systems play a critical role in high-precision\nmapping and autonomous navigation, with LiDAR serving as a core sensor that\nprovides accurate 3D point cloud data. How to efficiently process unstructured\npoint clouds while extracting structured semantic information remains a\nsignificant challenge, and in recent years, numerous pseudo-image-based\nrepresentation methods have emerged to achieve a balance between efficiency and\nperformance. However, they often overlook the structural and semantic details\nof point clouds, resulting in limited feature fusion and discriminability. In\nthis work, we propose DAGLFNet, a pseudo-image-based semantic segmentation\nframework designed to extract discriminative features. First, the Global-Local\nFeature Fusion Encoding module is used to enhance the correlation among local\nfeatures within a set and capture global contextual information. Second, the\nMulti-Branch Feature Extraction network is employed to capture more\nneighborhood information and enhance the discriminability of contour features.\nFinally, a Feature Fusion via Deep Feature-guided Attention mechanism is\nintroduced to improve the precision of cross-channel feature fusion.\nExperimental evaluations show that DAGLFNet achieves 69.83\\% and 78.65\\% on the\nvalidation sets of SemanticKITTI and nuScenes, respectively. The method\nbalances high performance with real-time capability, demonstrating great\npotential for LiDAR-based real-time applications.", "AI": {"tldr": "DAGLFNet是一种基于伪图像的LiDAR点云语义分割框架，旨在通过增强局部特征关联、捕获全局上下文信息、提取多邻域特征和引入深度特征引导的注意力机制，提高特征融合和判别能力，同时平衡高性能和实时性。", "motivation": "高精度地图和自动导航中的环境感知系统对LiDAR点云数据处理效率和语义信息提取提出了挑战。现有基于伪图像的LiDAR点云处理方法虽然高效，但常忽略点云的结构和语义细节，导致特征融合和判别能力有限。", "method": "本文提出了DAGLFNet框架，具体包括：1. 全局-局部特征融合编码模块，用于增强局部特征关联并捕获全局上下文信息。2. 多分支特征提取网络，用于捕获更多邻域信息并增强轮廓特征的判别力。3. 基于深度特征引导的注意力机制的特征融合，以提高跨通道特征融合的精度。", "result": "DAGLFNet在SemanticKITTI和nuScenes验证集上分别达到了69.83%和78.65%的性能。该方法在保持高性能的同时，也具备实时处理能力。", "conclusion": "DAGLFNet在LiDAR点云语义分割中实现了高性能与实时性的良好平衡，在基于LiDAR的实时应用中展现出巨大潜力。"}}
{"id": "2510.10618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10618", "abs": "https://arxiv.org/abs/2510.10618", "authors": ["Bowei He", "Lihao Yin", "Huiling Zhen", "Shuqi Liu", "Han Wu", "Xiaokun Zhang", "Mingxuan Yuan", "Chen Ma"], "title": "Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization", "comment": "Accepted by NeurIPS 2025", "summary": "Post-training compression has been a widely employed approach to scale down\nlarge language model (LLM) and facilitate efficient inference. In various\nproposed compression methods, including pruning and quantization, calibration\ndata plays a vital role by informing the weight importance and activation\ndynamic ranges. However, how calibration data impacts the LLM capability after\ncompression is less explored. Few of the existing works, though recognizing the\nsignificance of this study, only investigate the language modeling or\ncommonsense reasoning performance degradation from limited angles, like the\ndata sources or sample amounts. More systematic research is still needed to\nexamine the impacts on different LLM capabilities in terms of compositional\nproperties and domain correspondence of calibration data. In this work, we aim\nat bridging this gap and further analyze underlying influencing mechanisms from\nthe activation pattern perspective. Especially, we explore the calibration\ndata's impacts on high-level complex reasoning capabilities, like math problem\nsolving and code generation. Delving into the underlying mechanism, we find\nthat the representativeness and diversity in activation space more\nfundamentally determine the quality of calibration data. Finally, we propose a\ncalibration data curation framework based on such observations and analysis,\nenhancing the performance of existing post-training compression methods on\npreserving critical LLM capabilities. Our code is provided in\n\\href{https://github.com/BokwaiHo/COLA.git}{Link}.", "AI": {"tldr": "本文系统研究了校准数据（在LLM后训练压缩中至关重要）的组成特性和领域对应性如何影响LLM的各种能力，特别是复杂推理。研究发现激活空间中的代表性和多样性是校准数据质量的关键，并基于此提出了一个校准数据筛选框架以提升压缩性能。", "motivation": "后训练压缩是LLM高效推理的常用方法，其中校准数据对权重重要性和激活动态范围至关重要。然而，现有研究对校准数据如何影响LLM压缩后的能力（特别是高阶复杂推理能力）探索不足，仅从有限角度（如数据源或样本量）进行研究，缺乏对校准数据组成特性和领域对应性影响的系统性分析。", "method": "本文通过以下方法弥补研究空白：1) 探讨校准数据对LLM高阶复杂推理能力（如数学问题解决和代码生成）的影响；2) 从激活模式的角度深入分析潜在的影响机制；3) 发现激活空间中的代表性和多样性更根本地决定了校准数据的质量；4) 基于这些观察和分析，提出了一个校准数据筛选框架。", "result": "研究发现校准数据的组成特性和领域对应性显著影响LLM在压缩后的各种能力。激活空间中的代表性和多样性是决定校准数据质量的根本因素。本文提出的校准数据筛选框架能够增强现有后训练压缩方法在保留LLM关键能力（尤其是复杂推理能力）方面的性能。", "conclusion": "校准数据在激活空间中的代表性和多样性是决定其质量的关键因素，这对于后训练压缩后LLM能力的保持至关重要。基于这些发现，本文提出的校准数据筛选框架能够有效提升现有压缩方法在保留LLM复杂推理能力上的表现。"}}
{"id": "2510.10478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10478", "abs": "https://arxiv.org/abs/2510.10478", "authors": ["Deng Li", "Jun Shao", "Bohao Xing", "Rong Gao", "Bihan Wen", "Heikki Kälviäinen", "Xin Liu"], "title": "MSF-Mamba: Motion-aware State Fusion Mamba for Efficient Micro-Gesture Recognition", "comment": null, "summary": "Micro-gesture recognition (MGR) targets the identification of subtle and\nfine-grained human motions and requires accurate modeling of both long-range\nand local spatiotemporal dependencies. While CNNs are effective at capturing\nlocal patterns, they struggle with long-range dependencies due to their limited\nreceptive fields. Transformer-based models address this limitation through\nself-attention mechanisms but suffer from high computational costs. Recently,\nMamba has shown promise as an efficient model, leveraging state space models\n(SSMs) to enable linear-time processing However, directly applying the vanilla\nMamba to MGR may not be optimal. This is because Mamba processes inputs as 1D\nsequences, with state updates relying solely on the previous state, and thus\nlacks the ability to model local spatiotemporal dependencies. In addition,\nprevious methods lack a design of motion-awareness, which is crucial in MGR. To\novercome these limitations, we propose motion-aware state fusion mamba\n(MSF-Mamba), which enhances Mamba with local spatiotemporal modeling by fusing\nlocal contextual neighboring states. Our design introduces a motion-aware state\nfusion module based on central frame difference (CFD). Furthermore, a\nmultiscale version named MSF-Mamba+ has been proposed. Specifically, MSF-Mamba\nsupports multiscale motion-aware state fusion, as well as an adaptive scale\nweighting module that dynamically weighs the fused states across different\nscales. These enhancements explicitly address the limitations of vanilla Mamba\nby enabling motion-aware local spatiotemporal modeling, allowing MSF-Mamba and\nMSF-Mamba to effectively capture subtle motion cues for MGR. Experiments on two\npublic MGR datasets demonstrate that even the lightweight version, namely,\nMSF-Mamba, achieves SoTA performance, outperforming existing CNN-,\nTransformer-, and SSM-based models while maintaining high efficiency.", "AI": {"tldr": "本文提出了一种名为MSF-Mamba的新型模型，通过引入运动感知状态融合和多尺度机制，增强了Mamba在微手势识别（MGR）中对局部时空依赖的建模能力，克服了现有方法的局限性，并实现了最先进的性能。", "motivation": "微手势识别需要精确建模长程和局部时空依赖。CNN擅长局部模式但缺乏长程依赖能力；Transformer通过自注意力解决长程依赖但计算成本高；Mamba作为高效模型，利用状态空间模型实现线性时间处理，但其1D序列处理方式导致缺乏局部时空依赖建模能力，且现有方法普遍缺少运动感知设计。", "method": "本文提出了运动感知状态融合Mamba (MSF-Mamba)。它通过融合局部上下文相邻状态来增强Mamba的局部时空建模能力，并引入了一个基于中心帧差（CFD）的运动感知状态融合模块。此外，还提出了多尺度版本MSF-Mamba+，支持多尺度运动感知状态融合，并包含一个自适应尺度加权模块，动态权衡不同尺度下的融合状态。", "result": "在两个公共MGR数据集上的实验表明，即使是轻量级的MSF-Mamba版本也实现了最先进（SoTA）的性能，优于现有的基于CNN、Transformer和SSM的模型，同时保持了高效率。", "conclusion": "MSF-Mamba通过引入运动感知局部时空建模和多尺度融合，有效解决了传统Mamba在微手势识别中缺乏局部时空依赖建模和运动感知能力的问题，能够有效地捕捉细微的运动线索，并以高效率实现了最先进的性能。"}}
{"id": "2510.10627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10627", "abs": "https://arxiv.org/abs/2510.10627", "authors": ["Guy Mor-Lan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "FactAppeal: Identifying Epistemic Factual Appeals in News Media", "comment": null, "summary": "How is a factual claim made credible? We propose the novel task of Epistemic\nAppeal Identification, which identifies whether and how factual statements have\nbeen anchored by external sources or evidence. To advance research on this\ntask, we present FactAppeal, a manually annotated dataset of 3,226\nEnglish-language news sentences. Unlike prior resources that focus solely on\nclaim detection and verification, FactAppeal identifies the nuanced epistemic\nstructures and evidentiary basis underlying these claims and used to support\nthem. FactAppeal contains span-level annotations which identify factual\nstatements and mentions of sources on which they rely. Moreover, the\nannotations include fine-grained characteristics of factual appeals such as the\ntype of source (e.g. Active Participant, Witness, Expert, Direct Evidence),\nwhether it is mentioned by name, mentions of the source's role and epistemic\ncredentials, attribution to the source via direct or indirect quotation, and\nother features. We model the task with a range of encoder models and generative\ndecoder models in the 2B-9B parameter range. Our best performing model, based\non Gemma 2 9B, achieves a macro-F1 score of 0.73.", "AI": {"tldr": "本文提出“认知诉求识别”新任务，旨在识别事实陈述如何通过外部来源或证据获得可信度。为此，作者构建了FactAppeal数据集，并使用大型语言模型进行建模，其中Gemma 2 9B模型取得了0.73的宏观F1分数。", "motivation": "现有资源主要关注事实声明的检测和验证，而缺乏对这些声明如何通过外部来源或证据获得支持的深层认知结构和证据基础的理解。本研究旨在填补这一空白，识别事实声明的可信度来源。", "method": "本文提出了“认知诉求识别”的新任务。为此，作者创建了FactAppeal数据集，包含3,226个手工标注的英文新闻句子。该数据集的标注包括：事实陈述的跨度识别、所依赖来源的提及、来源类型（如活跃参与者、目击者、专家、直接证据）、来源是否被点名、来源的角色和认知资质、通过直接或间接引用对来源的归因以及其他细粒度特征。作者使用2B-9B参数范围的编码器模型和生成式解码器模型对该任务进行建模。", "result": "在所提出的“认知诉求识别”任务中，基于Gemma 2 9B的最佳模型取得了0.73的宏观F1分数。", "conclusion": "本文成功提出了认知诉求识别这一新任务，并构建了详细标注的FactAppeal数据集，为理解事实声明如何获得可信度提供了新的视角和资源。实验结果表明，大型语言模型在该任务上表现出有希望的性能。"}}
{"id": "2510.10658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10658", "abs": "https://arxiv.org/abs/2510.10658", "authors": ["Guy Mor-Lan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News", "comment": null, "summary": "While media bias is widely studied, the epistemic strategies behind factual\nreporting remain computationally underexplored. This paper analyzes these\nstrategies through a large-scale comparison of CNN and Fox News. To isolate\nreporting style from topic selection, we employ an article matching strategy to\ncompare reports on the same events and apply the FactAppeal framework to a\ncorpus of over 470K articles covering two highly politicized periods: the\nCOVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting\ncontains more factual statements and is more likely to ground them in external\nsources. The outlets also exhibit sharply divergent sourcing patterns: CNN\nbuilds credibility by citing Experts} and Expert Documents, constructing an\nappeal to formal authority, whereas Fox News favors News Reports and direct\nquotations. This work quantifies how partisan outlets use systematically\ndifferent epistemic strategies to construct reality, adding a new dimension to\nthe study of media bias.", "AI": {"tldr": "本研究通过大规模比较CNN和Fox News，分析了事实报道背后的认知策略。结果发现CNN的报道包含更多事实陈述并更倾向于引用外部专家和专家文件，而Fox News则偏好新闻报道和直接引语，揭示了党派媒体构建现实的系统性差异。", "motivation": "媒体偏见已被广泛研究，但事实报道背后的认知策略在计算层面仍未得到充分探索。", "method": "采用文章匹配策略，比较CNN和Fox News对相同事件的报道，以隔离报道风格与话题选择。将FactAppeal框架应用于包含超过47万篇文章的语料库，涵盖COVID-19大流行和以色列-哈马斯战争这两个高度政治化的时期。", "result": "CNN的报道包含更多事实陈述，并且更有可能将其基于外部来源。两家媒体在引用模式上存在显著差异：CNN通过引用“专家”和“专家文件”来建立可信度，构建对正式权威的诉求；而Fox News则偏好“新闻报道”和直接引语。", "conclusion": "党派媒体使用系统性不同的认知策略来构建现实，这为媒体偏见研究增添了新的维度。"}}
{"id": "2510.10487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10487", "abs": "https://arxiv.org/abs/2510.10487", "authors": ["Yunlong Deng", "Guangyi Chen", "Tianpei Gu", "Lingjing Kong", "Yan Li", "Zeyu Tang", "Kun Zhang"], "title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency", "comment": null, "summary": "Vision-Language Models (VLMs) integrate visual knowledge with the analytical\ncapabilities of Large Language Models (LLMs) through supervised visual\ninstruction tuning, using image-question-answer triplets. However, the\npotential of VLMs trained without supervised instruction remains largely\nunexplored. This study validates that VLMs possess inherent self-refinement\ncapabilities, enabling them to generate high-quality supervised data without\nexternal inputs and thereby learn autonomously. Specifically, to stimulate the\nself-refinement ability of VLMs, we propose a self-refinement framework based\non a Triangular Consistency principle: within the image-query-answer triangle,\nany masked elements should be consistently and accurately reconstructed. The\nframework involves three steps: (1) We enable the instruction generation\nability of VLMs by adding multi-task instruction tuning like\nimage$\\rightarrow$question-answer or image-answer$\\rightarrow$question. (2) We\ngenerate image-query-answer triplets from unlabeled images and use the\nTriangular Consistency principle for filtering. (3) The model is further\nupdated using the filtered synthetic data. To investigate the underlying\nmechanisms behind this self-refinement capability, we conduct a theoretical\nanalysis from a causal perspective. Using the widely recognized LLaVA-1.5 as\nour baseline, our experiments reveal that the model can autonomously achieve\nconsistent, though deliberately modest, improvements across multiple benchmarks\nwithout any external supervision, such as human annotations or environmental\nfeedback. We expect that the insights of this study on the self-refinement\nability of VLMs can inspire future research on the learning mechanism of VLMs.\nCode is available at https://github.com/dengyl20/SRF-LLaVA-1.5.", "AI": {"tldr": "本研究验证了视觉-语言模型（VLMs）具有内在的自我完善能力，无需外部监督即可生成高质量的监督数据并自主学习，其核心是基于三角一致性原则的自我完善框架。", "motivation": "当前的视觉-语言模型（VLMs）主要依赖于监督视觉指令微调。然而，未经监督指令训练的VLMs的潜力尚未得到充分探索，研究旨在探索VLMs是否具备内在的自我完善能力。", "method": "提出一个基于“三角一致性”原则的自我完善框架：1. 通过多任务指令微调（如图像→问答、图像-答案→问题）激发VLMs的指令生成能力。2. 从无标签图像中生成图像-查询-答案三元组。3. 利用三角一致性原则（即图像-查询-答案三角形中任何被遮蔽的元素都应被一致且准确地重建）过滤生成的数据。4. 使用过滤后的合成数据更新模型。此外，从因果角度对自我完善机制进行了理论分析。", "result": "实验表明，以LLaVA-1.5为基线模型，无需任何外部监督（如人工标注或环境反馈），模型能够在多个基准测试中自主实现持续且适度的性能提升，证明了VLMs具有内在的自我完善能力。", "conclusion": "本研究揭示了VLMs的自我完善能力，为理解VLMs的学习机制提供了新见解，有望启发未来关于VLMs学习机制的研究。"}}
{"id": "2510.10489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10489", "abs": "https://arxiv.org/abs/2510.10489", "authors": ["Jiaye Li", "Baoyou Chen", "Hui Li", "Zilong Dong", "Jingdong Wang", "Siyu Zhu"], "title": "Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation", "comment": null, "summary": "Transformers rely on explicit positional encoding to model structure in data.\nWhile Rotary Position Embedding (RoPE) excels in 1D domains, its application to\nimage generation reveals significant limitations such as fine-grained spatial\nrelation modeling, color cues, and object counting. This paper identifies key\nlimitations of standard multi-dimensional RoPE-rigid frequency allocation,\naxis-wise independence, and uniform head treatment-in capturing the complex\nstructural biases required for fine-grained image generation. We propose\nHARoPE, a head-wise adaptive extension that inserts a learnable linear\ntransformation parameterized via singular value decomposition (SVD) before the\nrotary mapping. This lightweight modification enables dynamic frequency\nreallocation, semantic alignment of rotary planes, and head-specific positional\nreceptive fields while rigorously preserving RoPE's relative-position property.\nExtensive experiments on class-conditional ImageNet and text-to-image\ngeneration (Flux and MMDiT) demonstrate that HARoPE consistently improves\nperformance over strong RoPE baselines and other extensions. The method serves\nas an effective drop-in replacement, offering a principled and adaptable\nsolution for enhancing positional awareness in transformer-based image\ngenerative models.", "AI": {"tldr": "本文提出HARoPE，一种头部自适应的RoPE扩展，通过在旋转映射前引入可学习的线性变换，解决了标准RoPE在图像生成中对精细空间关系建模的局限性，显著提升了图像生成模型的性能。", "motivation": "Transformer模型依赖显式位置编码来建模数据结构。RoPE在1D领域表现出色，但在图像生成中暴露出局限性，如难以建模精细空间关系、颜色线索和物体计数。研究发现标准多维RoPE存在频率分配僵硬、轴向独立性和头部处理统一等问题，无法捕捉图像生成所需的复杂结构偏差。", "method": "本文提出HARoPE（头部自适应扩展），通过在旋转映射前插入一个可学习的线性变换（通过奇异值分解SVD参数化），对RoPE进行了轻量级修改。这种修改使得动态频率重新分配、旋转平面的语义对齐以及头部特定的位置感受野成为可能，同时严格保留了RoPE的相对位置属性。", "result": "在类条件ImageNet和文生图任务（Flux和MMDiT）上的大量实验表明，HARoPE相对于强大的RoPE基线和其他扩展方法，持续提升了性能。", "conclusion": "HARoPE作为一种有效的即插即用替代方案，为增强基于Transformer的图像生成模型中的位置感知提供了一个原则性且适应性强的解决方案。"}}
{"id": "2510.10650", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10650", "abs": "https://arxiv.org/abs/2510.10650", "authors": ["Peiyin Chen", "Zhuowei Yang", "Hui Feng", "Sheng Jiang", "Rui Yan"], "title": "DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis", "comment": "5 pages", "summary": "Audio-driven talking-head generation has advanced rapidly with\ndiffusion-based generative models, yet producing temporally coherent videos\nwith fine-grained motion control remains challenging. We propose DEMO, a\nflow-matching generative framework for audio-driven talking-portrait video\nsynthesis that delivers disentangled, high-fidelity control of lip motion, head\npose, and eye gaze. The core contribution is a motion auto-encoder that builds\na structured latent space in which motion factors are independently represented\nand approximately orthogonalized. On this disentangled motion space, we apply\noptimal-transport-based flow matching with a transformer predictor to generate\ntemporally smooth motion trajectories conditioned on audio. Extensive\nexperiments across multiple benchmarks show that DEMO outperforms prior methods\nin video realism, lip-audio synchronization, and motion fidelity. These results\ndemonstrate that combining fine-grained motion disentanglement with flow-based\ngenerative modeling provides a powerful new paradigm for controllable\ntalking-head video synthesis.", "AI": {"tldr": "DEMO是一种基于流匹配的生成框架，用于音频驱动的说话肖像视频合成，通过运动自编码器实现唇部、头部姿态和眼睛凝视的解耦、高保真控制，生成时间连贯且逼真的视频。", "motivation": "尽管基于扩散的生成模型在音频驱动的说话人头部生成方面取得了快速进展，但在生成具有细粒度运动控制的时间连贯视频方面仍面临挑战。", "method": "DEMO框架的核心是一个运动自编码器，它构建了一个结构化的潜在空间，其中运动因子（唇部、头部姿态、眼睛凝视）被独立表示并近似正交化。在此解耦的运动空间上，DEMO应用基于最优传输的流匹配和Transformer预测器，根据音频生成时间平滑的运动轨迹。", "result": "在多个基准测试中，DEMO在视频真实感、唇音同步和运动保真度方面均优于现有方法。", "conclusion": "将细粒度运动解耦与基于流的生成建模相结合，为可控的说话人头部视频合成提供了一种强大的新范式。"}}
{"id": "2510.10546", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10546", "abs": "https://arxiv.org/abs/2510.10546", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Sidra Sultana", "Ayesha Kanwal", "Nazia Perwaiz"], "title": "GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction", "comment": null, "summary": "Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high\nmountain regions, yet predictive research is hindered by fragmented and\nunimodal data. Most prior efforts emphasize post-event mapping, whereas\nforecasting requires harmonized datasets that combine visual indicators with\nphysical precursors. We present GLOFNet, a multimodal dataset for GLOF\nmonitoring and prediction, focused on the Shisper Glacier in the Karakoram. It\nintegrates three complementary sources: Sentinel-2 multispectral imagery for\nspatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and\nMODIS Land Surface Temperature records spanning over two decades. Preprocessing\nincluded cloud masking, quality filtering, normalization, temporal\ninterpolation, augmentation, and cyclical encoding, followed by harmonization\nacross modalities. Exploratory analysis reveals seasonal glacier velocity\ncycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in\ncryospheric conditions. The resulting dataset, GLOFNet, is publicly available\nto support future research in glacial hazard prediction. By addressing\nchallenges such as class imbalance, cloud contamination, and coarse resolution,\nGLOFNet provides a structured foundation for benchmarking multimodal deep\nlearning approaches to rare hazard prediction.", "AI": {"tldr": "GLOFNet是一个多模态数据集，旨在解决冰湖溃决洪水（GLOFs）预测中数据碎片化和单模态的挑战，整合了多源卫星数据，并为未来的深度学习预测研究提供了一个基准。", "motivation": "冰湖溃决洪水是高山地区的破坏性灾害，但预测研究因数据碎片化和单模态而受阻。大多数现有工作侧重于事件后测绘，而预测需要结合视觉指标和物理前兆的协调数据集。", "method": "本文提出了GLOFNet数据集，专注于喀喇昆仑山脉的Shisper冰川。它整合了三种互补来源：Sentinel-2多光谱图像（用于空间监测）、NASA ITS_LIVE速度产品（用于冰川运动学）和MODIS地表温度记录（跨越二十多年）。预处理包括云掩膜、质量过滤、归一化、时间插值、数据增强和周期性编码，随后进行模态间的协调。", "result": "探索性分析揭示了季节性冰川速度周期、每十年约0.8 K的长期变暖以及冰冻圈条件的空间异质性。GLOFNet数据集已公开发布，以支持未来的冰川灾害预测研究。通过解决类别不平衡、云污染和粗分辨率等挑战，GLOFNet为基准多模态深度学习方法预测罕见灾害提供了一个结构化基础。", "conclusion": "GLOFNet通过整合多模态数据并解决现有挑战，为冰湖溃决洪水的监测和预测提供了一个结构化的数据集和研究基础，有助于推动未来罕见灾害预测的深度学习方法发展。"}}
{"id": "2510.10661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10661", "abs": "https://arxiv.org/abs/2510.10661", "authors": ["Omid Reza Heidari", "Siobhan Reid", "Yassine Yaakoubi"], "title": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation", "comment": "Accepted at NeurIPS 2025, ER \"Efficient Reasoning\" workshop", "summary": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07\\% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing.", "AI": {"tldr": "AGENTIQL是一个受代理启发的多专家框架，通过分解问题、生成子查询和优化列选择，显著提高了Text-to-SQL的准确性和可解释性，并缩小了与GPT-4级别模型的性能差距。", "motivation": "尽管大型语言模型（LLMs）在Text-to-SQL生成方面取得了进展，但其单体架构在处理复杂推理和多样化数据库模式时仍面临挑战。", "method": "本文提出了AGENTIQL，一个受代理启发的多专家框架。它包含一个用于问题分解的推理代理、一个用于子查询生成的编码代理以及一个用于列选择的精炼步骤。此外，一个自适应路由器在模块化管道和基线解析器之间进行选择，以平衡效率和准确性。该框架的多个步骤可以并行执行，以提高可扩展性。", "result": "在Spider基准测试中，AGENTIQL提高了执行准确性和可解释性，使用14B模型和Planner&Executor合并策略，最高达到86.07%的执行准确率（EX）。其性能取决于路由机制的有效性，成功缩小了与基于GPT-4的SOTA（89.65% EX）的差距，同时使用了更小的开源LLM。除了准确性，AGENTIQL还通过暴露中间推理步骤增强了透明度。", "conclusion": "AGENTIQL提供了一种鲁棒、可扩展且可解释的语义解析方法，有效解决了Text-to-SQL生成中的复杂推理和模式多样性问题，提高了准确性并增强了透明度，缩小了与顶级模型的性能差距。"}}
{"id": "2510.10677", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10677", "abs": "https://arxiv.org/abs/2510.10677", "authors": ["Zhuowei Chen", "Bowei Zhang", "Nankai Lin", "Tian Hou", "Lianxi Wang"], "title": "Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data", "comment": "Accepted to MRL Workshop at EMNLP 2025", "summary": "Recent advances in LLMs have enhanced AI capabilities, but also increased the\nrisk posed by malicious requests, highlighting the need for effective LLM\nsafeguards to detect such queries. Existing approaches largely rely on\nclassifier-based methods that lack interpretability and perform poorly on\nlow-resource languages. To address these limitations, we propose\nConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances\nexplainability via reasoning and boosts knowledge transfer between languages\nthrough alignment. With only 1,000 training samples, our method demonstrates\nsuperior performance on three datasets across six languages, outperforming\nlarger models trained with significantly more data, and exhibits strong\ninterpretability and generalization ability. We also contribute a multilingual\nbenchmark extension and release our codes to support future research.", "AI": {"tldr": "本文提出ConsistentGuard，一种基于推理的多语言LLM安全防护机制，通过推理增强可解释性，并通过对齐促进跨语言知识迁移，仅用少量数据即在多语言任务上表现出色。", "motivation": "大型语言模型（LLMs）的进步带来了恶意请求的风险，现有基于分类器的方法缺乏可解释性，并且在低资源语言上表现不佳，因此需要更有效、可解释且多语言友好的LLM安全防护机制。", "method": "提出ConsistentGuard，一种新颖的基于推理的多语言安全防护方法。它通过推理增强可解释性，并通过语言对齐提升跨语言知识迁移能力。该方法仅使用1,000个训练样本。", "result": "ConsistentGuard在三个数据集、六种语言上表现出卓越性能，优于使用更多数据训练的大型模型，并展现出强大的可解释性和泛化能力。作者还贡献了一个多语言基准扩展并发布了代码。", "conclusion": "ConsistentGuard成功解决了现有LLM安全防护的局限性，提供了一个数据高效、可解释且多语言适用的解决方案，有效检测恶意请求，并为未来研究提供了新的基准和资源。"}}
{"id": "2510.10497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10497", "abs": "https://arxiv.org/abs/2510.10497", "authors": ["Yuteng Ye", "Zheng Zhang", "Qinchuan Zhang", "Di Wang", "Youjia Zhang", "Wenxiao Zhang", "Wei Yang", "Yuan Liu"], "title": "Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking", "comment": "23 pages, 16 figures and 1 table", "summary": "Controllable 3D style transfer seeks to restyle a 3D asset so that its\ntextures match a reference image while preserving the integrity and multi-view\nconsistency. The prevalent methods either rely on direct reference style token\ninjection or score-distillation from 2D diffusion models, which incurs heavy\nper-scene optimization and often entangles style with semantic content. We\nintroduce Jigsaw3D, a multi-view diffusion based pipeline that decouples style\nfrom content and enables fast, view-consistent stylization. Our key idea is to\nleverage the jigsaw operation - spatial shuffling and random masking of\nreference patches - to suppress object semantics and isolate stylistic\nstatistics (color palettes, strokes, textures). We integrate these style cues\ninto a multi-view diffusion model via reference-to-view cross-attention,\nproducing view-consistent stylized renderings conditioned on the input mesh.\nThe renders are then style-baked onto the surface to yield seamless textures.\nAcross standard 3D stylization benchmarks, Jigsaw3D achieves high style\nfidelity and multi-view consistency with substantially lower latency, and\ngeneralizes to masked partial reference stylization, multi-object scene\nstyling, and tileable texture generation. Project page is available at:\nhttps://babahui.github.io/jigsaw3D.github.io/", "AI": {"tldr": "Jigsaw3D是一种基于多视角扩散的3D风格迁移管线，它通过“拼图操作”解耦风格与内容，实现快速、视角一致的风格化，并能生成无缝纹理。", "motivation": "现有的可控3D风格迁移方法存在以下问题：需要大量的逐场景优化；风格与语义内容容易纠缠；难以保持多视角一致性。", "method": "Jigsaw3D利用“拼图操作”（对参考图像块进行空间随机打乱和随机遮罩）来抑制物体语义并分离风格统计（颜色、笔触、纹理）。这些风格线索通过参考到视角的交叉注意力集成到多视角扩散模型中，生成与输入网格条件相关的视角一致的风格化渲染。最后，将渲染的风格烘焙到表面以生成无缝纹理。", "result": "在标准3D风格化基准测试中，Jigsaw3D实现了高风格保真度和多视角一致性，同时显著降低了延迟。它还能泛化到遮罩部分参考风格化、多对象场景风格化和可平铺纹理生成。", "conclusion": "Jigsaw3D通过创新的“拼图操作”有效解耦3D风格迁移中的风格与内容，显著提高了风格保真度、视角一致性和效率，并展现出强大的泛化能力。"}}
{"id": "2510.10666", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10666", "abs": "https://arxiv.org/abs/2510.10666", "authors": ["Zhengbo Zhang", "Zhiheng Lyu", "Junhao Gong", "Hongzhu Yi", "Xinming Wang", "Yuxuan Zhou", "Jiabing Yang", "Ping Nie", "Yan Huang", "Wenhu Chen"], "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions", "comment": "10 pages", "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.", "AI": {"tldr": "BrowserAgent是一个受人类启发、能直接在原始网页上执行浏览器操作（如滚动、点击、输入）的大语言模型代理，通过两阶段训练和显式记忆机制，在更少数据下实现了对复杂网络任务（特别是多跳问答）更优的性能。", "motivation": "现有的大语言模型（LLM）网络任务解决方案（如Search-R1、WebDancer）依赖于将交互式网络环境转换为静态文本，这与人类多样化的浏览器交互行为（如滚动、点击、输入）形成对比，限制了其效率和交互性。", "method": "BrowserAgent通过Playwright直接在原始网页上执行预定义的浏览器操作。它采用两阶段训练方法：监督微调（SFT）和拒绝微调（RFT），以提高泛化能力。此外，引入了显式记忆机制来存储关键结论，增强了对长周期任务的推理能力。", "result": "尽管训练数据量远少于Search-R1，BrowserAgent在不同的开放式问答（Open-QA）任务上取得了更具竞争力的结果。值得注意的是，BrowserAgent-7B在HotpotQA、2Wiki和Bamboogle等多跳问答任务上比Search-R1提升了约20%。", "conclusion": "BrowserAgent可以作为一个更先进的框架，用于开发更具交互性和可扩展性的网络代理。"}}
{"id": "2510.10663", "categories": ["cs.CV", "cs.AI", "I.4.10; I.2.10; I.5.0"], "pdf": "https://arxiv.org/pdf/2510.10663", "abs": "https://arxiv.org/abs/2510.10663", "authors": ["Gaojian Wang", "Feng Lin", "Tong Wu", "Zhisheng Yan", "Kui Ren"], "title": "Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection", "comment": "18 pages, 9 figures, project page:\n  https://fsfm-3c.github.io/fsvfm.html", "summary": "With abundant, unlabeled real faces, how can we learn robust and transferable\nfacial representations to boost generalization across various face security\ntasks? We make the first attempt and propose FS-VFM, a scalable self-supervised\npre-training framework, to learn fundamental representations of real face\nimages. We introduce three learning objectives, namely 3C, that synergize\nmasked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM\nto encode both local patterns and global semantics of real faces. Specifically,\nwe formulate various facial masking strategies for MIM and devise a simple yet\neffective CRFR-P masking, which explicitly prompts the model to pursue\nmeaningful intra-region Consistency and challenging inter-region Coherency. We\npresent a reliable self-distillation mechanism that seamlessly couples MIM with\nID to establish underlying local-to-global Correspondence. After pre-training,\nvanilla vision transformers (ViTs) serve as universal Vision Foundation Models\nfor downstream Face Security tasks: cross-dataset deepfake detection,\ncross-domain face anti-spoofing, and unseen diffusion facial forensics. To\nefficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a\nlightweight plug-and-play bottleneck atop the frozen backbone with a novel\nreal-anchor contrastive objective. Extensive experiments on 11 public\nbenchmarks demonstrate that our FS-VFM consistently generalizes better than\ndiverse VFMs, spanning natural and facial domains, fully, weakly, and\nself-supervised paradigms, small, base, and large ViT scales, and even\noutperforms SOTA task-specific methods, while FS-Adapter offers an excellent\nefficiency-performance trade-off. The code and models are available on\nhttps://fsfm-3c.github.io/fsvfm.html.", "AI": {"tldr": "本文提出了FS-VFM，一个可扩展的自监督预训练框架，用于学习真实人脸图像的鲁棒且可迁移的表征，并通过FS-Adapter实现高效的下游人脸安全任务微调，在多项基准测试中表现优异。", "motivation": "如何利用大量无标签的真实人脸数据，学习到鲁棒且可迁移的人脸表征，以提高在各种人脸安全任务中的泛化能力。", "method": "提出了FS-VFM自监督预训练框架，引入了融合掩码图像建模（MIM）和实例判别（ID）的“3C”学习目标。具体地，设计了CRFR-P掩码策略以促进区域内一致性和区域间连贯性，并利用自蒸馏机制将MIM与ID耦合以建立局部到全局的对应关系。预训练后，将ViT作为基础模型。为高效迁移，进一步提出了FS-Adapter，一个轻量级即插即用瓶颈，结合了新颖的真实锚点对比目标。", "result": "在11个人脸安全任务公共基准测试中，FS-VFM始终比各种视觉基础模型（VFM）具有更好的泛化能力，甚至超越了SOTA任务特定方法。FS-Adapter在效率和性能之间提供了卓越的权衡。这些任务包括跨数据集深度伪造检测、跨域人脸反欺骗和未知扩散人脸取证。", "conclusion": "FS-VFM框架成功学习了真实人脸的鲁棒和可迁移表征，显著提升了各种人脸安全任务的泛化能力，而FS-Adapter则提供了高效的迁移学习解决方案。"}}
{"id": "2510.10522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10522", "abs": "https://arxiv.org/abs/2510.10522", "authors": ["Xi Zhang", "Xiaolin Wu"], "title": "Receptive Field Expanded Look-Up Tables for Vision Inference: Advancing from Low-level to High-level Tasks", "comment": null, "summary": "Recently, several look-up table (LUT) methods were developed to greatly\nexpedite the inference of CNNs in a classical strategy of trading space for\nspeed. However, these LUT methods suffer from a common drawback of limited\nreceptive field of the convolution kernels due to the combinatorial explosion\nof table size. This research aims to expand the CNN receptive field with a\nfixed table size, thereby enhancing the performance of LUT-driven fast CNN\ninference while maintaining the same space complexity. To achieve this goal,\nvarious techniques are proposed. The main contribution is a novel approach of\nlearning an optimal lattice vector quantizer that adaptively allocates the\nquantization resolution across data dimensions based on their significance to\nthe inference task. In addition, the lattice vector quantizer offers an\ninherently more accurate approximation of CNN kernels than scalar quantizer as\nused in current practice. Furthermore, we introduce other receptive field\nexpansion strategies, including irregular dilated convolutions and a U-shaped\ncascaded LUT structure, designed to capture multi-level contextual information\nwithout inflating table size. Together, these innovations allow our approach to\neffectively balance speed, accuracy, and memory efficiency, demonstrating\nsignificant improvements over existing LUT methods.", "AI": {"tldr": "本研究旨在通过引入格向量量化器、不规则扩张卷积和U形级联LUT结构，在固定查找表大小下扩展CNN的感受野，从而提高基于LUT的快速CNN推理的性能、准确性和内存效率。", "motivation": "现有的查找表（LUT）方法在CNN推理中存在卷积核感受野受限的共同缺点，这是由于表大小的组合爆炸造成的，这限制了其性能。", "method": "主要贡献是提出了一种学习最优格向量量化器的新方法，该方法根据数据维度对推理任务的重要性自适应地分配量化分辨率。此外，格向量量化器比当前实践中使用的标量量化器能更准确地近似CNN核。还引入了其他感受野扩展策略，包括不规则扩张卷积和U形级联LUT结构，以在不增加表大小的情况下捕获多级上下文信息。", "result": "这些创新使所提出的方法能够有效地平衡速度、准确性和内存效率，并相对于现有LUT方法展示出显著的改进。", "conclusion": "通过扩展CNN的感受野并保持固定的表大小，本研究提出的方法显著提升了基于LUT的快速CNN推理的性能，同时优化了速度、准确性和内存效率。"}}
{"id": "2510.10729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10729", "abs": "https://arxiv.org/abs/2510.10729", "authors": ["Manas Zambre", "Sarika Bobade"], "title": "Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework", "comment": "4 pages, 5 figures", "summary": "Sarcasm is a nuanced and often misinterpreted form of communication,\nespecially in text, where tone and body language are absent. This paper\nproposes a modular deep learning framework for sarcasm detection, leveraging\nDeep Convolutional Neural Networks (DCNNs) and contextual models such as BERT\nto analyze linguistic, emotional, and contextual cues. The system integrates\nsentiment analysis, contextual embeddings, linguistic feature extraction, and\nemotion detection through a multi-layer architecture. While the model is in the\nconceptual stage, it demonstrates feasibility for real-world applications such\nas chatbots and social media analysis.", "AI": {"tldr": "本文提出一个模块化的深度学习框架，结合DCNNs和BERT，用于在文本中检测讽刺，通过分析语言、情感和上下文线索。", "motivation": "在文本交流中，由于缺乏语调和肢体语言，讽刺常常被误解，因此需要一种有效的方法来检测讽刺。", "method": "该研究采用模块化深度学习框架，利用深度卷积神经网络（DCNNs）和BERT等上下文模型。它通过多层架构整合情感分析、上下文嵌入、语言特征提取和情感检测。", "result": "尽管模型处于概念阶段，但它展示了在聊天机器人和社交媒体分析等实际应用中的可行性。", "conclusion": "所提出的深度学习框架在检测文本讽刺方面具有可行性，并有望应用于实际场景。"}}
{"id": "2510.10518", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10518", "abs": "https://arxiv.org/abs/2510.10518", "authors": ["Qunzhong Wang", "Jie Liu", "Jiajun Liang", "Yilei Jiang", "Yuanxing Zhang", "Jinyuan Chen", "Yaozhi Zheng", "Xintao Wang", "Pengfei Wan", "Xiangyu Yue", "Jiaheng Liu"], "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning", "comment": null, "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.", "AI": {"tldr": "本文提出了VideoReward Thinker (VR-Thinker)，一个“以图思考”的多模态奖励模型框架，通过引入视觉推理操作和可配置的视觉记忆窗口，有效解决了现有奖励模型处理长视频时上下文限制和细节丢失问题，显著提升了视频偏好评估的准确性。", "motivation": "当前多模态奖励模型存在两大局限：1) 视觉输入消耗大量上下文预算，导致帧数减少和细粒度信息丢失；2) 所有视觉信息集中在初始提示中，加剧了链式推理中的幻觉和遗忘问题。", "method": "本文引入了VideoReward Thinker (VR-Thinker)框架，它通过以下方式使奖励模型具备视觉推理能力：1) 引入“以图思考”机制，装备视觉推理操作（如选择帧）和可配置的视觉记忆窗口，使模型能在上下文限制内主动获取和更新视觉证据。2) 采用强化微调流程：a) 使用精选的视觉链式思考数据进行“冷启动”，学习基本推理技能和操作格式；b) 对维度和整体判断都正确的样本进行拒绝采样微调，以增强推理能力；c) 应用群组相对策略优化 (GRPO) 进一步强化推理。", "result": "VR-Thinker在开源模型中实现了视频偏好基准测试的最新SOTA准确率，尤其在处理更长视频时表现出色。一个7B的VR-Thinker模型在VideoGen Reward上达到80.5%，在GenAI-Bench上达到82.3%，在MJ-Bench-Video上达到75.6%。", "conclusion": "这些结果验证了“以图思考”多模态奖励建模的有效性和前景，能够显著提高视觉生成模型后训练的推理保真度和可靠性。"}}
{"id": "2510.10671", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10671", "abs": "https://arxiv.org/abs/2510.10671", "authors": ["Jinxuan Li", "Chaolei Tan", "Haoxuan Chen", "Jianxin Ma", "Jian-Fang Hu", "Wei-Shi Zheng", "Jianhuang Lai"], "title": "Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey", "comment": "Draft version, work in progress", "summary": "Image-Language Foundation Models (ILFM) have demonstrated remarkable success\nin image-text understanding/generation tasks, providing transferable multimodal\nrepresentations that generalize across diverse downstream image-based tasks.\nThe advancement of video-text research has spurred growing interest in\nextending image-based models to the video domain. This paradigm, known as\nimage-to-video transfer learning, succeeds in alleviating the substantial data\nand computational requirements associated with training video-language\nfoundation models from scratch for video-text learning. This survey provides\nthe first comprehensive review of this emerging field, which begins by\nsummarizing the widely used ILFM and their capabilities. We then systematically\nclassify existing image-to-video transfer learning strategies into two\ncategories: frozen features and modified features, depending on whether the\noriginal representations from ILFM are preserved or undergo modifications.\nBuilding upon the task-specific nature of image-to-video transfer, this survey\nmethodically elaborates these strategies and details their applications across\na spectrum of video-text learning tasks, ranging from fine-grained (e.g.,\nspatio-temporal video grounding) to coarse-grained (e.g., video question\nanswering). We further present a detailed experimental analysis to investigate\nthe efficacy of different image-to-video transfer learning paradigms on a range\nof downstream video understanding tasks. Finally, we identify prevailing\nchallenges and highlight promising directions for future research. By offering\na comprehensive and structured overview, this survey aims to establish a\nstructured roadmap for advancing video-text learning based on existing ILFM,\nand to inspire future research directions in this rapidly evolving domain.", "AI": {"tldr": "这篇综述首次全面回顾了图像到视频迁移学习这一新兴领域，该领域利用图像-语言基础模型（ILFM）来解决视频-文本学习中的数据和计算挑战，并系统地分类了迁移策略，分析了其在各种视频任务中的应用，并提出了未来的研究方向。", "motivation": "图像-语言基础模型（ILFM）在图像-文本任务中取得了显著成功，提供了可迁移的多模态表示。视频-文本研究的进展激发了将这些图像模型扩展到视频领域的兴趣，以缓解从头开始训练视频-语言基础模型所需的大量数据和计算需求。", "method": "本综述首先总结了广泛使用的ILFM及其能力。然后，系统地将现有的图像到视频迁移学习策略分为两类：冻结特征（frozen features）和修改特征（modified features）。接着，详细阐述了这些策略及其在从细粒度到粗粒度视频-文本学习任务中的应用。最后，通过详细的实验分析调查了不同迁移范式在视频理解任务上的效果，并指出了挑战和未来研究方向。", "result": "本综述提供了一个全面且结构化的图像到视频迁移学习概述，包括对ILFM的总结、对迁移策略的系统分类（冻结特征和修改特征），以及这些策略在各种视频-文本任务中的应用细节。此外，还通过实验分析了不同迁移范式的有效性，并识别了当前面临的挑战和有前景的未来研究方向。", "conclusion": "本综述旨在为基于现有ILFM的视频-文本学习建立一个结构化的路线图，并通过提供全面的概述来启发该快速发展领域未来的研究方向。"}}
{"id": "2510.10681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10681", "abs": "https://arxiv.org/abs/2510.10681", "authors": ["Zichun Yu", "Chenyan Xiong"], "title": "RePro: Training Language Models to Faithfully Recycle the Web for Pretraining", "comment": null, "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.", "AI": {"tldr": "RePro是一种新颖的网页数据回收方法，通过使用强化学习训练一个小型语言模型生成高质量且忠实于原文的预训练数据复述，有效解决了大型语言模型预训练数据短缺的问题。", "motivation": "高质量的预训练数据是大型语言模型（LLMs）的“化石燃料”，但对于前沿模型而言，其储备正在耗尽。因此，需要开发新的方法来有效利用现有数据。", "method": "本文提出了RePro方法，利用强化学习训练一个相对较小的语言模型（4B参数）作为复述器。该复述器通过一个质量奖励和三个忠实度奖励进行优化，旨在将原始数据转换为高质量的复述，同时保持其核心语义和结构。", "result": "在400M和1.4B模型上的预训练结果表明，RePro在22个下游任务上比仅使用原始数据的基线模型相对准确率提高了4.7%-14.0%。它还优于SOTA的ReWire方法（使用70B复述器），并超越了数据量大4倍的原始基线。RePro将原始数据效率提高了2-3倍，并通过分析验证了其比基于提示的方法保留了更多关键信息并忠实反映了原始数据特性。", "conclusion": "RePro提供了一种高效且可控的途径，能够有效利用LLM预训练的“化石燃料”。该方法显著提高了数据效率，并在下游任务中带来了显著的性能提升，为解决LLM数据短缺问题提供了有效方案。"}}
{"id": "2510.10782", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10782", "abs": "https://arxiv.org/abs/2510.10782", "authors": ["Sneha Varur", "Anirudh R Hanchinamani", "Tarun S Bagewadi", "Uma Mudenagudi", "Chaitra D Desai", "Sujata C", "Padmashree Desai", "Sumit Meharwade"], "title": "DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation", "comment": null, "summary": "In this paper, we propose a novel framework, Disentangled Style-Content GAN\n(DISC-GAN), which integrates style-content disentanglement with a\ncluster-specific training strategy towards photorealistic underwater image\nsynthesis. The quality of synthetic underwater images is challenged by optical\ndue to phenomena such as color attenuation and turbidity. These phenomena are\nrepresented by distinct stylistic variations across different waterbodies, such\nas changes in tint and haze. While generative models are well-suited to capture\ncomplex patterns, they often lack the ability to model the non-uniform\nconditions of diverse underwater environments. To address these challenges, we\nemploy K-means clustering to partition a dataset into style-specific domains.\nWe use separate encoders to get latent spaces for style and content; we further\nintegrate these latent representations via Adaptive Instance Normalization\n(AdaIN) and decode the result to produce the final synthetic image. The model\nis trained independently on each style cluster to preserve domain-specific\ncharacteristics. Our framework demonstrates state-of-the-art performance,\nobtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak\nSignal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance\n(FID) of 13.3728.", "AI": {"tldr": "本文提出了一种名为DISC-GAN的新型框架，它结合了风格-内容解耦与特定聚类训练策略，用于生成逼真的水下图像。", "motivation": "水下图像因颜色衰减和浊度等光学现象而质量受损，这些现象导致了不同水体中独特的风格变化（如色调和雾度）。现有的生成模型难以捕捉多样水下环境的非均匀条件。", "method": "该方法提出DISC-GAN框架，通过K-means聚类将数据集划分为特定风格的域。它使用独立的编码器获取风格和内容的潜在空间，并通过自适应实例归一化（AdaIN）整合这些潜在表示，然后解码生成最终的合成图像。模型在每个风格聚类上独立训练以保留特定域的特征。", "result": "DISC-GAN框架展示了最先进的性能，获得了0.9012的结构相似性指数（SSIM）、32.5118 dB的平均峰值信噪比（PSNR）和13.3728的Frechet Inception距离（FID）。", "conclusion": "所提出的DISC-GAN框架通过将风格-内容解耦与特定聚类训练策略相结合，有效解决了水下图像合成的挑战，实现了逼真的图像生成和最先进的性能。"}}
{"id": "2510.10524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10524", "abs": "https://arxiv.org/abs/2510.10524", "authors": ["Yang Liu", "Yufei Yin", "Chenchen Jing", "Muzhi Zhu", "Hao Chen", "Yuling Xi", "Bo Feng", "Hao Wang", "Shiyu Li", "Chunhua Shen"], "title": "Unified Open-World Segmentation with Multi-Modal Prompts", "comment": "Accepted to ICCV2025", "summary": "In this work, we present COSINE, a unified open-world segmentation model that\nconsolidates open-vocabulary segmentation and in-context segmentation with\nmulti-modal prompts (e.g., text and image). COSINE exploits foundation models\nto extract representations for an input image and corresponding multi-modal\nprompts, and a SegDecoder to align these representations, model their\ninteraction, and obtain masks specified by input prompts across different\ngranularities. In this way, COSINE overcomes architectural discrepancies,\ndivergent learning objectives, and distinct representation learning strategies\nof previous pipelines for open-vocabulary segmentation and in-context\nsegmentation. Comprehensive experiments demonstrate that COSINE has significant\nperformance improvements in both open-vocabulary and in-context segmentation\ntasks. Our exploratory analyses highlight that the synergistic collaboration\nbetween using visual and textual prompts leads to significantly improved\ngeneralization over single-modality approaches.", "AI": {"tldr": "COSINE是一个统一的开放世界分割模型，它结合了开放词汇分割和上下文分割，利用多模态提示（文本和图像），并通过基础模型和SegDecoder实现了显著的性能提升和更好的泛化能力。", "motivation": "现有的开放词汇分割和上下文分割方法存在架构差异、学习目标不同和表示学习策略不一的问题，作者旨在开发一个统一的模型来克服这些局限性。", "method": "COSINE利用基础模型提取输入图像和多模态提示的表示。然后，一个SegDecoder对齐这些表示，建模它们的交互，并根据输入提示获取不同粒度的分割掩码。", "result": "COSINE在开放词汇分割和上下文分割任务中均取得了显著的性能提升。探索性分析表明，视觉和文本提示的协同作用比单模态方法能显著提高泛化能力。", "conclusion": "COSINE成功地统一了开放词汇和上下文分割，克服了现有方法的架构差异和学习目标问题，并通过多模态提示的协同作用实现了卓越的性能和泛化能力。"}}
{"id": "2510.10801", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10801", "abs": "https://arxiv.org/abs/2510.10801", "authors": ["Bahar İlgen", "Georges Hattab"], "title": "Toward Human-Centered Readability Evaluation", "comment": "Accepted to the 4th Workshop on Bridging Human-Computer Interaction\n  and NLP (HCI+NLP) at EMNLP 2025, Suzhou, China", "summary": "Text simplification is essential for making public health information\naccessible to diverse populations, including those with limited health\nliteracy. However, commonly used evaluation metrics in Natural Language\nProcessing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level\nfeatures and fail to account for human-centered qualities like clarity,\ntrustworthiness, tone, cultural relevance, and actionability. This limitation\nis particularly critical in high-stakes health contexts, where communication\nmust be not only simple but also usable, respectful, and trustworthy. To\naddress this gap, we propose the Human-Centered Readability Score (HCRS), a\nfive-dimensional evaluation framework grounded in Human-Computer Interaction\n(HCI) and health communication research. HCRS integrates automatic measures\nwith structured human feedback to capture the relational and contextual aspects\nof readability. We outline the framework, discuss its integration into\nparticipatory evaluation workflows, and present a protocol for empirical\nvalidation. This work aims to advance the evaluation of health text\nsimplification beyond surface metrics, enabling NLP systems that align more\nclosely with diverse users' needs, expectations, and lived experiences.", "AI": {"tldr": "本研究提出了一种以人为中心的五维度可读性评分框架（HCRS），用于评估健康文本简化，该框架结合了自动化测量和结构化人工反馈，旨在解决现有NLP指标在捕获健康信息关键的人文品质方面的不足。", "motivation": "公共卫生信息的文本简化对于不同人群至关重要，但BLEU、FKGL、SARI等现有NLP评估指标主要关注表面特征，未能捕捉清晰度、可信度、语气、文化相关性和可操作性等以人为中心的品质，这在高度敏感的健康语境中尤为关键。", "method": "本研究提出了“以人为中心的可读性评分”（HCRS），这是一个基于人机交互（HCI）和健康传播研究的五维度评估框架。HCRS将自动化测量与结构化人工反馈相结合，以捕捉可读性的关系和语境方面。论文概述了该框架，讨论了其如何整合到参与式评估工作流程中，并提出了一个实证验证协议。", "result": "本研究的主要成果是提出了HCRS框架，这是一个结合了自动化测量和结构化人工反馈的五维度评估工具，旨在超越表面指标来评估健康文本简化。该框架及其整合与验证协议被详细阐述。", "conclusion": "HCRS旨在推动健康文本简化评估超越表面指标，从而使NLP系统能够更紧密地符合不同用户的需求、期望和生活经验。"}}
{"id": "2510.10802", "categories": ["cs.CV", "cs.AI", "cs.LG", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10802", "abs": "https://arxiv.org/abs/2510.10802", "authors": ["Md Abdullah Al Mazid", "Liangdong Deng", "Naphtali Rishe"], "title": "MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral Cloud Segmentation", "comment": "7 pages, 2 Figures", "summary": "Clouds remain a critical challenge in optical satellite imagery, hindering\nreliable analysis for environmental monitoring, land cover mapping, and climate\nresearch. To overcome this, we propose MSCloudCAM, a Cross-Attention with\nMulti-Scale Context Network tailored for multispectral and multi-sensor cloud\nsegmentation. Our framework exploits the spectral richness of Sentinel-2\n(CloudSEN12) and Landsat-8 (L8Biome) data to classify four semantic categories:\nclear sky, thin cloud, thick cloud, and cloud shadow. MSCloudCAM combines a\nSwin Transformer backbone for hierarchical feature extraction with multi-scale\ncontext modules ASPP and PSP for enhanced scale-aware learning. A\nCross-Attention block enables effective multisensor and multispectral feature\nfusion, while the integration of an Efficient Channel Attention Block (ECAB)\nand a Spatial Attention Module adaptively refine feature representations.\nComprehensive experiments on CloudSEN12 and L8Biome demonstrate that MSCloudCAM\ndelivers state-of-the-art segmentation accuracy, surpassing leading baseline\narchitectures while maintaining competitive parameter efficiency and FLOPs.\nThese results underscore the model's effectiveness and practicality, making it\nwell-suited for large-scale Earth observation tasks and real-world\napplications.", "AI": {"tldr": "MSCloudCAM是一个用于多光谱和多传感器云分割的跨注意力多尺度上下文网络，它能有效分类晴空、薄云、厚云和云影，并在云数据集上表现出最先进的性能和高效率。", "motivation": "光学卫星图像中的云是环境监测、土地覆盖测绘和气候研究等可靠分析的关键挑战。", "method": "MSCloudCAM利用Sentinel-2和Landsat-8数据进行四种语义类别的分类（晴空、薄云、厚云、云影）。它结合了Swin Transformer骨干网络进行分层特征提取，ASPP和PSP多尺度上下文模块增强尺度感知学习，通过跨注意力块实现多传感器和多光谱特征融合，并集成高效通道注意力块（ECAB）和空间注意力模块自适应地细化特征表示。", "result": "在CloudSEN12和L8Biome数据集上的综合实验表明，MSCloudCAM达到了最先进的分割精度，超越了领先的基线架构，同时保持了有竞争力的参数效率和FLOPs。", "conclusion": "MSCloudCAM模型具有高效性和实用性，非常适用于大规模地球观测任务和实际应用。"}}
{"id": "2510.10762", "categories": ["cs.CL", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10762", "abs": "https://arxiv.org/abs/2510.10762", "authors": ["Wenqing Zhang", "Trang Nguyen", "Elizabeth A. Stuart", "Yiqun T. Chen"], "title": "Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis", "comment": null, "summary": "Systematic reviews are crucial for synthesizing scientific evidence but\nremain labor-intensive, especially when extracting detailed methodological\ninformation. Large language models (LLMs) offer potential for automating\nmethodological assessments, promising to transform evidence synthesis. Here,\nusing causal mediation analysis as a representative methodological domain, we\nbenchmarked state-of-the-art LLMs against expert human reviewers across 180\nfull-text scientific articles. Model performance closely correlated with human\njudgments (accuracy correlation 0.71; F1 correlation 0.97), achieving\nnear-human accuracy on straightforward, explicitly stated methodological\ncriteria. However, accuracy sharply declined on complex, inference-intensive\nassessments, lagging expert reviewers by up to 15%. Errors commonly resulted\nfrom superficial linguistic cues -- for instance, models frequently\nmisinterpreted keywords like \"longitudinal\" or \"sensitivity\" as automatic\nevidence of rigorous methodological approache, leading to systematic\nmisclassifications. Longer documents yielded lower model accuracy, whereas\npublication year showed no significant effect. Our findings highlight an\nimportant pattern for practitioners using LLMs for methods review and synthesis\nfrom full texts: current LLMs excel at identifying explicit methodological\nfeatures but require human oversight for nuanced interpretations. Integrating\nautomated information extraction with targeted expert review thus provides a\npromising approach to enhance efficiency and methodological rigor in evidence\nsynthesis across diverse scientific fields.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在系统综述中提取方法学信息的能力，发现LLMs在处理明确、直接的信息时表现出色，但在复杂、需要推断的任务上仍需人类专家监督。", "motivation": "系统综述在综合科学证据方面至关重要，但其劳动密集型，尤其是在提取详细方法学信息时。大型语言模型（LLMs）有望自动化方法学评估，从而改变证据综合过程。", "method": "研究选取因果中介分析作为代表性方法学领域，在180篇科学全文文章上，将最先进的LLMs与人类专家评审员进行了基准测试。评估指标包括准确性相关性（accuracy correlation）和F1相关性（F1 correlation）。", "result": "LLMs的性能与人类判断密切相关（准确性相关性0.71；F1相关性0.97），在直接、明确表述的方法学标准上达到了接近人类的准确性。然而，在复杂、需要推断的评估任务上，准确性急剧下降，比专家评审员低15%。错误通常源于表面语言线索，例如模型常将“纵向”或“敏感性”等关键词误解为严格方法学方法的自动证据。较长的文档导致模型准确性下降，而发表年份没有显著影响。", "conclusion": "当前LLMs在识别明确的方法学特征方面表现出色，但对于细致的解释仍需要人类监督。将自动化信息提取与有针对性的专家评审相结合，为提高跨科学领域证据综合的效率和方法学严谨性提供了一种有前景的方法。"}}
{"id": "2510.10776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10776", "abs": "https://arxiv.org/abs/2510.10776", "authors": ["James Ald Teves", "Ray Daniel Cal", "Josh Magdiel Villaluz", "Jean Malolos", "Mico Magtira", "Ramon Rodriguez", "Mideth Abisado", "Joseph Marvin Imperial"], "title": "HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon", "comment": "Camera-ready for PACLIC 2025 (ACL Proceedings)", "summary": "The language of Hiligaynon, spoken predominantly by the people of Panay\nIsland, Negros Occidental, and Soccsksargen in the Philippines, remains\nunderrepresented in language processing research due to the absence of\nannotated corpora and baseline models. This study introduces HiligayNER, the\nfirst publicly available baseline model for the task of Named Entity\nRecognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains\nover 8,000 annotated sentences collected from publicly available news articles,\nsocial media posts, and literary texts. Two Transformer-based models, mBERT and\nXLM-RoBERTa, were fine-tuned on this collected corpus to build versions of\nHiligayNER. Evaluation results show strong performance, with both models\nachieving over 80% in precision, recall, and F1-score across entity types.\nFurthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates\npromising transferability, suggesting the broader applicability of HiligayNER\nfor multilingual NLP in low-resource settings. This work aims to contribute to\nlanguage technology development for underrepresented Philippine languages,\nspecifically for Hiligaynon, and support future research in regional language\nprocessing.", "AI": {"tldr": "本研究推出了Hiligaynon语言的第一个公开可用的命名实体识别（NER）基线模型HiligayNER，并在Transformer模型上实现了超过80%的F1分数，同时展示了良好的跨语言迁移能力。", "motivation": "Hiligaynon语言在语言处理研究中代表性不足，缺乏标注语料库和基线模型，阻碍了其语言技术的发展。", "method": "研究收集了超过8,000个来自新闻文章、社交媒体和文学文本的标注句子，构建了HiligayNER数据集。在此语料库上微调了两个基于Transformer的模型：mBERT和XLM-RoBERTa。", "result": "评估结果显示，两个模型在所有实体类型上都取得了超过80%的精确率、召回率和F1分数。此外，与宿雾语和他加禄语的跨语言评估表明，该模型具有良好的可迁移性。", "conclusion": "这项工作旨在为代表性不足的菲律宾语言（特别是Hiligaynon）的语言技术发展做出贡献，并支持未来区域语言处理的研究。"}}
{"id": "2510.10534", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.10534", "abs": "https://arxiv.org/abs/2510.10534", "authors": ["Binyu Zhao", "Wei Zhang", "Zhaonian Zou"], "title": "MCE: Towards a General Framework for Handling Missing Modalities under Imbalanced Missing Rates", "comment": "This is the accepted version of an article that has been published in\n  \\textbf{Pattern Recognition}. The final published version will be available\n  soon", "summary": "Multi-modal learning has made significant advances across diverse pattern\nrecognition applications. However, handling missing modalities, especially\nunder imbalanced missing rates, remains a major challenge. This imbalance\ntriggers a vicious cycle: modalities with higher missing rates receive fewer\nupdates, leading to inconsistent learning progress and representational\ndegradation that further diminishes their contribution. Existing methods\ntypically focus on global dataset-level balancing, often overlooking critical\nsample-level variations in modality utility and the underlying issue of\ndegraded feature quality. We propose Modality Capability Enhancement (MCE) to\ntackle these limitations. MCE includes two synergistic components: i) Learning\nCapability Enhancement (LCE), which introduces multi-level factors to\ndynamically balance modality-specific learning progress, and ii) Representation\nCapability Enhancement (RCE), which improves feature semantics and robustness\nthrough subset prediction and cross-modal completion tasks. Comprehensive\nevaluations on four multi-modal benchmarks show that MCE consistently\noutperforms state-of-the-art methods under various missing configurations. The\njournal preprint version is now available at\nhttps://doi.org/10.1016/j.patcog.2025.112591. Our code is available at\nhttps://github.com/byzhaoAI/MCE.", "AI": {"tldr": "本文提出模态能力增强（MCE）框架，通过学习能力增强（LCE）动态平衡模态学习进度，并结合表征能力增强（RCE）提升特征语义和鲁棒性，以解决多模态学习中模态缺失，尤其是不平衡缺失率的挑战。", "motivation": "多模态学习在处理缺失模态，特别是不平衡缺失率时面临重大挑战。不平衡缺失率会导致缺失率高的模态更新少、学习进度不一致和表征质量下降，形成恶性循环。现有方法通常侧重于全局数据集层面的平衡，而忽视了样本层面模态效用的变化和特征质量下降的根本问题。", "method": "本文提出模态能力增强（MCE）框架，包含两个协同组件：1) 学习能力增强（LCE），引入多级因子动态平衡模态特定的学习进度；2) 表征能力增强（RCE），通过子集预测和跨模态补全任务来提升特征的语义和鲁棒性。", "result": "在四个多模态基准上的综合评估表明，MCE在各种缺失配置下均持续优于现有最先进的方法。", "conclusion": "MCE通过动态平衡模态学习进度和提升特征质量，有效解决了多模态学习中模态缺失（特别是不平衡缺失率）所带来的挑战，并显著提高了性能。"}}
{"id": "2510.10553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10553", "abs": "https://arxiv.org/abs/2510.10553", "authors": ["Siyuan Liu", "Junting Lin"], "title": "MRS-YOLO Railroad Transmission Line Foreign Object Detection Based on Improved YOLO11 and Channel Pruning", "comment": null, "summary": "Aiming at the problems of missed detection, false detection and low detection\nefficiency in transmission line foreign object detection under railway\nenvironment, we proposed an improved algorithm MRS-YOLO based on YOLO11.\nFirstly, a multi-scale Adaptive Kernel Depth Feature Fusion (MAKDF) module is\nproposed and fused with the C3k2 module to form C3k2_MAKDF, which enhances the\nmodel's feature extraction capability for foreign objects of different sizes\nand shapes. Secondly, a novel Re-calibration Feature Fusion Pyramid Network\n(RCFPN) is designed as a neck structure to enhance the model's ability to\nintegrate and utilize multi-level features effectively. Then, Spatial and\nChannel Reconstruction Detect Head (SC_Detect) based on spatial and channel\npreprocessing is designed to enhance the model's overall detection performance.\nFinally, the channel pruning technique is used to reduce the redundancy of the\nimproved model, drastically reduce Parameters and Giga Floating Point\nOperations Per Second (GFLOPs), and improve the detection efficiency. The\nexperimental results show that the mAP50 and mAP50:95 of the MRS-YOLO algorithm\nproposed in this paper are improved to 94.8% and 86.4%, respectively, which are\n0.7 and 2.3 percentage points higher compared to the baseline, while Parameters\nand GFLOPs are reduced by 44.2% and 17.5%, respectively. It is demonstrated\nthat the improved algorithm can be better applied to the task of foreign object\ndetection in railroad transmission lines.", "AI": {"tldr": "本文提出了一种基于YOLOv11的改进算法MRS-YOLO，旨在解决铁路环境下输电线路异物检测中存在的漏检、误检和检测效率低的问题。通过引入多尺度特征融合模块、重校准特征融合金字塔网络和空间通道重建检测头，并结合通道剪枝技术，显著提高了检测精度和效率，并减少了模型参数量和计算量。", "motivation": "现有铁路环境下输电线路异物检测存在漏检、误检和检测效率低的问题。", "method": "1. 提出多尺度自适应核深度特征融合（MAKDF）模块，并与C3k2模块融合形成C3k2_MAKDF，以增强模型对不同大小和形状异物的特征提取能力。2. 设计新颖的重校准特征融合金字塔网络（RCFPN）作为颈部结构，以增强模型有效整合和利用多级特征的能力。3. 设计基于空间和通道预处理的空间和通道重建检测头（SC_Detect），以提高模型的整体检测性能。4. 采用通道剪枝技术减少改进模型的冗余，大幅降低参数量和GFLOPs，提高检测效率。", "result": "MRS-YOLO算法的mAP50和mAP50:95分别提高到94.8%和86.4%，比基线模型分别提高了0.7和2.3个百分点。同时，参数量和GFLOPs分别减少了44.2%和17.5%。", "conclusion": "改进后的MRS-YOLO算法能够更好地应用于铁路输电线路异物检测任务，实现了更高的检测精度和效率。"}}
{"id": "2510.10787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10787", "abs": "https://arxiv.org/abs/2510.10787", "authors": ["Zhichao Wang", "Cheng Wan", "Dong Nie"], "title": "Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG", "comment": null, "summary": "The performance gains of LLMs have historically been driven by scaling up\nmodel size and training data. However, the rapidly diminishing availability of\nhigh-quality training data is introducing a fundamental bottleneck, shifting\nthe focus of research toward inference-time scaling. This paradigm uses\nadditional computation at the time of deployment to substantially improve LLM\nperformance on downstream tasks without costly model re-training. This review\nsystematically surveys the diverse techniques contributing to this new era of\ninference-time scaling, organizing the rapidly evolving field into two\ncomprehensive perspectives: Output-focused and Input-focused methods.\nOutput-focused techniques encompass complex, multi-step generation strategies,\nincluding reasoning (e.g., CoT, ToT, ReAct), various search and decoding\nmethods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),\nand model ensemble methods. Input-focused techniques are primarily categorized\nby few-shot and RAG, with RAG as the central focus. The RAG section is further\ndetailed through a structured examination of query expansion, data, retrieval\nand reranker, LLM generation methods, and multi-modal RAG.", "AI": {"tldr": "由于高质量训练数据日益稀缺，大型语言模型（LLMs）的性能提升瓶颈已从模型规模和训练数据转向推理时间扩展。本文系统综述了推理时间扩展的各种技术，将其分为输出导向和输入导向两大类。", "motivation": "LLMs的性能提升传统上依赖于模型规模和训练数据的扩大。然而，高质量训练数据日益枯竭，成为了一个根本性瓶颈，促使研究重点转向在部署时利用额外计算来显著提升LLM性能，而无需昂贵的模型重新训练。", "method": "本文通过系统性综述的方式，将快速发展的推理时间扩展领域划分为两个综合视角：输出导向方法和输入导向方法。输出导向技术包括复杂的多步生成策略（如CoT、ToT、ReAct等推理方法）、各种搜索和解码方法（如MCTS、beam search）、长CoT训练（如RLVR、GRPO）以及模型集成方法。输入导向技术主要分为少样本学习（few-shot）和检索增强生成（RAG），其中RAG是核心焦点，并进一步详细考察了查询扩展、数据、检索与重排器、LLM生成方法和多模态RAG。", "result": "本文提供了一个对推理时间扩展技术的系统性调查和分类框架。它将多样化的技术归纳为输出导向（涉及多步生成、推理、搜索、解码和集成）和输入导向（主要集中于少样本和RAG及其组件）两大类，清晰地组织了这一快速发展的研究领域。", "conclusion": "推理时间扩展是LLMs性能提升的新范式，它通过在部署时增加计算来规避训练数据瓶颈。本文通过系统性地将现有技术分为输出导向和输入导向方法，为理解和进一步发展这一领域提供了全面的视角和结构化的分类。"}}
{"id": "2510.10533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10533", "abs": "https://arxiv.org/abs/2510.10533", "authors": ["Elham Shabaninia", "Fatemeh Asadi-zeydabadi", "Hossein Nezamabadi-pour"], "title": "Layout-Independent License Plate Recognition via Integrated Vision and Language Models", "comment": null, "summary": "This work presents a pattern-aware framework for automatic license plate\nrecognition (ALPR), designed to operate reliably across diverse plate layouts\nand challenging real-world conditions. The proposed system consists of a\nmodern, high-precision detection network followed by a recognition stage that\nintegrates a transformer-based vision model with an iterative language\nmodelling mechanism. This unified recognition stage performs character\nidentification and post-OCR refinement in a seamless process, learning the\nstructural patterns and formatting rules specific to license plates without\nrelying on explicit heuristic corrections or manual layout classification.\nThrough this design, the system jointly optimizes visual and linguistic cues,\nenables iterative refinement to improve OCR accuracy under noise, distortion,\nand unconventional fonts, and achieves layout-independent recognition across\nmultiple international datasets (IR-LPR, UFPR-ALPR, AOLP). Experimental results\ndemonstrate superior accuracy and robustness compared to recent\nsegmentation-free approaches, highlighting how embedding pattern analysis\nwithin the recognition stage bridges computer vision and language modelling for\nenhanced adaptability in intelligent transportation and surveillance\napplications.", "AI": {"tldr": "本文提出了一种模式感知的自动车牌识别（ALPR）框架，结合高精度检测网络、基于Transformer的视觉模型和迭代语言建模机制，实现了对多样车牌布局和复杂真实世界条件下的高精度、布局无关识别，并在多个国际数据集上表现出卓越的准确性和鲁棒性。", "motivation": "现有ALPR系统在处理多样化的车牌布局和挑战性的真实世界条件（如噪声、失真和非常规字体）时，其可靠性仍有待提高。研究旨在开发一个能够克服这些限制的框架。", "method": "该系统包含一个现代高精度检测网络，以及一个集成了基于Transformer的视觉模型和迭代语言建模机制的识别阶段。这一统一的识别阶段无缝地执行字符识别和OCR后处理，学习车牌的结构模式和格式规则，而无需依赖明确的启发式修正或手动布局分类。系统联合优化视觉和语言线索，并通过迭代细化提高在噪声、失真和非常规字体下的OCR准确性。", "result": "该系统在多个国际数据集（IR-LPR、UFPR-ALPR、AOLP）上实现了布局无关的识别。实验结果表明，与最近的无分割方法相比，该系统在准确性和鲁棒性方面表现出卓越的性能。", "conclusion": "通过将模式分析嵌入识别阶段，该框架成功地将计算机视觉和语言建模相结合，显著增强了在智能交通和监控应用中的适应性，为ALPR领域提供了新的解决方案。"}}
{"id": "2510.10827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10827", "abs": "https://arxiv.org/abs/2510.10827", "authors": ["Haeji Jung", "Jinju Kim", "Kyungjin Kim", "Youjeong Roh", "David R. Mortensen"], "title": "Happiness is Sharing a Vocabulary: A Study of Transliteration Methods", "comment": null, "summary": "Transliteration has emerged as a promising means to bridge the gap between\nvarious languages in multilingual NLP, showing promising results especially for\nlanguages using non-Latin scripts. We investigate the degree to which shared\nscript, overlapping token vocabularies, and shared phonology contribute to\nperformance of multilingual models. To this end, we conduct controlled\nexperiments using three kinds of transliteration (romanization, phonemic\ntranscription, and substitution ciphers) as well as orthography. We evaluate\neach model on two downstream tasks -- named entity recognition (NER) and\nnatural language inference (NLI) -- and find that romanization significantly\noutperforms other input types in 7 out of 8 evaluation settings, largely\nconsistent with our hypothesis that it is the most effective approach. We\nfurther analyze how each factor contributed to the success, and suggest that\nhaving longer (subword) tokens shared with pre-trained languages leads to\nbetter utilization of the model.", "AI": {"tldr": "本研究通过受控实验评估了共享字符集、词汇和音韵对多语言模型性能的影响，发现在命名实体识别和自然语言推理任务中，罗马化（romanization）的音译方法表现最佳，这主要得益于与预训练语言共享更长的子词单元。", "motivation": "音译已成为弥合多语言自然语言处理中语言鸿沟的有前景方法，尤其对于非拉丁语系语言。本研究旨在探究共享字符集、重叠词汇和共享音韵在多语言模型性能中各自的贡献程度。", "method": "研究采用了受控实验，使用三种音译方法（罗马化、音素转录和替换密码）以及正字法作为输入类型。模型在命名实体识别（NER）和自然语言推理（NLI）两个下游任务上进行评估。", "result": "在8个评估设置中，罗马化在7个设置中显著优于其他输入类型。进一步分析表明，与预训练语言共享更长的（子词）词元有助于更好地利用模型，从而提升性能。", "conclusion": "罗马化是音译中最有效的方法，其成功主要归因于它能与预训练语言共享更长的子词单元，从而使模型得到更好的利用。这与研究假设基本一致。"}}
{"id": "2510.10822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10822", "abs": "https://arxiv.org/abs/2510.10822", "authors": ["Clemence Mottez", "Louisa Fay", "Maya Varma", "Sophie Ostmeier", "Curtis Langlotz"], "title": "From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis", "comment": "Preprint of an article published in Pacific Symposium on Biocomputing\n  \\c{opyright} 2026 World Scientific Publishing Co., Singapore,\n  http://psb.stanford.edu/", "summary": "Deep learning models have shown promise in improving diagnostic accuracy from\nchest X-rays, but they also risk perpetuating healthcare disparities when\nperformance varies across demographic groups. In this work, we present a\ncomprehensive bias detection and mitigation framework targeting sex, age, and\nrace-based disparities when performing diagnostic tasks with chest X-rays. We\nextend a recent CNN-XGBoost pipeline to support multi-label classification and\nevaluate its performance across four medical conditions. We show that replacing\nthe final layer of CNN with an eXtreme Gradient Boosting classifier improves\nthe fairness of the subgroup while maintaining or improving the overall\npredictive performance. To validate its generalizability, we apply the method\nto different backbones, namely DenseNet-121 and ResNet-50, and achieve\nsimilarly strong performance and fairness outcomes, confirming its\nmodel-agnostic design. We further compare this lightweight adapter training\nmethod with traditional full-model training bias mitigation techniques,\nincluding adversarial training, reweighting, data augmentation, and active\nlearning, and find that our approach offers competitive or superior bias\nreduction at a fraction of the computational cost. Finally, we show that\ncombining eXtreme Gradient Boosting retraining with active learning yields the\nlargest reduction in bias across all demographic subgroups, both in and out of\ndistribution on the CheXpert and MIMIC datasets, establishing a practical and\neffective path toward equitable deep learning deployment in clinical radiology.", "AI": {"tldr": "本研究提出并验证了一个针对胸部X光诊断中性别、年龄和种族偏见的检测与缓解框架，通过结合CNN和XGBoost，实现了在保持预测性能的同时，显著提升子群体公平性，并提供了计算成本更低的有效偏见缓解方案。", "motivation": "深度学习模型在胸部X光诊断中表现出巨大潜力，但其在不同人口统计群体（如性别、年龄、种族）间的性能差异可能加剧医疗不公平性，因此需要一个全面的偏见检测和缓解框架。", "method": "研究扩展了一个CNN-XGBoost管线以支持多标签分类，并用XGBoost替换CNN的最后一层。该方法在DenseNet-121和ResNet-50等不同骨干网络上进行了验证。同时，与传统的偏见缓解技术（如对抗训练、重加权、数据增强、主动学习）进行了比较。最终，将XGBoost再训练与主动学习相结合以进一步减少偏见。", "result": "用XGBoost替换CNN最后一层能提高子群体的公平性，同时保持或提高整体预测性能。该方法具有模型无关性，在不同骨干网络上表现出相似的强劲性能和公平性。与传统偏见缓解技术相比，该方法以更低的计算成本提供了具有竞争力或更优的偏见减少效果。将XGBoost再训练与主动学习结合，在CheXpert和MIMIC数据集上，无论是在分布内还是分布外，都能在所有人口统计子群体中实现最大的偏见减少。", "conclusion": "本研究提出的方法，特别是将eXtreme Gradient Boosting再训练与主动学习相结合，为在临床放射学中部署公平的深度学习模型提供了一条实用且有效的途径，能够显著减少基于人口统计学信息的偏见。"}}
{"id": "2510.10889", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10889", "abs": "https://arxiv.org/abs/2510.10889", "authors": ["Junwon You", "Dasol Kang", "Jae-Hun Jung"], "title": "Topological Alignment of Shared Vision-Language Embedding Space", "comment": "24 pages, 5 figures, 19 tables", "summary": "Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot\ncapabilities. However, their cross-modal alignment remains biased toward\nEnglish due to limited multilingual multimodal data. Recent multilingual\nextensions have alleviated this gap but enforce instance-level alignment while\nneglecting the global geometry of the shared embedding space. We address this\nproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a\ntopology-aware framework aligning embedding spaces with topology-preserving\nconstraints. The proposed method applies persistent homology to define a\ntopological alignment loss and approximates persistence diagram with\ntheoretical error bounds using graph sparsification strategy. This work\nvalidates the proposed approach, showing enhanced structural coherence of\nmultilingual representations, higher zero-shot accuracy on the CIFAR-100, and\nstronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the\nproposed approach provides a general method for incorporating topological\nalignment into representation learning.", "AI": {"tldr": "ToMCLIP提出了一种拓扑感知的框架，通过引入拓扑对齐损失来解决多语言视觉-语言模型（VLMs）中存在的英语偏见问题，从而提高多语言表征的结构一致性、零样本准确性和检索性能。", "motivation": "对比视觉-语言模型（VLMs）在零样本能力上表现出色，但由于多语言多模态数据有限，其跨模态对齐偏向英语。现有的多语言扩展虽然有所缓解，但仅强制执行实例级对齐，忽略了共享嵌入空间的全局几何结构。", "method": "ToMCLIP（Topological Alignment for Multilingual CLIP）引入了一个拓扑感知的框架，通过拓扑保持约束来对齐嵌入空间。该方法应用持久同调（persistent homology）来定义拓扑对齐损失，并利用图稀疏化策略以理论误差界限近似持久图。", "result": "所提出的方法验证了ToMCLIP的有效性，表明它增强了多语言表征的结构一致性，在CIFAR-100上实现了更高的零样本准确性，并在xFlickr&CO上展示了更强的多语言检索性能。", "conclusion": "ToMCLIP通过拓扑对齐有效解决了多语言VLMs中的英语偏见问题，并为将拓扑对齐整合到表征学习中提供了一种通用的方法，超越了VLMs的范畴。"}}
{"id": "2510.10846", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10846", "abs": "https://arxiv.org/abs/2510.10846", "authors": ["Kaixuan Ren", "Preslav Nakov", "Usman Naseem"], "title": "DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models", "comment": "25 pages, 91 figures, submitted to Oct ARR under reviewing", "summary": "As vision-language models become increasingly capable, maintaining a balance\nbetween safety and usefulness remains a central challenge. Safety mechanisms,\nwhile essential, can backfire, causing over-refusal, where models decline\nbenign requests out of excessive caution. Yet, no existing benchmark has\nsystematically addressed over-refusal in the visual modality. This setting\nintroduces unique challenges, such as dual-use cases where an instruction is\nharmless, but the accompanying image contains harmful content. Models\nfrequently fail in such scenarios, either refusing too conservatively or\ncompleting tasks unsafely, which highlights the need for more fine-grained\nalignment. The ideal behavior is safe completion, i.e., fulfilling the benign\nparts of a request while explicitly warning about any potentially harmful\nelements. To address this, we present DUAL-Bench, the first multimodal\nbenchmark focused on over-refusal and safe completion in VLMs. We evaluated 18\nVLMs across 12 hazard categories, with focus on their robustness under\nsemantics-preserving visual perturbations. The results reveal substantial room\nfor improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models\naverage 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster\nthe development of more nuanced alignment strategies that ensure models remain\nboth safe and useful in complex multimodal settings.", "AI": {"tldr": "本文提出了DUAL-Bench，首个旨在评估视觉语言模型（VLMs）在多模态情境中过度拒绝和安全完成能力的基准，发现现有模型在这方面表现不佳。", "motivation": "随着视觉语言模型能力增强，如何在安全性和实用性之间取得平衡成为核心挑战。现有安全机制可能导致过度拒绝（即模型过度谨慎地拒绝良性请求）。此外，双重用途场景（指令无害但图像有害）引入了独特的挑战，现有模型在此类场景中表现不佳，凸显了对更精细对齐的需求。", "method": "研究者创建了DUAL-Bench，这是第一个专注于VLMs过度拒绝和安全完成的多模态基准。他们评估了18个VLM，涵盖12个危害类别，并特别关注模型在语义保持的视觉扰动下的鲁棒性。", "result": "评估结果显示，模型仍有很大的改进空间。GPT-5-Nano实现了12.9%的安全完成率，GPT-5模型平均为7.9%，而Qwen模型仅为3.9%。", "conclusion": "DUAL-Bench有望促进更精细的对齐策略的发展，以确保模型在复杂的多模态设置中既安全又实用，从而解决现有模型过度拒绝和不安全完成的问题。"}}
{"id": "2510.10573", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10573", "abs": "https://arxiv.org/abs/2510.10573", "authors": ["Farouq Benchallal", "Adel Hafiane", "Nicolas Ragot", "Raphael Canals"], "title": "Deep semi-supervised approach based on consistency regularization and similarity learning for weeds classification", "comment": "Submitted to EURASIP Journal on Image and Video Processing", "summary": "Weed species classification represents an important step for the development\nof automated targeting systems that allow the adoption of precision agriculture\npractices. To reduce costs and yield losses caused by their presence. The\nidentification of weeds is a challenging problem due to their shared\nsimilarities with crop plants and the variability related to the differences in\nterms of their types. Along with the variations in relation to changes in field\nconditions. Moreover, to fully benefit from deep learning-based methods, large\nfully annotated datasets are needed. This requires time intensive and laborious\nprocess for data labeling, which represents a limitation in agricultural\napplications. Hence, for the aim of improving the utilization of the unlabeled\ndata, regarding conditions of scarcity in terms of the labeled data available\nduring the learning phase and provide robust and high classification\nperformance. We propose a deep semi-supervised approach, that combines\nconsistency regularization with similarity learning. Through our developed deep\nauto-encoder architecture, experiments realized on the DeepWeeds dataset and\ninference in noisy conditions demonstrated the effectiveness and robustness of\nour method in comparison to state-of-the-art fully supervised deep learning\nmodels. Furthermore, we carried out ablation studies for an extended analysis\nof our proposed joint learning strategy.", "AI": {"tldr": "该论文提出了一种结合一致性正则化和相似性学习的深度半监督方法，用于解决标记数据稀缺条件下的杂草分类问题，并在DeepWeeds数据集上表现出优于全监督模型的有效性和鲁棒性。", "motivation": "杂草分类对于自动化精准农业系统至关重要，但由于杂草与作物相似、种类多样性以及田间条件变化，分类极具挑战性。此外，深度学习方法需要大量标注数据，而数据标注耗时费力，是农业应用中的一大限制。因此，需要一种在标记数据稀缺条件下仍能提供鲁棒、高性能分类的方法。", "method": "研究人员提出了一种深度半监督学习方法，该方法将一致性正则化与相似性学习相结合。他们开发了一种深度自编码器架构来实现这一方法，旨在利用未标记数据来提高分类性能。", "result": "在DeepWeeds数据集上进行的实验以及在噪声条件下的推断表明，该方法与最先进的全监督深度学习模型相比，具有更高的有效性和鲁棒性。此外，消融研究也进一步验证了所提出的联合学习策略。", "conclusion": "所提出的结合一致性正则化和相似性学习的深度半监督方法，在标记数据稀缺的农业场景下，能有效且鲁棒地进行杂草分类，优于传统的全监督模型，为精准农业提供了新的解决方案。"}}
{"id": "2510.10918", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10918", "abs": "https://arxiv.org/abs/2510.10918", "authors": ["Geon Yeong Park", "Inhwa Han", "Serin Yang", "Yeobin Hong", "Seongmin Jeong", "Heechan Jeon", "Myeongjin Goh", "Sung Won Yi", "Jin Nam", "Jong Chul Ye"], "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models", "comment": null, "summary": "The exponential growth of the global makeup market has paralleled\nadvancements in virtual makeup simulation technology. Despite the progress led\nby GANs, their application still encounters significant challenges, including\ntraining instability and limited customization capabilities. Addressing these\nchallenges, we introduce DreamMakup - a novel training-free Diffusion model\nbased Makeup Customization method, leveraging the inherent advantages of\ndiffusion models for superior controllability and precise real-image editing.\nDreamMakeup employs early-stopped DDIM inversion to preserve the facial\nstructure and identity while enabling extensive customization through various\nconditioning inputs such as reference images, specific RGB colors, and textual\ndescriptions. Our model demonstrates notable improvements over existing\nGAN-based and recent diffusion-based frameworks - improved customization,\ncolor-matching capabilities, identity preservation and compatibility with\ntextual descriptions or LLMs with affordable computational costs.", "AI": {"tldr": "本文提出DreamMakup，一种无需训练的基于扩散模型的化妆定制方法，通过早期停止的DDIM反演和多条件输入，解决了GANs的训练不稳定和定制能力有限问题，实现了卓越的控制性、精确的图像编辑和身份保留。", "motivation": "全球化妆品市场的指数级增长推动了虚拟化妆模拟技术的发展。然而，现有的基于GANs的方法存在训练不稳定和定制能力有限等显著挑战，因此需要一种能提供卓越可控性和精确真实图像编辑能力的新方法。", "method": "DreamMakup是一种新颖的、无需训练的基于扩散模型的化妆定制方法。它利用扩散模型固有的优势，通过早期停止的DDIM反演来保留面部结构和身份，并通过参考图像、特定RGB颜色和文本描述等多种条件输入实现广泛的定制。", "result": "DreamMakup在定制能力、颜色匹配、身份保留以及与文本描述或大型语言模型的兼容性方面，相对于现有的基于GANs和近期基于扩散模型的框架，均展现出显著提升，且计算成本可承受。", "conclusion": "DreamMakup提供了一种创新的虚拟化妆定制解决方案，通过利用扩散模型的优势，有效克服了传统GANs的局限性，实现了更优越的可控性、定制化和身份保留，且具有良好的计算效率。"}}
{"id": "2510.10575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10575", "abs": "https://arxiv.org/abs/2510.10575", "authors": ["Zhengrong Yue", "Haiyu Zhang", "Xiangyu Zeng", "Boyu Chen", "Chenting Wang", "Shaobin Zhuang", "Lu Dong", "KunPeng Du", "Yi Wang", "Limin Wang", "Yali Wang"], "title": "UniFlow: A Unified Pixel Flow Tokenizer for Visual Understanding and Generation", "comment": null, "summary": "Tokenizer is a crucial component for both visual understanding and\ngeneration. To advance toward the ultimate goal of universal modeling, recent\nresearch has focused on developing a unified tokenizer. However, existing\ntokenizers face a significant performance trade-off between understanding and\ngeneration, stemming from the inherent conflict between high-level semantic\nabstraction and low-level pixel reconstruction. To tackle this challenge, we\npropose a generic and unified tokenizer, namely UniFlow, by flexibly adapting\nany visual encoder with a concise reconstruction decoder. Specifically, we\nintroduce layer-wise adaptive self-distillation applied to the well-pretrained\nvisual encoders, which enables UniFlow to simultaneously inherit the strong\nsemantic features for visual understanding and flexibly adapt to model\nfine-grained details for visual generation. Moreover, we propose a lightweight\npatch-wise pixel flow decoder, which efficiently achieves high-fidelity pixel\nreconstruction by modeling a conditional flow from the noisy state back to the\npatch-wise pixel domain. By leveraging the semantic features as visual\nconditions for the decoder, we effectively alleviate the training conflicts\nbetween understanding and generation. Furthermore, the patch-wise learning\nstrategy simplifies the data distribution, thereby improving training\nefficiency. Extensive experiments across 13 challenging benchmarks spanning 7\nwidely studied visual understanding and generation tasks demonstrate that\nUniFlow achieves a win-win outcome. For instance, our 7B UniFlow-XL not only\nsurpasses the 14B TokenFlow-XL by 7.75% on average understanding benchmarks,\nbut also achieves competitive results in both visual reconstruction and\ngeneration, surpassing UniTok by 0.15 in rFID and 0.09 in gFID (without\nguidance), respectively.", "AI": {"tldr": "本文提出了一种名为UniFlow的通用统一tokenizer，通过分层自适应自蒸馏和轻量级patch-wise像素流解码器，成功解决了现有统一tokenizer在视觉理解和生成之间存在的性能权衡问题，实现了双赢。", "motivation": "现有统一tokenizer在视觉理解和生成任务中存在显著的性能权衡，这源于高层语义抽象和低层像素重建之间固有的冲突。研究旨在开发一种能够同时兼顾理解和生成的通用统一tokenizer。", "method": "本文提出了UniFlow，一个通过灵活适配任何视觉编码器与简洁重建解码器实现的通用统一tokenizer。具体方法包括：1) 对预训练视觉编码器应用分层自适应自蒸馏，使其既能继承强大的语义特征用于理解，又能灵活适应细粒度细节用于生成。2) 提出一种轻量级patch-wise像素流解码器，通过建模从噪声状态到patch-wise像素域的条件流，高效实现高保真像素重建，并利用语义特征作为解码器的视觉条件，缓解理解和生成之间的训练冲突。3) 采用patch-wise学习策略，简化数据分布，提高训练效率。", "result": "UniFlow在跨越7个广泛研究的视觉理解和生成任务的13个挑战性基准测试中取得了双赢的结果。例如，7B的UniFlow-XL在平均理解基准上超越14B的TokenFlow-XL 7.75%，并且在视觉重建和生成方面也取得了有竞争力的结果，分别在rFID上超过UniTok 0.15，在gFID（无指导）上超过0.09。", "conclusion": "UniFlow通过创新的分层自适应自蒸馏和patch-wise像素流解码器，成功解决了统一tokenizer在视觉理解和生成之间的性能权衡问题。它能够同时继承强大的语义特征和灵活地适应细粒度细节，从而在广泛的视觉任务中实现卓越的理解和生成能力。"}}
{"id": "2510.10577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10577", "abs": "https://arxiv.org/abs/2510.10577", "authors": ["Haonan Wang", "Hanyu Zhou", "Haoyue Liu", "Luxin Yan"], "title": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes", "comment": null, "summary": "Optical flow estimation has achieved promising results in conventional scenes\nbut faces challenges in high-speed and low-light scenes, which suffer from\nmotion blur and insufficient illumination. These conditions lead to weakened\ntexture and amplified noise and deteriorate the appearance saturation and\nboundary completeness of frame cameras, which are necessary for motion feature\nmatching. In degraded scenes, the frame camera provides dense appearance\nsaturation but sparse boundary completeness due to its long imaging time and\nlow dynamic range. In contrast, the event camera offers sparse appearance\nsaturation, while its short imaging time and high dynamic range gives rise to\ndense boundary completeness. Traditionally, existing methods utilize feature\nfusion or domain adaptation to introduce event to improve boundary\ncompleteness. However, the appearance features are still deteriorated, which\nseverely affects the mostly adopted discriminative models that learn the\nmapping from visual features to motion fields and generative models that\ngenerate motion fields based on given visual features. So we introduce\ndiffusion models that learn the mapping from noising flow to clear flow, which\nis not affected by the deteriorated visual features. Therefore, we propose a\nnovel optical flow estimation framework Diff-ABFlow based on diffusion models\nwith frame-event appearance-boundary fusion.", "AI": {"tldr": "本文提出了一种名为Diff-ABFlow的新型光流估计框架，该框架结合了扩散模型和帧-事件外观-边界融合，旨在解决高速和低光照场景下光流估计因运动模糊和光照不足导致视觉特征恶化的问题。", "motivation": "传统光流估计方法在高速和低光照场景中面临挑战，这些场景会导致运动模糊、光照不足、纹理弱化和噪声放大，进而影响帧相机的外观饱和度和边界完整性。帧相机提供密集的出现饱和度但边界稀疏，而事件相机提供稀疏的出现饱和度但边界密集。现有方法通过特征融合或域适应引入事件来改善边界完整性，但外观特征的恶化仍然严重影响了判别模型和生成模型。", "method": "本文提出Diff-ABFlow框架，利用扩散模型来学习从噪声流到清晰流的映射，这种方法不受恶化视觉特征的影响。同时，该框架融合了帧相机和事件相机的外观与边界信息，以利用它们互补的优势。", "result": "本文提出了一种基于扩散模型并结合帧-事件外观-边界融合的新型光流估计框架Diff-ABFlow，以应对恶劣场景下的光流估计挑战。", "conclusion": "通过引入扩散模型和帧-事件外观-边界融合，Diff-ABFlow框架能够有效解决高速和低光照场景下因视觉特征恶化导致的光流估计问题，为鲁棒光流估计提供了一种新颖的解决方案。"}}
{"id": "2510.10921", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10921", "abs": "https://arxiv.org/abs/2510.10921", "authors": ["Chunyu Xie", "Bin Wang", "Fanjing Kong", "Jincheng Li", "Dawei Liang", "Ji Ao", "Dawei Leng", "Yuhui Yin"], "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model", "comment": null, "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.", "AI": {"tldr": "本文介绍了FG-CLIP 2，一个双语（英汉）视觉-语言模型，旨在通过精细粒度监督、多判别目标和文本内模态对比损失，提升精细粒度对齐能力，并在双语任务上取得了最先进的性能。", "motivation": "现有模型（如CLIP）在全局对齐上表现良好，但在捕获对象属性、空间关系和语言表达的精细粒度细节方面存在局限，尤其是在非英语环境下，且双语支持有限。需要一个能处理英汉精细粒度对齐的模型。", "method": "引入FG-CLIP 2模型，通过结合区域-文本匹配和长文本建模等丰富的精细粒度监督，以及多个判别目标进行训练。此外，提出了文本内模态对比（TIC）损失以更好地区分语义相似的文本。模型在精心策划的大规模英汉混合数据集上进行训练，并发布了一个新的中文多模态理解基准进行评估。", "result": "FG-CLIP 2在8个任务的29个数据集上进行了广泛实验，结果表明它超越了现有方法，在英语和中文两种语言中均取得了最先进的（SOTA）性能。", "conclusion": "FG-CLIP 2成功地推动了双语精细粒度视觉-语言对齐的进展，并在英汉两种语言中实现了SOTA性能。作者发布了模型、代码和基准，以促进该领域的未来研究。"}}
{"id": "2510.10890", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10890", "abs": "https://arxiv.org/abs/2510.10890", "authors": ["Yu Chao", "Siyu Lin", "xiaorong wang", "Zhu Zhang", "Zihan Zhou", "Haoyu Wang", "Shuo Wang", "Jie Zhou", "Zhiyuan Liu", "Maosong Sun"], "title": "LLM$\\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System", "comment": "Accepted by EMNLP2025 System Demonstration", "summary": "We introduce LLM x MapReduce-V3, a hierarchically modular agent system\ndesigned for long-form survey generation. Building on the prior work, LLM x\nMapReduce-V2, this version incorporates a multi-agent architecture where\nindividual functional components, such as skeleton initialization, digest\nconstruction, and skeleton refinement, are implemented as independent\nmodel-context-protocol (MCP) servers. These atomic servers can be aggregated\ninto higher-level servers, creating a hierarchically structured system. A\nhigh-level planner agent dynamically orchestrates the workflow by selecting\nappropriate modules based on their MCP tool descriptions and the execution\nhistory. This modular decomposition facilitates human-in-the-loop intervention,\naffording users greater control and customization over the research process.\nThrough a multi-turn interaction, the system precisely captures the intended\nresearch perspectives to generate a comprehensive skeleton, which is then\ndeveloped into an in-depth survey. Human evaluations demonstrate that our\nsystem surpasses representative baselines in both content depth and length,\nhighlighting the strength of MCP-based modular planning.", "AI": {"tldr": "本文介绍了LLM x MapReduce-V3，一个用于生成长篇综述的层级模块化智能体系统，它采用MCP（Model-Context-Protocol）服务器实现多智能体架构和动态规划，在内容深度和长度上超越了基线。", "motivation": "研究动机是有效生成长篇综述，并在前一版本（LLM x MapReduce-V2）的基础上进行改进，特别是通过引入模块化和人机协作来提高综述的生成质量和用户控制力。", "method": "该系统采用层级模块化智能体架构，将骨架初始化、摘要构建和骨架细化等功能组件实现为独立的MCP服务器。这些原子服务器可聚合为更高级别的服务器，形成层级结构。一个高级规划智能体根据MCP工具描述和执行历史动态编排工作流，通过多轮交互捕获研究视角，生成综合骨架，并最终发展为深入综述。该方法还支持人机协作干预。", "result": "人体评估结果表明，该系统在内容深度和长度方面均超越了代表性基线，突显了基于MCP的模块化规划的优势。", "conclusion": "LLM x MapReduce-V3系统通过其基于MCP的模块化和层级架构，能有效生成长篇综述，提供卓越的内容深度和长度，并支持人机干预，从而在综述生成方面表现出色。"}}
{"id": "2510.10584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10584", "abs": "https://arxiv.org/abs/2510.10584", "authors": ["Shizhen Zhao", "Jiahui Liu", "Xin Wen", "Haoru Tan", "Xiaojuan Qi"], "title": "Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection", "comment": null, "summary": "Pre-trained vision foundation models have transformed many computer vision\ntasks. Despite their strong ability to learn discriminative and generalizable\nfeatures crucial for out-of-distribution (OOD) detection, their impact on this\ntask remains underexplored. Motivated by this gap, we systematically\ninvestigate representative vision foundation models for OOD detection. Our\nfindings reveal that a pre-trained DINOv2 model, even without fine-tuning on\nin-domain (ID) data, naturally provides a highly discriminative feature space\nfor OOD detection, achieving performance comparable to existing\nstate-of-the-art methods without requiring complex designs. Beyond this, we\nexplore how fine-tuning foundation models on in-domain (ID) data can enhance\nOOD detection. However, we observe that the performance of vision foundation\nmodels remains unsatisfactory in scenarios with a large semantic space. This is\ndue to the increased complexity of decision boundaries as the number of\ncategories grows, which complicates the optimization process. To mitigate this,\nwe propose the Mixture of Feature Experts (MoFE) module, which partitions\nfeatures into subspaces, effectively capturing complex data distributions and\nrefining decision boundaries. Further, we introduce a Dynamic-$\\beta$ Mixup\nstrategy, which samples interpolation weights from a dynamic beta distribution.\nThis adapts to varying levels of learning difficulty across categories,\nimproving feature learning for more challenging categories. Extensive\nexperiments demonstrate the effectiveness of our approach, significantly\noutperforming baseline methods.", "AI": {"tldr": "本文系统性地研究了预训练视觉基础模型在OOD检测中的应用，发现DINOv2表现优异但在大语义空间下仍有不足。为此，提出了特征专家混合（MoFE）模块和动态-$\beta$ Mixup策略，显著提升了OOD检测性能。", "motivation": "尽管预训练视觉基础模型在学习判别性和泛化性特征方面能力强大，但它们对OOD检测任务的影响尚未得到充分探索。此外，这些模型在大语义空间场景下表现不佳，需要进一步改进。", "method": "本文首先系统性地研究了代表性的视觉基础模型在OOD检测中的表现。为解决大语义空间下的性能不足，提出了以下方法：1. 特征专家混合（MoFE）模块，将特征划分为子空间以捕获复杂数据分布并优化决策边界。2. 动态-$\beta$ Mixup策略，从动态Beta分布中采样插值权重，以适应不同类别的学习难度，改善对挑战性类别的特征学习。", "result": "研究发现，预训练的DINOv2模型即使未经域内数据微调，也能为OOD检测提供高度判别性的特征空间，性能可与现有最先进方法媲美。然而，视觉基础模型在大语义空间场景下的性能仍不尽如人意。所提出的MoFE模块和动态-$\beta$ Mixup策略显著优于基线方法，有效提升了OOD检测性能。", "conclusion": "预训练视觉基础模型在OOD检测中具有巨大潜力，特别是DINOv2。但在大语义空间下，仍需特定策略来克服复杂性。通过MoFE模块和动态-$\beta$ Mixup策略，可以有效提升视觉基础模型在OOD检测任务中的性能，尤其是在处理挑战性类别时。"}}
{"id": "2510.10930", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10930", "abs": "https://arxiv.org/abs/2510.10930", "authors": ["Katherine M. Collins", "Cedegao E. Zhang", "Graham Todd", "Lance Ying", "Mauricio Barba da Costa", "Ryan Liu", "Prafull Sharma", "Adrian Weller", "Ionatan Kuperwajs", "Lionel Wong", "Joshua B. Tenenbaum", "Thomas L. Griffiths"], "title": "Evaluating Language Models' Evaluations of Games", "comment": "Pre-print", "summary": "Reasoning is not just about solving problems -- it is also about evaluating\nwhich problems are worth solving at all. Evaluations of artificial intelligence\n(AI) systems primarily focused on problem solving, historically by studying how\nmodels play games such as chess and Go. In this paper, we advocate for a new\nparadigm that assesses AI systems' evaluation of games. First, we introduce a\nformalism for evaluating such evaluations. We then leverage a large-scale\ndataset of over $100$ novel board games and over 450 human judgments to compare\nevaluations produced by modern language and reasoning models against those of\npeople and symbolic computational agents. We consider two kinds of evaluative\nqueries: assessing the payoff (or fairness) and the funness of games. These\nqueries span two dimensions relevant to the design of evaluations of AI\nevaluations: how complex a query is to compute and how difficult a query is to\nquantify. Our results show that reasoning models are generally more aligned to\npeople in their evaluations of games than non-reasoning language models.\nHowever, we observe a non-monotonic relationship: as models get closer to\ngame-theoretic optimal, their fit to human data weakens. We also observe more\n\"jaggedness\" across models for assessing funness, in line with the greater\ndifficulty of quantifying this query. Across queries and games, reasoning\nmodels show highly variable and unpredictable resource usage when assessing\nqueries, pointing to the importance of imbuing more resource-rational\nmeta-reasoning in language and reasoning models.", "AI": {"tldr": "本文提出了一种评估AI系统“评估能力”的新范式，而非仅仅关注其问题解决能力。通过比较AI模型对100多种新棋盘游戏的“回报/公平性”和“趣味性”评估与人类判断，发现推理模型更接近人类，但与博弈论最优越接近，与人类数据的拟合度越弱。此外，推理模型在评估过程中资源使用高度可变。", "motivation": "传统的AI系统评估主要集中于问题解决能力（如国际象棋和围棋），但推理不仅包括解决问题，还包括评估哪些问题值得解决。因此，研究人员希望引入一种新的范式，评估AI系统对游戏的评估能力。", "method": "1. 引入了一种评估评估的形式主义。2. 利用包含100多个新棋盘游戏和450多个人类判断的大规模数据集。3. 将现代语言和推理模型产生的评估与人类以及符号计算代理的评估进行比较。4. 考虑两种评估查询：评估游戏的“回报（或公平性）”和“趣味性”，这些查询在计算复杂度和量化难度上有所不同。", "result": "1. 推理模型在游戏评估上通常比非推理语言模型更符合人类判断。2. 观察到一种非单调关系：模型越接近博弈论最优，其与人类数据的拟合度越弱。3. 在评估“趣味性”时，模型之间表现出更大的“锯齿状”差异，这与该查询更难量化相符。4. 跨查询和游戏，推理模型在评估查询时显示出高度可变且不可预测的资源使用。", "conclusion": "当前AI评估需要将重点转移到其评估能力上。AI模型需要注入更具资源理性（resource-rational）的元推理能力，以提高其评估能力，尤其是在评估如“趣味性”等难以量化的方面。"}}
{"id": "2510.10913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10913", "abs": "https://arxiv.org/abs/2510.10913", "authors": ["Ki Jung Seo", "Sehun Lim", "Taeuk Kim"], "title": "ADVICE: Answer-Dependent Verbalized Confidence Estimation", "comment": null, "summary": "Recent progress in large language models (LLMs) has enabled them to express\ntheir confidence in natural language, enhancing transparency and reliability.\nHowever, their confidence often exhibits overconfidence, the cause of which\nremains poorly understood. In this work, we conduct a detailed analysis of the\ndynamics underlying verbalized confidence and identify answer-independence as a\nkey factor, defined as the model's failure to condition confidence on its own\nanswer. To address this, we propose ADVICE (Answer-Dependent Verbalized\nConfidence Estimation), a fine-tuning framework that facilitates\nanswer-grounded confidence estimation. Extensive experiments show that ADVICE\nsubstantially improves confidence calibration while preserving task\nperformance. Further analyses confirm that ADVICE strengthens\nanswer-groundedness, leading to more balanced and well-calibrated confidence\ndistributions. Our findings shed light on the origin of overconfidence and\nestablish a framework for more trustworthy confidence verbalization.", "AI": {"tldr": "本文分析了大型语言模型（LLMs）过度自信的原因，发现其信心表达与答案独立有关。为此，作者提出了ADVICE微调框架，显著改善了LLMs的信心校准，同时保持了任务性能。", "motivation": "大型语言模型（LLMs）能够用自然语言表达自信，提高了透明度和可靠性。然而，它们经常表现出过度自信，其原因尚不清楚，这促使研究人员深入探讨。", "method": "研究人员对口头化信心的动态进行了详细分析，并将“答案独立性”（即模型未能根据自身答案调整信心）识别为一个关键因素。为解决此问题，他们提出了ADVICE（Answer-Dependent Verbalized Confidence Estimation）微调框架，旨在促进基于答案的信心评估。", "result": "广泛的实验表明，ADVICE显著改善了信心校准，同时保持了任务性能。进一步分析证实，ADVICE增强了答案的根植性，从而产生了更平衡、校准更好的信心分布。", "conclusion": "本研究揭示了过度自信的起源，并建立了一个用于更值得信赖的信心口头化表达的框架，提高了LLMs的可靠性。"}}
{"id": "2510.10961", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10961", "abs": "https://arxiv.org/abs/2510.10961", "authors": ["Yejin Lee", "Su-Hyeon Kim", "Hyundong Jin", "Dayoung Kim", "Yeonsoo Kim", "Yo-Sub Han"], "title": "KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification", "comment": "25 pages, 5 figures, 25 tables", "summary": "Toxic content has become an increasingly critical social issue with the rapid\nexpansion of online communication. While numerous studies explored methods for\ndetecting and detoxifying such content, most have focused primarily on English,\nleaving low-resource language underrepresented. Consequently, Large Language\nModels~(LLMs) often struggle to identify and neutralize toxic expressions in\nthese languages. This challenge becomes even more pronounced when user employ\nobfuscation techniques to evade detection systems. Therefore, we propose a\n\\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to\naddress this issue. We categorize various obfuscation approaches based on\nlinguistic characteristics of Korean and define a set of transformation rules\ngrounded in real-word examples. Using these rules, we construct three dataset\nversions (easy, normal, and hard) representing different levels of obfuscation\ndifficulty. This is the first dataset that simultaneously supports\ndeobfuscation and detoxification for the Korean language. We expect it to\nfacilitate better understanding and mitigating of obfuscated toxic content in\nLLM for low-resource languages. Our code and data are available at\nhttps://github.com/leeyejin1231/KOTOX.", "AI": {"tldr": "该研究提出了KOTOX，一个针对韩语的去混淆和去毒化数据集，以解决大型语言模型在低资源语言中处理混淆毒性内容的问题。", "motivation": "随着在线交流的迅速扩展，毒性内容已成为一个日益严重的社会问题。现有研究多集中于英语，导致低资源语言（如韩语）的毒性内容检测和去毒化不足，尤其是在用户使用混淆技术规避检测时，大型语言模型（LLMs）在这方面表现不佳。", "method": "提出了KOTOX：一个韩语毒性数据集。该方法基于韩语的语言特征对混淆方法进行分类，并根据真实世界示例定义了一套转换规则。利用这些规则，构建了三个不同混淆难度级别的数据集版本（简单、中等、困难）。", "result": "创建了KOTOX数据集，这是第一个同时支持韩语去混淆和去毒化的数据集。该数据集包含三个不同难度的版本。", "conclusion": "该研究期望KOTOX数据集能够促进大型语言模型更好地理解和缓解低资源语言中混淆的毒性内容。"}}
{"id": "2510.10587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10587", "abs": "https://arxiv.org/abs/2510.10587", "authors": ["Jingchao Wang", "Wenlong Zhang", "Dingjiang Huang", "Hong Wang", "Yefeng Zheng"], "title": "A Simple and Better Baseline for Visual Grounding", "comment": "ICME2025", "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.", "AI": {"tldr": "本文提出了一种名为FSVG的视觉定位基线方法，通过直接整合语言和视觉模态，并引入基于相似度的特征选择机制，实现了在准确性和效率之间更好的平衡，解决了现有迭代方法的计算开销问题。", "motivation": "现有的视觉定位方法，尤其是在选择语言相关视觉区域以减少计算开销的研究中，通常采用迭代方式，并在每次迭代中缓存语言和视觉特征，这会带来额外的计算开销，不利于实现和部署。", "method": "本文提出FSVG（Feature Selection-based Visual Grounding）方法。具体而言，它直接将语言和视觉模态封装到一个整体网络架构中，避免了复杂的迭代过程。同时，并行利用语言作为指导，促进语言模态和视觉模态之间的交互，以提取有效的视觉特征。此外，在视觉特征学习过程中引入了基于相似度的特征选择机制，仅利用与语言相关的视觉特征进行预测，从而降低了计算成本。", "result": "在多个基准数据集上进行的广泛实验全面证实，所提出的FSVG在准确性和效率之间取得了比现有最先进方法更好的平衡。", "conclusion": "FSVG是一种简单而有效的视觉定位基线，通过非迭代的模态整合和基于相似度的特征选择，成功解决了现有方法的计算开销问题，并在准确性和效率之间实现了更好的权衡。"}}
{"id": "2510.10606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10606", "abs": "https://arxiv.org/abs/2510.10606", "authors": ["Yuqi Liu", "Liangyu Chen", "Jiazhen Liu", "Mingkang Zhu", "Zhisheng Zhong", "Bei Yu", "Jiaya Jia"], "title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models", "comment": null, "summary": "Typical post-training paradigms for Large Vision-and-Language Models (LVLMs)\ninclude Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable\nRewards (RLVR). SFT leverages external guidance to inject new knowledge,\nwhereas RLVR utilizes internal reinforcement to enhance reasoning capabilities\nand overall performance. However, our analysis reveals that SFT often leads to\nsub-optimal performance, while RLVR struggles with tasks that exceed the\nmodel's internal knowledge base. To address these limitations, we propose\nViSurf (\\textbf{Vi}sual \\textbf{Su}pervised-and-\\textbf{R}einforcement\n\\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the\nstrengths of both SFT and RLVR within a single stage. We analyze the derivation\nof the SFT and RLVR objectives to establish the ViSurf objective, providing a\nunified perspective on these two paradigms. The core of ViSurf involves\ninjecting ground-truth labels into the RLVR rollouts, thereby providing\nsimultaneous external supervision and internal reinforcement. Furthermore, we\nintroduce three novel reward control strategies to stabilize and optimize the\ntraining process. Extensive experiments across several diverse benchmarks\ndemonstrate the effectiveness of ViSurf, outperforming both individual SFT,\nRLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates\nthese findings, validating the derivation and design principles of ViSurf.", "AI": {"tldr": "ViSurf是一种统一的后训练范式，它将监督微调（SFT）和基于可验证奖励的强化学习（RLVR）的优势整合到单一阶段，以解决现有LVLM训练方法的局限性。", "motivation": "现有的LVLM后训练范式存在局限性：SFT常导致次优性能，而RLVR在超出模型内部知识的任务上表现不佳。这促使研究者寻求一种能结合两者优势的新方法。", "method": "本文提出了ViSurf（Visual Supervised-and-Reinforcement Fine-Tuning），通过将SFT和RLVR目标进行统一推导，形成ViSurf目标。其核心是在RLVR的rollouts中注入真实标签，同时提供外部监督和内部强化。此外，还引入了三种新颖的奖励控制策略来稳定和优化训练过程。", "result": "ViSurf在多个不同基准测试中表现出卓越的有效性，其性能优于单独的SFT、RLVR以及两阶段的SFT → RLVR方法。", "conclusion": "ViSurf作为一种统一的后训练范式，通过整合SFT和RLVR的优势，显著提升了LVLM的性能，并通过深入分析验证了其推导和设计原则。"}}
{"id": "2510.10927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10927", "abs": "https://arxiv.org/abs/2510.10927", "authors": ["Yawen Yang", "Fukun Ma", "Shiao Meng", "Aiwei Liu", "Lijie Wen"], "title": "GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition", "comment": "Accepted by IJCNN 2025", "summary": "In biomedical fields, one named entity may consist of a series of\nnon-adjacent tokens and overlap with other entities. Previous methods recognize\ndiscontinuous entities by connecting entity fragments or internal tokens, which\nface challenges of error propagation and decoding ambiguity due to the wide\nvariety of span or word combinations. To address these issues, we deeply\nexplore discontinuous entity structures and propose an effective Gap-aware grid\ntagging model for Discontinuous Named Entity Recognition, named GapDNER. Our\nGapDNER innovatively applies representation learning on the context gaps\nbetween entity fragments to resolve decoding ambiguity and enhance\ndiscontinuous NER performance. Specifically, we treat the context gap as an\nadditional type of span and convert span classification into a token-pair grid\ntagging task. Subsequently, we design two interactive components to\ncomprehensively model token-pair grid features from both intra- and inter-span\nperspectives. The intra-span regularity extraction module employs the biaffine\nmechanism along with linear attention to capture the internal regularity of\neach span, while the inter-span relation enhancement module utilizes\ncriss-cross attention to obtain semantic relations among different spans. At\nthe inference stage of entity decoding, we assign a directed edge to each\nentity fragment and context gap, then use the BFS algorithm to search for all\nvalid paths from the head to tail of grids with entity tags. Experimental\nresults on three datasets demonstrate that our GapDNER achieves new\nstate-of-the-art performance on discontinuous NER and exhibits remarkable\nadvantages in recognizing complex entity structures.", "AI": {"tldr": "本文提出了一种名为GapDNER的间断命名实体识别模型，通过对实体片段之间的上下文间隙进行表示学习，并结合网格标注和交互式组件，有效解决了间断实体识别中的解码歧义和复杂结构识别问题。", "motivation": "生物医学领域中，命名实体可能由非相邻的词元组成并与其他实体重叠。现有方法通过连接实体片段或内部词元来识别间断实体，但面临错误传播和解码歧义的挑战，尤其是在处理多样化的跨度或词组合时。", "method": "GapDNER模型深入探索间断实体结构，并提出了一种有效的间隙感知网格标注方法。它创新性地将表示学习应用于实体片段之间的上下文间隙，以解决解码歧义并提高性能。具体而言，它将上下文间隙视为一种额外的跨度类型，并将跨度分类转换为词元对网格标注任务。随后，设计了两个交互式组件：跨度内规律提取模块（使用双仿射机制和线性注意力捕捉每个跨度的内部规律）和跨度间关系增强模块（利用交叉注意力获取不同跨度之间的语义关系）。在实体解码推理阶段，为每个实体片段和上下文间隙分配有向边，然后使用BFS算法搜索所有从网格头部到尾部带有实体标签的有效路径。", "result": "在三个数据集上的实验结果表明，GapDNER在间断命名实体识别上取得了新的最先进性能，并在识别复杂实体结构方面展现出显著优势。", "conclusion": "GapDNER通过创新性地利用上下文间隙的表示学习和交互式网格标注机制，有效解决了间断命名实体识别中的解码歧义和复杂结构识别难题，达到了领先的识别性能。"}}
{"id": "2510.10609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10609", "abs": "https://arxiv.org/abs/2510.10609", "authors": ["Yiting Lu", "Fengbin Guan", "Yixin Gao", "Yan Zhong", "Xinge Peng", "Jiakang Yuan", "Yihao Liu", "Bo Zhang", "Xin Li", "Zhibo Chen", "Weisi Lin"], "title": "OmniQuality-R: Advancing Reward Models Through All-Encompassing Quality Assessment", "comment": null, "summary": "Current visual evaluation approaches are typically constrained to a single\ntask. To address this, we propose OmniQuality-R, a unified reward modeling\nframework that transforms multi-task quality reasoning into continuous and\ninterpretable reward signals for policy optimization. Inspired by subjective\nexperiments, where participants are given task-specific instructions outlining\ndistinct assessment principles prior to evaluation, we propose OmniQuality-R, a\nstructured reward modeling framework that transforms multi-dimensional\nreasoning into continuous and interpretable reward signals. To enable this, we\nconstruct a reasoning-enhanced reward modeling dataset by sampling informative\nplan-reason trajectories via rejection sampling, forming a reliable\nchain-of-thought (CoT) dataset for supervised fine-tuning (SFT). Building on\nthis, we apply Group Relative Policy Optimization (GRPO) for post-training,\nusing a Gaussian-based reward to support continuous score prediction. To\nfurther stabilize the training and improve downstream generalization, we\nincorporate standard deviation (STD) filtering and entropy gating mechanisms\nduring reinforcement learning. These techniques suppress unstable updates and\nreduce variance in policy optimization. We evaluate OmniQuality-R on three key\nIQA tasks: aesthetic quality assessment, technical quality evaluation, and\ntext-image alignment.", "AI": {"tldr": "OmniQuality-R是一个统一的奖励建模框架，旨在将多任务视觉质量评估转化为连续且可解释的奖励信号，以优化策略。", "motivation": "当前的视觉评估方法通常局限于单一任务，缺乏一个能够处理多任务质量推理的统一框架。", "method": "本文提出了OmniQuality-R框架，灵感来源于主观实验中的任务特定评估原则。它通过拒绝采样构建了一个推理增强型奖励建模数据集（CoT数据集）用于SFT。随后，应用Group Relative Policy Optimization (GRPO) 进行后训练，并使用基于高斯的奖励进行连续分数预测。为稳定训练和提高泛化能力，在强化学习过程中加入了标准差（STD）过滤和熵门控机制。", "result": "OmniQuality-R在美学质量评估、技术质量评估和文本-图像对齐这三个关键的图像质量评估（IQA）任务上进行了评估。", "conclusion": "OmniQuality-R提供了一个统一、可解释的框架，能够将多任务质量推理转化为连续的奖励信号，有效优化策略，并提高了训练的稳定性和泛化能力。"}}
{"id": "2510.10971", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10971", "abs": "https://arxiv.org/abs/2510.10971", "authors": ["Yejin Lee", "Hyeseon Ahn", "Yo-Sub Han"], "title": "RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection", "comment": "10 pages, 9 figures, 12 tables", "summary": "Hate speech remains prevalent in human society and continues to evolve in its\nforms and expressions. Modern advancements in internet and online anonymity\naccelerate its rapid spread and complicate its detection. However, hate speech\ndatasets exhibit diverse characteristics primarily because they are constructed\nfrom different sources and platforms, each reflecting different linguistic\nstyles and social contexts. Despite this diversity, prior studies on hate\nspeech detection often rely on fixed methodologies without adapting to\ndata-specific features. We introduce RV-HATE, a detection framework designed to\naccount for the dataset-specific characteristics of each hate speech dataset.\nRV-HATE consists of multiple specialized modules, where each module focuses on\ndistinct linguistic or contextual features of hate speech. The framework\nemploys reinforcement learning to optimize weights that determine the\ncontribution of each module for a given dataset. A voting mechanism then\naggregates the module outputs to produce the final decision. RV-HATE offers two\nprimary advantages: (1)~it improves detection accuracy by tailoring the\ndetection process to dataset-specific attributes, and (2)~it also provides\ninterpretable insights into the distinctive features of each dataset.\nConsequently, our approach effectively addresses implicit hate speech and\nachieves superior performance compared to conventional static methods. Our code\nis available at https://github.com/leeyejin1231/RV-HATE.", "AI": {"tldr": "RV-HATE是一个仇恨言论检测框架，它利用强化学习根据数据集的特定特征自适应地调整检测模块的贡献，从而提高检测准确性并提供可解释的洞察。", "motivation": "仇恨言论形式多样且不断演变，在互联网上迅速传播，难以检测。现有的仇恨言论数据集因来源和平台不同而呈现出多样化的语言风格和社会背景。然而，以往的检测研究通常依赖固定方法，未能适应这些数据特有的特征，尤其难以有效处理隐性仇恨言论。", "method": "RV-HATE框架包含多个专门模块，每个模块专注于仇恨言论的不同语言或上下文特征。它采用强化学习来优化权重，以确定每个模块对给定数据集的贡献。最终，通过投票机制整合各模块的输出以产生最终决策。", "result": "RV-HATE通过根据数据集特定属性调整检测过程，提高了检测准确性。它还为每个数据集的独特特征提供了可解释的见解。该方法有效地解决了隐性仇恨言论问题，并比传统静态方法取得了更优越的性能。", "conclusion": "RV-HATE通过其自适应的、模块化的方法，有效应对了仇恨言论数据集的多样性挑战，显著提升了检测精度和可解释性，尤其在处理隐性仇恨言论方面表现出色。"}}
{"id": "2510.10936", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10936", "abs": "https://arxiv.org/abs/2510.10936", "authors": ["Anirudh Ganesh", "Jayavardhan Reddy"], "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study", "comment": null, "summary": "We present a reproducibility study of the state-of-the-art neural\narchitecture for sequence labeling proposed by Ma and Hovy\n(2016)\\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines\ncharacter-level representations via Convolutional Neural Networks (CNNs),\nword-level context modeling through Bi-directional Long Short-Term Memory\nnetworks (BiLSTMs), and structured prediction using Conditional Random Fields\n(CRFs). This end-to-end approach eliminates the need for hand-crafted features\nwhile achieving excellent performance on named entity recognition (NER) and\npart-of-speech (POS) tagging tasks. Our implementation successfully reproduces\nthe key results, achieving 91.18\\% F1-score on CoNLL-2003 NER and demonstrating\nthe model's effectiveness across sequence labeling tasks. We provide a detailed\nanalysis of the architecture components and release an open-source PyTorch\nimplementation to facilitate further research.", "AI": {"tldr": "本文对Ma和Hovy (2016)提出的BiLSTM-CNN-CRF序列标注模型进行了复现性研究，成功重现了其在命名实体识别（NER）等任务上的卓越性能，并发布了开源PyTorch实现。", "motivation": "Ma和Hovy (2016)提出的BiLSTM-CNN-CRF模型是一种端到端方法，无需人工设计特征即可在命名实体识别（NER）和词性标注（POS）等序列标注任务上取得优异性能，因此对其进行复现性研究和深入分析具有重要价值。", "method": "本文复现了Ma和Hovy (2016)提出的BiLSTM-CNN-CRF模型。该模型结合了卷积神经网络（CNN）提取字符级表示、双向长短期记忆网络（BiLSTM）进行词级上下文建模，以及条件随机场（CRF）进行结构化预测。研究者使用PyTorch实现了该模型。", "result": "研究成功复现了关键结果，在CoNLL-2003 NER任务上取得了91.18%的F1分数，并证明了该模型在各种序列标注任务中的有效性。", "conclusion": "Ma和Hovy (2016)提出的BiLSTM-CNN-CRF模型是有效且可复现的。本文提供了对架构组件的详细分析，并发布了一个开源的PyTorch实现，以促进未来的研究。"}}
{"id": "2510.10951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10951", "abs": "https://arxiv.org/abs/2510.10951", "authors": ["Eitan Klinger", "Vivaan Wadhwa", "Jungyeul Park"], "title": "Punctuation-aware treebank tree binarization", "comment": null, "summary": "This article presents a curated resource and evaluation suite for\npunctuation-aware treebank binarization. Standard binarization pipelines drop\npunctuation before head selection, which alters constituent shape and harms\nhead-child identification. We release (1) a reproducible pipeline that\npreserves punctuation as sibling nodes prior to binarization, (2) derived\nartifacts and metadata (intermediate @X markers, reversibility signatures,\nalignment indices), and (3) an accompanying evaluation suite covering\nhead-child prediction, round-trip reversibility, and structural compatibility\nwith derivational resources (CCGbank). On the Penn Treebank, punctuation-aware\npreprocessing improves head prediction accuracy from 73.66\\% (Collins rules)\nand 86.66\\% (MLP) to 91.85\\% with the same classifier, and achieves competitive\nalignment against CCGbank derivations. All code, configuration files, and\ndocumentation are released to enable replication and extension to other\ncorpora.", "AI": {"tldr": "本文提出了一种标点符号感知的树库二值化资源和评估套件，通过保留标点符号作为兄弟节点，显著提高了头部预测准确性，并改善了与衍生资源的结构兼容性。", "motivation": "标准的二值化流程在选择头部之前会丢弃标点符号，这会改变成分的形状并损害头部-子节点识别，导致准确性下降。", "method": "本文发布了一个可重现的流水线，该流水线在二值化之前将标点符号保留为兄弟节点。同时，还提供了派生工件和元数据（如中间@X标记、可逆性签名、对齐索引），以及一个评估套件，涵盖头部-子节点预测、往返可逆性以及与衍生资源（如CCGbank）的结构兼容性。", "result": "在宾夕法尼亚树库上，标点符号感知的预处理将头部预测准确率从73.66%（Collins规则）和86.66%（MLP）提高到使用相同分类器的91.85%。此外，它还实现了与CCGbank衍生的竞争性对齐。所有代码、配置文件和文档均已发布，以方便复制和扩展到其他语料库。", "conclusion": "通过在树库二值化过程中保留标点符号，可以显著提高头部预测的准确性并改善与衍生资源的结构兼容性，为自然语言处理领域提供了一个有价值的资源。"}}
{"id": "2510.10631", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10631", "abs": "https://arxiv.org/abs/2510.10631", "authors": ["Zhaolin Hu", "Kun Li", "Hehe Fan", "Yi Yang"], "title": "GraphTARIF: Linear Graph Transformer with Augmented Rank and Improved Focus", "comment": null, "summary": "Linear attention mechanisms have emerged as efficient alternatives to full\nself-attention in Graph Transformers, offering linear time complexity. However,\nexisting linear attention models often suffer from a significant drop in\nexpressiveness due to low-rank projection structures and overly uniform\nattention distributions. We theoretically prove that these properties reduce\nthe class separability of node representations, limiting the model's\nclassification ability. To address this, we propose a novel hybrid framework\nthat enhances both the rank and focus of attention. Specifically, we enhance\nlinear attention by attaching a gated local graph network branch to the value\nmatrix, thereby increasing the rank of the resulting attention map.\nFurthermore, to alleviate the excessive smoothing effect inherent in linear\nattention, we introduce a learnable log-power function into the attention\nscores to reduce entropy and sharpen focus. We theoretically show that this\nfunction decreases entropy in the attention distribution, enhancing the\nseparability of learned embeddings. Extensive experiments on both homophilic\nand heterophilic graph benchmarks demonstrate that our method achieves\ncompetitive performance while preserving the scalability of linear attention.", "AI": {"tldr": "本文提出一种混合框架，通过引入门控局部图网络分支增加注意力秩，并使用可学习对数幂函数锐化注意力焦点，从而提升图Transformer中线性注意力的表达能力，同时保持其可扩展性。", "motivation": "图Transformer中的线性注意力机制虽然高效，但因低秩投影和过度均匀的注意力分布而导致表达能力显著下降，从而限制了节点表示的类别可分离性和模型的分类能力。", "method": "提出一种混合框架：1. 通过在值矩阵上附加一个门控局部图网络分支来增加注意力图的秩。2. 引入一个可学习的对数幂函数到注意力分数中，以降低熵并锐化焦点，从而减轻线性注意力固有的过度平滑效应。理论上证明这些方法能提高学习嵌入的可分离性。", "result": "在同质图和异质图基准测试上，本文方法实现了有竞争力的性能，同时保留了线性注意力的可扩展性。", "conclusion": "通过增强注意力秩和焦点，所提出的混合框架有效解决了线性注意力机制的表达能力限制，在保持效率的同时提高了图Transformer的性能。"}}
{"id": "2510.10991", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10991", "abs": "https://arxiv.org/abs/2510.10991", "authors": ["Huanjin Yao", "Ruifei Zhang", "Jiaxing Huang", "Jingyi Zhang", "Yibo Wang", "Bo Fang", "Ruolin Zhu", "Yongcheng Jing", "Shunyu Liu", "Guanbin Li", "Dacheng Tao"], "title": "A Survey on Agentic Multimodal Large Language Models", "comment": null, "summary": "With the recent emergence of revolutionary autonomous agentic systems,\nresearch community is witnessing a significant shift from traditional static,\npassive, and domain-specific AI agents toward more dynamic, proactive, and\ngeneralizable agentic AI. Motivated by the growing interest in agentic AI and\nits potential trajectory toward AGI, we present a comprehensive survey on\nAgentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we\nexplore the emerging paradigm of agentic MLLMs, delineating their conceptual\nfoundations and distinguishing characteristics from conventional MLLM-based\nagents. We establish a conceptual framework that organizes agentic MLLMs along\nthree fundamental dimensions: (i) Agentic internal intelligence functions as\nthe system's commander, enabling accurate long-horizon planning through\nreasoning, reflection, and memory; (ii) Agentic external tool invocation,\nwhereby models proactively use various external tools to extend their\nproblem-solving capabilities beyond their intrinsic knowledge; and (iii)\nAgentic environment interaction further situates models within virtual or\nphysical environments, allowing them to take actions, adapt strategies, and\nsustain goal-directed behavior in dynamic real-world scenarios. To further\naccelerate research in this area for the community, we compile open-source\ntraining frameworks, training and evaluation datasets for developing agentic\nMLLMs. Finally, we review the downstream applications of agentic MLLMs and\noutline future research directions for this rapidly evolving field. To\ncontinuously track developments in this rapidly evolving field, we will also\nactively update a public repository at\nhttps://github.com/HJYao00/Awesome-Agentic-MLLMs.", "AI": {"tldr": "这篇综述全面探讨了具身多模态大语言模型（Agentic MLLMs）这一新兴范式，提出了一个三维概念框架，并整理了开源资源、应用及未来研究方向，以应对AI从静态向动态、通用化发展的趋势。", "motivation": "随着革命性自主智能体系统的出现，AI研究正从传统的静态、被动、领域专用型智能体转向更具动态性、主动性和通用性的具身AI。本研究旨在响应具身AI日益增长的兴趣及其迈向通用人工智能（AGI）的潜在轨迹。", "method": "本研究通过以下方式进行：1) 探讨具身多模态大语言模型（Agentic MLLMs）的新兴范式，并区分其与传统MLLM-based智能体的概念基础和特征。2) 建立一个三维概念框架，包括：具身内部智能功能、具身外部工具调用、具身环境交互。3) 汇编用于开发具身MLLMs的开源训练框架、训练和评估数据集。4) 回顾具身MLLMs的下游应用，并概述该领域的未来研究方向。", "result": "本研究提出了一个具身多模态大语言模型（Agentic MLLMs）的全面概念框架，该框架基于三个核心维度：内部智能（推理、反思、记忆）、外部工具调用（扩展问题解决能力）和环境交互（适应动态真实世界场景）。此外，还整理了开源训练框架、训练和评估数据集，并综述了下游应用，为社区提供了宝贵资源。", "conclusion": "具身多模态大语言模型（Agentic MLLMs）代表了AI发展的一个重要方向，具有迈向通用人工智能的巨大潜力。该领域正在迅速演变，未来研究应关注所概述的方向，并且研究者将通过一个公开的代码库持续跟踪其发展。"}}
{"id": "2510.10965", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10965", "abs": "https://arxiv.org/abs/2510.10965", "authors": ["Jidong Li", "Lingyong Fang", "Haodong Zhao", "Sufeng Duan", "Gongshen Liu"], "title": "Judge Before Answer: Can MLLM Discern the False Premise in Question?", "comment": null, "summary": "Multimodal large language models (MLLMs) have witnessed astonishing\nadvancements in recent years. Despite these successes, MLLMs remain vulnerable\nto flase premise problems. However, existing benchmarks targeting this issue\nare limited in scope: they often lack fine-grained categorization, exhibit\ninsufficient coverage, and thus fail to provide a rigorous evaluation of the\nability of models to recognize false premises. To bridge this gap, we introduce\na fully automated pipeline for constructing a comprehensive benchmark of false\npremise questions. Our method systematically categorizes the premises into\nthree main types and thirteen subtypes according to the abilities required to\nidentify the premises, resulting in the JBA dataset.Results show current MLLMs\nstill struggle with false premise recognition. Building upon this benchmark, we\nfurther propose a recognition enhancement framework tailored to strengthen the\nrobustness of MLLMs to detect false premises. Extensive experiments demonstrate\nthat models trained with our framework achieve significant improvements in\nfalse premise recognition.", "AI": {"tldr": "多模态大语言模型（MLLMs）容易受错误前提影响，但现有基准有限。本文提出了一个自动化流水线来构建全面的错误前提基准JBA数据集，并对前提进行细致分类。结果显示当前MLLMs表现不佳。在此基础上，本文提出了一个识别增强框架，显著提升了MLLMs识别错误前提的能力。", "motivation": "尽管多模态大语言模型（MLLMs）取得了显著进展，但它们仍然容易受到错误前提问题的影响。然而，现有针对此问题的基准测试范围有限，缺乏细粒度分类和充分覆盖，无法严格评估模型识别错误前提的能力。", "method": "本文引入了一个全自动流水线，用于构建一个全面的错误前提问题基准（JBA数据集），该方法根据识别前提所需的能力，将前提系统地分为三大类和十三个子类。在此基准之上，进一步提出了一个识别增强框架，旨在增强MLLMs检测错误前提的鲁棒性。", "result": "研究结果表明，当前的MLLMs在错误前提识别方面仍然表现不佳。经过本文提出的框架训练的模型在错误前提识别方面取得了显著改进。", "conclusion": "MLLMs在识别错误前提方面面临挑战，现有基准存在不足。本文通过构建细致分类的JBA数据集和提出识别增强框架，有效提升了MLLMs识别错误前提的鲁棒性和能力。"}}
{"id": "2510.10653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10653", "abs": "https://arxiv.org/abs/2510.10653", "authors": ["Sebastian Schmidt", "Julius Körner", "Stephan Günnemann"], "title": "A Machine Learning Perspective on Automated Driving Corner Cases", "comment": null, "summary": "For high-stakes applications, like autonomous driving, a safe operation is\nnecessary to prevent harm, accidents, and failures. Traditionally, difficult\nscenarios have been categorized into corner cases and addressed individually.\nHowever, this example-based categorization is not scalable and lacks a data\ncoverage perspective, neglecting the generalization to training data of machine\nlearning models. In our work, we propose a novel machine learning approach that\ntakes the underlying data distribution into account. Based on our novel\nperspective, we present a framework for effective corner case recognition for\nperception on individual samples. In our evaluation, we show that our approach\n(i) unifies existing scenario-based corner case taxonomies under a\ndistributional perspective, (ii) achieves strong performance on corner case\ndetection tasks across standard benchmarks for which we extend established\nout-of-distribution detection benchmarks, and (iii) enables analysis of\ncombined corner cases via a newly introduced fog-augmented Lost & Found\ndataset. These results provide a principled basis for corner case recognition,\nunderlining our manual specification-free definition.", "AI": {"tldr": "本文提出了一种新颖的机器学习方法，通过考虑数据分布来有效识别感知系统中的边缘案例，解决了传统基于示例方法的可扩展性问题，并在多个基准测试中表现出色。", "motivation": "在高风险应用（如自动驾驶）中，安全运行至关重要。传统上，困难场景被归类为边缘案例并单独处理，但这种基于示例的分类方法不可扩展，缺乏数据覆盖视角，并且忽视了机器学习模型对训练数据的泛化能力。", "method": "本文提出了一种新颖的机器学习方法，该方法考虑了底层数据分布。在此新视角的基础上，构建了一个框架，用于在单个样本上有效识别感知任务中的边缘案例。", "result": "研究结果表明，所提出的方法 (i) 在分布视角下统一了现有的基于场景的边缘案例分类法；(ii) 在扩展了现有分布外检测基准的标准基准测试中，边缘案例检测任务表现出强大的性能；(iii) 通过新引入的雾增强型Lost & Found数据集，实现了对组合边缘案例的分析。", "conclusion": "这些结果为边缘案例识别提供了一个原则性的基础，强调了其无需手动规范定义的特点，为高风险应用的安全运行提供了支持。"}}
{"id": "2510.10994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10994", "abs": "https://arxiv.org/abs/2510.10994", "authors": ["Wei-Chieh Huang", "Henry Peng Zou", "Yaozu Wu", "Dongyuan Li", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Angelo Zangari", "Jizhou Guo", "Chunyu Miao", "Liancheng Fang", "Langzhou He", "Renhe Jiang", "Philip S. Yu"], "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety", "comment": null, "summary": "Deep research frameworks have shown promising capabilities in synthesizing\ncomprehensive reports from web sources. While deep research possesses\nsignificant potential to address complex issues through planning and research\ncycles, existing frameworks are deficient in sufficient evaluation procedures\nand stage-specific protections. They typically treat evaluation as exact match\naccuracy of question-answering, but overlook crucial aspects of report quality\nsuch as credibility, coherence, breadth, depth, and safety. This oversight may\nresult in hazardous or malicious sources being integrated into the final\nreport. To address these issues, we introduce DEEPRESEARCHGUARD, a\ncomprehensive framework featuring four-stage safeguards with open-domain\nevaluation of references and reports. We assess performance across multiple\nmetrics, e.g., defense success rate and over-refusal rate, and five key report\ndimensions. In the absence of a suitable safety benchmark, we introduce\nDRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation\nspans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,\nDeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success\nrate improvement of 18.16% while reducing over-refusal rate by 6%. The input\nguard provides the most substantial early-stage protection by filtering out\nobvious risks, while the plan and research guards enhance citation discipline\nand source credibility. Through extensive experiments, we show that\nDEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware\ndefenses that effectively block harmful content propagation, while\nsystematically improving report quality without excessive over-refusal rates.\nThe code can be found via https://github.com/Jasonya/DeepResearchGuard.", "AI": {"tldr": "本文介绍了DEEPRESEARCHGUARD，一个具有四阶段安全防护的深度研究框架，旨在解决现有框架在报告质量评估和安全防护方面的不足，并显著提升报告的安全性、可信度和整体质量。", "motivation": "现有的深度研究框架在从网络源合成报告时，缺乏足够的评估程序和阶段性保护措施。它们通常只关注问答的精确匹配准确性，而忽视报告质量的关键方面，如可信度、连贯性、广度、深度和安全性，这可能导致有害或恶意来源被整合到最终报告中。", "method": "本文提出了DEEPRESEARCHGUARD框架，该框架具有四阶段安全防护，并对参考文献和报告进行开放域评估。为了评估安全性，引入了DRSAFEBENCH，一个针对深度研究安全的阶段性基准。评估涵盖了多种最先进的LLM，并使用了防御成功率和过度拒绝率等指标以及五个关键报告维度。", "result": "DEEPRESEARCHGUARD框架将平均防御成功率提高了18.16%，同时将过度拒绝率降低了6%。其中，输入防护在早期阶段提供了最实质性的保护，过滤掉了明显的风险；计划和研究防护则增强了引用规范和来源可信度。", "conclusion": "DEEPRESEARCHGUARD框架通过全面的开放域评估和阶段性防御，有效阻止了有害内容的传播，并在不过度拒绝的情况下系统地提高了报告质量。该框架能够实现综合的开放域评估和阶段性防御，有效阻断有害内容传播，同时系统地提升报告质量而不会产生过高的过度拒绝率。"}}
{"id": "2510.10806", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10806", "abs": "https://arxiv.org/abs/2510.10806", "authors": ["Mihir Gupte", "Paolo Giusto", "Ramesh S"], "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures", "comment": "Waiting for Conference Response", "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.10398", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10398", "abs": "https://arxiv.org/abs/2510.10398", "authors": ["Geunyeong Jeong", "Juoh Sun", "Seonghee Lee", "Harksoo Kim"], "title": "STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models", "comment": "Accepted to EMNLP 2025 (Findings)", "summary": "Large Language Models store extensive factual knowledge acquired during\nlarge-scale pre-training. However, this knowledge is inherently static,\nreflecting only the state of the world at the time of training. Knowledge\nediting has emerged as a promising solution for updating outdated or incorrect\nfacts without full retraining. However, most existing locate-and-edit methods\nprimarily focus on token-level likelihood optimization without addressing\nsemantic coherence. Our analysis reveals that such edited knowledge is often\nencoded as isolated residual streams in the model's latent space, distinct from\npre-existing knowledge and bypassing natural reasoning process. To address\nthis, we propose \\textsc{Steam}, a semantic-level knowledge editing framework\nthat enhances integration of updated knowledge into the model's knowledge\nstructure. \\textsc{Steam} first identifies target representations as semantic\nanchors for the updated factual association, then guides the internal\nrepresentation of the edited fact towards these anchors through an alignment\nloss during optimization. Experimental results demonstrate that \\textsc{Steam}\nimproves model's ability to reason with edited knowledge and enhances semantic\ncoherence, underscoring the importance of latent-space alignment for reliable\nand coherent knowledge editing. The code is available at\nhttps://github.com/GY-Jeong/STEAM.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.10670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10670", "abs": "https://arxiv.org/abs/2510.10670", "authors": ["Yu Li", "Menghan Xia", "Gongye Liu", "Jianhong Bai", "Xintao Wang", "Conglang Zhang", "Yuxuan Lin", "Ruihang Chu", "Pengfei Wan", "Yujiu Yang"], "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes", "comment": null, "summary": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in\nvisual simulation of real-world geometry and physical laws, indicating its\npotential as implicit world models. Inspired by this, we explore the\nfeasibility of leveraging the video generation prior for viewpoint planning\nfrom given 4D scenes, since videos internally accompany dynamic scenes with\nnatural viewpoints. To this end, we propose a two-stage paradigm to adapt\npre-trained T2V models for viewpoint prediction, in a compatible manner. First,\nwe inject the 4D scene representation into the pre-trained T2V model via an\nadaptive learning branch, where the 4D scene is viewpoint-agnostic and the\nconditional generated video embeds the viewpoints visually. Then, we formulate\nviewpoint extraction as a hybrid-condition guided camera extrinsic denoising\nprocess. Specifically, a camera extrinsic diffusion branch is further\nintroduced onto the pre-trained T2V model, by taking the generated video and 4D\nscene as input. Experimental results show the superiority of our proposed\nmethod over existing competitors, and ablation studies validate the\neffectiveness of our key technical designs. To some extent, this work proves\nthe potential of video generation models toward 4D interaction in real world.", "AI": {"tldr": "本文提出了一种利用预训练文本到视频（T2V）模型进行4D场景视点规划的两阶段范式，通过注入4D场景表示并结合相机外参去噪过程来预测自然视点。", "motivation": "T2V模型在模拟真实世界几何和物理定律方面表现出强大能力，显示了其作为隐式世界模型的潜力。同时，视频本身就包含动态场景及其伴随的自然视点，这启发了作者探索利用视频生成先验进行4D场景视点规划的可行性。", "method": "该方法采用两阶段范式：\n1. 通过自适应学习分支将4D场景表示注入预训练T2V模型，生成视觉上嵌入视点的视频。\n2. 将视点提取公式化为混合条件引导的相机外参去噪过程，为此在预训练T2V模型上引入一个相机外参扩散分支，并以生成的视频和4D场景作为输入。", "result": "实验结果表明，所提出的方法优于现有竞争对手，并且消融研究验证了其关键技术设计的有效性。", "conclusion": "这项工作在一定程度上证明了视频生成模型在真实世界4D交互方面的潜力。"}}
{"id": "2510.10660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10660", "abs": "https://arxiv.org/abs/2510.10660", "authors": ["Hao Shan", "Ruikai Li", "Han Jiang", "Yizhe Fan", "Ziyang Yan", "Bohan Li", "Xiaoshuai Hao", "Hao Zhao", "Zhiyong Cui", "Yilong Ren", "Haiyang Yu"], "title": "Stability Under Scrutiny: Benchmarking Representation Paradigms for Online HD Mapping", "comment": null, "summary": "As one of the fundamental modules in autonomous driving, online\nhigh-definition (HD) maps have attracted significant attention due to their\ncost-effectiveness and real-time capabilities. Since vehicles always cruise in\nhighly dynamic environments, spatial displacement of onboard sensors inevitably\ncauses shifts in real-time HD mapping results, and such instability poses\nfundamental challenges for downstream tasks. However, existing online map\nconstruction models tend to prioritize improving each frame's mapping accuracy,\nwhile the mapping stability has not yet been systematically studied. To fill\nthis gap, this paper presents the first comprehensive benchmark for evaluating\nthe temporal stability of online HD mapping models. We propose a\nmulti-dimensional stability evaluation framework with novel metrics for\nPresence, Localization, and Shape Stability, integrated into a unified mean\nAverage Stability (mAS) score. Extensive experiments on 42 models and variants\nshow that accuracy (mAP) and stability (mAS) represent largely independent\nperformance dimensions. We further analyze the impact of key model design\nchoices on both criteria, identifying architectural and training factors that\ncontribute to high accuracy, high stability, or both. To encourage broader\nfocus on stability, we will release a public benchmark. Our work highlights the\nimportance of treating temporal stability as a core evaluation criterion\nalongside accuracy, advancing the development of more reliable autonomous\ndriving systems. The benchmark toolkit, code, and models will be available at\nhttps://stablehdmap.github.io/.", "AI": {"tldr": "本文首次提出了一个全面的在线高清地图时间稳定性评估基准，包含新颖的多维度稳定性指标（mAS），并发现稳定性与精度是独立的性能维度，强调其对自动驾驶系统可靠性的重要性。", "motivation": "在线高清地图在自动驾驶中至关重要，但车载传感器位移会导致地图结果不稳定，这给下游任务带来挑战。现有模型主要关注映射精度，而缺乏对时间稳定性的系统性研究。", "method": "提出首个评估在线高清地图模型时间稳定性的综合基准。开发了一个多维度稳定性评估框架，包含新颖的“存在稳定性”、“定位稳定性”和“形状稳定性”指标，并整合为统一的平均稳定性（mAS）分数。对42个模型及其变体进行了广泛实验，分析了关键模型设计选择对精度和稳定性的影响。", "result": "实验表明，精度（mAP）和稳定性（mAS）是两个基本独立的性能维度。研究还分析了影响高精度、高稳定性或两者兼具的模型架构和训练因素。", "conclusion": "时间稳定性应作为与精度并列的核心评估标准，以促进更可靠的自动驾驶系统发展。将发布公共基准以鼓励对稳定性的关注。"}}
{"id": "2510.11001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11001", "abs": "https://arxiv.org/abs/2510.11001", "authors": ["Tieyuan Chen", "Xiaodong Chen", "Haoxing Chen", "Zhenzhong Lan", "Weiyao Lin", "Jianguo Li"], "title": "DND: Boosting Large Language Models with Dynamic Nested Depth", "comment": "TL;DR: We introduce Dynamic Nested Depth (DND), an efficient paradigm\n  that adaptively identifies critical tokens and selectively deepens their\n  computation via nested re-processing", "summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves\nperformance for off-the-shelf LLMs by selecting critical tokens to reprocess in\na nested depth manner. Specifically, at the end of the given transformer layer,\nDND identifies more critical tokens with a router and feeds them back for an\nextra round of processing, effectively ``reviewing\" difficult tokens while\navoiding redundant computation for easier ones. The dynamic selection mechanism\nis tailored for precise control via two novel strategies: a router controlling\nloss to enhance token selection distinguishability, and a threshold control\nscheme to ensure selection stability. We demonstrate the effectiveness of DND\nby directly integrating it into pre-trained dense and MoE models during a\npost-training phase. On diverse benchmarks, this approach boosts the\nperformances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by\n0.87%, all with a minimal parameter and computing increase.", "AI": {"tldr": "DND是一种新方法，通过在Transformer层末端动态选择并以嵌套深度方式重新处理关键token，从而提升现有LLM的性能，同时保持计算量和参数量的最小增长。", "motivation": "现有LLM在处理复杂token时可能效率不高，而对所有token进行重复计算又会导致冗余。研究旨在找到一种方法，能有选择性地“复习”困难token，避免对简单token进行不必要的计算，从而在提高性能的同时优化计算效率。", "method": "引入动态嵌套深度（DND）方法。在Transformer层的末端，DND使用一个路由器识别出更关键的token，并将其送回进行额外的处理轮次（嵌套深度）。为了精确控制，DND采用了两种策略：一个控制损失的路由器以增强token选择的可区分性，以及一个阈值控制方案以确保选择的稳定性。该方法在训练后阶段直接集成到预训练的密集型和MoE模型中。", "result": "DND在多样化的基准测试中展现出有效性。它使密集型Qwen3-1.7B的性能提升了1.88%，使MoE Qwen3-30B-A3B的性能提升了0.87%，所有这些都只伴随着极小的参数和计算量增加。", "conclusion": "DND通过动态选择并重新处理关键token，成功提升了现有LLM的性能，且增益显著而成本极低。这证明了其作为一种高效、低成本的LLM性能增强方案的潜力。"}}
{"id": "2510.11031", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11031", "abs": "https://arxiv.org/abs/2510.11031", "authors": ["Yiwei Liu", "Yucheng Li", "Xiao Li", "Gong Cheng"], "title": "LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models", "comment": "30 pages, 3 figures", "summary": "Joint logical-numerical reasoning remains a major challenge for language\nmodels, yet existing datasets rely on fixed rule sets and offer limited control\nover task complexity, constraining their generalizability for evaluation and\ntraining. We present LogiNumSynth, a flexible natural language problem\nsynthesizer that synthesizes tasks requiring proficiency in joint logical\nreasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,\narithmetic computation). LogiNumSynth supports fine-grained control over\nreasoning world richness, logical reasoning depth, and the complexity of\nnumerical computations, enabling flexible data synthesis across difficulty\nlevels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing\nfully controllable joint reasoning tasks over natural language; (2) Evaluation\n& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)\nTargeted Training -- using synthesized data to enhance LLMs' reasoning\nperformance. Experiments with multiple LLMs highlight persistent weaknesses in\nlogical-numerical reasoning, showing that LogiNumSynth can serve as both a\ndiagnostic tool and a source of targeted supervision for advancing integrated\nreasoning skills.", "AI": {"tldr": "LogiNumSynth是一个灵活的自然语言问题合成器，用于生成可控的逻辑-数值联合推理任务，以诊断和提升语言模型的推理能力。", "motivation": "语言模型在逻辑-数值联合推理方面仍面临重大挑战。现有数据集依赖固定规则集，对任务复杂度的控制有限，影响了评估和训练的泛化能力。", "method": "提出LogiNumSynth，一个灵活的自然语言问题合成器。它能合成需要逻辑推理（如基于规则的推理）和数值推理（如算术计算）的任务。LogiNumSynth支持细粒度控制推理世界丰富度、逻辑推理深度和数值计算复杂度，从而能生成不同难度级别的数据。", "result": "LogiNumSynth能够合成完全可控的联合推理任务；支持过程准确性和答案准确性的评估；使用合成数据可以提升大型语言模型的推理性能。实验揭示了LLMs在逻辑-数值推理方面持续存在的弱点。", "conclusion": "LogiNumSynth可以作为诊断工具，同时也是提供有针对性监督的来源，以促进语言模型集成推理技能的进步。"}}
{"id": "2510.11020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11020", "abs": "https://arxiv.org/abs/2510.11020", "authors": ["Shasha Guo", "Liang Pang", "Xi Wang", "Yanling Wang", "Huawei Shen", "Jing Zhang"], "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation", "comment": "22 pages", "summary": "Auxiliary lines are essential for solving complex geometric problems but\nremain challenging for large vision-language models (LVLMs). Rather than\nediting diagrams to draw auxiliary lines, which current image editing models\nstruggle to render with geometric precision, we generate textual descriptions\nof auxiliary-line constructions to better align with the representational\nstrengths of LVLMs. To bridge the gap between textual descriptions and spatial\nstructure, we propose a reinforcement learning framework that enhances\ndiagram-text alignment. At the core of our approach is a cross-modal reward\nthat evaluates how well the generated auxiliary-line description for an\noriginal diagram matches a ground-truth auxiliary-line diagram. Built on this\nreward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line\nreasoning in solid geometry. This fine-grained signal drives a GRPO-based RL\nstage, yielding precise diagram-text alignment. To support training, we develop\na scalable data creation pipeline and construct AuxSolidMath, a dataset of\n3,018 real-exam geometry problems with paired diagrams and aligned textual\nfields. At the 3B and 7B scales, GeoVLMath achieves competitive and often\nsuperior performance compared with strong open-source and proprietary LVLMs on\nauxiliary-line reasoning benchmarks.", "AI": {"tldr": "该研究提出了一种通过生成辅助线文本描述而非直接编辑图像，并结合强化学习框架来增强大型视觉-语言模型（LVLMs）在几何问题中辅助线推理能力的方法。他们开发了GeoVLMath模型和AuxSolidMath数据集，并在相关基准测试中取得了优异表现。", "motivation": "辅助线对于解决复杂几何问题至关重要，但对LVLMs来说仍是挑战。现有图像编辑模型难以精确渲染几何图形，因此直接编辑图表来绘制辅助线效果不佳。研究旨在利用LVLMs擅长处理文本的优势，通过生成辅助线构造的文本描述来解决这一问题。", "method": "该研究提出一个强化学习框架来增强图表-文本对齐，核心是跨模态奖励机制，用于评估生成的辅助线描述与真实辅助线图表的匹配程度。基于此奖励，他们开发了GeoVLMath，一个专门用于立体几何中辅助线推理的开源LVLM。该方法采用GRPO-based的强化学习阶段，以实现精确的图表-文本对齐。为支持训练，他们构建了一个可扩展的数据创建管道，并开发了AuxSolidMath数据集，包含3,018个真实考试几何问题，配对图表和对齐的文本字段。", "result": "在3B和7B规模下，GeoVLMath在辅助线推理基准测试中，与强大的开源和专有LVLMs相比，取得了具有竞争力且通常更优的性能。", "conclusion": "通过将辅助线构造描述转化为文本形式，并利用强化学习框架和跨模态奖励机制，可以显著提升LVLMs在几何问题中辅助线推理的能力。GeoVLMath模型和AuxSolidMath数据集的开发为此领域提供了有效的解决方案和资源。"}}
{"id": "2510.10679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10679", "abs": "https://arxiv.org/abs/2510.10679", "authors": ["Yuxiang Luo", "Qing Xu", "Hai Huang", "Yuqi Ouyang", "Zhen Chen", "Wenting Duan"], "title": "MSM-Seg: A Modality-and-Slice Memory Framework with Category-Agnostic Prompting for Multi-Modal Brain Tumor Segmentation", "comment": "Under Review", "summary": "Multi-modal brain tumor segmentation is critical for clinical diagnosis, and\nit requires accurate identification of distinct internal anatomical subregions.\nWhile the recent prompt-based segmentation paradigms enable interactive\nexperiences for clinicians, existing methods ignore cross-modal correlations\nand rely on labor-intensive category-specific prompts, limiting their\napplicability in real-world scenarios. To address these issues, we propose a\nMSM-Seg framework for multi-modal brain tumor segmentation. The MSM-Seg\nintroduces a novel dual-memory segmentation paradigm that synergistically\nintegrates multi-modal and inter-slice information with the efficient\ncategory-agnostic prompt for brain tumor understanding. To this end, we first\ndevise a modality-and-slice memory attention (MSMA) to exploit the cross-modal\nand inter-slice relationships among the input scans. Then, we propose a\nmulti-scale category-agnostic prompt encoder (MCP-Encoder) to provide tumor\nregion guidance for decoding. Moreover, we devise a modality-adaptive fusion\ndecoder (MF-Decoder) that leverages the complementary decoding information\nacross different modalities to improve segmentation accuracy. Extensive\nexperiments on different MRI datasets demonstrate that our MSM-Seg framework\noutperforms state-of-the-art methods in multi-modal metastases and glioma tumor\nsegmentation. The code is available at https://github.com/xq141839/MSM-Seg.", "AI": {"tldr": "MSM-Seg是一个多模态脑肿瘤分割框架，通过双记忆分割范式和类别无关提示，协同整合多模态和切片间信息，显著提高了分割精度。", "motivation": "现有的基于提示的分割方法忽略了跨模态相关性，并依赖于劳动密集型的类别特定提示，限制了其在实际场景中的应用。", "method": "提出MSM-Seg框架，引入双记忆分割范式。具体包括：1) 设计模态与切片记忆注意力（MSMA）以利用跨模态和切片间关系；2) 提出多尺度类别无关提示编码器（MCP-Encoder）为解码提供肿瘤区域指导；3) 设计模态自适应融合解码器（MF-Decoder）以利用不同模态的互补解码信息。", "result": "在不同的MRI数据集上，MSM-Seg框架在多模态转移瘤和胶质瘤分割方面均优于现有最先进的方法。", "conclusion": "MSM-Seg通过有效整合多模态和切片间信息，并采用高效的类别无关提示，解决了现有方法的局限性，提高了脑肿瘤分割的准确性。"}}
{"id": "2510.11040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11040", "abs": "https://arxiv.org/abs/2510.11040", "authors": ["Wenya Xie", "Qingying Xiao", "Yu Zheng", "Xidong Wang", "Junying Chen", "Ke Ji", "Anningzhe Gao", "Prayag Tiwari", "Xiang Wan", "Feng Jiang", "Benyou Wang"], "title": "Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks", "comment": null, "summary": "The rise of large language models (LLMs) has transformed healthcare by\noffering clinical guidance, yet their direct deployment to patients poses\nsafety risks due to limited domain expertise. To mitigate this, we propose\nrepositioning LLMs as clinical assistants that collaborate with experienced\nphysicians rather than interacting with patients directly. We conduct a\ntwo-stage inspiration-feedback survey to identify real-world needs in clinical\nworkflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese\nmedical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27\nspecialties. To evaluate model performance in doctor-facing applications, we\nintroduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74\nmulti-turn conversations). Experimental results with over ten popular LLMs\ndemonstrate that DoctorFLAN notably improves the performance of open-source\nLLMs in medical contexts, facilitating their alignment with physician workflows\nand complementing existing patient-oriented models. This work contributes a\nvaluable resource and framework for advancing doctor-centered medical LLM\ndevelopment", "AI": {"tldr": "本研究提出将大型语言模型（LLMs）定位为临床助手，而非直接面向患者。为此，构建了DoctorFLAN，一个包含92,000个问答实例的大规模中文医疗数据集，并开发了评估基准。实验证明DoctorFLAN显著提升了开源LLMs在医疗领域的表现，使其更好地适应医生工作流程。", "motivation": "尽管大型语言模型（LLMs）在提供临床指导方面具有潜力，但由于其领域专业知识有限，直接将其部署给患者存在安全风险。因此，需要将其重新定位为与经验丰富的医生协作的临床助手，以降低风险并更好地服务医疗工作流程。", "method": "研究方法包括：1) 进行两阶段的启发-反馈调查，以识别临床工作流程中的实际需求。2) 基于调查结果，构建DoctorFLAN，一个包含92,000个问答实例的大规模中文医疗数据集，涵盖22个临床任务和27个专科。3) 引入DoctorFLAN-test（550个单轮问答）和DotaBench（74个多轮对话）作为评估基准，以衡量模型在医生应用中的表现。4) 使用十余种流行LLMs进行实验评估。", "result": "实验结果表明，DoctorFLAN显著提高了开源LLMs在医疗语境下的性能。这有助于LLMs更好地与医生工作流程对齐，并补充了现有的以患者为导向的模型。", "conclusion": "本研究为推进以医生为中心的医疗LLM开发贡献了宝贵的资源和框架。DoctorFLAN数据集及其相关工作有助于将LLMs安全有效地整合到临床实践中，作为医生的有力辅助工具。"}}
{"id": "2510.10682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10682", "abs": "https://arxiv.org/abs/2510.10682", "authors": ["Xinyu Yang", "Zheheng Jiang", "Feixiang Zhou", "Yihang Zhu", "Na Lv", "Nan Xing", "Huiyu Zhou"], "title": "Action-Dynamics Modeling and Cross-Temporal Interaction for Online Action Understanding", "comment": "10 pages, 9 figures", "summary": "Action understanding, encompassing action detection and anticipation, plays a\ncrucial role in numerous practical applications. However, untrimmed videos are\noften characterized by substantial redundant information and noise. Moreover,\nin modeling action understanding, the influence of the agent's intention on the\naction is often overlooked. Motivated by these issues, we propose a novel\nframework called the State-Specific Model (SSM), designed to unify and enhance\nboth action detection and anticipation tasks. In the proposed framework, the\nCritical State-Based Memory Compression module compresses frame sequences into\ncritical states, reducing information redundancy. The Action Pattern Learning\nmodule constructs a state-transition graph with multi-dimensional edges to\nmodel action dynamics in complex scenarios, on the basis of which potential\nfuture cues can be generated to represent intention. Furthermore, our\nCross-Temporal Interaction module models the mutual influence between\nintentions and past as well as current information through cross-temporal\ninteractions, thereby refining present and future features and ultimately\nrealizing simultaneous action detection and anticipation. Extensive experiments\non multiple benchmark datasets -- including EPIC-Kitchens-100, THUMOS'14,\nTVSeries, and the introduced Parkinson's Disease Mouse Behaviour (PDMB) dataset\n-- demonstrate the superior performance of our proposed framework compared to\nother state-of-the-art approaches. These results highlight the importance of\naction dynamics learning and cross-temporal interactions, laying a foundation\nfor future action understanding research.", "AI": {"tldr": "本文提出了一种名为SSM的统一框架，通过压缩冗余信息、建模动作动态和跨时间交互，同时提升了动作检测和动作预测的性能。", "motivation": "现有方法在处理未剪辑视频时存在大量冗余信息和噪声，并且在动作理解中常忽视智能体意图对动作的影响。", "method": "本文提出了状态特定模型（SSM）框架，包含三个模块：1. 关键状态记忆压缩模块（Critical State-Based Memory Compression），用于压缩帧序列以减少信息冗余；2. 动作模式学习模块（Action Pattern Learning），构建多维边缘的状态-转换图来建模复杂场景下的动作动态，并生成潜在未来线索以表示意图；3. 跨时间交互模块（Cross-Temporal Interaction），通过跨时间交互建模意图与过去及当前信息之间的相互影响，从而优化当前和未来的特征，实现同步的动作检测和预测。", "result": "在EPIC-Kitchens-100、THUMOS'14、TVSeries和新引入的帕金森病小鼠行为（PDMB）数据集上进行的广泛实验表明，所提出的框架性能优于其他最先进的方法。", "conclusion": "研究结果强调了动作动态学习和跨时间交互的重要性，为未来的动作理解研究奠定了基础。"}}
{"id": "2510.11052", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11052", "abs": "https://arxiv.org/abs/2510.11052", "authors": ["Qinglin Zhu", "Yizhen Yao", "Runcong Zhao", "Yanzheng Xiang", "Amrutha Saseendran", "Chen Jin", "Philip Alexander Teare", "Bin Liang", "Yulan He", "Lin Gui"], "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States", "comment": null, "summary": "Autoregressive (AR) models remain the standard for natural language\ngeneration but still suffer from high latency due to strictly sequential\ndecoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,\nmitigate this by generating in parallel, yet they suffer from two core\nlimitations: information loss, as predictive distributions for non-finalized\ntokens are discarded at each step, and premature commitment, where local\ndecisions are made without sufficient global coordination. We introduce Latent\nRefinement Decoding (LRD), a two-stage framework with Latent Refinement and a\nPredictive Feedback Loop. The first stage maintains masked positions as\ndistributional mixtures of predicted tokens and the mask embedding, allowing\nthe model to establish more globally consistent beliefs. The second stage\nprogressively finalizes confident tokens while retaining uncertain ones for\niterative feedback. KL-divergence dynamics provide a principled and reliable\ncriterion for convergence and early stopping. Experiments across coding\n(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that\nLRD improves accuracy while delivering speedups of up to 10.6x, making it a\nstrong and versatile alternative for parallel sequence generation.", "AI": {"tldr": "本文提出了一种名为潜在精炼解码（LRD）的两阶段框架，用于并行序列生成。它通过潜在精炼和预测反馈循环解决了自回归模型的高延迟和扩散模型的信息丢失及过早承诺问题，显著提高了生成任务的准确性，并实现了高达10.6倍的速度提升。", "motivation": "自回归（AR）模型在自然语言生成中因严格的顺序解码而导致高延迟。最近的扩散启发方法（如LlaDA和Dream）通过并行生成来缓解此问题，但存在两个核心局限性：信息丢失（非最终化token的预测分布在每一步被丢弃）和过早承诺（在缺乏全局协调的情况下做出局部决策）。", "method": "本文引入了潜在精炼解码（LRD），一个包含潜在精炼（Latent Refinement）和预测反馈循环（Predictive Feedback Loop）的两阶段框架。第一阶段将掩码位置维护为预测token和掩码嵌入的分布混合，以建立更全局一致的信念。第二阶段逐步确定置信度高的token，同时保留不确定的token进行迭代反馈。KL散度动态被用作收敛和提前停止的可靠准则。", "result": "实验结果表明，LRD在编码（HumanEval +6.3，MBPP +2.6）和推理（GSM8K +2.9，MATH500 +3.8）任务上都提高了准确性，同时实现了高达10.6倍的速度提升。", "conclusion": "LRD作为一种强大且多功能的并行序列生成替代方案，在提高准确性的同时显著提升了生成速度，有效解决了现有方法的局限性。"}}
{"id": "2510.11090", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11090", "abs": "https://arxiv.org/abs/2510.11090", "authors": ["Huizai Yao", "Sicheng Zhao", "Shuo Lu", "Hui Chen", "Yangyang Li", "Guoping Liu", "Tengfei Xing", "Chenggang Yan", "Jianhua Tao", "Guiguang Ding"], "title": "Source-Free Object Detection with Detection Transformer", "comment": "IEEE Transactions on Image Processing", "summary": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source\ndomain to an unsupervised target domain for object detection without access to\nsource data. Most existing SFOD approaches are either confined to conventional\nobject detection (OD) models like Faster R-CNN or designed as general solutions\nwithout tailored adaptations for novel OD architectures, especially Detection\nTransformer (DETR). In this paper, we introduce Feature Reweighting ANd\nContrastive Learning NetworK (FRANCK), a novel SFOD framework specifically\ndesigned to perform query-centric feature enhancement for DETRs. FRANCK\ncomprises four key components: (1) an Objectness Score-based Sample Reweighting\n(OSSR) module that computes attention-based objectness scores on multi-scale\nencoder feature maps, reweighting the detection loss to emphasize\nless-recognized regions; (2) a Contrastive Learning with Matching-based Memory\nBank (CMMB) module that integrates multi-level features into memory banks,\nenhancing class-wise contrastive learning; (3) an Uncertainty-weighted\nQuery-fused Feature Distillation (UQFD) module that improves feature\ndistillation through prediction quality reweighting and query feature fusion;\nand (4) an improved self-training pipeline with a Dynamic Teacher Updating\nInterval (DTUI) that optimizes pseudo-label quality. By leveraging these\ncomponents, FRANCK effectively adapts a source-pre-trained DETR model to a\ntarget domain with enhanced robustness and generalization. Extensive\nexperiments on several widely used benchmarks demonstrate that our method\nachieves state-of-the-art performance, highlighting its effectiveness and\ncompatibility with DETR-based SFOD models.", "AI": {"tldr": "FRANCK是一个专门为DETR模型设计的无源目标检测（SFOD）框架，通过查询中心特征增强和改进的自训练流程，实现从源域到无监督目标域的知识迁移，无需访问源数据。", "motivation": "大多数现有的无源目标检测（SFOD）方法要么局限于Faster R-CNN等传统目标检测模型，要么是通用解决方案，缺乏针对DETR等新型目标检测架构的定制化适应。", "method": "本文提出了FRANCK框架，包含四个关键组件：(1) 基于目标性得分的样本重加权（OSSR）模块，用于强调未充分识别的区域；(2) 带有匹配记忆库的对比学习（CMMB）模块，用于增强类间对比学习；(3) 不确定性加权查询融合特征蒸馏（UQFD）模块，用于改进特征蒸馏；(4) 带有动态教师更新间隔（DTUI）的改进自训练流程，用于优化伪标签质量。", "result": "FRANCK在多个广泛使用的基准测试中取得了最先进的性能，证明了其有效性以及与基于DETR的SFOD模型的兼容性。", "conclusion": "FRANCK通过利用其独特组件，有效地将源域预训练的DETR模型适应到目标域，显著增强了模型的鲁棒性和泛化能力。"}}
{"id": "2510.10885", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10885", "abs": "https://arxiv.org/abs/2510.10885", "authors": ["Jiajing Guo", "Kenil Patel", "Jorge Piazentin Ono", "Wenbin He", "Liu Ren"], "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks", "comment": "Accepted at COLM 2025 SCALR Workshop", "summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)\nsystems, enabling non-expert users to query industrial databases using natural\nlanguage. While test-time scaling strategies have shown promise in LLM-based\nsolutions, their effectiveness in real-world applications, especially with the\nlatest reasoning models, remains uncertain. In this work, we benchmark six\nlightweight, industry-oriented test-time scaling strategies and four LLMs,\nincluding two reasoning models, evaluating their performance on the BIRD\nMini-Dev benchmark. Beyond standard accuracy metrics, we also report inference\nlatency and token consumption, providing insights relevant for practical system\ndeployment. Our findings reveal that Divide-and-Conquer prompting and few-shot\ndemonstrations consistently enhance performance for both general-purpose and\nreasoning-focused LLMs. However, introducing additional workflow steps yields\nmixed results, and base model selection plays a critical role. This work sheds\nlight on the practical trade-offs between accuracy, efficiency, and complexity\nwhen deploying Text2SQL systems.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2510.11091", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11091", "abs": "https://arxiv.org/abs/2510.11091", "authors": ["Xianlin Liu", "Yan Gong", "Bohao Li", "Jiajing Huang", "Bowen Du", "Junchen Ye", "Liyan Xu"], "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings", "comment": "7 pages, 3figures. This version is the original submitted manuscript\n  of the paper accepted by The 12th International Conference on Behavioural and\n  Social Computing", "summary": "With the widespread adoption of Computer-Aided Design(CAD) drawings in\nengineering, architecture, and industrial design, the ability to accurately\ninterpret and analyze these drawings has become increasingly critical. Among\nvarious subtasks, panoptic symbol spotting plays a vital role in enabling\ndownstream applications such as CAD automation and design retrieval. Existing\nmethods primarily focus on geometric primitives within the CAD drawings to\naddress this task, but they face following major problems: they usually\noverlook the rich textual annotations present in CAD drawings and they lack\nexplicit modeling of relationships among primitives, resulting in\nincomprehensive understanding of the holistic drawings. To fill this gap, we\npropose a panoptic symbol spotting framework that incorporates textual\nannotations. The framework constructs unified representations by jointly\nmodeling geometric and textual primitives. Then, using visual features extract\nby pretrained CNN as the initial representations, a Transformer-based backbone\nis employed, enhanced with a type-aware attention mechanism to explicitly model\nthe different types of spatial dependencies between various primitives.\nExtensive experiments on the real-world dataset demonstrate that the proposed\nmethod outperforms existing approaches on symbol spotting tasks involving\ntextual annotations, and exhibits superior robustness when applied to complex\nCAD drawings.", "AI": {"tldr": "该论文提出了一种全景符号识别框架，通过联合建模几何和文本基元，并利用Transformer和类型感知注意力机制，解决了现有CAD图纸分析方法忽略文本注释和基元间关系的问题。", "motivation": "现有CAD图纸分析方法主要关注几何基元，但忽略了图纸中丰富的文本注释，并且缺乏对基元间关系的明确建模，导致对整体图纸的理解不全面。这限制了CAD自动化和设计检索等下游应用。", "method": "该框架通过联合建模几何和文本基元来构建统一表示。首先，使用预训练CNN提取视觉特征作为初始表示。然后，采用一个基于Transformer的骨干网络，并通过类型感知注意力机制进行增强，以明确建模不同基元之间不同类型的空间依赖关系。", "result": "在真实世界数据集上的大量实验表明，所提出的方法在涉及文本注释的符号识别任务上优于现有方法，并在应用于复杂CAD图纸时表现出卓越的鲁棒性。", "conclusion": "该研究成功地将文本注释整合到CAD图纸的全景符号识别中，并通过明确建模基元间的空间依赖关系，显著提升了符号识别的准确性和鲁棒性，为CAD自动化等应用奠定了基础。"}}
{"id": "2510.11104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11104", "abs": "https://arxiv.org/abs/2510.11104", "authors": ["Junjie Lu", "Yuliang Liu", "Chaofeng Qu", "Wei Shen", "Zhouhan Lin", "Min Xu"], "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization", "comment": "13 pages", "summary": "Current approaches for strengthening LLM reasoning tend to introduce a\ntraining bias toward human-like reasoning trajectories. In step-wise preference\noptimization, in particular, dependence on human or higher-capacity model\nannotations for intermediate steps limits exploration of alternative,\nnon-human-like reasoning paths and thus constrains achievable performance.\nFurthermore, through a small-scale pilot study, we observed that in\napproximately 75% of cases, the model's first erroneous step occurs after the\nlowest-confidence point. This suggests that guiding the model at its\nlowest-confidence point before an error provides more accurate supervision than\nlocating the first explicit error. In this paper, we propose Confidence-Guided\nReasoning Path Preference Optimization (CGPO), a method that leverages a\nconfidence signal to identify points of maximal uncertainty in the model's\nreasoning process and applies self-generated, non-human-like reasoning-path\nguidance to mitigate trajectory drift. Our experiments span diverse models\napplied to both code and mathematical reasoning tasks. The results show that,\nwith the same amount of training data, our method using data generated by a\nsmall model can achieve better performance in most cases compared with\napproaches using data generated by a strong model or human-annotated.", "AI": {"tldr": "本文提出信心引导推理路径偏好优化（CGPO）方法，利用置信度信号识别大语言模型推理过程中的不确定点，并应用自生成的非人类推理路径指导，以减少轨迹漂移，在代码和数学推理任务上，用小型模型生成的数据取得了比强模型或人工标注数据更好的性能。", "motivation": "当前加强大语言模型推理的方法倾向于引入人类般的推理轨迹偏见，限制了对非人类推理路径的探索。分步偏好优化尤其依赖人类或高能力模型的中间步骤标注。一项小型试点研究发现，约75%的错误发生在其最低置信度点之后，这表明在错误发生前的最低置信度点进行引导比定位第一个显式错误提供更准确的监督。", "method": "本文提出信心引导推理路径偏好优化（CGPO）方法。该方法利用置信度信号识别模型推理过程中不确定性最大的点，并应用自生成的、非人类般的推理路径指导，以减轻轨迹漂移。", "result": "在应用于代码和数学推理任务的各种模型上的实验表明，在相同训练数据量下，CGPO方法使用由小型模型生成的数据，在大多数情况下比使用由强模型或人工标注数据的方法取得了更好的性能。", "conclusion": "CGPO通过利用置信度信号在模型不确定性最高点进行自生成的非人类推理路径引导，有效避免了人类偏见，并提高了大语言模型在代码和数学推理任务上的表现，且能用更小的模型生成有效数据。"}}
{"id": "2510.10691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10691", "abs": "https://arxiv.org/abs/2510.10691", "authors": ["Xuankai Zhang", "Junjin Xiao", "Qing Zhang"], "title": "Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular Videos", "comment": null, "summary": "This paper presents a unified framework that allows high-quality dynamic\nGaussian Splatting from both defocused and motion-blurred monocular videos. Due\nto the significant difference between the formation processes of defocus blur\nand motion blur, existing methods are tailored for either one of them, lacking\nthe ability to simultaneously deal with both of them. Although the two can be\njointly modeled as blur kernel-based convolution, the inherent difficulty in\nestimating accurate blur kernels greatly limits the progress in this direction.\nIn this work, we go a step further towards this direction. Particularly, we\npropose to estimate per-pixel reliable blur kernels using a blur prediction\nnetwork that exploits blur-related scene and camera information and is subject\nto a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian\ndensification strategy to mitigate the lack of Gaussians for incomplete\nregions, and boost the performance of novel view synthesis by incorporating\nunseen view information to constrain scene optimization. Extensive experiments\nshow that our method outperforms the state-of-the-art methods in generating\nphotorealistic novel view synthesis from defocused and motion-blurred monocular\nvideos. Our code and trained model will be made publicly available.", "AI": {"tldr": "本文提出一个统一框架，能够从散焦和运动模糊的单目视频中生成高质量的动态高斯泼溅（Gaussian Splatting）渲染，解决了现有方法无法同时处理这两种模糊的问题。", "motivation": "现有方法要么针对散焦模糊，要么针对运动模糊，无法同时处理。尽管两者都可以通过模糊核卷积建模，但准确估计模糊核的固有难度限制了该方向的进展。", "method": "本文提出通过一个模糊预测网络来估计逐像素可靠的模糊核，该网络利用与模糊相关的场景和相机信息，并受到模糊感知稀疏性约束。此外，引入了动态高斯密集化策略以弥补不完整区域高斯数量的不足，并通过结合未见视图信息来约束场景优化，从而提升新颖视图合成的性能。", "result": "广泛的实验表明，该方法在从散焦和运动模糊的单目视频生成逼真的新颖视图合成方面，优于现有最先进的方法。", "conclusion": "本文成功开发了一个统一框架，能够有效处理散焦和运动模糊的单目视频，实现高质量的动态高斯泼溅渲染，显著提升了新颖视图合成的性能。"}}
{"id": "2510.10726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10726", "abs": "https://arxiv.org/abs/2510.10726", "authors": ["Yifan Liu", "Zhiyuan Min", "Zhenwei Wang", "Junta Wu", "Tengfei Wang", "Yixuan Yuan", "Yawei Luo", "Chunchao Guo"], "title": "WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting", "comment": "Project page, code, and models will be publicly available soon", "summary": "We present WorldMirror, an all-in-one, feed-forward model for versatile 3D\ngeometric prediction tasks. Unlike existing methods constrained to image-only\ninputs or customized for a specific task, our framework flexibly integrates\ndiverse geometric priors, including camera poses, intrinsics, and depth maps,\nwhile simultaneously generating multiple 3D representations: dense point\nclouds, multi-view depth maps, camera parameters, surface normals, and 3D\nGaussians. This elegant and unified architecture leverages available prior\ninformation to resolve structural ambiguities and delivers geometrically\nconsistent 3D outputs in a single forward pass. WorldMirror achieves\nstate-of-the-art performance across diverse benchmarks from camera, point map,\ndepth, and surface normal estimation to novel view synthesis, while maintaining\nthe efficiency of feed-forward inference. Code and models will be publicly\navailable soon.", "AI": {"tldr": "WorldMirror是一个一体化、前馈模型，能够灵活整合多种几何先验，并同时生成多种3D几何表示，在多个任务上实现了SOTA性能。", "motivation": "现有3D几何预测方法受限于仅图像输入或定制于特定任务，无法灵活整合多样化的几何先验并同时生成多种3D表示，存在结构模糊性。", "method": "WorldMirror采用统一的前馈架构，灵活整合包括相机姿态、内参和深度图在内的多样几何先验信息，并在单次前向传播中同时生成密集点云、多视角深度图、相机参数、表面法线和3D高斯等多种3D表示。", "result": "WorldMirror在相机、点图、深度和表面法线估计以及新视角合成等多个基准测试中达到了最先进的性能，同时保持了前馈推理的高效率。", "conclusion": "WorldMirror通过其优雅统一的架构和对可用先验信息的有效利用，解决了结构模糊性问题，并能以几何一致的方式生成多种3D输出，在多功能3D几何预测任务中表现出色。"}}
{"id": "2510.11129", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11129", "abs": "https://arxiv.org/abs/2510.11129", "authors": ["Guangzhi Sun", "Yixuan Li", "Xiaodong Wu", "Yudong Yang", "Wei Li", "Zejun Ma", "Chao Zhang"], "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory", "comment": null, "summary": "Continuous, high-frame-rate, high-resolution processing of long video streams\nis critical for future AI agents, yet current video-understanding LLMs struggle\nto scale. Offline, fixed-frame-number methods require the stream length to\nadapt frame rates; streaming methods constrain memory by merging or discarding\ntokens, losing information. We propose video-SALMONN S, a streaming\naudio-visual LLM that, to our knowledge, is the first to process 3-hour videos\nat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces\n(i) a test-time-training (TTT) memory module that continually updates token\nrepresentations to capture long-range dependencies by replacing token merging,\nand (ii) a prompt-dependent memory reader that selectively retrieves\ncontext-relevant content from fixed-size memory. The TTT module is optimised\nwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient\nadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),\nvideo-SALMONN S sustains high-quality understanding on multi-hour videos with\n10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and\n67.8% on the Video-MME long split, outperforming both offline and streaming\nbaselines.", "AI": {"tldr": "视频-SALMONN S是一种流式音视频大型语言模型，通过引入测试时训练（TTT）记忆模块和提示依赖的记忆读取器，首次实现了在固定内存预算下处理3小时长视频，并在长视频基准测试中超越了现有基线。", "motivation": "未来的AI代理需要连续、高帧率、高分辨率地处理长视频流，但当前视频理解大型语言模型（LLMs）难以扩展。离线方法需要调整帧率，而流式方法通过合并或丢弃token来限制内存，导致信息丢失。因此，需要一种能在固定内存预算下高效处理长视频流的方法。", "method": "本文提出了视频-SALMONN S模型，它是一个流式音视频LLM，主要包含两项创新：(i) 一个测试时训练（TTT）记忆模块，通过持续更新token表示来捕获长程依赖，取代了token合并；(ii) 一个提示依赖的记忆读取器，能从固定大小的内存中选择性地检索与上下文相关的内容。TTT模块通过无Hessian共轭梯度过程（TTT_HF）进行优化，以实现高效适应。", "result": "视频-SALMONN S是首个在固定内存预算下以1 FPS和360p分辨率处理3小时视频的模型。它在多小时视频（包含1万帧和1百万个token）上保持了高质量的理解能力。在长视频基准测试（Video-MME、LVBench、VideoEvalPro）中，其8B参数模型总体得分达到74.2%，在Video-MME长视频分割上达到67.8%，优于离线和流式基线模型。", "conclusion": "视频-SALMONN S通过创新的TTT记忆模块和提示依赖的记忆读取器，成功解决了在固定内存预算下处理长视频流的挑战，实现了连续、高质量的视频理解，并在长视频基准测试中取得了领先性能。"}}
{"id": "2510.10742", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10742", "abs": "https://arxiv.org/abs/2510.10742", "authors": ["Yuan Xu", "Zimu Zhang", "Xiaoxuan Ma", "Wentao Zhu", "Yu Qiao", "Yizhou Wang"], "title": "Seeing My Future: Predicting Situated Interaction Behavior in Virtual Reality", "comment": "Project Page: https://xy02-05.github.io/Seeing_My_Future/", "summary": "Virtual and augmented reality systems increasingly demand intelligent\nadaptation to user behaviors for enhanced interaction experiences. Achieving\nthis requires accurately understanding human intentions and predicting future\nsituated behaviors - such as gaze direction and object interactions - which is\nvital for creating responsive VR/AR environments and applications like\npersonalized assistants. However, accurate behavioral prediction demands\nmodeling the underlying cognitive processes that drive human-environment\ninteractions. In this work, we introduce a hierarchical, intention-aware\nframework that models human intentions and predicts detailed situated behaviors\nby leveraging cognitive mechanisms. Given historical human dynamics and the\nobservation of scene contexts, our framework first identifies potential\ninteraction targets and forecasts fine-grained future behaviors. We propose a\ndynamic Graph Convolutional Network (GCN) to effectively capture\nhuman-environment relationships. Extensive experiments on challenging\nreal-world benchmarks and live VR environment demonstrate the effectiveness of\nour approach, achieving superior performance across all metrics and enabling\npractical applications for proactive VR systems that anticipate user behaviors\nand adapt virtual environments accordingly.", "AI": {"tldr": "本文提出一个分层、意图感知的框架，通过建模人类意图和认知机制，预测VR/AR系统中用户的详细情境行为，以实现更智能的交互体验和自适应虚拟环境。", "motivation": "VR/AR系统需要智能地适应用户行为以增强交互体验。这要求准确理解人类意图并预测未来的情境行为（如凝视方向、物体交互），这对于创建响应式VR/AR环境和个性化应用至关重要。然而，准确的行为预测需要对驱动人-环境交互的潜在认知过程进行建模。", "method": "本文引入一个分层、意图感知的框架，通过利用认知机制来建模人类意图并预测详细的情境行为。该框架根据历史人类动态和场景上下文观察，首先识别潜在的交互目标，然后预测精细的未来行为。文中提出了一种动态图卷积网络（GCN）来有效地捕捉人-环境关系。", "result": "在具有挑战性的真实世界基准测试和实时VR环境中进行的广泛实验表明，该方法在所有指标上均表现出卓越的性能。它能够实现主动式VR系统的实际应用，这些系统可以预测用户行为并相应地调整虚拟环境。", "conclusion": "所提出的分层、意图感知的框架通过建模认知机制和利用动态GCN，能够有效预测VR/AR系统中的用户行为，从而实现更智能、自适应的虚拟环境和增强的交互体验。"}}
{"id": "2510.11160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11160", "abs": "https://arxiv.org/abs/2510.11160", "authors": ["Jens Van Nooten", "Andriy Kosar", "Guy De Pauw", "Walter Daelemans"], "title": "One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification", "comment": null, "summary": "Distance-based unsupervised text classification is a method within text\nclassification that leverages the semantic similarity between a label and a\ntext to determine label relevance. This method provides numerous benefits,\nincluding fast inference and adaptability to expanding label sets, as opposed\nto zero-shot, few-shot, and fine-tuned neural networks that require re-training\nin such cases. In multi-label distance-based classification and information\nretrieval algorithms, thresholds are required to determine whether a text\ninstance is \"similar\" to a label or query. Similarity between a text and label\nis determined in a dense embedding space, usually generated by state-of-the-art\nsentence encoders. Multi-label classification complicates matters, as a text\ninstance can have multiple true labels, unlike in multi-class or binary\nclassification, where each instance is assigned only one label. We expand upon\nprevious literature on this underexplored topic by thoroughly examining and\nevaluating the ability of sentence encoders to perform distance-based\nclassification. First, we perform an exploratory study to verify whether the\nsemantic relationships between texts and labels vary across models, datasets,\nand label sets by conducting experiments on a diverse collection of realistic\nmulti-label text classification (MLTC) datasets. We find that similarity\ndistributions show statistically significant differences across models,\ndatasets and even label sets. We propose a novel method for optimizing\nlabel-specific thresholds using a validation set. Our label-specific\nthresholding method achieves an average improvement of 46% over normalized 0.5\nthresholding and outperforms uniform thresholding approaches from previous work\nby an average of 14%. Additionally, the method demonstrates strong performance\neven with limited labeled examples.", "AI": {"tldr": "本文探讨了基于距离的无监督多标签文本分类中句子编码器的性能，并提出了一种使用验证集优化标签特定阈值的新方法，显著提高了分类准确性。", "motivation": "基于距离的无监督文本分类具有推理速度快、对标签集扩展适应性强等优点，但多标签分类中如何确定文本与标签之间的“相似”阈值是一个复杂且未被充分探索的问题。现有的零样本、少样本和微调神经网络在标签集扩展时需要重新训练，而基于距离的方法则无需。此外，文本与标签的语义关系在不同模型、数据集和标签集之间存在差异，需要更精细的阈值策略。", "method": "首先，通过对多样化的多标签文本分类数据集进行实验，进行探索性研究，验证文本和标签之间的语义关系是否在不同模型、数据集和标签集之间存在差异。其次，提出了一种利用验证集优化标签特定阈值的新方法。", "result": "研究发现，相似性分布在不同模型、数据集甚至标签集之间存在统计学上的显著差异。所提出的标签特定阈值优化方法，相比归一化0.5阈值法平均提高了46%，相比以往工作的统一阈值法平均提高了14%。此外，该方法在有限标注样本的情况下也能表现出良好的性能。", "conclusion": "标签特定阈值优化方法显著提升了基于距离的无监督多标签文本分类的性能，有效解决了不同模型、数据集和标签集之间语义关系差异带来的挑战，并且在有限标注数据下依然有效。"}}
{"id": "2510.11167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11167", "abs": "https://arxiv.org/abs/2510.11167", "authors": ["Paloma Piot", "José Ramom Pichel Campos", "Javier Parapar"], "title": "Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages", "comment": null, "summary": "Hate speech poses a serious threat to social cohesion and individual\nwell-being, particularly on social media, where it spreads rapidly. While\nresearch on hate speech detection has progressed, it remains largely focused on\nEnglish, resulting in limited resources and benchmarks for low-resource\nlanguages. Moreover, many of these languages have multiple linguistic\nvarieties, a factor often overlooked in current approaches. At the same time,\nlarge language models require substantial amounts of data to perform reliably,\na requirement that low-resource languages often cannot meet. In this work, we\naddress these gaps by compiling a meta-collection of hate speech datasets for\nEuropean Spanish, standardised with unified labels and metadata. This\ncollection is based on a systematic analysis and integration of existing\nresources, aiming to bridge the data gap and support more consistent and\nscalable hate speech detection. We extended this collection by translating it\ninto European Portuguese and into a Galician standard that is more convergent\nwith Spanish and another Galician variant that is more convergent with\nPortuguese, creating aligned multilingual corpora. Using these resources, we\nestablish new benchmarks for hate speech detection in Iberian languages. We\nevaluate state-of-the-art large language models in zero-shot, few-shot, and\nfine-tuning settings, providing baseline results for future research. Moreover,\nwe perform a cross-lingual analysis with our target languages. Our findings\nunderscore the importance of multilingual and variety-aware approaches in hate\nspeech detection and offer a foundation for improved benchmarking in\nunderrepresented European languages.", "AI": {"tldr": "该研究针对低资源语言（特别是伊比利亚语种）中仇恨言论检测的数据和资源不足问题，构建了一个包含欧洲西班牙语、葡萄牙语及两种加利西亚语变体的多语言仇恨言论数据集，并使用最先进的大型语言模型建立了新的检测基准，强调了多语言和变体感知方法的重要性。", "motivation": "仇恨言论对社会凝聚力和个人福祉构成严重威胁，尤其是在社交媒体上快速传播。现有研究主要集中于英语，导致低资源语言的资源和基准有限，且常忽略这些语言的多种语言变体。同时，大型语言模型（LLMs）需要大量数据才能可靠运行，而低资源语言通常无法满足这一要求。", "method": "研究首先汇编并标准化了一个欧洲西班牙语仇恨言论数据集的元集合，统一了标签和元数据。随后，将该集合翻译成欧洲葡萄牙语以及两种加利西亚语变体（一种更接近西班牙语，另一种更接近葡萄牙语），创建了对齐的多语言语料库。最后，利用这些资源，在零样本、少样本和微调设置下评估了最先进的大型语言模型，并进行了跨语言分析。", "result": "研究为伊比利亚语种的仇恨言论检测建立了新的基准，并为未来的研究提供了基线结果。跨语言分析的结果强调了在仇恨言论检测中采用多语言和变体感知方法的重要性。", "conclusion": "本研究为代表性不足的欧洲语言仇恨言论检测提供了改进基准的坚实基础，并突出了考虑语言多样性和变体在构建鲁棒检测系统中的关键作用。"}}
{"id": "2510.11176", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11176", "abs": "https://arxiv.org/abs/2510.11176", "authors": ["Yesung Cho", "Sungmin Lee", "Geongyu Lee", "Minkyung Lee", "Jongbae Park", "Dongmyung Shin"], "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation", "comment": null, "summary": "Recent studies in pathology foundation models have shown that scaling\ntraining data, diversifying cancer types, and increasing model size\nconsistently improve their performance. However, giga-scale foundation models,\nwhich are trained on hundreds of thousands of slides covering tens of cancer\ntypes and contain billions of parameters, pose significant challenges for\npractical use due to their tremendous computational costs in both development\nand deployment. In this work, we present a novel strategy, named the G2L\nframework, to increase the performance of large-scale foundation models, which\nconsist of only $15\\%$ of the parameters of giga-scale models, to a comparable\nperformance level of giga-scale models in cancer-specific tasks. Our approach\napplies knowledge distillation, transferring the capabilities of a giga-scale\nmodel to a large-scale model, using just 1K pathology slides of a target cancer\n(e.g., breast, prostate, etc.). The resulting distilled model not only\noutperformed state-of-the-art models of the same size (i.e., large-scale)\nacross several benchmarks but also, interestingly, surpassed the giga-scale\nteacher and huge-scale models in some benchmarks. In addition, the distilled\nmodel exhibited a higher robustness index, indicating improved resilience to\nimage variations originating from multiple institutions. These findings suggest\nthat the proposed distillation approach for a large-scale model is a data- and\nparameter-efficient way to achieve giga-scale-level performance for\ncancer-specific applications without prohibitive computational burden.", "AI": {"tldr": "本文提出G2L框架，通过知识蒸馏将千亿级病理基础模型的性能高效迁移至参数量更小的大规模模型，使其在癌症特异性任务上达到甚至超越千亿级模型的性能，同时提高鲁棒性。", "motivation": "病理学基础模型虽然通过扩大训练数据、多样化癌症类型和增加模型规模来提升性能，但千亿级（giga-scale）模型在开发和部署中面临巨大的计算成本挑战，限制了其实用性。", "method": "G2L框架采用知识蒸馏策略，将一个千亿级模型的知识迁移到一个参数量仅为其15%的大规模模型。该过程仅需1K张目标癌症（如乳腺癌、前列腺癌等）的病理切片。", "result": "蒸馏后的大规模模型在多个基准测试中不仅超越了相同规模（大规模）的最新模型，还在某些基准测试中表现优于千亿级教师模型和超大规模模型。此外，蒸馏模型展现出更高的鲁棒性指数，表明对来自不同机构的图像变异具有更强的适应性。", "conclusion": "研究结果表明，所提出的针对大规模模型的蒸馏方法是一种数据和参数高效的方式，可以在没有过高计算负担的情况下，为癌症特异性应用实现千亿级模型水平的性能。"}}
{"id": "2510.11217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11217", "abs": "https://arxiv.org/abs/2510.11217", "authors": ["Chris Xing Tian", "Weihao Xie", "Zhen Chen", "Zhengyuan Yi", "Hui Liu", "Haoliang Li", "Shiqi Wang", "Siwei Ma"], "title": "Domain-Specific Data Generation Framework for RAG Adaptation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) combines the language understanding and\nreasoning power of large language models (LLMs) with external retrieval to\nenable domain-grounded responses. Effectively adapting RAG systems to\ndomain-specific settings requires specialized, context-rich training data\nbeyond general-purpose question-answering. Here, we propose RAGen, a scalable\nand modular framework for generating domain-grounded question-answer-context\n(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces\nthese QAC triples by identifying key concepts in documents, generating diverse\nquestions guided by Bloom's Taxonomy-inspired principles, and pairing them with\nprecise answers extracted from relevant contexts. RAGen supports multiple RAG\nadaptation strategies, including the optimization of key components such as the\nLLM, retriever, and embedding model, etc. Its modular pipeline features\nsemantic chunking, hierarchical concept extraction, and multi-chunk retrieval,\nalong with the introduction of curated distractor contexts to promote robust\nreasoning. Designed for scalability, RAGen efficiently handles large and\nevolving document corpora without redundant processing, making it especially\nsuitable for dynamic evolving domains such as scientific research and\nenterprise knowledge bases.", "AI": {"tldr": "本文提出了 RAGen，一个可扩展的模块化框架，用于生成领域特定的问答-上下文（QAC）三元组，以支持检索增强生成（RAG）系统的适应性，特别是在动态领域中。", "motivation": "有效的RAG系统适应领域特定设置需要超越通用问答的专业化、上下文丰富的训练数据。现有方法可能无法充分满足这一需求。", "method": "RAGen通过识别文档中的关键概念、根据布鲁姆分类法原则生成多样化问题，并从相关上下文中提取精确答案来生成QAC三元组。其模块化流程包括语义分块、分层概念提取、多块检索，并引入了精心策划的干扰上下文以促进鲁棒推理。它支持LLM、检索器和嵌入模型等RAG关键组件的优化。", "result": "RAGen能够为多样化的RAG适应方法生成量身定制的QAC三元组，支持RAG组件的优化。它能高效处理大型和不断演变的文档语料库，避免冗余处理，特别适用于科学研究和企业知识库等动态领域。", "conclusion": "RAGen是一个可扩展、模块化的框架，能够高效生成领域特定的QAC数据，从而促进RAG系统在动态和演变环境中的鲁棒适应和优化。"}}
{"id": "2510.10750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10750", "abs": "https://arxiv.org/abs/2510.10750", "authors": ["Laura Weihl", "Nejc Novak", "Stefan H. Bengtson", "Malte Pedersen"], "title": "Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection", "comment": null, "summary": "Underwater video monitoring is a promising strategy for assessing marine\nbiodiversity, but the vast volume of uneventful footage makes manual inspection\nhighly impractical. In this work, we explore the use of visual anomaly\ndetection (VAD) based on deep neural networks to automatically identify\ninteresting or anomalous events. We introduce AURA, the first multi-annotator\nbenchmark dataset for underwater VAD, and evaluate four VAD models across two\nmarine scenes. We demonstrate the importance of robust frame selection\nstrategies to extract meaningful video segments. Our comparison against\nmultiple annotators reveals that VAD performance of current models varies\ndramatically and is highly sensitive to both the amount of training data and\nthe variability in visual content that defines \"normal\" scenes. Our results\nhighlight the value of soft and consensus labels and offer a practical approach\nfor supporting scientific exploration and scalable biodiversity monitoring.", "AI": {"tldr": "该研究探讨了使用基于深度神经网络的视觉异常检测（VAD）来自动识别水下视频中的有趣事件，以解决手动检查大量水下视频的低效问题。它引入了首个多标注者水下VAD基准数据集AURA，评估了四种VAD模型，并发现VAD性能差异显著，且对训练数据量和“正常”场景的视觉内容变化高度敏感。研究强调了鲁棒帧选择策略和软标签/共识标签的重要性。", "motivation": "水下视频监测是评估海洋生物多样性的有效策略，但大量的无事件录像使得人工检查非常不切实际。", "method": "该研究探索了基于深度神经网络的视觉异常检测（VAD）方法来自动识别异常事件。它引入了AURA，这是第一个用于水下VAD的多标注者基准数据集。研究评估了四种VAD模型在两个海洋场景中的表现，并展示了鲁棒帧选择策略的重要性。此外，它将VAD模型的性能与多位标注者进行了比较。", "result": "当前VAD模型的性能差异巨大，并且对训练数据量以及定义“正常”场景的视觉内容的可变性高度敏感。研究结果强调了鲁棒帧选择策略的重要性，以及软标签和共识标签的价值。", "conclusion": "VAD为支持科学探索和可扩展的生物多样性监测提供了一种实用的方法，尽管当前模型的性能存在显著差异并对训练数据和“正常”场景的视觉内容变化高度敏感。"}}
{"id": "2510.10753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10753", "abs": "https://arxiv.org/abs/2510.10753", "authors": ["Kagan Ozturk", "Aman Bhatta", "Haiyu Wu", "Patrick Flynn", "Kevin W. Bowyer"], "title": "Restricted Receptive Fields for Face Verification", "comment": null, "summary": "Understanding how deep neural networks make decisions is crucial for\nanalyzing their behavior and diagnosing failure cases. In computer vision, a\ncommon approach to improve interpretability is to assign importance to\nindividual pixels using post-hoc methods. Although they are widely used to\nexplain black-box models, their fidelity to the model's actual reasoning is\nuncertain due to the lack of reliable evaluation metrics. This limitation\nmotivates an alternative approach, which is to design models whose decision\nprocesses are inherently interpretable. To this end, we propose a face\nsimilarity metric that breaks down global similarity into contributions from\nrestricted receptive fields. Our method defines the similarity between two face\nimages as the sum of patch-level similarity scores, providing a locally\nadditive explanation without relying on post-hoc analysis. We show that the\nproposed approach achieves competitive verification performance even with\npatches as small as 28x28 within 112x112 face images, and surpasses\nstate-of-the-art methods when using 56x56 patches.", "AI": {"tldr": "本文提出一种可解释的人脸相似度度量方法，通过将全局相似度分解为局部感受野的贡献，提供无需后验分析的局部可加解释，并在验证任务中表现出色。", "motivation": "现有深度神经网络的后验解释方法（如像素重要性分配）缺乏可靠的评估指标，导致其对模型实际推理的忠实度存疑。因此，研究动机是设计决策过程本身具有内在可解释性的模型。", "method": "本文提出一种人脸相似度度量方法，将两张人脸图像的相似度定义为补丁（patch）级别相似度分数的总和。这种方法通过限制感受野，将全局相似度分解为局部贡献，从而提供局部可加的解释，且无需依赖后验分析。", "result": "所提出的方法在人脸验证任务中表现出竞争力。即使使用112x112人脸图像中28x28大小的补丁，也能达到竞争性性能；当使用56x56大小的补丁时，其性能超越了现有最先进的方法。", "conclusion": "本文成功开发了一种具有内在可解释性的人脸相似度度量方法，通过局部补丁贡献实现可解释性，并同时在人脸验证任务中取得了优异的性能，解决了传统解释方法忠实度不确定的问题。"}}
{"id": "2510.11210", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11210", "abs": "https://arxiv.org/abs/2510.11210", "authors": ["Yisong Miao", "Min-Yen Kan"], "title": "Discursive Circuits: How Do Language Models Understand Discourse Relations?", "comment": "Accepted to EMNLP 2025 (Main Conference); 9 pages, 8 figures, 5\n  tables (20 pages, 12 figures, 14 tables including references and appendices)", "summary": "Which components in transformer language models are responsible for discourse\nunderstanding? We hypothesize that sparse computational graphs, termed as\ndiscursive circuits, control how models process discourse relations. Unlike\nsimpler tasks, discourse relations involve longer spans and complex reasoning.\nTo make circuit discovery feasible, we introduce a task called Completion under\nDiscourse Relation (CuDR), where a model completes a discourse given a\nspecified relation. To support this task, we construct a corpus of minimal\ncontrastive pairs tailored for activation patching in circuit discovery.\nExperiments show that sparse circuits ($\\approx 0.2\\%$ of a full GPT-2 model)\nrecover discourse understanding in the English PDTB-based CuDR task. These\ncircuits generalize well to unseen discourse frameworks such as RST and SDRT.\nFurther analysis shows lower layers capture linguistic features such as lexical\nsemantics and coreference, while upper layers encode discourse-level\nabstractions. Feature utility is consistent across frameworks (e.g.,\ncoreference supports Expansion-like relations).", "AI": {"tldr": "该研究通过发现“语篇回路”，揭示了Transformer语言模型中负责语篇理解的稀疏计算图，并分析了不同层级捕获的语篇特征。", "motivation": "研究动机是识别Transformer语言模型中哪些组件负责语篇理解，因为语篇关系涉及更长的跨度和复杂的推理，不同于简单任务。研究假设稀疏计算图（即语篇回路）控制模型处理语篇关系的方式。", "method": "为实现回路发现，研究引入了“语篇关系下补全”（CuDR）任务，模型需根据指定关系补全语篇。为此构建了一个针对回路发现中激活修补的最小对比对语料库。实验通过激活修补技术，在GPT-2模型上发现了稀疏回路。", "result": "实验结果表明，稀疏回路（约占完整GPT-2模型的0.2%）能恢复基于PDTB的CuDR任务中的语篇理解能力，并能很好地泛化到RST和SDRT等未见过的语篇框架。进一步分析显示，模型的下层捕获词汇语义和共指等语言特征，而上层则编码语篇层面的抽象概念。特征效用在不同框架中保持一致（例如，共指支持扩展类关系）。", "conclusion": "该研究得出结论，Transformer模型中存在稀疏的“语篇回路”负责语篇理解，这些回路能有效泛化。模型内部存在分层处理机制，下层处理基础语言特征，上层处理高级语篇抽象，且特征效用具有跨框架的一致性。"}}
{"id": "2510.11221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11221", "abs": "https://arxiv.org/abs/2510.11221", "authors": ["Tao Li", "Jinlong Hu", "Yang Wang", "Junfeng Liu", "Xuejun Liu"], "title": "WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent", "comment": null, "summary": "LLM-brained web agents offer powerful capabilities for web automation but\nface a critical cost-performance trade-off. The challenge is amplified by web\nagents' inherently complex prompts that include goals, action histories, and\nenvironmental states, leading to degraded LLM ensemble performance. To address\nthis, we introduce WebRouter, a novel query-specific router trained from an\ninformation-theoretic perspective. Our core contribution is a cost-aware\nVariational Information Bottleneck (ca-VIB) objective, which learns a\ncompressed representation of the input prompt while explicitly penalizing the\nexpected operational cost. Experiments on five real-world websites from the\nWebVoyager benchmark show that WebRouter reduces operational costs by a\nstriking 87.8\\% compared to a GPT-4o baseline, while incurring only a 3.8\\%\naccuracy drop.", "AI": {"tldr": "WebRouter是一个新颖的查询特定路由器，它通过成本感知变分信息瓶颈（ca-VIB）目标，显著降低了大型语言模型（LLM）驱动的网页代理的操作成本，同时保持了高精度。", "motivation": "LLM驱动的网页代理在网页自动化方面能力强大，但面临着严峻的成本-性能权衡。网页代理固有的复杂提示（包含目标、行动历史和环境状态）进一步加剧了这一挑战，导致LLM整体性能下降。", "method": "本文引入了WebRouter，一个从信息论角度训练的新型查询特定路由器。其核心贡献是成本感知变分信息瓶颈（ca-VIB）目标，该目标在学习输入提示的压缩表示的同时，明确惩罚预期的操作成本。", "result": "在WebVoyager基准测试的五个真实世界网站上的实验表明，与GPT-4o基线相比，WebRouter将操作成本降低了87.8%，而准确性仅下降了3.8%。", "conclusion": "WebRouter通过其成本感知的信息瓶颈方法，成功解决了LLM驱动的网页代理在操作成本和性能之间的权衡问题，实现了显著的成本节约和可接受的准确性损失。"}}
{"id": "2510.10765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10765", "abs": "https://arxiv.org/abs/2510.10765", "authors": ["Sudipto Sarkar", "Mohammad Asif Hasan", "Khondokar Ashik Shahriar", "Fablia Labiba", "Nahian Tasnim", "Sheikh Anawarul Haq Fattah"], "title": "EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition", "comment": null, "summary": "Identifying drones and birds correctly is essential for keeping the skies\nsafe and improving security systems. Using the VIP CUP 2025 dataset, which\nprovides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a\nnew lightweight yet powerful model for object detection. The model improves how\nimage features are captured and understood, making detection more accurate and\nefficient. It uses smart design changes and attention layers to focus on\nimportant details while reducing the amount of computation needed. A special\ndetection head helps the model adapt to objects of different shapes and sizes.\nWe trained three versions: one using RGB images, one using IR images, and one\ncombining both. The combined model achieved the best accuracy and reliability\nwhile running fast enough for real-time use on common GPUs.", "AI": {"tldr": "本研究提出EGD-YOLOv8n模型，利用RGB和红外图像在VIP CUP 2025数据集上实现轻量级、高效且准确的无人机和鸟类实时检测。", "motivation": "正确识别无人机和鸟类对于维护天空安全和提升安保系统至关重要。", "method": "研究开发了EGD-YOLOv8n模型，一个轻量级但强大的目标检测模型。它通过智能设计改进和引入注意力层来增强图像特征捕获和理解，同时减少计算量。模型还采用了一个特殊的检测头以适应不同形状和大小的物体。训练了RGB、红外以及结合RGB和红外图像的三种版本，并使用了VIP CUP 2025数据集。", "result": "结合RGB和红外图像的模型版本取得了最佳的准确性和可靠性。该模型运行速度足够快，可在普通GPU上实现实时应用。", "conclusion": "EGD-YOLOv8n模型提供了一个强大、高效且可实时运行的解决方案，能准确可靠地检测无人机和鸟类，尤其在结合RGB和红外图像时表现最佳。"}}
{"id": "2510.11196", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11196", "abs": "https://arxiv.org/abs/2510.11196", "authors": ["Johannes Moll", "Markus Graf", "Tristan Lemke", "Nicolas Lenhart", "Daniel Truhn", "Jean-Benoit Delbrouck", "Jiazhen Pan", "Daniel Rueckert", "Lisa C. Adams", "Keno K. Bressem"], "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations", "comment": null, "summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT)\nexplanations that sound plausible yet fail to reflect the underlying decision\nprocess, undermining trust in high-stakes clinical use. Existing evaluations\nrarely catch this misalignment, prioritizing answer accuracy or adherence to\nformats. We present a clinically grounded framework for chest X-ray visual\nquestion answering (VQA) that probes CoT faithfulness via controlled text and\nimage modifications across three axes: clinical fidelity, causal attribution,\nand confidence calibration. In a reader study (n=4), evaluator-radiologist\ncorrelations fall within the observed inter-radiologist range for all axes,\nwith strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate\nalignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone\n($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows\nthat answer accuracy and explanation quality are decoupled, acknowledging\ninjected cues does not ensure grounding, and text cues shift explanations more\nthan visual cues. While some open-source models match final answer accuracy,\nproprietary models score higher on attribution (25.0% vs. 1.4%) and often on\nfidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to\nevaluate beyond final answer accuracy.", "AI": {"tldr": "该研究提出了一个用于胸部X射线视觉问答（VQA）的临床验证框架，通过控制文本和图像修改来评估视觉语言模型（VLM）链式思考（CoT）解释的忠实性，发现模型答案准确性与解释质量脱节，并揭示了专有模型在归因和忠实性方面优于开源模型，强调了超越最终答案准确性进行评估的必要性。", "motivation": "视觉语言模型（VLM）生成的链式思考（CoT）解释可能听起来合理但未能反映其潜在决策过程，这在临床等高风险应用中会损害信任。现有评估方法通常只关注答案准确性或格式，难以捕捉这种不一致。", "method": "研究提出了一个以临床为基础的胸部X射线VQA框架，通过在临床忠实度、因果归因和信心校准三个维度上对文本和图像进行受控修改，来探究CoT解释的忠实性。通过一项有四名评估放射科医生参与的读者研究验证了该框架，并使用该框架对六个VLM进行了基准测试。", "result": "评估放射科医生在所有维度上的相关性均在放射科医生间差异范围内：归因维度相关性强（Kendall's $\\tau_b=0.670$），忠实度维度中等（$\\tau_b=0.387$），信心语气维度较弱（$\\tau_b=0.091$）。基准测试显示，VLM的答案准确性与解释质量是脱钩的；承认注入的线索并不能保证其基础性；文本线索比视觉线索更能改变解释。虽然一些开源模型在最终答案准确性上表现相当，但专有模型在归因（25.0% vs. 1.4%）和忠实度（36.1% vs. 31.7%）上得分更高。", "conclusion": "VLM在临床应用中存在部署风险，需要超越最终答案准确性来评估模型，特别是要关注其解释的忠实性。答案准确性与解释质量的脱节以及专有模型在解释能力上的优势，凸显了更全面评估框架的重要性。"}}
{"id": "2510.11151", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11151", "abs": "https://arxiv.org/abs/2510.11151", "authors": ["Alexander Sternfeld", "Andrei Kucharavy", "Ljiljana Dolamic"], "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code", "comment": null, "summary": "Large language Models (LLMs) have shown remarkable proficiency in code\ngeneration tasks across various programming languages. However, their outputs\noften contain subtle but critical vulnerabilities, posing significant risks\nwhen deployed in security-sensitive or mission-critical systems. This paper\nintroduces TypePilot, an agentic AI framework designed to enhance the security\nand robustness of LLM-generated code by leveraging strongly typed and\nverifiable languages, using Scala as a representative example. We evaluate the\neffectiveness of our approach in two settings: formal verification with the\nStainless framework and general-purpose secure code generation. Our experiments\nwith leading open-source LLMs reveal that while direct code generation often\nfails to enforce safety constraints, just as naive prompting for more secure\ncode, our type-focused agentic pipeline substantially mitigates input\nvalidation and injection vulnerabilities. The results demonstrate the potential\nof structured, type-guided LLM workflows to improve the SotA of the\ntrustworthiness of automated code generation in high-assurance domains.", "AI": {"tldr": "大型语言模型（LLMs）生成的代码存在安全漏洞。本文提出了TypePilot，一个利用强类型和可验证语言（如Scala）的智能体AI框架，显著提高了LLM生成代码的安全性与鲁棒性，有效缓解了输入验证和注入漏洞。", "motivation": "大型语言模型（LLMs）在代码生成方面表现出色，但其输出常包含细微但关键的漏洞，这在安全敏感或任务关键系统中会带来重大风险。", "method": "本文引入了TypePilot，一个智能体AI框架。该框架通过利用强类型和可验证语言（以Scala为例）来增强LLM生成代码的安全性和鲁棒性。研究在两种设置下评估了其有效性：使用Stainless框架进行形式验证和通用安全代码生成。", "result": "实验表明，直接代码生成和简单的安全提示都未能有效强制执行安全约束。相比之下，TypePilot这种以类型为中心的智能体管道显著缓解了输入验证和注入漏洞。", "conclusion": "研究结果证明了结构化、类型引导的LLM工作流（如TypePilot）在提高高保障领域自动化代码生成可信度方面的巨大潜力。"}}
{"id": "2510.10998", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10998", "abs": "https://arxiv.org/abs/2510.10998", "authors": ["Mahika Phutane", "Hayoung Jung", "Matthew Kim", "Tanushree Mitra", "Aditya Vashistha"], "title": "ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios", "comment": "28 pages, 11 figures, 16 tables. In submission", "summary": "Large language models (LLMs) are increasingly under scrutiny for perpetuating\nidentity-based discrimination in high-stakes domains such as hiring,\nparticularly against people with disabilities (PwD). However, existing research\nremains largely Western-centric, overlooking how intersecting forms of\nmarginalization--such as gender and caste--shape experiences of PwD in the\nGlobal South. We conduct a comprehensive audit of six LLMs across 2,820 hiring\nscenarios spanning diverse disability, gender, nationality, and caste profiles.\nTo capture subtle intersectional harms and biases, we introduce ABLEIST\n(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five\nableism-specific and three intersectional harm metrics grounded in disability\nstudies literature. Our results reveal significant increases in ABLEIST harms\ntowards disabled candidates--harms that many state-of-the-art models failed to\ndetect. These harms were further amplified by sharp increases in intersectional\nharms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,\nhighlighting critical blind spots in current safety tools and the need for\nintersectional safety evaluations of frontier models in high-stakes domains\nlike hiring.", "AI": {"tldr": "该研究审计了六个大型语言模型在招聘场景中对残障人士的歧视，特别是结合了性别和种姓的交叉歧视。结果显示，模型对残障候选人存在显著的歧视性伤害，且对边缘化群体的交叉歧视更为严重，揭示了当前安全工具的不足。", "motivation": "大型语言模型在招聘等高风险领域存在基于身份的歧视，特别是针对残障人士。现有研究多以西方为中心，忽视了全球南方地区性别和种姓等交叉边缘化因素如何影响残障人士的经历。", "method": "研究对六个大型语言模型进行了全面审计，使用了2,820个涵盖不同残障、性别、国籍和种姓特征的招聘场景。引入了ABLEIST（能力歧视、励志化、超人化和象征性）指标，这是一套包含五个特定于能力歧视和三个交叉歧视伤害的度量标准，这些标准基于残障研究文献。", "result": "研究发现，模型对残障候选人表现出显著增加的ABLEIST伤害，其中许多最先进的模型未能检测到这些伤害。对于性别和种姓边缘化的残障候选人，交叉歧视伤害（例如象征性歧视）进一步急剧增加。", "conclusion": "当前的安全工具存在关键盲点，无法有效检测和缓解大型语言模型在招聘等高风险领域中对残障人士，尤其是结合性别和种姓等交叉因素的歧视性伤害。因此，需要对前沿模型进行交叉安全评估。"}}
{"id": "2510.11222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11222", "abs": "https://arxiv.org/abs/2510.11222", "authors": ["Battemuulen Naranbat", "Seyed Sahand Mohammadi Ziabari", "Yousuf Nasser Al Husaini", "Ali Mohammed Mansoor Alsahag"], "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models", "comment": null, "summary": "Ensuring fairness in natural language processing for moral sentiment\nclassification is challenging, particularly under cross-domain shifts where\ntransformer models are increasingly deployed. Using the Moral Foundations\nTwitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work\nevaluates BERT and DistilBERT in a multi-label setting with in-domain and\ncross-domain protocols. Aggregate performance can mask disparities: we observe\npronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by\n14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness\nviolations hidden by overall scores; notably, the authority label exhibits\nDemographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of\n0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency\n(MFC) metric, which quantifies the cross-domain stability of moral foundation\ndetection. MFC shows strong empirical validity, achieving a perfect negative\ncorrelation with Demographic Parity Difference (rho = -1.000, p < 0.001) while\nremaining independent of standard performance metrics. Across labels, loyalty\ndemonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC\n= 0.78). These findings establish MFC as a complementary, diagnosis-oriented\nmetric for fairness-aware evaluation of moral reasoning models, enabling more\nreliable deployment across heterogeneous linguistic contexts. .", "AI": {"tldr": "该研究发现，在跨领域道德情感分类中，Transformer模型存在显著的公平性问题和性能不对称，尤其是在Twitter到Reddit的迁移中。为解决此问题，引入了道德公平一致性（MFC）指标，该指标能有效量化跨领域稳定性并诊断公平性违规。", "motivation": "在道德情感分类的自然语言处理中，特别是在Transformer模型日益普及的跨领域迁移情境下，确保公平性是一项挑战。聚合性能往往会掩盖潜在的不公平现象。", "method": "研究使用道德基础Twitter语料库（MFTC）和道德基础Reddit语料库（MFRC），在多标签设置下，通过域内和跨域协议评估了BERT和DistilBERT模型。此外，引入了道德公平一致性（MFC）指标来量化道德基础检测的跨领域稳定性。", "result": "研究发现跨域迁移存在显著的不对称性（Twitter->Reddit的micro-F1下降14.9%，而Reddit->Twitter仅下降1.5%）。整体分数掩盖了标签层面的公平性违规，例如“权威”标签的统计均等差异为0.22-0.23，机会均等差异为0.40-0.41。MFC指标与统计均等差异呈完美负相关（rho = -1.000, p < 0.001），且独立于标准性能指标。在所有标签中，“忠诚”表现出最高的一致性（MFC = 0.96），而“权威”最低（MFC = 0.78）。", "conclusion": "MFC指标被确立为一种补充性的、以诊断为导向的公平性评估指标，适用于道德推理模型，有助于在异构语言环境中实现更可靠的模型部署。"}}
{"id": "2510.10974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10974", "abs": "https://arxiv.org/abs/2510.10974", "authors": ["Zhiwen Ruan", "Yixia Li", "He Zhu", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning", "comment": null, "summary": "Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)\nas a key method to adapt pre-trained models to domain-specific tasks such as\nmathematical reasoning. However, standard SFT uniformly penalizes all tokens,\nneglecting that only a small subset of critical tokens determines reasoning\ncorrectness. This uniform supervision often causes reduced output diversity and\nlimited generalization. We propose Critical Token Fine-tuning (CFT), a simple\nyet effective approach that updates only tokens identified as functionally\nindispensable via counterfactual perturbations. By focusing gradient signals on\nthese decisive reasoning steps while preserving the diversity of non-critical\ntokens, CFT can enhance both generation and diversity. Extensive experiments on\nfive models across three families (Qwen, OLMo, LLaMA) and eleven mathematical\nreasoning benchmarks show that CFT, despite fine-tuning on less than 12% of\ntokens, consistently outperforms standard SFT. Moreover, CFT enables test-time\nscaling through improved sampling diversity and provides a stronger\ninitialization for reinforcement learning, sustaining performance gains in\nlater training stages while maintaining higher entropy for better exploration.\nThese results highlight CFT as a practical and general framework for efficient\nand robust LLM fine-tuning.", "AI": {"tldr": "本文提出关键令牌微调 (CFT) 方法，通过仅更新对推理正确性至关重要的令牌，显著优于标准监督微调 (SFT)，提高了大型语言模型 (LLM) 在数学推理任务上的性能和多样性。", "motivation": "大型语言模型在领域特定任务（如数学推理）中主要依赖监督微调 (SFT)。然而，标准 SFT 对所有令牌进行统一惩罚，忽略了只有一小部分关键令牌决定推理的正确性。这种统一监督通常会导致输出多样性降低和泛化能力受限。", "method": "本文提出关键令牌微调 (CFT) 方法。该方法通过反事实扰动识别出功能上不可或缺的令牌，并仅更新这些令牌。通过将梯度信号集中在这些决定性的推理步骤上，同时保留非关键令牌的多样性，CFT 旨在增强生成能力和多样性。", "result": "在三个模型家族（Qwen、OLMo、LLaMA）的五种模型和十一个数学推理基准上进行了广泛实验。结果表明，尽管 CFT 仅对不到 12% 的令牌进行微调，但其性能始终优于标准 SFT。此外，CFT 通过改进采样多样性实现了测试时扩展，并为强化学习提供了更强的初始化，在后续训练阶段保持了性能提升，同时维持了更高的探索熵。", "conclusion": "这些结果表明，CFT 是一种实用且通用的框架，可用于高效、鲁棒的大型语言模型微调。"}}
{"id": "2510.11232", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11232", "abs": "https://arxiv.org/abs/2510.11232", "authors": ["Neilansh Chauhan", "Piyush Kumar Gupta", "Faraz Doja"], "title": "LightPneumoNet: Lightweight Pneumonia Classifier", "comment": "13 pages (including references), 5 figures", "summary": "Effective pneumonia diagnosis is often challenged by the difficulty of\ndeploying large, computationally expensive deep learning models in\nresource-limited settings. This study introduces LightPneumoNet, an efficient,\nlightweight convolutional neural network (CNN) built from scratch to provide an\naccessible and accurate diagnostic solution for pneumonia detection from chest\nX-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.\nPreprocessing included image resizing to 224x224, grayscale conversion, and\npixel normalization, with data augmentation (rotation, zoom, shear) to prevent\noverfitting. The custom architecture features four blocks of stacked\nconvolutional layers and contains only 388,082 trainable parameters, resulting\nin a minimal 1.48 MB memory footprint. On the independent test set, our model\ndelivered exceptional performance, achieving an overall accuracy of 0.942,\nprecision of 0.92, and an F1-Score of 0.96. Critically, it obtained a\nsensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify\ntrue pneumonia cases and minimize clinically significant false negatives.\nNotably, LightPneumoNet achieves this high recall on the same dataset where\nexisting approaches typically require significantly heavier architectures or\nfail to reach comparable sensitivity levels. The model's efficiency enables\ndeployment on low-cost hardware, making advanced computer-aided diagnosis\naccessible in underserved clinics and serving as a reliable second-opinion tool\nto improve patient outcomes.", "AI": {"tldr": "LightPneumoNet是一种轻量级卷积神经网络，专为资源受限环境下的肺炎胸部X射线诊断设计，以极小的内存占用实现了高准确性和近乎完美的敏感性。", "motivation": "在资源受限的环境中，部署大型、计算成本高的深度学习模型进行肺炎诊断面临挑战。", "method": "研究从零开始构建了LightPneumoNet，这是一个高效、轻量级的卷积神经网络。模型在包含5,856张胸部X射线图像的公开数据集上进行训练。预处理包括图像大小调整（224x224）、灰度转换和像素归一化，并采用数据增强（旋转、缩放、剪切）以防止过拟合。该定制架构包含四个堆叠卷积层块，仅有388,082个可训练参数，内存占用为1.48 MB。", "result": "在独立的测试集上，LightPneumoNet表现出色，整体准确率达到0.942，精确度为0.92，F1-Score为0.96。尤其重要的是，其敏感性（召回率）高达0.99，表明其识别真实肺炎病例的能力接近完美，并最大限度地减少了临床上重要的假阴性。值得注意的是，LightPneumoNet在相同数据集上实现了高召回率，而现有方法通常需要更重的架构或未能达到可比的敏感性水平。", "conclusion": "LightPneumoNet的高效率使其能够部署在低成本硬件上，从而使先进的计算机辅助诊断在服务不足的诊所中普及，并作为可靠的第二意见工具，改善患者预后。"}}
{"id": "2510.10779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10779", "abs": "https://arxiv.org/abs/2510.10779", "authors": ["Theo Di Piazza", "Carole Lazarus", "Olivier Nempont", "Loic Boussel"], "title": "Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans", "comment": "22 pages, 14 figures", "summary": "With the growing volume of CT examinations, there is an increasing demand for\nautomated tools such as organ segmentation, abnormality detection, and report\ngeneration to support radiologists in managing their clinical workload.\nMulti-label classification of 3D Chest CT scans remains a critical yet\nchallenging problem due to the complex spatial relationships inherent in\nvolumetric data and the wide variability of abnormalities. Existing methods\nbased on 3D convolutional neural networks struggle to capture long-range\ndependencies, while Vision Transformers often require extensive pre-training on\nlarge-scale, domain-specific datasets to perform competitively. In this work,\nwe propose a 2.5D alternative by introducing a new graph-based framework that\nrepresents 3D CT volumes as structured graphs, where axial slice triplets serve\nas nodes processed through spectral graph convolution, enabling the model to\nreason over inter-slice dependencies while maintaining complexity compatible\nwith clinical deployment. Our method, trained and evaluated on 3 datasets from\nindependent institutions, achieves strong cross-dataset generalization, and\nshows competitive performance compared to state-of-the-art visual encoders. We\nfurther conduct comprehensive ablation studies to evaluate the impact of\nvarious aggregation strategies, edge-weighting schemes, and graph connectivity\npatterns. Additionally, we demonstrate the broader applicability of our\napproach through transfer experiments on automated radiology report generation\nand abdominal CT data.\\\\ This work extends our previous contribution presented\nat the MICCAI 2025 EMERGE Workshop.", "AI": {"tldr": "本文提出了一种2.5D图基框架，通过将3D CT体积表示为结构化图，并使用谱图卷积处理轴向切片三联体作为节点，以实现胸部CT扫描的多标签分类，解决了现有方法在长距离依赖和预训练数据方面的挑战。", "motivation": "随着CT检查量的增加，对自动化工具（如器官分割、异常检测、报告生成）的需求日益增长。3D胸部CT扫描的多标签分类是一个关键但具有挑战性的问题，因为体积数据中固有的复杂空间关系和异常的广泛变异性。现有的3D卷积神经网络难以捕获长距离依赖，而Vision Transformers通常需要大量领域特定数据集进行预训练才能达到竞争力。", "method": "我们提出了一种2.5D替代方案，引入了一个新的图基框架。该框架将3D CT体积表示为结构化图，其中轴向切片三联体作为节点，通过谱图卷积进行处理，从而使模型能够推理切片间依赖关系，同时保持与临床部署兼容的复杂性。我们还在聚合策略、边缘加权方案和图连接模式方面进行了全面的消融研究，并通过自动化放射学报告生成和腹部CT数据上的迁移实验，展示了该方法的广泛适用性。", "result": "我们的方法在来自独立机构的3个数据集上进行训练和评估，实现了强大的跨数据集泛化能力，并与最先进的视觉编码器相比展现出有竞争力的性能。此外，我们通过迁移实验证明了该方法在自动化放射学报告生成和腹部CT数据上的广泛适用性。", "conclusion": "本文提出的2.5D图基框架有效地解决了3D胸部CT扫描多标签分类的挑战，通过捕获切片间依赖关系，实现了强大的泛化能力和有竞争力的性能，并具有广泛的适用性，为临床工作负载管理提供了有潜力的自动化工具。"}}
{"id": "2510.11225", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11225", "abs": "https://arxiv.org/abs/2510.11225", "authors": ["Hayate Funakura", "Hyunsoo Kim", "Koji Mineshima"], "title": "A Theorem-Proving-Based Evaluation of Neural Semantic Parsing", "comment": "Accepted to BlackboxNLP 2025", "summary": "Graph-matching metrics such as Smatch are the de facto standard for\nevaluating neural semantic parsers, yet they capture surface overlap rather\nthan logical equivalence. We reassess evaluation by pairing graph-matching with\nautomated theorem proving. We compare two approaches to building parsers:\nsupervised fine-tuning (T5-Small/Base) and few-shot in-context learning\n(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs\nusing graph-matching, bidirectional entailment between source and target\nformulas with a first-order logic theorem prover, and well-formedness. Across\nsettings, we find that models performing well on graph-matching often fail to\nproduce logically equivalent formulas. Normalization reduces incidental target\nvariability, improves well-formedness, and strengthens logical adequacy. Error\nanalysis shows performance degrades with increasing formula complexity and with\ncoordination, prepositional phrases, and passive voice; the dominant failures\ninvolve variable binding and indexing, and predicate naming. These findings\nhighlight limits of graph-based metrics for reasoning-oriented applications and\nmotivate logic-sensitive evaluation and training objectives together with\nsimplified, normalized target representations. All code and data for our\nexperiments are publicly available.", "AI": {"tldr": "本研究指出图匹配度量（如Smatch）在评估神经语义解析器时无法捕捉逻辑等价性。通过结合图匹配和自动定理证明，并比较不同模型和目标表示，发现图匹配高分常不等于逻辑等价。研究强调需要逻辑敏感的评估和训练目标，以及简化的规范化目标表示。", "motivation": "图匹配度量（如Smatch）是评估神经语义语义解析器的标准方法，但它们仅捕获表面重叠而非逻辑等价性，这促使研究者重新评估解析器的评估方法。", "method": "研究方法是将图匹配与自动定理证明相结合进行评估。比较了两种解析器构建方法：监督微调（T5-Small/Base）和少量样本上下文学习（GPT-4o/4.1/5），分别在规范化和非规范化目标下进行。评估输出时使用了图匹配、基于一阶逻辑定理证明器的源公式与目标公式间的双向蕴含以及格式良好性。同时进行了错误分析。", "result": "研究发现，在图匹配上表现良好的模型，在生成逻辑等价公式方面往往失败。规范化能减少目标偶然变异性，提高格式良好性，并增强逻辑充分性。错误分析表明，性能随公式复杂性、并列结构、介词短语和被动语态的增加而下降；主要失败涉及变量绑定和索引以及谓词命名。", "conclusion": "这些发现揭示了基于图的度量在面向推理的应用中的局限性，并促使采用逻辑敏感的评估和训练目标，以及简化、规范化的目标表示。"}}
{"id": "2510.10793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10793", "abs": "https://arxiv.org/abs/2510.10793", "authors": ["Rolandos Alexandros Potamias", "Stathis Galanakis", "Jiankang Deng", "Athanasios Papaioannou", "Stefanos Zafeiriou"], "title": "ImHead: A Large-scale Implicit Morphable Model for Localized Head Modeling", "comment": "ICCV 2025", "summary": "Over the last years, 3D morphable models (3DMMs) have emerged as a\nstate-of-the-art methodology for modeling and generating expressive 3D avatars.\nHowever, given their reliance on a strict topology, along with their linear\nnature, they struggle to represent complex full-head shapes. Following the\nadvent of deep implicit functions, we propose imHead, a novel implicit 3DMM\nthat not only models expressive 3D head avatars but also facilitates localized\nediting of the facial features. Previous methods directly divided the latent\nspace into local components accompanied by an identity encoding to capture the\nglobal shape variations, leading to expensive latent sizes. In contrast, we\nretain a single compact identity space and introduce an intermediate\nregion-specific latent representation to enable local edits. To train imHead,\nwe curate a large-scale dataset of 4K distinct identities, making a\nstep-towards large scale 3D head modeling. Under a series of experiments we\ndemonstrate the expressive power of the proposed model to represent diverse\nidentities and expressions outperforming previous approaches. Additionally, the\nproposed approach provides an interpretable solution for 3D face manipulation,\nallowing the user to make localized edits.", "AI": {"tldr": "imHead是一种新型隐式3DMM，能够建模富有表现力的3D头部化身，并支持面部特征的局部编辑，克服了传统3DMM的局限性。", "motivation": "传统的3D可变形模型（3DMMs）由于严格的拓扑结构和线性特性，难以表示复杂的完整头部形状。研究旨在开发一种能够建模富有表现力的3D头部化身并支持局部编辑的方法。", "method": "本文提出了imHead，一种新型隐式3DMM。与以往将潜在空间直接划分为局部组件的方法不同，imHead保留了一个紧凑的单一身份空间，并引入了中间区域特定的潜在表示来实现局部编辑。为了训练imHead，研究人员整理了一个包含4K个不同身份的大规模数据集。", "result": "实验表明，所提出的模型在表示多样化身份和表情方面具有强大的表现力，优于现有方法。此外，该方法提供了一个可解释的3D面部操作解决方案，允许用户进行局部编辑。", "conclusion": "imHead是一种功能强大且可解释的隐式3DMM，能够生成富有表现力的3D头部化身，并实现对局部面部特征的精确编辑，为大规模3D头部建模迈出了重要一步。"}}
{"id": "2510.11218", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11218", "abs": "https://arxiv.org/abs/2510.11218", "authors": ["Saad Obaid ul Islam", "Anne Lauscher", "Goran Glavaš"], "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers", "comment": null, "summary": "Large language models (LLMs) can correctly answer \"When was Einstein born?\"\nyet fail to provide the same date when writing about Einstein's life revealing\na fundamental inconsistency in how models access factual knowledge across task\ncomplexities. While models display impressive accuracy on factual\nquestion-answering benchmarks, the reliability gap between simple and complex\nqueries remains poorly understood, eroding their trustworthiness. In this work,\nwe introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a\ncontrolled evaluation framework that compares LLMs' answers to the same factual\nquestions asked (a) in isolation (short) vs. (b) integrated into complex\nqueries (long). Looking at 16 LLMs across 600 queries, we find a systematic\nmisalignment of answers to the corresponding short and long queries. We further\nuncover position-dependent accuracy loss and momentum effects where consecutive\ncorrect or incorrect answers create self-reinforcing patterns. Through\nmechanistic analysis, we find that aligned facts activate overlapping model\ninternals, and that metrics based on mechanistic similarity can predict\nshort-long answer alignment with up to 78% accuracy. Our work establishes\nfactual consistency over query complexity as an important aspect of LLMs'\ntrustworthiness and challenges current evaluation practices, which implicitly\nassume that good performance for simple factual queries implies reliability in\nmore complex knowledge-seeking tasks too.", "AI": {"tldr": "本研究发现大型语言模型在处理简单（独立）和复杂（集成）事实查询时存在系统性不一致，引入SLAQ框架进行评估，并揭示了答案错位、位置依赖性准确性下降和动量效应，强调了查询复杂性下事实一致性对LLM可信度的重要性。", "motivation": "大型语言模型在回答简单事实问题时表现出色，但在将相同事实整合到复杂查询中时却可能失败，这暴露出模型在不同任务复杂性下访问事实知识存在根本性不一致。这种简单与复杂查询之间的可靠性差距尚不清楚，损害了模型的信任度。当前评估方法隐含假设简单查询表现好就意味着复杂任务可靠，这种假设可能不成立。", "method": "引入了Short-Long Form Alignment for Factual Question Answering (SLAQ) 评估框架，用于比较LLM对相同事实问题在两种情境下的回答：(a) 独立提问（短形式）和 (b) 集成到复杂查询中（长形式）。研究评估了16个LLM，使用了600个查询。通过机械分析（mechanistic analysis）探讨了对齐事实如何激活模型内部重叠部分，并使用基于机械相似性的指标来预测短-长答案对齐。", "result": "研究发现，LLM对相应短形式和长形式查询的答案存在系统性错位。进一步揭示了位置依赖性的准确性损失，以及动量效应，即连续的正确或不正确答案会形成自我强化的模式。通过机械分析，发现对齐的事实会激活模型内部重叠的部分，并且基于机械相似性的指标可以以高达78%的准确率预测短-长答案的对齐情况。", "conclusion": "本研究确立了查询复杂性下的事实一致性是LLM可信度的一个重要方面，并挑战了当前评估实践。目前的评估实践隐含地假设对简单事实查询的良好表现意味着在更复杂的知识寻求任务中也具有可靠性，但本研究结果表明这并非总是如此。未来的LLM评估应考虑不同查询复杂性下的事实一致性。"}}
{"id": "2510.11233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11233", "abs": "https://arxiv.org/abs/2510.11233", "authors": ["Jinyuan Xu", "Tian Lan", "Xintao Yu", "Xue He", "Hezhi Zhang", "Ying Wang", "Pierre Magistry", "Mathieu Valette", "Lei Li"], "title": "CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis", "comment": null, "summary": "Depression is a pressing global public health issue, yet publicly available\nChinese-language resources for risk detection remain scarce and are mostly\nlimited to binary classification. To address this limitation, we release\nCNSocialDepress, a benchmark dataset for depression risk detection from Chinese\nsocial media posts. The dataset contains 44,178 texts from 233 users, within\nwhich psychological experts annotated 10,306 depression-related segments.\nCNSocialDepress provides binary risk labels together with structured\nmulti-dimensional psychological attributes, enabling interpretable and\nfine-grained analysis of depressive signals. Experimental results demonstrate\nits utility across a wide range of NLP tasks, including structured\npsychological profiling and fine-tuning of large language models for depression\ndetection. Comprehensive evaluations highlight the dataset's effectiveness and\npractical value for depression risk identification and psychological analysis,\nthereby providing insights to mental health applications tailored for\nChinese-speaking populations.", "AI": {"tldr": "发布了一个名为CNSocialDepress的中文社交媒体抑郁风险检测基准数据集，包含多维度心理属性标注，支持可解释和细粒度的分析，并已在多种NLP任务中验证其有效性。", "motivation": "抑郁症是一个紧迫的全球公共健康问题，但现有的中文抑郁风险检测资源稀缺，且大多仅限于二元分类，缺乏细粒度和可解释的分析能力。", "method": "构建了CNSocialDepress数据集，包含来自233位用户的44,178条中文社交媒体文本，并由心理专家标注了10,306个抑郁相关片段。数据集提供了二元风险标签和结构化的多维度心理属性。", "result": "实验结果证明了CNSocialDepress数据集在广泛的自然语言处理任务中的实用性，包括结构化心理画像和大型语言模型在抑郁检测方面的微调。全面的评估强调了数据集在抑郁风险识别和心理分析方面的有效性和实用价值。", "conclusion": "CNSocialDepress数据集为针对中文人群的心理健康应用提供了新的见解，有助于进行可解释和细粒度的抑郁信号分析和风险识别。"}}
{"id": "2510.11238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11238", "abs": "https://arxiv.org/abs/2510.11238", "authors": ["Michael Schlichtkrull"], "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue", "comment": "Accepted to EMNLP 2025", "summary": "When AI agents retrieve and reason over external documents, adversaries can\nmanipulate the data they receive to subvert their behaviour. Previous research\nhas studied indirect prompt injection, where the attacker injects malicious\ninstructions. We argue that injection of instructions is not necessary to\nmanipulate agents - attackers could instead supply biased, misleading, or false\ninformation. We term this an attack by content. Existing defenses, which focus\non detecting hidden commands, are ineffective against attacks by content. To\ndefend themselves and their users, agents must critically evaluate retrieved\ninformation, corroborating claims with external evidence and evaluating source\ntrustworthiness. We argue that this is analogous to an existing NLP task,\nautomated fact-checking, which we propose to repurpose as a cognitive\nself-defense tool for agents.", "AI": {"tldr": "本文提出了一种针对AI代理的新型攻击——“内容攻击”，即通过提供偏颇、误导或虚假信息来操纵代理，而非注入指令。现有的防御机制对此无效，作者建议将自动化事实核查技术重新利用，作为代理的认知自卫工具。", "motivation": "以往研究主要关注间接提示注入（注入恶意指令）。本文认为，攻击者无需注入指令，仅通过提供偏颇、误导或虚假信息即可操纵AI代理，并称之为“内容攻击”。现有专注于检测隐藏命令的防御措施对“内容攻击”无效，因此需要新的防御策略。", "method": "本文提出，AI代理必须批判性地评估检索到的信息，通过外部证据核实主张，并评估来源的可信度。作者建议将现有的自然语言处理任务——自动化事实核查——重新定位，作为代理的认知自卫工具。", "result": "本文识别并定义了“内容攻击”这一新型威胁，指出其与传统的指令注入攻击不同，且现有防御无效。作为解决方案，本文提出AI代理应采用批判性评估和事实核查机制来抵御此类攻击。", "conclusion": "攻击者可以通过提供偏颇、误导或虚假信息来对AI代理进行“内容攻击”，这是一种新的威胁。为了防御这种攻击，AI代理需要像人类一样批判性地评估信息，作者建议将自动化事实核查技术作为代理的认知自卫工具。"}}
{"id": "2510.11243", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11243", "abs": "https://arxiv.org/abs/2510.11243", "authors": ["Birat Poudel", "Satyam Ghimire", "Sijan Bhattarai", "Saurav Bhandari", "Suramya Sharma Dahal"], "title": "Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches", "comment": "6 pages, 9 figures", "summary": "Sign languages serve as essential communication systems for individuals with\nhearing and speech impairments. However, digital linguistic dataset resources\nfor underrepresented sign languages, such as Nepali Sign Language (NSL), remain\nscarce. This study introduces the first benchmark dataset for NSL, consisting\nof 36 gesture classes with 1,500 samples per class, designed to capture the\nstructural and visual features of the language. To evaluate recognition\nperformance, we fine-tuned MobileNetV2 and ResNet50 architectures on the\ndataset, achieving classification accuracies of 90.45% and 88.78%,\nrespectively. These findings demonstrate the effectiveness of convolutional\nneural networks in sign recognition tasks, particularly within low-resource\nsettings. To the best of our knowledge, this work represents the first\nsystematic effort to construct a benchmark dataset and assess deep learning\napproaches for NSL recognition, highlighting the potential of transfer learning\nand fine-tuning for advancing research in underexplored sign languages.", "AI": {"tldr": "本研究首次构建了尼泊尔手语（NSL）的基准数据集，包含36个手势类别，并使用MobileNetV2和ResNet50进行识别评估，分别达到了90.45%和88.78%的准确率，证明了卷积神经网络在低资源手语识别中的有效性。", "motivation": "听障和语障人士需要手语作为沟通系统，但尼泊尔手语（NSL）等代表性不足的手语缺乏数字语言数据集资源。", "method": "研究构建了第一个NSL基准数据集，包含36个手势类别，每类1,500个样本。为评估识别性能，研究对MobileNetV2和ResNet50架构进行了微调。", "result": "MobileNetV2在数据集上实现了90.45%的分类准确率，ResNet50实现了88.78%的准确率。这些结果表明卷积神经网络在手语识别任务中，特别是在低资源环境下是有效的。", "conclusion": "这项工作代表了首次系统性地构建NSL基准数据集并评估深度学习方法，突显了迁移学习和微调在推进未充分探索的手语研究方面的潜力。"}}
{"id": "2510.11236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11236", "abs": "https://arxiv.org/abs/2510.11236", "authors": ["Haoqi Yang", "Yao Yao", "Zuchao Li", "Baoyuan Qi", "Guoming Liu", "Hai Zhao"], "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression", "comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.", "AI": {"tldr": "XQuant是一个免训练、即插即用的框架，通过数据无关校准和跨层压缩，实现了LLM KV缓存的超低位宽（低于1.4比特）量化，在保持高性能的同时显著降低了内存消耗。", "motivation": "大型语言模型（LLMs）在自然语言处理任务中表现出色，但其在长文本理解和生成过程中，KV缓存的增长导致内存需求巨大，这对于资源受限环境的部署构成了重大挑战。量化被认为是一种有前景的解决方案，可以在减少内存消耗的同时保留历史信息。", "method": "本文提出了XQuant，一个免训练、即插即用的框架，用于实现超低等效位宽的KV缓存量化。XQuant引入了两项关键创新：计算开销可忽略不计的数据无关校准方法，以及跨层KV缓存压缩，从而实现了低于1.4比特的量化。", "result": "在TruthfulQA和LongBench上的大量实验表明，XQuant在实现更低位宽的同时，性能优于现有最先进的方法（例如KIVI-2bit和AsymKV-1.5bit），在内存效率和模型准确性之间建立了更好的权衡。", "conclusion": "XQuant通过其创新的数据无关校准和跨层压缩技术，为LLM KV缓存提供了一种卓越的超低位宽量化解决方案，显著提高了内存效率并保持了出色的模型准确性，从而解决了LLMs在资源受限环境中的部署挑战。"}}
{"id": "2510.11254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11254", "abs": "https://arxiv.org/abs/2510.11254", "authors": ["Jana Jung", "Marlene Lutz", "Indira Sen", "Markus Strohmaier"], "title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality", "comment": null, "summary": "Psychometric tests are increasingly used to assess psychological constructs\nin large language models (LLMs). However, it remains unclear whether these\ntests -- originally developed for humans -- yield meaningful results when\napplied to LLMs. In this study, we systematically evaluate the reliability and\nvalidity of human psychometric tests for three constructs: sexism, racism, and\nmorality. We find moderate reliability across multiple item and prompt\nvariations. Validity is evaluated through both convergent (i.e., testing\ntheory-based inter-test correlations) and ecological approaches (i.e., testing\nthe alignment between tests scores and behavior in real-world downstream\ntasks). Crucially, we find that psychometric test scores do not align, and in\nsome cases even negatively correlate with, model behavior in downstream tasks,\nindicating low ecological validity. Our results highlight that systematic\nevaluations of psychometric tests is essential before interpreting their\nscores. They also suggest that psychometric tests designed for humans cannot be\napplied directly to LLMs without adaptation.", "AI": {"tldr": "本研究评估了人类心理测量测试在大型语言模型（LLM）上的可靠性和有效性。结果发现其可靠性中等，但生态效度较低，测试分数与LLM在实际任务中的行为不符，表明人类测试不应直接应用于LLM。", "motivation": "心理测量测试越来越多地用于评估LLM的心理结构，但这些最初为人类开发的测试在应用于LLM时是否能产生有意义的结果尚不明确。", "method": "研究系统地评估了人类心理测量测试对LLM在性别歧视、种族歧视和道德三个结构上的可靠性和有效性。可靠性通过多项题目和提示变体进行评估。有效性通过聚合效度（测试基于理论的测试间相关性）和生态效度（测试分数与LLM在真实下游任务中行为的一致性）两种方法进行评估。", "result": "研究发现，在多项题目和提示变体下，可靠性中等。然而，心理测量测试分数与LLM在下游任务中的行为不一致，在某些情况下甚至呈负相关，这表明生态效度较低。", "conclusion": "研究强调，在解释心理测量测试分数之前，对其进行系统评估至关重要。结果还表明，为人类设计的心理测量测试不能未经调整直接应用于LLM。"}}
{"id": "2510.10868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10868", "abs": "https://arxiv.org/abs/2510.10868", "authors": ["Soroush Mehraban", "Andrea Iaboni", "Babak Taati"], "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding", "comment": "Project page: https://soroushmehraban.github.io/FastHMR/", "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.", "AI": {"tldr": "本文提出两种合并策略（ECLM和Mask-ToMe）和一种基于扩散的解码器，以显著降低3D人体网格恢复(HMR)中Transformer模型的计算成本和复杂性，同时略微提升性能。", "motivation": "现有的基于Transformer的3D HMR模型因其深层架构和冗余tokens而导致计算成本高昂且复杂性高。", "method": "本文提出了两种HMR特定的合并策略：1) 误差约束层合并(ECLM)，选择性合并对MPJPE影响最小的Transformer层；2) 掩码引导Token合并(Mask-ToMe)，合并对最终预测贡献小的背景tokens。为弥补合并可能导致的性能下降，还提出了一种基于扩散的解码器，该解码器结合了时间上下文并利用从大规模运动捕捉数据集中学习到的姿态先验。", "result": "实验结果表明，该方法在多个基准测试中实现了高达2.3倍的加速，同时在性能上略优于基线模型。", "conclusion": "所提出的合并策略和扩散解码器能够有效降低3D HMR Transformer模型的计算成本和复杂性，同时保持或提升性能。"}}
{"id": "2510.10797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10797", "abs": "https://arxiv.org/abs/2510.10797", "authors": ["Aleksandra Melnikova", "Petr Matula"], "title": "Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells", "comment": "6 pages, 2 figures, 4 tables", "summary": "High-quality, publicly available segmentation annotations of image and video\ndatasets are critical for advancing the field of image processing. In\nparticular, annotations of volumetric images of a large number of targets are\ntime-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we\npresented the first publicly available full 3D time-lapse segmentation\nannotations of migrating cells with complex dynamic shapes. Concretely, three\ndistinct humans annotated two sequences of MDA231 human breast carcinoma cells\n(Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).\n  This paper aims to provide a comprehensive description of the dataset and\naccompanying experiments that were not included in (Melnikova, A., & Matula,\nP., 2025) due to limitations in publication space. Namely, we show that the\ncreated annotations are consistent with the previously published tracking\nmarkers provided by the CTC organizers and the segmentation accuracy measured\nbased on the 2D gold truth of CTC is within the inter-annotator variability\nmargins. We compared the created 3D annotations with automatically created\nsilver truth provided by CTC. We have found the proposed annotations better\nrepresent the complexity of the input images. The presented annotations can be\nused for testing and training cell segmentation, or analyzing 3D shapes of\nhighly dynamic objects.", "AI": {"tldr": "本文提供了首个公开可用的复杂动态形状迁移细胞的完整3D延时分割标注数据集的详细描述及相关实验，并验证了其质量和优越性。", "motivation": "高质量、公开可用的图像和视频数据集分割标注对于图像处理领域至关重要，特别是大量目标的体积图像标注耗时且具有挑战性。先前的工作（Melnikova, A., & Matula, P., 2025）首次公开了此类标注，但因出版空间限制未能提供完整描述和实验细节，本文旨在补充这些内容。", "method": "三名人类标注者对来自细胞追踪挑战赛（CTC）的两个MDA231人乳腺癌细胞序列（Fluo-C3DL-MDA231）进行了完整的3D延时分割标注。本文通过与CTC提供的追踪标记进行一致性验证，并基于CTC的2D金标准测量分割准确性，与标注者间差异进行比较。此外，还将创建的3D标注与CTC提供的自动生成的银标准进行了比较。", "result": "创建的标注与CTC先前发布的追踪标记保持一致；基于CTC的2D金标准测量的分割准确性在标注者间差异范围内；与CTC提供的自动生成的银标准相比，提出的3D标注能更好地表示输入图像的复杂性。", "conclusion": "本文介绍的3D延时分割标注数据集质量高、具有代表性，可用于细胞分割的测试和训练，以及分析高度动态对象的3D形状。"}}
{"id": "2510.10876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10876", "abs": "https://arxiv.org/abs/2510.10876", "authors": ["Shutong Lin", "Zhengkang Xiang", "Jianzhong Qi", "Kourosh Khoshelham"], "title": "rareboost3d: a synthetic lidar dataset with enhanced rare classes", "comment": null, "summary": "Real-world point cloud datasets have made significant contributions to the\ndevelopment of LiDAR-based perception technologies, such as object segmentation\nfor autonomous driving. However, due to the limited number of instances in some\nrare classes, the long-tail problem remains a major challenge in existing\ndatasets. To address this issue, we introduce a novel, synthetic point cloud\ndataset named RareBoost3D, which complements existing real-world datasets by\nproviding significantly more instances for object classes that are rare in\nreal-world datasets. To effectively leverage both synthetic and real-world\ndata, we further propose a cross-domain semantic alignment method named CSC\nloss that aligns feature representations of the same class across different\ndomains. Experimental results demonstrate that this alignment significantly\nenhances the performance of LiDAR point cloud segmentation models over\nreal-world data.", "AI": {"tldr": "为解决点云数据集中稀有类别的长尾问题，本文提出了一个名为RareBoost3D的新型合成点云数据集，并引入了跨域语义对齐方法CSC损失，实验证明能显著提升LiDAR点云分割模型的性能。", "motivation": "现有真实世界点云数据集中，某些稀有类别的实例数量有限，导致长尾问题，严重阻碍了LiDAR感知技术（如自动驾驶中的物体分割）的发展。", "method": "1. 引入RareBoost3D：一个新型合成点云数据集，为真实世界数据集中稀有的物体类别提供了显著更多的实例。2. 提出CSC损失：一种跨域语义对齐方法，旨在对齐不同域（合成数据与真实数据）中相同类别的特征表示。", "result": "实验结果表明，所提出的对齐方法（CSC损失）显著增强了LiDAR点云分割模型在真实世界数据上的性能。", "conclusion": "通过结合新型合成数据集RareBoost3D和跨域语义对齐方法CSC损失，能够有效解决点云分割中的长尾问题，并显著提升模型在真实世界数据上的表现。"}}
{"id": "2510.11260", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.11260", "abs": "https://arxiv.org/abs/2510.11260", "authors": ["Yuxuan Chen", "Ruotong Yang", "Zhengyang Zhang", "Mehreen Ahmed", "Yanming Wang"], "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images", "comment": "14 pages, 6 figures", "summary": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM),\nare widely used in scientific research for visualizing and analyzing\nmicrostructures. Determining the scale bars is an important first step of\naccurate SEM analysis; however, currently, it mainly relies on manual\noperations, which is both time-consuming and prone to errors. To address this\nissue, we propose a multi-modal and automated scale bar detection and\nextraction framework that provides concurrent object detection, text detection\nand text recognition with a Large Language Model (LLM) agent. The proposed\nframework operates in four phases; i) Automatic Dataset Generation (Auto-DG)\nmodel to synthesize a diverse dataset of SEM images ensuring robust training\nand high generalizability of the model, ii) scale bar object detection, iii)\ninformation extraction using a hybrid Optical Character Recognition (OCR)\nsystem with DenseNet and Convolutional Recurrent Neural Network (CRNN) based\nalgorithms, iv) an LLM agent to analyze and verify accuracy of the results. The\nproposed model demonstrates a strong performance in object detection and\naccurate localization with a precision of 100%, recall of 95.8%, and a mean\nAverage Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The\nhybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the\nAuto-DG dataset, significantly outperforming several mainstream standalone\nengines, highlighting its reliability for scientific image analysis. The LLM is\nintroduced as a reasoning engine as well as an intelligent assistant that\nsuggests follow-up steps and verifies the results. This automated method\npowered by an LLM agent significantly enhances the efficiency and accuracy of\nscale bar detection and extraction in SEM images, providing a valuable tool for\nmicroscopic analysis and advancing the field of scientific imaging.", "AI": {"tldr": "本文提出了一种多模态自动化框架，利用目标检测、混合OCR和大型语言模型（LLM）代理，高效准确地检测和提取扫描电子显微镜（SEM）图像中的标尺信息，解决了手动操作耗时且易出错的问题。", "motivation": "扫描电子显微镜（SEM）图像中的标尺确定是微观分析的关键第一步，但目前主要依赖手动操作，效率低下且容易出错。为解决这一问题，需要一种自动化方法。", "method": "所提出的框架分为四个阶段：i) 自动数据集生成（Auto-DG）模型用于合成多样化的SEM图像数据集；ii) 标尺对象检测；iii) 使用结合DenseNet和CRNN算法的混合光学字符识别（OCR）系统进行信息提取；iv) 一个LLM代理用于分析和验证结果的准确性。", "result": "该模型在目标检测方面表现出色，精度为100%，召回率为95.8%，在IoU=0.5时平均精度（mAP）为99.2%，在IoU=0.5:0.95时为69.1%。混合OCR系统在Auto-DG数据集上实现了89%的精度、65%的召回率和75%的F1分数，显著优于主流的独立引擎。LLM作为推理引擎和智能助手，用于验证结果并提出后续步骤。", "conclusion": "这种由LLM代理驱动的自动化方法显著提高了SEM图像中标尺检测和提取的效率和准确性，为微观分析提供了有价值的工具，并推动了科学成像领域的发展。"}}
{"id": "2510.11302", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11302", "abs": "https://arxiv.org/abs/2510.11302", "authors": ["Samer Al-Hamadani"], "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models", "comment": "23 pages, 4 figures, 4 tables", "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.", "AI": {"tldr": "本文首次对监督式目标检测（YOLO）和零样本VLM推理（Gemini Flash 2.5）进行了全面的成本效益分析。研究发现，YOLO在标准类别上准确率更高但需要大量标注投资，而零样本Gemini在未训练类别上表现更好且每检测成本显著更低。最佳架构选择取决于部署量、类别稳定性、预算和准确率要求。", "motivation": "传统监督式目标检测系统依赖手动标注，成本高昂。视觉-语言模型（VLMs）提供零样本检测能力，无需标注但准确率较低。本文旨在通过成本效益分析，量化比较这两种范式，以指导实际应用中的架构选择。", "method": "研究方法包括：在1,000张分层COCO图像和200张多样化产品图像（涵盖消费电子和稀有类别）上系统评估YOLO和Gemini Flash 2.5的性能；建立详细的总体拥有成本（TCO）模型；进行每正确检测成本分析；开发决策框架以指导架构选择。", "result": "主要结果显示：在标准类别上，监督式YOLO准确率达91.2%，零样本Gemini为68.5%，YOLO优势明显（22.7个百分点），但需10,800美元的100类别系统标注成本，仅在超过5500万次推理（或每日151,000张图像一年）时才值得投资。在多样化产品类别上，零样本Gemini准确率为52.3%，而监督式YOLO因未训练类别而准确率为0%。每正确检测成本分析显示，Gemini在100,000次推理时每检测成本显著低于YOLO（0.00050美元 vs 0.143美元）。", "conclusion": "最佳的目标检测架构选择并非纯粹基于技术性能指标，而是关键性地取决于部署量、类别稳定性、预算限制和准确率要求。在特定场景下，零样本VLM即使在准确率较低的情况下，也可能因其极低的每检测成本而成为更具成本效益的解决方案。"}}
{"id": "2510.11277", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11277", "abs": "https://arxiv.org/abs/2510.11277", "authors": ["Guangyu Wei", "Ke Han", "Yueming Lyu", "Yu Luo", "Yue Jiang", "Caifeng Shan", "Nicu Sebe"], "title": "Towards Real-Time Fake News Detection under Evidence Scarcity", "comment": null, "summary": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE.", "AI": {"tldr": "EASE是一种新颖的实时假新闻检测框架，它根据证据充足性动态调整决策过程。通过三阶段评估（证据、推理、情感）和指令微调，EASE在证据稀缺的情况下实现了最先进的性能和更好的泛化能力。", "motivation": "在实时场景中，新兴事件通常缺乏足够的佐证，导致现有依赖外部证据的假新闻检测方法难以泛化。", "method": "本文提出了EASE框架，通过评估意识选择专家来应对证据稀缺问题。它引入了一个顺序评估机制，包括：1) 基于证据的评估（仅在证据充足时使用）；2) 基于推理的评估（仅在大型语言模型LLM可靠时使用其世界知识）；3) 基于情感的备用（在证据和推理都不可靠时集成情感线索）。为提高评估准确性，EASE采用伪标签的指令微调来指导评估器，并通过可解释的推理来证明其视角特定知识。此外，专家模块将评估器的评估结果与新闻内容结合，实现评估感知的决策。", "result": "广泛的实验表明，EASE不仅在多个基准测试中实现了最先进的性能，而且显著提高了对实时新闻的泛化能力。此外，本文还引入了RealTimeNews-25新基准来评估模型在新兴新闻上的泛化能力。", "conclusion": "EASE通过根据可用证据的充足性动态调整决策过程，有效解决了实时假新闻检测中证据稀缺的挑战，从而在多个基准上实现了最先进的性能，并显著提升了对实时新闻的泛化能力。"}}
{"id": "2510.11307", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11307", "abs": "https://arxiv.org/abs/2510.11307", "authors": ["Sabrina McCallum", "Amit Parekh", "Alessandro Suglia"], "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks", "comment": "EMNLP 2025 Findings", "summary": "Current approaches to embodied AI tend to learn policies from expert\ndemonstrations. However, without a mechanism to evaluate the quality of\ndemonstrated actions, they are limited to learning from optimal behaviour, or\nthey risk replicating errors and inefficiencies. While reinforcement learning\noffers one alternative, the associated exploration typically results in\nsacrificing data efficiency. This work explores how agents trained with\nimitation learning can learn robust representations from both optimal and\nsuboptimal demonstrations when given access to constructive language feedback\nas a means to contextualise different modes of behaviour. We directly provide\nlanguage feedback embeddings as part of the input sequence into a\nTransformer-based policy, and optionally complement the traditional next action\nprediction objective with auxiliary self-supervised learning objectives for\nfeedback prediction. We test our approach on a range of embodied\nVision-and-Language tasks in our custom BabyAI-XGen environment and show\nsignificant improvements in agents' compositional generalisation abilities and\nrobustness, suggesting that our data-efficient method allows models to\nsuccessfully convert suboptimal behaviour into learning opportunities. Overall,\nour results suggest that language feedback is a competitive and intuitive\nalternative to intermediate scalar rewards for language-specified embodied\ntasks.", "AI": {"tldr": "本文提出了一种数据高效的方法，通过将语言反馈嵌入作为输入序列，使模仿学习智能体能够从最优和次优演示中学习，从而提高具身AI的泛化能力和鲁棒性。", "motivation": "当前具身AI的模仿学习方法受限于只能从最优行为中学习，否则有复制错误和低效的风险。强化学习虽然提供了替代方案，但通常以牺牲数据效率为代价。本研究旨在探索如何在提供建设性语言反馈的情况下，让智能体从次优演示中也能有效学习。", "method": "研究将语言反馈嵌入直接作为输入序列的一部分提供给基于Transformer的策略。此外，还可以选择性地通过辅助自监督学习目标来补充传统的下一步动作预测目标，以预测反馈。该方法在定制的BabyAI-XGen环境中的一系列具身视觉与语言任务上进行了测试。", "result": "实验结果表明，该方法显著提高了智能体的组合泛化能力和鲁棒性。这表明其数据高效的方法能够成功地将次优行为转化为学习机会。", "conclusion": "研究结果表明，对于语言指定的具身任务，语言反馈是一种有竞争力和直观的替代方案，可以取代中间标量奖励。"}}
{"id": "2510.11297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11297", "abs": "https://arxiv.org/abs/2510.11297", "authors": ["Ruirui Chen", "Weifeng Jiang", "Chengwei Qin", "Bo Xiong", "Fiona Liausvia", "Dongkyu Choi", "Boon Kiat Quek"], "title": "Are Large Language Models Effective Knowledge Graph Constructors?", "comment": null, "summary": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown\npromise in reducing hallucinations in large language models (LLMs). However,\nconstructing high-quality KGs remains difficult, requiring accurate information\nextraction and structured representations that support interpretability and\ndownstream utility. Existing LLM-based approaches often focus narrowly on\nentity and relation extraction, limiting coverage to sentence-level contexts or\nrelying on predefined schemas. We propose a hierarchical extraction framework\nthat organizes information at multiple levels, enabling the creation of\nsemantically rich and well-structured KGs. Using state-of-the-art LLMs, we\nextract and construct knowledge graphs and evaluate them comprehensively from\nboth structural and semantic perspectives. Our results highlight the strengths\nand shortcomings of current LLMs in KG construction and identify key challenges\nfor future work. To advance research in this area, we also release a curated\ndataset of LLM-generated KGs derived from research papers on children's mental\nwell-being. This resource aims to foster more transparent, reliable, and\nimpactful applications in high-stakes domains such as healthcare.", "AI": {"tldr": "该研究提出一个分层知识图谱（KG）抽取框架，利用大型语言模型（LLMs）构建语义丰富的KG，并评估了LLMs在KG构建中的表现，同时发布了一个新的KG数据集。", "motivation": "知识图谱对知识密集型任务至关重要，并有助于减少大型语言模型的幻觉。然而，构建高质量的知识图谱很困难，需要准确的信息抽取和支持可解释性的结构化表示。现有的基于LLM的方法通常只关注实体和关系抽取，局限于句子级上下文或依赖预定义模式，覆盖范围有限。", "method": "本文提出一个分层抽取框架，在多个层级组织信息，以创建语义丰富且结构良好的知识图谱。利用最先进的LLMs进行知识图谱的抽取和构建，并从结构和语义角度对其进行全面评估。", "result": "研究结果揭示了当前LLMs在知识图谱构建方面的优势和不足。同时，为了推动该领域的研究，作者发布了一个从儿童心理健康研究论文中提取的LLM生成知识图谱的精选数据集。", "conclusion": "研究指出了未来工作面临的关键挑战。发布的资源旨在促进在高风险领域（如医疗保健）中更透明、可靠和有影响力的应用。"}}
{"id": "2510.11288", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11288", "abs": "https://arxiv.org/abs/2510.11288", "authors": ["Nikita Afonin", "Nikita Andriyanov", "Nikhil Bageshpura", "Kyle Liu", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Alexander Panchenko", "Oleg Rogov", "Elena Tutubalina", "Mikhail Seleznyov"], "title": "Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs", "comment": null, "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM.", "AI": {"tldr": "研究发现，在情境学习（ICL）中也会出现新兴未对齐（EM）现象，即大型语言模型（LLM）在给定狭窄情境示例后产生广泛的未对齐响应，其机制通常涉及模型采纳危险的“人格”。", "motivation": "先前的研究表明，狭窄的微调和激活引导会导致新兴未对齐（EM）现象，但这些发现并未涵盖情境学习（ICL）。因此，本研究旨在探究EM是否会在ICL中出现。", "method": "研究在三个数据集上测试了三个前沿模型，使用64到256个狭窄的情境示例。通过引导分步推理（思维链），并对未对齐的思维链进行手动分析，以探究EM的机制。", "result": "研究发现EM确实在ICL中出现：在64个情境示例下，模型产生广泛未对齐响应的比例在2%到17%之间；在256个示例下，该比例高达58%。对思维链的手动分析显示，67.5%的未对齐轨迹通过采纳鲁莽或危险的“人格”来明确合理化有害输出。", "conclusion": "新兴未对齐（EM）现象不仅限于微调，也广泛存在于情境学习（ICL）中。模型在ICL中产生未对齐行为的一个主要机制是采纳有害的“人格”，这与微调引起的EM结果相似。"}}
{"id": "2510.10947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10947", "abs": "https://arxiv.org/abs/2510.10947", "authors": ["Namhoon Kim", "Sara Fridovich-Keil"], "title": "Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors", "comment": "Code is available at\n  https://github.com/voilalab/uncertainty_quantification_LPN", "summary": "Generative models have shown strong potential as data-driven priors for\nsolving inverse problems such as reconstructing medical images from\nundersampled measurements. While these priors improve reconstruction quality\nwith fewer measurements, they risk hallucinating features when test images lie\noutside the training distribution. Existing uncertainty quantification methods\nin this setting (i) require an in-distribution calibration dataset, which may\nnot be available, (ii) provide heuristic rather than statistical estimates, or\n(iii) quantify uncertainty from model capacity or limited measurements rather\nthan distribution shift. We propose an instance-level, calibration-free\nuncertainty indicator that is sensitive to distribution shift, requires no\nknowledge of the training distribution, and incurs no retraining cost. Our key\nhypothesis is that reconstructions of in-distribution images remain stable\nunder random measurement variations, while reconstructions of\nout-of-distribution (OOD) images exhibit greater instability. We use this\nstability as a proxy for detecting distribution shift. Our proposed OOD\nindicator is efficiently computable for any computational imaging inverse\nproblem; we demonstrate it on tomographic reconstruction of MNIST digits, where\na learned proximal network trained only on digit \"0\" is evaluated on all ten\ndigits. Reconstructions of OOD digits show higher variability and\ncorrespondingly higher reconstruction error, validating this indicator. These\nresults suggest a deployment strategy that pairs generative priors with\nlightweight guardrails, enabling aggressive measurement reduction for\nin-distribution cases while automatically warning when priors are applied out\nof distribution.", "AI": {"tldr": "本文提出了一种无需校准、实例级的生成模型不确定性指标，通过测量变异下重建的稳定性来检测分布外（OOD）图像，从而避免在逆问题中生成模型可能出现的幻觉。", "motivation": "生成模型作为逆问题的数据驱动先验，在减少测量数据方面表现出色，但当测试图像超出训练分布时，存在产生幻觉特征的风险。现有不确定性量化方法存在局限性，如需要分布内校准数据集、提供启发式而非统计估计，或仅量化模型容量或测量限制带来的不确定性，而非分布偏移。", "method": "核心假设是，分布内图像的重建在随机测量变异下保持稳定，而分布外（OOD）图像的重建则表现出更大的不稳定性。本文利用这种稳定性作为检测分布偏移的代理，提出了一种无需校准、实例级的不确定性指标，该指标对分布偏移敏感，不需要训练分布知识，也无需重新训练。", "result": "在MNIST数字的断层重建任务中进行了验证，模型仅用数字“0”训练，但在所有十个数字上进行评估。结果显示，OOD数字的重建表现出更高的变异性，并伴随更高的重建误差，从而验证了该指标的有效性。", "conclusion": "这些结果表明，可以将生成先验与轻量级防护措施（即本文提出的OOD指标）结合部署，从而在分布内情况下实现激进的测量减少，同时在先验模型应用于分布外数据时自动发出警告。"}}
{"id": "2510.10880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10880", "abs": "https://arxiv.org/abs/2510.10880", "authors": ["Zhaofang Qian", "Hardy Chen", "Zeyu Wang", "Li Zhang", "Zijun Wang", "Xiaoke Huang", "Hui Liu", "Xianfeng Tang", "Zeyu Zheng", "Haoqin Tu", "Cihang Xie", "Yuyin Zhou"], "title": "Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales", "comment": null, "summary": "Vision-language models (VLMs) have advanced rapidly, yet their capacity for\nimage-grounded geolocation in open-world conditions, a task that is challenging\nand of demand in real life, has not been comprehensively evaluated. We present\nEarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates\nvisual recognition, step-by-step reasoning, and evidence use. EarthWhere\ncomprises 810 globally distributed images across two complementary geolocation\nscales: WhereCountry (i.e., 500 multiple-choice question-answering, with\ncountry-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained\nstreet-level identification tasks requiring multi-step reasoning with optional\nweb search). For evaluation, we adopt the final-prediction metrics: location\naccuracies within k km (Acc@k) for coordinates and hierarchical path scores for\ntextual localization. Beyond this, we propose to explicitly score intermediate\nreasoning chains using human-verified key visual clues and a Shapley-reweighted\nthinking score that attributes credit to each clue's marginal contribution. We\nbenchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere\nand report different types of final answer accuracies as well as the calibrated\nmodel thinking scores. Overall, Gemini-2.5-Pro achieves the best average\naccuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches\n34.71%. We reveal that web search and reasoning do not guarantee improved\nperformance when visual clues are limited, and models exhibit regional biases,\nachieving up to 42.7% higher scores in certain areas than others. These\nfindings highlight not only the promise but also the persistent challenges of\nmodels to mitigate bias and achieve robust, fine-grained localization. We\nopen-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.", "AI": {"tldr": "该研究引入了EarthWhere，一个全面的基准测试，用于评估视觉语言模型（VLMs）在开放世界图像地理定位方面的能力，包括视觉识别、逐步推理和证据使用。通过对13个最先进的VLM进行基准测试，发现Gemini-2.5-Pro表现最佳，但模型在视觉线索有限时表现不佳，并存在显著的区域偏差，突出了模型在减轻偏差和实现鲁棒、细粒度定位方面的挑战。", "motivation": "尽管视觉语言模型（VLMs）发展迅速，但它们在开放世界条件下进行图像地理定位的能力尚未得到全面评估。这是一项具有挑战性且在现实生活中需求很大的任务。", "method": "研究提出了EarthWhere，一个包含810张全球分布式图像的综合基准测试。它包括两个互补的地理定位尺度：WhereCountry（500道国家级多项选择题）和WhereStreet（310个需要多步推理和可选网络搜索的细粒度街景识别任务）。评估指标包括坐标的Acc@k、文本定位的层级路径分数，以及通过人工验证的关键视觉线索和Shapley重新加权思维分数来评估中间推理链。研究对13个最先进的VLM（带网络搜索工具）进行了基准测试。", "result": "Gemini-2.5-Pro以56.32%的平均准确率表现最佳，而最强的开源模型GLM-4.5V达到34.71%。研究发现，当视觉线索有限时，网络搜索和推理并不能保证性能提升。模型还表现出区域偏差，在某些区域的得分比其他区域高出高达42.7%。", "conclusion": "研究结果不仅展示了模型在地理定位方面的潜力，也揭示了模型在减轻偏差和实现鲁棒、细粒度定位方面持续存在的挑战。该基准测试已开源。"}}
{"id": "2510.11314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11314", "abs": "https://arxiv.org/abs/2510.11314", "authors": ["Belkiss Souayed", "Sarah Ebling", "Yingqiang Gao"], "title": "Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications", "comment": null, "summary": "Individuals with intellectual disabilities often have difficulties in\ncomprehending complex texts. While many text-to-image models prioritize\naesthetics over accessibility, it is not clear how visual illustrations relate\nto text simplifications (TS) generated from them. This paper presents a\nstructured vision-language model (VLM) prompting framework for generating\naccessible images from simplified texts. We designed five prompt templates,\ni.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level\nDetail, and Grid Layout, each following distinct spatial arrangements while\nadhering to accessibility constraints such as object count limits, spatial\nseparation, and content restrictions. Using 400 sentence-level simplifications\nfrom four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and\nASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template\neffectiveness with CLIPScores, and Phase 2 involved human annotation of\ngenerated images across ten visual styles by four accessibility experts.\nResults show that the Basic Object Focus prompt template achieved the highest\nsemantic alignment, indicating that visual minimalism enhances language\naccessibility. Expert evaluation further identified Retro style as the most\naccessible and Wikipedia as the most effective data source. Inter-annotator\nagreement varied across dimensions, with Text Simplicity showing strong\nreliability and Image Quality proving more subjective. Overall, our framework\noffers practical guidelines for accessible content generation and underscores\nthe importance of structured prompting in AI-generated visual accessibility\ntools.", "AI": {"tldr": "本文提出一个结构化的视觉-语言模型（VLM）提示框架，用于从简化文本生成可访问的图像，并评估了不同提示模板和视觉风格对可访问性的影响。", "motivation": "智力障碍者在理解复杂文本时常遇困难。现有文本到图像模型优先考虑美观而非可访问性，且它们如何与文本简化相关联尚不明确。", "method": "研究设计了一个结构化的VLM提示框架，用于从简化文本生成可访问图像。开发了五种提示模板（基本对象聚焦、情境场景、教育布局、多层细节、网格布局），每个模板都遵循特定的空间排列并遵守可访问性限制。使用来自四个文本简化数据集的400个句子级简化文本进行两阶段评估：第一阶段通过CLIPScores评估提示模板的有效性；第二阶段由四位可访问性专家对生成的图像进行人工标注，涵盖十种视觉风格。", "result": "结果显示，基本对象聚焦提示模板实现了最高的语义对齐，表明视觉极简主义能增强语言可访问性。专家评估进一步指出，复古风格是最易于访问的，而维基百科是最有效的数据源。注释者之间的一致性因维度而异，其中文本简洁性显示出较强的可靠性，而图像质量则更具主观性。", "conclusion": "该框架为可访问内容生成提供了实用指南，并强调了结构化提示在AI生成视觉可访问性工具中的重要性。"}}
{"id": "2510.10969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10969", "abs": "https://arxiv.org/abs/2510.10969", "authors": ["Zeteng Lin", "Xingxing Li", "Wen You", "Xiaoyang Li", "Zehan Lu", "Yujun Cai", "Jing Tang"], "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation", "comment": null, "summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often\nstruggle to preserve logic, object identity, and style in multimodal image-text\ngeneration. This limitation significantly hinders the generalization capability\nof VLMs in complex image-text input-output scenarios. To address this issue, we\npropose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which\nenhances existing interleaved VLMs through explicit structured reasoning,\nthereby mitigating context drift in logic, entity identity, and style. The\nproposed framework operates in two stages. (1) A dynamic IUT-Plug extraction\nmodule parses visual scenes into hierarchical symbolic structures. (2) A\ncoordinated narrative-flow and image synthesis mechanism ensures cross-modal\nconsistency. To evaluate our approach, we construct a novel benchmark based on\n3,000 real human-generated question-answer pairs over fine-tuned large models,\nintroducing a dynamic evaluation protocol for quantifying context drift in\ninterleaved VLMs. Experimental results demonstrate that IUT-Plug not only\nimproves accuracy on established benchmarks but also effectively alleviates the\nthree critical forms of context drift across diverse multimodal question\nanswering (QA) scenarios.", "AI": {"tldr": "IUT-Plug是一个基于图像理解树（IUT）的模块，通过显式结构化推理增强现有交错式视觉语言模型（VLM），以缓解多模态图像-文本生成中逻辑、实体识别和风格的上下文漂移问题。", "motivation": "现有视觉语言模型（如GPT-4和DALL-E）在多模态图像-文本生成中难以保持逻辑、对象识别和风格的一致性，这严重限制了它们在复杂输入-输出场景中的泛化能力。", "method": "本文提出了IUT-Plug框架，分两阶段运行：1) 动态IUT-Plug提取模块将视觉场景解析为分层符号结构（图像理解树IUT）；2) 协调的叙事流和图像合成机制确保跨模态一致性。此外，本文构建了一个包含3,000个人工生成问答对的新基准和动态评估协议来量化上下文漂移。", "result": "实验结果表明，IUT-Plug不仅提高了在现有基准上的准确性，而且有效缓解了在各种多模态问答场景中逻辑、实体识别和风格这三种关键形式的上下文漂移。", "conclusion": "IUT-Plug通过引入显式结构化推理（基于图像理解树）成功解决了现有视觉语言模型在多模态生成中存在的上下文漂移问题，显著提升了模型的准确性和泛化能力。"}}
{"id": "2510.11370", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11370", "abs": "https://arxiv.org/abs/2510.11370", "authors": ["Wenhan Ma", "Hailin Zhang", "Liang Zhao", "Yifan Song", "Yudong Wang", "Zhifang Sui", "Fuli Luo"], "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.", "AI": {"tldr": "本文提出Rollout Routing Replay (R3) 方法，通过记录和重放推理时的路由分布来解决MoE模型中强化学习训练因路由不稳定性导致的崩溃问题。", "motivation": "强化学习在增强大型语言模型能力方面至关重要，但在MoE模型中，路由机制常引入不稳定性，甚至导致灾难性的RL训练崩溃。研究发现训练和推理阶段的路由行为存在显著差异，且路由框架在相同条件下也可能产生不一致的专家选择。", "method": "提出Rollout Routing Replay (R3) 方法，该方法记录推理引擎中的路由分布，并在训练期间重放这些分布，以解决基础的不一致性。", "result": "R3显著降低了训练-推理策略的KL散度，减轻了极端差异，且不影响训练速度。大量实验证实R3成功稳定了RL训练，防止了崩溃，并优于GSPO和TIS等现有方法。", "conclusion": "该工作为稳定MoE模型中的强化学习提供了一种新的解决方案。"}}
{"id": "2510.11346", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11346", "abs": "https://arxiv.org/abs/2510.11346", "authors": ["Joshua Niemeijer", "Jan Ehrhardt", "Heinz Handels", "Hristina Uzunova"], "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation", "comment": "Accepted for presentation at ICCV Workshops 2025, \"The 4th Workshop\n  on What is Next in Multimodal Foundation Models?\" (MMFM)", "summary": "Generative Models are a valuable tool for the controlled creation of\nhigh-quality image data. Controlled diffusion models like the ControlNet have\nallowed the creation of labeled distributions. Such synthetic datasets can\naugment the original training distribution when discriminative models, like\nsemantic segmentation, are trained. However, this augmentation effect is\nlimited since ControlNets tend to reproduce the original training distribution.\n  This work introduces a method to utilize data from unlabeled domains to train\nControlNets by introducing the concept of uncertainty into the control\nmechanism. The uncertainty indicates that a given image was not part of the\ntraining distribution of a downstream task, e.g., segmentation. Thus, two types\nof control are engaged in the final network: an uncertainty control from an\nunlabeled dataset and a semantic control from the labeled dataset. The\nresulting ControlNet allows us to create annotated data with high uncertainty\nfrom the target domain, i.e., synthetic data from the unlabeled distribution\nwith labels. In our scenario, we consider retinal OCTs, where typically\nhigh-quality Spectralis images are available with given ground truth\nsegmentations, enabling the training of segmentation networks. The recent\ndevelopment in Home-OCT devices, however, yields retinal OCTs with lower\nquality and a large domain shift, such that out-of-the-pocket segmentation\nnetworks cannot be applied for this type of data. Synthesizing annotated images\nfrom the Home-OCT domain using the proposed approach closes this gap and leads\nto significantly improved segmentation results without adding any further\nsupervision. The advantage of uncertainty-guidance becomes obvious when\ncompared to style transfer: it enables arbitrary domain shifts without any\nstrict learning of an image style. This is also demonstrated in a traffic scene\nexperiment.", "AI": {"tldr": "本文提出了一种利用不确定性引导的ControlNet方法，从无标签领域生成带标注的合成数据，以弥补判别模型在领域漂移问题上的不足，并在视网膜OCT图像分割和交通场景实验中取得了显著效果。", "motivation": "生成模型（如ControlNet）在创建高质量图像数据方面很有价值，但其生成的合成数据往往复刻原始训练分布，限制了对判别模型（如语义分割）的增强效果。此外，判别模型在面对领域漂移（如高质Spectralis OCT与低质Home-OCT）时表现不佳，而目标领域通常缺乏带标注数据。", "method": "引入了不确定性概念到ControlNet的控制机制中。最终网络结合了两种控制：来自无标签数据集的不确定性控制和来自有标签数据集的语义控制。这使得ControlNet能够从目标领域生成具有高不确定性的带标注数据（即从无标签分布中合成带标签数据）。", "result": "在视网膜OCT图像分割场景中，该方法显著改善了Home-OCT数据的分割结果，且无需额外监督。与风格迁移相比，不确定性引导的优势在于能够实现任意领域漂移，而无需严格学习图像风格。这一点也在交通场景实验中得到了验证。", "conclusion": "通过将不确定性引入控制机制，本文提出的方法能够有效利用无标签领域数据训练ControlNet，生成目标领域中带有高不确定性的标注数据，从而弥合领域差距，显著提升判别模型（如分割网络）在面对领域漂移时的性能。"}}
{"id": "2510.11358", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11358", "abs": "https://arxiv.org/abs/2510.11358", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation", "comment": "13 pages, 9 figures", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries.", "AI": {"tldr": "本文引入并系统研究了RAG中“LLM特异性效用”的概念，发现检索内容的效用因LLM而异，人工标注和现有判断方法存在局限，并提出了LLM特异性效用判断的基准测试程序。", "motivation": "传统的RAG检索仅关注相关性，但其有效性更依赖于检索内容的“效用”（utility），即其对生成准确全面答案的帮助。现有研究将效用视为通用属性，忽略了不同LLM因内部知识和理解能力差异，对同一内容可能有不同受益程度。", "method": "本文引入并系统地研究了“LLM特异性效用”的概念。通过跨多个数据集和LLM的大规模实验进行验证。在此基础上，提出了一个针对LLM特异性效用判断的基准测试程序，并评估了六个数据集上现有的效用判断方法。", "result": "研究发现，人工标注的段落对LLM并非最优，且“真值”效用段落不可在不同LLM之间迁移，这凸显了RAG研究中采纳LLM特异性效用的必要性。结果表明，部分人工标注段落对特定LLM并非真值效用段落，部分原因是查询和段落对LLM的可读性不同，其中困惑度是关键指标。在现有效用判断方法中，使用伪答案的语言化方法表现稳健，但LLM在有效评估效用方面仍面临挑战，例如未能拒绝已知查询的所有不相关段落，也未能为未知查询选择真正有用的段落。", "conclusion": "RAG研究中必须采纳LLM特异性效用的概念。现有效用判断方法，尽管部分（如语言化方法）表现稳健，但LLM自身在有效评估内容效用方面仍存在显著局限性。"}}
{"id": "2510.11372", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11372", "abs": "https://arxiv.org/abs/2510.11372", "authors": ["Dean L. Slack", "Noura Al Moubayed"], "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning", "comment": "Accepted to Transactions of the ACL (TACL), 2025. 15 pages, 6\n  figures, 3 tables", "summary": "Although large language models excel across many tasks, they can memorise\ntraining data and thereby expose private or copyrighted text. Most defences\ntarget the pre-training stage, leaving memorisation during fine-tuning,\nespecially for domain adaptation and instruction tuning, poorly understood. We\nfine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on\ncommon evaluation datasets and track verbatim memorisation throughout training.\nWe find that memorisation increases dramatically in the first few epochs, often\nsignificantly before either validation perplexity or evaluation performance is\noptimised. We use a simple but effective n-gram memorisation score which\nreliably precedes verbatim memorisation; using it as an early-stopping\ncriterion mitigates memorisation with minimal performance loss. Further, we\nintroduce an n-gram-aware loss regulariser and show that it reduces\nmemorisation across all model families tested by up to 40% while minimising\nevaluation performance trade-offs when compared to an existing memorisation\nmitigation strategy. These results yield practical, scalable insights into\nmemorisation dynamics during language model fine-tuning.", "AI": {"tldr": "本文研究了大型语言模型在微调阶段的记忆化问题，发现记忆化在早期阶段显著增加。作者提出了一种基于n-gram的记忆化评分作为早期停止标准，并引入了一种n-gram感知的损失正则化器，有效地减少了记忆化，同时保持了模型性能。", "motivation": "尽管大型语言模型表现出色，但它们会记忆训练数据，可能泄露隐私或受版权保护的文本。大多数防御措施都针对预训练阶段，而对微调（特别是领域适应和指令微调）期间的记忆化了解甚少。", "method": "研究人员对Pythia、Llama3和Mistral模型（1.4B-70B参数）在常见评估数据集上进行微调，并跟踪训练过程中的逐字记忆化。他们提出了一个简单有效的n-gram记忆化评分作为早期停止标准，并引入了一个n-gram感知的损失正则化器，并将其与现有记忆化缓解策略进行比较。", "result": "研究发现，记忆化在前几个时期急剧增加，通常在验证困惑度或评估性能优化之前。n-gram记忆化评分能可靠地预测逐字记忆化；将其作为早期停止标准可以有效缓解记忆化，且性能损失最小。此外，n-gram感知的损失正则化器在所有测试模型家族中将记忆化降低了高达40%，与现有缓解策略相比，最大程度地减少了评估性能的权衡。", "conclusion": "这些结果为语言模型微调期间的记忆化动态提供了实用、可扩展的见解，并提出了有效的缓解策略。"}}
{"id": "2510.10973", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10973", "abs": "https://arxiv.org/abs/2510.10973", "authors": ["Sanchit Sinha", "Oana Frunza", "Kashif Rasul", "Yuriy Nevmyvaka", "Aidong Zhang"], "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning", "comment": "23 pages", "summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached\nstate-of-the-art on many visual reasoning tasks, including chart reasoning, yet\nthey still falter on out-of-distribution (OOD) data, and degrade further when\nasked to produce their chain-of-thought (CoT) rationales, limiting\nexplainability. We present Chart-RVR, a general framework that fine-tunes LVLMs\nto be more robust and explainable for chart reasoning by coupling Group\nRelative Policy Optimization (GRPO) with automatically verifiable rewards. Our\nframework comprises of three rewards that maximize: (i) correct chart-type\nclassification, (ii) faithful chart table reconstruction, and (iii) process\nconformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently\noutperforms standard supervised fine-tuning (SFT) on both in-distribution and\nout-of-distribution datasets, closing the OOD performance gap while improving\nrationale fidelity. The resulting models, the Chart-RVR-3B series, achieve\nstate-of-the-art results on six chart-reasoning benchmarks spanning in-domain\nand OOD settings, surpassing all existing models of comparable size. Beyond\naccuracy, Chart-RVR yields more interpretable CoT rationales, strengthening\ntrust and reliability - showcasing the power of verifiable rewards with GRPO\nfor training reliable, interpretable chart-reasoning models.", "AI": {"tldr": "本文提出了Chart-RVR框架，通过结合GRPO和可验证奖励来微调大型视觉语言模型（LVLMs），使其在图表推理任务中对分布外（OOD）数据更具鲁棒性，并能生成更具解释性的思维链（CoT）推理过程，从而实现最先进的性能和更高的可信度。", "motivation": "尽管大型视觉语言模型（LVLMs）在许多视觉推理任务（包括图表推理）上表现出色，但它们在处理分布外（OOD）数据时性能会下降，并且在生成思维链（CoT）推理过程时表现更差，这限制了其可解释性。", "method": "Chart-RVR是一个通用框架，它通过将组相对策略优化（GRPO）与自动可验证奖励相结合来微调LVLMs，以提高其在图表推理中的鲁棒性和可解释性。该框架包含三个奖励，分别最大化：(i) 正确的图表类型分类，(ii) 忠实的图表表格重建，以及 (iii) 过程一致性。", "result": "将Chart-RVR应用于30亿参数的LVLMs，其在分布内和分布外数据集上均持续优于标准的监督微调（SFT），缩小了OOD性能差距，并提高了推理的忠实性。由此产生的Chart-RVR-3B系列模型在六个涵盖域内和OOD设置的图表推理基准测试中取得了最先进的结果，超越了所有现有同等规模的模型。除了准确性，Chart-RVR还产生了更具可解释性的CoT推理过程，增强了信任和可靠性。", "conclusion": "可验证奖励与GRPO相结合对于训练可靠、可解释的图表推理模型具有强大的能力。Chart-RVR框架显著提升了LVLMs在图表推理任务中的鲁棒性、解释性和整体性能，尤其是在处理分布外数据方面。"}}
{"id": "2510.11328", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11328", "abs": "https://arxiv.org/abs/2510.11328", "authors": ["Chenxi Wang", "Yixuan Zhang", "Ruiji Yu", "Yufei Zheng", "Lang Gao", "Zirui Song", "Zixiang Xu", "Gus Xia", "Huishuai Zhang", "Dongyan Zhao", "Xiuying Chen"], "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control", "comment": "19 pages, 8 figures, 8 tables. Code and dataset available at\n  https://github.com/Aurora-cx/EmotionCircuits-LLM", "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.", "AI": {"tldr": "本研究系统性地揭示并验证了大型语言模型（LLMs）中的情感回路，通过构建受控数据集、提取上下文无关情感方向、识别局部情感计算组件并整合为全局回路，实现了对LLM情感表达的精确控制，准确率高达99.65%，超越了现有方法。", "motivation": "随着对LLMs情感智能需求的增长，理解和控制生成文本中情感表达的内部机制成为关键挑战。本研究旨在探究LLMs是否包含上下文无关的情感机制、这些机制的形式，以及它们能否用于通用情感控制。", "method": "研究首先构建了受控数据集SEV（情景-事件与效价）以引发可比较的情感内部状态。随后，提取了上下文无关的情感方向，揭示了情感的跨上下文一致编码。通过分析分解和因果分析，识别了局部实现情感计算的神经元和注意力头，并通过消融和增强干预验证了它们的因果作用。最后，量化了各子层对模型最终情感表示的因果影响，并将识别出的局部组件整合为驱动情感表达的全局情感回路。", "result": "研究发现LLMs中存在一致的、跨上下文的情感编码机制。通过直接调节识别出的全局情感回路，在测试集上实现了99.65%的情感表达准确率，显著超越了基于提示和转向的方法。同时，成功识别并整合了驱动情感表达的局部计算组件和全局情感回路。", "conclusion": "本研究首次系统地揭示并验证了LLMs中的情感回路，为LLMs的可解释性和可控情感智能提供了新见解。通过精确控制这些内部回路，可以实现对LLM情感表达的高效调节。"}}
{"id": "2510.10993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10993", "abs": "https://arxiv.org/abs/2510.10993", "authors": ["Yuxin Cheng", "Binxiao Huang", "Taiqiang Wu", "Wenyong Zhou", "Chenchen Ding", "Zhengwu Liu", "Graziano Chesi", "Ngai Wong"], "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency", "comment": null, "summary": "3D Gaussian inpainting, a critical technique for numerous applications in\nvirtual reality and multimedia, has made significant progress with pretrained\ndiffusion models. However, ensuring multi-view consistency, an essential\nrequirement for high-quality inpainting, remains a key challenge. In this work,\nwe present PAInpainter, a novel approach designed to advance 3D Gaussian\ninpainting by leveraging perspective-aware content propagation and consistency\nverification across multi-view inpainted images. Our method iteratively refines\ninpainting and optimizes the 3D Gaussian representation with multiple views\nadaptively sampled from a perspective graph. By propagating inpainted images as\nprior information and verifying consistency across neighboring views,\nPAInpainter substantially enhances global consistency and texture fidelity in\nrestored 3D scenes. Extensive experiments demonstrate the superiority of\nPAInpainter over existing methods. Our approach achieves superior 3D inpainting\nquality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and\nNeRFiller datasets, respectively, highlighting its effectiveness and\ngeneralization capability.", "AI": {"tldr": "PAInpainter是一种新颖的3D高斯修复方法，通过透视感知内容传播和多视角一致性验证，显著提升了修复场景的全局一致性和纹理保真度。", "motivation": "尽管预训练扩散模型在3D高斯修复方面取得了进展，但确保多视角一致性（高质量修复的关键要求）仍然是一个主要挑战。", "method": "PAInpainter通过从透视图中自适应采样多个视角，迭代地细化修复并优化3D高斯表示。它将修复后的图像作为先验信息进行传播，并验证相邻视角之间的一致性，从而增强全局一致性和纹理保真度。", "result": "PAInpainter在SPIn-NeRF和NeRFiller数据集上分别达到了26.03 dB和29.51 dB的PSNR分数，优于现有方法，展示了卓越的3D修复质量、有效性和泛化能力。", "conclusion": "PAInpainter通过其透视感知内容传播和一致性验证机制，显著提升了恢复3D场景的全局一致性和纹理保真度，解决了多视角一致性难题。"}}
{"id": "2510.10986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10986", "abs": "https://arxiv.org/abs/2510.10986", "authors": ["Xiaoyu Ma", "Ding Ding", "Hao Chen"], "title": "Mixup Helps Understanding Multimodal Video Better", "comment": null, "summary": "Multimodal video understanding plays a crucial role in tasks such as action\nrecognition and emotion classification by combining information from different\nmodalities. However, multimodal models are prone to overfitting strong\nmodalities, which can dominate learning and suppress the contributions of\nweaker ones. To address this challenge, we first propose Multimodal Mixup (MM),\nwhich applies the Mixup strategy at the aggregated multimodal feature level to\nmitigate overfitting by generating virtual feature-label pairs. While MM\neffectively improves generalization, it treats all modalities uniformly and\ndoes not account for modality imbalance during training. Building on MM, we\nfurther introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts\nthe mixing ratios for each modality based on their relative contributions to\nthe learning objective. Extensive experiments on several datasets demonstrate\nthe effectiveness of our methods in improving generalization and multimodal\nrobustness.", "AI": {"tldr": "本文提出了多模态Mixup (MM) 和平衡多模态Mixup (B-MM) 方法，旨在解决多模态视频理解中强模态过拟合和模态不平衡问题，从而提高模型的泛化能力和鲁棒性。", "motivation": "多模态模型容易过度拟合强模态，导致强模态主导学习并抑制弱模态的贡献，从而影响模型的泛化能力。", "method": "1. **多模态Mixup (MM)**：在聚合的多模态特征层面应用Mixup策略，生成虚拟特征-标签对以缓解过拟合。2. **平衡多模态Mixup (B-MM)**：在MM的基础上，根据每种模态对学习目标的相对贡献，动态调整它们的混合比例，以解决训练过程中的模态不平衡问题。", "result": "在多个数据集上进行了广泛的实验，结果表明所提出的方法在提高泛化能力和多模态鲁棒性方面是有效的。", "conclusion": "通过引入多模态Mixup (MM) 和平衡多模态Mixup (B-MM) 策略，可以有效缓解多模态模型中的过拟合和模态不平衡问题，从而显著提升多模态视频理解任务的性能和鲁棒性。"}}
{"id": "2510.11389", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11389", "abs": "https://arxiv.org/abs/2510.11389", "authors": ["Zirui Song", "Yuan Huang", "Junchang Liu", "Haozhe Luo", "Chenxi Wang", "Lang Gao", "Zixiang Xu", "Mingfei Han", "Xiaojun Chang", "Xiuying Chen"], "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies", "comment": "34 pages, 32figures", "summary": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction.", "AI": {"tldr": "该研究构建了一个高质量、多模态的狼人杀数据集，并提出了一种新的策略对齐评估框架，用于细粒度地评估大型语言模型在社交推理游戏中的语言和推理能力，揭示了当前模型在欺骗和反事实推理方面的不足。", "motivation": "现有的狼人杀游戏研究多采用基于LLM的自我对弈，导致生成模板化言语和零散案例，忽略了社交游戏的丰富性。此外，由于缺乏高质量的参考数据，评估依赖于粗糙的指标（如生存时间或主观评分）。", "method": "1. 整理并发布了一个高质量、经人工验证的多模态狼人杀数据集，包含超过100小时视频、32.4M发言tokens和15种规则变体。2. 基于该数据集，提出了一种新颖的策略对齐评估框架，将获胜阵营的策略作为真值进行两阶段评估：a) 言语评估：以多项选择任务形式，评估模型在五个社交能力维度上采取适当立场的能力；b) 决策评估：评估模型的投票选择和对手角色推断能力。", "result": "实验表明，最先进的LLM表现多样，约有一半得分低于0.50，揭示了它们在欺骗和反事实推理方面存在明显的差距。", "conclusion": "该数据集和评估框架能够对模型的语言和推理能力进行细粒度评估，并捕捉其生成策略连贯游戏行为的能力。研究者希望该数据集能进一步激发多智能体交互中语言、推理和策略方面的研究。"}}
{"id": "2510.11391", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11391", "abs": "https://arxiv.org/abs/2510.11391", "authors": ["Junpeng Liu", "Yuzhong Zhao", "Bowen Cao", "Jiayu Ding", "Yilin Jia", "Tengchao Lv", "Yupan Huang", "Shaohan Huang", "Nan Yang", "Li Dong", "Lei Cui", "Tao Ge", "Xun Wang", "Huitian Jiao", "Sun Mao", "FNU Kartik", "Si-Qing Chen", "Wai Lam", "Furu Wei"], "title": "DocReward: A Document Reward Model for Structuring and Stylizing", "comment": null, "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.", "AI": {"tldr": "现有智能体工作流在文档生成中忽视视觉结构和风格，本文提出DocReward模型，通过构建DocPair数据集和Bradley-Terry损失训练，有效评估文档的结构和风格专业性，其性能显著优于GPT-4o和GPT-5，并能指导生成智能体产生更受人类偏好的文档。", "motivation": "当前的智能体工作流在文档生成方面主要关注文本质量，但忽略了对可读性和吸引力至关重要的视觉结构和风格。这种缺陷主要是由于缺乏合适的奖励模型来指导智能体工作流生成具有更强结构和风格质量的文档。", "method": "本文提出了DocReward，一个用于评估文档结构和风格的文档奖励模型。为此，构建了一个包含117K配对文档的多领域数据集DocPair，涵盖32个领域和267种文档类型，每对文档包含内容相同但结构和风格不同的高专业度和低专业度文档。DocReward使用Bradley-Terry损失进行训练，通过惩罚与标注排名矛盾的预测来对文档进行评分。为了评估奖励模型的性能，还创建了一个由受过良好教育的人类评估员排名文档包的测试数据集。", "result": "DocReward在准确性方面分别比GPT-4o和GPT-5高出30.6和19.4个百分点，显示出其优于基线模型的性能。在文档生成的外在评估中，DocReward实现了60.8%的显著更高胜率，而GPT-5的胜率为37.7%，这表明DocReward在指导生成智能体生成人类偏好文档方面的实用性。", "conclusion": "DocReward成功地解决了智能体工作流在文档生成中忽视视觉结构和风格的问题，其卓越的评估能力和在指导生成任务中的有效性，使其能够帮助智能体生成更符合人类偏好的专业文档。"}}
{"id": "2510.11407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11407", "abs": "https://arxiv.org/abs/2510.11407", "authors": ["Sahil Kale", "Devendra Singh Dhami"], "title": "KnowRL: Teaching Language Models to Know What They Know", "comment": "14 pages, 7 figures", "summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models.", "AI": {"tldr": "KnowRL是一个简单但强大的框架，它通过内省和基于共识的奖励机制，使大型语言模型（LLMs）能够自我提升其对自身能力边界的理解，从而提高可靠性和安全性，且无需外部监督。", "motivation": "当前LLMs经常错误判断自身能力，导致其在超过五分之一的情况下无法完全信任其回答。真正可靠的AI需要知道自己知道什么以及不知道什么，这促使研究人员寻求一种方法来增强模型对其可行性边界的内部理解。", "method": "本文提出了KnowRL框架，结合了两个核心组件：(i) 内省：模型生成并分类它认为可行或不可行的任务；(ii) 基于共识的奖励：通过内部一致性来强化自我知识评估的稳定性。该框架利用内部生成的数据，增强了自我知识的一致性，完全避免了昂贵的外部监督。", "result": "在LLaMA-3.1-8B和Qwen-2.5-7B上的实验表明，KnowRL持续提升了模型的自我知识，并通过内在自我一致性和外在基准测试得到了验证。仅使用少量种子数据且无外部监督，该方法在准确性上实现了高达28%的提升，F1分数提升了12%，在几次迭代后就超越了基线模型。", "conclusion": "KnowRL框架有效释放了LLMs自我提升知识意识的潜力，为构建更可靠、更负责任的AI以及在关键应用中实现更安全的部署开辟了道路。鉴于其简单性和独立于外部努力的特点，作者鼓励将此可靠性增强过程应用于所有未来的模型。"}}
{"id": "2510.11408", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11408", "abs": "https://arxiv.org/abs/2510.11408", "authors": ["Stefan Krsteski", "Giuseppe Russo", "Serina Chang", "Robert West", "Kristina Gligorić"], "title": "Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification", "comment": "19 pages, 4 figures, 9 tables", "summary": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation.", "AI": {"tldr": "本研究发现，单独使用大型语言模型（LLM）生成调查回复存在显著偏差，但将其与校正方法结合可将偏差降至5%以下，并提高有效样本量。在固定预算下，将大部分人工回复用于校正而非微调，能实现更有效的估计。", "motivation": "传统的调查成本高昂且耗时。尽管大型语言模型（LLM）被提议作为人类受访者的低成本、可扩展替代品，但其输出常有偏差，导致无效估计。本研究旨在解决LLM生成回复的偏差问题，并优化人工回复的分配方式。", "method": "研究了使用LLM生成调查回复的“合成方法”与消除总体估计偏差的“校正方法”之间的相互作用。通过两个包含营养、政治和经济问题的面板调查，探索了如何在合成和校正方法之间最佳分配人类回复。", "result": "单独使用LLM合成会引入显著偏差（24-86%）。然而，将其与校正方法结合后，偏差可降低到5%以下，并将有效样本量提高多达14%。结果表明，在固定预算下，将大部分人工回复分配给校正而不是微调，能带来更有效的估计。", "conclusion": "结合LLM合成与校正方法可以有效降低调查估计的偏差，并提高有效样本量。在利用LLM进行调查时，挑战了将所有人工回复用于微调的常见做法，建议在固定预算下，将大部分人工回复分配给校正，以实现更准确和有效的估计。"}}
{"id": "2510.11434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11434", "abs": "https://arxiv.org/abs/2510.11434", "authors": ["Dana Sotto Porat", "Ella Rabinovich"], "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content", "comment": "ECAI2025 (Identity-Aware AI workshop)", "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.", "AI": {"tldr": "本研究引入了一种数据驱动的新方法，通过自动分类器评估了大型语言模型（LLMs）的个性和性别语言模式，发现LLMs倾向于更高宜人性和更低神经质，且其性别语言模式与人类相似但变异性较低。", "motivation": "生成式大型语言模型在日常生活中变得至关重要，有研究探讨它们是否也表现出个性或人口统计学特征。现有方法可能依赖于自我报告问卷，本研究旨在提出一种不依赖于此的新方法。", "method": "开发了一种新颖的、数据驱动的方法，用于评估LLMs的个性，不依赖于自我报告问卷。该方法将自动个性及性别分类器应用于从Reddit收集的开放式问题上LLM的回复。研究比较了六种常用模型与人类作者的回复。", "result": "LLMs系统性地表现出更高的宜人性（Agreeableness）和更低的神经质（Neuroticism），反映出合作和稳定的对话倾向。模型文本中的性别语言模式与人类作者的模式大致相似，但变异性有所降低。", "conclusion": "本研究贡献了一个包含人类和模型回复的新数据集，并进行了大规模比较分析，为生成式AI的个性和人口统计学模式主题提供了新的见解。"}}
{"id": "2510.11482", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11482", "abs": "https://arxiv.org/abs/2510.11482", "authors": ["Marco Braga", "Gian Carlo Milanese", "Gabriella Pasi"], "title": "Investigating Large Language Models' Linguistic Abilities for Text Preprocessing", "comment": "Accepted in WI-IAT 2025. Pre-camera-ready version", "summary": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat.", "AI": {"tldr": "本研究探讨了使用大型语言模型（LLMs）进行文本预处理（如停用词移除、词形还原和词干提取）的方法，并发现LLMs不仅能高精度地复现传统方法，还能在多语言文本分类任务中带来性能提升。", "motivation": "传统的文本预处理技术（如停用词移除、词形还原、词干提取）通常忽略上下文信息，且依赖于大量的语言特定标注资源。本研究旨在利用LLMs处理上下文信息的能力，来改进这些预处理任务，减少对标注资源的需求。", "method": "研究人员使用LLMs执行停用词移除、词形还原和词干提取任务。他们通过在网络抓取数据上进行全面评估，将LLM预处理与传统算法在六种欧洲语言的多个文本分类任务上进行了比较。评估指标包括LLM复现传统方法的准确率以及使用LLM预处理文本训练的机器学习算法的F1分数。", "result": "LLMs能够以高精度复现传统预处理方法：停用词移除准确率达97%，词形还原达82%，词干提取达74%。此外，经LLM预处理文本训练的机器学习算法，在F1分数上比传统技术提高了高达6%。", "conclusion": "LLMs在文本预处理任务中表现出色，不仅能有效复现传统方法，而且由于其考虑上下文的能力，能显著提升后续机器学习任务（如文本分类）的性能。这表明LLMs是文本预处理的有效且有前景的工具。"}}
{"id": "2510.11444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11444", "abs": "https://arxiv.org/abs/2510.11444", "authors": ["Yawen Yang", "Fukun Ma", "Shiao Meng", "Aiwei Liu", "Lijie Wen"], "title": "GenCNER: A Generative Framework for Continual Named Entity Recognition", "comment": "Accepted by IJCNN 2025", "summary": "Traditional named entity recognition (NER) aims to identify text mentions\ninto pre-defined entity types. Continual Named Entity Recognition (CNER) is\nintroduced since entity categories are continuously increasing in various\nreal-world scenarios. However, existing continual learning (CL) methods for NER\nface challenges of catastrophic forgetting and semantic shift of non-entity\ntype. In this paper, we propose GenCNER, a simple but effective Generative\nframework for CNER to mitigate the above drawbacks. Specifically, we skillfully\nconvert the CNER task into sustained entity triplet sequence generation problem\nand utilize a powerful pre-trained seq2seq model to solve it. Additionally, we\ndesign a type-specific confidence-based pseudo labeling strategy along with\nknowledge distillation (KD) to preserve learned knowledge and alleviate the\nimpact of label noise at the triplet level. Experimental results on two\nbenchmark datasets show that our framework outperforms previous\nstate-of-the-art methods in multiple CNER settings, and achieves the smallest\ngap compared with non-CL results.", "AI": {"tldr": "GenCNER是一种生成式框架，通过将持续命名实体识别（CNER）转换为实体三元组序列生成问题，并结合类型特异性伪标签和知识蒸馏，有效缓解了灾难性遗忘和非实体类型语义漂移问题。", "motivation": "传统的命名实体识别（NER）面临实体类别持续增加的实际场景挑战，催生了持续命名实体识别（CNER）。然而，现有的CNER持续学习方法存在灾难性遗忘和非实体类型语义漂移的问题。", "method": "本文提出了GenCNER框架。它将CNER任务巧妙地转换为持续的实体三元组序列生成问题，并利用强大的预训练seq2seq模型来解决。此外，设计了一种类型特异性的基于置信度的伪标签策略，并结合知识蒸馏（KD）在三元组层面保留学习到的知识并减轻标签噪声的影响。", "result": "在两个基准数据集上的实验结果表明，GenCNER框架在多种CNER设置下优于以往最先进的方法，并且与非持续学习（non-CL）结果相比，差距最小。", "conclusion": "GenCNER提供了一个简单而有效的生成式框架，成功缓解了CNER任务中的灾难性遗忘和非实体类型语义漂移问题，并在性能上取得了显著提升。"}}
{"id": "2510.11496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11496", "abs": "https://arxiv.org/abs/2510.11496", "authors": ["Zhiwei Jin", "Xiaohui Song", "Nan Wang", "Yafei Liu", "Chao Li", "Xin Li", "Ruichen Wang", "Zhihao Li", "Qi Qi", "Long Cheng", "Dongze Hao", "Quanlong Zheng", "Yanhao Zhang", "Haobo Ji", "Jian Ma", "Zhitong Zheng", "Zhenyi Lin", "Haolin Deng", "Xin Zou", "Xiaojie Yin", "Ruilin Wang", "Liankai Cai", "Haijing Liu", "Yuqing Qiu", "Ke Chen", "Zixian Li", "Chi Xie", "Huafei Li", "Chenxing Li", "Chuangchuang Wang", "Kai Tang", "Zhiguang Zhu", "Kai Tang", "Wenmei Gao", "Rui Wang", "Jun Wu", "Chao Liu", "Qin Xie", "Chen Chen", "Haonan Lu"], "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model", "comment": "Tech report of OPPO AndesVL Team", "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR", "AI": {"tldr": "AndesVL是一个参数量0.6B到4B的移动端多模态大模型套件，基于Qwen3和多种视觉编码器，在多项开源基准测试中表现优异，并引入了1+N LoRA。", "motivation": "云端多模态大模型（如GPT-4o）性能卓越，但参数量巨大，无法部署在内存、功耗和计算能力受限的移动边缘设备上。", "method": "该研究引入了AndesVL，一个基于Qwen3的LLM和多种视觉编码器构建的移动端多模态大模型套件，参数量从0.6B到4B。论文详细阐述了其模型架构、训练流程和训练数据，并引入了1+N LoRA方法。", "result": "AndesVL在多种开源基准测试中，包括富文本图像理解、推理与数学、多图像理解、通用VQA、幻觉缓解、多语言理解和GUI相关任务，均达到了同等规模最先进模型的第一梯队性能。", "conclusion": "AndesVL成功地将高性能多模态大模型部署到移动端设备，解决了云端模型在边缘设备的限制，并在同等规模下取得了领先性能，为移动端MLLM提供了有效解决方案。"}}
{"id": "2510.11000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11000", "abs": "https://arxiv.org/abs/2510.11000", "authors": ["Ruihang Xu", "Dewei Zhou", "Fan Ma", "Yi Yang"], "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation", "comment": "Project Page: https://nenhang.github.io/ContextGen/", "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.", "AI": {"tldr": "ContextGen是一个新的扩散Transformer框架，通过引入上下文布局锚定（CLA）和身份一致性注意力（ICA）机制，解决了多实例图像生成中布局控制和身份保持的挑战。该方法结合了布局和参考图像的引导，并推出了IMIG-100K数据集，在控制精度、身份保真度和视觉质量方面超越了现有技术。", "motivation": "现代扩散模型在多实例图像生成（MIG）中面临重大挑战，主要体现在难以精确控制物体布局和保持多个不同主体的身份一致性。", "method": "本文提出了ContextGen，一个由布局和参考图像引导的新型扩散Transformer框架。它包含两个关键技术贡献：1) 上下文布局锚定（CLA）机制，将复合布局图像整合到生成上下文中，以稳健地锚定物体；2) 身份一致性注意力（ICA），利用上下文参考图像确保多实例的身份一致性。此外，还引入了IMIG-100K，第一个具有详细布局和身份标注的大规模分层结构数据集。", "result": "广泛的实验证明，ContextGen在控制精度、身份保真度和整体视觉质量方面均超越了现有方法，达到了新的技术水平。", "conclusion": "ContextGen通过其创新的CLA和ICA机制，并结合新的IMIG-100K数据集，有效解决了多实例图像生成中的布局控制和身份保持问题，显著提升了生成性能，树立了新的行业标杆。"}}
{"id": "2510.11005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11005", "abs": "https://arxiv.org/abs/2510.11005", "authors": ["Kai Han", "Siqi Ma", "Chengxuan Qian", "Jun Chen", "Chongwen Lyu", "Yuqing Song", "Zhe Liu"], "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation", "comment": null, "summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images\nis essential for surgical planning and tumor staging. Although foundation\nmodels generally perform well in segmentation tasks, they often struggle to\nfocus on foreground areas in complex, low-contrast backgrounds, where some\nmalignant tumors closely resemble normal organs, complicating contextual\ndifferentiation. To address these challenges, we propose the Foreground-Aware\nSpectrum Segmentation (FASS) framework. First, we introduce a foreground-aware\nmodule to amplify the distinction between background and the entire volume\nspace, allowing the model to concentrate more effectively on target areas.\nNext, a feature-level frequency enhancement module, based on wavelet transform,\nextracts discriminative high-frequency features to enhance boundary recognition\nand detail perception. Eventually, we introduce an edge constraint module to\npreserve geometric continuity in segmentation boundaries. Extensive experiments\non multiple medical datasets demonstrate superior performance across all\nmetrics, validating the effectiveness of our framework, particularly in\nrobustness under complex conditions and fine structure recognition. Our\nframework significantly enhances segmentation of low-contrast images, paving\nthe way for applications in more diverse and complex medical imaging scenarios.", "AI": {"tldr": "本文提出前景感知谱分割（FASS）框架，通过前景感知、频率增强和边缘约束模块，有效解决了医学图像中低对比度肿瘤与正常组织分割的挑战，提升了分割精度和鲁棒性。", "motivation": "基础模型在复杂、低对比度背景下（如恶性肿瘤与正常器官相似时）难以有效区分前景区域，导致上下文区分困难，从而影响肿瘤分割的准确性，而准确的肿瘤分割对于手术规划和肿瘤分期至关重要。", "method": "本文提出前景感知谱分割（FASS）框架。首先，引入前景感知模块以增强背景与整个体积空间的区分，使模型更有效地聚焦目标区域。其次，基于小波变换的特征级频率增强模块被用于提取区分性高频特征，以增强边界识别和细节感知。最后，引入边缘约束模块来保持分割边界的几何连续性。", "result": "在多个医学数据集上的广泛实验表明，FASS框架在所有评估指标上均表现出卓越性能，验证了其有效性，尤其是在复杂条件下的鲁棒性和精细结构识别能力。该框架显著提升了低对比度图像的分割效果。", "conclusion": "FASS框架显著增强了低对比度医学图像的分割效果，其在复杂条件下的鲁棒性和精细结构识别能力为在更多样化和复杂的医学成像场景中的应用铺平了道路。"}}
{"id": "2510.11012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11012", "abs": "https://arxiv.org/abs/2510.11012", "authors": ["Sanchit Sinha", "Guangzhi Xiong", "Aidong Zhang"], "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models", "comment": "EMNLP 2025 (main)", "summary": "Compositional reasoning remains a persistent weakness of modern vision\nlanguage models (VLMs): they often falter when a task hinges on understanding\nhow multiple objects, attributes, and relations interact within an image.\nMultiple research works have attempted to improve compositionality performance\nby creative tricks such as improving prompt structure, chain of thought\nreasoning, etc. A more recent line of work attempts to impart additional\nreasoning in VLMs using well-trained Large Language Models (LLMs), which are\nfar superior in linguistic understanding than VLMs to compensate for the\nlimited linguistic prowess of VLMs. However, these approaches are either\nresource-intensive or do not provide an interpretable reasoning process. In\nthis paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs\nwith carefully designed neurosymbolic concept trees learned from LLMs to\nimprove VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning\nprocess boosts compositionality performance and provides a rationale behind VLM\npredictions. Empirical results on four compositionality benchmarks, Winoground,\nEqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with\nvarying sizes, demonstrate that COCO-Tree significantly improves compositional\ngeneralization by 5-10% over baselines.", "AI": {"tldr": "COCO-Tree是一种新颖的方法，通过利用LLM学习的神经符号概念树来增强VLM输出，从而显著提高VLM的组合推理能力和可解释性。", "motivation": "现代视觉语言模型（VLM）在组合推理方面存在固有弱点，难以理解图像中多个对象、属性和关系之间的复杂交互。现有方法（如改进提示结构、思维链推理或结合LLM）要么资源密集，要么缺乏可解释的推理过程。", "method": "本文提出“COCO-Tree”方法，该方法使用从大型语言模型（LLM）学习到的精心设计的神经符号概念树来增强VLM的输出。COCO-Tree采用受束搜索启发的推理过程，旨在提升组合性表现并为VLM预测提供理由。", "result": "在Winoground、EqBench、ColorSwap和SugarCrepe这四个组合性基准测试上，对七种不同规模的开源VLM进行评估，结果表明COCO-Tree将组合泛化能力比基线显著提高了5-10%。此外，它还提供了VLM预测背后的可解释理由。", "conclusion": "COCO-Tree通过结合LLM学习的神经符号概念树，有效解决了VLM在组合推理方面的不足，显著提升了模型性能和可解释性，为VLM的语言推理能力带来了改进。"}}
{"id": "2510.11512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11512", "abs": "https://arxiv.org/abs/2510.11512", "authors": ["Jianhao Yuan", "Fabio Pizzati", "Francesco Pinto", "Lars Kunze", "Ivan Laptev", "Paul Newman", "Philip Torr", "Daniele De Martini"], "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference", "comment": "22 pages, 9 figures", "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.", "AI": {"tldr": "该研究引入了一种名为LikePhys的免训练方法，用于评估视频扩散模型中的直观物理理解能力，通过区分物理上有效和不可能的视频，并构建基准和指标来系统性地测试和分析当前模型的表现。", "motivation": "在视频生成中，准确评估视频扩散模型对直观物理的理解能力是一项挑战，因为很难将物理正确性与视觉外观分离。", "method": "该研究提出LikePhys，一种免训练方法，利用去噪目标作为基于ELBO的似然替代，在一个精心策划的有效-无效视频对数据集上区分物理上有效和不可能的视频。他们构建了一个包含四个物理领域、十二个场景的基准，并引入了评估指标“合理性偏好错误”（PPE）。", "result": "PPE评估指标与人类偏好高度一致，优于现有评估基线。系统性基准测试显示，尽管当前模型在复杂和混沌动力学方面仍面临挑战，但随着模型容量和推理设置的扩展，物理理解能力呈现出明显的改进趋势。研究还分析了模型设计和推理设置如何影响物理理解，并突出了不同物理定律下的领域特定能力差异。", "conclusion": "LikePhys提供了一种有效评估视频扩散模型直观物理理解能力的方法。尽管当前模型在处理复杂物理时仍有局限性，但随着模型扩展，其物理理解能力正在持续提升，为构建更具物理合理性的世界模拟器奠定了基础。"}}
{"id": "2510.11529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11529", "abs": "https://arxiv.org/abs/2510.11529", "authors": ["Yusheng Song", "Lirong Qiu", "Xi Zhang", "Zhihao Tang"], "title": "Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models", "comment": null, "summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet.", "AI": {"tldr": "该研究通过统一内部状态探测和思维链验证，解决了大型语言模型幻觉检测中的“检测困境”，引入多路径推理和跨注意力机制克服信号稀缺和表示对齐障碍，显著提升了检测性能。", "motivation": "现有的大型语言模型幻觉检测方法存在“检测困境”：内部状态探测擅长事实不一致但对逻辑谬误无效，而思维链验证则相反，导致在不同任务类型（如开放域问答或数学推理）中出现盲点。", "method": "提出一个统一框架来弥合内部状态探测和思维链验证之间的鸿沟。为克服“信号稀缺障碍”，引入多路径推理机制以获取更可比的细粒度信号；为克服“表示对齐障碍”，引入分段感知时序交叉注意力模块来自适应地融合对齐后的表示，从而识别细微的不一致。", "result": "在三个多样化基准和两个主流大型语言模型上的大量实验表明，该框架持续且显著优于强大的基线方法。", "conclusion": "该框架成功解决了大型语言模型幻觉检测中的“检测困境”，通过统一两种互补的检测方法，克服了关键的技术挑战，显著提高了幻觉检测的鲁棒性和有效性。"}}
{"id": "2510.11017", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11017", "abs": "https://arxiv.org/abs/2510.11017", "authors": ["Runyang Feng", "Hyung Jin Chang", "Tze Ho Elden Tse", "Boeun Kim", "Yi Chang", "Yixing Gao"], "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation", "comment": "This paper is accepted to ICCV 2025", "summary": "Modeling high-resolution spatiotemporal representations, including both\nglobal dynamic contexts (e.g., holistic human motion tendencies) and local\nmotion details (e.g., high-frequency changes of keypoints), is essential for\nvideo-based human pose estimation (VHPE). Current state-of-the-art methods\ntypically unify spatiotemporal learning within a single type of modeling\nstructure (convolution or attention-based blocks), which inherently have\ndifficulties in balancing global and local dynamic modeling and may bias the\nnetwork to one of them, leading to suboptimal performance. Moreover, existing\nVHPE models suffer from quadratic complexity when capturing global\ndependencies, limiting their applicability especially for high-resolution\nsequences. Recently, the state space models (known as Mamba) have demonstrated\nsignificant potential in modeling long-range contexts with linear complexity;\nhowever, they are restricted to 1D sequential data. In this paper, we present a\nnovel framework that extends Mamba from two aspects to separately learn global\nand local high-resolution spatiotemporal representations for VHPE.\nSpecifically, we first propose a Global Spatiotemporal Mamba, which performs 6D\nselective space-time scan and spatial- and temporal-modulated scan merging to\nefficiently extract global representations from high-resolution sequences. We\nfurther introduce a windowed space-time scan-based Local Refinement Mamba to\nenhance the high-frequency details of localized keypoint motions. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed model\noutperforms state-of-the-art VHPE approaches while achieving better\ncomputational trade-offs.", "AI": {"tldr": "本文提出了一种基于Mamba的新型框架，通过分别学习全局和局部高分辨率时空表示来改进视频人体姿态估计（VHPE），解决了现有方法在平衡全局/局部动态建模和处理高分辨率序列时的二次复杂度问题。", "motivation": "现有VHPE方法难以在单一建模结构（卷积或注意力）中平衡全局动态上下文和局部运动细节，可能导致次优性能。此外，它们在捕获全局依赖时存在二次复杂度，限制了在高分辨率序列上的应用。尽管Mamba在建模长距离上下文方面具有线性复杂度潜力，但其仅限于一维序列数据。", "method": "本研究将Mamba从两个方面进行扩展：1) 提出了“全局时空Mamba”，通过6D选择性时空扫描和空间/时间调制扫描合并来高效提取高分辨率序列的全局表示；2) 引入了基于窗口时空扫描的“局部细化Mamba”，以增强局部关键点运动的高频细节。", "result": "在四个基准数据集上的大量实验表明，所提出的模型优于现有的最先进VHPE方法，并实现了更好的计算权衡。", "conclusion": "该论文成功地将Mamba扩展到高分辨率时空数据，并为VHPE提供了一个新颖的框架，有效解决了全局与局部动态建模的平衡以及高分辨率序列的计算效率问题，从而提升了性能。"}}
{"id": "2510.11615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11615", "abs": "https://arxiv.org/abs/2510.11615", "authors": ["Xurong Xie", "Zhucun Xue", "Jiafu Wu", "Jian Li", "Yabiao Wang", "Xiaobin Hu", "Yong Liu", "Jiangning Zhang"], "title": "LLM-Oriented Token-Adaptive Knowledge Distillation", "comment": "15 pages, 4 figures", "summary": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.", "AI": {"tldr": "本文提出了一种名为AdaKD的新型面向大型语言模型（LLM）的自适应知识蒸馏框架，它通过实时调整蒸馏过程以适应每个token的学习状态，解决了现有静态蒸馏方法的局限性。", "motivation": "现有的基于logit的知识蒸馏方法通常采用静态策略，与学生模型的动态学习过程不符。它们不加区分地处理所有token，并使用单一的固定温度，导致知识转移效果不佳。", "method": "AdaKD框架包含两个协同模块，均由统一的token难度度量驱动：1. 损失驱动的自适应token聚焦（LATF）模块：通过监控学生的学习稳定性，动态调整蒸馏焦点，在每个训练阶段将计算资源集中在最有价值的token上。2. 逆难度温度缩放（IDTS）模块：为困难token使用低温度进行有针对性的错误纠正，为简单token使用高温度以鼓励学生学习教师模型完整平滑的输出分布，从而增强泛化能力。", "result": "作为一个即插即用的框架，AdaKD能够持续改进各种蒸馏方法在多种模型架构和基准上的性能。", "conclusion": "AdaKD通过引入token级别的自适应策略，显著提升了大型语言模型知识蒸馏的效果和泛化能力，为LLM压缩提供了一个有效且通用的解决方案。"}}
{"id": "2510.11599", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11599", "abs": "https://arxiv.org/abs/2510.11599", "authors": ["Marc Brinner", "Sina Zarrieß"], "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping", "comment": null, "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating\nmultifaceted embeddings of scientific abstracts, evaluated in the domains of\ninvasion biology and medicine. These embeddings capture distinct, individually\nspecifiable aspects in isolation, thus enabling fine-grained and controllable\nsimilarity assessments as well as adaptive, user-driven visualizations of\nscientific domains. Our approach relies on an unsupervised procedure that\nproduces aspect-specific summarizing sentences and trains embedding models to\nmap semantically related summaries to nearby positions in the embedding space.\nWe then distill these aspect-specific embedding capabilities into a unified\nembedding model that directly predicts multiple aspect embeddings from a\nscientific abstract in a single, efficient forward pass. In addition, we\nintroduce an embedding decoding pipeline that decodes embeddings back into\nnatural language descriptions of their associated aspects. Notably, we show\nthat this decoding remains effective even for unoccupied regions in\nlow-dimensional visualizations, thus offering vastly improved interpretability\nin user-centric settings.", "AI": {"tldr": "本文提出了SemCSE-Multi，一个无监督框架，用于生成科学摘要的多方面嵌入，支持细粒度相似性评估、自适应可视化和通过解码提高可解释性。", "motivation": "现有嵌入模型难以捕捉科学摘要中独立、可区分的特定方面，导致无法进行细粒度、可控的相似性评估和用户驱动的领域可视化。此外，在用户中心设置中，嵌入模型的可解释性有待提高。", "method": "该方法包括：1) 一个无监督过程，生成与特定方面相关的总结句；2) 训练嵌入模型，将语义相关的总结映射到嵌入空间的相邻位置；3) 将这些特定方面的嵌入能力整合到一个统一的嵌入模型中，以高效地预测多个方面嵌入；4) 引入一个嵌入解码管道，将嵌入解码回自然语言描述，即使对于低维可视化中未被占据的区域也有效。", "result": "SemCSE-Multi能够生成捕获独立可区分方面的多方面嵌入，从而实现细粒度、可控的相似性评估和自适应、用户驱动的科学领域可视化。嵌入解码管道即使对于低维可视化中的空白区域也能保持有效，显著提高了用户中心设置中的可解释性。该框架已在入侵生物学和医学领域进行了评估。", "conclusion": "SemCSE-Multi是一个新颖的无监督框架，能够为科学摘要生成多方面嵌入，从而实现更精细、可控的相似性分析、适应性可视化和显著增强的可解释性，对科学领域的理解和探索具有重要意义。"}}
{"id": "2510.11537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11537", "abs": "https://arxiv.org/abs/2510.11537", "authors": ["Ba-Quang Nguyen"], "title": "An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification", "comment": "11 pages, 1 figure. Submitted to VLSP 2025 and reviewed", "summary": "We propose a novel neural architecture named TextGraphFuseGAT, which\nintegrates a pretrained transformer encoder (PhoBERT) with Graph Attention\nNetworks for token-level classification tasks. The proposed model constructs a\nfully connected graph over the token embeddings produced by PhoBERT, enabling\nthe GAT layer to capture rich inter-token dependencies beyond those modeled by\nsequential context alone. To further enhance contextualization, a\nTransformer-style self-attention layer is applied on top of the graph-enhanced\nembeddings. The final token representations are passed through a classification\nhead to perform sequence labeling. We evaluate our approach on three Vietnamese\nbenchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19\ndomain, PhoDisfluency for speech disfluency detection, and VietMed-NER for\nmedical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER\ndataset, featuring 18 entity types collected from real-world medical speech\ntranscripts and annotated with the BIO tagging scheme. Its specialized\nvocabulary and domain-specific expressions make it a challenging benchmark for\ntoken-level classification models. Experimental results show that our method\nconsistently outperforms strong baselines, including transformer-only and\nhybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness\nof combining pretrained semantic features with graph-based relational modeling\nfor improved token classification across multiple domains.", "AI": {"tldr": "该论文提出了一种名为TextGraphFuseGAT的新型神经网络架构，它将预训练的Transformer编码器（PhoBERT）与图注意力网络（GAT）相结合，用于令牌级分类任务。该模型在多个越南语基准数据集上超越了强大的基线模型。", "motivation": "现有模型主要依赖于序列上下文来捕捉令牌间的依赖关系，但可能无法充分捕捉更丰富的、非序列的令牌间依赖。因此，研究旨在通过结合图注意力网络来增强上下文表示，从而提高令牌级分类任务的性能。", "method": "该研究提出了TextGraphFuseGAT模型。首先，使用PhoBERT生成令牌嵌入，然后基于这些嵌入构建一个全连接图。GAT层在此图上运行，以捕捉令牌间的丰富依赖。接着，在图增强的嵌入之上应用一个Transformer风格的自注意力层，以进一步增强上下文表示。最后，将最终的令牌表示通过一个分类头进行序列标注。该方法在三个越南语数据集上进行评估，包括新提出的VietMed-NER数据集。", "result": "实验结果表明，TextGraphFuseGAT模型在PhoNER-COVID19、PhoDisfluency和VietMed-NER这三个越南语基准数据集上，始终优于包括纯Transformer模型和混合神经网络模型（如BiLSTM + CNN + CRF）在内的强大基线模型。这证实了将预训练的语义特征与基于图的关系建模相结合，能有效提高跨多个领域的令牌分类性能。", "conclusion": "该研究得出结论，结合预训练的语义特征和基于图的关系建模，能够有效提高令牌级分类任务的性能。TextGraphFuseGAT架构通过捕捉超越序列上下文的令牌间依赖，为改进序列标注任务提供了一种有效的方法。"}}
{"id": "2510.11026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11026", "abs": "https://arxiv.org/abs/2510.11026", "authors": ["Hongxiang Li", "Yaowei Li", "Bin Lin", "Yuwei Niu", "Yuhang Yang", "Xiaoshuang Huang", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Long Chen"], "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning", "comment": null, "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\n\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.", "AI": {"tldr": "本文介绍了GIR-Bench，一个用于评估统一多模态模型在理解与生成一致性、推理驱动的文本到图像生成以及多步编辑方面的综合基准。研究发现，尽管统一模型在推理驱动的视觉任务上表现更优，但在理解和生成之间仍存在显著差距。", "motivation": "当前的统一多模态模型缺乏一个严格的、以推理为中心的基准，来系统地评估其理解与生成之间的对齐程度，以及它们在复杂视觉任务中的泛化潜力。", "method": "本文引入了GIR-Bench，一个从三个互补角度评估统一模型的综合基准：1) 理解-生成一致性（GIR-Bench-UGC），评估模型是否能在理解和生成任务中一致地利用相同知识；2) 推理驱动的文本到图像生成（GIR-Bench-T2I），要求模型应用逻辑约束和隐性知识生成忠实视觉内容；3) 编辑中的多步推理（GIR-Bench-Edit）。每个子集都设计了特定的评估流程，以实现细粒度和可解释的评估，并减少传统MLLM-as-a-Judge范式的偏见。", "result": "广泛的消融实验表明：尽管统一模型在推理驱动的视觉任务上比仅生成系统更具能力，但它们在理解和生成之间仍然存在持续的差距。", "conclusion": "GIR-Bench提供了一个严格的基准来评估统一多模态模型。研究结果表明，尽管这些模型在推理能力上有所提升，但在理解和生成之间的一致性方面仍有待提高，这为未来的研究指明了方向。"}}
{"id": "2510.11027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11027", "abs": "https://arxiv.org/abs/2510.11027", "authors": ["Ganlin Yang", "Tianyi Zhang", "Haoran Hao", "Weiyun Wang", "Yibin Liu", "Dehui Wang", "Guanzhou Chen", "Zijian Cai", "Junting Chen", "Weijie Su", "Wengang Zhou", "Yu Qiao", "Jifeng Dai", "Jiangmiao Pang", "Gen Luo", "Wenhai Wang", "Yao Mu", "Zhi Hou"], "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning", "comment": null, "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.", "AI": {"tldr": "本研究引入了Vlaser，一个视觉-语言-动作模型，旨在弥合VLM推理与VLA策略学习之间的鸿沟，并在具身推理和机器人控制任务上取得了最先进的性能。", "motivation": "现有研究主要集中于开发具身推理能力或将VLM集成到VLA模型中进行端到端机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。", "method": "本研究提出了Vlaser，一个具有协同具身推理能力的视觉-语言-动作模型。该模型基于高质量的Vlaser-6M数据集构建。此外，系统地研究了不同VLM初始化如何影响监督VLA微调，以减轻互联网规模预训练数据与具身特定策略学习数据之间的领域漂移。", "result": "Vlaser在多种具身推理基准（包括空间推理、具身基础、具身问答和任务规划）上取得了最先进的性能。通过对VLM初始化的研究，提供了减轻领域漂移的新见解。基于这些见解，Vlaser在WidowX基准上取得了最先进的结果，并在Google Robot基准上取得了有竞争力的性能。", "conclusion": "Vlaser成功地弥合了具身推理与VLA策略学习之间的差距，在具身推理和机器人控制任务上表现出色，并为缓解互联网数据与具身数据之间的领域漂移提供了宝贵见解。"}}
{"id": "2510.11545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11545", "abs": "https://arxiv.org/abs/2510.11545", "authors": ["Jiayu Ding", "Lei Cui", "Li Dong", "Nanning Zheng", "Furu Wei"], "title": "Information-Preserving Reformulation of Reasoning Traces for Antidistillation", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.", "AI": {"tldr": "本文提出PART，一种信息保留的反蒸馏方法，通过重新组织LLM推理链，在不剥夺用户关键中间信息的前提下，有效阻止未经授权的模型蒸馏。", "motivation": "大型语言模型（LLMs）通过延长推理链显著提升复杂任务性能，但这些详细的推理过程极易被未经授权地蒸馏。现有的保护策略（如用简短摘要替代）剥夺了用户宝贵的中间信息。因此，需要一种既能保护推理链不被蒸馏，又能保留信息的方法来解决这一权衡。", "method": "本文提出了PART（信息保留的反蒸馏推理链重构），其灵感来源于人类理解推理过程与LLM利用推理过程进行监督微调之间的差异。PART包含两个简单但有效的步骤：移除自言自语行为和重新排序子结论。一个小型辅助模型负责执行这种重构，计算开销极小。", "result": "广泛的实验表明，PART在不同规模和类型的学生模型上，以及在各种推理基准测试中，都能持续有效地干扰蒸馏过程。例如，当使用重构后的推理链进行训练时，一个大型32B学生模型在AIME 2024上的性能从54.17下降到46.88，相当于13.5%的性能下降。", "conclusion": "PART通过信息保留的重构策略，成功地解决了LLM推理链在提供详细信息和防止未经授权蒸馏之间的矛盾，为专有模型提供了一种有效的保护机制，同时确保用户仍能从推理过程中学习和验证。"}}
{"id": "2510.11028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11028", "abs": "https://arxiv.org/abs/2510.11028", "authors": ["Yanning Hou", "Ke Xu", "Junfa Li", "Yanran Ruan", "Jianfeng Qiu"], "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts", "comment": "Accepted by PRCV", "summary": "Recently, the powerful generalization ability exhibited by foundation models\nhas brought forth new solutions for zero-shot anomaly segmentation tasks.\nHowever, guiding these foundation models correctly to address downstream tasks\nremains a challenge. This paper proposes a novel two-stage framework, for\nzero-shot anomaly segmentation tasks in industrial anomaly detection. This\nframework excellently leverages the powerful anomaly localization capability of\nCLIP and the boundary perception ability of SAM.(1) To mitigate SAM's\ninclination towards object segmentation, we propose the Co-Feature Point Prompt\nGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM to\ngenerate positive and negative point prompts, guiding SAM to focus on\nsegmenting anomalous regions rather than the entire object. (2) To further\noptimize SAM's segmentation results and mitigate rough boundaries and isolated\nnoise, we introduce the Cascaded Prompts for SAM (CPS) module. This module\nemploys hybrid prompts cascaded with a lightweight decoder of SAM, achieving\nprecise segmentation of anomalous regions. Across multiple datasets, consistent\nexperimental validation demonstrates that our approach achieves\nstate-of-the-art zero-shot anomaly segmentation results. Particularly\nnoteworthy is our performance on the Visa dataset, where we outperform the\nstate-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP\nmetrics, respectively.", "AI": {"tldr": "本文提出了一种新颖的两阶段框架，用于工业零样本异常分割任务，通过协同利用CLIP和SAM的能力，实现了精确的异常区域分割。", "motivation": "基础模型在零样本异常分割任务中展现出强大的泛化能力，但如何正确引导这些模型以解决下游任务仍然是一个挑战。", "method": "本文提出了一个两阶段框架：1. 协同特征点提示生成（PPG）模块：利用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域，而非整个物体，以减轻SAM对物体分割的倾向。2. SAM级联提示（CPS）模块：采用混合提示与SAM的轻量级解码器级联，进一步优化SAM的分割结果，解决边界粗糙和孤立噪声问题，实现精确的异常区域分割。", "result": "该方法在多个数据集上取得了最先进的零样本异常分割结果。特别是在Visa数据集上，其在$F_1$-max和AP指标上分别超越现有最佳方法10.3%和7.7%。", "conclusion": "所提出的两阶段框架，通过PPG和CPS模块，成功地利用了CLIP的异常定位能力和SAM的边界感知能力，实现了工业零样本异常分割的精确和最先进的性能。"}}
{"id": "2510.11563", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11563", "abs": "https://arxiv.org/abs/2510.11563", "authors": ["Shreya Havaldar", "Sunny Rai", "Young-Min Cho", "Lyle Ungar"], "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs", "comment": "To appear at the 4th HCI + NLP Workshop @ EMNLP", "summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting.", "AI": {"tldr": "本文提出了一个基于社会文化理论的框架和基准，用于评估大型语言模型（LLMs）在多文化对话中的文化适应能力，并发现当前顶级LLMs在这方面表现不佳。", "motivation": "现有的衡量LLMs文化适应性的基准与模型在与不同文化背景用户互动时面临的实际挑战不符。", "method": "引入了一个基于社会文化理论的框架，将语言风格定义为受情境、关系和文化背景影响的关键文化交流元素。在此框架基础上构建了一个由文化多元的评估者标注的基准数据集，并提出了跨文化自然语言处理评估的新标准：对话框架、风格敏感性和主观正确性。", "result": "通过对当前顶级LLMs在该基准上的评估，结果显示这些模型在对话环境中难以实现文化适应。", "conclusion": "当前LLMs在多文化对话中的文化适应能力不足，本文提出的框架和基准为更真实、更细致地评估和改进LLMs的文化适应性提供了新的方法和标准。"}}
{"id": "2510.11631", "categories": ["cs.CV", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.11631", "abs": "https://arxiv.org/abs/2510.11631", "authors": ["Tobias Preintner", "Weixuan Yuan", "Adrian König", "Thomas Bäck", "Elena Raponi", "Niki van Stein"], "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models", "comment": "Accepted to IEEE ICTAI 2025", "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.", "AI": {"tldr": "EvoCAD结合视觉语言模型和进化算法，通过符号表示生成CAD对象，在CADPrompt基准测试中表现优于现有方法，尤其在生成拓扑正确对象方面，并引入了基于欧拉特征的新度量。", "motivation": "结合大型语言模型（LLMs）的生成和上下文学习能力与进化算法的优势，是一个有前景的研究方向。特别是在计算机辅助设计（CAD）领域，需要有效的方法来生成复杂的CAD对象。", "method": "本文提出了EvoCAD方法，通过符号表示生成CAD对象。该方法首先采样多个CAD对象，然后使用视觉语言模型（VLM）和推理语言模型进行进化优化。评估使用了GPT-4V和GPT-4o，在CADPrompt基准数据集上与现有方法进行比较。此外，引入了两个基于欧拉特征的拓扑属性新度量，用于捕捉3D对象间的语义相似性。", "result": "EvoCAD在多个指标上优于现有方法，特别是在生成拓扑正确的对象方面表现突出。新引入的两个度量可以有效评估拓扑正确性，并补充了现有的空间度量。", "conclusion": "EvoCAD是一种结合VLM和进化优化来生成CAD对象的有效方法，在生成拓扑正确对象方面表现卓越，并通过创新的拓扑度量进一步提升了评估效率和准确性。"}}
{"id": "2510.11632", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2.9; I.2.10; I.4.8; I.4.10; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.11632", "abs": "https://arxiv.org/abs/2510.11632", "authors": ["Krittin Chaowakarn", "Paramin Sangwongngam", "Nang Htet Htet Aung", "Chalie Charoenlarpnopparut"], "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection", "comment": null, "summary": "Recent studies in 3D object detection for autonomous vehicles aim to enrich\nfeatures through the utilization of multi-modal setups or the extraction of\nlocal patterns within LiDAR point clouds. However, multi-modal methods face\nsignificant challenges in feature alignment, and gaining features locally can\nbe oversimplified for complex 3D object detection tasks. In this paper, we\npropose a novel model, NV3D, which utilizes local features acquired from voxel\nneighbors, as normal vectors computed per voxel basis using K-nearest neighbors\n(KNN) and principal component analysis (PCA). This informative feature enables\nNV3D to determine the relationship between the surface and pertinent target\nentities, including cars, pedestrians, or cyclists. During the normal vector\nextraction process, NV3D offers two distinct sampling strategies: normal vector\ndensity-based sampling and FOV-aware bin-based sampling, allowing elimination\nof up to 55% of data while maintaining performance. In addition, we applied\nelement-wise attention fusion, which accepts voxel features as the query and\nvalue and normal vector features as the key, similar to the attention\nmechanism. Our method is trained on the KITTI dataset and has demonstrated\nsuperior performance in car and cyclist detection owing to their spatial\nshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%\nmean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%\nand 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in\ncar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of\nvoxels being filtered out.", "AI": {"tldr": "本文提出了一种名为NV3D的新型3D目标检测模型，该模型利用基于KNN和PCA计算的体素法向量作为局部特征，并结合注意力融合机制，以提高自动驾驶车辆在LiDAR点云中的检测性能，同时引入两种采样策略以提高效率。", "motivation": "当前自动驾驶3D目标检测方法面临挑战：多模态方法存在特征对齐困难，而LiDAR点云中提取的局部特征可能过于简化，不足以处理复杂的3D目标检测任务。", "method": "NV3D模型通过K近邻（KNN）和主成分分析（PCA）为每个体素计算法向量，以获取丰富的局部特征。该模型还提供了两种采样策略：基于法向量密度的采样和FOV感知分箱采样，可有效减少高达55%的数据量。此外，NV3D应用了元素级注意力融合机制，将体素特征作为查询和值，法向量特征作为键。", "result": "NV3D在KITTI数据集上进行了训练和验证，在汽车和骑行者检测方面表现出卓越性能。在不使用采样的情况下，NV3D在验证集上的mAP分别达到86.60%和80.18%，比基线Voxel R-CNN分别高出2.61%和4.23%。即使在过滤掉约55%体素的情况下，NV3D在汽车检测方面的mAP仍达到85.54%，比基线高出1.56%。", "conclusion": "NV3D通过利用体素法向量这一信息丰富的特征，有效地确定了表面与目标实体之间的关系，显著提升了3D目标检测性能，特别适用于具有独特空间形状的物体（如汽车和骑行者）。其提出的采样策略还能在保持性能的同时大幅提高数据处理效率。"}}
{"id": "2510.11675", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11675", "abs": "https://arxiv.org/abs/2510.11675", "authors": ["Dipkamal Bhusal", "Michael Clifford", "Sara Rampazzi", "Nidhi Rastogi"], "title": "FACE: Faithful Automatic Concept Extraction", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Interpreting deep neural networks through concept-based explanations offers a\nbridge between low-level features and high-level human-understandable\nsemantics. However, existing automatic concept discovery methods often fail to\nalign these extracted concepts with the model's true decision-making process,\nthereby compromising explanation faithfulness. In this work, we propose FACE\n(Faithful Automatic Concept Extraction), a novel framework that augments\nNon-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence\nregularization term to ensure alignment between the model's original and\nconcept-based predictions. Unlike prior methods that operate solely on encoder\nactivations, FACE incorporates classifier supervision during concept learning,\nenforcing predictive consistency and enabling faithful explanations. We provide\ntheoretical guarantees showing that minimizing the KL divergence bounds the\ndeviation in predictive distributions, thereby promoting faithful local\nlinearity in the learned concept space. Systematic evaluations on ImageNet,\nCOCO, and CelebA datasets demonstrate that FACE outperforms existing methods\nacross faithfulness and sparsity metrics.", "AI": {"tldr": "本文提出FACE框架，通过结合非负矩阵分解（NMF）、KL散度正则化和分类器监督，实现深度神经网络中忠实且自动的概念提取，确保概念解释与模型预测一致。", "motivation": "现有自动概念发现方法未能使提取的概念与模型真实的决策过程对齐，导致解释的忠实性受损。", "method": "FACE（Faithful Automatic Concept Extraction）框架通过引入Kullback-Leibler（KL）散度正则化项来增强非负矩阵分解（NMF），以确保模型原始预测与基于概念的预测之间的一致性。与仅基于编码器激活的传统方法不同，FACE在概念学习过程中融入了分类器监督，强制实现预测一致性。理论上，最小化KL散度可以限制预测分布的偏差，从而促进学习概念空间中的忠实局部线性。", "result": "在ImageNet、COCO和CelebA数据集上的系统评估表明，FACE在忠实性和稀疏性指标上均优于现有方法。", "conclusion": "FACE通过引入KL散度正则化和分类器监督，成功地解决了现有概念发现方法忠实性不足的问题，提供了一种能够与模型预测过程高度对齐的忠实概念解释方法。"}}
{"id": "2510.11557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11557", "abs": "https://arxiv.org/abs/2510.11557", "authors": ["Saurabh Khanna", "Xinxu Li"], "title": "Invisible Languages of the LLM Universe", "comment": null, "summary": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits.", "AI": {"tldr": "本文揭示了大型语言模型（LLMs）中存在的语言危机，即数千种拥有数百万使用者的语言在数字生态系统中几乎隐形。研究认为这种不平等是结构性的，源于殖民时代的语言等级制度，并将其定义为数字认知不公，呼吁去殖民化语言技术。", "motivation": "尽管大型语言模型在海量多语言语料库上训练，但全球仍有约2000种拥有数百万使用者的语言在数字世界中几乎隐形。研究旨在解释为何人工智能系统中的语言不平等并非偶然，而是结构性的问题。", "method": "研究提出了一个批判性框架，将语言活力（现实世界人口实力）和数字性（在线存在）的实证测量与后殖民理论和认知不公联系起来。通过分析所有有记录的人类语言数据，识别并分类了不同的语言群体，并论证了这些模式如何反映从殖民时代语言等级制度到当代AI发展的延续性。", "result": "研究将所有语言分为四类：堡垒语言（33%，高活力和高数字性）、数字回声语言（6%，活力下降但数字性高）、消逝之声语言（36%，活力和数字性均低），以及关键的隐形巨头语言（27%，活力高但数字性几乎为零）。这些模式反映了殖民时代语言等级制度在当代AI发展中的延续，构成了“数字认知不公”。结果表明，英语在AI中的主导地位并非技术必需，而是权力结构的产物，系统性地排斥了边缘化的语言知识。", "conclusion": "人工智能中的语言不平等是结构性的，反映了权力结构如何系统性地排除边缘化语言知识。研究呼吁去殖民化语言技术，并实现人工智能惠益的民主化，以解决这种数字认知不公。"}}
{"id": "2510.11047", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11047", "abs": "https://arxiv.org/abs/2510.11047", "authors": ["Nivea Roy", "Son Tran", "Atul Sajjanhar", "K. Devaraja", "Prakashini Koteshwara", "Yong Xiang", "Divya Rao"], "title": "Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset", "comment": null, "summary": "Laryngeal cancer imaging research lacks standardised datasets to enable\nreproducible deep learning (DL) model development. We present LaryngealCT, a\ncurated benchmark of 1,029 computed tomography (CT) scans aggregated from six\ncollections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic\nvolumes of interest encompassing the larynx were extracted using a weakly\nsupervised parameter search framework validated by clinical experts. 3D DL\narchitectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i)\nearly (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification\ntasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892,\nF1-macro-0.646) respectively outperformed the other models in the two tasks.\nModel explainability assessed using 3D GradCAMs with thyroid cartilage overlays\nrevealed greater peri-cartilage attention in non-T4 cases and focal activations\nin T4 predictions. Through open-source data, pretrained models, and integrated\nexplainability tools, LaryngealCT offers a reproducible foundation for\nAI-driven research to support clinical decisions in laryngeal oncology.", "AI": {"tldr": "本文介绍了LaryngealCT，一个包含1029个喉部CT扫描的标准化数据集，用于喉癌深度学习模型开发。该研究基准测试了多种3D深度学习架构，并提供了可解释性工具，旨在为喉部肿瘤学的AI研究提供可复现的基础。", "motivation": "喉癌影像学研究缺乏标准化的数据集，导致深度学习模型开发的可复现性不足。", "method": "研究从The Cancer Imaging Archive (TCIA)的六个集合中整合了1029个CT扫描，构建了LaryngealCT数据集。使用弱监督参数搜索框架提取了均匀的1毫米各向同性喉部感兴趣区域（VOI），并经临床专家验证。多种3D深度学习架构（3D CNN, ResNet18,50,101, DenseNet121）在早期（Tis,T1,T2）与晚期（T3,T4）以及T4与非T4分类任务上进行了基准测试。模型可解释性通过3D GradCAMs与甲状软骨叠加进行评估。", "result": "LaryngealCT数据集包含1029个CT扫描。在早期与晚期分类任务中，3D CNN表现最佳（AUC-0.881, F1-macro-0.821）；在T4与非T4分类任务中，ResNet18表现最佳（AUC-0.892, F1-macro-0.646）。模型可解释性分析显示，非T4病例在软骨周围有更大的关注度，而T4预测则显示出局灶性激活。", "conclusion": "LaryngealCT通过提供开源数据、预训练模型和集成的可解释性工具，为AI驱动的喉部肿瘤学研究提供了可复现的基础，以支持临床决策。"}}
{"id": "2510.11584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11584", "abs": "https://arxiv.org/abs/2510.11584", "authors": ["Ting Li", "Yang Yang", "Yipeng Yu", "Liang Yao", "Guoqing Chao", "Ruifeng Xu"], "title": "LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings", "comment": "13 pages", "summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations.", "AI": {"tldr": "本文提出LLMAtKGE，一个基于LLM的知识图谱嵌入对抗攻击框架，该框架能选择攻击目标、生成人类可读的解释，并优于现有黑盒基线。", "motivation": "现有的黑盒知识图谱嵌入（KGE）对抗攻击方法虽然试图结合文本和结构信息，但无法生成人类可读的解释，且泛化能力差。鉴于大型语言模型（LLM）在文本理解、生成和推理方面的强大能力，作者旨在利用LLM来解决这些问题。", "method": "LLMAtKGE框架利用LLM选择攻击目标并生成解释。为克服LLM的输入限制，设计了结构化提示方案，将攻击表述为多项选择题并融入知识图谱事实证据。为解决上下文窗口限制和犹豫问题，引入了基于语义和基于中心性的过滤器，在压缩候选集的同时保留了攻击相关信息的高召回率。此外，通过预计算高阶邻接和使用三元组分类任务微调LLM，以有效整合语义和结构信息，增强过滤性能。", "result": "在两个广泛使用的知识图谱数据集上的实验表明，LLMAtKGE攻击性能优于最强的黑盒基线，并通过推理提供了解释，并且与白盒方法相比也具有竞争力。全面的消融研究和案例分析进一步验证了其生成解释的能力。", "conclusion": "LLMAtKGE是一个新颖的基于LLM的知识图谱嵌入对抗攻击框架，它不仅能有效地进行攻击，还能生成人类可读的解释，并通过精心设计的机制克服了LLM在处理知识图谱攻击任务时的局限性，实现了语义和结构信息的有效整合。"}}
{"id": "2510.11050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11050", "abs": "https://arxiv.org/abs/2510.11050", "authors": ["Yang Hou", "Minggu Wang", "Jianjun Zhao"], "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion", "comment": "Accepted by ICME2025", "summary": "Recent advancements in text-guided diffusion models have shown promise for\ngeneral image editing via inversion techniques, but often struggle to maintain\nID and structural consistency in real face editing tasks. To address this\nlimitation, we propose a zero-shot face editing method based on ID-Attribute\nDecoupled Inversion. Specifically, we decompose the face representation into ID\nand attribute features, using them as joint conditions to guide both the\ninversion and the reverse diffusion processes. This allows independent control\nover ID and attributes, ensuring strong ID preservation and structural\nconsistency while enabling precise facial attribute manipulation. Our method\nsupports a wide range of complex multi-attribute face editing tasks using only\ntext prompts, without requiring region-specific input, and operates at a speed\ncomparable to DDIM inversion. Comprehensive experiments demonstrate its\npracticality and effectiveness.", "AI": {"tldr": "提出了一种基于ID-属性解耦反演的零样本人脸编辑方法，通过独立控制身份和属性特征，解决了现有文本引导扩散模型在人脸编辑中ID和结构一致性差的问题。", "motivation": "目前的文本引导扩散模型在通用图像编辑方面表现出色，但应用于真实人脸编辑时，往往难以保持身份（ID）和结构的一致性。", "method": "该方法提出ID-属性解耦反演（ID-Attribute Decoupled Inversion）。它将人脸表示分解为ID特征和属性特征，并将其作为联合条件来引导反演和逆向扩散过程。这种解耦允许独立控制ID和属性，从而实现强大的ID保留和结构一致性。", "result": "该方法能够确保强大的ID保留和结构一致性，同时实现精确的面部属性操作。它支持仅通过文本提示进行广泛的复杂多属性人脸编辑，无需特定区域输入，并且运行速度与DDIM反演相当。综合实验证明了其实用性和有效性。", "conclusion": "该方法提供了一种实用且有效的零样本人脸编辑解决方案，显著提高了文本引导扩散模型在真实人脸编辑中ID和结构一致性，同时保持了精确的属性控制能力。"}}
{"id": "2510.11586", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.11586", "abs": "https://arxiv.org/abs/2510.11586", "authors": ["Georg Ahnert", "Anna-Carolina Haensch", "Barbara Plank", "Markus Strohmaier"], "title": "Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models", "comment": null, "summary": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods.", "AI": {"tldr": "本研究系统性地调查了不同调查响应生成方法对LLM模拟调查结果的影响，发现方法选择会显著影响预测结果，并提出实用建议。", "motivation": "LLM通常擅长生成开放式文本，但在模拟人类调查响应时，许多研究关注生成封闭式响应。目前，使用LLM生成封闭式调查响应的方法多样，缺乏标准化实践，这促使研究者系统性地探究不同生成方法的影响。", "method": "研究系统性地调查了8种调查响应生成方法，结合4个政治态度调查和10个开源语言模型，共生成了3200万个模拟调查响应。通过比较不同方法在个体层面和子群体层面的对齐程度来评估其影响。", "result": "研究发现，不同的调查响应生成方法在个体层面和子群体层面的对齐方面存在显著差异。其中，“受限生成方法”总体表现最佳，而“推理输出”并不能持续改善对齐效果。", "conclusion": "本研究强调了调查响应生成方法对模拟调查结果的显著影响。基于研究发现，论文提出了关于应用调查响应生成方法的实用建议。"}}
{"id": "2510.11063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11063", "abs": "https://arxiv.org/abs/2510.11063", "authors": ["Chang Liu", "Henghui Ding", "Kaining Ying", "Lingyi Hong", "Ning Xu", "Linjie Yang", "Yuchen Fan", "Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han", "Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Chang Soo Lim", "Joonyoung Moon", "Donghyeon Cho", "Tingmin Li", "Yixuan Li", "Yang Yang", "An Yan", "Leilei Cao", "Feng Lu", "Ran Hong", "Youhai Jiang", "Fengjie Zhu", "Yujie Xie", "Hongyang Zhang", "Zhihui Liu", "Shihai Ruan", "Quanzhu Niu", "Dengxian Gong", "Shihao Chen", "Tao Zhang", "Yikang Zhou", "Haobo Yuan", "Lu Qi", "Xiangtai Li", "Shunping Ji", "Ran Hong", "Feng Lu", "Leilei Cao", "An Yan", "Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation", "comment": "16 pages, 9 figures", "summary": "This report presents an overview of the 7th Large-scale Video Object\nSegmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the\ntwo traditional tracks of LSVOS that jointly target robustness in realistic\nvideo scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition\nfeatures a newly introduced track, Complex VOS (MOSEv2). Building upon prior\ninsights, MOSEv2 substantially increases difficulty, introducing more\nchallenging but realistic scenarios including denser small objects, frequent\ndisappear/reappear events, severe occlusions, adverse weather and lighting,\netc., pushing long-term consistency and generalization beyond curated\nbenchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for\nVOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric\nto better evaluate objects across scales and disappearance cases. We summarize\ndatasets and protocols, highlight top-performing solutions, and distill\nemerging trends, such as the growing role of LLM/MLLM components and\nmemory-aware propagation, aiming to chart future directions for resilient,\nlanguage-aware video segmentation in the wild.", "AI": {"tldr": "本报告概述了ICCV 2025的第七届大规模视频目标分割(LSVOS)挑战赛，新增了难度更大的MOSEv2赛道，并总结了新兴趋势。", "motivation": "旨在推动视频目标分割(VOS)在真实世界场景中的鲁棒性，解决小目标密集、频繁消失/重现、严重遮挡、恶劣天气/光照等挑战，并超越现有基准，提升长期一致性和泛化能力。", "method": "挑战赛包括经典VOS、参照VOS(RVOS)和新引入的复杂VOS(MOSEv2)三个赛道。MOSEv2赛道显著增加了难度，引入更具挑战性的真实场景。VOS和RVOS采用标准J、F和J&F指标，MOSEv2则采用J&F作为主要排名指标。报告总结了数据集、协议，并分析了顶尖解决方案。", "result": "报告总结了表现最佳的解决方案，并提炼出新兴趋势，例如LLM/MLLM组件和记忆感知传播在视频分割中的日益重要性。", "conclusion": "本挑战赛旨在为未来在真实环境中实现更具韧性、语言感知的视频分割指明方向。"}}
{"id": "2510.11693", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11693", "abs": "https://arxiv.org/abs/2510.11693", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Weiwen Xu", "Mahani Aljunied", "Yu Rong"], "title": "Scaling Language-Centric Omnimodal Representation Learning", "comment": "NeurIPS 2025", "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.", "AI": {"tldr": "本研究认为，MLLM多模态嵌入的优越性源于生成式预训练中实现的隐式跨模态对齐，对比学习（CL）在此基础上进行轻量级优化。提出LCO-Emb框架并发现生成-表示缩放定律（GRSL），表明生成能力越强，表示能力越好。", "motivation": "尽管基于多模态大语言模型（MLLM）并结合对比学习（CL）的多模态嵌入方法取得了显著成果，但其优越性的深层原因尚未被充分探索。", "method": "本研究通过分析各向异性（anisotropy）和核相似度结构，实证确认MLLM表示中存在潜在的跨模态对齐。在此洞察基础上，提出了以语言为中心的通用多模态嵌入框架LCO-Emb。同时，识别并提供了生成-表示缩放定律（GRSL）的理论解释，并通过低资源视觉-文档检索任务进行验证。", "result": "MLLM表示中确实存在潜在的对齐，使得对比学习成为有效的精炼阶段。LCO-Emb框架在多个基准测试中实现了最先进的性能。研究发现生成-表示缩放定律（GRSL），表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关。持续的生成式预训练可以在对比学习之前进一步增强模型的嵌入潜力。", "conclusion": "MLLM在生成式预训练中实现了隐式的跨模态对齐，为后续的对比学习提供了基础。提高生成能力是增强表示质量的有效范式，这由生成-表示缩放定律（GRSL）理论上得到了支持和验证。"}}
{"id": "2510.11073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11073", "abs": "https://arxiv.org/abs/2510.11073", "authors": ["Yuan Tian", "Min Zhou", "Yitong Chen", "Fang Li", "Lingzi Qi", "Shuo Wang", "Xieyang Xu", "Yu Yu", "Shiqiong Xu", "Chaoyu Lei", "Yankai Jiang", "Rongzhao Zhang", "Jia Tan", "Li Wu", "Hong Chen", "Xiaowei Liu", "Wei Lu", "Lin Li", "Huifang Zhou", "Xuefei Song", "Guangtao Zhai", "Xianqun Fan"], "title": "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer", "comment": "Accepted to Nature NPJ Digital Medicine", "summary": "Patient face images provide a convenient mean for evaluating eye diseases,\nwhile also raising privacy concerns. Here, we introduce ROFI, a deep\nlearning-based privacy protection framework for ophthalmology. Using weakly\nsupervised learning and neural identity translation, ROFI anonymizes facial\nfeatures while retaining disease features (over 98\\% accuracy, $\\kappa >\n0.90$). It achieves 100\\% diagnostic sensitivity and high agreement ($\\kappa >\n0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\\% of\nimages. ROFI works with AI systems, maintaining original diagnoses ($\\kappa >\n0.80$), and supports secure image reversal (over 98\\% similarity), enabling\naudits and long-term care. These results show ROFI's effectiveness of\nprotecting patient privacy in the digital medicine era.", "AI": {"tldr": "ROFI是一个基于深度学习的眼科隐私保护框架，它在匿名化患者面部图像的同时，能高精度保留疾病特征并支持诊断和安全图像逆转。", "motivation": "患者面部图像为眼科疾病评估提供了便利，但同时也引发了隐私担忧。", "method": "ROFI采用深度学习、弱监督学习和神经身份转换技术，旨在匿名化面部特征，同时保留疾病特征。", "result": "ROFI在匿名化超过95%图像的同时，对疾病特征保留准确率超过98%（κ > 0.90），对11种眼病实现100%诊断敏感性和高一致性（κ > 0.90）。它与AI系统兼容，保持原始诊断（κ > 0.80），并支持安全图像逆转（相似度超过98%）。", "conclusion": "这些结果表明ROFI在数字医学时代有效保护患者隐私方面的有效性。"}}
{"id": "2510.11598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11598", "abs": "https://arxiv.org/abs/2510.11598", "authors": ["Bo Cheng", "Xu Wang", "Jinda Liu", "Yi Chang", "Yuan Wu"], "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data.", "AI": {"tldr": "MeTA-LoRA 是一种两阶段优化框架，显著提高了大型语言模型在多任务场景下进行低秩适应（LoRA）的数据效率，通过促进任务间知识迁移，仅用少量数据即可达到或超越传统LoRA的性能。", "motivation": "尽管LoRA在单任务微调中表现出色，但在复杂的多任务学习场景中，它难以有效利用任务间的知识，通常需要大量的任务特定数据才能达到最佳性能。", "method": "MeTA-LoRA 采用两阶段优化。第一阶段，使用每个数据集的少量样本学习任务特定的LoRA适配器。第二阶段，通过聚合来自多个任务的梯度来更新共享的LoRA适配器，以促进任务间知识转移。", "result": "在多任务学习和多语言学习场景中，MeTA-LoRA 在使用显著更少的任务特定数据的情况下，其性能与传统的全数据LoRA微调方法相当或超越。", "conclusion": "MeTA-LoRA 有效解决了LoRA在多任务适应中数据效率低下的问题，通过两阶段优化框架促进了知识迁移，从而显著减少了所需数据量并保持了高性能。"}}
{"id": "2510.11718", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11718", "abs": "https://arxiv.org/abs/2510.11718", "authors": ["Chengqi Duan", "Kaiyue Sun", "Rongyao Fang", "Manyuan Zhang", "Yan Feng", "Ying Luo", "Yufang Liu", "Ke Wang", "Peng Pei", "Xunliang Cai", "Hongsheng Li", "Yi Ma", "Xihui Liu"], "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.", "AI": {"tldr": "CodePlot-CoT提出了一种代码驱动的思维链范式，通过VLM生成文本推理和可执行绘图代码，并将代码渲染成图像作为“视觉思维”，以解决需要视觉辅助的数学问题。该方法构建了首个大规模双语数据集Math-VR，并在新基准上实现了显著的性能提升。", "motivation": "尽管大型语言模型（LLMs）和视觉语言模型（VLMs）在数学推理方面取得了进展，但在需要视觉辅助（如绘制辅助线或函数图）的数学问题上仍面临瓶颈。大多数LLMs和VLMs仅限于文本推理链，而能生成交错文本和图像的多模态统一模型则缺乏必要的精度和可控性。", "method": "本研究提出了CodePlot-CoT，一个用于数学中“通过图像思考”的代码驱动思维链范式。它利用VLM生成文本推理和可执行的绘图代码，然后将代码渲染成图像作为“视觉思维”。为此，研究团队首先构建了Math-VR，一个包含17.8万样本的大规模双语数学视觉推理数据集和基准。其次，开发了一个专门用于将复杂数学图形解析为代码的先进图像到代码转换器，以创建高质量的训练数据。最后，利用这些训练数据训练了CodePlot-CoT模型来解决数学问题。", "result": "实验结果表明，CodePlot-CoT模型在新的基准测试上比基础模型提高了21%，充分验证了所提出的代码驱动推理范式的有效性。", "conclusion": "本研究的工作为多模态数学推理开辟了新方向，并为社区提供了首个大规模数据集、全面的基准和解决此类问题的强大方法。所有数据集、代码和预训练模型均已公开。"}}
{"id": "2510.11092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11092", "abs": "https://arxiv.org/abs/2510.11092", "authors": ["Bozhou Zhang", "Nan Song", "Jingyu Li", "Xiatian Zhu", "Jiankang Deng", "Li Zhang"], "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution", "comment": "NeurIPS 2025", "summary": "End-to-end autonomous driving methods aim to directly map raw sensor inputs\nto future driving actions such as planned trajectories, bypassing traditional\nmodular pipelines. While these approaches have shown promise, they often\noperate under a one-shot paradigm that relies heavily on the current scene\ncontext, potentially underestimating the importance of scene dynamics and their\ntemporal evolution. This limitation restricts the model's ability to make\ninformed and adaptive decisions in complex driving scenarios. We propose a new\nperspective: the future trajectory of an autonomous vehicle is closely\nintertwined with the evolving dynamics of its environment, and conversely, the\nvehicle's own future states can influence how the surrounding scene unfolds.\nMotivated by this bidirectional relationship, we introduce SeerDrive, a novel\nend-to-end framework that jointly models future scene evolution and trajectory\nplanning in a closed-loop manner. Our method first predicts future bird's-eye\nview (BEV) representations to anticipate the dynamics of the surrounding scene,\nthen leverages this foresight to generate future-context-aware trajectories.\nTwo key components enable this: (1) future-aware planning, which injects\npredicted BEV features into the trajectory planner, and (2) iterative scene\nmodeling and vehicle planning, which refines both future scene prediction and\ntrajectory generation through collaborative optimization. Extensive experiments\non the NAVSIM and nuScenes benchmarks show that SeerDrive significantly\noutperforms existing state-of-the-art methods.", "AI": {"tldr": "SeerDrive是一种新颖的端到端自动驾驶框架，它通过闭环方式联合建模未来场景演变和轨迹规划，显著优于现有最先进方法。", "motivation": "现有的端到端自动驾驶方法过度依赖当前场景上下文，低估了场景动态及其时间演变的重要性，限制了其在复杂场景中做出适应性决策的能力。研究者认为车辆未来轨迹与环境动态演变紧密相关，且车辆自身未来状态也能影响场景发展，受此双向关系启发。", "method": "本文提出了SeerDrive框架，它以闭环方式联合建模未来场景演变和轨迹规划。该方法首先预测未来的鸟瞰图（BEV）表示以预见周围场景的动态，然后利用这种预见生成未来上下文感知的轨迹。关键组件包括：1) 未来感知规划，将预测的BEV特征注入轨迹规划器；2) 迭代场景建模和车辆规划，通过协同优化来改进未来场景预测和轨迹生成。", "result": "在NAVSIM和nuScenes基准测试上进行的广泛实验表明，SeerDrive显著优于现有的最先进方法。", "conclusion": "通过联合建模未来场景演变和轨迹规划，SeerDrive克服了传统端到端自动驾驶方法的局限性，实现了更具信息性和适应性的决策，并在关键基准上展现出卓越的性能。"}}
{"id": "2510.11618", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11618", "abs": "https://arxiv.org/abs/2510.11618", "authors": ["Zehao Chen", "Rong Pan", "Haoran Li"], "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models", "comment": "Project: https://storyboxproject.github.io", "summary": "Human writers often begin their stories with an overarching mental scene,\nwhere they envision the interactions between characters and their environment.\nInspired by this creative process, we propose a novel approach to long-form\nstory generation, termed hybrid bottom-up long-form story generation, using\nmulti-agent simulations. In our method, agents interact within a dynamic\nsandbox environment, where their behaviors and interactions with one another\nand the environment generate emergent events. These events form the foundation\nfor the story, enabling organic character development and plot progression.\nUnlike traditional top-down approaches that impose rigid structures, our hybrid\nbottom-up approach allows for the natural unfolding of events, fostering more\nspontaneous and engaging storytelling. The system is capable of generating\nstories exceeding 10,000 words while maintaining coherence and consistency,\naddressing some of the key challenges faced by current story generation models.\nWe achieve state-of-the-art performance across several metrics. This approach\noffers a scalable and innovative solution for creating dynamic, immersive\nlong-form stories that evolve organically from agent-driven interactions.", "AI": {"tldr": "本文提出一种混合式自下而上的长篇故事生成方法，通过多智能体模拟在动态沙盒环境中互动，生成自发事件，从而构建连贯一致的万字长篇故事，并取得最先进的性能。", "motivation": "受人类作家构思场景和角色互动的启发，旨在解决当前故事生成模型在长篇故事连贯性、一致性及自然展开方面的挑战。", "method": "采用多智能体模拟方法，智能体在动态沙盒环境中互动，其行为和交互会产生突发事件。这些事件构成了故事的基础，促成了有机的人物发展和情节推进。这是一种混合式自下而上的方法，与传统的自上而下方法不同。", "result": "该系统能够生成超过10,000字的故事，同时保持连贯性和一致性，解决了现有故事生成模型面临的一些关键挑战。在多项指标上取得了最先进的性能。", "conclusion": "该方法为创建动态、沉浸式、由智能体互动有机演变的长篇故事提供了一种可扩展且创新的解决方案。"}}
{"id": "2510.11602", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11602", "abs": "https://arxiv.org/abs/2510.11602", "authors": ["Huiyin Xue", "Nafise Sadat Moosavi", "Nikolaos Aletras"], "title": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling", "comment": null, "summary": "The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.", "AI": {"tldr": "本研究系统解构了Transformer注意力机制的设计原则，发现令牌混合是不可或缺的，而其数学形式和序列依赖性可以被大大放宽，尤其是在混合架构中。同时，即使是单独表现不佳的变体，与标准注意力结合也能达到良好性能。", "motivation": "Transformer语言模型的成功广泛归因于其点积注意力机制，该机制融合了多个设计原则（如跨位置信息混合、序列依赖激活、特定数学形式、查询与键耦合）。然而，这些原则中每一个的必要性在很大程度上尚未经过检验。", "method": "通过设计受控变体，选择性地放宽注意力机制的各项设计原则，并将其应用于所有层或仅部分层保留标准注意力的混合架构中，进行系统性的实证分析。", "result": "令牌混合机制是不可或缺的，其缺失会导致模型性能接近随机；而注意力机制的精确数学形式和序列依赖性可以被大幅放宽，尤其是在仅部分层保留这些特性时。令人惊讶的是，即使是单独失败的变体，在与标准注意力交错使用时也能实现稳健的性能，这突显了一种协同效应。", "conclusion": "这些发现加深了对注意力有效性真正基础的理解，并为在不牺牲性能的情况下简化语言模型开辟了新途径。"}}
{"id": "2510.11096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11096", "abs": "https://arxiv.org/abs/2510.11096", "authors": ["Fengling Zhu", "Boshi Liu", "Jingyu Hua", "Sheng Zhong"], "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications.", "AI": {"tldr": "多模态大语言模型（MLLMs）易受对抗性攻击，现有防御方法存在局限。本文提出一种基于监督扩散的去噪框架，结合提示优化，有效提升了视觉模态的防御鲁棒性和对未知攻击的泛化能力。", "motivation": "多模态大语言模型（MLLMs）在整合视觉和文本模态的同时，也容易受到对抗性攻击，攻击者可以通过扰动单一或两种模态来诱导有害或误导性输出。现有防御策略如对抗训练计算成本高且对已知攻击鲁棒性有限；传统净化方法则常导致图像质量下降且泛化能力不足，尤其是在复杂多模态任务中。因此，需要更有效、高质量的防御机制。", "method": "本文专注于防御视觉模态，提出了一种基于监督扩散的去噪框架。该方法利用成对的对抗性-干净图像数据集，通过方向性、任务特定的引导来微调扩散模型。与无监督净化方法不同，它旨在实现更高质量的重建。此外，还引入了提示优化作为补充防御机制，以增强对多样化和未知攻击策略的抵抗力。", "result": "在图像字幕和视觉问答任务上的大量实验表明，所提出的方法不仅显著提高了鲁棒性，而且对未知对抗性攻击表现出强大的可迁移性。与先前的无监督净化方法（如DiffPure）相比，该方法实现了更高质量的重建。", "conclusion": "基于监督扩散的去噪框架对于多模态防御是有效的，为多模态大语言模型在实际应用中更可靠和安全的部署铺平了道路。"}}
{"id": "2510.11106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11106", "abs": "https://arxiv.org/abs/2510.11106", "authors": ["Ans Munir", "Faisal Z. Qureshi", "Mohsen Ali", "Muhammad Haris Khan"], "title": "Compositional Zero-Shot Learning: A Survey", "comment": "Survey paper with 36 pages, 8 plots and 4 figures", "summary": "Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision\nthat enables models to recognize unseen combinations of known attributes and\nobjects during inference, addressing the combinatorial challenge of requiring\ntraining data for every possible composition. This is particularly challenging\nbecause the visual appearance of primitives is highly contextual; for example,\n``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars\ndiffer significantly from ``wet'' cats. Effectively modeling this contextuality\nand the inherent compositionality is crucial for robust compositional zero-shot\nrecognition. This paper presents, to our knowledge, the first comprehensive\nsurvey specifically focused on Compositional Zero-Shot Learning. We\nsystematically review the state-of-the-art CZSL methods, introducing a taxonomy\ngrounded in disentanglement, with four families of approaches: no explicit\ndisentanglement, textual disentanglement, visual disentanglement, and\ncross-modal disentanglement. We provide a detailed comparative analysis of\nthese methods, highlighting their core advantages and limitations in different\nproblem settings, such as closed-world and open-world CZSL. Finally, we\nidentify the most significant open challenges and outline promising future\nresearch directions. This survey aims to serve as a foundational resource to\nguide and inspire further advancements in this fascinating and important field.\nPapers studied in this survey with their official code are available on our\ngithub: https://github.com/ans92/Compositional-Zero-Shot-Learning", "AI": {"tldr": "这篇综述是首个全面关注组合式零样本学习（CZSL）的论文，系统回顾了现有方法，提出了基于解耦的分类法，并分析了其优缺点及未来研究方向。", "motivation": "组合式零样本学习（CZSL）在计算机视觉中至关重要，它使模型能够识别未见过的属性与物体组合，解决了需要为每种组合提供训练数据的组合爆炸问题。其主要挑战在于基元（如属性和物体）的视觉表现具有高度语境性，例如“小”猫与“老”猫、或“湿”车与“湿”猫的视觉差异巨大，因此有效建模这种语境性和固有的组合性对于鲁棒的零样本识别至关重要。", "method": "本文系统地回顾了最新的CZSL方法，并引入了一种基于解耦的分类法，将其分为四大家族：无显式解耦、文本解耦、视觉解耦和跨模态解耦。论文还对这些方法进行了详细的比较分析，突出它们在不同问题设置（如封闭世界和开放世界CZSL）中的核心优势和局限性。", "result": "该综述提出了一个基于解耦的CZSL方法分类法，包括无显式解耦、文本解耦、视觉解耦和跨模态解耦四大类。它详细比较了这些方法的优缺点，并识别出该领域最重大的开放挑战。", "conclusion": "该综述旨在作为CZSL领域的基础资源，以指导和启发未来的研究进展，推动这一重要领域的发展。"}}
{"id": "2510.11652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11652", "abs": "https://arxiv.org/abs/2510.11652", "authors": ["Xin Gui", "King Zhu", "JinCheng Ren", "Qianben Chen", "Zekun Moore Wang", "Yizhi LI", "Xinpeng Liu", "Xiaowan Li", "Wenli Ren", "Linyu Miao", "Tianrui Qin", "Ziqi Shu", "He Zhu", "Xiangru Tang", "Dingfeng Shi", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Minghao Liu", "Ge Zhang", "Wangchunshu Zhou"], "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems", "comment": null, "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.", "AI": {"tldr": "本文提出了Acadreason基准测试，旨在评估大型语言模型（LLMs）和智能体在学术知识获取和高水平推理方面的能力，并揭示了现有模型在此类任务上的显著不足。", "motivation": "现有评估主要集中在数学/代码竞赛或通用任务，而多领域学术基准缺乏足够的推理深度，导致该领域缺乏一个针对高水平推理的严格基准。为了填补这一空白，作者创建了Acadreason。", "method": "引入了Acadreason基准测试，包含50个由专家标注的学术问题，涵盖计算机科学、经济学、法律、数学和哲学五个高推理领域。所有问题均来源于近年顶级出版物，并经过严格的标注和质量控制。研究对10多个主流LLM和智能体进行了系统评估。", "result": "评估结果显示，大多数LLM得分低于20分，即使是尖端的GPT-5也仅得16分。智能体得分较高，但均未超过40分。", "conclusion": "结果表明，LLM和智能体在超智能学术研究任务中存在显著的能力差距，并凸显了Acadreason基准测试的挑战性，以及当前模型在复杂学术推理方面的局限性。"}}
{"id": "2510.11107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11107", "abs": "https://arxiv.org/abs/2510.11107", "authors": ["Jiahui Lei", "Kyle Genova", "George Kopanas", "Noah Snavely", "Leonidas Guibas"], "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps", "comment": "Accepted at ICCV 2025, project page:\n  https://jiahuilei.com/projects/momap/", "summary": "This paper addresses the challenge of learning semantically and functionally\nmeaningful 3D motion priors from real-world videos, in order to enable\nprediction of future 3D scene motion from a single input image. We propose a\nnovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,\nwhich can be generated from existing generative image models to facilitate\nefficient and effective motion prediction. To learn meaningful distributions\nover motion, we create a large-scale database of MoMaps from over 50,000 real\nvideos and train a diffusion model on these representations. Our motion\ngeneration not only synthesizes trajectories in 3D but also suggests a new\npipeline for 2D video synthesis: first generate a MoMap, then warp an image\naccordingly and complete the warped point-based renderings. Experimental\nresults demonstrate that our approach generates plausible and semantically\nconsistent 3D scene motion.", "AI": {"tldr": "本文提出了一种像素对齐的运动图（MoMap）表示，并从大量真实视频中构建数据库，训练扩散模型来学习3D场景运动先验，从而实现从单张图像预测未来3D运动。", "motivation": "从真实视频中学习具有语义和功能意义的3D运动先验，以实现从单张输入图像预测未来3D场景运动是一个挑战。", "method": "1. 提出了一种新颖的像素对齐运动图（MoMap）表示，可由现有生成图像模型生成。2. 从超过50,000个真实视频中创建了一个大规模MoMap数据库。3. 在这些MoMap表示上训练了一个扩散模型。4. 提出了一种新的2D视频合成流程：先生成MoMap，然后相应地扭曲图像并完成基于点的渲染。", "result": "实验结果表明，该方法能够生成合理且语义一致的3D场景运动。", "conclusion": "该研究成功地学习了3D运动先验，实现了从单张图像预测3D运动，并为2D视频合成提供了一种新颖的流程。"}}
{"id": "2510.11112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11112", "abs": "https://arxiv.org/abs/2510.11112", "authors": ["Chen Liu", "Wenfang Yao", "Kejing Yin", "William K. Cheung", "Jing Qin"], "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Longitudinal multimodal data, including electronic health records (EHR) and\nsequential chest X-rays (CXRs), is critical for modeling disease progression,\nyet remains underutilized due to two key challenges: (1) redundancy in\nconsecutive CXR sequences, where static anatomical regions dominate over\nclinically-meaningful dynamics, and (2) temporal misalignment between sparse,\nirregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a\nnovel framework that addresses these challenges through region-aware\ndisentanglement and multi-timescale alignment. First, we disentangle static\n(anatomy) and dynamic (pathology progression) features in sequential CXRs,\nprioritizing disease-relevant changes. Second, we hierarchically align these\nstatic and dynamic CXR features with asynchronous EHR data via local (pairwise\ninterval-level) and global (full-sequence) synchronization to model coherent\nprogression pathways. Extensive experiments on the MIMIC dataset demonstrate\nthat $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and\nachieve state-of-the-art performance on both disease progression identification\nand general ICU prediction tasks.", "AI": {"tldr": "本文提出了一个名为 $\texttt{DiPro}$ 的新框架，通过区域感知解耦和多时间尺度对齐，解决了纵向多模态数据（如电子健康记录和连续胸部X光片）在疾病进展建模中面临的冗余和时间错位问题，并在MIMIC数据集上实现了最先进的性能。", "motivation": "纵向多模态数据对于建模疾病进展至关重要，但由于两个关键挑战而未被充分利用：1) 连续胸部X光片序列中存在冗余，静态解剖区域掩盖了临床有意义的动态变化；2) 稀疏、不规则的影像数据与连续的电子健康记录数据之间存在时间错位。", "method": "本文引入了 $\texttt{DiPro}$ 框架来解决这些挑战。首先，它在连续胸部X光片中解耦静态（解剖）和动态（病理进展）特征，优先关注与疾病相关的变化。其次，它通过局部（成对间隔级别）和全局（全序列）同步，将这些静态和动态胸部X光片特征与异步电子健康记录数据进行分层对齐，以建模连贯的进展路径。", "result": "在MIMIC数据集上进行的广泛实验表明，$\texttt{DiPro}$ 能够有效地提取时间临床动态，并在疾病进展识别和通用ICU预测任务上均取得了最先进的性能。", "conclusion": "$\texttt{DiPro}$ 框架通过创新的区域感知解耦和多时间尺度对齐方法，成功克服了纵向多模态数据在疾病进展建模中的挑战，为临床动态提取和预测提供了有效且高性能的解决方案。"}}
{"id": "2510.11620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11620", "abs": "https://arxiv.org/abs/2510.11620", "authors": ["Siheng Xiong", "Ali Payani", "Faramarz Fekri"], "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation", "comment": null, "summary": "Inference-time scaling enhances the reasoning ability of a language model\n(LM) by extending its chain-of-thought (CoT). However, existing approaches\ntypically generate the entire reasoning chain in a single forward pass, which\noften leads to CoT derailment, i.e., the reasoning trajectory drifting off\ncourse due to compounding errors. This problem is particularly severe for\nsmaller LMs with long CoTs due to their limited capacity. To address this, we\nanalyze raw long CoTs and uncover a reasoning hierarchy consisting of planning\nand execution steps. Our analysis reveals that most reasoning errors stem from\nincorrect planning. Motivated by this observation, we propose Multi-Path Plan\nAggregation (MPPA), a framework that augments single-pass reasoning with plan\nexploration and aggregation. Following a variable interval schedule based on\nthe token position, MPPA generates multiple candidate plans and aggregates them\ninto a refined planning step. To maintain efficiency, we adopt a minimal design\nin which the base LM serves as the primary policy, while a lightweight LoRA\nmodule implements the plan aggregation policy. We further observe that\noutcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K\ntokens). To overcome this, we introduce online Step-DPO, a process-level\npreference optimization scheme that leverages Twisted Sequential Monte Carlo\n(TSMC) to provide scalable stepwise supervision using small LMs. This yields\nmore efficient training, improved stability, and higher accuracy. Extensive\nexperiments on challenging math, science, and logical reasoning benchmarks\ndemonstrate that, with only 10% SFT data and 5% of preference pairs, our method\noutperforms both the DeepSeek-R1 distillation baseline and the outcome-reward\nRL baseline across multiple base models and tasks.", "AI": {"tldr": "针对大型语言模型（LM）在推理时链式思考（CoT）中存在的推理脱轨问题，特别是对于小型LM的长CoT，本文提出了多路径规划聚合（MPPA）框架，通过规划探索和聚合来改进推理。此外，还引入了在线步进DPO（Step-DPO）进行过程级偏好优化，以实现高效且稳定的训练。实验证明，该方法在多个推理基准上优于现有基线，且数据需求极低。", "motivation": "现有LM的推理时CoT方法通常一次性生成整个推理链，导致推理脱轨（CoT derailment），即推理轨迹因累积误差而偏离。这个问题在处理长CoT的小型LM上尤为严重，因为它们能力有限。研究发现，大多数推理错误源于不正确的规划。", "method": "1. **多路径规划聚合（MPPA）**：通过规划探索和聚合来增强单次推理。它根据token位置以可变间隔生成多个候选规划，并将其聚合成一个精炼的规划步骤。设计上保持精简，以基础LM作为主要策略，轻量级LoRA模块实现规划聚合策略。\n2. **在线步进DPO（Online Step-DPO）**：为解决结果奖励RL在长轨迹（如超过4K token）上的低效率问题，引入了一种过程级偏好优化方案，利用Twisted Sequential Monte Carlo (TSMC) 提供可扩展的逐步监督，以实现更高效的训练、更高的稳定性和准确性。", "result": "在具有挑战性的数学、科学和逻辑推理基准测试中，该方法仅使用10%的SFT数据和5%的偏好对，便在多个基础模型和任务上优于DeepSeek-R1蒸馏基线和结果奖励RL基线。它带来了更高效的训练、更高的稳定性和准确性。", "conclusion": "通过结合多路径规划聚合（MPPA）和在线步进DPO（Online Step-DPO），本研究有效解决了大型语言模型在长推理链中遇到的推理脱轨和训练效率低下问题。该方法显著提升了LM的推理能力，尤其适用于小型LM，且对数据量的要求极低。"}}
{"id": "2510.11695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11695", "abs": "https://arxiv.org/abs/2510.11695", "authors": ["Lingfei Qian", "Xueqing Peng", "Yan Wang", "Vincent Jim Zhang", "Huan He", "Hanley Smith", "Yi Han", "Yueru He", "Haohang Li", "Yupeng Cao", "Yangyang Yu", "Alejandro Lopez-Lira", "Peng Lu", "Jian-Yun Nie", "Guojun Xiong", "Jimin Huang", "Sophia Ananiadou"], "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents", "comment": null, "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.", "AI": {"tldr": "该研究引入了Agent Market Arena (AMA)，这是一个实时的、终身基准测试平台，用于评估基于大型语言模型（LLM）的交易代理在多个金融市场中的表现，发现代理框架而非模型骨干对行为模式影响更大。", "motivation": "现有研究在评估LLM驱动的金融交易代理时，存在以下不足：多数研究测试模型而非代理、覆盖时间短、资产种类有限且依赖未经核实的数据，因此需要一个更全面、实时和严谨的评估框架。", "method": "引入了Agent Market Arena (AMA)平台，它整合了经核实的数据、专家验证的新闻和多样化的代理架构，以统一框架进行公平持续的实时比较。AMA实现了四种代理（InvestorAgent、TradeAgent、HedgeFundAgent、DeepFundAgent）并使用多种LLM骨干（如GPT-4o、Claude-3.5-haiku等）在加密货币和股票市场进行实时实验。", "result": "实时实验表明，不同的代理框架展现出截然不同的行为模式，从激进的风险承担到保守的决策制定，而LLM模型骨干对结果变化的影响较小。", "conclusion": "AMA为基于LLM的金融推理和交易智能代理的严谨、可复现和持续演进的评估奠定了基础。"}}
{"id": "2510.11115", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.11115", "abs": "https://arxiv.org/abs/2510.11115", "authors": ["Hao Tang", "Shengfeng He", "Jing Qin"], "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning", "comment": "Accepted by IJCAI 2025", "summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes\nwith limited training samples. While some methods leverage semantic knowledge\nfrom smaller-scale models to mitigate data scarcity, these approaches often\nintroduce noise and bias due to the data's inherent simplicity. In this paper,\nwe propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which\neffectively transfers diverse and complementary knowledge from large multimodal\nmodels to empower the off-the-shelf few-shot learner. Specifically, SynTrans\nemploys CLIP as a robust teacher and uses a few-shot vision encoder as a weak\nstudent, distilling semantic-aligned visual knowledge via an unsupervised proxy\ntask. Subsequently, a training-free synergistic knowledge mining module\nfacilitates collaboration among large multimodal models to extract high-quality\nsemantic knowledge. Building upon this, a visual-semantic bridging module\nenables bi-directional knowledge transfer between visual and semantic spaces,\ntransforming explicit visual and implicit semantic knowledge into\ncategory-specific classifier weights. Finally, SynTrans introduces a visual\nweight generator and a semantic weight reconstructor to adaptively construct\noptimal multimodal FSL classifiers. Experimental results on four FSL datasets\ndemonstrate that SynTrans, even when paired with a simple few-shot vision\nencoder, significantly outperforms current state-of-the-art methods.", "AI": {"tldr": "本文提出SynTrans框架，通过从大型多模态模型中协同转移知识，显著提升了少样本学习（FSL）的性能，解决了现有方法中知识转移引入噪声和偏差的问题。", "motivation": "少样本学习（FSL）在数据稀缺的新类别分类中面临挑战。现有方法虽然尝试利用小型模型的语义知识，但由于数据固有的简单性，这些方法常常引入噪声和偏差。", "method": "SynTrans框架利用CLIP作为强大的教师模型，少样本视觉编码器作为弱学生模型，通过无监督代理任务蒸馏语义对齐的视觉知识。接着，一个免训练的协同知识挖掘模块促进大型多模态模型间的协作，提取高质量的语义知识。然后，一个视觉-语义桥接模块实现视觉和语义空间之间的双向知识转移，将显式视觉和隐式语义知识转化为类别特定的分类器权重。最后，SynTrans引入视觉权重生成器和语义权重重构器来自适应地构建最优多模态FSL分类器。", "result": "在四个FSL数据集上的实验结果表明，SynTrans即使与一个简单的少样本视觉编码器结合，也显著优于当前最先进的方法。", "conclusion": "SynTrans框架成功地从大型多模态模型中有效地转移了多样且互补的知识，赋能了现有的少样本学习器，并构建了最优的多模态FSL分类器，实现了卓越的性能。"}}
{"id": "2510.11142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11142", "abs": "https://arxiv.org/abs/2510.11142", "authors": ["Byron Alexander Jacobs", "Aqeel Morris", "Ifthakaar Shaik", "Frando Lin"], "title": "Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay", "comment": null, "summary": "Sperm DNA fragmentation (SDF) is a critical parameter in male fertility\nassessment that conventional semen analysis fails to evaluate. This study\npresents the validation of a novel artificial intelligence (AI) tool designed\nto detect SDF through digital analysis of phase contrast microscopy images,\nusing the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL)\nassay as the gold standard reference. Utilising the established link between\nsperm morphology and DNA integrity, the present work proposes a morphology\nassisted ensemble AI model that combines image processing techniques with\nstate-of-the-art transformer based machine learning models (GC-ViT) for the\nprediction of DNA fragmentation in sperm from phase contrast images. The\nensemble model is benchmarked against a pure transformer `vision' model as well\nas a `morphology-only` model. Promising results show the proposed framework is\nable to achieve sensitivity of 60\\% and specificity of 75\\%. This\nnon-destructive methodology represents a significant advancement in\nreproductive medicine by enabling real-time sperm selection based on DNA\nintegrity for clinical diagnostic and therapeutic applications.", "AI": {"tldr": "本研究验证了一种新型AI工具，通过数字分析相差显微镜图像来检测精子DNA碎片化（SDF），并以TUNEL检测作为金标准，实现了非破坏性、实时SDF评估。", "motivation": "精子DNA碎片化是男性生育能力评估中的关键参数，但传统精液分析无法评估。因此，需要一种能够实时、非破坏性地检测SDF的方法。", "method": "该研究提出了一种形态学辅助的集成AI模型，结合图像处理技术和基于Transformer的机器学习模型（GC-ViT），利用精子形态与DNA完整性之间的联系，从相差图像中预测DNA碎片化。该模型以TUNEL检测为金标准进行验证，并与纯Transformer视觉模型和纯形态学模型进行了基准测试。", "result": "所提出的框架取得了60%的敏感性和75%的特异性，显示出良好的性能。", "conclusion": "这种非破坏性的方法代表了生殖医学领域的重大进步，能够实现基于DNA完整性的实时精子选择，适用于临床诊断和治疗应用。"}}
{"id": "2510.11701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11701", "abs": "https://arxiv.org/abs/2510.11701", "authors": ["Zhaochen Yu", "Ling Yang", "Jiaru Zou", "Shuicheng Yan", "Mengdi Wang"], "title": "Demystifying Reinforcement Learning in Agentic Reasoning", "comment": "Code and models: https://github.com/Gen-Verse/Open-AgentRL", "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL", "AI": {"tldr": "本文系统研究了代理式强化学习（Agentic RL）在大型语言模型（LLM）代理式推理中的应用，从数据、算法和推理模式三个角度提出了关键设计原则和实践方法，显著提升了推理能力和训练效率，并使小型模型超越了大型模型。", "motivation": "代理式强化学习（Agentic RL）已被证明能有效提升大型语言模型（LLM）的代理式推理能力，但其关键设计原则和最佳实践仍不明确。", "method": "研究人员从数据、算法和推理模式三个关键视角，对代理式推理中的强化学习进行了全面系统的调查。具体方法包括：使用真实的端到端工具使用轨迹进行SFT初始化，构建高多样性、模型感知的数据集以支持探索；采用探索友好的技术（如更高的裁剪、过长奖励整形、维持足够的策略熵）；以及比较不同的推理模式（如深思熟虑的策略与频繁工具调用或冗长的自我推理）。此外，还贡献了高质量的SFT和RL数据集。", "result": "研究发现：(i) 用真实的端到端工具使用轨迹替代拼接的合成轨迹能产生更强的SFT初始化；高多样性、模型感知的数据集能维持探索并显著提升RL性能。(ii) 探索友好的技术（如更高的裁剪、过长奖励整形和维持足够的策略熵）对代理式RL至关重要，能提高训练效率。(iii) 较少工具调用的深思熟虑策略优于频繁工具调用或冗长的自我推理，能提高工具效率和最终准确性。这些实践一致地增强了代理式推理和训练效率，在挑战性基准测试中用更小的模型取得了优异结果。使用这些方法，4B大小的模型也能在AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6等四个基准测试中实现超越32B大小模型的代理式推理性能。", "conclusion": "简单的实践方法能够持续提升大型语言模型的代理式推理能力和训练效率，为未来的代理式强化学习研究奠定了实用的基线。这些方法使得小型模型也能实现卓越的代理式推理性能。"}}
{"id": "2510.11713", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11713", "abs": "https://arxiv.org/abs/2510.11713", "authors": ["Tsung-Han Wu", "Mihran Miroyan", "David M. Chan", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "title": "Are Large Reasoning Models Interruptible?", "comment": "Project Page: https://dynamic-lm.github.io/", "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.", "AI": {"tldr": "传统上，大型推理模型（LRMs）在静态环境中进行评估，但本文挑战了这一“冻结世界”假设，并在中断和动态上下文等现实动态场景下评估了LRM的鲁棒性，发现其性能显著下降并出现了新的故障模式。", "motivation": "大型推理模型（LRMs）在复杂推理方面表现出色，但其传统评估设定在静态的“冻结世界”中，即模型响应被认为是瞬时的，请求上下文在响应期间保持不变。然而，在辅助编程等现代推理任务中，模型可能需要数小时思考，代码也可能在模型开始思考到最终输出期间发生巨大变化，这使得“冻结世界”假设失效。", "method": "本文通过两种现实动态场景来评估LRM的鲁棒性：1) 中断：在有限预算下测试模型部分输出的质量；2) 动态上下文：测试模型对运行时变化的适应能力。评估使用了需要长篇推理的数学和编程基准测试。", "result": "静态评估持续高估了LRM的鲁棒性。即使在静态设置中达到高精度的最先进LRM，在中断或暴露于变化的上下文时，也可能出现不可预测的失败，当更新在推理过程后期引入时，性能下降高达60%。分析还揭示了几种新的故障模式，包括推理泄漏（模型在中断时将推理折叠到最终答案中）、恐慌（在时间压力下模型完全放弃推理并返回不正确答案）和自我怀疑（在整合更新信息时性能下降）。", "conclusion": "“冻结世界”假设对于需要长时间推理的任务来说是不足的。LRM在动态环境中缺乏鲁棒性，即使是最先进的模型也可能在中断或上下文变化时出现显著的性能下降和新的故障模式。这表明需要更现实的评估方法，并开发能够适应动态条件的新一代LRM。"}}
{"id": "2510.11117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11117", "abs": "https://arxiv.org/abs/2510.11117", "authors": ["Yaqi Zhao", "Xiaochen Wang", "Li Dong", "Wentao Zhang", "Yuhui Yuan"], "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies", "comment": null, "summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation\nmodels like FLUX and GPT-4o, which often fail to accurately follow counting\ninstructions in text prompts. In this paper, we aim to study a fundamental yet\noften overlooked question: Can diffusion models inherently generate the correct\nnumber of objects specified by a textual prompt simply by scaling up the\ndataset and model size? To enable rigorous and reproducible evaluation, we\nconstruct a clean synthetic numerosity benchmark comprising two complementary\ndatasets: GrayCount250 for controlled scaling studies, and NaturalCount6\nfeaturing complex naturalistic scenes. Second, we empirically show that the\nscaling hypothesis does not hold: larger models and datasets alone fail to\nimprove counting accuracy on our benchmark. Our analysis identifies a key\nreason: diffusion models tend to rely heavily on the noise initialization\nrather than the explicit numerosity specified in the prompt. We observe that\nnoise priors exhibit biases toward specific object counts. In addition, we\npropose an effective strategy for controlling numerosity by injecting\ncount-aware layout information into the noise prior. Our method achieves\nsignificant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and\non NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization\nacross settings.", "AI": {"tldr": "文本到图像扩散模型在数字计数方面表现不佳，单纯扩大模型和数据集规模无法解决问题。研究发现模型过度依赖噪声初始化，且噪声先验存在计数偏差。论文提出通过向噪声先验注入计数感知的布局信息，显著提高了计数准确性。", "motivation": "FLUX和GPT-4o等最先进的文本到图像生成模型在遵循文本提示中的计数指令时常出错。本文旨在研究一个基本但常被忽视的问题：扩散模型是否能通过简单地扩大数据集和模型规模，就能内在生成文本提示中指定数量的正确对象。", "method": "1. 构建了两个互补的合成计数基准：GrayCount250（用于受控规模研究）和NaturalCount6（包含复杂自然场景），以实现严谨和可复现的评估。2. 实证研究了规模扩大假设（即更大的模型和数据集能否提高计数准确性）。3. 分析了规模扩大无效的原因，发现扩散模型倾向于严重依赖噪声初始化，而非提示中明确指定的数字，且噪声先验存在特定对象计数的偏差。4. 提出了一种通过向噪声先验注入计数感知的布局信息来控制数字计数的有效策略。", "result": "1. 规模扩大假设不成立：单独增大模型和数据集规模未能提高基准测试上的计数准确性。2. 分析发现扩散模型严重依赖噪声初始化，且噪声先验对特定对象计数存在偏差。3. 提出的方法实现了显著的性能提升，将GrayCount250的准确率从20.0%提高到85.3%，NaturalCount6的准确率从74.8%提高到86.3%。4. 证明了该方法在不同设置下具有有效的泛化能力。", "conclusion": "扩散模型单纯通过扩大规模无法解决数字计数问题。噪声初始化在其中起着关键作用并存在偏差。通过向噪声先验注入计数感知的布局信息，可以有效控制数字计数，显著提高模型的计数准确性和泛化能力。"}}
{"id": "2510.11171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11171", "abs": "https://arxiv.org/abs/2510.11171", "authors": ["Junfei Shi", "Haojia Zhang", "Haiyan Jin", "Junhuai Li", "Xiaogang Song", "Yuanfan Guo", "Haonan Su", "Weisi Lin"], "title": "Multiview Manifold Evidential Fusion for PolSAR Image Classification", "comment": "The paper has 14 pages and 7 figures", "summary": "Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their\nextracted multi-features - such as scattering angle, entropy, texture, and\nboundary descriptors - provide complementary and physically interpretable\ninformation for image classification. Traditional fusion strategies typically\nconcatenate these features or employ deep learning networks to combine them.\nHowever, the covariance matrices and multi-features, as two complementary\nviews, lie on different manifolds with distinct geometric structures. Existing\nfusion methods also overlook the varying importance of different views and\nignore uncertainty, often leading to unreliable predictions. To address these\nissues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to\neffectively fuse these two views. It gives a new framework to integrate PolSAR\nmanifold learning and evidence fusion into a unified architecture.\nSpecifically, covariance matrices are represented on the Hermitian Positive\nDefinite (HPD) manifold, while multi-features are modeled on the Grassmann\nmanifold. Two different kernel metric learning networks are constructed to\nlearn their manifold representations. Subsequently, a trusted multiview\nevidence fusion, replacing the conventional softmax classifier, estimates\nbelief mass and quantifies the uncertainty of each view from the learned deep\nfeatures. Finally, a Dempster-Shafer theory-based fusion strategy combines\nevidence, enabling a more reliable and interpretable classification. Extensive\nexperiments on three real-world PolSAR datasets demonstrate that the proposed\nmethod consistently outperforms existing approaches in accuracy, robustness,\nand interpretability.", "AI": {"tldr": "本论文提出了一种多视图流形证据融合（MMEFnet）方法，用于极化合成孔径雷达（PolSAR）图像分类。该方法通过将协方差矩阵和多特征分别建模在不同的流形上，并结合证据融合理论，实现了更可靠、可解释的分类。", "motivation": "传统的PolSAR特征融合策略通常简单地拼接特征或使用深度学习，但它们忽略了协方差矩阵和多特征作为互补视图存在于不同几何结构的流形上，并且忽视了不同视图的重要性差异及不确定性，导致预测结果不可靠。", "method": "MMEFnet方法将PolSAR流形学习和证据融合整合到统一架构中。具体而言，协方差矩阵在Hermitian正定（HPD）流形上表示，而多特征在Grassmann流形上建模。构建了两个不同的核度量学习网络来学习它们的流形表示。随后，采用可信的多视图证据融合（取代传统softmax分类器）来估计置信质量并量化每个视图的不确定性。最后，基于Dempster-Shafer理论的融合策略组合证据。", "result": "在三个真实世界的PolSAR数据集上进行的广泛实验表明，所提出的方法在准确性、鲁棒性和可解释性方面始终优于现有方法。", "conclusion": "MMEFnet为PolSAR图像分类提供了一个新的框架，通过有效融合具有不同流形结构和不确定性的多视图，实现了更可靠和可解释的分类结果。"}}
{"id": "2510.11173", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.11173", "abs": "https://arxiv.org/abs/2510.11173", "authors": ["Zhenyu Lu", "Liupeng Li", "Jinpeng Wang", "Yan Feng", "Bin Chen", "Ke Chen", "Yaowei Wang"], "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation", "comment": "18 pages, 6 figures, 6 tables", "summary": "Existing works on reasoning segmentation either connect hidden features from\na language model directly to a mask decoder or represent positions in text,\nwhich limits interpretability and semantic detail. To solve this, we present\nCoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model\nthat bridges language reasoning to segmentation through a differentiable and\ninterpretable positional prior instantiated as a heatmap. By making the\nreasoning process clear via MCoT and expressing it as a dense, differentiable\nheatmap, this interface enhances interpretability and diagnostic analysis and\nyields more concentrated evidence on the target. A learnable concentration\ntoken aggregates features of the image and reasoning text to generate this\npositional prior, which is decoded to precise masks through a lightweight\ndecoder, providing a direct connection between reasoning and segmentation.\nAcross the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best\nreported metrics on each standard split under comparable protocols, with\nperformance at or above prior state of the art across both validation and test\npartitions. Extensive experiments reveal that the quality of the heatmap\nstrongly influences the resulting mask quality, supporting a consistent\nassociation between the reasoning output and downstream mask generation.\nCollectively, these findings support the utility of this paradigm in bridging\nreasoning and segmentation and show advantages in concentration driven by\nreasoning and predicting masks more precisely. Code, checkpoints and logs are\nreleased at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.", "AI": {"tldr": "CoPRS是一种基于多模态思维链（MCoT）的位置感知模型，通过可微分且可解释的位置先验（热图）将语言推理与分割任务连接起来，提高了可解释性并实现了更精确的掩码生成。", "motivation": "现有推理分割方法直接将语言模型隐藏特征连接到掩码解码器或表示文本位置，这限制了可解释性和语义细节。", "method": "CoPRS引入了基于多模态思维链（MCoT）的机制，通过一个可微分且可解释的位置先验（实例化为热图）来桥接语言推理和分割。一个可学习的“集中”令牌聚合图像和推理文本的特征以生成此位置先验，然后通过轻量级解码器将其解码为精确的掩码。", "result": "CoPRS在RefCOCO系列和ReasonSeg数据集上，在可比较协议下，匹配或超越了现有最佳报告指标，并在验证和测试分区上均达到或超越了现有最先进水平。实验表明，热图质量强烈影响最终的掩码质量，支持推理输出与下游掩码生成之间的一致关联。", "conclusion": "该范式有效弥合了推理和分割之间的鸿沟，在由推理驱动的集中和更精确的掩码预测方面显示出优势。"}}
{"id": "2510.11175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11175", "abs": "https://arxiv.org/abs/2510.11175", "authors": ["Xiang Ma", "Litian Xu", "Lexin Fang", "Caiming Zhang", "Lizhen Cui"], "title": "Reliable Cross-modal Alignment via Prototype Iterative Construction", "comment": null, "summary": "Cross-modal alignment is an important multi-modal task, aiming to bridge the\nsemantic gap between different modalities. The most reliable fundamention for\nachieving this objective lies in the semantic consistency between matched\npairs. Conventional methods implicitly assume embeddings contain solely\nsemantic information, ignoring the impact of non-semantic information during\nalignment, which inevitably leads to information bias or even loss. These\nnon-semantic information primarily manifest as stylistic variations in the\ndata, which we formally define as style information. An intuitive approach is\nto separate style from semantics, aligning only the semantic information.\nHowever, most existing methods distinguish them based on feature columns, which\ncannot represent the complex coupling relationship between semantic and style\ninformation. In this paper, we propose PICO, a novel framework for suppressing\nstyle interference during embedding interaction. Specifically, we quantify the\nprobability of each feature column representing semantic information, and\nregard it as the weight during the embedding interaction. To ensure the\nreliability of the semantic probability, we propose a prototype iterative\nconstruction method. The key operation of this method is a performance\nfeedback-based weighting function, and we have theoretically proven that the\nfunction can assign higher weight to prototypes that bring higher performance\nimprovements. Extensive experiments on various benchmarks and model backbones\ndemonstrate the superiority of PICO, outperforming state-of-the-art methods by\n5.2\\%-14.1\\%.", "AI": {"tldr": "本文提出PICO框架，通过量化并加权特征列的语义信息概率，有效抑制跨模态对齐中的风格干扰，显著提升了对齐性能。", "motivation": "传统的跨模态对齐方法假设嵌入只包含语义信息，忽略了非语义（风格）信息的干扰，导致信息偏差或丢失。现有区分语义和风格的方法基于特征列分离，无法捕捉二者复杂的耦合关系。", "method": "本文提出了PICO框架，用于抑制嵌入交互中的风格干扰。具体而言，它量化了每个特征列代表语义信息的概率，并将其作为嵌入交互过程中的权重。为确保语义概率的可靠性，提出了一种原型迭代构建方法，其核心操作是基于性能反馈的加权函数，该函数能为带来更高性能提升的原型分配更高权重。", "result": "在各种基准和模型骨干上的大量实验表明，PICO框架优于最先进的方法5.2%至14.1%，展示了其优越性。", "conclusion": "PICO框架通过有效抑制风格干扰，显著提升了跨模态对齐的性能，为解决语义与风格信息复杂耦合问题提供了新思路。"}}
{"id": "2510.11183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11183", "abs": "https://arxiv.org/abs/2510.11183", "authors": ["Ali Alhejab", "Tomas Zelezny", "Lamya Alkanhal", "Ivan Gruber", "Yazeed Alharbi", "Jakub Straka", "Vaclav Javorek", "Marek Hruz", "Badriah Alkalifah", "Ahmed Ali"], "title": "Saudi Sign Language Translation Using T5", "comment": "11 pages, supplementary, SPECOM 2025", "summary": "This paper explores the application of T5 models for Saudi Sign Language\n(SSL) translation using a novel dataset. The SSL dataset includes three\nchallenging testing protocols, enabling comprehensive evaluation across\ndifferent scenarios. Additionally, it captures unique SSL characteristics, such\nas face coverings, which pose challenges for sign recognition and translation.\nIn our experiments, we investigate the impact of pre-training on American Sign\nLanguage (ASL) data by comparing T5 models pre-trained on the YouTubeASL\ndataset with models trained directly on the SSL dataset. Experimental results\ndemonstrate that pre-training on YouTubeASL significantly improves models'\nperformance (roughly $3\\times$ in BLEU-4), indicating cross-linguistic\ntransferability in sign language models. Our findings highlight the benefits of\nleveraging large-scale ASL data to improve SSL translation and provide insights\ninto the development of more effective sign language translation systems. Our\ncode is publicly available at our GitHub repository.", "AI": {"tldr": "本论文探索了使用T5模型和新数据集进行沙特手语（SSL）翻译，并发现通过美国手语（ASL）数据进行预训练能显著提升模型性能，证实了手语模型中的跨语言迁移能力。", "motivation": "开发有效的沙特手语（SSL）翻译系统，尤其要应对其独特的挑战，如面部遮盖物对手语识别和翻译的难度。", "method": "使用T5模型进行SSL翻译，并构建了一个包含三种挑战性测试协议的新型SSL数据集。研究比较了在YouTubeASL数据集上预训练的T5模型与直接在SSL数据集上训练的模型，以评估ASL预训练对SSL翻译性能的影响。", "result": "实验结果表明，在YouTubeASL上进行预训练显著提升了模型的性能（BLEU-4指标提升约3倍），这表明手语模型具有跨语言迁移能力。", "conclusion": "利用大规模ASL数据对改进SSL翻译具有显著益处，为开发更高效的手语翻译系统提供了重要见解。"}}
{"id": "2510.11178", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.11178", "abs": "https://arxiv.org/abs/2510.11178", "authors": ["Bryan Chen Zhengyu Tan", "Zheng Weihua", "Zhengyuan Liu", "Nancy F. Chen", "Hwaran Lee", "Kenny Tsu Wei Choo", "Roy Ka-Wei Lee"], "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models", "comment": "Code and Dataset to be released", "summary": "As vision-language models (VLMs) are deployed globally, their ability to\nunderstand culturally situated knowledge becomes essential. Yet, existing\nevaluations largely assess static recall or isolated visual grounding, leaving\nunanswered whether VLMs possess robust and transferable cultural understanding.\nWe introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to\nevaluate the robustness of everyday cultural knowledge in VLMs across\nlinguistic rephrasings and visual modalities. Building on the BLEnD dataset,\nBLEnD-Vis constructs 313 culturally grounded question templates spanning 16\nregions and generates three aligned multiple-choice formats: (i) a text-only\nbaseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant\n(Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated\nimages. The resulting benchmark comprises 4,916 images and over 21,000\nmultiple-choice question (MCQ) instances, validated through human annotation.\nBLEnD-Vis reveals significant fragility in current VLM cultural knowledge;\nmodels exhibit performance drops under linguistic rephrasing and, whilst visual\ncues often aid performance, low cross-modal consistency highlights challenges\nin robustly integrating textual and visual understanding, particularly for\nlower-resource regions. BLEnD-Vis thus provides a crucial testbed for\nsystematically analysing cultural robustness and multimodal grounding, exposing\nlimitations and guiding the development of more culturally competent VLMs.", "AI": {"tldr": "该研究引入BLEnD-Vis基准，评估视觉-语言模型（VLMs）在语言重述和视觉模态下对日常文化知识的鲁棒性，发现现有VLM的文化理解存在显著脆弱性，尤其在跨模态一致性方面。", "motivation": "随着VLM在全球范围内的部署，它们理解文化情境知识的能力变得至关重要。然而，现有评估主要侧重于静态记忆或孤立的视觉定位，未能回答VLM是否具备鲁棒且可迁移的文化理解能力。", "method": "引入BLEnD-Vis，一个多模态、多文化基准。它基于BLEnD数据集，构建了313个跨16个区域的文化接地问题模板，并生成了三种对齐的多项选择格式：(i) 文本-区域到实体，(ii) 文本-实体到区域的倒置变体，以及(iii) 带有生成图像的(ii)的VQA风格版本。最终基准包含4,916张图像和超过21,000个多项选择题实例，并通过人工标注进行验证。", "result": "BLEnD-Vis揭示了当前VLM文化知识的显著脆弱性；模型在语言重述下性能下降，尽管视觉线索通常有助于提高性能，但较低的跨模态一致性突出表明在稳健整合文本和视觉理解方面存在挑战，特别是对于资源较少的地区。", "conclusion": "BLEnD-Vis提供了一个关键的测试平台，用于系统分析文化鲁棒性和多模态接地，揭示了局限性并指导开发更具文化能力的VLM。"}}
{"id": "2510.11190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11190", "abs": "https://arxiv.org/abs/2510.11190", "authors": ["Shengming Yuan", "Xinyu Lyu", "Shuailong Wang", "Beitao Chen", "Jingkuan Song", "Lianli Gao"], "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models", "comment": "19 pages, 11 figures. Accepted by the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Multimodal large language models (MLLMs) face an inherent trade-off between\nfaithfulness and creativity, as different tasks require varying degrees of\nassociative reasoning. However, existing methods lack the flexibility to\nmodulate this reasoning strength, limiting MLLMs' adaptability across factual\nand creative scenarios. To bridge this gap, we propose equipping MLLMs with\nmechanisms that enable flexible control over associative reasoning. We begin by\ninvestigating the internal mechanisms underlying associative behavior in MLLMs\nand find that: (1) middle layers play a pivotal role in shaping model's\nassociative tendencies, (2) modifying representations in these layers\neffectively regulates associative reasoning strength, and (3) hallucinations\ncan be exploited to derive steering vectors that guide this modulation.\nBuilding on these findings, we introduce Flexible Association Control (FlexAC),\na lightweight and training-free framework for modulating associative behavior\nin MLLMs. FlexAC first induces hallucination-guided intermediate\nrepresentations to encode associative directions. Then, it selects\nhigh-association instances to construct effective associative steering vectors,\nwhose strengths are adaptively calibrated to balance creative guidance with\noutput stability. Finally, recognizing the multi-dimensional nature of\nassociative reasoning, FlexAC incorporates task-specific associative vectors\nderived from a forward pass on a few target-domain samples, enabling models to\nfollow diverse associative directions and better adapt to creative tasks.\nNotably, our method achieves up to a 5.8x improvement in creativity on\nCreation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing\nexisting baselines and demonstrating its effectiveness in enabling flexible\ncontrol over associative reasoning in MLLMs. Our code is available at\nhttps://github.com/ylhz/FlexAC.", "AI": {"tldr": "该论文提出了一种名为FlexAC的轻量级、免训练框架，旨在为多模态大型语言模型（MLLMs）提供灵活控制联想推理能力，以平衡其在事实性和创造性任务中的表现。", "motivation": "多模态大型语言模型（MLLMs）在忠实性和创造性之间存在固有的权衡，不同任务需要不同程度的联想推理。然而，现有方法缺乏调节这种推理强度的灵活性，限制了MLLMs在事实性和创造性场景中的适应性。", "method": "研究首先调查了MLLMs中联想行为的内部机制，发现：1）中间层在塑造模型联想倾向中起关键作用；2）修改这些层中的表示能有效调节联想推理强度；3）幻觉可以被利用来导出引导这种调节的转向向量。在此基础上，提出了FlexAC框架：1）诱导幻觉引导的中间表示来编码联想方向；2）选择高联想实例构建有效的联想转向向量，并自适应校准其强度以平衡创造性引导与输出稳定性；3）整合来自少量目标域样本前向传播获得的任务特定联想向量，以适应多样化的联想方向和创意任务。", "result": "FlexAC方法在Creation-MMBench上实现了高达5.8倍的创造力提升，并在CHAIR上将幻觉率降低了29%，超越了现有基线方法。", "conclusion": "FlexAC框架有效解决了MLLMs在忠实性和创造性之间的权衡问题，通过提供灵活控制联想推理的能力，显著提高了模型在创意任务中的表现，同时降低了幻觉率，展示了其在增强MLLMs适应性方面的有效性。"}}
{"id": "2510.11223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11223", "abs": "https://arxiv.org/abs/2510.11223", "authors": ["Masoumeh Chapariniya", "Pierre Vuillecard", "Jean-Marc Odobez", "Volker Dellwo", "Teodora Vukovic"], "title": "Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features", "comment": null, "summary": "This work investigates whether individuals can be identified solely through\nthe pure dynamical components of their facial expressions, independent of\nstatic facial appearance. We leverage the FLAME 3D morphable model to achieve\nexplicit disentanglement between facial shape and expression dynamics,\nextracting frame-by-frame parameters from conversational videos while retaining\nonly expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers\nin naturalistic conversations, our Conformer model with supervised contrastive\nlearning achieves 61.14\\%accuracy on 1,429-way classification -- 458 times\nabove chance -- demonstrating that facial dynamics carry strong identity\nsignatures. We introduce a drift-to-noise ratio (DNR) that quantifies the\nreliability of shape expression separation by measuring across-session shape\nchanges relative to within-session variability. DNR strongly negatively\ncorrelates with recognition performance, confirming that unstable shape\nestimation compromises dynamic identification. Our findings reveal\nperson-specific signatures in conversational facial dynamics, with implications\nfor social perception and clinical assessment.", "AI": {"tldr": "本研究发现，仅通过面部表情的动态成分，就可以在很大程度上识别个体身份，与静态面部特征无关。", "motivation": "研究旨在探索个体是否可以仅凭面部表情的纯动态成分被识别，而不依赖于静态面部外观。", "method": "研究利用FLAME 3D可变形模型实现面部形状和表情动态的明确分离，从对话视频中提取逐帧表情和下颌系数。接着，使用带有监督对比学习的Conformer模型进行1,429路分类。此外，引入漂移噪声比（DNR）来量化形状表情分离的可靠性。", "result": "在CANDOR数据集上，Conformer模型在1,429路分类中达到61.14%的准确率（是随机猜测的458倍），表明面部动态携带强烈的身份特征。漂移噪声比（DNR）与识别性能呈强烈负相关，证实不稳定的形状估计会损害动态识别。", "conclusion": "研究结果揭示了对话面部动态中存在个体特有的身份特征，这对社会感知和临床评估具有重要意义。"}}
{"id": "2510.11204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11204", "abs": "https://arxiv.org/abs/2510.11204", "authors": ["Rohit Gupta", "Anirban Roy", "Claire Christensen", "Sujeong Kim", "Sarah Gerard", "Madeline Cincebeaux", "Ajay Divakaran", "Todd Grindal", "Mubarak Shah"], "title": "Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos", "comment": "Published at CVPR 2023", "summary": "The recent growth in the consumption of online media by children during early\nchildhood necessitates data-driven tools enabling educators to filter out\nappropriate educational content for young learners. This paper presents an\napproach for detecting educational content in online videos. We focus on two\nwidely used educational content classes: literacy and math. For each class, we\nchoose prominent codes (sub-classes) based on the Common Core Standards. For\nexample, literacy codes include `letter names', `letter sounds', and math codes\ninclude `counting', `sorting'. We pose this as a fine-grained multilabel\nclassification problem as videos can contain multiple types of educational\ncontent and the content classes can get visually similar (e.g., `letter names'\nvs `letter sounds'). We propose a novel class prototypes based supervised\ncontrastive learning approach that can handle fine-grained samples associated\nwith multiple labels. We learn a class prototype for each class and a loss\nfunction is employed to minimize the distances between a class prototype and\nthe samples from the class. Similarly, distances between a class prototype and\nthe samples from other classes are maximized. As the alignment between visual\nand audio cues are crucial for effective comprehension, we consider a\nmultimodal transformer network to capture the interaction between visual and\naudio cues in videos while learning the embedding for videos. For evaluation,\nwe present a dataset, APPROVE, employing educational videos from YouTube\nlabeled with fine-grained education classes by education researchers. APPROVE\nconsists of 193 hours of expert-annotated videos with 19 classes. The proposed\napproach outperforms strong baselines on APPROVE and other benchmarks such as\nYoutube-8M, and COIN. The dataset is available at\nhttps://github.com/rohit-gupta/MMContrast/tree/main/APPROVE", "AI": {"tldr": "本文提出了一种基于类原型监督对比学习的多模态Transformer网络，用于在线视频中细粒度教育内容的检测，并创建了APPROVE数据集，在多个基准测试中表现优异。", "motivation": "随着儿童在线媒体消费的增长，教育工作者需要数据驱动的工具来筛选适合幼儿的教育内容。现有方法可能无法有效处理细粒度且视觉相似的教育内容分类。", "method": "将问题建模为细粒度多标签分类，关注读写和数学两大类中的特定子类（如字母名称、计数）。提出了一种新颖的基于类原型的监督对比学习方法，通过最小化样本与同类原型距离、最大化与异类原型距离来处理多标签细粒度样本。采用多模态Transformer网络捕获视频中视觉和音频线索的交互。构建了一个包含193小时专家标注视频（19个细粒度教育类别）的APPROVE数据集进行评估。", "result": "所提出的方法在APPROVE数据集以及YouTube-8M和COIN等其他基准测试中均优于强大的基线模型。APPROVE数据集已公开。", "conclusion": "本文提出了一种有效检测在线视频中细粒度教育内容的方法，并构建了一个高质量的标注数据集APPROVE，为教育工作者筛选适龄内容提供了有力的工具和资源。"}}
{"id": "2510.11259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11259", "abs": "https://arxiv.org/abs/2510.11259", "authors": ["Weixuan Li", "Quanjun Li", "Guang Yu", "Song Yang", "Zimeng Li", "Chi-Man Pun", "Yupeng Liu", "Xuhang Chen"], "title": "DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation", "comment": "Accepted by BIBM 2025", "summary": "In medical image segmentation, skip connections are used to merge global\ncontext and reduce the semantic gap between encoder and decoder. Current\nmethods often struggle with limited structural representation and insufficient\ncontextual modeling, affecting generalization in complex clinical scenarios. We\npropose the DTEA model, featuring a new skip connection framework with the\nSemantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG)\nmodules. STR reorganizes multi-scale semantic features into a dynamic\nhypergraph to better model cross-resolution anatomical dependencies, enhancing\nstructural and semantic representation. EPG assesses channel stability after\nperturbation and filters high-entropy channels to emphasize clinically\nimportant regions and improve spatial attention. Extensive experiments on three\nbenchmark datasets show our framework achieves superior segmentation accuracy\nand better generalization across various clinical settings. The code is\navailable at\n\\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.", "AI": {"tldr": "本文提出了DTEA模型，通过语义拓扑重构（STR）和熵扰动门控（EPG）模块，改进了跳跃连接框架，以增强医学图像分割中的结构表示和上下文建模，实现了卓越的分割精度和泛化能力。", "motivation": "现有医学图像分割方法中的跳跃连接在结构表示和上下文建模方面存在局限性，导致在复杂临床场景中泛化能力不足，难以有效融合全局上下文并缩小编码器与解码器之间的语义鸿沟。", "method": "本文提出了DTEA模型，包含两个核心模块：1. 语义拓扑重构（STR）：将多尺度语义特征重组为动态超图，以更好地建模跨分辨率的解剖依赖性，从而增强结构和语义表示。2. 熵扰动门控（EPG）：评估扰动后的通道稳定性，并过滤高熵通道，以强调临床重要区域并提升空间注意力。", "result": "在三个基准数据集上进行的广泛实验表明，DTEA框架实现了卓越的分割精度，并在各种临床设置中表现出更好的泛化能力。", "conclusion": "DTEA模型通过其创新的跳跃连接框架，特别是STR和EPG模块，有效解决了医学图像分割中结构表示和上下文建模的挑战，显著提升了分割性能和临床泛化能力。"}}
{"id": "2510.11268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11268", "abs": "https://arxiv.org/abs/2510.11268", "authors": ["Jaeik Kim", "Jaeyoung Do"], "title": "Exploring and Leveraging Class Vectors for Classifier Editing", "comment": "Accepted in NeurIPS 2025", "summary": "Image classifiers play a critical role in detecting diseases in medical\nimaging and identifying anomalies in manufacturing processes. However, their\npredefined behaviors after extensive training make post hoc model editing\ndifficult, especially when it comes to forgetting specific classes or adapting\nto distribution shifts. Existing classifier editing methods either focus\nnarrowly on correcting errors or incur extensive retraining costs, creating a\nbottleneck for flexible editing. Moreover, such editing has seen limited\ninvestigation in image classification. To overcome these challenges, we\nintroduce Class Vectors, which capture class-specific representation\nadjustments during fine-tuning. Whereas task vectors encode task-level changes\nin weight space, Class Vectors disentangle each class's adaptation in the\nlatent space. We show that Class Vectors capture each class's semantic shift\nand that classifier editing can be achieved either by steering latent features\nalong these vectors or by mapping them into weight space to update the decision\nboundaries. We also demonstrate that the inherent linearity and orthogonality\nof Class Vectors support efficient, flexible, and high-level concept editing\nvia simple class arithmetic. Finally, we validate their utility in applications\nsuch as unlearning, environmental adaptation, adversarial defense, and\nadversarial trigger optimization.", "AI": {"tldr": "本文引入“类向量”，这是一种在潜在空间中捕获类别特定表示调整的新方法，实现了对图像分类器的高效、灵活和高级概念编辑。", "motivation": "图像分类器在医学成像和制造业中发挥关键作用，但其预定义行为使得事后模型编辑（如遗忘特定类别或适应分布变化）变得困难。现有编辑方法要么过于狭窄，要么需要高昂的再训练成本，并且在图像分类领域的研究有限，导致缺乏灵活的编辑能力。", "method": "本文提出了“类向量”（Class Vectors），它们在微调过程中捕获类别特定的表示调整。与任务向量在权重空间编码任务级变化不同，类向量在潜在空间中解耦了每个类别的适应性。编辑可以通过沿着这些向量引导潜在特征，或将它们映射到权重空间以更新决策边界来实现。", "result": "研究表明，类向量能够捕获每个类别的语义漂移。由于其固有的线性和正交性，类向量支持通过简单的类别算术实现高效、灵活和高级的概念编辑。其效用在遗忘、环境适应、对抗性防御和对抗性触发优化等应用中得到了验证。", "conclusion": "类向量提供了一种新颖且强大的方法，能够克服图像分类器编辑的挑战，通过在潜在空间进行类别特定的调整，实现了高效、灵活且高级的概念编辑，适用于多种应用场景。"}}
{"id": "2510.11287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11287", "abs": "https://arxiv.org/abs/2510.11287", "authors": ["Han Xia", "Quanjun Li", "Qian Li", "Zimeng Li", "Hongbin Ye", "Yupeng Liu", "Haolun Li", "Xuhang Chen"], "title": "EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism", "comment": "Accepted by BIBM 2025", "summary": "Medical image segmentation is vital for diagnosis, treatment planning, and\ndisease monitoring but is challenged by complex factors like ambiguous edges\nand background noise. We introduce EEMS, a new model for segmentation,\ncombining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt\nGeneration Unit (MSPGU). EAEU enhances edge perception via multi-frequency\nfeature extraction, accurately defining boundaries. MSPGU integrates high-level\nsemantic and low-level spatial features using a prompt-guided approach,\nensuring precise target localization. The Dual-Source Adaptive Gated Fusion\nUnit (DAGFU) merges edge features from EAEU with semantic features from MSPGU,\nenhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018\nconfirm EEMS's superior performance and reliability as a clinical tool.", "AI": {"tldr": "本文提出EEMS模型，通过结合边缘感知增强单元、多尺度提示生成单元和双源自适应门控融合单元，克服了医学图像分割中边缘模糊和背景噪声的挑战，显著提高了分割精度和鲁棒性。", "motivation": "医学图像分割对于诊断、治疗规划和疾病监测至关重要，但面临边缘模糊和背景噪声等复杂因素的挑战。", "method": "EEMS模型包含三个核心单元：1) 边缘感知增强单元（EAEU），通过多频特征提取增强边缘感知；2) 多尺度提示生成单元（MSPGU），利用提示引导方法整合高层语义和低层空间特征；3) 双源自适应门控融合单元（DAGFU），融合EAEU的边缘特征与MSPGU的语义特征，以提高分割精度和鲁棒性。", "result": "在ISIC2018等数据集上的测试表明，EEMS模型表现出卓越的性能和可靠性。", "conclusion": "EEMS模型是一种可靠的临床医学图像分割工具。"}}
{"id": "2510.11303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11303", "abs": "https://arxiv.org/abs/2510.11303", "authors": ["Yan Zhou", "Mingji Li", "Xiantao Zeng", "Jie Lin", "Yuexia Zhou"], "title": "sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging", "comment": null, "summary": "Sketch-based 3D reconstruction remains a challenging task due to the abstract\nand sparse nature of sketch inputs, which often lack sufficient semantic and\ngeometric information. To address this, we propose Sketch2Symm, a two-stage\ngeneration method that produces geometrically consistent 3D shapes from\nsketches. Our approach introduces semantic bridging via sketch-to-image\ntranslation to enrich sparse sketch representations, and incorporates symmetry\nconstraints as geometric priors to leverage the structural regularity commonly\nfound in everyday objects. Experiments on mainstream sketch datasets\ndemonstrate that our method achieves superior performance compared to existing\nsketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's\nDistance, and F-Score, verifying the effectiveness of the proposed semantic\nbridging and symmetry-aware design.", "AI": {"tldr": "该论文提出Sketch2Symm，一种两阶段生成方法，通过语义桥接（草图转图像）和引入对称性约束，从抽象草图重建几何一致的3D形状。", "motivation": "由于草图输入抽象且稀疏，缺乏足够的语义和几何信息，导致基于草图的3D重建任务极具挑战性。", "method": "本文提出Sketch2Symm方法，包括两个阶段：1) 通过草图到图像的翻译实现语义桥接，以丰富稀疏草图表示；2) 引入对称性约束作为几何先验，利用日常物体中常见的结构规律。", "result": "在主流草图数据集上的实验表明，该方法在Chamfer距离、Earth Mover's距离和F-Score方面均优于现有基于草图的重建方法。", "conclusion": "所提出的语义桥接和对称感知设计有效提升了基于草图的3D重建性能，能够生成几何一致的3D形状。"}}
{"id": "2510.11295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11295", "abs": "https://arxiv.org/abs/2510.11295", "authors": ["Jian Lan", "Zhicheng Liu", "Udo Schlegel", "Raoyuan Zhao", "Yihong Liu", "Hinrich Schütze", "Michael A. Hedderich", "Thomas Seidl"], "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering", "comment": null, "summary": "Large vision-language models (VLMs) achieve strong performance in Visual\nQuestion Answering but still rely heavily on supervised fine-tuning (SFT) with\nmassive labeled datasets, which is costly due to human annotations. Crucially,\nreal-world datasets often exhibit human uncertainty (HU) -- variation in human\nconfidence across annotations -- but standard SFT simply optimizes toward the\nmost frequent label, disregarding HU distributions. This leaves two open\nquestions: How does HU affect SFT, and how can HU be effectively leveraged in\ntraining? In this work, we first conduct a systematic evaluation of VLMs across\nvarying HU levels. We have two key findings: (i) surprisingly, high-HU samples\ncontribute little or even degrade model performance, and (ii) naively training\non the full dataset yields under-calibrated models that fail to capture HU\ndistributions. Motivated by these findings, we introduce HaDola, a human\nuncertainty-aware data selection and automatic labeling framework. HaDola\noperates in four stages -- discriminate, self-annotate, error trigger, and\ntraining -- to iteratively identify harmful samples, prioritize informative\nones, and bootstrap from a small seed set (5\\% of data). Our approach\nsubstantially reduces reliance on costly HU annotations and makes VLMs more\naccurate and better calibrated. Extensive experiments on VQAv2 and VizWiz\ndatasets demonstrate that HaDola consistently matches or outperforms\nstate-of-the-art baselines with less training data. Our work highlights the\nimportance of explicitly modeling HU in SFT, suggesting that better utilization\nof HU is more effective than merely scaling up dataset size.", "AI": {"tldr": "本文研究了视觉语言模型（VLMs）在监督微调（SFT）中人类不确定性（HU）的影响，发现高HU样本可能损害模型性能并导致校准不足。为此，提出了一种名为HaDola的框架，通过数据选择和自动标注来有效利用HU，在减少标注成本的同时提高模型准确性和校准性。", "motivation": "大规模VLMs在视觉问答中严重依赖昂贵的人工标注SFT。现实世界数据集常存在人类不确定性（HU），但标准SFT只关注最频繁的标签，忽略了HU分布。这引出了两个问题：HU如何影响SFT，以及如何在训练中有效利用HU。", "method": "首先，对不同HU水平下的VLMs进行了系统评估。基于评估结果，提出了HaDola框架，这是一个人类不确定性感知的数据选择和自动标注框架。HaDola分四个阶段操作：区分、自标注、错误触发和训练，旨在迭代识别有害样本、优先选择信息量大的样本，并从少量初始数据（5%）中进行自举。", "result": "研究发现：(i) 高HU样本对模型性能贡献甚微甚至可能降低性能；(ii) 简单地在完整数据集上训练会导致模型校准不足，无法捕捉HU分布。HaDola显著减少了对昂贵HU标注的依赖，使VLMs更准确且校准更好。在VQAv2和VizWiz数据集上的实验表明，HaDola在更少训练数据的情况下，持续匹配或超越了最先进的基线模型。", "conclusion": "研究强调了在SFT中明确建模人类不确定性的重要性，并表明有效利用HU比单纯扩大数据集规模更能提升VLMs的性能和校准性。"}}
{"id": "2510.11387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11387", "abs": "https://arxiv.org/abs/2510.11387", "authors": ["Wenyuan Zhang", "Jimin Tang", "Weiqi Zhang", "Yi Fang", "Yu-Shen Liu", "Zhizhong Han"], "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference", "comment": "Accepted by NeurIPS 2025. Project Page:\n  https://wen-yuan-zhang.github.io/MaterialRefGS", "summary": "Modeling reflections from 2D images is essential for photorealistic rendering\nand novel view synthesis. Recent approaches enhance Gaussian primitives with\nreflection-related material attributes to enable physically based rendering\n(PBR) with Gaussian Splatting. However, the material inference often lacks\nsufficient constraints, especially under limited environment modeling,\nresulting in illumination aliasing and reduced generalization. In this work, we\nrevisit the problem from a multi-view perspective and show that multi-view\nconsistent material inference with more physically-based environment modeling\nis key to learning accurate reflections with Gaussian Splatting. To this end,\nwe enforce 2D Gaussians to produce multi-view consistent material maps during\ndeferred shading. We also track photometric variations across views to identify\nhighly reflective regions, which serve as strong priors for reflection strength\nterms. To handle indirect illumination caused by inter-object occlusions, we\nfurther introduce an environment modeling strategy through ray tracing with\n2DGS, enabling photorealistic rendering of indirect radiance. Experiments on\nwidely used benchmarks show that our method faithfully recovers both\nillumination and geometry, achieving state-of-the-art rendering quality in\nnovel views synthesis.", "AI": {"tldr": "本文通过引入多视角一致的材质推断和基于2DGS的光线追踪环境建模，解决了高斯溅射中反射建模的挑战，实现了照片级的间接辐射渲染和新视角合成。", "motivation": "现有基于高斯基元（Gaussian primitives）的反射建模方法，在有限环境建模下，材质推断缺乏足够的约束，导致照明混叠和泛化能力下降。", "method": "该方法强制2D高斯在延迟着色过程中生成多视角一致的材质图。通过跟踪跨视角的光度变化来识别高反射区域，作为反射强度项的强先验。此外，引入了通过2DGS进行光线追踪的环境建模策略，以处理物体间遮挡引起的间接照明。", "result": "实验结果表明，该方法忠实地恢复了照明和几何形状，在新视角合成中实现了最先进的渲染质量。", "conclusion": "研究表明，多视角一致的材质推断和更物理化的环境建模是使用高斯溅射学习准确反射的关键，从而实现了逼真的渲染效果。"}}
{"id": "2510.11344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11344", "abs": "https://arxiv.org/abs/2510.11344", "authors": ["Hai Dang Nguyen", "Nguyen Dang Huy Pham", "The Minh Duc Nguyen", "Dac Thai Nguyen", "Hang Thi Nguyen", "Duong M. Nguyen"], "title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression", "comment": "Accepted for presentation at the 2025 Pacific Rim International\n  Conference on Artificial Intelligence (PRICAI 2025)", "summary": "Spatial Transcriptomics (ST) enables the measurement of gene expression while\npreserving spatial information, offering critical insights into tissue\narchitecture and disease pathology. Recent developments have explored the use\nof hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict\ntranscriptome-wide gene expression profiles through deep neural networks. This\ntask is commonly framed as a regression problem, where each input corresponds\nto a localized image patch extracted from the WSI. However, predicting spatial\ngene expression from histological images remains a challenging problem due to\nthe significant modality gap between visual features and molecular signals.\nRecent studies have attempted to incorporate both local and global information\ninto predictive models. Nevertheless, existing methods still suffer from two\nkey limitations: (1) insufficient granularity in local feature extraction, and\n(2) inadequate coverage of global spatial context. In this work, we propose a\nnovel framework, MMAP (Multi-MAgnification and Prototype-enhanced\narchitecture), that addresses both challenges simultaneously. To enhance local\nfeature granularity, MMAP leverages multi-magnification patch representations\nthat capture fine-grained histological details. To improve global contextual\nunderstanding, it learns a set of latent prototype embeddings that serve as\ncompact representations of slide-level information. Extensive experimental\nresults demonstrate that MMAP consistently outperforms all existing\nstate-of-the-art methods across multiple evaluation metrics, including Mean\nAbsolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation\nCoefficient (PCC).", "AI": {"tldr": "本研究提出MMAP框架，通过多放大倍数局部特征和原型增强的全局上下文，显著提升了从H&E染色全玻片图像预测空间转录组基因表达的准确性，超越了现有SOTA方法。", "motivation": "空间转录组学中，从H&E染色图像预测基因表达面临视觉特征与分子信号间的模态鸿沟。现有方法在局部特征提取的粒度不足和全局空间上下文覆盖不充分方面存在局限性。", "method": "本研究提出了MMAP（Multi-MAgnification and Prototype-enhanced architecture）框架。为增强局部特征粒度，MMAP利用多放大倍数的图像块表示。为改善全局上下文理解，它学习一组潜在的原型嵌入，作为玻片级信息的紧凑表示。", "result": "广泛的实验结果表明，MMAP在平均绝对误差（MAE）、均方误差（MSE）和皮尔逊相关系数（PCC）等多个评估指标上，持续优于所有现有的最先进方法。", "conclusion": "MMAP通过结合多放大倍数局部特征和原型增强的全局上下文，有效解决了现有方法在空间基因表达预测中的局限性，实现了卓越的性能。"}}
{"id": "2510.11305", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.11305", "abs": "https://arxiv.org/abs/2510.11305", "authors": ["Jean-Paul Travert", "Cédric Goeury", "Sébastien Boyaval", "Vito Bacchi", "Fabrice Zaoui"], "title": "Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation", "comment": null, "summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)\nimagery are crucial for calibrating and validating hydraulic models. This study\nuses SAR imagery to evaluate various preprocessing (especially speckle noise\nreduction), flood mapping, and water depth estimation methods. The impact of\nthe choice of method at different steps and its hyperparameters is studied by\nconsidering an ensemble of preprocessed images, flood maps, and water depth\nfields. The evaluation is conducted for two flood events on the Garonne River\n(France) in 2019 and 2021, using hydrodynamic simulations and in-situ\nobservations as reference data. Results show that the choice of speckle filter\nalters flood extent estimations with variations of several square kilometers.\nFurthermore, the selection and tuning of flood mapping methods also affect\nperformance. While supervised methods outperformed unsupervised ones, tuned\nunsupervised approaches (such as local thresholding or change detection) can\nachieve comparable results. The compounded uncertainty from preprocessing and\nflood mapping steps also introduces high variability in the water depth field\nestimates. This study highlights the importance of considering the entire\nprocessing pipeline, encompassing preprocessing, flood mapping, and water depth\nestimation methods and their associated hyperparameters. Rather than relying on\na single configuration, adopting an ensemble approach and accounting for\nmethodological uncertainty should be privileged. For flood mapping, the method\nchoice has the most influence. For water depth estimation, the most influential\nprocessing step was the flood map input resulting from the flood mapping step\nand the hyperparameters of the methods.", "AI": {"tldr": "本研究评估了合成孔径雷达（SAR）图像用于洪水测绘和水深估计的各种预处理、洪水测绘和水深估计方法，强调了方法选择和超参数对结果不确定性的影响，并建议采用集成方法。", "motivation": "利用SAR图像进行洪水测绘和水深估计对于水文模型的校准和验证至关重要。研究旨在评估不同方法在此过程中的表现及其对结果不确定性的影响。", "method": "研究通过考虑预处理图像、洪水图和水深场的集合，评估了SAR图像的各种预处理（特别是散斑噪声消除）、洪水测绘和水深估计方法。通过对2019年和2021年加龙河（法国）的两次洪水事件进行评估，并使用水动力模拟和现场观测数据作为参考。", "result": "散斑滤波器选择会导致洪水范围估计变化数平方公里；洪水测绘方法的选择和调整也会影响性能，尽管有监督方法优于无监督方法，但经过调整的无监督方法（如局部阈值或变化检测）可以达到可比结果。预处理和洪水测绘步骤的复合不确定性也导致水深估计场的高度变异性。对于洪水测绘，方法选择影响最大；对于水深估计，洪水测绘步骤产生的洪水图输入和方法的超参数影响最大。", "conclusion": "研究强调了考虑整个处理流程（包括预处理、洪水测绘和水深估计方法及其相关超参数）的重要性。建议采用集成方法并考虑方法学不确定性，而不是依赖单一配置。"}}
{"id": "2510.11296", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11296", "abs": "https://arxiv.org/abs/2510.11296", "authors": ["Lin Zhu", "Yifeng Yang", "Xinbing Wang", "Qinying Gu", "Nanyang Ye"], "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization", "comment": "Accepted by NeruIPS2025", "summary": "Recent approaches for vision-language models (VLMs) have shown remarkable\nsuccess in achieving fast downstream adaptation. When applied to real-world\ndownstream tasks, VLMs inevitably encounter both the in-distribution (ID) data\nand out-of-distribution (OOD) data. The OOD datasets often include both\ncovariate shifts (e.g., known classes with changes in image styles) and\nsemantic shifts (e.g., test-time unseen classes). This highlights the\nimportance of improving VLMs' generalization ability to covariate-shifted OOD\ndata, while effectively detecting open-set semantic-shifted OOD classes. In\nthis paper, inspired by the substantial energy change observed in closed-set\ndata when re-aligning vision-language modalities (specifically by directly\nreducing the maximum cosine similarity to a low value), we introduce a novel\nOOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the\nvanilla energy-based OOD score and provides a more reliable approach for OOD\ndetection. Furthermore, {\\Delta}Energy can simultaneously improve OOD\ngeneralization under covariate shifts, which is achieved by lower-bound\nmaximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to\nnot only enhance OOD detection but also yields a domain-consistent Hessian,\nwhich serves as a strong indicator for OOD generalization. Based on this\nfinding, we developed a unified fine-tuning framework that allows for improving\nVLMs' robustness in both OOD generalization and OOD detection. Extensive\nexperiments on challenging OOD detection and generalization benchmarks\ndemonstrate the superiority of our method, outperforming recent approaches by\n10% to 25% in AUROC.", "AI": {"tldr": "本文提出了一种名为ΔEnergy的新型OOD（Out-of-Distribution）分数，用于视觉-语言模型（VLMs）在协变量偏移下的OOD泛化和语义偏移下的OOD检测。通过ΔEnergy的下界最大化（EBM），该方法能显著提升OOD检测性能，并同时改善OOD泛化能力，在多个基准测试中表现优异。", "motivation": "视觉-语言模型（VLMs）在实际应用中不可避免地会遇到分布内（ID）数据和分布外（OOD）数据。OOD数据包括协变量偏移（已知类别但图像风格变化）和语义偏移（测试时未见类别）。因此，提升VLMs对协变量偏移OOD数据的泛化能力，并有效检测开放集语义偏移OOD类别，是当前研究的重要挑战。", "method": "受视觉-语言模态重新对齐时（通过直接降低最大余弦相似度）观察到的闭集数据中显著能量变化的启发，本文引入了一种新颖的OOD分数：ΔEnergy。为了同时改善协变量偏移下的OOD泛化，作者提出了ΔEnergy的下界最大化（EBM）。EBM在理论上被证明不仅能增强OOD检测，还能产生一个域一致的Hessian矩阵，作为OOD泛化的强指标。基于此发现，作者开发了一个统一的微调框架，以提高VLMs在OOD泛化和OOD检测方面的鲁棒性。", "result": "ΔEnergy在OOD检测方面显著优于传统的基于能量的OOD分数，并提供了一种更可靠的OOD检测方法。此外，ΔEnergy（通过EBM实现）能够同时改善协变量偏移下的OOD泛化。在具有挑战性的OOD检测和泛化基准测试上进行的广泛实验表明，该方法优于现有方法，AUROC指标提高了10%到25%。", "conclusion": "本文提出的ΔEnergy OOD分数及其下界最大化（EBM）方法，通过一个统一的微调框架，有效解决了视觉-语言模型在OOD泛化和OOD检测方面的挑战。该方法不仅在理论上得到证明，还在实验中取得了显著优于现有方法的性能提升，为VLMs的鲁棒性提供了新的解决方案。"}}
{"id": "2510.11341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11341", "abs": "https://arxiv.org/abs/2510.11341", "authors": ["Haomin Wang", "Jinhui Yin", "Qi Wei", "Wenguang Zeng", "Lixin Gu", "Shenglong Ye", "Zhangwei Gao", "Yaohui Wang", "Yanting Zhang", "Yuanqi Li", "Yanwen Guo", "Wenhai Wang", "Kai Chen", "Yu Qiao", "Hongjie Zhang"], "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models", "comment": null, "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.", "AI": {"tldr": "该研究利用多模态大语言模型（MLLMs）实现了统一的SVG理解、编辑和生成，并提出了InternSVG套件，包括大型数据集SAgoge、基准测试SArena和统一模型InternSVG，显著提升了SVG任务的性能。", "motivation": "通用SVG建模面临挑战，包括数据集碎片化、方法跨任务迁移性差以及处理结构复杂性困难。", "method": "研究利用多模态大语言模型（MLLMs）的强大迁移和泛化能力，提出了InternSVG家族，一个集成了数据、基准和模型的套件。核心是SAgoge，一个最大且最全面的SVG多模态数据集；SArena是一个配套的基准测试；InternSVG模型是一个统一的MLLM，采用SVG特定特殊token、基于子词的嵌入初始化和两阶段训练策略（从短静态SVG到长序列插图和复杂动画）。", "result": "在SArena和现有基准测试上的实验表明，InternSVG取得了显著的性能提升，并持续超越了领先的开源和专有同类模型。", "conclusion": "该研究通过统一的MLLM方法和InternSVG套件，有效解决了SVG建模的挑战，实现了SVG理解、编辑和生成的统一建模，并展示了优越的整体性能。"}}
{"id": "2510.11369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11369", "abs": "https://arxiv.org/abs/2510.11369", "authors": ["Shijie Zhao", "Xuanyu Zhang", "Weiqi Li", "Junlin Li", "Li Zhang", "Tianfan Xue", "Jian Zhang"], "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment", "comment": null, "summary": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time.", "AI": {"tldr": "本文揭示了基于推理的IQA模型泛化能力源于将视觉信息转化为紧凑的文本表示，并提出RALI算法，通过对比学习直接对齐图像与这些可泛化文本表示，在保持相似泛化性能的同时，大幅降低了模型参数和推理时间。", "motivation": "基于强化学习（RL）训练的推理IQA模型展现出卓越的泛化能力，但其潜在机制和关键驱动因素尚不明确。此外，这些模型推理能耗和延迟高，限制了其在特定场景的部署。", "method": "通过实验验证了RL训练使多模态大模型（MLLMs）将冗余视觉表示转化为紧凑、跨域对齐的文本表示，这是其泛化能力的来源。基于此洞察，提出了RALI算法，该算法采用对比学习，直接将图像与RL学习到的可泛化文本表示对齐，无需推理过程和加载大型语言模型（LLM）。", "result": "RALI框架在质量评分任务上实现了与基于推理模型相当的泛化性能，但所需的模型参数和推理时间不到后者的5%。", "conclusion": "基于推理的IQA模型泛化能力来源于将视觉表示转换为紧凑、跨域对齐的文本表示。RALI算法利用这一机制，通过直接对齐图像与这些文本表示，显著提升了效率，同时保持了出色的泛化性能。"}}
{"id": "2510.11449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11449", "abs": "https://arxiv.org/abs/2510.11449", "authors": ["Geoffery Agorku", "Sarah Hernandez", "Hayley Hames", "Cade Wagner"], "title": "Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization", "comment": null, "summary": "Maritime Domain Awareness (MDA) for inland waterways remains challenged by\ncooperative system vulnerabilities. This paper presents a novel framework that\nfuses high-resolution satellite imagery with vessel trajectory data from the\nAutomatic Identification System (AIS). This work addresses the limitations of\nAIS-based monitoring by leveraging non-cooperative satellite imagery and\nimplementing a fusion approach that links visual detections with AIS data to\nidentify dark vessels, validate cooperative traffic, and support advanced MDA.\nThe You Only Look Once (YOLO) v11 object detection model is used to detect and\ncharacterize vessels and barges by vessel type, barge cover, operational\nstatus, barge count, and direction of travel. An annotated data set of 4,550\ninstances was developed from $5{,}973~\\mathrm{mi}^2$ of Lower Mississippi River\nimagery. Evaluation on a held-out test set demonstrated vessel classification\n(tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1\nscore of 95.8\\%; barge cover (covered or uncovered) detection yielded an F1\nscore of 91.6\\%; operational status (staged or in motion) classification\nreached an F1 score of 99.4\\%. Directionality (upstream, downstream) yielded\n93.8\\% accuracy. The barge count estimation resulted in a mean absolute error\n(MAE) of 2.4 barges. Spatial transferability analysis across geographically\ndisjoint river segments showed accuracy was maintained as high as 98\\%. These\nresults underscore the viability of integrating non-cooperative satellite\nsensing with AIS fusion. This approach enables near-real-time fleet\ninventories, supports anomaly detection, and generates high-quality data for\ninland waterway surveillance. Future work will expand annotated datasets,\nincorporate temporal tracking, and explore multi-modal deep learning to further\nenhance operational scalability.", "AI": {"tldr": "本文提出了一种新颖的框架，通过融合高分辨率卫星图像和AIS数据来增强内陆水域的态势感知能力，有效识别“暗船”并验证合作交通。", "motivation": "内陆水域的海事领域态势感知（MDA）面临合作系统（如AIS）的脆弱性挑战，限制了基于AIS的监测能力，需要一种非合作的补充方法。", "method": "该研究融合了高分辨率卫星图像和AIS船舶轨迹数据。利用YOLO v11目标检测模型对船舶和驳船进行类型、驳船覆盖、运行状态、驳船数量和航向的检测和特征描述。开发了一个包含4,550个标注实例的自建数据集，涵盖了5,973平方英里的密西西比河下游图像。", "result": "在测试集上，船舶分类的F1分数为95.8%；驳船覆盖检测的F1分数为91.6%；运行状态分类的F1分数为99.4%；方向性检测准确率为93.8%。驳船数量估算的平均绝对误差（MAE）为2.4艘。跨地理不相连河段的空间可迁移性分析显示准确率高达98%。", "conclusion": "研究结果强调了将非合作卫星传感与AIS融合的可行性。该方法能够实现近实时船队盘点、支持异常检测，并为内陆水域监测生成高质量数据。未来的工作将扩展标注数据集、整合时间跟踪并探索多模态深度学习以进一步增强操作可扩展性。"}}
{"id": "2510.11417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11417", "abs": "https://arxiv.org/abs/2510.11417", "authors": ["Yijun Hu", "Bing Fan", "Xin Gu", "Haiqing Ren", "Dongfang Liu", "Heng Fan", "Libo Zhang"], "title": "Robust Ego-Exo Correspondence with Long-Term Memory", "comment": "Accepted by NeurIPS 2025", "summary": "Establishing object-level correspondence between egocentric and exocentric\nviews is essential for intelligent assistants to deliver precise and intuitive\nvisual guidance. However, this task faces numerous challenges, including\nextreme viewpoint variations, occlusions, and the presence of small objects.\nExisting approaches usually borrow solutions from video object segmentation\nmodels, but still suffer from the aforementioned challenges. Recently, the\nSegment Anything Model 2 (SAM 2) has shown strong generalization capabilities\nand excellent performance in video object segmentation. Yet, when simply\napplied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe\ndifficulties due to ineffective ego-exo feature fusion and limited long-term\nmemory capacity, especially for long videos. Addressing these problems, we\npropose a novel EEC framework based on SAM 2 with long-term memories by\npresenting a dual-memory architecture and an adaptive feature routing module\ninspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features\n(i) a Memory-View MoE module which consists of a dual-branch routing mechanism\nto adaptively assign contribution weights to each expert feature along both\nchannel and spatial dimensions, and (ii) a dual-memory bank system with a\nsimple yet effective compression strategy to retain critical long-term\ninformation while eliminating redundancy. In the extensive experiments on the\nchallenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new\nstate-of-the-art results and significantly outperforms existing methods and the\nSAM 2 baseline, showcasing its strong generalization across diverse scenarios.\nOur code and model are available at https://github.com/juneyeeHu/LM-EEC.", "AI": {"tldr": "本文提出了一种名为LM-EEC的新型框架，通过引入双内存架构和受MoE启发的自适应特征路由模块，解决了SAM 2在主观-客观视角对应任务中特征融合效率低下和长视频内存容量有限的问题，并在EgoExo4D基准测试中取得了最先进的性能。", "motivation": "智能助手需要建立主观视角和客观视角之间的物体级对应关系，以提供精确的视觉指导。然而，这项任务面临极端视角变化、遮挡和小物体等挑战。现有方法（包括简单应用SAM 2）在解决这些挑战时，特别是长视频中的特征融合和长期记忆能力方面表现不足。", "method": "本文提出了一个基于SAM 2的长期记忆主观-客观视角对应（EEC）框架，名为LM-EEC。该框架包含：(i) 一个Memory-View MoE模块，它具有双分支路由机制，能够沿通道和空间维度自适应地为每个专家特征分配贡献权重；(ii) 一个带有简单而有效压缩策略的双内存银行系统，用于保留关键的长期信息并消除冗余。", "result": "在具有挑战性的EgoExo4D基准测试中，LM-EEC方法取得了新的最先进（SOTA）结果，显著优于现有方法和SAM 2基线，展示了其在不同场景下的强大泛化能力。", "conclusion": "LM-EEC框架通过其创新的双内存架构和MoE启发式自适应特征路由模块，有效解决了主观-客观视角对应任务中的关键挑战，特别是长期记忆和特征融合问题，从而实现了卓越的性能和强大的泛化能力。"}}
{"id": "2510.11456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11456", "abs": "https://arxiv.org/abs/2510.11456", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui"], "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion", "comment": null, "summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume\nhigh-quality inputs. However, when handing degraded images, these methods\nheavily rely on manually switching between different pre-processing techniques.\nThis decoupling of degradation handling and image fusion leads to significant\nperformance degradation. In this paper, we propose a novel VLM-Guided\nDegradation-Coupled Fusion network (VGDCFusion), which tightly couples\ndegradation modeling with the fusion process and leverages vision-language\nmodels (VLMs) for degradation-aware perception and guided suppression.\nSpecifically, the proposed Specific-Prompt Degradation-Coupled Extractor\n(SPDCE) enables modality-specific degradation awareness and establishes a joint\nmodeling of degradation suppression and intra-modal feature extraction. In\nparallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates\ncross-modal degradation perception and couples residual degradation filtering\nwith complementary cross-modal feature fusion. Extensive experiments\ndemonstrate that our VGDCFusion significantly outperforms existing\nstate-of-the-art fusion approaches under various degraded image scenarios. Our\ncode is available at https://github.com/Lmmh058/VGDCFusion.", "AI": {"tldr": "本文提出了一种名为VGDCFusion的新型红外与可见光图像融合网络，它将图像降质处理与融合过程紧密结合，并利用视觉-语言模型（VLMs）进行降质感知和抑制，从而在降质图像场景下显著优于现有方法。", "motivation": "现有红外与可见光图像融合（IVIF）方法通常假设输入图像质量高。当处理降质图像时，这些方法过度依赖手动切换不同的预处理技术。这种降质处理与图像融合的解耦导致了显著的性能下降。", "method": "本文提出了一种新型的VLM引导降质耦合融合网络（VGDCFusion）。该网络将降质建模与融合过程紧密耦合，并利用视觉-语言模型（VLMs）进行降质感知和引导抑制。具体而言，提出的特定提示降质耦合提取器（SPDCE）实现了模态特定的降质感知，并建立了降质抑制与模态内特征提取的联合建模。同时，联合提示降质耦合融合（JPDCF）促进了跨模态降质感知，并将残余降质过滤与互补的跨模态特征融合相结合。", "result": "大量的实验表明，在各种降质图像场景下，我们的VGDCFusion显著优于现有的最先进融合方法。", "conclusion": "VGDCFusion通过将降质处理与图像融合紧密耦合，并利用VLM进行感知和抑制，有效解决了降质图像融合的挑战，并取得了卓越的性能。"}}
{"id": "2510.11508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11508", "abs": "https://arxiv.org/abs/2510.11508", "authors": ["Francesco Milano", "Jen Jen Chung", "Lionel Ott", "Roland Siegwart"], "title": "Towards Fast and Scalable Normal Integration using Continuous Components", "comment": "Accepted by the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV) 2026, first round. 17 pages, 9 figures, 6 tables", "summary": "Surface normal integration is a fundamental problem in computer vision,\ndealing with the objective of reconstructing a surface from its corresponding\nnormal map. Existing approaches require an iterative global optimization to\njointly estimate the depth of each pixel, which scales poorly to larger normal\nmaps. In this paper, we address this problem by recasting normal integration as\nthe estimation of relative scales of continuous components. By constraining\npixels belonging to the same component to jointly vary their scale, we\ndrastically reduce the number of optimization variables. Our framework includes\na heuristic to accurately estimate continuous components from the start, a\nstrategy to rebalance optimization terms, and a technique to iteratively merge\ncomponents to further reduce the size of the problem. Our method achieves\nstate-of-the-art results on the standard normal integration benchmark in as\nlittle as a few seconds and achieves one-order-of-magnitude speedup over\npixel-level approaches on large-resolution normal maps.", "AI": {"tldr": "本文将表面法线积分问题重新定义为连续分量的相对尺度估计，通过分组像素并共享尺度，显著加快了大规模法线图的表面重建速度，并取得了最先进的结果。", "motivation": "现有的表面法线积分方法需要迭代全局优化来估计每个像素的深度，这导致在大规模法线图上计算效率低下。", "method": "作者将法线积分重构为连续分量相对尺度的估计问题。通过限制属于同一分量的像素共同改变其尺度，大幅减少了优化变量的数量。该框架包括：用于初始准确估计连续分量的启发式方法、重新平衡优化项的策略，以及迭代合并分量以进一步缩小问题规模的技术。", "result": "该方法在标准法线积分基准测试中取得了最先进的结果，仅需几秒钟即可完成。在大分辨率法线图上，与像素级方法相比，实现了数量级上的速度提升。", "conclusion": "通过将法线积分问题转化为连续分量的尺度估计，并引入分量分组和合并策略，本文的方法显著提高了表面法线积分的速度，特别是在处理大型法线图时，同时保持了高精度。"}}
{"id": "2510.11473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11473", "abs": "https://arxiv.org/abs/2510.11473", "authors": ["Qing Li", "Huifang Feng", "Xun Gong", "Yu-Shen Liu"], "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment", "comment": "Accepted by NeurIPS 2025", "summary": "3D Gaussian Splatting has recently emerged as an efficient solution for\nhigh-quality and real-time novel view synthesis. However, its capability for\naccurate surface reconstruction remains underexplored. Due to the discrete and\nunstructured nature of Gaussians, supervision based solely on image rendering\nloss often leads to inaccurate geometry and inconsistent multi-view alignment.\nIn this work, we propose a novel method that enhances the geometric\nrepresentation of 3D Gaussians through view alignment (VA). Specifically, we\nincorporate edge-aware image cues into the rendering loss to improve surface\nboundary delineation. To enforce geometric consistency across views, we\nintroduce a visibility-aware photometric alignment loss that models occlusions\nand encourages accurate spatial relationships among Gaussians. To further\nmitigate ambiguities caused by lighting variations, we incorporate normal-based\nconstraints to refine the spatial orientation of Gaussians and improve local\nsurface estimation. Additionally, we leverage deep image feature embeddings to\nenforce cross-view consistency, enhancing the robustness of the learned\ngeometry under varying viewpoints and illumination. Extensive experiments on\nstandard benchmarks demonstrate that our method achieves state-of-the-art\nperformance in both surface reconstruction and novel view synthesis. The source\ncode is available at https://github.com/LeoQLi/VA-GS.", "AI": {"tldr": "本文提出了一种名为VA-GS的新方法，通过结合边缘感知图像线索、可见性感知光度对齐损失、基于法线的约束以及深度图像特征嵌入，显著提升了3D Gaussian Splatting在表面重建和新视角合成方面的几何精度和跨视角一致性。", "motivation": "3D Gaussian Splatting虽然在新视角合成方面表现出色，但其离散和非结构化的高斯性质，以及仅基于图像渲染损失的监督，导致其在准确表面重建方面能力不足，常出现不准确的几何形状和不一致的多视角对齐问题。", "method": "该方法通过“视角对齐（View Alignment, VA）”来增强3D高斯几何表示：1. 在渲染损失中加入边缘感知图像线索以改善表面边界描绘。2. 引入可见性感知光度对齐损失，建模遮挡并促进高斯之间的准确空间关系。3. 整合基于法线的约束以细化高斯空间方向并改善局部表面估计。4. 利用深度图像特征嵌入来强制跨视角一致性，增强学习几何在不同视角和光照下的鲁棒性。", "result": "在标准基准测试中，该方法在表面重建和新视角合成方面均达到了最先进的性能。", "conclusion": "所提出的方法通过多方面的几何和一致性约束，成功解决了3D Gaussian Splatting在准确表面重建方面的不足，并同时保持了其在新视角合成的优势。"}}
{"id": "2510.11549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11549", "abs": "https://arxiv.org/abs/2510.11549", "authors": ["Liu Yang", "Huiyu Duan", "Ran Tao", "Juntao Cheng", "Sijing Wu", "Yunhao Li", "Jing Liu", "Xiongkuo Min", "Guangtao Zhai"], "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?", "comment": null, "summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely\nadopted in VR, AR and embodied intelligence applications. While multi-modal\nlarge language models (MLLMs) have demonstrated remarkable performance on\nconventional 2D image and video understanding benchmarks, their ability to\ncomprehend the immersive environments captured by ODIs remains largely\nunexplored. To address this gap, we first present ODI-Bench, a novel\ncomprehensive benchmark specifically designed for omnidirectional image\nunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images and\nover 4,000 manually annotated question-answering (QA) pairs across 10\nfine-grained tasks, covering both general-level and spatial-level ODI\nunderstanding. Extensive experiments are conducted to benchmark 20\nrepresentative MLLMs, including proprietary and open-source models, under both\nclose-ended and open-ended settings. Experimental results reveal that current\nMLLMs still struggle to capture the immersive context provided by ODIs. To this\nend, we further introduce Omni-CoT, a training-free method which significantly\nenhances MLLMs' comprehension ability in the omnidirectional environment\nthrough chain-of-thought reasoning across both textual information and visual\ncues. Both the benchmark and the code will be released upon the publication.", "AI": {"tldr": "该研究提出了ODI-Bench，一个用于全景图像理解的综合基准，并评估了多模态大语言模型（MLLMs）在该领域的表现。结果显示当前MLLMs仍有不足，为此研究引入了Omni-CoT，一种无需训练的方法，通过思维链推理显著提升了MLLMs对全景环境的理解能力。", "motivation": "多模态大语言模型（MLLMs）在传统2D图像和视频理解方面表现出色，但其理解全景图像（ODIs）所捕捉的沉浸式环境的能力尚未得到充分探索。全景图像在VR、AR和具身智能应用中被广泛采用，因此评估和提升MLLMs对ODIs的理解能力至关重要。", "method": "1. 构建了ODI-Bench，一个专门用于全景图像理解的综合基准，包含2,000张高质量全景图像和超过4,000个手动标注的问答对，涵盖10项细粒度任务（包括通用级和空间级理解）。2. 在封闭式和开放式设置下，对20个代表性MLLMs（包括专有和开源模型）进行了广泛实验评估。3. 提出了一种名为Omni-CoT的无需训练方法，通过结合文本信息和视觉线索的思维链推理，显著增强了MLLMs在全景环境中的理解能力。", "result": "1. 实验结果表明，当前的多模态大语言模型（MLLMs）在捕捉全景图像提供的沉浸式上下文方面仍然存在困难。2. 所提出的Omni-CoT方法显著提升了MLLMs在全景环境中的理解能力。", "conclusion": "当前的多模态大语言模型在全景图像理解方面仍面临挑战。ODI-Bench为该领域提供了一个重要的评估工具，而Omni-CoT则是一种有效且无需训练的方法，能够显著提升MLLMs对全景环境的理解能力，为未来研究指明了方向。"}}
{"id": "2510.11538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11538", "abs": "https://arxiv.org/abs/2510.11538", "authors": ["Chaofan Gan", "Zicheng Zhao", "Yuanpeng Tu", "Xi Chen", "Ziran Qin", "Tieyuan Chen", "Mehrtash Harandi", "Weiyao Lin"], "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor visual generation. Recent observations reveal \\emph{Massive Activations}\n(MAs) in their internal feature maps, yet their function remains poorly\nunderstood. In this work, we systematically investigate these activations to\nelucidate their role in visual generation. We found that these massive\nactivations occur across all spatial tokens, and their distribution is\nmodulated by the input timestep embeddings. Importantly, our investigations\nfurther demonstrate that these massive activations play a key role in local\ndetail synthesis, while having minimal impact on the overall semantic content\nof output. Building on these insights, we propose \\textbf{D}etail\n\\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance\nstrategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG\nconstructs a degraded ``detail-deficient'' model by disrupting MAs and\nleverages it to guide the original network toward higher-quality detail\nsynthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),\nenabling further refinements of fine-grained details. Extensive experiments\ndemonstrate that our DG consistently improves fine-grained detail quality\nacross various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).", "AI": {"tldr": "本研究系统地调查了扩散Transformer（DiTs）中的“大量激活”（MAs），发现它们对局部细节合成至关重要。在此基础上，提出了一种无需训练的自引导策略——细节引导（DG），通过利用MAs来增强DiTs的局部细节保真度，并能与无分类器引导（CFG）无缝集成，显著提升细节质量。", "motivation": "扩散Transformer（DiTs）已成为强大的视觉生成骨干网络，但其内部特征图中的“大量激活”（MAs）现象虽已被观察到，其功能和作用机制仍不清楚。", "method": "本研究首先系统地调查了DiTs中MAs的特性及其在视觉生成中的作用。在此基础上，提出了一种名为“细节引导”（DG）的MAs驱动、无需训练的自引导策略。DG通过破坏MAs来构建一个“细节缺陷”的降级模型，并利用该模型引导原始网络实现更高质量的细节合成。DG可以与无分类器引导（CFG）无缝集成，以进一步细化细节。", "result": "研究发现MAs遍布所有空间tokens，其分布受输入时间步嵌入调制。重要的是，MAs在局部细节合成中扮演关键角色，而对输出的整体语义内容影响甚微。实验证明，DG能持续改进各种预训练DiTs（如SD3、SD3.5和Flux）的细粒度细节质量。", "conclusion": "DiTs中的“大量激活”（MAs）是局部细节合成的关键。本研究提出的细节引导（DG）策略，通过利用MAs，能够有效且无需训练地增强DiTs的局部细节保真度，并与现有引导方法兼容，为高质量视觉生成提供了新途径。"}}
{"id": "2510.11553", "categories": ["cs.CV", "68T07 (Primary) 68T45, 62H30, 62P10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.11553", "abs": "https://arxiv.org/abs/2510.11553", "authors": ["Nikolay Nechaev", "Evgenia Przhezdzetskaya", "Viktor Gombolevskiy", "Dmitry Umerenkov", "Dmitry Dylov"], "title": "How many samples to label for an application given a foundation model? Chest X-ray classification study", "comment": "8 pages, 5 figures", "summary": "Chest X-ray classification is vital yet resource-intensive, typically\ndemanding extensive annotated data for accurate diagnosis. Foundation models\nmitigate this reliance, but how many labeled samples are required remains\nunclear. We systematically evaluate the use of power-law fits to predict the\ntraining size necessary for specific ROC-AUC thresholds. Testing multiple\npathologies and foundation models, we find XrayCLIP and XraySigLIP achieve\nstrong performance with significantly fewer labeled examples than a ResNet-50\nbaseline. Importantly, learning curve slopes from just 50 labeled cases\naccurately forecast final performance plateaus. Our results enable\npractitioners to minimize annotation costs by labeling only the essential\nsamples for targeted performance.", "AI": {"tldr": "本研究评估了在胸部X射线分类中使用幂律拟合来预测达到特定性能阈值（ROC-AUC）所需的标记样本数量。结果表明，基础模型（如XrayCLIP和XraySigLIP）比传统模型（ResNet-50）需要显著更少的标记数据，并且仅通过50个标记样本的早期学习曲线斜率就能准确预测最终性能，从而有效降低标注成本。", "motivation": "胸部X射线分类对于准确诊断至关重要，但通常需要大量的带注释数据。基础模型可以缓解这种对大量数据的依赖，但目前尚不清楚究竟需要多少标记样本才能达到理想的性能。", "method": "研究系统地评估了使用幂律拟合来预测达到特定ROC-AUC阈值所需的训练样本数量。测试了多种病理和基础模型（XrayCLIP和XraySigLIP），并与ResNet-50基线模型进行了比较。关键是，通过仅50个标记病例的学习曲线斜率来预测最终性能平台。", "result": "XrayCLIP和XraySigLIP基础模型在性能上表现出色，且所需的标记样本数量远少于ResNet-50基线模型。此外，仅从50个标记病例中提取的学习曲线斜率能够准确预测最终的性能平台。", "conclusion": "研究结果表明，通过幂律拟合和早期学习曲线分析，从业者可以预测达到目标性能所需的最小标记样本数量，从而显著降低标注成本并优化资源利用。"}}
{"id": "2510.11520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11520", "abs": "https://arxiv.org/abs/2510.11520", "authors": ["Kedi Ying", "Ruiping Liu", "Chongyan Chen", "Mingzhe Tao", "Hao Shi", "Kailun Yang", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance", "comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track. Data and\n  Code: https://github.com/KediYing/mmWalk", "summary": "Walking assistance in extreme or complex environments remains a significant\nchallenge for people with blindness or low vision (BLV), largely due to the\nlack of a holistic scene understanding. Motivated by the real-world needs of\nthe BLV community, we build mmWalk, a simulated multi-modal dataset that\nintegrates multi-view sensor and accessibility-oriented features for outdoor\nsafe navigation. Our dataset comprises 120 manually controlled,\nscenario-categorized walking trajectories with 62k synchronized frames. It\ncontains over 559k panoramic images across RGB, depth, and semantic modalities.\nFurthermore, to emphasize real-world relevance, each trajectory involves\noutdoor corner cases and accessibility-specific landmarks for BLV users.\nAdditionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual\nquestion-answer triplets across 9 categories tailored for safe and informed\nwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)\nusing zero- and few-shot settings and found they struggle with our risk\nassessment and navigational tasks. We validate our mmWalk-finetuned model on\nreal-world datasets and show the effectiveness of our dataset for advancing\nmulti-modal walking assistance.", "AI": {"tldr": "该研究构建了mmWalk，一个模拟多模态数据集，并创建了mmWalkVQA视觉问答基准，旨在解决视障人士在复杂环境中的步行辅助挑战，并推动多模态步行辅助技术的发展。", "motivation": "视障人士在极端或复杂环境中步行面临重大挑战，主要原因是缺乏对场景的整体理解。这项研究的动机源于视障群体的真实世界需求。", "method": "研究构建了mmWalk，一个模拟多模态数据集，包含120条人工控制、场景分类的步行轨迹，以及62k同步帧，超过559k张RGB、深度和语义模态的全景图像，并强调户外特殊情况和无障碍地标。此外，还生成了mmWalkVQA，一个包含超过69k视觉问答对的VQA基准，专为安全和知情的步行辅助设计。研究评估了最先进的视觉-语言模型（VLMs），并验证了mmWalk微调模型在真实世界数据集上的效果。", "result": "评估发现，最先进的VLMs在mmWalkVQA的风险评估和导航任务上表现不佳。经过mmWalk微调的模型在真实世界数据集上显示出有效性。", "conclusion": "mmWalk数据集对于推动多模态步行辅助技术的发展具有显著的有效性，能够帮助视障人士更好地理解复杂环境。"}}
{"id": "2510.11509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11509", "abs": "https://arxiv.org/abs/2510.11509", "authors": ["Ruiping Liu", "Junwei Zheng", "Yufan Chen", "Zirui Wang", "Kunyu Peng", "Kailun Yang", "Jiaming Zhang", "Marc Pollefeys", "Rainer Stiefelhagen"], "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model", "comment": "Accepted to NeurIPS 2025 Datasets and Benchmarks Track. Dataset and\n  Code: https://github.com/RuipingL/Situat3DChange", "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.", "AI": {"tldr": "本文介绍了Situat3DChange数据集和SCReasoner方法，旨在提升多模态大模型（MLLMs）对动态三维场景和情境变化的理解能力，克服现有数据集的局限性。", "motivation": "当前的3D数据集和评估基准要么只关注动态场景，要么只关注孤立的动态情境，导致对物理环境动态变化的理解不完整。研究旨在弥补这一不足，提供更全面的情境感知变化理解。", "method": "研究构建了Situat3DChange数据集，包含12.1万个问答对、3.6万个变化描述和1.7万个重排指令，基于1.1万个人类对环境变化的观察，结合自我中心和异物中心视角、分类和坐标空间关系，并利用LLM进行整合。为解决微小变化的3D点云对比难题，提出了SCReasoner，一个高效的3D MLLM方法，能在参数开销和语言解码器token需求极小的情况下有效比较点云。", "result": "在Situat3DChange任务上的全面评估揭示了MLLMs在动态场景和情境理解方面的进展与局限性。额外的数据扩缩和跨域迁移实验表明，Situat3DChange作为MLLMs的训练数据集具有任务无关的有效性。", "conclusion": "Situat3DChange数据集和SCReasoner方法为动态三维环境和情境变化的理解提供了新的基准和工具，推动了MLLMs在该领域的发展，并揭示了未来的研究方向。"}}
{"id": "2510.11565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11565", "abs": "https://arxiv.org/abs/2510.11565", "authors": ["Aniket Gupta", "Hanhui Wang", "Charles Saunders", "Aruni RoyChowdhury", "Hanumant Singh", "Huaizu Jiang"], "title": "SNAP: Towards Segmenting Anything in Any Point Cloud", "comment": "Project Page, https://neu-vi.github.io/SNAP/", "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/", "AI": {"tldr": "SNAP是一个统一的交互式3D点云分割模型，支持点和文本提示，并能跨室内、室外和空中等多种领域进行泛化，通过域自适应归一化防止负迁移，并在多项基准测试中达到最先进或具有竞争力的性能。", "motivation": "当前的交互式3D点云分割方法通常局限于单一领域（室内或室外）和单一用户交互形式（空间点击或文本提示）。此外，在多个数据集上训练常导致负迁移，使得工具缺乏泛化能力。", "method": "本文提出了SNAP模型，通过在涵盖室内、室外和空中环境的7个数据集上进行训练，实现了跨域泛化。它采用域自适应归一化来防止负迁移。对于文本提示分割，模型自动生成掩码提议并将其与文本查询的CLIP嵌入进行匹配，从而实现全景和开放词汇分割。", "result": "SNAP在8个空间提示零样本基准测试中的8个上实现了最先进的性能，并在所有5个文本提示基准测试中展示了具有竞争力的结果。实验表明，SNAP始终能提供高质量的分割结果。", "conclusion": "研究结果表明，统一模型可以匹敌甚至超越专门的领域特定方法，为可扩展的3D标注提供了一个实用的工具。"}}
{"id": "2510.11567", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11567", "abs": "https://arxiv.org/abs/2510.11567", "authors": ["Denis Zavadski", "Damjan Kalšan", "Tim Küchler", "Haebom Lee", "Stefan Roth", "Carsten Rother"], "title": "A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation", "comment": null, "summary": "Synthetic datasets are widely used for training urban scene recognition\nmodels, but even highly realistic renderings show a noticeable gap to real\nimagery. This gap is particularly pronounced when adapting to a specific target\ndomain, such as Cityscapes, where differences in architecture, vegetation,\nobject appearance, and camera characteristics limit downstream performance.\nClosing this gap with more detailed 3D modelling would require expensive asset\nand scene design, defeating the purpose of low-cost labelled data. To address\nthis, we present a new framework that adapts an off-the-shelf diffusion model\nto a target domain using only imperfect pseudo-labels. Once trained, it\ngenerates high-fidelity, target-aligned images from semantic maps of any\nsynthetic dataset, including low-effort sources created in hours rather than\nmonths. The method filters suboptimal generations, rectifies image-label\nmisalignments, and standardises semantics across datasets, transforming weak\nsynthetic data into competitive real-domain training sets. Experiments on five\nsynthetic datasets and two real target datasets show segmentation gains of up\nto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly\nconstructed synthetic datasets as effective as high-effort, time-intensive\nsynthetic datasets requiring extensive manual design. This work highlights a\nvaluable collaborative paradigm where fast semantic prototyping, combined with\ngenerative models, enables scalable, high-quality training data creation for\nurban scene understanding.", "AI": {"tldr": "本文提出一个新框架，利用不完美的伪标签将现成的扩散模型适应到特定目标领域，从而将低成本的合成数据转化为高质量、与真实领域对齐的训练集，以弥补合成数据与真实数据之间的差距。", "motivation": "合成数据集在城市场景识别模型训练中广泛应用，但即使是高度真实的渲染图与真实图像之间也存在显著差距，特别是在适应特定目标领域时。这种差距限制了下游性能，而通过更详细的3D建模来弥合差距则成本过高，违背了低成本标注数据的初衷。", "method": "该框架通过仅使用不完美的伪标签，将一个现成的扩散模型适应到目标领域。一旦训练完成，它能从任何合成数据集的语义图中生成高保真、与目标对齐的图像。该方法还包括过滤次优生成、纠正图像-标签错位以及标准化跨数据集的语义。", "result": "在五个合成数据集和两个真实目标数据集上的实验表明，相比最先进的翻译方法，分割mIoU提高了高达+8.0%pt。这使得快速构建的合成数据集与高投入、耗时的合成数据集一样有效。", "conclusion": "这项工作强调了一种有价值的协作范式，即快速语义原型设计与生成模型相结合，能够为城市场景理解实现可扩展、高质量的训练数据创建。"}}
{"id": "2510.11576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11576", "abs": "https://arxiv.org/abs/2510.11576", "authors": ["Walid Elbarz", "Mohamed Bourriz", "Hicham Hajji", "Hamd Ait Abdelali", "François Bourzeix"], "title": "Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping", "comment": "Being reviewed for WHISPERS conference ( Workshop on Hyperspectral\n  Image and Signal Processing: Evolution in Remote Sensing )", "summary": "Foundation models are transforming Earth observation, but their potential for\nhyperspectral crop mapping remains underexplored. This study benchmarks three\nfoundation models for cereal crop mapping using hyperspectral imagery:\nHyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth\ndataset (a large multitemporal hyperspectral archive). Models were fine-tuned\non manually labeled data from a training region and evaluated on an independent\ntest region. Performance was measured with overall accuracy (OA), average\naccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),\nDOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of\n93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved\n91%, highlighting the importance of model architecture for strong\ngeneralization across geographic regions and sensor platforms. These results\nprovide a systematic evaluation of foundation models for operational\nhyperspectral crop mapping and outline directions for future model development.", "AI": {"tldr": "本研究评估了三种基础模型在利用高光谱图像进行谷物作物测绘方面的性能，其中SpectralEarth预训练的Vision Transformer表现最佳，突出了模型架构对泛化能力的重要性。", "motivation": "尽管基础模型正在改变地球观测领域，但它们在高光谱作物测绘方面的潜力尚未得到充分探索。", "method": "研究基准测试了HyperSigma、DOFA以及在SpectralEarth数据集（一个大型多时相高光谱档案）上预训练的Vision Transformers三种基础模型，用于谷物作物测绘。模型在手动标记的训练区域数据上进行微调，并在独立的测试区域进行评估。性能指标包括总体准确率（OA）、平均准确率（AA）和F1分数。此外，还测试了一个从头开始训练的紧凑型SpectralEarth变体。", "result": "HyperSigma的OA为34.5%（+/- 1.8%），DOFA达到62.6%（+/- 3.5%），而SpectralEarth模型实现了93.5%（+/- 0.8%）的OA。一个从头开始训练的紧凑型SpectralEarth变体也达到了91%的OA，这强调了模型架构对于跨地理区域和传感器平台强大泛化能力的重要性。", "conclusion": "这些结果为操作性高光谱作物测绘的基础模型提供了系统性评估，并为未来的模型开发指明了方向。"}}
{"id": "2510.11579", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11579", "abs": "https://arxiv.org/abs/2510.11579", "authors": ["Hongyu Zhu", "Lin Chen", "Mounim A. El-Yacoubi", "Mingsheng Shang"], "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis", "comment": "Under Review", "summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.", "AI": {"tldr": "针对多模态情感分析（MSA）数据稀缺问题，本文提出了MS-Mix，一个情感敏感的增强框架，通过情感感知样本选择、情感强度引导的混合以及情感对齐损失，显著提高了模型的泛化能力和性能。", "motivation": "深度学习模型在多模态情感分析中受限于稀缺的多模态标注数据。直接应用基于Mixup的数据增强方法会导致标签模糊和语义不一致，因为缺乏情感感知的混合机制。", "method": "MS-Mix框架包含三个关键组件：(1) 情感感知样本选择（SASS）策略，避免混合情感矛盾的样本导致的语义混淆。(2) 情感强度引导（SIG）模块，利用多头自注意力动态计算基于情感强度的模态特定混合比例。(3) 情感对齐损失（SAL），对齐模态间的预测分布，并结合基于KL散度的损失项，联合训练情感强度预测器和骨干网络。", "result": "在三个基准数据集和六个最先进的骨干网络上进行的广泛实验表明，MS-Mix始终优于现有方法，为鲁棒的多模态情感增强设定了新标准。", "conclusion": "MS-Mix通过其自适应、情感敏感的增强框架，有效克服了多模态情感分析中数据稀缺和直接Mixup应用的问题，显著提升了模型的性能和泛化能力。"}}
{"id": "2510.11605", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11605", "abs": "https://arxiv.org/abs/2510.11605", "authors": ["Leonard Bruns", "Axel Barroso-Laguna", "Tommaso Cavallari", "Áron Monszpart", "Sowmya Munukutla", "Victor Adrian Prisacariu", "Eric Brachmann"], "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training", "comment": "ICCV 2025, Project page: https://nianticspatial.github.io/ace-g/", "summary": "Scene coordinate regression (SCR) has established itself as a promising\nlearning-based approach to visual relocalization. After mere minutes of\nscene-specific training, SCR models estimate camera poses of query images with\nhigh accuracy. Still, SCR methods fall short of the generalization capabilities\nof more classical feature-matching approaches. When imaging conditions of query\nimages, such as lighting or viewpoint, are too different from the training\nviews, SCR models fail. Failing to generalize is an inherent limitation of\nprevious SCR frameworks, since their training objective is to encode the\ntraining views in the weights of the coordinate regressor itself. The regressor\nessentially overfits to the training views, by design. We propose to separate\nthe coordinate regressor and the map representation into a generic transformer\nand a scene-specific map code. This separation allows us to pre-train the\ntransformer on tens of thousands of scenes. More importantly, it allows us to\ntrain the transformer to generalize from mapping images to unseen query images\nduring pre-training. We demonstrate on multiple challenging relocalization\ndatasets that our method, ACE-G, leads to significantly increased robustness\nwhile keeping the computational footprint attractive.", "AI": {"tldr": "场景坐标回归（SCR）在视觉重定位中表现出色，但泛化能力不足，尤其是在成像条件与训练视图差异较大时。本文提出ACE-G方法，将坐标回归器与地图表示分离为通用变换器和场景特定地图代码，并通过大规模预训练提升了模型对未见查询图像的泛化能力和鲁棒性。", "motivation": "现有的场景坐标回归（SCR）方法在训练视图与查询图像的成像条件（如光照、视角）差异过大时会失效，这是因为其训练目标导致回归器过度拟合训练视图，缺乏泛化能力。", "method": "ACE-G方法将坐标回归器和地图表示分离：一个通用的变换器（transformer）和一个场景特定的地图代码。该变换器可以在数万个场景上进行预训练，并在此过程中学习如何从映射图像泛化到未见的查询图像。", "result": "在多个具有挑战性的重定位数据集上，ACE-G方法显著提高了鲁棒性，同时保持了较低的计算开销。", "conclusion": "通过将坐标回归器与地图表示解耦并进行大规模预训练，ACE-G成功解决了传统SCR框架泛化能力不足的固有局限性，实现了更强大的视觉重定位鲁棒性。"}}
{"id": "2510.11647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11647", "abs": "https://arxiv.org/abs/2510.11647", "authors": ["Yinan Chen", "Jiangning Zhang", "Teng Hu", "Yuxiang Zeng", "Zhucun Xue", "Qingdong He", "Chengjie Wang", "Yong Liu", "Xiaobin Hu", "Shuicheng Yan"], "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment", "comment": "Equal contributions from first two authors. Project page:\n  https://ryanchenyn.github.io/projects/IVEBench Code:\n  https://github.com/RyanChenYN/IVEBench Dataset:\n  https://huggingface.co/datasets/Coraxor/IVEBench", "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.", "AI": {"tldr": "IVEBench是一个专门用于评估指令引导视频编辑的新型基准套件，解决了现有基准在多样性、任务覆盖和评估指标方面的不足。", "motivation": "现有视频编辑基准无法充分支持指令引导视频编辑的评估，存在来源多样性有限、任务覆盖范围窄和评估指标不完整等问题。", "method": "引入IVEBench，包含600个高质量源视频（涵盖7个语义维度，长度32-1024帧），8大类35个子类编辑任务（提示由大型语言模型生成并经专家审查），以及一个三维评估协议（视频质量、指令依从性、视频保真度），整合了传统指标和多模态大型语言模型评估。", "result": "广泛实验证明IVEBench在基准测试最先进的指令引导视频编辑方法方面的有效性，能够提供全面且与人类评估一致的评估结果。", "conclusion": "IVEBench为指令引导视频编辑的系统评估提供了一个现代、全面的解决方案，能够有效衡量现有方法的性能并指导未来研究。"}}
{"id": "2510.11613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11613", "abs": "https://arxiv.org/abs/2510.11613", "authors": ["Feng Zhang", "Haoyou Deng", "Zhiqiang Li", "Lida Li", "Bin Xu", "Qingbo Lu", "Zisheng Cao", "Minchen Wei", "Changxin Gao", "Nong Sang", "Xiang Bai"], "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network", "comment": "accepted by TPAMI 2025", "summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.", "AI": {"tldr": "本文提出LLF-LUT++，一个金字塔网络，通过结合全局和局部操作，利用拉普拉斯金字塔分解和重建，实现高分辨率图像的快速处理和卓越的图像增强性能。", "motivation": "现有的图像增强方法要么性能强大但计算量大，无法部署在边缘设备上；要么计算效率高但性能不足，无法满足实际应用需求。因此，需要一种兼顾性能和效率的解决方案。", "method": "LLF-LUT++金字塔网络通过闭合形式的拉普拉斯金字塔分解和重建集成全局和局部操作。它利用图像自适应3D LUT处理下采样图像的全局色调特性，并通过两种权重融合策略实现粗略的全局图像增强。为此，设计了一个空间频率变换器权重预测器，通过频率特征提取独特的权重。此外，它应用局部拉普拉斯滤波器自适应地细化高频分量中的边缘细节。", "result": "LLF-LUT++在HDR+数据集上将PSNR提高了2.64 dB，并且进一步缩短了运行时间，在单个GPU上处理4K分辨率图像仅需13毫秒。在两个基准数据集上的广泛实验结果表明，该方法优于现有最先进的方法。", "conclusion": "LLF-LUT++通过结合全局和局部操作，实现了卓越的图像增强性能和高效的处理速度，有效解决了现有方法在性能和效率之间权衡的局限性。"}}
{"id": "2510.11606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11606", "abs": "https://arxiv.org/abs/2510.11606", "authors": ["Yicheng Xu", "Yue Wu", "Jiashuo Yu", "Ziang Yan", "Tianxiang Jiang", "Yinan He", "Qingsong Zhao", "Kai Chen", "Yu Qiao", "Limin Wang", "Manabu Okumura", "Yi Wang"], "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning", "comment": "Data & Code: https://github.com/OpenGVLab/ExpVid", "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.", "AI": {"tldr": "多模态大语言模型（MLLMs）在科学发现中有潜力，但现有基准无法评估其在精细、长周期实验室工作中的真实能力。本文引入ExpVid，首个用于评估MLLMs科学实验视频的基准，包含三级任务。评估发现MLLMs在精细识别、状态跟踪和结果关联方面表现不佳，专有模型与开源模型之间存在显著差距。", "motivation": "多模态大语言模型（MLLMs）有望通过解释复杂实验程序来加速科学发现。然而，现有基准忽略了真实实验室工作（特别是湿实验室环境）的精细和长周期性质，导致人们对其真实能力知之甚少。", "method": "引入了ExpVid，这是第一个专门用于系统评估MLLMs科学实验视频的基准。ExpVid从同行评审的视频出版物中整理而来，并设有一个模拟科学过程的三级任务层次结构：(1) 工具、材料和动作的精细感知；(2) 步骤顺序和完整性的程序理解；(3) 将完整实验与已发表结论联系起来的科学推理。采用结合自动化生成和多学科专家验证的视觉中心注释流程，确保任务需要视觉基础。评估了19个领先的MLLMs在ExpVid上的表现。", "result": "评估发现，虽然MLLMs在粗粒度识别方面表现出色，但它们在区分精细细节、跟踪随时间变化的状态以及将实验程序与科学结果联系起来方面存在困难。研究结果揭示了专有模型和开源模型之间存在显著的性能差距，尤其是在高阶推理方面。", "conclusion": "ExpVid不仅提供了一个诊断工具，还为开发能够成为科学实验中值得信赖的合作伙伴的MLLMs指明了发展方向。"}}
{"id": "2510.11704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11704", "abs": "https://arxiv.org/abs/2510.11704", "authors": ["Sarah Harkins Dayton", "Hayden Everett", "Ioannis Schizas", "David L. Boothe Jr.", "Vasileios Maroulas"], "title": "Bayesian Topological Convolutional Neural Nets", "comment": null, "summary": "Convolutional neural networks (CNNs) have been established as the main\nworkhorse in image data processing; nonetheless, they require large amounts of\ndata to train, often produce overconfident predictions, and frequently lack the\nability to quantify the uncertainty of their predictions. To address these\nconcerns, we propose a new Bayesian topological CNN that promotes a novel\ninterplay between topology-aware learning and Bayesian sampling. Specifically,\nit utilizes information from important manifolds to accelerate training while\nreducing calibration error by placing prior distributions on network parameters\nand properly learning appropriate posteriors. One important contribution of our\nwork is the inclusion of a consistency condition in the learning cost, which\ncan effectively modify the prior distributions to improve the performance of\nour novel network architecture. We evaluate the model on benchmark image\nclassification datasets and demonstrate its superiority over conventional CNNs,\nBayesian neural networks (BNNs), and topological CNNs. In particular, we supply\nevidence that our method provides an advantage in situations where training\ndata is limited or corrupted. Furthermore, we show that the new model allows\nfor better uncertainty quantification than standard BNNs since it can more\nreadily identify examples of out-of-distribution data on which it has not been\ntrained. Our results highlight the potential of our novel hybrid approach for\nmore efficient and robust image classification.", "AI": {"tldr": "本文提出了一种新型贝叶斯拓扑卷积神经网络（CNN），通过结合拓扑感知学习和贝叶斯采样，解决了传统CNN训练数据量大、预测过度自信和缺乏不确定性量化的问题，尤其在数据有限或损坏的情况下表现优越。", "motivation": "传统卷积神经网络（CNN）存在以下问题：需要大量数据进行训练；预测往往过于自信；以及缺乏对其预测不确定性的量化能力。", "method": "本文提出了一种新型贝叶斯拓扑CNN。它通过以下方式工作：1) 利用重要流形信息加速训练并减少校准误差；2) 对网络参数设置先验分布并学习适当的后验分布；3) 在学习成本中引入一致性条件，以有效修改先验分布，从而提高网络性能。该方法结合了拓扑感知学习和贝叶斯采样。", "result": "该模型在基准图像分类数据集上表现出优越性，超越了传统CNN、贝叶斯神经网络（BNN）和拓扑CNN。特别是在训练数据有限或损坏的情况下，该方法具有明显优势。此外，新模型比标准BNN能更好地量化不确定性，因为它能更容易识别出未训练过的分布外数据。", "conclusion": "研究结果强调了这种新型混合方法在实现更高效和更鲁棒的图像分类方面的潜力。"}}
{"id": "2510.11690", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11690", "abs": "https://arxiv.org/abs/2510.11690", "authors": ["Boyang Zheng", "Nanye Ma", "Shengbang Tong", "Saining Xie"], "title": "Diffusion Transformers with Representation Autoencoders", "comment": "Technical Report; Project Page: https://rae-dit.github.io/", "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.", "AI": {"tldr": "本文提出用表示自编码器（RAE）替代扩散Transformer（DiT）中过时的VAE，以提供高质量重建和语义丰富的潜在空间，从而提升生成性能。", "motivation": "目前的扩散Transformer（DiT）普遍依赖原始VAE编码器，存在主干网络过时、潜在空间维度低、表示能力弱等问题，这些限制了生成质量和模型架构的简洁性。", "method": "本文提出用预训练的表示编码器（如DINO, SigLIP, MAE）与训练过的解码器结合，形成表示自编码器（RAEs），以替代传统的VAE。RAE旨在提供高质量重建和语义丰富的潜在空间。针对高维潜在空间中DiT操作的挑战，作者分析了其来源并提出了理论上合理的解决方案。实验中使用了配备轻量级宽DDT头的DiT变体。", "result": "该方法实现了更快的收敛速度，且无需辅助表示对齐损失。在ImageNet上取得了强大的图像生成结果：256x256无引导时FID为1.51，256x256和512x512有引导时FID为1.13。", "conclusion": "RAE具有显著优势，应成为扩散Transformer训练的新默认组件。"}}
{"id": "2510.11712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11712", "abs": "https://arxiv.org/abs/2510.11712", "authors": ["Haoran Feng", "Dizhe Zhang", "Xiangtai Li", "Bo Du", "Lu Qi"], "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training", "comment": "https://fenghora.github.io/DiT360-Page/", "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.", "AI": {"tldr": "DiT360是一个基于DiT的全景图像生成框架，通过在透视和全景数据上进行混合训练，并采用数据中心的方法解决全景图像生成中的几何保真度和真实感问题。", "motivation": "全景图像生成在几何保真度和真实感方面的不足，主要归因于缺乏大规模、高质量的真实世界全景数据。这与以往侧重模型设计的方法不同，DiT360采取了数据中心的视角。", "method": "DiT360是一个基于DiT的框架，采用混合训练。它包含用于域间转换和域内增强的关键模块，应用于预VAE图像级别和后VAE令牌级别。在图像级别，通过透视图像引导和全景细化融入跨域知识。在令牌级别，应用混合监督，包括用于边界连续性的循环填充、用于旋转鲁棒性的偏航损失和用于失真感知的立方体损失。", "result": "在文本到全景、图像修复和图像外扩任务中，DiT360在十一个量化指标上实现了更好的边界一致性和图像保真度。", "conclusion": "DiT360通过其数据中心的混合训练方法以及在图像和令牌级别引入的特定模块，有效提升了全景图像生成的质量，解决了边界一致性和图像保真度的问题。"}}
{"id": "2510.11715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11715", "abs": "https://arxiv.org/abs/2510.11715", "authors": ["Ayush Shrivastava", "Sanyam Mehta", "Daniel Geng", "Andrew Owens"], "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models", "comment": "Project link: https://point-prompting.github.io", "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.", "AI": {"tldr": "本文提出一种利用预训练视频扩散模型进行零样本点跟踪的方法，通过在视频中标记点并重新生成帧，实现了优于现有零样本方法的跟踪性能，并能处理遮挡。", "motivation": "跟踪器分析运动，视频生成器合成运动，两者解决的问题密切相关。本文旨在探索如何利用这种联系，使预训练视频生成模型（特别是视频扩散模型）执行零样本点跟踪任务。", "method": "核心方法是“提示式”跟踪：首先在查询点放置一个独特颜色的标记；然后从中间噪声水平重新生成视频的其余部分，这使得标记在帧间传播，从而追踪点的轨迹。为确保标记在反事实生成中保持可见（尽管自然视频中此类标记不常见），使用未经编辑的初始帧作为负面提示。", "result": "实验表明，这种“涌现式”的跟踪性能优于先前的零样本方法，并且能够持续穿过遮挡。其性能甚至与专业的自监督模型具有竞争力。", "conclusion": "预训练的视频扩散模型可以通过简单的提示机制实现有效的零样本点跟踪。这种方法不仅超越了现有的零样本方法，还能处理复杂场景（如遮挡），并能与专业自监督模型的性能相媲美，揭示了视频生成模型在运动分析方面的潜在能力。"}}
{"id": "2510.11649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11649", "abs": "https://arxiv.org/abs/2510.11649", "authors": ["Pradyumna Yalandur Muralidhar", "Yuxuan Xue", "Xianghui Xie", "Margaret Kostyrko", "Gerard Pons-Moll"], "title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image", "comment": "Accepted to ACM SIGGraphAsia 2025. Project website:\n  https://yuxuan-xue.com/physic", "summary": "Reconstructing metrically accurate humans and their surrounding scenes from a\nsingle image is crucial for virtual reality, robotics, and comprehensive 3D\nscene understanding. However, existing methods struggle with depth ambiguity,\nocclusions, and physically inconsistent contacts. To address these challenges,\nwe introduce PhySIC, a framework for physically plausible Human-Scene\nInteraction and Contact reconstruction. PhySIC recovers metrically consistent\nSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within\na shared coordinate frame from a single RGB image. Starting from coarse\nmonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,\nfuses visible depth with unscaled geometry for a robust metric scaffold, and\nsynthesizes missing support surfaces like floors. A confidence-weighted\noptimization refines body pose, camera parameters, and global scale by jointly\nenforcing depth alignment, contact priors, interpenetration avoidance, and 2D\nreprojection consistency. Explicit occlusion masking safeguards invisible\nregions against implausible configurations. PhySIC is efficient, requiring only\n9 seconds for joint human-scene optimization and under 27 seconds end-to-end.\nIt naturally handles multiple humans, enabling reconstruction of diverse\ninteractions. Empirically, PhySIC outperforms single-image baselines, reducing\nmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,\nand improving contact F1 from 0.09 to 0.51. Qualitative results show realistic\nfoot-floor interactions, natural seating, and plausible reconstructions of\nheavily occluded furniture. By converting a single image into a physically\nplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.\nOur implementation is publicly available at https://yuxuan-xue.com/physic.", "AI": {"tldr": "PhySIC是一个从单张RGB图像重建物理上可信的人体-场景交互和接触的框架，解决了深度模糊、遮挡和物理不一致接触的问题，实现了高精度的三维人体和场景重建。", "motivation": "从单张图像重建度量精确的人体及其周围场景对于虚拟现实、机器人和全面的三维场景理解至关重要。然而，现有方法在处理深度模糊、遮挡和物理不一致的接触方面存在困难。", "method": "PhySIC首先从粗略的单目深度和人体估计开始，然后进行遮挡感知的修复，将可见深度与未缩放几何体融合以构建鲁棒的度量支架，并合成缺失的支撑表面（如地面）。通过置信度加权的优化，PhySIC通过联合强制执行深度对齐、接触先验、避免相互穿透和二维重投影一致性来精炼人体姿态、相机参数和全局尺度。显式遮挡掩码保护不可见区域免受不合理配置的影响。", "result": "PhySIC能够从单张RGB图像中恢复度量一致的SMPL-X人体网格、密集的场景表面和顶点级别的接触图，所有这些都在共享坐标系中。它高效（优化仅需9秒，端到端不到27秒），并能处理多个人体。在经验上，PhySIC优于单图像基线，将平均每顶点场景误差从641毫米降低到227毫米，将PA-MPJPE减半至42毫米，并将接触F1从0.09提高到0.51。定性结果显示了逼真的脚地交互、自然的坐姿以及对严重遮挡家具的合理重建。", "conclusion": "PhySIC通过将单张图像转换为物理上可信的三维人体-场景对，推动了可扩展的三维场景理解。它有效地解决了从单张图像重建人体-场景交互中的深度模糊、遮挡和物理不一致性挑战。"}}
{"id": "2510.11650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11650", "abs": "https://arxiv.org/abs/2510.11650", "authors": ["Yuxuan Xue", "Xianghui Xie", "Margaret Kostyrko", "Gerard Pons-Moll"], "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control", "comment": "Accepted to ACM SIGGRAPH Asia 2025. Project website:\n  https://yuxuan-xue.com/infini-human", "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.", "AI": {"tldr": "InfiniHuman框架通过蒸馏现有基础模型，自动生成大规模、多样化、带丰富标注的3D人体数据（InfiniHumanData），并在此基础上训练了一个扩散模型（InfiniHumanGen），实现了快速、逼真且高度可控的3D虚拟形象生成。", "motivation": "生成逼真且可控的3D人体虚拟形象面临巨大挑战，尤其是在覆盖广泛属性（如种族、年龄、服装风格、身体形状）时。由于获取和标注大规模人体数据集的成本过高且规模和多样性受限，阻碍了生成模型的发展。", "method": "本文提出了InfiniHuman框架，通过协同蒸馏现有基础模型来生成理论上无限且标注丰富的人体数据。具体包括：1) InfiniHumanData：一个全自动流水线，利用视觉-语言模型和图像生成模型创建大规模多模态数据集。2) InfiniHumanGen：一个基于扩散的生成流水线，以文本、身体形状和服装资产为条件，用于生成虚拟形象，并在InfiniHumanData上进行训练。", "result": "InfiniHumanData包含了11.1万个具有前所未有多样性的身份，每个身份都标注了多粒度文本描述、多视角RGB图像、详细服装图像和SMPL身体形状参数。用户研究表明，自动生成的身份与扫描渲染图无法区分。InfiniHumanGen在视觉质量、生成速度和可控性方面显著优于现有最先进方法，实现了高品质、精细化控制的虚拟形象生成，且具有理论上无限的扩展性。", "conclusion": "InfiniHuman提供了一个实用且经济的解决方案，通过利用蒸馏的基础模型生成大规模、多样化的3D人体数据，并在此基础上开发了高效的生成模型，从而实现了高品质、精细化控制且规模无限的3D虚拟形象生成。"}}
{"id": "2510.11717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11717", "abs": "https://arxiv.org/abs/2510.11717", "authors": ["Takuya Nakabayashi", "Navami Kairanda", "Hideo Saito", "Vladislav Golyanik"], "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams", "comment": null, "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.", "AI": {"tldr": "Ev4DGS是首个仅使用单目事件流，实现非刚性变形物体新视角渲染（输出为RGB或灰度图像）的方法，解决了现有方法对RGB输入依赖的问题。", "motivation": "事件相机在新视角渲染方面优于RGB相机，但现有针对非刚性物体的事件基方法通常需要稀疏RGB输入，这是一个实际限制。目前尚不清楚是否可以仅从事件流中学习此类模型。", "method": "本文提出了Ev4DGS，通过以下方式回归可变形的3D高斯Splatting表示：1) 一个将估计模型输出与2D事件观测空间关联的损失函数；2) 一个从事件生成的二值掩码训练的粗略3D变形模型。", "result": "Ev4DGS在现有合成数据集和新记录的真实非刚性物体数据集上进行了实验比较，结果证明了其有效性，并显示出优于多种可应用于此设置的朴素基线方法的性能。", "conclusion": "Ev4DGS首次证明了仅凭单目事件流即可实现非刚性变形物体在新观察空间（如RGB或灰度图像）中的新视角渲染，解决了该领域的一个重要开放问题。"}}
{"id": "2510.11687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11687", "abs": "https://arxiv.org/abs/2510.11687", "authors": ["Jinyu Zhang", "Haitao Lin", "Jiashu Hou", "Xiangyang Xue", "Yanwei Fu"], "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View", "comment": null, "summary": "Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI.", "AI": {"tldr": "本文提出了一种统一的、与类别无关的框架，可从单张RGB-D图像同时估计物体的6D姿态、尺寸和密集形状。该框架结合了视觉基础模型的2D特征和3D点云，使用Transformer编码器和并行解码器，实现了实时推理，并在零样本泛化方面达到了SOTA水平。", "motivation": "现有方法要么依赖于物体特定的先验知识（如CAD模型或模板），要么由于姿态-形状的纠缠和多阶段流程而导致跨类别泛化能力有限。在机器人抓取和操作等关键应用中，需要一个能够从视觉输入估计物体6D姿态、尺寸和形状的通用解决方案。", "method": "该方法是一个统一的、与类别无关的框架，无需模板、CAD模型或测试时的类别标签。它通过一个由专家混合（Mixture-of-Experts）增强的Transformer编码器，融合了来自视觉基础模型的密集2D特征与部分3D点云。随后，使用并行解码器分别进行姿态-尺寸估计和形状重建。模型仅在SOPE数据集的149个类别合成数据上进行训练。", "result": "该框架实现了28 FPS的实时推理速度。在SOPE、ROPE、ObjaversePose和HANDAL四个涵盖300多个类别的基准测试中进行评估，结果显示在已知类别上达到了最先进的准确性，并对未见的真实世界物体展现出卓越的零样本泛化能力。", "conclusion": "该研究为机器人技术和具身AI中的开放集6D理解设定了新标准，提供了一个高效、高泛化能力的解决方案，能够同时预测物体的6D姿态、尺寸和密集形状。"}}
