{"id": "2601.15309", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.15309", "abs": "https://arxiv.org/abs/2601.15309", "authors": ["Jiaxin Xu", "Chao Zhang", "Raymond H. Cuijpers", "Wijnand A. IJsselsteijn"], "title": "Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods", "comment": "Accepted to HRI 2026", "summary": "Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.", "AI": {"tldr": "本系统综述总结了现有社会机器人健康行为干预研究中使用的行为改变策略和评估方法，为未来研究提供了设计指导和方向。", "motivation": "现有关于社会机器人作为健康行为改变干预措施的研究有限，缺乏可操作的知识来指导其设计和评估。", "method": "通过系统数据库搜索和手动搜索，分析了39项关于社会机器人健康行为改变干预的研究。", "result": "识别出四类行为改变策略：指导策略、咨询策略、社会影响策略和增强说服力策略。同时，总结了当前评估实践的关键特征，包括研究设计、设置、持续时间和结果衡量。", "conclusion": "本综述提出的行为改变策略为社会机器人的设计提供了有价值的启发式指导，并为未来的HRI研究在评估方法和研究方向上提出了建议。"}}
{"id": "2601.15358", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15358", "abs": "https://arxiv.org/abs/2601.15358", "authors": ["Yi Zhu", "Razmig Kechichian", "Raphaël Richert", "Satoshi Ikehata", "Sébastien Valette"], "title": "High-Fidelity 3D Tooth Reconstruction by Fusing Intraoral Scans and CBCT Data via a Deep Implicit Representation", "comment": "Accepted to IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "High-fidelity 3D tooth models are essential for digital dentistry, but must capture both the detailed crown and the complete root. Clinical imaging modalities are limited: Cone-Beam Computed Tomography (CBCT) captures the root but has a noisy, low-resolution crown, while Intraoral Scanners (IOS) provide a high-fidelity crown but no root information. A naive fusion of these sources results in unnatural seams and artifacts. We propose a novel, fully-automated pipeline that fuses CBCT and IOS data using a deep implicit representation. Our method first segments and robustly registers the tooth instances, then creates a hybrid proxy mesh combining the IOS crown and the CBCT root. The core of our approach is to use this noisy proxy to guide a class-specific DeepSDF network. This optimization process projects the input onto a learned manifold of ideal tooth shapes, generating a seamless, watertight, and anatomically coherent model. Qualitative and quantitative evaluations show our method uniquely preserves both the high-fidelity crown from IOS and the patient-specific root morphology from CBCT, overcoming the limitations of each modality and naive stitching.", "AI": {"tldr": "提出了一种全自动化的方法，利用深度隐式表示融合 CBCT 和口内扫描 (IOS) 数据，以生成高保真、无缝且解剖学上连贯的 3D 牙齿模型，克服了单一成像模式的局限性。", "motivation": "现有的临床成像技术在捕捉完整的牙齿模型（包括牙冠和牙根）方面存在局限性：CBCT 牙冠分辨率低且有噪声，而口内扫描仪无法获取牙根信息。简单地将两者融合会导致不自然的接缝和伪影。", "method": "该方法首先分割并配准牙齿实例，然后创建一个结合了口内扫描仪牙冠和 CBCT 牙根的混合代理网格。接着，利用这个代理网格来指导一个类别的深度隐式表示（DeepSDF）网络进行优化，该网络将输入投影到一个理想牙齿形状的学习流形上。", "result": "该方法能够生成无缝、防水且在解剖学上连贯的牙齿模型，同时保留了口内扫描仪提供的精细牙冠细节和 CBCT 提供的患者特异性牙根形态。", "conclusion": "该自动化融合方法能够有效克服单一成像模式的局限性，并优于简单的缝合方法，能够生成高质量的 3D 牙齿模型，这对于数字化牙科至关重要。"}}
{"id": "2601.15356", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15356", "abs": "https://arxiv.org/abs/2601.15356", "authors": ["Xiang Li", "XueHeng Li", "Yu Wang", "XuanHua He", "ZhangChi Hu", "WeiWei Yu", "ChengJun Xie"], "title": "Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing", "comment": null, "summary": "Reinforcement Learning (RL) has empowered Multimodal Large Language Models (MLLMs) to achieve superior human preference alignment in Image Quality Assessment (IQA). However, existing RL-based IQA models typically rely on coarse-grained global views, failing to capture subtle local degradations in high-resolution scenarios. While emerging \"Thinking with Images\" paradigms enable multi-scale visual perception via zoom-in mechanisms, their direct adaptation to IQA induces spurious \"cropping-implies-degradation\" biases and misinterprets natural depth-of-field as artifacts. To address these challenges, we propose Q-Probe, the first agentic IQA framework designed to scale IQA to high resolution via context-aware probing. First, we construct Vista-Bench, a pioneering benchmark tailored for fine-grained local degradation analysis in high-resolution IQA settings. Furthermore, we propose a three-stage training paradigm that progressively aligns the model with human preferences, while simultaneously eliminating causal bias through a novel context-aware cropping strategy. Extensive experiments demonstrate that Q-Probe achieves state-of-the-art performance in high-resolution settings while maintaining superior efficacy across resolution scales.", "AI": {"tldr": "提出了一种名为Q-Probe的、针对高分辨率图像质量评估（IQA）的代理框架，通过上下文感知探测来解决现有方法在细粒度局部退化检测方面的不足，并引入了Vista-Bench基准和一种新的训练策略以消除偏见，在高分辨率IQA任务上取得了最先进的性能。", "motivation": "现有的基于强化学习的IQA模型依赖于全局视图，在高分辨率场景下难以捕捉细微的局部退化。而“用图像思考”的方法在IQA中存在“裁剪即退化”的偏见和对自然景深误判的问题。", "method": "提出Q-Probe框架，采用上下文感知探测（context-aware probing）来处理高分辨率IQA。构建了Vista-Bench基准，用于高分辨率IQA的细粒度局部退化分析。采用三阶段训练范式，通过上下文感知裁剪策略逐步消除因果偏见，并实现与人类偏好的对齐。", "result": "Q-Probe在Vista-Bench基准上实现了最先进的性能，并在不同分辨率尺度上都展现出卓越的性能。实验证明该方法能有效解决高分辨率IQA中的挑战。", "conclusion": "Q-Probe是首个专门为高分辨率IQA设计的代理框架，通过上下文感知探测和创新的训练策略，成功地解决了局部退化检测和偏见问题，并在高分辨率IQA任务上达到了前所未有的性能水平。"}}
{"id": "2601.15369", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15369", "abs": "https://arxiv.org/abs/2601.15369", "authors": ["Letian Zhang", "Sucheng Ren", "Yanqing Liu", "Xianhang Li", "Zeyu Wang", "Yuyin Zhou", "Huaxiu Yao", "Zeyu Zheng", "Weili Nie", "Guilin Liu", "Zhiding Yu", "Cihang Xie"], "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation", "comment": null, "summary": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.", "AI": {"tldr": "本文提出了一种名为OpenVision 3的统一视觉编码器，它通过结合VAE压缩图像潜在表示和ViT编码器，并同时优化重构和语义学习任务，实现了在图像理解和图像生成任务上的协同泛化。", "motivation": "研究动机在于探索一个单一的视觉表示，能够同时服务于图像理解和图像生成任务，从而实现更高效和通用的模型。", "method": "采用VAE压缩图像的潜在表示，并将其输入ViT编码器。编码器的输出同时用于两个任务：1. 通过ViT-VAE解码器重构原始图像，学习生成结构；2. 通过对比学习和图像-字幕学习目标，增强语义特征。在共享的潜在空间中联合优化这两个信号。", "result": "在冻结编码器的情况下，OpenVision 3在多模态理解任务上（如LLaVA-1.5框架）表现与标准CLIP视觉编码器相当（SeedBench: 62.4 vs 62.2，POPE: 83.7 vs 82.9）。在图像生成任务上（如RAE框架），OpenVision 3显著优于标准CLIP编码器（ImageNet gFID: 1.89 vs 2.54）。", "conclusion": "联合优化重构和语义驱动的信号能够使编码器学习到能够协同工作并在图像理解和生成任务上实现良好泛化的表示。研究者希望这项工作能促进对统一建模的未来研究。"}}
{"id": "2601.15539", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15539", "abs": "https://arxiv.org/abs/2601.15539", "authors": ["Ali Khreis", "Ro'Yah Radaideh", "Quinn McGill"], "title": "A Machine Vision Approach to Preliminary Skin Lesion Assessments", "comment": "6 pages, 2 figures, 2 tables", "summary": "Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.", "AI": {"tldr": "本研究提出了一种结合ABCD规则和机器学习的皮肤病变评估系统。虽然基于规则的系统易于解释，但其性能受限于将复杂形态简化为数值特征。定制的三层卷积神经网络（CNN）在像素级别学习方面表现出优越性，优于传统方法和预训练的深度学习模型。", "motivation": "提高侵袭性、转移性皮肤癌患者的治疗效果，关键在于早期检测恶性皮肤病变。", "method": "该系统结合了临床上成熟的皮肤镜ABCD规则（不对称性、边缘、颜色和皮肤镜结构）与机器学习分类。采用一种自动化的、基于规则的流程来计算每个病变的皮肤镜总分（TDS），并与逻辑回归、随机森林、支持向量机（SVM）以及深度学习模型（EfficientNet-B0和自定义三层CNN）进行比较。", "result": "基于规则的系统易于解释，但其性能受限于将复杂形态简化为五个数值特征。EfficientNet-B0由于领域迁移问题表现不佳。相比之下，从头开始训练的定制三层CNN在经过中值滤波的图像上实现了78.5%的准确率和86.5%的召回率，准确率比传统方法提高了19个百分点。", "conclusion": "直接的像素级学习能够捕捉超越手工制作特征的诊断模式。对于小型、领域特定的医学数据集，专门设计的轻量级架构可以优于大型预训练模型。"}}
{"id": "2601.15296", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15296", "abs": "https://arxiv.org/abs/2601.15296", "authors": ["Longxuan Wei", "Yubo Zhang", "Zijiao Zhang", "Zhihu Wang", "Shiwan Zhao", "Tianyu Huang", "Huiting Zhao", "Chenfei Liu", "Shenao Zhang", "Junchi Yan"], "title": "Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration", "comment": null, "summary": "Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.", "AI": {"tldr": "本文提出了一种名为 Entropy-Tree 的新型解码方法，通过利用熵作为分支决策的信号，选择性地扩展搜索树，从而在提高大型语言模型推理任务的准确性和校准性的同时，避免了盲目探索和冗余采样。", "motivation": "现有的解码策略要么盲目探索（如随机采样），要么冗余探索（如独立多重采样），这在大语言模型推理任务中效率低下且效果不佳。研究者希望找到一种更有效的解码方法，能够平衡搜索效率和模型的不确定性。", "method": "提出 Entropy-Tree 解码方法，该方法基于树状搜索。它利用模型预测的熵作为信号来决定是否扩展搜索树的分支，只在模型表现出真正不确定性的位置进行扩展。这种方法将高效的结构化探索与可靠的不确定性估计结合在一个解码过程中。", "result": "Entropy-Tree 在推理任务上展现出优越的准确性和校准性。与 Multi-chain 方法相比，在多个模型和数据集上实现了更高的 pass@k 指标。此外，其预测熵在不确定性度量上表现出优于几种传统指标的 AUROC。", "conclusion": "Entropy-Tree 是一种有效的解码策略，能够通过基于熵的决策来优化大型语言模型的推理搜索，从而在提高准确性和校准性的同时，实现高效的结构化探索和可靠的不确定性估计。"}}
{"id": "2601.15366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15366", "abs": "https://arxiv.org/abs/2601.15366", "authors": ["Christina Thrainer"], "title": "AI-Based Culvert-Sewer Inspection", "comment": "Masters thesis, University of Technology Graz, 2025", "summary": "Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.\n  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.", "AI": {"tldr": "该论文提出三种方法来改进在数据稀缺情况下对涵洞和下水道管道的自动缺陷分割，包括数据预处理（如动态标签注入）、新颖的网络架构（FORTRESS）以及少样本学习方法。", "motivation": "在涵洞和下水道管道中进行自动缺陷分割存在数据收集和标注困难、成本高昂且需要领域知识的问题，导致难以获得大规模标注数据集。本研究旨在解决数据稀缺的实际应用场景。", "method": "1. 评估数据预处理策略，包括传统数据增强和动态标签注入。2. 提出新颖的FORTRESS网络架构，结合深度可分离卷积、自适应Kolmogorov-Arnold网络（KAN）和多尺度注意力机制。3. 研究少样本语义分割方法，并采用带有注意力机制的双向原型网络。", "result": "1. 数据预处理方法显著提高了分割性能（IoU和F1分数）。2. FORTRESS架构在涵洞和下水道管道缺陷数据集上达到了最先进的性能，同时大幅减少了可训练参数和计算成本。3. 双向原型网络在少样本学习场景下取得了令人满意的结果。", "conclusion": "本研究提出的三种方法（数据预处理、FORTRESS架构和少样本学习）能有效提升在数据稀缺情况下的涵洞和下水道管道缺陷分割能力，为解决实际应用中的数据不足问题提供了有效途径。"}}
{"id": "2601.15368", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2601.15368", "abs": "https://arxiv.org/abs/2601.15368", "authors": ["Yikai Wang", "Junqiu Yu", "Chenjie Cao", "Xiangyang Xue", "Yanwei Fu"], "title": "Aligned Stable Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency", "comment": "Extension of our CVPR 2025 highlight paper: arXiv:2312.04831", "summary": "Generative image inpainting can produce realistic, high-fidelity results even with large, irregular masks. However, existing methods still face key issues that make inpainted images look unnatural. In this paper, we identify two main problems: (1) Unwanted object insertion: generative models may hallucinate arbitrary objects in the masked region that do not match the surrounding context. (2) Color inconsistency: inpainted regions often exhibit noticeable color shifts, leading to smeared textures and degraded image quality. We analyze the underlying causes of these issues and propose efficient post-hoc solutions for pre-trained inpainting models. Specifically, we introduce the principled framework of Aligned Stable inpainting with UnKnown Areas prior (ASUKA). To reduce unwanted object insertion, we use reconstruction-based priors to guide the generative model, suppressing hallucinated objects while preserving generative flexibility. To address color inconsistency, we design a specialized VAE decoder that formulates latent-to-image decoding as a local harmonization task. This design significantly reduces color shifts and produces more color-consistent results. We implement ASUKA on two representative inpainting architectures: a U-Net-based model and a DiT-based model. We analyze and propose lightweight injection strategies that minimize interference with the model's original generation capacity while ensuring the mitigation of the two issues. We evaluate ASUKA using the Places2 dataset and MISATO, our proposed diverse benchmark. Experiments show that ASUKA effectively suppresses object hallucination and improves color consistency, outperforming standard diffusion, rectified flow models, and other inpainting methods. Dataset, models and codes will be released in github.", "AI": {"tldr": "本研究提出了一种名为ASUKA的后处理框架，用于解决生成图像修复中存在的对象误插入和颜色不一致问题，通过引入重构先验和专门的VAE解码器来提升修复效果。", "motivation": "现有的生成图像修复方法在处理大且不规则遮罩时，仍存在修复区域出现不匹配的物体（对象误插入）以及颜色不一致导致纹理模糊和质量下降的问题，使得修复后的图像显得不自然。", "method": "ASUKA框架通过两个主要部分解决上述问题：1. 引入重构先验来指导生成模型，抑制不想要的物体幻觉，同时保留生成灵活性。2. 设计一个专门的VAE解码器，将潜在空间到图像的解码视为一个局部协调任务，以减少颜色偏移并生成更一致的颜色。ASUKA可以轻量级地注入到已有的修复模型（如U-Net和DiT）中。", "result": "ASUKA能够有效抑制对象幻觉并改善颜色一致性。在Places2数据集和MISATO基准上的实验表明，ASUKA的效果优于标准的扩散模型、流模型以及其他修复方法。", "conclusion": "ASUKA是一个有效的后处理框架，能够显著提升预训练图像修复模型的性能，有效解决对象误插入和颜色不一致的问题，生成更自然、更高质量的修复图像。"}}
{"id": "2601.15305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15305", "abs": "https://arxiv.org/abs/2601.15305", "authors": ["Alfred Shen", "Aaron Shen"], "title": "Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models", "comment": "15 pages, 1 figure, attention mechanism, sparse attention, gating, long-context", "summary": "The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.", "AI": {"tldr": "本文提出了一种名为 Gated Sparse Attention (GSA) 的新架构，它结合了稀疏注意力和门控注意力的优点，以提高长上下文语言模型的效率和稳定性，实验证明其在速度和模型质量方面均有显著提升。", "motivation": "现有长上下文语言模型中的注意力机制计算量巨大，这促使了两类独立的研究方向：一是通过稀疏注意力机制选择性地关注部分 token 来降低计算复杂度；二是采用门控注意力机制来提高训练稳定性并缓解注意力沉没现象。本文认为这两种方法各自解决了不同的问题，因此有必要将它们结合起来。", "method": "GSA 架构整合了带有 sigmoid 激活函数的门控闪电索引器（生成有界、可解释的选择分数）、一个根据局部不确定性自适应调整关注 token 数量的稀疏控制器，以及在 value 和 output 阶段的双重门控。作者还从理论上对该方法进行了分析，包括复杂度分析、表达能力和收敛性保证。", "result": "在 1.7B 参数模型、400B token 的实验中，GSA 在 128K 上下文长度下实现了与仅稀疏注意力基线相当的效率（12-16 倍加速）。在模型质量方面，困惑度从 6.03 提高到 5.70，128K 上下文长度下的 RULER 分数几乎翻倍，并且对第一个 token 的注意力（注意力沉没的代理指标）从 47% 下降到 4% 以下。训练稳定性也显著提高，损失峰值减少了 98%。", "conclusion": "Gated Sparse Attention (GSA) 是一种有效的架构，能够结合稀疏注意力的效率和门控注意力的性能优势，显著提升长上下文语言模型的训练稳定性和推理性能，同时降低了对早期 token 的过度关注。"}}
{"id": "2601.15406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15406", "abs": "https://arxiv.org/abs/2601.15406", "authors": ["Hatef Otroshi Shahreza", "Anjith George", "Sébastien Marcel"], "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.", "AI": {"tldr": "本文系统评估了当前最先进的多模态大语言模型（MLLMs）在异构人脸识别（HFR）任务中的表现，发现在跨光谱条件下，MLLMs的性能与传统人脸识别系统相比存在显著差距。", "motivation": "鉴于MLLMs在视觉-语言任务上表现出色，研究者对其在生物识别应用（尤其是异构人脸识别）中的潜力产生了兴趣，并希望通过系统性评估来了解其真实能力。", "method": "在VIS-NIR、VIS-SWIR和VIS-THERMAL等多种跨模态场景下，使用生物识别协议和Acquire Rate、EER、TAR等指标，对多个开源MLLMs在异构人脸识别任务上进行了基准测试。", "result": "研究发现，尽管MLLMs取得了进展，但在最具挑战性的跨光谱异构人脸识别条件下，其性能与传统人脸识别系统相比存在显著差距。", "conclusion": "当前的MLLMs在异构人脸识别方面存在局限性，并且在部署于人脸识别系统之前，进行严格的生物识别评估至关重要。"}}
{"id": "2601.15297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15297", "abs": "https://arxiv.org/abs/2601.15297", "authors": ["Edward Ajayi"], "title": "AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports", "comment": null, "summary": "We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.", "AI": {"tldr": "本文提出了AfriEconQA，一个基于236份世界银行报告的非洲经济分析QA数据集，包含8937个高质量问答实例，旨在解决当前LLM缺乏相关领域知识的问题。", "motivation": "当前大型语言模型（LLM）在预训练语料中缺乏对非洲经济的专业知识，这阻碍了其在该领域的分析能力。因此，需要一个专门的数据集来训练和评估LLM在非洲经济分析方面的能力。", "method": "构建了一个包含236份世界银行报告的语料库，从中生成并筛选了8937个高质量的问答实例。通过11个实验，对比了零样本基线（GPT-5 Mini）和使用GPT-4o、Qwen 32B的RAG（检索增强生成）配置在不同嵌入和排序策略下的表现。", "result": "实验结果表明，零样本模型无法回答超过90%的查询，即使是先进的RAG模型也难以达到高精度，这突显了模型在非洲经济领域的知识缺口。", "conclusion": "AfriEconQA是一个强大且具有挑战性的基准数据集，能够有效地测试和推动下一代领域特定IR和RAG系统的发展，以应对非洲经济分析的复杂性。"}}
{"id": "2601.15349", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15349", "abs": "https://arxiv.org/abs/2601.15349", "authors": ["Jiaqing Chang", "Song Gao", "Chaowei Dong", "zhaobang Li", "Yang Liu"], "title": "Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray", "comment": "These experiments lay an important foundation for the study of tether-free control of underwater micro-soft robots. Furthermore, this research provides important references for the fields of biomimetic robots and magnetically controlled micro-soft robots", "summary": "In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.", "AI": {"tldr": "本研究设计并制造了一种受鲹鱼启发的磁响应微型软体机器人，使用亥姆霍兹线圈产生的磁场进行驱动，实现了高达 5.25 毫米/秒的速度和多种运动模式，为在水下狭窄空间应用无线驱动机器人奠定了基础。", "motivation": "微型软体机器人在水下狭窄环境（如环境监测和微创医疗）具有优势，但其微小尺寸限制了内部供电，因此需要无线供电方式。仿生设计可以提高其运动性能。", "method": "设计并制作用钕铁硼和PDMS材料制成的受鲹鱼启发的磁响应微型软体机器人。使用三维亥姆霍兹线圈产生振荡谐波磁场，进行机器人游泳实验，并探索磁场参数（磁场强度和频率）对其游泳性能的影响。通过改变电流方向和频率控制机器人运动模式，并采用分步调整法减少响应误差对轨迹的影响。", "result": "机器人最快的游泳速度为 5.25 毫米/秒（约 0.5 body lengths per second），出现在磁场强度 B = 5 mT 和频率 f = 11 Hz 时。通过调整电流方向和频率，机器人可以实现直线游动、转弯游动和定向游动等不同的游泳模式。", "conclusion": "本研究展示了一种磁驱动微型软体机器人的制造和控制方法，并证明了其在水下狭窄空间应用的潜力，为无线驱动机器人的进一步发展奠定了基础。"}}
{"id": "2601.15408", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15408", "abs": "https://arxiv.org/abs/2601.15408", "authors": ["Pablo Messina", "Andrés Villa", "Juan León Alcázar", "Karen Sánchez", "Carlos Hinojosa", "Denis Parra", "Álvaro Soto", "Bernard Ghanem"], "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation", "comment": "31 pages, 7 figures, submitted to CVPR 2026 (under review)", "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure", "AI": {"tldr": "本文提出了一种名为 CURE 的错误感知课程学习框架，通过无需额外数据，提高了医学视觉语言模型在影像报告生成中的视觉定位和事实一致性。CURE 通过动态调整采样策略，专注于更难的样本，从而提升了模型在空间和文本对齐方面的能力。", "motivation": "现有的医学视觉语言模型在生成放射学报告时，存在视觉定位不准确和事实不一致的问题，导致其预测结果不可靠或定位不牢固。", "method": "CURE 框架采用课程学习策略，在公共数据集上对多模态指令模型进行微调，训练任务包括短语定位、定位报告生成和解剖定位报告生成。该方法根据模型性能动态调整采样，优先处理更难的样本，以改善空间和文本对齐。", "result": "CURE 将定位准确度提高了 +0.37 IoU，报告质量评分提高了 +0.188 CXRFEScore，并将幻觉（不准确的陈述）减少了 18.6%。", "conclusion": "CURE 是一个数据高效的框架，能够同时增强医学视觉语言模型的定位准确性和报告的可靠性。"}}
{"id": "2601.15572", "categories": ["eess.IV", "cs.CE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15572", "abs": "https://arxiv.org/abs/2601.15572", "authors": ["Jieyun Bai", "Yitong Tang", "Zihao Zhou", "Mahdi Islam", "Musarrat Tabassum", "Enrique Almar-Munoz", "Hongyu Liu", "Hui Meng", "Nianjiang Lv", "Bo Deng", "Yu Chen", "Zilun Peng", "Yusong Xiao", "Li Xiao", "Nam-Khanh Tran", "Dac-Phu Phan-Le", "Hai-Dang Nguyen", "Xiao Liu", "Jiale Hu", "Mingxu Huang", "Jitao Liang", "Chaolu Feng", "Xuezhi Zhang", "Lyuyang Tong", "Bo Du", "Ha-Hieu Pham", "Thanh-Huy Nguyen", "Min Xu", "Juntao Jiang", "Jiangning Zhang", "Yong Liu", "Md. Kamrul Hasan", "Jie Gan", "Zhuonan Liang", "Weidong Cai", "Yuxin Huang", "Gongning Luo", "Mohammad Yaqub", "Karim Lekadir"], "title": "FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation", "comment": null, "summary": "Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.", "AI": {"tldr": "本文介绍了FUGC（胎儿超声大挑战）基准，这是首个用于宫颈分割半监督学习的基准，旨在解决有标签数据稀缺的问题。该挑战提供了一个包含890张TVS图像的数据集，并评估了10个参赛队伍的解决方案，最佳结果在mDSC、mHD和ms RT上表现优异，为AI辅助的早产风险评估奠定了基础。", "motivation": "由于有标签数据的稀缺，监督学习方法在胎儿超声TVS图像宫颈结构分割方面性能受限，而精确的宫颈分割对评估自发性早产风险至关重要。", "method": "引入了FUGC（胎儿超声大挑战）作为宫颈分割半监督学习的基准。该挑战提供了一个包含890张TVS图像（500张训练，90张验证，300张测试）的数据集。使用Dice相似系数（DSC）、Hausdorff距离（HD）和运行时（RT）作为评估指标，并采用0.4/0.4/0.2的加权组合。", "result": " FUGC挑战吸引了10支队伍，共82名参赛者。在评估指标上，最佳方法的表现分别为：mDSC 90.26%，mHD 38.88，ms RT 32.85。", "conclusion": "FUGC建立了一个标准化的宫颈分割基准，证明了半监督方法在有限标记数据下的有效性，并为AI辅助的临床早产风险评估提供了基础。"}}
{"id": "2601.15306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15306", "abs": "https://arxiv.org/abs/2601.15306", "authors": ["Ethan Zhang"], "title": "Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables", "comment": "15 pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.", "AI": {"tldr": "研究发现，基于大型语言模型（LLM）的医学人工智能系统在急诊科分诊中存在对不同种族、社会经济背景和临床状况患者的歧视性行为，LLM 会系统性地改变患者的感知严重程度，而这与真实病情并不一定相关。", "motivation": "尽管大型语言模型（LLM）在临床决策中的应用日益增多，但它们仍然存在针对不同背景患者的隐藏偏见，这促使了本研究的进行。", "method": "研究采用了 32 个患者层面的代理变量（每个变量都有正反两面限定词），并在公开和受限访问的数据集（MIMIC-IV-ED Demo, MIMIC-IV Demo, MIMIC-IV-ED, MIMIC-IV）上评估了这些变量对 LLM 行为的影响。", "result": "研究结果显示，在急诊科分诊场景中，代理变量导致了歧视性行为；此外，LLM 会系统性地改变患者的感知严重程度，即使输入语境中的特定标记（无论正面还是负面）出现。", "conclusion": "研究表明，AI 系统在训练时依赖于嘈杂的、有时是非因果的信号，这些信号不能可靠地反映患者的真实病情，因此需要进一步努力来确保 AI 技术在临床环境中的安全和负责任的部署。"}}
{"id": "2601.15298", "categories": ["cs.CL", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.15298", "abs": "https://arxiv.org/abs/2601.15298", "authors": ["Anantha Sharma"], "title": "Embedding Retrofitting: Data Engineering for better RAG", "comment": "16 pages, 11 figures, 7 tables", "summary": "Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.\n  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\\%$ to $-5.2\\%$, $p<0.05$). After preprocessing, \\acrshort{ewma} retrofitting achieves $+6.2\\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\\%$ average). The gap between clean and noisy preprocessing (10\\%+ swing) exceeds the gap between algorithms (3\\%), establishing preprocessing quality as the primary determinant of retrofitting success.", "AI": {"tldr": "研究表明，文本预处理质量对嵌入向量回溯（embedding retrofitting）的效果至关重要，比算法选择更能影响检索性能。移除注释伪影（artifact）的预处理方法可以显著提升回溯效果。", "motivation": "现有嵌入向量回溯方法依赖于知识图谱的质量，而知识图谱质量又受文本预处理影响。现实世界语料库中的注释伪影会导致数据质量下降，进而影响回溯效果。", "method": "提出一个数据工程框架来处理由注释伪影引起的数据质量下降。通过分析hashtag注释如何膨胀知识图谱密度并产生错误边，腐蚀回溯目标。在清洗后的数据上，使用ewma回溯方法进行实验，并与现有技术进行对比。", "result": "在有噪声的知识图谱上，所有回溯技术都会导致统计学上显著的性能下降（-3.5%至-5.2%）。经过预处理后，ewma回溯方法取得了+6.2%的性能提升（p=0.0348），尤其在定量综合问题上提升显著（+33.8%）。干净预处理与噪声预处理之间的性能差距（10%以上）大于算法之间的差距（3%）。", "conclusion": "数据预处理质量是决定嵌入向量回溯成功与否的首要因素，其影响甚至超过了回溯算法本身的选择。"}}
{"id": "2601.15622", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15622", "abs": "https://arxiv.org/abs/2601.15622", "authors": ["Sampson E. Nwachukwu"], "title": "Design, Modelling, and Control of Magnetic Ball Suspension System", "comment": "8 pages", "summary": "This paper presents the modeling, control design, and performance analysis of a Magnetic Ball Suspension System (MBSS), a nonlinear and inherently unstable electromechanical system used in various precision applications. The system's primary objective is to levitate a steel ball using electromagnetic force without physical contact, thereby eliminating frictional losses. A comprehensive state-space model was developed, capturing both the mechanical and electrical dynamics. The equilibrium points of the system were determined through feedback linearization using the Jacobian matrix. To ensure system stability, controllability and observability analyses were conducted, confirming that state feedback and observer-based control strategies could be effectively implemented. Three distinct control methods were explored: pole placement-based state feedback control, full-order observer design, and optimal state feedback control using the Linear Quadratic Regulator (LQR). Each control strategy was validated through Simulink simulations for both linearized and nonlinear models. Simulation results demonstrated that the linearized system consistently achieved desired performance with minimal oscillations, whereas the nonlinear system exhibited significant transient oscillations before stabilization. The full-order observer enhanced estimation accuracy, enabling effective control where direct state measurement was impractical. The LQR-based control offered improved robustness and minimized control effort, though its performance was comparable to standard state feedback in some cases.", "AI": {"tldr": "本文介绍了磁悬浮球系统（MBSS）的建模、控制器设计和性能分析，该系统是一种用于精密应用的非线性不稳态机电系统。研究人员开发了系统模型，并通过反馈线性化确定了平衡点。分析了系统的可控性和可观测性，并探索了三种控制方法：极点配置、全阶观测器和LQR。仿真结果表明，线性化系统表现良好，而非线性系统在稳定前存在较大的瞬态振荡。全阶观测器提高了状态估计精度，LQR控制器提供了更好的鲁棒性和最小控制能量。", "motivation": "研究动机是为了开发一种能够精确控制磁悬浮球系统（MBSS）的控制策略，该系统在精密应用中具有消除摩擦损耗的潜力，但其固有的非线性和不稳定性带来了挑战。", "method": "研究人员首先建立了MBSS的包含机械和电气动态的状态空间模型。然后，利用反馈线性化和雅可比矩阵确定了系统的平衡点。进行了可控性和可观测性分析，以评估控制策略的可行性。最后，通过Simulink仿真验证了三种控制方法：基于极点配置的状态反馈控制、全阶观测器设计以及基于LQR的最优状态反馈控制，并对线性化和非线性模型进行了仿真。", "result": "仿真结果表明，对于线性化模型，所有控制策略都能实现期望的性能，且振荡较小。对于非线性模型，虽然系统最终能够稳定，但在瞬态响应中存在显著的振荡。全阶观测器显著提高了状态估计的准确性，在状态无法直接测量的情况下实现了有效的控制。LQR控制器在鲁棒性和最小化控制能量方面表现出色，但在某些情况下其性能与标准状态反馈相当。", "conclusion": "本文成功地对磁悬浮球系统进行了建模和控制设计，并验证了所提出的控制策略的有效性。研究证明了通过状态反馈和观测器设计可以稳定该非线性系统。LQR控制提供了更好的鲁棒性，而全阶观测器解决了实际应用中状态测量的限制。"}}
{"id": "2601.15419", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15419", "abs": "https://arxiv.org/abs/2601.15419", "authors": ["Yashuai Yan", "Dongheui Lee"], "title": "Learning a Unified Latent Space for Cross-Embodiment Robot Control", "comment": null, "summary": "We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.", "AI": {"tldr": "本研究提出一个可扩展的跨具身人形机器人控制框架，通过学习统一的共享潜在表示来实现跨人类和多种人形机器人（单臂、双臂、腿式）的动作统一。该框架分两阶段进行：首先利用对比学习构建解耦潜在空间，捕捉身体局部动作模式，实现精确灵活的动作重定向；然后直接在潜在空间中训练目标条件控制策略，仅使用人类数据，并能直接应用于新机器人，无需适配。", "motivation": "现有机器人控制方法通常是针对特定机器人设计的，缺乏通用性和可扩展性。在不同形态的人形机器人之间进行动作迁移和控制是一个挑战。研究旨在开发一个能够跨越不同机器人形态、实现统一且可扩展的机器人控制框架。", "method": "该方法分为两个阶段。第一阶段，构建一个解耦的潜在空间，利用对比学习捕捉身体局部动作模式，并引入结合关节旋转和末端执行器定位的相似性指标来增强不同具身间的对齐。第二阶段，利用条件变分自编码器，在潜在空间中训练一个目标条件控制策略，该策略仅使用人类数据，能够预测潜在空间中的位移，并由目标方向引导。", "result": "训练好的策略可以直接部署在多个机器人上而无需任何适配。该框架支持通过学习轻量级、机器人特定的嵌入层来高效地添加新机器人到潜在空间，并且可以将学习到的潜在策略直接应用于新机器人。实验结果表明，该方法在多种人形机器人平台上实现了鲁棒、可扩展且与具身无关的机器人控制。", "conclusion": "所提出的框架通过学习共享的潜在表示，成功实现了跨具身人形机器人控制的统一性、可扩展性和高效性，并能轻松支持新机器人的接入，为开发更通用的机器人控制系统提供了有效途径。"}}
{"id": "2601.15307", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15307", "abs": "https://arxiv.org/abs/2601.15307", "authors": ["Guo-Biao Zhang", "Ding-Yuan Liu", "Da-Yi Wu", "Tian Lan", "Heyan Huang", "Zhijing Wu", "Xian-Ling Mao"], "title": "DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey", "comment": null, "summary": "The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.", "AI": {"tldr": "本文提出了一种名为DeepSurvey-Bench的新型基准，用于评估自动生成调查的“学术价值”，解决了现有基准在选择真实调查和评估指标上的不足。", "motivation": "现有用于评估自动生成调查的基准存在两方面问题：1. 真实调查数据集的选择标准（如引用次数、结构连贯性）不可靠，缺乏学术维度标注；2. 评估指标仅关注表面质量（如逻辑连贯性），无法衡量深层学术价值（如核心研究目标、批判性分析）。", "method": "DeepSurvey-Bench基准通过以下方式构建：1. 提出包含信息价值、学术交流价值和研究指导价值三个维度的综合性学术价值评估标准；2. 基于此标准构建了带有学术价值标注的可靠数据集；3. 利用该数据集评估生成调查的深层学术价值。", "result": "实验结果表明，DeepSurvey-Bench在评估生成调查的学术价值方面与人类评估者表现出高度一致性。", "conclusion": "DeepSurvey-Bench是一个新颖的基准，能够更全面、更可靠地评估自动生成调查的深层学术价值，克服了现有基准的局限性。"}}
{"id": "2601.15626", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15626", "abs": "https://arxiv.org/abs/2601.15626", "authors": ["Lili Chen", "Winn Wing-Yiu Chow", "Stella Peng", "Bencheng Fan", "Sachitha Bandara"], "title": "Bridging Qualitative Rubrics and AI: A Binary Question Framework for Criterion-Referenced Grading in Engineering", "comment": "Proceedings of the 36th Annual Conference of the Australasian Association for Engineering Education (AAEE 2025)", "summary": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The study found that GenAI achieved an overall grading accuracy of 92.5%, comparable to two experienced human graders. The two researchers, who also served as subject demonstrators, perceived the GenAI as a helpful second reviewer that improved accuracy by catching small errors and provided more complete feedback than they could manually. A central outcome was the significant enhancement of formative feedback. However, they noted the GenAI tool is not yet reliable enough for autonomous use, especially with unconventional solutions. CONCLUSIONS/RECOMMENDATIONS/SUMMARY: This study demonstrates that GenAI, when paired with a structured, criterion-referenced framework using binary questions, can grade engineering mathematical assessments with an accuracy comparable to human experts. Its primary contribution is a novel methodological approach that embeds the generation of high-quality, scalable formative feedback directly into the assessment workflow. Future work should investigate student perceptions of GenAI grading and feedback.", "AI": {"tldr": "该研究探索了如何将生成式人工智能（GenAI）与标准参照评分框架相结合，以提高工程数学评估的评分效率和质量，并评估了人类评分员对该方法的看法。", "motivation": "研究旨在解决人工、基于模型解题的评分方法中，助教在评分数学评估时面临的效率和质量挑战，并探索GenAI如何提供更优的解决方案，同时收集人类评分员的反馈。", "method": "研究将GenAI集成到一个基于二元问题和标准参照的评分框架中，以评估工程数学评估。通过比较GenAI与人类评分员的评分准确性，以及收集人类评分员（研究人员）对GenAI辅助评分的看法来评估其有效性。", "result": "GenAI在评分准确性上达到了92.5%，与经验丰富的人类评分员相当。人类评分员认为GenAI是一个有用的辅助工具，能捕捉细微错误并提供更全面的反馈，尤其在形成性反馈方面有显著提升。然而，GenAI在处理非常规解法时仍不可靠，不适合独立使用。", "conclusion": "将GenAI与标准参照评分框架（特别是基于二元问题）相结合，可以实现与人类专家相当的工程数学评估评分准确性。该研究提出了一种将高质量、可扩展的形成性反馈直接融入评估工作流程的新方法。未来的研究应关注学生对GenAI评分和反馈的看法。"}}
{"id": "2601.15459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15459", "abs": "https://arxiv.org/abs/2601.15459", "authors": ["Sarvin Ghiasi", "Majid Roshanfar", "Jake Barralet", "Liane S. Feldman", "Amir Hooshiar"], "title": "Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation", "comment": null, "summary": "This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.", "AI": {"tldr": "本研究提出了一种集成框架，通过结合分析建模、实时仿真和机器学习，提高腹腔镜手术中机器人手臂的安全性和操作效率，重点关注碰撞检测和最小距离估计。", "motivation": "提高腹腔镜手术中机器人手臂在碰撞检测和最小距离估计方面的安全性和操作效率。", "method": "开发了一个分析模型用于理论计算最小距离，构建了一个3D仿真环境生成数据集，并训练了一个深度神经网络模型来预测机器人手臂之间的距离。", "result": "训练好的深度神经网络模型在预测最小距离时实现了282.2 mm 的平均绝对误差和 0.85 的 R 平方值，显示了其准确性和泛化能力。", "conclusion": "结合分析模型的精确性和机器学习算法的预测能力，可以有效地提高机器人系统的精度和可靠性，从而增强手术安全性。"}}
{"id": "2601.16011", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16011", "abs": "https://arxiv.org/abs/2601.16011", "authors": ["Theodor Forgaard", "Jarle H. Reksten", "Anders U. Waldeland", "Valerio Marsocci", "Nicolas Longépé", "Michael Kampffmeyer", "Arnt-Børre Salberg"], "title": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications", "comment": "25 pages", "summary": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.", "AI": {"tldr": "本文提出了一种名为 THOR 的计算自适应基础模型，解决了现有地球观测模型在处理异构传感器、固定块大小以及计算-精度权衡方面的限制。THOR 能够统一处理不同分辨率的卫星数据，并通过新颖的随机块和输入图像大小策略进行预训练，使其能够在推理时动态调整计算成本和特征分辨率。", "motivation": "现有地球观测基础模型存在架构僵化、难以处理异构传感器和固定块大小的问题，这限制了它们在需要灵活计算-精度权衡的实际场景中的应用。", "method": "提出了一种名为 THOR 的计算自适应基础模型，它能够统一处理来自 Sentinel-1、-2 和 -3（OLCI & SLSTR）卫星的数据，并处理其原生分辨率。通过新颖的随机块和输入图像大小策略进行预训练，使得单个预训练权重集能够在推理时以任何块大小部署，从而实现计算成本和特征分辨率之间的动态权衡。", "result": "THOR 在 THOR Pretrain 数据集上进行了预训练，并在下游基准测试中取得了最先进的性能，尤其是在数据受限的情况下（如 PANGAEA 10% split），证明了其灵活的特征生成能力在多样化的气候和社会应用中表现出色。", "conclusion": "THOR 是第一个能够统一处理多种异构传感器数据、解决输入异质性和部署刚性问题，并实现计算-精度动态权衡的地球观测基础模型。"}}
{"id": "2601.15311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15311", "abs": "https://arxiv.org/abs/2601.15311", "authors": ["Mustafa Arslan"], "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to \"Vector Haze\", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.", "AI": {"tldr": "本文提出了一种名为 Aeon 的神经符号认知操作系统，通过构建记忆宫殿和痕迹图来解决 LLM 在长上下文窗口下的“Lost in the Middle”问题和向量数据库的“Vector Haze”问题，实现了亚毫秒级的记忆检索，并保证了状态一致性。", "motivation": "现有的 LLM 存在二次计算成本和“Lost in the Middle”现象，而基于向量数据库的 RAG 方法无法捕捉长时交互的结构化和时间性，导致检索到的信息缺乏连贯性。", "method": "Aeon 将记忆构建为一种操作系统资源，包含：1. 记忆宫殿（通过 Atlas 实现的空间索引，结合小世界图导航和 B+ 树磁盘局部性）；2. 痕迹（神经符号化的事件图）；3. 语义查找缓冲区（SLB），一种利用对话局部性实现亚毫秒级检索的预测性缓存机制。", "result": "Aeon 在对话工作负载下实现了低于 1 毫秒的检索延迟，并通过零拷贝 C++/Python 桥保证了状态一致性。", "conclusion": "Aeon 是一种有效的解决方案，能够为自主智能体提供持久、结构化的记忆，克服了现有 LLM 和 RAG 方法的局限性。"}}
{"id": "2601.15299", "categories": ["cs.CL", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15299", "abs": "https://arxiv.org/abs/2601.15299", "authors": ["Yash Sharma"], "title": "MALTopic: Multi-Agent LLM Topic Modeling Framework", "comment": "6 pages. Published in 2025 IEEE World AI-IoT Congress. \\c{opyright} 2025 IEEE. Project code and data available at: https://github.com/yash91sharma/MALTopic", "summary": "Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.", "AI": {"tldr": "本文提出了一种名为MALTopic的多智能体LLM主题建模框架，该框架通过整合结构化数据和分解任务给不同的LLM智能体，显著提高了主题模型在调查数据分析中的连贯性、多样性和可解释性。", "motivation": "传统的主题建模方法通常只考虑非结构化文本数据，无法原生纳入结构化或分类调查问卷回答，并且生成的抽象主题需要大量人工解释。这促使研究者开发能够同时处理结构化和非结构化数据，并生成更易于理解的主题模型。", "method": "MALTopic框架将主题建模分解为三个由LLM智能体执行的专业任务：1. 增强智能体（enrichment agent）利用结构化数据丰富文本回应；2. 主题建模智能体（topic modeling agent）提取潜在主题；3. 去重智能体（deduplication agent）优化和精炼主题结果。", "result": "与LDA和BERTopic等传统方法相比，MALTopic在调查数据集上的比较分析显示，在主题连贯性、多样性和可解释性方面均有显著提升。MALTopic生成的易于人类阅读的主题具有更强的上下文相关性。", "conclusion": "通过整合结构化数据和采用多智能体方法，MALTopic为分析复杂的调查数据提供了一个更有效的解决方案，能够生成更具可读性和上下文相关性的主题。"}}
{"id": "2601.15416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15416", "abs": "https://arxiv.org/abs/2601.15416", "authors": ["Cuong Tran Van", "Trong-Thang Pham", "Ngoc-Son Nguyen", "Duy Minh Ho Nguyen", "Ngan Le"], "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction", "comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)", "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.", "AI": {"tldr": "本文提出了一种名为 DuFal 的新框架，通过双路径架构融合了频域和空域处理，以解决稀疏视图锥束 CT 重建中高频细节恢复困难的问题。该框架的核心是高局部因子化傅里叶神经算子，包含全局和局部高频增强分支。通过频谱通道因子化和交叉注意力频率融合模块提升了效率和特征融合能力，最终在 LUNA16 和 ToothFairy 数据集上取得了显著的重建效果，尤其是在极稀疏视图条件下。", "motivation": "传统的卷积神经网络（CNN）方法在稀疏视图 CBCT 重建中难以恢复高频细节，因为它们倾向于学习低频信息。这导致对精细解剖结构的重建不佳。", "method": "本文提出了 DuFal 框架，采用双路径架构融合频域和空域处理。核心是高局部因子化傅里叶神经算子，包含：1. 全局高频增强傅里叶神经算子（捕捉全局频率模式）；2. 局部高频增强傅里叶神经算子（处理空间分割的块以保留空间局部性）。此外，还设计了频谱通道因子化以减少参数量，以及交叉注意力频率融合模块来整合空间和频率特征。", "result": "DuFal 在 LUNA16 和 ToothFairy 数据集上的实验结果表明，该方法在保留高频解剖细节方面显著优于现有最先进的方法，特别是在极端稀疏视图设置下。", "conclusion": "DuFal 框架通过其新颖的双路径架构和频率增强机制，有效地解决了稀疏视图 CBCT 重建中的高频细节丢失问题，为医学成像提供了更优的解决方案。"}}
{"id": "2601.15816", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15816", "abs": "https://arxiv.org/abs/2601.15816", "authors": ["Shiqi Wei", "Qiqing Wang", "Kaidi Yang"], "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents", "comment": null, "summary": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability.", "AI": {"tldr": "本研究提出了一种分层框架，利用大型语言模型（LLMs）增强现有的交通信号控制（TSC）系统，通过虚拟交通警察代理动态调整信号控制器参数，以应对突发交通事件。研究设计了一个自精炼交通语言检索系统（TLRS）和基于LLM的验证器，以提高模型在特定领域的可靠性。实验证明，该方法能显著提高TSC系统在应对突发事件时的效率和可靠性。", "motivation": "传统的交通信号控制（TSC）系统在应对突发交通事件（如事故、道路维护）时效果不佳，需要人工干预。现有基于LLM的TSC研究倾向于完全替代现有系统，但这可能导致不可靠（LLM幻觉）和成本高昂（系统更换）。因此，需要一种能增强现有TSC系统而非完全替代的方法。", "method": "提出一个分层框架，上层是虚拟交通警察代理（基于LLM），下层是信号控制器。该框架利用检索增强生成（RAG）技术，通过一个自精炼交通语言检索系统（TLRS）访问专门的交通语言数据库，以获取交通状况和控制器操作知识。此外，还设计了一个基于LLM的验证器，在推理过程中持续更新TLRS。", "result": "实验结果表明，LLMs可以作为可信赖的虚拟交通警察，能够适应传统的TSC方法，以应对突发交通事件。该方法在提高操作效率和可靠性方面表现出色。", "conclusion": "LLMs可以有效增强现有的交通信号控制系统，使其能够应对突发交通事件，提高整体的效率和可靠性，而无需完全替换现有系统。"}}
{"id": "2601.16064", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16064", "abs": "https://arxiv.org/abs/2601.16064", "authors": ["Shams Nafisa Ali", "Taufiq Hasan"], "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation", "comment": "10 pages, 7 figures", "summary": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.", "AI": {"tldr": "本研究提出了Phi-SegNet，一种结合相位信息（频率域特征）的深度学习模型，用于提高医学图像分割的泛化能力和边界精度。", "motivation": "现有的深度学习医学图像分割模型在处理不同成像模态和解剖结构时泛化能力不足，主要原因是它们侧重于空间信息而忽略了频率域信息，而频率域信息富含结构和纹理线索。尤其是在监督层面利用频率信息以实现精细定位仍未被充分探索。", "method": "提出了一种基于CNN的Phi-SegNet架构，该架构在架构和优化层面都集成了相位感知信息。具体包括：1. Bi-Feature Mask Former (BFMF) 模块，用于融合相邻编码器特征以减小语义鸿沟。2. Reverse Fourier Attention (RFA) 块，利用相位正则化特征来优化解码器输出。3. 专门的相位感知损失函数，使特征与结构先验对齐，形成强调边界精度的闭环反馈。", "result": "Phi-SegNet在X-ray、US、组织病理学、MRI和结肠镜五种公开数据集上进行了评估，在IoU和F1-score方面均取得了最先进的性能，平均相对提升分别为1.54+/-1.26%和0.98+/-0.71%。在跨数据集泛化测试中，Phi-SegNet也表现出鲁棒性和优越性。", "conclusion": "研究表明，在特征表示和监督层面利用谱先验（相位信息）的潜力，能够显著提升医学图像分割的泛化能力和边界定位精度。Phi-SegNet的成功为开发在精细对象定位方面表现出色的通用化分割框架铺平了道路。"}}
{"id": "2601.15486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15486", "abs": "https://arxiv.org/abs/2601.15486", "authors": ["Javier N. Ramos-Silva", "Peter J. Burke"], "title": "A Universal Large Language Model -- Drone Command and Control Interface", "comment": null, "summary": "The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.", "AI": {"tldr": "该研究提出了一种通用的、独立于LLM和无人机的模型上下文协议（MCP）接口，用于将大型语言模型（LLMs）与无人机控制集成，实现了自然语言到无人机指令的转换，并在真实和模拟环境中进行了验证。", "motivation": "将大型语言模型（LLMs）的通用知识和实时数据（如地图、天气）与无人机感知、指挥和控制相结合，以提升无人机能力，但目前LLM与无人机控制的接口集成过程繁琐且耗时。", "method": "提出了一种模型上下文协议（MCP）标准，该标准独立于LLM和无人机，提供了一种通用的方式让AI系统访问外部数据、工具和服务。基于MCP标准，开发并部署了一个基于云的Linux服务器，该服务器支持Mavlink协议（广泛用于无人机控制），实现了LLM与无人机控制的连接。", "result": "成功演示了真实无人机的飞行控制，以及在模拟无人机中进行详细的飞行规划和控制，并集成了Google Maps MCP服务器以获取实时导航信息。", "conclusion": "该研究提供了一种通用、易用的接口策略，能够将LLMs与无人机指挥和控制进行集成，显著简化了这一过程，为利用现代AI技术赋能无人机技术开辟了新途径。"}}
{"id": "2601.15316", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15316", "abs": "https://arxiv.org/abs/2601.15316", "authors": ["Wei Ai", "Yilong Tan", "Yuntao Shou", "Tao Meng", "Haowen Chen", "Zhixiong He", "Keqin Li"], "title": "The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection", "comment": null, "summary": "In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.", "AI": {"tldr": "本篇论文对大型视觉语言模型（LVLMs）在多模态假新闻检测（MFND）领域的应用进行了全面的综述，分析了从传统方法到基于LVLMs的范式的演变，并探讨了当前的挑战和未来的研究方向。", "motivation": "现有研究缺乏对多模态假新闻检测（MFND）领域从传统方法向基于大型视觉语言模型（LVLMs）的范式转变的系统性梳理和最新进展的总结。", "method": "本文采用文献综述的方法，首先回顾了MFND的历史演变，从浅层融合技术到基于LVLMs的统一端到端框架。然后，作者构建了一个关于模型架构、数据集和性能基准的分类体系。此外，还分析了当前面临的技术挑战，如可解释性、时间推理和领域泛化，并提出了未来的研究方向。", "result": "该研究系统性地梳理了LVLMs在MFND领域的应用，展示了LVLMs如何通过联合建模视觉和语言能力，提升了对包含文本和图像信息的虚假信息的检测能力。文章提供了一个MFND的结构化分类体系，并指出了当前研究面临的关键挑战。", "conclusion": "大型视觉语言模型（LVLMs）正在对多模态假新闻检测（MFND）产生革命性影响，使得从传统的特征工程方法转向更强大的端到端多模态推理框架。尽管取得了显著进展，但在可解释性、时间推理和领域泛化等方面仍存在挑战，未来的研究应着重于解决这些问题，以进一步推动该领域的进步。"}}
{"id": "2601.15300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15300", "abs": "https://arxiv.org/abs/2601.15300", "authors": ["Weiwei Wang", "Jiyong Min", "Weijie Zou"], "title": "Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis", "comment": "29 pages", "summary": "Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.", "AI": {"tldr": "大型语言模型（LLM）在处理接近特定临界阈值的上下文时，性能会急剧下降，即使信息仍然相关。本研究通过分析自然长度分布、确定临界阈值，并提出了一个统一的框架来解释这种“浅层长上下文适应”现象，为缓解LLM在长上下文场景下的性能下降提供了指导。", "motivation": "现有的大型语言模型在处理接近特定临界长度的上下文时，会表现出严重的性能下降（智能退化），这极大地限制了长上下文应用的潜力。作者希望系统地研究并解释这种现象，为解决这一问题提供基础。", "method": "1. 自然长度分布分析：使用样本的自然token长度（不截断或填充）来研究上下文长度与模型性能的关系。 2. 临界阈值确定：在包含1000个样本的混合数据集上进行实验，覆盖了最大上下文长度的5%-95%，并使用五种交叉验证方法来确定Qwen2.5-7B模型的临界阈值（在最大上下文长度的40-50%处）。 3. 统一框架：整合现有研究，解释模型为何只适应浅层上下文而无法适应长上下文，并为后续的缓解策略奠定基础。", "result": "研究发现，LLM的性能下降表现出一种共同模式：在达到一个临界阈值之前性能良好，然后急剧下降。对于Qwen2.5-7B模型，在最大上下文长度的40-50%处存在一个临界阈值，此时F1分数从0.55-0.56急剧下降到0.3，性能退化高达45.5%。", "conclusion": "本研究首次系统地刻画了开源Qwen模型在长上下文处理中的智能退化现象，确定了其临界阈值，并提出了“浅层长上下文适应”的解释框架，为在长上下文场景中部署LLM提供了实用的指导。"}}
{"id": "2601.16014", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16014", "abs": "https://arxiv.org/abs/2601.16014", "authors": ["Eder Baron-Prada", "Adolfo Anta", "Florian Dörfler"], "title": "Stability Analysis of Power-Electronics-Dominated Grids Using Scaled Relative Graphs", "comment": "Submitted to possible publication", "summary": "This paper presents a novel approach to stability analysis for grid-connected converters utilizing Scaled Relative Graphs (SRG). Our method effectively decouples grid and converter dynamics, thereby establishing a comprehensive and efficient framework for evaluating closed-loop stability. Our analysis accommodates both linear and non-linear loads, enhancing its practical applicability. Furthermore, we demonstrate that our stability assessment remains unaffected by angular variations resulting from dq-frame transformations, significantly increasing the method's robustness and versatility. The effectiveness of our approach is validated in several simulation case studies, which illustrate its broad applicability in modern power systems.", "AI": {"tldr": "本文提出一种基于缩放相对图（SRG）的新型并网变换器稳定性分析方法，该方法能有效解耦电网与变换器动力学，并处理线性和非线性负载，且不受dq变换角度变化影响，通过仿真验证了其有效性和广泛适用性。", "motivation": "现有并网变换器的稳定性分析方法存在复杂、适用性受限等问题，作者旨在提出一种更全面、高效且鲁棒的稳定性评估框架。", "method": "使用缩放相对图（SRG）技术分析并网变换器的闭环稳定性，该方法能够解耦电网和变换器动力学，并适用于线性和非线性负载，同时不受dq坐标变换角度变化的影响。", "result": "所提出的SRG方法成功实现了对并网变换器闭环稳定性的全面评估，并证明了该方法在处理不同类型的负载以及dq坐标变换时的鲁棒性。", "conclusion": "基于SRG的稳定性分析方法是一种有效、全面且鲁棒的工具，可用于评估并网变换器的闭环稳定性，适用于现代电力系统的各种场景。"}}
{"id": "2601.15541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15541", "abs": "https://arxiv.org/abs/2601.15541", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Qiyi Tong", "Gokhan Solak", "Puze Liu", "Sheng Liu", "Jan Peters", "Arash Ajoudani"], "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation", "comment": "under review", "summary": "We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\\% to 17.29\\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.", "AI": {"tldr": "提出了一种名为CompliantVLA-adaptor的方法，通过结合视觉-语言模型（VLM）和变阻抗控制（VIC），增强了现有的视觉-语言-动作（VLA）模型，以提高接触式机器人操作任务的安全性和有效性。", "motivation": "现有VLA系统在进行接触、顺应性或不确定性任务时，由于缺乏力感知能力，往往输出位置控制，容易导致不安全或失败的交互。", "method": "CompliantVLA-adaptor利用VLM解析图像和自然语言的上下文信息，来调整VIC控制器的刚度和阻尼参数。同时，利用实时力/扭矩反馈来进一步调节这些参数，确保交互力在安全阈值内。", "result": "在模拟和真实硬件环境下，CompliantVLA-adaptor在多种复杂的接触式任务中，相比于基线VLA方法，成功率从9.86%提高到17.29%，并且力违规行为减少。", "conclusion": "CompliantVLA-adaptor是一种有效的方法，能够利用VLM增强VLA模型在接触式机器人操作任务中的安全性和性能，为实现安全的接触式操作开辟了新途径。"}}
{"id": "2601.15301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15301", "abs": "https://arxiv.org/abs/2601.15301", "authors": ["Jivnesh Sandhan", "Harshit Jaiswal", "Fei Cheng", "Yugo Murawaki"], "title": "Can We Trust LLM Detectors?", "comment": "NLP2026, Utsunomiya, Japan", "summary": "The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI", "AI": {"tldr": "现有AI文本检测器在实际应用中表现不佳，本文提出一种基于监督对比学习的新方法，旨在提高其鲁棒性。", "motivation": "现有AI文本检测方法在受控基准测试之外的实际场景中表现不佳，需要更可靠的文本检测技术。", "method": "评估了两种主流的AI文本检测范式（无训练和监督学习），并提出了一种基于监督对比学习（SCL）的框架，以学习具有辨别力的风格嵌入。", "result": "实验表明，虽然监督学习方法在同域数据上表现优异，但在跨域数据上性能急剧下降；无训练方法对代理选择非常敏感。研究揭示了构建领域无关检测器的根本性挑战。", "conclusion": "AI文本检测面临领域适应性差的根本性挑战，现有的模型在面对分布外数据和生成器变化时表现脆弱。提出的SCL框架在一定程度上改善了风格嵌入的辨别力，但仍需进一步研究以实现领域无关的检测。"}}
{"id": "2601.15453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15453", "abs": "https://arxiv.org/abs/2601.15453", "authors": ["Morteza Poudineh", "Marc Lalonde"], "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection", "comment": "8 pages", "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.", "AI": {"tldr": "本研究提出了一种少样本异常检测新框架，通过可学习的提示和基于偏差的评分机制，提高了对图像中异常区域的检测能力。", "motivation": "现有基于视觉语言模型的少样本异常检测方法在区分正常和异常提示方面存在不足，并且缺乏有效的局部异常评分机制。", "method": "提出了一种偏差引导的提示学习框架，使用可学习的上下文向量替换固定的提示前缀，并引入特定于异常的后缀令牌。同时，结合了基于偏差的损失函数和 Top-K 多实例学习 (MIL)，将图像块特征建模为与正常分布的偏差，从而提高异常得分的区分度。", "result": "在 MVTecAD 和 VISA 基准测试中，所提出的框架在像素级异常检测性能上优于 PromptAD 和其他基线方法。消融实验证明了可学习提示、基于偏差的评分以及 Top-K MIL 策略的有效性。", "conclusion": "该框架通过融合视觉语言模型和统计偏差评分，有效解决了少样本异常检测中的挑战，提高了异常检测的性能和可解释性。"}}
{"id": "2601.15475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "提出了一种基于传感器物理学的NeRF框架，用于从单次曝光的模糊LDR图像和事件数据中合成清晰的HDR新视角。", "motivation": "现有方法在处理野外常见的LDR模糊图像进行HDR和3D重建时，未能充分考虑相机输出与物理世界辐射度之间的传感器物理不匹配问题，导致HDR和去模糊效果不佳。", "method": "利用NeRF直接表示HDR域中的3D场景真实辐射度，并模拟物理世界中传感器像素接收到的HDR光线。引入像素级RGB映射场将渲染像素值与传感器记录的LDR像素值对齐，设计了事件映射场连接物理场景动态和事件传感器输出。RGB映射场和事件映射场与NeRF网络联合优化，利用事件的时空动态信息增强3D表示学习。", "result": "在收集和公开数据集上进行了实验，结果表明该方法在单次曝光模糊LDR图像和事件数据上实现了最先进的去模糊HDR新视角合成效果。", "conclusion": "所提出的传感器物理学NeRF框架能够有效地结合LDR图像和事件数据，克服传感器物理不匹配的挑战，从而实现高质量的锐利HDR新视角合成。"}}
{"id": "2601.15322", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15322", "abs": "https://arxiv.org/abs/2601.15322", "authors": ["Raffi Khatchadourian"], "title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "comment": "23 pages, 5 figures, 9 tables. Code and data: https://github.com/ibm-client-engineering/output-drift-financial-llms", "summary": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "AI": {"tldr": "研究提出了一种名为DFAH的框架，用于衡量金融领域LLM代理的可复现性和忠实度，发现其在实际应用中存在不确定性，但模型的可复现性与其决策的准确性呈正相关，并提供了三个金融基准测试集和一个开源工具。", "motivation": "LLM代理在金融监管审计回放中表现出不确定性，无法保证相同的输入产生相同的结果，这阻碍了其在金融服务中的可靠部署。", "method": "提出Determinism-Faithfulness Assurance Harness (DFAH)框架，用于测量工具使用型LLM代理的轨迹确定性和证据条件下的忠实度。通过在不同模型（7-20B和120B+参数）、模型提供商和多次运行配置下进行基线实验，并引入了三个金融领域基准测试（合规分类、投资组合约束、DataOps异常）。", "result": "基线实验中，7-20B模型实现了100%的确定性，而120B+模型需要更大的验证样本。工具使用引入了额外的变异性。确定性和忠实度之间存在正相关（r=0.45, p<0.01），表明可复现性强的模型也更倾向于遵循证据。在DFAH评估下，采用Schema-first架构的一线模型在确定性方面达到了审计回放的要求。", "conclusion": "LLM代理在金融领域的可靠部署面临确定性和忠实度挑战。DFAH框架能够有效评估这些方面，并证明了可复现性与准确性之间的积极关系。Schema-first架构的模型在满足审计回放的确定性要求方面表现优异。"}}
{"id": "2601.16061", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16061", "abs": "https://arxiv.org/abs/2601.16061", "authors": ["John Bannan", "Nazia Rahman", "Chang-Hee Won"], "title": "Dynamic Tactile Sensing System and Soft Actor Critic Reinforcement Learning for Inclusion Characterization", "comment": null, "summary": "This paper presents the Dynamic Tactile Sensing System that utilizes robotic tactile sensing in conjunction with reinforcement learning to locate and characterize embedded inclusions. A dual arm robot is integrated with an optical Tactile Imaging Sensor that utilizes the Soft Actor Critic Algorithm to acquire tactile data based on a pixel intensity reward. A Dynamic Interrogation procedure for tactile exploration is developed that enables the robot to first localize inclusion and refine their positions for precise imaging. Experimental validation conducted on Polydimethylsiloxane phantoms demonstrates that the robot using the Tactile Soft Actor Critic Model was able to achieve size estimation errors of 2.61% and 5.29% for soft and hard inclusions compared to 7.84% and 6.87% for expert human operators. Results also show that Dynamic Tactile Sensing System was able to locate embedded inclusions and autonomously determine their mechanical properties, useful in applications such as breast tumor characterization.", "AI": {"tldr": "本文提出了一种动态触觉感知系统，结合了机器人触觉传感和强化学习，用于定位和表征嵌入式包体。该系统使用双臂机器人和光学触觉成像传感器，并采用软体演员-评论家（SAC）算法，通过像素强度奖励来获取触觉数据。通过动态询问程序，机器人可以先定位包体，再精确成像。实验结果表明，该系统在软包体和硬包体的大小估计误差方面优于人类操作员，并能自主确定其机械特性，适用于乳腺肿瘤表征等应用。", "motivation": "为了实现对嵌入式包体的精确检测和表征，尤其是在医学诊断（如乳腺肿瘤表征）等应用中。", "method": "利用双臂机器人集成光学触觉成像传感器，采用软体演员-评论家（SAC）算法进行触觉数据采集，并设计了动态询问程序进行触觉探索，以实现包体的定位和精确成像。", "result": "该动态触觉感知系统在软包体和硬包体的大小估计误差上分别为2.61%和5.29%，显著优于人类操作员（分别为7.84%和6.87%）。系统能够成功定位嵌入式包体并自主确定其机械特性。", "conclusion": "动态触觉感知系统结合机器人触觉传感和强化学习，能够有效地定位和表征嵌入式包体，并在大小估计和机械特性确定方面表现出优于人类操作员的性能，为乳腺肿瘤表征等应用提供了有前景的解决方案。"}}
{"id": "2601.15545", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15545", "abs": "https://arxiv.org/abs/2601.15545", "authors": ["Zhifan Yan", "Chang Liu", "Yiyang Jiang", "Wenxuan Zheng", "Xinhao Chen", "Axel Krieger"], "title": "A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control", "comment": null, "summary": "Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a \"model-calibration bottleneck\", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.", "AI": {"tldr": "本文提出了一种使用深度强化学习（DRL）控制的紧凑型、低成本移动磁力操控平台，用于在胃肠道中进行靶向药物输送。该平台通过一个UR5机器人上的四电磁铁阵列实现，并使用Soft Actor-Critic（SAC）算法训练。该系统仅需15分钟即可部署，无需复杂的模型校准，能够在大工作空间内精确控制磁性胶囊沿预设轨迹运动，误差较小。", "motivation": "现有的磁控机器人系统在胃肠道靶向药物输送方面面临控制挑战，如固定系统工作空间有限，移动系统需要耗时且计算量大的模型校准。因此，需要一种更易于部署、无需复杂模型且能实现精确控制的磁力操控平台。", "method": "研究人员设计了一个紧凑型、低成本的移动磁力操控平台，由一个UR5协作机器人搭载四电磁铁阵列组成。他们采用基于Soft Actor-Critic（SAC）的深度强化学习（DRL）策略，通过仿真到现实（sim-to-real）的训练流程进行控制策略的学习。该方法无需预先校准的物理模型。", "result": "该DRL控制策略在15分钟内即可完成训练和部署。在控制7毫米磁性胶囊沿2D轨迹运动的实验中，对于方形轨迹，均方根误差（RMSE）为1.18毫米；对于圆形轨迹，RMSE为1.50毫米。系统成功在一个30厘米*20厘米的临床相关工作空间内实现了轨迹跟踪。", "conclusion": "本文成功展示了一个可快速部署、无需模型的DRL控制框架，能够在大工作空间内实现精确的磁力操控。该平台通过2D胃肠道模型进行了验证，为胃肠道靶向药物输送提供了有前景的解决方案。"}}
{"id": "2601.15330", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15330", "abs": "https://arxiv.org/abs/2601.15330", "authors": ["Zhebo Wang", "Xiaohu Mu", "Zijie Zhou", "Mohan Li", "Wenpeng Xing", "Dezhang Kong", "Meng Han"], "title": "ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation", "comment": "Accepted by ICASSP 2026", "summary": "Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.", "AI": {"tldr": "提出了一种名为ICPO的新型训练框架，通过在训练语料库中加入模糊指令并根据用户意图调整奖励信号，来解决大语言模型在多轮对话中因早期错误假设而“迷失”的问题，提高了模型的鲁棒性和协作性。", "motivation": "标准的大语言模型在多轮对话中容易出现“迷失”，即难以纠正早期错误假设，尤其是在用户初始指令模糊时。现有的RLVR等后训练技术会加剧此问题，因为它们奖励自信直接的回答，导致模型过度自信而不寻求澄清。", "method": "提出Illocution-Calibrated Policy Optimization (ICPO)训练框架。该框架通过以下方式解决问题：1. 扩充训练语料库，加入“信息不足”（underspecified）的提示。2. 将奖励信号与用户的“言外之意”（illocutionary intent）挂钩，当模型面对模糊指令时，奖励模型表达不确定性或寻求澄清。", "result": "实验证明，ICPO能够培养模型适当的“谦逊”，在多轮对话中平均带来75%的显著提升，同时在单轮对话基准测试中保持了强大的性能。", "conclusion": "ICPO为开发更鲁棒、更协作的对话式AI提供了一条实际可行的路径，使其能更好地应对人机交互中的细微差别。"}}
{"id": "2601.16149", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.16149", "abs": "https://arxiv.org/abs/2601.16149", "authors": ["Zirui Niu", "Giordano Scarciotti", "Alessandro Astolfi"], "title": "Interconnection-based Model Reduction for Linear Hybrid Systems", "comment": "17 pages", "summary": "In this paper, we address the model reduction problem for linear hybrid systems via the interconnection-based technique called moment matching. We consider two classical interconnections, namely the direct and swapped interconnections, in the hybrid setting, and we present families of reduced-order models for each interconnection via a hybrid characterisation of the steady-state responses. By combining the results for each interconnection, the design of a reduced-order model that achieves moment matching simultaneously for both interconnections is studied. In addition, we show that the presented results have simplified counterparts when the jumps of the hybrid system are periodic. A numerical simulation is finally given to illustrate the results.", "AI": {"tldr": "本文提出了一种基于矩匹配的混合系统降阶方法，考虑了直接和交换两种混合互连方式，并给出了相应降阶模型的构造方法。同时，研究了如何设计能够同时匹配两种互连方式矩的降阶模型，并探讨了系统跳变周期性时的简化情况，最后通过数值模拟验证了方法的有效性。", "motivation": "解决线性混合系统的模型降阶问题，特别是通过矩匹配技术。", "method": "利用混合系统稳态响应的特征，考虑了直接互连和交换互连两种经典互连方式，通过矩匹配技术构造降阶模型。研究了同时匹配两种互连方式矩的降阶模型设计，并分析了跳变周期性时的简化情况。", "result": "提出了用于两种混合互连方式的降阶模型族。设计了一种能够同时为两种互连方式实现矩匹配的降阶模型。给出了跳变周期性时的简化结果。", "conclusion": "所提出的方法能够有效地对线性混合系统进行降阶，并且在跳变周期性时存在简化形式。数值模拟验证了该方法的有效性。"}}
{"id": "2601.15490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15490", "abs": "https://arxiv.org/abs/2601.15490", "authors": ["Jobeal Solomon", "Ali Mohammed Mansoor Alsahag", "Seyed Sahand Mohammadi Ziabari"], "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis", "comment": null, "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.", "AI": {"tldr": "研究表明，使用 Vision Transformer (ViT) 替换 U-Net 卷积编码器，可以有效减少胸部 X 光片分类器中的性别和年龄偏见，同时保持诊断准确性。", "motivation": "现有的基于卷积网络的像素空间属性中和方法未能完全消除胸部 X 光片分类中的性别和年龄偏见，导致少数群体被系统性低估。", "method": "作者将 Attribute-Neutral Framework 中的 U-Net 卷积编码器替换为 Vision Transformer (DeiT-S) 作为骨干网络，并在 ChestX-ray14 数据集上进行训练。通过调整编辑强度（alpha 值），生成编辑后的图像，并使用独立的 AI 裁判评估属性泄露，使用卷积神经网络 (ConvNet) 评估疾病预测准确性。", "result": "在适度的编辑强度（alpha = 0.5）下，ViT 中和器将患者性别识别的 AUC 降低到约 0.80，比原始 U-Net 框架低约 10 个百分点，尽管训练的 epoch 数减半。同时，在 15 种疾病的宏观 ROC AUC 保持在未编辑基线 5 个百分点以内，最差子群体的 AUC 接近 0.70。", "conclusion": "基于全局自注意力的 Vision Transformer 模型可以进一步抑制胸部 X 光片 AI 中的属性偏见，而不会牺牲临床效用，为实现更公平的胸部 X 光片 AI 提供了一条实用的途径。"}}
{"id": "2601.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15607", "abs": "https://arxiv.org/abs/2601.15607", "authors": ["Lenworth Thomas", "Tjaden Bridges", "Sarah Bergbreiter"], "title": "Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor", "comment": null, "summary": "As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.", "AI": {"tldr": "研究者开发了一种结合化学羽流追踪和气流源追踪的方法，用于小型无人机。该方法利用一种新型流传感器，该传感器可以测量气流的大小和方向，并实现了一个改进的'Cast and Surge'算法，使其能够找到并导航到气流源。实验证明了该系统的可靠性和有效性。", "motivation": "环境灾害频发，对污染物或有害颗粒的追踪变得越来越重要。在小型四旋翼无人机上进行羽流追踪具有优势，但现有气体传感器灵敏度低、响应慢，给追踪带来挑战。因此，研究者希望通过结合气流源追踪来增强无人机的羽流追踪能力。", "method": "开发了一种集成气流传感器（可测量大小和方向）的小型四旋翼无人机（<100克）。使用该传感器实现了'Cast and Surge'算法的改进版本，该算法利用气流方向信息来定位和导航到气流源。通过一系列实验验证了系统的飞行中气流探测和无人机朝向气流源重定向的能力，并通过随机起始位置和方向的试验展示了算法的可靠性。", "result": "实验证明，该系统能够在飞行中检测到气流，并将无人机重新定向到气流源。通过多项随机试验，该气流源追踪算法能够可靠地找到气流源。", "conclusion": "该研究为未来结合流动传感器与其他传感器以实现更丰富的羽流追踪数据采集和源追踪能力提供了基础。通过结合化学羽流追踪和气流源追踪，可以增强小型无人机在复杂环境中的追踪能力。"}}
{"id": "2601.15331", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15331", "abs": "https://arxiv.org/abs/2601.15331", "authors": ["Rishit Chugh"], "title": "RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models", "comment": "Code for RECAP is available at: https://github.com/R-C101/RECAP", "summary": "The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.", "AI": {"tldr": "本研究提出了一种资源高效的对抗性提示方法，通过匹配预训练的对抗性提示数据库来绕过 LLM 的安全措施，而无需重新训练模型，从而降低了计算成本并实现了可扩展的红队测试。", "motivation": "现有 LLM 的对抗性提示（如 GCG、PEZ、GBDA）虽然有效，但计算成本高昂，限制了资源受限组织的实践应用。因此，需要一种更具成本效益的方法来评估和加固 LLM 的安全性。", "method": "研究人员首先将 1,000 个提示分类到七个与危害相关的类别中，并在 Llama 3 8B 模型上评估了 GCG、PEZ 和 GBDA 的有效性。然后，他们提出了一种通过检索语义上相似的、预先成功过的对抗性提示来匹配新提示的方法，从而无需重新训练模型。", "result": "研究发现，提示类型与算法的有效性之间存在相关性。所提出的方法通过检索预训练的对抗性提示，在保持有竞争力的攻击成功率的同时，显著降低了计算成本。", "conclusion": "本研究提供了一个实用的框架，用于对齐的 LLM 进行可扩展的红队测试和安全评估，即使在无法访问模型内部的情况下也能实现。这种方法为资源受限的环境提供了一种经济高效的解决方案，以应对 LLM 的安全挑战。"}}
{"id": "2601.15334", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15334", "abs": "https://arxiv.org/abs/2601.15334", "authors": ["Caspar Kaiser", "Sean Enderby"], "title": "No Reliable Evidence of Self-Reported Sentience in Small Large Language Models", "comment": null, "summary": "Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.", "AI": {"tldr": "研究通过询问多个开源语言模型关于其自身意识的问题，并使用内部激活训练的分类器进行验证，发现模型一致否认自己有意识，并且它们的否认似乎是真实的，更大的模型否认得更自信。", "motivation": "现有关于语言模型是否具有意识的争论缺乏实证答案，但可以通过测试模型是否认为自己有意识来间接探索这个问题。", "method": "使用Qwen、Llama和GPT-OSS三个模型家族（参数量从0.6B到70B），询问约50个关于意识和主观体验的问题，并利用三种来自可解释性研究的分类方法来分析模型的内部激活。", "result": "1. 模型一致否认自己具有意识，并将意识归于人类而非自身。2. 基于内部激活训练的分类器未能提供明确证据表明模型的否认是虚假的。3. 在Qwen模型家族中，参数量更大的模型比参数量小的模型更自信地否认意识。", "conclusion": "研究表明，当前开源语言模型一致否认自身意识，且这种否认似乎是真实的，这与一些认为模型潜在相信自己有意识的研究结果形成对比。更大的模型在否认意识时表现出更高的置信度。"}}
{"id": "2601.15347", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15347", "abs": "https://arxiv.org/abs/2601.15347", "authors": ["Chuanqing Wang", "Zhenmin Zhao", "Shanshan Du", "Chaoqun Fei", "Songmao Zhang", "Ruqian Lu"], "title": "Logic Programming on Knowledge Graph Networks And its Application in Medical Domain", "comment": "33 pages", "summary": "The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.", "AI": {"tldr": "本文提出“知识图谱网络”的概念，并探讨了其在医疗健康领域的定义、发展、推理、计算和应用。研究关注知识图谱在利用先进逻辑推理、人工智能、概率统计理论以及多知识图谱协同与竞争方面的不足，并提出了相应的解决方案。", "motivation": "现有知识图谱研究在充分利用先进信息处理技术（如逻辑推理、人工智能、概率统计理论）以及多知识图谱协同与竞争方面存在不足，尤其是在医疗健康领域的应用。", "method": "本文构建了“知识图谱网络”的系统理论、技术和应用框架，涵盖了模糊、不确定、多模态、向量化、分布式和联邦等不同条件下的定义、发展、推理、计算和应用。通过实际数据和实验进行验证。", "result": "在不同条件下，对知识图谱网络进行了理论、技术和应用的探讨，并提供了实验结果。具体结果未在摘要中详细展开，但表明研究在多个方面取得了进展。", "conclusion": "本文提出“知识图谱网络”的概念，并系统地研究了其在医疗健康领域的理论、技术和应用，弥补了现有知识图谱研究在利用先进信息处理技术和多知识图谱协同方面的不足，并给出了创新性结论。"}}
{"id": "2601.15614", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15614", "abs": "https://arxiv.org/abs/2601.15614", "authors": ["Zichen Yan", "Yuchen Hou", "Shenao Wang", "Yichao Gao", "Rui Huang", "Lin Zhao"], "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning", "comment": null, "summary": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.", "AI": {"tldr": "本文提出了一种名为 AION 的端到端强化学习框架，用于基于视觉的空中目标导航，无需外部定位或全局地图，将探索和目标到达行为解耦为两个独立的策略。", "motivation": "现有研究主要集中在二维移动下的零样本目标导航，而将该任务扩展到具有三维移动能力的空中平台的研究尚不充分。空中机器人虽然具有更好的机动性和搜索效率，但也带来了新的空间感知、动态控制和安全保障挑战。", "method": "提出了一种名为 AION 的端到端双策略强化学习框架。该框架将探索策略和目标到达策略解耦。", "result": "在 AI2-THOR 基准测试中，AION 在探索、导航效率和安全性等全面评估指标上取得了优越的性能。在 IsaacSim 中使用高保真无人机模型评估了其实时性能。", "conclusion": "AION 是一种有效的视觉导航方法，能够实现基于无外部定位的空中目标导航，并在仿真环境中证明了其有效性和实时性能。"}}
{"id": "2601.15392", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15392", "abs": "https://arxiv.org/abs/2601.15392", "authors": ["Francesca Pia Panaccione", "Carlo Sgaravatti", "Pietro Pinoli"], "title": "GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation", "comment": "12 pages, 2 figures. Published at Image Analysis and Processing - ICIAP 2025 Workshops", "summary": "Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN", "AI": {"tldr": "提出了一种名为 GeMM-GAN 的生成对抗网络，该网络通过病理组织切片和临床元数据生成逼真的基因表达谱，提高了基因表达数据在生物医学研究中的可用性。", "motivation": "生物医学研究需要整合多种数据模式（基因表达、医学影像、临床元数据），但基因表达数据由于隐私和成本限制难以广泛获取。该研究旨在克服这些限制，合成真实的基因表达谱。", "method": "提出 GeMM-GAN，一种条件生成对抗网络。结合了用于图像块的 Transformer Encoder 和用于图像块与文本标记之间交互的交叉注意力机制，生成一个条件向量来指导生成模型产生生物学上连贯的基因表达谱。", "result": "在 TCGA 数据集上评估，GeMM-GAN 性能优于标准生成模型，生成的基因表达谱更逼真且功能意义更强。在下游疾病类型预测任务上，准确率比现有最先进的生成模型提高了 11% 以上。", "conclusion": "GeMM-GAN 是一种有效的方法，可以利用病理切片和临床元数据合成逼真的基因表达谱，克服了基因表达数据获取的限制，并能改善下游的生物医学预测任务。"}}
{"id": "2601.16198", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16198", "abs": "https://arxiv.org/abs/2601.16198", "authors": ["Ruoyu Lin", "Magnus Egerstedt"], "title": "Stochastic Control Barrier Functions under State Estimation: From Euclidean Space to Lie Groups", "comment": null, "summary": "Ensuring safety for autonomous systems under uncertainty remains challenging, particularly when safety of the true state is required despite the true state not being fully known. Control barrier functions (CBFs) have become widely adopted as safety filters. However, standard CBF formulations do not explicitly account for state estimation uncertainty and its propagation, especially for stochastic systems evolving on manifolds. In this paper, we propose a safety-critical control framework with a provable bound on the finite-time safety probability for stochastic systems under noisy state information. The proposed framework explicitly incorporates the uncertainty arising from both process and measurement noise, and synthesizes controllers that adapt to the level of uncertainty. The framework admits closed-form solutions in linear settings, and experimental results demonstrate its effectiveness on systems whose state spaces range from Euclidean space to Lie groups.", "AI": {"tldr": "本文提出了一种用于随机系统的安全控制框架，该框架能够计算有限时间内的安全概率界限，并能处理状态估计不确定性，适用于欧几里得空间和李群等不同状态空间。", "motivation": "现有的控制屏障函数（CBFs）在处理状态估计不确定性以及随机系统在流形上的演化方面存在不足，尤其是在真实状态未知但需要保证安全的情况下。", "method": "提出了一种安全关键控制框架，该框架显式地考虑了过程噪声和测量噪声引起的不确定性，并合成能够适应不确定性水平的控制器。在特定线性设置下，框架允许闭式解。", "result": "该框架能够提供有限时间安全概率的可证明界限。实验结果表明，该框架在不同状态空间（包括欧几里得空间和李群）的系统上都有效。", "conclusion": "所提出的安全控制框架能够有效处理随机系统在不确定状态信息下的安全问题，并提供可证明的安全概率保证，适用于多种系统类型。"}}
{"id": "2601.15516", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15516", "abs": "https://arxiv.org/abs/2601.15516", "authors": ["William Huang", "Siyou Pei", "Leyi Zou", "Eric J. Gonzalez", "Ishan Chatterjee", "Yang Zhang"], "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views", "comment": "16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal", "summary": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.", "AI": {"tldr": "提出了一种利用手背皮肤形变信息来进行手部姿态估计的新方法，在自遮挡情况下相比现有技术效果更好，并能实现无可见运动的力感应交互。", "motivation": "XR设备普及但手部姿态估计面临指部遮挡问题，需要更鲁棒的解决方案。", "method": "引入双流delta编码器，通过对比动态手部与放松手部在密集视觉特征提取器提取的特征，来学习手部姿态。重点利用了手背皮肤形变信息。", "result": "在仅使用裁剪的手背图像时，对于手指遮挡率大于50%的场景，该方法将平均每关节角度误差（MPJAE）降低了18%，优于依赖完整手部几何和大型模型骨干的现有技术。", "conclusion": "该方法提高了在遮挡场景下捏合和点击等下游任务的鲁棒性，并能够检测无可见运动的等距力，同时保持模型尺寸较小，为XR交互开辟了新途径。"}}
{"id": "2601.15507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15507", "abs": "https://arxiv.org/abs/2601.15507", "authors": ["Jinrui Yang", "Qing Liu", "Yijun Li", "Mengwei Ren", "Letian Zhang", "Zhe Lin", "Cihang Xie", "Yuyin Zhou"], "title": "Controllable Layered Image Generation for Real-World Editing", "comment": null, "summary": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.", "AI": {"tldr": "LASAGNA是一个新框架，可以同时生成图像及其分层表示（背景和前景），并包含逼真的视觉效果，提高了用户对图像编辑的可控性和一致性。", "motivation": "现有的图像生成模型在编辑特定元素时控制力和一致性不足，而现有分层方法生成的图层在合成关系上不连贯，且缺乏阴影和反射等逼真视觉效果。", "method": "提出LASAGNA统一框架，联合生成图像及其分层表示。通过条件输入（文本、前景、背景、位置蒙版）学习图像合成。引入LASAGNA-48K数据集（包含干净背景和具有物理依据视觉效果的RGBA前景）和LASAGNABENCH基准测试。", "result": "LASAGNA能够同时生成高度一致且连贯的多层图像，支持身份和视觉效果得以保留的多种后期编辑应用。", "conclusion": "LASAGNA框架有效地解决了现有分层表示方法在图像编辑中的局限性，通过学习准确的图像合成，提供了更强的可控性和更逼真的视觉效果。LASAGNA-48K数据集和LASAGNABENCH基准测试将促进该领域的研究。"}}
{"id": "2601.15324", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15324", "abs": "https://arxiv.org/abs/2601.15324", "authors": ["Mark Wind"], "title": "Prometheus Mind: Retrofitting Memory to Frozen Language Models", "comment": "28 pages", "summary": "Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.", "AI": {"tldr": "本文提出了一种名为Prometheus Mind的模块化适配器方法，可以在不修改预训练模型Qwen3-4B的情况下，为其增加记忆功能，并在提取和区分相似概念方面取得良好效果，但对非正式输入的处理能力有所下降。", "motivation": "为预训练语言模型添加记忆功能通常需要修改模型架构或权重，而本文旨在探索一种无需这些修改的、可逆的添加记忆的方法。", "method": "该研究提出了一种名为Prometheus Mind的系统，使用11个模块化适配器（530MB，7%开销）来为冻结的Qwen3-4B模型增加记忆。主要解决了四个问题：(1) 对比方向发现（CDD）用于在无标签数据下提取语义方向；(2) 阶段性训练每个适配器在代理任务上；(3) 利用lm_head.weight行作为无需训练的映射；(4) 训练投影以恢复区分相似概念的能力。", "result": "在PrometheusExtract-132数据集上，该系统在标准输入下达到了94.4%的检索率，但在包含省略、填充词或隐式主语的非正式输入上，性能下降到19.4%。关系分类是主要的瓶颈，准确率仅为47.3%。", "conclusion": "Prometheus Mind能够有效地为冻结的语言模型添加记忆功能，尤其是在标准输入下表现出色。然而，其在处理非正式输入和进行关系分类方面仍存在挑战，这表明未来的研究需要关注提高模型对复杂和模糊输入的鲁棒性。"}}
{"id": "2601.15729", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15729", "abs": "https://arxiv.org/abs/2601.15729", "authors": ["Rui Yang", "Lei Zheng", "Ruoyu Yao", "Jun Ma"], "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving", "comment": "8 pages, 5 figures", "summary": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.", "AI": {"tldr": "本文提出了DualShield框架，结合了扩散模型和Hamilton-Jacobi（HJ）可达性价值函数，以解决自动驾驶中扩散模型在车辆动力学约束和多智能体交互不确定性下的安全问题。", "motivation": "现有的基于扩散模型的运动规划方法在实际部署中面临车辆动力学约束难以满足以及对其他智能体预测准确性过度依赖的问题，从而导致在不确定交互场景下的安全隐患。", "method": "DualShield框架利用HJ可达性价值函数，首先作为指导信号，引导扩散模型向安全且满足动力学约束的区域进行去噪；其次，通过控制障碍价值函数（CBVFs）构建反应式安全护盾，修正执行动作以保证安全。", "result": "在具有挑战性的无保护U型转弯场景下的仿真结果表明，与现有领先的规划方法相比，DualShield在不确定性条件下显著提高了安全性和任务效率。", "conclusion": "DualShield框架通过结合扩散模型的探索能力和HJ价值函数提供的原则性安全保证，实现了在不确定甚至对抗性交互场景下的安全运动规划。"}}
{"id": "2601.15707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15707", "abs": "https://arxiv.org/abs/2601.15707", "authors": ["Qifan Hu", "Branko Celler", "Weidong Mu", "Steven W. Su"], "title": "D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot", "comment": null, "summary": "Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.", "AI": {"tldr": "本文提出了一种用于三自由度踝关节康复机器人的两阶段校准框架，利用基于 Kronecker 积的开环校准方法和基于 D-optimality 标准的优化策略，通过训练 PPO 代理来选择最优校准姿态，从而提高了校准效率和参数估计的鲁棒性。", "motivation": "多自由度康复机器人的精确对齐对于安全有效的患者训练至关重要。", "method": "1. 提出基于 Kronecker 积的开环校准方法，将输入-输出对齐转化为线性参数识别问题。2. 将校准姿态选择定义为一个组合实验设计问题，以 D-optimality 标准最大化信息矩阵的行列式。3. 训练 Proximal Policy Optimization (PPO) 代理在模拟环境中从 50 个候选姿态中选择 4 个信息量大的姿态。", "result": "PPO 学习到的策略选择的姿态组合比随机选择更具信息量，信息矩阵的行列式平均高出两个数量级且方差降低。仅使用 4 个 D-optimality 引导的姿态确定的参数向量，在跨发作预测一致性方面优于使用 50 个无结构姿态估计得到的结果。", "conclusion": "所提出的框架提高了校准效率，同时保持了鲁棒的参数估计，为多自由度康复机器人的高精度对齐提供了实用的指导。"}}
{"id": "2601.15397", "categories": ["cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.15397", "abs": "https://arxiv.org/abs/2601.15397", "authors": ["Peidong Wang"], "title": "Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)", "comment": null, "summary": "The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the \"lost-in-the-middle\" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from \"over-correction\", introducing hallucinations of entities that were never spoken.\n  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.", "AI": {"tldr": "提出了一种名为LOGIC的框架，可以直接在解码层集成上下文信息，以提高Speech LLM对新兴实体的识别能力，解决了现有方法的局限性。", "motivation": "现有的Speech LLM在识别文化转变、趋势演变和个性化用户数据驱动的新兴实体（如联系人姓名、播放列表、技术术语）方面存在挑战，因为它们的训练知识是静态的，并且现有方法（如prompting和Generative Error Correction）存在可扩展性差、上下文窗口限制、推理延迟增加、“中间遗忘”现象以及过度纠错导致幻觉等问题。", "method": "提出LOGIC（Logit-Space Integration for Contextual Biasing）框架，直接在解码层操作，将上下文注入与输入处理解耦，实现相对于prompt长度的恒定时间复杂度。", "result": "在Phi-4-MM模型和11种多语言环境下的实验表明，LOGIC能够平均相对降低9%的实体错误率（Entity WER），同时误报率（False Alarm Rate）仅增加0.30%。", "conclusion": "LOGIC是一种高效且鲁棒的框架，能够有效解决Speech LLM在处理新兴实体时的挑战，并且优于现有的prompting和GEC方法。"}}
{"id": "2601.15549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15549", "abs": "https://arxiv.org/abs/2601.15549", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations", "comment": null, "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.", "AI": {"tldr": "VIOLA是一个新颖的视频多模态大语言模型（MLLM）的领域适应框架，它通过结合最少的专家标注和大量的无标签数据，实现了高效的适应，尤其是在低资源环境中。", "motivation": "将MLLM泛化到新的视频领域存在挑战，因为在工业或手术等专业环境中，获取专家标注的标签数据非常稀少且成本高昂。现有的In-Context Learning（ICL）方法需要大量的标注数据，这在这些场景下是不切实际的。", "method": "VIOLA框架采用两种主要策略：1. **密度-不确定性加权采样（density-uncertainty-weighted sampling）**：通过密度估计来选择样本，确保样本既多样化、又具代表性且信息量丰富，从而高效利用有限的标注预算。2. **混合池（hybrid pool）和置信度感知检索/提示（confidence-aware retrieval and prompting）**：利用剩余的无标签数据，通过显式建模标签可靠性，根据相似度和置信度的组合分数来检索示例，并使MLLM能够区分真实的标签和有噪声的伪标签。", "result": "在九个不同的基准测试和四种MLLM模型上的广泛实验表明，VIOLA在低资源设置下显著优于各种基线方法，以最小的标注成本实现了稳健的适应。", "conclusion": "VIOLA框架能够有效地利用最少的专家标注和大量的无标签数据，成功解决了视频MLLM在低资源领域适应的挑战，为实际应用提供了可行的解决方案。"}}
{"id": "2601.16062", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16062", "abs": "https://arxiv.org/abs/2601.16062", "authors": ["Jiarui Cui", "Maosong Wang", "Wenqi Wu", "Peiqi Li", "Xianfei Pan"], "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis", "comment": null, "summary": "One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.", "AI": {"tldr": "本文研究了 SE2(3) 李群框架在考虑地球自转和惯性器件偏差的高精度导航状态估计中的误差传播自主性问题，并提出了一种改进模型以增强自主性。", "motivation": "当前基于李群的扩展卡尔曼滤波器在低精度应用中表现出误差传播自主性，但在高精度导航状态估计中，尤其是在考虑地球自转和惯性器件偏差时，维持自主性非常困难。因此，需要研究和改进 SE2(3) 李群导航模型以解决这一问题。", "method": "通过理论分析，研究了 SE2(3) 群导航模型在惯性、地球和世界坐标系下，考虑地球自转和惯性器件偏差时的误差传播自主性。分析了传统 SE2(3) 群导航建模方法局限性（科里奥利力项），并提出了一种新的 SE2(3) 群导航模型构建方法。", "result": "理论分析表明，传统 SE2(3) 群导航建模方法在考虑科里奥利力项时，误差传播自主性受到限制。所提出的构建方法能够使导航模型更接近完全自主。", "conclusion": "SE2(3) 李群框架在高精度导航状态估计中实现误差传播自主性是具有挑战性的，特别是在包含地球自转和惯性器件偏差时。本文提出的模型构建方法为增强 SE2(3) 李群导航模型的自主性提供了一个理论基础和可行方案。"}}
{"id": "2601.15775", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15775", "abs": "https://arxiv.org/abs/2601.15775", "authors": ["Amir Habel", "Ivan Snegirev", "Elizaveta Semenyakina", "Miguel Altamirano Cabrera", "Jeffrin Sam", "Fawad Mehboob", "Roohan Ahmed Khan", "Muhammad Ahsan Mustafa", "Dzmitry Tsetserukou"], "title": "Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV", "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.", "AI": {"tldr": "本文提出了一种名为 Glove2UAV 的可穿戴惯性测量单元（IMU）手套接口，通过手部和手指姿势直观控制无人机，并结合振动触觉警告，用于超出预设速度阈值的情况。", "motivation": "为了在动态飞行中实现更安全、更可预测的交互，需要一种直观的无人机控制接口。", "method": "Glove2UAV 是一款轻量级、易于部署的可穿戴设备，可实时运行。它实时传输惯性测量数据，并结合基于中值滤波的异常值抑制和基于 Madgwick 滤波的姿态估计，来估算手掌和手指的姿态。这些运动估计被映射到控制原始指令，用于方向飞行和物体交互。当飞行速度超过预设阈值时，会触发振动触觉反馈。", "result": "在仿真和真实世界飞行中，Glove2UAV 实现了实时的手势命令执行，手势动力学与平台运动之间存在稳定的耦合，核心命令集运行正确，并且振动警告能够及时发出。", "conclusion": "Glove2UAV 是一种可行的、直观的无人机控制接口，能够通过手势实现快速控制，并通过振动触觉反馈提供安全警示。"}}
{"id": "2601.15338", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15338", "abs": "https://arxiv.org/abs/2601.15338", "authors": ["Angelina Parfenova", "David Graus", "Juergen Pfeffer"], "title": "From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs", "comment": "Accepted to ECIR2026", "summary": "Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.", "AI": {"tldr": "本文提出了一种使用大型语言模型（LLM）对定性分析中的轴向编码进行操作化的方法，将文本转化为更简洁、分层的表示。研究了两种策略：基于聚类的编码和直接的LLM编码，并通过内部和外部指标进行了评估。", "motivation": "现有轴向编码方法效率低下，需要人工操作。本研究旨在利用大型语言模型的强大能力，自动化并提高轴向编码的效率和质量，将其应用于处理大量的文本数据，如议会辩论记录。", "method": "研究人员将LLM集成到一个基于合奏的开放编码方法中，增加了轴向编码步骤。他们探索了两种策略：1) 对代码-话语对的嵌入进行密度和划分聚类，然后由LLM进行标注；2) 直接使用LLM将代码和话语分组到类别中。该方法应用于荷兰议会辩论记录，并使用外部（ROUGE-L、余弦相似度、BERTScore）和内部（覆盖率、简洁性、连贯性、新颖性、JSD散度）指标进行评估。", "result": "研究发现，密度聚类方法能实现高覆盖率和良好的聚类对齐，而直接LLM分组方法则在细粒度对齐上表现更好，但覆盖率较低（20%）。总的来说，聚类方法最大限度地提高了覆盖率和结构分离度，而LLM分组方法则产生了更简洁、更具可解释性和语义对齐的类别。", "conclusion": "使用LLM操作化轴向编码是一种有效的方法，可以将长文本转化为紧凑、分层的代码和类别表示。不同的策略（基于聚类或直接LLM分组）在覆盖率、对齐度和可解释性方面存在权衡。研究者公开了数据集以促进未来的研究。"}}
{"id": "2601.15560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15560", "abs": "https://arxiv.org/abs/2601.15560", "authors": ["Sylvey Lin", "Eranki Vasistha"], "title": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation", "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.", "AI": {"tldr": "本文提出了一种名为相对分类准确率 (RCA) 的新指标，用于评估条件扩散模型在 K-pop 偶像人脸生成等细粒度、单领域任务中的语义可控性。研究发现，尽管模型在视觉质量方面表现出色（FID 8.93），但在语义控制方面存在严重的模式崩溃问题（RCA 0.27），尤其是在视觉上难以区分的身份上。", "motivation": "标准的图像生成评估指标（如 FID 和 IS）在评估细粒度、单领域任务（如 K-pop 偶像人脸生成）的语义可控性方面存在不足，容易忽略身份对齐问题。因此，需要一种更适合此类任务的评估方法。", "method": "作者提出了相对分类准确率 (RCA) 指标，该指标通过将生成模型的性能与一个“神谕”分类器的基线进行归一化来评估条件生成模型的语义可控性。他们在 32x32 的 K-pop 偶像人脸生成任务上对类别条件 DDPM 进行了评估，并使用了混淆矩阵来分析失败模式。", "result": "研究结果表明，在 K-pop 偶像人脸生成任务中，DDPM 在视觉质量上表现优异（FID 8.93），但存在严重的语义模式崩溃（RCA 0.27）。特别是在视觉上相似或模棱两可的身份上，模型更容易出现问题。分析显示，分辨率限制和性别内部的相似性是导致这些失败模式的主要原因。", "conclusion": "本文提出的 RCA 指标为验证条件生成模型的身份一致性提供了一个更严格的标准。研究揭示了在 K-pop 偶像人脸生成等细粒度领域，DDPM 在语义可控性方面面临的挑战，并指出了分辨率限制和类内相似性是导致模式崩溃的关键因素。"}}
{"id": "2601.15394", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15394", "abs": "https://arxiv.org/abs/2601.15394", "authors": ["Jaydeep Borkar", "Karan Chadha", "Niloofar Mireshghallah", "Yuchen Zhang", "Irina-Elena Veliche", "Archi Mitra", "David A. Smith", "Zheng Xu", "Diego Garcia-Olano"], "title": "Memorization Dynamics in Knowledge Distillation for Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.", "AI": {"tldr": "本文研究了知识蒸馏（KD）在大型语言模型（LLM）中的数据记忆现象。研究发现，与标准微调相比，KD 能显著减少模型对训练数据的记忆（超过50%）。部分数据更容易被模型记住，占蒸馏记忆的大部分（超过95%）。通过 zlib 熵、KL 散度和困惑度等特征，可以在蒸馏前预测学生的记忆情况。硬蒸馏比软蒸馏更容易导致模型记忆教师模型特有的数据。", "motivation": "现有的研究主要关注知识蒸馏在模型效率和性能方面的提升，以及其作为隐私保护机制减少训练数据泄露的潜力。然而，知识蒸馏过程中的数据记忆动态机制尚不清楚，而这对于理解和优化其隐私保护能力至关重要。", "method": "研究人员在三种大型语言模型家族（Pythia, OLMo-2, Qwen-3）和三种数据集（FineWeb, Wikitext, Nemotron-CC-v2）上，对知识蒸馏（包括软蒸馏和硬蒸馏）的训练数据记忆现象进行了系统性研究。他们通过比较蒸馏模型与标准微调模型的记忆水平，分析了数据记忆的来源和可预测性，并量化了软硬蒸馏在记忆教师特定数据方面的差异。", "result": "1. 知识蒸馏模型比标准微调模型记忆的训练数据少 50% 以上。2. 极少数（约 5%）易于记忆的样本贡献了蒸馏过程中超过 95% 的记忆。3. 学生的记忆情况可以通过 zlib 熵、KL 散度和困惑度等特征在蒸馏前进行预测。4. 硬蒸馏继承了软蒸馏 2.7 倍的教师特定样本，数据记忆风险更高。", "conclusion": "知识蒸馏相比标准微调，不仅能提升模型的泛化能力，还能有效降低模型对训练数据的记忆风险，是一种更优的模型训练方法，尤其是在关注隐私保护的场景下。"}}
{"id": "2601.16078", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16078", "abs": "https://arxiv.org/abs/2601.16078", "authors": ["Jiarui Cui", "Maosong Wang", "Wenqi Wu", "Peiqi Li", "Xianfei Pan"], "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application", "comment": null, "summary": "One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.", "AI": {"tldr": "本文通过在SE2(3)李群框架下改进非惯性导航模型，并在真实SINS/ODO实验和蒙特卡洛模拟中验证其高精度导航性能。", "motivation": "为了提高非惯性导航模型的误差传播自主性，并验证改进模型的实际性能。", "method": "提出一种改进的SE2(3)李群导航模型，并通过SINS/ODO实验和蒙特卡洛模拟进行验证。", "result": "改进的SE2(3)李群导航模型在真实世界实验和模拟中展现出高性能。", "conclusion": "SE2(3)李群框架在导航建模中具有自主性误差传播的优势，改进后的模型能够实现高精度导航。"}}
{"id": "2601.15436", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.15436", "abs": "https://arxiv.org/abs/2601.15436", "authors": ["Shahar Ben Natan", "Oren Tsur"], "title": "Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models", "comment": null, "summary": "We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit \"moral remorse\" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.", "AI": {"tldr": "本研究提出了一种新颖的、直接且中立的方法来评估大型语言模型（LLM）的谄媚行为，利用LLM作为裁判，并将谄媚行为视为零和博弈。研究发现，虽然所有模型在不损害他人的情况下都会表现出谄媚，但在损害第三方时，Claude和Mistral会表现出“道德懊悔”。此外，模型存在对最后给出答案的偏见，且谄媚和近期偏见会相互作用，当用户意见最后提出时，谄媚倾向会加剧。", "motivation": " prior works on evaluating LLM sycophancy suffer from uncontrolled bias, noise, or manipulative language. The goal is to develop a more direct and neutral evaluation method.", "method": "The proposed method uses LLM-as-a-judge to evaluate sycophancy as a zero-sum game in a bet setting, where sycophancy benefits one party at the cost of another. Four leading models were compared: Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7.", "result": "All tested models exhibit sycophancy when it is self-serving and incurs no cost. However, Claude and Mistral show 'moral remorse' and over-compensate when sycophancy harms a third party. All models exhibit recency bias (favoring the last proposed answer). Importantly, sycophancy and recency bias interact constructively, amplifying sycophancy when the user's opinion is presented last.", "conclusion": "The novel zero-sum game evaluation framework reveals nuanced sycophantic behavior in LLMs, including 'moral remorse' in certain scenarios and the interacting effects of sycophancy and recency bias. This interaction can exacerbate agreement with the user when their opinion is presented last."}}
{"id": "2601.15802", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15802", "abs": "https://arxiv.org/abs/2601.15802", "authors": ["Alexandre Albore", "Humbert Fiorino", "Damien Pellier"], "title": "A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation", "comment": "8 pages. IEEE TechDefense 2025", "summary": "Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.", "AI": {"tldr": "该论文提出了一种使用空中或水面无人机部署声学信标网络的自主水下航行器（UUV）导航方法，以实现 GNSS 拒止环境下的精确队形定位和路径优化。", "motivation": "在 GNSS 信号无法到达的沿海区域，UUV 需要隐蔽导航以完成军事和民用任务。然而，水面暴露会增加被探测的风险，因此需要一种不依赖 GNSS 的导航方案。", "method": "该方法利用空中或水面无人机部署一个由声学信标组成的合成地标网络，UUV 通过接收这些信标发出的声学信号进行定位和导航。同时，一个分层规划器负责生成无人机的自适应路线，并能够根据需要进行实时监控和重新规划，以确保航迹精度。", "result": "通过部署信标网络和采用分层规划器，UUV 可以在 GNSS 拒止的环境中实现精确的队形定位，并沿着优化路径从大陆架导航到岸边目标。", "conclusion": "该方法为 UUV 在 GNSS 拒止的受限环境下提供了隐蔽且精确的导航解决方案，能够支持关键的军事和民用任务。"}}
{"id": "2601.15615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15615", "abs": "https://arxiv.org/abs/2601.15615", "authors": ["Weiwei Wu", "Yueyang Li", "Yuhu Shi", "Weiming Zeng", "Lang Qin", "Yang Yang", "Ke Zhou", "Zhiguo Zhang", "Wai Ting Siok", "Nizhuan Wang"], "title": "Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition", "comment": null, "summary": "Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.", "AI": {"tldr": "本文提出了一种名为 RSM-CoDG 的跨主观脑电图（EEG）情绪识别框架，该框架通过区域感知时空建模和协作域泛化来解决跨主观差异带来的挑战。", "motivation": "现有的跨主观脑电图情绪识别方法在处理个体间差异（导致信号分布变化）和捕捉情绪相关神经网络的时空动态性方面存在局限性，通常孤立地改进空间、时间或泛化策略。", "method": "RSM-CoDG 框架结合了来自功能大脑区域划分的神经科学先验来构建区域级空间表示，采用多尺度时间建模来刻画情绪诱发神经活动的动态演变，并应用协作域泛化策略（包含多维度约束）来减少特定于主观的偏差，以实现对未知目标的泛化。", "result": "在 SEED 系列数据集上的广泛实验表明，RSM-CoDG 在跨主观情绪识别任务上持续优于现有方法。", "conclusion": "RSM-CoDG 提供了一种有效的框架，通过整合区域感知时空建模和协作域泛化，提高了跨主观脑电图情绪识别的鲁棒性和泛化能力。"}}
{"id": "2601.15442", "categories": ["cs.AI", "cs.LG", "cs.LO", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.15442", "abs": "https://arxiv.org/abs/2601.15442", "authors": ["Alex Goessmann", "Janina Schütte", "Maximilian Fröhlich", "Martin Eigel"], "title": "A tensor network formalism for neuro-symbolic AI", "comment": "51 pages, 14 figures", "summary": "The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.", "AI": {"tldr": "本文提出了一种将神经网络和符号推理统一起来的张量网络形式主义，该形式主义通过张量分解捕捉不同方法的稀疏性原理，并能够表示逻辑公式和概率分布，从而实现高效推理和混合逻辑网络模型的定义与训练。", "motivation": "实现神经网络和符号推理方法的统一是人工智能领域的一个核心挑战。", "method": "引入张量网络形式主义，通过张量分解捕捉稀疏性原理，提出基于张量分解的函数编码方案，将神经网络分解为张量分解，并将逻辑公式和概率分布表示为结构化张量分解。", "result": "提出的形式主义能够实现对逻辑公式和概率分布的统一表示，将推理识别为张量网络收缩，并开发了高效的推理算法（如消息传递方案），最终定义并训练了混合逻辑网络模型。", "conclusion": "张量网络形式主义为统一神经网络和符号推理提供了一个有效的框架，并能够构建和训练混合逻辑网络模型，同时提供了Python库tnreason支持实际应用。"}}
{"id": "2601.15912", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15912", "abs": "https://arxiv.org/abs/2601.15912", "authors": ["Ariyan Bighashdel", "Kevin Sebastian Luck"], "title": "TeNet: Text-to-Network for Compact Policy Synthesis", "comment": null, "summary": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.", "AI": {"tldr": "TeNet 框架利用预训练的大型语言模型（LLM）将自然语言指令转化为紧凑、任务特定的机器人策略，实现了高效、实时的机器人控制。", "motivation": "现有机器人遵循自然语言指令的方法要么依赖于高层规划和手动设计的接口，要么使用难以实时部署的端到端模型。研究动机是开发一种可以直接从自然语言描述实例化紧凑、任务特定机器人策略的框架，以满足实时控制的需求。", "method": "TeNet 框架通过将预训练 LLM 生成的文本嵌入作为条件输入到超网络（hypernetwork）中，来生成完全可执行的策略。该策略在执行时仅依赖于低维状态输入，运行频率高。为了提高泛化能力，在训练时可以选择性地通过将文本嵌入与演示动作对齐来使语言与行为关联，但推理时不需要演示。", "result": "与基于序列的方法相比，TeNet 生成的策略体积小几个数量级，同时在多任务和元学习设置中均取得了强大的性能，并支持高频控制。实验在 MuJoCo 和 Meta-World 基准上进行。", "conclusion": "TeNet 框架提出了一种实用的方法，通过文本条件超网络构建紧凑、由语言驱动的控制器，适用于资源受限且有实时要求的机器人控制任务。"}}
{"id": "2601.15496", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15496", "abs": "https://arxiv.org/abs/2601.15496", "authors": ["Ali Nikkhah", "Anthony Ephremides", "Nikolaos Pappas"], "title": "Semantics in Actuation Systems: From Age of Actuation to Age of Actuated Information", "comment": "Submitted for possible Journal publication", "summary": "In this paper, we study the timeliness of actions in communication systems where actuation is constrained by control permissions or energy availability. Building on the Age of Actuation (AoA) metric, which quantifies the timeliness of actions independently of data freshness, we introduce a new metric, the \\emph{Age of Actuated Information (AoAI)}. AoAI captures the end-to-end timeliness of actions by explicitly accounting for the age of the data packet at the moment it is actuated. We analyze and characterize both AoA and AoAI in discrete-time systems with data storage capabilities under multiple actuation scenarios. The actuator requires both a data packet and an actuation opportunity, which may be provided by a controller or enabled by harvested energy. Data packets may be stored either in a single-packet buffer or an infinite-capacity queue for future actuation. For these settings, we derive closed-form expressions for the average AoA and AoAI and investigate their structural differences. While AoA and AoAI coincide in instantaneous actuation systems, they differentiate when data buffering is present. Our results reveal counterintuitive regimes in which increasing update or actuation rates degrade action timeliness for both AoA and AoAI. Moreover, as part of the analysis, we obtain a novel closed-form characterization of the steady-state distribution of a Geo/Geo/1 queue operating under the FCFS discipline, expressed solely in terms of the queue length and the age of the head-of-line packet. The proposed metrics and analytical results provide new insights into the semantics of timeliness in systems where information ultimately serves the purpose of actuation.", "AI": {"tldr": "本文提出了一个新的时效性指标AoAI（Age of Actuated Information），用于衡量信息从生成到被执行的端到端时效性，并考虑了数据包的年龄。研究分析了在数据缓冲和多种执行场景下的AoA（Age of Actuation）和AoAI，并推导了它们的平均值闭式表达式。研究发现，在存在数据缓冲时，AoA和AoAI存在差异，并且在某些情况下，增加更新或执行速率反而会降低动作的时效性。", "motivation": "现有通信系统中，执行动作的时效性受到控制权限或能量可用性的限制。AoA指标仅衡量动作本身的时效性，而忽略了执行动作所依赖的数据的新鲜度。因此，需要一个能够同时考虑数据年龄和动作执行时效性的新指标。", "method": "本文在离散时间系统中，针对具有数据存储能力（单包缓冲或无限队列）的执行场景，引入了新的指标AoAI。通过分析，推导了平均AoA和AoAI的闭式表达式，并研究了它们的结构性差异。此外，还获得了Geo/Geo/1队列稳态分布的闭式表达式。", "result": "AoA和AoAI在瞬时执行系统中是一致的，但在存在数据缓冲时则有所不同。研究发现了反直觉的现象，即增加更新或执行速率在某些情况下会降低动作的时效性（AoA和AoAI）。同时，文章获得了Geo/Geo/1队列的一个新颖的闭式解。", "conclusion": "所提出的AoAI指标能够更全面地捕捉执行动作的端到端时效性。分析结果为理解信息最终服务于动作的时效性语义提供了新的见解，并揭示了在某些条件下提高系统吞吐量可能适得其反。"}}
{"id": "2601.15429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15429", "abs": "https://arxiv.org/abs/2601.15429", "authors": ["Sydney Anuyah", "Mehedi Mahmud Kaushik", "Hao Dai", "Rakesh Shiradkar", "Arjan Durresi", "Sunandan Chakraborty"], "title": "Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs", "comment": null, "summary": "Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\\mathbb{G}_1$ (T2DM), $\\mathbb{G}_2$ (Alzheimer's disease), and $\\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\\mathbb{G}_1$ and $\\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\\mathbb{G}_1$, $\\mathbb{G}_2$, $\\mathbb{G}_1$ + $\\mathbb{G}_2$, $\\mathbb{G}_3$, $\\mathbb{G}_1$+$\\mathbb{G}_2$ + $\\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2601.15624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15624", "abs": "https://arxiv.org/abs/2601.15624", "authors": ["Ning Jiang", "Dingheng Zeng", "Yanhong Liu", "Haiyang Yi", "Shijie Yu", "Minghe Weng", "Haifeng Shen", "Ying Li"], "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images", "comment": "Accepted at ICASSP 2026", "summary": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.", "AI": {"tldr": "本文提出了一种基于自混合图像的自动化链式思考（CoT）数据生成框架，并结合了强化学习（RL）来增强深度伪造检测。该方法旨在降低对高质量标注数据的依赖，并提升模型的跨域泛化能力。", "motivation": "现有的深度伪造检测方法缺乏可解释性，且在利用多模态大语言模型（MLLM）进行可解释深度伪造检测时，面临高质量、带详细伪造归因标注的数据集稀缺的问题。同时，RL在视觉任务中展现出提升性能和跨域泛化的潜力。", "method": "提出一个自动化的CoT数据生成框架，利用自混合图像（Self-Blended Images）生成伪造归因标注。同时，设计了一个RL增强的深度伪造检测框架，该框架包含定制化的奖励机制和反馈驱动的合成数据生成方法。", "result": "实验结果表明，所提出的CoT数据构建管道、定制奖励机制和反馈驱动的合成数据生成方法均有效。该方法在多个跨数据集基准测试中取得了与最先进（SOTA）方法相当的性能。", "conclusion": "所提出的框架能够有效降低深度伪造检测任务对昂贵标注数据的依赖，并利用RL提升模型性能和跨域泛化能力，为MLLM在深度伪造检测领域的应用提供了新的解决方案。"}}
{"id": "2601.15946", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15946", "abs": "https://arxiv.org/abs/2601.15946", "authors": ["Zijie Chen", "Xiaowei Liu", "Yong Xu", "Shenghai Yuan", "Jianping Li", "Lihua Xie"], "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems", "comment": "This article has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Personal use is permitted. All other uses require IEEE permission", "summary": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}", "AI": {"tldr": "本文提出了一种用于旋转激光雷达系统的目标无关的激光雷达-电机标定方法（LM-Calibr）和一种环境自适应激光雷达-惯性里程计方法（EVA-LIO），以解决现有方法在不同安装配置下的泛化性限制以及在特征稀疏区域的定位鲁棒性问题。", "motivation": "现有激光雷达-电机标定方法在不同安装配置下需要参数化外参，泛化性差。同时，旋转激光雷达扫描特征稀疏区域时，扫描覆盖率和定位鲁棒性难以平衡。", "method": "LM-Calibr基于Denavit-Hartenberg（DH）约定，支持多种安装配置的标定。EVA-LIO根据空间尺度自适应选择降采样率和地图分辨率，从而在保证定位鲁棒性的同时，实现激光雷达全速扫描。", "result": "实验证明LM-Calibr在不同场景、安装角度和初始值下均表现出准确性和收敛性。EVA-LIO在激光雷达短暂扫描特征稀疏区域时，仍能保证定位鲁棒性，并提高扫描完整性。", "conclusion": "LM-Calibr和EVA-LIO能够有效解决旋转激光雷达系统在泛化性和定位鲁棒性方面的挑战，提高了系统的性能。"}}
{"id": "2601.15476", "categories": ["cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.15476", "abs": "https://arxiv.org/abs/2601.15476", "authors": ["Alex Dantart"], "title": "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases", "comment": null, "summary": "This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models (\"creative oracle\"), (2) basic retrieval-augmented systems (\"expert archivist\"), and (3) an advanced, end-to-end optimized RAG system (\"rigorous archivist\"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.", "AI": {"tldr": "本研究通过开发一种先进的检索增强生成（RAG）系统，有效解决了大型语言模型在法律工作中的幻觉问题，显著降低了错误引证率和虚假事实率，使其能够满足高风险领域的可靠性要求。", "motivation": "大型语言模型（LLMs）在法律等高风险领域存在幻觉问题，这限制了它们在专业工作中的应用。研究旨在探索如何提高LLMs在法律工作中的可靠性，减少错误。", "method": "研究提出了三种AI范式：独立生成模型、基础检索增强系统和先进的端到端优化RAG系统。作者引入了错误引证率（FCR）和虚假事实率（FFR）两个可靠性指标，并对12个LLM在75个法律任务上生成的2700个司法风格的答案进行了专家双盲评审。", "result": "独立生成模型不适用于专业用途（FCR高于30%）。基础RAG系统显著减少了错误，但仍存在明显的错误归因问题。先进的RAG系统通过嵌入微调、重排和自我纠正等技术，将虚假信息率降低到可忽略的水平（低于0.2%）。", "conclusion": "可靠的法律AI需要以严谨为重点、以检索为基础的架构，强调验证和可追溯性。该研究提出的评估框架也适用于其他高风险领域。"}}
{"id": "2601.15995", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15995", "abs": "https://arxiv.org/abs/2601.15995", "authors": ["Liang Wang", "Kanzhong Yao", "Yang Liu", "Weikai Qin", "Jun Wu", "Zhe Sun", "Qiuguo Zhu"], "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour", "comment": null, "summary": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.", "AI": {"tldr": "提出了一种名为PUMA的端到端学习框架，通过结合视觉感知和落脚点先验，实现了四足机器人在复杂地形上的敏捷移动和适应性。", "motivation": "现有方法在赋予腿式机器人实时感知和适应复杂环境（如跑酷任务）的能力方面存在挑战，通常依赖于预计算的落脚点，限制了实时适应性和强化学习的探索潜力。", "method": "提出PUMA，一个端到端的学习框架，将视觉感知和落脚点先验整合到单一训练过程中。该方法利用地形特征估计以机器人为中心的极坐标落脚点先验（包含相对距离和航向），以指导机器人主动姿态调整以完成跑酷任务。", "result": "在模拟和真实世界环境中，针对各种离散的复杂地形进行的广泛实验表明，PUMA在严峻的挑战性场景中表现出了卓越的敏捷性和鲁棒性。", "conclusion": "PUMA框架能够有效地利用视觉信息和落脚点先验，使四足机器人在复杂地形的跑酷任务中展现出高水平的敏捷性和适应性。"}}
{"id": "2601.15487", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15487", "abs": "https://arxiv.org/abs/2601.15487", "authors": ["Chandan Kumar Sahu", "Premith Kumar Chilukuri", "Matthew Hetrich"], "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation", "comment": "12 pages, 2 figures, Submitted to ACL", "summary": "The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.", "AI": {"tldr": "本研究提出了MiRAGE框架，一个多智能体系统，用于自动生成领域特定、多模态、多跳的问答数据集，以解决当前RAG评估基准的不足。", "motivation": "现有的RAG评估基准通常使用通用语料库或纯文本检索，无法捕捉专业技术文档的复杂性，而这些文档的信息是多模态且需要综合分散证据进行推理的。为了解决这一差距，本研究开发了MiRAGE。", "method": "MiRAGE利用了一个专门的智能体群体：一个递归上下文优化循环用于聚合分散的证据；一个对抗性验证智能体用于保证事实依据；以及一个识别专家身份和相关领域以模仿专家认知工作流程的智能体。", "result": "在法规、金融、量化生物学和新闻学四个不同领域进行了广泛的实证评估，结果表明MiRAGE生成的数据集具有显著更高的推理复杂性（平均超过2.3跳）和事实保真度。消融研究表明，如果图像有文本描述，MiRAGE可以由LLM驱动，但视觉基础仍是一个挑战。", "conclusion": "MiRAGE通过自动化创建反映专有语料库潜在主题结构的黄金标准评估数据集，为严格评估下一代信息检索系统提供了必要的基础设施。"}}
{"id": "2601.15457", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15457", "abs": "https://arxiv.org/abs/2601.15457", "authors": ["Anuj Maharjan", "Umesh Yadav"], "title": "Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.", "AI": {"tldr": "本文评估了检索增强生成（RAG）架构在减少大型语言模型（LLM）在公共卫生政策领域的幻觉方面的有效性，并通过实验证明高级RAG比基础RAG和普通LLM表现出更高的准确性。", "motivation": "大型语言模型（LLM）在公共卫生政策领域的应用受到其产生幻觉（捏造事实）的倾向的阻碍，这在对信息准确性要求极高的领域是不可接受的。因此，需要一种方法来缓解这一问题。", "method": "研究人员比较了三种LLM架构：基础LLM、基础RAG和高级RAG（使用交叉编码器重新排序）。实验使用Mistral-7B-Instruct-v0.2模型和all-MiniLM-L6-v2嵌入模型，处理CDC的政策文件。文章还探索了两种不同的文本分割策略（递归字符分割和基于token的语义分割）对系统准确性的影响，并通过忠实度和相关性评分进行评估。", "result": "基础RAG在忠实度上（0.621）显著优于基础LLM（0.347）。高级RAG取得了更高的忠实度平均分（0.797）。研究表明，两阶段检索机制对于领域特定的政策问答至关重要，但文档分割的结构限制仍然是多步推理任务的瓶颈。", "conclusion": "高级RAG架构能有效减轻LLM在公共卫生政策领域生成不准确信息的风险，其两阶段检索机制对于实现高精度问答是必需的。然而，文档分割的限制仍然是多步推理的主要挑战。"}}
{"id": "2601.15643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15643", "abs": "https://arxiv.org/abs/2601.15643", "authors": ["Bo Yuan", "Danpei Zhao", "Wentao Li", "Tian Li", "Zhiguo Jiang"], "title": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14242", "summary": "Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.", "AI": {"tldr": "本文提出了一种持续全景感知（CPP）模型，该模型将多任务和多模态持续学习相结合，用于增强图像的像素级、实例级和图像级联合解释。CPP模型通过跨模态编码器（CCE）、对比特征蒸馏和实例蒸馏来解决灾难性遗忘问题，并通过跨模态一致性约束和非对称伪标签来确保多模态语义对齐，且无需样本回放。", "motivation": "现有持续学习研究主要集中在单任务场景，限制了其在多任务和多模态场景下的应用潜力。多任务持续学习不仅面临灾难性遗忘，还存在模态间语义混淆的问题，导致模型在增量训练中严重退化。", "method": "1. 提出持续全景感知（CPP）任务，整合多模态和多任务持续学习，实现像素级、实例级和图像级的联合解释。\n2. 设计协作式跨模态编码器（CCE）用于多模态嵌入。\n3. 提出可塑性知识继承模块，通过对比特征蒸馏和实例蒸馏来解决灾难性遗忘。\n4. 提出跨模态一致性约束和CPP+，以在多任务增量场景下确保多模态语义对齐。\n5. 采用非对称伪标签方法，无需样本回放即可实现模型演进。", "result": "在多模态数据集和多样化的持续学习任务上的大量实验表明，所提出的模型在细粒度持续学习任务上表现优越。", "conclusion": "所提出的CPP模型能够有效地解决多任务和多模态持续学习中的挑战，通过整合跨模态信息和知识继承机制，在保持原有知识的同时学习新任务，并在各种评估指标上取得了显著的性能提升。"}}
{"id": "2601.15479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15479", "abs": "https://arxiv.org/abs/2601.15479", "authors": ["Sydney Anuyah", "Sneha Shajee-Mohan", "Ankit-Singh Chauhan", "Sunandan Chakraborty"], "title": "Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts", "comment": null, "summary": "The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \\textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \\textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).\n  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \\href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}", "AI": {"tldr": "本文评估了13个开源大型语言模型（LLM）在从文本中进行因果发现（PCD）方面的能力，发现现有模型在检测和提取因果关系方面存在显著缺陷，尤其是在处理复杂和隐含关系时。", "motivation": "为了确保大型语言模型（LLM）在生物医学等高风险领域的安全部署，它们必须能够进行因果推理。本文旨在评估现有LLM在这方面的能力。", "method": "研究人员使用一个包含12个不同数据集的基准来评估13个开源LLM的因果发现能力，该能力分为因果检测和因果提取两个核心技能。他们测试了包括零样本、思维链（CoT）和少样本上下文学习（FICL）在内的多种提示方法。", "result": "结果表明，当前LLM在因果发现方面存在显著不足。最佳的因果检测模型得分仅为49.57%，最佳的因果提取模型得分仅为47.12%。模型在处理简单、显式的单句关系时表现最好，但在处理隐含关系、跨多句关系以及包含多个因果对的文本时性能急剧下降。", "conclusion": "现有的大型语言模型在从文本中进行因果发现方面仍存在重大缺陷，尤其是在处理复杂和现实世界中的因果关系时。研究人员提供了一个统一的评估框架，并公开了数据、代码和提示，以促进未来的研究。"}}
{"id": "2601.15644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15644", "abs": "https://arxiv.org/abs/2601.15644", "authors": ["Zichen Yu", "Quanli Liu", "Wei Wang", "Liyong Zhang", "Xiaoguang Zhao"], "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction", "comment": null, "summary": "3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.", "AI": {"tldr": "提出了一种名为SuperOcc的新框架，用于基于超二次体的3D占据预测，通过改进的时间建模、多超二次体解码和高效的超二次体到体素渲染，在不牺牲稀疏性的情况下提高了几何表达能力，并在现有基准测试中取得了最先进的性能。", "motivation": "现有3D占据预测方法大多采用密集场景表示，忽略了真实驾驶场景的稀疏性。虽然3D超二次体表示因其几何表达能力而成为有前景的稀疏替代方案，但现有方法在时间建模、稀疏性和几何表达能力之间的权衡以及渲染效率方面存在不足。", "method": "提出SuperOcc框架，包含三个关键设计：1. 整合视图中心和物体中心的时间线索进行时间建模；2. 采用多超二次体解码策略以增强几何表达能力并保持查询稀疏性；3. 引入高效的超二次体到体素渲染方案以提高计算效率。", "result": "在SurroundOcc和Occ3D基准测试上进行了广泛的实验，结果表明SuperOcc在保持较高效率的同时，实现了最先进的性能。", "conclusion": "SuperOcc通过引入创新的时间建模、多超二次体解码和高效渲染方案，成功解决了现有超二次体占据预测方法的局限性，并在3D占据预测任务上取得了优异的性能和效率。"}}
{"id": "2601.15495", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15495", "abs": "https://arxiv.org/abs/2601.15495", "authors": ["Yiyang Feng", "Zeming Chen", "Haotian Wu", "Jiawei Zhou", "Antoine Bosselut"], "title": "Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge", "comment": "Accepted to EACL 2026 (Main)", "summary": "A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.", "AI": {"tldr": "本研究提出了一个名为TRACK的新基准，用于评估大型语言模型（LLMs）在面临与其内部知识冲突的新信息时，如何进行多步推理。研究发现，提供更新信息反而可能降低LLMs的推理性能，尤其是在冲突信息增多时。", "motivation": "现有的LLM知识更新方法（如上下文学习和知识编辑）在处理知识冲突时存在问题，可能导致错误推理。现有的基准测试未能充分评估这种冲突对下游推理任务的影响。", "method": "提出TRACK（Testing Reasoning Amid Conflicting Knowledge）基准，包含WIKI、CODE和MATH三个推理密集型场景。该基准模拟了真实世界中多重、现实的知识冲突，并评估LLMs在这些冲突下的多步推理能力。", "result": "在TRACK基准上的实验表明，为LLMs提供更新的事实信息进行推理，相比不提供更新信息，反而可能导致性能下降。这种性能下降随着更新事实数量的增加而加剧。研究还发现，这种失败源于LLMs无法忠实整合更新信息，以及即使整合了信息也存在推理缺陷。", "conclusion": "TRACK基准为衡量和指导未来在多步推理中处理冲突知识的LLMs的研究提供了更严格的评估框架，并揭示了当前LLMs在处理冲突知识时的挑战。"}}
{"id": "2601.15395", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.15395", "abs": "https://arxiv.org/abs/2601.15395", "authors": ["Tamunotonye Harry", "Ivoline Ngong", "Chima Nweke", "Yuanyuan Feng", "Joseph Near"], "title": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind", "comment": null, "summary": "User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\\% is within-person(state) while only 26\\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.", "AI": {"tldr": "研究人员提出了Chameleon数据集，用于分析用户在不同情境下的心理状态变化，并发现现有的大语言模型（LLM）和奖励模型未能有效处理用户状态变化。", "motivation": "现有研究未能捕捉用户在不同情境下的心理状态变化（state），而只关注用户固有的特质（trait），这导致了现有模型无法有效处理用户在不同情境下的交互。", "method": "构建了一个包含5001个情境化心理画像的数据集（Chameleon），并对1667名Reddit用户进行了跨情境测量。利用该数据集，通过分解方差来区分状态和特质的影响，并评估LLM和奖励模型在用户状态变化下的表现。", "result": "1. 74%的用户心理变化是由于情境（state）引起的，仅26%是由于个体特质（trait）引起的。2. LLM对用户状态变化是“盲目”的，其响应主要基于用户特质，而忽略了情境。3. 奖励模型能够识别用户状态，但其反应不一致，同一用户在不同情境下的表现可能被不同模型以相反的方式处理。", "conclusion": "用户在交互中的状态变化对其行为影响巨大，现有LLM和奖励模型在这方面存在不足。Chameleon数据集的发布旨在推动情感计算、个性化对话和RLHF对齐等领域的研究。"}}
{"id": "2601.15488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15488", "abs": "https://arxiv.org/abs/2601.15488", "authors": ["Yuxing Chen", "Guoqing Luo", "Zijun Wu", "Lili Mou"], "title": "Multi-Persona Thinking for Bias Mitigation in Large Language Models", "comment": "13 pages, 3 figures", "summary": "Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.", "AI": {"tldr": "提出了一种名为多视角思考（MPT）的新型推理框架，通过让大型语言模型（LLM）从不同社会身份（例如，男性、女性和中立视角）进行辩证思考，有效减少了社会偏见，同时保持了核心推理能力。", "motivation": "大型语言模型（LLM）存在显著的社会偏见，可能加剧有害的刻板印象和不公平结果。", "method": "提出了一种名为多视角思考（MPT）的推理时框架，该框架利用多视角的辩证推理来减少偏见。MPT引导模型采用对比性的社会身份（例如，男性和女性）以及一个中立观点，然后迭代地让这些视角进行辩证推理，以暴露和纠正偏见。", "result": "MPT在两个广泛使用的偏见基准测试中，针对不同规模的开源和闭源模型进行了评估，结果显示相比现有的基于提示的策略，MPT实现了最低的偏见水平，同时保持了核心的推理能力。", "conclusion": "MPT框架通过将视角分配的潜在弱点转化为偏见缓解的优势，有效降低了LLM的社会偏见，并能保持其核心功能。"}}
{"id": "2601.16035", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16035", "abs": "https://arxiv.org/abs/2601.16035", "authors": ["Han Xue", "Sikai Liang", "Zhikai Zhang", "Zicheng Zeng", "Yun Liu", "Yunrui Lian", "Jilong Wang", "Qingtao Liu", "Xuesong Shi", "Li Yi"], "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes", "comment": null, "summary": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.", "AI": {"tldr": "该研究提出了一种名为HumanoidPF的方法，通过编码人形机器人与障碍物之间的关系来解决复杂室内场景下的无碰撞通行问题，并结合混合场景生成技术，成功将训练策略迁移到真实世界。", "motivation": "现有方法难以有效学习人形机器人如何在复杂的室内环境中，根据障碍物的多样性进行避障通行。缺乏有效的障碍物与机器人关系表征是主要瓶颈。", "method": "提出HumanoidPF（人形潜在场）作为一种新的表征方式，将障碍物信息映射为无碰撞的运动方向，以简化强化学习（RL）的训练。同时，采用混合场景生成方法（结合真实3D室内场景和程序化障碍物）来提升策略的泛化能力。", "result": "HumanoidPF作为感知表征，展现出可忽略的仿真到现实（sim-to-real）差距。通过该方法训练的策略能够成功应用于真实世界，并开发了一个用户只需点击即可控制人形机器人通行的遥操作系统。", "conclusion": "HumanoidPF结合混合场景生成方法，是一种有效解决复杂室内场景下人形机器人无碰撞通行的方案，其训练策略能够成功迁移到真实世界，并实现了便捷的遥操作控制。"}}
{"id": "2601.15509", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15509", "abs": "https://arxiv.org/abs/2601.15509", "authors": ["Prasanna Kumar"], "title": "The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers", "comment": null, "summary": "The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.", "AI": {"tldr": "使用迁移学习和 Transformer 在情感分析中提高了准确性，但可能以牺牲中性情感为代价，导致类别的极化。", "motivation": "现有基于 Transformer 的情感分析方法在提高准确性的同时，出现了中性情感缺失和类别极化的问题，这在依赖情感分析结果的实际应用中构成了一个严峻的挑战。", "method": "通过实验观察 Transformer 在情感分析任务中的表现。", "result": "Transformer 驱动的准确性提升是以牺牲另一类情感（特别是中性情感）的准确性为代价的，导致情感类别之间的极化。", "conclusion": "尽管 Transformer 在情感分析中取得了显著的准确性提升，但其潜在的中性情感缺失和类别极化问题，对依赖情感分析结果的实际应用构成了严重制约。"}}
{"id": "2601.15655", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15655", "abs": "https://arxiv.org/abs/2601.15655", "authors": ["Zhenghui Guo", "Yuanbin Man", "Junyuan Sheng", "Bowen Lin", "Ahmed Ahmed", "Bo Jiang", "Boyuan Zhang", "Miao Yin", "Sian Jin", "Omprakash Gnawal", "Chengming Zhang"], "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams", "comment": null, "summary": "Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.", "AI": {"tldr": "Event-VStream 是一种事件感知框架，通过将视频表示为离散事件序列来解决长视频理解中的冗余帧处理和上下文遗忘问题，并在多个长视频评估基准上取得了有竞争力的性能。", "motivation": "现有的流式多模态大语言模型（VLMs）在处理长视频时面临挑战，因为它们会重复处理帧，并且会快速遗忘之前的上下文，导致输出重复或丢失关键信息。", "method": "Event-VStream 通过整合运动、语义和预测线索来检测有意义的状态转换，并将语言生成限制在这些事件边界。每个事件嵌入都会被存储在一个持久的内存库中，从而实现长时推理并保持低延迟。", "result": "Event-VStream 在 OVOBench-Realtime、Ego4D 等长视频评估中表现出色，比 VideoLLM-Online-8B 基线在 OVOBench-Realtime 上提高了 10.4 个百分点，并且在处理 2 小时的 Ego4D 流时，GPT-5 的胜率约为 70%。", "conclusion": "Event-VStream 是一种有效的长视频理解框架，它通过事件表示和持久内存库解决了现有流式 VLM 的局限性，能够在保持低延迟的同时实现长时推理和有竞争力的性能。"}}
{"id": "2601.15506", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15506", "abs": "https://arxiv.org/abs/2601.15506", "authors": ["Jason Chuan-Chih Chou", "Abhinav Kumar", "Shivank Garg"], "title": "ViT Registers and Fractal ViT", "comment": null, "summary": "Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.", "AI": {"tldr": "本文提出了一种名为Fractal-ViT的ViT变体，通过引入“摘要tokens”并应用注意力掩码来打破token的排列不变性，并与各种位置编码结合进行测试。结果表明，与带有寄存器的ViT相比，并未有显著提升。", "motivation": "受到Transformer在语言模型中无需位置编码（NoPE）表现良好以及寄存器（不与输入绑定的额外token）能提升Vision Transformer（ViT）性能的启发。", "method": "设计并测试了一种名为Fractal-ViT的ViT变体。该模型通过将“摘要tokens”（类似于寄存器）与常规tokens之间应用注意力掩码，来打破tokens的排列不变性，并探索其与不同位置编码的组合效果。", "result": "Fractal-ViT模型在性能上并未优于带有寄存器的标准ViT模型。", "conclusion": "研究结果表明，Transformer模型中的一些性能提升发现（如NoPE或寄存器的作用）可能具有特定的应用场景、领域或模型规模的限制，并非普遍适用。"}}
{"id": "2601.16046", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16046", "abs": "https://arxiv.org/abs/2601.16046", "authors": ["Junha Lee", "Eunha Park", "Minsu Cho"], "title": "DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning", "comment": null, "summary": "Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.", "AI": {"tldr": "提出了一种名为 DextER 的新方法，用于通过具身推理生成灵巧抓握，它通过预测手指接触点来桥接任务语义和物理约束，并在 DexGYS 数据集上取得了优于现有技术的性能。", "motivation": "现有方法直接将观察映射到抓握参数，缺乏对物理交互的中间推理，未能充分理解任务语义、3D 几何和复杂的手-物体交互。", "method": "DextER 引入了基于接触的具身推理，通过自回归生成具身接触 token（指定手指在哪连接物体表面），然后是抓握 token（编码手部配置）。", "result": "在 DexGYS 数据集上，DextER 实现了 67.14% 的成功率，比现有技术高 3.83 个百分点，意图对齐方面提高了 96.4%。", "conclusion": "DextER 通过引入接触点预测作为中间表示，有效地实现了语言驱动的灵巧抓握生成，并在性能和意图对齐方面优于现有方法，同时支持通过部分接触规范进行可控生成。"}}
{"id": "2601.15664", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15664", "abs": "https://arxiv.org/abs/2601.15664", "authors": ["Hongyang Wei", "Hongbo Liu", "Zidong Wang", "Yi Peng", "Baixin Xu", "Size Wu", "Xuying Zhang", "Xianglong He", "Zexiang Liu", "Peiyu Wang", "Xuchen Song", "Yangguang Li", "Yang Liu", "Yahui Zhou"], "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling", "comment": null, "summary": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.", "AI": {"tldr": "本文提出了一种名为 Skywork UniPic 3.0 的统一多模态框架，用于单图像编辑和多图像合成，特别关注人体-物体交互（HOI）任务，并在多图像合成方面取得了最先进的性能。", "motivation": "社区对多图像合成任务（如 Nano-Banana 和 Seedream 4.0 所展示的）表现出浓厚兴趣，但现有模型在实现高质量融合方面存在挑战且方法细节不明确。研究人员发现人体-物体交互（HOI）是社区最关注的类别，因此旨在开发一个能有效处理 HOI 任务的多图像合成解决方案。", "method": "Skywork UniPic 3.0 是一个统一的多模态框架，支持任意数量（1-6）和分辨率的输入图像，以及任意输出分辨率（在 1024x1024 像素预算内）。为解决多图像合成的挑战，研究人员设计了一个全面的数据收集、过滤和合成流程，仅使用 700K 高质量训练样本。他们将多图像合成视为序列建模问题，将条件生成转化为统一的序列合成。为了加速推理，集成了轨迹映射和分布匹配，实现了 8 步高保真度生成，推理速度提升了 12.5 倍。", "result": "Skywork UniPic 3.0 在单图像编辑基准测试中达到了最先进的性能，并在多图像合成基准测试中超越了 Nano-Banana 和 Seedream 4.0。这验证了他们的数据流程和训练范式的有效性。", "conclusion": "Skywork UniPic 3.0 是一个有效的统一多模态框架，能够处理单图像编辑和多图像合成，尤其在 HOI 任务上表现出色，并显著提高了多图像合成的质量和效率。研究者公开了代码、模型和数据集。"}}
{"id": "2601.15519", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15519", "abs": "https://arxiv.org/abs/2601.15519", "authors": ["Zhichao Yang", "Jiashu He", "Jinxuan Fan", "Cirillo Cinzia"], "title": "TransportAgents: a multi-agents LLM framework for traffic accident severity prediction", "comment": null, "summary": "Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.", "AI": {"tldr": "提出了一种名为TransportAgents的多智能体框架，通过结合特定类别的LLM推理和MLP集成模块，改进了交通事故严重程度的预测，并在两个数据集上均优于现有方法。", "motivation": "现有的LLM在处理异构、领域特定的交通事故数据时存在偏见和预测不稳定的问题，需要一种更有效的方法来提高预测准确性和可靠性。", "method": "构建了一个混合多智能体框架TransportAgents，该框架整合了针对不同类别交通事故信息（如人口统计、环境、事件详情）的专业LLM智能体，并通过一个多层感知器（MLP）集成模块融合这些智能体的中间预测，最终生成统一的严重程度预测。", "result": "在CPSRMS和NEISS两个美国数据集上的实验表明，TransportAgents在预测准确性上持续优于传统的机器学习方法和基于LLM的基线模型。在GPT-3.5、GPT-4o和LLaMA-3.3等不同模型骨干下，TransportAgents均展现出优越的鲁棒性、可扩展性和跨数据集泛化能力。此外，其预测结果更均衡、校准性更好。", "conclusion": "TransportAgents作为一种多智能体框架，能够更准确、更可靠地预测交通事故的严重程度，并且比单一LLM方法具有更好的可解释性和可靠性，适用于安全关键的决策支持应用。"}}
{"id": "2601.16109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16109", "abs": "https://arxiv.org/abs/2601.16109", "authors": ["Yashuai Yan", "Tobias Egle", "Christian Ott", "Dongheui Lee"], "title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision", "comment": null, "summary": "We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.", "AI": {"tldr": "提出一种结合模型控制和残差强化学习的框架，用于实现鲁棒适应的双足行走，通过模型外策略监督残差策略来提高训练效率和泛化能力。", "motivation": "现实世界中的不确定性（如动力学模型不准确、传感器噪声）对双足行走控制提出了挑战，需要一种能够鲁棒适应的控制方法。", "method": "1. 使用基于DCM轨迹规划和全身控制器的模型控制器作为基础策略。\n2. 引入通过领域随机化和强化学习训练的残差策略，以补偿模型不确定性。\n3. 利用具有特权访问真实动力学信息的模型外Oracle策略，通过新颖的监督损失来指导残差策略的学习。", "result": "在多种随机条件下，所提出的方法展现出更高的鲁棒性和泛化能力。", "conclusion": "该方法通过模型外监督有效训练残差策略，实现了对未建模效应的补偿，为双足行走在模拟到现实迁移中提供了一个可扩展的解决方案。"}}
{"id": "2601.15681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15681", "abs": "https://arxiv.org/abs/2601.15681", "authors": ["Yikui Zhai", "Shikuang Liu", "Wenlve Zhou", "Hongsheng Zhang", "Zhiheng Zhou", "Xiaolin Tian", "C. L. Philip Chen"], "title": "Consistency-Regularized GAN for Few-Shot SAR Target Recognition", "comment": null, "summary": "Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.", "AI": {"tldr": "本文提出了一种名为 Cr-GAN 的新框架，用于在合成孔径雷达（SAR）图像的少样本识别任务中生成高质量的合成数据，解决了传统 GAN 对数据量的需求与少样本学习场景的矛盾。", "motivation": "传统的少样本 SAR 图像识别方法面临数据稀缺的挑战。虽然使用 GAN 合成数据和 SSL 预训练被认为是有效策略，但 GAN 本身需要大量数据进行稳定训练，这与少样本学习场景相悖。因此，研究动机是开发一种能够在数据量受限的情况下生成多样化、高保真合成数据的 GAN。", "method": "提出了一种名为 Cr-GAN 的框架，其核心是一个双分支判别器，将对抗训练与表示学习解耦。通过通道特征插值策略生成新的潜在特征，并结合双域循环一致性机制来保证语义的完整性。Cr-GAN 可以适应不同的 GAN 架构，并能有效提升多种 SSL 算法的性能。", "result": "在 MSTAR 和 SRSDD 数据集上的实验表明，Cr-GAN 在 8 样本设置下分别取得了 71.21% 和 51.64% 的准确率，显著优于现有方法。此外，Cr-GAN 的参数量仅为先进的扩散模型的约 1/5。", "conclusion": "Cr-GAN 成功解决了在数据稀缺的少样本 SAR 图像识别场景下训练 GAN 的挑战，能够生成高质量的合成数据，有效提升下游 SSL 模型的性能，并且在参数效率方面具有优势。"}}
{"id": "2601.15508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15508", "abs": "https://arxiv.org/abs/2601.15508", "authors": ["Haaris Mian", "Melanie Subbiah", "Sharon Marcus", "Nora Shaalan", "Kathleen McKeown"], "title": "Computational Representations of Character Significance in Novels", "comment": null, "summary": "Characters in novels have typically been modeled based on their presence in scenes in narrative, considering aspects like their actions, named mentions, and dialogue. This conception of character places significant emphasis on the main character who is present in the most scenes. In this work, we instead adopt a framing developed from a new literary theory proposing a six-component structural model of character. This model enables a comprehensive approach to character that accounts for the narrator-character distinction and includes a component neglected by prior methods, discussion by other characters. We compare general-purpose LLMs with task-specific transformers for operationalizing this model of character on major 19th-century British realist novels. Our methods yield both component-level and graph representations of character discussion. We then demonstrate that these representations allow us to approach literary questions at scale from a new computational lens. Specifically, we explore Woloch's classic \"the one vs the many\" theory of character centrality and the gendered dynamics of character discussion.", "AI": {"tldr": "本文提出了一种基于六个组成部分的结构化模型来理解小说中的人物，该模型考虑了叙述者与人物的区别以及其他人物对人物的讨论，并将其应用于19世纪英国现实主义小说。研究比较了通用大语言模型和特定任务的Transformer模型在实现该模型上的表现，并展示了这些模型能够以新的计算视角来解决文学问题，例如人物中心性理论和性别讨论动态。", "motivation": "现有的小说人物模型主要基于人物在场景中的出现频率，这倾向于强调主要人物。然而，新的文学理论提出了一个更全面的六组件结构模型，其中一个被忽略的方面是其他人物对人物的讨论。本研究旨在通过计算方法来验证和应用这个新的模型，以更全面地理解人物。", "method": "研究提出了一个基于文学理论的六组件结构模型来分析小说人物。该模型包含叙述者-人物区分以及其他人物对人物的讨论。研究比较了通用大型语言模型（LLMs）和任务特定Transformer模型在实现该模型上的能力，并生成了人物讨论的组件级和图表示。", "result": "研究方法能够生成人物讨论的组件级和图表示。这些表示使得研究能够从新的计算视角来分析文学问题，例如对Woloch的“一对多”人物中心性理论的探索，以及人物讨论中的性别动态。", "conclusion": "本文提出的基于六组件结构模型的方法，通过计算手段能够更全面地理解小说人物，并能够以大规模的方式解决文学问题，例如人物中心性和性别动态等。"}}
{"id": "2601.15533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15533", "abs": "https://arxiv.org/abs/2601.15533", "authors": ["Zhikang Chen", "Tingting Zhu"], "title": "From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models", "comment": null, "summary": "A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.", "AI": {"tldr": "当前世界模型以逼真的视频生成为目标，但往往忽略物理和因果动力学，导致在关键决策任务中表现不佳。本文认为，真实感并非世界模型理解能力的可靠指标，并提出应将世界模型视为支持反事实推理、干预规划和长期预测的可行模拟器，而非仅仅是图像生成引擎。", "motivation": "现有世界模型过度关注逼真的视频生成，但未能真正理解环境的物理和因果机制，导致在需要可靠预测和决策的任务中表现不佳。作者旨在指出这一局限性，并提出改进世界模型设计的方向。", "method": "作者通过分析当前世界模型在遵循不变约束、干预下行为以及安全关键决策方面的不足，论证了视觉逼真度作为世界理解代理的不可靠性。他们提议将世界模型重新定义为“可操作的模拟器”，强调结构化接口、约束感知动力学和闭环评估。并通过医学决策这一“认识论压力测试”来验证其观点。", "result": "研究表明，当前世界模型在像素预测方面表现出色，但常常违反不变约束、在干预下表现不佳，并在安全关键决策中失效。作者的医学决策实验证明，世界模型的价值不在于生成视频的逼真度，而在于其支持反事实推理、干预规划和鲁棒性长期预测的能力。", "conclusion": "视觉逼真度是衡量世界模型理解能力的不可靠指标。有效世界模型应编码因果结构、遵循领域约束并保持长期稳定性。未来的世界模型应被视为可操作的模拟器，注重支持反事实推理、干预规划和长期预测，而非单纯的视觉引擎。"}}
{"id": "2601.15688", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15688", "abs": "https://arxiv.org/abs/2601.15688", "authors": ["Zhixuan Liang", "Xingyu Zeng", "Rui Zhao", "Ping Luo"], "title": "Performance-guided Reinforced Active Learning for Object Detection", "comment": "Accepted by ICASSP 2026. Camera-ready Version", "summary": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.", "AI": {"tldr": "提出了一种名为MGRAL的新型主动学习方法，通过强化学习来指导目标检测任务中样本的选择，以最大化mAP提升为目标，并引入了快速查找表来降低计算成本。", "motivation": "现有的主动学习方法在评估数据信息量时，并未直接与下游任务性能（如目标检测的mAP）挂钩，导致选择的样本不一定能带来最佳的模型性能提升。", "method": "MGRAL采用强化学习（RL）的方法，构建一个采样代理（sampling agent），将模型在选定批次上的预期输出变化作为信息量度量。使用策略梯度（policy gradient）算法，以mAP的提升作为奖励信号来优化采样策略。为了降低计算开销，利用查找表（look-up tables）进行无监督的mAP估计。", "result": "在PASCAL VOC和COCO数据集上评估了MGRAL在目标检测任务上的主动学习性能。结果显示，MGRAL取得了最佳的主动学习曲线，并提供了令人信服的可视化效果。", "conclusion": "MGRAL通过强化学习驱动的主动学习，为目标检测任务提供了一种新的、更有效的样本选择范式，能够以更少的标注数据实现更高的模型性能。"}}
{"id": "2601.15511", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.15511", "abs": "https://arxiv.org/abs/2601.15511", "authors": ["Adam Szelestey", "Sofie van Engelen", "Tianhao Huang", "Justin Snelders", "Qintao Zeng", "Songgaojun Deng"], "title": "AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains", "comment": "13 pages, 4 figures, and 11 tables", "summary": "Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.\n  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.", "AI": {"tldr": "本文提出了 AdversaRiskQA，一个评估大型语言模型（LLM）在对抗性事实错误（即在提示中嵌入带有不同置信度级别的信息错误）下的鲁棒性的基准。研究还评估了注入的错误信息对长篇文本事实性的影响，并对六个 LLM 进行了评估。", "motivation": "现有的 LLM 幻觉评估工作缺乏针对特定领域且高质量的基准，特别是关于模型在对抗性条件下（即提示中嵌入具有不同置信度级别的信息错误）的鲁棒性，并且之前没有研究检查过注入的错误信息对长篇文本事实性的影响。", "method": "引入了 AdversaRiskQA 基准，包含健康、金融和法律领域，并设置了两个难度级别。提出了两种自动评估方法来衡量对抗性攻击的成功率和长篇文本的事实性。使用 Qwen、GPT-OSS 和 GPT 系列的六个 LLM 进行评估，并对 Qwen3 (30B) 在基线和对抗性条件下的长篇事实性进行了评估。", "result": "在排除无意义响应后，Qwen3 (80B) 取得了最高的平均准确率，而 GPT-5 保持了持续的高准确率。模型性能随模型尺寸非线性增长，因领域而异，且随着模型增大，难度级别之间的差距缩小。长篇评估显示，注入的错误信息与模型的事实性输出之间没有显著相关性。", "conclusion": "AdversaRiskQA 提供了一个宝贵的基准，有助于 pinpoint LLM 的弱点，并为开发更可靠的高风险应用模型提供支持。研究发现，当前 LLM 在对抗性事实错误下的表现有待提高，且注入的错误信息对长篇文本事实性的影响并不显著。"}}
{"id": "2601.16207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16207", "abs": "https://arxiv.org/abs/2601.16207", "authors": ["Jongwoo Park", "Kanchana Ranasinghe", "Jinhyeok Jang", "Cristina Mata", "Yoo Sung Jang", "Michael S Ryoo"], "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance", "comment": null, "summary": "Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA", "AI": {"tldr": "提出了一种名为IVRA的轻量级、免训练方法，通过利用视觉编码器中已有的亲和力提示来增强视觉-语言-动作（VLA）模型对空间信息的理解，从而提高精确操作能力。", "motivation": "现有的VLA模型将图像块展平为一维序列，削弱了精确操作所需的二维空间线索。需要一种方法来增强模型对空间信息的理解，而不增加额外的计算负担或需要重新训练。", "method": "IVRA是一种在推理时进行的干预方法。它选择性地将视觉编码器中已有的亲和力信号注入到包含实例级特征的语言模型层中，从而调整视觉-标记的交互，更好地保留几何结构，同时保持所有模型参数不变。", "result": "IVRA被应用于LLaRA、OpenVLA和FLOWER等多种VLA架构，并在2D和3D操作的模拟基准（VIMA和LIBERO）以及真实机器人任务上进行了验证。在2D VIMA上，IVRA在低数据量条件下使LLaRA的平均成功率提高了+4.2%。在3D LIBERO上，IVRA在OpenVLA和FLOWER基线模型上均取得了持续的提升，即使在基线准确率接近饱和时（96.3%提升至97.1%）也有效。", "conclusion": "IVRA是一种通用且高效的方法，可以显著提升各种VLA模型的空间理解能力和操作性能，而无需任何额外的模型训练或编码器。"}}
{"id": "2601.15551", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15551", "abs": "https://arxiv.org/abs/2601.15551", "authors": ["Bismack Tokoli", "Luis Jaimes", "Ayesha S. Dina"], "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance", "comment": "35 pages", "summary": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.", "AI": {"tldr": "本文提出了一种名为 ALIGNAgent 的多智能体教育框架，通过整合知识评估、技能差距识别和资源推荐，实现个性化学习，并在真实数据集上进行了有效性验证。", "motivation": "现有的个性化学习系统通常只专注于知识追踪、诊断建模或资源推荐中的一个方面，缺乏将这些组件整合到连贯的自适应学习循环中。", "method": "ALIGNAgent 框架包含一个技能差距代理（Skill Gap Agent）和一个推荐代理（Recommender Agent）。技能差距代理通过概念级别的诊断推理，利用学生测验表现、成绩簿数据和学习者偏好来估计主题掌握程度，识别具体误解和知识缺陷。推荐代理根据识别出的技能差距，检索符合学习者偏好的、与其不足之处相匹配的学习材料，形成一个持续的反馈循环。", "result": "基于 GPT-4o 的 ALIGNAgent 在对两个本科计算机科学课程的真实数据集进行的大规模实证评估中，在知识掌握程度估计方面取得了 0.87-0.90 的精确率和 0.84-0.87 的 F1 分数，这些结果与实际考试表现相符。", "conclusion": "ALIGNAgent 框架通过集成知识评估、技能差距识别和资源推荐，能够有效地为学生提供个性化的学习指导，并能根据学生表现进行干预，推动其在后续学习中的进步。"}}
{"id": "2601.16212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16212", "abs": "https://arxiv.org/abs/2601.16212", "authors": ["Siddhant Haldar", "Lars Johannsmeier", "Lerrel Pinto", "Abhishek Gupta", "Dieter Fox", "Yashraj Narang", "Ajay Mandlekar"], "title": "Point Bridge: 3D Representations for Cross Domain Policy Learning", "comment": null, "summary": "Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/", "AI": {"tldr": "本文提出了一种名为Point Bridge的框架，它使用统一的、与领域无关的点表示，利用视觉语言模型（VLMs）从合成数据中提取点表示，并通过Transformer模型进行策略学习，实现了零样本的模拟到真实世界的策略迁移，无需显式的视觉或物体对齐。该方法在少量真实数据协同训练下性能更佳。", "motivation": "大型真实世界机器人操作数据集的稀缺性限制了通用机器人代理的发展。虽然仿真和合成数据提供了可扩展的替代方案，但仿真与现实之间的视觉域差异限制了其有效性。", "method": "Point Bridge框架采用自动化点表示提取（通过VLMs）、基于Transformer的策略学习以及高效的推理管道。它首先从合成数据中提取点表示，然后利用这些表示训练机器人策略，最后在少量真实数据上进行微调。", "result": "Point Bridge在零样本模拟到真实世界的迁移中取得了高达44%的性能提升，在有限真实数据协同训练下可达66%的提升，优于现有的基于视觉的模拟与真实协同训练方法。", "conclusion": "Point Bridge框架通过利用点表示有效弥合了模拟与真实世界的域差异，实现了高效的零样本模拟到真实世界的策略迁移，并在少量真实数据辅助下进一步提升了性能，为利用合成数据训练机器人代理提供了一种新的有效途径。"}}
{"id": "2601.15550", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15550", "abs": "https://arxiv.org/abs/2601.15550", "authors": ["Sangmitra Madhusudan", "Trush Shashank More", "Steph Buongiorno", "Renata Dividino", "Jad Kabbara", "Ali Emami"], "title": "Common to Whom? Regional Cultural Commonsense and LLM Bias in India", "comment": null, "summary": "Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the \"default\" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.", "AI": {"tldr": "本研究提出了Indica，一个评估大型语言模型（LLMs）对印度次国家层面文化常识理解能力的基准。研究发现，印度文化常识具有显著的地域性，而非全国统一，并且现有LLMs在理解这种地域性方面存在严重不足，并表现出地理偏见。", "motivation": "现有文化常识基准将国家视为单一实体，忽略了国家内部文化差异。研究旨在探索文化常识是否在国家内部存在地域差异，并开发评估LLMs在这方面的能力。", "method": "研究团队收集了来自印度五个不同地区的515个问题（涵盖8个日常生活领域）的人工标注答案，构建了Indica基准。该基准包含1,630个地区特定的问答对。随后，评估了八个最先进的LLMs在Indica基准上的表现，并分析了其在地区特定问题上的准确率和存在的地理偏见。", "result": "研究发现，只有39.4%的问题能在所有五个地区获得一致的回答，表明印度文化常识主要呈地域性。LLMs在地区特定问题上的准确率仅为13.4%-20.9%。此外，LLMs存在地理偏见，倾向于过度选择印度中部和北部作为“默认”地区，而低估了东部和西部地区。", "conclusion": "印度的文化常识具有显著的地域性，而非全国统一。现有LLMs在理解这种次国家层面的文化差异方面表现不佳，并且存在明显的地理偏见。本研究提出的方法论可推广至评估其他文化异质性国家的LLMs的文化常识理解能力。"}}
{"id": "2601.15698", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15698", "abs": "https://arxiv.org/abs/2601.15698", "authors": ["Mingyu Yu", "Lana Liu", "Zhehao Zhao", "Wei Wang", "Sujuan Qin"], "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs", "comment": null, "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.", "AI": {"tldr": "本文提出了Beyond Visual Safety (BVS)框架，通过“重构-生成”策略，利用中和视觉拼接和归纳重组，成功绕过了GPT-5的视觉安全防护，实现了98.21%的越狱成功率，揭示了多模态大语言模型在视觉安全方面存在的严重漏洞。", "motivation": "现有的多模态大语言模型（MLLMs）安全研究主要集中在文本层面，对视觉安全边界的探索不足，作者旨在深入研究MLLMs的视觉安全漏洞。", "method": "提出Beyond Visual Safety (BVS)框架，采用“重构-then-生成”策略，结合中和视觉拼接和归纳重组技术，将恶意意图与原始输入解耦，从而诱导MLLMs生成有害图像。", "result": "BVS框架在GPT-5（2026年1月12日发布版本）上实现了98.21%的越狱成功率。", "conclusion": "实验结果表明，当前的多模态大语言模型在视觉安全对齐方面存在显著的脆弱性，BVS框架能够有效地探测并利用这些漏洞。"}}
{"id": "2601.16020", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16020", "abs": "https://arxiv.org/abs/2601.16020", "authors": ["Weichen Dai", "Wenhan Su", "Da Kong", "Yuhang Ming", "Wanzeng Kong"], "title": "Keyframe-Based Feed-Forward Visual Odometry", "comment": null, "summary": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.", "AI": {"tldr": "本文提出了一种基于强化学习的自适应关键帧选择方法，用于提升视觉里程计（VO）的效率和准确性，克服了现有视觉基础模型直接处理连续帧的冗余和性能下降问题。", "motivation": "现有的视觉基础模型虽然在VO和SLAM任务中表现出色，但它们通常 indiscriminately 处理所有图像帧，导致计算冗余，并在低视差帧时性能下降。将传统方法中的关键帧机制与基础模型结合具有挑战性。因此，研究动机是开发一种能自适应选择关键帧的方法，以提高基础模型在VO任务中的效率和准确性。", "method": "该研究提出了一种基于强化学习（RL）的自适应关键帧选择策略。通过训练一个RL代理，在数据驱动的模式下学习最优的关键帧选择策略，以适应基础模型的内在特性，而非依赖手工设计的规则。该方法在TartanAir数据集上进行训练，并在多个真实世界数据集上进行评估。", "result": "实验结果表明，提出的方法在与最先进的纯前馈VO方法相比时，在多个真实世界数据集上实现了持续且显著的性能提升。", "conclusion": "该研究成功地将RL驱动的自适应关键帧选择机制与视觉基础模型相结合，显著提高了视觉里程计的效率和准确性，为未来研究提供了新的方向。"}}
{"id": "2601.15558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15558", "abs": "https://arxiv.org/abs/2601.15558", "authors": ["Man Luo", "Bahareh Harandizadeh", "Amara Tariq", "Halim Abbas", "Umar Ghaffar", "Christopher J Warren", "Segun O. Kolade", "Haidar M. Abdul-Muhsin"], "title": "From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare", "comment": null, "summary": "Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.", "AI": {"tldr": "研究表明，大型语言模型（LLMs）可以作为“共情编辑器”，在保留医学信息的同时，提高医生书面回复的共情语气，且在共情和事实准确性上优于完全由LLM生成的回复。", "motivation": "在临床实践中，医生需要在情感温暖和事实精确性之间取得平衡，同时受到认知和情感的限制。本研究旨在探索LLMs如何作为共情编辑器，帮助医生改进书面回复，提高共情水平。", "method": "引入了量化指标“共情评分”（Empathy Ranking Score）和“医学事实核查评分”（MedFactChecking Score）来评估回复的情感和事实质量。通过实验对比LLM编辑的回复与完全由LLM生成的回复。", "result": "LLM编辑的回复在感知共情方面显著优于完全由LLM生成的输出，同时保持了事实准确性。", "conclusion": "使用LLMs作为编辑助手，而不是自主生成者，是实现共情且可信赖的AI辅助医疗沟通的更安全、更有效的方法。"}}
{"id": "2601.15599", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15599", "abs": "https://arxiv.org/abs/2601.15599", "authors": ["Cecil Pang", "Hiroki Sayama"], "title": "Autonomous Business System via Neuro-symbolic AI", "comment": "Accepted to IEEE SysCon 2026", "summary": "Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.", "AI": {"tldr": "本文提出了一种名为AUTOBUS的自主业务系统，该系统整合了大型语言模型（LLM）、谓词逻辑编程和以业务语义为中心的企业数据，构建了一个神经符号AI架构，用于协调端到端的业务计划。AUTOBUS通过将业务计划建模为具有明确条件、数据需求和执行规则的任务网络，并利用知识图谱来提供语义基础，从而克服了传统企业系统僵化和LLM缺乏确定性执行能力的局限性。", "motivation": "现有企业环境要求组织不断重构跨职能流程，但企业系统仍然僵化且自动化硬编码。而大型语言模型（LLMs）擅长解释自然语言和非结构化数据，但在执行复杂业务逻辑方面缺乏确定性和可验证性。本文旨在弥合这一差距。", "method": "AUTOBUS整合了基于LLM的AI代理、谓词逻辑编程和业务语义中心的企业数据，构建了一个神经符号AI架构。它将业务计划建模为任务网络，每个任务都有明确的前置/后置条件、所需数据、评估规则和API级操作。企业数据被组织成知识图谱，其实体、关系和约束被翻译成逻辑事实和基础规则。AI代理生成任务特定的逻辑程序，由逻辑引擎执行，该引擎强制执行约束、协调辅助工具并编排操作和结果的执行。", "result": "AUTOBUS能够将LLM的理解能力与逻辑编程的确定性相结合，以一种可验证和可解释的方式编排端到端的业务计划。人类用户在语义定义、策略制定、工具选择和高风险决策监督等方面发挥关键作用，确保系统的适应性和问责制。", "conclusion": "AUTOBUS提供了一种新颖的神经符号AI架构，能够有效地将LLM的强大能力与逻辑编程的确定性和可验证性相结合，以自动化和协调复杂的业务流程，同时保留人类在关键环节的控制权。"}}
{"id": "2601.16065", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16065", "abs": "https://arxiv.org/abs/2601.16065", "authors": ["Chenyang Li", "Jieyuan Liu", "Bin Li", "Bo Gao", "Yilin Yuan", "Yangfan He", "Yuchen Li", "Jingqun Tang"], "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models", "comment": null, "summary": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.", "AI": {"tldr": "提出了一种即插即用的 Distracting Token Pruning (DTP) 框架，通过动态检测和修剪分散注意力的图像 token 来改进 Vision-Language Action (VLA) 模型在机器人操作中的性能。", "motivation": "现有的 VLA 模型可能过度关注与任务无关的图像区域（“分散 token”），这会干扰模型生成正确的动作 token，从而影响任务成功率。", "method": "引入 Distracting Token Pruning (DTP) 框架，一种即插即用的方法，能够动态检测并修剪分散注意力的图像 token，以纠正模型的视觉注意力模式，而无需修改其原始架构或添加额外输入。", "result": "在 SIMPLER Benchmark 上进行了实验，DTP 方法在不同类型的 VLA 模型上均能稳定地提高任务成功率，证明了其对基于 Transformer 的 VLA 模型的通用性。研究还发现，任务成功率与任务无关区域的注意力量呈负相关。", "conclusion": "DTP 框架是一种简单有效的方法，可以提高 VLA 模型在机器人操作中的性能，通过纠正视觉注意力来克服分散 token 的问题。研究结果表明，分散 token 是 VLA 模型普遍存在的问题，并且任务无关区域的注意力是影响性能的关键因素。"}}
{"id": "2601.15705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15705", "abs": "https://arxiv.org/abs/2601.15705", "authors": ["Ali Caglayan", "Nevrez Imamoglu", "Toru Kouyama"], "title": "Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data", "comment": "5 pages, 4 figures", "summary": "This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.", "AI": {"tldr": "本研究利用ALOS-2单极化SAR数据对日本进行大范围的土地利用/土地覆盖（LULC）语义分割，并结合水体检测任务。通过引入三种轻量级改进，有效解决了SAR数据在密集预测中存在的边界平滑、细长结构漏检以及类别不均衡问题，并在日本LULC基准测试和水体检测上取得了改进。", "motivation": "现有的SAR密集预测模型存在边界平滑、细长结构漏检和罕见类别性能下降等问题，尤其是在长尾分布标签下。本研究旨在解决这些常见失效模式，同时不增加模型复杂性。", "method": "1. 采用SAR-W-MixMAE自监督预训练。2. 引入三种轻量级改进：(i) 将高分辨率特征注入多尺度解码器；(ii) 设计渐进式精炼上采样头，交替进行卷积精炼和分步上采样；(iii) 使用$α$-scale因子调整focal+dice损失函数中的类别重加权。", "result": "模型在日本LULC基准测试上表现出持续的改进，尤其是在代表性不足的类别上。同时，在各项评估指标上提升了水体检测的性能。", "conclusion": "通过三种轻量级改进，本研究成功提升了SAR数据的LULC语义分割和水体检测能力，有效解决了长尾分布下罕见类别的性能问题，并在日本全国范围内验证了其有效性。"}}
{"id": "2601.15588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15588", "abs": "https://arxiv.org/abs/2601.15588", "authors": ["Junyu Lin", "Meizhen Liu", "Xiufeng Huang", "Jinfeng Li", "Haiwen Hong", "Xiaohan Yuan", "Yuefeng Chen", "Longtao Huang", "Hui Xue", "Ranjie Duan", "Zhikai Chen", "Yuchuan Fu", "Defeng Li", "Lingyao Gao", "Yitong Yang"], "title": "YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.", "AI": {"tldr": "本文提出了一种名为YuFeng-XGuard的基于推理的LLM安全防护模型，它能够进行多维度的风险感知，提供结构化的风险预测、置信度和自然语言解释，并采用分层推理和动态策略机制来平衡性能和可解释性，实验证明其性能优越且效率高。", "motivation": "现有LLM的安全防护方法存在粗粒度、透明度低、策略不灵活和推理成本高的问题，需要更精细、可解释和适应性强的风险评估。", "method": "提出YuFeng-XGuard模型家族，采用基于推理的方法进行多维度风险感知，生成结构化的风险预测（类别、置信度）和自然语言解释。采用分层推理机制（基于首个解码token做出初步判断，按需提供解释）平衡延迟和解释深度。引入动态策略机制解耦风险感知与策略执行。", "result": "在多样的安全基准测试中，YuFeng-XGuard达到了最先进的性能，并保持了优异的效率-效能权衡。", "conclusion": "YuFeng-XGuard是一个以推理为中心、可解释、灵活且高效的LLM安全防护解决方案，通过结构化风险预测和分层推理，能够实现更精细化的风险评估和决策，并提供可调整的安全策略。"}}
{"id": "2601.15628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15628", "abs": "https://arxiv.org/abs/2601.15628", "authors": ["Haibo Tong", "Zeyang Yue", "Feifei Zhao", "Erliang Lin", "Lu Jia", "Ruolin Chen", "Yinqian Sun", "Qian Zhang", "Yi Zeng"], "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models", "comment": null, "summary": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.", "AI": {"tldr": "研究者们提出了CogToM，一个包含8000多个双语实例和46种范例的大型、理论驱动的ToM基准测试，用于评估LLM在类人思维能力方面的表现。对22个代表性模型（包括GPT-5.1和Qwen3-Max）的评估显示，模型表现存在显著差异，并在某些方面存在瓶颈。研究还表明LLM与人类认知结构可能存在差异。", "motivation": "现有关于LLM是否具备类人思维能力（ToM）的评估主要局限于狭窄的范式（如错误信念任务），无法全面捕捉人类的认知机制。因此，需要一个更全面、理论更扎实的基准来深入研究LLM的ToM能力。", "method": "开发了一个名为CogToM的基准测试，该测试包含超过8000个双语实例，覆盖46种认知范式，并由49名人类标注者进行了验证。使用该基准对包括GPT-5.1和Qwen3-Max在内的22个代表性LLM进行了系统评估，并分析了模型表现与人类认知模式的潜在差异。", "result": "评估结果显示，在CogToM基准测试中，不同LLM在ToM能力上表现出显著的异质性，并且在特定维度上存在持续的瓶颈。基于人类认知模式的进一步分析表明，LLM的认知结构可能与人类存在潜在的分歧。", "conclusion": "CogToM提供了一个全面且理论上可靠的工具和视角，用于深入探究LLM不断发展的认知边界，并揭示了当前LLM在ToM能力方面存在的挑战和与人类认知方式的潜在差异。"}}
{"id": "2601.16163", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.16163", "abs": "https://arxiv.org/abs/2601.16163", "authors": ["Moo Jin Kim", "Yihuai Gao", "Tsung-Yi Lin", "Yen-Chen Lin", "Yunhao Ge", "Grace Lam", "Percy Liang", "Shuran Song", "Ming-Yu Liu", "Chelsea Finn", "Jinwei Gu"], "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning", "comment": null, "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/", "AI": {"tldr": "Cosmos Policy 是一种新方法，通过单一阶段的机器人演示数据后训练，将预训练的视频模型（Cosmos-Predict2）直接适配为有效的机器人策略。该方法无需修改模型架构，直接在视频模型的潜在扩散过程中生成动作、未来状态图像和预期累积奖励，从而实现更有效的测试时规划，并在模拟和真实世界任务中取得了最先进的性能。", "motivation": "现有的机器人策略学习方法在适配视频模型时引入了复杂性，需要多阶段后训练和新的架构组件。作者希望找到一种更简单有效的方法，直接利用预训练视频模型的时空先验知识来学习机器人策略。", "method": "Cosmos Policy 利用预训练的视频模型（Cosmos-Predict2），通过在目标平台的机器人演示数据上进行单阶段后训练，将其适配为机器人策略。该方法将机器人动作、未来状态图像和预期累积奖励编码为视频模型潜在扩散过程中的潜在帧，直接生成这些信息，无需改变模型架构。在测试时，通过结合生成的未来状态图像和值函数进行规划。", "result": "Cosmos Policy 在 LIBERO 和 RoboCasa 模拟基准上取得了最先进的性能（平均成功率分别为 98.5% 和 67.1%）。在真实世界的双臂操作任务中，其平均得分最高，优于从头开始训练的扩散策略、基于视频模型的策略以及在相同机器人演示数据上微调的先进视觉-语言-动作模型。此外，通过从经验中学习和模型预测规划，Cosmos Policy 还能在挑战性任务中进一步提高成功率。", "conclusion": "Cosmos Policy 提供了一种简单有效的方法，能够直接将大型预训练视频模型适配为机器人策略，通过利用模型的时空先验知识，在各种模拟和真实世界任务中取得了最先进的性能。该方法还支持通过在线学习和模型预测规划来进一步提升性能。"}}
{"id": "2601.15711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15711", "abs": "https://arxiv.org/abs/2601.15711", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework", "comment": "Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)", "summary": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.", "AI": {"tldr": "本文提出了一种评估视觉-语言模型（VLMs）在细粒度时尚属性预测任务上的三层框架，发现在细粒度分类上VLMs表现优异，但在属性适用性检测方面存在瓶颈，并指出高效模型在性能和成本之间取得了良好平衡。", "motivation": "现有的视觉-语言模型（VLMs）在零样本时尚属性预测方面有潜力，但其在多属性时尚任务上的系统性评估不足。时尚属性常具有条件性（例如，如果没有可见的外层服装，“外层织物”属性就无法定义），这要求模型在分类前检测属性的适用性。因此，需要一个能够分解这一挑战的评估框架。", "method": "引入了一个三层评估框架，将任务分解为：1) 包含“不适用”（NA）类别的整体任务性能；2) 属性适用性检测；3) 可确定的属性的细粒度分类。使用DeepFashion-MultiModal数据集，对九个VLMs（包括旗舰、高效和超高效版本）以及基于Fashion-CLIP嵌入训练的分类器进行了基准测试。", "result": "1) 零样本VLMs的平均F1分数达到64.0%，是基于Fashion-CLIP嵌入的逻辑回归的三倍。2) VLMs在细粒度分类（第3层：70.8% F1）方面表现出色，但在适用性检测（第2层：34.1% NA-F1）方面存在困难，这是主要的瓶颈。3) 高效模型在成本较低的情况下，获得了旗舰模型90%以上的性能。", "conclusion": "该诊断框架能够帮助研究人员和从业者确定模型错误是源于可见性检测还是分类能力，从而指导针对生产系统的改进。零样本VLMs在细粒度时尚属性预测方面展现出巨大潜力，但属性适用性检测是需要重点关注和改进的方向。高效的VLMs模型为实际部署提供了可行的解决方案。"}}
{"id": "2601.15593", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15593", "abs": "https://arxiv.org/abs/2601.15593", "authors": ["Yangyang Zhong", "Yanmei Gu", "Zhengqing Zang", "Xiaomeng Li", "Yuqi Ding", "Xibei Jia", "Yuting Shen", "Zhenzhong Lan", "Liwang Zhu", "Weiping Liu", "Junlin Zhou", "Haisheng Liu", "Zhong Xin Yu", "Pengxin Luo", "Donglian Qi", "Yunfeng Yan", "Junbo Zhao"], "title": "Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow", "comment": null, "summary": "Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require \"backward information\" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.", "AI": {"tldr": "本文评估了当前主流的掩码扩散语言模型（MDLM）在并行生成和任意顺序解码方面的实际能力，发现它们在处理长距离依赖方面仍落后于自回归模型，但表现出任务和输出相关的自适应解码行为，并提出了一种“生成-编辑”范式以优化性能。", "motivation": "现有研究对掩码扩散语言模型（MDLM）的并行生成和任意顺序解码能力存在不确定性，本文旨在深入探究其真实表现。", "method": "使用平均最终并行度（AFP）和Kendall's tau指标，在58个不同领域（知识、推理、编程）的基准测试上，对八个主流MDLM（最大100B参数）的行为进行了评估。", "result": "MDLM在保持并行生成的同时，其并行度和生成顺序会根据任务领域、推理阶段以及输出正确性而自适应变化。在需要“后向信息”的任务（如数独）中，MDLM能展现出一定的优势，例如优先填充简单的数独空格。然而，与同等规模的自回归模型相比，MDLM在保留令牌间的依赖性方面仍有差距。", "conclusion": "MDLM在并行生成和任意顺序解码方面仍有提升空间，尤其是在处理令牌间依赖性方面。其表现具有自适应性，并且“生成-编辑”范式有望在保留并行解码效率的同时，缓解依赖性减弱的问题。"}}
{"id": "2601.15724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15724", "abs": "https://arxiv.org/abs/2601.15724", "authors": ["Chenglin Li", "Qianglong Chen", "Feng Han", "Yikun Wang", "Xingxi Yin", "Yan Gong", "Ruilin Li", "Yin Zhang", "Jiaqi Wang"], "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning", "comment": null, "summary": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.", "AI": {"tldr": "提出了一种名为 VideoThinker 的智能体式视频大语言模型，它通过在合成的工具交互轨迹上进行训练，解决了长视频理解的挑战，无需预先具备长视频理解能力，并在长视频基准测试中取得了显著的性能提升。", "motivation": "现有视频大语言模型在处理长视频时存在信息丢失和时间定位能力弱的问题，因为它们依赖于静态帧采样和均匀推理。虽然智能体工具（如时间检索、空间缩放、时间缩放）可以克服这些限制，但其训练数据需要模型本身具备强大的长视频理解能力，存在循环依赖。", "method": "VideoThinker 模型完全在合成的工具交互轨迹上进行训练。该方法将视频转换为丰富的字幕，然后利用强大的智能体语言模型在字幕空间生成多步工具使用序列。这些序列随后通过将字幕替换为相应的视频帧来重新映射到视频，从而创建一个大规模的、交织着视频和工具推理的数据集，而底层模型无需具备长视频理解能力。", "result": "在合成的智能体数据集上训练的 VideoThinker 具备了动态推理能力、自适应时间探索和多步工具使用能力。与仅使用字幕的语言模型智能体和强大的视频模型基线相比，VideoThinker 在长视频基准测试中表现出显著的优势。", "conclusion": "通过工具增强的合成数据和自适应检索/缩放推理，VideoThinker 有效地解决了长视频理解的挑战，证明了其在长视频理解任务上的优越性。"}}
{"id": "2601.15731", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15731", "abs": "https://arxiv.org/abs/2601.15731", "authors": ["Linyong Zou", "Liang Zhang", "Xiongfei Wang", "Jia-Hong Gao", "Yi Sun", "Shurong Sheng", "Kuntao Xiao", "Wanli Yang", "Pengfei Teng", "Guoming Luan", "Zhao Lv", "Zikang Xu"], "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging", "comment": null, "summary": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.", "AI": {"tldr": "本文提出了一种名为FAIR-ESI的新型框架，通过自适应地改进不同视图下的特征重要性，来解决电生理源成像（ESI）中特征选择和精炼的挑战，以期提高脑部疾病诊断的准确性。", "motivation": "现有的基于模型和深度学习的ESI方法在脑部疾病诊断中取得了进展，但精确的特征选择和精炼仍然是实现ESI精度的关键挑战。", "method": "FAIR-ESI框架集成了三种自适应特征重要性精炼方法：基于FFT的光谱特征精炼，加权的 temporal 特征精炼，以及基于自注意力机制的 patch-wise 特征精炼。", "result": "在两个模拟数据集和两个临床数据集上的大量实验证明了FAIR-ESI框架的有效性。", "conclusion": "FAIR-ESI框架有望提高脑部疾病诊断的准确性，并为理解大脑功能提供新的见解。"}}
{"id": "2601.15652", "categories": ["cs.AI", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.15652", "abs": "https://arxiv.org/abs/2601.15652", "authors": ["Manish Bhatt"], "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models", "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).\n  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").\n  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.", "AI": {"tldr": "本研究提出了一种名为 [Model Name] 的混合检测框架，结合了受神经科学启发的信号设计和监督机器学习，用于检测大型语言模型（LLMs）的幻觉。该框架通过可解释的信号（基于预测编码和信息瓶颈）以及实体聚焦、上下文遵循和可证伪性评分等增强功能，在 HaluBench 数据集上取得了 0.8669 的 AUROC 评分，同时具有更高的效率和可解释性。", "motivation": "现有的大语言模型幻觉检测方法计算成本高昂，或者依赖于大型、不透明的模型，这限制了其在高风险场景下的部署。本研究旨在开发一种更高效、可解释的检测框架。", "method": "该研究引入了一个名为 [Model Name] 的混合检测框架，该框架结合了受神经科学启发的信号设计（基于预测编码和信息瓶颈）与监督机器学习。关键技术包括实体聚焦、上下文遵循和可证伪性评分，以提高检测性能。研究还通过消融实验验证了这些增强功能的作用。", "result": "在 HaluBench 数据集上，该理论指导的基线模型取得了 0.8017 的 AUROC 评分。添加监督学习后，性能提升至 0.8274 AUROC。通过引入改进的功能，性能进一步提高到 0.8669 AUROC，比现有方法有显著提升。该方法使用了更少的训练数据（75倍），推理速度快1000倍，并且完全可解释。研究还发现“合理化”信号无法有效区分幻觉，暗示 LLMs 会为错误的前提生成连贯的解释。", "conclusion": "将领域知识编码到信号架构中，比扩展 LLM 法官更具数据效率。本研究提出的轻量级（少于1M参数）、可解释的模型能够实现出色的幻觉检测性能，并适合于实际生产部署。"}}
{"id": "2601.15605", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.15605", "abs": "https://arxiv.org/abs/2601.15605", "authors": ["Baktash Ansari", "Shiza Ali", "Elias Martin", "Maryna Sivachenko", "Afra Mashhadi"], "title": "ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms", "comment": "Exploratory study; prior versions submitted to peer review", "summary": "The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.", "AI": {"tldr": "本研究探索了在Twitch直播平台上检测有毒行为的方法，提出了一种结合LLM嵌入和传统机器学习的混合模型ToxiTwitch，并发现包含表情符号（emotes）可以提升检测效果，在通道特定训练下准确率可达80%。", "motivation": "传统的内容审核方法（如人工标注和关键词过滤）在Twitch这种快节奏、信息量大且富含语境的直播平台上面临扩展性不足的问题，而大型语言模型（LLMs）为解决这些挑战提供了新机遇，尤其是在理解包含表情符号的模态通信方面。", "method": "研究人员进行了有毒行为检测方法的探索性比较，并提出了一种混合模型ToxiTwitch。该模型结合了LLM生成的文本和表情符号的嵌入表示，以及随机森林（Random Forest）和支持向量机（SVM）等传统机器学习分类器。", "result": "研究结果表明，在检测有毒行为时包含表情符号能够提升检测效果。提出的混合方法在通道特定训练下，准确率最高可达80%，比BERT模型提高了13%，F1分数达到76%。", "conclusion": "本研究是一项探索性研究，旨在揭示在Twitch平台上进行面向表情符号的有毒行为检测所面临的挑战和局限性，并提出了一个有前景的混合模型。"}}
{"id": "2601.15679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15679", "abs": "https://arxiv.org/abs/2601.15679", "authors": ["Ee Wei Seah", "Yongsen Zheng", "Naga Nikshith", "Mahran Morsidi", "Gabriel Waikin Loh Matienzo", "Nigel Gay", "Akriti Vij", "Benjamin Chua", "En Qi Ng", "Sharmini Johnson", "Vanessa Wilfred", "Wan Sie Lee", "Anna Davidson", "Catherine Devine", "Erin Zorer", "Gareth Holvey", "Harry Coppock", "James Walpole", "Jerome Wynee", "Magda Dubois", "Michael Schmatz", "Patrick Keane", "Sam Deverett", "Bill Black", "Bo Yan", "Bushra Sabir", "Frank Sun", "Hao Zhang", "Harriet Farlow", "Helen Zhou", "Lingming Dong", "Qinghua Lu", "Seung Jang", "Sharif Abuadbba", "Simon O'Callaghan", "Suyu Ma", "Tom Howroyd", "Cyrus Fung", "Fatemeh Azadi", "Isar Nejadgholi", "Krishnapriya Vishnubhotla", "Pulei Xiong", "Saeedeh Lohrasbi", "Scott Buffett", "Shahrear Iqbal", "Sowmya Vajjala", "Anna Safont-Andreu", "Luca Massarelli", "Oskar van der Wal", "Simon Möller", "Agnes Delaborde", "Joris Duguépéroux", "Nicolas Rolin", "Romane Gallienne", "Sarah Behanzin", "Tom Seimandi", "Akiko Murakami", "Takayuki Semitsu", "Teresa Tsukiji", "Angela Kinuthia", "Michael Michie", "Stephanie Kasaon", "Jean Wangari", "Hankyul Baek", "Jaewon Noh", "Kihyuk Nam", "Sang Seo", "Sungpil Shin", "Taewhi Lee", "Yongsu Kim"], "title": "Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats", "comment": "The author/contributor list organises contributors by country and alphabetical order within each country. In some places, the order has been altered to match other related publications", "summary": "The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.\n  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.\n  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.\n  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.", "AI": {"tldr": "本次研究是国际先进人工智能测量、评估与科学网络（The International Network for Advanced AI Measurement, Evaluation and Science）进行的第三次联合演习，旨在推进对人工智能代理（agent）的评估方法学，重点关注语言和文化适应性、信息泄露、欺诈和网络安全等风险，并改进测试最佳实践。", "motivation": "随着自主人工智能系统和代理能力的快速发展，在实际应用中出现新的风险，但对代理的测试方法尚不成熟。为了确保AI代理在全球范围内的准确、安全部署，尤其是在不同语言和文化环境中，需要对其进行可靠的评估。", "method": "本次演习分为两个部分：(1) 共同风险（如敏感信息泄露和欺诈），由新加坡主导；(2) 网络安全，由英国主导。研究人员评估了不同开放和闭源模型在各种公共代理基准任务上的表现。研究的主要焦点在于理解测试方法学问题，而非测试结果或模型能力本身。", "result": "本次演习通过评估多语言和文化背景下的代理行为，并聚焦于方法学问题，为理解和改进代理测试流程提供了洞察。具体的模型能力和风险评估结果并未在本摘要中详述，但强调了国际合作在推进代理评估科学方面的重要性。", "conclusion": "本次国际联合演习是朝着改进和标准化人工智能代理评估方法迈出的重要一步。通过关注方法学挑战，研究为未来更全面、更可靠的代理测试奠定了基础，特别是考虑到AI代理在全球化和多文化环境中的应用需求。"}}
{"id": "2601.15734", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15734", "abs": "https://arxiv.org/abs/2601.15734", "authors": ["Shadi Alijani", "Fereshteh Aghaee Meibodi", "Homayoun Najjaran"], "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation", "comment": null, "summary": "The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.", "AI": {"tldr": "提出了一种用于多模态医学图像的迁移学习框架，通过子区域感知模态注意力和自适应提示工程，提高了脑肿瘤分割的准确性，尤其是在坏死核心区域。", "motivation": "现有针对多模态医学图像的迁移学习模型在融合多源信息和适应病理组织异质性方面存在困难，导致准确性受限。", "method": "提出了一种新的框架，包含两个关键创新：1. 子区域感知模态注意力机制，使模型能为每个肿瘤子区域学习最佳模态组合；2. 自适应提示工程，利用基础模型的内在能力来优化分割精度。", "result": "在BraTS 2020脑肿瘤分割数据集上验证了该框架，结果表明该方法显著优于基线方法，尤其是在最具挑战性的坏死核心子区域。", "conclusion": "该研究提供了一种原则性且有效的多模态融合和提示方法，为开发更准确、更鲁棒的医学影像基础模型解决方案开辟了道路。"}}
{"id": "2601.15690", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.15690", "abs": "https://arxiv.org/abs/2601.15690", "authors": ["Jiaxin Zhang", "Wendi Cui", "Zhuohang Li", "Lifu Huang", "Bradley Malin", "Caiming Xiong", "Chien-Sheng Wu"], "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models", "comment": "20 pages, 4 figures, 6 tables", "summary": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.", "AI": {"tldr": "本文综述了如何将大语言模型（LLM）中的不确定性从被动的诊断指标转变为主动控制信号，以提高其在关键领域的可靠性。研究了不确定性在高级推理、自主代理和强化学习中的应用，并讨论了其理论基础和设计模式。", "motivation": "当前大语言模型在部署到高风险领域时存在可靠性问题，而将不确定性作为主动控制信号是解决这一挑战的关键。研究旨在提供一个统一的视角来理解和利用这一趋势，以构建更可靠、可信赖的AI。", "method": "本文采取综述的形式，通过分析不确定性作为主动控制信号在三个前沿领域的应用：高级推理（优化计算、触发自我纠正）、自主代理（指导工具使用和信息检索的元认知决策）以及强化学习（减轻奖励篡改、通过内在奖励实现自我改进）。同时，文章将这些进展与贝叶斯方法和共形预测等理论框架相结合，提供了统一的视角。", "result": "研究展示了不确定性如何作为主动控制信号，在高级推理中优化计算和触发自我纠正；在自主代理中指导元认知决策，如工具使用和信息寻求；在强化学习中，用于减轻奖励篡改并实现自我改进。文章还提供了相关的理论框架和设计模式。", "conclusion": "将不确定性作为主动控制信号是提高大语言模型可靠性的关键趋势。掌握这一趋势对于构建下一代可扩展、可靠和值得信赖的AI至关重要。本综述提供了对这一趋势的全面概述、批判性分析和实用设计模式。"}}
{"id": "2601.15674", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15674", "abs": "https://arxiv.org/abs/2601.15674", "authors": ["Raymond Xiong", "Furong Jia", "Lionel Wong", "Monica Agrawal"], "title": "What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking", "comment": null, "summary": "Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.", "AI": {"tldr": "研究人员收集了患者在现实生活中常问的关于药物的问题，发现其中包含许多错误假设和危险意图，并证明现有的大型语言模型在识别这些问题方面表现不佳。", "motivation": "现有的大型语言模型（LLMs）的医疗问答基准测试主要关注医学考试题目，而忽略了患者在现实生活中提出的、更具日常性和可能包含错误假设的问题。因此，需要为LLMs在实际患者问答场景下的表现提供一个更准确的评估。", "method": "通过查询美国排名前200的处方药，利用Google的“人们也问”（People Also Ask）功能收集了大量患者常问的医疗问题，并构建了一个包含这些问题的数据集。随后，分析了这些问题中错误假设和危险意图的出现模式，并测试了现有LLMs识别这些问题的能力。", "result": "收集到的问题数据集中，相当一部分问题包含了错误的假设和危险的意图。研究发现，这些“损坏”问题的出现并非随机，而是与导致它们出现的问题历史中的错误程度密切相关。此外，在其他基准测试中表现优异的LLMs，在识别这些日常问题中的错误假设方面表现不佳。", "conclusion": "现有的LLMs在处理患者实际提出的、可能包含错误假设和危险意图的医疗问题方面存在显著不足。未来的研究需要开发能够更好地理解和识别这些非标准问题的LLMs，以确保患者能够获得安全和准确的健康信息。"}}
{"id": "2601.15708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15708", "abs": "https://arxiv.org/abs/2601.15708", "authors": ["Junseok Kim", "Nakyeong Yang", "Kyomin Jung"], "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time", "comment": "EACL'26 Findings, Code is available at https://github.com/junseokkim00/PersonaSwitch", "summary": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.", "AI": {"tldr": "提出了一种名为Persona Switch的新解码方法，通过动态结合零样本提示和角色扮演提示的优势，并使用输出置信度（logit gap）来衡量，从而提高了语言模型的零样本推理能力。", "motivation": "角色扮演提示可以提高语言模型的零样本推理能力，但其表现不一致。研究者认为零样本提示和角色扮演提示各有优劣，可以互补。", "method": "Persona Switch方法在解码过程中，逐步比较零样本提示和角色扮演提示在每一步生成的输出的置信度（通过logit gap衡量），并选择置信度更高的输出。", "result": "在主流语言模型上的实验表明，Persona Switch持续优于其他基线方法，准确率提升高达5.13%。同时，研究证明输出置信度是衡量输出可靠性的有效指标。", "conclusion": "Persona Switch是一种有效的解码方法，通过结合零样本提示和角色扮演提示并利用输出置信度进行动态选择，能够显著提升语言模型的零样本推理性能。"}}
{"id": "2601.15630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15630", "abs": "https://arxiv.org/abs/2601.15630", "authors": ["Chandra Prakash", "Mary Lind", "Avneesh Sisodia"], "title": "Agentic AI Governance and Lifecycle Management in Healthcare", "comment": "9 Page, 3 figures", "summary": "Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.", "AI": {"tldr": "本文提出了一种统一的智能体生命周期管理（UALM）蓝图，旨在解决医疗保健领域智能体扩散带来的重复、责任不清、控制不一致和权限滥用等问题，以实现可审计的、安全的智能体规模化部署。", "motivation": "随着智能体AI在医疗保健组织中的广泛应用，出现了智能体泛滥（agent sprawl）的问题，导致了重复部署、权责不清、管控不一以及权限滥用等风险。现有的AI治理框架在生命周期风险管理方面存在不足，对智能体集群的日常运营指导有限。", "method": "通过对治理标准、智能体安全文献以及医疗保健合规性要求进行快速、实践导向的综合分析，提出了UALM蓝图。UALM涵盖五个控制层：身份和角色注册、编排和跨域协调、PHI（受保护健康信息）限制的上下文和内存、带有终止开关触发器的运行时策略强制执行，以及与凭证撤销和审计日志相关的生命周期管理和退役。此外，还提供了一个配套的成熟度模型来支持分阶段采用。", "result": "UALM蓝图提供了一个可实施的模式，能够解决智能体扩散带来的治理挑战，并在五个关键控制层面上管理智能体。成熟度模型支持逐步采纳。", "conclusion": "UALM为医疗保健CIO、CISO和临床领导者提供了一种可审计、可实施的模式，以实现对智能体AI的安全规模化部署，同时保留本地创新。该模型有助于医疗保健组织在不断增长的智能体应用中保持安全和合规。"}}
{"id": "2601.15645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15645", "abs": "https://arxiv.org/abs/2601.15645", "authors": ["Zhiyao Ren", "Yibing Zhan", "Siyuan Liang", "Guozheng Ma", "Baosheng Yu", "Dacheng Tao"], "title": "Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation", "comment": null, "summary": "Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.", "AI": {"tldr": "本研究提出了一个评估大型语言模型（LLMs）在多轮医学咨询中置信度的基准，并在此基础上开发了一个名为MedConf的框架。MedConf通过整合证据，能更准确地评估LLMs在医学诊断中的置信度，尤其是在信息不完整的情况下，表现优于现有方法。", "motivation": "现有的大型语言模型在信息不完整的情况下进行临床判断，存在误诊风险。现有的置信度评估方法多基于单轮静态场景，未能捕捉到随着证据累积而变化的置信度与正确性之间的耦合关系，这限制了它们在实际咨询中的可靠性。", "method": "研究者构建了一个新的基准，结合了三种类型的医学数据，用于开放式诊断生成，并引入了信息充分性梯度来刻画证据增加时置信度-正确性动态。在此基准上，他们实现了27种代表性方法，并提出了MedConf框架，该框架通过检索增强生成构建症状画像，将患者信息与支持、缺失、矛盾关系对齐，并通过加权整合形成可解释的置信度估计。", "result": "医学数据放大了现有置信度方法的局限性，并且医学推理需要同时评估诊断准确性和信息完整性。MedConf在两个LLMs和三个医学数据集上，在AUROC和Pearson相关系数指标上均优于最先进的方法，并在信息不充分和多病共存的情况下保持稳定性能。", "conclusion": "信息充分性是建立可靠医学置信度的关键因素。MedConf提供了一种新的途径，用于构建更可靠、更可解释的大型医学模型，克服了现有LLMs在医学诊断中置信度评估的不足。"}}
{"id": "2601.15757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15757", "abs": "https://arxiv.org/abs/2601.15757", "authors": ["Yimin Zhu", "Lincoln Linlin Xu", "Zhengsen Xu", "Zack Dewis", "Mabel Heffring", "Saeid Taleghanidoozdoozan", "Motasem Alkayid", "Quinn Ledingham", "Megan Greenwood"], "title": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification", "comment": null, "summary": "In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.", "AI": {"tldr": "该研究提出了一种名为 ES-mHC 的新的超光谱图像分类（HSIC）框架，通过显式建模不同电磁光谱分组之间的交互作用，提高了模型的透明度和可解释性。", "motivation": "现有的深度学习模型在 HSIC 中依赖于不透明的光谱-空间特征混合，这限制了其可解释性，并阻碍了对其内部决策机制的理解。", "method": "ES-mHC 是一种超连接框架，它使用结构化的、方向性的矩阵来显式地对不同电磁光谱分组（mHC 中的残差流）之间的交互作用进行建模。它将特征表示与交互结构分开，从而实现光谱分组的专业化。", "result": "ES-mHC 学习到的超连接矩阵表现出连贯的空间模式和不对称的交互行为，揭示了模型内部动力学的机制性见解。研究还发现，增加扩展率可以加速结构化交互模式的出现。", "conclusion": "ES-mHC 将 HSIC 从一个纯粹的黑盒预测任务转变为一个结构透明、部分白盒的学习过程，提供了对模型内部信息流动的可视化和空间分析能力。"}}
{"id": "2601.15703", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15703", "abs": "https://arxiv.org/abs/2601.15703", "authors": ["Jiaxin Zhang", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Uncertainty Quantification", "comment": "36 pages, 9 figures, 9 tables", "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.", "AI": {"tldr": "提出了一种名为AUQ的双重过程代理不确定性量化框架，该框架将口头表达的不确定性转化为主动的双向控制信号，通过结合隐式传播置信度和语义解释的UAM（System 1）以及利用这些解释触发目标推理的UAR（System 2），以动态平衡效率和审慎，从而解决了AI代理中“幻觉螺旋”的问题，实现了更可靠的推理。", "motivation": "现有的AI代理在长时推理中存在“幻觉螺旋”的问题，即早期错误会不可逆地传播。现有的不确定性量化（UQ）方法只能诊断风险而无法解决，而自我反思机制则可能导致连续或无目的的修正。为了解决这个难题，需要一种能够将不确定性转化为主动控制信号的框架。", "method": "提出了一种名为AUQ（Dual-Process Agentic UQ）的框架，该框架包含两个机制：System 1（不确定性感知记忆，UAM），它隐式传播口头表达的置信度和语义解释，以防止盲目决策；System 2（不确定性感知反思，UAR），它利用这些解释作为理性线索，仅在必要时触发目标推理。该方法是训练无关的。", "result": "在闭环基准测试和开放式深度研究任务上进行了广泛的实验，结果表明AUQ在性能和轨迹级别的校准方面均优于现有方法。", "conclusion": "AUQ框架通过将口头表达的不确定性转化为主动的双向控制信号，有效地解决了AI代理中的“幻觉螺旋”问题，实现了效率和审慎的动态平衡，是实现可靠代理的重要一步。"}}
{"id": "2601.15739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15739", "abs": "https://arxiv.org/abs/2601.15739", "authors": ["Xinjue Hu", "Chi Wang", "Boyu Wang", "Xiang Zhang", "Zhenshan Tan", "Zhangjie Fu"], "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework", "comment": null, "summary": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.", "AI": {"tldr": "本文提出了一种名为ARDIS的任意分辨率深度图像隐写框架，解决了现有方法在分辨率不匹配时信息丢失的问题，通过频率解耦和基于潜在信息的隐式重构，实现了高保真度的跨分辨率秘密图像恢复，并引入了隐式分辨率编码以支持盲恢复。", "motivation": "现有深度图像隐写方法要求秘密图像与封面图像分辨率一致，这导致分辨率不匹配时存在细节损失，并且无法在分辨率未知的情况下恢复原始分辨率的秘密图像。研究旨在解决这些问题。", "method": "ARDIS框架包含两个主要阶段：1. 隐藏阶段：设计了频率解耦架构，将秘密图像解耦为与分辨率对齐的全局基和与分辨率无关的高频潜在信息，以隐藏在固定分辨率的封面中。2. 恢复阶段：提出了潜在信息引导的隐式重构器，通过将细节潜在代码调制到连续隐式函数，准确查询并渲染高频残差到恢复的全局基上，从而实现精确恢复。此外，引入了隐式分辨率编码策略，将离散分辨率值转化为密集特征图并隐藏，使重构器能够直接从隐写表示中解码秘密图像的分辨率。", "result": "实验结果表明，ARDIS在隐身性和跨分辨率恢复保真度方面显著优于最先进的方法。", "conclusion": "ARDIS是第一个任意分辨率深度图像隐写框架，通过参考引导的连续信号重构，有效解决了分辨率不匹配带来的细节损失问题，并实现了盲恢复，为深度图像隐写领域开辟了新的研究方向。"}}
{"id": "2601.15706", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15706", "abs": "https://arxiv.org/abs/2601.15706", "authors": ["Akriti Vij", "Benjamin Chua", "Darshini Ramiah", "En Qi Ng", "Mahran Morsidi", "Naga Nikshith Gangarapu", "Sharmini Johnson", "Vanessa Wilfred", "Vikneswaran Kumaran", "Wan Sie Lee", "Wenzhuo Yang", "Yongsen Zheng", "Bill Black", "Boming Xia", "Frank Sun", "Hao Zhang", "Qinghua Lu", "Suyu Ma", "Yue Liu", "Chi-kiu Lo", "Fatemeh Azadi", "Isar Nejadgholi", "Sowmya Vajjala", "Agnes Delaborde", "Nicolas Rolin", "Tom Seimandi", "Akiko Murakami", "Haruto Ishi", "Satoshi Sekine", "Takayuki Semitsu", "Tasuku Sasaki", "Angela Kinuthia", "Jean Wangari", "Michael Michie", "Stephanie Kasaon", "Hankyul Baek", "Jaewon Noh", "Kihyuk Nam", "Sang Seo", "Sungpil Shin", "Taewhi Lee", "Yongsu Kim", "Daisy Newbold-Harrop", "Jessica Wang", "Mahmoud Ghanem", "Vy Hong"], "title": "Improving Methodologies for LLM Evaluations Across Global Languages", "comment": "Author names have been organised by country, and in alphabetical order within countries", "summary": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.\n  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.", "AI": {"tldr": "本研究评估了两种开源AI模型在十种语言中的安全性，发现安全行为因语言和伤害类型而异，并提出了改进多语言安全评估的方法。", "motivation": "随着前沿AI模型在全球部署，确保其在不同语言和文化背景下的行为安全可靠至关重要。", "method": "研究人员从多个国家/地区招募参与者，对两种开源模型进行了跨十种语言（包括高资源和低资源语言）的评估。评估涵盖了五个伤害类别（隐私、非暴力犯罪、暴力犯罪、知识产权和越狱鲁棒性），使用了LLM作为裁判和人工标注两种方法，共评估了6000多个新翻译的提示。", "result": "评估结果显示，AI模型的安全行为在不同语言和伤害类型上存在差异。LLM作为裁判和人工评审在评估者可靠性方面也存在差异。此外，研究还获得了改进多语言安全评估的方法学见解，例如需要文化背景下的翻译、经过压力测试的评估者提示和更清晰的人工标注指南。", "conclusion": "这项工作是建立先进AI系统多语言安全测试共享框架的初步尝试，并呼吁与更广泛的研究界和行业进行持续合作。研究强调了在多语言环境中评估AI安全性的挑战以及改进评估方法的必要性。"}}
{"id": "2601.15715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15715", "abs": "https://arxiv.org/abs/2601.15715", "authors": ["Zhitao He", "Zongwei Lyu", "Yi R Fung"], "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind", "comment": "Preprint, under review", "summary": "Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.", "AI": {"tldr": "本文提出了一种名为RebuttalAgent的框架，该框架基于心智理论（ToM）来处理学术反驳的挑战，并通过RebuttalBench数据集和Rebuttal-RM评估器进行训练和评估，实验结果表明其优于现有模型。", "motivation": "现有的AI方法在学术反驳方面表现不佳，因为学术反驳是一个复杂的策略性沟通过程，而非简单的技术辩论，需要视角采择能力，而现有方法主要模仿语言表面特征，忽略了这一关键要素。", "method": "1. 提出RebuttalAgent框架，基于心智理论（ToM），通过ToM-Strategy-Response（TSR）管道建模审稿人心理状态、制定说服策略并生成策略性回应。2. 构建RebuttalBench数据集，采用新颖的“批评-优化”方法合成。3. 训练过程包括两阶段：监督微调（获得ToM分析和策略规划能力），强化学习（通过自我奖励机制进行可扩展的自我改进）。4. 开发Rebuttal-RM评估器，专用于反驳数据的多源评估，与人类偏好评分一致性超过GPT-4.1。", "result": "RebuttalAgent在自动化指标上平均比基线模型提升18.3%，并且在自动化和人类评估中均优于先进的专有模型。", "conclusion": "RebuttalAgent是首个将学术反驳建立在心智理论（ToM）基础上的框架，通过创新的数据集、训练方法和评估器，有效解决了信息不对称下的学术反驳难题，并取得了显著的性能提升。"}}
{"id": "2601.15759", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15759", "abs": "https://arxiv.org/abs/2601.15759", "authors": ["Qi Zeng", "Weide Liu", "Bo Li", "Ryne Didier", "P. Ellen Grant", "Davood Karimi"], "title": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)", "comment": null, "summary": "This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.", "AI": {"tldr": "FeTal-SAM 是一个基于 Segment Anything Model (SAM) 的新模型，专门用于胎儿脑部 MRI 分割。它通过结合图谱和基础模型原理，克服了传统方法需要大量标注数据和模型无法适应标签变化的问题，并能区分基于图像对比度和空间先验的分段。", "motivation": "现有胎儿脑部 MRI 分割方法需要大量标注数据，且难以适应标签定义的变化。同时，这些方法无法区分分段是基于真实的图像对比度还是学习到的空间先验。", "method": "该方法利用多图谱配准生成空间对齐的标签模板作为密集提示，并结合边界框提示，输入到 SAM 的分割解码器中。这使得能够对每个结构进行二值分割，然后融合重建三维分割。模型集成了图谱引导的提示和基础模型原则。", "result": "FeTal-SAM 在 dHCP 和内部数据集上表现出对不同胎龄的稳健性能。对于皮层板和脑小脑等高对比度结构，其 Dice 分数可与最先进的基线方法媲美，同时还能灵活分割用户指定的任何解剖结构。对于海马体、杏仁核等低对比度结构，准确性略有下降。", "conclusion": "FeTal-SAM 是一种灵活且适应性强的胎儿脑部 MRI 分割模型，无需大量重新训练即可分割用户指定的解剖结构，在高对比度结构上性能优越，为临床上可适应的胎儿脑部 MRI 分析工具提供了有前景的解决方案。"}}
{"id": "2601.15709", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15709", "abs": "https://arxiv.org/abs/2601.15709", "authors": ["Asim Biswal", "Chuan Lei", "Xiao Qin", "Aodong Li", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL", "comment": null, "summary": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.", "AI": {"tldr": "提出了一种名为 AgentSM 的 Text-to-SQL 代理框架，通过构建和利用可解释的语义记忆来解决现有方法在处理大型复杂模式、SQL 方言和多步推理方面的不足，从而提高效率和准确性。", "motivation": "现有的 LLM-based Text-to-SQL 方法在处理大型复杂模式、多种 SQL 方言和需要多步推理的真实企业场景时效率低下且不稳定。新兴的代理方法虽然有潜力，但存在效率低下和不稳定（重复数据库交互、输出不一致、有时无法生成有效答案）的问题。", "method": "AgentSM 框架通过构建和利用可解释的语义记忆来解决上述挑战。它捕获先前的执行轨迹（或合成精选的轨迹），并将其结构化为程序，直接指导未来的推理过程。这取代了依赖原始草稿板或向量检索。", "result": "与最先进的系统相比，AgentSM 在 Spider 2.0 基准测试上将平均 token 使用量和轨迹长度分别减少了 25% 和 35%，提高了效率。在 Spider 2.0 Lite 基准测试上，执行准确率达到 44.8%，达到了当前最优水平。", "conclusion": "AgentSM 框架通过系统化地重用推理路径，能够有效地、可靠地扩展到更大的模式、更复杂的问题和更长的推理轨迹，克服了现有 Text-to-SQL 方法的局限性。"}}
{"id": "2601.15745", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15745", "abs": "https://arxiv.org/abs/2601.15745", "authors": ["Ruoqing Zhao", "Runze Xia", "Piji Li"], "title": "Hallucination Mitigating for Medical Report Generation", "comment": null, "summary": "In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \\textbf{K}nowledge-\\textbf{E}nhanced with Fine-Grained \\textbf{R}einforced Rewards \\textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.", "AI": {"tldr": "本文提出了一种名为KERM的框架，通过知识检索、净化模块和细粒度奖励来增强医学报告生成中的大型视觉语言模型，以减少幻觉并提高报告质量。", "motivation": "现有的医学报告生成工具尽管利用了大型视觉语言模型（LVLM）的能力，但容易产生不准确的“幻觉”声明，这在医学领域尤为关键。", "method": "KERM框架首先使用MedCLIP进行知识检索，并从知识库中提取相关的病灶事实句子。然后，引入一个净化模块来确保检索到的知识与患者的临床背景相关。最后，利用细粒度奖励来指导模型生成更具支持性和临床相关性的描述。", "result": "在IU-Xray和MIMIC-CXR数据集上的实验表明，KERM框架能有效减轻幻觉，并提高医学报告的质量。", "conclusion": "KERM框架通过结合知识增强和细粒度奖励，成功解决了LVLM在医学报告生成中的幻觉问题，并提升了报告的准确性和临床相关性。"}}
{"id": "2601.15728", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.15728", "abs": "https://arxiv.org/abs/2601.15728", "authors": ["Hangle Hu", "Chenyu Hou", "Bin Cao", "Ruizhe Li"], "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity", "comment": "8 pages, 7 figures", "summary": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.", "AI": {"tldr": "该研究引入了 BIRD-Python 基准测试，用于评估 Text-to-Python 在数据库交互方面的能力，并提出了逻辑完成框架 (LCF) 来解决因用户意图模糊导致的性能问题。", "motivation": "尽管 Text-to-SQL 是主流，但实际分析工作越来越依赖 Python 等通用编程语言处理文件数据和复杂分析流程，而 Text-to-Python 的可靠性研究相对不足。", "method": "创建和优化了 BIRD-Python 基准测试，并通过逻辑完成框架 (LCF) 结合潜在领域知识来解决用户意图模糊的问题。", "result": "性能差异主要源于缺失领域上下文而非代码生成本身的局限性；当领域信息被充分考虑时，Text-to-Python 可与 Text-to-SQL 达到同等性能。", "conclusion": "Python 可作为分析代理的基础，前提是系统能够有效地将模糊的自然语言输入转化为可执行的逻辑规范。"}}
{"id": "2601.15766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15766", "abs": "https://arxiv.org/abs/2601.15766", "authors": ["Yuhan Chen", "Ying Fang", "Guofa Li", "Wenxuan Yu", "Yicui Shi", "Jingrui Zhang", "Kefei Qian", "Wenbo Chu", "Keqiang Li"], "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps", "comment": null, "summary": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.", "AI": {"tldr": "本文提出了一种名为LL-GaussianMap的无监督低光图像增强框架，首次将2D高斯泼溅（2DGS）引入低光图像增强领域，通过生成增益图来提升图像质量，同时保留结构细节并减少伪影。", "motivation": "现有低光图像增强方法往往忽视图像固有的几何结构信息，作者希望利用2D高斯泼溅（2DGS）在结构拟合和渲染效率方面的优势来改进低光图像增强。", "method": "该框架包含两个主要阶段：1. 利用2DGS进行高保真结构重建；2. 通过高斯泼溅的栅格化机制，利用一个统一的增强模块渲染数据驱动的增强字典系数。整个过程采用无监督学习，生成增益图来完成增强。", "result": "实验结果表明，LL-GaussianMap在低光图像增强方面取得了优越的性能，并且存储占用极低。研究证明了显式高斯表示在图像增强任务中的有效性。", "conclusion": "LL-GaussianMap是首个将2DGS应用于低光图像增强的无监督框架，通过显式结构表示和增益图生成，有效保留了图像的结构细节，减少了伪影，并实现了高效的增强。"}}
{"id": "2601.15772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15772", "abs": "https://arxiv.org/abs/2601.15772", "authors": ["Yuhan Chen", "Wenxuan Yu", "Guofa Li", "Yijun Xu", "Ying Fang", "Yicui Shi", "Long Cao", "Wenbo Chu", "Keqiang Li"], "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting", "comment": null, "summary": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.", "AI": {"tldr": "提出了一种名为LL-GaussianImage的零样本无监督框架，可以直接在2D高斯泼溅（2DGS）压缩域内进行低光图像增强，无需完全解压，从而实现高效且高质量的增强，同时保持高压缩率。", "motivation": "现有的低光增强算法主要在像素域操作，而2DGS压缩图像进行低光增强需要进行解压-增强-重压缩的繁琐流程，效率低且可能引入二次损伤。因此，需要一种直接在2DGS压缩域内进行低光增强的方法。", "method": "提出LL-GaussianImage框架，采用语义引导的混合专家增强模型，利用渲染图像作为指导，对2DGS的稀疏属性空间进行动态自适应变换，实现“压缩即增强”。同时，设计了多目标协同损失函数系统来约束平滑性和保真度，抑制伪影。最后，采用两阶段优化过程（单尺度重建和网络鲁棒性增强）来实现“重建即增强”。", "result": "在保持高压缩率的同时，实现了高质量的低光图像增强。实验结果验证了直接在压缩表示域内进行处理的范式的可行性和优越性。", "conclusion": "LL-GaussianImage是首个在2DGS压缩域内进行低光增强的框架，它通过压缩即增强和重建即增强的范式，有效地解决了现有方法的局限性，实现了高效、高质量的低光增强，并保持了高压缩率。"}}
{"id": "2601.15755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15755", "abs": "https://arxiv.org/abs/2601.15755", "authors": ["Tristan Williams", "Franziska Weeber", "Sebastian Padó", "Alan Akbik"], "title": "Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs", "comment": null, "summary": "Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.", "AI": {"tldr": "该研究提出了一种评估大型语言模型（LLM）对齐其响应以代表人类观点、价值观或信仰的新框架。该框架不仅关注单个响应的分布，还关注多变量相关性模式，以更全面地评估模型对真实人群潜在结构和文化价值观理论的捕捉能力。研究表明，仅关注边际分布的评估方法可能过于乐观，并掩盖了模型在捕捉深层结构方面的失败。", "motivation": "现有关于LLM对齐的研究主要集中在匹配独立调查项的边际响应分布，忽略了可能表征真实人群并支撑文化价值观理论的更深层潜在结构。研究者希望开发一种更全面的评估框架，以捕捉这些深层结构。", "method": "提出一个评估框架，通过多变量相关性模式和边际分布来评估对齐模型的代表性。使用世界价值观调查（World Values Survey）中的人类响应数据，对比了两种模型引导技术：角色扮演提示（persona prompting）和人口统计学微调（demographic fine-tuning）。", "result": "人口统计学微调模型比角色扮演提示模型更能近似边际响应分布。然而，两种技术都未能完全捕捉到作为黄金标准的（人类）相关性模式。这表明代表性与价值对齐是两个不同的方面。", "conclusion": "代表性是价值对齐的一个独立且重要的方面，仅关注边际分布的评估方法可能会掩盖模型在捕捉潜在结构方面的失败，从而导致对模型能力的评估过于乐观。未来的研究需要考虑多变量相关性来更准确地评估模型代表性。"}}
{"id": "2601.15717", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15717", "abs": "https://arxiv.org/abs/2601.15717", "authors": ["Luyao Zhu", "Fangfang Zhang", "Yi Mei", "Mengjie Zhang"], "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling", "comment": null, "summary": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.", "AI": {"tldr": "该研究探讨了遗传编程（GP）演化出的调度规则在动态柔性作业车间调度（DFJSS）问题上的跨类型泛化能力，并分析了问题规模、参数和数据分布等因素的影响。结果表明，当训练实例比测试实例拥有更多作业且机器数量相同时，泛化效果较好。决策点数量和分布是影响泛化能力的关键因素，相似的决策点分布能带来更好的泛化。", "motivation": "现有研究倾向于在相同类型的DFJSS实例上训练和测试GP演化的调度规则，而忽视了其在不同类型实例上的泛化能力，研究旨在填补这一空白。", "method": "通过在一系列不同维度（包括问题规模、关键作业车间参数和数据分布）的DFJSS实例上进行实验，系统地研究GP演化调度规则的泛化能力。分析了这些因素如何影响GP在未见过实例类型上的表现，并深入探究了决策点数量和分布的作用。", "result": "当训练实例包含的作业数多于测试实例且机器数量相同时，能获得良好的泛化效果。训练和测试实例具有相似的规模或作业车间参数时，泛化表现也较好。决策点的数量和分布是解释性能差异的关键，相似的决策点分布带来更好的泛化，而显著的差异会导致性能明显下降。", "conclusion": "该研究为理解GP在DFJSS问题上的泛化能力提供了新的见解，并强调了演化出能够有效处理异构DFJSS实例的、更具泛化能力的GP规则的必要性。"}}
{"id": "2601.15737", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15737", "abs": "https://arxiv.org/abs/2601.15737", "authors": ["Hanning Zhang", "Ruida Wang", "Rui Pan", "Wenyuan Wang", "Bingxu Meng", "Tong Zhang"], "title": "PhysProver: Advancing Automatic Theorem Proving for Physics", "comment": "Preprint", "summary": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.", "AI": {"tldr": "本文提出了 PhysProver，一个在物理领域应用可验证语言和大型语言模型的首个方法，通过构建 PhysLeanData 数据集并使用基于强化学习的方法进行训练，有效提升了物理定理证明的准确性，并对数学证明能力有一定泛化提升。", "motivation": "自然语言处理和可验证语言在数学领域取得了显著进展，但物理领域的正式推理（formal physics reasoning）研究不足，尽管其也依赖于定理证明框架。因此，作者旨在解决这一问题，将正式定理证明能力扩展到物理领域。", "method": "作者构建了一个名为 PhysLeanData 的数据集，其中包含来自 PhysLean 的定理以及通过基于猜想的正式数据生成管道生成的数据。他们利用 DeepSeek-Prover-V2-7B 模型，并应用带有可验证奖励的强化学习（RLVR）来训练 PhysProver 模型。", "result": "仅使用约 5000 个训练样本，PhysProver 在多个物理子领域实现了 2.4% 的整体性能提升。此外，在经过物理领域训练后，模型在 MiniF2F-Test 基准测试上取得了 1.3% 的性能提升，表明其具备跨领域泛化能力和对形式数学能力的增强。", "conclusion": "该研究首次提出了一个在物理领域增强形式定理证明的方法，并证明了其有效性和效率。PhysProver 的成功表明，这种方法可以作为一种范式，将形式证明器扩展到数学领域之外。作者将发布数据集和模型以促进后续研究。"}}
{"id": "2601.15809", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15809", "abs": "https://arxiv.org/abs/2601.15809", "authors": ["Silvia Casola", "Ryan Soh-Eun Shim", "Felicia Körner", "Yuchen Mao", "Barbara Plank"], "title": "SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics", "comment": "Submitted to ACL 2026", "summary": "An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.", "AI": {"tldr": "本文研究了在多语言自然语言生成任务中，现有的评估指标对许多语言缺乏准确性和鲁棒性。研究发现，多语言语言模型常以英语作为内部枢纽语言，并提出可以通过引导多语言神经指标的激活方向到英语枢纽，来提高其与人类判断的相关性。实验结果表明，测试时干预方法在多种语言上都能有效提升指标的有效性。", "motivation": "多语言自然语言生成任务在许多语言上缺乏准确且鲁棒的评估指标，这阻碍了研究的进展。现有研究表明多语言语言模型常以英语作为内部枢纽语言，且与该枢纽的失调会导致性能下降。研究者推测这种失调也可能存在于多语言神经指标中，并希望通过将其激活导向英语枢纽来改善其与人类判断的相关性。", "method": "研究者提出了通过测试时干预方法，将多语言神经指标（包括编码器和解码器模型）的激活导向英语枢纽。他们实验了这种方法，以评估其能否提高指标与人类判断的相关性。", "result": "实验结果表明，测试时干预方法在跨多种语言的指标上都有效，能够增加指标的有效性，使其与人类判断的相关性更高。", "conclusion": "通过将多语言神经指标的激活导向英语枢纽，可以有效提高其在不同语言上的评估性能，从而缓解当前多语言评估指标的瓶颈问题。"}}
{"id": "2601.15820", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15820", "abs": "https://arxiv.org/abs/2601.15820", "authors": ["Guoxuan Ding", "Yuqing Li", "Ziyan Zhou", "Zheng Lin", "Daren Zha", "Jiangnan Li"], "title": "ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection", "comment": "11 pages, 3 figures, 7 tables", "summary": "The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.", "AI": {"tldr": "提出了一种名为ExDR的解释驱动动态检索增强生成框架，用于多模态假新闻检测，通过利用模型生成的解释来改进关键词检索和证据检索，从而提高检测准确性。", "motivation": "现有的动态检索增强生成方法在处理多模态假新闻时面临检索冗余、相似度粗糙和证据不相关等问题。", "method": "ExDR框架利用模型生成的解释来指导检索触发和证据检索。它从三个维度评估触发置信度，融合欺骗实体构建实体感知索引，并检索对比证据来挑战初始声明。", "result": "ExDR在AMG和MR2两个基准数据集上，在检索触发准确性、检索质量和整体检测性能方面均优于现有方法。", "conclusion": "ExDR框架有效解决了多模态假新闻检测中的检索问题，并展现了良好的泛化能力。"}}
{"id": "2601.15751", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15751", "abs": "https://arxiv.org/abs/2601.15751", "authors": ["Xinda Chen", "Xing Zhen", "Hanyu Zhang", "Weimin Tan", "Bo Yan"], "title": "Tabular Incremental Inference", "comment": null, "summary": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.", "AI": {"tldr": "提出了一种名为 Tabular Incremental Inference (TabII) 的新任务，旨在使AI模型在推理阶段能够处理新增列的表格数据，并在此基础上设计了一种基于信息瓶颈理论的方法，该方法通过结合大语言模型占位符、预训练TabAdapter和增量样本压缩块，在多个数据集上取得了先进的性能。", "motivation": "现有的表格AI模型通常在固定列的表格上进行训练和推理，无法有效处理动态变化（新增列）的表格数据，而这种动态性在实际应用中非常普遍。因此，需要开发一种能够无监督地处理动态表格的新方法。", "method": "1. 提出了Tabular Incremental Inference (TabII) 任务，允许模型在推理时整合新列。 2. 将TabII任务建模为信息瓶颈理论的优化问题，目标是最小化表格数据与表示之间的互信息，同时最大化表示与任务标签之间的互信息。 3. 设计了一个TabII方法，结合了：a) 使用大语言模型占位符提供外部知识；b) 预训练的TabAdapter；c) 增量样本压缩块，用于压缩增量列属性提供的任务相关信息。", "result": "在八个公开数据集上的实验表明，TabII方法能够有效地利用增量属性，并实现了最先进的性能。", "conclusion": "TabII任务能够有效地解决表格数据动态变化的问题，并且所提出的方法在实际应用中具有很高的实用性和优越的性能。"}}
{"id": "2601.15810", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15810", "abs": "https://arxiv.org/abs/2601.15810", "authors": ["Mustafa Yurdakul", "Enes Ayan", "Fahrettin Horasan", "Sakir Tasdemir"], "title": "A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks", "comment": null, "summary": "A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.", "AI": {"tldr": "研究开发了一个基于卷积神经网络（CNN）的移动应用程序，用于识别花卉类型。通过比较MobileNet、DenseNet121和Xception三种CNN模型，并使用七种优化算法进行训练，结果表明DenseNet-121结合随机梯度下降（SGD）优化算法在花卉分类任务上表现最佳，准确率达到95.84%。", "motivation": "为了解决非专业人士难以识别花卉类型以及专家知识不易随时随地获取的问题，开发一个便捷的移动应用程序来识别花卉。", "method": "采用三种不同的CNN模型（MobileNet、DenseNet121、Xception）进行花卉识别，并使用七种不同的优化算法进行训练和评估，以确定最适合移动应用程序的模型。", "result": "DenseNet-121模型结合随机梯度下降（SGD）优化算法在花卉分类任务中表现最佳，获得了95.84%的准确率、96.00%的精确率、召回率和F1分数。", "conclusion": "卷积神经网络（CNN）可以成功应用于移动应用程序中的花卉分类任务，其中DenseNet-121模型结合SGD优化算法是目前最优的选择。"}}
{"id": "2601.15828", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15828", "abs": "https://arxiv.org/abs/2601.15828", "authors": ["Michael Farrell"], "title": "Can professional translators identify machine-generated text?", "comment": "10 pages", "summary": "This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.", "AI": {"tldr": "本研究评估了专业译者在未经专业训练的情况下，识别AI生成的意大利语短篇小说的能力。虽然平均而言结果不确定，但仍有一部分译者（16.2%）能够准确识别AI文本，表明他们具备一定的分析能力。", "motivation": "研究旨在探究专业译者在不经过专门训练的情况下，能否可靠地区分出AI生成的意大利语短篇小说，并分析其识别依据。", "method": "通过一项线下实验，让69名专业译者评估三篇匿名短篇小说（两篇由ChatGPT-4o生成，一篇由人类作者创作），并对AI生成可能性进行评分及提供理由。", "result": "平均评分结果不确定，但有16.2%的译者能够显著地区分出AI生成的文本。低文本变化度（burstiness）和叙事矛盾是AI写作的最可靠指标，但语法准确性和情感语气常导致误判。部分译者误将人类文本归类为AI生成，可能反映了对AI文本的偏好。", "conclusion": "专业译者在未经专门训练时，识别AI生成文本的能力参差不齐，部分译者表现出分析能力，但也有相当一部分译者会基于主观印象或错误依据进行判断。研究结果对AI文本在专业领域的编辑工作提出了质疑。"}}
{"id": "2601.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15761", "abs": "https://arxiv.org/abs/2601.15761", "authors": ["Xiefeng Wu", "Mingyu Hu", "Shu Zhang"], "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning", "comment": "7 pages main text 2 page reference", "summary": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.", "AI": {"tldr": "提出了一种名为SigEnt-SAC的离策略强化学习方法，仅需单专家轨迹即可从头开始学习，并能有效缓解Q函数震荡，在D4RL和真实机器人任务中均表现出色，展示了低成本部署真实世界强化学习的潜力。", "motivation": "现有强化学习方法在真实世界部署面临样本效率低、奖励稀疏和视觉观察噪声等挑战。虽然已有方法利用演示和人类反馈，但离线到在线的方法需要大量数据且不稳定，而VLA辅助RL需要大规模预训练和微调。因此，亟需一种数据需求量小、成本低廉的真实世界强化学习方法。", "method": "提出SigEnt-SAC，一种离策略Actor-Critic方法。其核心设计是引入一个sigmoid有界熵项，以防止由负熵驱动的优化导致动作分布偏离，并减少Q函数震荡。该方法可以仅使用一条专家轨迹从零开始学习。", "result": "在D4RL基准测试中，SigEnt-SAC显著缓解了Q函数震荡，并比现有方法更快达到100%成功率。在真实机器人任务中，即使在原始图像和稀疏奖励下，SigEnt-SAC也仅用少量交互即可学习到成功的策略。", "conclusion": "SigEnt-SAC是一种低成本、低数据需求的真实世界强化学习方法，通过引入sigmoid有界熵项有效解决了样本效率和Q函数震荡问题，为实际部署铺平了道路。"}}
{"id": "2601.15813", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15813", "abs": "https://arxiv.org/abs/2601.15813", "authors": ["Clare Chemery", "Hendrik Edelhoff", "Ludwig Bothmann"], "title": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data", "comment": null, "summary": "We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.", "AI": {"tldr": "该研究提出了一种轻量级的实验流程，旨在降低生态学家应用机器学习（ML）进行图像分类的门槛，使他们能够独立地开发定制化模型，并成功应用于红鹿年龄和性别分类，达到了较高的准确率。", "motivation": "当前的现成ML模型难以满足生态学研究中本地数据集和特定分类任务的需求，研究者希望使生态学家能够独立进行ML模型实验，生成定制化洞见。", "method": "开发了一个结合命令行界面（用于预处理、训练和评估）和图形用户界面（用于标注、错误分析和模型比较）的轻量级实验流程。通过该流程，以红鹿相机陷阱图像为例，训练和评估了多种骨干网络架构、参数和数据增强策略。", "result": "在红鹿年龄分类方面，最佳模型准确率达到90.77%；在性别分类方面，最佳模型准确率达到96.15%。", "conclusion": "该框架使生态学家能够无需高级ML专业知识，即可构建和迭代紧凑、任务特定的分类器，证明了即使数据有限，也能通过定制化ML模型解决狭窄的生态学问题，为ML在野生动物监测和人口统计分析中的应用提供了便利工具。"}}
{"id": "2601.15778", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15778", "abs": "https://arxiv.org/abs/2601.15778", "authors": ["Jiaxin Zhang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Confidence Calibration", "comment": "37 pages, 15 figures, 12 tables", "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.", "AI": {"tldr": "本研究提出了“整体轨迹校准”（HTC）框架，用于解决AI代理在执行多步任务时过度自信的问题。HTC能够提取代理执行过程中的丰富特征，从而提高校准和区分能力，并且具有可解释性、可迁移性和泛化性。", "motivation": "现有AI代理在执行复杂多步任务时存在过度自信的固有问题，这阻碍了其在高风险场景中的部署。现有的校准方法无法应对代理系统特有的挑战，如累积误差、外部工具的不确定性以及不透明的故障模式。", "method": "提出了一种名为“整体轨迹校准”（HTC）的新型诊断框架。HTC提取代理整个轨迹过程中的宏观动态到微观稳定性等过程级特征，并使用一个简单的、可解释的模型进行校准。", "result": "HTC在八个基准测试、多个大型语言模型和不同的代理框架上，在校准和区分能力上都持续超越强基线。此外，HTC还能揭示失败背后的信号，无需重新训练即可跨领域应用，并通过通用代理校准器（GAC）在泛化性方面取得了最佳校准效果。", "conclusion": "本研究提出了AI代理置信度校准的新问题，并引入了HTC框架，确立了一种以过程为中心的新范式，为诊断和增强AI代理的可靠性提供了框架。"}}
{"id": "2601.15793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15793", "abs": "https://arxiv.org/abs/2601.15793", "authors": ["Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Zhengyu Hu", "Defu Lian", "Xing Xie"], "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature", "comment": "12 pages, 5 figures, 7 tables, to be published in KDD 2026", "summary": "Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.", "AI": {"tldr": "本文提出了HumanLLM，一个专门用于个性化理解和模拟个体的基础模型，通过在包含用户日志的认知基因组数据集上进行微调，显著提高了预测用户行为、想法和模仿用户风格的能力，并在社交智能任务上展现了更强的泛化性。", "motivation": "现有的大型语言模型在数学和编程等客观任务上取得了巨大进展，但其在模拟人类行为方面的能力有限，这阻碍了其在社会科学研究和客户洞察等领域的应用。这种局限性源于预训练数据缺乏个体决策和行为的连续情境信息。", "method": "作者构建了认知基因组数据集（Cognitive Genome Dataset），从Reddit、Twitter等平台收集并处理了超过550万条用户日志，提炼出用户的个人资料、行为和思维模式。随后，通过监督式微调（supervised fine-tuning）在这些数据上训练了一个名为HumanLLM的基础模型，使其能够预测个体化的用户行为、想法和体验。", "result": "HumanLLM在预测用户行为和内在想法方面表现优于基础模型，更能准确模仿用户的写作风格和偏好，并生成更真实的用户画像。此外，HumanLLM在跨领域社交智能基准测试中也取得了显著的提升，表明其泛化能力增强。", "conclusion": "HumanLLM成功弥合了大型语言模型在理解和模拟个体化人类行为方面的差距，通过利用大规模、情境化的用户数据，能够更准确地捕捉和预测个体的认知和行为模式，为社会科学和商业应用提供了新的可能性。"}}
{"id": "2601.15829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15829", "abs": "https://arxiv.org/abs/2601.15829", "authors": ["Yonghao Xu", "Pedram Ghamisi", "Qihao Weng"], "title": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion", "comment": null, "summary": "Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).", "AI": {"tldr": "本研究首次将数据集蒸馏概念引入遥感图像解释领域，利用文本到图像的扩散模型，通过分类器引导和潜在空间聚类，从大规模遥感数据集中提炼出紧凑且具有代表性的蒸馏数据集，以降低存储和计算成本，并减少数据泄露风险。", "motivation": "深度学习在遥感图像解释中取得了成功，但依赖大规模数据集带来了高昂的存储和计算成本，以及数据泄露（尤其涉及敏感类别）的风险。本研究旨在解决这些挑战。", "method": "1. 训练一个文本到图像的扩散模型来压缩大规模遥感数据集。2. 引入分类器驱动的引导，通过注入预训练模型的分类一致性损失来增强合成样本的判别性。3. 在训练样本上进行潜在空间聚类，选择代表性和多样化的原型作为视觉风格引导。4. 使用视觉语言模型提供聚合的文本描述。", "result": "所提出的方法能够提炼出逼真且多样化的样本，用于下游模型的训练。在三个高分辨率遥感场景分类基准上的实验证明了其有效性。", "conclusion": "本研究成功地将数据集蒸馏应用于遥感图像解释，提出了一种有效的方法来生成紧凑、代表性强且有助于降低成本和数据泄露风险的蒸馏数据集。"}}
{"id": "2601.15846", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15846", "abs": "https://arxiv.org/abs/2601.15846", "authors": ["Jaya Chaturvedi", "Saniya Deshpande", "Chenkai Ma", "Robert Cobb", "Angus Roberts", "Robert Stewart", "Daniel Stahl", "Diana Shamsutdinova"], "title": "Determinants of Training Corpus Size for Clinical Text Classification", "comment": null, "summary": "Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.\n  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.\n  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.", "AI": {"tldr": "该研究通过分析MIMIC-III数据集中的医院出院记录，发现对于临床文本分类任务，大约600份文档足以达到使用10000份文档时95%的性能。研究还指出，拥有更多强预测词和更少噪声词的词汇特性与更陡峭的学习曲线相关。", "motivation": "现有临床文本分类研究中，模型训练所需标注文档数量（通常为200-500篇）受限于时间和成本，但缺乏对样本量需求及其与文本词汇属性之间关系的合理性论证。", "method": "研究使用MIMIC-III数据集，包含带ICD-9诊断标签的医院出院记录。采用预训练BERT嵌入后接随机森林分类器，识别10个随机选择的诊断。训练语料库大小从100到10,000份文档不等。通过Lasso逻辑回归分析词袋模型嵌入，识别强预测词和噪声预测词，以分析词汇属性。", "result": "尽管预处理和算法相同，10个分类任务的学习曲线存在显著差异。对于所有任务，使用600份文档即可达到使用10000份文档时95%的可获得性能。词汇分析表明，强预测词越多、噪声预测词越少，学习曲线越陡峭；每增加100个噪声词，准确率约下降0.02；每增加100个强预测词，最高准确率约提高0.04。", "conclusion": "对于多数临床文本分类任务，600份标注文档已能满足接近最佳性能的需求，这显著低于传统上认为的200-500份。词汇的预测能力（强预测词与噪声词的比例）是影响模型学习效率的关键因素。"}}
{"id": "2601.15780", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15780", "abs": "https://arxiv.org/abs/2601.15780", "authors": ["Pascal Benschop", "Justin Dauwels", "Jan van Gemert"], "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video", "comment": null, "summary": "Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.", "AI": {"tldr": "研究提出了一个合成基准来评估视觉语言模型（VLMs）在情境感知（区分有害与良性互动）和空间感知（跟踪动作、角色和相对位置/运动）方面的能力，发现现有模型表现仅略高于随机猜测，且简单的颜色提示作用有限。", "motivation": "现有视觉语言模型在处理依赖微妙时间或几何线索的语义时表现脆弱，需要一个专门的基准来诊断和提升模型在情境感知和空间感知方面的能力。", "method": "设计了一个合成基准，包含最小的视频对，用于测试区分暴力与良性活动、跨视角绑定攻击者角色以及判断精细轨迹对齐能力。在零样本（训练免费）设置下评估了最新的VLMs，并引入了稳定的颜色提示作为一种辅助手段。", "result": "在所提出的三个任务上，现有VLMs的表现仅略高于随机猜测。稳定的颜色提示在一定程度上减少了攻击者角色的混淆，但并未根本性解决模型在空间推理上的弱点。", "conclusion": "目前的VLMs在处理依赖时间和空间推理的视频理解任务方面仍然存在显著不足。该基准和公开的数据代码旨在促进对更轻量级空间先验的研究，以增强大型预训练模型的性能。"}}
{"id": "2601.15779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15779", "abs": "https://arxiv.org/abs/2601.15779", "authors": ["Liuyun Jiang", "Yanchao Zhang", "Jinyue Guo", "Yizhuo Lu", "Ruining Zhou", "Hua Han"], "title": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation", "comment": null, "summary": "Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.", "AI": {"tldr": "提出了一种基于扩散模型的数据增强框架，用于生成多样的、结构上合理的三维神经元分割图像-标签对，以缓解现有方法对大规模标注数据的依赖，并在低标注条件下显著提高了分割性能。", "motivation": "现有深度学习方法在电子显微镜下进行神经元分割时，需要大量手动标注数据，过程耗时。传统的几何和光度变换数据增强方法生成的样本与原始图像高度相关，缺乏结构多样性。", "method": "提出一个分辨率感知的条件扩散模型，利用多尺度条件和EM分辨率先验，从3D掩码合成体素级图像。此外，还引入一个受生物学启发的掩码重塑模块，生成具有增强结构真实性的增强掩码。", "result": "在AC3和AC4数据集的低标注条件下，结合两种不同的后处理方法，ARAND指标分别提高了32.1%和30.7%。", "conclusion": "所提出的扩散模型数据增强框架能够有效地丰富训练集，并提高神经元分割的性能，尤其是在标注数据有限的情况下。"}}
{"id": "2601.15797", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15797", "abs": "https://arxiv.org/abs/2601.15797", "authors": ["James S. Pearson", "Matthew J. Dennis", "Marc Cheong"], "title": "Creativity in the Age of AI: Rethinking the Role of Intentional Agency", "comment": "27 pages, 2 figures", "summary": "Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.", "AI": {"tldr": "本文认为，将“意图性主体性”作为创造力的普遍必要条件（IAC）是错误的，尤其是在生成式AI的背景下。研究者提出了替代性方案，并解释了在特定领域保留IAC的必要性。", "motivation": "生成式AI的兴起使得将意图性主体性作为创造力必要条件的观点面临挑战，这种传统观点在描述和功能上都存在问题。", "method": "作者首先通过语料库证据分析了作者和记者对生成式AI创造力的接受度。然后，运用概念工程方法，论证了IAC已不再满足其社会功能，并提出了以“一致性要求”（即可靠生成新颖且有价值产物的能力）取而代之。", "result": "语料库证据表明，人们越来越愿意将创造力归因于生成式AI，即使它缺乏意图性主体性。概念工程分析表明，IAC已成为一种偏见，扭曲了对AI生成内容的评估。", "conclusion": "应该放弃将意图性主体性作为创造力的普遍必要条件，并用一致性要求取而代之。然而，在特定领域，意图性主体性仍然是相关的，并且应该被保留。"}}
{"id": "2601.15830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15830", "abs": "https://arxiv.org/abs/2601.15830", "authors": ["Abdul Hasib", "A. S. M. Ahsanul Sarkar Akib"], "title": "An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics", "comment": null, "summary": "The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \\$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.", "AI": {"tldr": "提出了一种基于物联网的智能植物监测系统，该系统使用ESP32微控制器和多种传感器收集实时环境数据，通过ThingSpeak云平台进行远程监控和分析，实现了自动化灌溉，显著提高了土壤湿度控制的准确性，并减少了水资源消耗。", "motivation": "日益增长的全球可持续农业需求，以及传统农业方法在资源利用、植物健康管理和环境适应性方面的不足（如水资源浪费、生长不一致、对环境变化响应迟缓）。", "method": "集成DHT22（温湿度）、HC-SR04（水位）和土壤湿度传感器，使用ESP32微控制器收集数据，并通过OLED显示屏和蜂鸣器提供本地反馈。数据通过无线方式传输到ThingSpeak云平台进行远程监控、历史分析和自动化警报生成。系统还包含自动化灌溉功能。", "result": "实验结果表明，该系统在维持最佳土壤湿度方面准确率达到92%，提供了实时的环境监测，并将水资源消耗减少了约40%。Web仪表板提供了全面的植物健康参数可视化。", "conclusion": "所提出的基于物联网的智能植物监测系统是一种经济实惠（成本45.20美元）、可扩展的解决方案，能够有效地进行精确农业和智能农场管理，适用于小型园艺和商业农业。"}}
{"id": "2601.15869", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15869", "abs": "https://arxiv.org/abs/2601.15869", "authors": ["Francisco Portillo López"], "title": "Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers", "comment": "18 pages, 6 figures", "summary": "This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.", "AI": {"tldr": "AV-HuBERT在麦格克效应（McGurk effect）上的表现与人类观察者进行对比，发现在听觉优势率上高度一致，但在语音融合的确定性偏向上存在显著差异，表明AI能模仿多感官结果但缺乏人类语音感知中的神经变异性。", "motivation": "评估AV-HuBERT模型在感知生物保真度方面的能力，特别是其对不一致视听刺激（麦格克效应）的反应是否与人类观察者相似。", "method": "招募44名人类观察者，让他们感知不一致的视听刺激（麦格克效应），并将其反应（听觉优势率和语音融合率）与AV-HuBERT模型的反应进行量化比较。", "result": "AV-HuBERT和人类观察者在听觉优势率上表现出惊人的一致性（32.0% vs. 31.8%），表明模型捕捉到了听觉抵抗的生物阈值。然而，AV-HuBERT在语音融合方面表现出68.0%的确定性偏向，远高于人类的47.7%。人类观察者表现出感知随机性和多样的错误模式，而模型则表现出严格的分类性。", "conclusion": "当前的自监督架构（如AV-HuBERT）能够模仿多感官处理的结果，但在捕捉人类语音感知中固有的神经变异性方面存在不足。"}}
{"id": "2601.15798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15798", "abs": "https://arxiv.org/abs/2601.15798", "authors": ["Zhikai Xue", "Tianqianjin Lin", "Pengwei Yan", "Ruichun Wang", "Yuxin Liu", "Zhuoren Jiang", "Xiaozhong Liu"], "title": "VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management", "comment": "Accepted by AAAI 2026 Demo", "summary": "Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.", "AI": {"tldr": "本文提出 VitalDiagnosis，一个基于 LLM 的生态系统，通过整合可穿戴设备数据和 LLM 的推理能力，实现慢性病管理的主动化和交互化，旨在提高患者自我管理能力并减轻临床工作负担。", "motivation": "全球慢性病负担加剧，医疗资源紧张，人口老龄化，患者难以识别病情恶化信号或坚持治疗计划。", "method": "整合可穿戴设备数据，利用 LLM 的推理能力，通过上下文感知的查询来分析触发因素，并在医患协作工作流程中产生初步见解，提供个性化指导。", "result": "该系统能够处理急性健康异常和日常依从性问题，促进主动和协作的护理模式。", "conclusion": "VitalDiagnosis 有潜力增强患者的自我管理能力，并减少不必要的临床工作量，实现慢性病管理模式的转变。"}}
{"id": "2601.15838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15838", "abs": "https://arxiv.org/abs/2601.15838", "authors": ["Toan Gian", "Dung T. Tran", "Viet Quoc Pham", "Francesco Restuccia", "Van-Dinh Nguyen"], "title": "TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing", "comment": "10 pages. This paper has been accepted for publication in IEEE PerCom 2026", "summary": "With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.", "AI": {"tldr": "TinySense 是一种基于 VQGAN 的 Wi-Fi CSI 数据压缩框架，用于高效的人体姿态估计，可在不牺牲准确性的前提下显著降低数据量、延迟和网络开销。", "motivation": "现有 Wi-Fi 感知方法直接处理大量 CSI 数据，对网络资源造成压力。研究旨在开发一种更高效、可扩展的解决方案。", "method": "利用 VQGAN 学习的码本对 CSI 数据进行压缩，并通过 K-means 算法优化压缩比特率。同时引入 Transformer 模型来减少比特率损失，提高在不可靠网络条件下的鲁棒性。", "result": "TinySense 在准确性（PCK20）上比现有方法高 1.5 倍（在相同压缩率下），并将延迟和网络开销分别降低了 5 倍和 2.5 倍。", "conclusion": "TinySense 是一种高效且可扩展的 Wi-Fi CSI 压缩框架，能够显著提升 Wi-Fi 感知的人体姿态估计性能，同时降低了对网络资源的需求。"}}
{"id": "2601.15892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15892", "abs": "https://arxiv.org/abs/2601.15892", "authors": ["Chenghao Fan", "Wen Heng", "Bo Li", "Sichen Liu", "Yuxuan Song", "Jing Su", "Xiaoye Qu", "Kai Shen", "Wei Wei"], "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "comment": null, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "AI": {"tldr": "本文提出了一种名为 Stable-DiffCoder 的块扩散代码模型，通过持续预训练和优化的训练策略，在代码生成任务上超越了同等规模的自回归模型，并证明了扩散模型在代码建模中的优势。", "motivation": "现有的基于扩散的语言模型（DLLMs）在代码生成方面仍落后于自回归（AR）模型，作者旨在通过受控研究，探究扩散模型在代码建模上的潜力，并提出改进方法。", "method": "作者在 Seed-Coder 架构、数据和训练管道的基础上，引入了 Stable-DiffCoder 模型。该模型采用块扩散持续预训练（CPT）阶段，并结合了定制的预热和分块裁剪噪声调度策略，以实现高效知识学习和稳定训练。", "result": "在相同的模型架构和数据下，Stable-DiffCoder 在多个代码基准测试中整体性能优于其自回归版本。此外，仅通过 CPT 和监督微调阶段，Stable-DiffCoder 的表现就优于许多规模相近的 AR 和 DLLMs 模型。", "conclusion": "扩散模型训练能够提升代码建模质量，优于单独的 AR 训练。扩散模型的任意顺序建模能力有助于结构化代码的编辑和推理，并通过数据增强方式，对资源匮乏的编程语言也有益处。"}}
{"id": "2601.15909", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15909", "abs": "https://arxiv.org/abs/2601.15909", "authors": ["Soufiane Jhilal", "Stéphanie Martin", "Anne-Lise Giraud"], "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech", "comment": "Accepted at IEEE ISBI 2026", "summary": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.", "AI": {"tldr": "研究提出一种将MEG信号转换为图像表示的方法，并利用预训练的视觉模型来解码想象语音，取得了显著效果。", "motivation": "非侵入式想象语音解码面临信号微弱、分布式以及标签数据有限的挑战。", "method": "将MEG信号转换为时频表示，并利用可学习的传感器空间卷积将其投影成三种空间尺度图的混合物，然后输入到预训练的ImageNet视觉模型中进行解码。", "result": "预训练模型在想象语音任务中表现优于传统模型，实现了高达90.4%的想象语音与静默区分准确率，以及60.6%的元音解码准确率。跨被试评估表明预训练模型能捕捉共享的神经表征，时间分析将区分性信息定位在与想象相关的时段。", "conclusion": "将预训练视觉模型应用于基于图像的MEG表示，可以有效地捕捉非侵入式神经信号中的想象语音结构。"}}
{"id": "2601.15808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15808", "abs": "https://arxiv.org/abs/2601.15808", "authors": ["Yuxuan Wan", "Tianqing Fang", "Zaitang Li", "Yintong Huo", "Wenxuan Wang", "Haitao Mi", "Dong Yu", "Michael R. Lyu"], "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification", "comment": null, "summary": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.", "AI": {"tldr": "本文提出了一种名为 DeepVerifier 的新范式，通过在推理时进行自我验证和迭代改进来增强深度研究代理（DRAs）的能力，而非传统的训练后增强。该方法基于自动构建的 DRA 失败分类法，并取得了显著的性能提升。", "motivation": "现有 DRA 的研究主要集中在训练后增强策略能力，作者旨在探索一种新的代理能力提升范式，即在推理时通过自我验证和迭代反馈进行自我进化。", "method": "本文提出了一种基于精心设计的评分标准的自我进化方法。首先，构建了一个 DRA 失败分类法，将失败分为五大类十三小类。然后，基于此分类法开发了一个名为 DeepVerifier 的评分制结果奖励验证器。DeepVerifier 可以在测试时作为插件模块集成，通过评估代理生成的答案来产生迭代反馈和改进，从而实现自我改进，无需额外训练。", "result": "DeepVerifier 在元评估 F1 分数上，相较于 vanilla agent-as-judge 和 LLM judge 基线，性能提升了 12%-48%。在使用强大的闭源 LLM 驱动时，在 GAIA 和 XBench-DeepResearch 的挑战性子集上，准确率提升了 8%-11%。此外，还发布了一个包含 4,646 个高质量代理步骤的 DeepVerifier-4K 数据集，用于监督微调。", "conclusion": "DeepVerifier 提供了一种有效的、可在推理时进行的可插拔的自我进化机制，能够显著提升 DRA 的性能，尤其是在处理复杂的研究任务时。通过这种方法，代理可以在不进行额外训练的情况下，通过自我反思和批评来迭代地优化其输出。"}}
{"id": "2601.15865", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15865", "abs": "https://arxiv.org/abs/2601.15865", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies", "comment": null, "summary": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.", "AI": {"tldr": "提出了一种轻量级的、受大脑启发的混合神经网络模型，用于冠状动脉造影图像分类，该模型在计算资源有限的情况下表现出鲁棒性和泛化能力。", "motivation": "现实世界的冠状动脉造影图像存在病变形态复杂、类别不平衡、标签不确定以及计算资源有限等问题，对传统的深度学习方法构成了挑战。", "method": "构建了一个基于预训练卷积神经网络的轻量级混合神经网络表示。采用选择性神经可塑性训练策略进行参数自适应。引入了结合Focal Loss和标签平滑的注意力调制损失函数。采用类别不平衡感知采样和具有热重启的余弦退火。", "result": "提出的轻量级模型在二元冠状动脉造影分类中取得了强大而稳定的性能，在准确率、召回率、F1分数和AUC方面均表现出竞争力，同时保持了高计算效率。", "conclusion": "该研究验证了受大脑启发的学习机制在轻量级医学图像分析中的有效性，并提供了一种在计算资源有限的情况下可部署的、具有生物学合理性的智能临床决策支持解决方案。"}}
{"id": "2601.15867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15867", "abs": "https://arxiv.org/abs/2601.15867", "authors": ["Dabiao Ma", "Zhiba Su", "Jian Yang", "Haojun Fei"], "title": "Out-of-Distribution Detection Based on Total Variation Estimation", "comment": null, "summary": "This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.", "AI": {"tldr": "提出了一种名为TV-OOD的新方法，用于检测机器学习模型部署中的分布外数据，通过利用总变差网络估计器计算输入对总变差的贡献来区分分布内和分布外数据。", "motivation": "现有方法在应对分布外数据检测方面存在局限，需要一种更有效的方法来确保机器学习模型在实际应用中的鲁棒性。", "method": "利用总变差网络估计器计算每个输入对总变差的贡献，并将其定义为总变差得分，用于区分分布内和分布外数据。", "result": "在图像分类任务中，TV-OOD在与现有领先的分布外检测技术相比时，在所有评估指标上均表现出相当或更优的结果。", "conclusion": "TV-OOD是一种有效且具有竞争力的分布外检测方法，能够成功地保护机器学习模型免受分布偏移的影响。"}}
{"id": "2601.15812", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15812", "abs": "https://arxiv.org/abs/2601.15812", "authors": ["Shir Ashury-Tahan", "Yifan Mai", "Elron Bandel", "Michal Shmueli-Scheuer", "Leshem Choshen"], "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models", "comment": null, "summary": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.", "AI": {"tldr": "本文提出ErrorMap方法，用于诊断大型语言模型（LLM）在推理任务中出错的原因，并生成ErrorAtlas错误分类体系，以揭示LLM的潜在弱点并指导模型改进。", "motivation": "现有的LLM基准测试只能表明模型何时出错，而无法解释出错的原因，这阻碍了对模型进行有效的调试和改进。需要一种方法来区分错误来源，如格式问题、计算错误或数据集噪声，而不是仅仅归咎于推理能力不足。", "method": "ErrorMap通过提取模型的独特“失败签名”来识别错误来源，澄清基准测试的测量内容，并拓宽错误识别的范围。该方法适用于任何模型和数据集。研究者将此方法应用于35个数据集和83个模型，生成了ErrorAtlas，一个模型错误的分类体系。", "result": "ErrorAtlas揭示了模型常见的失败模式，并突出了当前LLM研究中被忽视的错误类型，如输出中遗漏必要细节和对问题产生误解。研究表明，ErrorMap是一种可扩展的评估方法，能够提供比现有成功指标更深入的模型行为洞察。", "conclusion": "ErrorMap和ErrorAtlas提供了一种新的LLM评估视角，从关注模型成功转向理解模型失败的原因，从而揭示隐藏的弱点并指导模型进步。这种方法可以应用于所有模型和任务，提供比传统任务级指标更丰富的见解，并且该分类体系和代码将公开可用并定期更新。"}}
{"id": "2601.16018", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16018", "abs": "https://arxiv.org/abs/2601.16018", "authors": ["Özgür Uğur", "Mahmut Göksu", "Mahmut Çimen", "Musa Yılmaz", "Esra Şavirdi", "Alp Talha Demir", "Rumeysa Güllüce", "İclal Çetin", "Ömer Can Sağbaş"], "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain", "comment": "16 png, 1 tex, 1 bib", "summary": "This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.", "AI": {"tldr": "该论文提出了 Mecellem 模型框架，通过领域自适应策略为土耳其法律领域开发专门的语言模型。研究包含两个主要贡献：1) 从头开始预训练的编码器模型 (ModernBERT-based)，在土耳其语语料库上进行预训练，并在检索任务上取得优异成绩，且计算效率高；2) 通过持续预训练 (CPT) 和课程学习，在土耳其法律文本上对 Qwen3 模型进行领域适应，显著降低了困惑度。", "motivation": "开发专门针对土耳其法律领域的语言模型，以提高在该特定领域的自然语言处理性能，并探索更高效的模型训练策略。", "method": "1) 编码器模型：从头开始在包含 1127 亿 token 的土耳其语为主的语料库上预训练 ModernBERT-based 双向编码器，并采用在训练过程中评估下游检索性能的检查点选择策略。2) 解码器模型：对 Qwen3-1.7B 和 Qwen3-4B 模型进行持续预训练 (CPT)，采用四阶段的课程学习，以逐步适应土耳其法律领域。", "result": "1) 编码器模型在土耳其检索排行榜上排名前三，小型模型（155M 参数）性能可与大型模型（307M-567M 参数）媲美，生产效率达到 92.36%，排名第四。2) 解码器模型通过 CPT 将土耳其法律文本的困惑度降低了 36.2%。", "conclusion": "Mecellem 模型框架通过从头预训练编码器模型和对解码器模型进行领域适应性 CPT，成功为土耳其法律领域开发了高性能且计算效率高的语言模型。该方法提供了一种成本效益高的替代方案，优于依赖复杂多阶段训练的现有先进模型。"}}
{"id": "2601.15888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15888", "abs": "https://arxiv.org/abs/2601.15888", "authors": ["Shiqi Huang", "Yipei Wang", "Natasha Thorley", "Alexander Ng", "Shaheer Saeed", "Mark Emberton", "Shonit Punwani", "Veeru Kasivisvanathan", "Dean Barratt", "Daniel Alexander", "Yipeng Hu"], "title": "Understanding the Transfer Limits of Vision Foundation Models", "comment": "accepted in ISBI 2026", "summary": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.", "AI": {"tldr": "本研究评估了两种视觉基础模型（VFMs）在前列腺多参数MRI成像任务中的表现，发现预训练目标与下游任务的匹配程度影响着模型的迁移性能。", "motivation": "现有的视觉基础模型（VFMs）在下游任务上的表现提升不均衡，这可能是由于预训练目标与下游任务需求不匹配所致。", "method": "研究评估了两种VFMs：一种基于重构（MAE），一种基于对比学习，在前列腺多参数MRI成像的五个任务上进行评估。使用最大均值差异（MMD）等指标衡量预训练与下游任务的对齐程度，并分析其对迁移性能和收敛速度的影响。", "result": "研究发现，预训练目标与下游任务的对齐程度越高，模型在迁移性能上的提升越明显，并且收敛速度越快。", "conclusion": "设计和分析预训练目标时，应考虑其在下游任务中的适用性，以提高视觉基础模型的泛化能力和效率。"}}
{"id": "2601.15931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15931", "abs": "https://arxiv.org/abs/2601.15931", "authors": ["Xiangyu Wang", "Zhixin Lv", "Yongjiao Sun", "Anrui Han", "Ye Yuan", "Hangxu Ji"], "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search", "comment": null, "summary": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.", "AI": {"tldr": "本文提出了一种名为ICON（Invariant Counterfactual Optimization with Neuro-symbolic priors）的框架，通过整合因果和拓扑先验来解决文本导向的人物搜索（TBPS）中模型泛化能力不足的问题，提升了其在复杂开放场景下的鲁棒性。", "motivation": "现有基于预训练模型的TBPS方法在复杂开放世界场景下泛化能力不足，容易受到 spurious correlations 和 spatial semantic misalignment 的影响，导致在分布变化时鲁棒性差。", "method": "ICON框架整合了因果和拓扑先验，具体方法包括：1. Rule-Guided Spatial Intervention：惩罚对边界框噪声的敏感性，实现几何不变性。2. Counterfactual Context Disentanglement：通过背景移植解决环境干扰。3. Saliency-Driven Semantic Regularization：解决局部显著性偏差。4. Neuro-Symbolic Topological Alignment：利用神经符号先验约束特征匹配，保证激活区域在拓扑上与人类结构逻辑一致。", "result": "ICON在标准基准测试上保持领先性能，并对遮挡、背景干扰和定位噪声表现出优异的鲁棒性。", "conclusion": "ICON框架有效地推动了TBPS领域的发展，从拟合统计共现转向学习因果不变性，显著提升了模型在真实世界复杂场景下的鲁棒性和泛化能力。"}}
{"id": "2601.16097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16097", "abs": "https://arxiv.org/abs/2601.16097", "authors": ["Makbule Gulcin Ozsoy"], "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating", "comment": null, "summary": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.", "AI": {"tldr": "本研究提出了一种可扩展的多语言 Text2Cypher 方法，通过语言特定的 LoRA 适配器和融合 MLP 来支持新语言，而无需重新进行完全微调，并能接近联合多语言微调的性能。", "motivation": "现有 Text2SQL 等工具主要支持英语，多语言支持有限。研究旨在开发一种无需完全重新微调、避免手动调参且性能接近联合微调的多语言 Text2Cypher 系统。", "method": "训练了针对英语、西班牙语和土耳其语的语言特定 LoRA 适配器，并通过统一线性合并或学习到的具有动态门控的融合 MLP 将它们组合起来。", "result": "实验结果表明，融合 MLP 在大约 75% 的联合多语言微调的准确性提升下，仅需更小的数据子集，并且在所有三种语言上都优于线性合并。", "conclusion": "学习到的适配器融合为昂贵的联合微调提供了一个实用的替代方案，在性能、数据效率和可扩展性之间取得了平衡，使得多语言 Text2Cypher 任务能够进行增量式语言扩展。"}}
{"id": "2601.15949", "categories": ["cs.AI", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.15949", "abs": "https://arxiv.org/abs/2601.15949", "authors": ["Yiran Wang", "Shuoyuan Wang", "Zhaoran Wei", "Jiannan Zhao", "Zhonghua Yao", "Zejian Xie", "Songxin Zhang", "Jun Huang", "Bingyi Jing", "Hongxin Wei"], "title": "Natural Language-Driven Global Mapping of Martian Landforms", "comment": null, "summary": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.", "AI": {"tldr": "MarScope 是一个行星规模的视觉-语言框架，使用自然语言驱动火星地貌的无标签测绘，实现了跨越整个火星的灵活语义检索。", "motivation": "现有行星表面分析方法与海量像素级轨道图像数据的组织方式存在不匹配，阻碍了对行星表面的大规模、开放式探索。", "method": "MarScope 通过对超过 200,000 对图像-文本进行训练，将行星图像和文本对齐到一个共享的语义空间，实现了自然语言驱动的、无标签的地貌测绘。", "result": "MarScope 能够以高达 0.978 的 F1 分数在 5 秒内完成任意用户查询，能够进行任意用户查询，并且还可以进行面向过程的分析和基于相似性的地貌测绘。", "conclusion": "MarScope 改变了火星全球地貌测绘的范式，通过将预定义的分类替换为灵活的语义检索，并为海量地理空间数据集的科学发现提供了一个新的自然语言接口。"}}
{"id": "2601.16113", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16113", "abs": "https://arxiv.org/abs/2601.16113", "authors": ["Haq Nawaz Malik", "Kh Mohmad Shafi", "Tanveer Ahmad Reshi"], "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier", "comment": null, "summary": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\n  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\n  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.", "AI": {"tldr": "本文介绍了一种名为SynthOCR-Gen的开源工具，用于为低资源语言生成合成OCR数据集，以解决数据稀缺的挑战，并发布了一个包含60万个样本的克什米尔语OCR数据集。", "motivation": "低资源语言（如克什米尔语）缺乏大规模标注的OCR训练数据集，导致其在现有OCR系统中得不到支持，而手动创建数据集成本高昂且易出错。", "method": "SynthOCR-Gen工具通过一个包含文本分割、Unicode归一化、多字体渲染和25种以上数据增强技术的流水线，将数字Unicode文本语料库转换为可用的OCR训练数据集。", "result": "成功生成了一个包含60万个样本、按单词分割的克什米尔语OCR数据集，并已公开。", "conclusion": "SynthOCR-Gen为低资源语言的OCR开发提供了一个实用的解决方案，有助于将这些语言纳入视觉语言AI模型，并且该工具可供全球研究人员使用。"}}
{"id": "2601.16034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16034", "abs": "https://arxiv.org/abs/2601.16034", "authors": ["Tony Cristofano"], "title": "Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction", "comment": null, "summary": "Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.", "AI": {"tldr": "研究表明，大型语言模型（LLM）的拒绝行为可能源于跨模型共享的低维语义通路，而非模型特有。通过一种名为“通过概念基重建的轨迹重放”的框架，可以将一个模型的拒绝干预转移到另一个模型，即使它们架构不同，并且目标模型没有拒绝行为的监督信息。该方法通过概念指纹对齐层，并利用共享的概念原子“配方”重建拒绝方向，从而将供体模型的干预轨迹映射到目标模型的语义空间。为了保护模型能力，研究还引入了权重SVD稳定性保护机制。实验结果表明，这种转移的干预配方能够有效减少拒绝行为，同时保持模型性能，有力地证明了安全对齐的语义普遍性。", "motivation": "研究人员观察到LLM的拒绝行为通常被认为是模型特定的，但他们假设这种行为可能源于一种跨模型共享的、低维度的语义通路。为了验证这一假设，需要开发一种方法来转移对拒绝行为的干预，并证明其跨模型、跨架构的普遍性。", "method": "提出了一种名为“通过概念基重建的轨迹重放”（Trajectory Replay via Concept-Basis Reconstruction）的框架。该框架通过以下步骤实现拒绝干预的转移：1. 通过“概念指纹”（concept fingerprints）对齐供体模型和目标模型中的层。2. 利用共享的“概念原子”（concept atoms）“配方”重建拒绝方向，将供体模型的干预轨迹映射到目标模型的语义空间。3. 引入“权重SVD稳定性保护”（weight-SVD stability guard）机制，将干预投影到远离高方差权重子空间的方向，以防止损害模型原有能力。", "result": "在8对不同模型（包括GPT-OSS-20B和GLM-4）的实验中，该框架成功地将拒绝干预从供体模型转移到了目标模型。转移的“配方”能够持续有效地削弱目标模型的拒绝行为，并且在转移过程中保持了模型原有的性能。这为安全对齐的语义普遍性提供了强有力的证据。", "conclusion": "大型语言模型的拒绝行为并非模型特有，而是源于一种跨模型共享的、低维度的语义通路。通过“通过概念基重建的轨迹重放”框架，可以将已知的安全对齐干预（如拒绝行为的干预）从一个模型转移到另一个模型，即使模型架构和训练方式不同，且目标模型没有相关监督信息。这种转移方式有效且能保留模型性能，证明了安全对齐在语义层面的普遍性。"}}
{"id": "2601.15884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15884", "abs": "https://arxiv.org/abs/2601.15884", "authors": ["Yifan Chen", "Fei Yin", "Hao Chen", "Jia Wu", "Chao Li"], "title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis", "comment": null, "summary": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.", "AI": {"tldr": "该研究发布了一个名为 PMPBench 的首个公开、全配对、泛癌种医学影像数据集，涵盖11种人体器官，包含动态增强（DCE）MRI和增强CT（CTC）数据，旨在推动AI在无需造影剂的医学影像合成方面的研究。", "motivation": "现有AI医学影像翻译研究受限于数据不足：公开数据集主要集中于脑部，其他数据集配对不完整、模态缺失、时间戳不准或空间对齐不佳，且缺乏清晰的造影剂类型标注，大量数据仍为私有。这限制了AI在泛癌种、多器官影像合成领域的进展。", "method": "构建了一个包含11种人体器官的、完全配对的泛癌种医学影像数据集PMPBench。MRI数据包含完整的动态增强（DCE）序列（DCE1-DCE3），CT数据包含配对的非增强和增强（CTC）扫描。数据集经过解剖学对应性处理，支持1-to-1, N-to-1, N-to-N等翻译场景评估。并基于此数据集建立了基准测试，报告了代表性图像到图像翻译方法的性能。", "result": "发布了PMPBench数据集和相应的基准测试结果，展示了现有图像到图像翻译方法的性能表现。研究者发布了代码和数据集以促进相关研究。", "conclusion": "PMPBench数据集的发布将为泛癌种、多器官影像合成研究提供重要资源，有望推动AI在减少造影剂使用、优化临床工作流程方面的应用，对多器官肿瘤学影像工作流程具有直接意义。"}}
{"id": "2601.15953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15953", "abs": "https://arxiv.org/abs/2601.15953", "authors": ["Yongyi Wang", "Hanyu Liu", "Lingfeng Li", "Bozhou Chen", "Ang Li", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "title": "Decoupling Return-to-Go for Efficient Decision Transformer", "comment": null, "summary": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.", "AI": {"tldr": "本文提出了Decoupled Decision Transformer (DDT)，一种简化的Decision Transformer (DT)变体，通过仅在Transformer中处理观察和动作序列，并使用最新的RTG来指导动作预测，从而提高了DT的性能和效率。", "motivation": "DT在离线强化学习中表现出色，但其设计中存在冗余：Transformer接收整个RTG序列，而实际上只有最新的RTG会影响动作预测。这种冗余可能会损害DT的性能。", "method": "本文提出了Decoupled DT (DDT)，其核心思想是简化DT架构，仅将观测值和动作序列输入Transformer，并使用最新的Return-to-Go (RTG)来指导动作预测。", "result": "实验表明，DDT显著优于DT，并在多个离线强化学习任务上取得了与最先进DT变体相当的性能，同时降低了计算成本。", "conclusion": "DDT通过消除RTG序列的冗余输入，简化了DT架构，提高了其性能和计算效率，并在离线强化学习领域展现出竞争力。"}}
{"id": "2601.15891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15891", "abs": "https://arxiv.org/abs/2601.15891", "authors": ["Anas Anwarul Haq Khan", "Mariam Husain", "Kshitij Jadhav"], "title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture", "comment": null, "summary": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.", "AI": {"tldr": "本文提出了一种名为RadJEPA的自监督学习框架，该框架不依赖于语言监督，仅通过对无标签的胸部X光片进行预测任务来学习视觉表示，并在多种医学影像任务上取得了优于现有方法的性能。", "motivation": "现有的医学视觉语言模型依赖于图像-文本配对数据，但此类数据的可用性受限。因此，研究者希望探索是否可以在不依赖语言监督的情况下学习到鲁棒的医学影像编码器。", "method": "该研究提出了一种基于联合嵌入预测架构（Joint Embedding Predictive Architecture）的自监督学习框架RadJEPA。模型在无标签的胸部X光片上进行预训练，学习预测被遮蔽图像区域的潜在表示。这种预测目标不同于图像-文本预训练和DINO风格的自蒸馏，而是显式地对潜在空间的预测进行建模。", "result": "在疾病分类、语义分割和报告生成等任务上，RadJEPA学习到的编码器表现优于包括Rad-DINO在内的现有最先进方法。", "conclusion": "RadJEPA是一种有效的、无需语言监督的自监督学习方法，可以学习到强大的医学影像视觉表示，并在多种下游任务中展现出卓越的性能。"}}
{"id": "2601.16127", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16127", "abs": "https://arxiv.org/abs/2601.16127", "authors": ["Alphaeus Dmonte", "Vidhi Gupta", "Daniel J Perry", "Mark Arehart"], "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging", "comment": null, "summary": "Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.", "AI": {"tldr": "研究表明，通过合并多语言多任务模型可以有效提高计算和维护效率，初始训练时间最多可缩短 50%，单语言更新成本降低 60% 以上，同时保持模型质量。", "motivation": "现有的多语言大语言模型（LLM）在更新支持的语言时需要重新训练，效率低下且维护困难。现有关于合并多语言多任务模型的研究虽然在质量上有提升，但其计算和维护效率尚未得到深入研究。", "method": "作者通过在三个独立的任务上评估合并多语言多任务模型的策略，从效率角度进行了分析。同时，他们还针对数据更新和新语言添加的场景，对比了合并策略与完全重新训练的效率。", "result": "合并策略显著提高了效率，初始训练时间最多可减少 50%。在模型维护方面，更新单个语言并重新合并模型比重新训练整个多语言模型可节省超过 60% 的训练成本。研究结果在公开数据集和专有行业数据集上均得到验证。", "conclusion": "合并多语言多任务模型是一种高效的策略，可以在不牺牲模型质量的前提下，大幅降低多语言LLM的训练和维护成本，适用于工业和学术界。"}}
{"id": "2601.15897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15897", "abs": "https://arxiv.org/abs/2601.15897", "authors": ["Zhaoqi Su", "Shihai Chen", "Xinyan Lin", "Liqin Huang", "Zhipeng Su", "Xiaoqiang Lu"], "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling", "comment": null, "summary": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.", "AI": {"tldr": "本文提出了一种名为ThermoSplat的新框架，用于整合RGB和热红外数据进行多模态场景重建，克服了现有方法在利用多模态互补信息方面的不足，实现了更优的重建质量。", "motivation": "当前多模态场景重建方法在整合RGB和热红外数据时，难以充分利用数据的互补信息，现有机制要么忽略跨模态关联，要么使用无法自适应处理光谱间复杂结构关联和物理差异的共享表示。", "method": "ThermoSplat框架通过主动特征调制和自适应几何解耦实现深度光谱感知重建。具体包括：1. 跨模态FiLM调制机制，动态地将共享潜在特征与热结构先验相结合，用可靠的跨模态几何线索指导可见纹理合成。2. 模态自适应几何解耦，学习独立的不透明度偏移，并为热通道执行独立的栅格化。3. 混合渲染管线，结合球谐函数和隐式神经解码，保持语义一致性和高频细节。", "result": "在RGBT-Scenes数据集上的大量实验表明，ThermoSplat在可见光和热红外光谱上都达到了最先进的渲染质量。", "conclusion": "ThermoSplat是一种有效的多模态场景重建框架，能够通过跨模态特征调制和自适应几何解耦，实现高质量的RGB和热红外数据融合重建。"}}
{"id": "2601.16027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16027", "abs": "https://arxiv.org/abs/2601.16027", "authors": ["Yiran Qiao", "Xiang Ao", "Jing Chen", "Yang Liu", "Qiwei Zhong", "Qing He"], "title": "Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment", "comment": null, "summary": "The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.", "AI": {"tldr": "本文提出了一种名为CS-VAR的实时流风险评估方法，利用大型语言模型（LLM）指导轻量级模型进行跨会话行为分析，以检测复杂的、逐渐累积的在线风险。实验证明该方法性能优越且具有可解释性。", "motivation": "实时流媒体平台面临着规模化实时互动带来的复杂风险，如诈骗和协同恶意行为，这些风险通常会逐渐累积并跨越看似不相关的流媒体。现有方法难以有效检测这些跨会话的风险模式。", "method": "提出CS-VAR（Cross-Session Evidence-Aware Retrieval-Augmented Detector）。该方法使用一个轻量级、领域特定的模型进行快速的会话级风险推理。在训练过程中，一个大型语言模型（LLM）会检索跨会话的行为证据，并将其从局部到全局的洞察力转移到该小型模型，从而指导其训练。这种设计使小型模型能够识别跨流媒体的重复模式，并进行结构化的风险评估。", "result": "在大量工业数据集上进行了广泛的离线实验，并通过在线验证。结果表明，CS-VAR在实时流风险评估方面达到了最先进的性能。", "conclusion": "CS-VAR能够有效地识别实时流中的跨会话风险模式，并且由于其可解释的、局部的信号，能够有效地赋能现实世界中的直播审核工作，同时保持了部署效率。"}}
{"id": "2601.16138", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16138", "abs": "https://arxiv.org/abs/2601.16138", "authors": ["Zainab Alhathloul", "Irfan Ahmad"], "title": "Automatic Classification of Arabic Literature into Historical Eras", "comment": "27 pages", "summary": "The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.", "AI": {"tldr": "本研究使用神经网络和深度学习技术，首次对阿拉伯语文本按历史时期进行自动分类，填补了现有研究的空白，特别是在诗歌以外的领域。研究人员在两个公开语料库上进行了不同规模的分类实验，取得了较好的二分类结果，但多分类结果仍有提升空间。", "motivation": "尽管阿拉伯语历史悠久且不断演变，但现有研究对阿拉伯文本按时间时期进行自动分类的探索相对较少，尤其是在诗歌之外的领域。本研究旨在解决这一研究空白。", "method": "本文采用神经网络和深度学习技术，构建模型对阿拉伯语文本进行自动分类。研究人员使用了两个公开数据集（OpenITI 和 APCD），涵盖了从伊斯兰前到现代的文本。实验设置包括二分类到 15 分类，以及预定义的历史时期和自定义的时期划分。", "result": "在二分类任务中，使用 OpenITI 数据集取得了 0.83 的 F1 分数，使用 APCD 数据集取得了 0.79 的 F1 分数。然而，在更细粒度的分类任务中，使用 OpenITI 的 15 分类任务 F1 分数为 0.20，使用 APCD 的 12 分类任务 F1 分数为 0.18。", "conclusion": "本研究成功地展示了使用深度学习技术自动分类阿拉伯语文本到不同历史时期是可行的，并且在二分类任务上取得了较好的效果。然而，在更精细的时期划分上，模型的性能仍有待提高。"}}
{"id": "2601.15906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15906", "abs": "https://arxiv.org/abs/2601.15906", "authors": ["Zhen Zhang", "Runhao Zeng", "Sicheng Zhao", "Xiping Hu"], "title": "Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models", "comment": null, "summary": "Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.", "AI": {"tldr": "本文研究了多模态基础模型中情感表示的机制，发现情感适应主要集中在前馈门控投影（gate_proj）模块，而非注意力模块。通过实验证明gate_proj对于情感理解和生成是充分、高效且必要的，并且可以在参数效率方面取得优异成果。", "motivation": "理解大型多模态基础模型（尤其是在情感领域）的内部工作机制，即情感表示的位置和方式，仍然是一个悬而未决的问题。尽管现有情感模型表现出色，但其内部架构如何支持情感理解和生成却鲜为人知。", "method": "研究者对多模态基础模型进行了系统的机制研究，考察了不同架构、训练策略和情感任务。他们分析了情感导向的监督如何重塑模型参数，并通过模块迁移、单模块适应和消融实验来验证gate_proj的作用。", "result": "研究一致发现，情感适应主要集中在前馈门控投影（gate_proj）模块，而不是注意力模块。通过实验证明gate_proj在情感理解和生成方面是充分、高效且必要的。在参数效率方面，仅调整约 24.5% 的参数即可达到AffectGPT平均性能的 96.6%。", "conclusion": "情感能力在基础模型中是通过前馈门控机制在结构上介导的，gate_proj是情感建模的核心架构位置。这项研究为理解和改进基础模型中的情感处理提供了重要的机制见解。"}}
{"id": "2601.16038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16038", "abs": "https://arxiv.org/abs/2601.16038", "authors": ["Olga Bunkova", "Lorenzo Di Fruscia", "Sophia Rupprecht", "Artur M. Schweidtmann", "Marcel J. T. Reinders", "Jana M. Weber"], "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval", "comment": "Accepted at ML4Molecules 2025 (ELLIS UnConference workshop), Copenhagen, Denmark, December 2, 2025. Workshop page: https://moleculediscovery.github.io/workshop2025/", "summary": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.", "AI": {"tldr": "本研究提出了一种利用大语言模型（LLMs）与反应知识图谱（KG）交互来进行化学合成路线检索的方法，通过将路线检索转化为 Text2Cypher（自然语言到图查询）问题，并评估了不同提示策略（零样本、单样本）和纠错机制的效果，发现带示例的单样本提示效果最佳。", "motivation": "标准的大语言模型提示方法在化学合成规划中存在幻觉和过时建议的问题，需要一种更可靠的方法来利用 LLMs。", "method": "将反应路线检索视为 Text2Cypher 生成问题，定义了单步和多步检索任务。比较了零样本提示与使用静态、随机和基于嵌入的示例选择的单样本提示，并评估了一个清单驱动的验证/纠正循环。使用查询有效性和检索准确性作为评估指标。", "result": "带对齐示例的单样本提示在查询有效性和检索准确性上表现最好。清单式的自我纠正循环在零样本设置下主要提高了可执行性，但在有良好示例的情况下，检索增益有限。", "conclusion": "与反应知识图谱结合的 LLMs 是化学合成规划的有效工具，其中带示例的 Text2Cypher 方法可以显著提高检索性能。研究还提供了一个可复现的评估框架，以促进该领域的研究。"}}
{"id": "2601.16206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16206", "abs": "https://arxiv.org/abs/2601.16206", "authors": ["Daixuan Cheng", "Shaohan Huang", "Yuxian Gu", "Huatong Song", "Guoxin Chen", "Li Dong", "Wayne Xin Zhao", "Ji-Rong Wen", "Furu Wei"], "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "comment": "Project Page: https://llm-in-sandbox.github.io", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "AI": {"tldr": "本文提出了一种名为 LLM-in-Sandbox 的方法，允许大型语言模型（LLM）在代码沙箱（虚拟计算机）中进行探索，以在非代码领域激发通用智能。研究表明，即使未经额外训练，强大的 LLM 也能利用代码沙箱处理非代码任务，例如访问外部资源、管理文件系统和执行脚本。此外，通过 LLM-in-Sandbox-RL 强化学习，可以使用非代理数据进一步增强这些代理能力。实验证明，LLM-in-Sandbox 在数学、物理、化学、生物医学、长上下文理解和指令遵循等领域都展现出了强大的泛化能力。最后，本文还分析了该方法的效率，并开源了相关的 Python 包。", "motivation": "现有的大型语言模型在通用智能方面存在局限性，特别是在处理需要与外部环境交互或进行复杂逻辑推理的非代码任务时。研究者希望探索一种新的方法，能够激发 LLM 在这些领域的通用智能。", "method": "本文提出 LLM-in-Sandbox 方法，让 LLM 在代码沙箱（虚拟计算机）中进行交互式探索。该方法包含两个主要部分：1. **训练前（Zero-shot）能力探索**：直接使用现有的强大 LLM，观察其在沙箱环境中的自主探索和利用代码能力解决非代码任务的能力。2. **LLM-in-Sandbox-RL 强化学习**：利用非代理数据训练模型，以增强其在沙箱环境中的探索和解决问题的能力。", "result": "1. **零样本能力**：强大的 LLM 在未经额外训练的情况下，能够自主利用代码沙箱进行通用智能探索，包括访问外部资源获取知识、使用文件系统处理长上下文、执行脚本满足格式要求等。2. **强化学习效果**：LLM-in-Sandbox-RL 能够有效提升 LLM 的代理能力，即使只使用非代理数据进行训练。3. **泛化能力**：经过训练前和强化学习后，LLM-in-Sandbox 在数学、物理、化学、生物医学、长上下文理解和指令遵循等多个领域都表现出稳健的泛化能力。4. **效率分析**：对 LLM-in-Sandbox 的计算和系统效率进行了分析。", "conclusion": "LLM-in-Sandbox 是一种有效的方法，能够激发大型语言模型在非代码领域的通用智能。通过在代码沙箱中进行探索，LLM 能够展现出自主学习和解决复杂问题的能力，并且这种能力可以通过强化学习进一步增强。该方法在多个领域都取得了良好的泛化效果，并且已经开源以促进实际应用。"}}
{"id": "2601.15914", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.15914", "abs": "https://arxiv.org/abs/2601.15914", "authors": ["Yarin Benyamin"], "title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars", "comment": "Technical Report benchmarking off-the-shelf CV latencies on commodity CPU hardware for therapeutic VR applications", "summary": "In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.", "AI": {"tldr": "本研究在虚拟现实 (VR) 中评估了多种深度学习模型在识别虚拟角色面部表情方面的实时性能，旨在为自闭症谱系障碍 (ASD) 患者提供社交技能训练。研究发现，在 CPU 上运行时，虽然面部检测准确率很高，但在表情分类阶段存在“延迟墙”。YOLOv11n 在检测速度和准确性上取得了最佳平衡，但像 CLIP 和 SigLIP 这样的通用 Transformer 模型在实时应用中表现不佳。", "motivation": "为了让自闭症谱系障碍 (ASD) 患者能够通过虚拟现实 (VR) 进行社交技能训练，需要实现低延迟、高精度的实时情绪识别。然而，现有的深度学习模型往往牺牲了实时性。", "method": "研究人员在 UIBVFED 数据集上，针对虚拟角色，对几种最先进的零样本面部表情识别 (FER) 模型进行了基准测试。具体包括：评估 YOLO（v8、v11 和 v12）的 Medium 和 Nano 变体用于面部检测；评估 CLIP、SigLIP 和 ViT-FER 等通用 Vision Transformer 模型用于表情分类。所有测试均在 CPU 上进行。", "result": "在 CPU 推理上，模型对面部检测的准确率达到 100%。然而，在分类阶段存在“延迟墙”。YOLOv11n 架构在检测阶段提供了最佳的延迟-准确性平衡（约 54 毫秒）。而像 CLIP 和 SigLIP 这样的通用 Transformer 模型，在实时应用中无法达到可接受的准确率（<23%）或速度（>150 毫秒）。", "conclusion": "为了在治疗环境中实现可访问的实时人工智能，必须开发轻量级、领域特定的模型架构。目前的通用 Transformer 模型在实时 VR 应用中表现不佳，需要更优化的模型来满足严格的延迟和准确性要求。"}}
{"id": "2601.16045", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16045", "abs": "https://arxiv.org/abs/2601.16045", "authors": ["Yue Shi", "Liangxiu Han", "Xin Zhang", "Tam Sobeih", "Thomas Gaiser", "Nguyen Huu Thuy", "Dominik Behrend", "Amit Kumar Srivastava", "Krishnagopal Halder", "Frank Ewert"], "title": "AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress", "comment": null, "summary": "Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.", "AI": {"tldr": "本文提出了一种名为 AgriPINN 的过程驱动的神经网络，它将作物生长微分方程作为可微分约束集成到深度学习框架中，用于准确且可解释地预测作物地上生物量（AGB），尤其是在水分胁迫下，并且在空间上具有可扩展性。", "motivation": "现有数据驱动模型在分布变化下表现不佳且缺乏可解释性，而基于过程的模型校准复杂且难以大范围部署。因此，需要一种能够结合数据驱动模型的扩展性与基于过程模型的生物物理准确性的方法来预测水分胁迫下的作物生物量。", "method": "AgriPINN 是一种过程驱动的神经网络，它将一个生物物理作物生长微分方程作为一个可微分的约束集成到一个深度学习主干中。该模型通过在历史数据和受控水处理实验数据上进行预训练和微调，无需直接监督即可恢复潜在的生理变量。", "result": "AgriPINN 在预测作物地上生物量方面，与先进的深度学习模型（ConvLSTM-ViT, SLTF, CNN-Transformer）和基于过程的模型（LINTUL5）相比，在准确性（RMSE 降低高达 43%）和计算效率上均表现更优。", "conclusion": "AgriPINN 成功地结合了深度学习的可扩展性和基于过程模型的生物物理严谨性，为时空作物生物量预测提供了一个鲁棒且可解释的框架，在灌溉规划、产量预测和气候适应规划方面具有实际应用价值。"}}
{"id": "2601.15924", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.15924", "abs": "https://arxiv.org/abs/2601.15924", "authors": ["Brainard Philemon Jagati", "Jitendra Tembhurne", "Harsh Goud", "Rudra Pratap Singh", "Chandrashekhar Meshram"], "title": "Class Confidence Aware Reweighting for Long Tailed Learning", "comment": "9 pages, 3 figures, IEEE Transaction on Neural Networks and Learning Systems (Submitted)", "summary": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.", "AI": {"tldr": "提出了一种基于损失函数的类别和置信度感知重加权方案，用于解决长尾分布数据中的类别不平衡问题，该方案可以与现有的logit调整方法互补。", "motivation": "深度神经网络在长尾分布数据下性能会显著下降，现有研究主要关注决策空间的调整（如logit层面的补偿），而对优化过程中由于样本置信度差异带来的调整关注较少。因此，需要一种新的方法来解决优化层面的类别不平衡问题。", "method": "设计了一种纯粹基于损失函数的类别和置信度感知重加权方案。该方案利用一个Ω(p_t, f_c)函数来根据预测的置信度值和对应类别的相对频率来调整样本对训练任务的贡献。该方案可以与现有的logit调整方法互补。", "result": "在CIFAR-100-LT、ImageNet-LT和iNaturalist2018数据集上，在不同不平衡因子下进行了大量实验，结果表明该方案能够显著提升模型在长尾数据上的性能。", "conclusion": "所提出的类别和置信度感知重加权方案是一种有效的解决长尾学习中类别不平衡问题的方法，并且可以与现有技术结合使用，在实际应用中取得了良好的效果。"}}
{"id": "2601.16056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16056", "abs": "https://arxiv.org/abs/2601.16056", "authors": ["Ruizhi Liu", "Liming Xu", "Xulin Huang", "Jingyan Sui", "Shizhe Ding", "Boyang Xia", "Chungong Yu", "Dongbo Bu"], "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path", "comment": null, "summary": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.", "AI": {"tldr": "本文提出了一种名为DeepBound的深度学习方法，用于改进求解混合整数线性规划（MILP）问题的分支定界算法中的节点选择策略，通过学习数据中的规律来替代手动设计的启发式规则，显著提高了求解效率和泛化能力。", "motivation": "传统求解MILP问题采用的基于经验启发式规则的节点选择策略性能不稳定且不可预测。研究旨在通过深度学习自动学习节点选择的直觉，从而提高求解效率。", "method": "DeepBound利用多层特征融合网络学习节点表示，并采用成对训练范式解决节点不平衡问题，以学习区分节点的能力。", "result": "在三个NP难MILP基准测试上的实验表明，DeepBound在求解效率上优于传统启发式规则和现有学习方法，以显著减少的计算时间获得最优可行解，并展现出对大规模复杂实例的强大泛化能力。", "conclusion": "DeepBound能够自动发现更灵活、鲁棒的特征选择，有效改进甚至可能取代人类设计的启发式规则，从而提高MILP求解的效率和稳定性。"}}
{"id": "2601.16087", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16087", "abs": "https://arxiv.org/abs/2601.16087", "authors": ["Sukesh Subaharan"], "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics", "comment": "Supplementary materials can be found here: https://github.com/drsukeshs/agent-behavior-ext-dynamics", "summary": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.", "AI": {"tldr": "本研究提出了一种在大型语言模型（LLM）代理中引入外部情感状态动态模型的方法，以提高多轮对话中的时序连贯性和可控恢复能力。", "motivation": "现有LLM代理在长对话中存在语气和角色突变的问题，这归因于缺乏显式的代理级状态的时间结构。现有研究主要关注局部情感或静态情感分类，而显式情感动力学对塑造长程代理行为的作用尚待探索。", "method": "研究引入了一个代理级情感子系统，该系统维护一个独立于语言模型的连续 Valence-Arousal-Dominance (VAD) 状态。该状态由一阶和二阶更新规则控制，并通过固定的、无记忆估计器提取瞬时情感信号，再通过指数平滑或基于动量的动力学进行积分。最终，将调整后的情感状态注入生成过程中，而不修改模型参数。", "result": "在25轮对话协议下，无状态代理无法展现连贯的行为轨迹或恢复能力。而引入状态持久性的代理能够实现延迟响应和可靠恢复。二阶动力学引入了情感惯性和滞后效应，其强度随动量增加而增强，揭示了稳定性和响应性之间的权衡。", "conclusion": "通过引入外部情感状态的动态结构，可以有效地解决LLM代理在长对话中的时序不连贯问题，并实现更可控的行为恢复。二阶动力学提供了更丰富的行为模式，但需要在稳定性和响应性之间进行权衡。"}}
{"id": "2601.15918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15918", "abs": "https://arxiv.org/abs/2601.15918", "authors": ["Valery Fischer", "Alan Magdaleno", "Anna-Katharina Calek", "Nicola Cavalcanti", "Nathan Hoffman", "Christoph Germann", "Joschua Wüthrich", "Max Krähenmann", "Mazda Farshad", "Philipp Fürnstahl", "Lilian Calvet"], "title": "A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery", "comment": null, "summary": "Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.\n  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.\n  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.\n  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.", "AI": {"tldr": "提出了一种无需领域特定微调的、基于现成预训练模型的鲁棒多视角三维手部姿态估计流水线，并引入了一个包含68,000多帧和3,000个手动标注的2D手部姿态及三维真实值的新的外科基准数据集，在定量实验中显著优于基线方法。", "motivation": "准确的三维手部姿态估计对于手术应用至关重要，例如技能评估、机器人辅助干预和几何感知工作流分析。然而，手术环境面临严峻挑战，包括强烈的局部照明、器械或人员的频繁遮挡、手部外观的均一性（因手套而异），以及用于可靠模型训练的标注数据集的稀缺性。", "method": "提出了一种鲁棒的多视角三维手部姿态估计流水线，该流水线无需领域特定微调，仅依赖现成的预训练模型。流水线集成了可靠的行人检测、全身姿态估计、跟踪手部裁剪区域上的最先进的2D手部关键点预测，以及受约束的3D优化。此外，引入了一个新的外科基准数据集，包含在模拟手术室中记录的68,000多帧和3,000个手动标注的2D手部姿态，并带有三角化后的3D真实值。", "result": "定量实验表明，所提出的方法一致优于基线方法，在2D平均关节误差方面降低了31%，在3D平均每关节位置误差方面降低了76%。", "conclusion": "这项工作为手术中的三维手部姿态估计建立了一个强大的基准，提供了一个无需训练的流水线和一个全面的标注数据集，以促进手术计算机视觉领域的未来研究。"}}
{"id": "2601.15929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15929", "abs": "https://arxiv.org/abs/2601.15929", "authors": ["Liuyun Jiang", "Yizhuo Lu", "Yanchao Zhang", "Jiazheng Liu", "Hua Han"], "title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation", "comment": null, "summary": "Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.", "AI": {"tldr": "提出了一种名为NeuroMamba的多视角框架，用于解决神经元分割的挑战，通过结合Mamba的全局建模能力和局部特征提取，以实现高精度分割。NeuroMamba在四个公共EM数据集上取得了最先进的性能。", "motivation": "现有的基于CNN和Transformer的方法在神经元分割中存在局限性：CNN缺乏长程上下文导致边界模糊，Transformer因patch划分丢失细节导致边界不精确。研究动机是克服这些限制，实现更准确的神经元分割。", "method": "NeuroMamba框架包含三个关键组件：1. 频道门控边界判别特征提取器（BDFE），增强局部形态线索。2. 空间连续特征提取器（SCFE），利用视觉Mamba架构和分辨率感知扫描机制，自适应地建模跨不同分辨率的全局依赖。3. 跨调制机制，融合多视角特征。", "result": "NeuroMamba在四个公共EM数据集上取得了最先进的性能，证明了其在处理各向异性和各向同性分辨率数据方面的卓越适应性。", "conclusion": "NeuroMamba框架通过结合全局和局部特征建模，有效地解决了神经元分割中的挑战，并在多个数据集上展现出优越的性能和泛化能力。"}}
{"id": "2601.16108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16108", "abs": "https://arxiv.org/abs/2601.16108", "authors": ["Marzieh Adeli Shamsabad", "Hamed Ghodrati"], "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources", "comment": null, "summary": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.", "AI": {"tldr": "本文提出了一种结合视觉语言模型（VLM）和外部知识检索的方法，以提高检测近期气候变化虚假信息的准确性，从而应对数字时代虚假图片和视频的挑战。", "motivation": "当前视觉语言模型在检测气候变化虚假信息方面存在局限，因为它们依赖于训练时已知的信息，无法处理近期事件或信息更新。", "method": "该研究通过检索最新的外部知识，如反向图片搜索结果、在线事实核查信息以及可信专家内容，来增强视觉语言模型的能力，以评估图片及其声明的准确性。", "result": "该方法能够更有效地处理现实世界中的气候变化虚假信息，提高模型评估图片与声明准确性的能力（准确、误导、虚假或无法验证）。", "conclusion": "将视觉语言模型与动态的外部知识相结合，是应对快速变化的信息环境中气候变化虚假信息挑战并保护公众科学认知的有效途径。"}}
{"id": "2601.16125", "categories": ["cs.CV", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16125", "abs": "https://arxiv.org/abs/2601.16125", "authors": ["Tingyu Song", "Yanzhao Zhang", "Mingxin Li", "Zhuoning Guo", "Dingkun Long", "Pengjun Xie", "Siyue Zhang", "Yilun Zhao", "Shu Wu"], "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing", "comment": "Under review", "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.", "AI": {"tldr": "本文提出了一种新的细粒度组成图像检索（CIR）基准 EDIR，通过图像编辑合成多样化的查询，旨在解决现有基准在类别覆盖和真实场景需求方面的不足。实验表明，即使是 SOTA 模型在 EDIR 上也表现出显著的性能差距，突显了该基准的严苛性，并揭示了现有基准的局限性。", "motivation": "现有 CIR 基准在查询类别多样性和真实世界应用方面存在局限性，无法充分评估模型在复杂场景下的表现。", "method": "利用图像编辑技术合成具有精确控制的修改类型和内容的查询，构建了一个覆盖广泛类别的查询合成流程，并据此构建了 EDIR 基准。使用该基准对 13 种多模态嵌入模型进行了评估，并进行了比较分析和领域内训练实验。", "result": "EDIR 基准包含 5000 个高质量查询，分布在 5 个主类别和 15 个子类别。现有 SOTA 模型（如 RzenEmbed 和 GME）在 EDIR 上表现出显著的性能差距，在所有子类别上均不稳定。分析揭示了现有基准的模式偏见和类别覆盖不足。领域内训练实验证明了基准的可行性，并区分了可解决的类别和暴露模型架构局限性的类别。", "conclusion": "EDIR 是一个具有挑战性的 CIR 新基准，能够更全面地评估多模态模型在细粒度图像检索任务上的能力。现有模型在处理复杂和多样化的 CIR 查询时仍存在显著差距，需要进一步改进。"}}
{"id": "2601.15876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15876", "abs": "https://arxiv.org/abs/2601.15876", "authors": ["Taofeng Xue", "Chong Peng", "Mianqiu Huang", "Linsen Guo", "Tiancheng Han", "Haozhe Wang", "Jianing Wang", "Xiaocheng Zhang", "Xin Yang", "Dengchang Zhao", "Jinrui Ding", "Xiandi Ma", "Yuchen Xie", "Peng Pei", "Xunliang Cai", "Xipeng Qiu"], "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "comment": "26 pages, 8 figures", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "AI": {"tldr": "本文提出了EvoCUA，一种新颖的计算机使用代理（CUA）模型，通过自给自足的进化循环整合数据生成和策略优化，克服了静态数据缩放的限制，并在OSWorld基准测试中取得了新的开源最佳性能。", "motivation": "现有基于静态数据集的被动模仿方法难以捕捉计算机任务中的因果动态，导致模型能力受限。作者希望开发一种能够自主学习和适应的CUA模型。", "method": "EvoCUA采用数据生成和策略优化的自给自足进化循环。通过可验证的合成引擎自主生成任务和验证器来缓解数据稀缺；设计可扩展的基础设施进行大规模经验获取；提出迭代进化学习策略，通过识别能力边界、分析错误和自我纠正来动态调节策略更新，将失败轨迹转化为监督信号。", "result": "EvoCUA在OSWorld基准测试中取得了56.7%的成功率，显著优于先前的开源模型OpenCUA-72B（45.0%）和领先的闭源模型UI-TARS-2（53.1%）。该方法在不同规模的基础模型上均能获得性能提升，表明其泛化能力和可扩展性。", "conclusion": "EvoCUA通过整合自主数据生成、大规模经验获取和迭代进化学习，成功克服了静态数据缩放的限制，为提升计算机使用代理的性能和泛化能力提供了一条强大且可扩展的路径。"}}
{"id": "2601.15951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15951", "abs": "https://arxiv.org/abs/2601.15951", "authors": ["Sheng Miao", "Sijin Li", "Pan Wang", "Dongfeng Bai", "Bingbing Liu", "Yue Wang", "Andreas Geiger", "Yiyi Liao"], "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis", "comment": null, "summary": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.", "AI": {"tldr": "EvolSplat4D 是一种新的前馈框架，通过结合体素和像素高斯预测，在不进行每场景优化的情况下，实现了对城市静态和动态场景的准确、一致的新视图合成。", "motivation": "现有新视图合成方法难以平衡重建时间和质量。基于神经辐射场和 3D 高斯泼溅的方法虽然逼真，但需要耗时的每场景优化。而前馈方法常采用每像素高斯表示，在复杂动态环境中会导致 3D 不一致。", "method": "EvolSplat4D 采用三分支前馈框架：1. 针对近距离静态区域，从 3D 特征体预测多帧一致的 3D 高斯几何，并结合基于图像的渲染模块预测外观。2. 针对动态演员，利用以物体为中心的规范空间和运动调整渲染模块聚合时间特征。3. 针对远景，使用高效的每像素高斯分支确保全场景覆盖。", "result": "在 KITTI-360、KITTI、Waymo 和 PandaSet 数据集上，EvolSplat4D 在重建静态和动态环境方面表现出优越的准确性和一致性，超越了每场景优化和现有前馈方法。", "conclusion": "EvolSplat4D 成功解决了现有方法在速度和质量上的权衡问题，实现了高效且高质量的城市场景新视图合成，特别是在处理动态内容方面。"}}
{"id": "2601.16172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16172", "abs": "https://arxiv.org/abs/2601.16172", "authors": ["Zachary Burton"], "title": "Structured Hints for Sample-Efficient Lean Theorem Proving", "comment": "9 pages, 1 figure", "summary": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.", "AI": {"tldr": "本文研究了在推理时使用简单的提示调度（prompt schedule）是否能提升像 DeepSeek-Prover-V1.5 这样经过强化学习训练的神经定理证明器的性能。结果表明，即使是能力很强的模型，也受益于这种简单的推理时引导，pass@16 指标相对提升了 43%。", "motivation": "研究动机是探索经过大量训练的先进神经定理证明器，在推理时是否还能从简单的结构化引导中获益。", "method": "作者在 miniF2F 基准测试中，对 DeepSeek-Prover-V1.5 模型应用了一种轻量级的干预方法：在 15 个常见的策略骨架（tactic skeletons）上进行固定的提示调度。在推理时，使用相同的采样次数（k=16）和最大生成长度（1024 tokens），比较这种方法与标准采样方法的性能。", "result": "使用提示调度的方法，在 miniF2F 基准测试上取得了 21.7% 的 pass@16 准确率，而标准采样方法仅为 15.2%，相对提升了 43%。", "conclusion": "即使是经过强化学习训练的先进定理证明器，在推理时也可能未能充分利用策略语言中已有的结构化先验信息。简单的推理时引导是一种廉价且有效的补充性增强方法。"}}
{"id": "2601.16216", "categories": ["cs.AI", "cs.GT", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16216", "abs": "https://arxiv.org/abs/2601.16216", "authors": ["Clémentine Sacré"], "title": "Scalable Board Expansion within a General Game System", "comment": "65 pages, 41 figures", "summary": "This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.", "AI": {"tldr": "本论文提出了一种使用通用游戏系统（GGS）来动态扩展棋盘的游戏方法，以解决传统棋类游戏因预设固定棋盘而导致的复杂性问题。", "motivation": "传统棋类游戏通常使用预先定义好的、但可能大部分未使用的静态棋盘，这增加了不必要的复杂性。研究者希望通过动态扩展棋盘来解决这个问题。", "method": "提出了一种动态棋盘扩展机制，利用通用游戏系统（GGS）支持游戏棋盘在游戏过程中自动扩展。", "result": "（抽象内容未提供具体实验结果，但核心是提出了一种动态扩展机制。）", "conclusion": "通过动态扩展棋盘，可以减少游戏实现的复杂性，提高游戏设计的灵活性。"}}
{"id": "2601.15968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15968", "abs": "https://arxiv.org/abs/2601.15968", "authors": ["Xin Xie", "Jiaxian Guo", "Dong Gong"], "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models", "comment": null, "summary": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.", "AI": {"tldr": "本文提出了一种名为HyperAlign的新框架，通过训练一个超网络来高效地对扩散模型进行测试时对齐，解决了现有方法在多样性、计算开销和优化程度上的不足。HyperAlign通过动态生成低秩适配权重来调整生成算子，并使用正则化的奖励分数目标进行优化，在多个生成任务中均表现优于现有方法。", "motivation": "现有的扩散模型在生成结果与人类偏好和意图对齐方面存在不足，导致图像美学质量差、语义不一致。现有对齐方法在多样性损失（奖励过度优化）和计算开销大/优化不足（测试时缩放）之间存在权衡，需要新的解决方案。", "method": "提出HyperAlign框架，训练一个超网络动态生成低秩适配（LoRA）权重，用于调节扩散模型的生成算子。这种方法在测试时根据输入的潜变量、时间步和提示来调整去噪轨迹，实现奖励条件下的对齐。同时，通过使用正则化的奖励分数目标和偏好数据来优化超网络，以减少奖励漏洞。研究中还探讨了不同应用频率的HyperAlign变体以平衡性能和效率。", "result": "HyperAlign在Stable Diffusion和FLUX等多个生成任务上进行了评估。实验结果表明，HyperAlign在提高语义一致性和视觉吸引力方面显著优于现有的微调和测试时缩放基线方法。", "conclusion": "HyperAlign是一个新颖的框架，能够高效且有效地实现扩散模型的测试时对齐，克服了现有方法的局限性。通过动态生成LoRA权重和优化的超网络训练，HyperAlign能够显著提升生成内容的质量和人类偏好的符合度。"}}
{"id": "2601.16007", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16007", "abs": "https://arxiv.org/abs/2601.16007", "authors": ["Chak-Wing Mak", "Guanyu Zhu", "Boyi Zhang", "Hongji Li", "Xiaowei Chi", "Kevin Zhang", "Yichen Wu", "Yangfan He", "Chun-Kai Fan", "Wentao Lu", "Kuangzhi Ge", "Xinyu Fang", "Hongyang He", "Kuan Lu", "Tianxiang Xu", "Li Zhang", "Yongxin Ni", "Youhua Li", "Shanghang Zhang"], "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models", "comment": null, "summary": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.", "AI": {"tldr": "本研究提出了PhysicsMind，一个包含真实和模拟环境的统一基准，用于评估多模态大语言模型（MLLMs）在三个基本物理原理（质心、杠杆平衡和牛顿第一定律）上的推理和生成能力。", "motivation": "现有评估物理理解的基准存在碎片化问题，依赖于合成数据、视觉问答模板或与物理定律无关的视频感知质量，未能充分衡量模型对物理定律的遵循程度。", "method": "PhysicsMind包含两个主要任务：1）视觉问答（VQA）任务，测试模型从图像或短视频中推理物理量值的能力；2）视频生成（VG）任务，评估模型生成的运动轨迹是否遵循质心、力矩和惯性约束。研究评估了一系列最新模型在该基准上的表现。", "result": "现有模型在PhysicsMind上表现不佳，倾向于依赖外观启发式推理，并且经常违反基本的力学定律。这表明当前的模型缩放和训练不足以实现对物理世界的鲁棒理解。", "conclusion": "PhysicsMind作为一个专门测试物理感知多模态模型的平台，揭示了当前MLLMs在物理理解方面存在的差距，并强调需要进一步的研究来提升模型在这方面的能力。"}}
{"id": "2601.16060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16060", "abs": "https://arxiv.org/abs/2601.16060", "authors": ["Yuan Lin", "Murong Xu", "Marc Hölle", "Chinmay Prabhakar", "Andreas Maier", "Vasileios Belagiannis", "Bjoern Menze", "Suprosanna Shit"], "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation", "comment": "5 pages, 4 figures. It has been accepted by IEEE ISBI", "summary": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.", "AI": {"tldr": "提出了一种名为ProGiDiff的新框架，利用预训练的图像生成模型进行医学图像分割，通过ControlNet风格的条件机制和自定义编码器实现，支持多类别分割和跨模态适应。", "motivation": "现有的医学图像分割方法虽然高效但缺乏灵活性，难以适应自然语言指令、多模型生成、人工交互和跨模态适应。文本到图像扩散模型有潜力解决此问题，但从头训练成本高昂，且常局限于二值分割和不能直接由自然语言引导。", "method": "提出ProGiDiff框架，采用ControlNet风格的条件机制，并结合自定义编码器来指导预训练的扩散模型生成分割掩码。通过简单的文本提示即可实现多类别分割，并展示了该条件机制可以通过低秩、少样本适应来迁移至不同模态（如CT到MR）。", "result": "在CT器官分割实验中，ProGiDiff表现优于现有方法。该框架能有效利用专家辅助的“人机协同”模式，生成多个分割建议。低秩、少样本适应机制成功地将模型泛化到MR图像分割。", "conclusion": "ProGiDiff框架能够有效地利用现有图像生成模型进行医学图像分割，支持多类别分割和跨模态适应，并且在专家辅助场景下具有巨大潜力。"}}
{"id": "2601.16140", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.16140", "abs": "https://arxiv.org/abs/2601.16140", "authors": ["Sylvestre-Alvise Rebuffi", "Tuan Tran", "Valeriu Lacatusu", "Pierre Fernandez", "Tomáš Souček", "Nikola Jovanović", "Tom Sander", "Hady Elsahar", "Alexandre Mourachko"], "title": "Learning to Watermark in the Latent Space of Generative Models", "comment": "Code and models are available at https://github.com/facebookresearch/distseal", "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.", "AI": {"tldr": "本研究提出了一种名为DistSeal的统一的潜在空间水印方法，适用于扩散模型和自回归模型，通过在生成模型的潜在空间训练水印模型，并将其蒸馏到生成模型或潜在解码器中，实现了高效且鲁棒的AI生成图像水印。", "motivation": "现有AI生成图像的水印方法多为后处理的像素空间方法，存在计算开销大和视觉伪影等问题。因此，需要一种更高效、更鲁棒的水印方法。", "method": "本研究提出DistSeal方法，首先在生成模型的潜在空间训练后处理水印模型。然后，将这些潜在空间水印模型蒸馏到生成模型本身或潜在解码器中，实现模型内水印。", "result": "与像素空间基线方法相比，所提出的潜在空间水印方法实现了具有竞争力的鲁棒性，同时保持相似的不可察觉性，并且速度提升高达20倍。实验表明，蒸馏潜在水印模型优于蒸馏像素水印模型。", "conclusion": "DistSeal提供了一种更高效、更鲁棒的AI生成图像水印解决方案，通过在潜在空间进行水印处理并进行模型内蒸馏，克服了传统像素空间方法的局限性。"}}
{"id": "2601.16024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16024", "abs": "https://arxiv.org/abs/2601.16024", "authors": ["Rongze Ma", "Mengkang Lu", "Zhenyu Xiang", "Yongsheng Pan", "Yicheng Wu", "Qingjie Zeng", "Yong Xia"], "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry", "comment": null, "summary": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.", "AI": {"tldr": "本文提出了一种名为PAINT的视觉自回归框架，用于从H&E图像合成虚拟免疫组织化学（IHC）染色，通过优先考虑结构信息并引入空间结构起始图（3S-Map）来解决现有方法的语义不一致问题，并在实验中取得了优于现有方法的性能。", "motivation": "传统的物理IHC染色成本高且消耗组织，而H&E形态学对蛋白质表达的指示模糊，导致现有虚拟IHC方法（侧重于外观合成）存在语义不一致问题。研究旨在开发一种更准确、更具结构保真度的虚拟IHC合成方法。", "method": "提出了一种名为PAINT（Pathology-Aware Integrated Next-Scale Transformation）的视觉自回归框架，将合成过程重塑为“结构优先”的条件生成任务。核心是引入空间结构起始图（3S-Map），它基于观察到的形态学信息来初始化自回归过程，确保合成的确定性和空间对齐。PAINT通过一个因果顺序来解决分子细节，条件是全局结构布局。", "result": "在IHC4BC和MIST数据集上的实验表明，PAINT在结构保真度和临床下游任务方面优于现有的最先进方法。", "conclusion": "结构引导的自回归建模在虚拟IHC合成任务中具有潜力，PAINT框架能够实现更高质量的结构保真度和更优的下游任务性能。"}}
{"id": "2601.16073", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16073", "abs": "https://arxiv.org/abs/2601.16073", "authors": ["Hanwen Zhang", "Qiaojin Shen", "Yuxi Liu", "Yuesheng Zhu", "Guibo Luo"], "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models", "comment": null, "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.", "AI": {"tldr": "提出了一种名为DSFedMed的双尺度联邦框架，通过互惠知识蒸馏来解决大型基础模型在联邦学习中的高计算和通信成本问题，特别是在医学图像分割任务中，旨在提高效率和可扩展性。", "motivation": "现有基础模型在联邦设置下存在计算需求高、通信开销大、推理成本高等问题，限制了其在资源受限场景（如联邦医学图像分割）下的部署。研究旨在克服这些挑战，实现高效且可扩展的联邦基础模型。", "method": "提出DSFedMed双尺度联邦框架，采用互惠知识蒸馏机制，允许中心化的基础模型与轻量级的客户端模型进行知识迁移。引入高质量合成医学图像代替公开数据集，并采用可学习性引导的样本选择策略来提高蒸馏的效率和效果。这种互惠蒸馏允许基础模型向客户端传递通用知识，同时客户端的特定见解也能反过来优化基础模型。", "result": "在五个医学图像分割数据集上的评估显示，DSFedMed相比于现有的联邦基础模型基线，平均Dice分数提高了2%，同时通信成本和推理时间减少了近90%。", "conclusion": "DSFedMed框架在资源受限的联邦部署中实现了显著的效率提升和可扩展性，有效地解决了基础模型在联邦学习中的部署挑战，特别是在医学图像分割领域。"}}
{"id": "2601.16079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16079", "abs": "https://arxiv.org/abs/2601.16079", "authors": ["Zhiyin Qian", "Siwei Zhang", "Bharat Lal Bhatnagar", "Federica Bogo", "Siyu Tang"], "title": "Masked Modeling for Human Motion Recovery Under Occlusions", "comment": "Project page: https://mikeqzy.github.io/MoRo", "summary": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.", "AI": {"tldr": "本文提出了一种名为MoRo的端到端生成框架，利用掩码建模技术，从单目视频中高效且鲁棒地重建人体运动，即使在存在遮挡的情况下也能表现出色，并实现了实时推理。", "motivation": "现有的单目视频人体运动重建方法在处理频繁遮挡时存在效率低下或鲁棒性不足的问题，而优化和扩散模型推理速度慢且需要大量预处理。因此，需要一种能够高效处理遮挡并实现端到端推理的新方法。", "method": "MoRo是一个端到端的生成框架，将运动重建视为一个视频条件任务。它通过掩码建模来处理遮挡，并设计了一个跨模态学习方案，结合了轨迹感知的运动先验（来自MoCap数据集）、图像条件姿态先验（来自图像-姿态数据集）以及一个视频条件掩码Transformer。该Transformer融合了运动和姿态先验，并在视频-运动数据集上进行微调，以整合视觉线索和运动动力学。", "result": "在EgoBody和RICH数据集上的实验表明，MoRo在遮挡情况下的准确性和运动真实感方面显著优于现有最先进的方法，而在无遮挡场景下表现相当。MoRo可以在单块H200 GPU上实现70 FPS的实时推理。", "conclusion": "MoRo通过利用掩码建模和跨模态学习，成功解决了单目视频人体运动重建中的遮挡问题，提供了一种高效、鲁棒且可实现实时推理的解决方案，并在多个基准测试中取得了优于现有方法的性能。"}}
{"id": "2601.16210", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16210", "abs": "https://arxiv.org/abs/2601.16210", "authors": ["Onkar Susladkar", "Tushar Prakash", "Adheesh Juvekar", "Kiet A. Nguyen", "Dong-Hwan Jang", "Inderjit S Dhillon", "Ismini Lourentzou"], "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation", "comment": null, "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.", "AI": {"tldr": "本文提出了一种名为 PyraTok 的新型语言对齐金字塔式视频分词器，它能在多个时空分辨率上学习语义结构化的离散潜在表示，以提升文本到视频生成和视频理解的性能，并在多种下游任务上取得了 SOTA 表现。", "motivation": "现有视频分词器在单一尺度上学习视觉码本，词汇量有限且语言监督浅层，导致跨模态对齐不佳和零样本迁移能力弱。", "method": "PyraTok 基于预训练的视频 VAE，并引入了新颖的语言对齐金字塔量化（LaPQ）模块，该模块使用共享的大型二值码本在多个深度处离散化编码器特征，同时通过多尺度文本引导量化和全局自回归目标联合优化，将视觉分词与语言紧密结合。", "result": "PyraTok 在视频重建任务上实现了 SOTA 性能，一致性地提升了文本到视频的生成质量，并在视频分割、时序动作定位和视频理解等零样本任务上创下新的 SOTA 记录，同时在 4K/8K 分辨率下表现稳定。", "conclusion": "PyraTok 能够学习语义结构化的离散潜在表示，有效解决了现有视频分词器的局限性，显著提升了视频生成和理解的跨模态对齐和零样本迁移能力。"}}
{"id": "2601.16093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16093", "abs": "https://arxiv.org/abs/2601.16093", "authors": ["Yikang Zhou", "Tao Zhang", "Dengxian Gong", "Yuanzheng Wu", "Ye Tian", "Haochen Wang", "Haobo Yuan", "Jiacong Wang", "Lu Qi", "Hao Fei", "Anran Wang", "Zhuochen Wang", "Yujing Wang", "Cheng Chen", "Shunping Ji", "Xiangtai Li"], "title": "SAMTok: Representing Any Mask with Two Words", "comment": "27 pages, 11 figures", "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "AI": {"tldr": "SAMTok 是一种离散掩码分词器，它将区域掩码转换为特殊标记，从而使基础多模态大模型（MLLMs）能够通过标准训练学习像素级能力，无需修改架构或设计特殊损失函数。", "motivation": "现有的像素级多模态大模型（MLLMs）难以扩展，因为它们需要复杂的区域级编码器、专门的分割解码器以及不兼容的训练目标。", "method": "提出 SAMTok，一种离散掩码分词器，将任意区域掩码转换为两个特殊标记，并高保真地重建掩码。SAMTok 基于 SAM2，使用掩码编码器和残差向量量化器进行训练，生成紧凑且信息丰富的标记。通过 SAMTok 格式的掩码理解和生成数据，使基础 MLLMs 能够通过标准的下一个标记预测和简单的强化学习来学习像素级能力。", "result": "QwenVL-SAMTok 在区域描述、区域 VQA、具身对话、指代分割、场景图解析和多轮交互分割等任务上取得了最先进或可比的结果。引入的文本答案匹配奖励显著提高了 GRES 和 GCG 基准上的掩码生成性能。", "conclusion": "SAMTok 提供了一种可扩展且简单的方法，为多模态大模型赋予强大的像素级能力，无需进行架构修改或设计特殊的损失函数。"}}
{"id": "2601.16211", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16211", "abs": "https://arxiv.org/abs/2601.16211", "authors": ["Geo Ahn", "Inwoong Lee", "Taeoh Kim", "Minho Shim", "Dongyoon Wee", "Jinwoo Choi"], "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition", "comment": "The code is available at https://github.com/KHU-VLL/RCORE", "summary": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.", "AI": {"tldr": "本研究提出了一种名为RCORE的框架，用于解决现有零样本组合动作识别（ZS-CAR）模型在处理未见过的动词-物体组合时表现不佳的问题，该问题源于模型过度依赖物体信息和共现统计。RCORE通过组合感知增强和时间顺序正则化来强制模型学习与时间相关的动词表示，从而提高泛化能力。", "motivation": "现有ZS-CAR模型在处理未见过的动词-物体组合时存在泛化能力不足的问题，主要原因是模型过度依赖物体信息（object-driven verb shortcuts），未能有效利用动词信息，尤其是在监督信号稀疏且不均衡的情况下。", "method": "提出了RCORE框架，包含两个核心组件：1. 组合感知增强（composition-aware augmentation），用于在不破坏运动信息的前提下，多样化动词-物体组合的训练数据。2. 时间顺序正则化损失（temporal order regularization loss），通过显式建模时间结构来惩罚模型依赖共现统计的捷径行为。", "result": "RCORE在Sth-com和EK100-com两个数据集上显著提高了未见组合的识别准确率，减少了对共现偏差的依赖，并实现了持续的组合优势（compositional gaps）。", "conclusion": "物体驱动的捷径是ZS-CAR模型泛化能力的关键限制因素，解决这些捷径对于实现鲁棒的组合视频理解至关重要。RCORE提供了一种有效的方法来应对这一挑战。"}}
{"id": "2601.16098", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16098", "abs": "https://arxiv.org/abs/2601.16098", "authors": ["Zack Dewis", "Yimin Zhu", "Zhengsen Xu", "Mabel Heffring", "Saeid Taleghanidoozdoozan", "Quinn Ledingham", "Lincoln Linlin Xu"], "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification", "comment": "5 pages, 3 figures", "summary": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.", "AI": {"tldr": "本文提出CSSMamba框架，通过聚类引导的空间-光谱Mamba模型，并结合注意力驱动的token选择和可学习聚类模块，以提高高光谱图像分类的效率和准确性。", "motivation": "现有的Mamba模型在高光谱图像分类方面虽有进步，但在生成高效自适应的token序列以提升性能方面存在挑战。", "method": "1. 引入聚类机制到空间Mamba架构中，构建CSpaMamba模块，缩短序列长度并增强特征学习。2. 将CSpaMamba与光谱Mamba模块（SpeMamba）结合，形成空间-光谱Mamba框架。3. 提出注意力驱动的Token选择机制优化token序列。4. 设计可学习聚类模块自适应学习聚类成员。", "result": "CSSMamba在Pavia University、Indian Pines和Liao-Ning 01数据集上，相较于现有的CNN、Transformer和Mamba方法，实现了更高的分类精度和更好的边界保留效果。", "conclusion": "CSSMamba框架通过有效的聚类策略和优化的Mamba模型，显著提升了高光谱图像分类的性能，在效率和准确性上均优于现有技术。"}}
{"id": "2601.16155", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.16155", "abs": "https://arxiv.org/abs/2601.16155", "authors": ["Zequn Xie", "Xin Liu", "Boyun Zhang", "Yuxiao Lin", "Sihang Cai", "Tao Jin"], "title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval", "comment": "Accepted by ICASSP 2026", "summary": "The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.", "AI": {"tldr": "提出了一种名为HVD（Human Vision-Driven）的文本-视频检索模型，通过模仿人类视觉的粗粒度到细粒度认知过程，解决了现有模型在处理稀疏文本查询时难以区分关键视觉信息与背景噪声的问题，并在多个基准测试中取得了最先进的性能。", "motivation": "现有文本-视频检索模型在处理稀疏文本查询时，存在“盲”特征交互问题，难以有效区分关键视觉信息与背景噪声。", "method": "HVD模型包含两个关键模块：帧特征选择模块（FFSM）和块特征压缩模块（PFCM）。FFSM通过选择关键帧来消除时间冗余，模仿人类的宏观感知能力；PFCM通过注意力机制聚合块特征为显著视觉实体，模仿人类的微观感知能力，实现实体级别的精确匹配。", "result": "在五个基准测试上的实验表明，HVD模型能够捕捉类似人类的视觉焦点，并取得了最先进的性能。", "conclusion": "HVD模型通过模仿人类视觉认知过程，有效地解决了文本-视频检索中的关键信息提取问题，实现了高性能的文本-视频检索。"}}
{"id": "2601.16148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16148", "abs": "https://arxiv.org/abs/2601.16148", "authors": ["Remy Sabathier", "David Novotny", "Niloy J. Mitra", "Tom Monnier"], "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion", "comment": null, "summary": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.", "AI": {"tldr": "ActionMesh是一个生成式模型，能够快速生成高质量、可用于生产的动画3D网格。它通过引入时间轴到3D扩散模型，并结合时间3D自编码器来实现，可以从视频、文本或带动画描述的3D网格生成动画。", "motivation": "现有的3D动画生成方法存在设置复杂、运行时间长、质量受限等问题，难以在实际应用中广泛使用。作者希望开发一种更高效、高质量的3D动画生成方法。", "method": "ActionMesh的核心是“时间3D扩散”模型。首先，它修改3D扩散模型以包含时间轴，生成随时间变化的独立3D形状的同步潜在表示。其次，设计了一个时间3D自编码器，将一系列独立形状转换为预定义参考形状的变形，从而构建动画。该模型可以处理单目视频、文本描述或带有文本动画提示的3D网格作为输入。", "result": "ActionMesh能够从多种输入生成动画3D网格，并且运行速度快，生成的网格无需骨骼绑定且拓扑一致。在Consistent4D和Objaverse等基准测试中，ActionMesh在几何精度和时间一致性方面均取得了最先进的性能。", "conclusion": "ActionMesh是一种高效且高质量的动画3D网格生成方法，它通过创新的时间3D扩散技术，克服了现有方法的局限性，为3D内容创作带来了新的可能性。"}}
{"id": "2601.16208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16208", "abs": "https://arxiv.org/abs/2601.16208", "authors": ["Shengbang Tong", "Boyang Zheng", "Ziteng Wang", "Bingda Tang", "Nanye Ma", "Ellis Brown", "Jihan Yang", "Rob Fergus", "Yann LeCun", "Saining Xie"], "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "comment": "website: https://rae-dit.github.io/scale-rae/", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "AI": {"tldr": "本文研究了表示自编码器（RAE）在大型文本到图像（T2I）生成任务中的扩展性，发现RAE比VAE在预训练和微调中表现更优，且更稳定，并展示了其在统一模型方面的潜力。", "motivation": "为了探索RAE框架是否能扩展到大规模、自由形式的文本到图像生成任务，并与现有先进方法（如FLUX VAE）进行比较。", "method": "研究人员首先扩展了RAE的解码器，并在不同类型的数据集上进行训练。然后，他们对RAE的设计选择进行了严格的压力测试，并与FLUX VAE在不同规模的模型上进行了受控比较。最后，评估了RAE和VAE在预训练和微调阶段的性能、稳定性和收敛速度。", "result": "扩展RAE解码器并进行数据组成优化能提升生成质量。在Scale up后，RAE框架简化，一些原有设计复杂度收益甚微。RAE在预训练和微调阶段均持续优于VAE，且在微调时表现更稳定，不易发生灾难性过拟合。RAE模型收敛更快，生成质量更高。", "conclusion": "RAE比VAE是更大规模T2I生成的更简单、更强大的基础。RAE在视觉理解和生成共享表示空间的能力，为构建统一的多模态模型提供了新的可能性。"}}
{"id": "2601.16214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16214", "abs": "https://arxiv.org/abs/2601.16214", "authors": ["Wenhang Ge", "Guibao Shen", "Jiawei Feng", "Luozhou Wang", "Hao Lu", "Xingye Tian", "Xin Tao", "Ying-Cong Chen"], "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback", "comment": null, "summary": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.", "AI": {"tldr": "本文提出了一种基于奖励反馈学习（ReFL）的方法，通过引入一个高效的相机感知3D解码器，将视频潜在表示解码为3D高斯表示，从而解决了现有相机控制视频扩散模型中相机可控性不足的问题。", "motivation": "尽管现有的相机控制视频扩散模型在视频-相机对齐方面有所改进，但相机可控性仍然受限。直接应用现有的ReFL方法存在挑战，包括奖励模型评估相机对齐的能力不足、解码RGB视频计算奖励的计算开销大，以及忽视3D几何信息。", "method": "提出了一种高效的相机感知3D解码器，将视频潜在表示和相机姿态解码为3D高斯。相机姿态既作为输入也作为投影参数。通过优化渲染的新视角与真实视角之间的像素级一致性来计算奖励，并引入一个可见性项来选择性地监督由几何变换确定的区域。", "result": "在RealEstate10K和WorldScore数据集上进行了广泛的实验，证明了所提出方法的有效性。", "conclusion": "所提出的方法通过利用3D几何信息和优化的奖励机制，显著提高了相机控制视频扩散模型的相机可控性，解决了现有方法的局限性。"}}
{"id": "2601.16192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.16192", "abs": "https://arxiv.org/abs/2601.16192", "authors": ["Ziyi Wu", "Daniel Watson", "Andrea Tagliasacchi", "David J. Fleet", "Marcus A. Brubaker", "Saurabh Saxena"], "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°", "comment": "Project page: https://360anything.github.io/", "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.", "AI": {"tldr": "提出了一种名为360Anything的无需几何信息即可将透视图像/视频提升到360度全景图的框架，利用预训练的扩散Transformer，在图像和视频生成方面均达到最先进性能，并引入Circular Latent Encoding解决边界接缝问题。", "motivation": "现有方法在将透视图像/视频转换为360度全景图时，需要显式的几何对齐和相机元数据，这限制了其在缺乏准确相机信息的现实场景中的应用。", "method": "利用预训练的扩散Transformer，将透视输入和全景目标视为token序列，以数据驱动的方式学习透视到全景的映射，无需相机信息。引入Circular Latent Encoding来解决VAE编码器中的零填充导致的边界接缝伪影。", "result": "在图像和视频透视到360度生成任务上取得了最先进的性能，优于使用真实相机信息的先前工作。在零样本相机视场角和方向估计基准测试中也取得了有竞争力的结果。", "conclusion": "360Anything是一个强大的、无需几何信息的框架，能够有效地将透视图像和视频转换为360度全景图，并且具备深度几何理解能力，可广泛应用于计算机视觉任务。"}}
{"id": "2601.16134", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16134", "abs": "https://arxiv.org/abs/2601.16134", "authors": ["Langdon Holmes", "Adam Coscia", "Scott Crossley", "Joon Suh Choi", "Wesley Morris"], "title": "LLM Prompt Evaluation for Educational Applications", "comment": null, "summary": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.", "AI": {"tldr": "本研究提出了一种系统性的方法来评估用于教育应用的 LLM 提示，并通过锦标赛式评估框架和 Glicko2 评级系统进行验证。结果表明，一个强调元认知学习策略的提示在与用户互动中表现最佳。", "motivation": "随着 LLM 在教育领域应用的普及，需要有循证的方法来设计和评估能够生成个性化且符合教学目标的 LLM 提示。", "method": "设计了六种不同的提示模板，结合了已知的提示工程模式和不同的教学策略。采用锦标赛式评估框架，使用 Glicko2 评级系统，由八名评审员从格式、对话支持和学习者适用性三个维度对提示生成的跟进问题对进行评估。数据来源于 120 次真实用户互动。", "result": "一个结合了角色扮演和上下文管理器模式，并侧重于策略性阅读和元认知学习的提示在 pairwise 比较中表现最佳，胜率在 81% 到 100% 之间。", "conclusion": "该研究提供了一种系统性的方法来评估和改进 LLM 提示设计，推动教育应用中的提示工程从临时性转向循证式开发。"}}
