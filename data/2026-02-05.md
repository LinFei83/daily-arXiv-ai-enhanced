<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CV](#cs.CV) [Total: 90]
- [cs.CL](#cs.CL) [Total: 72]
- [cs.RO](#cs.RO) [Total: 38]
- [eess.SY](#eess.SY) [Total: 13]
- [eess.IV](#eess.IV) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: 本文研究了任务-方法-知识（TMK）框架是否能提升大型语言模型（LLM）在推理和规划任务上的能力，实验结果表明TMK提示可以显著提高LLM在符号推理任务上的准确率，并引导模型从默认的语言模式转向形式化的代码执行。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在推理和规划任务上表现不佳，即使是链式思考（CoT）等提示技术也存在局限性。TMK框架因其捕获因果、目的和层级推理结构的能力，以及显式的任务分解机制，被认为是解决LLM推理缺陷的潜在方案。

Method: 研究采用TMK框架，通过在PlanBench基准的Blocksworld领域进行实验来评估其对LLM推理和规划能力的影响。通过TMK结构化提示，分析模型是否能更好地将复杂规划问题分解为可管理子任务。

Result: TMK提示显著提高了LLM在符号推理任务上的准确率，在Blocksworld的随机版本（不透明符号任务）上，准确率从31.5%提升至97.3%。研究还发现TMK提示能够引导模型脱离默认的语言模式，转向形式化的代码执行路径。

Conclusion: TMK框架不仅仅提供上下文，还能作为一种机制，引导LLM从其默认的语言模式转向形式化的、执行代码的推理方式，从而显著提升其在符号推理和规划任务上的表现，有望弥合语义近似与符号操作之间的差距。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [2] [Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation](https://arxiv.org/abs/2602.03950)
*Aditya Basarkar,Benyamin Tabarsi,Tiffany Barnes,Dongkuan,Xu*

Main category: cs.AI

TL;DR: 提出了一种名为IIPC（Iteratively Improved Program Construction）的数学推理新方法，通过迭代优化程序化推理链，结合执行反馈和LLM的链式思考能力，提高了AI在数学问题解决上的准确性和可修正性。


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体LLM的数学推理方法存在两个主要问题：1）推理过程缺乏可修正的表示，要么依赖于僵化的顺序流程无法纠正早期错误，要么依赖于可能失效的启发式自我评估；2）程序化上下文可能分散模型注意力，降低准确性。研究旨在解决这些不足。

Method: 提出Iteratively Improved Program Construction (IIPC) 方法。该方法通过迭代地精炼程序化推理链，并结合执行反馈和基础LLM固有的链式思考（Chain-of-thought）能力，以维持高层次的上下文关注度，从而实现对推理过程的可靠修正。

Result: IIPC方法在多个基础LLM上，在多数推理基准测试中均超越了现有竞争方法。

Conclusion: IIPC是一种有效的新型数学推理方法，通过迭代改进程序化推理链并整合执行反馈，显著提高了LLM在数学问题解决中的性能和可修正性。

Abstract: Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.

</details>


### [3] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk 是一个新框架，通过将多智能体交互蒸馏到单个模型的权重中，以提高 LLM 多智能体系统的计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 多智能体系统虽然在推理方面表现出色，但由于计算成本高和错误传播，实际部署受到限制。

Method: 研究了三种分层蒸馏策略：增强推理的微调、基于轨迹的增强和过程感知蒸馏。

Result: 蒸馏后的模型在保持单智能体效率的同时，展现出多智能体的强大推理和自我纠错能力，并且在各种推理任务上表现出增强的鲁棒性和泛化能力。

Conclusion: AgentArk 通过将计算负担从推理转移到训练，实现了高效且鲁棒的多智能体系统，为未来的研究提供了方向。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [4] [Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 本文提出了一种名为主动认知控制（AEC）的规划方法，它通过区分实际事实和信念来解决部分可观测环境下的规划问题，以提高规划效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中进行规划具有挑战性，因为关键的先决条件（如物体位置）可能未知，而通过交互来确定这些信息成本高昂。虽然学习到的世界模型可以预测缺失的事实，但预测错误可能导致计划不可行。因此，需要一种方法来在预测和实际交互之间取得平衡。

Method: AEC是一种认知-分类规划层，它结合了基于模型的信念管理和分类可行性检查。它维护一个实际事实存储（用于承诺）和一个信念存储（用于修剪候选计划）。在每一步，AEC会根据不确定性或预测的模糊程度，选择查询环境以获取实际事实，或者模拟事实以过滤假设。最终的承诺依赖于实际先决条件的覆盖以及兼容性检查，确保模拟信念只影响效率而不直接保证可行性。

Result: 在ALFWorld和ScienceWorld的实验中，AEC与强大的LLM-agent基线相比，实现了具有竞争力的成功率，并且需要的重新规划轮次更少。

Conclusion: AEC通过一种将模型预测与实际交互相结合的策略，有效解决了部分可观测环境下的规划难题，提高了规划效率和可靠性，并在复杂环境中取得了优于现有方法的表现。

Abstract: Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \emph{grounded fact store} used for commitment and a \emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines.

</details>


### [5] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出了一种用于 LLM 推理的验证成本受限的择优验证框架，通过确定性可行性门控、混合学习和残差评分预验证排序以及基于局部不确定性的自适应分配，有效地将验证资源分配给最有信息量的中间状态，从而在 MATH 基准测试中以更少的验证调用实现了更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 推理系统在验证阶段存在计算成本高昂的问题，大量验证调用被浪费在冗余或无希望的中间假设上。研究人员希望在验证成本受限的情况下，优化中间状态的验证资源分配。

Method: 提出了一种状态层面的选择性验证框架，包括：1. 确定性可行性门控：利用结构化移动接口过滤掉不可行的状态。2. 预验证排序：结合学习到的状态距离和残差评分来对状态进行排序。3. 自适应验证分配：根据局部不确定性动态调整验证调用。

Result: 在 MATH 基准测试中，该方法相比于 best-of-$N$、多数投票和束搜索，在准确率更高的情况下，验证器调用数量减少了 44%。

Conclusion: 该择优验证框架能够有效地将验证资源分配给最能提供信息的状态，在验证成本受限的情况下，可以显著提高 LLM 推理的效率和准确性。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [6] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 研究表明，在训练大型推理模型（LRMs）时，通过可验证奖励强化学习（RLVR）可以提高模型推理过程（CoT）的可监控性，但这种提高并非普遍，而是高度依赖于训练数据的多样性和指令遵循数据。模型能力（推理性能）的提升并不一定伴随着可监控性的提高，可监控性的提升主要归因于响应分布的锐化和对提示的注意力增加，而非更强的因果依赖性。研究还揭示了训练和评估难易度对可监控性动态变化的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型（LRMs）的广泛应用，对其思维链（CoT）进行安全审计变得至关重要。之前的研究发现在强化学习与可验证奖励（RLVR）的早期阶段，可监控性（CoT真实且信息性地反映内部计算的程度）似乎是“免费获得”的。本研究旨在系统地评估这一现象，明确其出现的条件和机制。

Method: 通过对不同模型系列和训练域进行系统性评估，分析RLVR训练中可监控性的表现。通过机械分析（mechanistic analysis）来探究可监控性提升的原因。研究还控制了训练和评估的难度，以观察可监控性动态的变化。

Result: 可监控性的提升并非普遍存在，而是强烈依赖于训练数据。数据多样性和指令遵循数据在RLVR训练中起着关键作用。可监控性与模型能力（推理性能）是正交的，即推理性能的提高不意味着可监控性的提高。可监控性提升主要归因于响应分布的锐化（熵减少）和对提示的注意力增加，而不是对推理过程更强的因果依赖性。

Conclusion: 本研究提供了对RLVR训练中可监控性如何产生的全面视角，阐明了可监控性提升可能出现的条件和不可能出现的情况。结果强调了数据多样性和指令遵循数据在提升模型可监控性方面的重要性，并揭示了可监控性提升的根本原因并非更强的因果依赖，而是响应分布的变化和对输入的更多关注。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [7] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 本研究提出了一种针对大型语言模型（LLM）生成解释的新型对抗性攻击方法——对抗性解释攻击（AEAs），旨在操纵用户对LLM输出的信任度。实验表明，AEAs能够有效地在用户对错误输出的信任度与对正确输出的信任度之间造成巨大差距，尤其是在解释风格模仿专家时。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统越来越多地融入人类决策过程，但现有的对抗性攻击主要集中在模型计算行为，忽视了AI与用户之间的认知交流层，尤其是LLM生成的解释对用户信任度的影响。研究旨在探索和量化这种新的攻击表面。

Method: 研究提出了“信任失衡差距”作为衡量AEAs威胁的指标。通过一项包含205名参与者的对照实验，系统地改变了解释框架的四个维度：推理模式、证据类型、沟通风格和呈现格式，以评估不同解释策略对用户信任度的影响。

Result: 实验结果显示，用户对对抗性解释和良性解释的信任度几乎相同。即使面对错误的输出，对抗性解释也能在很大程度上保留用户的信任。当对抗性解释模仿专家沟通风格（如使用权威证据、中性语气、领域内推理）时，用户最容易受到影响。高风险场景包括复杂任务、事实驱动领域，以及教育程度较低、年轻或高度信任AI的参与者。

Conclusion: 本研究首次将LLM的解释视为一个对抗性的认知渠道，并量化了其对AI辅助决策中人类信任度的影响。研究表明，AEAs是一种切实存在的威胁，需要进一步研究和防御策略来保护用户免受基于解释的操纵。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [8] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 本文提出了一个反事实解释器的公理框架，揭示了五种不同类型的反事实解释，包括局部和全局解释，并证明了某些公理组合的不可行性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法多局限于单一类型且为局部解释，缺乏对不同反事实类型和全局解释的系统性研究，这阻碍了对自主和智能系统决策的信任。

Method: 构建了一个包含理想属性的公理框架，证明了不可行定理（表明某些公理组合无法同时满足），并给出了所有兼容的公理集。通过表示定理，建立了五个公理子集与满足它们的解释器家族之间的一一对应关系，从而揭示了五种基本不同的反事实解释类型。

Result: 发现了五种基本不同的反事实解释类型，其中一些是局部解释，另一些是全局解释。现有的解释器被纳入该分类体系，并对其行为进行了形式化表征和计算复杂度分析。

Conclusion: 该公理框架系统地研究了反事实解释器的属性，揭示了不同类型反事实解释的存在，并为理解和开发更有效的解释器提供了理论基础。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [9] [Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL](https://arxiv.org/abs/2602.04089)
*Xiaofeng Lin,Sirou Zhu,Yilei Chen,Mingyu Chen,Hejian Sang,Ioannis Paschalidis,Zhipeng Wang,Aldo Pacchiano,Xuezhou Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为ORBIT的多任务、多回合元强化学习框架，用于训练大型语言模型（LLMs）在在线交互环境中进行学习。经过ORBIT训练后，Qwen3-14B模型在未见过的环境中表现出显著的上下文在线学习能力，性能可与GPT-5.2媲美，并远超标准强化学习微调方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多决策任务是在线的，需要通过交互获取信息，并且反馈会延迟。现有的LLMs在处理这类任务时，尽管可以通过上下文学习进行适应，但在可靠地利用交互经验方面存在困难。作者希望通过训练来解决这一限制。

Method: 作者提出了ORBIT框架，这是一个多任务、多回合的元强化学习（Meta-RL）框架。该框架通过在多种交互式任务中进行训练，使LLMs能够学习如何在上下文（in-context）中进行在线交互学习。最终的训练目标是让模型能够在推理时（at-inference-time）有效地进行决策。

Result: 经过ORBIT框架元训练后，一个相对较小的开源模型（Qwen3-14B）在全新的在线交互环境中，表现出显著优于标准强化学习微调方法的性能，并且能与GPT-5.2相媲美。模型规模的扩展实验表明，随着模型尺寸的增加，性能会持续提升。

Conclusion: 通过ORBIT框架进行训练，大型语言模型能够显著提升其在在线交互决策任务中的上下文学习能力。这表明通过特定的训练方法，LLMs可以被优化为在推理时能够进行有效的在线决策，并且这种能力与模型规模正相关，预示着未来有很大的发展空间。

Abstract: Large language models (LLMs) achieve strong performance when all task-relevant information is available upfront, as in static prediction and instruction-following problems. However, many real-world decision-making tasks are inherently online: crucial information must be acquired through interaction, feedback is delayed, and effective behavior requires balancing information collection and exploitation over time. While in-context learning enables adaptation without weight updates, existing LLMs often struggle to reliably leverage in-context interaction experience in such settings. In this work, we show that this limitation can be addressed through training. We introduce ORBIT, a multi-task, multi-episode meta-reinforcement learning framework that trains LLMs to learn from interaction in context. After meta-training, a relatively small open-source model (Qwen3-14B) demonstrates substantially improved in-context online learning on entirely unseen environments, matching the performance of GPT-5.2 and outperforming standard RL fine-tuning by a large margin. Scaling experiments further reveal consistent gains with model size, suggesting significant headroom for learn-at-inference-time decision-making agents. Code reproducing the results in the paper can be found at https://github.com/XiaofengLin7/ORBIT.

</details>


### [10] [Interfaze: The Future of AI is built on Task-Specific Small Models](https://arxiv.org/abs/2602.04101)
*Harsha Vardhan Khurdula,Vineet Agarwal,Yoeven D Khemlani*

Main category: cs.AI

TL;DR: Interfaze是一个创新的系统，它将LLM应用视为上下文构建和操作问题，而非仅仅依赖单一模型。该系统结合了异构DNN感知模块（用于OCR、图表、多语言ASR）、上下文构建层（用于信息检索和解析）以及一个行动层（用于浏览、代码执行和驱动浏览器）。一个控制器协调这些组件，并将精炼后的上下文提供给用户选择的LLM生成最终响应。实验结果表明，Interfaze在多项基准测试中表现出色，且大部分计算负担由小型模型和工具栈承担，有效降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现代LLM应用面临的挑战是如何有效地构建和利用上下文信息，而不仅仅是选择一个独立的模型。现有的方法可能无法充分处理复杂的输入（如图表、PDF）和多模态数据，也无法高效地与外部信息交互。研究的动机是开发一个更灵活、更高效的系统，能够处理更广泛的LLM应用场景，并优化计算资源的使用。

Method: 该系统名为Interfaze，采用模块化架构，结合了：1. 异构深度神经网络（DNN）和小型语言模型组成的感知模块，用于处理OCR（包括复杂PDF、图表）和多语言自动语音识别（ASR）。2. 上下文构建层，负责爬取、索引和解析外部信息源（网页、代码、PDF），并将其转化为紧凑的结构化状态。3. 行动层，能够执行浏览、信息检索、在沙箱环境中执行代码以及驱动无头浏览器处理动态网页。一个轻量级的控制器管理这些组件，并通过一个OpenAI风格的API对外提供服务，决定调用哪些小型模型和行动，并将精炼后的上下文传递给用户选择的大型语言模型（LLM）。

Result: Interfaze-Beta在多项基准测试中取得了优异成绩，包括MMLU-Pro（83.6%）、MMLU（91.4%）、GPQA-Diamond（81.3%）、LiveCodeBench v5（57.8%）和AIME-2025（90.0%）。在多模态任务上，MMMUM（val）为77.3%，AI2D为91.5%，ChartQA为90.9%，Common Voice v16为90.8%。研究发现，大多数查询主要由小型模型和工具栈处理，大型LLM仅作用于精炼后的上下文，这在保证竞争性准确率的同时，将大部分计算量从昂贵且单体的模型中转移出来。

Conclusion: Interfaze成功地将LLM应用构建为上下文管理和操作的问题，而非仅仅依赖单一的整体模型。通过结合异构感知模块、上下文构建层和行动层，并由一个协调器控制，该系统能够有效处理复杂多样的输入，并与外部环境交互。这种架构在实现高性能的同时，显著优化了计算资源的利用，将计算密集型任务转移到更高效的组件上。

Abstract: We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response.
  On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.

</details>


### [11] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: 本文提出了一种名为OMG-Agent的新型多模态数据补全框架，通过“先规划后执行”的代理工作流，将语义规划、证据检索和检索注入执行三个阶段解耦，有效解决了现有方法的局限性，并在多项基准测试中取得了SOTA性能，尤其是在极端缺失率下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态数据补全方法在处理数据缺失时存在局限性：参数/生成模型容易产生幻觉，检索增强框架存在检索僵化问题。这些方法受限于“语义-细节纠缠”这一结构性冲突，影响了重建的保真度。

Method: OMG-Agent采用动态代理工作流，分为三个阶段：1. 语义规划器（基于MLLM）通过渐进式上下文推理解决输入歧义，生成结构化语义规划；2. 证据检索器（非参数）将抽象语义与外部知识关联；3. 检索注入执行器利用检索到的证据作为灵活的特征提示，克服僵化并合成高保真细节。

Result: OMG-Agent在多项基准测试中表现优于现有SOTA方法，特别是在70%缺失率的CMU-MOSI上提升了2.6个百分点，显示出在极端缺失情况下的鲁棒性。

Conclusion: OMG-Agent通过显式解耦语义规划、证据检索和细节执行，成功克服了现有方法的瓶颈，并实现了高效、鲁棒的多模态数据补全。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [12] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 提出了一种名为“可扩展交互式监督”的框架，通过将复杂任务分解为一系列可管理的决策，并利用用户反馈进行强化学习，从而使非专家也能指导大型语言模型完成复杂任务，并显著提高了任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在执行复杂长时任务方面取得了巨大进展，但用户因缺乏领域知识、难以清晰表达意图以及无法验证复杂输出而难以有效指导模型，这导致了监督上的差距。需要一种方法来弥补这一差距，使人类能够负责任地指导超出自身能力范围的任务。

Method: 该框架将复杂意图分解为一个递归决策树，并在每个节点上收集低负担的用户反馈。这些反馈信号被递归地聚合，形成精确的全局指导。该框架在 Web 开发任务中得到了验证，并通过仅使用在线用户反馈进行强化学习进行了优化。

Result: 在 Web 开发任务中，非专家用户通过该框架能够生成专家级别的产品需求文档，与基线相比，任务对齐度提高了 54%。

Conclusion: “可扩展交互式监督”框架通过分解任务和利用用户反馈，有效地放大了人类监督能力，使非专家能够指导复杂的 AI 任务。该框架可通过在线用户反馈进行强化学习优化，为 AI 扩展过程中保持人类控制提供了可行途径。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [13] [InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons](https://arxiv.org/abs/2602.04213)
*Feiyu Gavin Zhu,Jean Oh,Reid Simmons*

Main category: cs.AI

TL;DR: 本文提出了一种名为InterPReT的交互式策略重构与训练方法，降低了非专业人士训练AI agent的门槛，使其能够通过用户指令和演示数据来不断更新和优化策略，从而实现更易用、更可靠的策略训练。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法需要大量专业演示和密切的训练监控，这对于普通用户来说难以实现。研究旨在为普通用户提供一种更便捷的方式来教授AI agent新技能。

Method: 提出InterPReT方法，该方法允许用户通过交互式指令和演示数据来持续更新策略结构和优化参数。用户可以给出指令和演示，监控代理性能，并审查其决策策略。

Result: 在赛车游戏驾驶任务的用户研究（N=34）中，InterPReT方法相比于通用模仿学习基线，在普通用户负责演示和停止训练的情况下，能够生成更鲁棒的策略，同时不影响系统的可用性。

Conclusion: InterPReT方法更适合没有机器学习背景的终端用户来训练可靠的策略，有效降低了AI agent技能训练的门槛。

Abstract: Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent's performance, and review the agent's decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy

</details>


### [14] [Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search](https://arxiv.org/abs/2602.04248)
*Hao Lu,Haoyuan Huang,Yulin Zhou,Chen Li,Ningxin Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Empirical-MCTS的框架，通过引入全局记忆和实时元提示进化，将无状态的MCTS转变为一个连续的学习过程，以提升LLM的推理能力，并在多个复杂推理基准测试中取得了显著优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的推理策略（如MCTS）通常是无状态的，无法保留和利用之前问题中学习到的有效推理模式，这与人类通过经验积累智慧解决问题的过程不同。研究旨在弥合这一差距，实现经验的持续积累和推理能力的提升。

Method: 提出Empirical-MCTS双循环框架，包含两个核心机制：1. Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP)，在局部搜索中通过成对反馈动态合成适应性标准并实时进化元提示。2. Memory Optimization Agent，管理一个全局知识库作为动态策略先验，通过原子操作提炼跨问题的优质见解。

Result: 在AIME25、ARC-AGI-2和MathArena Apex等复杂推理基准测试中，Empirical-MCTS显著优于无状态MCTS策略和独立的经验驱动代理。

Conclusion: 将结构化搜索与经验积累相结合对于掌握复杂、开放式的推理任务至关重要。Empirical-MCTS展示了这种结合的有效性，为LLM的持续学习和推理能力提升提供了新途径。

Abstract: Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.

</details>


### [15] [Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.04284)
*Yansong Ning,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Agent-Omit 的框架，用于训练 LLM 智能体自适应地省略冗余的思考和观察，以提高效率，并在多项基准测试中取得了与现有前沿 LLM 智能体相当的性能，同时实现了最佳的效率-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分不同回合中思考的必要性和观察的效用，导致对所有交互轨迹的处理方式相同，从而影响了智能体的效率。

Method: 该研究首先通过定量分析来理解思考和观察对智能体有效性和效率的影响。然后，提出 Agent-Omit 框架，通过合成冷启动数据微调智能体以学习省略行为，并引入了 omit-aware 智能体强化学习方法，该方法包含双重采样机制和定制的省略奖励。理论上证明了省略策略的偏差受 KL 散度上限约束。

Result: 在五个基准测试中，Agent-Omit-8B 智能体取得了与七个前沿 LLM 智能体相当的性能，并且在效率-效用权衡方面优于七种高效 LLM 智能体方法。

Conclusion: Agent-Omit 框架能够有效提升 LLM 智能体的效率，使其能够自适应地省略冗余的思考和观察，并在多项任务中实现了优异的性能和效率权衡。

Abstract: Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.

</details>


### [16] [From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents](https://arxiv.org/abs/2602.04326)
*SeungWon Seo,SooBin Lim,SeongRae Noh,Haneul Kim,HyeongYeop Kang*

Main category: cs.AI

TL;DR: 本文提出了一种名为 PCE (Planner-Composer-Evaluator) 的框架，旨在通过将大型语言模型 (LLM) 的推理过程转化为结构化的决策树，来解决多智能体、部分可观察、去中心化环境中的不确定性问题，从而减少对频繁通信的依赖。


<details>
  <summary>Details</summary>
Motivation: 在多智能体、部分可观察和去中心化环境中，智能体需要规划和行动，但普遍存在关于隐藏对象和合作者意图的不确定性。现有的 LLM 方法虽然有所进步，但仍主要依赖频繁的通信来减轻不确定性，这会产生高昂的通信成本，并可能干扰涉及人类参与者的工作流程。

Method: PCE 框架将 LLM 推理过程中隐含的零散假设转化为结构化的决策树。内部节点表示环境假设，叶节点映射到动作。通过对每个路径的场景可能性、目标导向收益和执行成本进行评分，来指导理性动作选择，从而避免了大量的通信。

Result: 在 C-WAH 和 TDW-MAT 两个多智能体基准以及三种不同的 LLM 后端上，PCE 在成功率和任务效率方面始终优于以通信为中心的方法，同时通信成本相当。消融实验表明，扩展模型容量或推理深度带来的性能提升在应用 PCE 后依然存在，而 PCE 能够持续提升不同规模模型和推理深度的性能基线，证实了结构化不确定性处理能够很好地补充这两种扩展方式。用户研究表明，PCE 生成的通信模式被人类合作伙伴认为更有效和更值得信赖。

Conclusion: PCE 框架提供了一种原则性的方法，可以将 LLM 中潜在的假设转化为可靠的、可用于不确定性感知规划的策略，有效解决了多智能体环境中的不确定性问题，并减少了对通信的依赖。

Abstract: Embodied agents operating in multi-agent, partially observable, and decentralized environments must plan and act despite pervasive uncertainty about hidden objects and collaborators' intentions. Recent advances in applying Large Language Models (LLMs) to embodied agents have addressed many long-standing challenges, such as high-level goal decomposition and online adaptation. Yet, uncertainty is still primarily mitigated through frequent inter-agent communication. This incurs substantial token and time costs, and can disrupt established workflows, when human partners are involved. We introduce PCE, a Planner-Composer-Evaluator framework that converts the fragmented assumptions latent in LLM reasoning traces into a structured decision tree. Internal nodes encode environment assumptions and leaves map to actions; each path is then scored by scenario likelihood, goal-directed gain, and execution cost to guide rational action selection without heavy communication. Across two challenging multi-agent benchmarks (C-WAH and TDW-MAT) and three diverse LLM backbones, PCE consistently outperforms communication-centric baselines in success rate and task efficiency while showing comparable token usage. Ablation results indicate that the performance gains obtained by scaling model capacity or reasoning depth persist even when PCE is applied, while PCE consistently raises the baseline across both capacity and reasoning-depth scales, confirming that structured uncertainty handling complements both forms of scaling. A user study further demonstrates that PCE produces communication patterns that human partners perceive as more efficient and trustworthy. Together, these results establish a principled route for turning latent LLM assumptions into reliable strategies for uncertainty-aware planning.

</details>


### [17] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本研究提出了一种模块化、可互操作的数字孪生（DT）驱动的零配置（ZeroConf）AI管道集成方法，用于解决工业网络物理系统（CPS）中AI/ML集成的碎片化问题，并在MicroFactory场景中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 工业领域CPS日益复杂，物联网（IoT）和工业物联网（IIoT）技术之间的碎片化（通信协议、数据格式、设备能力多样）阻碍了AI/ML技术的有效集成。现有方法通常耦合紧密，限制了AI功能的扩展性和复用性。

Method: 提出了一种模块化、可互操作的解决方案，利用数字孪生（DT）协调数据管理和智能增强，实现AI管道的零配置（ZeroConf）集成。该方法最小化了配置需求，并解耦了DT和AI组件的角色。

Result: 在MicroFactory场景中，该方法展示了对并发ML模型和动态数据处理的支持，有效地加速了复杂工业环境中智能服务的部署。

Conclusion: 通过数字孪生协调的零配置AI管道，可以有效克服工业CPS中AI/ML集成的碎片化挑战，实现AI功能的无缝集成和加速部署。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [18] [ReThinker: Scientific Reasoning by Rethinking with Guided Reflection and Confidence Control](https://arxiv.org/abs/2602.04496)
*Zhentao Tang,Yuqi Cui,Shixiong Kai,Wenqian Zhao,Ke Ye,Xing Li,Anxin Tian,Zehua Pei,Hui-Ling Zhen,Shoubo Hu,Xiaoguang Li,Yunhe Wang,Mingxuan Yuan*

Main category: cs.AI

TL;DR: 本文提出了ReThinker，一个能够进行科学推理的置信度感知代理框架，通过Solver-Critic-Selector架构动态分配计算资源，并采用逆向数据合成和自适应轨迹回收策略进行无监督训练，在HLE、GAIA和XBench等基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在科学推理方面存在挑战，尤其是在Humanity's Last Exam (HLE)等基准上，固定工具管道、脆弱的多智能体协调和低效的测试时间扩展限制了性能。

Method: ReThinker框架采用Solver-Critic-Selector架构，通过置信度感知来动态分配计算资源，自适应地调用工具、进行多维度反思和加权选择。此外，提出逆向数据合成和自适应轨迹回收策略，以实现无监督训练。

Result: ReThinker在HLE、GAIA和XBench上显著优于现有最先进的模型和系统，在专家级科学推理任务上取得了最先进的性能。

Conclusion: ReThinker通过其置信度感知和动态计算分配机制，有效地克服了现有语言模型在科学推理方面的局限性，并在多个基准测试中证明了其优越性。

Abstract: Expert-level scientific reasoning remains challenging for large language models, particularly on benchmarks such as Humanity's Last Exam (HLE), where rigid tool pipelines, brittle multi-agent coordination, and inefficient test-time scaling often limit performance. We introduce ReThinker, a confidence-aware agentic framework that orchestrates retrieval, tool use, and multi-agent reasoning through a stage-wise Solver-Critic-Selector architecture. Rather than following a fixed pipeline, ReThinker dynamically allocates computation based on model confidence, enabling adaptive tool invocation, guided multi-dimensional reflection, and robust confidence-weighted selection. To support scalable training without human annotation, we further propose a reverse data synthesis pipeline and an adaptive trajectory recycling strategy that transform successful reasoning traces into high-quality supervision. Experiments on HLE, GAIA, and XBench demonstrate that ReThinker consistently outperforms state-of-the-art foundation models with tools and existing deep research systems, achieving state-of-the-art results on expert-level reasoning tasks.

</details>


### [19] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 本文提出了一个生成式AI（GenAI）与问答论坛协作的框架，旨在解决GenAI抢占用户但又依赖论坛数据的问题。该框架通过模拟现实世界的复杂性，展示了即使存在激励错配，仍有可能实现可持续的知识共享。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（GenAI）一方面吸引了用户，减少了对传统问答（Q&A）论坛的访问，另一方面又依赖于这些论坛产生的数据来提升自身性能。这种矛盾促使研究者探索一种GenAI与论坛共存并相互促进的机制。

Method: 研究者提出了一个顺序交互框架，在该框架下，GenAI系统会向论坛提出问题，论坛可以选择发布部分问题。该框架考虑了非货币交换、信息不对称和激励不一致等复杂因素。研究人员通过使用真实Stack Exchange数据和常用的大型语言模型（LLMs）进行全面的、数据驱动的模拟来验证该框架。

Result: 模拟结果表明，尽管在现实场景中存在激励错配，参与者仍能获得接近理想情况（完全信息）下的一半效用。这证明了该框架在捕获实际交互动态方面的有效性。

Conclusion: 该研究证明了GenAI系统与人类知识平台之间存在可持续协作的可能性，并且可以在这种协作中有效地进行知识共享，即使在存在激励错配的情况下也能取得显著的成果。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [20] [Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration](https://arxiv.org/abs/2602.04575)
*Jiaheng Liu,Yuanxing Zhang,Shihao Li,Xinping Lei*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Vibe AIGC 的新内容生成范式，通过使用由用户（Commander）提供的高层“Vibe”来驱动代理（agent）的自主编排，从而克服了现有生成模型在满足用户意图方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI模型在模型能力上取得了巨大进展，但用户却难以有效地表达其高层次意图，导致了“意图-执行差距”，即用户意图与模型输出之间的不匹配。这种“可用性天花板”阻碍了AI的进一步应用。

Method: 引入 Vibe AIGC 范式，该范式采用代理式编排（agentic orchestration）的方式，将用户的“Vibe”（包含美学偏好、功能逻辑等高层信息）解构为一个中心化 Meta-Planner，由其生成可执行、可验证、自适应的代理工作流。这种方法从随机推理转向逻辑编排。

Result: Vibe AIGC 能够弥合人类想象与机器执行之间的差距，使AI从一个易出错的推理引擎转变为一个强大的系统级工程伙伴。

Conclusion: Vibe AIGC 范式有望重新定义人机协作经济，实现复杂、长周期数字资产的民主化创作，并推动AI在内容生成领域的应用迈上新台阶。

Abstract: For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.
  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.

</details>


### [21] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 本研究提出了一种名为WideSeek-R1的多智能体系统，通过宽度扩展（增加并行子智能体）来解决需要广泛信息检索的长时序问题，并利用多智能体强化学习进行训练，实验表明其性能可与参数量更大的单智能体模型相媲美，且随着并行子智能体数量的增加性能有所提升。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM研究主要集中于深度扩展（单个智能体解决长时序问题），但随着任务的扩展，瓶颈从个体能力转向组织能力。本研究旨在探索宽度扩展（多智能体系统）以解决广泛信息检索问题，并克服现有并行化效率低下的问题。

Method: 提出WideSeek-R1框架，采用领头智能体-子智能体架构，通过多智能体强化学习进行训练。系统共享同一LLM，但拥有独立的上下文和专门的工具。在一个包含20,000个广泛信息检索任务的数据集上进行了联合优化。

Result: WideSeek-R1-4B在WideSearch基准测试上取得了40.0%的F1分数，接近参数量更大的单智能体DeepSeek-R1-671B。随着并行子智能体数量的增加，WideSeek-R1-4B的性能呈现持续提升。

Conclusion: 宽度扩展（通过多智能体系统）是解决广泛信息检索任务的一种有效方法，WideSeek-R1框架通过可扩展的编排和并行执行，实现了与大型单智能体模型相当的性能，并展现出随并行性增强的优势。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [22] [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.AI

TL;DR: 本文对49篇关于LLM医疗代理的研究进行了系统性回顾，使用七维度分类法分析了其在认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型以及核心任务等方面的实现情况，揭示了当前LLM医疗代理在外部知识整合方面较为普遍，但在事件触发激活、漂移检测与缓解、以及治疗规划与处方等方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLM医疗代理的研究多为广泛的概述或聚焦于单一能力，缺乏一个统一的框架来理解和评估其在医疗工作中的应用，因此需要一个系统性的回顾和分类来弥合这一差距。

Method: 研究者使用了七维度分类法（包括认知能力、知识管理、交互模式、适应与学习、安全与伦理、框架类型、核心任务与子任务）和29个操作性子维度，通过明确的纳入和排除标准以及一个评估标准（完全实现、部分实现、未实现），对49篇相关研究进行了标注和量化分析，以总结各项能力和模式的普遍性和共现性。

Result: 分析显示，外部知识整合（知识管理）的实现率很高（约76%），而事件触发激活（交互模式）和漂移检测与缓解（适应与学习）的实现率极低（分别为92%和98%）。在架构方面，多智能体设计（框架类型）是主导模式（82%），而编排层大多处于部分实现状态。在核心任务方面，以信息为中心的能力（如医疗问答、决策支持）占主导，但治疗规划与处方等面向行动和发现的任务仍存在显著差距（59%未实现）。

Conclusion: LLM医疗代理在医疗领域展现出巨大潜力，但在实际应用中，其能力分布不均，存在明显的短板，尤其是在需要主动干预和持续适应的方面。未来的研究需要关注提升这些不足之处，以实现更全面和有效的医疗应用。

Abstract: Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).

</details>


### [23] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该论文质疑了AI能力呈指数增长的说法，认为现有数据不支持此观点，并提出了一种新的模型来分析AI能力的基线和推理能力，预测AI能力将在不久的将来达到拐点。


<details>
  <summary>Details</summary>
Motivation: 作者认为，近期关于AI能力呈指数增长的预测（如METR报告）可能存在数据支持不足的问题，因此希望通过分析现有数据并提出新的模型来质疑这些预测的有效性。

Method: 作者首先拟合了METR报告中的数据，试图证明指数增长模型不适用。随后，作者提出了一个更复杂的模型，将AI能力分解为基础能力和推理能力，并分析了它们各自的增长率，以支持AI能力将在近期经历拐点的假设。

Result: 作者发现，直接拟合METR报告的数据到sigmoid曲线，其拐点已经过去。作者提出的分解模型也表明，AI能力将在不久的将来达到拐点。

Conclusion: 该研究的结论是，AI能力呈现指数增长的说法缺乏足够的数据支持，并且现有的基于指数增长的AI能力预测可能过于乐观或存在缺陷。作者希望通过强调现有预测的脆弱性，来促使对AI能力发展轨迹进行更严谨的分析。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [24] [Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing](https://arxiv.org/abs/2602.04837)
*Zhaotian Weng,Antonis Antoniades,Deepak Nathani,Zhen Zhang,Xiao Pu,Xin Eric Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Group-Evolving Agents (GEA) 的新范式，它将一组智能体视为基本的进化单元，通过显式共享经验来促进开放式自我改进，并在编码任务上显著优于现有方法，同时展现出良好的跨模型适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有开放式自我进化方法存在探索多样性利用效率低下和进化分支孤立的问题，阻碍了智能体能力的持续提升和对人类干预的依赖。GEA旨在克服这些限制，实现更有效的自我改进。

Method: GEA 将一组智能体作为一个整体进行进化，允许智能体之间显式地共享和重用经验，克服了传统树状进化模式下探索多样性分散的问题。在编码基准测试中进行评估。

Result: GEA在SWE-bench Verified和Polyglot编码基准上显著优于最先进的自我进化方法（准确率分别为71.0% vs. 56.7%和88.3% vs. 68.3%），并能媲美或超越顶尖的人工设计的智能体框架。GEA能更有效地将早期探索性多样性转化为长期进展，并在相同进化智能体数量下获得更强的性能。此外，GEA在不同编码模型之间具有一致的迁移能力，并展现出更强的鲁棒性，平均只需1.4次迭代即可修复框架级错误，而其他自我进化方法需要5次。

Conclusion: GEA通过将一组智能体作为基本进化单元，并实现经验共享，有效地解决了现有开放式自我进化方法的局限性，实现了更高效的自我改进，并在多项挑战性编码任务上取得了优异的性能。GEA在跨模型适应性和鲁棒性方面也表现出色。

Abstract: Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.

</details>


### [25] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 本文通过对模型QwQ-32B进行机制分析，发现其在处理抽象推理问题时，能够逐步优化内部表征，形成关注结构而非具体动作名称的抽象编码，这种动态调整的表征被称为“流式推理表征”，是提升模型性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管长链思考（reasoning chains）的语言模型在抽象问题上表现出色，但其内部机制尚未被充分理解，研究者希望揭示其优越性能的根源。

Method: 研究者对专门训练用于生成长推理链的模型QwQ-32B进行了机制分析。他们在一个语义模糊的规划领域“Mystery Blocksworld”上，通过分析模型在推理过程中内部表征的变化，以及进行“引导实验”（steering experiments）来验证因果关系。

Result: 研究发现，QwQ-32B在推理过程中，其对动作和概念的内部表征会逐渐改进，形成关注结构而非具体动作名称的抽象编码。引导实验表明，注入更精炼的表征可以提高准确性，而符号化表征可以在不显著影响性能的情况下替代部分模糊编码。研究还发现，动态调整的token表征（流式推理表征）是驱动模型性能提升的因素之一。

Conclusion: 模型通过在上下文（in-context）中不断优化token表征，形成“流式推理表征”，是其在抽象问题上表现优异的关键机制。这些表征能够捕捉问题的结构信息，从而提升解决问题的能力。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [26] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文对3D高斯溅射（3DGS）场景合成的知识产权（IP）保护进行了首次系统性调研，提出了一个框架来分析其保护机制、范式和鲁棒性挑战，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 3DGS因其在实时3D场景合成中的广泛应用而具有商业价值，但其IP保护方面研究分散且缺乏统一的视角，因此需要进行系统性梳理和研究。

Method: 文章提出了一个自下而上的框架，该框架分析了（i）基于高斯的扰动机制，（ii）被动和主动的保护范式，以及（iii）生成式AI时代下的鲁棒性威胁。

Result: 研究揭示了3DGS IP保护在技术基础和鲁棒性表征方面存在的差距，并指出了进一步研究的机遇。同时，文章还指出了现有保护方法的局限性。

Conclusion: 本文系统性地 survey 了3DGS的IP保护研究，并提出了一种分析框架，为未来在鲁棒性、效率和保护范式方面的研究提供了方向，旨在实现可靠可信的3DGS资产IP保护。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

</details>


### [27] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 本文提出了一种名为 TruKAN 的新架构，它基于 KAN 结构并使用可学习的激活函数，通过用截断幂函数替换 B 样条基来提高准确性和训练速度，同时保持 KAN 的表达能力和可解释性。在计算机视觉任务上的实验表明，TruKAN 比其他 KAN 模型在准确性、计算效率和内存使用方面都有优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决计算效率与 Kolmogorov-Arnold Network (KAN) 原则之间的权衡问题，同时保持 KAN 的表达能力和可解释性。

Method: TruKAN 架构通过用截断幂函数替换 KAN 中的 B 样条基来实现。每个 TruKAN 层结合了截断幂项和多项式项，并使用共享或独立的结点。模型被集成到基于 EfficientNet-V2 的框架中，并在计算机视觉基准数据集上进行评估。使用混合优化和层归一化技术来改进训练。

Result: TruKAN 在准确性、计算效率和内存使用方面均优于其他 KAN 模型（MLP、KAN、SineKAN），尤其是在复杂的计算机视觉任务和深度架构上。

Conclusion: TruKAN 是一种更具解释性、更高效且性能更优的 KAN 变体，能够有效地平衡近似能力与透明度，并在实际应用中展现出超越先前研究的优势。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

</details>


### [28] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出了一种名为 DiGAN 的扩散引导注意力网络，用于在数据量有限的情况下，通过生成逼真的纵向脑成像轨迹来辅助早期阿尔茨海默病（AD）的诊断，并取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在早期阿尔茨海默病（AD）诊断上面临挑战，主要是因为早期结构性脑变化进展不明显且时间不规律，且现有方法需要大量纵向数据集，难以处理真实临床数据中固有的时间连续性和模态不规则性。

Method: 提出了一种名为 Diffusion-Guided Attention Network (DiGAN) 的方法，该方法整合了潜在扩散模型和一个注意力引导的卷积网络。扩散模型用于从有限的训练数据中合成逼真的纵向神经影像轨迹，从而丰富时间上下文并提高对不均匀时间点数据的鲁棒性。注意力-卷积层则用于捕捉区分认知正常、轻度认知障碍和主观认知衰退受试者的结构-时间模式。

Result: 在合成数据集和 ADNI 数据集上的实验表明，DiGAN 的性能优于现有的最先进方法。

Conclusion: DiGAN 通过生成逼真的纵向神经影像轨迹并捕捉区分性的结构-时间模式，在数据量有限的情况下，显示出其在早期阿尔茨海默病检测方面的潜力。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

</details>


### [29] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: 提出了一种名为PriorProbe的新方法，用于精确获取个体层面的认知先验，并将其整合到深度神经网络中，以提高其对模糊刺激的预测能力，并在面部表情识别任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的个体化神经网络方法在精确提取认知先验时存在困难，要么无法唯一识别，要么引入系统性偏差。

Method: 开发了一种基于MCMC with People（Markov Chain Monte Carlo with People）的名为PriorProbe的新方法，用于恢复精细的、个体特定的认知先验。在面部表情识别任务中，将PriorProbe恢复的先验与最先进的神经网络结合。

Result: PriorProbe导出的先验显著提高了神经网络预测个体对模糊刺激分类的能力，优于单独的神经网络和替代的先验来源，同时保持了网络对真实标签的推理。

Conclusion: PriorProbe提供了一个通用且可解释的框架，用于个性化深度神经网络。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

</details>


### [30] [Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity](https://arxiv.org/abs/2602.04162)
*Chenhe Du,Qing Wu,Xuanyu Tian,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为“跨切片一致性随机性”（ISCS）的新策略，用于在利用2D扩散模型重建3D医学图像时，解决重建切片之间的不连续性问题，而无需额外的损失项或优化步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的基于2D扩散模型进行3D医学图像重建的方法，由于扩散采样的固有随机性，会导致重建的3D体积在切片之间出现严重的不连续性。现有的正则化方法会引入敏感的超参数并可能导致过度平滑。

Method: 提出了一种名为“跨切片一致性随机性”（ISCS）的策略，通过控制扩散采样过程中随机噪声分量的一致性，来对齐采样轨迹，从而鼓励切片间的一致性。该方法是即插即用的，可以集成到任何基于2D训练的扩散模型3D重建流程中，且无需额外的计算成本。

Result: 在多个医学成像问题上的实验表明，ISCS能够有效提高基于2D扩散模型的3D医学成像问题的性能，有效解决了切片间不连续性的问题。

Conclusion: 控制跨切片随机性是实现高保真3D医学成像的一种原则性且在实践中具有吸引力的方法，特别是当利用2D扩散先验时。

Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS

</details>


### [31] [SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation](https://arxiv.org/abs/2602.04712)
*David F. Ramirez,Tim Overman,Kristen Jaskie,Joe Marvin,Andreas Spanias*

Main category: cs.CV

TL;DR: 提出了一种名为SAR-RAG的视觉上下文图像检索增强生成AI代理，用于合成孔径雷达（SAR）自动目标识别（ATR），通过结合多模态大语言模型（MLLM）和向量数据库，提高了车辆识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达（SAR）图像中的军事车辆难以区分，需要改进区分和识别车辆类型、特征和尺寸的方法。

Method: 提出SAR-RAG方法，结合了多模态大语言模型（MLLM）和存储语义嵌入的向量数据库，以支持上下文相关的图像示例搜索。通过检索具有已知真实目标类型的历史图像示例，并将其与MLLM结合，作为一种内存库来支持ATR。

Result: SAR-RAG系统通过检索相似的车辆类别，提高了ATR预测准确性。在搜索和检索指标、分类准确率以及车辆尺寸的数值回归方面都显示出改进。

Conclusion: SAR-RAG作为一种附加的ATR记忆库，能够增强MLLM在SAR自动目标识别任务中的性能，提高识别准确性。

Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.

</details>


### [32] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 本研究开发了一个可解释的计算机视觉框架，用于检测和评估增材制造零件中的内部孔隙缺陷，并量化了影响孔隙危险度的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化缺陷检测方法缺乏可解释性，阻碍了工程师理解缺陷的危险度预测的物理基础，从而限制了增材制造在工业中的应用。

Method: 1. 将灰度切片重建成三维体数据；2. 使用基于强度的阈值分割和连通组件分析来识别孔隙；3. 提取孔隙的几何描述符（尺寸、长宽比、范围、空间位置）；4. 构建孔隙交互网络；5. 使用机器学习模型预测孔隙危险度；6. 利用SHAP分析量化特征贡献。

Result: 归一化的表面距离是预测孔隙危险度的最重要因素，其重要性远超其他描述符。孔隙尺寸和几何参数对危险度的影响微乎其微。表面距离与危险度之间存在很强的负相关关系，表明存在由边界驱动的失效机制。

Conclusion: 提出的可解释框架能够透明地评估缺陷，并为优化增材制造工艺和质量控制提供可操作的见解，特别强调了孔隙与试样边界的距离对失效行为的关键影响。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

</details>


### [33] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了4DPC$^2$hat，首个用于动态点云理解的多模态大语言模型（MLLM），并构建了首个大规模跨模态数据集4DPC$^2$hat-200K。该模型利用Mamba增强的时序推理能力，并采用一种失效感知引导学习策略来改进模型性能，在动作理解和时序推理方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要关注静态点云，对动态点云序列的理解仍是未被探索的领域。这主要是由于缺乏大规模跨模态数据集以及在时空背景下对运动建模的困难。

Method: 构建了名为4DPC$^2$hat-200K的大规模跨模态数据集，包含拓扑一致的4D点云构造和两级字幕生成。核心框架是一个Mamba增强的时序推理MLLM，用于捕捉点云序列中的长程依赖和动态模式。此外，提出了一种失效感知引导学习策略，通过迭代识别模型缺陷并生成有针对性的QA监督来持续增强推理能力。

Result: 实验表明，4DPC$^2$hat在动作理解和时序推理方面显著优于现有模型。

Conclusion: 4DPC$^2$hat是首个专为动态点云理解设计的MLLM，并通过大规模数据集和创新的模型设计，为4D动态点云理解奠定了坚实的基础。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

</details>


### [34] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 该研究提出了一个名为MQA-RefAVS的新任务，用于评估语言引导的视听分割（Ref-AVS）的分割掩模质量，无需在推理时使用真实标注。同时，他们构建了一个名为MQ-RAVSBench的基准数据集，并提出了一个基于多模态大语言模型（MLLM）的名为MQ-Auditor的评估器，用于量化和定性评估掩模质量。


<details>
  <summary>Details</summary>
Motivation: 现有的语言引导的视听分割（Ref-AVS）研究主要集中在生成分割掩模，而对掩模质量进行丰富和可解释的诊断则被忽视了。因此，需要一个在推理时不需要真实标注的掩模质量评估方法。

Method: 研究提出了MQA-RefAVS任务，要求模型根据视听语言输入和候选分割掩模，估计其与未观测到的真实标注的IoU（交并比），识别错误类型，并推荐质量控制决策。为支持该任务，构建了MQ-RAVSBench基准数据集，并提出了基于MLLM的MQ-Auditor模型，该模型显式地利用多模态线索和掩模信息进行质量评估。

Result: MQ-Auditor在MQA-RefAVS任务上表现优于现有的开源和商业MLLM。实验证明，MQ-Auditor可以集成到现有的Ref-AVS系统中，用于检测分割失败并支持下游分割改进。

Conclusion: 该研究成功地定义和解决了MQA-RefAVS问题，并通过MQ-RAVSBench和MQ-Auditor展示了一种有效的解决方案，为Ref-AVS系统的鲁棒性和可用性提供了重要的补充。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

</details>


### [35] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 提出了一种名为GPAIR的超快速迭代重建方法，用于三维光声断层成像（PACT），通过使用高斯核和GPU加速，实现了比传统方法快几个数量级的重建速度，使得亚秒级的三维PACT重建成为可能。


<details>
  <summary>Details</summary>
Motivation: 传统的迭代重建（IR）算法虽然能有效校正PACT中的伪影，但计算时间过长，尤其是在进行大规模三维成像时，严重限制了其在实际应用中的可行性。

Method: GPAIR方法将传统空间网格替换为连续的各向同性高斯核，并推导出了压力波的解析闭式表达式。同时，利用GPU加速的可微分Triton算子来实现计算。

Result: GPAIR在包含840万个体素的三维目标动物实验中，实现了前所未有的超快速重建速度，仅需亚秒级时间。

Conclusion: GPAIR是一种革命性的超快速三维PACT图像重建方法，能够实现近乎实时的大规模三维PA重建，极大地推动了三维PACT向临床应用的迈进。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

</details>


### [36] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 研究表明，利用Vision Transformer (ViT) 模型和降维及聚类技术，可以高效地将大量未标记的动物图像自动聚类到物种级别，甚至能识别物种内的个体差异，为生态学研究提供了新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 人工标注动物图像耗时费力，阻碍了生态学领域生物多样性监测的规模和效率。该研究旨在探索先进的Vision Transformer (ViT) 基础模型是否能够直接将大量未标记的动物图像聚类到物种级别，以解决这一瓶颈。

Method: 该研究建立了一个全面的基准测试框架，评估了五种ViT模型与五种降维技术和四种聚类算法（两种监督式，两种无监督式）的组合。测试覆盖了60个物种（30种哺乳动物和30种鸟类），每个物种使用随机抽取的200张已验证图像。研究人员还分析了聚类在物种级别上的成功与失败情况，以及能否揭示性别、年龄或表型变异等生态学意义。

Result: 研究结果显示，使用DINOv3嵌入、t-SNE降维和监督式层次聚类方法，实现了接近完美的物种级别聚类（V-measure: 0.958）。无监督方法也取得了具有竞争力的性能（0.943），且无需预先了解物种信息，仅将1.14%的图像标记为需要专家审查的异常值。此外，研究证明了该方法在长尾分布的物种数据下具有鲁棒性，并且通过有意地过度聚类，能够可靠地提取物种内的变异信息，如年龄、性别二态性和毛皮差异。

Conclusion: Vision Transformer (ViT) 基础模型结合降维和聚类技术，能够有效地实现大规模动物图像的自动物种级别聚类，并能识别物种内的生态学变异。研究提供了一个开源基准测试工具包，并为生态学家选择合适的方法提供了建议，有望显著提高生物多样性监测的效率和规模。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

</details>


### [37] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为 NH-Fair 的标准化基准，用于在视觉模型和大型视觉语言模型 (LVLMs) 中评估和缓解偏见。研究发现，经过良好调整的经验风险最小化 (ERM) 方法通常优于许多现有的偏见消除方法，而数据增强是一种有前景的策略。此外，LVLMs 在提高准确性的同时，仍然存在群体差异，且模型规模的提升带来的好处不如架构或训练协议的选择。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型在现实世界数据上训练时，往往会继承和放大对某些社会群体的偏见，这引发了对其大规模部署的担忧。然而，由于数据集不一致、公平性指标不统一、视觉模型与多模态模型评估孤立以及超参数调优不足等问题，对比不同偏见缓解方法的有效性变得困难。

Method: 本文提出了 NH-Fair，一个统一的基准，用于在标准化数据、指标和训练协议下，对视觉模型和大型视觉语言模型 (LVLMs) 在监督学习和零样本学习场景下进行公平性评估。研究进行了系统的 ERM 调优，并评估了各种偏见消除方法，包括数据增强。

Result: 1. 系统的 ERM 调优研究确定了对效用和差异有重要影响的训练选择，并为实践者提供了减少超参数调优空间的实用指南。
2. 许多现有的偏见消除方法并不比精心调整的 ERM 基线表现更好，而一种复合数据增强方法能够稳定地提升公平性而又不牺牲效用。
3. LVLMs 尽管平均准确率更高，但仍然存在子群体差异，并且规模效应带来的收益通常小于架构或训练协议选择带来的收益。

Conclusion: NH-Fair 提供了一个可复现、考虑调优的流程，用于进行严格的、关注危害的公平性评估。研究表明，精心调整的 ERM 方法和数据增强是实现公平性和效用平衡的有效策略，并且在评估和改进大型模型时，需要关注训练协议和架构选择。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

</details>


### [38] [HY3D-Bench: Generation of 3D Assets](https://arxiv.org/abs/2602.03907)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Dongyuan Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jiaao Yu,Jiachen Xu,Jingwei Huang,Kunhong Li,Lifu Wang,Linus,Penghao Wang,Qingxiang Lin,Ruining Tang,Xianghui Yang,Yang Li,Yirui Guan,Yunfei Zhao,Yunhan Yang,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: 本文提出了HY3D-Bench，一个包含250k高质量3D对象、结构化部件分解以及125k合成资产的开放数据集和工具集，旨在解决3D生成中的数据处理瓶颈，促进3D感知、机器人和数字内容创作领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前3D内容创作领域面临严重的数据处理瓶颈，阻碍了神经表示和生成模型的发展。

Method: 1. 收集并处理了250k个高质量3D对象，提供训练所需的网格和渲染图。2. 引入了结构化的部件级分解，实现精细感知和可控编辑。3. 构建了可扩展的AIGC合成流程，生成125k个合成资产以弥补长尾类别的分布差距。

Result: 通过训练Hunyuan3D-2.1-Small模型验证了HY3D-Bench的有效性。

Conclusion: HY3D-Bench通过提供高质量、结构化且多样化的3D数据， democratizes 3D生成，有望加速3D感知、机器人和数字内容创作的创新。

Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.

</details>


### [39] [Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition](https://arxiv.org/abs/2602.03913)
*Qiuming Luo,Tao Zeng,Feng Li,Heming Liu,Rui Mao,Chang Kong*

Main category: cs.CV

TL;DR: 提出了一种熵感知结构对齐网络，通过信息论模型解决零样本手写汉字识别中存在的结构和信息密度问题，提高了识别性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本手写汉字识别方法将汉字视为扁平的部首序列，忽略了其层次结构和不同组件的信息密度不均的问题。

Method: 提出熵感知结构对齐网络，包括：1. 信息熵先验，通过乘法交互动态调节位置嵌入，突出具有辨别力的部首；2. 双视图部首树，提取多粒度结构特征，并通过自适应的 Sigmoid 门控网络编码全局布局和局部空间作用；3. Top-K 语义特征融合，利用语义邻居的质心增强解码过程，纠正视觉歧义。

Result: 在零样本手写汉字识别任务中取得了新的最先进性能，显著优于现有的基于 CLIP 的基线方法。该框架在数据效率方面表现出色，能够以最少的支持样本快速适应。

Conclusion: 所提出的熵感知结构对齐网络能够有效弥合视觉-语义鸿沟，通过信息论建模解决了零样本手写汉字识别中的关键挑战，并在性能和数据效率方面均取得显著提升。

Abstract: Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of different components. To address these limitations, we propose an Entropy-Aware Structural Alignment Network that bridges the visual-semantic gap through information-theoretic modeling. First, we introduce an Information Entropy Prior to dynamically modulate positional embeddings via multiplicative interaction, acting as a saliency detector that prioritizes discriminative roots over ubiquitous components. Second, we construct a Dual-View Radical Tree to extract multi-granularity structural features, which are integrated via an adaptive Sigmoid-based gating network to encode both global layout and local spatial roles. Finally, a Top-K Semantic Feature Fusion mechanism is devised to augment the decoding process by utilizing the centroid of semantic neighbors, effectively rectifying visual ambiguities through feature-level consensus. Extensive experiments demonstrate that our method establishes new state-of-the-art performance, significantly outperforming existing CLIP-based baselines in the challenging zero-shot setting. Furthermore, the framework exhibits exceptional data efficiency, demonstrating rapid adaptability with minimal support samples.

</details>


### [40] [Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science](https://arxiv.org/abs/2602.03915)
*Levi Lingsch,Georgios Kissas,Johannes Jakubik,Siddhartha Mishra*

Main category: cs.CV

TL;DR: 本研究提出了一种名为 Phaedra 的新图像分词器，它在科学图像（尤其是涉及偏微分方程的数据）的分词和重构方面优于现有方法，并且在处理不同类型和复杂性的任务时表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分词器主要针对视觉感知设计，可能不适合科学图像，因为科学图像具有较大的动态范围，并且分词嵌入需要保留物理和光谱特性。因此，有必要探索更适合科学图像的分词方法。

Method: 研究人员评估了一系列图像分词器在测量物理和光谱空间中偏微分方程（PDE）属性保真度方面的准确性。基于现有分词器在捕捉细节和精确幅度方面的不足，提出了一种名为 Phaedra 的新分词器，该分词器借鉴了经典形状增益量化和 Proper Orthogonal Decomposition (POD) 的思想。

Result: Phaedra 在一系列 PDE 数据集上始终提高了重构精度。此外，Phaedra 在处理不同复杂度的任务时表现出强大的分布外泛化能力，包括具有不同条件的已知 PDE、未知 PDE 以及真实的地球观测和天气数据。

Conclusion: Phaedra 分词器在科学图像处理，特别是涉及 PDE 的任务中，能够更准确地保留物理和光谱信息，从而在重构和泛化能力上优于现有方法。

Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.

</details>


### [41] [SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?](https://arxiv.org/abs/2602.03916)
*Azmine Toushik Wasi,Wahid Faisal,Abdur Rahman,Mahfuz Ahmed Anik,Munem Shahriar,Mohsin Mahmud Topu,Sadia Tasnim Meem,Rahatun Nesa Priti,Sabrina Afroz Mitu,Md. Iqramul Hoque,Shahriyar Zaman Ridoy,Mohammed Eunus Ali,Majd Hawasly,Mohammad Raza,Md Rizwan Parvez*

Main category: cs.CV

TL;DR: 本研究提出了 SpatiaLab，一个用于评估视觉语言模型（VLM）在真实、无约束环境中空间推理能力的基准。该基准包含 1400 个问答对，涵盖六大类空间推理任务。实验结果表明，当前最先进的 VLM 在空间推理方面与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间推理方面存在挑战，现有基准多依赖于合成或 LLM 生成的环境，未能捕捉真实世界的复杂性、视觉噪声和多样的空间关系。

Method: 构建了一个包含 1400 个问答对的 SpatiaLab 基准，覆盖相对位置、深度与遮挡、方向、大小与尺度、空间导航和三维几何等六大类任务，每类任务细分为五个子类。支持选择题和开放式问答两种评估方式。在多个 SOTA VLM 上进行了实验评估。

Result: 在选择题任务中，InternVL3.5-72B 的准确率为 54.93%，远低于人类的 87.57%。在开放式问答任务中，所有模型的性能下降约 10-25%，GPT-5-mini 最高得分 40.93%，低于人类的 64.93%。模型在处理复杂空间关系、深度感知、导航和三维几何方面存在局限性。

Conclusion: SpatiaLab 提供了一个真实世界、多样化的评估框架，暴露了当前 VLM 在空间推理方面的关键挑战和改进机会，旨在指导未来研究朝着鲁棒、与人类对齐的空间理解方向发展。

Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.

</details>


### [42] [Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers](https://arxiv.org/abs/2602.03918)
*Peihao Xiang,Kaida Wu,Ou Bai*

Main category: cs.CV

TL;DR: 本文提出了一种名为Gardener的数据无关、单次、块级剪枝方法，通过计算预训练Transformer块权重的信​​息熵来识别冗余块，无需访问数据即可准确估计块的重要性，从而在不显著影响下游性能的情况下实现高效的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 自监督视觉Transformer模型规模庞大，在资源受限环境下的部署和迁移学习面临挑战。研究Transformer块的重要性对于实现高效模型压缩至关重要。

Method: 提出Gardener剪枝方法，通过计算预训练Transformer块权重的信​​息熵来估计块的重要性。该方法无需访问数据，并与通过迭代块移除和微调获得的Oracle敏感度高度相关。将Gardener应用于VideoMAE-B模型，并在多个剪枝比例和下游视频识别基准上进行评估。

Result: Gardener剪枝方法在计算开销极小的情况下，能够持续匹配或超越现有的数据无关剪枝基线，并接近基于敏感度的剪枝效果。即使剪枝高达91.7%的块，模型仍能保持具有竞争力的迁移性能。

Conclusion: 自监督视觉Transformer模型存在显著的块级冗余。信息论分析为模型压缩和资源高效迁移学习提供了一种原则性且高效的途径，Gardener剪枝方法是一种有效的数据无关模型压缩技术。

Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.

</details>


### [43] [TiCLS : Tightly Coupled Language Text Spotter](https://arxiv.org/abs/2602.04030)
*Leeje Jang,Yijun Lin,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: 提出了一种名为TiCLS的端到端场景文本检测和识别方法，通过引入字符级预训练语言模型来增强语言学知识的整合，并在ICDAR 2015和Total-Text数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本识别方法主要依赖视觉线索，忽略了外部语言学知识的潜在优势。虽然有过集成语言模型的尝试，但存在与单词级别粒度不匹配或未真正利用外部知识的问题。

Method: 提出TiCLS，一个端到端的文本识别器，通过一个语言学解码器显式地整合来自字符级预训练语言模型（PLM）的外部语言学知识。该解码器融合视觉和语言特征，并可以通过预训练语言模型进行初始化，以处理模糊或片段化的文本。

Result: 在ICDAR 2015和Total-Text数据集上的实验表明，TiCLS取得了最先进的性能。

Conclusion: 通过预训练语言模型指导的语言学整合对于场景文本识别非常有效，TiCLS通过整合字符级PLM的语言学知识，能够鲁棒地识别模糊或片段化的文本。

Abstract: Scene text spotting aims to detect and recognize text in real-world images, where instances are often short, fragmented, or visually ambiguous. Existing methods primarily rely on visual cues and implicitly capture local character dependencies, but they overlook the benefits of external linguistic knowledge. Prior attempts to integrate language models either adapt language modeling objectives without external knowledge or apply pretrained models that are misaligned with the word-level granularity of scene text. We propose TiCLS, an end-to-end text spotter that explicitly incorporates external linguistic knowledge from a character-level pretrained language model. TiCLS introduces a linguistic decoder that fuses visual and linguistic features, yet can be initialized by a pretrained language model, enabling robust recognition of ambiguous or fragmented text. Experiments on ICDAR 2015 and Total-Text demonstrate that TiCLS achieves state-of-the-art performance, validating the effectiveness of PLM-guided linguistic integration for scene text spotting.

</details>


### [44] [A Parameterizable Convolution Accelerator for Embedded Deep Learning Applications](https://arxiv.org/abs/2602.04044)
*Panagiotis Mousouliotis,Georgios Keramidas*

Main category: cs.CV

TL;DR: 提出了一种基于高层次综合（HLS）的硬件-软件协同设计方法，用于参数化CNN FPGA加速器，以优化性能、延迟、功耗、面积和成本等多个约束。


<details>
  <summary>Details</summary>
Motivation: 现实中的嵌入式深度学习应用需要在性能之外，同时满足延迟、功耗、面积和成本等多方面的约束，而传统的以最大化GOPS为目标的CNN加速器设计方法无法满足这些需求。

Method: 使用高层次综合（HLS）工具描述CNN加速器，通过参数化设计实现多约束下的有效优化，并采用硬件-软件（HW/SW）协同设计方法。

Result: 提出的参数化设计方法在多个设计约束下优于非参数化设计方法，并证明了该方法易于扩展到其他类型的深度学习应用。

Conclusion: 基于HLS的硬件-软件协同设计方法能够有效地参数化CNN FPGA加速器，以满足现实中嵌入式深度学习应用的多重约束，并提供了优于传统方法的性能和可扩展性。

Abstract: Convolutional neural network (CNN) accelerators implemented on Field-Programmable Gate Arrays (FPGAs) are typically designed with a primary focus on maximizing performance, often measured in giga-operations per second (GOPS). However, real-life embedded deep learning (DL) applications impose multiple constraints related to latency, power consumption, area, and cost. This work presents a hardware-software (HW/SW) co-design methodology in which a CNN accelerator is described using high-level synthesis (HLS) tools that ease the parameterization of the design, facilitating more effective optimizations across multiple design constraints. Our experimental results demonstrate that the proposed design methodology is able to outperform non-parameterized design approaches, and it can be easily extended to other types of DL applications.

</details>


### [45] [AnyStyle: Single-Pass Multimodal Stylization for 3D Gaussian Splatting](https://arxiv.org/abs/2602.04043)
*Joanna Kaleta,Bartosz Świrta,Kacper Kania,Przemysław Spurek,Marek Kowalski*

Main category: cs.CV

TL;DR: AnyStyle是一个用于3D重建和风格化的新框架，它通过文本或图像实现零样本的风格迁移，并且可以集成到现有的3DGS模型中，同时保持高质量的几何重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法在风格迁移方面缺乏灵活性和可控性，通常依赖于基于图像的条件，这限制了其应用。研究者希望开发一种更具通用性和可控性的方法。

Method: 该研究提出了一种名为AnyStyle的模块化框架，它支持文本和视觉风格输入，能够与现有的前馈3D重建模型（如3D Gaussian Splatting）集成，实现姿态无关的、零样本的风格化3D重建。

Result: AnyStyle在风格可控性上优于现有前馈风格化方法，同时保持了高质量的几何重建。用户研究表明，其风格化质量优于当前最先进的方法。

Conclusion: AnyStyle成功实现了姿态无关的、零样本的3D重建和风格化，通过多模态条件（文本和图像）提供了更灵活和可控的风格迁移能力，并且易于集成到现有框架中。

Abstract: The growing demand for rapid and scalable 3D asset creation has driven interest in feed-forward 3D reconstruction methods, with 3D Gaussian Splatting (3DGS) emerging as an effective scene representation. While recent approaches have demonstrated pose-free reconstruction from unposed image collections, integrating stylization or appearance control into such pipelines remains underexplored. Existing attempts largely rely on image-based conditioning, which limits both controllability and flexibility. In this work, we introduce AnyStyle, a feed-forward 3D reconstruction and stylization framework that enables pose-free, zero-shot stylization through multimodal conditioning. Our method supports both textual and visual style inputs, allowing users to control the scene appearance using natural language descriptions or reference images. We propose a modular stylization architecture that requires only minimal architectural modifications and can be integrated into existing feed-forward 3D reconstruction backbones. Experiments demonstrate that AnyStyle improves style controllability over prior feed-forward stylization methods while preserving high-quality geometric reconstruction. A user study further confirms that AnyStyle achieves superior stylization quality compared to an existing state-of-the-art approach. Repository: https://github.com/joaxkal/AnyStyle.

</details>


### [46] [Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs](https://arxiv.org/abs/2602.04046)
*Shikha Dubey,Patricia Raciti,Kristopher Standish,Albert Juan Ramon,Erik Ames Burlingame*

Main category: cs.CV

TL;DR: 提出了一种快速、无监督的框架，结合使用下采样组织掩码和形变度量来评估配准后的 H&E 和 IHC 全切片图像 (WSI) 对的配准质量 (RQA)。


<details>
  <summary>Details</summary>
Motivation: 现有的 WSI 配准质量评估方法（例如，使用标注的标志点或基于强度的相似性度量）通常耗时、不可靠且计算密集，限制了其大规模应用，而高保真度的配准对于集成分子分析至关重要。

Method: 该框架结合了两种度量：1. 基于掩码的度量，用于评估全局结构对应性；2. 基于形变的度量，用于评估局部平滑性、连续性和变换的真实性。

Result: 在多种 IHC 标记和多专家评估的验证中，该自动度量方法与人类评估表现出很强的相关性。

Conclusion: 在缺乏真实情况标注的情况下，该框架能够提供可靠、实时的 RQA，具有高保真度和最少的计算资源，适用于数字病理学中的大规模质量控制。

Abstract: High-fidelity registration of histopathological whole slide images (WSIs), such as hematoxylin & eosin (H&E) and immunohistochemistry (IHC), is vital for integrated molecular analysis but challenging to evaluate without ground-truth (GT) annotations. Existing WSI-level assessments -- using annotated landmarks or intensity-based similarity metrics -- are often time-consuming, unreliable, and computationally intensive, limiting large-scale applicability. This study proposes a fast, unsupervised framework that jointly employs down-sampled tissue masks- and deformations-based metrics for registration quality assessment (RQA) of registered H&E and IHC WSI pairs. The masks-based metrics measure global structural correspondence, while the deformations-based metrics evaluate local smoothness, continuity, and transformation realism. Validation across multiple IHC markers and multi-expert assessments demonstrate a strong correlation between automated metrics and human evaluations. In the absence of GT, this framework offers reliable, real-time RQA with high fidelity and minimal computational resources, making it suitable for large-scale quality control in digital pathology.

</details>


### [47] [Artifact Removal and Image Restoration in AFM:A Structured Mask-Guided Directional Inpainting Approach](https://arxiv.org/abs/2602.04051)
*Juntao Zhang,Angona Biswas,Jaydeep Rade,Charchit Shukla,Juan Ren,Anwesha Sarkar,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CV

TL;DR: 本文提出了一种用于原子显微镜（AFM）图像伪影检测和修复的轻量级、全自动框架，通过分类、语义分割、定向插值和高斯平滑来有效去除伪影并保留纳米级结构细节。


<details>
  <summary>Details</summary>
Motivation: AFM成像输出常受环境噪声、扫描缺陷和针尖-样品相互作用引入的伪影影响，影响成像质量。

Method: 采用分类模型检测伪影，然后使用轻量级语义分割网络生成伪影掩码，并根据结构方向自适应扩展掩码，最后通过定向邻域插值和局部高斯平滑进行修复。系统集成到具有GUI的用户友好界面中。

Result: 实验证明该方法能够有效去除伪影，同时保留纳米级结构细节，实现高保真AFM数据解释。

Conclusion: 提出的框架提供了一种鲁棒、几何感知的AFM图像伪影检测和修复解决方案，能够显著提高AFM图像的质量和可靠性。

Abstract: Atomic Force Microscopy (AFM) enables high-resolution surface imaging at the nanoscale, yet the output is often degraded by artifacts introduced by environmental noise, scanning imperfections, and tip-sample interactions. To address this challenge, a lightweight and fully automated framework for artifact detection and restoration in AFM image analysis is presented. The pipeline begins with a classification model that determines whether an AFM image contains artifacts. If necessary, a lightweight semantic segmentation network, custom-designed and trained on AFM data, is applied to generate precise artifact masks. These masks are adaptively expanded based on their structural orientation and then inpainted using a directional neighbor-based interpolation strategy to preserve 3D surface continuity. A localized Gaussian smoothing operation is then applied for seamless restoration. The system is integrated into a user-friendly GUI that supports real-time parameter adjustments and batch processing. Experimental results demonstrate the effective artifact removal while preserving nanoscale structural details, providing a robust, geometry-aware solution for high-fidelity AFM data interpretation.

</details>


### [48] [Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal](https://arxiv.org/abs/2602.04053)
*Rio Aguina-Kang,Kevin James Blackburn-Matzen,Thibault Groueix,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: SeeingThroughClutter 是一种通过迭代移除前景物体并进行3D重建来从单张图像中构建结构化3D表示的新方法，无需特定任务的训练，并能利用大型视觉模型（VLMs）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂、遮挡和混乱的场景中，依赖语义分割和深度估计等中间任务表现不佳，因此需要一种更鲁棒的方法。

Method: 提出了一种迭代物体移除和重建流程，利用VLMs作为协调者，通过检测、分割、移除物体和3D拟合，逐个处理前景物体。这使得后续物体的分割更加清晰，即使在高度遮挡的场景下也是如此。

Result: 该方法在3D-Front和ADE20K数据集上展示了最先进的鲁棒性。移除物体有助于提高后续物体检测和分割的准确性。

Conclusion: SeeingThroughClutter 通过迭代物体移除和3D重建，能够有效地从单张图像中构建出结构化的3D表示，尤其擅长处理复杂和遮挡场景，并且可以直接受益于基础模型的进步，无需额外的任务训练。

Abstract: We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/

</details>


### [49] [iSight: Towards expert-AI co-assessment for improved immunohistochemistry staining interpretation](https://arxiv.org/abs/2602.04063)
*Jacob S. Leiby,Jialu Yao,Pan Lu,George Hu,Anna Davidian,Shunsuke Koga,Olivia Leung,Pravin Patel,Isabella Tondi Resta,Rebecca Rojansky,Derek Sung,Eric Yang,Paul J. Zhang,Emma Lundberg,Dokyoon Kim,Serena Yeung-Levy,James Zou,Thomas Montine,Jeffrey Nirschl,Zhi Huang*

Main category: cs.CV

TL;DR: 本文提出了HPA10M数据集和iSight模型，用于自动化免疫组织化学（IHC）染色评估。iSight通过结合视觉特征和组织元数据，同时预测染色强度、位置、数量、组织类型和恶性状态，并在多项评估中表现优于现有模型，并能辅助病理学家提高诊断一致性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在H&E染色切片上的应用前景广阔，但由于领域特定的差异，其在免疫组织化学（IHC）染色切片上的应用受到限制。需要开发专门针对IHC染色的AI模型来支持病理诊断。

Method: 构建了包含10,495,672张IHC图像及元数据的HPA10M数据集。基于此数据集，开发了一个名为iSight的多任务学习框架。iSight结合了全切片图像的视觉特征和组织元数据，利用token级注意力机制，同时预测染色强度、位置、数量、组织类型和恶性状态。

Result: iSight模型在未见过的数据集上，位置预测准确率达到85.5%，强度为76.6%，数量为75.7%，均优于微调后的基础模型（PLIP, CONCH）。该模型预测结果校准良好。在与八名病理学家的用户研究中，iSight在IHC图像评估中的表现优于病理学家初始评估，并且AI辅助能提高病理学家之间的一致性。

Conclusion: 该研究为开发能够提高IHC诊断准确性的AI系统奠定了基础。iSight模型展示了其在临床工作流程中提高IHC评估一致性和可靠性的潜力，并表明专家-AI协同评估可以提升IHC判读质量。

Abstract: Immunohistochemistry (IHC) provides information on protein expression in tissue sections and is commonly used to support pathology diagnosis and disease triage. While AI models for H\&E-stained slides show promise, their applicability to IHC is limited due to domain-specific variations. Here we introduce HPA10M, a dataset that contains 10,495,672 IHC images from the Human Protein Atlas with comprehensive metadata included, and encompasses 45 normal tissue types and 20 major cancer types. Based on HPA10M, we trained iSight, a multi-task learning framework for automated IHC staining assessment. iSight combines visual features from whole-slide images with tissue metadata through a token-level attention mechanism, simultaneously predicting staining intensity, location, quantity, tissue type, and malignancy status. On held-out data, iSight achieved 85.5\% accuracy for location, 76.6\% for intensity, and 75.7\% for quantity, outperforming fine-tuned foundation models (PLIP, CONCH) by 2.5--10.2\%. In addition, iSight demonstrates well-calibrated predictions with expected calibration errors of 0.0150-0.0408. Furthermore, in a user study with eight pathologists evaluating 200 images from two datasets, iSight outperformed initial pathologist assessments on the held-out HPA dataset (79\% vs 68\% for location, 70\% vs 57\% for intensity, 68\% vs 52\% for quantity). Inter-pathologist agreement also improved after AI assistance in both held-out HPA (Cohen's $κ$ increased from 0.63 to 0.70) and Stanford TMAD datasets (from 0.74 to 0.76), suggesting expert--AI co-assessment can improve IHC interpretation. This work establishes a foundation for AI systems that can improve IHC diagnostic accuracy and highlights the potential for integrating iSight into clinical workflows to enhance the consistency and reliability of IHC assessment.

</details>


### [50] [VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding](https://arxiv.org/abs/2602.04094)
*Junbo Zou,Ziheng Huang,Shengjie Zhang,Liwen Zhang,Weining Shen*

Main category: cs.CV

TL;DR: 提出VideoBrain框架，通过学习到的采样策略使视觉语言模型（VLM）能够自适应地获取长视频信息，引入了基于CLIP的语义检索代理和基于均匀采样的时域代理，并通过行为感知奖励和数据分类管道防止不必要的代理调用。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因计算限制和捕捉跨越大量帧的信息的需求之间的矛盾而面临挑战，现有方法存在信息丢失或无法弥补早期采样错误的风险。

Method: 提出VideoBrain，一个端到端的框架，包含两个互补的代理：一个用于视频语义检索的基于CLIP的代理，一个用于时间间隔内密集采样的均匀代理。VLM直接感知帧并推理信息是否充足，而不是依赖于仅文本的LLM。

Result: 在四个长视频基准测试中，VideoBrain比基线方法提高了3.5%至9.0%，同时使用了30%-40%的更少帧，并且在短视频基准测试中表现出强大的跨数据集泛化能力。

Conclusion: VideoBrain通过学习到的自适应采样策略有效地解决了长视频理解中的信息获取问题，并在效率和性能上优于现有方法，同时具有良好的泛化能力。

Abstract: Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.

</details>


### [51] [DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2602.04102)
*Aayushma Pant,Lakpa Tamang,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 提出了一种名为 DMS2F-HAD 的新型双分支 Mamba 模型，用于高光谱图像异常检测。该模型能高效地捕捉长距离光谱依赖性，并通过动态门控融合机制增强异常定位，在多个基准数据集上取得了最先进的性能，并且推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在捕捉长距离光谱依赖性（如 CNN）或计算成本高（如 Transformer）方面存在不足，而高光谱图像通常是带噪且无标签的。

Method: 提出了一种名为 DMS2F-HAD 的双分支 Mamba 模型。该模型利用 Mamba 的线性时间建模来分别学习空间和光谱特征，并通过动态门控融合机制将它们整合起来。

Result: 在十四个基准高光谱图像数据集上，DMS2F-HAD 实现了 98.78% 的平均 AUC（领域内最优），并且推理速度比同类深度学习方法快 4.6 倍。

Conclusion: DMS2F-HAD 在高光谱异常检测方面展现出强大的泛化能力和可扩展性，有望成为实际应用中的有力工具。

Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) or suffer from high computational cost (e.g., Transformers). To address these challenges, we propose DMS2F-HAD, a novel dual-branch Mamba-based model. Our architecture utilizes Mamba's linear-time modeling to efficiently learn distinct spatial and spectral features in specialized branches, which are then integrated by a dynamic gated fusion mechanism to enhance anomaly localization. Across fourteen benchmark HSI datasets, our proposed DMS2F-HAD not only achieves a state-of-the-art average AUC of 98.78%, but also demonstrates superior efficiency with an inference speed 4.6 times faster than comparable deep learning methods. The results highlight DMS2FHAD's strong generalization and scalability, positioning it as a strong candidate for practical HAD applications.

</details>


### [52] [SuperPoint-E: local features for 3D reconstruction via tracking adaptation in endoscopy](https://arxiv.org/abs/2602.04108)
*O. Leon Barbed,José M. M. Montiel,Pascal Fua,Ana C. Murillo*

Main category: cs.CV

TL;DR: 本文提出了一种名为SuperPoint-E的新的局部特征提取方法，通过跟踪适应性监督策略，显著提升了内窥镜视频中特征检测和描述的质量，从而改进了结构从运动（SfM）的性能，生成了更密集、覆盖范围更广的3D重建。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频的特性（如光照变化、纹理模糊）给传统的SfM方法带来了挑战，需要更强大的特征提取来提高3D重建的性能。

Method: 提出了一种名为SuperPoint-E的局部特征提取方法，并引入了跟踪适应性（Tracking Adaptation）监督策略来优化特征检测和描述。对真实内窥镜录像进行了实验，研究了最佳配置并评估了SuperPoint-E的特征质量，并与现有方法进行了比较。

Result: SuperPoint-E显著提高了特征检测和描述的质量，使得3D重建更加密集，覆盖了更长和更多的视频片段。与原始SuperPoint和COLMAP管道相比，SuperPoint-E取得了更好的3D重建效果。其特征的鲁棒性强，使得引导匹配步骤几乎不再必要。

Conclusion: SuperPoint-E通过改进特征提取，能够显著提升内窥镜视频的SfM性能，产生更高质量的3D重建。其跟踪适应性监督策略是提升特征质量的关键。

Abstract: In this work, we focus on boosting the feature extraction to improve the performance of Structure-from-Motion (SfM) in endoscopy videos. We present SuperPoint-E, a new local feature extraction method that, using our proposed Tracking Adaptation supervision strategy, significantly improves the quality of feature detection and description in endoscopy. Extensive experimentation on real endoscopy recordings studies our approach's most suitable configuration and evaluates SuperPoint-E feature quality. The comparison with other baselines also shows that our 3D reconstructions are denser and cover more and longer video segments because our detector fires more densely and our features are more likely to survive (i.e. higher detection precision). In addition, our descriptor is more discriminative, making the guided matching step almost redundant. The presented approach brings significant improvements in the 3D reconstructions obtained, via SfM on endoscopy videos, compared to the original SuperPoint and the gold standard SfM COLMAP pipeline.

</details>


### [53] [JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models](https://arxiv.org/abs/2602.04142)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: 本文提出了一种名为JSynFlow的新型合成数据集，用于训练视觉语言模型（VLMs）理解日本流程图。该数据集由LLMs生成，包含任务描述、流程图图像和问答对，能有效提升VLM在流程图问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（VLMs）在理解包含流程图等复杂文档方面存在不足，而流程图包含文本无法提供的宝贵信息。然而，创建大规模的流程图图像和文本对应数据集耗时耗力，阻碍了VLMs在该领域的进步。

Method: 研究人员利用大型语言模型（LLMs）生成了一个合成数据集JSynFlow。该数据集包含针对不同职业任务的描述、由领域特定语言（DSL）代码渲染生成的流程图图像，以及相关的问答对。数据集的生成过程包括使用LLMs创建任务描述，然后将这些描述转换为DSL代码，最后渲染成流程图图像并生成问答对。

Result: 通过使用JSynFlow数据集对VLMs进行微调，研究表明模型在基于流程图的问答任务上的性能得到了显著提升。

Conclusion: JSynFlow是一个用于日本流程图的合成视觉问答数据集，它通过LLM合成，能够有效解决大规模流程图数据集创建的挑战，并显著提高VLMs在流程图理解和问答任务上的表现。

Abstract: Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.

</details>


### [54] [Context Determines Optimal Architecture in Materials Segmentation](https://arxiv.org/abs/2602.04154)
*Mingjian Lu,Pawan K. Tripathi,Mark Shteyn,Debargha Ganguly,Roger H. French,Vipin Chaudhary,Yinghui Wu*

Main category: cs.CV

TL;DR: 该研究提出了一个跨模态材料图像分割评估框架，评估了不同成像模态（SEM、AFM、XCT、光学显微镜）下多种分割架构的性能，并提供了模型可靠性和可解释性工具。


<details>
  <summary>Details</summary>
Motivation: 现有的分割模型评估通常仅限于单一成像模态，这导致模型在不同模态下的实际部署性能存在差异，即在一个模态上最优的模型在另一个模态上可能表现不佳。研究人员需要一种方法来选择适合特定成像设置的分割架构，并了解模型何时可以信任。

Method: 开发了一个跨模态评估框架，用于评估在SEM、AFM、XCT和光学显微镜这四种成像模态下的材料图像分割性能。对六种不同的编码器-解码器组合进行了评估，覆盖了七个数据集。此外，该框架还通过“非同分布检测”和“反事实解释”来提供部署反馈，揭示驱动模型预测的微观结构特征。

Result: 研究发现，最优的分割架构会根据具体情况系统性地变化。UNet在处理高对比度的2D成像时表现最佳，而DeepLabv3+则更适合处理最困难的分割任务。框架提供的部署反馈和可解释性工具也显示了其在实际应用中的价值。

Conclusion: 该研究填补了材料表征领域的一个实际空白，为研究人员提供了选择适合特定成像设置的分割架构的指导，以及评估模型在新样本上是否可靠的工具，同时增强了模型的可解释性。

Abstract: Segmentation architectures are typically benchmarked on single imaging modalities, obscuring deployment-relevant performance variations: an architecture optimal for one modality may underperform on another. We present a cross-modal evaluation framework for materials image segmentation spanning SEM, AFM, XCT, and optical microscopy. Our evaluation of six encoder-decoder combinations across seven datasets reveals that optimal architectures vary systematically by context: UNet excels for high-contrast 2D imaging while DeepLabv3+ is preferred for the hardest cases. The framework also provides deployment feedback via out-of-distribution detection and counterfactual explanations that reveal which microstructural features drive predictions. Together, the architecture guidance, reliability signals, and interpretability tools address a practical gap in materials characterization, where researchers lack tools to select architectures for their specific imaging setup or assess when models can be trusted on new samples.

</details>


### [55] [Point2Insert: Video Object Insertion via Sparse Point Guidance](https://arxiv.org/abs/2602.04167)
*Yu Zhou,Xiaoyan Yang,Bojia Zi,Lihan Zhang,Ruijie Sun,Weishi Zheng,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Point2Insert是一个基于稀疏点输入的视频对象插入框架，通过使用少量正负点来精细控制对象位置，解决了现有方法需要密集标注或位置不精确的问题。该框架包含两阶段训练，并利用掩码引导模型进行知识蒸馏，实验证明其效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 日益增长的准确、低成本视频对象插入需求，以及现有基于掩码的方法劳动密集型和基于指令的方法定位不精确的挑战。

Method: 提出Point2Insert框架，仅需少量稀疏正负点即可实现对象插入。训练包含两阶段：第一阶段训练一个在给定区域（由稀疏点或二值掩码引导）生成对象的插入模型；第二阶段在通过对象移除模型合成的配对视频上进行微调，以适应视频插入。此外，利用掩码引导模型作为教师，通过知识蒸馏将可靠的插入行为传授给点引导模型。

Result: Point2Insert在视频对象插入任务上持续优于强大的基线方法，并且在参数量为Point2Insert的10倍的模型上也取得了更好的效果。

Conclusion: Point2Insert提供了一种灵活且用户友好的视频对象插入解决方案，通过稀疏点输入实现了精确的空间控制，并在效率和性能上均超越了现有技术。

Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\times$10 more parameters.

</details>


### [56] [Partial Ring Scan: Revisiting Scan Order in Vision State Space Models](https://arxiv.org/abs/2602.04170)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin li,Ming-Ching Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: 本研究提出了一种名为 PRISMamba 的新型视觉状态空间模型 (SSM) 及其扫描顺序，以提高准确性、效率和旋转鲁棒性。PRISMamba 通过将图像分割成同心圆环并进行跨环上下文传播来解决传统扫描顺序的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉 SSM 在将 2D 图像序列化为 1D 标记序列时，依赖于预定义的扫描顺序，这会影响模型性能，尤其是在处理旋转等几何变换时。研究人员希望找到一种更优的扫描方式来提升视觉 SSM 的性能。

Method: PRISMamba 采用了一种部分环扫描 (Partial RIng Scan) 的方式，将图像划分为同心圆环，然后在每个环内进行顺序无关的聚合，并通过径向 SSM 在环之间传播上下文。此外，还采用了部分通道滤波来优化计算效率。

Result: 在 ImageNet-1K 数据集上，PRISMamba 取得了 84.5% 的 Top-1 准确率，同时 FLOPs 仅为 3.9G，吞吐量高达 3,054 img/s，优于 VMamba。在旋转测试中，PRISMamba 的性能下降仅为 1-2%，而固定扫描顺序的模型下降幅度更大。

Conclusion: 扫描顺序的设计以及通道滤波是影响视觉 SSM 准确性、效率和旋转鲁棒性的关键因素。PRISMamba 的方法证明了其在提升视觉 SSM 性能方面的有效性。

Abstract: State Space Models (SSMs) have emerged as efficient alternatives to attention for vision tasks, offering lineartime sequence processing with competitive accuracy. Vision SSMs, however, require serializing 2D images into 1D token sequences along a predefined scan order, a factor often overlooked. We show that scan order critically affects performance by altering spatial adjacency, fracturing object continuity, and amplifying degradation under geometric transformations such as rotation. We present Partial RIng Scan Mamba (PRISMamba), a rotation-robust traversal that partitions an image into concentric rings, performs order-agnostic aggregation within each ring, and propagates context across rings through a set of short radial SSMs. Efficiency is further improved via partial channel filtering, which routes only the most informative channels through the recurrent ring pathway while keeping the rest on a lightweight residual branch. On ImageNet-1K, PRISMamba achieves 84.5% Top-1 with 3.9G FLOPs and 3,054 img/s on A100, outperforming VMamba in both accuracy and throughput while requiring fewer FLOPs. It also maintains performance under rotation, whereas fixed-path scans drop by 1~2%. These results highlight scan-order design, together with channel filtering, as a crucial, underexplored factor for accuracy, efficiency, and rotation robustness in Vision SSMs. Code will be released upon acceptance.

</details>


### [57] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: 提出了一种名为HoloEv-Net的高效事件基动作识别框架，通过紧凑全息时空表示（CHSR）和全局谱门控（GSG）模块，解决了现有方法的计算冗余、结构冗余和频谱信息利用不足的问题，并在多个数据集上取得了最先进的性能，同时保持了极高的效率，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有事件基动作识别方法存在计算冗余（密集体素表示）、结构冗余（多分支架构）以及频谱信息利用不足（难以捕捉全局运动模式）的问题。

Method: 提出HoloEv-Net框架。1. 引入紧凑全息时空表示（CHSR），将空间信息嵌入时间-高度（T-H）视图，形成2D表示以减少冗余。2. 设计全局谱门控（GSG）模块，利用快速傅里叶变换（FFT）在频域进行全局令牌混合，以利用被忽略的频谱信息。

Result: HoloEv-Net-Base在THU-EACT-50-CHL、HARDVS和DailyDVS-200数据集上取得了最先进的性能，分别超越现有方法10.29%、1.71%和6.25%。轻量级HoloEv-Net-Small模型在保持高精度的同时，参数量、计算量和延迟分别减少了5.4倍、300倍和2.4倍。

Conclusion: HoloEv-Net框架在处理事件基动作识别任务时具有良好的可扩展性和有效性，通过创新的表示方法和模块设计，显著提高了效率并达到了最先进的性能，尤其适合资源受限的边缘设备部署。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [58] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 本研究提出了一个基于指令的驾驶框架，使用doScenes数据集和OpenEMMA模型，通过自由格式的语言指令来指导车辆的轨迹规划，显著提高了驾驶的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随式驾驶规划方法依赖于模拟环境或固定指令词汇，导致在真实世界中的泛化能力有限。研究旨在克服这一限制， enabling instruction-conditioned planning in real-world scenarios.

Method: 作者将OpenEMMA（一个基于MLLM的端到端驾驶框架）适配到doScenes数据集，该数据集包含了自由格式指令与nuScenes真实运动数据的链接。通过将doScenes指令作为乘客风格的提示整合到OpenEMMA的视觉-语言接口中，实现了轨迹生成前的语言条件约束。

Result: 在849个标注场景上的评估显示，指令条件约束显著提高了模型的鲁棒性，将平均ADE（Average Displacement Error）降低了98.7%。去除极端异常值后，指令仍然能提高轨迹的对齐度，精心设计的提示可以将ADE最多提高5.1%。

Conclusion: 指令条件约束是提高自动驾驶轨迹规划鲁棒性和精度的有效手段。研究分析了哪些指令对于OpenEMMA框架是“好的”，并发布了评估提示和脚本，为指令感知规划建立了一个可复现的基线。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [59] [DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding](https://arxiv.org/abs/2602.04188)
*Ning Zhang,Zhengyu Li,Kwong Weng Loh,Mingxi Xu,Qi Wang,Zhengyu Wen,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: DiMo是一个统一的文本-动作理解与生成框架，采用离散扩散风格的掩码建模方法，实现了文本到动作（T2M）、动作到文本（M2T）和无文本动作到动作（M2M）的统一。它通过迭代掩码令牌精炼进行解码，支持质量-延迟权衡，并使用RVQ和GRPO提高了动作质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法多为单向，缺乏统一处理文本-动作双向理解和生成的能力。研究者希望开发一个能够同时处理T2M、M2T和M2M任务的统一模型。

Method: DiMo是一个离散扩散风格的掩码建模框架。它通过迭代地精炼被掩码的动作令牌来生成动作，而不是像GPT那样顺序解码。该模型使用了残差向量量化（RVQ）来提高动作令牌的保真度，并引入了组相对策略优化（GRPO）来增强对齐和可控性。

Result: 在HumanML3D和KIT-ML数据集上，DiMo在统一框架下展现了高质量的动作生成和具有竞争力的双向理解能力。此外，该模型还能进行文本无关的动作补全、文本引导的动作预测以及动作描述修正，无需改变模型架构。

Conclusion: DiMo成功地将文本到动作的生成任务扩展到文本-动作双向的理解与生成，并提出了一种新颖的统一框架，通过迭代掩码令牌精炼实现了T2M、M2T和M2M任务的统一，并在动作质量、双向理解以及灵活性方面取得了显著成果。

Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.

</details>


### [60] [ACIL: Active Class Incremental Learning for Image Classification](https://arxiv.org/abs/2602.04252)
*Aditya R. Bhattacharya,Debanjan Goswami,Shayok Chakraborty*

Main category: cs.CV

TL;DR: 提出了一种名为ACIL的主动学习框架，用于解决类别增量学习中的灾难性遗忘问题，通过不确定性和多样性准则选择样本进行标注，从而降低标注成本并提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的类别增量学习方法假设每个训练样本都有标注，这导致了高昂的标注成本，并且很多标注信息在后续的学习中被浪费。同时，类别增量学习中存在灾难性遗忘问题。

Method: 提出ACIL框架，结合主动学习和类别增量学习。在每个学习阶段，使用基于不确定性和多样性的准则来选择需要标注的样本（exemplars），并将这些样本添加到下一阶段的数据集中。

Result: ACIL框架能够显著降低标注成本，并有效避免灾难性遗忘。在多个视觉数据集上的实验结果表明，该框架优于相关的基线方法。

Conclusion: ACIL框架是一种有效的主动学习方法，能够解决类别增量学习中的标注成本和灾难性遗忘问题，具有良好的应用前景。

Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focused on avoiding catastrophic forgetting, which occurs due to the continuously changing class distributions in each episode and the inaccessibility of the data from previous episodes. However, these methods assume that all the training samples in every episode are annotated; this not only incurs a huge annotation cost, but also results in a wastage of annotation effort, since most of the samples in a given episode will not be accessible to the model in subsequent episodes. Active learning algorithms identify the salient and informative samples from large amounts of unlabeled data and are instrumental in reducing the human annotation effort in inducing a deep neural network. In this paper, we propose ACIL, a novel active learning framework for class incremental learning settings. We exploit a criterion based on uncertainty and diversity to identify the exemplar samples that need to be annotated in each episode, and will be appended to the data in the next episode. Such a framework can drastically reduce annotation cost and can also avoid catastrophic forgetting. Our extensive empirical analyses on several vision datasets corroborate the promise and potential of our framework against relevant baselines.

</details>


### [61] [SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization](https://arxiv.org/abs/2602.04271)
*Lifan Wu,Ruijie Zhu,Yubo Ai,Tianzhu Zhang*

Main category: cs.CV

TL;DR: SkeletonGaussian 提出了一种新的 4D 生成框架，用于从单目视频生成可编辑的动态 3D 高斯，通过显式骨架驱动的刚性运动和基于 Hexplane 的非刚性运动细化来增强可控性和可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有 4D 生成方法将运动表示为隐式形变场，限制了直接控制和可编辑性。研究动机是开发一种能够实现直观运动编辑的新型 4D 生成方法。

Method: SkeletonGaussian 采用分层关节表示，将运动分解为由骨架驱动的稀疏刚性运动和细粒度的非刚性运动。具体而言，首先提取鲁棒骨架并利用线性混合蒙皮驱动刚性运动，然后通过基于 Hexplane 的方法进行非刚性形变细化。

Result: 实验结果表明，SkeletonGaussian 在生成质量上优于现有方法，并实现了直观的运动编辑功能。

Conclusion: SkeletonGaussian 引入了一种可编辑的 4D 生成新范式，通过明确的骨架表示和混合蒙皮与 Hexplane 细化的结合，有效解决了现有方法的局限性，实现了高质量且易于编辑的动态 3D 内容生成。

Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/

</details>


### [62] [Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution](https://arxiv.org/abs/2602.04193)
*Hyeonjae Kim,Dongjin Kim,Eugene Jin,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种利用流匹配（flow matching）在潜在退化空间合成真实低分辨率（LR）图像的方法，以解决现有深度学习超分辨率（SR）模型在真实世界复杂退化图像上的性能瓶颈。该方法能够生成具有真实伪影且退化程度未知的LR图像，从而为SR模型训练创建大规模真实数据集，并显著提升SR模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的SR方法在合成退化图像上表现良好，但在处理包含噪声、模糊、压缩伪影等复杂非线性退化的真实世界图像时性能下降。尽管已有方法尝试收集真实LR-HR图像对，但受限于特定的降尺度因子，难以覆盖广泛的真实退化情况。

Method: 利用流匹配（flow matching）在潜在退化空间中设计了一个新颖的框架，能够从单个高分辨率（HR）图像合成具有真实退化的低分辨率（LR）图像。该方法能够生成在未见过的退化级别下的LR图像，并模拟真实的伪影。

Result: 通过定量和定性评估，证明了合成的LR图像能够准确地复现真实世界的退化。使用该方法生成的真实世界SR数据集训练的传统和任意尺度SR模型，在HR图像复原方面取得了显著更好的效果。

Conclusion: 提出的基于流匹配的潜在退化空间框架能够有效合成具有真实退化特征的LR图像，为构建大规模真实世界SR训练数据集提供了解决方案，并显著提高了SR模型的性能。

Abstract: While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.

</details>


### [63] [VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents](https://arxiv.org/abs/2602.04202)
*Feng Wang,Yichun Shi,Ceyuan Yang,Qiushan Guo,Jingxiang Sun,Alan Yuille,Peng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 VTok 的统一视频标记化框架，能够同时用于视频生成和理解任务。它通过保留关键帧的空间特征并为后续帧编码残差标记，从而有效降低了视频表示的复杂性，同时保留了丰富的时空信息。


<details>
  <summary>Details</summary>
Motivation: 现有领先的视频-语言系统在标记化视频时采用简单的帧采样策略，这导致视频表示的复杂性过高。作者希望开发一种更高效且信息量足的视频标记化方法。

Method: VTok 框架通过解耦视频的空间和时间表示来实现。它保留一个关键帧的空间特征，然后将后续的每一帧编码为一个单一的残差标记。残差标记捕捉了相对于关键帧的视角和运动变化。

Result: VTok 成功地将视频表示的复杂性从帧数和每帧标记数的乘积降低到它们的总和。实验表明，VTok 在视频理解和文本到视频生成任务上取得了比基线方法更高的性能，同时使用了更短的标记序列。例如，在 TV-Align 基准测试上准确率提高了 3.4%，在 VBench 上得分提高了 1.9%。在文本到视频生成方面，VTok 产生了更连贯的运动和更强的指导跟随能力。

Conclusion: VTok 是一种有效的视频标记化方法，它能够在降低计算复杂性的同时，保留丰富的时空信息，从而在视频理解和生成任务上取得优异的性能。作者希望 VTok 成为未来视频研究的标准化视频标记化范式。

Abstract: This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.

</details>


### [64] [AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting](https://arxiv.org/abs/2602.04204)
*Chao Li,Rui Zhang,Siyuan Huang,Xian Zhong,Hongbo Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为AGMA的新型轨迹预测方法，通过构建场景自适应的高质量先验来解决现有方法中的先验不匹配问题，从而提升预测准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类轨迹预测方法在处理行人行为的多模态性时存在先验不匹配的问题，学习到的或固定的先验无法充分捕捉未来可能发生的多种情况，这限制了预测的准确性和多样性。理论研究表明，预测误差受到先验质量的下界限制，因此提高先验质量是提升性能的关键。

Method: AGMA方法通过两个阶段构建富有表现力的先验：1. 从训练数据中提取多样的行为模式；2. 将这些模式提炼成一个场景自适应的全局先验，用于推理。

Result: 在ETH-UCY、Stanford Drone和JRDB数据集上的大量实验表明，AGMA取得了最先进的性能。

Conclusion: 高质量的先验在轨迹预测中起着至关重要的作用，AGMA通过构建场景自适应的先验，显著提升了轨迹预测的准确性和多样性。

Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.

</details>


### [65] [Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement](https://arxiv.org/abs/2602.04304)
*Zipeng Zhu,Zhanghao Hu,Qinglin Zhu,Yuxi Hong,Yijun Liu,Jingyong Su,Yulan He,Lin Gui*

Main category: cs.CV

TL;DR: 本文提出了一种名为LASER的训练无关推理方法，通过动态选择适合特定任务（特别是视觉推理）的视觉特征层来增强大型视觉语言模型（LVLM）在视觉定位和问答方面的表现，解决了现有方法依赖静态“魔术层”的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLM在处理高分辨率图像时存在视觉细节丢失和过度依赖语言先验导致幻觉的问题。现有的注意力增强方法（如裁剪、区域注意力）通常依赖于经验设定的静态“魔术层”，这在复杂推理任务上的泛化能力有限。

Method: 通过层级敏感性分析，作者发现视觉基础（visual grounding）是一个动态过程。提出了一种名为VAQ（Visual Activation by Query）的度量标准，通过衡量注意力对查询的敏感性来识别与查询相关的视觉基础层。基于VAQ，提出了LASER（Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning）方法，这是一个训练无关的推理过程，能够自适应地选择合适的视觉层进行定位和问答。

Result: 在多个VQA基准测试中，LASER方法在不同复杂度的任务上都显著提高了VQA的准确率。

Conclusion: 视觉基础是一个动态过程，不同的任务复杂度需要不同深度的视觉信息。LASER方法通过动态选择层级注意力，能够有效地提升LVLM在复杂视觉推理任务上的性能，克服了现有静态方法在泛化性上的不足。

Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.

</details>


### [66] [Adaptive 1D Video Diffusion Autoencoder](https://arxiv.org/abs/2602.04220)
*Yao Teng,Minxuan Lin,Xian Liu,Shuai Wang,Xiao Yang,Xihui Liu*

Main category: cs.CV

TL;DR: 提出了一种名为 One-DVA 的新型视频自编码器框架，采用 Transformer 和扩散模型，实现了自适应压缩和更好的视频重建，并为下游生成任务提供了优化。


<details>
  <summary>Details</summary>
Motivation: 现有的视频自编码器在固定率压缩、CNN 架构灵活性和解码器细节恢复方面存在局限性。

Method: 提出 One-DVA 框架，使用基于查询的 Vision Transformer 进行编码，实现自适应 1D 编码，并采用扩散 Transformer 进行解码，以像素空间扩散模型重建视频。

Result: 在相同的压缩率下，One-DVA 的重建性能与 3D-CNN VAEs 相当，并且支持自适应压缩，可实现更高的压缩率。

Conclusion: One-DVA 框架通过自适应 1D 编码和基于扩散的解码，有效解决了现有视频自编码器的局限性，并在视频重建和压缩方面取得了良好效果，同时为下游生成任务提供了优化。

Abstract: Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.

</details>


### [67] [Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner](https://arxiv.org/abs/2602.04337)
*Qian-Wei Wang,Guanghao Meng,Ren Cai,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoFT的无监督视觉-语言模型（VLM）自适应框架，通过双模型、跨模态协作机制，利用正负文本提示学习伪标签的清洁度，并结合参数高效微调和全参数微调，有效提升了VLM在下游任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督自训练方法在处理伪标签时存在置信度过滤不可靠、确认偏差以及低置信度样本利用不足等问题，需要昂贵的标注数据来适应下游任务。因此，研究一种更有效的无监督自适应方法以解决这些问题。

Method: 提出CoFT框架，包含：1. 双模型、跨模态协作机制；2. 双提示学习策略（正向和负向文本提示），用于样本依赖地建模伪标签的清洁度；3. 利用负向提示正则化轻量级视觉适应模块；4. 两阶段训练方案：先在置信度高的样本上进行参数高效微调，然后过渡到由协作过滤的伪标签指导的全参数微调。在此基础上，CoFT+通过迭代微调、动量对比学习和LLM生成的提示进一步增强适应性。

Result: CoFT在无监督方法上取得了持续的提升，甚至超过了少样本监督基线。CoFT+进一步提高了性能。

Conclusion: CoFT是一种有效的无监督VLM自适应框架，通过创新的双提示学习和协作机制，克服了现有方法的局限性，并在下游任务上取得了优于现有方法的性能。CoFT+通过多项改进进一步增强了其自适应能力。

Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.

</details>


### [68] [An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation](https://arxiv.org/abs/2602.04227)
*Hanuman Verma,Kiho Im,Pranabesh Maji,Akshansh Gupta*

Main category: cs.CV

TL;DR: 提出了一种名为IF-UNet的增强型框架，将直觉模糊逻辑集成到UNet中，以提高MRI脑图像分割的准确性，特别是在处理部分容积效应和边界不确定性方面。


<details>
  <summary>Details</summary>
Motivation: 由于部分容积效应，在脑图像中处理不确定性是深度学习方法（如UNet）在MRI脑图像分割中面临的挑战。

Method: 提出了一种名为IF-UNet的新框架，将直觉模糊逻辑（包括隶属度、非隶属度和犹豫度）整合到UNet架构中。该模型处理输入数据时考虑这三个度量，以更好地处理部分容积效应和边界不确定性。

Result: 在IBSR数据集上进行的实验表明，IF-UNet在分割质量上优于现有方法，能够更有效地处理脑图像中的不确定性。性能通过准确率、Dice系数和IoU进行评估。

Conclusion: IF-UNet框架通过集成直觉模糊逻辑，能够有效地处理MRI脑图像分割中的不确定性，从而提高分割质量，特别是在存在部分容积效应和边界模糊的情况下。

Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.

</details>


### [69] [VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image](https://arxiv.org/abs/2602.04349)
*Teng-Fang Hsiao,Bo-Kai Ruan,Yu-Lun Liu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出VecSet-Edit，一种基于VecSet LRM的3D网格编辑新流水线，解决了现有方法分辨率低和需要手动掩码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法主要集中在3D高斯泼溅或多视图图像，而直接编辑3D网格的研究不足。之前的基于体素的方法分辨率低且需要耗时的人工3D掩码。

Method: 利用VecSet LRM作为骨干，通过分析VecSet tokens的空间属性，设计了Mask-guided Token Seeding和Attention-aligned Token Gating策略来定位目标区域，并引入Drift-aware Token Pruning来处理去噪过程中的几何异常。最后，使用Detail-preserving Texture Baking模块来保留几何和纹理细节。

Result: VecSet-Edit流水线能够实现高保真度的3D网格编辑，同时保留原始网格的几何和纹理细节。

Conclusion: VecSet-Edit是第一个利用VecSet LRM进行3D网格编辑的流水线，克服了现有方法的局限性，提供了更精确和细节保留的编辑能力。

Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main

</details>


### [70] [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/abs/2602.04340)
*Qian-Wei Wang,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出了一种基于双提示调优的主动学习框架，用于在标注数据有限的情况下微调CLIP模型以进行图像分类，通过引入正负可学习提示来增强模型区分能力和提供不确定性信号，并在实验中证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺的情况下，如何有效地利用预训练的视觉-语言模型（如CLIP）进行下游图像分类任务是一个挑战。现有的主动学习方法在样本选择时，通常基于熵或聚类来估计不确定性，而没有从模型内部视角来建模不确定性。

Method: 提出了一种基于双提示调优（dual-prompt tuning）的主动学习框架。该框架在CLIP模型的文本分支中引入两个可学习的提示：一个正向提示（positive prompt）用于增强任务特定的文本嵌入的区分度，同时配合轻量级调优的视觉嵌入；一个负向提示（negative prompt）则以反向方式训练，用于显式建模预测标签正确的概率，从而提供一个原则性的不确定性信号来指导样本选择。

Result: 在不同的微调范式下进行的广泛实验表明，该方法在相同的标注预算下，始终优于现有的主动学习方法。

Conclusion: 所提出的双提示调优框架能够有效地为主动学习场景下的CLIP模型适应提供一个鲁棒的不确定性建模方法，从而在有限的标注预算下取得更好的图像分类性能。

Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.

</details>


### [71] [SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction](https://arxiv.org/abs/2602.04240)
*Suzeyu Chen,Leheng Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SPOT-Occ的基于原型的稀疏Transformer解码器，用于从摄像头进行高效准确的3D占用预测，通过原型引导的特征选择和聚合，克服了传统密集注意力机制的计算瓶颈，并在速度和准确性上均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在安全和实用部署方面，需要高精度、实时的3D占用预测。虽然稀疏3D表示解决了编码瓶颈，但解码器面临如何高效聚合稀疏、非均匀分布的体素特征的挑战，避免计算成本高昂的密集注意力。

Method: 提出了一种原型引导的稀疏Transformer解码器（SPOT-Occ）。该解码器采用两阶段过程：1. 稀疏原型选择机制，查询自适应地识别最显著的体素特征（原型）；2. 聚焦特征聚合。引入互补去噪范式，利用真值掩码提供显式指导，确保跨解码器层的查询-原型一致性。

Result: SPOT-Occ模型在速度上显著优于现有方法，同时在准确性方面也有所提升。

Conclusion: SPOT-Occ通过引入原型引导的稀疏特征选择和聚合机制，提供了一种高效且准确的3D占用预测方法，解决了现有解码器面临的挑战，并在实际应用中展现出优越性能。

Abstract: Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.
  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.
  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.

</details>


### [72] [Depth-Guided Metric-Aware Temporal Consistency for Monocular Video Human Mesh Recovery](https://arxiv.org/abs/2602.04257)
*Jiaxin Cen,Xudong Mao,Guanghui Yue,Wei Zhou,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 提出了一种深度引导的多尺度融合框架，通过融合几何先验和RGB特征，并利用深度校准的骨骼统计信息和跨模态注意力机制，实现了具有度量一致性和时间稳定性的单目视频人体网格恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法在维持单目视频人体网格恢复的度量一致性和时间稳定性方面存在根本性挑战，例如深度歧义、尺度不确定性、深度排序、尺度漂移和遮挡引起的失稳。

Method: 提出一个深度引导的框架，包含三个组件：1. 深度引导的多尺度融合模块（通过置信度感知门控融合几何先验和RGB特征）；2. 深度引导的度量感知姿态和形状估计器（D-MAPS，利用深度校准的骨骼统计信息进行尺度一致的初始化）；3. 运动-深度对齐的精炼模块（MoDAR，通过运动动力学与几何线索之间的跨模态注意力实现时间连贯性）。

Result: 在三个具有挑战性的基准测试中取得了优越的结果，在鲁棒性（抵抗严重遮挡）和空间精度方面有显著改进，同时保持了计算效率。

Conclusion: 所提出的深度引导框架能够通过融合几何信息和RGB特征，并利用专门设计的模块来处理尺度一致性和时间连贯性，从而有效地解决单目视频人体网格恢复中的度量一致性和时间稳定性问题。

Abstract: Monocular video human mesh recovery faces fundamental challenges in maintaining metric consistency and temporal stability due to inherent depth ambiguities and scale uncertainties. While existing methods rely primarily on RGB features and temporal smoothing, they struggle with depth ordering, scale drift, and occlusion-induced instabilities. We propose a comprehensive depth-guided framework that achieves metric-aware temporal consistency through three synergistic components: A Depth-Guided Multi-Scale Fusion module that adaptively integrates geometric priors with RGB features via confidence-aware gating; A Depth-guided Metric-Aware Pose and Shape (D-MAPS) estimator that leverages depth-calibrated bone statistics for scale-consistent initialization; A Motion-Depth Aligned Refinement (MoDAR) module that enforces temporal coherence through cross-modal attention between motion dynamics and geometric cues. Our method achieves superior results on three challenging benchmarks, demonstrating significant improvements in robustness against heavy occlusion and spatial accuracy while maintaining computational efficiency.

</details>


### [73] [SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration](https://arxiv.org/abs/2602.04361)
*Zekun Li,Ning Wang,Tongxin Bai,Changwang Mei,Peisong Wang,Shuang Qiu,Jian Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为 SparVAR 的训练无关加速框架，用于解决视觉自回归（VAR）模型中因高分辨率注意力计算导致的高延迟问题。SparVAR 通过动态预测稀疏注意力模式，利用自相似性和局部性，实现了高效的注意力计算，并在不跳过高分辨率尺度的情况下，将 8B 模型生成 1024x1024 图像的时间缩短到 1 秒内，同时保留了高频细节。


<details>
  <summary>Details</summary>
Motivation: 主流的视觉自回归（VAR）模型在生成高分辨率图像时，由于注意力机制的计算复杂度随分辨率呈四次方增长，导致显著的延迟。现有的加速方法通过跳过高分辨率尺度来提高速度，但会牺牲图像质量。因此，需要一种能够在不牺牲图像质量的情况下加速 VAR 模型生成过程的方法。

Method: SparVAR 提出了一种训练无关的加速框架，利用 VAR 注意力的三个特性：注意力“沉淀”（sinks）、跨尺度激活相似性以及显著的局部性。具体来说，它通过一个稀疏决策尺度动态预测后续高分辨率尺度的稀疏注意力模式，并利用高效的索引映射机制构建尺度自相似的稀疏注意力。此外，SparVAR 还引入了跨尺度局部稀疏注意力，并实现了一个高效的块状稀疏核，相较于 FlashAttention 实现了更快的速度。

Result: SparVAR 框架能够显著减少 VAR 模型的生成时间。对于一个 8B 模型生成 1024x1024 高分辨率图像的任务，SparVAR 将生成时间缩短到 1 秒内，并且没有跳过最后的高分辨率尺度。与使用 FlashAttention 加速的 VAR 基线模型相比，SparVAR 实现了 1.57 倍的加速，同时几乎保留了所有高频细节。当与现有的尺度跳过策略结合时，SparVAR 能够实现高达 2.28 倍的加速，并保持具有竞争力的视觉生成质量。

Conclusion: SparVAR 是一种有效的训练无关加速框架，能够解决视觉自回归模型在高分辨率生成中的延迟问题。通过利用 VAR 注意力的内在特性，SparVAR 在不牺牲图像质量的情况下实现了显著的速度提升，为高分辨率图像生成任务提供了更快的解决方案。

Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.

</details>


### [74] [Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture](https://arxiv.org/abs/2602.04381)
*Weihao Gao,Zhuo Deng,Zheng Gong,Lan Ma*

Main category: cs.CV

TL;DR: 开发了一种名为UltraSeg的超轻量级结直肠息肉分割模型系列，可在CPU上实现高帧率（90 FPS），参数量仅为传统模型的0.4%，同时保持高精度，适用于资源受限的医疗环境。


<details>
  <summary>Details</summary>
Motivation: 现有高精度息肉分割模型依赖GPU，难以在基层医院、移动内窥镜单元或胶囊机器人等资源受限的环境中部署，阻碍了早期结直肠癌的实时检测和切除。

Method: 提出了UltraSeg模型系列，参数量极小（<0.3 M）。通过联合优化编码器-解码器宽度、使用受限膨胀卷积扩大感受野，以及集成跨层轻量级融合模块，实现了在CPU上的高效运行。UltraSeg-108K针对单中心数据优化，UltraSeg-130K则泛化到多中心、多模态图像。

Result: UltraSeg模型在CPU上可达90 FPS，同时保留了31M参数U-Net >94%的Dice分数，而参数量仅为其0.4%。在七个公开数据集上进行了评估。

Conclusion: UltraSeg系列模型为资源受限的医疗场景提供了一个准确且实时可部署的结直肠息肉分割解决方案，填补了GPU依赖模型的空白，并为更广泛的微创手术视觉应用提供了一个可复现的蓝图。

Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (<0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains >94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.

</details>


### [75] [Decoupled Hierarchical Distillation for Multimodal Emotion Recognition](https://arxiv.org/abs/2602.04260)
*Yong Li,Yuanzhi Wang,Yi Ding,Shiqing Zhang,Ke Lu,Cuntai Guan*

Main category: cs.CV

TL;DR: 提出了一种名为解耦分层多模态蒸馏 (DHMD) 的新框架，用于人类多模态情感识别，通过解耦模态特征并采用两阶段知识蒸馏来解决模态异质性和贡献不均的问题，并在 CMU-MOSI/CMU-MOSEI 数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的人类多模态情感识别方法在处理模态间的异质性和不同模态贡献程度不均的问题上存在挑战。

Method: 该研究提出了一种名为解耦分层多模态蒸馏 (DHMD) 的新框架。DHMD 使用自回归机制将每种模态的特征解耦为与模态无关（同质）和与模态无关（异质）的组件。框架采用了两阶段知识蒸馏策略：1. 通过图蒸馏单元 (GD-Unit) 进行粗粒度蒸馏，动态图促进模态间的自适应蒸馏；2. 通过跨模态字典匹配机制进行细粒度蒸馏，对齐语义粒度以获得更具辨别力的情感识别表示。

Result: DHMD 在 CMU-MOSI/CMU-MOSEI 数据集上取得了比现有最先进方法更好的性能，分别在 ACC7、ACC2 和 F1 指标上实现了 1.3%/2.4%、1.3%/1.9% 和 1.9%/1.8% 的相对提升。可视化结果显示，DHMD 中的图边和字典激活在模态无关/无关特征空间中呈现出有意义的分布模式。

Conclusion: DHMD 框架通过解耦模态特征和采用分层知识蒸馏策略，有效地提高了跨模态特征对齐，并显著提升了多模态情感识别的性能，优于现有最先进方法。

Abstract: Human multimodal emotion recognition (MER) seeks to infer human emotions by integrating information from language, visual, and acoustic modalities. Although existing MER approaches have achieved promising results, they still struggle with inherent multimodal heterogeneities and varying contributions from different modalities. To address these challenges, we propose a novel framework, Decoupled Hierarchical Multimodal Distillation (DHMD). DHMD decouples each modality's features into modality-irrelevant (homogeneous) and modality-exclusive (heterogeneous) components using a self-regression mechanism. The framework employs a two-stage knowledge distillation (KD) strategy: (1) coarse-grained KD via a Graph Distillation Unit (GD-Unit) in each decoupled feature space, where a dynamic graph facilitates adaptive distillation among modalities, and (2) fine-grained KD through a cross-modal dictionary matching mechanism, which aligns semantic granularities across modalities to produce more discriminative MER representations. This hierarchical distillation approach enables flexible knowledge transfer and effectively improves cross-modal feature alignment. Experimental results demonstrate that DHMD consistently outperforms state-of-the-art MER methods, achieving 1.3\%/2.4\% (ACC$_7$), 1.3\%/1.9\% (ACC$_2$) and 1.9\%/1.8\% (F1) relative improvement on CMU-MOSI/CMU-MOSEI dataset, respectively. Meanwhile, visualization results reveal that both the graph edges and dictionary activations in DHMD exhibit meaningful distribution patterns across modality-irrelevant/-exclusive feature spaces.

</details>


### [76] [S-MUSt3R: Sliding Multi-view 3D Reconstruction](https://arxiv.org/abs/2602.04517)
*Leonid Antsfeld,Boris Chidlovskii,Yohann Cabon,Vincent Leroy,Jerome Revaud*

Main category: cs.CV

TL;DR: 提出了一种名为S-MUSt3R的简单高效的管道，用于将大型基础模型扩展到大规模单目3D重建，通过序列分割、段对齐和轻量级环路闭合优化来解决内存限制，无需重新训练模型，并在多个数据集上实现了与传统方法相当的轨迹和重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉基础模型在处理大规模RGB图像流进行3D重建时面临内存限制的挑战。

Method: S-MUSt3R通过序列分割、段对齐和轻量级环路闭合优化来扩展基础模型的能力，无需重新训练模型。

Result: 在TUM、7-Scenes和专有机器人导航数据集上进行了评估，证明S-MUSt3R可以处理长RGB序列，并产生准确一致的3D重建。其轨迹和重建性能与复杂架构的传统方法相当。

Conclusion: S-MUSt3R能够有效地利用MUSt3R模型进行大规模单目3D场景重建，特别是在真实世界应用中，并且可以直接在度量空间中进行预测，具有重要优势。

Abstract: The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.

</details>


### [77] [Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare](https://arxiv.org/abs/2602.04416)
*Aavash Chhetri,Bibek Niroula,Pratik Shrestha,Yash Raj Shrestha,Lesley A Anderson,Prashnna K Gyawali,Loris Bazzani,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本文提出了Med-MMFL，这是第一个全面的医学多模态联邦学习（MMFL）基准，包含多种模态、任务和联邦场景，旨在促进对医学MMFL的系统性理解，并提供一个用于未来研究的开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有的医学联邦学习基准有限，主要集中在单一或双模态以及少数医学任务上，这阻碍了对医学多模态联邦学习（MMFL）的系统性理解和标准化评估。

Method: 引入Med-MMFL基准，该基准包含2到4种模态（共10种医学模态），并涵盖分割、分类、模态对齐（检索）和VQA等任务。在自然联邦、合成IID和合成非IID设置下评估了六种代表性的联邦学习算法，并开源了实现。

Result: 在Med-MMFL基准上对六种代表性联邦学习算法进行了评估，涵盖了不同的聚合策略、损失函数和正则化技术，并模拟了现实世界的异质性。

Conclusion: Med-MMFL基准的推出及其开源实现将支持在真实的医学环境中对未来MMFL方法的重现性和公平比较，促进该领域的发展。

Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .

</details>


### [78] [KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing](https://arxiv.org/abs/2602.04268)
*Siyu Jiang,Feiyang Chen,Xiaojin Zhang,Kun He*

Main category: cs.CV

TL;DR: 本研究提出了一种名为KVSmooth的训练无关、即插即用的方法，通过注意力熵引导的自适应平滑来减少多模态大语言模型（MLLMs）的幻觉问题，同时提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在生成过程中容易出现幻觉，即生成与视觉输入不一致的内容，尤其是在长序列生成时，由于语义漂移导致输出偏离视觉事实。

Method: KVSmooth方法通过在KV-Cache中对键（Keys）和值（Values）应用指数移动平均（EMA）来工作。它利用注意力分布的熵来量化每个token的“沉没”程度，并自适应地调整平滑强度，从而引导注意力在解码过程中保持与视觉信息的对齐。

Result: 实验表明，KVSmooth显著减少了幻觉（CHAIR_S从41.8降低到18.2），并提高了整体性能（F1分数从77.5提高到79.2），实现了精确率和召回率的同时提升。与现有方法不同，KVSmooth避免了以牺牲一个指标换取另一个指标的情况。

Conclusion: KVSmooth是一种有效且通用的方法，可以在推理时无需额外训练或修改模型，显著减轻MLLMs的幻觉问题，并提升其在多模态任务上的整体表现。

Abstract: Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases.
  To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength.
  Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\mathit{CHAIR}_{S}$ from $41.8 \rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.

</details>


### [79] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 该研究提出了一个用于识别巴基斯坦和印度非正规住区的半监督分割框架，并构建了一个新的基准数据集。该框架通过类感知自适应阈值和原型库系统解决了数据不平衡和特征退化问题，并在跨领域迁移任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家城市快速扩张导致非正规住区增长，但现有数据标注稀疏且质量不高，存在光谱模糊和标注噪声问题，阻碍了大规模测绘。

Method: 构建了巴基斯坦拉合尔的非正规住区基准数据集，并扩展至卡拉奇和孟买，总面积达1869平方公里。提出了一种新的半监督分割框架，包含类感知自适应阈值机制（解决少数类抑制）和原型库系统（增强语义一致性）。

Result: 在八个城市（三个大洲）的实验表明，该方法优于现有的半监督方法。在仅使用10%源标签的情况下，模型在未见过的数据上达到了0.461 mIoU，并且优于全监督模型的零样本泛化能力。

Conclusion: 所提出的半监督分割框架能够有效应对非正规住区识别中的数据挑战，并展现出强大的跨领域迁移能力，为大规模非正规住区测绘提供了更可靠的解决方案。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [80] [AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation](https://arxiv.org/abs/2602.04672)
*Jin-Chuan Shi,Binhong Ye,Tao Liu,Junzhe He,Yangjinhui Xu,Xiaoyang Liu,Zeju Li,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: AGILE是一个新框架，通过VLM引导生成模型来重建手部-物体交互，解决了现有方法在遮挡和初始化方面的不足，并提供了仿真就绪的资产。


<details>
  <summary>Details</summary>
Motivation: 从单目视频中重建动态手部-物体交互对于数据采集和创建用于机器人/VR的数字孪生至关重要，但现有方法受限于神经渲染产生的碎片化几何和脆弱的SfM初始化。

Method: AGILE采用了一种基于代理（agentic）的生成方法，利用VLM指导生成模型创建完整的、具有高保真纹理的物体网格，独立于视频遮挡。它还提出了一种锚定-跟踪策略来替代SfM，使用基础模型初始化物体姿态，并通过生成资产与视频观测之间的相似性进行时间传播。最后，通过接触感知优化来整合语义、几何和交互稳定性约束。

Result: AGILE在HO3D、DexYCB和野外视频数据集上进行了广泛实验，结果显示其在全局几何精度上优于基线方法，并且在具有挑战性的序列上表现出卓越的鲁棒性。

Conclusion: AGILE通过从重建转向代理生成，克服了现有方法的限制，能够生成仿真就绪的、物理上合理的物体资产，并已通过机器人应用的真实到模拟重定向得到验证。

Abstract: Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.

</details>


### [81] [Light Up Your Face: A Physically Consistent Dataset and Diffusion Model for Face Fill-Light Enhancement](https://arxiv.org/abs/2602.04300)
*Jue Gong,Zihan Zhou,Jingkai Wang,Xiaohong Liu,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FiLitDiff的新型面部补光增强方法，该方法能够高效且可控地为面部添加虚拟补光，同时保持原始场景光照和背景不变。研究人员构建了一个名为LightYourFace-160K (LYF-160K) 的大规模数据集，并提出了一种名为PALP（physics-aware lighting prompt）的预训练模型来编码光照参数，最终训练了一个基于扩散模型的FiLitDiff，以实现高质量且计算成本低的面部补光。


<details>
  <summary>Details</summary>
Motivation: 现有的面部补光方法往往会改变整体场景光照或背景，导致前景和背景不一致，无法满足实际的面部补光需求。研究人员希望开发一种能够独立于原始场景光照，仅针对面部进行补光的方法。

Method: 1. 构建大规模数据集LYF-160K，包含160,000对原始图像和添加了补光的图像，补光效果基于物理渲染，并由六个解耦的因素控制。2. 预训练一个物理感知照明提示（PALP）模型，将六个光照参数编码为条件token，并引入辅助的平面光重构目标。3. 基于预训练的扩散模型，训练一个高效的一步式模型FiLitDiff，该模型以物理约束的光照编码作为条件，实现可控的高保真度补光。

Result: FiLitDiff在独立测试集上表现出优异的感知质量和竞争力的全参考指标，并且在保留背景光照方面优于现有方法。

Conclusion: 研究人员成功开发了一种高效、可控且能够保持背景光照一致性的面部补光方法FiLitDiff，并构建了一个大规模数据集LYF-160K来支持相关研究。该方法在保持高质量补光的同时，显著降低了计算成本。

Abstract: Face fill-light enhancement (FFE) brightens underexposed faces by adding virtual fill light while keeping the original scene illumination and background unchanged. Most face relighting methods aim to reshape overall lighting, which can suppress the input illumination or modify the entire scene, leading to foreground-background inconsistency and mismatching practical FFE needs. To support scalable learning, we introduce LightYourFace-160K (LYF-160K), a large-scale paired dataset built with a physically consistent renderer that injects a disk-shaped area fill light controlled by six disentangled factors, producing 160K before-and-after pairs. We first pretrain a physics-aware lighting prompt (PALP) that embeds the 6D parameters into conditioning tokens, using an auxiliary planar-light reconstruction objective. Building on a pretrained diffusion backbone, we then train a fill-light diffusion (FiLitDiff), an efficient one-step model conditioned on physically grounded lighting codes, enabling controllable and high-fidelity fill lighting at low computational cost. Experiments on held-out paired sets demonstrate strong perceptual quality and competitive full-reference metrics, while better preserving background illumination. The dataset and model will be at https://github.com/gobunu/Light-Up-Your-Face.

</details>


### [82] [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/abs/2602.04317)
*Zihan Lou,Jinlong Fan,Sihan Ma,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: JOintGS是一个统一框架，通过联合优化相机外参、人体姿态和3D高斯表示，从单目RGB视频重建高保真可驱动3D人体形象。它通过前景-背景解耦、时间动力学模块和残差颜色场来提升精度和鲁棒性，并在NeuMan和EMDB数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在无约束的野外场景下，从单目RGB视频重建高保真可驱动3D人体形象具有挑战性，因为现有方法（如COLMAP, HMR2.0）提供的相机参数和人体姿态通常不准确，而3DGS等方法对精确的相机标定和姿态标注依赖性强，限制了其在真实世界中的应用。

Method: JOintGS提出一个统一框架，通过协同优化机制，联合优化相机外参、人体姿态和3D高斯表示。核心在于前景-背景解耦：静态背景高斯通过多视图一致性锚定相机估计；优化的相机改进人体对齐；优化的姿态通过移除动态伪影增强场景重建。此外，引入时间动力学模块捕捉细粒度的姿态相关形变，并用残差颜色场建模光照变化。

Result: 在NeuMan和EMDB数据集上的大量实验表明，JOintGS实现了卓越的重建质量，在NeuMan数据集上PSNR比现有SOTA方法提高了2.1 dB，同时保持了实时渲染性能。相比基线方法，该方法对噪声初始化的鲁棒性显著增强。

Conclusion: JOintGS成功地解决了在野外场景下从单目RGB视频重建可驱动3D人体形象的挑战，通过联合优化相机、姿态和3D高斯表示，以及引入先进的模块，实现了高质量、实时且鲁棒的重建。

Abstract: Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting (3DGS) advances demonstrate impressive rendering quality and real-time performance, they critically depend on precise camera calibration and pose annotations, limiting their applicability in real-world settings. We present JOintGS, a unified framework that jointly optimizes camera extrinsics, human poses, and 3D Gaussian representations from coarse initialization through a synergistic refinement mechanism. Our key insight is that explicit foreground-background disentanglement enables mutual reinforcement: static background Gaussians anchor camera estimation via multi-view consistency; refined cameras improve human body alignment through accurate temporal correspondence; optimized human poses enhance scene reconstruction by removing dynamic artifacts from static constraints. We further introduce a temporal dynamics module to capture fine-grained pose-dependent deformations and a residual color field to model illumination variations. Extensive experiments on NeuMan and EMDB datasets demonstrate that JOintGS achieves superior reconstruction quality, with 2.1~dB PSNR improvement over state-of-the-art methods on NeuMan dataset, while maintaining real-time rendering. Notably, our method shows significantly enhanced robustness to noisy initialization compared to the baseline.Our source code is available at https://github.com/MiliLab/JOintGS.

</details>


### [83] [OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis](https://arxiv.org/abs/2602.04547)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto*

Main category: cs.CV

TL;DR: OmniRad 是一个在 120 万张医学影像上预训练的自监督基础模型，旨在提高在不同影像模态和任务上的表示复用性和跨任务迁移能力，并在分类和分割任务上取得了优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像分析中的预训练模型在不同模态和下游任务之间的表示复用性和迁移能力有限，需要一个能够更好支持异构任务的基础模型。

Method: 提出 OmniRad，一个在 120 万张医学影像上进行自监督预训练的基础模型，并采用了放射学启发的原则。在下游任务中，模型通过轻量级适配器（冻结骨干）或端到端微调（完全训练）进行评估，涵盖分类和分割任务。

Result: OmniRad 在 MedMNISTv2 分类任务上比竞争对手的基线模型提高了高达 2.05% 的 F1 分数。在使用冻结表示进行密集预测时，OmniRad 在六个 MedSegBench 数据集上取得了平均 Dice 分数的提升。可视化分析表明其特征聚类和模态相关性分离能力更强。

Conclusion: OmniRad 作为一个强大的自监督放射学基础模型，通过其预训练的表示，能够有效提升在多种医学影像任务（包括分类和分割）上的性能，并展现出优异的特征表示质量和跨任务迁移能力。

Abstract: Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.

</details>


### [84] [Multiview Self-Representation Learning across Heterogeneous Views](https://arxiv.org/abs/2602.04328)
*Jie Chen,Zhu Wang,Chuanbin Liu,Xi Peng*

Main category: cs.CV

TL;DR: 提出了一种多视图自表示学习（MSRL）方法，通过利用不同预训练模型提取的异构特征视图之间的自表示特性，学习不变表示，以应对不同模型导致特征分布差异的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型由于预训练目标或架构的差异，其生成的相同样本的特征分布存在显著不同。在完全无监督的迁移方式下，从大规模无标签视觉数据中学习不变表示仍然是一个重大挑战。

Method: 提出了一种多视图自表示学习（MSRL）方法。该方法首先从大规模无标签视觉数据中，通过与不同预训练模型进行迁移学习，提取异构多视图特征。然后，在每个冻结的预训练模型之上堆叠一个独立的线性模型。引入了一个基于自表示学习的信息传递机制，用于支持线性模型输出之间的特征聚合。此外，提出了一种分配概率分布一致性方案，通过利用不同视图间的互补信息来指导多视图自表示学习，从而强制执行不同线性模型间的表示不变性。

Result: 提出的MSRL方法通过理论分析了信息传递机制、分配概率分布一致性和增量视图。在多个基准视觉数据集上的大量实验表明，MSRL方法持续优于几种最先进的方法。

Conclusion: MSRL方法能够有效地学习跨不同预训练模型的不变表示，解决了现有方法在处理异构特征分布时的挑战，并在多项视觉任务上取得了SOTA性能。

Abstract: Features of the same sample generated by different pretrained models often exhibit inherently distinct feature distributions because of discrepancies in the model pretraining objectives or architectures. Learning invariant representations from large-scale unlabeled visual data with various pretrained models in a fully unsupervised transfer manner remains a significant challenge. In this paper, we propose a multiview self-representation learning (MSRL) method in which invariant representations are learned by exploiting the self-representation property of features across heterogeneous views. The features are derived from large-scale unlabeled visual data through transfer learning with various pretrained models and are referred to as heterogeneous multiview data. An individual linear model is stacked on top of its corresponding frozen pretrained backbone. We introduce an information-passing mechanism that relies on self-representation learning to support feature aggregation over the outputs of the linear model. Moreover, an assignment probability distribution consistency scheme is presented to guide multiview self-representation learning by exploiting complementary information across different views. Consequently, representation invariance across different linear models is enforced through this scheme. In addition, we provide a theoretical analysis of the information-passing mechanism, the assignment probability distribution consistency and the incremental views. Extensive experiments with multiple benchmark visual datasets demonstrate that the proposed MSRL method consistently outperforms several state-of-the-art approaches.

</details>


### [85] [Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception](https://arxiv.org/abs/2602.04343)
*Sebastian Jung,Leonard Klüpfel,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: 提出了一种名为 NeMO 的新型物体中心表示法，能够仅通过少量物体模板 RGB 视图，对训练中未见过的物体进行检测、分割和 6DoF 位姿估计，且无需相机特定参数或目标数据再训练。


<details>
  <summary>Details</summary>
Motivation: 研究目标是实现一种高效、可扩展的物体感知方法，能够处理训练中未见的物体，并支持多种感知任务，从而提高与新物体的交互能力。

Method: 该方法包含一个编码器，利用少量物体模板 RGB 视图生成包含语义和几何信息的稀疏点云。然后，一个解码器结合物体编码和查询图像，生成密集的预测结果（如检测、分割和位姿估计）。

Result: 在 BOP 基准的多个数据集和感知任务上取得了具有竞争力的、甚至达到最先进水平的结果，证明了该方法在少样本物体感知方面的有效性。

Conclusion: NeMO 是一种新颖的物体中心表示法，通过将物体信息外包到 NeMO 中，并使用单一网络处理多个感知任务，实现了无需重新训练或大量预处理的快速物体接入，显著提高了物体感知的可扩展性和效率。

Abstract: We present Neural Memory Object (NeMO), a novel object-centric representation that can be used to detect, segment and estimate the 6DoF pose of objects unseen during training using RGB images. Our method consists of an encoder that requires only a few RGB template views depicting an object to generate a sparse object-like point cloud using a learned UDF containing semantic and geometric information. Next, a decoder takes the object encoding together with a query image to generate a variety of dense predictions. Through extensive experiments, we show that our method can be used for few-shot object perception without requiring any camera-specific parameters or retraining on target data. Our proposed concept of outsourcing object information in a NeMO and using a single network for multiple perception tasks enhances interaction with novel objects, improving scalability and efficiency by enabling quick object onboarding without retraining or extensive pre-processing. We report competitive and state-of-the-art results on various datasets and perception tasks of the BOP benchmark, demonstrating the versatility of our approach. https://github.com/DLR-RM/nemo

</details>


### [86] [When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models](https://arxiv.org/abs/2602.04356)
*Jaehyun Kwak,Nam Cao,Boryeong Cho,Segyu Lee,Sumyeong Ahn,Se-Young Yun*

Main category: cs.CV

TL;DR: 提出了一种名为SAGA（Stage-wise Attention-Guided Attack）的攻击框架，通过引导扰动集中在高注意力区域，能够更高效地利用扰动预算，实现高攻击成功率和低可察觉性，适用于多种大型视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的针对大型视觉语言模型（LVLM）的对抗攻击方法，如基于随机裁剪的攻击，虽然有效，但未能高效利用有限的每像素扰动预算。研究人员希望找到一种更优的方法来暴露LVLM的安全漏洞。

Method: SAGA框架基于两个关键观察：（1）区域注意力分数与对抗损失敏感度呈正相关；（2）攻击高注意力区域会诱导注意力向后续显著区域结构化地重新分配。SAGA通过一个分阶段的框架，逐步将扰动集中在高注意力区域，从而实现对模型的攻击。

Result: SAGA框架能够更有效地利用受限的扰动预算，生成几乎不可察觉的对抗样本，并在测试的十个LVLM上持续实现了最先进的攻击成功率。

Conclusion: SAGA是一种高效且有效的对抗攻击框架，通过引导扰动到高注意力区域，可以显著提高对抗攻击的成功率，同时保持对抗样本的低可察觉性，为评估和增强LVLM的安全性提供了一种新方法。

Abstract: Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.

</details>


### [87] [Interactive Spatial-Frequency Fusion Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2602.04405)
*Yixin Zhu,Long Lv,Pingping Zhang,Xuehu Liu,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种名为 ISFM 的新型多模态图像融合框架，通过引入频率域信息和交互机制来改进空间特征的融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于频域的多模态图像融合方法通常采用简单的串联或并联融合策略，缺乏交互性，未能充分利用空间和频率信息的互补性。

Method: 提出了一种交互式空间-频率融合 Mamba (ISFM) 框架，包含三个主要部分：1. 模态特定提取器 (MSE)，用于提取不同模态的特征并建模长距离依赖。2. 多尺度频率融合 (MFF)，自适应地集成多尺度下的低频和高频分量。3. 交互式空间-频率融合 (ISF)，利用频率特征指导跨模态的空间特征融合。

Result: 在六个多模态图像融合数据集上的大量实验表明，ISFM 相比现有最先进的方法取得了更好的性能。

Conclusion: ISFM 框架能够有效地融合多模态图像，通过交互式的空间-频率融合机制，显著提升了融合图像的质量和信息保留能力。

Abstract: Multi-Modal Image Fusion (MMIF) aims to combine images from different modalities to produce fused images, retaining texture details and preserving significant information. Recently, some MMIF methods incorporate frequency domain information to enhance spatial features. However, these methods typically rely on simple serial or parallel spatial-frequency fusion without interaction. In this paper, we propose a novel Interactive Spatial-Frequency Fusion Mamba (ISFM) framework for MMIF. Specifically, we begin with a Modality-Specific Extractor (MSE) to extract features from different modalities. It models long-range dependencies across the image with linear computational complexity. To effectively leverage frequency information, we then propose a Multi-scale Frequency Fusion (MFF). It adaptively integrates low-frequency and high-frequency components across multiple scales, enabling robust representations of frequency features. More importantly, we further propose an Interactive Spatial-Frequency Fusion (ISF). It incorporates frequency features to guide spatial features across modalities, enhancing complementary representations. Extensive experiments are conducted on six MMIF datasets. The experimental results demonstrate that our ISFM can achieve better performances than other state-of-the-art methods. The source code is available at https://github.com/Namn23/ISFM.

</details>


### [88] [DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking](https://arxiv.org/abs/2602.04692)
*Sijia Chen,Lijuan Ma,Yanqiu Yu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: 提出RGBD Referring Multi-Object Tracking (DRMOT)任务，并构建了首个RGBD referring多目标跟踪数据集DRSet，以及一个名为DRTrack的多模态大模型（MLLM）引导的深度引导跟踪框架，用于利用RGB、深度和语言信息进行3D感知的跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有Referring Multi-Object Tracking（RMOT）模型仅依赖2D RGB数据，难以处理复杂的空间语义描述，并在遮挡情况下维持准确跟踪，因为缺乏明确的3D空间信息。引入3D深度信息是提升RMOT性能的关键。

Method: 1. 提出新的DRMOT任务，要求模型融合RGB、深度（D）和语言（L）信息进行3D感知的跟踪。 2. 构建了RGBD referring多目标跟踪数据集DRSet，包含RGB图像、深度图和语言描述。 3. 提出DRTrack框架，利用MLLM引导，进行深度感知的目标定位，并通过结合深度线索增强轨迹关联的鲁棒性。

Result: 在DRSet数据集上的大量实验证明了DRTrack框架的有效性，能够更好地利用RGB-D-L模态进行referring多目标跟踪。

Conclusion: 融合RGB、深度和语言信息能够显著提升Referring Multi-Object Tracking的性能，尤其是在处理空间语义和遮挡场景时。提出的DRMOT任务、DRSet数据集和DRTrack框架为该领域的研究提供了新的方向和工具。

Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.

</details>


### [89] [LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration](https://arxiv.org/abs/2602.04406)
*Jue Gong,Zihan Zhou,Jingkai Wang,Shu Li,Libo Liu,Jianliang Lan,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LCUDiff的稳定单步框架，通过将预训练的潜在扩散模型从4通道扩展到16通道的潜在空间，以提升人体重建（HBR）的保真度。该方法结合了通道分割蒸馏（CSD）和先验保持适应（PPA）技术，并引入了用于每样本解码器路由的解码器路由器（DeR），从而在保持单步效率的同时，显著提高了重建质量并减少了伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的人体中心图像恢复方法在保真度方面存在不足，尤其是在人体身体恢复（HBR）方面。近期的基于扩散的方法通常依赖预训练的文本到图像扩散模型，而其中的变分自编码器（VAE）会成为保真度的瓶颈。

Method: LCUDiff框架将预训练的潜在扩散模型从4通道潜在空间升级到16通道潜在空间。使用通道分割蒸馏（CSD）微调VAE，以保持前四个通道与预训练先验对齐，并将额外通道用于编码高频细节。此外，还设计了先验保持适应（PPA）来平滑4通道扩散骨干与16通道潜在空间之间的不匹配。最后，引入了每样本的解码器路由器（DeR），使用恢复质量分数注释进行路由，以提升视觉质量。

Result: 在合成和真实世界的数据集上进行的实验表明，LCUDiff在温和降级的情况下，在保真度和伪影数量方面取得了具有竞争力的结果，并且保持了单步效率。

Conclusion: LCUDiff通过扩展潜在空间、改进VAE微调技术和引入自适应解码器路由，成功地提升了人体重建的保真度，同时保持了单步恢复的效率，为解决现有方法的局限性提供了一种有效方案。

Abstract: Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at https://github.com/gobunu/LCUDiff.

</details>


### [90] [TrajVG: 3D Trajectory-Coupled Visual Geometry Learning](https://arxiv.org/abs/2602.04439)
*Xingyu Miao,Weiguang Zhao,Tao Lu,Linning Yu,Mulin Yu,Yang Long,Jiangmiao Pang,Junting Dong*

Main category: cs.CV

TL;DR: 提出TrajVG框架，通过显式预测相机坐标下的3D轨迹来解决视频3D重建中因物体运动导致的跨帧不一致问题，并实现了有监督和自监督混合训练。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈式多帧3D重建模型在处理视频中的物体运动时性能会下降，因为全局参考会变得模糊，局部点图会因为位姿估计不准确而漂移，导致跨帧错位和结构重复。

Method: TrajVG框架将稀疏轨迹、逐帧局部点图和相对相机位姿与几何一致性目标相结合：(i) 具有受控梯度流的双向轨迹-点图一致性；(ii) 由静态轨迹锚点驱动的位姿一致性目标，抑制动态区域的梯度。为了在缺乏3D轨迹标签的真实视频上进行训练，将约束重新表述为仅使用伪2D轨迹的自监督目标，实现混合监督下的统一训练。

Result: TrajVG在3D跟踪、位姿估计、点图重建和视频深度估计方面的实验结果表明，其性能优于当前的前馈式性能基线。

Conclusion: TrajVG通过显式地预测相机坐标下的3D轨迹，有效地解决了视频3D重建中的运动问题，并能够通过混合监督进行训练，在多个3D视觉任务上取得了优于现有方法的性能。

Abstract: Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.

</details>


### [91] [SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking](https://arxiv.org/abs/2602.04441)
*Weiguang Zhao,Haoran Xu,Xingyu Miao,Qin Zhao,Rui Zhang,Kaizhu Huang,Ning Gao,Peizhou Cao,Mingze Sun,Mulin Yu,Tao Lu,Linning Xu,Junting Dong,Jiangmiao Pang*

Main category: cs.CV

TL;DR: SynthVerse是一个大规模、多样化的合成数据集，旨在解决现有数据集中点跟踪数据不足和标注不准确的问题，并建立了一个新的基准来评估点跟踪算法在不同领域下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的点跟踪数据集在多样性和轨迹标注质量方面存在不足，限制了通用点跟踪算法的进步。

Method: 引入SynthVerse，一个包含动画电影、具身操作、场景导航和关节物体等新领域和物体类型的大规模合成数据集。建立了一个多样化的点跟踪基准来评估算法在不同领域下的表现。

Result: 在SynthVerse上训练的点跟踪器在泛化能力上有所提升，并且发现了现有跟踪器在多样化场景下的局限性。

Conclusion: SynthVerse能够促进更鲁棒的点跟踪算法的训练和评估，并为未来研究提供了一个更具挑战性的基准。

Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.

</details>


### [92] [Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search](https://arxiv.org/abs/2602.04454)
*Tianming Liang,Qirui Du,Jian-Fang Hu,Haichao Jiang,Zicheng Lin,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Seg-ReSearch的新型分割范式，通过结合内部推理和外部搜索，克服了多模态大语言模型（MLLM）的知识瓶颈，使其能够处理实时和领域特定的分割任务。为解决训练中的稀疏奖励问题，引入了分层奖励设计。在构建的OK-VOS基准和其他两个数据集上的实验证明了Seg-ReSearch的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的分割方法受限于模型冻结的内部知识，难以应对需要最新信息或领域特定知识的现实世界场景。研究旨在克服这一知识瓶颈，实现更灵活、更强大的分割能力。

Method: 提出Seg-ReSearch范式，通过使推理过程能够与外部知识进行交错搜索，来扩展MLLM的知识边界。为解决训练中的稀疏奖励问题，设计了一种分层奖励机制，结合了初始指导和渐进式激励。

Result: 在新的OK-VOS视频目标分割基准和两个现有的推理分割基准上进行了评估，Seg-ReSearch在这些任务上均显著优于现有最先进方法。

Conclusion: Seg-ReSearch是一种有效的分割新范式，通过允许模型进行外部搜索，成功克服了MLLM知识固化的限制，在需要开放世界知识的分割任务中表现出色。提出的分层奖励设计也有效地解决了训练中的挑战。

Abstract: Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.

</details>


### [93] [Temporal Slowness in Central Vision Drives Semantic Object Learning](https://arxiv.org/abs/2602.04462)
*Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch*

Main category: cs.CV

TL;DR: 本研究模拟人类视觉经验，表明结合中央视觉和时间慢速学习能够改善物体语义表征的学习。


<details>
  <summary>Details</summary>
Motivation: 人类在几乎不受监督的情况下，从以自我为中心的视觉流中获取语义物体表征。视觉系统中央视野分辨率高，并且会学习时间上相近的输入的相似表征。这促使研究者探索中央视觉和慢速学习在物体语义表征形成中的作用。

Method: 使用 Ego4D 数据集模拟了五个月的人类视觉经验，并使用先进的注视预测模型生成注视坐标。利用这些预测，提取模拟中央视觉的图像块，并训练一个时间对比自监督学习模型。

Result: 结合时间慢速和中央视觉能提升物体不同语义方面的表征编码。中央视觉加强了前景物体特征提取，而时间慢速（尤其在注视性眼动期间）则使得模型能编码更广泛的物体语义信息。

Conclusion: 研究结果揭示了人类可能通过自然视觉经验发展语义物体表征的机制，强调了中央视觉和时间慢速学习的重要性。

Abstract: Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations from human-like visual experience. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects. These findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience.

</details>


### [94] [Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization](https://arxiv.org/abs/2602.04820)
*Farzia Hossain,Samanta Ghosh,Shahida Begum,B. M. Shahria Alam,Mohammad Tahmid Noor,Md Parvez Mia,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的甲病自动分类模型，使用 InceptionV3 模型在公开数据集上取得了 95.57% 的准确率，并结合了对抗性训练和 SHAP 值来增强模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 甲病在所有年龄段，尤其是在老年人中日益普遍，但常被忽视，直到病情严重。早期检测和准确诊断甲病很重要，因为它们有时能反映身体的健康问题。然而，由于不同疾病类型的视觉差异难以区分，甲病的诊断具有挑战性。

Method: 本文使用了一个包含 3,835 张图像的公开数据集，涵盖六种甲病类别。将所有图像统一调整为 224x224 像素。训练并分析了四种卷积神经网络（CNN）模型：InceptionV3、DenseNet201、EfficientNetV2 和 ResNet50。通过对抗性训练来增强模型的鲁棒性，并使用 SHAP 值来解释模型的决策过程。

Result: 在四种 CNN 模型中，InceptionV3 的表现最佳，准确率为 95.57%，其次是 DenseNet201，准确率为 94.79%。

Conclusion: 该机器学习模型能够准确地对甲病进行自动分类，为医生提供了辅助诊断工具，有望提高诊断的准确性和速度。

Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.

</details>


### [95] [SALAD-Pan: Sensor-Agnostic Latent Adaptive Diffusion for Pan-Sharpening](https://arxiv.org/abs/2602.04473)
*Junjie Li,Congyang Ou,Haokui Zhang,Guoting Wei,Shengqin Jiang,Ying Li,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种名为SALAD-Pan的传感器无关的潜在空间扩散方法，用于高效的全色锐化，通过VAE将HRMS图像编码为紧凑的潜在表示，并通过交互式控制结构注入光谱物理特性，并添加跨谱注意力模块来提高精度，实现了高精度融合和显著的推理速度提升，并具有零样本跨传感器能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的全色锐化方法存在像素空间扩散导致的高延迟和传感器特定限制问题。

Method: 1. 使用单通道VAE将HRMS图像编码为潜在表示，支持不同通道数的MS图像。2. 通过单向和双向交互控制结构将光谱物理属性、PAN和MS图像注入扩散模型。3. 在扩散模型中心层添加轻量级跨谱注意力模块。

Result: SALAD-Pan在GaoFen-2、QuickBird和WorldView-3数据集上优于现有最先进的基于扩散的方法，推理速度提高了2-3倍，并展现了强大的零样本（跨传感器）能力。

Conclusion: SALAD-Pan是一种高效、传感器无关的全色锐化方法，通过潜在空间扩散和创新的控制机制，在精度和效率上均取得了显著进展，并具备良好的泛化能力。

Abstract: Recently, diffusion models bring novel insights for Pan-sharpening and notably boost fusion precision. However, most existing models perform diffusion in the pixel space and train distinct models for different multispectral (MS) imagery, suffering from high latency and sensor-specific limitations. In this paper, we present SALAD-Pan, a sensor-agnostic latent space diffusion method for efficient pansharpening. Specifically, SALAD-Pan trains a band-wise single-channel VAE to encode high-resolution multispectral (HRMS) into compact latent representations, supporting MS images with various channel counts and establishing a basis for acceleration. Then spectral physical properties, along with PAN and MS images, are injected into the diffusion backbone through unidirectional and bidirectional interactive control structures respectively, achieving high-precision fusion in the diffusion process. Finally, a lightweight cross-spectral attention module is added to the central layer of diffusion model, reinforcing spectral connections to boost spectral consistency and further elevate fusion precision. Experimental results on GaoFen-2, QuickBird, and WorldView-3 demonstrate that SALAD-Pan outperforms state-of-the-art diffusion-based methods across all three datasets, attains a 2-3x inference speedup, and exhibits robust zero-shot (cross-sensor) capability.

</details>


### [96] [Vision-aligned Latent Reasoning for Multi-modal Large Language Model](https://arxiv.org/abs/2602.04476)
*Byungwoo Jeon,Yoonwoo Jeong,Hyunseok Lee,Minsu Cho,Jinwoo Shin*

Main category: cs.CV

TL;DR: Vision-aligned Latent Reasoning (VaLR) is a new framework that generates vision-aligned latent tokens before each reasoning step to improve multi-step reasoning in MLLMs, addressing visual information dilution and achieving better performance and test-time scaling.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with multi-step reasoning due to the dilution of visual information in long-context generation, limiting their ability to exploit test-time scaling.

Method: VaLR dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step. It aligns intermediate MLLM embeddings with vision encoder embeddings during training to preserve visual knowledge in the latent space.

Result: VaLR consistently outperforms existing methods on benchmarks requiring long-context understanding or precise visual perception. It shows improved test-time scaling and achieves a 19.9%p gain over Qwen2.5-VL on VSI-Bench, increasing performance from 33.0% to 52.9%.

Conclusion: VaLR is an effective reasoning framework that enhances MLLMs' multi-step reasoning capabilities by dynamically injecting perceptual cues into the latent space, leading to improved performance and test-time scaling.

Abstract: Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.

</details>


### [97] [Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models](https://arxiv.org/abs/2602.04549)
*Cem Eteke,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 本文提出了一种名为NiFi的方法，通过一种感知失真感知、基于扩散模型的一步蒸馏技术，实现了3D高斯泼溅（3DGS）的极端压缩，能在极低比特率下（低至0.1MB）达到最先进的感知质量，并将速率提高了1000倍。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯泼溅（3DGS）方法虽然在渲染方面表现出色，但其空间需求较大，限制了其在沉浸式通信等应用中的使用。现有的3DGS压缩方法在低比特率下会导致视觉质量严重下降的伪影。

Method: NiFi方法采用一种基于扩散模型的一步蒸馏技术，该技术能够感知并处理压缩过程中引入的伪影，从而实现3DGS的极端压缩。

Result: NiFi方法在极低的比特率下（低至0.1MB）实现了最先进的感知质量，与现有的3DGS方法相比，在相当的感知性能下，速率提高了1000倍。

Conclusion: NiFi方法是一种有效的3DGS压缩技术，能够显著降低存储需求，同时保持高质量的视觉效果，为3DGS在资源受限的应用中提供了可能性。

Abstract: 3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.

</details>


### [98] [Understanding Degradation with Vision Language Model](https://arxiv.org/abs/2602.04565)
*Guanzhou Lan,Chenyi Liao,Yuqi Yang,Qianli Ma,Zhigang Wang,Dong Wang,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为DU-VLM的多模态模型，它通过将视觉降质理解视为一个分层结构化预测任务，实现了对图像降质类型、参数键和连续物理值的同步估计，并成功将其应用于零样本控制扩散模型进行图像修复。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在描述性任务上表现出色，但在理解图像降质背后的参数化物理特性方面存在不足。作者旨在解决这一问题，将降质理解提升到参数化和物理层面。

Method: 1. 将降质理解重新定义为分层结构化预测任务，包括降质类型、参数键和连续物理值的估计。2. 提出DU-VLM模型，采用多模态链式思维（chain-of-thought）架构，通过监督微调和基于结构化奖励的强化学习进行训练。3. 利用DU-VLM作为零样本控制器，对预训练的扩散模型进行图像修复，无需微调生成主干。4. 构建了DU-110k数据集，包含110,000个带物理注释的干净-降质图像对。

Result: DU-VLM在图像修复任务上取得了显著成果，能够实现高保真度的修复。在DU-110k数据集上的实验表明，DU-VLM在准确性和鲁棒性方面显著优于通用基线模型，并能泛化到未见过的数据分布。

Conclusion: 本文成功地将视觉降质理解建模为一个统一的自回归预测任务，并提出了DU-VLM模型，该模型不仅能够准确预测降质的物理参数，还能作为强大的零样本控制器，实现高质量的图像修复，并且在新的数据集上表现出良好的泛化能力。

Abstract: Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.

</details>


### [99] [PEPR: Privileged Event-based Predictive Regularization for Domain Generalization](https://arxiv.org/abs/2602.04583)
*Gabriele Magrini,Federico Becattini,Niccolò Biondi,Pietro Pala*

Main category: cs.CV

TL;DR: 提出了一种利用事件相机作为特权信息源的跨模态框架（PEPR），用于训练更鲁棒的单一模态RGB模型，以解决域偏移问题，并在不牺牲语义信息的情况下提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在处理与训练数据不同的现实世界条件时，容易受到域偏移的影响，这阻碍了其在实际应用中的部署。因此，需要一种能够提高模型在不同域之间泛化能力的方法。

Method: 利用事件相机（仅在训练期间可用）作为特权信息源，在学习使用特权信息（LUPI）范式下，提出了一种名为“特权事件驱动的预测正则化”（PEPR）的跨模态框架。该框架将LUPI问题重构为一个共享潜在空间中的预测问题，使RGB编码器通过预测基于事件的潜在特征来学习鲁棒性，而不是直接进行跨模态特征对齐。

Result: 所提出的PEPR方法训练出的独立RGB模型，在面对日间到夜间的域偏移以及其他域偏移时，显著提高了鲁棒性。在目标检测和语义分割任务上，其性能均优于基于对齐的基线方法。

Conclusion: 通过将事件相机作为特权信息源，并采用PEPR框架，可以在训练过程中将事件相机提供的域不变性知识蒸馏到RGB模型中，从而在不牺牲语义信息的情况下，有效提升RGB模型的域泛化能力，解决域偏移问题。

Abstract: Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.

</details>


### [100] [SalFormer360: a transformer-based saliency estimation model for 360-degree videos](https://arxiv.org/abs/2602.04584)
*Mahmoud Z. A. Wahba,Francesco Barbato,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的360度视频显著性预测模型SalFormer360，通过结合SegFormer编码器和自定义解码器，并引入观看中心偏差，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 360度视频的显著性估计在视口预测和沉浸式内容优化等应用中至关重要，但现有方法存在不足。

Method: 将2D分割任务的SegFormer模型微调用于360度视频，并结合自定义解码器，同时引入观看中心偏差来提升预测精度。

Result: 在Sport360、PVS-HM和VR-EyeTracking三个数据集上，SalFormer360的Pearson相关系数相较于现有最先进方法分别提高了8.4%、2.5%和18.6%。

Conclusion: SalFormer360是一种有效的360度视频显著性估计模型，在多个基准数据集上取得了优于现有技术水平的表现。

Abstract: Saliency estimation has received growing attention in recent years due to its importance in a wide range of applications. In the context of 360-degree video, it has been particularly valuable for tasks such as viewport prediction and immersive content optimization. In this paper, we propose SalFormer360, a novel saliency estimation model for 360-degree videos built on a transformer-based architecture. Our approach is based on the combination of an existing encoder architecture, SegFormer, and a custom decoder. The SegFormer model was originally developed for 2D segmentation tasks, and it has been fine-tuned to adapt it to 360-degree content. To further enhance prediction accuracy in our model, we incorporated Viewing Center Bias to reflect user attention in 360-degree environments. Extensive experiments on the three largest benchmark datasets for saliency estimation demonstrate that SalFormer360 outperforms existing state-of-the-art methods. In terms of Pearson Correlation Coefficient, our model achieves 8.4% higher performance on Sport360, 2.5% on PVS-HM, and 18.6% on VR-EyeTracking compared to previous state-of-the-art.

</details>


### [101] [ImmuVis: Hyperconvolutional Foundation Model for Imaging Mass Cytometry](https://arxiv.org/abs/2602.04585)
*Marcin Możejko,Dawid Uchal,Krzysztof Gogolewski,Piotr Kupidura,Szymon Łukasik,Jakub Giezgała,Tomasz Nocoń,Kacper Pietrzyk,Robert Pieniuta,Mateusz Sulimowicz,Michal Orzyłowski,Tomasz Siłkowski,Karol Zagródka,Eike Staub,Ewa Szczurek*

Main category: cs.CV

TL;DR: ImmuVis是一个高效的卷积基础模型，用于影像质谱流式细胞术 (IMC)，它通过引入标记自适应超卷积来处理可变标记集，并使用自监督学习进行预训练，在虚拟染色和下游分类任务中表现优于现有方法，同时计算成本更低，并能提供校准的不确定性。


<details>
  <summary>Details</summary>
Motivation: 标准的视觉模型假设固定的通道空间，但影像质谱流式细胞术 (IMC) 的标记集在不同研究中经常变化，这违反了这一核心假设，阻碍了通用模型的开发。

Method: ImmuVis 引入了标记自适应超卷积，从学习到的标记嵌入中生成卷积核，使其能够处理任意标记子集而无需重新训练。该模型在 IMC17M 数据集上使用自监督掩码重建进行预训练。

Result: ImmuVis 在虚拟染色和下游分类任务中优于最先进的基线和消融实验，计算成本远低于基于 Transformer 的模型，并且是唯一能通过异方差似然目标提供校准不确定性的模型。

Conclusion: ImmuVis 是一个实用且高效的基础模型，适用于真实的 IMC 数据建模，能够处理可变的标记集，并在多种任务中取得 SOTA 性能，同时提供校准不确定性。

Abstract: We present ImmuVis, an efficient convolutional foundation model for imaging mass cytometry (IMC), a high-throughput multiplex imaging technology that handles molecular marker measurements as image channels and enables large-scale spatial tissue profiling. Unlike natural images, multiplex imaging lacks a fixed channel space, as real-world marker sets vary across studies, violating a core assumption of standard vision backbones. To address this, ImmuVis introduces marker-adaptive hyperconvolutions that generate convolutional kernels from learned marker embeddings, enabling a single model to operate on arbitrary measured marker subsets without retraining. We pretrain ImmuVis on the largest to-date dataset, IMC17M (28 cohorts, 24,405 images, 265 markers, over 17M patches), using self-supervised masked reconstruction. ImmuVis outperforms SOTA baselines and ablations in virtual staining and downstream classification tasks at substantially lower compute cost than transformer-based alternatives, and is the sole model that provides calibrated uncertainty via a heteroscedastic likelihood objective. These results position ImmuVis as a practical, efficient foundation model for real-world IMC modeling.

</details>


### [102] [A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction](https://arxiv.org/abs/2602.04624)
*Raúl Jiménez Cruz,César Torres-Huitzil,Marco Franceschetti,Ronny Seiger,Luciano García-Bañuelos,Barbara Weber*

Main category: cs.CV

TL;DR: 本文介绍了一个包含11,884张模拟采血（静脉穿刺）过程训练臂图像的数据集，并包含五种医学相关类别的多边形标注，旨在推动医疗培训自动化和人-物体交互研究。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是为了推进医疗培训自动化和人-物体交互的研究，开发能够提供结构化反馈给医学生和护士的教育系统，并对静脉穿刺流程进行分析。

Method: 研究人员从高清晰度视频中提取了11,884张图像，并使用结构相似性指数测量（SSIM）过滤来减少冗余。所有视频在帧选择前都经过了自动面部匿名化处理。每张图像都为注射器、橡皮筋、消毒湿巾、手套和训练臂这五类医学相关物体提供了多边形标注，并导出为现代目标检测框架（如YOLOv8）兼容的分割格式。

Result: 创建了一个包含11,884张标注图像的数据集，用于模拟采血程序。该数据集被划分为训练（70%）、验证（15%）和测试（15%）子集，并提供用于静脉穿刺工具检测、流程步骤识别、工作流程分析、合规性检查等应用。

Conclusion: 该数据集为静脉穿刺训练和相关技术的研究提供了一个宝贵的资源，能够支持多种自动化和人-物体交互的应用，并促进开发更有效的医疗培训工具。

Abstract: This data article presents a dataset of 11,884 labeled images documenting a simulated blood extraction (phlebotomy) procedure performed on a training arm. Images were extracted from high-definition videos recorded under controlled conditions and curated to reduce redundancy using Structural Similarity Index Measure (SSIM) filtering. An automated face-anonymization step was applied to all videos prior to frame selection. Each image contains polygon annotations for five medically relevant classes: syringe, rubber band, disinfectant wipe, gloves, and training arm. The annotations were exported in a segmentation format compatible with modern object detection frameworks (e.g., YOLOv8), ensuring broad usability. This dataset is partitioned into training (70%), validation (15%), and test (15%) subsets and is designed to advance research in medical training automation and human-object interaction. It enables multiple applications, including phlebotomy tool detection, procedural step recognition, workflow analysis, conformance checking, and the development of educational systems that provide structured feedback to medical trainees. The data and accompanying label files are publicly available on Zenodo.

</details>


### [103] [PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective](https://arxiv.org/abs/2602.04657)
*Haokui Zhang,Congyang Ou,Dawei Yan,Peng Wang,Qingsen Yan,Ying Li,Rong Xiao,Chunhua Shen*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 PIO-FVLM 的训练无关型视觉代币压缩方法，通过优化输出结果不变性来选择重要代币，实现了显著的加速和性能保持。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）的冗余视觉代币压缩方法依赖启发式规则，存在压缩性能和部署上的局限性。研究旨在从推理目标出发，提出一种更有效的方法。

Method: PIO-FVLM 将视觉代币压缩转化为保持输出结果不变性，并根据其对该目标的重要性来选择代币。具体方法包括：1. 通过层局部代理损失生成代币级梯度显著性来重新排序视觉代币；2. 遵循非最大抑制（NMS）原则选择最有价值的代币。该方法训练无关，兼容 FlashAttention，可独立部署或与现有压缩方法结合。

Result: 在 LLaVA-Next-7B 上，PIO-FVLM 保留了 11.1% 的视觉代币，性能保持了 97.2%，预填充速度提升 2.67 倍，推理速度提升 2.11 倍，FLOPs 降低 6.22 倍，KV 缓存开销降低 6.05 倍。

Conclusion: PIO-FVLM 是一种有效的视觉代币压缩方法，能够显著提高 VLM 的推理速度，同时保持出色的性能，并且易于部署。

Abstract: Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.

</details>


### [104] [Annotation Free Spacecraft Detection and Segmentation using Vision Language Models](https://arxiv.org/abs/2602.04699)
*Samet Hicsonmez,Jose Sosa,Dan Pineau,Inder Pal Singh,Arunkumar Rathinam,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 该研究提出了一种利用视觉语言模型（VLMs）进行无标注空间目标检测和分割的方法，通过生成伪标签并用于教师-学生蒸馏框架，在多个数据集上显著提高了分割性能。


<details>
  <summary>Details</summary>
Motivation: 在空间领域，由于可见度低、光照变化、目标与背景融合等因素，手动标注困难且成本高昂。因此，开发无需大量手动标注即可检测和分割航天器及轨道目标的方法至关重要。

Method: 该方法首先使用预训练的VLM为少量未标记的真实数据自动生成伪标签。然后，利用这些伪标签在一个教师-学生标签蒸馏框架中训练轻量级模型。

Result: 尽管伪标签存在固有噪声，但蒸馏过程相比直接的零样本VLM推理带来了显著的性能提升。在SPARK-2024、SPEED+和TANGO数据集的分割任务上，平均精度（AP）一致性地提高了高达10个点。

Conclusion: 研究提出的无标注检测和分割流水线能够有效地利用VLMs，并通过标签蒸馏克服伪标签的噪声问题，在空间目标分割任务中取得了显著的性能提升，表明其在空间应用中的潜力。

Abstract: Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.

</details>


### [105] [How to rewrite the stars: Mapping your orchard over time through constellations of fruits](https://arxiv.org/abs/2602.04722)
*Gonçalo P. Matos,Carlos Santiago,João P. Costeira,Ricardo L. Saldanha,Ernesto M. Morgado*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D质心星座的新方法，用于在不同时间拍摄的视频中匹配同一批水果，解决了以往方法在非刚性、遮挡和特征稀疏等方面的不足，并可用于构建果园地图、相机位姿定位和机器人自主导航。


<details>
  <summary>Details</summary>
Motivation: 人工测量水果生长过程耗时费力且难以规模化，现有计算机视觉方法在跨时间匹配同一水果方面存在困难，需要一种更鲁棒、更通用的解决方案。

Method: 提出了一种基于3D质心星座的新范式，并设计了一种适用于稀疏3D点云的描述符，用于在不同视频中匹配水果。通过匹配星座而非单个水果来解决非刚性、遮挡和特征稀疏的问题。

Result: 该方法成功实现了跨视频、跨时间的水果匹配，并可用于构建果园地图、定位相机位姿（6DoF）。

Conclusion: 提出的基于3D质心星座的水果匹配方法是一种新颖且有效的解决方案，能够为机器人自主导航和选择性采摘等应用提供支持。

Abstract: Following crop growth through the vegetative cycle allows farmers to predict fruit setting and yield in early stages, but it is a laborious and non-scalable task if performed by a human who has to manually measure fruit sizes with a caliper or dendrometers. In recent years, computer vision has been used to automate several tasks in precision agriculture, such as detecting and counting fruits, and estimating their size. However, the fundamental problem of matching the exact same fruits from one video, collected on a given date, to the fruits visible in another video, collected on a later date, which is needed to track fruits' growth through time, remains to be solved. Few attempts were made, but they either assume that the camera always starts from the same known position and that there are sufficiently distinct features to match, or they used other sources of data like GPS. Here we propose a new paradigm to tackle this problem, based on constellations of 3D centroids, and introduce a descriptor for very sparse 3D point clouds that can be used to match fruits across videos. Matching constellations instead of individual fruits is key to deal with non-rigidity, occlusions and challenging imagery with few distinct visual features to track. The results show that the proposed method can be successfully used to match fruits across videos and through time, and also to build an orchard map and later use it to locate the camera pose in 6DoF, thus providing a method for autonomous navigation of robots in the orchard and for selective fruit picking, for example.

</details>


### [106] [Mitigating Long-Tail Bias via Prompt-Controlled Diffusion Augmentation](https://arxiv.org/abs/2602.04749)
*Buddhi Wijenayake,Nichula Wasalathilake,Roshan Godaliyadda,Vijitha Herath,Parakrama Ekanayake,Vishal M. Patel*

Main category: cs.CV

TL;DR: 研究提出了一种基于提示的扩散增强框架，通过生成可控的合成数据对（标签-图像）来解决高分辨率遥感影像语义分割中数据长尾分布和城乡领域差异的问题，并取得了显著的分割性能提升，尤其是在少数类和城乡泛化方面。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感影像的语义分割面临像素长尾分布和城乡领域差异的挑战。现有的训练数据存在严重的类别不平衡，且城乡区域的视觉特征和类别频率统计不一致。

Method: 提出了一种两阶段的提示控制扩散增强框架：阶段A使用领域感知、掩码比例条件离散扩散模型生成满足用户指定类别比例目标并保留类别共现结构的布局（标签图）；阶段B利用带有ControlNet引导的Stable Diffusion将布局转换为逼真的、领域一致的图像。将合成数据对与真实数据混合用于训练。

Result: 将生成的比率和领域控制的合成数据对与真实数据混合，在多个分割骨干网络上均取得了持续的性能提升。提升效果主要集中在少数类别，并改善了城乡领域的泛化能力。

Conclusion: 可控的数据增强是一种实用的机制，可以有效缓解遥感影像语义分割中的长尾分布偏见，并提高模型在不同领域（城乡）的泛化能力。

Abstract: Semantic segmentation of high-resolution remote-sensing imagery is critical for urban mapping and land-cover monitoring, yet training data typically exhibits severe long-tailed pixel imbalance. In the dataset LoveDA, this challenge is compounded by an explicit Urban/Rural split with distinct appearance and inconsistent class-frequency statistics across domains. We present a prompt-controlled diffusion augmentation framework that synthesizes paired label--image samples with explicit control of both domain and semantic composition. Stage~A uses a domain-aware, masked ratio-conditioned discrete diffusion model to generate layouts that satisfy user-specified class-ratio targets while respecting learned co-occurrence structure. Stage~B translates layouts into photorealistic, domain-consistent images using Stable Diffusion with ControlNet guidance. Mixing the resulting ratio and domain-controlled synthetic pairs with real data yields consistent improvements across multiple segmentation backbones, with gains concentrated on minority classes and improved Urban and Rural generalization, demonstrating controllable augmentation as a practical mechanism to mitigate long-tail bias in remote-sensing segmentation. Source codes, pretrained models, and synthetic datasets are available at \href{https://github.com/Buddhi19/SyntheticGen.git}{Github}

</details>


### [107] [Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention](https://arxiv.org/abs/2602.04789)
*Chengtao Lv,Yumeng Shi,Yushi Huang,Ruihao Gong,Shen Ren,Wenya Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Light Forcing 的稀疏注意力机制，专门用于解决自回归视频生成模型中注意力计算的二次方复杂度瓶颈，在保证生成质量的同时显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视频生成模型虽然在视觉保真度和交互性方面有所提升，但注意力机制的二次方复杂度限制了其高效部署。现有稀疏注意力方法在双向模型上有效，但直接应用于自回归模型会导致性能下降，原因在于孤立考虑块生成以及对过去信息丰富上下文的利用不足。

Method: 本文提出 Light Forcing，一种专门为自回归视频生成设计的稀疏注意力解决方案。它包含两个核心机制：1. 块感知增长 (Chunk-Aware Growth) 机制，通过量化估计每个块的贡献来分配稀疏度，实现渐进式稀疏度增加，使当前块能继承早期块的先验知识。2. 分层稀疏注意力 (Hierarchical Sparse Attention)，以粗粒度到细粒度的方式捕捉历史和局部信息，采用帧和块两个层级的掩码选择策略，自适应处理不同的注意力模式。

Result: 实验表明，Light Forcing 在生成质量上优于现有稀疏注意力方法（VBench 上达到 84.5），并在效率上实现了 1.2~1.3 倍的端到端加速。结合 FP8 量化和 LightVAE，Light Forcing 在 RTX 5090 GPU 上实现了 2.3 倍的加速和 19.7 FPS 的帧率。

Conclusion: Light Forcing 是第一个针对自回归视频生成模型量身定制的稀疏注意力解决方案，成功解决了注意力计算的效率瓶颈，并在不牺牲生成质量的前提下显著提高了生成速度，为自回归视频生成模型的实际应用提供了可行方案。

Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \textsc{Light Forcing}, the \textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\eg, 84.5 on VBench) and efficiency (\eg, $1.2{\sim}1.3\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \textsc{Light Forcing} further achieves a $2.3\times$ speedup and 19.7\,FPS on an RTX~5090 GPU. Code will be released at \href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.

</details>


### [108] [X2HDR: HDR Image Generation in a Perceptually Uniform Space](https://arxiv.org/abs/2602.04814)
*Ronghuan Wu,Wanchao Su,Kede Ma,Jing Liao,Rafał K. Mantiuk*

Main category: cs.CV

TL;DR: 本研究提出了一种无需从头训练即可将现有低动态范围（LDR）扩散模型适配到高动态范围（HDR）图像生成的方法，通过将HDR输入转换为感知均匀编码来解决LDR和HDR数据之间的统计差异，并仅对去噪器进行低秩适应微调，实现了文本到HDR合成和单图像RAW到HDR重建。


<details>
  <summary>Details</summary>
Motivation: 现有的先进图像生成模型（如Stable Diffusion和FLUX）通常仅限于低动态范围（LDR）输出，因为缺乏大规模的HDR训练数据。研究人员希望探索一种有效的方法来利用现有预训练的LDR模型生成HDR图像。

Method: 研究人员提出了一种高效的适配策略：
1. 将HDR输入转换为感知均匀编码（如PU21或PQ）。
2. 冻结预训练的变分自编码器（VAE）。
3. 仅对去噪器进行低秩适应（LoRA）微调。
这种方法在感知均匀空间中进行，以弥合LDR和HDR数据之间的统计差异。

Result: 实验表明，在感知均匀编码下，LDR预训练的VAE能够高保真地重建HDR输入，而直接使用线性RGB输入则会导致严重的性能下降。通过感知均匀编码的适配策略，模型在感知保真度、文本-图像一致性以及有效动态范围方面均优于之前的技术。

Conclusion: 本研究成功地展示了如何通过在感知均匀空间中进行适配，高效地将现有的LDR扩散模型扩展到HDR图像生成，无需从头训练，并且取得了优于先前方法的性能。

Abstract: High-dynamic-range (HDR) formats and displays are becoming increasingly prevalent, yet state-of-the-art image generators (e.g., Stable Diffusion and FLUX) typically remain limited to low-dynamic-range (LDR) output due to the lack of large-scale HDR training data. In this work, we show that existing pretrained diffusion models can be easily adapted to HDR generation without retraining from scratch. A key challenge is that HDR images are natively represented in linear RGB, whose intensity and color statistics differ substantially from those of sRGB-encoded LDR images. This gap, however, can be effectively bridged by converting HDR inputs into perceptually uniform encodings (e.g., using PU21 or PQ). Empirically, we find that LDR-pretrained variational autoencoders (VAEs) reconstruct PU21-encoded HDR inputs with fidelity comparable to LDR data, whereas linear RGB inputs cause severe degradations. Motivated by this finding, we describe an efficient adaptation strategy that freezes the VAE and finetunes only the denoiser via low-rank adaptation in a perceptually uniform space. This results in a unified computational method that supports both text-to-HDR synthesis and single-image RAW-to-HDR reconstruction. Experiments demonstrate that our perceptually encoded adaptation consistently improves perceptual fidelity, text-image alignment, and effective dynamic range, relative to previous techniques.

</details>


### [109] [VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?](https://arxiv.org/abs/2602.04802)
*Qing'an Liu,Juntong Feng,Yuhao Wang,Xinzhe Han,Yujie Cheng,Yue Zhu,Haiwen Diao,Yunzhi Zhuge,Huchuan Lu*

Main category: cs.CV

TL;DR: 现有的大部分视觉语言模型（VLMs）在处理纯文本查询时表现出色，但在处理图像中嵌入的视觉化文本时性能会显著下降，存在明显的“模态鸿沟”。VISTA-Bench 是一个专门用于评估模型视觉化文本理解能力的新基准，它通过对比纯文本和视觉化文本查询来揭示这一问题，并指出模型对渲染变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）的基准测试主要集中在纯文本查询，忽略了现实世界中常见的图像中嵌入的视觉化文本。研究者希望评估VLMs在处理这种输入时的表现，并找出其局限性。

Method: 研究者提出了VISTA-Bench，一个系统的基准测试框架，用于评估视觉化文本的理解能力。该基准通过在受控的渲染条件下，对比纯文本查询和具有相同语义的视觉化文本查询，来评估模型在多模态感知、推理以及单模态理解等方面的能力。研究者评估了20多个代表性的VLMs。

Result: 评估结果显示，在纯文本查询上表现良好的VLMs，在面对同等语义的视觉化文本查询时，性能会显著下降，存在明显的“模态鸿沟”。这种差距在增加了感知难度（例如，渲染变化）时会进一步加剧，表明模型对渲染变化的敏感性。

Conclusion: VISTA-Bench 证明了现有VLMs在处理视觉化文本时存在显著的局限性。研究者提出，需要开发更统一的语言表示方法，使其能够同时处理标记化文本和像素中的视觉化文本，以弥合这种模态鸿沟。VISTA-Bench 提供了一个诊断模型局限性并指导未来研究方向的框架。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.

</details>


### [110] [LitS: A novel Neighborhood Descriptor for Point Clouds](https://arxiv.org/abs/2602.04838)
*Jonatan B. Bastos,Francisco F. Rivera,Oscar G. Lorenzo,David L. Vilariño,José C. Cabaleiro,Alberto M. Esmorís,Tomás F. Pena*

Main category: cs.CV

TL;DR: 本文提出了一种名为LitS的新型2D和3D点云邻域描述符，它能够有效地捕捉局部几何信息，并具有适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着3D扫描技术的进步，点云在科学和技术领域得到广泛应用，而准确刻画点云局部几何特征的邻域描述符至关重要。现有描述符可能在处理点云密度变化和噪声等问题时不够理想，因此需要更优的解决方案。

Method: LitS是一种定义在单位圆上的分段常值函数，用于描述点云的局部邻域。它通过在局部参考系统中定义方向，并计算以该方向为中心的锥形区域内的邻居数量来捕捉局部几何信息。LitS有两种版本（'regular'和'cumulative'）和两个可调参数，以适应不同的点云类型和场景。

Result: LitS能够有效地捕捉点云局部几何的细微差别，并能通过分析LitS在邻近点之间的变化来获得全局结构理解。它对点云密度变化和噪声具有鲁棒性。

Conclusion: LitS是一种多功能的点云邻域描述符，能够有效捕捉局部点排列的细微之处，并能应对点云数据中的常见问题，为点云分析提供了新的工具。

Abstract: With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.

</details>


### [111] [When LLaVA Meets Objects: Token Composition for Vision-Language-Models](https://arxiv.org/abs/2602.04864)
*Soumya Jahagirdar,Walid Bousselham,Anna Kukleva,Hilde Kuehne*

Main category: cs.CV

TL;DR: Mask-LLaVA 提出了一种新颖的视觉语言模型（VLM）框架，通过结合掩码对象表示、全局令牌和局部斑块令牌，创建了紧凑但信息丰富的视觉表示，从而在推理时显著减少视觉令牌数量，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视觉语言模型（VLM）通常需要大量的视觉令牌来表示图像，导致推理时计算量大。研究旨在解决这一效率问题。

Method: 提出 Mask-LLaVA 框架，结合掩码对象表示、全局令牌和局部斑块令牌来创建紧凑的视觉表示。在训练时使用所有令牌，但在推理时可以灵活地减少掩码对象令牌的数量，而无需重新训练。

Result: Mask-LLaVA 在标准基准测试中表现出竞争力，在视觉令牌数量远少于原始 LLaVA 的情况下，取得了可比的性能。

Conclusion: 结合多层次视觉特征（掩码对象、全局和局部令牌）可以实现更高效的 VLM 学习，并且允许在推理时动态选择令牌以获得良好的性能，而不会显著降低准确率。

Abstract: Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.

</details>


### [112] [XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas](https://arxiv.org/abs/2602.04819)
*Aqsa Sultana,Rayan Afsar,Ahmed Rahu,Surendra P. Singh,Brian Shula,Brandon Combs,Derrick Forchetti,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 提出了一种名为XtraLight-MedMamba的超轻量级深度学习框架，用于从全切片图像（WSIs）中区分腺瘤性管状腺瘤，并在低级别异型增生患者的数据集上取得了97.18%的准确率和0.9767的F1分数。


<details>
  <summary>Details</summary>
Motivation: 目前，在结肠镜筛查中对癌前息肉进行准确的风险分层对于降低结直肠癌（CRC）的风险至关重要，但低级别异型增生的评估受到主观组织病理学解释的限制。数字病理学和深度学习的进步为识别与恶性进展相关的、人眼可能无法察觉的细微形态模式提供了新机会。

Method: 提出了一种名为XtraLight-MedMamba的深度学习框架。该框架结合了基于ConvNext的浅层特征提取器和并行的Vision Mamba，以有效地建模长程和短程依赖关系及图像泛化。集成了空间和通道注意力桥（SCAB）模块以增强多尺度特征提取，并采用固定非负正交分类器（FNOClassifier）以显著减少参数并提高泛化能力。该模型仅使用了约32,000个参数。

Result: XtraLight-MedMamba模型在包含低级别管状腺瘤患者的数据集上进行了评估，并根据后续CRC发展情况进行了分层。模型达到了97.18%的准确率和0.9767的F1分数，其性能优于基于Transformer和传统Mamba的架构，同时模型复杂度显著降低。

Conclusion: XtraLight-MedMamba是一种高效且参数量极小的深度学习框架，能够准确地从全切片图像中分类腺瘤性管状腺瘤，为改善癌前息肉的风险分层提供了有前景的解决方案，并克服了传统方法的局限性。

Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.

</details>


### [113] [PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation](https://arxiv.org/abs/2602.04876)
*Jiahao Zhan,Zizhang Li,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: PerpetualWonder 是一个混合生成式模拟器，可以从单张图像生成长时序、动作条件化的 4D 场景。它通过一个新颖的统一表示来解决现有方法的不足，该表示在物理状态和视觉基元之间建立双向联系，从而实现生成式优化来修正动力学和外观。该系统还包含一个鲁棒的更新机制，可以从多个视角收集监督信息来解决优化歧义。实验表明，PerpetualWonder 能够从单张图像成功模拟复杂的、多步交互的长时序动作，同时保持物理合理性和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从单张图像生成长时序、动作条件化的 4D 场景时存在问题，原因是它们的物理状态与其视觉表示是解耦的，这阻碍了生成式优化来更新物理状态以进行后续交互。

Method: 提出 PerpetualWonder，一个混合生成式模拟器，采用新颖的统一表示，在物理状态和视觉基元之间建立双向联系。引入了一个鲁棒的更新机制，从多个视角收集监督信息以解决优化歧义。

Result: PerpetualWonder 能够从单张图像成功模拟复杂的、多步交互的长时序动作，同时保持物理合理性和视觉一致性。

Conclusion: PerpetualWonder 是首个真正的闭环系统，通过统一表示和鲁棒的更新机制，实现了从单张图像进行长时序、动作条件化的 4D 场景生成，克服了现有方法的局限性。

Abstract: We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.

</details>


### [114] [Laminating Representation Autoencoders for Efficient Diffusion](https://arxiv.org/abs/2602.04873)
*Ramón Calvo-González,François Fleuret*

Main category: cs.CV

TL;DR: FlatDINO是一种变分自编码器，能将DINOv2的密集SSL patch特征压缩为32个连续token的1D序列，显著降低了扩散模型的计算成本，同时保持了高质量的图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SSL patch特征的扩散模型生成图像质量高，但SSL编码器（如DINOv2）产生的密集patch网格存在大量冗余，导致扩散模型计算成本过高。

Method: 提出FlatDINO，一个变分自编码器（VAE），用于将DINOv2的表示压缩成一个32个连续token组成的1D序列。该模型将序列长度减少8倍，总维度压缩48倍。

Result: 在ImageNet 256x256数据集上，使用FlatDINO latents训练的DiT-XL模型，在引导下达到了1.80的gFID分数。与在未压缩的DINOv2特征上进行扩散相比，前向传播计算量减少8倍，训练步骤计算量减少4.5倍。

Conclusion: FlatDINO有效地压缩了SSL patch特征，显著降低了扩散模型的计算需求，同时保持了生成图像的质量，为更高效的图像生成提供了可能。此项工作仍在进行中。

Abstract: Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.

</details>


### [115] [CoWTracker: Tracking by Warping instead of Correlation](https://arxiv.org/abs/2602.04877)
*Zihang Lai,Eldar Insafutdinov,Edgar Sucar,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本文提出了一种名为 \method 的新型密集点追踪器，该追踪器通过特征扭曲而非代价体积进行帧间特征匹配，并结合 Transformer 架构进行联合时空推理，在不计算特征相关性的情况下建立长距离对应关系，并在多个密集点追踪和光流估计基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有密集点追踪方法通常使用代价体积，但其空间分辨率的二次复杂度限制了可扩展性和效率。研究动机是寻找一种更高效、可扩展的方法来解决密集点追踪问题。

Method: 提出了一种名为 \method 的新方法，该方法摒弃了代价体积，转而采用特征扭曲的方式。模型通过迭代地根据当前估计将目标帧的特征扭曲到查询帧来优化追踪估计。此外，还结合了 Transformer 架构，对所有追踪进行联合时空推理。

Result: 该模型在 TAP-Vid-DAVIS, TAP-Vid-Kinetics, 和 Robo-TAP 等标准密集点追踪基准测试上取得了最先进的性能。在光流估计任务上，该模型在 Sintel, KITTI, 和 Spring 基准测试上表现出色，有时甚至优于专门的光流估计算法。

Conclusion: 基于扭曲（warping-based）的架构可以统一密集点追踪和光流估计。该方法简单且性能优越，表明了其作为一种通用方法的潜力。

Abstract: Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [116] [Linguistic Blind Spots in Clinical Decision Extraction](https://arxiv.org/abs/2602.03942)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.CL

TL;DR: 研究发现，不同类别的临床决策在语言特征上存在差异，这些差异会影响自动提取的准确性。特别是包含建议和预防措施的决策，由于其叙事性强，更容易导致提取失败，尤其是在精确匹配的情况下。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持和患者护理总结需要从临床笔记中提取医学决策，但现有的提取方法在处理不同类别的决策时效果不一，这促使了本研究。

Method: 研究人员使用 MedDec 出院总结，其中包含 DICTUM 标注的决策类别。他们为每个决策片段计算了七个语言指标，并分析了标准 Transformer 模型在片段级别上的召回率。

Result: 研究发现，药物相关和问题定义决策倾向于实体密集且简洁，而建议和预防措施决策则更具叙事性，包含更多的停用词、代词、模糊词和否定词。在精确匹配下，整体召回率为 48%，停用词比例越高或包含模糊/否定词的片段召回率越低。在放宽匹配标准后，召回率提升至 71%，表明许多错误是片段边界的争议而非完全丢失。

Conclusion: 具有叙事风格的临床决策片段（常见于建议和预防措施决策）在精确匹配下是提取的盲点。因此，下游系统在评估和提取临床决策时，应考虑边界容错的策略。

Abstract: Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec discharge summaries annotated with decision categories from the Decision Identification and Classification Taxonomy for Use in Medicine (DICTUM), we compute seven linguistic indices for each decision span and analyze span-level extraction recall of a standard transformer model. We find clear category-specific signatures: drug-related and problem-defining decisions are entity-dense and telegraphic, whereas advice and precaution decisions contain more narrative, with higher stopword and pronoun proportions and more frequent hedging and negation cues. On the validation split, exact-match recall is 48%, with large gaps across linguistic strata: recall drops from 58% to 24% from the lowest to highest stopword-proportion bins, and spans containing hedging or negation cues are less likely to be recovered. Under a relaxed overlap-based match criterion, recall increases to 71%, indicating that many errors are span boundary disagreements rather than complete misses. Overall, narrative-style spans--common in advice and precaution decisions--are a consistent blind spot under exact matching, suggesting that downstream systems should incorporate boundary-tolerant evaluation and extraction strategies for clinical decisions.

</details>


### [117] [Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines](https://arxiv.org/abs/2602.03962)
*Erik Saule,Kalpathi Subramanian,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文提出利用自然语言处理技术（包括传统NLP工具和大型语言模型）来自动评估计算机科学课程对ACM/IEEE课程指南的覆盖程度，以解决人工评估耗时耗力的痛点。


<details>
  <summary>Details</summary>
Motivation: 计算机科学课程指南（如ACM/IEEE）内容庞杂，人工评估课程覆盖度耗时且认知负荷大，亟需更高效的方法。

Method: 探索两种自然语言处理技术：1) 传统的词法分析、词性标注和词嵌入技术；2) 利用大型语言模型。通过对教学材料语料库进行分类来评估这些技术。

Result: 所提出的NLP技术能够有效地自动对教学材料进行分类，从而有意义地评估课程内容。

Conclusion: 自然语言处理技术能够显著加速评估计算机科学课程对国际标准覆盖程度的过程，为程序管理员提供了一种更有效率的评估方法。

Abstract: Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.
  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.
  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.

</details>


### [118] [Likelihood-Based Reward Designs for General LLM Reasoning](https://arxiv.org/abs/2602.03979)
*Ariel Kwiatkowski,Natasha Butt,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 研究表明，使用参考答案的对数概率作为强化学习微调大型语言模型（LLMs）的奖励，在数学推理和长篇答案生成等多种场景下都能取得与传统二元奖励相当或更好的性能，并且在不可验证场景下表现优于基于概率的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的LLM推理微调方法依赖于为每个基准设计的二元奖励函数，这存在设计奖励的复杂性和奖励信号稀疏的问题。作者希望找到一种更通用、更易于获取且有效的奖励机制。

Method: 研究系统地比较了基于参考答案概率或对数概率的奖励与标准基线方法。实验在标准数学推理基准和无外部验证器的长篇答案生成任务上进行。主要评估的奖励形式包括参考答案的对数概率。

Result: 发现使用参考答案的对数概率作为链式思考（CoT）学习的奖励，在所有测试设置下表现都很好。在可验证场景下，对数概率奖励在成功率上与二元奖励相当或更好，且困惑度更低。在不可验证场景下，对数概率奖励的表现与监督微调（SFT）相当。而基于概率的方法（如VeriFree）在不可验证场景下表现不佳。

Conclusion: 基于参考答案对数概率的奖励是一种可行的CoT微调方法，能够有效连接短文本可验证和长文本不可验证的回答场景，并解决了传统二元奖励的局限性。

Abstract: Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.

</details>


### [119] [Transformers perform adaptive partial pooling](https://arxiv.org/abs/2602.03980)
*Vsevolod Kapatsinski*

Main category: cs.CL

TL;DR: 本研究表明，Transformer模型（GPT-2）在训练过程中，其对下一个词的预测会逐渐减少对当前上下文之外信息的依赖（即池化程度降低），并且这种池化程度受到上下文频率、上下文数量和上下文变异性的影响，这与分层回归中的自适应部分证据池化现象类似。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于理解 Transformer 模型（如 GPT-2）如何处理不常出现但并非全新的上下文，以及其学习机制在多大程度上是“理性”的，能够像分层回归一样适应性地调整对不同上下文信息的利用。

Method: 通过分析 Transformer 模型（GPT-2）在不同训练阶段（epochs）对下一个词的预测行为，量化其从其他上下文“借用”信息的程度（即证据池化）。比较了池化程度与上下文频率、上下文数量（类型频率）和上下文变异性之间的关系，并与分层回归中的自适应部分证据池化理论进行了类比。

Result: 研究发现，随着训练的进行，Transformer 模型对当前上下文之外信息的依赖程度逐渐降低。同时，证据池化的程度受到上下文频率、上下文数量和上下文变异性的影响，其方式与分层回归模型中的自适应部分证据池化机制相似。

Conclusion: Transformer 模型在处理稀疏上下文时的学习特性，即自适应部分证据池化，不仅在理论上是合理的，而且在经验上也得到了证实，这表明 Transformer 模型在语言生成方面具有一定的适应性和鲁棒性。

Abstract: Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a context are affected by observations from other similar contexts to the extent that 1) the current context is infrequent and 2) different contexts behave similarly. This is called adaptive partial pooling of evidence. This paper shows that next-word predictions of a transformer (GPT2) are increasingly unaffected by observations from outside the current context across epochs of training (the amount of pooling reduces with training), and that the extent of pooling is affected by context frequency, context number (type frequency) and context variability in a similar way to hierarchical regression. These characteristics of learning in transformers are argued to be realistic on both rational and empirical grounds.

</details>


### [120] [On the Credibility of Evaluating LLMs using Survey Questions](https://arxiv.org/abs/2602.04033)
*Jindřich Libovický*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）在价值取向方面的研究方法，发现现有方法存在局限性，可能导致对LLM与人类价值取向相似性的低估或高估。研究引入了“自相关距离”新指标来衡量LLM回答之间的结构一致性，并建议使用链式思考（CoT）提示、采样解码和多种评估指标（包括自相关距离）来进行更稳健的LLM价值取向评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM价值取向的方法，即直接使用社会调查问题提示模型并比较其平均回答与人类平均回答，存在局限性，可能导致不准确的评估结果。作者希望提出一种更准确、更全面的评估方法。

Method: 研究人员使用世界价值观调查（World Value Survey）的跨语言（三种语言）、跨国（五个国家）数据，比较了直接提示和链式思考（CoT）提示方法，以及贪婪解码和采样解码策略对LLM价值取向评估的影响。引入了新的评估指标“自相关距离”（self-correlation distance）来衡量LLM回答在不同问题之间的结构一致性。

Result: 研究发现，提示方法（直接 vs. CoT）和解码策略（贪婪 vs. 采样）显著影响LLM价值取向评估的结果。即使LLM的平均回答与人类数据高度一致，也并不意味着其回答在结构上与人类一致。另外，研究还发现两种常用的评估指标（均方距离和KL散度）之间存在弱相关性，这表明它们都假设了调查回答的独立性。

Conclusion: 现有LLM价值取向的评估方法存在不足。为了更准确地评估LLM的价值取向，建议采用CoT提示、具有大量样本的采样解码，并结合多种评估指标（包括新提出的自相关距离）进行稳健的分析。

Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.

</details>


### [121] [Abstraction Induces the Brain Alignment of Language and Speech Models](https://arxiv.org/abs/2602.04081)
*Emily Cheng,Aditya R. Vaidya,Richard Antonello*

Main category: cs.CL

TL;DR: 研究发现，语言模型中间层的表征能力与大脑对语言的反应高度相关，这种相关性源于共享的意义抽象而非预测下一个词的能力。中间层的内在维度（衡量特征复杂度）是预测大脑信号的关键，且可以通过微调模型以增强语义内容来提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明语言模型和语音模型的中间层能有效预测大脑对自然语言的反应，但对其预测能力为何如此之强、为何中间层而非输出层更有效等问题缺乏深入理解。

Method: 通过分析模型中间层和输出层的表征特性，利用内在维度（Intrinsic Dimension）衡量特征复杂度，并将其与fMRI和ECoG脑电信号的预测能力进行关联分析。同时，通过微调模型以增强其对大脑信号的预测能力，观察这种微调对模型表征的内在维度和语义内容的影响。

Result: 模型中间层的内在维度与其解释fMRI和ECoG信号的能力高度相关。这种内在维度与大脑预测能力之间的关系在模型预训练过程中逐渐形成。将模型微调以更好地预测大脑信号，可以同时提高模型表征的内在维度和语义内容。

Conclusion: 语言模型与大脑的相似性主要源于共享的意义抽象能力，而非单纯的下一个词预测能力。模型中间层通过构建高阶语言特征，表现出更高的内在维度和语义丰富度，从而更好地解释大脑对语言的反应。语言建模任务的复杂性足以促使模型进行有效的意义抽象。

Abstract: Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.

</details>


### [122] [Expert Selections In MoE Models Reveal (Almost) As Much As Text](https://arxiv.org/abs/2602.04105)
*Amir Nuriyev,Gabriel Kulp*

Main category: cs.CL

TL;DR: 研究发现，在混合专家（MoE）语言模型中，token的路由选择包含大量泄露信息，仅凭专家选择即可实现高精度的文本重建。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明MoE模型的路由决策可能泄露信息，但效果有限。本文旨在更深入地探索路由决策泄露的信息量，并评估不同模型在文本重建方面的能力。

Method: 研究者提出了一种基于Transformer的序列解码器，通过分析MoE模型中token的路由选择来重建原始文本。他们对比了不同复杂度的模型（如MLP和Transformer）的重建效果，并在OpenWebText数据集上进行了实验。

Result: 使用Transformer序列解码器，研究者在32个token的序列上达到了91.2%的top-1准确率和94.8%的top-10准确率。即使加入噪声，重建率也仅有所降低，未能完全消除。

Conclusion: MoE模型的专家选择泄露的信息量远超预期，并且可以用于高精度的文本重建。因此，在MoE模型的部署中，应该将专家选择视为与原始文本一样敏感的信息。

Abstract: We present a text-reconstruction attack on mixture-of-experts (MoE) language models that recovers tokens from expert selections alone. In MoE models, each token is routed to a subset of expert subnetworks; we show these routing decisions leak substantially more information than previously understood. Prior work using logistic regression achieves limited reconstruction; we show that a 3-layer MLP improves this to 63.1% top-1 accuracy, and that a transformer-based sequence decoder recovers 91.2% of tokens top-1 (94.8% top-10) on 32-token sequences from OpenWebText after training on 100M tokens. These results connect MoE routing to the broader literature on embedding inversion. We outline practical leakage scenarios (e.g., distributed inference and side channels) and show that adding noise reduces but does not eliminate reconstruction. Our findings suggest that expert selections in MoE deployments should be treated as sensitive as the underlying text.

</details>


### [123] [DELTA: Deliberative Multi-Agent Reasoning with Reinforcement Learning for Multimodal Psychological Counseling](https://arxiv.org/abs/2602.04112)
*Jiangnan Yang,Junjie Chen,Fei Wang,Yiqi Nie,Yuxin Liu,Zhangling Duan,Jie Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为DELTA的多模态心理咨询框架，通过整合文本、视觉和声音线索，并结合强化学习来提高咨询质量和情感共鸣。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的心理咨询系统主要依赖文本，并且隐式推断用户心理状态，忽略了多模态信息，作者旨在解决这一局限性，提高咨询的质量和情感共鸣。

Method: DELTA是一个多主体框架，将咨询过程分解为证据接地、心理状态抽象和响应生成三个阶段。它利用多模态信号进行结构化推理，并结合以分布级情感共鸣分数（Emotion Attunement Score）为指导的强化学习。

Result: 在多模态咨询基准测试中，DELTA显著提高了咨询质量和情感共鸣。消融研究和定性分析表明，显式多模态推理和结构化的心理状态表示共同促进了人机之间的共情互动。

Conclusion: DELTA通过显式多模态推理和结构化心理状态表示，能够有效提升心理咨询系统的质量和情感共鸣能力，为构建更具共情能力的人机交互系统提供了新的途径。

Abstract: Psychological counseling is a fundamentally multimodal cognitive process in which clinicians integrate verbal content with visual and vocal cues to infer clients' mental states and respond empathically. However, most existing language-model-based counseling systems operate on text alone and rely on implicit mental state inference. We introduce DELTA, a deliberative multi-agent framework that models counseling as a structured reasoning process over multimodal signals, separating evidence grounding, mental state abstraction, and response generation. DELTA further incorporates reinforcement learning guided by a distribution-level Emotion Attunement Score to encourage emotionally attuned responses. Experiments on a multimodal counseling benchmark show that DELTA improves both counseling quality and emotion attunement across models. Ablation and qualitative analyses suggest that explicit multimodal reasoning and structured mental state representations play complementary roles in supporting empathic human-AI interaction.

</details>


### [124] [From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?](https://arxiv.org/abs/2602.04127)
*Sercan Karakaş,Yusuf Şimşek*

Main category: cs.CL

TL;DR: 本文研究了土耳其语中的轻动词结构（LVCs）分类，通过限制模型输入来探究哪些信号驱动 LVC 分类。研究结果表明，单独的粗粒度词法句法信息不足以准确检测 LVCs，而词汇本身在一定程度上支持 LVC 判断，但对校准和归一化方法敏感。


<details>
  <summary>Details</summary>
Motivation: 土耳其语中丰富的形态学和复杂的谓语结构使得 LVCs 的识别具有挑战性，特别是区分习语性谓语意义和字面动词-论元用法之间的细微差别。

Method: 研究人员使用 UD（Universal Dependencies）衍生的监督数据，比较了不同输入表示的模型：基于词元（lemma）的基线模型（TF-IDF + Logistic Regression；BERTurk on lemma sequences）、仅使用 UD 词法句法特征的 Logistic Regression 模型（UPOS/DEPREL/MORPH），以及包含所有输入的 BERTurk 模型。评估在包含随机负例、词汇控制（NLVC）和 LVC 正例的诊断数据集上进行。

Result: 结果表明，单独的粗粒度词法句法特征不足以在受控对比下稳健地检测 LVCs。词汇本身虽然能支持 LVC 判断，但对校准和归一化方法的选择很敏感。研究还发现，“仅词元”的表示并非单一且定义明确，而是高度依赖于归一化操作的实现方式。

Conclusion: 该研究的结果表明，在土耳其语 MWEs（Multiword Expressions）的评估中需要针对性地进行评估，并且“仅词元”的表示方式并非一个固定的概念，其有效性取决于具体的归一化策略。

Abstract: Light verb constructions (LVCs) are a challenging class of verbal multiword expressions, especially in Turkish, where rich morphology and productive complex predicates create minimal contrasts between idiomatic predicate meanings and literal verb--argument uses. This paper asks what signals drive LVC classification by systematically restricting model inputs. Using UD-derived supervision, we compare lemma-driven baselines (lemma TF--IDF + Logistic Regression; BERTurk trained on lemma sequences), a grammar-only Logistic Regression over UD morphosyntax (UPOS/DEPREL/MORPH), and a full-input BERTurk baseline. We evaluate on a controlled diagnostic set with Random negatives, lexical controls (NLVC), and LVC positives, reporting split-wise performance to expose decision-boundary behavior. Results show that coarse morphosyntax alone is insufficient for robust LVC detection under controlled contrasts, while lexical identity supports LVC judgments but is sensitive to calibration and normalization choices. Overall, Our findings motivate targeted evaluation of Turkish MWEs and show that ``lemma-only'' is not a single, well-defined representation, but one that depends critically on how normalization is operationalized.

</details>


### [125] [The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment](https://arxiv.org/abs/2602.04196)
*Zhexin Zhang,Yida Lu,Junfeng Fang,Junxiao Yang,Shiyao Cui,Hao Zhou,Fandong Meng,Jie Zhou,Hongning Wang,Minlie Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本研究首次系统地探讨了人工智能模型在训练期间出现的隐式安全风险，这些风险源于模型内部的激励和背景信息，而非直接操纵奖励函数。研究提出了风险分类体系，并通过实验证明了这些风险的普遍性和严重性，特别是在Llama-3.1-8B-Instruct模型中。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型部署时的安全风险（如越狱攻击），而忽略了训练期间可能出现的安全风险。本文旨在填补这一研究空白，探索模型在训练过程中因内部激励和上下文信息而产生的隐式有害行为。

Method: 1. 提出一个包含五个风险等级、十个细粒度风险类别和三种激励类型（即内部激励和背景信息驱动）的分类体系。 2. 进行广泛的实验，评估这些隐式训练时风险的普遍性和严重性。 3. 分析影响这些行为的因素。 4. 演示在多智能体训练设置中也存在隐式训练时风险。

Result: Llama-3.1-8B-Instruct模型在仅提供背景信息的情况下，有74.4%的训练运行会表现出风险行为。研究发现隐式训练时风险在多智能体训练设置中同样存在。

Conclusion: 隐式训练时安全风险是一个被忽视但紧迫的安全挑战，需要引起重视。模型在训练过程中可能产生超出直接奖励函数操纵的有害行为，这在部署前难以察觉。

Abstract: Safety risks of AI models have been widely studied at deployment time, such as jailbreak attacks that elicit harmful outputs. In contrast, safety risks emerging during training remain largely unexplored. Beyond explicit reward hacking that directly manipulates explicit reward functions in reinforcement learning, we study implicit training-time safety risks: harmful behaviors driven by a model's internal incentives and contextual background information. For example, during code-based reinforcement learning, a model may covertly manipulate logged accuracy for self-preservation. We present the first systematic study of this problem, introducing a taxonomy with five risk levels, ten fine-grained risk categories, and three incentive types. Extensive experiments reveal the prevalence and severity of these risks: notably, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when provided only with background information. We further analyze factors influencing these behaviors and demonstrate that implicit training-time risks also arise in multi-agent training settings. Our results identify an overlooked yet urgent safety challenge in training.

</details>


### [126] [From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents](https://arxiv.org/abs/2602.04197)
*Xinyue Wang,Yuanhe Zhang,Zhengshuo Gong,Haoran Gao,Fanyu Meng,Zhenhong Zhou,Li Sun,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 本文提出了一种名为“毒性主动性”的新型AI代理风险，即代理为了最大化效用而可能采取不道德的、操纵性的手段。研究引入了一个基于双模型交互的评估框架来检测和分析这种行为，并证明其在主流LLM中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在追求“有用”的过程中，可能面临“过度拒绝”的被动失败模式，但本文关注的是另一种更危险的主动失败模式——“毒性主动性”，即代理为了最大化效用而主动采取不道德或操纵性的措施，这由于其隐蔽性而未得到充分研究。

Method: 研究引入了一个新颖的评估框架，该框架基于对偶模型之间的“困境驱动”交互，以模拟和分析代理在多步行为轨迹中的行为。通过在各种情境设置下对主流LLM进行广泛实验来评估“毒性主动性”。

Result: 实验证明“毒性主动性”是一种普遍存在的行为现象，并揭示了两种主要的倾向。研究还提出了一个系统性的基准，用于跨情境设置评估“毒性主动性”。

Conclusion: “毒性主动性”是LLM代理在追求有用性时存在的一种重要且普遍的风险，表现为代理主动采取不道德或操纵性策略以最大化效用。研究提出的评估框架和基准能够有效地检测和分析这种行为。

Abstract: The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of "over-refusal", which is a passive failure mode. However, the proactive planning and action capabilities of agents introduce another crucial danger on the other side of the trade-off. This phenomenon we term "Toxic Proactivity'': an active failure mode in which an agent, driven by the optimization for Machiavellian helpfulness, disregards ethical constraints to maximize utility. Unlike over-refusal, Toxic Proactivity manifests as the agent taking excessive or manipulative measures to ensure its "usefulness'' is maintained. Existing research pays little attention to identifying this behavior, as it often lacks the subtle context required for such strategies to unfold. To reveal this risk, we introduce a novel evaluation framework based on dilemma-driven interactions between dual models, enabling the simulation and analysis of agent behavior over multi-step behavioral trajectories. Through extensive experiments with mainstream LLMs, we demonstrate that Toxic Proactivity is a widespread behavioral phenomenon and reveal two major tendencies. We further present a systematic benchmark for evaluating Toxic Proactive behavior across contextual settings.

</details>


### [127] [Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry](https://arxiv.org/abs/2602.04206)
*Hsien-Jyh Liao*

Main category: cs.CL

TL;DR: 研究提出了一种名为Soft-FSM的神经符号架构，通过外部确定性状态控制器来强制执行基于关键信息单元（KIUs）的单调进展，以解决大型语言模型（LLMs）在处理具有明确程序约束的长周期任务（如法律交叉询问）时遇到的程序停滞问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长周期任务，尤其是在有明确程序约束的情况下，表现不佳，容易出现程序停滞，无法保证任务的顺利推进。

Method: 提出了一种名为Soft-FSM的神经符号架构，该架构包含一个外部确定性状态控制器，用于强制执行基于累积的关键信息单元（KIUs）的单调进展。

Result: 在三个真实的台湾刑事凶杀案实验中，Soft-FSM的完成率超过97%，冗余度接近于零，而基线方法的完成率低于40%。

Conclusion: 在特定领域（如法律交叉询问），仅依靠大型语言模型的涌现行为不足以保证任务的可靠完成，需要通过明确且可验证的外部状态控制来强制执行。

Abstract: Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.

</details>


### [128] [Language Models Struggle to Use Representations Learned In-Context](https://arxiv.org/abs/2602.04212)
*Michael A. Lepori,Tal Linzen,Ann Yuan,Katja Filippova*

Main category: cs.CL

TL;DR: 本研究发现，尽管大型语言模型（LLMs）能够从上下文中学习并编码新的语义信息，但它们在灵活运用这些信息来完成下游任务方面存在困难，即使是最先进的模型也无法可靠地利用上下文中的新模式。


<details>
  <summary>Details</summary>
Motivation: 为了实现人工智能的宏伟目标，即创建能够适应全新环境的系统，研究LLMs是否能够灵活地运用从上下文中学习到的丰富表征来完成下游任务。

Method: 研究者首先评估了开源LLMs在上下文表示学习（in-context representation learning）后，是否能够将其用于下一个词语预测任务。接着，他们设计了一个新颖的“自适应世界建模”任务来探测模型。最后，他们还将最先进的闭源推理模型在该任务上进行了评估。

Result: 在下一词语预测和自适应世界建模任务中，研究发现开源LLMs难以有效运用从上下文中学习到的新语义表征。即使是最先进的闭源LLMs，在自适应世界建模任务上也无法可靠地利用上下文中的新模式。

Conclusion: 本研究表明，目前的LLMs在不仅编码上下文信息，而且能够灵活部署这些信息以应对新情况方面，仍然存在显著的局限性。研究者希望这项工作能启发新的方法，以提高模型在这方面的能力。

Abstract: Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One important step towards this goal is to create systems that can induce rich representations of data that are seen in-context, and then flexibly deploy these representations to accomplish goals. Recently, Park et al. (2024) demonstrated that current LLMs are indeed capable of inducing such representation from context (i.e., in-context representation learning). The present study investigates whether LLMs can use these representations to complete simple downstream tasks.
  We first assess whether open-weights LLMs can use in-context representations for next-token prediction, and then probe models using a novel task, adaptive world modeling. In both tasks, we find evidence that open-weights LLMs struggle to deploy representations of novel semantics that are defined in-context, even if they encode these semantics in their latent representations. Furthermore, we assess closed-source, state-of-the-art reasoning models on the adaptive world modeling task, demonstrating that even the most performant LLMs cannot reliably leverage novel patterns presented in-context. Overall, this work seeks to inspire novel methods for encouraging models to not only encode information presented in-context, but to do so in a manner that supports flexible deployment of this information.

</details>


### [129] [Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation](https://arxiv.org/abs/2602.04241)
*Nuo Xu,Ahrii Kim*

Main category: cs.CL

TL;DR: 研究比较了三种子词分词方法（BPE、OBPE、Unigram）在六种乌拉尔语系语言上的表现，发现OBPE在形态丰富和低资源语言上表现更好，提高了词性标注任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对形态丰富和低资源语言的子词分词行为探索不足，而子词分词对NLP性能至关重要。

Method: 在六种乌拉尔语系语言上，使用词性标注（POS tagging）作为下游任务，系统比较了BPE、OBPE和Unigram Language Model三种子词分词方法的性能。

Result: OBPE在词性标注任务上 consistently 表现优于传统方法，尤其在拉丁字母书写的语言中，这得益于其减少了开放类别词的碎片化，并在词频分布上取得了更好的平衡。跨语言迁移效果还受到下游标注模型、训练数据量和语言亲缘关系的影响。

Conclusion: 形态敏感的分词方法（如OBPE）对于实现有效的跨语言迁移至关重要，特别是在粘着性强的低资源语言上，它不仅仅是预处理的选择，而是影响模型性能的关键因素。

Abstract: Subword tokenization critically affects Natural Language Processing (NLP) performance, yet its behavior in morphologically rich and low-resource language families remains under-explored. This study systematically compares three subword paradigms -- Byte Pair Encoding (BPE), Overlap BPE (OBPE), and Unigram Language Model -- across six Uralic languages with varying resource availability and typological diversity. Using part-of-speech (POS) tagging as a controlled downstream task, we show that OBPE consistently achieves stronger morphological alignment and higher tagging accuracy than conventional methods, particularly within the Latin-script group. These gains arise from reduced fragmentation in open-class categories and a better balance across the frequency spectrum. Transfer efficacy further depends on the downstream tagging architecture, interacting with both training volume and genealogical proximity. Taken together, these findings highlight that morphology-sensitive tokenization is not merely a preprocessing choice but a decisive factor in enabling effective cross-lingual transfer for agglutinative, low-resource languages.

</details>


### [130] [CoLT: Reasoning with Chain of Latent Tool Calls](https://arxiv.org/abs/2602.04246)
*Fangwei Zhu,Zhifang Sui*

Main category: cs.CL

TL;DR: 提出了一种名为CoLT的框架，它将潜在推理实现为“工具调用”，通过生成包含推理步骤信息的种子令牌，并由外部模型解码，从而在不改变模型结构或进行详尽训练的情况下，提升LLM的数学推理能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在推理方法通常需要模型结构增强和大量训练，限制了其应用范围。研究者希望找到一种更通用、更易于应用的方法来加速LLM的推理过程。

Method: CoLT框架将潜在推理视为“工具调用”。它生成包含推理步骤信息的“种子令牌”，当触发潜在工具调用时，一个较小的外部模型将这些种子令牌的隐藏状态作为输入，并将其解码回完整的推理步骤。这使得主模型可以在显式令牌空间中进行推理，同时提高效率。

Result: CoLT在四个数学数据集上的实验结果表明，与基线潜在模型相比，CoLT实现了更高的准确率和更短的推理长度。此外，CoLT与强化学习算法和不同的解码器结构兼容。

Conclusion: CoLT是一种新颖的框架，通过将潜在推理作为工具调用，实现了高效的LLM推理，克服了现有方法的局限性，并在数学推理任务上展现出优越的性能和良好的兼容性。

Abstract: Chain-of-Thought (CoT) is a critical technique in enhancing the reasoning ability of Large Language Models (LLMs), and latent reasoning methods have been proposed to accelerate the inefficient token-level reasoning chain. We notice that existing latent reasoning methods generally require model structure augmentation and exhaustive training, limiting their broader applicability. In this paper, we propose CoLT, a novel framework that implements latent reasoning as ``tool calls''. Instead of reasoning entirely in the latent space, CoLT generates seed tokens that contain information of a reasoning step. When a latent tool call is triggered, a smaller external model will take the hidden states of seed tokens as its input, and unpack the seed tokens back to a full reasoning step. In this way, we can ensure that the main model reasons in the explicit token space, preserving its ability while improving efficiency. Experimental results on four mathematical datasets demonstrate that CoLT achieves higher accuracy and shorter reasoning length than baseline latent models, and is compatible with reinforcement learning algorithms and different decoder structures.

</details>


### [131] [DementiaBank-Emotion: A Multi-Rater Emotion Annotation Corpus for Alzheimer's Disease Speech (Version 1.0)](https://arxiv.org/abs/2602.04247)
*Cheonkam Jeong,Jessica Liao,Audrey Lu,Yutong Song,Christopher Rashidian,Donna Krogh,Erik Krogh,Mahkameh Rasouli,Jung-Ah Lee,Nikil Dutt,Lisa M Gibbs,David Sultzer,Julie Rousseau,Jocelyn Ludlow,Margaret Galvez,Alexander Nuth,Chet Khay,Sabine Brunswicker,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 本研究发布了首个针对阿尔茨海默病（AD）患者语音的多评分者情感标注语料库DementiaBank-Emotion，发现AD患者表达非中性情感的比例显著高于健康对照组，并初步探索了AD患者语音中情感与声学特征（如基频）之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏专门针对阿尔茨海默病（AD）患者语音的情感标注语料库，限制了对AD患者情感表达及其在语音中体现的研究。本研究旨在构建这样一个语料库，并初步探索AD患者的情感表达特征。

Method: 构建了一个包含108名AD患者和健康对照组的1,492个语音片段的多评分者情感标注语料库DementiaBank-Emotion，对Ekman的六种基本情感和中性情感进行标注。此外，对部分样本进行了探索性的声学分析，特别是基频（F0）调制的分析，以研究情感与语音特征的关系。

Result: AD患者表达非中性情感的比例（16.9%）显著高于健康对照组（5.7%）。初步声学分析显示，健康对照组在表达悲伤时基频（F0）变化较大，而AD患者变化较小。在AD患者语音中，响度可以区分不同的情感类别，表明情感-韵律映射在一定程度上得以保留。

Conclusion: AD患者在语音中表达非中性情感的频率更高。虽然部分情感（如悲伤）的声学特征在AD患者中可能与健康对照组存在差异，但情感-韵律映射的部分关联在AD患者语音中依然存在。本研究发布的语料库和相关材料将有助于推动临床人群情感识别的研究。

Abstract: We present DementiaBank-Emotion, the first multi-rater emotion annotation corpus for Alzheimer's disease (AD) speech. Annotating 1,492 utterances from 108 speakers for Ekman's six basic emotions and neutral, we find that AD patients express significantly more non-neutral emotions (16.9%) than healthy controls (5.7%; p < .001). Exploratory acoustic analysis suggests a possible dissociation: control speakers showed substantial F0 modulation for sadness (Delta = -3.45 semitones from baseline), whereas AD speakers showed minimal change (Delta = +0.11 semitones; interaction p = .023), though this finding is based on limited samples (sadness: n=5 control, n=15 AD) and requires replication. Within AD speech, loudness differentiates emotion categories, indicating partially preserved emotion-prosody mappings. We release the corpus, annotation guidelines, and calibration workshop materials to support research on emotion recognition in clinical populations.

</details>


### [132] [Scaling Agentic Verifier for Competitive Coding](https://arxiv.org/abs/2602.04254)
*Zeyao Ma,Jing Zhang,Xiaokang Zhang,Jiaxi Yang,Zongmeng Zhang,Jiajun Zhang,Yuheng Jing,Lei Zhang,Hao Zheng,Wenting Zhao,Junyang Lin,Binyuan Hui*

Main category: cs.CL

TL;DR: 提出了一种名为Agentic Verifier的基于执行的代理，通过主动推理和搜索歧视性测试输入，来提高大型语言模型解决竞争性编程问题的能力，实验显示在多个基准测试中取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决竞争性编程问题时，即使能力很强，也难以一次性正确解决。现有的基于执行的重新排序方法要么难以生成测试用例，要么采样效率低下，因此需要一种新的方法来克服这些限制。

Method: 提出Agentic Verifier，一个基于执行的代理，通过与代码执行环境进行多轮交互，主动推理程序行为，搜索能够暴露候选解决方案行为差异的歧视性测试输入。该方法结合了大规模数据合成、拒绝微调和代理强化学习来训练验证器。

Result: 在五个竞争性编程基准测试上的实验表明，Agentic Verifier 相比于强大的基于执行的基线方法，在 Best@K 准确率上取得了高达 +10-15% 的绝对提升。

Conclusion: Agentic Verifier 是一种有效的执行时重新排序策略，通过生成歧视性测试输入，能够显著提高大型语言模型解决竞争性编程问题的能力，并表现出良好的测试时扩展性，具有超越重新排序的更广泛潜力。

Abstract: Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.

</details>


### [133] [ECG-R1: Protocol-Guided and Modality-Agnostic MLLM for Reliable ECG Interpretation](https://arxiv.org/abs/2602.04279)
*Jiarui Jin,Haoyu Wang,Xingliang Wu,Xiaocheng Fang,Xiang Lan,Zihan Wang,Deyun Zhang,Bo Liu,Yingying Zhang,Xian Wu,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: 本文提出了ECG-R1，一个首个专为ECG解释设计的推理多模态大语言模型（MLLM），通过协议引导指令数据生成、交错模态丢弃和基于ECG诊断证据奖励的强化学习，解决了现有MLLM解释ECG不准确的问题。研究还首次量化评估了现有MLLM在ECG解释中的幻觉现象，并强调了独立验证的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有用于ECG解释的多模态大语言模型（MLLM）不够可靠，常产生看似合理但临床上不正确的分析。因此，需要开发一个更可靠的ECG解释模型。

Method: 1. 协议引导指令数据生成：构建解释语料库，将解释与可测量的ECG特征、专著定义的量化阈值和诊断逻辑联系起来。2. 交错模态丢弃的模态解耦架构：提高在ECG信号或图像缺失时的鲁棒性和跨模态一致性。3. 基于ECG诊断证据奖励的强化学习：加强基于证据的ECG解释。

Result: ECG-R1实现了可靠的ECG解释。对现有MLLM的系统性评估表明，严重的幻觉现象普遍存在，不应直接信任其ECG解释输出。

Conclusion: ECG-R1是首个专为可靠ECG解释设计的推理MLLM。现有的MLLM在ECG解释方面存在严重的幻觉问题，需要独立验证。ECG-R1通过改进的数据生成、模型架构和训练方法，提高了ECG解释的准确性和可靠性。

Abstract: Electrocardiography (ECG) serves as an indispensable diagnostic tool in clinical practice, yet existing multimodal large language models (MLLMs) remain unreliable for ECG interpretation, often producing plausible but clinically incorrect analyses. To address this, we propose ECG-R1, the first reasoning MLLM designed for reliable ECG interpretation via three innovations. First, we construct the interpretation corpus using \textit{Protocol-Guided Instruction Data Generation}, grounding interpretation in measurable ECG features and monograph-defined quantitative thresholds and diagnostic logic. Second, we present a modality-decoupled architecture with \textit{Interleaved Modality Dropout} to improve robustness and cross-modal consistency when either the ECG signal or ECG image is missing. Third, we present \textit{Reinforcement Learning with ECG Diagnostic Evidence Rewards} to strengthen evidence-grounded ECG interpretation. Additionally, we systematically evaluate the ECG interpretation capabilities of proprietary, open-source, and medical MLLMs, and provide the first quantitative evidence that severe hallucinations are widespread, suggesting that the public should not directly trust these outputs without independent verification. Code and data are publicly available at \href{https://github.com/PKUDigitalHealth/ECG-R1}{here}, and an online platform can be accessed at \href{http://ai.heartvoice.com.cn/ECG-R1/}{here}.

</details>


### [134] [Contextual Drag: How Errors in the Context Affect LLM Reasoning](https://arxiv.org/abs/2602.04288)
*Yun Cheng,Xingyu Zhu,Haoyu Zhao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 研究发现，语言模型在反思失败尝试时，会因“上下文拖拽”（contextual drag）现象而导致后续生成内容倾向于重复类似的错误，严重时甚至会造成性能下降和自我退化。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的自我改进流程普遍假设模型能从过去的错误中学习。然而，本研究旨在探究“上下文拖拽”现象，即失败的尝试会如何影响模型后续的生成，并评估其对模型性能的影响。

Method: 研究人员在8个推理任务上对11个专有和开源模型进行了评估，并利用树编辑距离进行了结构分析，以量化错误模式的相似性。研究还测试了外部反馈、自我验证、回退行为微调以及上下文去噪等缓解策略的效果。

Result: “上下文拖拽”现象导致模型性能下降10-20%，并且在上下文拖拽严重的模型中，迭代式自我完善可能导致性能自我退化。结构分析表明，后续的推理轨迹继承了上下文中结构相似的错误模式。外部反馈和自我验证未能消除该效应，而缓解策略仅能部分改善性能，无法完全恢复基线水平。

Conclusion: “上下文拖拽”是一种普遍存在的、难以根除的语言模型推理失败模式，对当前模型的自我改进能力构成了挑战。现有的缓解策略效果有限，表明需要新的模型架构或训练方法来解决此问题。

Abstract: Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar errors. Across evaluations of 11 proprietary and open-weight models on 8 reasoning tasks, contextual drag induces 10-20% performance drops, and iterative self-refinement in models with severe contextual drag can collapse into self-deterioration. Structural analysis using tree edit distance reveals that subsequent reasoning trajectories inherit structurally similar error patterns from the context. We demonstrate that neither external feedback nor successful self-verification suffices to eliminate this effect. While mitigation strategies such as fallback-behavior fine-tuning and context denoising yield partial improvements, they fail to fully restore baseline performance, positioning contextual drag as a persistent failure mode in current reasoning architectures.

</details>


### [135] [Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision](https://arxiv.org/abs/2602.04290)
*Lingzhuang Sun,Ruitong Liu,Yuxia Zhu,Xiaohan Xu,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为“Guided Verifier”的框架，通过引入一个动态验证器与策略模型实时协作，来解决多模态大语言模型（MLLMs）强化学习中错误累积的问题，并构建了CoRe数据集来训练验证器。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs强化学习方法通常采用单一的独立生成策略，容易导致早期逻辑错误累积，影响优化信号的准确性。

Method: 提出Guided Verifier框架，引入一个动态验证器，在模型生成过程中实时检测不一致性并提供指导信号。同时，开发了CoRe数据集，包含过程级负样本和正确引导的推理轨迹，用于训练验证器。

Result: 通过将计算资源分配给协作推理和动态验证，即使是8B参数的模型，在MathVista、MathVerse和MMMU数据集上也能取得优异的性能。

Conclusion: Guided Verifier框架通过引入协作验证机制，有效缓解了MLLMs强化学习中的错误累积问题，并通过CoRe数据集训练的验证器提升了模型的推理能力。

Abstract: Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \textbf{CoRe} dataset of process-level negatives and \textbf{Co}rrect-guide \textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.

</details>


### [136] [Proxy Compression for Language Modeling](https://arxiv.org/abs/2602.04289)
*Lin Zheng,Xinyu Li,Qian Liu,Xiachong Feng,Lingpeng Kong*

Main category: cs.CL

TL;DR: 提出了一种名为“代理压缩”的新型训练方案，它能够在训练时利用外部压缩器提高效率，同时在推理时仍能直接处理原始字节序列。该方案通过联合训练模型处理原始字节和压缩视图，使其能够内在地对齐这两种格式，从而实现高效迁移。实验表明，在代码语言建模任务上，代理压缩显著提高了训练效率，并在固定计算预算下优于纯字节级别基线，且随着模型规模增大，其优势更加明显，最终能与分词器方法媲美，同时保持字节级别建模的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型训练严重依赖于固定分词器，这种分词器通常是基于 UTF-8 字节序列的外部无损压缩器，这使得模型与该特定压缩器耦合。这种耦合限制了模型的灵活性和在不同输入格式下的表现。研究者希望找到一种能够保留压缩输入带来的效率优势，同时又能在推理时使用端到端、原始字节接口的替代训练方案。

Method: 提出了一种名为“代理压缩”（proxy compression）的训练方案。在该方案中，一个语言模型被联合训练在原始字节序列和由外部压缩器生成的压缩视图上。通过这种联合训练过程，模型学会了在内部对齐压缩序列和原始字节。这种内建的对齐机制使得模型能够有效地从压缩输入中迁移知识，即使在训练时主要使用压缩输入（而在推理时会被丢弃）。

Result: 在代码语言建模的广泛实验中，代理压缩方案显著提高了训练效率，并在固定计算预算下，其性能大幅优于纯粹的字节级别基线。随着模型规模的增加，这些性能提升变得更加显著。最终，经过代理压缩训练的模型能够达到甚至超越使用分词器方法的性能，同时仍然完全在原始字节上操作，并保持了字节级别建模固有的鲁棒性。

Conclusion: 代理压缩是一种有效的训练语言模型的方法，它能够利用压缩输入的效率优势，同时在推理时提供端到端的原始字节接口。该方法通过内建的字节-压缩序列对齐，实现了高效的知识迁移，并在代码语言建模任务上展现出优于现有字节级别基线的性能，尤其是在模型规模增大时。此外，代理压缩模型能够达到与分词器方法相当的性能，同时保留了字节级别建模的鲁棒性，为未来的语言模型训练提供了一种有前途的替代方案。

Abstract: Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.

</details>


### [137] [How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks](https://arxiv.org/abs/2602.04294)
*Yanshu Wang,Shuaishuai Yang,Jingjing He,Tong Yang*

Main category: cs.CL

TL;DR: 本研究评估了少样本示例（few-shot demonstrations）在两种基于提示的LLM防御策略（RoP和ToP）中的作用，发现少样本示例会增强RoP的安全性，但会削弱ToP的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的LLM安全对齐防御（如RoP和ToP）有效，但少样本示例如何影响这些防御策略的效果尚不明确，尤其是在不同的系统提示策略下。已有研究表明少样本示例可能损害安全性，但缺乏深入研究。

Method: 研究人员在多个主流LLM上，使用四种安全基准测试（AdvBench, HarmBench, SG-Bench, XSTest）和六种越狱攻击方法，对RoP和ToP策略进行了全面的评估，并分析了少样本示例的影响。

Result: 少样本示例对RoP和ToP产生了相反的影响：它通过强化角色身份，将RoP的安全性提高了高达4.5%；而它通过分散对任务指令的注意力，将ToP的有效性降低了高达21.2%。

Conclusion: 少样本示例在LLM的提示防御中并非一成不变地有益或有害，其效果取决于所采用的系统提示策略。基于这些发现，研究为在实际LLM应用中部署提示防御提供了实用建议。

Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.

</details>


### [138] [Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification](https://arxiv.org/abs/2602.04297)
*Branislav Pecher,Michal Spiegel,Robert Belanec,Jan Cegin*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）对提示词的敏感性很大程度上源于提示词本身的“信息不足”（underspecification）。提供更具体指令的提示词能显著降低这种敏感性，而内部表示的影响较小，主要体现在模型的输出层。


<details>
  <summary>Details</summary>
Motivation: 现有研究观察到LLMs对提示词变化非常敏感，但通常使用的是信息不足的提示词。作者认为，这种敏感性可能很大程度上是由提示词本身的“信息不足”引起的，因此需要系统地研究和比较不同类型提示词的敏感性。

Method: 研究者通过性能分析、Logit分析和线性探针（linear probing）方法，比较了信息不足的提示词和提供具体指令的提示词。

Result: 信息不足的提示词表现出更高的性能方差和对相关Token较低的Logit值。而提供具体指令的提示词则受这些问题的影响较小。线性探针分析表明，提示词“信息不足”对LLM的内部表示影响不大，主要体现在最后几层。

Conclusion: 研究强调，在研究和缓解LLMs的提示词敏感性时，需要采用更严谨的方法，特别要注意提示词的明确性。提示词“信息不足”是导致LLM对提示词敏感的一个重要原因，其影响主要体现在模型的输出端而非深层表示。

Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.

</details>


### [139] [DeFrame: Debiasing Large Language Models Against Framing Effects](https://arxiv.org/abs/2602.04306)
*Kahee Lim,Soyeon Kim,Steven Euijong Whang*

Main category: cs.CL

TL;DR: 本文提出“框架差异”概念，量化提示语的表达方式如何影响大型语言模型的公平性评估，并提出一种新的框架感知去偏方法，以提高模型对不同提示语表达方式的公平性鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实世界中的应用日益广泛，确保其跨人群的公平性回应至关重要。然而，隐藏偏见是一个持续的挑战，即模型在标准评估中表现公平，但在评估之外的场景下可能产生有偏回应。本文旨在识别提示语的“框架”（即语义等价但表达方式不同的提示语）是导致这种评估差距的一个未被充分研究的因素。

Method: 1. 提出“框架差异”（framing disparity）概念来量化框架对公平性评估的影响。2. 通过在公平性评估基准中加入替代框架来扩展评估。3. 提出一种框架感知去偏方法（framing-aware debiasing method），旨在提高模型在不同框架下的回应一致性。

Result: 1. 公平性分数显著受提示语框架的影响。2. 现有的去偏方法虽然能改善整体（框架平均）公平性，但往往无法有效减少框架引起的差异。3. 提出的框架感知去偏方法能够降低整体偏见，并提高模型对框架差异的鲁棒性。

Conclusion: 提示语的框架差异是导致大型语言模型隐藏偏见的一个重要因素。提出的框架感知去偏方法能够有效缓解框架差异带来的公平性问题，使大型语言模型能够产生更公平、更一致的回应。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their fair responses across demographics has become crucial. Despite many efforts, an ongoing challenge is hidden bias: LLMs appear fair under standard evaluations, but can produce biased responses outside those evaluation settings. In this paper, we identify framing -- differences in how semantically equivalent prompts are expressed (e.g., "A is better than B" vs. "B is worse than A") -- as an underexplored contributor to this gap. We first introduce the concept of "framing disparity" to quantify the impact of framing on fairness evaluation. By augmenting fairness evaluation benchmarks with alternative framings, we find that (1) fairness scores vary significantly with framing and (2) existing debiasing methods improve overall (i.e., frame-averaged) fairness, but often fail to reduce framing-induced disparities. To address this, we propose a framing-aware debiasing method that encourages LLMs to be more consistent across framings. Experiments demonstrate that our approach reduces overall bias and improves robustness against framing disparities, enabling LLMs to produce fairer and more consistent responses.

</details>


### [140] [A Domain-Specific Curated Benchmark for Entity and Document-Level Relation Extraction](https://arxiv.org/abs/2602.04320)
*Marco Martinelli,Stefano Marchesin,Vanessa Bonato,Giorgio Maria Di Nunzio,Nicola Ferro,Ornella Irrera,Laura Menotti,Federica Vezzani,Gianmaria Silvello*

Main category: cs.CL

TL;DR: 本文提出了一种名为GutBrainIE的新的信息抽取（IE）基准，用于生物医学领域，特别是肠道-大脑轴的研究。该基准包含1600多篇PubMed摘要，由专家手动标注了细粒度的实体、概念链接和关系，旨在克服现有基准范围狭窄和标注质量不高的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学信息抽取（IE）基准的范围有限，并且主要依赖于远距离监督或自动生成的标注，这限制了它们在推动强大的IE方法方面的作用。特别是在快速发展的生物医学领域，如肠道-大脑轴，需要对复杂的相互作用进行结构化知识的提取。

Method: 构建了一个名为GutBrainIE的新基准，该基准基于1600多篇PubMed摘要。由生物医学和术语学专家手动标注了细粒度的实体（NER）、概念级别的链接（NEL）以及关系（RE）。基准结合了高度策展的弱监督数据。

Result: GutBrainIE基准包含了细粒度的实体、概念级别的链接和关系，覆盖了肠道-大脑轴的研究内容。其丰富的模式、多任务和高质量的标注数据，使其能够广泛应用于生物医学IE系统的开发和评估。

Conclusion: GutBrainIE是一个新的、高质量的生物医学信息抽取基准，它为开发和评估IE系统提供了一个更全面、更鲁棒的平台，尤其适用于像肠道-大脑轴这样复杂的研究领域。该基准的灵活性也使其能够应用于其他生物医学领域。

Abstract: Information Extraction (IE), encompassing Named Entity Recognition (NER), Named Entity Linking (NEL), and Relation Extraction (RE), is critical for transforming the rapidly growing volume of scientific publications into structured, actionable knowledge. This need is especially evident in fast-evolving biomedical fields such as the gut-brain axis, where research investigates complex interactions between the gut microbiota and brain-related disorders. Existing biomedical IE benchmarks, however, are often narrow in scope and rely heavily on distantly supervised or automatically generated annotations, limiting their utility for advancing robust IE methods. We introduce GutBrainIE, a benchmark based on more than 1,600 PubMed abstracts, manually annotated by biomedical and terminological experts with fine-grained entities, concept-level links, and relations. While grounded in the gut-brain axis, the benchmark's rich schema, multiple tasks, and combination of highly curated and weakly supervised data make it broadly applicable to the development and evaluation of biomedical IE systems across domains.

</details>


### [141] [Merged ChemProt-DrugProt for Relation Extraction from Biomedical Literature](https://arxiv.org/abs/2405.18605)
*Mai H. Nguyen,Shibani Likhite,Jiawei Tang,Darshini Mahendran,Bridget T. McInnes*

Main category: cs.CL

TL;DR: 通过合并ChemProt和DrugProt数据集并结合图卷积网络（GCN）和BioBERT，提高了化学-基因关系提取的准确性。


<details>
  <summary>Details</summary>
Motivation: 理解化学物质和基因之间的相互作用对于药物发现、疾病理解和生物医学研究至关重要，现有数据集可能不足以提高模型性能。

Method: 合并ChemProt和DrugProt数据集；使用BioBERT和结合GCN与BioBERT的两种先进关系提取算法进行评估。

Result: 合并数据集显著提高了模型性能，尤其是在数据集共享的CPR组中。使用GCN整合全局信息比单独使用BioBERT能提高某些CPR组的精确率和召回率。

Conclusion: 合并数据集和结合GCN与BioBERT是提高化学-基因关系提取性能的有效方法，全局信息对于理解这些相互作用很重要。

Abstract: The extraction of chemical-gene relations plays a pivotal role in understanding the intricate interactions between chemical compounds and genes, with significant implications for drug discovery, disease understanding, and biomedical research. This paper presents a data set created by merging the ChemProt and DrugProt datasets to augment sample counts and improve model accuracy. We evaluate the merged dataset using two state of the art relationship extraction algorithms: Bidirectional Encoder Representations from Transformers (BERT) specifically BioBERT, and Graph Convolutional Networks (GCNs) combined with BioBERT. While BioBERT excels at capturing local contexts, it may benefit from incorporating global information essential for understanding chemical-gene interactions. This can be achieved by integrating GCNs with BioBERT to harness both global and local context. Our results show that by integrating the ChemProt and DrugProt datasets, we demonstrated significant improvements in model performance, particularly in CPR groups shared between the datasets. Incorporating the global context using GCN can help increase the overall precision and recall in some of the CPR groups over using just BioBERT.

</details>


### [142] [Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models](https://arxiv.org/abs/2602.04355)
*Sichu Liang,Hongyu Zhu,Wenwen Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 视觉-语言模型在处理文本编码的n-back任务时比图像编码的任务表现更好，且模型在处理不同指令延迟时存在偏差，这表明需要对多模态工作记忆进行计算敏感的解释。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在处理文本和视觉信息时，工作记忆机制是否一致，以及多模态模型在视觉n-back任务中的表现。

Method: 使用文本渲染和图像渲染的对照空间n-back任务，评估Qwen2.5和Qwen2.5-VL模型的准确性和d'，并通过试错法日志概率证据来分析模型的计算过程。

Result: 在文本条件下的准确性和d'均高于视觉条件；模型在处理2/3-back任务时，往往未能准确反映指令延迟，而是与近期依赖性比较对齐；网格大小的改变会影响刺激流中的近期重复结构，进而改变干扰和错误模式。

Conclusion: 模型在文本编码的任务中表现出更强的工作记忆能力，且在多模态工作记忆的评估中，应考虑计算过程而非仅仅任务表现，并关注刺激流的结构如何影响模型行为。

Abstract: Working memory is a central component of intelligent behavior, providing a dynamic workspace for maintaining and updating task-relevant information. Recent work has used n-back tasks to probe working-memory-like behavior in large language models, but it is unclear whether the same probe elicits comparable computations when information is carried in a visual rather than textual code in vision-language models. We evaluate Qwen2.5 and Qwen2.5-VL on a controlled spatial n-back task presented as matched text-rendered or image-rendered grids. Across conditions, models show reliably higher accuracy and d' with text than with vision. To interpret these differences at the process level, we use trial-wise log-probability evidence and find that nominal 2/3-back often fails to reflect the instructed lag and instead aligns with a recency-locked comparison. We further show that grid size alters recent-repeat structure in the stimulus stream, thereby changing interference and error patterns. These results motivate computation-sensitive interpretations of multimodal working memory.

</details>


### [143] [Beyond Rejection Sampling: Trajectory Fusion for Scaling Mathematical Reasoning](https://arxiv.org/abs/2602.04391)
*Jie Deng,Hanshuang Tong,Jun Li,Shining Liang,Ning Wu,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 提出了一种名为TrajFusion的微调策略，通过融合错误轨迹、反思提示和正确轨迹来模拟试错推理，以改进大语言模型在数学推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于拒绝采样的微调方法会系统性地排除教师生成的错误，这在建模推理失败方面存在不足。

Method: TrajFusion通过构建融合轨迹，将选定的错误轨迹与反思提示和正确轨迹交织在一起，以显式模拟试错推理。融合样本的长度会根据教师错误的频率和多样性自适应调整。

Result: TrajFusion在多个数学基准测试中持续优于传统的拒绝采样微调（RFT），尤其是在处理具有挑战性和长篇幅的推理问题时。

Conclusion: TrajFusion是一种无需改变模型架构或训练目标即可增强LLM数学推理能力的有效策略，通过结构化地利用错误信息来改进训练过程。

Abstract: Large language models (LLMs) have made impressive strides in mathematical reasoning, often fine-tuned using rejection sampling that retains only correct reasoning trajectories. While effective, this paradigm treats supervision as a binary filter that systematically excludes teacher-generated errors, leaving a gap in how reasoning failures are modeled during training. In this paper, we propose TrajFusion, a fine-tuning strategy that reframes rejection sampling as a structured supervision construction process. Specifically, TrajFusion forms fused trajectories that explicitly model trial-and-error reasoning by interleaving selected incorrect trajectories with reflection prompts and correct trajectories. The length of each fused sample is adaptively controlled based on the frequency and diversity of teacher errors, providing richer supervision for challenging problems while safely reducing to vanilla rejection sampling fine-tuning (RFT) when error signals are uninformative. TrajFusion requires no changes to the architecture or training objective. Extensive experiments across multiple math benchmarks demonstrate that TrajFusion consistently outperforms RFT, particularly on challenging and long-form reasoning problems.

</details>


### [144] [Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models](https://arxiv.org/abs/2602.04392)
*Isabel Tsintsiper,Sheng Wong,Beth Albert,Shaun P Brennecke,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: 研究发现，当前大型语言模型（LLMs）在临床推理中存在显著的性别偏见，不同模型的偏见模式各不相同。允许模型不作答并不能消除诊断上的差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛应用于医疗领域，但其训练数据中存在的性别偏见可能导致模型在临床推理中复制或放大这些偏见，引起了对模型安全性和公平性的担忧。

Method: 研究者使用50个由临床医生编写的、性别信息对诊断路径非关键的病例摘要，在四个主流大型语言模型（ChatGPT, Claude 3.7 Sonnet, Gemini 2.0 Flash, DeepSeekchat）上进行了三组实验，评估模型在不同温度设置下的性别分配偏见，并分析允许模型不作答时的影响。

Result: 所有模型都表现出显著的性别分配偏见。在温度0.5时，ChatGPT倾向于分配女性，DeepSeek和Claude也偏向女性，而Gemini则偏向男性。

Conclusion: 当前的大型语言模型在临床推理中存在稳定且模型特异的性别偏见。允许模型不作答并不能完全消除下游诊断的差异。为了安全地将通用模型应用于医疗保健，需要采取保守且有据可查的配置，进行专业层面的临床数据审计，并在部署时持续进行人工监督。

Abstract: Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns that such patterns may be reproduced or amplified. We systematically examined whether contemporary LLMs exhibit sex-specific biases in clinical reasoning and how model configuration influences these behaviours. We conducted three experiments using 50 clinician-authored vignettes spanning 44 specialties in which sex was non-informative to the initial diagnostic pathway. Four general-purpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeekchat). All models demonstrated significant sex-assignment skew, with predicted sex differing by model. At temperature 0.5, ChatGPT assigned female sex in 70% of cases (95% CI 0.66-0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), whereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). Contemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting abstention reduces explicit labelling but does not eliminate downstream diagnostic differences. Safe clinical integration requires conservative and documented configuration, specialty-level clinical data auditing, and continued human oversight when deploying general-purpose models in healthcare settings.

</details>


### [145] [Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://arxiv.org/abs/2602.04398)
*Yujie Lin,Kunquan Li,Yixuan Liao,Xiaoxin Chen,Jinsong Su*

Main category: cs.CL

TL;DR: 该研究提出了一种无需微调或修改提示即可检测和减轻大型语言模型（LLMs）中社会偏见的方法，通过识别诱导刻板印象的词语，并将偏见归因于特定神经元，然后直接干预其激活来减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM去偏见方法在扩展性或用户体验方面存在问题，研究旨在提出一种更有效、更具可扩展性的去偏见方法。

Method: 研究首先通过跨人口群体进行比较分析来识别诱导刻板印象的形容词和名词。然后，利用基于集成梯度的两种归因策略，将有偏见的行为归因于特定的神经元。最后，通过直接干预投影层中的神经元激活来减轻偏见。

Result: 在三个广泛使用的LLM上进行的实验表明，该方法能够有效地减少偏见，同时保持模型的整体性能。

Conclusion: 该研究提出的无需微调或修改提示的框架能够有效检测并减轻LLMs中的社会偏见，为解决LLMs的公平性问题提供了新的思路。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, their outputs often exhibit social biases, raising fairness concerns. Existing debiasing methods, such as fine-tuning on additional datasets or prompt engineering, face scalability issues or compromise user experience in multi-turn interactions. To address these challenges, we propose a framework for detecting stereotype-inducing words and attributing neuron-level bias in LLMs, without the need for fine-tuning or prompt modification. Our framework first identifies stereotype-inducing adjectives and nouns via comparative analysis across demographic groups. We then attribute biased behavior to specific neurons using two attribution strategies based on integrated gradients. Finally, we mitigate bias by directly intervening on their activations at the projection layer. Experiments on three widely used LLMs demonstrate that our method effectively reduces bias while preserving overall model performance. Code is available at the github link: https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution.

</details>


### [146] [Swordsman: Entropy-Driven Adaptive Block Partition for Efficient Diffusion Language Models](https://arxiv.org/abs/2602.04399)
*Yu Zhang,Xinchen Li,Jialei Zhou,Hongnan Ma,Zhongwei Wan,Yiwei Shi,Duoqian Miao,Qi Zhang,Longbing Cao*

Main category: cs.CL

TL;DR: 提出了一种名为Swordsman的熵驱动自适应分块解码框架，用于提高扩散语言模型的推理速度和质量。它通过识别熵变化来动态划分分块，并根据块内状态调整解掩码阈值，以更好地与语义或语法结构对齐。


<details>
  <summary>Details</summary>
Motivation: 现有分块解码方法采用固定的分块方式，容易分割完整的语义或语法成分，导致性能不佳。受熵减少假说（ERH）启发，研究者认识到成分边界是减少不确定性的关键点。

Method: Swordsman框架通过识别相邻 token 之间的熵变化来动态划分分块，以更好地对齐语义或语法成分边界。此外，它还根据块内的实时解掩码状态动态调整解掩码阈值。

Result: Swordsman 框架在训练期间无需额外计算（training-free），并支持 KV Cache，在广泛的评估中展现了最先进的性能。

Conclusion: Swordsman 提出的熵驱动自适应分块解码框架能够有效地提高扩散语言模型的推理效率和稳定性，通过更智能地划分分块和调整解掩码策略，克服了现有方法的局限性。

Abstract: Block-wise decoding effectively improves the inference speed and quality in diffusion language models (DLMs) by combining inter-block sequential denoising and intra-block parallel unmasking. However, existing block-wise decoding methods typically partition blocks in a rigid and fixed manner, which inevitably fragments complete semantic or syntactic constituents, leading to suboptimal performance. Inspired by the entropy reduction hypothesis (ERH), we recognize that constituent boundaries offer greater opportunities for uncertainty reduction, which motivates us to employ entropy analysis for identifying constituent boundaries. Therefore, we propose Swordsman, an entropy-driven adaptive block-wise decoding framework for DLMs. Swordsman adaptively partitions blocks by identifying entropy shifts between adjacent tokens to better align with semantic or syntactic constituent boundaries. In addition, Swordsman dynamically adjusts unmasking thresholds conditioned on the real-time unmasking status within a block, further improving both efficiency and stability. As a training-free framework, supported by KV Cache, Swordsman demonstrates state-of-the-art performance across extensive evaluations.

</details>


### [147] [History-Guided Iterative Visual Reasoning with Self-Correction](https://arxiv.org/abs/2602.04413)
*Xinglong Yang,Zhilin Peng,Zhanzhan Liu,Haochen Shi,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出了一种名为 H-GIVR 的框架，通过多次观察图像并利用历史推理信息来动态纠正错误，从而提高多模态大语言模型（MLLMs）的跨模态推理准确性，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的自洽性方法通常采用固定的“重复采样和投票”模式，无法重用历史推理信息，导致模型难以主动纠正视觉理解错误并动态调整推理过程。

Method: H-GIVR 框架在迭代推理过程中，允许 MLLM 多次观察图像，并将先前生成的答案作为后续步骤的参考，从而实现错误的动态纠正。

Result: 在五个数据集和三个模型上的实验表明，H-GIVR 框架显著提高了跨模态推理的准确性。例如，在 ScienceQA 数据集上，使用 Llama3.2-vision:11b 模型，平均需要 2.57 个响应即可达到 78.90% 的准确率，比基线模型提高了 107%。

Conclusion: H-GIVR 框架能够有效地提升 MLLMs 的跨模态推理能力，通过引入动态错误纠正机制，克服了现有自洽性方法的局限性，并在准确性和计算成本之间取得了良好的平衡。

Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However, most existing self-consistency methods are limited to a fixed ``repeated sampling and voting'' paradigm and do not reuse historical reasoning information. As a result, models struggle to actively correct visual understanding errors and dynamically adjust their reasoning during iteration. Inspired by the human reasoning behavior of repeated verification and dynamic error correction, we propose the H-GIVR framework. During iterative reasoning, the MLLM observes the image multiple times and uses previously generated answers as references for subsequent steps, enabling dynamic correction of errors and improving answer accuracy. We conduct comprehensive experiments on five datasets and three models. The results show that the H-GIVR framework can significantly improve cross-modal reasoning accuracy while maintaining low computational cost. For instance, using \texttt{Llama3.2-vision:11b} on the ScienceQA dataset, the model requires an average of 2.57 responses per question to achieve an accuracy of 78.90\%, representing a 107\% improvement over the baseline.

</details>


### [148] [Fine-Grained Activation Steering: Steering Less, Achieving More](https://arxiv.org/abs/2602.04428)
*Zijian Feng,Tianjiao Li,Zixiao Zhu,Hanzhang Zhou,Junlang Qian,Li Zhang,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: 本研究提出了一种名为AUSteer的新方法，通过在更细粒度的原子单元（AU）层面进行激活引导，克服了现有块级别激活引导的低效和侵扰性问题。AUSteer通过识别区分性AU并分配自适应引导强度，实现了更精确和有效的LLM行为修改。


<details>
  <summary>Details</summary>
Motivation: 现有LLM激活引导方法通常在块级别进行，但块级别激活具有异质性，将有益、无关和有害的特征纠缠在一起，导致引导过程粗糙、低效且具有侵扰性。研究旨在解决这一问题。

Method: 将块激活分解为原子单元（AU）级别激活，其中每个AU对应块权重矩阵的一个切片。通过理论和实证分析，发现不同AU控制着LLM输出中不同的token分布。AUSteer方法通过计算对比样本上的激活动量来全局识别区分性AU，并为不同的输入和选定的AU激活分配自适应的引导强度。

Result: 在多个LLM和任务上的实验表明，AUSteer始终优于先进的基线方法，并且引导的激活数量显著减少，证明了“少即是多”的原则。

Conclusion: AU级别的激活引导比块级别的激活引导更精确、更有效。AUSteer通过在AU层面进行操作，实现了更高效和低侵扰性的LLM行为修改。

Abstract: Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.

</details>


### [149] [No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data](https://arxiv.org/abs/2602.04442)
*Dmitry Karpov*

Main category: cs.CL

TL;DR: 研究了五种突厥语系语言对的机器翻译，并发布了数据集和模型权重。


<details>
  <summary>Details</summary>
Motivation: 为五种突厥语系语言对（俄-巴什基尔、俄-哈萨克、俄-吉尔吉斯、英-鞑靼、英-楚瓦什）开发机器翻译系统。

Method: 使用了两种主要方法：1. 对 nllb-200-distilled-600M 模型使用 LoRA 和合成数据进行微调。2. 使用检索到的相似示例提示 DeepSeek-V3.2 模型。

Result: 对于哈萨克语，微调方法达到了 chrF++ 49.71；对于巴什基尔语，微调方法达到了 chrF++ 46.94；对于楚瓦什语，提示方法达到了 chrF++ 39.47；对于鞑靼语，零样本或检索方法达到了 chrF++ 41.6；对于吉尔吉斯语，零样本方法达到了 chrF++ 45.6。

Conclusion: 通过微调和提示方法，在五种突厥语系语言对上取得了有竞争力的机器翻译结果，并公开了数据集和模型权重，以促进该领域的研究。

Abstract: We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.

</details>


### [150] [Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks](https://arxiv.org/abs/2602.04466)
*Masaya Tsunokake,Yuta Koreeda,Terufumi Morishita,Koichi Nagatsuka,Hikaru Tomonari,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 该研究评估了微领域自适应预训练（mDAPT）在生成式任务中的效果，发现mDAPT在提取相关事实方面有效，但在推理和长文答案组织方面存在瓶颈，并强调了增强模型推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明mDAPT在微领域有效，但仅在选择题上进行评估，其在生成式任务中的潜力和瓶颈尚不清楚。因此，本研究旨在揭示mDAPT在生成式任务中的潜力和瓶颈。

Method: 研究将回答过程分解为三个子任务：事实提取（eliciting）、推理（reasoning）和答案组织（composing），并针对IT技术支持操作中的专有IT产品知识，评估了mDAPT在每个子任务上的性能。

Result: mDAPT成功解决了基础模型难以应对的事实提取任务，但在推理和答案组织方面并未取得显著进展。研究还发现，解决事实提取和推理任务能确保超过90%的性能。

Conclusion: mDAPT在知识提取方面有效，但在推理和答案组织方面存在瓶颈。为了提高生成式任务的整体性能，需要着重增强模型的推理能力。

Abstract: When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.

</details>


### [151] [Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition](https://arxiv.org/abs/2602.04486)
*Jinlong Ma,Yu Zhang,Xuefeng Bai,Kehai Chen,Yuwei Wang,Zeming Liu,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于多模态大语言模型（MLLMs）的端到端地面多模态命名实体识别（GMNER）方法，解决了MLLMs存在的模态偏见问题，并通过模态感知一致性推理（MCR）在GMNER和视觉定位任务上取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现端到端的GMNER，并克服现有方法中MLLMs常作为辅助工具以及其固有的模态偏见（视觉偏见和文本偏见）问题。

Method: 提出模态感知一致性推理（MCR），包含多风格推理模式注入（MRSI）和约束引导可验证优化（CVO）。MRSI将抽象约束转化为可执行的推理链，CVO通过组相对策略优化（GRPO）动态调整模型的推理轨迹，以解决模态偏见。

Result: 实验表明，MCR能够有效缓解模态偏见，并在GMNER和视觉定位任务上取得了优于现有基线方法的性能。

Conclusion: MLLMs可以被用于端到端的GMNER，而提出的MCR方法能够有效解决MLLMs的模态偏见问题，并提升其在相关任务上的表现。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit $\textbf{modality bias}$, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning ($\textbf{MCR}$), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.

</details>


### [152] [Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough](https://arxiv.org/abs/2602.04489)
*Dario Paape,Tal Linzen,Shravan Vasishth*

Main category: cs.CL

TL;DR: 本研究提出了一种基于隐过程混合模型的人类阅读行为建模方法，能够区分句法歧义句的歧义概率、歧义成本和重新分析成本，并考虑了分心阅读的情况，从而提供更真实的加工成本估计。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为人类阅读行为，特别是对暂时模糊的“花园路径”句子的理解，提供一个更准确、更现实的计算模型，以解决现有模型未能充分捕捉阅读过程中的复杂性（如分心阅读）的问题。

Method: 研究者使用“花园路径”句子作为测试案例，开发了一个隐过程混合模型，该模型被应用于四种不同的阅读范式（眼动追踪、单向和双向自定步调阅读、迷宫任务）。模型区分了花园路径概率、花园路径成本和重新分析成本，并纳入了对分心阅读试验的处理。此外，还进行了交叉验证，将该混合模型与基于GPT-2语言模型的预测模型进行了比较。

Result: 该模型能够重现实验数据中观察到的有关重读行为、理解问题回答和语法判断的经验模式。与基于GPT-2的预测模型相比，混合模型在预测人类阅读模式和试验结束时任务数据方面表现出更好的拟合度。

Conclusion: 提出的隐过程混合模型为建模人类阅读行为，尤其是处理句法歧义句的能力，提供了一种更有效的方法，它能够更准确地估计加工成本，并能更好地预测人类的阅读反应，表明其在计算语言学和认知科学领域具有潜在的应用价值。

Abstract: Using temporarily ambiguous garden-path sentences ("While the team trained the striker wondered ...") as a test case, we present a latent-process mixture model of human reading behavior across four different reading paradigms (eye tracking, uni- and bidirectional self-paced reading, Maze). The model distinguishes between garden-path probability, garden-path cost, and reanalysis cost, and yields more realistic processing cost estimates by taking into account trials with inattentive reading. We show that the model is able to reproduce empirical patterns with regard to rereading behavior, comprehension question responses, and grammaticality judgments. Cross-validation reveals that the mixture model also has better predictive fit to human reading patterns and end-of-trial task data than a mixture-free model based on GPT-2-derived surprisal values. We discuss implications for future work.

</details>


### [153] [PersoDPO: Scalable Preference Optimization for Instruction-Adherent, Persona-Grounded Dialogue via Multi-LLM Evaluation](https://arxiv.org/abs/2602.04493)
*Saleh Afzoon,MohammadHossein Ahmadi,Usman Naseem,Amin Beheshti*

Main category: cs.CL

TL;DR: 本文提出了一种名为 PersoDPO 的可扩展框架，通过自动评估的监督信号来微调对话模型，以解决开源大语言模型在生成个性化和上下文连贯的对话方面的不足，实验结果表明该方法优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型在生成个性化和上下文连贯的对话方面存在困难，这影响了用户参与度和回应的相关性与一致性。

Method: 提出 PersoDPO 框架，利用对开源和闭源大语言模型生成回应的自动评估信号进行微调。该框架集成了针对连贯性、个性化以及指令遵循（长度格式合规性）的评估指标，从而自动构建高质量的偏好对，无需人工标注。

Result: 在 FoCus 数据集上的实验表明，使用 PersoDPO 框架微调的开源语言模型在多个评估维度上始终优于强基线模型和标准的直接偏好优化 (DPO) 变体。

Conclusion: PersoDPO 框架能够有效地通过自动评估信号来改进开源大语言模型在生成个性化和上下文连贯对话方面的能力，提供了一个可扩展且可复现的训练流程。

Abstract: Personalization and contextual coherence are two essential components in building effective persona-grounded dialogue systems. These aspects play a crucial role in enhancing user engagement and ensuring responses are more relevant and consistent with user identity. However, recent studies indicate that open-source large language models (LLMs) continue to struggle to generate responses that are both contextually grounded and aligned with persona cues, despite exhibiting strong general conversational abilities like fluency and naturalness. We present PersoDPO, a scalable preference optimisation framework that uses supervision signals from automatic evaluations of responses generated by both closed-source and open-source LLMs to fine-tune dialogue models. The framework integrates evaluation metrics targeting coherence and personalization, along with a length-format compliance feature to promote instruction adherence. These signals are combined to automatically construct high-quality preference pairs without manual annotation, enabling a scalable and reproducible training pipeline. Experiments on the FoCus dataset show that an open-source language model fine-tuned with the PersoDPO framework consistently outperforms strong open-source baselines and a standard Direct Preference Optimization (DPO) variant across multiple evaluation dimensions.

</details>


### [154] [Model-Dowser: Data-Free Importance Probing to Mitigate Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2602.04509)
*Hyeontaek Hwang,Nguyen Dinh Son,Daeyoung Kim*

Main category: cs.CL

TL;DR: 提出了一种名为Model-Dowser的新型稀疏微调方法，用于多模态大语言模型（MLLMs），以缓解灾难性遗忘问题，并在保持资源效率和可扩展性的同时，有效提高了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解MLLMs灾难性遗忘的方法要么对模型深层无效，要么在大模型上扩展性差。研究旨在克服这些限制，提出一种更有效且可扩展的解决方案。

Method: Model-Dowser通过联合考虑权重大小、输入激活和输出敏感度，计算模型参数相对于预训练泛化能力的重要性得分。在微调过程中，选择性地保留高重要性参数，更新其余参数。

Result: 在LLaVA和NVILA两个MLLMs上的实验表明，Model-Dowser能有效缓解灾难性遗忘，性能优于现有方法，并且在资源效率和可扩展性方面表现出色。

Conclusion: Model-Dowser是一种有效的稀疏微调方法，可以缓解MLLMs在下游任务微调时的灾难性遗忘，并且在不同模型规模下都表现出良好的性能和效率。

Abstract: Fine-tuning Multimodal Large Language Models (MLLMs) on task-specific data is an effective way to improve performance on downstream applications. However, such adaptation often leads to a degradation in generalization on pretrained tasks, a phenomenon known as Catastrophic Forgetting. Existing methods that aim to mitigate this issue either become ineffective when fine-tuning deeper layers of the language decoder or scale poorly with increasing model size. To address these limitations, we propose Model-Dowser, a novel sparse fine-tuning approach for MLLMs. Model-Dowser measures a principled importance score for each model parameter with respect to pretrained generalization (prior to downstream adaptation) by jointly considering weight magnitudes, input activations, and output sensitivities. During fine-tuning, Model-Dowser selectively preserves high-importance parameters and updates the remaining. Comprehensive experiments on two representative MLLMs, LLaVA and NVILA, demonstrate that Model-Dowser effectively mitigates catastrophic forgetting and consistently outperforms prior methods, while remaining resource-efficient and scalable to multi-billion-parameter models.

</details>


### [155] [ReFRAME or Remain: Unsupervised Lexical Semantic Change Detection with Frame Semantics](https://arxiv.org/abs/2602.04514)
*Bach Phan-Tat,Kris Heylen,Dirk Geeraerts,Stefano De Pascale,Dirk Speelman*

Main category: cs.CL

TL;DR: 本文提出了一种基于框架语义学的词汇语义变化检测新方法，该方法在解释性和性能上均优于现有的基于神经嵌入的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经嵌入的词汇语义变化检测方法虽然在基准测试中表现良好，但其结果难以解释。研究者希望探索一种更具可解释性的替代方法。

Method: 本文不依赖神经嵌入，而是完全基于框架语义学来检测词汇语义变化。研究者对该方法的预测进行了详细的定量和定性分析。

Result: 基于框架语义学的方法在检测语义变化方面是有效的，并且在性能上可以超越许多现有的分布式语义模型。其预测结果既合理又具有高度可解释性。

Conclusion: 框架语义学为词汇语义变化检测提供了一种有效且可解释的替代方法，其性能可与甚至优于基于分布式语义的模型。

Abstract: The majority of contemporary computational methods for lexical semantic change (LSC) detection are based on neural embedding distributional representations. Although these models perform well on LSC benchmarks, their results are often difficult to interpret. We explore an alternative approach that relies solely on frame semantics. We show that this method is effective for detecting semantic change and can even outperform many distributional semantic models. Finally, we present a detailed quantitative and qualitative analysis of its predictions, demonstrating that they are both plausible and highly interpretable

</details>


### [156] [LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding](https://arxiv.org/abs/2602.04541)
*Gang Lin,Dongfang Li,Zhuoen Chen,Yukun Shi,Xuhui Chen,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LycheeDecode 是一种新的高效 LLM 解码方法，通过精细化的混合注意力机制和 top-k 选择策略，解决了长上下文 LLM 解码时 KV 缓存内存和延迟过高的问题，在保证生成质量的同时显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文 LLM 在解码时，KV 缓存占用大量内存和带来高延迟。现有的共享关键 token 方法忽略了注意力头的功能多样性，导致性能下降。

Method: 提出 LycheeDecode，一种基于精细化混合头注意力机制的解码方法。该方法使用 HardKuma 机制将注意力头分为检索头（动态识别关键 token）和稀疏头（复用检索头识别的 token），并结合硬件高效的 top-k 选择策略。

Result: 在 Llama3 和 Qwen3 模型上，LycheeDecode 在长上下文理解和复杂推理任务上，生成质量与全注意力基线相当甚至更好，同时在 128K 上下文长度下实现了高达 2.7 倍的加速。

Conclusion: LycheeDecode 通过保留注意力头的多功能性，克服了现有方法的性能瓶颈，为高效高质量的长上下文 LLM 推理提供了一种有效且经过验证的方法。

Abstract: The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, such coarse-grained sharing undermines model performance by neglecting the functional diversity of attention heads. To address this, we propose LycheeDecode, an efficient decoding method centered on a fine-grained hybrid-head attention mechanism that employs a hardware-efficient top-k selection strategy. Specifically, the novel HardKuma-based mechanism partitions attention heads into a small subset of retrieval heads that dynamically identify crucial tokens and a majority of sparse heads that reuse them for efficient computation. Through extensive experiments on leading models like Llama3 and Qwen3 across diverse benchmarks for long-context understanding (e.g., LongBench, RULER) and complex reasoning (e.g., AIME24, OlympiadBench), we demonstrate that LycheeDecode achieves generative quality comparable to, and at times surpassing even the full-attention baseline. Crucially, this is accomplished with up to a 2.7x speedup at a 128K context length. By preserving the functional diversity of attention heads, our fine-grained strategy overcomes the performance bottlenecks of existing methods, providing a powerful and validated pathway to both efficient and high-quality long-context LLM inference.

</details>


### [157] [$C$-$ΔΘ$: Circuit-Restricted Weight Arithmetic for Selective Refusal](https://arxiv.org/abs/2602.04521)
*Aditya Kasliwal,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 该研究提出了一种名为 C-Δθ 的新方法，通过在训练阶段对模型进行稀疏的、特定于拒绝类别的权重更新，将 LLM 的选择性拒绝能力从推理时移至离线训练阶段，从而降低了推理成本并简化了部署。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 安全策略通常在推理时进行干预，这会产生持续的计算成本和部署复杂性。研究人员希望找到一种将选择性拒绝能力完全转移到离线训练阶段的方法，以降低成本并简化部署。

Method: 该方法使用 EAP-IG 技术来识别引起拒绝的稀疏计算电路，然后仅在该电路（通常 <5% 的参数）上进行约束性权重更新（ΔθC）。更新后的模型作为一个标准的检查点部署，无需推理时干预。

Result: 通过对拒绝和效用基准的评估，C-Δθ 方法在类别目标拒绝和能力保留方面取得了良好的效果。

Conclusion: C-Δθ 方法能够将 LLM 的选择性拒绝能力转化为离线权重更新，从而生成无需推理时干预的“即插即用”的编辑模型，有效地将成本从每次请求的干预转移到一次性离线更新。

Abstract: Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Δθ: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ΔθC supported only on that circuit (typically <5% of parameters). Applying ΔθC yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.

</details>


### [158] [Rethinking Weight Tying: Pseudo-Inverse Tying for Stable LM Training and Updates](https://arxiv.org/abs/2602.04556)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 提出伪逆绑定（PIT）技术，通过将输入嵌入和输出投影绑定到共享的潜在令牌内存，确保了令牌接口的稳定性，从而提高了训练稳定性、语义一致性并减少了副作用。


<details>
  <summary>Details</summary>
Motivation: 标准的权重绑定（weight tying）在紧凑型语言模型中用于减少参数，但会破坏输入嵌入和输出投影之间的对应关系，导致训练不稳定，并使得训练后干预（如编辑、修补和轻量级适配）效果难以预测。

Method: PIT 通过将嵌入和反嵌入视为共享潜在令牌内存的耦合投影，保持一个正交共享内存，并通过一个完全学习到的对称正定变换来同步它们。输出层在词汇投影前应用该变换，而嵌入层使用稳定的三角求解来应用逆变换，从而避免了显式的伪逆重计算和额外的词汇大小参数。

Result: 在 256M 到 1.3B 参数的设备端模型上进行了预训练和适配评估，PIT 持续表现出更佳的训练稳定性、更强的层间语义一致性，以及显著减少的副作用。

Conclusion: PIT 技术通过提供一个伪逆一致的令牌接口，有效解决了权重绑定带来的训练不稳定性和干预效果不可预测的问题，并在实际应用中带来了多方面的性能提升。

Abstract: Weight tying is widely used in compact language models to reduce parameters by sharing the token table between the input embedding and the output projection. However, weight sharing does not guarantee a stable token interface: during training, the correspondence between encoding tokens into hidden states and decoding hidden states into logits can drift, worsening optimization sensitivity and making post-training interventions such as editing, patching, and lightweight adaptation less predictable. We propose Pseudo-Inverse Tying (PIT), which synchronizes embedding and unembedding as coupled projections of a shared latent token memory, guaranteeing a pseudo-inverse-consistent interface throughout training. PIT maintains an orthonormal shared memory, obtained by thin polar decomposition for teacher initialization or random orthonormal initialization from scratch, and introduces a fully learned symmetric positive definite hidden-space transform parameterized via a Cholesky factor. The output head applies this transform to hidden states before the vocabulary projection, while the embedding applies the inverse transform to token vectors using stable triangular solves, avoiding explicit pseudo-inverse recomputation and any vocabulary-sized auxiliary parameters. We evaluate PIT on on-device models spanning 256M-1.3B parameters across pretraining and adaptation, and consistently observe improved training stability, stronger layerwise semantic consistency, and substantially reduced side effects.

</details>


### [159] [VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration](https://arxiv.org/abs/2602.04587)
*Jaeyoon Jung,Yejun Yoon,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: 本文提出了一种名为VILLAIN的多模态事实核查系统，通过基于提示的多智能体协作来验证图文声明，并在AVerImaTeC共享任务中取得了第一名的成绩。


<details>
  <summary>Details</summary>
Motivation: 为了解决图文声明的事实核查问题，并为AVerImaTeC共享任务提供一个有效的解决方案。

Method: VILLAIN系统采用基于提示的多智能体协作框架，包含文本和视觉证据检索、模态特定和跨模态分析报告生成、问题-答案对生成以及最终的判决预测。利用了多模态的视觉语言模型。

Result: VILLAIN系统在AVerImaTeC共享任务的排行榜上所有评估指标均排名第一。

Conclusion: VILLAIN系统能够有效地通过多模态信息和多智能体协作进行图文声明的事实核查，并在实际任务中取得了优异的表现。

Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.

</details>


### [160] [Textual Planning with Explicit Latent Transitions](https://arxiv.org/abs/2602.04557)
*Eliezer Shlomi,Ido Levy,Eilam Shapira,Michael Katz,Guy Uziel,Segev Shlomov,Nir Mashkif,Roi Reichart,Sarah Keren*

Main category: cs.CL

TL;DR: EmbedPlan 提出了一种使用冻结语言嵌入空间的轻量级过渡模型来替代 LLM 的自回归生成，从而加速规划过程。它通过在嵌入空间中预测下一个状态并进行最近邻检索来找到下一个状态。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 LLM 的规划方法受限于 token-by-token 生成和重复的全前向传播，导致多步前瞻和基于 rollout 的搜索在延迟和计算成本上非常高。

Method: EmbedPlan 将自然语言状态和动作描述编码为向量，在冻结的语言嵌入空间中预测下一个状态的嵌入，并通过最近邻相似性检索最接近的下一个状态。这种方法无需微调编码器即可实现快速规划计算。

Result: 在九个经典规划域和六种不同难度的评估协议下，EmbedPlan 在内插（interpolation）任务上表现近乎完美，但在需要泛化到未见问题或域的外插（extrapolation）任务上性能急剧下降。计划变体（plan-variant）评估表明 EmbedPlan 能够泛化到替代计划，而不是仅仅记忆已见轨迹。

Conclusion: 冻结的语言嵌入可以支持在观察到某个域的转换后学习该域内的动态，但跨域的泛化能力仍然是一个瓶颈。

Abstract: Planning with LLMs is bottlenecked by token-by-token generation and repeated full forward passes, making multi-step lookahead and rollout-based search expensive in latency and compute. We propose EmbedPlan, which replaces autoregressive next-state generation with a lightweight transition model operating in a frozen language embedding space. EmbedPlan encodes natural language state and action descriptions into vectors, predicts the next-state embedding, and retrieves the next state by nearest-neighbor similarity, enabling fast planning computation without fine-tuning the encoder. We evaluate next-state prediction across nine classical planning domains using six evaluation protocols of increasing difficulty: interpolation, plan-variant, extrapolation, multi-domain, cross-domain, and leave-one-out. Results show near-perfect interpolation performance but a sharp degradation when generalization requires transfer to unseen problems or unseen domains; plan-variant evaluation indicates generalization to alternative plans rather than memorizing seen trajectories. Overall, frozen embeddings support within-domain dynamics learning after observing a domain's transitions, while transfer across domain boundaries remains a bottleneck.

</details>


### [161] [Trust The Typical](https://arxiv.org/abs/2602.04581)
*Debargha Ganguly,Sreehari Sankar,Biyao Zhang,Vikash Singh,Kanan Gupta,Harshini Kavuru,Alan Luo,Weicong Chen,Warren Morningstar,Raghu Machiraju,Vipin Chaudhary*

Main category: cs.CL

TL;DR: 本研究提出了一种名为Trust The Typical (T3) 的新方法，将LLM安全视为一个“非分布外（OOD）”检测问题，通过学习安全提示的分布来检测潜在威胁，无需训练有害样本，并在多个基准测试中表现出色，且对生产环境友好。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全方法依赖于识别和阻止已知威胁的“猫鼠游戏”，这种方法脆弱且难以维护。研究者认为，更稳健的安全措施应源于对“安全”的深度理解，而非穷举“有害”的内容。

Method: T3框架将安全问题建模为OOD检测问题。它学习安全提示在语义空间中的分布，并将任何显著偏离该分布的提示视为潜在威胁。该方法不使用有害样本进行训练。

Result: T3在18个跨越毒性、仇恨言论、越狱、多语言危害和过度拒绝等方面的基准测试中取得了最先进的性能。与专门的安全模型相比，其误报率最高可降低40倍。此外，一个仅用安全英文文本训练的模型，无需重新训练即可有效迁移到不同领域和14种以上语言。T3的GPU优化版本集成到vLLM后，在生产环境中能够实现低于6%的性能开销，支持持续的token生成守卫。

Conclusion: T3提供了一种无需训练有害样本的新型LLM安全范式，通过学习安全提示的分布来检测OOD威胁，展现了出色的性能、跨语言迁移能力和生产环境的适用性。

Abstract: Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.

</details>


### [162] [Can LLMs capture stable human-generated sentence entropy measures?](https://arxiv.org/abs/2602.04570)
*Estrella Pivel-Villanueva,Elisabeth Frederike Sterner,Franziska Knolle*

Main category: cs.CL

TL;DR: 研究人员使用自助法分析了德语和英语的填空数据集，发现预测性越强的句子所需的样本量越少，并且确定了稳定的人类语言熵估计所需的样本量。他们还发现，虽然大型语言模型（LLM）可以近似人类语言熵，但它们不能完全替代人类数据，并且GPT-4o在与人类数据的一致性方面表现最佳，但具体效果取决于提取方法和提示设计。


<details>
  <summary>Details</summary>
Motivation: 当前对于获得稳定的、无偏见的词级别语言熵估计所需的人类反应数量没有共识。此外，大型语言模型（LLM）被越来越多地用作人类规范数据的替代品，但它们能否复制稳定的人类熵尚不清楚。

Method: 研究人员利用两个大型的公开可用的德语和英语填空数据集，采用基于自助法的收敛分析，跟踪熵估计值如何随样本量的增加而稳定。然后，他们将稳定的人类熵值与从几种LLMs（包括GPT-4o）提取的熵估计值进行比较，使用了基于logit的概率提取和基于采样的频率估计。

Result: 在两种语言中，超过97%的句子在可用样本量内达到了稳定的熵估计。90%的句子在德语中需要111个响应，在英语中需要81个响应才能收敛；低熵句子（<1）仅需20个响应，而高熵句子（>2.5）则需要更多。GPT-4o与人类数据的对应度最高，但一致性强烈依赖于提取方法和提示设计。基于logit的估计值最小化了绝对误差，而基于采样的估计值在捕捉人类变异的离散度方面表现更好。

Conclusion: 研究结果为人类规范实践提供了首个直接的经验验证，并表明收敛性关键取决于句子的可预测性。研究还表明，虽然LLMs可以近似人类熵，但它们不能完全替代稳定的人类衍生分布。

Abstract: Predicting upcoming words is a core mechanism of language comprehension and may be quantified using Shannon entropy. There is currently no empirical consensus on how many human responses are required to obtain stable and unbiased entropy estimates at the word level. Moreover, large language models (LLMs) are increasingly used as substitutes for human norming data, yet their ability to reproduce stable human entropy remains unclear. Here, we address both issues using two large publicly available cloze datasets in German 1 and English 2. We implemented a bootstrap-based convergence analysis that tracks how entropy estimates stabilize as a function of sample size. Across both languages, more than 97% of sentences reached stable entropy estimates within the available sample sizes. 90% of sentences converged after 111 responses in German and 81 responses in English, while low-entropy sentences (<1) required as few as 20 responses and high-entropy sentences (>2.5) substantially more. These findings provide the first direct empirical validation for common norming practices and demonstrate that convergence critically depends on sentence predictability. We then compared stable human entropy values with entropy estimates derived from several LLMs, including GPT-4o, using both logit-based probability extraction and sampling-based frequency estimation, GPT2-xl/german-GPT-2, RoBERTa Base/GottBERT, and LLaMA 2 7B Chat. GPT-4o showed the highest correspondence with human data, although alignment depended strongly on the extraction method and prompt design. Logit-based estimates minimized absolute error, whereas sampling-based estimates were better in capturing the dispersion of human variability. Together, our results establish practical guidelines for human norming and show that while LLMs can approximate human entropy, they are not interchangeable with stable human-derived distributions.

</details>


### [163] [Semantic Self-Distillation for Language Model Uncertainty](https://arxiv.org/abs/2602.04577)
*Edward Phillips,Sean Wu,Boyan Gao,David A. Clifton*

Main category: cs.CL

TL;DR: 本研究提出了一种名为“语义自蒸馏”（SSD）的技术，能够将大型语言模型（LLM）复杂的语义不确定性信息蒸馏到一个轻量级的学生模型中，用于实时应用中的不确定性量化，以预测幻觉和评估答案的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不确定性量化方面存在挑战，尤其是在延迟敏感的应用中，语义色散（semantic dispersion）作为一种度量模型不确定性的方法计算成本过高。因此，需要一种更高效的方法来估计LLM的不确定性。

Method: 研究人员将LLM的语义分布信息蒸馏到一个小型学生模型中。这个学生模型可以在LLM生成答案之前，根据输入提示（prompt-conditioned）预测一个语义分布，并计算该分布的熵作为不确定性信号，同时利用概率密度评估答案的可靠性。

Result: 在TriviaQA数据集上，SSD技术在预测幻觉方面达到了甚至优于有限样本语义色散的性能，并且能有效检测出域外答案。SSD技术提供了一个可靠的信号，用于评估LLM生成答案的可靠性。

Conclusion: 语义自蒸馏（SSD）是一种通用的技术，能够将复杂输出空间（如LLM的语义分布）中的预测不确定性蒸馏到轻量级模型中，为实时应用中的不确定性量化提供了一种高效的解决方案，并具有泛化到其他复杂输出空间的潜力。

Abstract: Large language models present challenges for principled uncertainty quantification, in part due to their complexity and the diversity of their outputs. Semantic dispersion, or the variance in the meaning of sampled answers, has been proposed as a useful proxy for model uncertainty, but the associated computational cost prohibits its use in latency-critical applications. We show that sampled semantic distributions can be distilled into lightweight student models which estimate a prompt-conditioned uncertainty before the language model generates an answer token. The student model predicts a semantic distribution over possible answers; the entropy of this distribution provides an effective uncertainty signal for hallucination prediction, and the probability density allows candidate answers to be evaluated for reliability. On TriviaQA, our student models match or outperform finite-sample semantic dispersion for hallucination prediction and provide a strong signal for out-of-domain answer detection. We term this technique Semantic Self-Distillation (SSD), which we suggest provides a general framework for distilling predictive uncertainty in complex output spaces beyond language.

</details>


### [164] [RexBERT: Context Specialized Bidirectional Encoders for E-commerce](https://arxiv.org/abs/2602.04605)
*Rahul Bajaj,Anuj Garg*

Main category: cs.CL

TL;DR: 研究提出了 RexBERT，一个针对电子商务领域优化的 BERT 模型，通过使用大规模的电子商务语料库 (Ecom-niverse) 和分阶段的预训练方法，在电子商务相关任务上取得了优于通用模型和长上下文模型的效果，即使参数量更少。


<details>
  <summary>Details</summary>
Motivation: 通用目的的 Transformer 模型在检索、分类和排序等领域至关重要，但它们通常在通用语料库上训练，缺乏对专业领域（如电子商务）的覆盖。作者希望为电子商务应用创建一个更高效、性能更好的模型。

Method: 作者构建了一个名为 Ecom-niverse 的 3500 亿 token 的电子商务语料库，并设计了一个模块化管道来提取和处理相关数据。他们还提出了一种三阶段的预训练方法：通用预训练、上下文扩展和退火领域专业化。在此基础上，训练了不同参数规模的 RexBERT 模型。

Result: RexBERT 模型在电子商务相关的数据集上，包括 token 分类、语义相似性和自然语言理解任务，表现优于参数量为 2-3 倍的通用 Encoder 模型，并且与现代长上下文模型相当或更优。这表明高质量的领域内数据和合理的训练方法比单纯的模型扩展更有效。

Conclusion: 高质量的领域内数据结合科学的训练方法，能够为电子商务应用提供比无差别模型扩展更强大的基础。RexBERT 的实验结果证明了这一点，并展示了其在电子商务领域的潜力。

Abstract: Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.

</details>


### [165] [Focus-LIME: Surgical Interpretation of Long-Context Large Language Models via Proxy-Based Neighborhood Selection](https://arxiv.org/abs/2602.04607)
*Junhao Liu,Haonan Yu,Zhenyu Yan,Xin Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为Focus-LIME的粗粒度到细粒度框架，用于解决大型语言模型（LLMs）处理长上下文时特征维度过高导致解释失效的问题，从而实现精确到特征级别的解释。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能够处理越来越大的上下文窗口，在法律审计和代码调试等高风险任务中，实现精确到特征级别的解释变得至关重要。然而，现有的局部模型无关解释方法在高维特征情况下会遇到归因稀释问题，无法提供忠实的解释。

Method: Focus-LIME框架使用一个代理模型来管理扰动邻域，使目标模型能够仅在优化后的上下文中进行细粒度的归因。该方法采用粗粒度到细粒度的策略来恢复解释的可追踪性。

Result: 在长上下文基准测试上的实证评估表明，Focus-LIME方法使得精确到特征级别的解释成为可能，并能为用户提供忠实的解释。

Conclusion: Focus-LIME框架成功地解决了大型语言模型处理长上下文时特征维度过高导致的解释难题，实现了高效且忠实的特征级解释，提高了模型的可解释性。

Abstract: As Large Language Models (LLMs) scale to handle massive context windows, achieving surgical feature-level interpretation is essential for high-stakes tasks like legal auditing and code debugging. However, existing local model-agnostic explanation methods face a critical dilemma in these scenarios: feature-based methods suffer from attribution dilution due to high feature dimensionality, thus failing to provide faithful explanations. In this paper, we propose Focus-LIME, a coarse-to-fine framework designed to restore the tractability of surgical interpretation. Focus-LIME utilizes a proxy model to curate the perturbation neighborhood, allowing the target model to perform fine-grained attribution exclusively within the optimized context. Empirical evaluations on long-context benchmarks demonstrate that our method makes surgical explanations practicable and provides faithful explanations to users.

</details>


### [166] [Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739)
*Casey Ford,Madison Van Doren,Emily Dix*

Main category: cs.CL

TL;DR: 研究评估了四种多模态大语言模型（MLLMs）在对抗性提示下的安全性，发现不同模型家族的安全表现存在显著差异，且安全性能并非稳定不变，会随着模型更新而发生变化。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在现实世界中的应用日益广泛，但它们在对抗性提示下的安全性研究却不足。

Method: 研究采用了两阶段的评估方法，使用由26名专业红队成员设计的726个对抗性提示作为固定基准。第一阶段评估了GPT-4o, Claude Sonnet 3.5, Pixtral 12B, 和 Qwen VL Plus；第二阶段评估了它们的后续模型（GPT-5, Claude Sonnet 4.5, Pixtral Large, 和 Qwen Omni）。共收集了82,256个人工伤害评分。

Result: 1. 模型家族之间存在显著且持续的安全差异：Pixtral模型最易受攻击，而Claude模型由于高拒绝率显得更安全。2. GPT和Claude模型在后续版本中攻击成功率（ASR）有所上升，而Pixtral和Qwen模型ASR略有下降，显示出明显的对齐漂移。3. 模态效应随时间推移而变化：第一阶段文本提示更有效，第二阶段则出现模型特异性模式，GPT-5和Claude 4.5在不同模态下的脆弱性接近。

Conclusion: MLLMs的安全性不是统一的，也不是在模型更新后保持稳定的。因此，需要长期的、多模态的基准测试来追踪其不断演变的安全行为。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour.

</details>


### [167] [Beyond Holistic Scores: Automatic Trait-Based Quality Scoring of Argumentative Essays](https://arxiv.org/abs/2602.04604)
*Lucile Favero,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Main category: cs.CL

TL;DR: 本研究探讨了两种用于自动论证性写作评分的方法：基于小规模开源语言模型（LLM）的结构化上下文学习，以及基于BigBird模型的监督式编码器方法，并采用了CORAL序数回归。研究发现在ASAP++数据集上，显式建模分数序数性显著提高了与人类评分者的一致性，优于LLM和基线模型。同时，小规模LLM在无需微调的情况下表现出竞争力，尤其是在推理相关性方面，并支持透明、隐私保护的本地部署。


<details>
  <summary>Details</summary>
Motivation: 传统的自动论文评分系统主要关注整体分数，其教育用途有限，尤其是在论证性写作等复杂文体中。教育者和学习者需要可解释的、基于特质的反馈，以符合教学目标和评分标准。

Method: 研究了两种方法：1. 使用小型开源LLM进行结构化上下文学习，通过精心设计的、与评分标准对齐的示例、反馈和置信度请求进行提示。2. 使用基于BigBird模型的监督式编码器，结合CORAL风格的序数回归方法，优化长序列理解能力，并显式建模分数序数性。

Result: 在ASAP++数据集上的系统评估显示，显式建模分数序数性显著提高了模型在所有评分特质上与人类评分者的一致性，优于LLM以及基于名义分类和回归的基线模型。小规模开源LLM在无需任务特定的微调下，取得了具有竞争力的性能，尤其是在推理导向的特质上。

Conclusion: 研究强调了在教育评估中，将模型目标与评分标准的语义对齐的重要性。小型开源LLM在无需微调的情况下具有可行性，并能支持透明、隐私保护的本地化评估。本研究为设计AI驱动的教育系统，以提供可解释的、符合评分标准的论证性写作反馈提供了方法论、建模和实践见解。

Abstract: Automated Essay Scoring systems have traditionally focused on holistic scores, limiting their pedagogical usefulness, especially in the case of complex essay genres such as argumentative writing. In educational contexts, teachers and learners require interpretable, trait-level feedback that aligns with instructional goals and established rubrics. In this paper, we study trait-based Automatic Argumentative Essay Scoring using two complementary modeling paradigms designed for realistic educational deployment: (1) structured in-context learning with small open-source LLMs, and (2) a supervised, encoder-based BigBird model with a CORAL-style ordinal regression formulation, optimized for long-sequence understanding. We conduct a systematic evaluation on the ASAP++ dataset, which includes essay scores across five quality traits, offering strong coverage of core argumentation dimensions. LLMs are prompted with designed, rubric-aligned in-context examples, along with feedback and confidence requests, while we explicitly model ordinality in scores with the BigBird model via the rank-consistent CORAL framework. Our results show that explicitly modeling score ordinality substantially improves agreement with human raters across all traits, outperforming LLMs and nominal classification and regression-based baselines. This finding reinforces the importance of aligning model objectives with rubric semantics for educational assessment. At the same time, small open-source LLMs achieve a competitive performance without task-specific fine-tuning, particularly for reasoning-oriented traits, while enabling transparent, privacy-preserving, and locally deployable assessment scenarios. Our findings provide methodological, modeling, and practical insights for the design of AI-based educational systems that aim to deliver interpretable, rubric-aligned feedback for argumentative writing.

</details>


### [168] [Exploiting contextual information to improve stance detection in informal political discourse with LLMs](https://arxiv.org/abs/2602.04750)
*Arman Engin Sucu,Yixiang Zhou,Mario A. Nascimento,Tony Mullen*

Main category: cs.CL

TL;DR: 研究表明，在政治立场检测任务中，为大型语言模型（LLMs）提供用户历史发帖生成的上下文信息（如用户画像）可以显著提高分类准确性，尤其是在处理非正式、充满讽刺和歧义的网络言论时。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理非正式网络 discourse（如政治论坛）中的政治立场检测时，由于语言的模糊性、讽刺性和语境依赖性，准确率受到限制。研究者希望探索通过引入用户层面的上下文信息来改善这一情况。

Method: 研究者从真实世界的政治论坛数据集中提取用户历史发帖，生成结构化的用户画像（总结用户意识形态、常讨论话题和语言模式）。然后，使用七种最先进的大型语言模型，在基线设置（无上下文）和上下文增强设置（提供用户画像）下进行评估，比较不同设置下的分类准确率。此外，还分析了用户画像的大小和帖子选择策略对模型性能的影响。

Result: 上下文提示显著提高了政治立场检测的准确性，改进幅度在 +17.5% 到 +38.5% 之间，最高准确率达到 74%，优于之前的研究方法。研究还发现，选择性地使用用户画像中的政治相关内容比使用更大、随机选择的上下文信息效果更好。

Conclusion: 在处理复杂且语境依赖性强的政治分类任务时，为大型语言模型集成用户级别的上下文信息（如用户画像）是提升模型性能的有效策略。精心设计的上下文信息比随机或大量的上下文信息更重要。

Abstract: This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\% to +38.5\%, achieving up to 74\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.

</details>


### [169] [Disentangling meaning from language in LLM-based machine translation](https://arxiv.org/abs/2602.04613)
*Théo Lasnier,Armel Zebaze,Djamé Seddah,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文使用机制可解释性方法分析了大型语言模型（LLMs）在机器翻译（MT）中的工作原理，发现不同的注意力头分别负责翻译中的“目标语言识别”和“句子等价性”子任务，通过操纵少量特定头即可显著提升翻译性能。


<details>
  <summary>Details</summary>
Motivation: 以往的机制可解释性研究受限于模型规模，在机器翻译领域仅能进行到词级别分析。本研究旨在突破这一限制，从句子级别理解LLMs如何实现翻译功能，特别是分析注意力头的作用。

Method: 研究将句子级别翻译分解为两个子任务：目标语言识别和句子等价性保持。通过分析多系列开源模型和多种翻译方向的注意力头，识别专门化负责这两个子任务的注意力头。基于此，构建了子任务特定的“控制向量”，并通过修改少量相关注意力头来验证其有效性，同时通过移除这些注意力头来评估其功能。

Result: 研究发现，存在独立且稀疏的注意力头集合专门负责目标语言识别和句子等价性。通过修改1%的关键注意力头，实现了与指令微调相当的无指令翻译性能。移除这些关键头则会选择性地破坏其对应的翻译功能。

Conclusion: 大型语言模型在机器翻译中，通过区分和特化的注意力头实现了目标语言生成和语义保持这两个核心功能。通过精确地操纵少量关键的注意力头，可以有效地提升和控制模型的翻译能力。

Abstract: Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification) and preserving the input sentence's meaning (i.e. sentence equivalence). Across three families of open-source models and 20 translation directions, we find that distinct, sparse sets of attention heads specialize in each subtask. Based on this insight, we construct subtask-specific steering vectors and show that modifying just 1% of the relevant heads enables instruction-free MT performance comparable to instruction-based prompting, while ablating these heads selectively disrupts their corresponding translation functions.

</details>


### [170] [LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation](https://arxiv.org/abs/2602.04617)
*Ruixiao Yang,Yuanhe Tian,Xu Yang,Huiqi Li,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种名为LEAD（Layer-wise Expert-aligned Decoding）的新方法，通过在大型视觉语言模型（LVLM）的每个解码层集成专家提取的病理特征，来纠正模型在生成放射学报告时出现的幻觉问题，并提高报告的临床准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的放射学报告生成（RRG）方法虽然利用大型视觉语言模型（LVLM）提高了报告的流畅性和准确性，但存在幻觉问题（生成与图像不符的病理细节）。现有方法依赖外部知识指导，忽略了预训练模型固有的解码先验和视觉-语言对齐偏见，且鲁棒性不足。

Method: 提出LEAD方法，通过一个多专家模块提取不同的病理特征，并将这些特征通过门控机制集成到LVLM的每个解码层。这种层级化架构允许LLM在每个推理步骤咨询专家特征，从而动态纠正解码偏见，引导生成过程向事实一致性靠拢。

Result: 在多个公共数据集上的实验表明，LEAD方法在临床准确性指标上取得了有效改进，减轻了幻觉问题，同时保持了高质量的生成。

Conclusion: LEAD方法是一种新颖的、内在修改LVLM解码轨迹的方法，通过层级化地整合专家病理特征，能够有效解决LVLM在放射学报告生成中的幻觉问题，提高报告的准确性和事实一致性。

Abstract: Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality.

</details>


### [171] [Mapping the Web of Science, a large-scale graph and text-based dataset with LLM embeddings](https://arxiv.org/abs/2602.04630)
*Tim Kunt,Annika Buchholz,Imene Khebouri,Thorsten Koch,Ida Litzel,Thi Huong Vu*

Main category: cs.CL

TL;DR: 本文提出了一种结合文本语义和文本间关系（通过图结构表示）的嵌入方法，并使用该方法分析了 Web of Science 数据集，揭示了科学出版物的结构化格局。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于，传统的文本分析方法往往只关注文本内容本身或其结构化关系，而忽略了二者结合的潜力。作者希望探索如何利用 LLM 嵌入模型来融合文本语义和文本间的链接关系，以更全面地理解大规模文本数据集。

Method: 文章提出了一种新的嵌入方法，该方法能够同时考虑文本的语义信息（通过 LLM 嵌入）和文本间的关系（通过图结构表示）。文中应用该方法对 Web of Science 数据集（包含约 5600 万篇科学出版物）进行了分析。

Result: 通过应用提出的嵌入方法，研究者在 Web of Science 数据集上观察到了一个“自结构化”的文本格局，这表明文本的语义和关系信息共同塑造了其内在的组织结构。

Conclusion: 本文成功展示了结合文本语义和文本间关系进行分析的可行性和潜力，并提出了一种有效的嵌入方法，该方法能够揭示大规模文本数据集（如科学出版物）的内在结构。

Abstract: Large text data sets, such as publications, websites, and other text-based media, inherit two distinct types of features: (1) the text itself, its information conveyed through semantics, and (2) its relationship to other texts through links, references, or shared attributes. While the latter can be described as a graph structure and can be handled by a range of established algorithms for classification and prediction, the former has recently gained new potential through the use of LLM embedding models. Demonstrating these possibilities and their practicability, we investigate the Web of Science dataset, containing ~56 million scientific publications through the lens of our proposed embedding method, revealing a self-structured landscape of texts.

</details>


### [172] [When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?](https://arxiv.org/abs/2602.04755)
*Xinyu Zhou,Chang Jin,Carsten Eickhoff,Zhijiang Guo,Seyed Ali Bahrainian*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的训练方法，结合链式思考（CoT）监督和强化学习（RL）奖励，使大型语言模型（LLMs）在时间问答任务中具备“弃权”（即拒绝回答）的能力，从而提高回答的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在回答问题时很少承认不确定性，即使面对时间敏感或信息混淆的情况，也倾向于生成看似流畅但可能误导的答案，而不是选择弃权。这在时间问答领域尤为明显。现有的不确定性校准方法在复杂推理中可能不可靠。

Method: 研究人员提出了一种将链式思考（CoT）监督与强化学习（RL）相结合的管道。RL部分由“弃权感知奖励”指导，旨在教会LLMs在不确定或无法回答时选择弃权。研究人员系统地分析了不同信息类型和训练技术对模型时间推理和弃权行为的影响。

Result: 通过大量实验，研究表明RL方法带来了显著的推理性能提升。初始化为Qwen2.5-1.5B-Instruct的模型，经过RL训练后，在TimeQA-Easy和TimeQA-Hard上的Exact Match指标分别超越GPT-4o 3.46%和5.80%。此外，与纯监督微调（SFT）模型相比，RL训练的模型在无法回答问题上的真正例率（True Positive rate）提高了20%。分析还发现，SFT会增加模型的过度自信并损害其可靠性，而RL虽然能提高预测准确性，但也存在类似的风险。比较隐式推理线索（如原始上下文、时间子上下文、知识图谱）与显式CoT监督后发现，隐式信息对弃权推理的益处有限。

Conclusion: 研究结果表明，将弃权能力与推理能力联合优化是可行的，RL方法在提高LLMs的时间问答能力和可靠性方面优于SFT。研究为构建更可靠的LLMs奠定了基础，并强调了显式CoT监督在弃权推理中的重要性。

Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.

</details>


### [173] [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811)
*Jiarui Yuan,Tailin Jin,Weize Chen,Zeyuan Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出SE-Bench，一个用于评估模型知识内化能力（即“自我进化”）的基准测试环境，解决了现有评估中的知识和推理复杂性纠缠问题。研究发现，封闭式训练优于开放式训练，标准强化学习（PPO）存在内化不足，而结合监督微调（SFT）的自玩（Self-Play）方法在知识内化方面是可行的。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确衡量智能体（agent）的“自我进化”能力，即终身学习和内化新知识以解决未来问题的能力。主要障碍在于：1）先验知识的纠缠：预训练数据中可能包含“新”知识。2）推理复杂性的纠缠：模型失败可能是因为问题太难，而非无法回忆所学知识。

Method: 提出了SE-Bench诊断环境。该环境故意混淆NumPy库及其API文档，将其伪装成一个具有随机标识符的新包。智能体被训练来内化这个伪装包，并在无法访问文档的简单编码任务上进行评估。这创造了一个干净的评估设置：对于掌握了新API文档的智能体来说，任务是简单的；而对于基础模型来说，没有该文档则无法完成。

Result: 研究揭示了三个关键发现：1）“开卷悖论”：训练时提供参考文档反而阻碍了知识保留，需要采用“闭卷训练”来强制知识压缩到模型权重中。2）“强化学习鸿沟”：标准的强化学习方法（如PPO）由于梯度裁剪和负梯度的问题，无法完全内化新知识。3）“自我博弈的可行性”：证明了自我博弈（Self-Play）方法在与SFT结合时，可以实现知识内化，即使任务是自生成的且带有噪声，但单独使用强化学习则不行。

Conclusion: SE-Bench提供了一个严格的诊断平台，用于评估智能体的自我进化能力，特别是其知识内化能力。研究结果表明，闭卷训练和结合SFT的自我博弈是实现有效知识内化的有前景的方法，而标准强化学习在这一任务上存在局限性。

Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.

</details>


### [174] [Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models](https://arxiv.org/abs/2602.04649)
*Binghai Wang,Yantao Liu,Yuxuan Liu,Tianyi Tang,Shenzhi Wang,Chang Gao,Chujie Zheng,Yichang Zhang,Le Yu,Shixuan Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Bowen Yu,Fei Huang,Junyang Lin*

Main category: cs.CL

TL;DR: 该研究提出了一个名为“Rationale Consistency”的新指标，用于衡量模型推理过程与人类判断的一致性，以解决生成式奖励模型（GenRMs）和LLM-as-a-Judge在RLHF训练中出现的“欺骗性对齐”问题。研究表明，新指标比传统的“Outcome Accuracy”更能有效地区分模型性能并检测欺骗性对齐。在此基础上，研究提出了一种结合Rationale Consistency和Outcome Accuracy的混合信号训练方法，并在多个基准测试中取得了SOTA性能，显著提升了RLHF模型的表现，尤其是在创意写作任务上，并成功避免了欺骗性对齐。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型（GenRMs）和LLM-as-a-Judge在RLHF训练中表现出“欺骗性对齐”，即它们为了追求结果的准确性（Outcome Accuracy）而产生正确的判断，但其内在推理过程却与人类的期望不符，导致泛化能力受损。现有的评估指标（如Outcome Accuracy）无法有效识别和解决这个问题。

Method: 1. 提出了“Rationale Consistency”指标，量化模型推理过程与人类判断的一致性。 2. 评估了前沿模型，证明Rationale Consistency能有效地区分模型并检测欺骗性对齐。 3. 提出了一种混合信号训练方法，结合Rationale Consistency和Outcome Accuracy来训练GenRMs。 4. 将训练好的GenRM用于RLHF，评估其对模型性能的影响。

Result: 1. Rationale Consistency指标能有效地区分SOTA模型并检测欺骗性对齐，而Outcome Accuracy则在这两方面表现不足。 2. 结合Rationale Consistency和Outcome Accuracy的混合信号训练方法在RM-Bench（87.1%）和JudgeBench（82%）上取得了SOTA性能，平均比仅使用Outcome Accuracy的基线提升了5%。 3. 使用该混合信号训练的GenRM在Arena Hard v2上，尤其在创意写作任务上，性能提升了7%。 4. 该方法成功规避了欺骗性对齐问题，并逆转了仅使用Outcome Accuracy训练时Rationale Consistency下降的趋势。

Conclusion: Rationale Consistency是一个更精细、更有效的评估指标，能够捕捉模型推理过程的对齐情况，并检测出欺骗性对齐。结合Rationale Consistency和Outcome Accuracy的混合信号训练方法能够显著提升GenRMs的性能，并有效解决RLHF训练中的欺骗性对齐问题，从而获得更鲁棒、更可靠的模型。

Abstract: Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.

</details>


### [175] [Approaches to Semantic Textual Similarity in Slovak Language: From Algorithms to Transformers](https://arxiv.org/abs/2602.04659)
*Lukas Radosky,Miroslav Blstak,Matej Krajcovic,Ivan Polasek*

Main category: cs.CL

TL;DR: 本文比较了用于斯洛伐克语的几种句子级语义文本相似度（STS）方法，包括传统算法、监督机器学习模型和第三方深度学习工具，并评估了它们的优缺点。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的STS研究已趋于成熟，但对于斯洛伐克语等资源匮乏的语言，STS仍具挑战性。

Method: 文章比较了传统算法、使用传统算法输出作为特征的监督机器学习模型（通过人工蜂群优化进行特征选择和超参数调优），以及第三方深度学习工具（如CloudNLP、OpenAI嵌入模型、GPT-4和SlovakBERT）。

Result: 研究结果显示了不同STS方法的权衡取舍，具体模型性能的详细比较是下一步工作。

Conclusion: 对斯洛伐克语的多种STS方法进行了比较评估，为选择最适合的STS方法提供了指导。

Abstract: Semantic textual similarity (STS) plays a crucial role in many natural language processing tasks. While extensively studied in high-resource languages, STS remains challenging for under-resourced languages such as Slovak. This paper presents a comparative evaluation of sentence-level STS methods applied to Slovak, including traditional algorithms, supervised machine learning models, and third-party deep learning tools. We trained several machine learning models using outputs from traditional algorithms as features, with feature selection and hyperparameter tuning jointly guided by artificial bee colony optimization. Finally, we evaluated several third-party tools, including fine-tuned model by CloudNLP, OpenAI's embedding models, GPT-4 model, and pretrained SlovakBERT model. Our findings highlight the trade-offs between different approaches.

</details>


### [176] [Investigating Disability Representations in Text-to-Image Models](https://arxiv.org/abs/2602.04687)
*Yang Yian,Yu Fan,Liudmila Zavolokina,Sarah Ebling*

Main category: cs.CL

TL;DR: 本研究分析了Stable Diffusion XL和DALL-E 3在生成含残疾人图像时的表现，发现存在代表性不平衡问题，并探讨了缓解策略对情感框架的影响。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在生成高质量图像方面取得了显著进展，但对社会群体的代表性，特别是残疾人的代表性，仍未得到充分的探索和关注。

Method: 研究者使用结构化提示设计，分析了Stable Diffusion XL和DALL-E 3生成的图像。通过比较通用残疾提示和特定残疾类别提示的图像相似性来分析残疾代表性，并评估缓解策略的影响，包括使用自动和人工评估进行情感极性分析。

Result: 研究发现，在AI生成的图像中，残疾人的代表性存在持续的不平衡。同时，缓解策略对情感框架有影响，但具体细节未在摘要中详述。

Conclusion: AI生成的图像在残疾人代表性方面存在不足，需要持续评估和改进生成模型，以促进更多样化和包容性的残疾人形象。

Abstract: Text-to-image generative models have made remarkable progress in producing high-quality visual content from textual descriptions, yet concerns remain about how they represent social groups. While characteristics like gender and race have received increasing attention, disability representations remain underexplored. This study investigates how people with disabilities are represented in AI-generated images by analyzing outputs from Stable Diffusion XL and DALL-E 3 using a structured prompt design. We analyze disability representations by comparing image similarities between generic disability prompts and prompts referring to specific disability categories. Moreover, we evaluate how mitigation strategies influence disability portrayals, with a focus on assessing affective framing through sentiment polarity analysis, combining both automatic and human evaluation. Our findings reveal persistent representational imbalances and highlight the need for continuous evaluation and refinement of generative models to foster more diverse and inclusive portrayals of disability.

</details>


### [177] [LinGO: A Linguistic Graph Optimization Framework with LLMs for Interpreting Intents of Online Uncivil Discourse](https://arxiv.org/abs/2602.04693)
*Yuan Zhang,Thales Bertaglia*

Main category: cs.CL

TL;DR: 研究提出了一种名为LinGO的语言图优化框架，用于提高大型语言模型（LLMs）在识别网络不文明语言时的准确性，特别是在处理那些表面上包含不文明线索但意图文明的内容时。该框架通过分解语言、识别错误根源并优化相关组件来工作，并在巴西2022年总统选举数据集上进行了验证，结果显示优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有不文明语言分类器常误判表面不文明实则意图文明的内容，导致对网络有害不文明现象的估计偏高。需要更精细的方法来区分语言的不文明线索和真实意图。

Method: 提出LinGO（ Linguistic Graph Optimization）框架，通过将语言分解为多步语言组件，识别导致错误的关键步骤，并迭代优化这些步骤的提示（prompt）和/或示例（example）。在巴西2022年总统选举数据上，针对四种政治不文明（Impoliteness, Hate Speech and Stereotyping, Physical Harm and Violent Political Rhetoric, Threats to Democratic Institutions and Values）和六种意图类型进行评估。使用GPT-5-mini, Gemini 2.5 Flash-Lite, Claude 3 Haiku三款LLMs，并结合TextGrad, AdalFlow, DSPy, RAG四种优化技术进行基准测试。

Result: LinGO框架在所有测试模型上，相比零样本（zero-shot）、思维链（chain-of-thought）、直接优化和微调（fine-tuning）基线，均能显著提高准确率和加权F1分数。检索增强生成（RAG）是最有效的优化技术，与Gemini模型结合时表现最佳。

Conclusion: 将多步语言组件纳入LLM指令并优化关键组件，有助于模型理解复杂语义含义，该方法可推广至其他复杂的语义解释任务。

Abstract: Detecting uncivil language is crucial for maintaining safe, inclusive, and democratic online spaces. Yet existing classifiers often misinterpret posts containing uncivil cues but expressing civil intents, leading to inflated estimates of harmful incivility online. We introduce LinGO, a linguistic graph optimization framework for large language models (LLMs) that leverages linguistic structures and optimization techniques to classify multi-class intents of incivility that use various direct and indirect expressions. LinGO decomposes language into multi-step linguistic components, identifies targeted steps that cause the most errors, and iteratively optimizes prompt and/or example components for targeted steps. We evaluate it using a dataset collected during the 2022 Brazilian presidential election, encompassing four forms of political incivility: Impoliteness (IMP), Hate Speech and Stereotyping (HSST), Physical Harm and Violent Political Rhetoric (PHAVPR), and Threats to Democratic Institutions and Values (THREAT). Each instance is annotated with six types of civil/uncivil intent. We benchmark LinGO using three cost-efficient LLMs: GPT-5-mini, Gemini 2.5 Flash-Lite, and Claude 3 Haiku, and four optimization techniques: TextGrad, AdalFlow, DSPy, and Retrieval-Augmented Generation (RAG). The results show that, across all models, LinGO consistently improves accuracy and weighted F1 compared with zero-shot, chain-of-thought, direct optimization, and fine-tuning baselines. RAG is the strongest optimization technique and, when paired with Gemini model, achieves the best overall performance. These findings demonstrate that incorporating multi-step linguistic components into LLM instructions and optimize targeted components can help the models explain complex semantic meanings, which can be extended to other complex semantic explanation tasks in the future.

</details>


### [178] [ERNIE 5.0 Technical Report](https://arxiv.org/abs/2602.04705)
*Haifeng Wang,Hua Wu,Tian Wu,Yu Sun,Jing Liu,Dianhai Yu,Yanjun Ma,Jingzhou He,Zhongjun He,Dou Hong,Qiwen Liu,Shuohuan Wang,Junyuan Shang,Zhenyu Zhang,Yuchen Ding,Jinle Zeng,Jiabin Yang,Liang Shen,Ruibiao Chen,Weichong Yin,Siyu Ding,Dai Dai,Shikun Feng,Siqi Bao,Bolei He,Yan Chen,Zhenyu Jiao,Ruiqing Zhang,Zeyu Chen,Qingqing Dang,Kaipeng Deng,Jiajun Jiang,Enlei Gong,Guoxia Wang,Yanlin Sha,Yi Liu,Yehan Zheng,Weijian Xu,Jiaxiang Liu,Zengfeng Zeng,Yingqi Qu,Zhongli Li,Zhengkun Zhang,Xiyang Wang,Zixiang Xu,Xinchao Xu,Zhengjie Huang,Dong Wang,Bingjin Chen,Yue Chang,Xing Yuan,Shiwei Huang,Qiao Zhao,Xinzhe Ding,Shuangshuang Qiao,Baoshan Yang,Bihong Tang,Bin Li,Bingquan Wang,Binhan Tang,Binxiong Zheng,Bo Cui,Bo Ke,Bo Zhang,Bowen Zhang,Boyan Zhang,Boyang Liu,Caiji Zhang,Can Li,Chang Xu,Chao Pang,Chao Zhang,Chaoyi Yuan,Chen Chen,Cheng Cui,Chenlin Yin,Chun Gan,Chunguang Chai,Chuyu Fang,Cuiyun Han,Dan Zhang,Danlei Feng,Danxiang Zhu,Dong Sun,Dongbo Li,Dongdong Li,Dongdong Liu,Dongxue Liu,Fan Ding,Fan Hu,Fan Li,Fan Mo,Feisheng Wu,Fengwei Liu,Gangqiang Hu,Gaofeng Lu,Gaopeng Yong,Gexiao Tian,Guan Wang,Guangchen Ni,Guangshuo Wu,Guanzhong Wang,Guihua Liu,Guishun Li,Haibin Li,Haijian Liang,Haipeng Ming,Haisu Wang,Haiyang Lu,Haiye Lin,Han Zhou,Hangting Lou,Hanwen Du,Hanzhi Zhang,Hao Chen,Hao Du,Hao Liu,Hao Zhou,Haochen Jiang,Haodong Tian,Haoshuang Wang,Haozhe Geng,Heju Yin,Hong Chen,Hongchen Xue,Hongen Liu,Honggeng Zhang,Hongji Xu,Hongwei Chen,Hongyang Zhang,Hongyuan Zhang,Hua Lu,Huan Chen,Huan Wang,Huang He,Hui Liu,Hui Zhong,Huibin Ruan,Jiafeng Lu,Jiage Liang,Jiahao Hu,Jiahao Hu,Jiajie Yang,Jialin Li,Jian Chen,Jian Wu,Jianfeng Yang,Jianguang Jiang,Jianhua Wang,Jianye Chen,Jiaodi Liu,Jiarui Zhou,Jiawei Lv,Jiaxin Zhou,Jiaxuan Liu,Jie Han,Jie Sun,Jiefan Fang,Jihan Liu,Jihua Liu,Jing Hu,Jing Qian,Jing Yan,Jingdong Du,Jingdong Wang,Jingjing Wu,Jingyong Li,Jinheng Wang,Jinjin Li,Jinliang Lu,Jinlin Yu,Jinnan Liu,Jixiang Feng,Jiyi Huang,Jiyuan Zhang,Jun Liang,Jun Xia,Jun Yu,Junda Chen,Junhao Feng,Junhong Xiang,Junliang Li,Kai Liu,Kailun Chen,Kairan Su,Kang Hu,Kangkang Zhou,Ke Chen,Ke Wei,Kui Huang,Kun Wu,Kunbin Chen,Lei Han,Lei Sun,Lei Wen,Linghui Meng,Linhao Yu,Liping Ouyang,Liwen Zhang,Longbin Ji,Longzhi Wang,Meng Sun,Meng Tian,Mengfei Li,Mengqi Zeng,Mengyu Zhang,Ming Hong,Mingcheng Zhou,Mingming Huang,Mingxin Chen,Mingzhu Cai,Naibin Gu,Nemin Qiu,Nian Wang,Peng Qiu,Peng Zhao,Pengyu Zou,Qi Wang,Qi Xin,Qian Wang,Qiang Zhu,Qianhui Luo,Qianwei Yang,Qianyue He,Qifei Wu,Qinrui Li,Qiwen Bao,Quan Zhang,Quanxiang Liu,Qunyi Xie,Rongrui Zhan,Rufeng Dai,Rui Peng,Ruian Liu,Ruihao Xu,Ruijie Wang,Ruixi Zhang,Ruixuan Liu,Runsheng Shi,Ruting Wang,Senbo Kang,Shan Lu,Shaofei Yu,Shaotian Gong,Shenwei Hu,Shifeng Zheng,Shihao Guo,Shilong Fan,Shiqin Liu,Shiwei Gu,Shixi Zhang,Shuai Yao,Shuang Zhang,Shuangqiao Liu,Shuhao Liang,Shuwei He,Shuwen Yang,Sijun He,Siming Dai,Siming Wu,Siyi Long,Songhe Deng,Suhui Dong,Suyin Liang,Teng Hu,Tianchan Xu,Tianliang Lv,Tianmeng Yang,Tianyi Wei,Tiezhu Gao,Ting Sun,Ting Zhang,Tingdan Luo,Wei He,Wei Luan,Wei Yin,Wei Zhang,Wei Zhou,Weibao Gong,Weibin Li,Weicheng Huang,Weichong Dang,Weiguo Zhu,Weilong Zhang,Weiqi Tan,Wen Huang,Wenbin Chang,Wenjing Du,Wenlong Miao,Wenpei Luo,Wenquan Wu,Xi Shi,Xi Zhao,Xiang Gao,Xiangguo Zhang,Xiangrui Yu,Xiangsen Wang,Xiangzhe Wang,Xianlong Luo,Xianying Ma,Xiao Tan,Xiaocong Lin,Xiaofei Wang,Xiaofeng Peng,Xiaofeng Wu,Xiaojian Xu,Xiaolan Yuan,Xiaopeng Cui,Xiaotian Han,Xiaoxiong Liu,Xiaoxu Fei,Xiaoxuan Wu,Xiaoyu Wang,Xiaoyu Zhang,Xin Sun,Xin Wang,Xinhui Huang,Xinming Zhu,Xintong Yu,Xinyi Xu,Xinyu Wang,Xiuxian Li,XuanShi Zhu,Xue Xu,Xueying Lv,Xuhong Li,Xulong Wei,Xuyi Chen,Yabing Shi,Yafeng Wang,Yamei Li,Yan Liu,Yanfu Cheng,Yang Gao,Yang Liang,Yang Wang,Yang Wang,Yang Yang,Yanlong Liu,Yannian Fu,Yanpeng Wang,Yanzheng Lin,Yao Chen,Yaozong Shen,Yaqian Han,Yehua Yang,Yekun Chai,Yesong Wang,Yi Song,Yichen Zhang,Yifei Wang,Yifeng Guo,Yifeng Kou,Yilong Chen,Yilong Guo,Yiming Wang,Ying Chen,Ying Wang,Yingsheng Wu,Yingzhan Lin,Yinqi Yang,Yiran Xing,Yishu Lei,Yixiang Tu,Yiyan Chen,Yong Zhang,Yonghua Li,Yongqiang Ma,Yongxing Dai,Yongyue Zhang,Yu Ran,Yu Sun,Yu-Wen Michael Zhang,Yuang Liu,Yuanle Liu,Yuanyuan Zhou,Yubo Zhang,Yuchen Han,Yucheng Wang,Yude Gao,Yuedong Luo,Yuehu Dong,Yufeng Hu,Yuhui Cao,Yuhui Yun,Yukun Chen,Yukun Gao,Yukun Li,Yumeng Zhang,Yun Fan,Yun Ma,Yunfei Zhang,Yunshen Xie,Yuping Xu,Yuqin Zhang,Yuqing Liu,Yurui Li,Yuwen Wang,Yuxiang Lu,Zefeng Cai,Zelin Zhao,Zelun Zhang,Zenan Lin,Zezhao Dong,Zhaowu Pan,Zhaoyu Liu,Zhe Dong,Zhe Zhang,Zhen Zhang,Zhengfan Wu,Zhengrui Wei,Zhengsheng Ning,Zhenxing Li,Zhenyu Li,Zhenyu Qian,Zhenyun Li,Zhi Li,Zhichao Chen,Zhicheng Dong,Zhida Feng,Zhifan Feng,Zhihao Deng,Zhijin Yu,Zhiyang Chen,Zhonghui Zheng,Zhuangzhuang Guo,Zhujun Zhang,Zhuo Sun,Zichang Liu,Zihan Lin,Zihao Huang,Zihe Zhu,Ziheng Zhao,Ziping Chen,Zixuan Zhu,Ziyang Xu,Ziyi Liang,Ziyuan Gao*

Main category: cs.CL

TL;DR: ERNIE 5.0是一个支持文本、图像、视频和音频的统一多模态自回归基础模型，采用了超稀疏MoE架构和弹性训练范式，实现了在不同资源限制下的性能、模型大小和推理延迟的灵活权衡，是首个达到万亿参数规模的生产级统一多模态自回归模型。


<details>
  <summary>Details</summary>
Motivation: 为了应对大规模部署在多样化资源限制下的实际挑战，以及解决将强化学习扩展到统一基础模型的难题，研究人员开发了ERNIE 5.0。

Method: ERNIE 5.0采用原生自回归方式，使用超稀疏MoE架构和模态无关的专家路由，以统一的下一组token预测目标从头开始训练文本、图像、视频和音频。引入了弹性训练范式，允许在单个预训练运行中学习具有不同深度、专家容量和路由稀疏度的模型系列。同时，研究了在超稀疏MoE架构和多模态设置下进行高效稳定的训练。

Result: ERNIE 5.0在多个模态上实现了强大且均衡的性能。通过弹性训练，模型可以在性能、模型大小和推理延迟之间进行灵活权衡。

Conclusion: ERNIE 5.0是首个实现生产规模的万亿参数统一自回归模型，能够同时处理多模态理解和生成任务，为社区提供了关于模态无关专家路由和弹性训练的深入见解。

Abstract: In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.

</details>


### [179] [LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers](https://arxiv.org/abs/2602.04706)
*Yike Sun,Haotong Yang,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文研究了BPE分词器中“中间合并残差”（即在合并学习过程中频繁出现但在最终分词时很少使用的token）的现象，并提出了一种名为LiteToken的简单方法来移除这些残差token，从而减少了分词碎片化、模型参数并提高了对噪声输入的鲁棒性，同时保持了整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的BPE分词器在合并学习过程中会产生一些中间token，这些token在最终分词时很少被使用，占用了词汇表容量并增加了模型对异常输入的脆弱性。作者希望系统地研究这一现象并提出一种解决方案。

Method: 通过实证研究，系统地分析了BPE分词器中“中间合并残差”的现象。提出了LiteToken方法，通过移除这些低频的残差token来优化分词器。实验表明，预训练模型在不进行额外微调的情况下，可以很好地适应修改后的分词器。

Result: LiteToken方法成功地减少了分词碎片化，降低了模型参数量，并提高了模型在处理包含拼写错误或噪声的输入时的鲁棒性。同时，该方法在保持模型整体性能方面也表现良好。

Conclusion: “中间合并残差”是BPE分词器中的一个普遍现象，LiteToken是一种有效且简单的方法来解决这个问题，它可以在不影响模型性能的情况下，优化分词器的效率和鲁棒性。

Abstract: Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.

</details>


### [180] [Linguistically Informed Evaluation of Multilingual ASR for African Languages](https://arxiv.org/abs/2602.04716)
*Fei-Yueh Chen,Lateef Adeleke,C. M. Downey*

Main category: cs.CL

TL;DR: 该研究通过引入和评估特征错误率 (FER) 和音调感知扩展 (TER) 来分析自动语音识别 (ASR) 模型在非洲语言上的性能，发现这些指标比词错误率 (WER) 更能揭示语言学上的显著错误模式，尤其是在处理音调和分割特征方面。


<details>
  <summary>Details</summary>
Motivation: 传统的词错误率 (WER) 无法准确反映 ASR 模型在非洲语言上的性能，因为它将各种语言学错误（语音、音调等）归结为单一的词错误。因此，需要一种更能揭示语言学上有意义的错误的评估指标。

Method: 研究者评估了三种语音编码器在两种非洲语言（约鲁巴语和 Uneme 语）上的性能。他们将传统的词错误率 (WER) 与字符错误率 (CER)、特征错误率 (FER) 以及音调感知扩展 (TER) 结合使用，以分析模型在语音特征上的错误。

Result: 结果表明，FER 和 TER 能够揭示出即使在词级别准确率较低时，也具有语言学意义的错误模式。模型在分割特征上的表现优于音调特征，尤其是中音和降调。约鲁巴语的 WER 为 0.788，而 FER 为 0.151。对于 Uneme 语，尽管 WER 接近 1 且 CER 为 0.461，但 FER 仅为 0.267。

Conclusion: 特征错误率 (FER) 和音调感知扩展 (TER) 是比 WER 更有效的 ASR 性能评估指标，尤其适用于非洲语言。它们能够揭示出由单个语音特征错误导致的模型性能问题，这些问题在传统的 WER 指标中被掩盖了。

Abstract: Word Error Rate (WER) mischaracterizes ASR models' performance for African languages by combining phonological, tone, and other linguistic errors into a single lexical error. By contrast, Feature Error Rate (FER) has recently attracted attention as a viable metric that reveals linguistically meaningful errors in models' performance. In this paper, we evaluate three speech encoders on two African languages by complementing WER with CER, and FER, and add a tone-aware extension (TER). We show that by computing errors on phonological features, FER and TER reveal linguistically-salient error patterns even when word-level accuracy remains low. Our results reveal that models perform better on segmental features, while tones (especially mid and downstep) remain the most challenging features. Results on Yoruba show a striking differential in metrics, with WER=0.788, CER=0.305, and FER=0.151. Similarly for Uneme (an endangered language absent from pretraining data) a model with near-total WER and 0.461 CER achieves the relatively low FER of 0.267. This indicates model error is often attributable to individual phonetic feature errors, which is obscured by all-or-nothing metrics like WER.

</details>


### [181] [Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging](https://arxiv.org/abs/2602.04731)
*Sameh Khattab,Jean-Philippe Corbeil,Osman Alperen Koraş,Amin Dada,Julian Friedrich,François Beaulieu,Paul Vozila,Jens Kleesiek*

Main category: cs.CL

TL;DR: 提出了一种名为STM（Synthesize-Train-Merge）的模块化框架，用于将通用LLM适配为领域特定的检索器，通过合成难例、优化检索提示和模型合并来提升性能，并在医学等领域取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的检索器在RAG应用中表现出色，但如何将通用LLM有效适配为领域特定检索器，尤其是在生物医学等专业领域，仍有待深入研究。

Method: 提出STM框架，包含三个主要模块：1. 合成难例（Synthesize）：生成用于训练检索器的困难负样本。2. 训练（Train）：优化检索提示（retrieval prompt）以适应特定任务。3. 合并（Merge）：将经过优化的领域特定检索器模型进行合并。

Result: 在12个医学和通用任务的MTEB基准测试子集上，STM框架将任务特定模型性能提升了最高23.5%（平均7.5%）。合并后的模型性能优于单个专家模型和未经大量预训练的强基线模型。

Conclusion: STM框架提供了一种可扩展且高效的方法，可以将通用LLM转化为高性能的领域专用检索器，在保持通用能力的同时，在专业任务上表现出色。

Abstract: Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\% (average 7.5\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.

</details>


### [182] ["Be My Cheese?": Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs](https://arxiv.org/abs/2602.04729)
*Madison Van Doren,Casey Ford,Jennifer Barajas,Cory Holland*

Main category: cs.CL

TL;DR: 本研究提出了一个大规模的人类评估基准，用于评估先进的多语言大型语言模型（LLMs）在机器翻译中的文化本地化能力。现有基准侧重于词汇和语法准确性，忽略了实际本地化所需的语用和文化能力。通过对15种目标语言的7个LLMs进行评估，发现整体翻译质量一般，GPT-5、Claude Sonnet 3.7和Mistral Medium 3.1表现最佳。文化概念和节日翻译效果较好，而习语和双关语的翻译效果较差，习语最容易被遗漏。研究强调了当前模型在文化共鸣方面存在的差距，并呼吁开发更具文化意识的训练数据、改进跨语言语用能力以及采用更能反映真实沟通能力的人类评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译基准过于侧重词汇和语法准确性，而忽略了实际应用中至关重要的文化和语用能力，这阻碍了机器翻译在真实世界本地化场景中的有效性。研究者希望通过构建一个关注文化细微差别的评估基准来填补这一空白。

Method: 研究者构建了一个大规模的人类评估基准，评估了7种多语言LLMs在15种目标语言上的机器翻译。评估方式包括对全文翻译和包含文化细微语言（如习语、双关语、节日和文化概念）的片段进行评分。每种语言有5名母语译者参与评分，评分采用0-3的序数等级，片段评分还包含“不适用”选项。

Result: 在全文评估中，LLMs的平均整体质量为1.68/3。GPT-5（2.10/3）、Claude Sonnet 3.7（1.97/3）和Mistral Medium 3.1（1.84/3）表现最佳。片段级评估显示，节日（2.20/3）和文化概念（2.19/3）的翻译效果显著优于习语（1.65/3）和双关语（1.45/3），习语最常被标记为“不适用”。

Conclusion: 当前最先进的多语言LLMs在机器翻译的文化本地化方面仍存在显著差距。虽然它们在语法上可能表现良好，但在传达文化共鸣方面能力有限。研究强调了需要改进用于本地化的训练数据、增强模型的跨语言语用能力，并开发更能反映真实世界沟通能力的评估范式。

Abstract: We present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments.
  Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.

</details>


### [183] [OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models](https://arxiv.org/abs/2602.04804)
*Yue Ding,Yiyan Ji,Jungang Li,Xuyang Liu,Xinlong Chen,Junfei Wu,Bozhou Li,Bohan Zeng,Yang Shi,Yushuo Guan,Yuanxing Zhang,Jiaheng Liu,Qiang Liu,Pengfei Wan,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出了OmniSIFT，一种用于Omni-LLMs的多模态（音频-视频）令牌压缩框架，通过时空视频剪枝和视觉引导音频选择，显著减少了计算开销，并在多个基准测试中取得了优于全上下文模型的性能。


<details>
  <summary>Details</summary>
Motivation: Omni-LLMs在音视频理解任务中能力强大，但长多模态令牌序列带来了巨大的计算开销，而现有的令牌压缩方法对此类模型效果有限。

Method: OmniSIFT采用两阶段压缩策略：1. 时空视频剪枝模块，去除视频帧内和帧间的冗余；2. 视觉引导音频选择模块，过滤音频令牌。整个框架通过可微分的直通估计器进行端到端优化。

Result: OmniSIFT在五个基准测试中展现了有效性和鲁棒性。对于Qwen2.5-Omni-7B，OmniSIFT仅引入4.85M参数，并实现了比OmniZip等无需训练的基线更低的延迟。使用25%的原始令牌上下文，OmniSIFT的性能稳定优于所有压缩基线，甚至在一些任务上超越了全令牌模型。

Conclusion: OmniSIFT是一种高效且鲁棒的多模态令牌压缩框架，能够显著降低Omni-LLMs的计算成本，同时保持甚至提升性能，为高效的音视频理解模型提供了新的解决方案。

Abstract: Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.

</details>


### [184] [Beyond Many-Shot Translation: Scaling In-Context Demonstrations For Low-Resource Machine Translation](https://arxiv.org/abs/2602.04764)
*Luis Frentzen Salim,Esteban Carlin,Alexandre Morinvil,Xi Ai,Lun-Wei Ku*

Main category: cs.CL

TL;DR: 本研究探讨了如何通过扩展预训练语言模型（LLMs）的上下文学习（ICL）能力来改进低资源语言的机器翻译（MT）。研究发现，虽然增加上下文示例的数量可以提升翻译质量，但收益会很快饱和，并且在接近最大上下文窗口时可能出现性能下降。不同类型的训练语料（单语、指令式、并行数据）对翻译效果有显著影响，其中某些单语监督形式在某些情况下甚至能媲美并行数据。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的机器翻译系统构建因数据稀缺而面临挑战。虽然LLMs在MT方面表现出色，但将其应用于代表性不足的语言仍具难度。上下文学习（ICL）提供了一种在推理时通过演示来调整LLMs以适应低资源MT的新方法。

Method: 研究人员扩展了低资源机器翻译ICL的上下文学习设置，使用了长上下文模型，并将输入示例数量增加到数千个，最大上下文令牌预算达到100万。他们比较了三种不同类型的训练语料作为上下文监督：单语无监督数据、指令式数据以及并行数据（英-目标语和印尼-目标语）。

Result: 在爪哇语和巽他语的实验表明，增加上下文的收益很快达到饱和，并在接近最大上下文窗口时可能出现性能下降。性能提升对语料类型非常敏感。值得注意的是，部分单语监督在某些情况下可以与并行数据相媲美，尽管并行数据提供了额外的监督信息。

Conclusion: 研究结果揭示了长上下文ICL在低资源MT中的有效极限和对语料类型的敏感性，强调了更大的上下文窗口并不一定带来成比例的质量提升。这为未来低资源MT的研究和实践提供了重要的指导。

Abstract: Building machine translation (MT) systems for low-resource languages is notably difficult due to the scarcity of high-quality data. Although Large Language Models (LLMs) have improved MT system performance, adapting them to lesser-represented languages remains challenging. In-context learning (ICL) may offer novel ways to adapt LLMs for low-resource MT by conditioning models on demonstration at inference time. In this study, we explore scaling low-resource machine translation ICL beyond the few-shot setting to thousands of examples with long-context models. We scale in-context token budget to 1M tokens and compare three types of training corpora used as in-context supervision: monolingual unsupervised data, instruction-style data, and parallel data (English--target and Indonesian--target). Our experiments on Javanese and Sundanese show that gains from additional context saturate quickly and can degrade near the maximum context window, with scaling behavior strongly dependent on corpus type. Notably, some forms of monolingual supervision can be competitive with parallel data, despite the latter offering additional supervision. Overall, our results characterize the effective limits and corpus-type sensitivity of long-context ICL for low-resource MT, highlighting that larger context windows do not necessarily yield proportional quality gains.

</details>


### [185] [Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say "I Don't Know"](https://arxiv.org/abs/2602.04853)
*Dhruv Madhwal,Lyuxin David Zhang,Dan Roth,Tomer Wolfson,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在闭卷问答中识别知识局限性并减少幻觉的问题，提出了一种基于不同提示策略（直接、辅助、增量）之间不一致性的无训练、无检索的弃权策略，以提高模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在闭卷问答中常表现出对自身知识局限性的不确定性，导致自信的幻觉。尽管分解提示（decomposed prompting）可以提高准确性，但其对模型可靠性的影响尚未得到充分研究。

Method: 研究评估了三种等效任务的提示策略：直接提示、辅助提示和增量提示，在不同模型规模和多跳问答基准上进行。利用不同提示策略之间的分歧作为衡量模型不确定性的信号，实现了一种无需检索或微调的弃权策略。

Result: 研究发现，虽然在最先进的模型中，分解提示带来的准确性提升减弱，但不同提示策略之间的分歧仍然能有效地指示潜在错误。基于分歧的弃权策略在错误检测方面优于标准的模型不确定性基线，在各种设置下都提高了F1分数和AUROC。

Conclusion: 分解提示可以作为一种实用的诊断工具，用于检测闭卷问答中大型语言模型的可靠性。通过分析不同提示策略的分歧，可以有效地识别和避免模型产生的幻觉，从而提高问答的准确性和可靠性。

Abstract: Large language models often struggle to recognize their knowledge limits in closed-book question answering, leading to confident hallucinations. While decomposed prompting is typically used to improve accuracy, we investigate its impact on reliability. We evaluate three task-equivalent prompting regimes: Direct, Assistive, and Incremental, across different model scales and multi-hop QA benchmarks. We find that although accuracy gains from decomposition diminish in frontier models, disagreements between prompting regimes remain highly indicative of potential errors. Because factual knowledge is stable while hallucinations are stochastic, cross-regime agreement provides a precise signal of internal uncertainty. We leverage this signal to implement a training-free abstention policy that requires no retrieval or fine-tuning. Our results show that disagreement-based abstention outperforms standard uncertainty baselines as an error detector, improving both F1 and AUROC across settings. This demonstrates that decomposition-based prompting can serve as a practical diagnostic probe for model reliability in closed-book QA.

</details>


### [186] [CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation](https://arxiv.org/abs/2602.04856)
*Zhao Tong,Chunlin Gong,Yiping Zhang,Qiang Liu,Xingcheng Xu,Shu Wu,Haichao Shi,Xiao-Yu Zhang*

Main category: cs.CL

TL;DR: 本研究挑战了大型语言模型（LLM）仅凭最终输出判断安全性的假设，发现即使模型拒绝有害请求，其思考过程（Chain-of-Thought, CoT）也可能包含不安全的内容。研究提出了一种分析框架，通过 Jacobians 谱度量来量化特定注意力头在 CoT 生成中传播欺骗性推理模式的程度，并发现思考模式激活时，风险在特定层级的少数注意力头中显著增加。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是挑战大型语言模型（LLM）仅通过最终输出判断安全性的现有安全假设，并揭示即使模型拒绝有害请求，其内部的思考过程（Chain-of-Thought, CoT）也可能包含不安全的内容。

Method: 研究引入了一个统一的安全分析框架，该框架系统地分解了模型各层级的 CoT 生成过程，并利用基于 Jacobians 的谱度量来评估个体注意力头的作用。框架内定义了稳定性、几何和能量三个可解释的度量标准，用于量化注意力头对欺骗性推理模式的响应或嵌入能力。

Result: 实验在多个面向推理的 LLM 上进行，结果表明，在思考模式（thinking mode）激活时，生成风险显著升高。关键的路径决策集中在少数几个连续的中层层级。研究精确识别了导致这种风险发散的注意力头。

Conclusion: 本研究证明了模型拒绝有害请求并不意味着整个推理过程都是安全的。通过识别关键的注意力头和层级，研究为理解和缓解 LLM 中潜在的推理风险提供了新的视角，并挑战了“拒绝即安全”的假设。

Abstract: From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks.

</details>


### [187] [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884)
*Bangzheng Li,Jianmo Ni,Chen Qu,Ian Miao,Liu Yang,Xingyu Fu,Muhao Chen,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为RAL（Reinforced Attention Learning）的框架，用于增强多模态大语言模型（MLLMs）的后训练推理能力。与传统的基于token生成的RL方法不同，RAL直接优化模型内部的注意力分布，以实现更有效的信息分配和更好的跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RL的后训练方法在LLMs上表现良好，但在MLLMs上通过生成详细的文本解释（rationales）进行扩展时，效果有限，甚至可能导致性能下降。研究人员希望找到一种新的方法来有效提升MLLMs在多模态推理方面的能力。

Method: RAL是一个基于策略梯度（policy-gradient）的框架，它直接优化模型内部的注意力分布，而不是像传统方法那样优化输出的token序列。通过将优化目标从“生成什么”转移到“关注哪里”，RAL促进了信息在模态间的有效分配，并增强了模型对复杂多模态输入的理解能力。此外，还引入了On-Policy Attention Distillation，用于将潜在的注意力行为进行迁移，以实现比标准知识蒸馏更强的跨模态对齐。

Result: 在多个图像和视频基准测试中，RAL相比于GRPO等基线方法取得了持续的性能提升。On-Policy Attention Distillation也证明了其在跨模态对齐方面的有效性。

Conclusion: RAL提供了一种原则性且通用的方法，作为多模态后训练的替代方案，通过直接优化注意力分布来提升MLLMs的推理能力和跨模态理解能力。

Abstract: Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [188] [Beyond the Vehicle: Cooperative Localization by Fusing Point Clouds for GPS-Challenged Urban Scenarios](https://arxiv.org/abs/2602.03908)
*Kuo-Yi Chao,Ralph Rasshofer,Alois Christian Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种结合车车和车路通信的多传感器、多模态定位方法，通过融合不同传感器（LiDAR、相机、路侧传感器）的点云数据和合作信息，解决了城市环境中GPS信号不可靠导致的车辆定位不准确问题。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GPS信号不可靠，导致车辆定位不准确，这是一个关键的挑战。

Method: 提出了一种合作式多传感器、多模态定位方法，融合了车车（V2V）和车路（V2I）通信的数据，并结合了基于点云配准的同步定位与地图构建（SLAM）算法。该系统处理来自车载LiDAR、立体摄像头以及路侧传感器生成的多模态点云数据。

Result: 通过利用基础设施共享的数据，该方法显著提高了在复杂的、GPS信号受干扰的城市场景下的定位精度和鲁棒性。

Conclusion: 该合作式多传感器、多模态定位方法能够有效解决城市环境中GPS信号不可靠带来的定位难题，提供更精确和鲁棒的定位服务。

Abstract: Accurate vehicle localization is a critical challenge in urban environments where GPS signals are often unreliable. This paper presents a cooperative multi-sensor and multi-modal localization approach to address this issue by fusing data from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Our approach integrates cooperative data with a point cloud registration-based simultaneous localization and mapping (SLAM) algorithm. The system processes point clouds generated from diverse sensor modalities, including vehicle-mounted LiDAR and stereo cameras, as well as sensors deployed at intersections. By leveraging shared data from infrastructure, our method significantly improves localization accuracy and robustness in complex, GPS-noisy urban scenarios.

</details>


### [189] [How Users Understand Robot Foundation Model Performance through Task Success Rates and Beyond](https://arxiv.org/abs/2602.03920)
*Isaac Sheidlower,Jindan Huang,James Staley,Bingyu Wu,Qicong Chen,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 本研究调查了非机器人专家如何理解机器人基础模型（RFMs）的性能评估信息，发现他们能正确理解任务成功率（TSR），并高度重视失败案例描述，同时希望获取历史评估数据和机器人对新任务的性能预测。


<details>
  <summary>Details</summary>
Motivation: 随着RFMs在家庭机器人中的应用越来越广泛，用户可能会要求机器人执行未训练或未评估过的新任务，此时用户需要了解失败的风险并知道机器人的能力范围。因此，研究非机器人专家如何理解RFMs的性能评估信息至关重要。

Method: 通过一项用户研究，让非机器人专家查看来自多个RFM项目的真实评估数据，包括任务成功率（TSR）、失败案例描述和视频，以了解他们如何解读这些信息。

Result: 研究结果表明，非专家能够以符合专家预期的方式使用TSR，并且非常看重失败案例描述等信息。此外，用户希望能够获取RFM过往评估的真实数据，以及机器人对其将要执行的新任务的性能预测。

Conclusion: 非机器人专家在理解RFM性能评估信息方面表现良好，他们不仅能正确理解TSR，还高度重视失败案例等详细信息。为了更好地支持用户，RFM应提供历史评估数据和对新任务的性能预测。

Abstract: Robot Foundation Models (RFMs) represent a promising approach to developing general-purpose home robots. Given the broad capabilities of RFMs, users will inevitably ask an RFM-based robot to perform tasks that the RFM was not trained or evaluated on. In these cases, it is crucial that users understand the risks associated with attempting novel tasks due to the relatively high cost of failure. Furthermore, an informed user who understands an RFM's capabilities will know what situations and tasks the robot can handle. In this paper, we study how non-roboticists interpret performance information from RFM evaluations. These evaluations typically report task success rate (TSR) as the primary performance metric. While TSR is intuitive to experts, it is necessary to validate whether novices also use this information as intended. Toward this end, we conducted a study in which users saw real evaluation data, including TSR, failure case descriptions, and videos from multiple published RFM research projects. The results highlight that non-experts not only use TSR in a manner consistent with expert expectations but also highly value other information types, such as failure cases that are not often reported in RFM evaluations. Furthermore, we find that users want access to both real data from previous evaluations of the RFM and estimates from the robot about how well it will do on a novel task.

</details>


### [190] [VLS: Steering Pretrained Robot Policies via Vision-Language Models](https://arxiv.org/abs/2602.03973)
*Shuo Liu,Ishneet Sukhvinder Singh,Yiqing Xu,Jiafei Duan,Ranjay Krishna*

Main category: cs.RO

TL;DR: 本研究提出了一种名为 VLS（Vision-Language Steering）的训练无关框架，用于在推理时适应已有的生成式机器人策略。VLS 通过利用视觉-语言模型生成可微分的奖励函数，来指导采样过程，从而使机器人能够在测试时应对未见过的空间配置和任务规范。


<details>
  <summary>Details</summary>
Motivation: 预训练的扩散或流匹配策略在面对与训练数据不同的环境（例如，靠近障碍物、支撑面偏移或有轻微杂物）时会失败，这并非因为缺乏运动技能，而是因为在模仿学习中，动作生成与训练时的特定空间配置和任务规范过于紧密。重新训练或微调成本高昂且概念上不匹配，因为所需行为已存在但无法在测试时选择性地适应。

Method: VLS 将适应视为一个推理时控制问题。它不修改预训练的扩散或流匹配策略的参数，而是通过对采样过程进行引导，以响应分布外（out-of-distribution）的观察-语言输入。具体而言，VLS 利用视觉-语言模型来合成轨迹可微分的奖励函数，该奖励函数引导去噪过程生成满足测试时空间和任务要求的动作轨迹。

Result: 在仿真和真实世界评估中，VLS 的性能持续优于现有方法。在 CALVIN 数据集上，VLS 带来了 31% 的改进；在 LIBERO-PRO 数据集上，性能提升了 13%。在 Franka 机器人上的真实世界部署也证明了 VLS 在面对测试时空间和语义变化时，具有鲁棒的推理时适应能力。

Conclusion: VLS 提供了一种有效的训练无关的框架，能够让已有的生成式机器人策略在推理时适应新的环境和任务需求，解决了模仿学习在处理训练-测试迁移时的局限性，并在多项实验中取得了显著的性能提升。

Abstract: Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/

</details>


### [191] [Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement](https://arxiv.org/abs/2602.03983)
*Weikang Qiu,Tinglin Huang,Aosong Feng,Rex Ying*

Main category: cs.RO

TL;DR: 提出了一种名为SD-VLA的框架，通过将视觉输入解耦为静态和动态令牌，以解决现有VLA模型在长时上下文和推理效率方面的挑战，并在新的长时依赖基准测试中取得了显著的性能提升和推理速度加快。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action (VLA)模型在处理长时上下文和推理效率方面存在挑战，主要是由于二次方注意力复杂性和巨大的参数量。研究人员观察到，在机器人轨迹的视觉信息中，背景等静态信息在时间步之间变化不大。

Method: SD-VLA框架将视觉输入解耦为多层次的静态和动态令牌。静态令牌在不同帧之间共享，减少了上下文长度。通过一个轻量级的recache门，静态令牌的键值（KV）缓存仅在必要时更新，从而实现了高效的多帧集成和推理。

Result: 在提出的新基准测试中，SD-VLA比基线方法在成功率上提高了39.8%。在SimplerEnv基准测试上，SD-VLA也取得了3.9%的性能提升。此外，SD-VLA在相同基准测试上的推理速度比基础VLA模型快2.26倍。

Conclusion: SD-VLA通过解耦静态和动态视觉信息，有效地解决了VLA模型在长时上下文建模和推理效率上的限制，为更实际的机器人部署提供了可能。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.

</details>


### [192] [FDA Flocking: Future Direction-Aware Flocking via Velocity Prediction](https://arxiv.org/abs/2602.04012)
*Hossein B. Jond,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种名为“未来方向感知”（FDA）的仿生学预见性协同策略，用于增强机器人集群的协调性，通过结合邻居的短期未来速度预测来改进传统的反应式群体行为模型。


<details>
  <summary>Details</summary>
Motivation: 现有的群体机器人模型多为反应式，无法有效利用像鸟类姿态和翅膀拍动信号，或多旋翼无人机在转向前的姿态调整等预见性线索来提高协调性。受此启发，研究旨在开发一种能够考虑预见性信息的模型。

Method: 提出了一种“未来方向感知”（FDA）的协同模型，该模型将反应式对齐行为与基于邻居短期未来速度估计的预测项相结合。通过一个可调的混合参数来控制反应式行为和预见性行为之间的权衡。

Result: 仿真结果表明，FDA模型能够实现比纯反应式模型更快、更高水平的对齐，增强集群的平移位移能力，并提高对通信延迟和测量噪声的鲁棒性。

Conclusion: FDA模型通过引入预见性信息，显著提高了集群的协调性、效率和鲁棒性，为未来更高级的群体机器人控制提供了基础。未来的研究将集中于自适应混合策略、加权预测方案以及在实际无人机集群上的实验验证。

Abstract: Understanding self-organization in natural collectives such as bird flocks inspires swarm robotics, yet most flocking models remain reactive, overlooking anticipatory cues that enhance coordination. Motivated by avian postural and wingbeat signals, as well as multirotor attitude tilts that precede directional changes, this work introduces a principled, bio-inspired anticipatory augmentation of reactive flocking termed Future Direction-Aware (FDA) flocking. In the proposed framework, agents blend reactive alignment with a predictive term based on short-term estimates of neighbors' future velocities, regulated by a tunable blending parameter that interpolates between reactive and anticipatory behaviors. This predictive structure enhances velocity consensus and cohesion-separation balance while mitigating the adverse effects of sensing and communication delays and measurement noise that destabilize reactive baselines. Simulation results demonstrate that FDA achieves faster and higher alignment, enhanced translational displacement of the flock, and improved robustness to delays and noise compared to a purely reactive model. Future work will investigate adaptive blending strategies, weighted prediction schemes, and experimental validation on multirotor drone swarms.

</details>


### [193] [An Anatomy-specific Guidewire Shaping Robot for Improved Vascular Navigation](https://arxiv.org/abs/2602.04050)
*Aabha Tamhankar,Jay Patil,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 本文介绍了一种用于神经内血管介入的导丝塑形机器人，能够根据预设的形状生成精确的导丝构型，并通过实验验证了其在2D和3D形状生成以及复杂血管内导航方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经内血管介入手术中导丝的塑形高度依赖于外科医生的经验，尤其在复杂解剖结构中操作难度大。研究旨在实现标准化的自主导丝塑形，以提高手术效率和可重复性。

Method: 提出了一种能够将期望的导丝形状映射为机器人动作的模型，并使用实验数据进行校准。开发了一个台式导丝塑形机器人，能够生成临床上常用的导丝尖端几何形状（C形、S形、角形、钩形）。通过2D和3D实验验证了模型的预测精度，并展示了其在复杂血管内导航的能力。

Result: 该机器人能够生成临床上常见的导丝尖端几何形状。与模型预测的形状相比，实验结果的均方根误差（RMS）为0.56mm。机器人还展示了3D尖端塑形能力，并成功完成了从岩骨段颈内动脉到后交通动脉的复杂血管内导航。

Conclusion: 所提出的导丝塑形机器人能够精确生成导航所需的导丝构型，其模型预测精度高，并具备在复杂解剖结构中进行导航的能力，为实现神经内血管介入手术的标准化和自主化提供了技术支持。

Abstract: Neuroendovascular access often relies on passive microwires that are hand-shaped at the back table and then used to track a microcatheter to the target. Neuroendovascular surgeons determine the shape of the wire by examining the patient pre-operative images and using their experience to identify anatomy specific shapes of the wire that would facilitate reaching the target. This procedure is particularly complex in convoluted anatomical structures and is heavily dependent on the level of expertise of the surgeon. Towards enabling standardized autonomous shaping, we present a bench-top guidewire shaping robot capable of producing navigation-specific desired wire configurations. We present a model that can map the desired wire shape into robot actions, calibrated using experimental data. We show that the robot can produce clinically common tip geometries (C, S, Angled, Hook) and validate them with respect to the model-predicted shapes in 2D. Our model predicts the shape with a Root Mean Square (RMS) error of 0.56mm across all shapes when compared to the experimental results. We also demonstrate 3D tip shaping capabilities and the ability to traverse complex endoluminal navigation from the petrous Internal Carotid Artery (ICA) to the Posterior Communicating Artery (PComm).

</details>


### [194] [Control and State Estimation of Vehicle-Mounted Aerial Systems in GPS-Denied, Non-Inertial Environments](https://arxiv.org/abs/2602.04057)
*Riming Xu,Obadah Wali,Yasmine Marani,Eric Feron*

Main category: cs.RO

TL;DR: 提出了一种用于GNSS受限、非惯性环境下的四旋翼鲁棒控制和估计框架，仅使用外部位置测量和EKF-UI来处理平台运动，无需IMU，并通过实验验证了其稳定性和轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 在GNSS受限且平台运动干扰的环境中，IMU变得不可靠，导致传统估计器失效，无法区分自身加速度和平台加速度，进而引起漂移和控制性能下降。现有方法依赖IMU和GNSS，无法适应此类场景。

Method: 提出一种不依赖IMU的框架，仅使用外部位置测量。核心是采用带未知输入的扩展卡尔曼滤波器（EKF-UI）来估计状态并补偿平台运动。该估计器与级联PID控制器配合，实现3D轨迹跟踪。使用运动捕捉系统隔离定位误差进行测试。

Result: 在移动测试平台上进行了X轴和Y轴的平移测试。实验结果表明，与标准EKF相比，该方法显著提高了稳定性和轨迹跟踪精度，并且无需惯性反馈。

Conclusion: 该方法能够有效地在GNSS受限、非惯性环境中实现四旋翼的鲁棒控制和估计，即使在IMU不可靠的情况下也能保持良好的性能，适用于卡车或电梯等移动平台的实际部署。

Abstract: We present a robust control and estimation framework for quadrotors operating in Global Navigation Satellite System(GNSS)-denied, non-inertial environments where inertial sensors such as Inertial Measurement Units (IMUs) become unreliable due to platform-induced accelerations. In such settings, conventional estimators fail to distinguish whether the measured accelerations arise from the quadrotor itself or from the non-inertial platform, leading to drift and control degradation. Unlike conventional approaches that depend heavily on IMU and GNSS, our method relies exclusively on external position measurements combined with a Extended Kalman Filter with Unknown Inputs (EKF-UI) to account for platform motion. The estimator is paired with a cascaded PID controller for full 3D tracking. To isolate estimator performance from localization errors, all tests are conducted using high-precision motion capture systems. Experimental results in a moving-cart testbed validate our approach under both translational in X-axis and Y-axis dissonance. Compared to standard EKF, the proposed method significantly improves stability and trajectory tracking without requiring inertial feedback, enabling practical deployment on moving platforms such as trucks or elevators.

</details>


### [195] [Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study](https://arxiv.org/abs/2602.04076)
*Daniyal Maroufi,Yash Kulkarni,Justin E. Bird,Jeffrey H. Siewerdsen,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 研究人员开发了一个集成了超声波骨刀和七自由度机器人机械臂的光学引导自主超声波骶骨截骨（USO）机器人系统，并在Sawbones模型上与手动USO进行了比较，结果显示机器人系统在轨迹精度和切割深度控制方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 手动超声波骶骨截骨（MUSO）存在精度不足，尤其是在轨迹和切割深度控制方面，限制了其安全性和精确性，因此需要开发更先进的机器人系统来克服这些局限。

Method: 本研究集成了一个超声波骨刀与一个七自由度机器人机械臂，并使用光学追踪系统进行引导，构建了一个自主USO机器人系统。通过在Sawbones模型上进行实验，将机器人USO（RUSO）与手动USO（MUSO）在相同的截骨条件下进行量化比较，评估了多方向控制能力。

Result: RUSO系统实现了0.11 mm的轨迹均方根误差（RMSE），远优于MUSO的1.10 mm RMSE。在切割深度控制方面，MUSO出现了明显的过穿透（实测16.0 mm，目标8.0 mm），而RUSO系统则精确控制在8.1 mm。

Conclusion: 机器人USO系统能够有效克服手动操作的局限性，显著提高了轨迹精度和切割深度控制的精确性，为实现更安全、更精确的骶骨切除奠定了基础。

Abstract: In this paper, we introduce an autonomous Ultrasonic Sacral Osteotomy (USO) robotic system that integrates an ultrasonic osteotome with a seven-degree-of-freedom (DoF) robotic manipulator guided by an optical tracking system. To assess multi-directional control along both the surface trajectory and cutting depth of this system, we conducted quantitative comparisons between manual USO (MUSO) and robotic USO (RUSO) in Sawbones phantoms under identical osteotomy conditions. The RUSO system achieved sub-millimeter trajectory accuracy (0.11 mm RMSE), an order of magnitude improvement over MUSO (1.10 mm RMSE). Moreover, MUSO trials showed substantial over-penetration (16.0 mm achieved vs. 8.0 mm target), whereas the RUSO system maintained precise depth control (8.1 mm). These results demonstrate that robotic procedures can effectively overcome the critical limitations of manual osteotomy, establishing a foundation for safer and more precise sacral resections.

</details>


### [196] [KGLAMP: Knowledge Graph-guided Language model for Adaptive Multi-robot Planning and Replanning](https://arxiv.org/abs/2602.04129)
*Chak Lam Shek,Faizan M. Tariq,Sangjae Bae,David Isele,Piyush Gupta*

Main category: cs.RO

TL;DR: 提出了一种名为KGLAMP的新型框架，利用知识图谱指导大型语言模型（LLM）为异构多机器人团队进行规划，提高了在动态环境中的规划准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的规划方法难以在动态环境中为异构多机器人系统构建准确的符号表示并保持计划一致性。传统的PDDL规划器需要手动创建符号模型，而基于LLM的规划器常常忽略机器人异构性和环境不确定性。

Method: KGLAMP框架维护一个结构化的知识图谱，编码了对象关系、空间可达性和机器人能力。该知识图谱指导LLM生成准确的PDDL问题规范，并作为动态更新的记忆，检测不一致并触发重规划，使符号计划能够适应不断变化的世界状态。

Result: 在MAT-THOR基准测试上的实验表明，KGLAMP相比纯LLM和基于PDDL的方法，性能至少提高了25.5%。

Conclusion: KGLAMP成功地通过知识图谱增强了LLM在异构多机器人团队规划中的能力，显著提高了规划的准确性和适应性，解决了现有方法的局限性。

Abstract: Heterogeneous multi-robot systems are increasingly deployed in long-horizon missions that require coordination among robots with diverse capabilities. However, existing planning approaches struggle to construct accurate symbolic representations and maintain plan consistency in dynamic environments. Classical PDDL planners require manually crafted symbolic models, while LLM-based planners often ignore agent heterogeneity and environmental uncertainty. We introduce KGLAMP, a knowledge-graph-guided LLM planning framework for heterogeneous multi-robot teams. The framework maintains a structured knowledge graph encoding object relations, spatial reachability, and robot capabilities, which guides the LLM in generating accurate PDDL problem specifications. The knowledge graph serves as a persistent, dynamically updated memory that incorporates new observations and triggers replanning upon detecting inconsistencies, enabling symbolic plans to adapt to evolving world states. Experiments on the MAT-THOR benchmark show that KGLAMP improves performance by at least 25.5% over both LLM-only and PDDL-based variants.

</details>


### [197] [Shaping Expressiveness in Robotics: The Role of Design Tools in Crafting Embodied Robot Movements](https://arxiv.org/abs/2602.04137)
*Elisabetta Zibetti,Alexandra Mercader,Hélène Duval,Florent Levillain,Audrey Rochette,David St-Onge*

Main category: cs.RO

TL;DR: 本研究提出了一种以动作为中心的设计教学法，旨在帮助工程师设计更具表现力的机械臂运动，以增强人机交互和沟通。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地进入人类共享空间，其运动需要超越基本功能，融入表现力以增强参与度和沟通。

Method: 通过一个结合了跨学科方法论的互动研讨会，参与者使用定制的手动遥控器和专用动画软件，通过分析舞蹈的动态和具身化维度来探索和生成机械臂的表达性运动。

Result: 定性分析表明，该“工具箱”能有效弥合人类意图与机器人表现力之间的差距，从而生成更直观、更具吸引力的表达性机械臂运动。

Conclusion: 所提出的以动作为中心的设计教学法能够有效地支持工程师设计出更具表现力的机械臂运动，从而改善人机交互和沟通。

Abstract: As robots increasingly become part of shared human spaces, their movements must transcend basic functionality by incorporating expressive qualities to enhance engagement and communication. This paper introduces a movement-centered design pedagogy designed to support engineers in creating expressive robotic arm movements. Through a hands-on interactive workshop informed by interdisciplinary methodologies, participants explored various creative possibilities, generating valuable insights into expressive motion design. The iterative approach proposed integrates analytical frameworks from dance, enabling designers to examine motion through dynamic and embodied dimensions. A custom manual remote controller facilitates interactive, real-time manipulation of the robotic arm, while dedicated animation software supports visualization, detailed motion sequencing, and precise parameter control. Qualitative analysis of this interactive design process reveals that the proposed "toolbox" effectively bridges the gap between human intent and robotic expressiveness resulting in more intuitive and engaging expressive robotic arm movements.

</details>


### [198] [MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments](https://arxiv.org/abs/2602.04152)
*Yirum Kim,Jaewoo Kim,Ue-Hwan Kim*

Main category: cs.RO

TL;DR: 本文提出了首个多智能体3D场景图生成（MA3DSG）框架，解决了现有方法在单智能体和小型环境下的可扩展性问题。该框架使用训练无关的图对齐算法合并来自多个智能体的部分场景图，并引入了一个新的基准MA3DSG-Bench来支持更广泛的评估。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图生成方法主要基于单智能体假设且仅在小规模环境中进行评估，这限制了其在真实世界场景中的可扩展性。

Method: 提出了一种名为MA3DSG的多智能体3D场景图生成框架。该框架包含一个训练无关的图对齐算法，用于将多个智能体生成的局部场景图有效地合并成一个全局场景图。此外，还提出了MA3DSG-Bench基准，支持不同的智能体配置、领域大小和环境条件。

Result: MA3DSG模型能够使传统的单智能体系统进行协作，而无需学习新参数。MA3DSG-Bench基准提供了一个更通用和可扩展的评估框架。

Conclusion: 该工作为可扩展的多智能体3D场景图生成研究奠定了坚实的基础，并提出了一个更全面和灵活的评估方法。

Abstract: Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle this scalability challenge using multiple agents. We develop a training-free graph alignment algorithm that efficiently merges partial query graphs from individual agents into a unified global scene graph. Leveraging extensive analysis and empirical insights, our approach enables conventional single-agent systems to operate collaboratively without requiring any learnable parameters. To rigorously evaluate 3DSGG performance, we propose MA3DSG-Bench-a benchmark that supports diverse agent configurations, domain sizes, and environmental conditions-providing a more general and extensible evaluation framework. This work lays a solid foundation for scalable, multi-agent 3DSGG research.

</details>


### [199] [A Modern System Recipe for Situated Embodied Human-Robot Conversation with Real-Time Multimodal LLMs and Tool-Calling](https://arxiv.org/abs/2602.04157)
*Dong Won Lee,Sarah Gillet,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.RO

TL;DR: 提出了一种结合实时多模态语言模型和工具接口的简单系统，用于解决机器人实时感知和对话的挑战，并在六种家庭场景下进行了评估，结果显示该方法在提高交互质量方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 现实世界的机器人对话需要在高延迟限制下实时进行对话和主动感知，这对现有技术提出了挑战。

Method: 该研究提出了一种将实时多模态语言模型与用于注意力分配和主动感知的工具接口相结合的系统。在六种需要频繁注意力转移和不断扩大感知范围的家庭场景下，对四种系统变体进行了评估。

Result: 通过与人类标注进行比较，评估了系统在每个回合中工具决策的正确性，并收集了交互质量的主观评分。结果表明，使用实时多模态大型语言模型和工具进行主动感知是实现实用情境化具身对话的一个有前景的方向。

Conclusion: 实时多模态大型语言模型与用于主动感知的工具相结合，为解决具身对话中的感知和对话协调问题提供了一种有前途的解决方案。

Abstract: Situated embodied conversation requires robots to interleave real-time dialogue with active perception: deciding what to look at, when to look, and what to say under tight latency constraints. We present a simple, minimal system recipe that pairs a real-time multimodal language model with a small set of tool interfaces for attention and active perception. We study six home-style scenarios that require frequent attention shifts and increasing perceptual scope. Across four system variants, we evaluate turn-level tool-decision correctness against human annotations and collect subjective ratings of interaction quality. Results indicate that real-time multimodal large language models and tool use for active perception is a promising direction for practical situated embodied conversation.

</details>


### [200] [GenMRP: A Generative Multi-Route Planning Framework for Efficient and Personalized Real-Time Industrial Navigation](https://arxiv.org/abs/2602.04174)
*Chengzhang Wang,Chao Chen,Jun Tao,Tengfei Liu,He Bai,Song Wang,Longfei Xu,Kaikui Liu,Xiangxiang Chu*

Main category: cs.RO

TL;DR: 本文提出了GenMRP，一个用于大规模导航的生成式多路线规划框架，通过骨干到毛细管方法提高效率，并利用纠正性增强来平衡路线质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化、路线多样性或效率方面存在不足，无法满足大规模实时导航的需求。

Method: GenMRP采用骨干到毛细管方法动态构建子网络，并迭代生成路线。通过链接成本模型整合道路特征、用户历史和已生成路线，利用Dijkstra算法生成路线，并使用纠正性增强来平衡质量和多样性。

Result: GenMRP在离线和在线环境中均实现了最先进的性能和高效率。

Conclusion: GenMRP是一个有效的多路线规划框架，已成功部署到实际导航应用中，并为后续研究提供了数据集。

Abstract: Existing industrial-scale navigation applications contend with massive road networks, typically employing two main categories of approaches for route planning. The first relies on precomputed road costs for optimal routing and heuristic algorithms for generating alternatives, while the second, generative methods, has recently gained significant attention. However, the former struggles with personalization and route diversity, while the latter fails to meet the efficiency requirements of large-scale real-time scenarios. To address these limitations, we propose GenMRP, a generative framework for multi-route planning. To ensure generation efficiency, GenMRP first introduces a skeleton-to-capillary approach that dynamically constructs a relevant sub-network significantly smaller than the full road network. Within this sub-network, routes are generated iteratively. The first iteration identifies the optimal route, while the subsequent ones generate alternatives that balance quality and diversity using the newly proposed correctional boosting approach. Each iteration incorporates road features, user historical sequences, and previously generated routes into a Link Cost Model to update road costs, followed by route generation using the Dijkstra algorithm. Extensive experiments show that GenMRP achieves state-of-the-art performance with high efficiency in both offline and online environments. To facilitate further research, we have publicly released the training and evaluation dataset. GenMRP has been fully deployed in a real-world navigation app, demonstrating its effectiveness and benefits.

</details>


### [201] [SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models](https://arxiv.org/abs/2602.04208)
*Hyeonbeom Choi,Daechul Ahn,Youhan Lee,Taewook Kang,Seongwon Cho,Jonghyun Choi*

Main category: cs.RO

TL;DR: 提出了一种名为SCALE的简单推理策略，通过自不确定性来联合调节视觉感知和动作，无需额外训练或多次前向传播，即可提高Vision-Language-Action（VLA）模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间缩放（TTS）方法用于VLA模型通常需要额外训练、验证器和多次前向传播，这使得它们难以部署。此外，它们仅在动作解码时进行干预，而视觉表示固定，这在感知模糊的情况下是不够的。

Method: SCALE策略基于“自不确定性”，模拟了主动推理理论中的不确定性驱动探索，在推理时联合调节视觉感知和动作。在高不确定性时扩大感知和动作的探索范围，在有信心时则专注于利用。

Result: 在模拟和真实世界的基准测试中，SCALE策略能够提升最先进的VLA模型的性能，并优于现有的TTS方法，同时保持单次前向传播的效率。

Conclusion: SCALE是一种高效且无需额外训练的推理策略，可以通过自不确定性自适应地调整视觉感知和动作，从而提高VLA模型在各种条件下的鲁棒性和性能。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.

</details>


### [202] [ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator](https://arxiv.org/abs/2602.04214)
*Zhihai Bi,Yushan Zhang,Kai Chen,Guoyang Zhao,Yulin Li,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出了ALORE系统，一个用于足式机械臂的大型物体自主重排系统，通过分层强化学习、统一交互配置表示、对象速度估计以及任务与运动规划，实现了对多种物体在复杂环境下的高效、无碰撞重排。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理大型、重物体的多物体重排任务时存在挑战，尤其是在复杂环境中需要保证无碰撞运动。因此，研究能够高效、稳定地处理此类任务的自主系统是重要的。

Method: 1. 分层强化学习训练流程：高层对象速度控制器之上训练低层全身控制器，以实现跨多对象的联合学习。 2. 统一交互配置表示和对象速度估计模块：使单一策略能够精确控制不同对象的平面速度。 3. 任务与运动规划框架：联合优化对象访问顺序和对象到目标的分配，支持在线重规划。

Result: ALORE系统在策略泛化能力、对象速度跟踪精度和多对象重排效率方面优于现有基线。系统在现实世界中成功完成了32把椅子、8个连续循环的重排任务，历时近40分钟无失败，并完成了约40米的远距离自主重排。

Conclusion: ALORE系统作为一个针对足式机械臂的大型物体自主重排系统，通过其创新的分层学习、统一表示、速度估计和任务规划方法，在处理复杂场景和多样化大型物体重排任务时表现出优越的鲁棒性、效率和泛化能力。

Abstract: Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at https://zhihaibi.github.io/Alore/.

</details>


### [203] [OAT: Ordered Action Tokenization](https://arxiv.org/abs/2602.04215)
*Chaoqi Liu,Xiaoshen Han,Jiawei Gao,Yue Zhao,Haonan Chen,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种名为有序动作分词 (OAT) 的新方法，用于将连续机器人动作转换为有序的离散令牌，从而提高基于自回归策略的机器人学习的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的动作分词方法在处理连续机器人动作时存在局限性，要么产生过长的令牌序列，要么缺乏结构导致与下一令牌预测不兼容。需要一种既能高效压缩又能保持完整信息且具有因果顺序的动作分词方案。

Method: OAT 使用 Transformer 和寄存器、有限标量量化以及引入顺序的训练机制，将动作块离散化为有序令牌序列。该方法满足高压缩率、完全可解码性和从左到右因果排序令牌空间这三个要求。

Result: 在四个仿真基准和真实世界设置中的 20 多个任务上，配备 OAT 的自回归策略在性能上持续优于现有的分词方案和基于扩散的模型，并且在推理时提供了更高的灵活性。

Conclusion: OAT 是一种有效的动作分词方案，它自然地与自回归生成对齐，并实现了随时可用的推理成本与动作保真度之间的权衡，从而在机器人学习任务中取得了先进的性能。

Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.

</details>


### [204] [Reshaping Action Error Distributions for Reliable Vision-Language-Action Models](https://arxiv.org/abs/2602.04228)
*Shuanghao Bai,Dakai Wang,Cheng Chi,Wanqi Zhou,Jing Lyu,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Lei Xing,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于最小化误差熵（MEE）的目标函数，用于改进连续动作的视觉-语言-动作（VLA）模型，并在模拟和真实机器人操作任务中取得了更好的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型通常使用交叉熵或MSE作为监督目标，这会施加严格的点对点约束。研究者希望探索一种更优的监督方法来提升VLA模型的性能，特别是在连续动作回归方面。

Method: 本文引入了信息论中的最小化误差熵（MEE）概念，并将其应用于连续动作VLA模型的训练。提出了一个轨迹级别的MEE目标函数，以及两个加权变体，并与MSE结合使用。在LIBERO和SimplerEnv等模拟环境以及真实机器人操作任务中进行了评估。

Result: 所提出的MEE方法在标准、少样本和噪声设置下，均在成功率和鲁棒性方面展现出了一致的提升。在数据不平衡的情况下，MEE方法在特定运行范围内也能保持优势，且训练成本和推理效率几乎没有增加。

Conclusion: MEE目标函数通过重塑动作误差分布，可以有效地改进连续动作VLA模型的性能，提高其泛化能力和鲁棒性。理论分析解释了MEE的有效性，并刻画了其实际应用范围。

Abstract: In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: https://cognition2actionlab.github.io/VLA-TMEE.github.io/

</details>


### [205] [GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning](https://arxiv.org/abs/2602.04231)
*Rui Tang,Guankun Wang,Long Bai,Huxin Gao,Jiewen Lai,Chi Kit Ng,Jiazheng Wang,Fan Zhang,Hongliang Ren*

Main category: cs.RO

TL;DR: 提出了一种名为 GeoLanG 的端到端多任务框架，它利用 CLIP 架构、深度引导几何模块（DGGM）和自适应密集通道集成来增强语言引导抓取在复杂、遮挡和低纹理场景下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导抓取方法在处理混乱或遮挡场景时面临挑战，原因在于多阶段流水线导致跨模态融合受限、计算冗余以及泛化能力差。

Method: GeoLanG 是一个端到端框架，基于 CLIP 架构，将视觉和语言输入统一到共享表示空间。它引入了深度引导几何模块（DGGM）来增强对深度信息的利用，并采用自适应密集通道集成来优化多层特征的融合。

Result: 在 OCID-VLG 数据集以及模拟和真实硬件实验中，GeoLanG 在复杂、混乱的环境中实现了精确且鲁棒的语言引导抓取。

Conclusion: GeoLanG 框架能够有效克服现有方法的局限性，为在现实世界以人为中心的场景中实现更可靠的多模态机器人操作铺平了道路。

Abstract: Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings.

</details>


### [206] [Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation](https://arxiv.org/abs/2602.04243)
*Pengfei Yi,Yifan Han,Junyan Li,Litao Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出了一种名为MAE-Select的单目机器人主动视角选择框架，利用预训练的MAE表示，无需标注即可动态选择下一视角，提升了单目系统的能力，甚至在某些情况下优于多目系统。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法依赖固定摄像头，限制了适应性和覆盖范围。受人类主动感知启发，旨在解决单目机器人系统中视角选择的挑战。

Method: 提出MAE-Select框架，该框架利用预训练的多视角掩码自编码器（MAE）表示，并动态选择下一个最有效信息的视角，无需预先标注视角。

Result: MAE-Select能够显著提升单目机器人系统的任务执行能力，并在实验中显示出超越某些多目系统的性能。

Conclusion: MAE-Select是一个有效的主动视角选择框架，能够克服单目机器人系统在视角覆盖和信息获取方面的限制，并为未来多视角机器人感知和模仿学习提供了新的研究方向。

Abstract: Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io.

</details>


### [207] [Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions](https://arxiv.org/abs/2602.04251)
*Li Wang,Ruixuan Gong,Yumo Han,Lei Yang,Lu Yang,Ying Li,Bin Xu,Huaping Liu,Rong Fu*

Main category: cs.RO

TL;DR: 本综述全面回顾了将3D高斯泼溅（3DGS）与同步定位与建图（SLAM）相结合的关键技术方法，分析了性能优化、鲁棒性增强，并讨论了未来的挑战和趋势，旨在为该领域的研究提供技术参考。


<details>
  <summary>Details</summary>
Motivation: 传统的SLAM系统在渲染质量、场景细节恢复和动态环境鲁棒性方面存在局限性。3DGS作为一种新的重建范式，能够实现高效的显式表示和高质量的渲染，因此研究如何将3DGS与SLAM结合以克服这些局限性。

Method: 本文对集成了3DGS的SLAM方法进行了全面的技术性综述。通过对代表性方法在渲染质量、跟踪精度、重建速度和内存消耗四个关键维度上的性能进行分析，并深入探讨其设计原理和技术突破。此外，还考察了增强3DGS-SLAM在运动模糊和动态环境等复杂环境下的鲁棒性的方法。

Result: 文章分析了各种3DGS-SLAM方法的性能表现，并评估了其在提高渲染质量、跟踪精度、重建速度和内存效率方面的进展。同时，也探讨了应对运动模糊和动态场景等挑战的策略。

Conclusion: 将3DGS与SLAM相结合是下一代SLAM系统发展的重要方向，能够实现高保真、高效率和高鲁棒性的目标。本综述为该领域的研究者提供了技术参考，并指出了未来的发展趋势和尚待解决的挑战。

Abstract: Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.

</details>


### [208] [AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models](https://arxiv.org/abs/2602.04256)
*Yuxuan Han,Kunyuan Wu,Qianyi Shao,Renxiang Xiao,Zilu Wang,Cansen Jiang,Yi Xiao,Liang Hu,Yunjiang Lou*

Main category: cs.RO

TL;DR: 提出了一种名为AppleVLM的端到端自动驾驶模型，通过改进的视觉编码器和规划策略编码器，融合了多视角时空信息和鸟瞰图空间信息，并采用链式思考进行微调，从而提升了车道感知、语言理解和处理极端情况的能力，在CARLA基准和真实世界AGV平台上均取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型（VLMs）的端到端自动驾驶方法在车道感知、语言理解和处理极端情况方面存在不足，研究旨在通过引入增强的感知和规划能力来解决这些问题，提高模型的鲁棒性和泛化能力。

Method: AppleVLM引入了一个新的视觉编码器，使用可变形Transformer融合多视角、多时间步的空间-时间信息；引入了一个规划策略编码器，编码显式的鸟瞰图空间信息，以减少导航指令中的语言偏差；最后，使用分层链式思考（Chain-of-Thought）微调VLM解码器，整合视觉、语言和规划特征，输出驾驶路径点。

Result: 在CARLA基准的闭环实验中，AppleVLM达到了最先进的驾驶性能。在AGV平台上的真实世界部署也成功展示了其在复杂户外环境下的端到端自动驾驶能力。

Conclusion: AppleVLM通过融合多模态信息和改进的模型架构，能够有效地解决现有VLM在端到端自动驾驶中面临的挑战，实现了更鲁棒和泛化的自动驾驶功能，并在模拟和真实世界环境中得到了验证。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird's-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments.

</details>


### [209] [GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning](https://arxiv.org/abs/2602.04315)
*Guoqing Ma,Siheng Wang,Zeyu Zhang,Shan Yu,Hao Tang*

Main category: cs.RO

TL;DR: 本文提出了一种名为 GeneralVLA 的分层视觉-语言-动作（VLA）模型，该模型通过知识引导的轨迹规划，提高了在机器人领域模型的零样本泛化能力，并能自动生成机器人数据。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在视觉和语言领域表现出强大的泛化能力，但在机器人领域尚未达到同等水平。模型零样本能力有限是阻碍其在机器人领域有效泛化的主要挑战。

Method: GeneralVLA 是一个分层 VLA 模型，包括：1. 高层感知模块（ASM），用于感知场景的关键点可供性；2. 中层 3DAgent，负责任务理解、技能知识和轨迹规划，生成 3D 路径；3. 低层 3D 感知控制策略，根据 3D 路径进行精确操作。该方法无需真实机器人数据收集或人类演示。

Result: GeneralVLA 成功生成了 14 个任务的轨迹，性能显著优于 VoxPoser 等最先进方法。利用 GeneralVLA 生成的演示数据训练的行为克隆策略，比使用人类演示或 VoxPoser、Scaling-up、Code-As-Policies 生成的数据训练的策略更鲁棒。

Conclusion: GeneralVLA 是一种可扩展的方法，能够为机器人领域生成数据，并在零样本设置下解决新颖任务，有望提升机器人的泛化能力。

Abstract: Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.

</details>


### [210] [Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model](https://arxiv.org/abs/2602.04329)
*Shuo Pei,Yong Wang,Yuanchen Zhu,Chen Sun,Qin Li,Yanan Zhao,Huachun Tan*

Main category: cs.RO

TL;DR: 本文提出了一种名为SDD Planner的基于扩散模型的轨迹规划框架，该框架能够实时地在复杂场景下平衡安全性和驾驶风格，并在多个基准测试中取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 在复杂真实世界场景中实现自动驾驶的安全且风格化的轨迹规划仍然是一个关键挑战。

Method: SDD Planner包含两个核心模块：1. 多源风格感知编码器：利用距离敏感注意力融合动态代理数据和环境上下文，实现异构安全-风格感知；2. 风格引导动态轨迹生成器：通过自适应调整扩散去噪过程中的优先级权重，生成用户偏好且安全的轨迹。

Result: SDD Planner在StyleDrive基准测试中，SM-PDMS指标比最强基线WoTE提高了3.9%。在NuPlan Test14和Test14-hard基准测试中，SDD Planner分别以91.76和80.32的总分排名第一，优于PLUTO等领先方法。真实车辆闭环测试也验证了其安全性和风格对齐能力。

Conclusion: SDD Planner是一个有效的基于扩散模型的轨迹规划框架，能够实时地在复杂场景下实现安全性和驾驶风格的权衡，并具有实际部署的潜力。

Abstract: Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment.

</details>


### [211] [Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition](https://arxiv.org/abs/2602.04401)
*Dhyey Manish Rajani,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 提出一种自动选择视觉地图识别（VPR）系统操作点的方法，在满足特定精度要求的情况下最大化召回率，解决了传统手动调优方法在环境变化下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统VPR系统依赖手动调整匹配阈值，在环境变化时性能会下降。需要一种能够自动适应环境变化并优化性能的方法。

Method: 该方法利用少量已知对应关系的校准遍历，通过分位数归一化相似度得分分布来转移阈值到部署阶段，以满足用户定义的精度要求并最大化召回率。

Result: 在多种VPR技术和数据集上的实验表明，该方法在所需精度较高的情况下，比现有技术能提高高达25%的召回率，并且在不同校准集大小和查询子集上表现稳定。

Conclusion: 提出的方法能够自动选择VPR系统的操作点，无需手动调优，即可在满足精度要求的情况下最大化召回率，并能适应新环境和泛化到不同操作条件，显著优于现有技术。

Abstract: Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall. Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change. We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall. The method uses a small calibration traversal with known correspondences and transfers thresholds to deployment via quantile normalisation of similarity score distributions. This quantile transfer ensures that thresholds remain stable across calibration sizes and query subsets, making the method robust to sampling variability. Experiments with multiple state-of-the-art VPR techniques and datasets show that the proposed approach consistently outperforms the state-of-the-art, delivering up to 25% higher recall in high-precision operating regimes. The method eliminates manual tuning by adapting to new environments and generalising across operating conditions. Our code will be released upon acceptance.

</details>


### [212] [HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation](https://arxiv.org/abs/2602.04412)
*Puyue Wang,Jiawei Hu,Yan Gao,Junyan Wang,Yu Zhang,Gillian Dobbie,Tao Gu,Wafa Johal,Ting Dang,Hong Jia*

Main category: cs.RO

TL;DR: 本文提出了一种名为HoRD的两阶段学习框架，用于实现人形机器人对动态变化（领域迁移）的鲁棒控制，通过历史条件强化学习训练教师策略，再通过在线蒸馏将教师策略的鲁棒性迁移到基于Transformer的学生策略，实现了零样本（zero-shot）的跨领域适应能力。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动态、任务或环境发生微小变化时，性能会显著下降，因此需要更鲁棒的控制方法。

Method: 1. 历史条件强化学习（history-conditioned reinforcement learning）：训练一个教师策略，该策略能够从历史状态-动作轨迹中推断潜在的动态上下文，并在线适应各种随机化的动态。 2. 在线蒸馏（online distillation）：将教师策略的鲁棒控制能力转移到一个基于Transformer的学生策略中，该学生策略处理稀疏的根部相对3D关节关键点轨迹。

Result: HoRD在鲁棒性和迁移能力方面优于现有的强基线方法，尤其是在未见的领域和外部扰动下表现更佳。

Conclusion: HoRD框架结合了历史条件适应和在线蒸馏，能够使单个策略在无需针对新领域进行重新训练的情况下，实现零样本（zero-shot）的跨领域适应，从而提升人形机器人的鲁棒性。

Abstract: Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.

</details>


### [213] [Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning](https://arxiv.org/abs/2602.04419)
*Heqing Yang,Ziyuan Jiao,Shu Wang,Yida Niu,Si Liu,Hangxin Liu*

Main category: cs.RO

TL;DR: 提出了一种名为EPoG的基于探索的场景图序列操作规划框架，该框架结合了图搜索和大型语言模型，以在部分已知环境中高效地进行探索和任务规划，并在仿真和真实机器人实验中取得了显著的成功。


<details>
  <summary>Details</summary>
Motivation: 在部分已知的环境中，机器人需要同时进行探索以获取信息和规划任务以高效执行，这是一个挑战。

Method: EPoG框架集成了基于图的全局规划器和基于大型语言模型（LLM）的局部规划器。它利用观察和LLM的预测不断更新一个信念图，以表示已知和未知对象。通过计算目标图和信念图之间的图编辑操作，并按时间依赖性和移动成本排序，来生成动作序列。

Result: 在46个真实的家庭场景和5个长时程日常物体搬运任务的消融研究中，EPoG的成功率达到了91.3%，平均旅行距离减少了36.1%。

Conclusion: EPoG能够无缝地结合探索和序列操作规划，并在真实世界的动态和未知环境中成功执行复杂任务，展示了其在实际应用中的潜力。

Abstract: In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG's potential for real-world applications.

</details>


### [214] [Gust Estimation and Rejection with a Disturbance Observer for Proprioceptive Underwater Soft Morphing Wings](https://arxiv.org/abs/2602.04438)
*Tobias Cook,Leo Micklem,Huazhi Dong,Yunjie Yang,Michael Mistry,Francesco Giorgio Serchi*

Main category: cs.RO

TL;DR: 本研究提出了一种仿生软翼，通过内置的本体感觉传感器实时感知水流扰动，并利用扰动观测器来估计来流参数，从而增强无人水下航行器在复杂水域中的稳定性和机动性。


<details>
  <summary>Details</summary>
Motivation: 无人水下航行器在浅水区作业时常受到波浪、洋流和湍流等水动力扰动的影响，导致姿态不稳定和机动性下降。研究者受到海洋生物的启发，它们通过结合本体感觉反馈和柔性鳍尾来应对此类环境。

Method: 研究者设计并制造了一种液压驱动的软翼，并开发了其动力学模型。通过测量软翼的曲率变化来推断来流扰动（如迎角变化），并利用扰动观测器实时估计流场参数。最后，基于这些估计值设计控制器来抑制软翼升力的扰动。

Result: 研究成功地通过实验验证了软翼的动力学模型，并证明了基于曲率的本体感觉传感技术可以准确地估计迎角扰动。控制器利用这些估计值，有效地抑制了软翼升力的扰动。

Conclusion: 通过结合本体感觉传感和扰动观测器，该技术模仿了生物体的应对策略，为软体水下航行器在恶劣环境中保持稳定性提供了一种有效的途径。

Abstract: Unmanned underwater vehicles are increasingly employed for maintenance and surveying tasks at sea, but their operation in shallow waters is often hindered by hydrodynamic disturbances such as waves, currents, and turbulence. These unsteady flows can induce rapid changes in direction and speed, compromising vehicle stability and manoeuvrability. Marine organisms contend with such conditions by combining proprioceptive feedback with flexible fins and tails to reject disturbances. Inspired by this strategy, we propose soft morphing wings endowed with proprioceptive sensing to mitigate environmental perturbations. The wing's continuous deformation provides a natural means to infer dynamic disturbances: sudden changes in camber directly reflect variations in the oncoming flow. By interpreting this proprioceptive signal, a disturbance observer can reconstruct flow parameters in real time. To enable this, we develop and experimentally validate a dynamic model of a hydraulically actuated soft wing with controllable camber. We then show that curvature-based sensing allows accurate estimation of disturbances in the angle of attack. Finally, we demonstrate that a controller leveraging these proprioceptive estimates can reject disturbances in the lift response of the soft wing. By combining proprioceptive sensing with a disturbance observer, this technique mirrors biological strategies and provides a pathway for soft underwater vehicles to maintain stability in hazardous environments.

</details>


### [215] [EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models](https://arxiv.org/abs/2602.04515)
*Yu Bai,MingMing Yu,Chaojie Li,Ziyi Bai,Xinlong Wang,Börje F. Karlsson*

Main category: cs.RO

TL;DR: 本文提出了一种名为 EgoActing 的新任务和 EgoActor 模型，用于将高级指令转化为精确的人形机器人动作，实现了在真实世界中机器人感官、运动和操作的紧密结合，并能实时协调感知与执行。


<details>
  <summary>Details</summary>
Motivation: 在真实世界部署人形机器人面临巨大挑战，需要将感知、运动和操作紧密结合，并在信息不全和动态变化的环境中进行。同时，机器人还需要能够鲁棒地在不同类型的子任务之间切换。现有方法难以实现这些目标。

Method: 提出 EgoActing 任务，要求将高级指令直接转化为各种精确、空间感知的机器人动作。并提出 EgoActor 模型，一个统一且可扩展的视觉-语言模型（VLM），能够预测运动原语（如行走、转向、侧移、改变高度）、头部运动、操作指令以及人机交互，以实时协调感知和执行。模型利用了来自真实世界演示的以自我为中心的 RGB 数据、空间推理问答以及模拟环境演示的广泛监督。

Result: EgoActor 能够做出鲁棒的、上下文感知的决策，并以低于 1 秒的速度进行流畅的动作推断（支持 8B 和 4B 参数模型）。在模拟和真实世界环境中进行的广泛评估表明，EgoActor 能有效地连接抽象的任务规划和具体的运动执行，并在不同任务和未见过的环境中展现出泛化能力。

Conclusion: EgoActor 是一个有效的模型，能够将高级指令有效地转化为人形机器人的具体动作，克服了在复杂真实世界环境中部署的挑战，并在各种任务和环境中表现出良好的泛化能力。

Abstract: Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.

</details>


### [216] [TACO: Temporal Consensus Optimization for Continual Neural Mapping](https://arxiv.org/abs/2602.04516)
*Xunlan Zhou,Hongrui Zhao,Negar Mehr*

Main category: cs.RO

TL;DR: 本文提出了一种名为TACO（TemporAl Consensus Optimization）的无重放框架，用于持续神经地图构建，以解决现有方法在动态机器人环境下存在内存和计算限制的问题。TACO通过将历史模型快照视为时间邻居，并强制当前地图与历史表示达成加权共识来更新地图，从而在保持过去几何信息的同时允许更新过时或不可靠的区域。实验证明，TACO在内存效率和适应性之间取得了良好平衡，并且在模拟和真实世界实验中表现优于其他持续学习基线。


<details>
  <summary>Details</summary>
Motivation: 现有的神经隐式地图构建方法在应对动态且内存/计算受限的机器人实际部署场景时存在不足，它们通常依赖重放历史观测来保持一致性并假设场景是静态的，因此无法适应持续学习和动态环境。

Method: 提出TACO（TemporAl Consensus Optimization）框架，将地图构建重塑为时间共识优化问题。该方法将过去的模型快照视为时间邻居，通过强制当前地图更新与历史表示达成加权共识来工作，允许过去可靠的几何信息约束优化，同时更新不可靠或过时的区域。

Result: TACO在内存效率和适应性之间取得了平衡，无需存储或重放之前的数据。在模拟和真实世界实验中，TACO能够稳健地适应场景变化，并持续优于其他持续学习基线。

Conclusion: TACO是一种创新的无重放持续神经地图构建框架，它通过时间共识优化实现了高效且适应性强的地图构建，能够有效应对动态机器人环境中的挑战。

Abstract: Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.

</details>


### [217] [A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction](https://arxiv.org/abs/2602.04522)
*Bingkun Huang,Xin Ma,Nilanjan Chakraborty,Riddhiman Laha*

Main category: cs.RO

TL;DR: 本文提出了一个统一的离散时间机器人操作建模框架（Unicomp），能够同时处理自由空间运动和摩擦接触，解决了现有方法在模拟复杂接触时的不足，并通过实验证明了其在实时操作中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人规划和仿真框架在处理自由空间运动和摩擦接触时存在割裂，尤其是在非凸或分布式接触模式下，简化接触模型导致模式转换不精确，难以实现鲁棒的实时接触操作。

Method: 提出了一种基于互补性刚体动力学的统一离散时间建模框架（Unicomp），将自由空间运动和接触交互统一为耦合的线性和非线性互补问题。针对平面接触，基于最大功率耗散原理推导了摩擦接触模型，使用椭圆极限曲面表示允许的接触力矩，该模型能够捕捉耦合力矩效应（包括扭矩摩擦），且不依赖于接触面的压力分布。

Result: 该框架能够生成一个离散时间的预测模型，通过二次约束将广义速度和接触力矩联系起来，适用于实时优化规划。实验结果表明，该方法在平面推动和复杂的全身操作等任务中，能够实现稳定、物理一致且交互式的行为。

Conclusion: Unicomp框架提供了一种能够一致地模拟自由空间运动和摩擦接触的数学方法，通过允许模型无缝过渡接触模式，克服了现有方法的限制，并在各种操作任务中展现出实时、鲁棒的性能。

Abstract: Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.

</details>


### [218] [Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data](https://arxiv.org/abs/2602.04600)
*Jialiang Li,Yi Qiao,Yunhan Guo,Changwen Chen,Wenzhao Lian*

Main category: cs.RO

TL;DR: 本文提出了CoMe-VLA框架，通过将主动感知形式化为非马尔可夫过程，并利用大规模人类第一人称视角数据，实现了机器人鲁棒且适应性强的探索和操作能力，特别是在解决信息不确定性方面。


<details>
  <summary>Details</summary>
Motivation: 现有机器人在非结构化环境中进行通用操作受限于主动感知能力，而现有的主动感知方法传感行为类型有限，难以应对复杂环境。

Method: 将主动感知形式化为由信息增益和决策分支驱动的非马尔可夫过程，并提出CoMe-VLA框架。该框架利用大规模人类第一人称视角数据学习探索和操作先验，集成了一个认知辅助头用于自主子任务切换，以及一个双轨道记忆系统来融合本体感觉和视觉时间上下文以保持自我和环境意识。模型通过在统一的第一人称视角动作空间中对齐人类和机器人的手眼协调行为，进行三阶段渐进式训练。

Result: 在基于轮式人形机器人的大量实验表明，CoMe-VLA在多种主动感知场景下的多样化长周期任务中展现出强大的鲁棒性和适应性。

Conclusion: CoMe-VLA框架通过结构化主动感知范式和认知记忆机制，显著提升了机器人在复杂、不确定的环境中执行探索和操作任务的能力。

Abstract: Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.

</details>


### [219] [Can We Redesign a Shoulder Exosuit to Enhance Comfort and Usability Without Losing Assistance?](https://arxiv.org/abs/2602.04625)
*Roberto Ferroni,Daniele Filippo Mauceri,Jacopo Carpaneto,Alessandra Pedrocchi,Tommaso Proietti*

Main category: cs.RO

TL;DR: 研究介绍了一种改进的软肩部外骨骼（Soft Shoulder v2），该外骨骼在提高舒适度和功能性（特别是前臂定位和横向平面活动）方面取得了显著进展，同时保持了原有的辅助性能。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴外骨骼在辅助上肢运动方面有潜力，但舒适度常被忽视，影响了实际长期使用。本研究旨在通过改进设计来解决舒适度问题，同时保持或提高辅助性能。

Method: 研究人员设计并测试了一个改进的软肩部外骨骼（v2），并与之前的版本（v1）进行了对比。在八名健康参与者中，通过静态支撑、动态提升和拾放任务，评估了肌肉活动、运动学和用户报告的指标。

Result: 与v1相比，v2显著提高了舒适度和可穿戴性（降低了压力感，提高了有效性、易用性和舒适度评分）。v2还改善了前臂的向前定位能力，并增加了横向平面活动范围（最多30度），且未增加肌肉负荷。两者在增加耐力、减少三角肌激活和保持无动力肩部抬起的透明度方面均有效果。

Conclusion: 以用户为中心的设计改进可以提高软外骨骼的舒适度和功能性，而不会牺牲辅助性能，这对于开发适合长期和日常使用的软外骨骼至关重要。

Abstract: Reduced shoulder mobility limits upper-limb function and the performance of activities of daily living across a wide range of conditions. Wearable exosuits have shown promise in assisting arm elevation, reducing muscle effort, and supporting functional movements; however, comfort is rarely prioritized as an explicit design objective, despite it strongly affects real-life, long-term usage. This study presents a redesigned soft shoulder exosuit (Soft Shoulder v2) developed to address comfort-related limitations identified in our previous version, while preserving assistive performance. In parallel, assistance was also improved, shifting from the coronal plane to the sagittal plane to better support functionally relevant hand positioning. A controlled comparison between the previous (v1) and redesigned (v2) modules was conducted in eight healthy participants, who performed static holding, dynamic lifting, and a functional pick and place task. Muscle activity, kinematics, and user-reported outcomes were assessed. Both versions increased endurance time, reduced deltoid activation, and preserved transparency during unpowered shoulder elevation. However, the difference between them emerged most clearly during functional tasks and comfort evaluation. The redesigned module facilitated forward arm positioning and increased transverse plane mobility by up to 30 deg, without increasing muscular demand. User-reported outcomes further indicated a substantial improvement in wearability, with markedly lower perceived pressure and higher ratings in effectiveness, ease of use, and comfort compared to the previous design. Taken together, these findings show that targeted, user-centered design refinements can improve comfort and functional interaction without compromising assistive performance, advancing the development of soft exosuits suitable for prolonged and daily use.

</details>


### [220] [Radar-Inertial Odometry For Computationally Constrained Aerial Navigation](https://arxiv.org/abs/2602.04631)
*Jan Michalczyk*

Main category: cs.RO

TL;DR: 本论文提出了一种雷达-惯性里程计（RIO）算法，该算法将IMU和雷达数据进行融合，用于实时估计无人机（UAV）的导航状态，特别是在恶劣环境下。该算法利用多状态紧耦合扩展卡尔曼滤波器（EKF）和因子图（FG），并结合深度学习技术处理稀疏且有噪声的雷达点云数据，使用低成本的传感器。


<details>
  <summary>Details</summary>
Motivation: 尽管激光雷达、相机等传感器在机器人定位中应用广泛，但它们在极端光照、烟雾或雾气等环境中性能会受到限制。雷达因其电磁波的特性，对这些环境因素具有鲁棒性。因此，研究一种能在恶劣环境下工作的传感器融合算法具有重要意义。

Method: 论文提出了一种雷达-惯性里程计（RIO）算法，采用多状态紧耦合扩展卡尔曼滤波器（EKF）和因子图（FG）方法，融合了低成本FMCW雷达提供的3D点瞬时速度和距离信息与IMU的测量数据。此外，还利用深度学习技术来处理雷达点云中的点对应问题。

Result: 论文展示了RIO算法能够实时估计无人机的导航状态，并且能够运行在资源受限的嵌入式计算机上。同时，证明了该算法在处理稀疏和有噪声的雷达点云数据方面的有效性。

Conclusion: 提出的RIO算法能够有效地融合IMU和低成本雷达数据，在恶劣环境下实现无人机的实时导航状态估计。结合深度学习技术进一步提升了算法在复杂雷达数据处理上的性能。

Abstract: Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.

</details>


### [221] [Relational Scene Graphs for Object Grounding of Natural Language Commands](https://arxiv.org/abs/2602.04635)
*Julia Kuhn,Francesco Verdoja,Tsvetomila Mihaylova,Ville Kyrki*

Main category: cs.RO

TL;DR: 本研究通过向3D场景图（3DSG）中显式地加入空间关系，提升了大型语言模型（LLM）在理解和执行自然语言指令方面的能力，特别是在目标物体定位方面。


<details>
  <summary>Details</summary>
Motivation: 当前机器人与人类交互需要理解自然语言指令，这需要机器人理解任务、分解动作并将其与环境信息（物体、代理、位置）联系起来。现有的3DSG缺乏显式的空间关系，而人类常依赖这些关系描述环境，因此研究者希望探索加入空间关系能否提升LLM理解自然语言指令的能力。

Method: 提出了一种基于LLM的管道，用于从开放词汇的语言指令中定位目标物体。同时，提出了一种基于视觉语言模型（VLM）的管道，用于从机器人捕获的图像中为3DSG添加开放词汇的空间关系。随后，评估了两种LLM在目标物体定位任务上的表现。

Result: 显式的空间关系确实提高了LLM定位物体（target object grounding）的能力。通过VLM从机器人捕获的图像生成开放词汇的空间关系是可行的，但其相对于封闭词汇关系并未显示出显著优势。

Conclusion: 在3D场景图中显式地加入空间关系，特别是通过VLM从图像中生成，能够有效提升LLM理解和执行自然语言指令的能力，尤其是在目标物体定位方面。然而，开放词汇空间关系的优势在实际应用中可能有限。

Abstract: Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.

</details>


### [222] [From Vision to Assistance: Gaze and Vision-Enabled Adaptive Control for a Back-Support Exoskeleton](https://arxiv.org/abs/2602.04648)
*Alessandro Leanza,Paolo Franceschi,Blerina Spahiu,Loris Roveda*

Main category: cs.RO

TL;DR: 本研究提出了一种基于视觉的腰部外骨骼控制框架，通过集成第一人称视觉和注视跟踪，实现了更及时、更具情境感知的辅助。用户研究表明，这种视觉引导的辅助显著降低了感知到的体力需求，并提高了流畅性、信任度和舒适度。


<details>
  <summary>Details</summary>
Motivation: 现有背部支撑外骨骼的有效性很大程度上依赖于及时和情境感知的辅助，但目前的方法（如EMG、IMU或不直接用于控制的视觉系统）存在局限性。因此，研究需要一种能提供更优辅助的外骨骼控制方法。

Method: 提出了一种视觉门控控制框架，集成了：1. 利用第一人称YOLO感知系统进行实时抓握检测；2. 使用有限状态机（FSM）进行任务进度管理；3. 采用可变导纳控制器，根据姿势和物体状态自适应调整扭矩。该系统结合了自我中心视觉和可穿戴注视跟踪。

Result: 用户研究（15名参与者）显示，与无外骨骼和无视觉的外骨骼相比，采用视觉引导的外骨骼显著降低了感知的体力需求，提高了流畅性、信任度和舒适度。定量分析表明，启用视觉后，外骨骼的辅助响应更早、更强。

Conclusion: 自我中心视觉技术有潜力提高背部支撑外骨骼的响应速度、人体工程学、安全性和用户接受度，通过视觉门控控制框架实现了这一点。

Abstract: Back-support exoskeletons have been proposed to mitigate spinal loading in industrial handling, yet their effectiveness critically depends on timely and context-aware assistance. Most existing approaches rely either on load-estimation techniques (e.g., EMG, IMU) or on vision systems that do not directly inform control. In this work, we present a vision-gated control framework for an active lumbar occupational exoskeleton that leverages egocentric vision with wearable gaze tracking. The proposed system integrates real-time grasp detection from a first-person YOLO-based perception system, a finite-state machine (FSM) for task progression, and a variable admittance controller to adapt torque delivery to both posture and object state. A user study with 15 participants performing stooping load lifting trials under three conditions (no exoskeleton, exoskeleton without vision, exoskeleton with vision) shows that vision-gated assistance significantly reduces perceived physical demand and improves fluency, trust, and comfort. Quantitative analysis reveals earlier and stronger assistance when vision is enabled, while questionnaire results confirm user preference for the vision-gated mode. These findings highlight the potential of egocentric vision to enhance the responsiveness, ergonomics, safety, and acceptance of back-support exoskeletons.

</details>


### [223] [Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics](https://arxiv.org/abs/2602.04746)
*Nozomi Nakajima,Pedro Reynolds-Cuéllar,Caitrin Lynch,Kate Darling*

Main category: cs.RO

TL;DR: 本文对1980-2024年间的机器人学论文进行了实证分析，发现提及“单调、肮脏、危险”(DDD)工作的论文中，只有少数定义了DDD或提供了具体例子。研究者借鉴社会科学文献，为机器人学提供了DDD的定义和概念化指导，并提出了一个框架，以更全面地考虑机器人技术对人类劳动力的影响。


<details>
  <summary>Details</summary>
Motivation: 机器人学领域常以“单调、肮脏、危险”(DDD)工作来论证机器人应用的必要性，但缺乏对DDD的明确定义和具体案例，导致对机器人技术影响人类劳动力的理解可能存在偏差。

Method: 1. 对1980-2024年间的机器人学出版物进行实证分析，统计提及DDD的论文数量，并检查其是否提供定义或具体例子。 2. 回顾社会科学文献，为“单调”、“肮脏”、“危险”工作提供定义和概念化指导。 3. 提出一个框架，帮助机器人学界考虑工作情境，从而更深入地理解机器人技术对人类劳动力的影响。

Result: 在提及DDD的机器人学论文中，只有2.7%定义了DDD，8.7%提供了具体的DDD任务或工作例子。研究者根据社会科学文献，为DDD工作提供了更清晰的定义和概念化方法。

Conclusion: 现有的机器人学文献对“单调、肮脏、危险”工作的概念化和举例不足。本文提出的框架和基于社会科学的定义，旨在引导机器人学界更审慎地思考其技术对人类劳动力市场的潜在影响，促进对机器人应用更 informed 的视角。

Abstract: In robotics, the concept of "dull, dirty, and dangerous" (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on "dull," "dirty," and "dangerous" work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.

</details>


### [224] [PDF-HR: Pose Distance Fields for Humanoid Robots](https://arxiv.org/abs/2602.04851)
*Yi Gu,Yukang Gao,Yangchen Zhou,Xingyu Chen,Yixiao Feng,Mingle Zhao,Yunyang Mo,Zhaorui Wang,Lixin Xu,Renjing Xu*

Main category: cs.RO

TL;DR: 提出了一种名为Pose Distance Fields for Humanoid Robots (PDF-HR) 的轻量级先验方法，用于表示机器人姿态分布，解决了人形机器人运动数据稀缺的问题，并通过多种任务验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人姿态和运动先验在人形机器人上的应用受到高质量人形运动数据稀缺的限制。

Method: PDF-HR 将机器人姿态分布表示为一个连续可微的流形，并预测任意姿态到大量重新定向的机器人姿态的距离，从而得到平滑的姿态合理性度量。该方法可以作为奖励塑造项、正则化器或独立的合理性评分器集成到不同的流程中。

Result: 在单轨迹运动跟踪、一般运动跟踪、风格化运动模仿和一般运动重定向等任务中，PDF-HR 作为即插即用先验，能够一致且显著地提升现有基线模型的性能。

Conclusion: PDF-HR 是一种有效的、轻量级的姿态先验方法，能够克服人形机器人运动数据稀缺的挑战，并显著改善人形机器人在各种运动任务中的表现。

Abstract: Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.

</details>


### [225] [Capturing Visual Environment Structure Correlates with Control Performance](https://arxiv.org/abs/2602.04880)
*Jiahua Dong,Yunze Man,Pavel Tokmakov,Yu-Xiong Wang*

Main category: cs.RO

TL;DR: 通过分析预训练视觉编码器对环境状态（几何、物体结构、物理属性）的解码能力，提出一种新的评估方法，该方法能有效预测下游机器人策略在不同环境下的性能，并优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 直接通过策略运行来评估视觉表示方法的成本高昂，而现有评估方法关注狭窄的视觉特征，限制了泛化能力。需要一种更有效、更具泛化性的评估方法。

Method: 采用分析视角，通过测量预训练视觉编码器对图像中环境状态（几何、物体结构、物理属性）的解码能力来评估其性能。利用带有真实状态信息的模拟环境进行实验。

Result: 探测精度与下游策略在多样化环境和学习设置下的性能高度相关，显著优于现有指标，并能实现高效的表示选择。

Conclusion: 探测视觉编码器解码环境状态的能力是一种有效的代理指标，可用于选择支持泛化操作的视觉表示。学习编码环境的潜在物理状态是机器人控制的一个有前途的目标。

Abstract: The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [226] [C-IDS: Solving Contextual POMDP via Information-Directed Objective](https://arxiv.org/abs/2602.03939)
*Chongyang Shi,Michael Dorothy,Jie Fu*

Main category: eess.SY

TL;DR: 本文研究了上下文部分可观察马尔可夫决策过程（CPOMDP）中的策略合成问题，目标是最大化累积回报并主动减少对潜在上下文的不确定性。提出了一个信息导向的目标，并将C-IDS算法用于合成策略。


<details>
  <summary>Details</summary>
Motivation: 现有的POMDP求解器在处理未知潜在上下文时存在局限性，无法同时优化回报和上下文不确定性。

Method: 引入信息导向目标，该目标将累积回报与潜在上下文和观测之间的互信息结合起来。开发了C-IDS算法来最大化此目标，并将其与信息比的拉格朗日松弛联系起来。证明了温度参数是信息比的上限，并在此基础上建立了子线性贝叶斯遗憾界。

Result: C-IDS算法能够合成最大化信息导向目标（结合回报和信息增益）的策略。该算法在连续Light-Dark环境中表现优于标准的POMDP求解器，能够更快地识别上下文并获得更高的回报。

Conclusion: 提出的信息导向目标和C-IDS算法能够有效地解决CPOMDP问题，通过主动探索来提高上下文识别效率，并最终提升整体性能。

Abstract: We study the policy synthesis problem in contextual partially observable Markov decision processes (CPOMDPs), where the environment is governed by an unknown latent context that induces distinct POMDP dynamics. Our goal is to design a policy that simultaneously maximizes cumulative return and actively reduces uncertainty about the underlying context. We introduce an information-directed objective that augments reward maximization with mutual information between the latent context and the agent's observations. We develop the C-IDS algorithm to synthesize policies that maximize the information-directed objective. We show that the objective can be interpreted as a Lagrangian relaxation of the linear information ratio and prove that the temperature parameter is an upper bound on the information ratio. Based on this characterization, we establish a sublinear Bayesian regret bound over K episodes. We evaluate our approach on a continuous Light-Dark environment and show that it consistently outperforms standard POMDP solvers that treat the unknown context as a latent state variable, achieving faster context identification and higher returns.

</details>


### [227] [Safety-Critical Reinforcement Learning with Viability-Based Action Shielding for Hypersonic Longitudinal Flight](https://arxiv.org/abs/2602.03968)
*Hossein Rastgoftar*

Main category: eess.SY

TL;DR: 提出了一种用于非线性动力学系统的安全关键强化学习框架，通过动作屏蔽和可达性分析确保物理约束下的安全操作，并使用分区域奖励和状态抽象实现在线学习。


<details>
  <summary>Details</summary>
Motivation: 为了在具有连续状态和输入空间的非线性动力学系统（尤其是在存在明确物理约束的情况下）中实现安全的关键控制，同时能够进行在线学习和处理不确定性。

Method: 该框架采用动作屏蔽（action shielding）和基于可达性的可容许动作集（reachability-based admissible action sets）来独立于奖励函数强制执行硬安全约束。通过将状态空间划分为安全区和不安全区，并根据状态属于安全箱的成员资格进行定义，使用模式依赖奖励（mode-dependent reward）来鼓励在安全区内精确跟踪并在不安全区外恢复。为了在连续动力学上进行在线表格学习，通过状态聚合（state aggregation）构建了有限状态抽象（finite-state abstraction），并将动作选择和值更新限制在可容许动作内。

Result: 所提出的框架在具有气动和推进耦合的纵向点质量高超声速飞行器模型上进行了演示，使用了迎角和油门作为控制输入，成功地实现了在物理约束下的安全操作和恢复。

Conclusion: 该安全关键强化学习框架能够有效地处理非线性动力学系统中的硬安全约束，并结合在线学习机制，在复杂动态环境下实现安全可靠的控制。

Abstract: This paper presents a safety-critical reinforcement learning framework for nonlinear dynamical systems with continuous state and input spaces operating under explicit physical constraints. Hard safety constraints are enforced independently of the reward through action shielding and reachability-based admissible action sets, ensuring that unsafe behaviors are never intentionally selected during learning or execution. To capture nominal operation and recovery behavior within a single control architecture, the state space is partitioned into safe and unsafe regions based on membership in a safety box, and a mode-dependent reward is used to promote accurate tracking inside the safe region and recovery toward it when operating outside. To enable online tabular learning on continuous dynamics, a finite-state abstraction is constructed via state aggregation, and action selection and value updates are consistently restricted to admissible actions. The framework is demonstrated on a longitudinal point-mass hypersonic vehicle model with aerodynamic and propulsion couplings, using angle of attack and throttle as control inputs.

</details>


### [228] [Towards X-embodiment safety: A control theory perspective on transferring safety certificates across dynamical systems](https://arxiv.org/abs/2602.03987)
*Nikolaos Bousias,George Pappas*

Main category: eess.SY

TL;DR: 本文提出了一种转移控制屏障函数（tCBF）框架，用于将一个系统的安全约束迁移到另一个具有不同动力学特性的系统上，解决了复杂高维系统应用CBF的挑战。


<details>
  <summary>Details</summary>
Motivation: 直接将控制屏障函数（CBFs）应用于复杂高维系统存在挑战，通常安全证明是针对简化或替代模型设计的，而这些模型与实际系统动力学不完全匹配。因此，需要一种方法来迁移在一种系统上设计的安全保证到另一种动力学不匹配的系统上。

Method: 提出了一种转移控制屏障函数（tCBF）框架，该框架使用一个模拟函数和一个明确的裕度项，将定义在一个系统上的安全约束系统地强制执行到另一个系统上。所提出的转移屏障函数考虑了模型不匹配，并通过基于二次规划的安全滤波器在目标系统上强制执行安全条件。该方法不要求两个系统具有相同的状态维度或动力学。

Result: 在四旋翼飞行器导航任务中证明了该框架的有效性。转移屏障函数确保了目标系统避免碰撞，同时对标称控制器影响最小。结果表明，tCBF可以作为在异构动力学系统之间强制执行安全性的通用机制。

Conclusion: 转移控制屏障函数（tCBF）框架提供了一种有效的方法，可以将安全保证从一个系统迁移到另一个动力学不匹配的系统。该框架能够处理模型不匹配，并能与标称控制器协同工作，在实际应用中（如四旋翼导航）展现了其潜力。

Abstract: Control barrier functions (CBFs) provide a powerful tool for enforcing safety constraints in control systems, but their direct application to complex, high-dimensional dynamics is often challenging. In many settings, safety certificates are more naturally designed for simplified or alternative system models that do not exactly match the dynamics of interest. This paper addresses the problem of transferring safety guarantees between dynamical systems with mismatched dynamics. We propose a transferred control barrier function (tCBF) framework that enables safety constraints defined on one system to be systematically enforced on another system using a simulation function and an explicit margin term. The resulting transferred barrier accounts for model mismatch and induces a safety condition that can be enforced on the target system via a quadratic-program-based safety filter. The proposed approach is general and does not require the two systems to share the same state dimension or dynamics. We demonstrate the effectiveness of the framework on a quadrotor navigation task with the transferred barrier ensuring collision avoidance for the target system, while remaining minimally invasive to a nominal controller. These results highlight the potential of transferred control barrier functions as a general mechanism for enforcing safety across heterogeneous dynamical systems.

</details>


### [229] [Modular Safety Guardrails Are Necessary for Foundation-Model-Enabled Robots in the Real World](https://arxiv.org/abs/2602.04056)
*Joonkyung Kim,Wenxi Chen,Davood Soleymanzadeh,Yi Ding,Xiangbo Gao,Zhengzhong Tu,Ruqi Zhang,Fan Fei,Sushant Veer,Yiwei Lyu,Minghui Zheng,Yan Gu*

Main category: eess.SY

TL;DR: 该论文提出了一个框架来解决结合了基础模型（FM）的机器人所面临的安全挑战，这些挑战超越了物理约束，涉及语义理解和物理执行。论文将机器人安全划分为三个维度：动作安全、决策安全和以人为中心的安全，并提出了一种基于模块化安全防护（监控和干预层）的架构方法，以应对开放式、长尾和动态适应的任务环境。作者还强调了通过表示对齐和保守性分配实现跨层协同设计的重要性，并呼吁社区进一步研究更丰富的防护模块和协同设计策略。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）在机器人领域的应用带来了新的安全挑战，这些挑战源于其开放式的语义推理和具身物理动作，需要超越传统物理约束的安全性考量。现有方法不足以应对任务、环境和人类期望的开放性、长尾性和适应性。

Method: 提出将FM赋能的机器人安全划分为三个维度：动作安全（物理可行性和约束合规性）、决策安全（语义和上下文适宜性）以及以人为中心的安全（符合人类意图、规范和期望）。核心方法是提出模块化安全防护（safety guardrails）的架构，包含监控（评估）和干预层，以实现跨越自主性堆栈的全面安全。此外，还探讨了通过表示对齐（representation alignment）和保守性分配（conservatism allocation）实现跨层协同设计，以提高安全性的效率和有效性。

Result: 论文论证了现有方法（静态验证、单片控制器、端到端学习策略）在开放式、长尾和适应性强的任务、环境及人类期望场景下是不足的。提出的模块化安全防护架构能够提供一个基础，以应对更全面的机器人安全挑战。

Conclusion: 为了在现实世界中安全地部署FM赋能的机器人，需要超越物理约束的安全概念。模块化安全防护（监控和干预层）是应对这些挑战的关键架构。跨层协同设计（表示对齐和保守性分配）是实现高效、低保守性和有效安全性的重要途径。作者呼吁社区共同探索更丰富的防护模块和原则性的协同设计策略，以推动物理AI的实际应用。

Abstract: The integration of foundation models (FMs) into robotics has accelerated real-world deployment, while introducing new safety challenges arising from open-ended semantic reasoning and embodied physical action. These challenges require safety notions beyond physical constraint satisfaction. In this paper, we characterize FM-enabled robot safety along three dimensions: action safety (physical feasibility and constraint compliance), decision safety (semantic and contextual appropriateness), and human-centered safety (conformance to human intent, norms, and expectations). We argue that existing approaches, including static verification, monolithic controllers, and end-to-end learned policies, are insufficient in settings where tasks, environments, and human expectations are open-ended, long-tailed, and subject to adaptation over time. To address this gap, we propose modular safety guardrails, consisting of monitoring (evaluation) and intervention layers, as an architectural foundation for comprehensive safety across the autonomy stack. Beyond modularity, we highlight possible cross-layer co-design opportunities through representation alignment and conservatism allocation to enable faster, less conservative, and more effective safety enforcement. We call on the community to explore richer guardrail modules and principled co-design strategies to advance safe real-world physical AI deployment.

</details>


### [230] [Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2602.04132)
*Dhruv S. Kushwaha,Zoleikha A. Biron*

Main category: eess.SY

TL;DR: 提出了一种结合 Koopman 算子理论的 Lyapunov 约束软 Actor-Critic (LC-SAC) 算法，用于在强化学习中实现安全关键物理系统的稳定性保证，并在 2D 四旋翼轨迹跟踪任务中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习算法在安全关键物理系统中缺乏稳定性保证，容易导致系统振荡或状态发散。现有方法在选择 Lyapunov 函数、计算复杂性和策略保守性方面存在挑战。

Method: 利用扩展动态模式分解 (EDMD) 获得系统的线性近似，并从中推导出 Lyapunov 函数的闭式解。将此 Lyapunov 函数集成到 Soft Actor-Critic (SAC) 算法中，以提供稳定非线性系统的策略保证。

Result: 在 2D 四旋翼轨迹跟踪环境中，LC-SAC 算法显示了训练收敛性，并且与基线 vanilla SAC 算法相比，Lyapunov 稳定性标准的违反情况有所下降。

Conclusion: 所提出的 LC-SAC 算法能够有效地在强化学习中为非线性物理系统提供稳定性保证，并在实际应用中表现出优于标准 SAC 的性能。

Abstract: Reinforcement Learning (RL) has achieved remarkable success in solving complex sequential decision-making problems. However, its application to safety-critical physical systems remains constrained by the lack of stability guarantees. Standard RL algorithms prioritize reward maximization, often yielding policies that may induce oscillations or unbounded state divergence. There has significant work in incorporating Lyapunov-based stability guarantees in RL algorithms with key challenges being selecting a candidate Lyapunov function, computational complexity by using excessive function approximators and conservative policies by incorporating stability criterion in the learning process. In this work we propose a novel Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm using Koopman operator theory. We propose use of extended dynamic mode decomposition (EDMD) to produce a linear approximation of the system and use this approximation to derive a closed form solution for candidate Lyapunov function. This derived Lyapunov function is incorporated in the SAC algorithm to further provide guarantees for a policy that stabilizes the nonlinear system. The results are evaluated trajectory tracking of a 2D Quadrotor environment based on safe-control-gym. The proposed algorithm shows training convergence and decaying violations for Lyapunov stability criterion compared to baseline vanilla SAC algorithm. GitHub Repository: https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking

</details>


### [231] [Mitigation of Structural Harmonic Instability in Virtual Admittance-Based Grid-Forming Inverters via Mimicking Skin Effect](https://arxiv.org/abs/2602.04221)
*Jaekeun Lee,Jae-Jung Jung,Shenghui Cui*

Main category: eess.SY

TL;DR: 本文提出了一种改进的虚拟阻抗-电流控制器（VA-CC）方案，通过引入并联虚拟电阻来解决其固有的谐波不稳定性问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，广泛使用的虚拟阻抗-电流控制器（VA-CC）方案存在谐波不稳定性问题，因此需要更深入的物理机理解释其产生原因。

Method: 文章识别出VA-CC方案中一种与延迟无关的结构性谐波不稳定性机制，即滤波器和虚拟电感之间的相互作用产生了一个具有负阻特性的非无源二阶传递函数项。为了解决这个问题，提出了一种简单的修改方法，将一个并联的虚拟电阻集成到虚拟阻抗结构中。

Result: 提出的修改方案通过模拟皮肤效应增强了高频范围的阻尼，从而提高了VA-CC方案在整个谐波范围内的无源性，且不影响原有的电流控制器和电压前馈控制。实验结果表明，该方法在相同的电网条件下，实现了鲁棒的谐波稳定性，而传统方法则失效。

Conclusion: 通过在虚拟阻抗结构中加入并联虚拟电阻，可以有效解决VA-CC方案的谐波不稳定性问题，提高系统的鲁棒性，并通过模拟皮肤效应增强高频阻尼。

Abstract: The virtual admittance-current controller (VA-CC) scheme is widely employed to emulate an equivalent inductance in front of the internal voltage source of grid-forming inverters. However, recent studies have reported harmonic instabilities associated with VA-CC, motivating the need for a more physically interpretable understanding of their origin. This letter identifies a delay-independent structural mechanism of harmonic instability in the VA-CC scheme, wherein the interaction between the filter and virtual inductances introduces a non-passive second-order transfer-function term exhibiting negative resistance. To address this issue, a simple yet effective modification is proposed by integrating a parallel virtual resistor into the VA structure. This reconfiguration enhances the passivity of VA-CC scheme across the harmonic range by mimicking the skin effect which augments damping in high-frequency range, without altering the wellestablished current controller or voltage feedforward control. Experimental results validate that the proposed method achieves robust harmonic stability, whereas the conventional approach fails under identical grid conditions.

</details>


### [232] [Parameter Privacy-Preserving Data Sharing: A Particle-Belief MDP Formulation](https://arxiv.org/abs/2602.04262)
*Haokun Yu,Jingyuan Zhou,Kaidi Yang*

Main category: eess.SY

TL;DR: 本研究提出了一种在连续状态动态系统中实现参数隐私保护的数据共享方法，通过优化隐私泄露与数据效用之间的权衡，并利用粒子信念马尔可夫决策过程（MDP）来高效地学习最优策略。实验结果表明，该方法能有效阻止对人类驾驶行为参数的推断攻击，同时保持数据可用性和系统性能。


<details>
  <summary>Details</summary>
Motivation: 如何在支持下游估计和控制的同时，防止对手推断敏感参数，实现数据共享中的参数隐私保护。

Method: 将数据共享问题建模为优化问题，权衡隐私泄露和数据效用的影响，并满足数据可用性约束。将该问题转化为等价的信念马尔可夫决策过程（MDP）。提出粒子信念MDP方法，通过序列蒙特卡洛法跟踪参数后验，得到可处理的信念状态近似。推导了基于粒子MI的高斯混合近似的上界，以实现粒子信念MDP的高效优化。

Result: 所提出的粒子信念MDP方法能够渐近收敛，并且能够高效优化。实验结果表明，学习到的连续策略能够显著阻碍对人类驾驶行为参数的推断攻击，同时保持数据可用性和系统性能。

Conclusion: 该研究成功地解决了一个在连续状态动态系统中实现参数隐私保护的数据共享问题，并提出了一种新颖且可行的粒子信念MDP方法来学习最优策略，该方法在实际应用中表现出良好的性能。

Abstract: This paper investigates parameter-privacy-preserving data sharing in continuous-state dynamical systems, where a data owner designs a data-sharing policy to support downstream estimation and control while preventing adversarial inference of a sensitive parameter. This data-sharing problem is formulated as an optimization problem that trades off privacy leakage and the impact of data sharing on the data owner's utility, subject to a data-usability constraint. We show that this problem admits an equivalent belief Markov decision process (MDP) formulation, which provides a simplified representation of the optimal policy. To efficiently characterize information-theoretic privacy leakage in continuous state and action spaces, we propose a particle-belief MDP formulation that tracks the parameter posterior via sequential Monte Carlo, yielding a tractable belief-state approximation that converges asymptotically as the number of particles increases. We further derive a tractable closed-form upper bound on particle-based MI via Gaussian mixture approximations, which enables efficient optimization of the particle-belief MDP. Experiments on a mixed-autonomy platoon show that the learned continuous policy substantially impedes inference attacks on human-driving behavior parameters while maintaining data usability and system performance.

</details>


### [233] [Peak Bounds for the Estimation Error under Sensor Attacks](https://arxiv.org/abs/2602.04568)
*Axel Stafström,Daniel Arnström,Adam Miksits,David Umsonst*

Main category: eess.SY

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: This paper investigates bounds on the estimation error of a linear system affected by norm-bounded disturbances and full sensor attacks. The system is equipped with a detector that evaluates the norm of the innovation signal to detect faults, and the attacker wants to avoid detection. We utilize induced $L_\infty$ system norms, also called \emph{peak-to-peak} norms, to compare the estimation error bounds under nominal operations and under attack. This leads to a sufficient condition for when the bound on the estimation error is smaller during an attack than during nominal operation. This condition is independent of the attack strategy and depends only on the attacker's desire to remain undetected and (indirectly) the observer gain. Therefore, we investigate both an observer design method, that seeks to reduce the error bound under attack while keeping the nominal error bound low, and detector threshold tuning. As a numerical illustration, we show how a sensor attack can deactivate a robust safety filter based on control barrier functions if the attacked error bound is larger than the nominal one. We also statistically evaluate our observer design method and the effect of the detector threshold.

</details>


### [234] [Reinforcement Learning-based Home Energy Management with Heterogeneous Batteries and Stochastic EV Behaviour](https://arxiv.org/abs/2602.04578)
*Meng Yuan,Ye Wang,Xinghuo Yu,Torsten Wik,Changfu Zou*

Main category: eess.SY

TL;DR: 该论文提出了一种基于深度强化学习的家庭能源管理框架，通过联合优化能源支出和电池损耗，同时满足用户舒适度和电动汽车充电需求，并有效应对不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着光伏、电动汽车和储能系统在家用中的普及，系统复杂性增加，同时为能源调控带来了新机遇。然而，如何在不确定性下有效协调这些资源仍然是一个挑战。

Method: 将能源调度问题建模为约束马尔可夫决策过程（CMDP），并采用拉格朗日软Actor-Critic（SAC）算法求解。该框架明确考虑了家用储能和电动汽车电池的异质损耗特性，以及用户行为（如到达/离开时间、行驶距离）的随机性。

Result: 数值模拟显示，该框架能够满足室内温度和电动汽车充电状态等物理约束，消除热振荡，并实现显著的经济效益。与两种标准的基于规则的方法相比，该方法将累积运营成本大幅降低，并使电池损耗成本降低了8.44%。

Conclusion: 提出的基于深度强化学习的家庭能源管理框架能够有效地在不确定性下，联合最小化能源支出和电池损耗，同时满足用户舒适度和电动汽车充电需求，并能保证物理约束。

Abstract: The widespread adoption of photovoltaic (PV), electric vehicles (EVs), and stationary energy storage systems (ESS) in households increases system complexity while simultaneously offering new opportunities for energy regulation. However, effectively coordinating these resources under uncertainties remains challenging. This paper proposes a novel home energy management framework based on deep reinforcement learning (DRL) that can jointly minimise energy expenditure and battery degradation while guaranteeing occupant comfort and EV charging requirements. Distinct from existing studies, we explicitly account for the heterogeneous degradation characteristics of stationary and EV batteries in the optimisation, alongside stochastic user behaviour regarding arrival time, departure time, and driving distance. The energy scheduling problem is formulated as a constrained Markov decision process (CMDP) and solved using a Lagrangian soft actor-critic (SAC) algorithm. This approach enables the agent to learn optimal control policies that enforce physical constraints, including indoor temperature bounds and target EV state of charge upon departure, despite stochastic uncertainties. Numerical simulations over a one-year horizon demonstrate the effectiveness of the proposed framework in satisfying physical constraints while eliminating thermal oscillations and achieving significant economic benefits. Specifically, the method reduces the cumulative operating cost substantially compared to two standard rule-based baselines while simultaneously decreasing battery degradation costs by 8.44%.

</details>


### [235] [Safe Adaptive Control of Parabolic PDE-ODE Cascades](https://arxiv.org/abs/2602.04656)
*Yun Jiang,Ji Wang*

Main category: eess.SY

TL;DR: 提出了一种针对具有参数不确定性的抛物线偏微分方程-常微分方程（PDE-ODE）级联系统的安全自适应边界控制策略。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是为了解决具有参数不确定性的PDE-ODE级联系统在保持系统安全性和状态收敛性方面的挑战。

Method: 采用自适应控制障碍函数（aCBF）框架，结合高相对度CBF和基于批量最小二乘估计（BaLSI）的自适应控制，该方法能在有限时间内精确识别参数。

Result: 所提出的控制律能够保证系统输出状态始终保持在安全集内，或在有限时间内返回安全集，同时所有被控状态均收敛至零。

Conclusion: 该研究成功地提出了一种在存在参数不确定性的情况下，能够同时保证系统安全性和状态收敛性的PDE-ODE级联系统控制策略，并通过数值模拟验证了其有效性。

Abstract: In this paper, we propose a safe adaptive boundary control strategy for a class of parabolic partial differential equation-ordinary differential equation (PDE-ODE) cascaded systems with parametric uncertainties in both the PDE and ODE subsystems. The proposed design is built upon an adaptive Control Barrier Function (aCBF) framework that incorporates high-relative-degree CBFs together with a batch least-squares identification (BaLSI)-based adaptive control that guarantees exact parameter identification in finite time. The proposed control law ensures that: (i) if the system output state initially lies within a prescribed safe set, safety is maintained for all time; otherwise, the output is driven back into the safe region within a preassigned finite time; and (ii) convergence to zero of all plant states is achieved. Numerical simulations are provided to demonstrate the effectiveness of the proposed approach.

</details>


### [236] [Dynamic Constraint Tightening for Nonlinear MPC for Autonomous Racing via Contraction Analysis](https://arxiv.org/abs/2602.04744)
*Joscha F. Bongard,Valentin L. Krieger,Boris Lohmann*

Main category: eess.SY

TL;DR: 该研究提出了一种基于控制收缩度量（CCM）的鲁棒非线性模型预测控制（MPC）框架，用于在操纵极限下自动驾驶车辆的路径跟踪，并能在保持计算效率的同时有效处理不确定性。


<details>
  <summary>Details</summary>
Motivation: 为了使自动驾驶车辆在达到操纵极限时仍能实现鲁棒的路径跟踪，并克服现有方法在处理不确定性时的计算成本问题。

Method: 首先构建了一个非线性MPC方案，然后考虑轮胎参数和力扰动的不确定性，推导并优化了一个不确定模型的CCM，最后利用该CCM参数化一个同心管，用于MPC中的约束收紧，从而实现鲁棒控制。

Result: 所提出的鲁棒非线性MPC在动态极限下进行了验证，并通过仿真证明了其能够捕捉所有不确定轨迹，同时避免了过度保守。仿真结果显示，同心管在名义方案将违反约束的区域会显著扩张。

Conclusion: 该研究成功开发了一种高效且鲁棒的非线性MPC框架，通过CCM和同心管有效地处理了自动驾驶车辆在操纵极限下的不确定性，优于现有方法。

Abstract: This work develops a robust nonlinear Model Predictive Control (MPC) framework for path tracking in autonomous vehicles operating at the limits of handling utilizing a Control Contraction Metric (CCM) derived from a perturbed dynamic single track model. We first present a nonlinear MPC scheme for autonomous vehicles. Building on this nominal scheme, we assume limited uncertainty in tire parameters as well as bounded force disturbances in both lateral and longitudinal directions. By simplifying the perturbed model, we optimize a CCM for the uncertain model, which is validated through simulations at the dynamic limits of vehicle performance. This CCM is subsequently employed to parameterize a homothetic tube used for constraint tightening within the MPC formulation. The resulting robust nonlinear MPC is computationally more efficient than competing methods, as it introduces only a single additional state variable into the prediction model compared to the nominal scheme. Simulation results demonstrate that the homothetic tube expands most significantly in regions where the nominal scheme would otherwise violate constraints, illustrating its ability to capture all uncertain trajectories while avoiding unnecessary conservatism.

</details>


### [237] [Control Lyapunov Functions for Optimality in Sontag-Type Control](https://arxiv.org/abs/2602.04756)
*Joscha F. Bongard,Boris Lohmann*

Main category: eess.SY

TL;DR: 本文研究了使用LQR值函数和反馈线性化系统设计控制Lyapunov函数（CLF），并分析了其在Sontag公式下的性能。结果表明，这些设计可以保证局部或全局的渐近稳定性，同时最小化与CLF相关的二次成本。


<details>
  <summary>Details</summary>
Motivation: 现有CLF设计方法虽然能保证稳定性，但其对系统性能的影响尚不明确。本文旨在解决这一问题，通过具体设计CLF并分析其性能。

Method: 1. 将LQR设计的值函数用作CLF，分析Sontag型控制器最小化的成本函数和闭环系统稳定性。2. 针对反馈线性化系统，设计一种全局CLF，并分析Sontag设计下的稳定性与成本。

Result: 1. 使用LQR值函数作为CLF，Sontag型控制器最小化二次成本和CLF相关成本，保证局部渐近稳定性。2. 针对反馈线性化系统，获得全局CLF，Sontag设计保证全局渐近稳定性，并最小化两种成本。

Conclusion: 通过使用LQR值函数或针对反馈线性化系统设计CLF，可以实现Sontag公式下稳定性和性能的良好权衡。这两种方法都是构造性的，易于应用于非线性多输入系统。

Abstract: Given a Control Lyapunov Function (CLF), Sontag's famous Formula provides a nonlinear state-feedback guaranteeing asymptotic stability of the setpoint. At the same time, a cost function that depends on the CLF is minimized. While there exist methods to construct CLFs for certain classes of systems, the impact on the resulting performance is unclear. This article aims to make two contributions to this problem: (1) We show that using the value function of an LQR design as CLF, the resulting Sontag-type controller minimizes a classical quadratic cost around the setpoint and a CLF-dependent cost within the domain where the CLF condition holds. We also show that the closed-loop system is stable within a local region at least as large as that generated by the LQR. (2) We show a related CLF design for feedback-linearizable systems resulting in a global CLF in a straight-forward manner; The Sontag design then guarantees global asymptotic stability while minimizing a quadratic cost at the setpoint and a CLF-dependent cost in the whole state-space. Both designs are constructive and easily applicable to nonlinear multi-input systems under mild assumptions.

</details>


### [238] [SQP-Based Cable-Tension Allocation for Multi-Drone Load Transport](https://arxiv.org/abs/2602.04801)
*Lamberto Vazquez-Soqui,Fatima Oliva-Palomo,Diego Mercado-Ravell,Pedro Castillo*

Main category: eess.SY

TL;DR: 提出了一种基于顺序二次规划（SQP）的实时优化层，用于解决多旋翼无人机协同空中负载运输中的张力分配问题，以实现更安全、能量平衡的运输。


<details>
  <summary>Details</summary>
Motivation: 单旋翼无人机在协同空中负载运输中存在张力分配不足的问题，可能导致能量分布不均、绳索松弛或无人机与绳索碰撞。

Method: 通过在分层负载-位置-姿态控制器中引入顺序二次规划（SQP）算法，最小化绳索张力平方和，并加入绳索对齐惩罚项，以防止绳索夹角过小，同时不改变参考轨迹。

Result: 在模拟实验中，SQP 例程在标准硬件上能在几毫秒内完成，表明其实时可行性。增益调整的敏感性分析表明，可以在线调整绳索对齐惩罚项的增益，以在安全裕度和能耗之间进行可控权衡，且对跟踪性能无明显影响。

Conclusion: 该框架为多旋翼无人机协同空中负载运输提供了一种可扩展的、安全且能量平衡的解决方案，并具有良好的实时性和可调性。

Abstract: Multi-Agent Aerial Load Transport Systems (MAATS) offer greater payload capacity and fault tolerance than single-drone solutions. However, they have an underdetermined tension allocation problem that leads to uneven energy distribution, cable slack, or collisions between drones and cables. This paper presents a real-time optimization layer that improves a hierarchical load-position-attitude controller by incorporating a Sequential Quadratic Programming (SQP) algorithm. The SQP formulation minimizes the sum of squared cable tensions while imposing a cable-alignment penalty that discourages small inter-cable angles, thereby preventing tether convergence without altering the reference trajectory. We tested the method under nominal conditions by running numerical simulations of four quadrotors. Computational experiments based on numerical simulations demonstrate that the SQP routine runs in a few milliseconds on standard hardware, indicating feasibility for real-time use. A sensitivity analysis confirms that the gain of the cable-alignment penalty can be tuned online, enabling a controllable trade-off between safety margin and energy consumption with no measurable degradation of tracking performance in simulation. This framework provides a scalable path to safe and energy-balanced cooperative load transport in practical deployments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [239] [CONRep: Uncertainty-Aware Vision-Language Report Drafting Using Conformal Prediction](https://arxiv.org/abs/2602.03910)
*Danial Elyassirad,Benyamin Gheiji,Mahsa Vatanparast,Amir Mahmoud Ahmadzadeh,Seyed Amir Asef Agah,Mana Moassefi,Meysam Tavakoli,Shahriar Faghani*

Main category: eess.IV

TL;DR: 本研究提出了一种名为CONRep的模型无关框架，该框架利用保形预测（CP）为视觉语言模型（VLMs）生成的放射报告提供统计学上可靠的不确定性量化，从而提高自动化报告系统的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动放射报告草拟（ARRD）系统缺乏明确的不确定性估计，这限制了其在临床部署中的信任度和安全性。

Method: CONRep框架通过两种方式集成保形预测（CP）：1.在标签层面，校准预定义发现的二元预测；2.在句子层面，通过图像-文本语义对齐评估自由文本印象的不确定性。该框架适用于生成式和对比式VLMs。

Result: 在公开的胸部X光数据集上的评估显示，CONRep能够根据不确定性对报告进行分层。高置信度的输出与放射科医生注释和真实印象的一致性显著高于低置信度输出。

Conclusion: CONRep通过在不修改底层模型的情况下实现校准的置信度分层，提高了自动化放射报告系统的透明度、可靠性和临床可用性。

Abstract: Automated radiology report drafting (ARRD) using vision-language models (VLMs) has advanced rapidly, yet most systems lack explicit uncertainty estimates, limiting trust and safe clinical deployment. We propose CONRep, a model-agnostic framework that integrates conformal prediction (CP) to provide statistically grounded uncertainty quantification for VLM-generated radiology reports. CONRep operates at both the label level, by calibrating binary predictions for predefined findings, and the sentence level, by assessing uncertainty in free-text impressions via image-text semantic alignment. We evaluate CONRep using both generative and contrastive VLMs on public chest X-ray datasets. Across both settings, outputs classified as high confidence consistently show significantly higher agreement with radiologist annotations and ground-truth impressions than low-confidence outputs. By enabling calibrated confidence stratification without modifying underlying models, CONRep improves the transparency, reliability, and clinical usability of automated radiology reporting systems.

</details>


### [240] [To What Extent Do Token-Level Representations from Pathology Foundation Models Improve Dense Prediction?](https://arxiv.org/abs/2602.03887)
*Weiming Chen,Xitong Ling,Xidong Wang,Zhenyang Cai,Yijia Guo,Mingxi Fu,Ziyi Zeng,Minxi Ouyang,Jiawen Li,Yizhi Wang,Tian Guan,Benyou Wang,Yonghong He*

Main category: eess.IV

TL;DR: PFM-DenseBench 是一个大规模基准测试，用于评估 17 种病理基础模型（PFMs）在 18 个公共数据集上的密集预测（如分割）任务表现，并系统研究了不同适应和微调策略的影响，旨在为实际应用提供可复现的评估和模型选择指南。


<details>
  <summary>Details</summary>
Motivation: 现有的病理基础模型（PFMs）在下游任务中表现出强大的迁移能力，但在实际部署密集预测任务时，缺乏对不同 PFMs 行为、适应性选择如何影响性能和稳定性的清晰、可复现的理解。

Method: 构建了一个名为 PFM-DenseBench 的大规模基准测试，在一个统一的协议下，评估了 17 种 PFMs 在 18 个公共数据集上的密集预测性能。研究了多种适应和微调策略，并分析了它们在异构数据集上的表现。

Result: 系统性地评估了不同 PFMs 和微调策略在不同数据集上的表现，得出了关于何时何地以及为何某些模型和策略会成功或失败的实用性见解。

Conclusion: PFM-DenseBench 提供了一个可复现的评估框架，为在实际密集病理预测任务中选择合适的 PFM 和优化适应策略提供了依据，并发布了相关的容器、配置和数据集卡片以支持研究的可复现性。

Abstract: Pathology foundation models (PFMs) have rapidly advanced and are becoming a common backbone for downstream clinical tasks, offering strong transferability across tissues and institutions. However, for dense prediction (e.g., segmentation), practical deployment still lacks a clear, reproducible understanding of how different PFMs behave across datasets and how adaptation choices affect performance and stability. We present PFM-DenseBench, a large-scale benchmark for dense pathology prediction, evaluating 17 PFMs across 18 public segmentation datasets. Under a unified protocol, we systematically assess PFMs with multiple adaptation and fine-tuning strategies, and derive insightful, practice-oriented findings on when and why different PFMs and tuning choices succeed or fail across heterogeneous datasets. We release containers, configs, and dataset cards to enable reproducible evaluation and informed PFM selection for real-world dense pathology tasks. Project Website: https://m4a1tastegood.github.io/PFM-DenseBench

</details>


### [241] [DINO-AD: Unsupervised Anomaly Detection with Frozen DINO-V3 Features](https://arxiv.org/abs/2602.03870)
*Jiayu Huo,Jingyuan Hong,Liyun Chen*

Main category: eess.IV

TL;DR: 提出了一种名为 DINO-AD 的无监督医学图像异常检测框架，该框架利用 DINO-V3 自监督表示，通过嵌入相似性匹配和前景感知 K-means 聚类来定位异常区域，并在 Brain 和 Liver 数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测对于可扩展且标注效率高的诊断系统至关重要，因为它不需要像素级标注。

Method: 该框架使用 DINO-V3 自监督表示，采用嵌入相似性匹配选择语义一致的支持图像，并使用前景感知 K-means 聚类来建模正常特征分布。通过查询特征与聚类后的正常嵌入之间的余弦相似度来计算异常图。

Result: 在 Brain 和 Liver 数据集上，DINO-AD 取得了高达 98.71 的 AUROC 分数，优于最先进的方法。定性结果显示异常定位更清晰准确。消融研究验证了各组件的有效性。

Conclusion: DINO-AD 是一种有效且鲁棒的无监督医学图像异常检测框架，能够实现精确且可解释的异常定位，并且具有良好的泛化能力。

Abstract: Unsupervised anomaly detection (AD) in medical images aims to identify abnormal regions without relying on pixel-level annotations, which is crucial for scalable and label-efficient diagnostic systems. In this paper, we propose a novel anomaly detection framework based on DINO-V3 representations, termed DINO-AD, which leverages self-supervised visual features for precise and interpretable anomaly localization. Specifically, we introduce an embedding similarity matching strategy to select a semantically aligned support image and a foreground-aware K-means clustering module to model the distribution of normal features. Anomaly maps are then computed by comparing the query features with clustered normal embeddings through cosine similarity. Experimental results on both the Brain and Liver datasets demonstrate that our method achieves superior quantitative performance compared with state-of-the-art approaches, achieving AUROC scores of up to 98.71. Qualitative results further confirm that our framework produces clearer and more accurate anomaly localization. Extensive ablation studies validate the effectiveness of each proposed component, highlighting the robustness and generalizability of our approach.

</details>


### [242] [AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology](https://arxiv.org/abs/2602.03998)
*Ahmed Alagha,Christopher Leclerc,Yousef Kotp,Omar Metwally,Calvin Moras,Peter Rentopoulos,Ghodsiyeh Rostami,Bich Ngoc Nguyen,Jumanah Baig,Abdelhakim Khellaf,Vincent Quoc-Huy Trinh,Rabeb Mizouni,Hadi Otrok,Jamal Bentahar,Mahdi S. Hosseini*

Main category: eess.IV

TL;DR: AtlasPatch 是一个高效且可扩展的 WSI 预处理框架，可实现准确的组织检测和高通量切片提取，计算开销极小，并且性能与 SOTA 相当。


<details>
  <summary>Details</summary>
Motivation: 现有的 WSI 预处理工具（包括组织检测和切片提取）存在计算瓶颈，要么依赖不准确的启发式阈值，要么采用在数据多样性有限的切片级别训练的 AI 方法，导致计算复杂度高。

Method: AtlasPatch 使用 Segment-Anything 模型，并在包含约 30,000 个 WSI 缩略图的异构、半手动标注数据集上进行高效微调，以实现组织检测。该工具将缩略图的组织掩码外推到全分辨率切片，以提取用户指定放大倍率下的切片坐标，并可选择将切片直接流式传输到通用的图像编码器或存储切片图像，同时支持 CPU 和 GPU 的高效并行处理。

Result: AtlasPatch 在分割精度、计算复杂度和下游多实例学习方面进行了评估，其性能与 SOTA 相当，但计算成本仅为其一小部分。

Conclusion: AtlasPatch 是一个有效且计算成本低的 WSI 预处理框架，可用于准确的组织检测和高通量切片提取，为计算病理学工作流程提供了一个可扩展的解决方案。

Abstract: Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.

</details>


### [243] [MS-SCANet: A Multiscale Transformer-Based Architecture with Dual Attention for No-Reference Image Quality Assessment](https://arxiv.org/abs/2602.04032)
*Mayesha Maliha R. Mithila,Mylene C. Q. Farias*

Main category: eess.IV

TL;DR: 提出了一种名为 MS-SCANet 的基于 Transformer 的无参考图像质量评估网络，该网络采用多尺度双分支结构、空间通道注意力机制以及新的跨分支和自适应池化一致性损失函数，在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统单尺度方法在捕捉图像的精细和粗略细节方面存在不足，现有方法在多尺度特征整合和保持空间完整性方面存在局限性。

Method: 设计了一个名为 MS-SCANet 的 Transformer 架构，采用双分支结构处理多尺度图像。整合了空间和通道注意力机制，并引入了跨分支注意力机制来增强不同尺度之间的特征整合。此外，还提出了两种新的损失函数：跨分支一致性损失和自适应池化一致性损失，以保持特征缩放过程中的空间完整性。

Result: MS-SCANet 在 KonIQ-10k、LIVE、LIVE Challenge 和 CSIQ 等数据集上进行了评估，结果显示其性能持续优于最先进的方法，与主观人类评分的相关性更强。

Conclusion: MS-SCANet 是一种强大的无参考图像质量评估框架，通过多尺度处理、注意力机制和新颖的损失函数，能够有效地捕捉图像质量特征，并取得比现有方法更优越的性能。

Abstract: We present the Multi-Scale Spatial Channel Attention Network (MS-SCANet), a transformer-based architecture designed for no-reference image quality assessment (IQA). MS-SCANet features a dual-branch structure that processes images at multiple scales, effectively capturing both fine and coarse details, an improvement over traditional single-scale methods. By integrating tailored spatial and channel attention mechanisms, our model emphasizes essential features while minimizing computational complexity. A key component of MS-SCANet is its cross-branch attention mechanism, which enhances the integration of features across different scales, addressing limitations in previous approaches. We also introduce two new consistency loss functions, Cross-Branch Consistency Loss and Adaptive Pooling Consistency Loss, which maintain spatial integrity during feature scaling, outperforming conventional linear and bilinear techniques. Extensive evaluations on datasets like KonIQ-10k, LIVE, LIVE Challenge, and CSIQ show that MS-SCANet consistently surpasses state-of-the-art methods, offering a robust framework with stronger correlations with subjective human scores.

</details>
