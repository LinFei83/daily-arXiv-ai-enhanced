{"id": "2602.09144", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09144", "abs": "https://arxiv.org/abs/2602.09144", "authors": ["Jasper Juchem", "Mia Loccufier"], "title": "Shaping Energy Exchange with Gyroscopic Interconnections: a geometric approach", "comment": "Conference paper submitted to the 10th IEEE Conference on Control Technology and Applications (CCTA) 2026 In Vancouver, and is currently under review", "summary": "Gyroscopic interconnections enable redistribution of energy among degrees of freedom while preserving passivity and total energy, and they play a central role in controlled Lagrangian methods and IDA-PBC. Yet their quantitative effect on transient energy exchange and subsystem performance is not well characterised. We study a conservative mechanical system with constant skew-symmetric velocity coupling. Its dynamics are integrable and evolve on invariant two-tori, whose projections onto subsystem phase planes provide geometric description of energy exchange. When the ratio of normal-mode frequencies is rational, these projections become closed resonant Lissajous curves, enabling structured analysis of subsystem trajectories. To quantify subsystem behaviour, we introduce the inscribed-radius metric: the radius of the largest origin-centred circle contained in a projected trajectory. This gives a lower bound on attainable subsystem energy and acts as an internal performance measure. We derive resonance conditions and develop an efficient method to compute or certify the inscribed radius without time-domain simulation. Our results show that low-order resonances can strongly restrict energy depletion through phase-locking, whereas high-order resonances recover conservative bounds. These insights lead to an explicit interconnection-shaping design framework for both energy absorption and containment control strategies, while taking responsiveness into account.", "AI": {"tldr": "研究了具有恒定斜对称速度耦合的保守机械系统，利用李萨如图形分析了陀螺耦合对能量交换和子系统性能的影响，并提出了“内切半径”度量来量化子系统行为，开发了无仿真计算内切半径的方法，为能量吸收和保持控制策略提供了设计框架。", "motivation": "当前对陀螺耦合在瞬态能量交换和子系统性能方面的量化影响认识不足。", "method": "研究了一个具有恒定斜对称速度耦合的保守机械系统，分析了其在有理数频率比下的动力学演化（李萨如图形），提出了“内切半径”度量来量化子系统行为，并开发了一种无需时域仿真的计算方法。", "result": "低阶共振会通过相位锁定显著限制能量耗散，高阶共振则恢复了保守边界。内切半径度量提供了子系统能量的下界和内部性能度量。", "conclusion": "研究结果为设计能量吸收和保持控制策略提供了见解和设计框架，并考虑了响应性。"}}
{"id": "2602.09213", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09213", "abs": "https://arxiv.org/abs/2602.09213", "authors": ["Md Mahfuzur Rahman Chy", "Md Rifat Al Amin Khan", "Md Sultan Mahamud", "Anwarul Islam Sifat", "Fiona J. Stevens McFadden"], "title": "Real-time Load Current Monitoring of Overhead Lines Using GMR Sensors", "comment": "5 pages, 6 figures, to be published in 2026 IEEE PES Transmission and Distribution (T&D) Conference and Exposition, Chicago, IL, USA", "summary": "Non-contact current monitoring has emerged as a prominent research focus owing to its non-intrusive characteristics and low maintenance requirements. However, while they offer high sensitivity, contactless sensors necessitate sophisticated design methodologies and thorough experimental validation. In this study, a Giant Magneto-Resistance (GMR) sensor is employed to monitor the instantaneous currents of a three-phase 400-volt overhead line, and its performance is evaluated against that of a conventional contact-based Hall effect sensor. A mathematical framework is developed to calculate current from the measured magnetic field signals. Furthermore, a MATLAB-based dashboard is implemented to enable real-time visualization of current measurements from both sensors under linear and non-linear load conditions. The GMR current sensor achieved a relative accuracy of 64.64% to 91.49%, with most phases above 80%. Identified improvements over this are possible, indicating that the sensing method has potential as a basis for calculating phase currents.", "AI": {"tldr": "本研究使用 GMR 传感器进行非接触式三相电流监测，并与霍尔效应传感器进行比较，结果显示 GMR 传感器具有潜在的应用价值。", "motivation": "非接触式电流监测因其非侵入性和低维护性而受到关注，但高灵敏度传感器需要复杂的设计和验证。", "method": "使用 GMR 传感器监测三相 400 伏架空线路的瞬时电流，并与接触式霍尔效应传感器进行性能评估。开发了数学框架用于从磁场信号计算电流，并使用 MATLAB 实现了一个实时可视化仪表板。", "result": "GMR 传感器实现了 64.64% 至 91.49% 的相对准确率，大多数相位的准确率高于 80%。", "conclusion": "GMR 传感方法具有作为计算相电流基础的潜力，并且在现有基础上还有改进空间。"}}
{"id": "2602.09026", "categories": ["eess.IV", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.09026", "abs": "https://arxiv.org/abs/2602.09026", "authors": ["Charles Wood"], "title": "Operator-Based Information Theory for Imaging: Entropy, Capacity, and Irreversibility in Physical Measurement Systems", "comment": null, "summary": "Imaging systems are commonly described using resolution, contrast, and signal-to-noise ratio, but these quantities do not provide a general account of how physical transformations affect the flow of information. This paper introduces an operator-based formulation of information theory for imaging. The approach models the imaging chain as a composition of bounded operators acting on functions, and characterises information redistribution using the spectral properties of these operators. Three measures are developed. Operator entropy quantifies how an operator distributes energy across its singular spectrum. Operator information capacity describes the number of modes that remain recoverable above a noise-dependent threshold. An irreversibility index measures the information lost through suppression or elimination of modes and captures the accumulation of information loss under operator composition. The framework applies to linear, nonlinear, and stochastic operators and does not depend on the specific imaging modality. Analytical examples show how attenuation, blur, and sampling affect entropy, capacity, and irreversibility in different ways. The results provide a general structure for analysing the physical limits of imaging and form the basis for subsequent work on information geometry, spatiotemporal budgets, nonlinear channels, and reconstruction algorithms.", "AI": {"tldr": "本文提出一种基于算子和信息论的成像系统分析框架，通过算子谱特性量化信息在成像链中的流动与损失，并发展了算子熵、算子信息容量和不可逆性指数三个度量。", "motivation": "现有成像系统描述（如分辨率、对比度、信噪比）无法全面解释物理变换如何影响信息流，需要一个更通用的信息论框架。", "method": "将成像链建模为作用于函数的有界算子组合，利用算子谱特性（如奇异值谱）来描述信息分布和损失。开发了算子熵、算子信息容量和不可逆性指数。该框架适用于线性、非线性及随机算子。", "result": "算子熵量化能量在奇异谱上的分布，算子信息容量指示可恢复模式的数量，不可逆性指数衡量模式抑制或消除导致的信息损失。分析表明，衰减、模糊和采样对这些度量有不同的影响。", "conclusion": "该框架提供了一种通用的分析成像物理极限的方法，并为后续信息几何、时空预算、非线性信道和重建算法的研究奠定了基础。"}}
{"id": "2602.09206", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09206", "abs": "https://arxiv.org/abs/2602.09206", "authors": ["Jie Lu", "Peihao Yan", "Huacheng Zeng"], "title": "EExApp: GNN-Based Reinforcement Learning for Radio Unit Energy Optimization in 5G O-RAN", "comment": "Accepted by IEEE INFOCOM 2026", "summary": "With over 3.5 million 5G base stations deployed globally, their collective energy consumption (projected to exceed 131 TWh annually) raises significant concerns over both operational costs and environmental impacts. In this paper, we present EExAPP, a deep reinforcement learning (DRL)-based xApp for 5G Open Radio Access Network (O-RAN) that jointly optimizes radio unit (RU) sleep scheduling and distributed unit (DU) resource slicing. EExAPP uses a dual-actor-dual-critic Proximal Policy Optimization (PPO) architecture, with dedicated actor-critic pairs targeting energy efficiency and quality-of-service (QoS) compliance. A transformer-based encoder enables scalable handling of variable user equipment (UE) populations by encoding all-UE observations into fixed-dimensional representations. To coordinate the two optimization objectives, a bipartite Graph Attention Network (GAT) is used to modulate actor updates based on both critic outputs, enabling adaptive tradeoffs between power savings and QoS. We have implemented EExAPP and deployed it on a real-world 5G O-RAN testbed with live traffic, commercial RU and smartphones. Extensive over-the-air experiments and ablation studies confirm that EExAPP significantly outperforms existing methods in reducing the energy consumption of RU while maintaining QoS.", "AI": {"tldr": "本文提出了一种基于深度强化学习（DRL）的xApp，名为EExAPP，用于5G开放无线接入网（O-RAN），通过联合优化无线单元（RU）的休眠调度和分布式单元（DU）的资源切片，以降低能耗同时保证服务质量（QoS）。", "motivation": "全球5G基站数量庞大，能耗巨大，引发了运营成本和环境影响的担忧。需要一种方法来有效降低5G网络的能耗。", "method": "使用双Actor-双Critic的近端策略优化（PPO）架构，针对能效和QoS合规性设计了专门的Actor-Critic对。采用基于Transformer的编码器处理可变数量的用户设备（UE）观测。利用二分图注意力网络（GAT）来协调能效和QoS的权衡，实现自适应的策略更新。", "result": "在真实的5G O-RAN测试平台部署EExAPP，并进行了实际无线实验。结果表明，EExAPP在降低RU能耗方面显著优于现有方法，同时能有效维持QoS。", "conclusion": "EExAPP是一种有效的DRL驱动的xApp，能够成功地在5G O-RAN中实现能耗与QoS的联合优化，为构建更节能的5G网络提供了可行方案。"}}
{"id": "2602.09140", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09140", "abs": "https://arxiv.org/abs/2602.09140", "authors": ["Hamed Faghihian", "Arman Sargolzaei"], "title": "An Actor-Critic-Identifier Control Design for Increasing Energy Efficiency of Automated Electric Vehicles", "comment": "Accepted for American Control Conference (ACC 2026)", "summary": "Electric vehicles (EVs) are increasingly deployed, yet range limitations remain a key barrier. Improving energy efficiency via advanced control is therefore essential, and emerging vehicle automation offers a promising avenue. However, many existing strategies rely on indirect surrogates because linking power consumption to control inputs is difficult. We propose a neural-network (NN) identifier that learns this mapping online and couples it with an actor-critic reinforcement learning (RL) framework to generate optimal control commands. The resulting actor-critic-identifier architecture removes dependence on explicit models relating total power, recovered energy, and inputs, while maintaining accurate speed tracking and maximizing efficiency. Update laws are derived using Lyapunov stability analysis, and performance is validated in simulation. Compared to a traditional controller, the method increases total energy recovery by 12.84%, indicating strong potential for improving EV energy efficiency.", "AI": {"tldr": "提出了一种结合神经网络识别器和 Actor-Critic 强化学习框架的架构，用于在线学习电动汽车（EV）的功率消耗与控制输入之间的映射，从而实现最优控制并提高能量回收效率。", "motivation": "电动汽车续航里程的限制是推广应用的关键障碍，而提高能量效率的先进控制策略至关重要，但现有方法依赖于间接替代项，因为直接建立功率消耗与控制输入之间的联系很困难。", "method": "提出了一种神经网络（NN）识别器，用于在线学习功率消耗与控制输入之间的映射。将该识别器与 Actor-Critic 强化学习框架相结合，生成最优控制指令。使用 Lyapunov 稳定性分析推导了更新律。", "result": "所提出的 Actor-Critic-Identifier 架构消除了对显式模型（连接总功率、回收能量和输入）的依赖，同时保持了精确的速度跟踪和最大化效率。与传统控制器相比，该方法将总能量回收提高了 12.84%。", "conclusion": "所提出的结合神经网络识别器和 Actor-Critic 强化学习的架构能够有效地在线学习电动汽车的功率消耗模型，并生成最优控制指令，显著提高了能量回收效率，显示出提升电动汽车能效的巨大潜力。"}}
{"id": "2602.09147", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09147", "abs": "https://arxiv.org/abs/2602.09147", "authors": ["Janek Bevendorff", "Maik Fröbe", "André Greiner-Petter", "Andreas Jakoby", "Maximilian Mayerl", "Preslav Nakov", "Henry Plutz", "Martin Potthast", "Benno Stein", "Minh Ngoc Ta", "Yuxia Wang", "Eva Zangerle"], "title": "Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection", "comment": null, "summary": "The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated authorship scenarios, (2) Text Watermarking, a new task that aims to find new and benchmark the robustness of existing text watermarking schemes, (3) Multi-author Writing Style Analysis, a continued task that aims to find positions of authorship change, (4) Generative Plagiarism Detection, a continued task that targets source retrieval and text alignment between generated text and source documents, and (5) Reasoning Trajectory Detection, a new task that deals with source detection and safety detection of LLM-generated or human-written reasoning trajectories. As in previous years, PAN invites software submissions as easy-to-reproduce Docker containers for most of the tasks. Since PAN 2012, more than 1,100 submissions have been made this way via the TIRA experimentation platform.", "AI": {"tldr": "PAN 2026 研讨会旨在通过五项任务推进计算风格计量学和文本取证：生成式 AI 检测、文本水印、多作者写作风格分析、生成式抄袭检测以及推理轨迹检测。研讨会接受 Docker 容器形式的软件提交，并使用 TIRA 平台进行评估。", "motivation": "为了在计算风格计量学和文本取证领域取得进展，通过客观和可重现的评估来推动该领域的研究。", "method": "研讨会设置了五项具体任务，包括生成式 AI 检测（重点关注混合和混淆作者身份场景）、文本水印（评估新方案和现有方案的鲁棒性）、多作者写作风格分析（识别作者身份变更位置）、生成式抄袭检测（追踪生成文本与源文档的对应关系）以及推理轨迹检测（识别 LLM 生成或人类书写的推理轨迹的来源和安全性）。大多数任务接受以 Docker 容器形式提交的软件。", "result": "PAN 研讨会已成功举办多年，自 2012 年以来，通过 TIRA 实验平台已收到 1100 多份软件提交。", "conclusion": "PAN 2026 研讨会将继续通过五项具有挑战性的任务，包括两项新任务，来推进文本取证和计算风格计量学领域的研究，并继续采用 Docker 容器和 TIRA 平台作为软件评估和重现性评估的标准方法。"}}
{"id": "2602.09082", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09082", "abs": "https://arxiv.org/abs/2602.09082", "authors": ["Veuns-Team", ":", "Changlong Gao", "Zhangxuan Gu", "Yulin Liu", "Xinyu Qiu", "Shuheng Shen", "Yue Wen", "Tianyu Xia", "Zhenyu Xu", "Zhengwen Zeng", "Beitong Zhou", "Xingran Zhou", "Weizhi Chen", "Sunhao Dai", "Jingya Dou", "Yichen Gong", "Yuan Guo", "Zhenlin Guo", "Feng Li", "Qian Li", "Jinzhen Lin", "Yuqi Zhou", "Linchao Zhu", "Liang Chen", "Zhenyu Guo", "Changhua Meng", "Weiqiang Wang"], "title": "UI-Venus-1.5 Technical Report", "comment": null, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "AI": {"tldr": "本文提出了一种名为 UI-Venus-1.5 的统一端到端 GUI Agent，旨在实现通用性和高性能。它通过在中期训练、在线强化学习和模型合并等关键技术改进，显著提高了在 ScreenSpot-Pro、VenusBench-GD 和 AndroidWorld 等基准测试上的性能，并展示了在真实世界中文移动应用场景下的强大导航和指令执行能力。", "motivation": "当前 GUI Agent 在通用性和任务性能之间存在权衡，研究者旨在开发一个既通用又高性能的 GUI Agent，以满足真实世界应用的鲁棒性需求。", "method": "UI-Venus-1.5 引入了三个关键技术改进：1) 包含 100 亿 token 的中期训练阶段，以建立 GUI 语义；2) 在线强化学习，使用全轨迹回放来优化长时程动态导航；3) 通过模型合并，将领域特定模型（如 grounding、web 和 mobile）整合成一个统一的 GUI Agent。", "result": "UI-Venus-1.5 在 ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和 AndroidWorld（77.6%）等基准测试上取得了新的 state-of-the-art 性能，显著优于之前的强基线。此外，该模型在各种中文移动应用中表现出鲁棒的导航能力。", "conclusion": "UI-Venus-1.5 作为一个统一的 GUI Agent，通过创新的技术手段，成功地在通用性和实际任务性能上取得了突破，为自动化 GUI 交互的应用提供了强大的解决方案。"}}
{"id": "2602.09046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09046", "abs": "https://arxiv.org/abs/2602.09046", "authors": ["Mohammad Jabari", "Carmen Visconte", "Giuseppe Quaglia", "Med Amine Laribi"], "title": "Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm", "comment": null, "summary": "This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.", "AI": {"tldr": "该研究提出了一种基于可行静工作空间（FSW）的最优肌腱驱动连续机器人（TDCR）设计方法，通过遗传算法优化肌腱力以最大化机器人末端位置的欧氏范数，并在考虑外部载荷的情况下验证了其有效性。", "motivation": "为了在存在外部载荷的情况下，通过优化肌腱力来最大化肌腱驱动连续机器人（TDCR）的可行静工作空间（FSW）。", "method": "采用遗传算法优化技术，将肌腱力作为设计变量，以最大化机器人末端位置的欧氏范数来定义可行静工作空间。仿真过程中考虑了外部力矩和力载荷。", "result": "所提出的方法能够有效地识别最优肌腱力，从而最大化TDCR的可行静工作空间，即使在存在外部载荷的情况下也能实现。", "conclusion": "该研究成功地开发了一种基于FSW的最优TDCR设计方法，该方法通过遗传算法优化肌腱力，能够有效处理外部载荷并最大化机器人的工作空间。"}}
{"id": "2602.09112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09112", "abs": "https://arxiv.org/abs/2602.09112", "authors": ["Russ Webb", "Jason Ramapuram"], "title": "A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation", "comment": null, "summary": "What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \\$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\\% accuracy while GPT-5 has 95\\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.", "AI": {"tldr": "本文介绍了一个名为 Cadmus 的系统，该系统使用小型模型和整数虚拟机（VM）来研究程序补全。该系统成本低廉，允许研究人员深入分析模型行为，并证明其在某些任务上优于 GPT-5。", "motivation": "大型语言模型（LLMs）在程序合成研究中存在分布外问题、微调效果不明、分词影响未知以及计算和存储需求高等问题。作者希望开发一个成本更低、可控性更强、更易于进行细粒度分析的系统。", "method": "构建了一个包含整数虚拟机（VM）、真实程序数据集和自回归 Transformer 模型的 Cadmus 系统。该系统在不到 200 美元的计算成本下进行训练，并可用于研究程序补全、分布外表示、归纳推理和指令遵循。", "result": "Cadmus 模型在领域特定语言（DSL）的整数算术程序补全任务上取得了 100% 的准确率，优于 GPT-5 的 95%。此外，研究表明 GPT-5 在解决相同任务时会引入未知的先验知识，影响了对训练集与任务之间关系的深入理解。", "conclusion": "使用小型模型进行程序合成研究，如 Cadmus 系统所示，可以提供对模型行为更有效的控制和更低的实验成本，尤其是在需要深入理解训练集与任务关系的研究场景中。小型模型能够进行大规模模型难以负担的细粒度仪器化和调查。"}}
{"id": "2602.09150", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09150", "abs": "https://arxiv.org/abs/2602.09150", "authors": ["Andrey Gorbunov", "Youhong Chen", "Petr Vorobev", "Jin Ma", "Gregor Verbic"], "title": "Dynamic Passivity Multipliers for Plug-and-Play Stability Certificates of Converter-Dominated Grids", "comment": null, "summary": "Ensuring small-signal stability in power systems with a high share of inverter-based resources (IBRs) is hampered by two factors: (i) device and network parameters are often uncertain or completely unknown, and (ii) brute-force enumeration of all topologies is computationally intractable. These challenges motivate plug-and-play (PnP) certificates that verify stability locally yet hold globally. Passivity is an attractive property because it guarantees stability under feedback and network interconnections; however, strict passivity rarely holds for practical controllers such as Grid Forming Inverters (GFMs) employing P-Q droop. This paper extends the passivity condition by constructing a dynamic, frequency-dependent multiplier that enables PnP stability certification of each component based solely on its admittance, without requiring any modification to the controller design. The multiplier is parameterised as a linear filter whose coefficients are tuned under a passivity goal. Numerical results for practical droop gains confirm the PnP rules, substantially enlarging the certified stability region while preserving the decentralised, model-agnostic nature of passivity-based PnP tests.", "AI": {"tldr": "本研究提出一种动态、频率相关的乘数方法，用于在参数不确定且计算量大的情况下，对包含高比例逆变器（IBRs）的电力系统的即插即用（PnP）稳定性进行认证。该方法基于组件的导纳，无需修改控制器设计，可扩展稳定认证区域。", "motivation": "在高比例逆变器（IBRs）的电力系统中，参数不确定和计算复杂性阻碍了小信号稳定性的保证，这促使人们需要一种能够局部验证全局稳定性的即插即用（PnP）证书。", "method": "研究者构建了一个动态的、频率依赖的乘数，扩展了无源性条件。该乘数被参数化为线性滤波器，其系数根据无源性目标进行调整，允许基于每个组件的导纳进行PnP稳定性认证，而无需修改控制器设计。", "result": "数值结果表明，所提出的动态乘数方法成功地证实了PnP规则，显著扩大了认证的稳定性区域，同时保持了基于无源性的PnP测试的去中心化和模型无关特性。", "conclusion": "该研究提出了一种基于动态频率相关乘数的PnP稳定性认证方法，能够有效应对参数不确定性和计算复杂性问题，为高比例IBRs电力系统的稳定性分析提供了新的途径。"}}
{"id": "2602.09050", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09050", "abs": "https://arxiv.org/abs/2602.09050", "authors": ["Jiahao Qin"], "title": "SAS-Net: Scene-Appearance Separation Network for Robust Spatiotemporal Registration in Bidirectional Photoacoustic Microscopy", "comment": "21 pages, 6 figures, 3 tables", "summary": "High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional scanning enables rapid functional brain imaging but introduces severe spatiotemporal\n  misalignment from coupled scan-direction-dependent domain shift and geometric distortion. Conventional registration methods rely on brightness constancy, an assumption\n  violated under bidirectional scanning, leading to unreliable alignment. A unified scene-appearance separation framework is proposed to jointly address domain shift and\n  spatial misalignment. The proposed architecture separates domain-invariant scene content from domain-specific appearance characteristics, enabling cross-domain\n  reconstruction with geometric preservation. A scene consistency loss promotes geometric correspondence in the latent space, linking domain shift correction with spatial\n  registration within a single framework. For in vivo mouse brain vasculature imaging, the proposed method achieves normalized cross-correlation (NCC) of 0.961 and\n  structural similarity index (SSIM) of 0.894, substantially outperforming conventional methods. Ablation studies demonstrate that domain alignment loss is critical,\n  with its removal causing 82% NCC reduction (0.961 to 0.175), while scene consistency and cycle consistency losses provide complementary regularization for optimal\n  performance. The method achieves 11.2 ms inference time per frame (86 fps), substantially exceeding typical OR-PAM acquisition rates and enabling real-time processing.\n  These results suggest that the proposed framework enables robust high-speed bidirectional OR-PAM for reliable quantitative and longitudinal functional imaging. The code will be publicly available at https://github.com/D-ST-Sword/SAS-Net", "AI": {"tldr": "提出了一种统一的场景外观分离框架，用于解决双向扫描OR-PAM中的配准问题，通过联合解决域漂移和空间错位，实现了高精度、实时的脑血管成像。", "motivation": "传统的OR-PAM双向扫描会引起严重的空时错配，而现有的配准方法依赖于亮度恒定假设，在该场景下失效，导致配准不可靠。", "method": "提出了一种统一的场景外观分离框架，该框架将域不变的场景内容与域相关的外观特征分离开来，从而实现跨域重建和几何保持。通过场景一致性损失在潜在空间中促进几何对应，将域漂移校正与空间配准结合在一个框架内。", "result": "在小鼠脑血管成像中，该方法实现了0.961的归一化互相关（NCC）和0.894的结构相似性指数（SSIM），显著优于传统方法。消融研究表明，域对齐损失至关重要，移除后NCC降低82%。该方法每帧推理时间为11.2 ms（86 fps），实现了实时处理。", "conclusion": "所提出的框架能够实现鲁棒的高速双向OR-PAM，为可靠的定量和纵向功能成像提供了可能。"}}
{"id": "2602.09121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09121", "abs": "https://arxiv.org/abs/2602.09121", "authors": ["Rémi Grzeczkowicz", "Eric Soriano", "Ali Janati", "Miyu Zhang", "Gerard Comas-Quiles", "Victor Carballo Araruna", "Aneesh Jonelagadda"], "title": "Uncertainty-Aware Multimodal Emotion Recognition through Dirichlet Parameterization", "comment": "8 pages, 3 figures", "summary": "In this work, we present a lightweight and privacy-preserving Multimodal Emotion Recognition (MER) framework designed for deployment on edge devices. To demonstrate framework's versatility, our implementation uses three modalities - speech, text and facial imagery. However, the system is fully modular, and can be extended to support other modalities or tasks. Each modality is processed through a dedicated backbone optimized for inference efficiency: Emotion2Vec for speech, a ResNet-based model for facial expressions, and DistilRoBERTa for text. To reconcile uncertainty across modalities, we introduce a model- and task-agnostic fusion mechanism grounded in Dempster-Shafer theory and Dirichlet evidence. Operating directly on model logits, this approach captures predictive uncertainty without requiring additional training or joint distribution estimation, making it broadly applicable beyond emotion recognition. Validation on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS and CREMA-D) show that our method achieves competitive accuracy while remaining computationally efficient and robust to ambiguous or missing inputs. Overall, the proposed framework emphasizes modularity, scalability, and real-world feasibility, paving the way toward uncertainty-aware multimodal systems for healthcare, human-computer interaction, and other emotion-informed applications.", "AI": {"tldr": "提出了一种轻量级、隐私保护的多模态情感识别（MER）框架，可在边缘设备上部署，使用语音、文本和面部图像，并采用基于Dempster-Shafer理论的融合机制来处理不确定性，在多个数据集上实现了有竞争力的准确率和高计算效率。", "motivation": "为了在边缘设备上实现轻量级、隐私保护的多模态情感识别，并解决跨模态的不确定性问题。", "method": "使用三个独立的骨干网络分别处理语音（Emotion2Vec）、面部表情（ResNet）和文本（DistilRoBERTa）。采用基于Dempster-Shafer理论和Dirichlet证据的无模型、无任务融合机制，直接在模型logits上操作，以整合预测不确定性。", "result": "在五个基准数据集（eNTERFACE05, MEAD, MELD, RAVDESS, CREMA-D）上，该方法在保持计算效率和对模糊或缺失输入的鲁棒性的同时，实现了具有竞争力的准确率。", "conclusion": "提出的框架具有模块化、可扩展性和实际可行性，为医疗保健、人机交互等情感感知应用中的不确定性感知多模态系统铺平了道路。"}}
{"id": "2602.09076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09076", "abs": "https://arxiv.org/abs/2602.09076", "authors": ["Nhat Le", "Daeun Song", "Xuesu Xiao"], "title": "Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception", "comment": "Accepted to IEEE ICRA 2026", "summary": "Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.", "AI": {"tldr": "该研究通过结合2D/3D骨骼关键点和生物力学线索，显著提高了人类轨迹预测的准确性，尤其强调了下半身3D关键点的重要性。", "motivation": "现有方法多将人类视为质点，未能充分利用人类的身体信息来提高轨迹预测的准确性，尤其是在拥挤环境中社会机器人的导航需求日益增长的背景下。", "method": "研究者系统地评估了2D和3D骨骼关键点以及衍生的生物力学线索作为输入。实验在JRDB数据集和新的360度全景视频数据集上进行，以评估不同输入特征的预测效用。", "result": "研究发现，专注于下半身3D关键点可将平均位移误差（ADE）降低13%。将3D关键点输入与相应的生物力学线索结合，可进一步提升1-4%的性能。即使使用从全景图像中提取的2D关键点，性能提升依然存在。", "conclusion": "人类轨迹预测可以通过关注其下半身骨骼信息（尤其是3D关键点和生物力学线索）得到显著改进。全景单目视觉也能捕获到对运动预测有用的信息。研究结果为设计能够有效预测人类运动的社会机器人提供了可操作的见解。"}}
{"id": "2602.09084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09084", "abs": "https://arxiv.org/abs/2602.09084", "authors": ["Ruijie Ye", "Jiayi Zhang", "Zhuoxin Liu", "Zihao Zhu", "Siyuan Yang", "Li Li", "Tianfu Fu", "Franck Dernoncourt", "Yue Zhao", "Jiacheng Zhu", "Ryan Rossi", "Wenhao Chai", "Zhengzhong Tu"], "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling", "comment": "Project Website: agent-banana.github.io", "summary": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.", "AI": {"tldr": "本文提出了Agent Banana框架，用于解决专业级指令图像编辑中的三大挑战：过度编辑、多轮编辑中的对象失真以及评估分辨率不足。该框架通过Context Folding和Image Layer Decomposition机制，实现了高保真、对象感知和深思熟虑的编辑，并在新构建的4K分辨率基准HDD-Bench上取得了优异的多轮一致性和背景保真度。", "motivation": "现有指令图像编辑模型在专业工作流中存在三大挑战：1. 过度编辑导致偏离用户意图；2. 单轮编辑模型难以处理多轮交互，可能导致对象失真；3. 标准评估分辨率（约1K）与专业工作流（如4K）不匹配。", "method": "提出了Agent Banana，一个分层的代理规划-执行框架。关键机制包括：1. Context Folding：压缩长交互历史为结构化记忆，实现长期稳定控制；2. Image Layer Decomposition：执行局部层编辑，保留非目标区域，并支持原生分辨率输出。同时构建了HDD-Bench基准，包含高分辨率（4K）、对话式编辑、可验证的逐步目标。", "result": "在HDD-Bench上，Agent Banana实现了最佳的多轮一致性和背景保真度（IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12），同时在指令遵循方面也表现具有竞争力。在标准单轮编辑基准上也取得了强劲性能。", "conclusion": "Agent Banana框架能够实现可靠的、专业级的代理图像编辑，并有望集成到实际工作流程中，解决了现有技术在专业工作流中面临的挑战。"}}
{"id": "2602.09484", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.09484", "abs": "https://arxiv.org/abs/2602.09484", "authors": ["Liming Liu", "Jiangkai Wu", "Xinggong Zhang"], "title": "Smaller is Better: Generative Models Can Power Short Video Preloading", "comment": "6 pages, 7 figures, to appear in ICC 2026", "summary": "Preloading is widely used in short video platforms to minimize playback stalls by downloading future content in advance. However, existing strategies face a tradeoff. Aggressive preloading reduces stalls but wastes bandwidth, while conservative strategies save data but increase the risk of playback stalls. This paper presents PromptPream, a computation powered preloading paradigm that breaks this tradeoff by using local computation to reduce bandwidth demand. Instead of transmitting pixel level video chunks, PromptPream sends compact semantic prompts that are decoded into high quality frames using generative models such as Stable Diffusion. We propose three core techniques to enable this paradigm: (1) a gradient based prompt inversion method that compresses frames into small sets of compact token embeddings; (2) a computation aware scheduling strategy that jointly optimizes network and compute resource usage; and (3) a scalable searching algorithm that addresses the enlarged scheduling space introduced by scheduler. Evaluations show that PromptStream reduces both stalls and bandwidth waste by over 31%, and improves Quality of Experience (QoE) by 45%, compared to traditional strategies.", "AI": {"tldr": "PromptPream 是一种新的视频预加载范式，它利用本地计算和生成模型（如 Stable Diffusion）通过发送紧凑的语义提示而不是像素级视频块来减少带宽消耗，从而解决了预加载在减少卡顿和节省带宽之间的权衡问题。", "motivation": "现有的视频预加载策略在减少播放卡顿（需要激进的预加载）和节省带宽（需要保守的预加载）之间存在固有的权衡。本文旨在打破这种权衡。", "method": "PromptPream 提出了一种计算驱动的预加载范式，其核心技术包括：1. 基于梯度的提示反演方法，将帧压缩为紧凑的 token 嵌入；2. 计算感知调度策略，联合优化网络和计算资源；3. 缩放搜索算法，处理由调度器引入的扩大调度空间。", "result": "与传统策略相比，PromptPream 将卡顿和带宽浪费减少了 31% 以上，并将用户体验质量（QoE）提高了 45%。", "conclusion": "PromptPream 成功地通过利用本地计算和生成模型，在不牺牲服务质量的情况下显著降低了视频预加载的带宽需求，从而有效解决了预加载的权衡问题。"}}
{"id": "2602.09269", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09269", "abs": "https://arxiv.org/abs/2602.09269", "authors": ["Jaeyoon Choi", "Nia Nixon"], "title": "Measuring Inclusion in Interaction: Inclusion Analytics for Human-AI Collaborative Learning", "comment": null, "summary": "Inclusion, equity, and access are widely valued in AI and education, yet are often assessed through coarse sample descriptors or post-hoc self-reports that miss how inclusion is shaped moment by moment in collaborative problem solving (CPS). In this proof-of-concept paper, we introduce inclusion analytics, a discourse-based framework for examining inclusion as a dynamic, interactional process in CPS. We conceptualize inclusion along three complementary dimensions -- participation equity, affective climate, and epistemic equity -- and demonstrate how these constructs can be made analytically visible using scalable, interaction-level measures. Using both simulated conversations and empirical data from human-AI teaming experiments, we illustrate how inclusion analytics can surface patterns of participation, relational dynamics, and idea uptake that remain invisible to aggregate or post-hoc evaluations. This work represents an initial step toward process-oriented approaches to measuring inclusion in human-AI collaborative learning environments.", "AI": {"tldr": "本文提出了一种名为“包容性分析”（inclusion analytics）的基于话语的框架，用于动态、交互式地衡量人工智能与教育协作解决问题（CPS）过程中的包容性，关注参与公平、情感氛围和认知公平三个维度。", "motivation": "现有的包容性、公平性和可及性评估方法（如粗略的样本描述或事后自我报告）无法捕捉协作解决问题过程中包容性如何被实时塑造，因此需要一种更动态、交互式的方法。", "method": "提出并演示了“包容性分析”框架，该框架基于话语分析，将包容性概念化为参与公平、情感氛围和认知公平三个维度，并开发了可扩展的、面向交互的度量方法来量化这些维度。使用模拟对话和人类-AI团队实验的实证数据进行说明。", "result": "包容性分析能够揭示在聚合或事后评估中无法察觉的参与模式、关系动态和想法采纳情况，展示了其在检测细粒度包容性模式方面的有效性。", "conclusion": "包容性分析是一个初步的、面向过程的度量方法，旨在更好地理解和评估人类-AI协作学习环境中的包容性，为未来研究提供了新的方向。"}}
{"id": "2602.09398", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09398", "abs": "https://arxiv.org/abs/2602.09398", "authors": ["Hansini Ramachandran", "Bhaskar Krishnamachari"], "title": "Escaping Local Minima: A Finite-Time Markov Chain Analysis of Constant-Temperature Simulated Annealing", "comment": "6 pages, 10 figures", "summary": "Simulated Annealing (SA) is a widely used stochastic optimization algorithm, yet much of its theoretical understanding is limited to asymptotic convergence guarantees or general spectral bounds. In this paper, we develop a finite-time analytical framework for constant-temperature SA by studying a piecewise linear cost function that permits exact characterization. We model SA as a discrete-state Markov chain and first derive a closed-form expression for the expected time to escape a single linear basin in a one-dimensional landscape. We show that this expression also accurately predicts the behavior of continuous-state searches up to a constant scaling factor, which we analyze empirically and explain via variance matching, demonstrating convergence to a factor of sqrt(3) in certain regimes.\n  We then extend the analysis to a two-basin landscape containing a local and a global optimum, obtaining exact expressions for the expected time to reach the global optimum starting from the local optimum, as a function of basin geometry, neighborhood radius, and temperature. Finally, we demonstrate how the predicted basin escape time can be used to guide the design of a simple two-temperature switching strategy.", "AI": {"tldr": "本文提出了恒温模拟退火（SA）的有限时间分析框架，通过研究分段线性成本函数，推导了跳出单个线性盆地的精确期望时间，并将其扩展到包含局部和全局最优值的双盆地景观。该框架还可用于指导双温切换策略的设计。", "motivation": "现有的模拟退火（SA）理论理解大多局限于渐近收敛保证或一般的谱界，缺乏对有限时间内的行为分析。", "method": "1. 将SA建模为离散状态马尔可夫链。 2. 研究分段线性成本函数，推导出跳出单盆地的精确期望时间。 3. 通过方差匹配分析，解释连续状态搜索与离散状态的关联，并预测比例因子。 4. 扩展到双盆地景观，推导出到达全局最优值的精确期望时间。 5. 利用盆地逃逸时间预测来设计双温切换策略。", "result": "1. 得到了跳出单盆地的期望时间的闭式表达式，该表达式可准确预测连续状态搜索的行为。 2. 预测的连续状态搜索与离散状态的比例因子在某些情况下收敛到 sqrt(3)。 3. 得到了到达全局最优值的期望时间的精确表达式。 4. 提出的双温切换策略能够指导SA的有效设计。", "conclusion": "本文为恒温模拟退火（SA）提供了一个有限时间分析框架，并通过对简单景观的精确分析，揭示了其有限时间内的行为特性，为SA算法的设计和优化提供了理论指导。"}}
{"id": "2602.09146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09146", "abs": "https://arxiv.org/abs/2602.09146", "authors": ["Saar Huberman", "Kfir Goldberg", "Or Patashnik", "Sagie Benaim", "Ron Mokady"], "title": "SemanticMoments: Training-Free Motion Similarity via Third Moment Features", "comment": null, "summary": "Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.", "AI": {"tldr": "本研究提出了一种名为SemanticMoments的无训练方法，通过计算预训练语义模型中特征的高阶时间统计量来解决基于语义运动的视频检索问题，并在新的基准测试中取得了优于现有方法的性能。", "motivation": "现有视频表示方法过度依赖静态外观和场景上下文，忽视了运动动力学，而传统的以运动为中心的输入（如光流）又缺乏语义理解能力。这种偏见导致在基于语义运动的视频检索任务上表现不佳。", "method": "引入SimMotion基准测试集（包含合成和真实世界数据）。提出SemanticMoments方法，该方法无需训练，通过计算预训练语义模型输出的特征的高阶时间统计量（矩）来表示视频。", "result": "在SimMotion基准测试中，SemanticMoments方法在检索性能上一致优于现有的基于RGB、光流和文本监督的方法。", "conclusion": "时间统计量在语义特征空间中为以运动为中心的视频理解提供了一个可扩展且感知基础的方法，能够有效解决现有模型在运动动力学理解上的不足。"}}
{"id": "2602.09276", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09276", "abs": "https://arxiv.org/abs/2602.09276", "authors": ["Archiki Prasad", "Mandar Joshi", "Kenton Lee", "Mohit Bansal", "Peter Shaw"], "title": "Effective Reasoning Chains Reduce Intrinsic Dimensionality", "comment": "20 pages, 3 figures", "summary": "Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.", "AI": {"tldr": "本研究提出使用内在维度来量化链式思考（CoT）推理策略的有效性。研究发现，有效的CoT策略能够降低任务的内在维度，并与模型在各种数据集上的泛化能力呈负相关。", "motivation": "尽管链式思考（CoT）显著提升了语言模型在复杂推理任务上的表现，但其促进泛化的具体机制仍不明确。现有解释（如增加测试计算或结构指导）与泛化之间的量化联系难以建立。", "method": "研究者们引入了“内在维度”作为衡量推理链有效性的量化指标，它表示达到给定准确率所需的最小模型维度。通过在固定模型架构下改变任务表述（采用不同推理策略），研究者们观察内在维度的变化。", "result": "研究表明，有效的推理策略一致地降低了任务的内在维度。在GSM8K数据集上，使用Gemma-1B和4B模型进行验证，发现推理策略的内在维度与其在分布内和分布外数据上的泛化性能之间存在强烈的负相关。", "conclusion": "本研究提出的内在维度是一种分析推理过程的新量化指标。有效的推理链通过用更少的参数更好地压缩任务来促进学习，从而提高泛化能力。"}}
{"id": "2602.09123", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09123", "abs": "https://arxiv.org/abs/2602.09123", "authors": ["Jackson Habala", "Gabriel B. Margolis", "Tianyu Wang", "Pratyush Bhatt", "Juntao He", "Naheel Naeem", "Zhaochen Xu", "Pulkit Agrawal", "Daniel I. Goldman", "Di Luo", "Baxi Chong"], "title": "Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality", "comment": "10 pages, 7 figures. arXiv admin note: text overlap with arXiv:2302.03019", "summary": "Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.", "AI": {"tldr": "本研究提出了一种基于几何力学和统计力学自旋模型对偶性的新控制框架，用于发现多足机器人（特别是六足机器人）的运动策略，成功识别出一种改进了传统步态的非对称运动方式，并验证了其在模拟和物理实验中的有效性。", "motivation": "现有足式机器人研究主要集中在双足或四足机器人，忽视了多足机器人提升运动性能的潜力。现有控制方法因高维接触协调问题而难以充分利用多足系统的优势，通常沿用低维步态。因此，需要一种新的控制框架来探索和利用多足系统带来的新对称性和控制机会。", "method": "研究者使用几何力学将富含接触的运动规划问题转化为图优化问题。同时，引入统计力学的自旋模型对偶性框架，利用对称性破缺来指导最优步态的重组。", "result": "该方法成功为六足机器人发现了一种非对称的运动策略，比传统步态的运动速度提高了50%（达到每周期0.61个身体长度）。这种非对称性体现在控制层面（身体朝向在快顺时针和慢逆时针转向阶段之间不对称振荡）和硬件层面（同侧两腿可不被驱动，并用刚性部件替代而不影响性能）。", "conclusion": "提出的基于几何力学和自旋模型对偶性的框架能够有效地发现多足机器人的新型控制结构，并识别出由高维具身系统中的对称性重构产生的新颖运动行为。这种方法为多足机器人控制研究提供了新的理论基础和实践指导。"}}
{"id": "2602.09138", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09138", "abs": "https://arxiv.org/abs/2602.09138", "authors": ["Haitao Jiang", "Lin Ge", "Hengrui Cai", "Rui Song"], "title": "PABU: Progress-Aware Belief Update for Efficient LLM Agents", "comment": null, "summary": "Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.", "AI": {"tldr": "提出了一种名为PABU（Progress-Aware Belief Update）的信念状态框架，通过显式建模任务进度并选择性地保留历史信息，来紧凑地表示LLM智能体的状态，从而提高任务完成率并降低推理成本。", "motivation": "现有的LLM智能体通常将完整的动作-观察历史作为决策的条件，这引入了任务无关信息，导致冗余动作和更高的推理成本。", "method": "PABU框架在每一步预测相对于上一轮的任务进度，并决定是否存储新的交互信息。未来的决策仅基于保留的子集。该方法显式地建模任务进度，并选择性地保留相关信息。", "result": "在AgentGym基准的八个环境中，PABU实现了81.0%的任务完成率，比使用完整历史信念的先前最先进模型高出23.9%。同时，平均交互步骤减少到9.5步，降低了26.9%。", "conclusion": "显式的进度预测和选择性保留对于PABU实现鲁棒的信念学习和性能提升至关重要，表明PABU能够有效解决LLM智能体因历史信息冗余带来的问题。"}}
{"id": "2602.09414", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09414", "abs": "https://arxiv.org/abs/2602.09414", "authors": ["Nazanin S. Hashkavaei", "Abhijit Dongare", "Neon Srinivasu", "Amit K. Sanyal"], "title": "Finite-time Stable Pose Estimation on TSE(3) using Point Cloud and Velocity Sensors", "comment": "17 pages, 8 figures, submitted to Automatica", "summary": "This work presents a finite-time stable pose estimator (FTS-PE) for rigid bodies undergoing rotational and translational motion in three dimensions, using measurements from onboard sensors that provide position vectors to inertially-fixed points and body velocities. The FTS-PE is a full-state observer for the pose (position and orientation) and velocities and is obtained through a Lyapunov analysis that shows its stability in finite time and its robustness to bounded measurement noise. Further, this observer is designed directly on the state space, the tangent bundle of the Lie group of rigid body motions, SE(3), without using local coordinates or (dual) quaternion representations. Therefore, it can estimate arbitrary rigid body motions without encountering singularities or the unwinding phenomenon and be readily applied to autonomous vehicles. A version of this observer that does not need translational velocity measurements and uses only point clouds and angular velocity measurements from rate gyros, is also obtained. It is discretized using the framework of geometric mechanics for numerical and experimental implementations. The numerical simulations compare the FTS-PE with a dual-quaternion extended Kalman filter and our previously developed variational pose estimator (VPE). The experimental results are obtained using point cloud images and rate gyro measurements obtained from a Zed 2i stereo depth camera sensor. These results validate the stability and robustness of the FTS-PE.", "AI": {"tldr": "本文提出了一种用于三维刚体运动的有限时间稳定姿态估计器（FTS-PE），该估计器利用惯性固定点的位置向量和本体速度测量，并直接在SE(3)流形上设计，避免了奇异点和解缠绕问题，并通过数值模拟和实验验证了其稳定性和鲁棒性。", "motivation": "为了实现对三维刚体运动的精确姿态估计，并克服现有方法（如基于四元数的EKF）中存在的奇异点和解缠绕问题，从而能够处理任意刚体运动，特别是对于自主车辆的应用。", "method": "利用李群SE(3)上的李代数so(3)和切空间分析，设计了一个基于李雅普诺夫稳定性理论的全状态观测器，可以直接在流形上进行姿态和速度估计。还提出了一种不需要平移速度测量的变体。最后，通过几何力学框架进行离散化，以便数值和实验实现。", "result": "FTS-PE在有限时间内是稳定的，并且对有界测量噪声具有鲁棒性。与双四元数扩展卡尔曼滤波器和先前的变分姿态估计器（VPE）相比，数值模拟结果显示了其性能。实验结果（使用Zed 2i立体深度相机）验证了FTS-PE的稳定性和鲁棒性。", "conclusion": "所提出的FTS-PE是一种在有限时间内稳定且对噪声鲁棒的姿态估计器，其直接在SE(3)流形上设计，避免了传统方法的局限性，能够处理任意刚体运动，并已通过数值模拟和实验得到验证，适用于自主车辆等应用。"}}
{"id": "2602.09312", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09312", "abs": "https://arxiv.org/abs/2602.09312", "authors": ["Shu-Ting Pi", "Pradeep Bagavan", "Yejia Li", "Disha", "Qun Liu"], "title": "Don't Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention", "comment": "EMNLP 2024: Industry Track; 8 pages, 2 figures, 1 table", "summary": "Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resources. In this paper, we present a topic continuity model aimed at assessing whether a response aligns with the initial conversation topic. Our model is built upon the expansion of the corresponding natural language understanding (NLU) model into quantifiable terms using a Naive Bayes approach. Subsequently, we have introduced an attention mechanism and logarithmic nonlinearity to enhance its capability to capture topic continuity. This approach allows us to convert the NLU model into an interpretable analytical formula. In contrast to many NLU models constrained by token limits, our proposed model can seamlessly handle conversations of any length with linear time complexity. Furthermore, the attention mechanism significantly improves the model's ability to identify topic continuity in complex conversations. According to our experiments, our model consistently outperforms traditional methods, particularly in handling lengthy and intricate conversations. This unique capability offers us an opportunity to ensure the responsible and interpretable use of LLMs.", "AI": {"tldr": "本研究提出了一种基于朴素贝叶斯、注意力机制和对数非线性的主题连续性模型，用于评估LLM聊天机器人的回复是否与初始对话主题一致，并能处理任意长度的对话。", "motivation": "大型语言模型（LLM）在商业场景中作为聊天机器人时，维持对话主题的连续性是一个挑战，不连续的话题会导致用户体验下降和计算资源浪费。", "method": "研究基于扩展的自然语言理解（NLU）模型，采用朴素贝叶斯方法将其量化。引入了注意力机制和对数非线性来增强模型捕捉主题连续性的能力，从而将NLU模型转化为可解释的分析公式。该模型具有线性时间复杂度，不受限于Token数量。", "result": "该模型能够处理任意长度的对话，其注意力机制显著提高了模型在复杂对话中识别主题连续性的能力。实验结果表明，与传统方法相比，该模型在处理冗长和复杂的对话方面表现更优。", "conclusion": "所提出的主题连续性模型能够有效确保LLM负责任和可解释的使用，尤其是在处理长对话和复杂对话时，能够显著提升对话的连贯性。"}}
{"id": "2602.09154", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.09154", "abs": "https://arxiv.org/abs/2602.09154", "authors": ["Andrea Filiberto Lucas", "Dylan Seychell"], "title": "A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video", "comment": "7 pages, 5 figures. Accepted for publication at the 2026 IEEE Conference on Artificial Intelligence (CAI)", "summary": "The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.\n  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.", "AI": {"tldr": "该研究提出了一种自动检测和提取新闻视频中个人姓名的框架，与生成式多模态方法相比，该框架在可解释性和可审计性方面表现更佳，尽管原始准确率略低。", "motivation": "新闻视频内容激增，需要可靠的方法来提取屏幕信息，但手动索引因图形布局、字体和平台设计的多样性而变得不切实际。", "method": "提出了一种可解释、模块化的提取流程，该流程在一个精心策划和平衡的标注帧语料库上进行训练，以捕捉当代新闻图形的多样性。该流程与生成式多模态方法进行了对比评估。", "result": "提出的检测器实现了 95.8% 的 mAP@0.5。虽然生成式系统在原始准确率上略有优势（F1：84.18% vs 77.08%），但它们缺乏新闻和分析所需的透明数据溯源。提出的流程实现了 79.9% 的精确率和 74.4% 的召回率，避免了错误信息，并提供了完整的流程可追溯性。用户研究表明 59% 的受访者在快节奏的广播中难以阅读屏幕上的姓名。", "conclusion": "该研究建立了一个方法严谨且可解释的混合多模态信息提取基准，适用于现代新闻媒体。虽然生成式方法在原始准确率上略有优势，但提出的流程在可审计性和避免错误信息方面更胜一筹，这对于新闻和分析至关重要。"}}
{"id": "2602.09153", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.09153", "abs": "https://arxiv.org/abs/2602.09153", "authors": ["Nicholas Pfaff", "Thomas Cohn", "Sergey Zakharov", "Rick Cory", "Russ Tedrake"], "title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes", "comment": "Project page: https://scenesmith.github.io/", "summary": "Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stages$\\unicode{x2013}$from architectural layout to furniture placement to small object population$\\unicode{x2013}$each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.", "AI": {"tldr": "SceneSmith 是一个基于 VLM 代理的框架，可以根据自然语言生成高度逼真且物理复杂的室内模拟环境，用于机器人训练和评估。", "motivation": "现有室内机器人训练和评估的模拟环境在多样性和物理复杂性方面存在不足，生成场景过于稀疏，缺乏实际室内空间的密集杂乱、可动家具和物理属性。", "method": "SceneSmith 采用分层代理框架，通过设计师、评论员和协调员 VLM 代理的交互，从建筑布局到家具放置再到小物体填充，逐步构建场景。该框架集成了文本到 3D 合成（静态物体）、数据集检索（可动家具）和物理属性估算。", "result": "SceneSmith 生成的物体数量是先前方法的 3-6 倍，物体间碰撞率低于 2%，96% 的物体在物理模拟下保持稳定。在用户研究中，其生成的场景的真实感和提示忠实度评分为 92% 和 91%，优于基线方法。", "conclusion": "SceneSmith 能够生成高质量、物理准确且符合用户要求的模拟室内环境，为机器人策略的自动评估提供了一个端到端的流水线。"}}
{"id": "2602.09427", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09427", "abs": "https://arxiv.org/abs/2602.09427", "authors": ["Luigi Romano", "Ole Morten Aamo", "Jan Åslund", "Erik Frisk"], "title": "Lateral tracking control of all-wheel steering vehicles with intelligent tires", "comment": "17 pages, 12 figures. Under review at IEEE Transactions on Intelligent Vehicles", "summary": "The accurate characterization of tire dynamics is critical for advancing control strategies in autonomous road vehicles, as tire behavior significantly influences handling and stability through the generation of forces and moments at the tire-road interface. Smart tire technologies have emerged as a promising tool for sensing key variables such as road friction, tire pressure, and wear states, and for estimating kinematic and dynamic states like vehicle speed and tire forces. However, most existing estimation and control algorithms rely on empirical correlations or machine learning approaches, which require extensive calibration and can be sensitive to variations in operating conditions. In contrast, model-based techniques, which leverage infinite-dimensional representations of tire dynamics using partial differential equations (PDEs), offer a more robust approach. This paper proposes a novel model-based, output-feedback lateral tracking control strategy for all-wheel steering vehicles that integrates distributed tire dynamics with smart tire technologies. The primary contributions include the suppression of micro-shimmy phenomena at low speeds and path-following via force control, achieved through the estimation of tire slip angles, vehicle kinematics, and lateral tire forces. The proposed controller and observer are based on formulations using ODE-PDE systems, representing rigid body dynamics and distributed tire behavior. This work marks the first rigorous control strategy for vehicular systems equipped with distributed tire representations in conjunction with smart tire technologies.", "AI": {"tldr": "本文提出了一种基于模型、输出反馈的全轮转向车辆横向跟踪控制策略，该策略集成了分布式轮胎动力学和智能轮胎技术，能够抑制低速下的微摇现象并通过力控制实现路径跟踪。", "motivation": "为了提高自动驾驶车辆的控制策略，精确表征轮胎动力学至关重要。现有的基于经验或机器学习的方法需要大量校准且对工况变化敏感，而基于无限维轮胎动力学模型（偏微分方程）的模型方法更为鲁棒。", "method": "本文提出了一种新的模型预测控制策略，结合了使用常微分方程-偏微分方程（ODE-PDE）系统表示的刚体动力学和分布式轮胎行为，以及智能轮胎技术。控制器和观测器基于此集成模型。", "result": "该策略成功地抑制了低速下的微摇现象，并通过力控制实现了路径跟踪，同时估计了轮胎滑移角、车辆运动学和横向轮胎力。", "conclusion": "这是首次将分布式轮胎动力学与智能轮胎技术严格结合的车辆系统控制策略，证明了其在抑制微摇和实现精确路径跟踪方面的有效性。"}}
{"id": "2602.09159", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.09159", "abs": "https://arxiv.org/abs/2602.09159", "authors": ["Yichen Wu", "Yujin Oh", "Sangjoon Park", "Kailong Fan", "Dania Daye", "Hana Farzaneh", "Xiang Li", "Raul Uppot", "Quanzheng Li"], "title": "CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective", "comment": "9 pages, 3 figures", "summary": "Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.", "AI": {"tldr": "CoMMa是一个去中心化的LLM-Agent框架，用于肿瘤学决策支持，通过博弈论目标协调专家，并使用确定性嵌入投影进行贡献度评估，从而实现可解释、稳定且准确的决策。", "motivation": "现有的多Agent框架在处理动态、异构的患者数据以支持肿瘤学决策方面存在不足，需要更鲁棒、可解释和稳定的方法。", "method": "提出CoMMa框架，采用去中心化LLM-Agent架构，专家在分区证据上操作，并通过博弈论目标进行协调。关键在于使用确定性嵌入投影来近似贡献度分配，估计每个Agent的边际效用，从而实现明确的证据归因。", "result": "在肿瘤学基准测试（包括真实世界的多学科肿瘤委员会数据集）上，CoMMa相比数据中心化和基于角色的多Agent基线，实现了更高的准确性和更稳定的性能。", "conclusion": "CoMMa通过其贡献度感知和确定性的信用分配机制，能够生成可解释、数学上合理且性能优越的肿瘤学决策路径，克服了现有方法的局限性。"}}
{"id": "2602.09787", "categories": ["eess.IV", "physics.app-ph", "physics.bio-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.09787", "abs": "https://arxiv.org/abs/2602.09787", "authors": ["Sooyong Chae", "Dani Giammattei", "Ajmal Ajmal", "Junzhu Pei", "Amanda Sanchez", "Tananant Boonya-ananta", "Andres Rodriguez", "Tatiana Novikova", "Jessica Ramella-Roman"], "title": "Intensity-based Segmentation of Tissue Images Using a U-Net with a Pretrained ResNet-34 Encoder: Application to Mueller Microscopy", "comment": "9 pages, 7 figures, 1 table", "summary": "Manual annotation of the images of thin tissue sections remains a time-consuming step in Mueller microscopy and limits its scalability. We present a novel automated approach using only the total intensity M11 element of the Mueller matrix as an input to a U-Net architecture with a pretrained ResNet-34 encoder. The network was trained to distinguish four classes in the images of murine uterine cervix sections: background, internal os, cervical tissue, and vaginal wall. With only 70 cervical tissue sections, the model achieved 89.71% pixel accuracy and 80.96% mean tissue Dice coefficient on the held-out test dataset. Transfer learning from ImageNet enables accurate segmentation despite limited size of training dataset typical of specialized biomedical imaging. This intensity-based framework requires minimal preprocessing and is readily extensible to other imaging modalities and tissue types, with publicly available graphical annotation tools for practical deployment.", "AI": {"tldr": "本文提出了一种使用U-Net和预训练ResNet-34编码器的自动化方法，仅利用穆勒矩阵的M11总强度元素来分割小鼠子宫颈组织图像，实现了90%的像素准确率和80%的Dice系数。", "motivation": "手动标注穆勒显微镜图像中的薄组织切片非常耗时，限制了其应用规模。", "method": "使用U-Net架构，预训练的ResNet-34作为编码器，以穆勒矩阵的M11总强度元素作为输入，训练网络区分背景、内口、宫颈组织和阴道壁四个类别。", "result": "在仅有70个宫颈组织切片的训练数据集下，模型在独立测试集上达到了89.71%的像素准确率和80.96%的平均组织Dice系数。", "conclusion": "基于强度的框架只需要最少的预处理，并且可以轻松扩展到其他成像模态和组织类型，这得益于ImageNet的迁移学习，使得在小型生物医学图像数据集上也能实现精确分割。模型易于部署，并提供公开的图形标注工具。"}}
{"id": "2602.09331", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09331", "abs": "https://arxiv.org/abs/2602.09331", "authors": ["Mykola Khandoga", "Rui Yuan", "Vinay Kumar Sankarapu"], "title": "Beyond Uniform Credit: Causal Credit Assignment for Policy Optimization", "comment": "12 pages, 1 figure", "summary": "Policy gradient methods for language model reasoning, such as GRPO and DAPO, assign uniform credit to all generated tokens - the filler phrase \"Let me think\" receives the same gradient update as the critical calculation \"23 + 45 = 68.\" We propose counterfactual importance weighting: mask reasoning spans, measure the drop in answer probability, and upweight tokens accordingly during policy gradient updates. Our method requires no auxiliary models or external annotation, instead importance is estimated directly from the policy model's own probability shifts. Experiments on GSM8K across three models spanning the Qwen and Llama families demonstrate consistent improvements over uniform baselines and faster convergence to equivalent accuracy. Inverting the importance signal hurts performance, confirming we capture genuine causal structure rather than noise. Analysis shows the method correctly prioritizes calculation steps over scaffolding text. We view these findings as establishing counterfactual importance weighting as a foundation for further research rather than a complete solution.", "AI": {"tldr": "提出一种反事实重要性加权方法，通过掩码推理片段并测量答案概率下降来改进策略梯度方法，以解决现有方法对所有生成token分配统一信用值的问题。", "motivation": "现有策略梯度方法（如GRPO和DAPO）对语言模型推理中所有生成的token赋予相同的信用值，即使是像“让我想想”这样的填充短语与关键计算（如“23 + 45 = 68”）获得相同的梯度更新，这不够有效。", "method": "提出反事实重要性加权（counterfactual importance weighting）。具体做法是：1. 掩码（mask）推理片段。2. 测量答案概率的下降程度。3. 在策略梯度更新过程中，根据重要性程度对token进行上采样（upweight）。此方法不需要辅助模型或外部标注，而是直接从策略模型自身的概率变化中估计重要性。", "result": "在GSM8K数据集上，使用Qwen和Llama系列的三种模型进行实验，结果显示：1. 相比于统一基线，该方法能带来一致的性能提升。2. 能够更快地收敛到同等准确度。3. 反转重要性信号会损害性能，这证实了该方法捕捉到了真实的因果结构而非噪声。4. 分析表明该方法能正确地优先考虑计算步骤而非脚手架文本。", "conclusion": "反事实重要性加权是一种有效的方法，能够改进策略梯度在语言模型推理中的应用，并已初步证明其有效性。研究者认为该方法是未来研究的基础，而非一个最终的解决方案。"}}
{"id": "2602.09203", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09203", "abs": "https://arxiv.org/abs/2602.09203", "authors": ["Amy Koike", "Ge", "Guo", "Xinning He", "Callie Y. Kim", "Dakota Sullivan", "Bilge Mutlu"], "title": "Elements of Robot Morphology: Supporting Designers in Robot Form Exploration", "comment": "10 pages, 5 figures, Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI '26)", "summary": "Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.", "AI": {"tldr": "本文提出一个名为“机器人形态要素”的框架，该框架将机器人形态分解为感知、关节、末端执行器、运动和结构五个基本要素，并开发了“形态探索模块”（MEB）工具包，以支持对机器人形态的系统性探索、分析、构思和协作设计。", "motivation": "现有研究缺乏关于如何系统性探索机器人形态的设计框架，而机器人形态对人机交互（HRI）至关重要。", "method": "通过分析现有机器人，提炼出“机器人形态要素”框架，并开发了“形态探索模块”（MEB）这一实体积木工具包，用于支持动手和协作的机器人形态实验。", "result": "通过案例研究和设计工作坊，证明了该框架和工具包能够有效地支持机器人形态的分析、构思、反思和协作设计。", "conclusion": "“机器人形态要素”框架和“形态探索模块”工具包为系统性地探索和设计机器人形态提供了一种有效的方法，能够促进对机器人形态的理解和创新。"}}
{"id": "2602.09155", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09155", "abs": "https://arxiv.org/abs/2602.09155", "authors": ["Ahmed Rahu", "Brian Shula", "Brandon Combs", "Aqsa Sultana", "Surendra P. Singh", "Vijayan K. Asari", "Derrick Forchetti"], "title": "Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images", "comment": "20 pages, 5 figures", "summary": "Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.", "AI": {"tldr": "本研究旨在利用卷积神经网络（CNN）分析低级别腺瘤的数字病理全切片图像（WSIs），以识别预示患者长期结直肠癌（CRC）风险的细微组织学特征。", "motivation": "尽管筛查能降低CRC发病率，但部分低级别腺瘤患者仍可能发展为CRC。传统组织学评估可能无法完全捕捉到预示恶性潜力的细微特征，因此需要更客观的方法来识别高风险患者，以进行个体化监测和预防。", "method": "本研究使用卷积神经网络（CNN）算法分析低级别腺瘤的数字病理全切片图像（WSIs），旨在检测可预测患者长期结直肠癌风险的细微组织学特征。", "result": "（摘要中未提供具体结果，但研究目的是检测和识别这些特征。）", "conclusion": "（摘要中未提供结论，但研究预期CNN能有效识别WSIs中预示CRC风险的组织学特征。）"}}
{"id": "2602.09163", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.09163", "abs": "https://arxiv.org/abs/2602.09163", "authors": ["Xingjian Zhang", "Sophia Moylan", "Ziyang Xiong", "Qiaozhu Mei", "Yichen Luo", "Jiaqi W. Ma"], "title": "FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases", "comment": null, "summary": "Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.", "AI": {"tldr": "本文提出了FlyBench，一个评估AI代理端到端本体知识库策展能力的基准，通过从大量文献中提取基因本体论注释。在基准测试中，多代理架构优于简单模型，但扩展模型规模的回报递减，表明在检索增强科学推理方面仍有很大改进空间。", "motivation": "现有的科学知识库的维护需要专家进行大量人工工作，而现有的基准测试未能全面捕捉这种端到端的策展工作流程，因此需要一个能够评估AI代理完成这类任务能力的基准。", "method": "提出了FlyBench基准，包括16,898篇全文文献和7,397个专家策展的注释，覆盖100个基因。评估了四种基线AI代理架构：记忆、固定流水线、单代理和多代理。", "result": "多代理设计在性能上优于其他基线架构，但扩大主干模型规模带来的性能提升有限。AI代理主要利用检索来确认已知信息而非发现新信息。", "conclusion": "FlyBench可以推动检索增强科学推理能力的发展。AI代理在端到端本体策展方面仍有很大的改进空间，未来的研究应关注提高代理发现新信息的能力。"}}
{"id": "2602.09336", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09336", "abs": "https://arxiv.org/abs/2602.09336", "authors": ["Siyuan Huang", "Ziyu Wang", "Chao Pan", "Han Zhao"], "title": "FM SO.P: A Progressive Task Mixture Framework with Automatic Evaluation for Cross-Domain SOP Understanding", "comment": null, "summary": "Standard Operating Procedures (SOPs) are critical for enterprise operations, yet existing language models struggle with SOP understanding and cross-domain generalization. Current methods fail because joint training cannot differentiate between reasoning capabilities that SOP requires: terminology precision, sequential ordering, and constraint reasoning. We propose FM SO.P, solving these challenges through two novelties. First, we introduce progressive task mixtures that build capabilities by stages across three task types with cumulative data: concept disambiguation for terminology precision, action sequence understanding for procedural correctness, and scenario-aware graph reasoning for conditional logic. Second, we propose an automatic multi-agent evaluation system consisting of three agents that adaptively generate rubrics, stratified test sets, and rubric scoring, adapting to domains (e.g., temporal constraints for DMV, regulatory compliance for banking). Evaluated on SOPBench across seven domains (Bank, DMV, Healthcare, Market, University, Library, Hotel), FM SO.P achieves 48.3\\% pass rate with our 32B model and 34.3\\% with our opensource 7B model, matching Qwen-2.5-72B-Instruct baseline (34.4\\%) with 10x fewer parameters.", "AI": {"tldr": "本文提出了一种名为 FM SO.P 的新方法，用于提高语言模型对标准操作程序（SOP）的理解能力，特别是在跨领域泛化方面。该方法通过分阶段的任务混合训练和自动多智能体评估系统来解决现有模型的不足。", "motivation": "现有语言模型在理解 SOP 时存在困难，尤其是在术语精确性、顺序性以及约束推理方面，并且跨领域泛化能力不足。这阻碍了其在实际企业运营中的应用。", "method": "FM SO.P 包含两个核心创新：1. 渐进式任务混合训练：通过概念消歧、动作序列理解和场景感知图推理三个阶段，逐步构建模型的能力。2. 自动多智能体评估系统：包含三个智能体，能够自适应地生成评估标准、分层测试集和评分，以适应不同领域（如 DMV、银行）的特点。", "result": "在 SOPBench 数据集（涵盖银行、DMV、医疗、市场、大学、图书馆、酒店七个领域）上，FM SO.P 的 32B 模型达到了 48.3% 的通过率，7B 模型达到了 34.3%，后者在参数量仅为基线模型（Qwen-2.5-72B-Instruct，34.4%）的十分之一的情况下，取得了相当的性能。", "conclusion": "FM SO.P 方法通过其创新的训练策略和评估系统，显著提升了语言模型在 SOP 理解和跨领域应用方面的能力，并在保持模型规模优势的同时，取得了具有竞争力的性能。"}}
{"id": "2602.09429", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09429", "abs": "https://arxiv.org/abs/2602.09429", "authors": ["Luigi Romano", "Ole Morten Aamo", "Jan Åslund", "Erik Frisk"], "title": "First-order friction models with bristle dynamics: lumped and distributed formulations", "comment": "15 pages, 9 figures. Under review at IEEE Transactions on Control Systems Technology", "summary": "Dynamic models, particularly rate-dependent models, have proven effective in capturing the key phenomenological features of frictional processes, whilst also possessing important mathematical properties that facilitate the design of control and estimation algorithms. However, many rate-dependent formulations are built on empirical considerations, whereas physical derivations may offer greater interpretability. In this context, starting from fundamental physical principles, this paper introduces a novel class of first-order dynamic friction models that approximate the dynamics of a bristle element by inverting the friction characteristic. Amongst the developed models, a specific formulation closely resembling the LuGre model is derived using a simple rheological equation for the bristle element. This model is rigorously analyzed in terms of stability and passivity -- important properties that support the synthesis of observers and controllers. Furthermore, a distributed version, formulated as a hyperbolic partial differential equation (PDE), is presented, which enables the modeling of frictional processes commonly encountered in rolling contact phenomena. The tribological behavior of the proposed description is evaluated through classical experiments and validated against the response predicted by the LuGre model, revealing both notable similarities and key differences.", "AI": {"tldr": "本文提出了一种基于物理原理的一阶动态摩擦模型，该模型通过反演摩擦特性来近似模拟“鬃毛”单元的动力学行为。其中一个模型与 LuGre 模型相似，并进行了稳定性与无源性分析。此外，还提出了一个分布式版本（双曲偏微分方程），用于模拟滚动接触摩擦。通过实验评估和与 LuGre 模型对比，验证了模型的有效性并发现了其独特性。", "motivation": "现有 rate-dependent 摩擦模型多为经验模型，缺乏物理可解释性。本文旨在从基本物理原理出发，建立具有更强可解释性的动态摩擦模型。", "method": "1. 基于物理原理，通过反演摩擦特性近似模拟“鬃毛”单元动力学，提出一类新的第一阶动态摩擦模型。 2. 推导了一个与 LuGre 模型相似的特定模型，并对其进行了严格的稳定性与无源性分析。 3. 提出了一个分布式版本，用双曲偏微分方程来描述，以处理滚动接触摩擦。 4. 通过经典实验和与 LuGre 模型对比来评估和验证所提模型的摩擦学行为。", "result": "提出了一类新的第一阶动态摩擦模型，其中一个模型与 LuGre 模型有相似之处。所提模型通过了稳定性与无源性分析。分布式模型可以处理滚动接触摩擦。实验结果表明，新模型在模拟摩擦行为方面与 LuGre 模型有相似之处，但也存在一些关键差异。", "conclusion": "本文成功地从物理原理出发，提出了一种新的、可解释性更强的动态摩擦模型及其分布式版本。所提模型具有良好的数学性质（稳定性、无源性），并在实验上得到验证，为理解和控制摩擦过程提供了新的工具。"}}
{"id": "2602.09165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09165", "abs": "https://arxiv.org/abs/2602.09165", "authors": ["Hirunima Jayasekara", "Chuong Huynh", "Yixuan Ren", "Christabel Acquaye", "Abhinav Shrivastava"], "title": "All-in-One Conditioning for Text-to-Image Synthesis", "comment": null, "summary": "Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.", "AI": {"tldr": "本文提出了一种基于场景图结构的零样本文本到图像合成新方法，通过ASQL条件器生成软视觉引导，以解决现有模型在处理复杂多对象、多属性和空间关系提示时的语义保真度和结构一致性问题。", "motivation": "现有文本到图像合成模型在处理包含多个对象、属性和空间关系的复杂提示时，难以保持语义保真度和结构一致性，尽管能生成逼真图像，但在语义理解和结构组织方面存在不足。", "method": "提出一种零样本、基于场景图的条件生成机制，核心是ASQL（Attribute-Size-Quantity-Location）条件器。该条件器使用轻量级语言模型生成视觉条件，并通过推理时优化引导扩散模型生成图像，从而实现轻量级、连贯且多样的图像合成。", "result": "该方法能够提高文本到图像合成在处理复杂提示时的文本-图像对齐能力，同时支持轻量级、连贯且多样的图像生成。", "conclusion": "基于场景图和ASQL条件器的零样本方法可以有效提升文本到图像合成在处理复杂提示时的组合能力、语义保真度和结构一致性，并提供了比使用预定义布局图更灵活和多样化的生成方式。"}}
{"id": "2602.09286", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09286", "abs": "https://arxiv.org/abs/2602.09286", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Human Control Is the Anchor, Not the Answer: Early Divergence of Oversight in Agentic AI Communities", "comment": null, "summary": "Oversight for agentic AI is often discussed as a single goal (\"human control\"), yet early adoption may produce role-specific expectations. We present a comparative analysis of two newly active Reddit communities in Jan--Feb 2026 that reflect different socio-technical roles: r/OpenClaw (deployment and operations) and r/Moltbook (agent-centered social interaction). We conceptualize this period as an early-stage crystallization phase, where oversight expectations form before norms reach equilibrium.\n  Using topic modeling in a shared comparison space, a coarse-grained oversight-theme abstraction, engagement-weighted salience, and divergence tests, we show the communities are strongly separable (JSD =0.418, cosine =0.372, permutation $p=0.0005$). Across both communities, \"human control\" is an anchor term, but its operational meaning diverges: r/OpenClaw} emphasizes execution guardrails and recovery (action-risk), while r/Moltbook} emphasizes identity, legitimacy, and accountability in public interaction (meaning-risk). The resulting distinction offers a portable lens for designing and evaluating oversight mechanisms that match agent role, rather than applying one-size-fits-all control policies.", "AI": {"tldr": "本文通过对比分析两个活跃的Reddit社区（r/OpenClaw和r/Moltbook），探讨了在AI早期部署阶段，不同社会技术角色下形成的异质化AI监管期望。研究发现，尽管“人类控制”是共同关注点，但在实际应用中，其含义根据社区角色（部署运维 vs. 社交互动）而存在显著差异，揭示了设计角色定制化AI监管机制的重要性。", "motivation": "文章的动机在于，当前关于AI监管的讨论往往将“人类控制”视为一个统一目标，但实际早期采用可能产生特定于角色的监管期望。作者希望通过实证研究揭示这种差异。", "method": "研究采用了主题建模、共享比较空间、粗粒度监管主题抽象、参与度加权显著性以及散度检验等方法，分析了2026年1月至2月两个Reddit社区（r/OpenClaw和r/Moltbook）的数据。", "result": "研究结果表明，这两个社区在监管期望上具有显著差异（JSD =0.418, cosine =0.372, permutation $p=0.0005$）。虽然“人类控制”是共同主题，但r/OpenClaw社区侧重于执行的保障措施和恢复（行动风险），而r/Moltbook社区则关注公开互动中的身份、合法性和问责制（意义风险）。", "conclusion": "研究结论认为，AI的监管期望并非单一维度，而是会根据AI所扮演的社会技术角色而分化。这种角色区分为了设计和评估AI监管机制提供了一个可迁移的视角，强调应根据AI的具体角色而非采用统一的控制策略。"}}
{"id": "2602.09204", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09204", "abs": "https://arxiv.org/abs/2602.09204", "authors": ["Ozan Kaya", "Emir Cem Gezer", "Roger Skjetne", "Ingrid Bouwer Utne"], "title": "Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications", "comment": null, "summary": "Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.", "AI": {"tldr": "本文提出了一种混合风险感知导航架构，用于在不确定的海洋环境中实现自主水面航行器（ASV）的鲁棒导航。该系统结合了概率障碍物建模和B样条轨迹优化，能够生成安全、平滑且能适应环境风险变化的路径。", "motivation": "在不断变化的海洋环境中，自主系统需要具备在不确定性下进行感知、推理和行动的能力，以实现鲁棒导航。现有方法在处理动态障碍物和量化风险方面存在不足。", "method": "1. 构建概率风险图，包含静态和动态障碍物信息。2. 使用风险导向的RRT*规划器，结合三种重连模式（最小路径长度、最小风险、路径长度与风险的组合优化）生成无碰撞路径。3. 利用B样条算法优化轨迹，确保连续性。", "result": "在包含静态和动态障碍物的实验场景中，该系统成功实现了安全导航、保持了平滑的轨迹，并能动态适应环境风险的变化。与仅使用LIDAR或视觉的传统方法相比，该方法在操作安全性方面有所提升。", "conclusion": "所提出的混合风险感知导航架构是一种有前途的解决方案，能够有效提高自主水面航行器在不确定和动态海洋环境中的运行安全性和自主性。"}}
{"id": "2602.09536", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09536", "abs": "https://arxiv.org/abs/2602.09536", "authors": ["Aamer Mohamed Huroon", "Li-Chun Wang"], "title": "UAV-Assisted 6G Communication Networks for Railways: Technologies, Applications, and Challenges", "comment": "5 pages , 2 figures accepted to the INNOVARail Conference 2026", "summary": "Unmanned Aerial Vehicles (UAVs) are crucial for advancing railway communication by offering reliable connectivity, adaptive coverage, and mobile edge services . This survey examines UAV-assisted approaches for 6G railway needs including ultra-reliable low-latency communication (URLLC) and integrated sensing and communication (ISAC). We cover railway channel models, reconfigurable intelligent surfaces (RIS), and UAV-assisted mobile edge computing (MEC). Key challenges include coexistence with existing systems, handover management, Doppler effect, and security. The roadmap suggests work on integrated communication-control systems and AI-driven optimization for intelligent railway networks.", "AI": {"tldr": "本综述探讨了无人机辅助通信在 6G 铁路网络中的应用，重点关注超可靠低延迟通信 (URLLC) 和集成传感与通信 (ISAC)，并提出了未来研究方向。", "motivation": "为了满足 6G 铁路通信对可靠连接、自适应覆盖和移动边缘服务的需求，需要一种先进的技术来实现这些目标。", "method": "本文通过对无人机辅助的 6G 铁路通信进行综述，涵盖了铁路信道模型、可重构智能表面 (RIS) 和无人机辅助移动边缘计算 (MEC)。", "result": "讨论了无人机在铁路通信中的应用，包括 URLLC 和 ISAC，并指出了信道模型、RIS 和 MEC 等关键技术。同时，也识别了诸如与其他系统共存、切换管理、多普勒效应和安全等挑战。", "conclusion": "无人机辅助通信是实现 6G 铁路网络的关键技术，未来的研究应侧重于集成通信-控制系统和基于 AI 的优化，以实现智能铁路网络。"}}
{"id": "2602.09339", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09339", "abs": "https://arxiv.org/abs/2602.09339", "authors": ["Jianfeng Zhu", "Karin G. Coifman", "Ruoming Jin"], "title": "Understanding Risk and Dependency in AI Chatbot Use from User Discourse", "comment": "21 pages, 5 figures", "summary": "Generative AI systems are increasingly embedded in everyday life, yet empirical understanding of how psychological risk associated with AI use emerges, is experienced, and is regulated by users remains limited. We present a large-scale computational thematic analysis of posts collected between 2023 and 2025 from two Reddit communities, r/AIDangers and r/ChatbotAddiction, explicitly focused on AI-related harm and distress. Using a multi-agent, LLM-assisted thematic analysis grounded in Braun and Clarke's reflexive framework, we identify 14 recurring thematic categories and synthesize them into five higher-order experiential dimensions. To further characterize affective patterns, we apply emotion labeling using a BERT-based classifier and visualize emotional profiles across dimensions. Our findings reveal five empirically derived experiential dimensions of AI-related psychological risk grounded in real-world user discourse, with self-regulation difficulties emerging as the most prevalent and fear concentrated in concerns related to autonomy, control, and technical risk. These results provide early empirical evidence from lived user experience of how AI safety is perceived and emotionally experienced outside laboratory or speculative contexts, offering a foundation for future AI safety research, evaluation, and responsible governance.", "AI": {"tldr": "本研究通过对Reddit上两个社区（r/AIDangers 和 r/ChatbotAddiction）的大规模计算主题分析，识别出用户在使用生成式AI时经历的五种心理风险体验维度，并发现自我调节困难最为普遍，对自主性、控制和技术风险的担忧导致恐惧。", "motivation": "现有研究对AI使用带来的心理风险如何产生、用户如何体验以及如何进行自我调节的实证理解有限，因此需要深入研究用户真实经验。", "method": "采用多主体、LLM辅助的主题分析方法，结合Braun和Clarke的反射框架，对从Reddit社区收集的用户帖子进行分析，识别出14个主题类别并归纳为5个高阶体验维度。此外，使用基于BERT的情感分类器对用户情感进行标记和可视化。", "result": "识别出AI相关心理风险的五个实证体验维度，其中自我调节困难是报告最多的问题。用户对自主性、控制权和技术风险的担忧导致了普遍的恐惧情绪。", "conclusion": "该研究提供了来自用户真实经验的早期实证证据，揭示了AI安全在实验室或推测性环境之外是如何被感知和情感化体验的，为未来的AI安全研究、评估和负责任的治理奠定了基础。"}}
{"id": "2602.09227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09227", "abs": "https://arxiv.org/abs/2602.09227", "authors": ["Ananya Yammanuru", "Maria Lusardi", "Nancy M. Amato", "Katherine Driggs-Campbell"], "title": "From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers", "comment": "17 pages, 5 figures", "summary": "In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.", "AI": {"tldr": "提出了一种名为 MMLO-LMP 的运动规划问题，旨在为具有不同动机和有限视角的观察者生成既可读又不可读的轨迹，并开发了一个名为 DUBIOUS 的轨迹优化器来解决该问题。", "motivation": "在现实世界的协作或对抗环境中，机器人需要根据观察者的动机（积极或消极）和可见性限制来调整其意图的传达方式，以提高效率或安全性。", "method": "引入了混合动机有限可观测性可读运动规划 (MMLO-LMP) 问题。提出了一种名为 DUBIOUS 的轨迹优化器来解决 MMLO-LMP 问题，并探索了实现目标的不同策略。", "result": "DUBIOUS 能够生成在可读性、观察者动机和有限可见性区域之间取得平衡的轨迹。", "conclusion": "MMLO-LMP 问题和 DUBIOUS 优化器能够有效地解决机器人运动规划中涉及复杂观察者情况下的意图传达问题，为未来研究（如移动观察者和观察者协同）奠定了基础。"}}
{"id": "2602.09340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09340", "abs": "https://arxiv.org/abs/2602.09340", "authors": ["Yang Ba", "Mohammad Sadeq Abolhasani", "Michelle V Mancenido", "Rong Pan"], "title": "Measuring Dataset Diversity from a Geometric Perspective", "comment": null, "summary": "Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.", "AI": {"tldr": "提出了一种基于拓扑数据分析（TDA）和持久性景观（PLs）的新颖数据集多样性度量方法（PLDiv），该方法能够量化数据的几何结构，弥补了现有指标仅关注分布方差或熵的不足，并通过实验证明了其有效性和可解释性。", "motivation": "现有数据集多样性度量方法主要关注统计学上的变异性或信息熵，忽视了数据集的几何结构。研究者希望开发一种新的度量方法，能够从拓扑学的角度捕捉数据的几何和结构属性，从而更全面地衡量数据集的多样性。", "method": "利用拓扑数据分析（TDA）和持久性景观（PLs）技术，从数据中提取和量化其几何特征，并基于这些特征构建一种新的多样性度量指标（PLDiv）。", "result": "实验结果表明，PLDiv指标在捕捉数据多样性的几何结构方面表现出强大、可靠和可解释的能力，并且能够直接将数据多样性与其底层几何结构联系起来。", "conclusion": "基于持久性景观的多样性度量（PLDiv）是一种理论上可靠的、能够超越熵的度量方法，能够有效捕捉数据集的几何和结构特性，为数据集的构建、增强和评估提供了一个基础性工具。"}}
{"id": "2602.09500", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.09500", "abs": "https://arxiv.org/abs/2602.09500", "authors": ["Liming Liu", "Zhidong Jia", "Li Jiang", "Wei Zhang", "Lan Xie", "Feng Qian", "Leju Yan", "Bing Yan", "Qiang Ma", "Zhou Sha", "Wei Yang", "Yixuan Ban", "Xinggong Zhang"], "title": "Camel: Frame-Level Bandwidth Estimation for Low-Latency Live Streaming under Video Bitrate Undershooting", "comment": "8 pages, 20 figures, to appear in WWW 2026", "summary": "Low-latency live streaming (LLS) has emerged as a popular web application, with many platforms adopting real-time protocols such as WebRTC to minimize end-to-end latency. However, we observe a counter-intuitive phenomenon: even when the actual encoded bitrate does not fully utilize the available bandwidth, stalling events remain frequent. This insufficient bandwidth utilization arises from the intrinsic temporal variations of real-time video encoding, which cause conventional packet-level congestion control algorithms to misestimate available bandwidth. When a high-bitrate frame is suddenly produced, sending at the wrong rate can either trigger packet loss or increase queueing delay, resulting in playback stalls.\n  To address these issues, we present Camel, a novel frame-level congestion control algorithm (CCA) tailored for LLS. Our insight is to use frame-level network feedback to capture the true network capacity, immune to the irregular sending pattern caused by encoding. Camel comprises three key modules: the Bandwidth and Delay Estimator and the Congestion Detector, which jointly determine the average sending rate, and the Bursting Length Controller, which governs the emission pattern to prevent packet loss.\n  We evaluate Camel on both large-scale real-world deployments and controlled simulations. In the real-world platform with 250M users and 2B sessions across 150+ countries, Camel achieves up to a 70.8% increase in 1080P resolution ratio, a 14.4% increase in media bitrate, and up to a 14.1% reduction in stalling ratio. In simulations under undershooting, shallow buffers, and network jitter, Camel outperforms existing congestion control algorithms, with up to 19.8% higher bitrate, 93.0% lower stalling ratio, and 23.9% improvement in bandwidth estimation accuracy.", "AI": {"tldr": "本文提出了一种名为Camel的帧级拥塞控制算法，用于解决低延迟直播（LLS）中因视频编码的时变性导致的带宽利用不足和卡顿问题。Camel通过帧级网络反馈来更准确地估计网络容量，并调整发送模式以避免丢包。", "motivation": "观察到即使可用带宽未满，低延迟直播中仍然频繁出现卡顿现象。这归因于视频编码的内在时间变化导致传统包级拥塞控制算法误估可用带宽，从而引发丢包或增加排队延迟。", "method": "提出了一种名为Camel的新型帧级拥塞控制算法（CCA）。Camel包含三个模块：带宽和延迟估计器、拥塞检测器（共同确定平均发送速率）以及突发长度控制器（控制发送模式以防止丢包）。", "result": "在真实世界大规模部署（2.5亿用户，20亿会话）中，Camel实现了1080P分辨率比例提高高达70.8%，媒体比特率提高14.4%，卡顿率降低高达14.1%。在模拟环境中，Camel在比特率、卡顿率和带宽估计准确性方面均优于现有算法。", "conclusion": "Camel是一种针对低延迟直播的帧级拥塞控制算法，能够有效解决视频编码时变性带来的带宽利用不足和卡顿问题，并在真实世界和模拟环境中均表现出优越的性能。"}}
{"id": "2602.09605", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09605", "abs": "https://arxiv.org/abs/2602.09605", "authors": ["Moa Johannesson", "Lina Brink", "Alvin Combrink", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A General Formulation for the Teaching Assignment Problem: Computational Analysis Over a Real-World Dataset", "comment": null, "summary": "The Teacher Assignment Problem is a combinatorial optimization problem that involves assigning teachers to courses while guaranteeing that all courses are covered, teachers do not teach too few or too many hours, teachers do not switch assigned courses too often and possibly teach the courses they favor. Typically the problem is solved manually, a task that requires several hours every year. In this work we present a mathematical formulation for the problem and an experimental evaluation of the model implemented using state-of-the-art SMT, CP, and MILP solvers. The implementations are tested over a real-world dataset provided by the Division of Systems and Control at Chalmers University of Technology, and produce teacher assignments with smaller workload deviation, a more even workload distribution among the teachers, and a lower number of switched courses.", "AI": {"tldr": "本文提出了一种数学模型来解决教师分配问题，并使用 SMT、CP 和 MILP 求解器进行了实验评估，该模型在实际数据集上实现了更小的工时偏差、更均匀的工时分配以及更少的课程切换。", "motivation": "教师分配问题通常需要耗费大量人力和时间来手动解决，作者希望通过数学模型和计算方法来自动化这一过程，并提高分配质量。", "method": "提出问题的数学模型，并使用状态最先进的 SMT、CP 和 MILP 求解器进行实现和实验评估。", "result": "与手动分配相比，所提出的模型在实际数据集上产生了更小的工时偏差、更均匀的工时分布以及更少的课程切换。", "conclusion": "数学模型结合 SMT、CP 和 MILP 求解器能够有效地解决教师分配问题，并能生成比手动分配更优的教师课程安排。"}}
{"id": "2602.09209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09209", "abs": "https://arxiv.org/abs/2602.09209", "authors": ["Michael D. Murray", "James Tung", "Richard W. Nuckols"], "title": "Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain", "comment": "19 pages excluding references and comments, 5 figures, 3 tables", "summary": "Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.", "AI": {"tldr": "研究表明，使用轻量级CNN-RNN模型，仅凭视觉数据即可在脚部接触地面之前预测脚部压力中心（COP）和撞击时间（TOI），这对于辅助设备中的预测性控制具有重要意义。", "motivation": "现有计算机视觉技术在环境分类和辅助系统控制方面有应用，但预测脚部接触变化环境的能力研究不足。", "method": "使用安装在受试者小腿上的RGB-D相机和带有传感器的鞋垫，收集了8名受试者从平地过渡到楼梯时的步态数据。训练了一个CNN-RNN模型，在脚部撞击前250毫秒的时间窗口内，连续预测COP和TOI。", "result": "在预测时间窗口为150ms、100ms和50ms时，COP的平均绝对误差（MAE）分别为29.42mm、26.82mm和23.72mm。TOI的MAE分别为21.14ms、20.08ms和17.73ms。躯干速度对预测误差无影响，但更快的脚趾摆动速度可以提高COP的预测精度。脚部着地点更靠前时，COP预测精度会下降。", "conclusion": "使用轻量级模型，仅凭视觉数据预测脚部撞击前的COP和TOI是可行的，这为在辅助系统中实现预测性控制提供了可能性。该模型能够在消费级笔记本电脑或边缘计算设备上以60 FPS运行。"}}
{"id": "2602.09346", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09346", "abs": "https://arxiv.org/abs/2602.09346", "authors": ["Yoshifumi Kawasaki"], "title": "Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs", "comment": null, "summary": "This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge using two survey-style question formats: Yes-No questions and multiple-choice questions. To this end, we exploited a large-scale, expert-curated database of Spanish lexical variation. Our evaluation covers more than 900 lexical items across 21 Spanish-speaking countries and is conducted at both the country and dialectal area levels. Across both evaluation formats, the results reveal systematic differences in how LLMs represent Spanish language varieties. Lexical variation associated with Spain, Equatorial Guinea, Mexico & Central America, and the La Plata River is recognized more accurately by the models, while the Chilean variety proves particularly difficult for the models to distinguish. Importantly, differences in the volume of country-level digital resources do not account for these performance patterns, suggesting that factors beyond data quantity shape dialectal representation in LLMs. By providing a fine-grained, large-scale evaluation of geographic lexical variation, this work advances empirical understanding of dialectal knowledge in LLMs and contributes new evidence to discussions of Digital Linguistic Bias in Spanish.", "AI": {"tldr": "本研究评估了大型语言模型（LLM）在西班牙语地理词汇变异方面的表现，发现模型对西班牙、赤道几内亚、墨西哥和中美洲、拉普拉塔河地区的变异识别更准确，而智利方言则较难区分。模型表现与国家级数字资源数量无关。", "motivation": "大型语言模型（LLM）在处理具有显著区域差异的语言（如西班牙语）时，其地理词汇变异的捕捉能力尚不明确。", "method": "将LLM视为虚拟信息源，使用“是/否”问题和多项选择题两种问卷形式，结合专家整理的大规模西班牙语词汇变异数据库，评估了900多个词条在21个西班牙语国家及方言区的表现。", "result": "LLM在区分西班牙语不同地区变异方面存在系统性差异。西班牙、赤道几内亚、墨西哥与中美洲、拉普拉塔河地区的词汇变异识别准确度较高，而智利方言则难以区分。模型表现与国家级数字资源的数量无显著关联。", "conclusion": "研究结果表明，LLM对西班牙语地理词汇变异的掌握程度不一，数字资源数量并非决定性因素。本研究为理解LLM的方言知识以及数字语言偏差提供了实证依据。"}}
{"id": "2602.09255", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09255", "abs": "https://arxiv.org/abs/2602.09255", "authors": ["Mingfeng Yuan", "Hao Zhang", "Mahan Mohammadi", "Runhao Li", "Jinjun Shan", "Steven L. Waslander"], "title": "STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory", "comment": null, "summary": "Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.", "AI": {"tldr": "本文提出了一种名为STaR的智能体推理框架，用于构建可扩展的长期记忆，以支持移动机器人在开放动态环境中进行长期导航和任务执行，并在实际部署中取得了良好效果。", "motivation": "为了使移动机器人在长期、开放和动态的环境中能够有效地规划、检索和推理，需要一种能够处理各种粒度指令并提供精确导航答案的可扩展的长期记忆。", "method": "STaR框架包含两个主要部分：1. 构建一个任务无关、多模态的长期记忆，能够泛化到未见过的查询，并保留精细的环境语义（对象属性、空间关系和动态事件）。2. 引入基于信息瓶颈原理的可扩展任务条件检索算法，从长期记忆中提取紧凑、无冗余、信息丰富的候选记忆用于上下文推理。", "result": "在NaVQA和WH-VQA数据集上，STaR的性能优于现有方法，提高了成功率并显著降低了空间误差。在真实Husky轮式机器人上的部署也证明了其在室内外环境中的鲁棒性、可扩展性和实用性。", "conclusion": "STaR框架能够有效地构建和利用多模态长期记忆，为移动机器人在复杂环境中执行长距离导航和任务提供了强大的推理能力，并具有实际应用价值。"}}
{"id": "2602.09673", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09673", "abs": "https://arxiv.org/abs/2602.09673", "authors": ["Arya Abdollahi"], "title": "Community-Centered Resilience Enhancement of Urban Power and Gas Networks via Microgrid Partitioning, Mobile Energy Storage, and Data-Driven Risk Assessment", "comment": null, "summary": "Urban energy systems face increasing challenges due to high penetration of renewable energy sources, extreme weather events, and other high-impact, low-probability disruptions. This project proposes a community-centered, open-access framework to enhance the resilience and reliability of urban power and gas networks by integrating microgrid partitioning, mobile energy storage deployment, and data-driven risk assessment. The approach involves converting passive distribution networks into active, self-healing microgrids using distributed energy resources and remotely controlled switches to enable flexible reconfiguration during normal and emergency operations. To address uncertainties from intermittent renewable generation and variable load, an adjustable interval optimization method combined with a column and constraint generation algorithm is developed, providing robust planning solutions without requiring probabilistic information. Additionally, a real-time online risk assessment tool is proposed, leveraging 25 multi-dimensional indices including load, grid status, resilient resources, emergency response, and meteorological factors to support operational decision-making during extreme events. The framework also optimizes the long-term sizing and allocation of mobile energy storage units while incorporating urban traffic data for effective routing during emergencies. Finally, a novel time-dependent resilience and reliability index is introduced to quantify system performance under diverse operating conditions. The proposed methodology aims to enable resilient, efficient, and adaptable urban energy networks capable of withstanding high-impact disruptions while maximizing operational and economic benefits.", "AI": {"tldr": "本研究提出了一个以社区为中心、开放获取的框架，通过集成微电网分区、移动储能部署和数据驱动的风险评估，来增强城市电力和天然气网络的弹性和可靠性。", "motivation": "城市能源系统面临高渗透率可再生能源、极端天气事件和其他高影响低概率扰动带来的挑战。", "method": "研究将无源配电网转换为具有自愈能力的微电网，利用分布式能源和远程控制开关实现灵活重构。开发了一种可调区间优化方法和列-约束生成算法，用于不确定性下的鲁棒规划。提出了实时在线风险评估工具，包含25个多维度指数。优化了移动储能单元的长期选址和配置，并整合了城市交通数据用于应急调度。引入了时间依赖的弹性和可靠性指标。", "result": "提出了一种能够提高城市能源网络在正常和紧急情况下的弹性和可靠性的框架，并通过优化移动储能的部署和有效的风险评估来最大化运营和经济效益。", "conclusion": "该框架旨在实现弹性、高效和适应性强的城市能源网络，使其能够承受高影响的扰动，同时最大限度地提高运营和经济效益。"}}
{"id": "2602.09214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09214", "abs": "https://arxiv.org/abs/2602.09214", "authors": ["Chenyu Wang", "Tianle Chen", "H. M. Sabbir Ahmad", "Kayhan Batmanghelich", "Wenchao Li"], "title": "VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models", "comment": null, "summary": "Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.", "AI": {"tldr": "本文提出了VLM-UQBench基准，用于评估视觉语言模型（VLMs）在图像、文本或两者之间不匹配方面的不确定性。研究发现，现有UQ方法在特定模态和模型上存在特异性，且与幻觉的相关性较弱，无法有效检测细粒度的实例级不确定性。", "motivation": "确保视觉语言模型（VLMs）的安全性和可靠性，关键在于量化和定位模型不确定性的来源（图像、文本或两者之间的不匹配）。", "method": "构建了VLM-UQBench基准，包含VizWiz数据集的600个样本，并划分为清洁、图像、文本和跨模态不确定性子集。设计了一个包含8种视觉、5种文本和3种跨模态扰动的流程。提出了两个新指标，用于量化UQ分数对扰动的敏感性及其与幻觉的相关性。最后，在四个VLMs和三个数据集上评估了多种UQ方法。", "result": "1. 现有UQ方法表现出显著的模态特异性和对底层VLM的依赖性。2. 模态特异性不确定性常伴随幻觉，但当前UQ分数提供的风险信号较弱且不一致。3. UQ方法在公开、群体级歧义检测上可与推理型链式思考基线媲美，但在检测扰动引入的细微、实例级歧义方面表现不佳。", "conclusion": "当前UQ实践与可靠部署VLM所需的精细、模态感知的不确定性之间存在显著差距，需要开发更有效的UQ方法来解决细粒度的实例级不确定性问题。"}}
{"id": "2602.09341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09341", "abs": "https://arxiv.org/abs/2602.09341", "authors": ["Wei Yang", "Shixuan Li", "Heng Ping", "Peiyu Zhang", "Paul Bogdan", "Jesse Thomason"], "title": "Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge", "comment": null, "summary": "Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.", "AI": {"tldr": "该研究提出了一种名为AgentAuditor的新方法，用于解决多智能体系统（MAS）中大型语言模型（LLMs）推理输出的聚合问题，通过使用基于推理树的路径搜索替代多数投票，并结合反共识偏好优化（ACPO）来提高准确性，尤其是在存在共识性错误的情况下。", "motivation": "现有的多智能体系统（MAS）框架倾向于使用多数投票来聚合大型语言模型（LLMs）的输出，这种方法忽略了推理过程中的证据结构，并且容易在“共识性错误”（即多个智能体共享偏见并得出错误结论）的情况下失效。", "method": "研究引入了AgentAuditor，它使用基于推理树（Reasoning Tree）的路径搜索来替代多数投票。该方法显式地表示了不同智能体推理轨迹中的一致和分歧点，并通过在关键分歧点比较推理分支来解决冲突。此外，还提出了反共识偏好优化（ACPO），用于训练决策器（adjudicator），使其能够从多数失败的案例中学习，并奖励基于证据的少数选择。", "result": "AgentAuditor在5种流行的MAS设置中，相比多数投票，准确率提升了高达5个百分点；相比LLM-as-Judge，准确率提升了高达3个百分点。AgentAuditor不受MAS设置的限制。", "conclusion": "AgentAuditor通过将全局仲裁转化为局部验证，并结合ACPO来处理共识性错误，能够有效提升多智能体系统中LLMs的推理能力和准确性，尤其是在面对推理偏差和错误共识时。"}}
{"id": "2602.09366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09366", "abs": "https://arxiv.org/abs/2602.09366", "authors": ["Jianyu Zheng"], "title": "Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only", "comment": "16 pages, 6 figures, 7 tables, under review", "summary": "Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.", "AI": {"tldr": "本文提出了一种完全无监督的跨语言词性标注框架，仅使用单语语料库，通过无监督神经机器翻译（UNMT）生成伪平行语料，然后利用词语对齐进行词性投影，并引入多源投影技术提升性能。", "motivation": "现有低资源语言的词性标注方法通常依赖于平行语料库，而这类语料库对于许多低资源语言是稀缺的。因此，研究一种仅依赖单语语料库的方法非常有必要。", "method": "1. 使用无监督神经机器翻译（UNMT）系统将高资源语言翻译成低资源语言，构建伪平行句子对。\n2. 基于构建的伪平行句子对，利用词语对齐进行词性投影，训练目标语言的词性标注器。\n3. 提出多源投影技术来校准投影的词性标签，以训练更有效的词性标注器。", "result": "在28种语言对上的实验表明，所提出的方法在性能上可以与基于真实平行语料库的基线方法相媲美，甚至在某些目标语言上表现更优。多源投影技术将平均性能提升了1.3%。", "conclusion": "该研究成功提出了一种仅依赖单语语料库的完全无监督跨语言词性标注框架，并通过多源投影技术进一步提升了性能，为低资源语言的词性标注提供了新的解决方案。"}}
{"id": "2602.09343", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09343", "abs": "https://arxiv.org/abs/2602.09343", "authors": ["Michail S. Alexiou", "J. Sukarno Mertoguno"], "title": "Not-in-Perspective: Towards Shielding Google's Perspective API Against Adversarial Negation Attacks", "comment": null, "summary": "The rise of cyberbullying in social media platforms involving toxic comments has escalated the need for effective ways to monitor and moderate online interactions. Existing solutions of automated toxicity detection systems, are based on a machine or deep learning algorithms. However, statistics-based solutions are generally prone to adversarial attacks that contain logic based modifications such as negation in phrases and sentences. In that regard, we present a set of formal reasoning-based methodologies that wrap around existing machine learning toxicity detection systems. Acting as both pre-processing and post-processing steps, our formal reasoning wrapper helps alleviating the negation attack problems and significantly improves the accuracy and efficacy of toxicity scoring. We evaluate different variations of our wrapper on multiple machine learning models against a negation adversarial dataset. Experimental results highlight the improvement of hybrid (formal reasoning and machine-learning) methods against various purely statistical solutions.", "AI": {"tldr": "提出了一种基于形式推理的包装器，用于增强现有的机器学习毒性检测系统，以应对基于逻辑的对抗性攻击（特别是否定攻击），并提高了准确性和有效性。", "motivation": "社交媒体上网络欺凌的增加，特别是涉及有毒评论，促使需要更有效的在线互动监控和管理方法。现有的基于机器学习的毒性检测系统容易受到包含否定等逻辑修改的对抗性攻击。", "method": "提出了一套形式推理方法，作为机器学习毒性检测系统的预处理和后处理步骤，以解决否定攻击问题。评估了不同包装器变体在多种机器学习模型上的性能，并使用否定对抗性数据集进行了测试。", "result": "实验结果表明，混合（形式推理和机器学习）方法在对抗否定攻击方面比纯粹的统计解决方案有显著的改进，提高了毒性评分的准确性和有效性。", "conclusion": "基于形式推理的包装器可以有效地增强现有的机器学习毒性检测系统，使其对基于逻辑的对抗性攻击（如否定）更具鲁棒性，并提高其整体性能。"}}
{"id": "2602.09695", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.09695", "abs": "https://arxiv.org/abs/2602.09695", "authors": ["Gian Carlo Maffettone", "Davide Salzano", "Mario di Bernardo"], "title": "Robust Macroscopic Density Control of Heterogeneous Multi-Agent Systems", "comment": null, "summary": "Modern applications, such as orchestrating the collective behavior of robotic swarms or traffic flows, require the coordination of large groups of agents evolving in unstructured environments, where disturbances and unmodeled dynamics are unavoidable. In this work, we develop a scalable macroscopic density control framework in which a feedback law is designed directly at the level of an advection--diffusion partial differential equation. We formulate the control problem in the density space and prove global exponential convergence towards the desired behavior in $\\mathcal{L}^2$ with guaranteed asymptotic rejection of bounded unknown drift terms, explicitly accounting for heterogeneous agent dynamics, unmodeled behaviors, and environmental perturbations. Our theoretical findings are corroborated by numerical experiments spanning heterogeneous oscillators, traffic systems, and swarm robotics in partially unknown environments.", "AI": {"tldr": "提出了一种基于偏微分方程的宏观密度控制框架，用于大规模群体智能体的协调，可处理异构动力学、未建模行为和环境扰动，并证明了全局指数收敛性。", "motivation": "现代应用（如机器人集群或交通流的协调）需要处理大规模、在非结构化环境中进化的智能体，这些环境不可避免地会受到干扰和未建模动力学的影响。", "method": "将控制问题在密度空间中进行数学建模，并设计一个直接作用于平流-扩散偏微分方程的反馈律，以实现宏观密度控制。", "result": "理论上证明了系统能够全局指数收敛到期望行为，并能渐近地抑制有界未知漂移项，同时明确考虑了智能体的异构动力学、未建模行为和环境扰动。", "conclusion": "提出的宏观密度控制框架在理论上是有效的，并且通过异构振荡器、交通系统和部分未知环境中的群体机器人等数值实验得到了验证，表明其在实际应用中的潜力。"}}
{"id": "2602.09252", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.09252", "abs": "https://arxiv.org/abs/2602.09252", "authors": ["Ange Lou", "Yamin Li", "Qi Chang", "Nan Xi", "Luyuan Xie", "Zichao Li", "Tianyu Luan"], "title": "VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models", "comment": null, "summary": "Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.", "AI": {"tldr": "提出了一种名为IR-SIS的迭代式手术图像分割系统，该系统利用自然语言描述进行分割，并通过视觉-语言模型和自适应精炼策略来提高准确性，支持医生通过自然语言进行交互。该方法在 EndoVis2017 和 EndoVis2018 数据集上取得了最先进的性能。", "motivation": "现有手术图像分割方法受限于预定义类别、一次性预测且缺乏医生交互机制。作者旨在开发一个更灵活、更智能、可与医生交互的手术分割系统。", "method": "IR-SIS 系统首先使用微调的 SAM3 进行初始分割，然后利用视觉-语言模型检测器械并评估分割质量。通过一种代理工作流，系统自适应地选择精炼策略。此外，该系统支持医生通过自然语言反馈进行人机协作。研究人员还构建了一个多粒度语言标注数据集。", "result": "IR-SIS 在现域和跨领域（out-of-distribution）数据上均取得了最先进的性能。医生交互进一步提升了分割效果。该系统是首个具备自适应自精炼能力且基于语言的手术分割框架。", "conclusion": "IR-SIS 是一种创新的手术图像分割框架，通过迭代精炼、视觉-语言模型和医生交互，克服了现有方法的局限性，在手术分割任务中表现出色，并为未来的研究开辟了新方向。"}}
{"id": "2602.09372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09372", "abs": "https://arxiv.org/abs/2602.09372", "authors": ["Zexu Sun", "Bokai Ji", "Hengyi Cai", "Shuaiqiang Wang", "Lei Wang", "Guangxia Li", "Xu Chen"], "title": "AgentSkiller: Scaling Generalist Agent Intelligence through Semantically Integrated Cross-Domain Data Synthesis", "comment": "33 pages, 9 figures", "summary": "Large Language Model agents demonstrate potential in solving real-world problems via tools, yet generalist intelligence is bottlenecked by scarce high-quality, long-horizon data. Existing methods collect privacy-constrained API logs or generate scripted interactions lacking diversity, which struggle to produce data requisite for scaling capabilities. We propose AgentSkiller, a fully automated framework synthesizing multi-turn interaction data across realistic, semantically linked domains. It employs a DAG-based architecture with explicit state transitions to ensure determinism and recoverability. The pipeline builds a domain ontology and Person-Centric Entity Graph, defines tool interfaces via Service Blueprints for Model Context Protocol servers, and populates environments with consistent databases and strict Domain Policies. A cross-domain fusion mechanism links services to simulate complex tasks. Finally, the pipeline creates user tasks by verifying solution paths, filtering via execution-based validation, and generating queries using a Persona-based Simulator for automated rollout. This produces reliable environments with clear state changes. To demonstrate effectiveness, we synthesized $\\approx$ 11K interaction samples; experimental results indicate that models trained on this dataset achieve significant improvements on function calling over baselines, particularly in larger parameter regimes.", "AI": {"tldr": "本文提出 AgentSkiller 框架，通过自动化流程合成高质量、长周期的多轮交互数据，以解决大型语言模型（LLM）智能体在实际应用中数据稀缺的问题，实验表明基于此数据训练的模型在函数调用能力上有所提升。", "motivation": "现有 LLM 智能体在解决现实问题时，受限于高质量、长周期的训练数据不足，现有方法生成的交互数据缺乏多样性，难以支撑 LLM 能力的扩展。", "method": "AgentSkiller 采用基于 DAG 的架构，通过构建领域本体、以 Person-Centric Entity Graph 为基础，定义工具接口（Service Blueprints），并配置一致性数据库和领域策略，最后通过 Persona-based Simulator 生成用户任务，实现交互数据的自动化合成。", "result": "AgentSkiller 合成了约 11,000 个交互样本。实验表明，使用这些数据训练的模型在函数调用能力上相较于基线模型有显著提升，尤其在大参数模型上效果更佳。", "conclusion": "AgentSkiller 框架能够有效地合成用于训练 LLM 智能体的高质量、多领域交互数据，解决了数据稀缺的瓶颈，并显著提升了模型在函数调用等方面的性能。"}}
{"id": "2602.09287", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09287", "abs": "https://arxiv.org/abs/2602.09287", "authors": ["Minja Axelsson", "Henry Shevlin"], "title": "Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction", "comment": "6 pages, 0 figures. Accepted at Human-Robot Interaction (HRI) conference (ACM/IEEE) 2026, Late-Breaking Reports", "summary": "In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.", "AI": {"tldr": "本文初步区分了拟人化（用户感知机器人具有人类特质）和拟态（开发者设计机器人具有人类特质）这两个概念在人机交互（HRI）和社会机器人领域的理论概念，旨在为未来的HRI研究提供清晰的理论基础，以便于未来的机器人设计和评估。", "motivation": "目前HRI和机器人领域对“拟人化”和“拟态”这两个概念的理解存在混淆，作者希望通过初步的理论梳理，为未来的机器人设计和评估研究提供清晰的理论基础。", "method": "通过界定和区分“拟人化”和“拟态”这两个概念，明确了产生人类特质的责任方（拟人化是用户感知，拟态是开发者设计）。", "result": "提出了拟人化（anthropomorphism）和拟态（anthropomimesis）的定义，并区分了它们在HRI中的作用主体。", "conclusion": "对拟人化和拟态的理论概念进行了初步的区分，为HRI研究者提供了更清晰的理论框架，以促进未来的机器人设计和评估研究。"}}
{"id": "2602.09347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09347", "abs": "https://arxiv.org/abs/2602.09347", "authors": ["Jana G. Delfino", "Jason L. Granstedt", "Frank W. Samuelson", "Robert Ochs", "Krishna Juluru"], "title": "Image Quality in the Era of Artificial Intelligence", "comment": "16 pages, 3 figures", "summary": "Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.", "AI": {"tldr": "人工智能（AI）在放射学中的应用迅速，能够提高图像质量和效率。然而，AI也带来了新的故障模式，可能导致感知质量与实际信息内容脱节。了解AI图像重建和增强的局限性对于安全有效使用AI至关重要。", "motivation": "人工智能在放射学中的快速普及，带来了图像质量的提升和效率的提高，但同时也引入了新的失效模式，并可能加剧图像感知质量与信息内容之间的脱节。因此，研究AI图像重建和增强的局限性对于安全有效应用该技术至关重要。", "method": "本文通过沟通和提高意识的方式，探讨了AI在放射学图像重建和增强方面的局限性。", "result": "AI在提高图像清晰度、平滑度和细节方面表现出色，并能加快图像采集和审查速度。然而，AI的使用会引入新的失效模式，并且可能导致感知图像质量与实际信息内容之间出现差异。", "conclusion": "了解AI在放射学图像重建和增强过程中的局限性，对于用户在享受技术优势的同时，最大限度地降低风险是至关重要的。"}}
{"id": "2602.10007", "categories": ["cs.RO", "cs.AI", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.10007", "abs": "https://arxiv.org/abs/2602.10007", "authors": ["Bharathkumar Hegde", "Melanie Bouroche"], "title": "A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging", "comment": "Accepted in IEEE IV 2026", "summary": "Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS", "AI": {"tldr": "本文提出了一种名为 MARL-MASS 的多智能体安全护盾 (MASS) 方法，该方法结合了控制障碍函数 (CBF) 和多智能体强化学习 (MARL)，用于在拥堵交通中实现安全且协作的车道变换，同时平衡了安全性和交通效率。", "motivation": "现有车道变换控制器要么侧重于安全性，要么侧重于交通效率，但无法同时考虑这两个相互冲突的目标，尤其是在拥挤的交通环境中。", "method": "1. 提出 Multi-Agent Safety Shield (MASS)，利用控制障碍函数 (CBF) 来保证安全。2. 通过构建交互拓扑图来捕捉多智能体之间的交互。3. 扩展现有的 Multi-Agent Reinforcement Learning (MARL) 控制器，并集成 MASS 来确保安全。4. 定义了一个定制的奖励函数，优先考虑效率提升。5. 将上述方法结合，提出 MARL-MASS。", "result": "MASS 能够保证安全且协作的车道变换，严格遵守安全约束。定制的奖励函数提高了 MARL 策略在安全护盾下的稳定性。MARL-MASS 在模拟的拥堵匝道合并场景中，成功平衡了安全性和交通效率。", "conclusion": "MARL-MASS 通过在遵守安全约束的同时鼓励探索协作车道变换策略，有效地解决了拥挤交通中安全性和交通效率之间的权衡问题。"}}
{"id": "2602.09268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09268", "abs": "https://arxiv.org/abs/2602.09268", "authors": ["Nikita Starodubcev", "Daniil Pakhomov", "Zongze Wu", "Ilya Drobyshevskiy", "Yuchen Liu", "Zhonghao Wang", "Yuqian Zhou", "Zhe Lin", "Dmitry Baranchuk"], "title": "Rethinking Global Text Conditioning in Diffusion Transformers", "comment": "Accepted at ICLR26", "summary": "Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.", "AI": {"tldr": "本文研究了扩散 Transformer 中基于池化文本嵌入的调制机制的必要性。研究发现，常规用法下该机制贡献不大，但将其用作引导可以带来性能提升，且该方法训练免费、易实现，并适用于多种扩散模型和任务。", "motivation": "近期研究倾向于抛弃扩散 Transformer 中的调制机制，仅依赖注意力机制。本文旨在探究基于池化文本嵌入的调制机制是否仍有必要，以及其是否能带来性能上的优势。", "method": "通过分析扩散 Transformer 中常规使用的池化文本嵌入，并探索将其作为引导，实现向更理想属性的可控迁移。该方法无需训练，实现简单，运行时开销可忽略。", "result": "研究发现，在常规用法下，池化嵌入对整体性能贡献甚微，注意力机制通常已足够。然而，当池化嵌入作为引导使用时，能够显著提升性能，实现对模型输出属性的可控调整。", "conclusion": "基于池化文本嵌入的调制机制在常规用法下并非必要，但作为一种训练免费、易于实现的引导方式，可以显著提升扩散模型的性能，并在文本到图像/视频生成和图像编辑等任务上带来改进。"}}
{"id": "2602.09259", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09259", "abs": "https://arxiv.org/abs/2602.09259", "authors": ["Yizhou Li", "Shuyuan Yang", "Jiaji Su", "Zonghe Chua"], "title": "Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation", "comment": "8 pages, conference submission pre-print", "summary": "In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.", "AI": {"tldr": "本研究提出了一个手术眼动数据集，并分析了不同技能水平和感知模式（主动执行 vs. 被动观看）如何影响学习到的注意力模型。结果表明，被动眼动数据可以在一定程度上替代主动眼动数据进行模型训练，并且初学者的被动眼动标签可以接近中级学习者的被动眼动目标，为手术指导和感知模型提供了一种可扩展的众包眼动监督途径。", "motivation": "机器人辅助微创手术（RMIS）中，触觉反馈和深度线索的减少使得对专家视觉感知的依赖性增强。这促使了基于凝视引导的训练和基于学习的手术感知模型的研究。然而，收集专家的操作凝视数据成本高昂，并且不清楚凝视监督的来源（技能水平和感知模式）如何影响注意力模型学到的内容。", "method": "研究人员在da Vinci SimNow模拟器上，针对四种手术练习，收集了一个配对的主动-被动、多任务手术凝视数据集。主动凝视数据是在任务执行过程中使用带有眼动追踪的VR头显记录的，相应的视频被用作刺激，收集观察者的被动凝视数据。通过凝视密度重叠分析和单帧显著性建模，量化了技能和模态依赖的凝视组织差异，并评估了被动凝视替代主动凝视监督的可行性。", "result": "在不同设置下，MSI-Net 模型产生了稳定、可解释的预测，而 SalGAN 模型不稳定且与人类凝视点对齐较差。使用被动凝视训练的模型在很大程度上恢复了中级水平的主动注意力，但存在可预测的性能下降，并且在主动和被动目标之间的迁移是不对称的。值得注意的是，初学者的被动标签在高质量演示上可以近似中级水平的被动目标，损失有限。", "conclusion": "研究结果表明，被动凝视数据可以在一定程度上替代主动凝视数据用于手术感知模型的训练，并且初学者的被动凝视标签是训练模型的一种实用且可扩展的替代方案，尤其是在高质量演示数据可用的情况下，为手术教学和感知建模提供了新的思路。"}}
{"id": "2602.09373", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09373", "abs": "https://arxiv.org/abs/2602.09373", "authors": ["Yasmin Moslem", "Aman Kassahun Wassie", "Amanuel Gizachew Abebe"], "title": "AfriNLLB: Efficient Translation Models for African Languages", "comment": "Accepted at AfricaNLP 2026 (oral)", "summary": "In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).\n  AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.\n  Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.", "AI": {"tldr": "本文介绍了AfriNLLB，一系列轻量级非洲语言翻译模型，支持15种语言对，通过剪枝和量化优化，并在资源受限环境下实现高效部署。", "motivation": "为了在资源受限的环境下实现非洲语言的高效翻译部署。", "method": "基于NLLB-200 600M模型，采用迭代层剪枝和量化进行压缩，并使用知识蒸馏在精选的非洲语言平行语料库上进行微调。", "result": "AfriNLLB模型在性能与基线模型相当的情况下，显著提高了翻译速度，并发布了Transformers和CTranslate2两个版本，同时公开了训练数据。", "conclusion": "AfriNLLB模型能够高效部署，为非洲语言的翻译研究和应用提供了有力的支持。"}}
{"id": "2602.09367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09367", "abs": "https://arxiv.org/abs/2602.09367", "authors": ["Jinghan Yang", "Jingyi Hou", "Xinbo Yu", "Wei He", "Yifan Wu"], "title": "CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments", "comment": null, "summary": "Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.", "AI": {"tldr": "CAPER框架通过明确约束学习和推理在规划和控制流程中的位置，解决了机器人科学实验中的长时程操作、有限监督和低示范学习的挑战，提高了可控性、鲁棒性和数据效率。", "motivation": "现有的端到端视觉-语言-动作（VLA）模型难以应对科学实验室中程序敏感且需要长时程操作、低监督和低示范数据的机器人任务，因为其对错误可恢复性和数据驱动策略学习的假设在这些场景下失效。", "method": "CAPER框架采用责任分离的结构：任务级推理生成符合程序且有明确约束的动作序列；中层多模态融合实现子任务，不将空间决策交给大型语言模型；低层控制通过少量示范的强化学习适应物理不确定性。通过可解释的中间表示编码程序性承诺，限制执行时的逻辑违规。", "result": "在科学工作流程基准和公开的长时程操作数据集上进行实验，CAPER框架在成功率和程序正确性方面均取得一致性改进，尤其在低数据量和长时程场景下效果显著。", "conclusion": "CAPER框架通过显式约束学习和推理的边界，为机器人科学实验提供了一种更可控、更鲁棒、数据效率更高的方法，成功解决了现有VLA模型在特定实验场景下的局限性。"}}
{"id": "2602.09383", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09383", "abs": "https://arxiv.org/abs/2602.09383", "authors": ["Peng Lai", "Zhihao Ou", "Yong Wang", "Longyue Wang", "Jian Yang", "Yun Chen", "Guanhua Chen"], "title": "BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation", "comment": "Accepted to ICLR 2026", "summary": "LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bias, which has primarily been studied in terms of known biases and their impact on evaluation outcomes, while automated and systematic exploration of potential unknown biases is still lacking. Nevertheless, such exploration is crucial for enhancing the robustness and reliability of evaluations. To bridge this gap, we propose BiasScope, a LLM-driven framework for automatically and at scale discovering potential biases that may arise during model evaluation. BiasScope can uncover potential biases across different model families and scales, with its generality and effectiveness validated on the JudgeBench dataset. It overcomes the limitations of existing approaches, transforming bias discovery from a passive process relying on manual effort and predefined bias lists into an active and comprehensive automated exploration. Moreover, based on BiasScope, we propose JudgeBench-Pro, an extended version of JudgeBench and a more challenging benchmark for evaluating the robustness of LLM-as-a-judge. Strikingly, even powerful LLMs as evaluators show error rates above 50\\% on JudgeBench-Pro, underscoring the urgent need to strengthen evaluation robustness and to mitigate potential biases further.", "AI": {"tldr": "提出BiasScope框架，自动化发现LLM作为评判者时的潜在偏见，并基于此构建了更具挑战性的评测基准JudgeBench-Pro，结果显示即使是强大的LLM作为评判者也存在超过50%的错误率，凸显了加强评测鲁棒性和缓解偏见的需求。", "motivation": "当前LLM-as-a-Judge在评估中的鲁棒性和可靠性仍是关键问题，现有研究主要关注已知偏见，缺乏自动化系统性探索未知偏见的方法。这种探索对于提升评估的鲁棒性和可靠性至关重要。", "method": "提出BiasScope，一个由LLM驱动的框架，用于自动化、大规模地发现模型评估中可能出现的潜在偏见。BiasScope在JudgeBench数据集上进行了验证，能够发现跨不同模型家族和规模的潜在偏见。此外，基于BiasScope提出了JudgeBench-Pro，一个扩展版的JudgeBench，作为评估LLM-as-a-Judge鲁棒性的更具挑战性的基准。", "result": "BiasScope能够跨模型家族和规模发现潜在偏见。在JudgeBench-Pro基准上，即使是强大的LLM作为评估者，错误率也超过50%，表明现有LLM评估方法的不足。", "conclusion": "BiasScope为自动化、大规模地发现LLM评估中的潜在偏见提供了一种有效方法。JudgeBench-Pro基准揭示了LLM-as-a-Judge在鲁棒性方面存在的严峻挑战，亟需加强评估鲁棒性并进一步缓解偏见。"}}
{"id": "2602.09284", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09284", "abs": "https://arxiv.org/abs/2602.09284", "authors": ["Pranav Kulkarni", "Junfeng Guo", "Heng Huang"], "title": "X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging", "comment": null, "summary": "High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.", "AI": {"tldr": "提出了一种名为X-Mark的医学影像（特别是胸部X光片）版权保护方法，该方法通过在影像的显著区域生成样本特定的水印，以应对现有方法在医学影像上存在的局限性，并能在黑盒环境下进行模型溯源。", "motivation": "现有针对自然图像的数据集所有权验证方法难以直接应用于医学影像，因为医学影像具有分辨率高、视觉多样性有限、解剖结构微妙等特点，静态水印难以适应动态扫描且可能影响诊断质量。因此，需要一种专门针对医学影像的版权保护方法。", "method": "提出X-Mark方法，使用条件U-Net为每个样本生成独特的、仅在显著区域内存在的微小扰动作为水印。设计了一个多组件训练目标，以确保水印的有效性、鲁棒性（抵抗动态缩放）和诊断质量，并加入拉普拉斯正则化来惩罚高频扰动，实现水印的尺度不变性。所有权验证在黑盒设置下进行，通过检测可疑模型中的特征行为。", "result": "在CheXpert数据集上的实验表明，X-Mark的有效性得到了验证，实现了100%的水印检测率（WSR），并将诱导模型（Ind-M）场景下的误报率降低了12%，同时对潜在的自适应攻击具有抵抗力。", "conclusion": "X-Mark是一种有效的、样本特定的、干净标签的医学影像（胸部X光片）版权保护方法，它能够生成尺度不变且不影响诊断质量的水印，并在黑盒设置下实现可靠的所有权验证，优于现有方法。"}}
{"id": "2602.09443", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09443", "abs": "https://arxiv.org/abs/2602.09443", "authors": ["Yun Luo", "Futing Wang", "Qianjia Cheng", "Fangchen Yu", "Haodi Lei", "Jianhao Yan", "Chenxi Li", "Jiacheng Chen", "Yufeng Zhao", "Haiyuan Wan", "Yuchen Zhang", "Shenghe Zheng", "Junchi Yao", "Qingyang Zhang", "Haonan He", "Wenxuan Zeng", "Li Sheng", "Chengxing Xie", "Yuxin Zuo", "Yizhuo Li", "Yulun Wu", "Rui Huang", "Dongzhan Zhou", "Kai Chen", "Yu Qiao", "Lei Bai", "Yu Cheng", "Ning Ding", "Bowen Zhou", "Peng Ye", "Ganqu Cui"], "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads", "comment": null, "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.", "AI": {"tldr": "本文提出了P1-VL，一个为科学推理设计的开源视觉语言模型，通过课程强化学习和代理增强技术，在物理学奥赛级别考试中取得了显著成果，成为首个在HiPhO基准测试中获得12枚金牌的开源VLM，并在STEM领域展现出强大的泛化能力。", "motivation": "大型语言模型在从符号操作过渡到科学级推理时，面临着将抽象逻辑与物理现实相结合的挑战。物理学需要模型遵循宇宙法则，这需要多模态感知来将抽象逻辑与现实联系起来。奥赛级别的物理考试中，图示包含重要的文字未提及的约束（如边界条件、空间对称性），需要弥合视觉-逻辑的差距。", "method": "提出P1-VL模型系列，采用课程强化学习（Curriculum Reinforcement Learning）来稳定训练后的模型，以及代理增强（Agentic Augmentation）技术，实现推理时的迭代自我验证。", "result": "在HiPhO基准测试（包含13场2024-2025年的考试）上，P1-VL-235B-A22B成为首个获得12枚金牌的开源VLM，并在开源模型中达到最先进性能。其代理增强系统在全球排名第二，仅次于Gemini-3-Pro。P1-VL在STEM基准测试中也展现出卓越的科学推理能力和泛化性。", "conclusion": "P1-VL通过开源，为实现通用的物理智能奠定了基础，旨在更好地将视觉感知与抽象物理定律对齐，以促进机器的科学发现。"}}
{"id": "2602.09368", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09368", "abs": "https://arxiv.org/abs/2602.09368", "authors": ["Wei-Chen Li", "Glen Chou"], "title": "Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes", "comment": null, "summary": "Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.", "AI": {"tldr": "本研究提出了一种新的梯度优化方法，通过平滑混合接触动力学并量化补偿由此产生的误差，来解决接触丰富操作中的控制器优化难题，并为真实混合动力学提供了约束满足和目标可达性的形式化保证。", "motivation": "现有的基于梯度的方法难以处理接触丰富操作，因为混合接触动力学中的梯度不连续或消失。虽然平滑动力学可以获得连续梯度，但模型不匹配可能导致控制器在真实系统上失效。本研究旨在解决这种平滑和模型不匹配之间的权衡问题。", "method": "该方法通过一个基于凸优化的新型可微分模拟器，对接触动力学和几何进行平滑处理，并将真实动力学与平滑动力学之间的差异描述为集合值偏差。通过对系统可达集的分析界限来约束时间变化的仿射反馈策略的优化，从而为真实的闭环混合动力学提供鲁棒的约束满足保证，同时仅依赖于平滑动力学提供的有意义的梯度。", "result": "在平面推、物体旋转和手中灵巧操作等接触丰富的任务上进行的评估显示，与基线方法相比，本方法实现了保证的约束满足，且安全违规和目标误差更低。", "conclusion": "本研究首次实现了用于接触丰富操作的可认证的基于梯度策略合成方法，它通过将可微分物理与集合值鲁棒控制相结合，为在不牺牲真实动力学下的控制器优化提供了有效解决方案。"}}
{"id": "2602.09315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09315", "abs": "https://arxiv.org/abs/2602.09315", "authors": ["Subba Reddy Oota", "Vijay Rowtula", "Shahid Mohammed", "Jeffrey Galitz", "Minghsun Liu", "Manish Gupta"], "title": "A Deep Multi-Modal Method for Patient Wound Healing Assessment", "comment": "4 pages, 2 figures", "summary": "Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.", "AI": {"tldr": "该研究提出了一种深度多模态方法，结合伤口变量和伤口图像来预测患者住院风险，并开发了一种基于迁移学习的伤口评估解决方案，可预测伤口变量和愈合轨迹。", "motivation": "住院治疗是伤口护理成本高昂的主要原因之一。该研究旨在通过预测住院风险来降低成本，并希望通过早期检测伤口复杂性来减少临床医生诊断时间。", "method": "采用深度多模态方法，结合伤口变量和伤口图像进行风险预测。开发了一种基于迁移学习的伤口评估解决方案，可以从伤口图像预测伤口变量及其愈合轨迹。", "result": "该方法能够从伤口图像预测伤口变量和愈合轨迹，有望提高住院风险预测的置信度。", "conclusion": "提出的深度多模态模型通过整合伤口变量和伤口图像，能够更准确地预测患者的住院风险，并有助于早期发现影响愈合过程的复杂情况，从而减少临床诊断时间。"}}
{"id": "2602.09370", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09370", "abs": "https://arxiv.org/abs/2602.09370", "authors": ["Minsung Yoon", "Jeil Jeong", "Sung-Eui Yoon"], "title": "Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation", "comment": "Accepted at ICRA 2026. Supplementary Video: https://www.youtube.com/watch?v=bCNfdQ3RYKg. M. Yoon and J. Jeong contributed equally", "summary": "Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases. To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases. Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.", "AI": {"tldr": "提出了一种名为PAPL（Phase-Aware Policy Learning）的强化学习框架，用于解决四足机器人滑板控制中的感知驱动交互和多模态控制目标问题，实现了在模拟和真实世界中的有效滑板控制和效率提升。", "motivation": "控制滑板车（一种个人出行设备）的 legged robots 具有挑战性，因为存在感知驱动的交互和跨不同滑板阶段的多模态控制目标。", "method": "引入了 Phase-Aware Policy Learning (PAPL) 框架，一种用于滑板四足机器人的强化学习框架。PAPL 利用滑板的周期性，通过将相位条件特征归一化线性调制层集成到 actor 和 critic 网络中，实现了一个统一的策略，该策略捕获依赖于相位的行为，并跨相位共享机器人特定的知识。", "result": "在模拟中对命令跟踪精度进行了评估，并进行了消融研究量化了每个组件的贡献。与腿部和轮腿基线相比，推进效率得到了提升，并展示了现实世界的迁移能力。", "conclusion": "PAPL 框架能够有效地处理滑板控制中的感知驱动交互和多模态控制目标，实现了在模拟和现实世界中的鲁棒控制和效率提升。"}}
{"id": "2602.09463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09463", "abs": "https://arxiv.org/abs/2602.09463", "authors": ["Furong Jia", "Ling Dai", "Wenjin Deng", "Fan Zhang", "Chen Hu", "Daxin Jiang", "Yu Liu"], "title": "SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.", "AI": {"tldr": "SpotAgent 是一个框架，通过将地理定位形式化为使用外部工具进行验证的代理推理过程，来解决大型视觉语言模型 (LVLM) 在现实世界稀疏、长尾和模糊视觉线索下表现不佳的问题。", "motivation": "现有的 LVLM 在处理现实世界中稀疏、长尾和高度模糊的视觉线索时，在地理定位方面存在困难，并且它们的预测往往缺乏可验证性。", "method": "SpotAgent 采用代理推理方法，利用 ReAct 框架结合外部工具（如网络搜索、地图）进行视觉线索的探索和验证。该框架通过一个包含监督微调 (SFT)、利用多代理框架合成轨迹的代理冷启动阶段，以及基于空间感知动态过滤策略的强化学习 (RL) 的三阶段后训练流程。", "result": "SpotAgent 在标准基准测试中取得了最先进的性能，有效地减少了幻觉，并实现了精确且可验证的地理定位。", "conclusion": "SpotAgent 通过将地理定位构建为一个主动探索和工具辅助验证的代理推理过程，成功克服了 LVLM 在现实世界地理定位任务中的局限性，提高了准确性和可信度。"}}
{"id": "2602.09384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09384", "abs": "https://arxiv.org/abs/2602.09384", "authors": ["Eliza Mik"], "title": "Contractual Deepfakes: Can Large Language Models Generate Contracts?", "comment": "Accepted for publication", "summary": "Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas. Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction. This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.", "AI": {"tldr": "大型语言模型（LLM）在生成文本方面能力非凡，但它们并不真正理解语言的含义、上下文或进行推理。虽然它们能生成看似合理的合同文本，但这些文本可能是不一致或不适合特定交易的，因此LLM并不会威胁到法律行业的生存。", "motivation": "作者对将大型语言模型（LLM）视为可以轻易完成法律任务（如起草合同）的通用工具的观点感到担忧，并试图纠正这种“不合理的想法”。", "method": "本文采用批判性分析和论证的方法，对比了LLM的词语预测能力与法律实践中对语言的真正理解和法律推理能力。", "result": "LLM可以生成通用且表面上看似合理的合同文件，但这些文件可能包含不一致的条款，或者虽然可执行但却不适合特定交易。LLM的输出仅是统计学上占主导地位的词语模式的近似。", "conclusion": "LLM并不能真正理解语言或进行法律推理，因此它们生成的合同文件可能存在严重问题。将LLM视为对法律行业生存的威胁是一种过于简化的假设，作者认为这种想法是站不住脚的。"}}
{"id": "2602.09318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09318", "abs": "https://arxiv.org/abs/2602.09318", "authors": ["Lin-Guo Gao", "Suxing Liu"], "title": "GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification", "comment": null, "summary": "Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a \"blackbox\" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent \"IF-THEN\" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.", "AI": {"tldr": "提出了一种名为GAFRNet的图注意力和模糊规则网络，用于在标注数据有限的情况下对乳腺癌组织病理学图像进行可解释的分类，并在多个数据集上取得了优于现有方法的性能。", "motivation": "传统的深度学习模型在处理标注数据不足时性能下降，且缺乏可解释性，阻碍了其在临床上的应用。因此，研究需要一种在数据稀疏的情况下仍然鲁棒且可解释的组织病理学图像分类方法。", "method": "GAFRNet通过构建基于相似性的图来建模样本间的关系，并使用多头图注意力机制提取复杂的特征。同时，引入一个可微分的模糊规则模块，将节点度、聚类系数和标签一致性等拓扑描述符编码为可解释的“IF-THEN”逻辑，模仿医学专家的推理过程。", "result": "在BreakHis、Mini-DDSM和ICIAR2018三个基准数据集上进行了广泛评估，GAFRNet在不同放大倍率和分类任务上均一致优于现有的最先进方法。", "conclusion": "GAFRNet是一种鲁棒且可解释的图注意力和模糊规则网络，能够有效处理弱监督下的组织病理学图像分类问题，并提供清晰的推理过程，是一个可靠的决策支持工具。"}}
{"id": "2602.09485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09485", "abs": "https://arxiv.org/abs/2602.09485", "authors": ["Yizhi Wang", "Linan Yue", "Min-Ling Zhang"], "title": "Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models", "comment": null, "summary": "Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.", "AI": {"tldr": "本文提出了一种名为XMCC的可解释多模态长推理链压缩器，通过强化学习优化压缩过程，以缩短推理长度同时保留关键信息和答案正确性，并生成可解释的压缩决策理由。", "motivation": "现有的长推理链在多模态复杂任务中虽然能捕捉详细视觉信息，但存在过长和冗余的问题，影响推理效率。现有的压缩方法可能损害视觉-文本推理的完整性，且缺乏可解释性。", "method": "将压缩过程建模为序列决策问题，并利用强化学习进行优化。XMCC通过生成自然语言解释来阐述其压缩决策。", "result": "XMCC能够有效缩短推理轨迹，同时保留关键推理步骤和答案的正确性。实验证明XMCC在多模态推理基准测试中，不仅缩短了推理长度，还提供了可解释的理由。", "conclusion": "XMCC是一种有效且可解释的多模态长推理链压缩方法，能够克服现有方法的局限性，在保证推理质量的同时提高效率。"}}
{"id": "2602.09388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09388", "abs": "https://arxiv.org/abs/2602.09388", "authors": ["Jianyu Zheng"], "title": "Effective vocabulary expanding of multilingual language models for extremely low-resource languages", "comment": "12 pages, 5 figures, 7 tables, under review", "summary": "Multilingual pre-trained language models(mPLMs) offer significant benefits for many low-resource languages. To further expand the range of languages these models can support, many works focus on continued pre-training of these models. However, few works address how to extend mPLMs to low-resource languages that were previously unsupported. To tackle this issue, we expand the model's vocabulary using a target language corpus. We then screen out a subset from the model's original vocabulary, which is biased towards representing the source language(e.g. English), and utilize bilingual dictionaries to initialize the representations of the expanded vocabulary. Subsequently, we continue to pre-train the mPLMs using the target language corpus, based on the representations of these expanded vocabulary. Experimental results show that our proposed method outperforms the baseline, which uses randomly initialized expanded vocabulary for continued pre-training, in POS tagging and NER tasks, achieving improvements by 0.54% and 2.60%, respectively. Furthermore, our method demonstrates high robustness in selecting the training corpora, and the models' performance on the source language does not degrade after continued pre-training.", "AI": {"tldr": "提出一种扩展多语言预训练语言模型（mPLMs）支持低资源语言的方法，通过扩充词汇表、利用双语词典初始化新词表示，并结合目标语言语料进行继续预训练，有效提升了模型在低资源语言上的性能，且不影响源语言性能。", "motivation": "现有工作多关注已支持语言的继续预训练，但鲜有研究解决如何将mPLMs扩展到之前不支持的低资源语言的问题。", "method": "1. 扩展模型词汇表，使用目标语言语料库；2. 筛选出模型原始词汇表中偏向源语言（如英语）的词汇子集；3. 利用双语词典初始化新扩展词汇的表示；4. 基于初始化后的词汇表示，使用目标语言语料库继续预训练mPLMs。", "result": "所提方法在POS tagging和NER任务上优于随机初始化新词表示的基线方法，分别提升0.54%和2.60%。该方法在选择训练语料时表现出高鲁棒性，且继续预训练后源语言性能无明显下降。", "conclusion": "通过词汇表扩展和双语词典初始化，能够有效将mPLMs扩展到新的低资源语言，且该方法在性能提升和鲁棒性方面均优于基线方法，同时能保持对源语言的性能。"}}
{"id": "2602.09430", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09430", "abs": "https://arxiv.org/abs/2602.09430", "authors": ["Yiwen Pang", "Bo Zhou", "Changjin Li", "Xuanhao Wang", "Shengxiang Xu", "Deng-Bao Wang", "Min-Ling Zhang", "Shimin Di"], "title": "Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments", "comment": null, "summary": "Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.", "AI": {"tldr": "提出了一种用于科学实验长时序任务的Agentic VLA推理插件，通过LLM驱动的代理推理机制，无需额外训练即可引导VLA模型执行复合任务，提高了实验成功率，并可从模拟环境迁移至真实实验室。", "motivation": "现有的视觉-语言-动作（VLA）模型在执行长时序的科学实验复合任务时存在困难，即使模型能执行单个原子任务，也难以组合这些原子任务以完成复杂工作流，这源于训练和推理时任务分布的错配。", "method": "提出一个Agentic VLA推理插件，该插件基于LLM的代理推理机制，在执行序列操作任务时进行干预。插件通过显式推理和生成过渡性机器人动作代码，指导VLA模型完成缺失的过渡步骤，从而实现复合科学工作流的可靠执行，而无需额外的训练。", "result": "在模拟环境中，该方法将每原子任务的平均成功率提高了42%。同时，证明了该方法可以轻松地从模拟环境迁移到真实的科学实验室。", "conclusion": "提出的Agentic VLA推理插件能够有效地解决VLA模型在科学实验长时序任务中的挑战，通过推理干预实现无需训练的复合任务执行，具有计算和数据效率，适用于开放式的长时序机器人实验室任务。"}}
{"id": "2602.09337", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09337", "abs": "https://arxiv.org/abs/2602.09337", "authors": ["Michail S. Alexiou", "Nikolaos G. Bourbakis"], "title": "Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents", "comment": null, "summary": "Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.", "AI": {"tldr": "本文提出了一种名为Kyrtos的方法，用于自动识别和分析技术文档图片中的曲线图表，将其转化为有意义的归属图和自然语言描述，并最终用于构建随机Petri网。", "motivation": "技术文档中包含大量有价值的知识，而对其进行深度理解是吸引人的研究领域。理解技术文档的整体含义需要准确分析其中的各种模态（如图形、表格、文本等）及其相互关联。特别地，本文关注图形中的曲线图表。", "method": "Kyrtos方法包括两个主要部分：1. 识别：采用基于聚类的方法识别构成曲线的线段的中间点。2. 分析：解析提取的线段以捕捉方向、趋势等行为特征，并将这些关系转换为归属图，以保留曲线的结构特征。最后，将图关系转化为自然语言文本，并用于生成随机Petri网。", "result": "Kyrtos方法能够准确地识别和分析曲线图表，生成近似的曲线，并有效地将其结构特征转化为归属图、自然语言描述和随机Petri网。实验结果证明了其识别和分析方法的准确性。", "conclusion": "Kyrtos方法为自动识别和分析技术文档中的曲线图表提供了一种有效的方法，能够捕捉其结构特征并将其转化为多种形式（归属图、自然语言、随机Petri网），从而增强技术文档的理解和分析能力。"}}
{"id": "2602.09489", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09489", "abs": "https://arxiv.org/abs/2602.09489", "authors": ["Lars Henry Berge Olsen", "Dennis Christensen"], "title": "Computing Conditional Shapley Values Using Tabular Foundation Models", "comment": null, "summary": "Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.", "AI": {"tldr": "本文研究了使用TabPFN等表格型基础模型来高效计算Shapley值的方法，发现在大多数情况下TabPFN在性能和计算效率上优于现有技术。", "motivation": "传统的Shapley值计算方法计算成本高昂，尤其是在特征依赖时。现有方法难以充分利用深度学习加速计算，因此需要更有效率的方法来计算Shapley值。", "method": "使用TabPFN及其变体，并结合样本外学习（in-context learning）的能力，来近似计算Shapley值所需的条件期望。通过在模拟和真实数据集上进行实验，将TabPFN的性能与现有最先进方法进行比较。", "result": "在大多数情况下，TabPFN在计算Shapley值时性能最优。即使在性能稍逊的情况下，其结果也仅比最佳方法略差，但计算时间却大大缩短。", "conclusion": "表格型基础模型（如TabPFN）为高效计算Shapley值提供了一种有前景的解决方案。本文验证了TabPFN在计算Shapley值方面的优势，并提出了未来进一步改进和模型适配的方向。"}}
{"id": "2602.09472", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09472", "abs": "https://arxiv.org/abs/2602.09472", "authors": ["Shuyuan Hu", "Tao Lin", "Kai Ye", "Yang Yang", "Tianwei Zhang"], "title": "LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration", "comment": null, "summary": "While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.", "AI": {"tldr": "提出了一种神经符号框架，将大型语言模型（LLM）的推理能力与线性时序逻辑（LTL）相结合，用于解决多机器人任务分配和规划（STAP）问题，并能处理动态环境变化，实验证明其性能优于基线方法。", "motivation": "现有的大型语言模型虽然方便非专家指定多机器人任务，但生成的计划在运动学上不可行且效率低下，尤其是在长时程任务中。而形式化方法（如LTL）虽然能保证正确性和最优性，但通常局限于静态、离线场景且计算可扩展性差。现有方法在灵活性和效率方面存在不足。", "method": "提出了一种神经符号框架，将LLM推理能力整合到分层LTL规范中，并解决相应的STAP问题。采用倒推视界规划（RHP）循环，结合实时感知，能够解决随机环境变化（如移动用户或更新指令），并通过分层状态空间动态优化计划。", "result": "该方法在成功率和交互流畅性方面显著优于基线方法，同时最小化了规划延迟。在真实世界实验中得到了验证。", "conclusion": "该神经符号框架能够有效弥合LLM的易用性和形式化方法的严谨性之间的差距，并且通过结合RHP和实时感知，能够应对动态多机器人任务环境的挑战，实现高效、可行的任务规划。"}}
{"id": "2602.09416", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.09416", "abs": "https://arxiv.org/abs/2602.09416", "authors": ["Andrew Shaw", "Christina Hahn", "Catherine Rasgaitis", "Yash Mishra", "Alisa Liu", "Natasha Jaques", "Yulia Tsvetkov", "Amy X. Zhang"], "title": "Are Language Models Sensitive to Morally Irrelevant Distractors?", "comment": null, "summary": "With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this \"situationist\" view of moral psychology to evaluate whether LLMs exhibit similar cognitive moral biases to humans. We curate a novel multimodal dataset of 60 \"moral distractors\" from existing psychological datasets of emotionally-valenced images and narratives which have no moral relevance to the situation presented. After injecting these distractors into existing moral benchmarks to measure their effects on LLM responses, we find that moral distractors can shift the moral judgements of LLMs by over 30% even in low-ambiguity scenarios, highlighting the need for more contextual moral evaluations and more nuanced cognitive moral modeling of LLMs.", "AI": {"tldr": "本研究发现，与人类类似，大型语言模型（LLMs）在道德判断时也会受到道德无关的“干扰因素”影响，例如情绪化图片或叙述，这表明LLMs的道德评估需要更强的背景感知和更细致的认知建模。", "motivation": "随着大型语言模型在高风险领域的广泛应用，确保其行为符合人类价值观变得至关重要。以往的研究假设LLMs的道德偏好是稳定的，但人类道德心理学的研究表明，人类的道德判断会受到无关的“情境主义”因素影响。因此，本研究旨在检验LLMs是否也表现出类似的人类认知道德偏见。", "method": "研究者从现有心理学数据集中收集了60个“道德干扰项”（包括情绪化图片和叙述），这些干扰项与所呈现的道德情境本身没有道德相关性。然后，研究者将这些干扰项注入到现有的道德基准测试中，以衡量它们对LLM响应的影响。", "result": "研究发现，即使在低歧义情境下，道德干扰项也能使LLMs的道德判断发生超过30%的偏移。这表明LLMs的道德判断并非完全稳定，并且容易受到无关情境因素的影响。", "conclusion": "本研究证明了LLMs在道德判断方面表现出与人类相似的“情境主义”偏见，即容易受到道德无关的干扰因素影响。这强调了在评估LLMs的道德性时，需要进行更具情境性的考量，并开发更精细的LLMs认知道德模型。"}}
{"id": "2602.09324", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09324", "abs": "https://arxiv.org/abs/2602.09324", "authors": ["Ahmad Chaddad", "Yihang Wu", "Xianrui Chen"], "title": "Deep Modeling and Interpretation for Bladder Cancer Classification", "comment": "Accepted in IEEE SMC 2025", "summary": "Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\\sim 60\\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.", "AI": {"tldr": "该研究评估了多种深度学习模型（CNN 和 ViT）在膀胱癌图像分类任务中的表现，发现 ConvNeXt 系列泛化能力有限，而 ViT 系列在模型校准和解释性方面表现更优，但没有模型能完全满足所有需求，ConvNeXt 适合 in-distribution 样本，ViT 适合 out-of-distribution 样本。", "motivation": "自然图像数据集上的深度学习模型（ViT 和 CNN）表现出色，但在医学影像中，异常区域通常只占图像一小部分，导致这些模型在医学影像任务上可能表现不佳。因此，本研究旨在评估最新的深度模型在膀胱癌分类任务上的表现。", "method": "研究评估了 13 种深度模型（4 种 CNN 和 8 种 Transformer 模型），进行了标准分类、校准分析（评估模型在膀胱癌分类中的校准程度），并使用 GradCAM++ 评估模型的可解释性。研究在公开的多中心膀胱癌数据集上模拟了约 300 个实验，并引入了测试时增强（test time augmentation）来提高模型的可解释性。", "result": "ConvNeXt 系列模型在膀胱癌图像分类任务上的泛化能力有限，准确率约为 60%。ViT 系列模型比 ConvNeXt 和 Swin Transformer 系列模型具有更好的校准效果。研究发现没有模型能够提供一个万能的、可解释的模型解决方案。", "conclusion": "ConvNeXt 系列模型适用于 in-distribution 样本，而 ViT 及其变体适用于解释 out-of-distribution 样本。在膀胱癌分类任务中，没有单一模型能提供最优的性能、校准和可解释性。"}}
{"id": "2602.09533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09533", "abs": "https://arxiv.org/abs/2602.09533", "authors": ["Masanari Oi", "Mahiro Ukai", "Masahiro Kaneko", "Naoaki Okazaki", "Nakamasa Inoue"], "title": "Autoregressive Direct Preference Optimization", "comment": null, "summary": "Direct preference optimization (DPO) has emerged as a promising approach for aligning large language models (LLMs) with human preferences. However, the widespread reliance on the response-level Bradley-Terry (BT) model may limit its full potential, as the reference and learnable models are assumed to be autoregressive only after deriving the objective function. Motivated by this limitation, we revisit the theoretical foundations of DPO and propose a novel formulation that explicitly introduces the autoregressive assumption prior to applying the BT model. By reformulating and extending DPO, we derive a novel variant, termed Autoregressive DPO (ADPO), that explicitly integrates autoregressive modeling into the preference optimization framework. Without violating the theoretical foundations, the derived loss takes an elegant form: it shifts the summation operation in the DPO objective outside the log-sigmoid function. Furthermore, through theoretical analysis of ADPO, we show that there exist two length measures to be considered when designing DPO-based algorithms: the token length $μ$ and the feedback length $μ$'. To the best of our knowledge, we are the first to explicitly distinguish these two measures and analyze their implications for preference optimization in LLMs.", "AI": {"tldr": "本文提出了一种名为ADPO的新型DPO变体，通过在应用Bradley-Terry模型前明确引入自回归假设，并将DPO目标函数中的求和操作移至log-sigmoid函数之外，从而改进了LLM与人类偏好的对齐。ADPO区分并分析了token长度和反馈长度两种度量，这是该领域首次进行此类区分。", "motivation": "现有DPO方法依赖于响应级Bradley-Terry模型，该模型在推导目标函数后才假定参考模型和可学习模型是自回归的，这可能限制了其潜力。", "method": "通过在应用Bradley-Terry模型之前明确引入自回归假设，并重新构建和扩展DPO，推导出了Autoregressive DPO (ADPO)。ADPO的目标函数形式为将DPO目标中的求和操作移至log-sigmoid函数之外。对ADPO进行了理论分析，区分了token长度（μ）和反馈长度（μ'）这两种长度度量。", "result": "ADPO的损失函数形式得到了优化。理论分析表明，存在两种长度度量（token长度μ和反馈长度μ'）需要考虑，并且ADPO显式地区分了它们，这是LLM偏好优化领域首次做到。", "conclusion": "ADPO通过将自回归假设显式地集成到偏好优化框架中，并区分token长度和反馈长度，为改进LLM与人类偏好的对齐提供了一个新的理论和实践方向。"}}
{"id": "2602.09438", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09438", "abs": "https://arxiv.org/abs/2602.09438", "authors": ["Taewoong Yoon", "Geunyeong Jeong", "Geon Park", "Sihyeong Yeom", "Harksoo Kim"], "title": "Breaking the Pre-Sampling Barrier: Activation-Informed Difficulty-Aware Self-Consistency", "comment": null, "summary": "Self-Consistency (SC) is an effective decoding strategy that improves the reasoning performance of Large Language Models (LLMs) by generating multiple chain-of-thought reasoning paths and selecting the final answer via majority voting. However, it suffers from substantial inference costs because it requires a large number of samples. To mitigate this issue, Difficulty-Adaptive Self-Consistency (DSC) was proposed to reduce unnecessary token usage for easy problems by adjusting the number of samples according to problem difficulty. However, DSC requires additional model calls and pre-sampling to estimate difficulty, and this process is repeated when applying to each dataset, leading to significant computational overhead. In this work, we propose Activation-Informed Difficulty-Aware Self-Consistency (ACTSC) to address these limitations. ACTSC leverages internal difficulty signals reflected in the feed-forward network neuron activations to construct a lightweight difficulty estimation probe, without any additional token generation or model calls. The probe dynamically adjusts the number of samples for SC and can be applied to new datasets without requiring pre-sampling for difficulty estimation. To validate its effectiveness, we conduct experiments on five benchmarks. Experimental results show that ACTSC effectively reduces inference costs while maintaining accuracy relative to existing methods.", "AI": {"tldr": "提出了一种名为ACTSC（Activation-Informed Difficulty-Aware Self-Consistency）的新型解码策略，通过利用前馈网络激活信号来动态调整自洽性（Self-Consistency）的采样数量，从而在不增加额外计算成本的情况下，有效降低推理成本并保持准确性。", "motivation": "现有的自洽性（SC）策略虽然能提升LLM的推理能力，但推理成本高昂。后来的DSC策略虽然尝试优化，但引入了额外的模型调用和预采样来估计难度，计算开销依然很大。因此，研究者希望找到一种更轻量级、无需额外调参就能自适应调整采样数量的方法。", "method": "ACTSC利用前馈网络（FFN）中的神经元激活信号作为内部难度指示器，构建了一个轻量级的难度估计探针。该探针无需额外的token生成或模型调用，即可动态调整SC策略所需的采样数量，并且可以直接应用于新数据集，无需进行预采样来估计难度。", "result": "在五个基准测试上的实验表明，ACTSC能够有效地降低推理成本，同时在准确性方面与现有方法相当。", "conclusion": "ACTSC是一种有效的、轻量级的解码策略，它通过利用模型内部激活信号来适应性地调整自洽性采样数量，从而解决了现有方法在推理成本和计算开销方面存在的问题，并在保持性能的同时显著降低了计算需求。"}}
{"id": "2602.09563", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.09563", "abs": "https://arxiv.org/abs/2602.09563", "authors": ["Lucas Palazzolo", "Mickaël Binois", "Laëtitia Giraldi"], "title": "Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization", "comment": "16 pages, 16 figures", "summary": "Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.", "AI": {"tldr": "本文将微型游泳器的轨迹跟踪问题转化为一个最优控制问题，并结合B样条插值和贝叶斯优化方法进行求解，成功实现了对旗帜状磁性游泳器和三球游泳器的轨迹跟踪，并能适应壁面水动力学效应。该方法通用性强，适用于不同精度的模型。", "motivation": "在微机器人领域，低雷诺数动力学使得微型游泳器的轨迹跟踪控制设计尤为复杂，需要一种能够处理高计算成本且无需复杂梯度计算的有效方法。", "method": "将轨迹跟踪问题表述为最优控制问题，并利用B样条插值法进行参数化，结合贝叶斯优化方法进行求解。该方法应用于旗帜状磁性游泳器和三球游泳器模型。", "result": "所提出的方法能够成功复现多种目标轨迹，包括生物启发式的路径，并且能够适应并部分补偿壁面引起的水动力学效应。该优化策略适用于不同精度的模型，从低维ODE模型到高精度PDE模型。", "conclusion": "贝叶斯优化是一种处理微尺度运动学复杂流体-结构相互作用问题的通用最优控制策略的有效工具，具有鲁棒性和普适性。"}}
{"id": "2602.09597", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.09597", "abs": "https://arxiv.org/abs/2602.09597", "authors": ["Martin Bauw"], "title": "Detecting radar targets swarms in range profiles with a partially complex-valued neural network", "comment": null, "summary": "Correctly detecting radar targets is usually challenged by clutter and waveform distortion. An additional difficulty stems from the relative proximity of several targets, the latter being perceived as a single target in the worst case, or influencing each other's detection thresholds. The negative impact of targets proximity notably depends on the range resolution defined by the radar parameters and the adaptive threshold adopted. This paper addresses the matter of targets detection in radar range profiles containing multiple targets with varying proximity and distorted echoes. Inspired by recent contributions in the radar and signal processing literature, this work proposes partially complex-valued neural networks as an adaptive range profile processing. Simulated datasets are generated and experiments are conducted to compare a common pulse compression approach with a simple neural network partially defined by complex-valued parameters. Whereas the pulse compression processes one pulse length at a time, the neural network put forward is a generative architecture going through the entire received signal in one go to generate a complete detection profile.", "AI": {"tldr": "本文提出使用部分复值神经网络来处理雷达回波信号，以解决杂波、波形失真以及目标近距离带来的检测挑战。", "motivation": "雷达检测面临杂波、波形失真以及多个目标近距离造成的相互干扰和检测阈值问题。目标之间的近距离对雷达参数决定的距离分辨率和自适应阈值有负面影响。", "method": "提出使用部分复值神经网络作为一种自适应距离剖面处理方法。通过生成仿真数据集进行实验，将提出的神经网络方法与常用的脉冲压缩方法进行比较。神经网络是一种生成式架构，可以一次性处理整个接收信号，生成完整的检测剖面。", "result": "实验表明，提出的神经网络方法在处理具有不同近距离和失真回波的多个目标时，优于传统的脉冲压缩方法。脉冲压缩一次处理一个脉冲长度，而神经网络一次处理整个信号。", "conclusion": "部分复值神经网络是处理雷达距离剖面中存在近距离和失真目标的一种有效且自适应的方法，能够克服传统脉冲压缩方法的局限性。"}}
{"id": "2602.09442", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09442", "abs": "https://arxiv.org/abs/2602.09442", "authors": ["Shweta Parihar", "Lu Cheng"], "title": "Evaluating Social Bias in RAG Systems: When External Context Helps and Reasoning Hurts", "comment": "Accepted as a full paper with an oral presentation at the 30th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2026)", "summary": "Social biases inherent in large language models (LLMs) raise significant fairness concerns. Retrieval-Augmented Generation (RAG) architectures, which retrieve external knowledge sources to enhance the generative capabilities of LLMs, remain susceptible to the same bias-related challenges. This work focuses on evaluating and understanding the social bias implications of RAG. Through extensive experiments across various retrieval corpora, LLMs, and bias evaluation datasets, encompassing more than 13 different bias types, we surprisingly observe a reduction in bias in RAG. This suggests that the inclusion of external context can help counteract stereotype-driven predictions, potentially improving fairness by diversifying the contextual grounding of the model's outputs. To better understand this phenomenon, we then explore the model's reasoning process by integrating Chain-of-Thought (CoT) prompting into RAG while assessing the faithfulness of the model's CoT. Our experiments reveal that the model's bias inclinations shift between stereotype and anti-stereotype responses as more contextual information is incorporated from the retrieved documents. Interestingly, we find that while CoT enhances accuracy, contrary to the bias reduction observed with RAG, it increases overall bias across datasets, highlighting the need for bias-aware reasoning frameworks that can mitigate this trade-off.", "AI": {"tldr": "研究发现，检索增强生成（RAG）模型在引入外部知识后，反而能减少社会偏见，但结合思维链（CoT）提示时，虽然提高了准确性，却增加了偏见，表明需要偏见感知型推理框架。", "motivation": "大型语言模型（LLMs）固有的社会偏见带来了公平性问题，而检索增强生成（RAG）架构也面临同样的挑战。因此，研究的动机在于评估和理解RAG在社会偏见方面的影响。", "method": "通过在不同的检索语料库、LLMs和包含13种以上偏见类型的评估数据集中进行大量实验来评估RAG的偏见。随后，通过集成思维链（CoT）提示并评估其忠实度来探索模型的推理过程。", "result": "实验结果显示，RAG在外部语境的引入下，出人意料地减少了偏见。而CoT虽然提高了准确性，但却增加了整体偏见，并且模型的偏见倾向会在引入更多检索信息时，在刻板印象和反刻板印象回答之间转移。", "conclusion": "外部语境可以帮助抵消由刻板印象驱动的预测，从而可能通过使模型输出的语境更加多样化来提高公平性。然而，CoT的引入虽然提高了准确性，却会增加偏见，这凸显了开发能够减轻这种权衡的偏见感知型推理框架的必要性。"}}
{"id": "2602.09444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09444", "abs": "https://arxiv.org/abs/2602.09444", "authors": ["Takumi Ohashi", "Hitoshi Iyatomi"], "title": "Conceptual Cultural Index: A Metric for Cultural Specificity via Relative Generality", "comment": "9 pages, 2 figures, 8 tables. Accepted at the First Workshop on Multilingual Multicultural Evaluation (MME) @ EACL 2026", "summary": "Large language models (LLMs) are increasingly deployed in multicultural settings; however, systematic evaluation of cultural specificity at the sentence level remains underexplored. We propose the Conceptual Cultural Index (CCI), which estimates cultural specificity at the sentence level. CCI is defined as the difference between the generality estimate within the target culture and the average generality estimate across other cultures. This formulation enables users to operationally control the scope of culture via comparison settings and provides interpretability, since the score derives from the underlying generality estimates. We validate CCI on 400 sentences (200 culture-specific and 200 general), and the resulting score distribution exhibits the anticipated pattern: higher for culture-specific sentences and lower for general ones. For binary separability, CCI outperforms direct LLM scoring, yielding more than a 10-point improvement in AUC for models specialized to the target culture. Our code is available at https://github.com/IyatomiLab/CCI .", "AI": {"tldr": "本研究提出了一种名为概念文化指数（CCI）的新方法，用于在句子层面量化语言模型的文化特异性，并证明了其有效性。", "motivation": "随着大型语言模型（LLMs）在多元文化环境中的广泛应用，对语言模型在句子层面的文化特异性进行系统性评估的需求日益增长，而现有研究对这一领域的探索不足。", "method": "研究提出了一种概念文化指数（CCI），该指数定义为目标文化内部的通用性估计值与跨其他文化的平均通用性估计值之间的差异。这种方法允许用户通过比较设置来操作性地控制文化的范围，并通过基础通用性估计值提供可解释性。", "result": "通过对400个句子（200个文化特异性句子和200个通用句子）的验证，CCI得分分布呈现出预期的模式：文化特异性句子的得分更高，通用句子的得分更低。在二元可分性方面，CCI比直接的LLM评分表现更好，对于针对目标文化优化的模型，AUC（Area Under the Curve）提高了10个百分点以上。", "conclusion": "CCI是一种有效的方法，可以在句子层面量化大型语言模型的文化特异性，并且在区分文化特异性句子和通用句子方面优于直接的LLM评分方法。"}}
{"id": "2602.09580", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09580", "abs": "https://arxiv.org/abs/2602.09580", "authors": ["Chenyu Yang", "Denis Tarasov", "Davide Liconti", "Hehui Zheng", "Robert K. Katzschmann"], "title": "Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows", "comment": null, "summary": "Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.", "AI": {"tldr": "本文提出了SOFT-FLOW，一种用于真实世界灵巧操作策略的样本高效的离线微调框架，通过使用归一化流（NF）生成策略和动作分块的批评者，克服了数据效率低和动作分布多模态的挑战。", "motivation": "真实世界灵巧操作策略的微调面临数据不足和多模态动作分布的挑战。现有的基于扩散的策略由于动作概率不可追踪，无法进行保守的基于似然的更新；而传统的高斯策略在多模态情况下会失效，尤其是在分块执行动作时，标准的每步批评者也无法与分块执行对齐，导致信用分配不佳。", "method": "SOFT-FLOW框架使用归一化流（NF）来建模多模态动作分块，从而获得精确的似然，实现保守稳定的策略更新。同时，引入一个动作分块的批评者来评估整个动作序列，使其价值估计与策略的时间结构对齐，从而改善长时信用分配。", "result": "在真实的剪刀割胶带和手中立方体旋转两个具有挑战性的灵巧操作任务上，SOFT-FLOW实现了稳定且样本高效的适应性，而标准方法则表现不佳。", "conclusion": "SOFT-FLOW是首个将基于似然的多模态生成策略与真实机器人硬件上的分块级价值学习相结合的框架，成功解决了真实世界灵巧操作策略微调中的样本效率和多模态挑战。"}}
{"id": "2602.09583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09583", "abs": "https://arxiv.org/abs/2602.09583", "authors": ["Marco Moletta", "Michael C. Welle", "Danica Kragic"], "title": "Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation", "comment": null, "summary": "Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.", "AI": {"tldr": "本研究提出了一种名为 RKO 的新方法，用于将预训练的视觉运动扩散策略调整为反映人类对可变形物体（如衣物）操纵任务的偏好，仅通过有限的演示。结果表明 RKO 在真实世界的折叠衣物任务中优于其他偏好学习方法。", "motivation": "当前机器人操纵领域，尤其是处理可变形物体时，对人类用户细微且难以言喻的操纵偏好研究不足。为了提高机器人的个性和用户满意度，需要研究如何让机器人适应这些偏好。", "method": "提出了一种名为 RKO 的新偏好对齐方法，结合了 RPO 和 KTO 两个现有框架的优点。该方法用于将预训练的视觉运动扩散策略（visuomotor diffusion policies）根据有限的演示数据调整为反映用户偏好。", "result": "在真实世界的多种衣物折叠任务上，RKO 方法与 RPO、KTO 以及基线扩散策略相比，在偏好对齐策略方面取得了更优越的性能和更高的样本效率。", "conclusion": "结构化的偏好学习对于在复杂的可变形物体操纵任务中扩展个性化机器人行为是重要且可行的。RKO 方法在这种场景下展现了其有效性。"}}
{"id": "2602.09407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09407", "abs": "https://arxiv.org/abs/2602.09407", "authors": ["Yan Luo", "Advaith Ravishankar", "Serena Liu", "Yutong Yang", "Mengyu Wang"], "title": "Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D", "comment": null, "summary": "A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.", "AI": {"tldr": "该研究评估了五种最先进的图像到3D基础模型在单切片医学图像到3D重建方面的零样本性能，发现它们在医学数据上的表现不稳定，尤其是深度重建失败，并强调了多视图重建的必要性。", "motivation": "当前医学成像成本高且等待时间长，而图像到3D基础模型有望从2D模态重建3D数据。然而，这些模型在自然图像上学到的几何先验是否能迁移到医学数据上尚不清楚，因此需要进行评估。", "method": "研究提出了一个对照零样本基准，评估了SAM3D、Hunyuan3D-2.1、Direct3D、Hi3DGen和TripoSG这五种最先进的图像到3D模型，在六个医学数据集和两个自然数据集上进行了单切片医学图像到3D重建的实验，并使用了基于体素的指标和点云距离指标进行评估。", "result": "在医学数据集上，所有模型基于体素的重叠度适中，这与从单切片推断体积时出现的深度重建失败模式一致。然而，全局距离指标显示模型间存在差异：SAM3D在拓扑相似性上表现最佳，而其他模型则倾向于过度简化重建。", "conclusion": "研究结果量化了单切片医学重建的局限性，并突出了2D医学数据平面性导致的深度歧义问题，这表明需要进行多视图图像到3D重建以实现可靠的医学3D推理。"}}
{"id": "2602.09378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09378", "abs": "https://arxiv.org/abs/2602.09378", "authors": ["Jun Li"], "title": "Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation", "comment": "Accepted by ESWA 2026", "summary": "Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.", "AI": {"tldr": "本文提出了一种名为 DBiSL 的全可微分双向协同学习框架，用于医学图像分割的半监督学习。该框架通过在线的双向任务协作，整合了监督学习、一致性正则化、伪监督学习和不确定性估计，并在两个基准数据集上取得了最先进的性能。", "motivation": "医学图像分析中高质量标注数据的稀缺是主要挑战，标注成本高且需要专业知识。半监督学习（SSL）有潜力解决这一瓶颈，但现有方法（如伪标签和一致性正则化）在利用无标签数据方面存在局限。特别是，当前双任务协同学习方法仅限于单向交互，未能充分利用在线双向交叉任务协作的潜力。", "method": "提出全可微分双向协同学习（DBiSL）框架。该框架无缝集成并增强了四个关键的 SSL 组件：监督学习、一致性正则化、伪监督学习和不确定性估计。通过在线的双向任务协作，实现分割结果向回归输出的转换，以及回归结果向分割输出的反馈，从而实现更深层次的跨任务协同。", "result": "在两个医学图像分割的基准数据集上进行了实验，证明了 DBiSL 框架的性能优于现有最先进的方法。", "conclusion": "DBiSL 框架通过实现全可微分的双向任务协同，有效提升了半监督医学图像分割的性能，克服了现有方法的局限性。该研究为统一 SSL 框架设计提供了新见解，并为双任务驱动的 SSL 奠定了新的架构基础，同时该框架也适用于更广泛的计算机视觉应用。"}}
{"id": "2602.09620", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.09620", "abs": "https://arxiv.org/abs/2602.09620", "authors": ["Jorge Fandinno", "Pedro Cabalar", "Philipp Wanko", "Torsten Schaub"], "title": "FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints", "comment": null, "summary": "Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.", "AI": {"tldr": "本文提出了一种名为 FLINGO 的新语言和工具，它将 ASP 中数值属性的丰富表达能力（如默认值、非确定性赋值）集成到 CASP 的数值约束中，并提供了一种将其转换为标准 CASP 程序的翻译方法。", "motivation": "现有的 CASP 求解器在表示数值属性时，其表达能力不如 ASP，导致 ASP 中的一些重要特性（如默认值、非确定性赋值）在 CASP 中丢失，而这些特性在实际应用中很重要。", "method": "提出 FLINGO 语言和工具，将 ASP 中数值属性的丰富表达能力（如默认值、非确定性赋值、聚合值）直接纳入数值约束。基于已有的语义基础，实现了一个将 FLINGO 语法翻译成 CLINGCON 输入格式的 CASP 程序的翻译器。", "result": "FLINGO 语言和工具能够表达 ASP 中数值属性的丰富特性，并提供了一种将其转换为标准 CASP 程序的方法。文中通过多个示例展示了 FLINGO 的用法。", "conclusion": "FLINGO 语言和工具有效地弥合了 ASP 和 CASP 在数值属性表示方面的差距，为在 CASP 中实现更丰富的数值约束表达提供了新的途径，并支持现有的 CASP 输入格式。"}}
{"id": "2602.09355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09355", "abs": "https://arxiv.org/abs/2602.09355", "authors": ["Yihang Wu", "Ahmad Chaddad"], "title": "Impact of domain adaptation in deep learning for medical image classifications", "comment": "Accepted in IEEE SMC 2025", "summary": "Domain adaptation (DA) is a quickly expanding area in machine learning that involves adjusting a model trained in one domain to perform well in another domain. While there have been notable progressions, the fundamental concept of numerous DA methodologies has persisted: aligning the data from various domains into a shared feature space. In this space, knowledge acquired from labeled source data can improve the model training on target data that lacks sufficient labels. In this study, we demonstrate the use of 10 deep learning models to simulate common DA techniques and explore their application in four medical image datasets. We have considered various situations such as multi-modality, noisy data, federated learning (FL), interpretability analysis, and classifier calibration. The experimental results indicate that using DA with ResNet34 in a brain tumor (BT) data set results in an enhancement of 4.7\\% in model performance. Similarly, the use of DA can reduce the impact of Gaussian noise, as it provides $\\sim 3\\%$ accuracy increase using ResNet34 on a BT dataset. Furthermore, simply introducing DA into FL framework shows limited potential (e.g., $\\sim 0.3\\%$ increase in performance) for skin cancer classification. In addition, the DA method can improve the interpretability of the models using the gradcam++ technique, which offers clinical values. Calibration analysis also demonstrates that using DA provides a lower expected calibration error (ECE) value $\\sim 2\\%$ compared to CNN alone on a multi-modality dataset.", "AI": {"tldr": "该研究探讨了深度学习模型在不同领域自适应（DA）方面的应用，尤其是在医学影像领域。研究表明，DA可以提高模型性能，尤其是在处理噪声数据和增强模型可解释性方面，但其在联邦学习中的效果有限。", "motivation": "尽管领域自适应（DA）在机器学习领域取得了显著进展，但现有方法大多依赖于将不同领域的数据对齐到共享特征空间。本研究旨在探索DA在更广泛的医学影像场景中的应用，并评估其在不同情况下的有效性。", "method": "研究使用了10种深度学习模型来模拟常见的DA技术，并将其应用于四个医学影像数据集。研究人员考虑了多模态、噪声数据、联邦学习（FL）、可解释性分析和分类器校准等多种情况。", "result": "在脑肿瘤（BT）数据集上，使用ResNet34和DA可以将模型性能提高4.7%。DA还可以通过ResNet34在BT数据集上提高约3%的准确率来减轻高斯噪声的影响。然而，在皮肤癌分类任务中，将DA引入FL框架仅能带来约0.3%的性能提升。此外，DA结合gradcam++技术可以提高模型的解释性，并在多模态数据集上通过降低约2%的ECE值来改善分类器校准。", "conclusion": "领域自适应（DA）在医学影像分析中具有潜力，可以提高模型性能、减轻噪声影响并增强模型的可解释性。然而，DA在联邦学习中的效果有限，需要进一步研究以优化其在特定场景下的应用。"}}
{"id": "2602.09653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09653", "abs": "https://arxiv.org/abs/2602.09653", "authors": ["Shiwei Lyu", "Xidong Wang", "Lei Liu", "Hao Zhu", "Chaohe Zhang", "Jian Wang", "Jinjie Gu", "Benyou Wang", "Yue Shen"], "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference", "comment": null, "summary": "Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.", "AI": {"tldr": "该研究提出了一种名为HealthRubrics和HealthPrinciples的两阶段框架，以解决大型语言模型（LLMs）在医学领域的开放式输出与细粒度临床医生偏好对齐的挑战。该框架通过 physician-verified 偏好数据和可复用的临床原则，实现了模型的高效临床对齐，并在HealthBench-Hard基准测试中取得了优于更大模型的性能。", "motivation": "现有方法在将LLM的开放式医学知识与临床医生细粒度偏好对齐方面存在挑战，通常依赖于粗粒度目标或与专业指南弱相关的不可靠的自动评估者。", "method": "研究提出一个两阶段框架：1.构建HealthRubrics数据集，包含7,034个由医生验证的偏好示例，医生在此基础上完善LLM起草的评估标准以符合严格的医学标准。2.将这些评估标准提炼为119个可复用的、临床可解释的HealthPrinciples，并将其用于离线对齐（合成未标记查询的评估标准）和推理时引导模型自我修正。", "result": "使用该框架训练的30B参数模型（推理时激活3B参数）在HealthBench-Hard基准测试中取得了33.4%的成绩，超越了Deepseek-R1和o3等更大模型，并为资源高效的临床对齐树立了基准。", "conclusion": "提出的两阶段框架，结合HealthRubrics数据集和HealthPrinciples，能够有效实现LLM在医学领域的细粒度临床对齐，并且在资源效率方面表现出色，为后续研究提供了有价值的资源和方法。"}}
{"id": "2602.09469", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09469", "abs": "https://arxiv.org/abs/2602.09469", "authors": ["Huu-Huy-Hoang Tran", "Gia-Bao Duong", "Quoc-Viet-Anh Tran", "Thi-Hai-Yen Vuong", "Hoang-Quynh Le"], "title": "NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts", "comment": null, "summary": "Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.", "AI": {"tldr": "本文提出了一种多输出集成系统，用于从西班牙语的电子健康记录中提取毒品使用信息，以应对低资源和领域特定的挑战，并在ToxHabits共享任务中取得了良好的性能。", "motivation": "从非结构化的电子健康记录中提取药物使用信息在临床自然语言处理中仍是一个重大挑战。尽管大型语言模型取得了进展，但在临床NLP中的应用受限于对信任、控制和效率的担忧。本项目旨在解决这些问题。", "method": "提出一个多输出集成系统，结合了BETO模型和CRF层进行序列标注，并采用了多样的训练策略和句子过滤技术来提高精度，以同时处理ToxNER（毒品实体识别）和ToxUse（毒品使用检测）两个子任务。", "result": "在ToxHabits共享任务中，该系统在触发词检测（Trigger Detection）方面取得了0.94的F1分数和0.97的精确率，在论元检测（Argument Detection）方面取得了0.91的F1分数。", "conclusion": "该多输出集成系统有效地解决了西班牙语临床文本中毒品使用信息提取的低资源和领域特定挑战，并在ToxHabits共享任务中取得了优异的性能。"}}
{"id": "2602.09486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09486", "abs": "https://arxiv.org/abs/2602.09486", "authors": ["Koduvayur Subbalakshmi", "Sabbir Hossain Ujjal", "Venkata Krishna Teja Mangichetty", "Nastaran Jamalipour Soofi"], "title": "Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement", "comment": "Preprint, 23 pages, 13 tables, 12 figures", "summary": "Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs. We further propose a self-information gated variant, CoCoA-SIG, that dynamically modulates this penalty to selectively target high-surprise, unstable generations. Extensive experiments on diverse tasks, including question-answering, summarization and code generation demonstrate that CoCoA significantly improves factual correctness across multiple model families (e.g., Llama-3, Qwen-2.5, Mistral). By leveraging model-intrinsic signals, CoCoA offers an effective and broadly applicable method for enhancing the trustworthiness of LLMs at inference time, without requiring any model retraining.", "AI": {"tldr": "本研究提出了一种名为CoCoA（Confusion and Consistency Aware）的解码算法，它通过检测LLM内部中间层的表征不稳定性来减少生成文本的幻觉，无需重新训练模型。", "motivation": "大型语言模型（LLM）容易产生流畅但事实错误的文本（幻觉），这降低了它们在下游任务中的可靠性和实用性。研究者假设，生成文本的 factuality 与其在模型内部层之间的表征稳定性相关。", "method": "提出CoCoA解码器，一种无需训练的算法，通过监测模型中间层的表征不稳定性来减轻幻觉。具体来说，提出了两个量化中间层不稳定的指标，并用它们来惩罚表现出高内部混淆的输出，从而引导模型生成更一致、事实更准确的内容。还提出了一种自信息门控变体CoCoA-SIG，动态调整惩罚以选择性地针对高惊喜度、不稳定的生成。", "result": "在问答、摘要和代码生成等多个任务上的实验表明，CoCoA显著提高了Llama-3、Qwen-2.5、Mistral等多个模型系列的 factual correctness。", "conclusion": "CoCoA利用模型内在信号，在推理时有效且广泛地提高了LLM的生成事实准确性，而无需重新训练模型，从而增强了LLM的可信度。"}}
{"id": "2602.09628", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09628", "abs": "https://arxiv.org/abs/2602.09628", "authors": ["Jie Li", "Bing Tang", "Feng Wu", "Rongyun Cao"], "title": "TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior", "comment": null, "summary": "Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.", "AI": {"tldr": "本文提出了一种名为TeleGate的统一全身遥操作框架，用于人形机器人，通过一个轻量级门控网络动态激活专业策略，并结合基于VAE的运动先验模块进行预测控制，实现了对多种动态运动的高精度实时遥操作，优于现有方法。", "motivation": "现有的人形机器人全身遥操作方法在支持多样化运动时存在性能下降问题，尤其是在处理高动态运动时。本文旨在开发一种统一的控制器，能够鲁棒地支持各种人类运动，并避免知识蒸馏带来的性能损失。", "method": "TeleGate框架通过训练一个轻量级门控网络，动态地根据本体感觉状态和参考轨迹激活特定的专业策略。此外，引入了一个基于VAE的运动先验模块，通过从历史观测中提取隐式未来运动意图，实现对需要预测的运动（如跳跃、站起）的预判控制。", "result": "在仿真和Unitree G1人形机器人上的实验表明，TeleGate在仅使用2.5小时动作捕捉数据进行训练的情况下，能够实现对跑步、跌倒恢复、跳跃等多样化动态运动的高精度实时遥操作，并在跟踪精度和成功率方面显著优于基线方法。", "conclusion": "TeleGate是一个有效的统一全身遥操作框架，通过动态激活策略和运动预测，能够克服现有方法的局限性，在各种动态运动中实现高性能的人形机器人遥操作。"}}
{"id": "2602.09411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09411", "abs": "https://arxiv.org/abs/2602.09411", "authors": ["Zhikai Li", "Jiatong Li", "Xuewen Liu", "Wangbo Zhao", "Pan Du", "Kaicheng Zhou", "Qingyi Gu", "Yang You", "Zhen Dong", "Kurt Keutzer"], "title": "K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge", "comment": "ICLR 2026. Code is available at: https://github.com/zkkli/K-Sort-Eval", "summary": "The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.", "AI": {"tldr": "本文提出了一种名为 K-Sort Eval 的 VLM 评估框架，通过后验校正和动态匹配技术，实现了比现有方法更可靠、更高效的视觉生成模型评估。", "motivation": "现有的大规模视觉生成模型评估方法（如众包 Arena 平台）成本高昂且效率低下。利用 VLM 作为评估工具是一个有前景的方向，但 VLM 的幻觉和偏见会影响评估的可靠性，且静态评估方法效率不高。", "method": "1. **数据收集**: 从 K-Sort Arena 收集包含 K 个模型输出和排名的用户投票数据。 2. **模型评估**: 新模型与现有模型进行 (K+1) 路自由比较，并由 VLM 进行排名。 3. **后验校正**: 基于 VLM 预测与人类监督的一致性，自适应地调整贝叶斯更新中的后验概率，以提高准确性。 4. **动态匹配**: 采用平衡不确定性和多样性的策略，最大化每次比较的预期收益，提高评估效率。", "result": "K-Sort Eval 能够提供与 K-Sort Arena 相当的评估结果，且通常仅需少于 90 次模型运行即可完成，证明了其效率和可靠性。", "conclusion": "K-Sort Eval 是一种可靠且高效的 VLM 评估框架，通过创新的后验校正和动态匹配方法，克服了 VLM 评估的局限性，能够有效且低成本地评估视觉生成模型。"}}
{"id": "2602.09794", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09794", "abs": "https://arxiv.org/abs/2602.09794", "authors": ["Jiaquan Zhang", "Chaoning Zhang", "Shuxu Chen", "Xudong Wang", "Zhenzhen Huang", "Pengcheng Zheng", "Shuai Yuan", "Sheng Zheng", "Qigan Sun", "Jie Zou", "Lik-Hang Lee", "Yang Yang"], "title": "GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis", "comment": "23pages", "summary": "Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.", "AI": {"tldr": "提出了一种名为 GHS-TDA 的新方法，通过构建全局假设图和使用拓扑数据分析来解决链式思考（CoT）方法中早期错误传播和缺乏结构化分析的问题，提高了 LLM 的推理准确性和鲁棒性。", "motivation": "现有链式思考（CoT）方法存在两个主要局限性：1. 早期错误容易传播和放大，难以纠正。2. 缺乏过滤冗余推理和提取关键特征的结构化分析技术，导致推理不稳定且可解释性差。", "method": "GHS-TDA 方法包括两个主要部分：1. 构建语义丰富的全局假设图，聚合、对齐和协调多个候选推理路径，提供全局纠错机制。2. 应用基于持久同调的拓扑数据分析，捕捉多尺度结构，去除冗余和不一致，提取可靠的推理骨架。", "result": "GHS-TDA 通过结合推理多样性和拓扑稳定性，实现了自适应收敛，生成高置信度且可解释的推理路径。在多个推理基准测试中，其准确性和鲁棒性均优于现有强基线方法。", "conclusion": "GHS-TDA 有效地克服了传统 CoT 方法的局限性，通过全局协调和拓扑结构分析，显著提升了大型语言模型的推理能力，并提供了更稳定和可解释的推理过程。"}}
{"id": "2602.09617", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09617", "abs": "https://arxiv.org/abs/2602.09617", "authors": ["Ruoxuan Feng", "Yuxuan Zhou", "Siyu Mei", "Dongzhan Zhou", "Pengwei Wang", "Shaowei Cui", "Bin Fang", "Guocai Yao", "Di Hu"], "title": "AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception", "comment": "Accepted by ICLR 2026", "summary": "Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.", "AI": {"tldr": "该研究提出了一个名为ToucHD的大规模分层触觉数据集，以及一个名为AnyTouch 2的通用触觉表征学习框架，旨在解决当前触觉数据集和模型在捕捉精细触觉时序动态方面的不足，以提升机器人进行接触式操作的能力。", "motivation": "现实世界中需要机器人感知触觉时序反馈、捕捉表面形变并推理物体属性和力学动力学，但现有的触觉数据集和模型在提供丰富的动态信息方面存在局限，主要关注物体层面的属性而忽略了物理交互中的精细触觉时序动态。", "method": "构建了一个名为ToucHD的大规模分层触觉数据集，包含触觉原子动作、真实世界操作以及触觉-力配对数据。在此基础上，提出了AnyTouch 2框架，一个通用的触觉表征学习框架，能够统一物体层面的理解与精细、力感知的动态感知，通过捕捉像素级和动作相关的形变，并明确建模物理力学动力学。", "result": "在涵盖静态物体属性、动态物理属性以及真实世界操作任务的基准测试中，ToucHD数据集和AnyTouch 2框架在不同传感器和任务上都表现出了一致且强大的性能，覆盖了从基础物体层面理解到力感知的灵巧操作等多个动态感知能力层级。", "conclusion": "通过构建大规模分层触觉数据集ToucHD和提出AnyTouch 2通用学习框架，研究成功地提升了机器人进行动态触觉感知的能力，能够更好地处理现实世界中复杂的接触式操作任务。"}}
{"id": "2602.09798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09798", "abs": "https://arxiv.org/abs/2602.09798", "authors": ["Matteo Cardellini", "Enrico Giunchiglia"], "title": "Symbolic Pattern Temporal Numeric Planning with Intermediate Conditions and Effects", "comment": "Under review at the Artificial Intelligence Journal", "summary": "Recently, a Symbolic Pattern Planning (SPP) approach was proposed for numeric planning where a pattern (i.e., a finite sequence of actions) suggests a causal order between actions. The pattern is then encoded in a SMT formula whose models correspond to valid plans. If the suggestion by the pattern is inaccurate and no valid plan can be found, the pattern is extended until it contains the causal order of actions in a valid plan, making the approach complete. In this paper, we extend the SPP approach to the temporal planning with Intermediate Conditions and Effects (ICEs) fragment, where $(i)$ actions are durative (and thus can overlap over time) and have conditions/effects which can be checked/applied at any time during an action's execution, and $(ii)$ one can specify plan's conditions/effects that must be checked/applied at specific times during the plan execution. Experimental results show that our SPP planner Patty $(i)$ outperforms all other planners in the literature in the majority of temporal domains without ICEs, $(ii)$ obtains comparable results with the SoTA search planner for ICS in literature domains with ICEs, and $(iii)$ outperforms the same planner in a novel domain based on a real-world application.", "AI": {"tldr": "本文将符号模式规划（SPP）方法扩展到包含中间条件和效果（ICEs）的临时规划领域，通过实验证明了其在临时规划任务上的优越性，尤其是在引入ICEs的复杂场景下。", "motivation": "现有SPP方法仅适用于数值规划，而现实世界中的规划问题通常涉及时间、动作持续性以及在动作执行过程中和计划执行特定时间点的中间条件/效果。因此，将SPP扩展到ICEs的临时规划领域具有重要意义。", "method": "将SPP方法扩展到ICEs的临时规划领域。具体来说，通过将动作表示为具有持续时间的动作，并允许在动作执行的任何时间点检查/应用条件和效果，以及在计划执行的特定时间点检查/应用计划的条件和效果。然后，将这些约束编码为SMT公式，并使用SMT求解器来查找有效的计划。当初始模式无效时，通过扩展模式来确保算法的完备性。", "result": "所提出的SPP规划器Patty在不含ICEs的临时规划领域性能优于文献中的其他规划器；在含ICEs的文献领域，与最先进的搜索规划器性能相当；在一个基于真实世界应用的新领域，其性能优于同一搜索规划器。", "conclusion": "SPP方法成功扩展到了ICEs的临时规划领域，并且在各种临时规划任务中展现出了强大的性能，尤其是在处理复杂的中间条件和效果时，证明了其在解决实际问题中的潜力。"}}
{"id": "2602.09501", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09501", "abs": "https://arxiv.org/abs/2602.09501", "authors": ["Hikaru Asano", "Tadashi Kozuno", "Kuniaki Saito", "Yukino Baba"], "title": "Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models", "comment": "15 pages, 6 figures", "summary": "Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We demonstrate that leveraging this oracle unmasking order significantly enhances final generation quality, particularly on logical reasoning benchmarks. Building on this insight, we train a supervised unmasking planner via learning-to-rank to imitate the oracle ordering from masked contexts. The resulting planner integrates into standard MDLM sampling to select where-to-unmask, improving reasoning accuracy without modifying the token prediction model.", "AI": {"tldr": "本文提出了一种名为Gt-Margin的新方法，用于优化掩码扩散语言模型（MDLM）在生成文本时选择哪些位置进行填充（where-to-unmask）的策略。通过利用真实标签的概率差作为得分，Gt-Margin能够确定一个近似最优的填充顺序，从而提高生成文本的质量，特别是在逻辑推理任务中。在此基础上，研究人员训练了一个监督式规划器来模仿这种最优顺序，实现了在不修改文本预测模型的情况下提升推理准确性。", "motivation": "现有MDLM在推理时，确定填充位置的策略（where-to-unmask）依赖于启发式方法或成本高昂的强化学习。这促使研究者寻找一种更有效、更易于训练的策略来优化填充顺序，从而提升生成质量，尤其是在对顺序敏感的逻辑推理任务中。", "method": "1. 提出Gt-Margin：一种基于真实标签概率差的位置分数，用于衡量填充位置的“容易度”。2. 证明Gt-Margin能够产生一个近似最优的填充顺序（oracle unmasking order），优先填充更易于确定的位置。3. 训练一个监督式无掩码规划器（supervised unmasking planner），通过学习排序（learning-to-rank）模仿Gt-Margin的排序策略。4. 将该规划器集成到标准的MDLM采样过程中，以指导where-to-unmask的选择，从而提高生成质量。", "result": "利用Gt-Margin确定的填充顺序显著提高了最终生成文本的质量，尤其是在逻辑推理基准测试上。训练的监督式规划器在集成到MDLM后，能够在不改变文本预测模型（what-to-unmask）的情况下，提升推理的准确性。", "conclusion": "通过引入基于真实标签概率差的Gt-Margin，研究者提出了一种有效的策略来优化MDLM的填充位置选择，实现了在不修改核心预测模型的前提下，显著提升了模型在逻辑推理等任务上的生成性能。"}}
{"id": "2602.09657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09657", "abs": "https://arxiv.org/abs/2602.09657", "authors": ["Xiaolou Sun", "Wufei Si", "Wenhui Ni", "Yuntian Li", "Dongming Wu", "Fei Xie", "Runwei Guan", "He-Yang Xu", "Henghui Ding", "Yuan Wu", "Yutao Yue", "Yongming Huang", "Hui Xiong"], "title": "AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild", "comment": "Acceped by ICLR 2026", "summary": "Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.", "AI": {"tldr": "本文提出了AutoFly，一个端到端的视觉-语言-动作（VLA）模型，用于自主无人机导航。该模型使用伪深度编码器提取深度感知特征，并通过两阶段训练策略来整合视觉、深度和语言信息与动作策略。此外，研究人员还构建了一个新的数据集，以模拟真实的自主导航场景，强调连续的避障和自主规划。", "motivation": "现有的大多数视觉语言导航（VLN）研究，特别是针对无人机的，依赖于详细的预设指令，这与现实世界中在未知环境中进行的探索不符。现实世界的导航通常只能提供粗粒度的位置或方向指引，需要无人机进行自主的连续规划和避障。", "method": "提出AutoFly模型，一个端到端的VLA模型。该模型包含一个伪深度编码器，用于从RGB图像中提取深度感知特征，以增强空间推理能力。采用渐进式两阶段训练策略，有效地将视觉、深度和语言表征与动作策略进行对齐。同时，构建了一个新的自主导航数据集，侧重于收集包含连续避障、自主规划和识别流程的轨迹，并整合了真实的无人机数据。", "result": "AutoFly在模拟和真实环境中均取得了与最先进的VLA基线模型相比3.9%的成功率提升。实验结果表明，AutoFly在自主导航任务上表现出色。", "conclusion": "AutoFly通过引入深度感知特征和创新的训练策略，有效提升了无人机在缺乏详细指令的复杂环境中的自主导航能力。新数据集的构建也为推动更真实的自主导航研究奠定了基础。"}}
{"id": "2602.09413", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09413", "abs": "https://arxiv.org/abs/2602.09413", "authors": ["Xinyu Wang", "Ke Deng", "Fei Dou", "Jinbo Bi", "Jin Lu"], "title": "LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging", "comment": "14 pages, 9 figures, 6 tables", "summary": "Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.", "AI": {"tldr": "提出了一种名为LARV的层感知自适应重缩放方法，用于提升模型合并（model merging）的性能，尤其是在处理视觉Transformer时。LARV通过为不同层分配不同的任务向量缩放比例，有效抑制浅层干扰并增强深层特征对齐，且无需额外训练或修改现有合并方法。", "motivation": "现有的模型合并方法通常统一处理所有层，但忽略了视觉Transformer中层与层之间的异质性（heterogeneity）。浅层对任务干扰敏感，而深层则包含稳定的任务特定特征。因此，需要一种能够感知层级差异并进行自适应缩放的方法来提高模型合并的效果。", "method": "LARV是一种训练无关、数据无关、可插入任何任务向量合并器的方法。它为每个任务向量在合并前分配一个每层的缩放比例。LARV使用简单的确定性调度来完成这一过程，通过计算数据无关的层代理（layer proxies）并将其转换为缩放比例。研究了多种实例化方案，包括分层（tiered）和连续（continuous）缩放。", "result": "LARV在FusionBench上的Vision Transformers上，一致地提升了所有任务向量基线（task-vector baselines）的性能，涵盖了8/14/20任务的设置。例如，Iso-C + LARV在ViT-B/32、ViT-B/16和ViT-L/14上分别达到了85.9%、89.2%和92.6%的准确率。层级分析和鲁棒性测试表明，LARV能够抑制浅层干扰并增强深层特征，使模型合并过程从统一化转变为层感知的。", "conclusion": "LARV是首个用于任务向量合并的层感知缩放方法，它能够有效缓解视觉Transformer中的层间异质性问题，显著提升模型合并的性能和鲁棒性，而无需额外的训练成本或对现有合并器的修改。"}}
{"id": "2602.09415", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.09415", "abs": "https://arxiv.org/abs/2602.09415", "authors": ["Joe-Mei Feng", "Hsin-Hsiung Kao"], "title": "Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting", "comment": null, "summary": "We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.", "AI": {"tldr": "该研究提出了一种基于算子理论的框架，用于分析具有块结构参数的非线性逆问题的稳定性和统计收敛性，并推导出了与具体重建算法无关的参数误差界限。具体地，将该框架应用于高斯喷绘渲染算子，揭示了稳定性和分辨率之间的内在权衡。", "motivation": "研究的动机在于为具有块结构参数的非线性逆问题提供一个统一的理论框架，以分析其稳定性和统计收敛性，并克服对特定重建算法的依赖性，最终能够理解和量化这类问题在现代成像和可微分渲染中的固有局限性。", "method": "研究开发了一个算子理论框架，结合了块Lipschitz几何、局部可辨识性和亚高斯噪声等假设，推导出了确定性稳定性不等式、最小二乘失配函数的全局Lipschitz界以及非渐近收敛估计。通过将该框架应用于高斯喷绘渲染算子，验证了其假设并导出了Lipschitz连续性和分辨率相关的可观测性常数。", "result": "研究得出了高概率的参数误差界限，这些界限独立于任何具体的重建算法，而是由正向算子决定的。对于高斯喷绘渲染算子，研究推导出了明确的常数，并揭示了一个根本性的稳定-分辨率权衡，表明估计误差受到图像分辨率与模型复杂性比率的固有约束。", "conclusion": "该分析表征了针对现代成像和可微分渲染中出现的高维非线性逆问题，其算子层面的固有稳定性与分辨率之间的限制关系。研究提出的框架能够为一类广泛的逆问题提供理论上的稳定性和收敛性保证。"}}
{"id": "2602.09813", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09813", "abs": "https://arxiv.org/abs/2602.09813", "authors": ["Dexun Li", "Sidney Tio", "Pradeep Varakantham"], "title": "Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning", "comment": null, "summary": "Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.", "AI": {"tldr": "提出一种基于分层马尔可夫决策过程（HMDP）的无监督环境设计（UED）方法，通过利用学生策略表示和生成模型来提高效率，减少教师-学生互动次数，适用于资源受限场景。", "motivation": "现有的UED方法依赖于随机过程进行无限环境生成，在资源受限、教师-学生互动机会有限的情况下不切实际。", "method": "提出一个分层MDP框架，教师智能体利用学生在评估环境中学习到的策略表示来生成训练环境。引入一个生成模型来扩充教师的训练数据集，减少与学生的互动。", "result": "在多个领域实验中，该方法在需要更少教师-学生互动次数的情况下优于基线方法。", "conclusion": "该方法适用于训练机会有限的UED场景。"}}
{"id": "2602.09661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09661", "abs": "https://arxiv.org/abs/2602.09661", "authors": ["Ameer Alhashemi", "Layan Abdulhadi", "Karam Abuodeh", "Tala Baghdadi", "Suryanarayana Datla"], "title": "RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination", "comment": "7 pages, IEEEtran format", "summary": "This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.", "AI": {"tldr": "本文提出了一种名为RANT的受蚂蚁启发的机器人探索框架，用于在存在噪声和不确定性的环境中进行多机器人探索。该框架通过粒子滤波定位、基于行为的控制器和基于虚拟信息素的协调机制，有效地实现了环境覆盖和热点探测，并分析了团队规模、定位精度和协调机制的影响。", "motivation": "在具有噪声和不确定性的复杂环境中，实现高效的多机器人探索和信息收集是一个挑战。研究者希望开发一种能够克服这些障碍的框架，特别是能够精确地定位和采集热点区域。", "method": "RANT框架结合了以下技术：1. 粒子滤波定位：用于在噪声环境中估计机器人的位置。2. 基于行为的控制器：采用梯度驱动的方式来开采热点区域。3. 轻量级的无重复访问协调机制：利用虚拟信息素阻塞来避免机器人重复探索同一区域。4. 实验分析：通过改变团队规模、定位精度和协调方式来评估框架性能。", "result": "实验结果表明：1. 粒子滤波对于可靠的热点探测至关重要。2. 协调机制能够显著减少机器人之间的重叠探索。3. 增加团队规模可以提高环境覆盖率，但由于干扰效应，收益会递减。", "conclusion": "RANT框架是一种有效的受蚂蚁启发的探索策略，适用于在噪声和不确定环境中进行多机器人探索。粒子滤波、协调机制和团队规模的优化对于提高探索效率和热点探测的准确性至关重要。"}}
{"id": "2602.09514", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09514", "abs": "https://arxiv.org/abs/2602.09514", "authors": ["Xavier Hu", "Jinxiang Xia", "Shengze Xu", "Kangqi Song", "Yishuo Yuan", "Guibin Zhang", "Jincheng Ren", "Boyu Feng", "Li Lu", "Tieyong Zeng", "Jiaheng Liu", "Minghao Liu", "Yuchen Elenor Jiang", "Wei Wang", "He Zhu", "Wangchunshu Zhou"], "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies", "comment": "work in progress", "summary": "Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.", "AI": {"tldr": "本文提出了 EcoGym，一个用于评估 LLM 智能体在互动经济中进行长期规划和执行的通用基准测试框架。该框架包含三个不同环境，并基于实际业务指标进行评估，发现在现有 LLM 模型中，没有模型能在所有场景下都表现出色，且模型在策略制定或执行效率方面存在系统性不足。", "motivation": "现有针对 LLM 智能体的长期规划评估框架存在不足，如过于情景化、领域特定或与持续经济动力学脱节。研究者旨在创建一个更通用、更贴近实际经济动态的评估框架。", "method": "提出了 EcoGym，一个包含 Vending、Freelance 和 Operation 三个环境的通用基准测试框架。该框架采用统一的决策过程和标准化接口，支持近乎无限的行动执行（超过 1000 步），并基于净资产、收入和日活跃用户数等业务相关结果进行评估，以考察长期战略一致性和在部分可观察及随机性下的鲁棒性。", "result": "对十一种主流 LLM 进行的实验表明，没有单一模型能在所有三个环境中都表现最佳。研究发现，模型在制定高层策略或高效执行动作方面均存在显著的次优性。", "conclusion": "EcoGym 是一个开源且可扩展的测试平台，可用于透明地评估智能体的长期规划能力，并研究在现实经济环境中可控性与效用之间的权衡。"}}
{"id": "2602.09432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09432", "abs": "https://arxiv.org/abs/2602.09432", "authors": ["Yang Zhao", "Shizhao Sun", "Meisheng Zhang", "Yingdong Shi", "Xubo Yang", "Jiang Bian"], "title": "SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL", "comment": null, "summary": "Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.", "AI": {"tldr": "提出SceneReVis框架，通过迭代的“诊断-行动”循环，利用多模态反馈解决3D场景合成中的空间冲突。同时构建SceneChain-12k数据集，并采用两阶段训练策略，使模型成为主动的空间规划器。", "motivation": "当前单通道3D场景合成方法存在空间幻觉（如碰撞），缺乏审慎推理能力。", "method": "提出SceneReVis框架，采用“诊断-行动”迭代循环，利用多模态反馈解决空间冲突。构建SceneChain-12k数据集，包含因果构建轨迹。采用监督微调（SFT）到强化学习（RL）的两阶段训练方法。", "result": "SceneReVis在保真度生成和目标导向优化方面达到最先进水平，并展现出对长尾领域的鲁棒泛化能力。", "conclusion": "SceneReVis框架通过结合视觉推理和自我反思，能够有效地生成高保真3D场景并解决空间冲突，克服了现有方法的局限性。"}}
{"id": "2602.09714", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09714", "abs": "https://arxiv.org/abs/2602.09714", "authors": ["Alejandro Gonzalez-Garcia", "Sebastiaan Wyns", "Sonia De Santis", "Jan Swevers", "Wilm Decré"], "title": "Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments", "comment": "ICRA 2026", "summary": "We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.", "AI": {"tldr": "提出一个用于在复杂结构化环境中快速规划非完整性自主移动机器人运动的完整框架，通过确定性自由空间分解生成紧凑的矩形走廊图，并结合在线运动规划和解析规划器生成近最优的运动轨迹。", "motivation": "现有的基于网格的规划器在可扩展性上存在问题，而许多运动学上可行的规划器由于搜索空间复杂而带来巨大的计算负担。研究旨在克服这些限制，实现大规模导航的快速规划。", "method": "采用确定性的自由空间分解方法，构建一个由重叠矩形走廊组成的紧凑图。然后，通过在线运动规划在图中寻找路径，并利用解析规划器生成近乎实时最优且运动学上可行的轨迹。", "result": "该框架能够显著减小搜索空间，同时保持路径分辨率，实现高效的大规模导航。通过大量仿真和物理机器人实验进行了验证。", "conclusion": "所提出的框架为在复杂结构化环境中规划非完整性移动机器人的运动提供了一个高效且可扩展的解决方案，并且实现了近时间最优和运动学上可行的轨迹生成。"}}
{"id": "2602.09802", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09802", "abs": "https://arxiv.org/abs/2602.09802", "authors": ["Manon Reusens", "Sofie Goethals", "Toon Calders", "David Martens"], "title": "Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.", "AI": {"tldr": "本研究使用多项逻辑回归模型分析大型语言模型（LLMs）在旅行助手场景下的决策，以估算其隐含支付意愿（WTP），并与人类基准进行比较。研究发现，虽然大型LLMs可以产生有意义的WTP值，但在属性层面存在系统性偏差，且总体上高估了人类的WTP，尤其是在引入昂贵选项或以商务人士为提示时。基于用户先前偏好（如倾向于选择更便宜的选项）来调整模型，可以使其估算值更接近人类基准。", "motivation": "随着大型语言模型（LLMs）在旅行助手和购物支持等应用中得到越来越广泛的应用，它们经常需要在没有客观正确答案的情况下代表用户做出主观选择。本研究旨在评估LLMs在旅行助手场景下的决策能力，以及它们如何做出主观选择。", "method": "研究通过向LLMs展示选择困境，并使用多项逻辑回归模型分析它们的响应，以推导出隐含的支付意愿（WTP）估计值。这些WTP值随后与经济学文献中的人类基准值进行比较。除了基线设置外，研究还考察了在更现实的条件下，如提供用户过去选择的信息和基于角色的提示（persona-based prompting），模型行为如何变化。", "result": "研究结果表明，虽然可以从更大的LLMs中推导出有意义的WTP值，但它们在属性层面也表现出系统性偏差。此外，LLMs倾向于总体上高估人类的WTP，尤其是在引入昂贵选项或以商务人士为提示时。通过将模型调整为偏好便宜选项的先前偏好，可以获得更接近人类基准的估值。", "conclusion": "研究结果突显了使用LLMs进行主观决策支持的潜力和局限性。在实际部署此类系统时，仔细选择模型、设计提示以及代表用户尤为重要。"}}
{"id": "2602.09516", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09516", "abs": "https://arxiv.org/abs/2602.09516", "authors": ["Julia Maria Struß", "Sebastian Schellhammer", "Stefan Dietze", "Venktesh V", "Vinay Setty", "Tanmoy Chakraborty", "Preslav Nakov", "Avishek Anand", "Primakov Chungkham", "Salim Hafid", "Dhruv Sahnan", "Konstantin Todorov"], "title": "The CLEF-2026 CheckThat! Lab: Advancing Multilingual Fact-Checking", "comment": "misinformation, disinformation, fact-checking, claim source retrieval, generating fact-checking articles", "summary": "The CheckThat! lab aims to advance the development of innovative technologies combating disinformation and manipulation efforts in online communication across a multitude of languages and platforms. While in early editions the focus has been on core tasks of the verification pipeline (check-worthiness, evidence retrieval, and verification), in the past three editions, the lab added additional tasks linked to the verification process. In this year's edition, the verification pipeline is at the center again with the following tasks: Task 1 on source retrieval for scientific web claims (a follow-up of the 2025 edition), Task 2 on fact-checking numerical and temporal claims, which adds a reasoning component to the 2025 edition, and Task 3, which expands the verification pipeline with generation of full-fact-checking articles. These tasks represent challenging classification and retrieval problems as well as generation challenges at the document and span level, including multilingual settings.", "AI": {"tldr": "CheckThat! 实验室旨在通过多语言和跨平台的方式，开发打击在线虚假信息和操纵的新技术。今年的版本重新聚焦于验证流程的核心任务，包括科学网络声明的来源检索、包含推理成分的数值和时间声明的事实核查，以及生成完整事实核查文章，这些任务涵盖了分类、检索和生成方面的挑战。", "motivation": "研究的动机在于应对在线交流中日益严峻的虚假信息和操纵问题，并推动相关技术的进步，特别是在多语言和跨平台环境中。", "method": "通过举办 CheckThat! 实验室，设置一系列与事实核查流程相关的任务，包括来源检索、数值和时间声明的事实核查（增加了推理成分），以及事实核查文章的生成。这些任务旨在挑战多语言环境下的分类、检索和生成能力。", "result": "尽管摘要中没有直接列出实验结果，但可以推断，这些任务的设置旨在推动在科学声明来源检索、数值/时间声明事实核查和事实核查文章生成方面的技术发展，并为评估这些技术提供了一个平台。", "conclusion": "CheckThat! 实验室通过设置具有挑战性的多语言任务，推动了事实核查技术的进步，并扩展了事实核查的范围，从单一声明的验证延伸到生成完整的解释性文章。"}}
{"id": "2602.09425", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09425", "abs": "https://arxiv.org/abs/2602.09425", "authors": ["Yiqiao Li", "Bo Shang", "Jie Wei"], "title": "Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification", "comment": "12 pages, 10 figures, 4 tables", "summary": "Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a \"Semantic Anchor\" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.", "AI": {"tldr": "本研究提出了一种基于视觉语言模型（VLM）的框架，通过将稀疏的 LiDAR 点云转换为深度编码的 2D 图像，实现了无需参数微调的卡车细粒度分类，并在极少样本下（16-30个样本/类）取得了具有竞争力的准确率，同时发现了“语义锚定”效应，并展示了其作为冷启动策略的潜力。", "motivation": "现有的基于 LiDAR 的卡车细粒度分类方法依赖于监督式深度学习和大量手动标注，难以扩展。而视觉语言模型（VLM）在少样本泛化方面表现出色，但其在路侧 LiDAR 应用中存在稀疏 3D 点云与密集 2D 图像之间的模态鸿沟。", "method": "提出了一种框架，通过深度感知图像生成管道，对稀疏、有遮挡的 LiDAR 扫描进行去噪、时空配准、定向校正、形态学操作和各向异性平滑处理，将其转换为深度编码的 2D 视觉代理，从而适配现成的 VLM 进行卡车细粒度分类，且无需参数微调。", "result": "在包含 20 个车辆类别的真实世界数据集上进行验证，该方法在每个类别仅有 16-30 个样本的情况下，达到了具有竞争力的分类准确率。研究还发现，在极少样本（k < 4）情况下，基于文本的引导会产生“语义锚定”效应，提高性能；而在样本稍多的情况下，由于语义不匹配，反而会降低准确率。此外，该框架可作为冷启动策略，用于引导轻量级监督模型的训练。", "conclusion": "该框架克服了 LiDAR 数据与 VLM 之间的模态鸿沟，提供了一种可扩展的、无需大量手动标注的卡车细粒度分类方法。研究发现了“语义锚定”效应，并证明了该方法在冷启动策略上的有效性，对于智能交通系统（ITS）具有实际应用价值。"}}
{"id": "2602.09517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09517", "abs": "https://arxiv.org/abs/2602.09517", "authors": ["Sangwon Yu", "Ik-hwan Kim", "Donghun Kang", "Bongkyu Hwang", "Junhwa Choi", "Suk-hoon Jung", "Seungki Hong", "Taehee Lee", "Sungroh Yoon"], "title": "Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models", "comment": null, "summary": "Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.", "AI": {"tldr": "本文提出了一种名为SAKE（Self-Anchored Knowledge Encoding）的推理时策略，通过在推理过程的开始和结束锚定检索到的知识，以解决大型语言模型（LLMs）在检索增强推理中出现的知识整合衰减（KID）问题，从而提高模型在多跳问答和复杂推理任务上的性能。", "motivation": "现有LLMs在检索增强推理中存在一个未被充分研究的瓶颈，即知识整合衰减（KID）。模型在生成较长推理链后，越来越难以将检索到的证据整合到后续推理中，限制了性能。", "method": "提出了一种名为SAKE（Self-Anchored Knowledge Encoding）的训练无关的推理时策略。该策略通过在推理过程的开始和结束锚定检索到的知识，以防止检索到的知识被先前的上下文所覆盖，从而保持其语义完整性。", "result": "SAKE显著缓解了KID问题，并在多跳问答和复杂推理基准测试中提升了模型性能。实验证明，SAKE是一种轻量级且有效的解决方案。", "conclusion": "SAKE是一种有效的推理时策略，可以稳定LLMs对检索到的知识的利用，克服知识整合衰减，从而在需要外部知识的复杂任务中提高LLMs的性能。"}}
{"id": "2602.09937", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09937", "abs": "https://arxiv.org/abs/2602.09937", "authors": ["Taeyoon Kim", "Woohyeok Park", "Hoyeong Yun", "Kyungyong Lee"], "title": "Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?", "comment": null, "summary": "Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.", "AI": {"tldr": "本研究对基于大型语言模型（LLM）的云系统根因分析（RCA）代理进行了流程层面的故障分析，识别出12种常见的故障类型，并发现许多故障源于共享的代理架构而非模型本身。研究表明，提示工程无法完全解决主要故障，但改进代理间通信协议可有效减少相关故障。", "motivation": "现有基于LLM的云系统RCA代理在检测准确率上表现不佳，且现有的评估框架仅关注最终结果的正确性，未能揭示代理推理失败的原因，因此需要对LLM-based RCA代理的故障进行更深入的分析。", "method": "研究人员在OpenRCA基准测试中运行了五种不同的LLM模型，共计1,675次代理运行，并将观察到的故障归类为12种类型，涵盖代理内部推理、代理间通信以及代理与环境的交互。此外，还进行了受控的缓解实验，包括提示工程和改进代理间通信协议。", "result": "分析发现，即使是能力更强的模型，也普遍存在数据解释中的幻觉和探索不完整等问题。这些问题存在于所有模型中，表明故障源于共享的代理架构。受控实验表明，提示工程无法解决主要故障，但改进的代理间通信协议可以将通信相关的故障减少高达15个百分点。", "conclusion": "本研究提出的故障分类法和诊断方法为设计更可靠的云RCA自主代理奠定了基础。研究强调，LLM-based RCA代理的可靠性改进不仅在于模型本身，还在于其整体架构和通信机制。"}}
{"id": "2602.09722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09722", "abs": "https://arxiv.org/abs/2602.09722", "authors": ["Ye Wang", "Sipeng Zheng", "Hao Luo", "Wanpeng Zhang", "Haoqi Yuan", "Chaoyi Xu", "Haiweng Xu", "Yicheng Feng", "Mingyang Yu", "Zhiyu Kang", "Zongqing Lu", "Qin Jin"], "title": "Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization", "comment": null, "summary": "While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard \"scale data\" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla", "AI": {"tldr": "本研究系统性地研究了视觉-语言-动作（VLA）模型在机器人控制中的规模化问题，发现盲目混合异构机器人数据会导致负迁移，并提出了改进跨体迁移的EEF相对动作表示法。", "motivation": "标准的大规模数据训练方法在机器人领域是否有效尚不清楚，因为机器人训练数据在具身、传感器和动作空间上具有异质性。本研究旨在系统地研究VLA模型在机器人领域的规模化问题，并为训练大型VLA策略提供指导。", "method": "使用了一个结合了视觉-语言骨干和流匹配的VLA框架，并在匹配的条件下消融了关键的设计决策，随后在模拟和真实机器人实验中进行了广泛评估。引入了分组盲集（Grouped Blind Ensemble）协议来提高真实世界结果的可靠性。研究了三个维度的VLA规模化：物理对齐（EEF相对动作表示）、具身混合（数据池的构成）和训练正则化。", "result": "1. 统一的末端执行器（EEF）相对动作表示法对跨具身迁移至关重要。2. 盲目混合异构机器人数据集通常会导致负迁移，而非增益。3. 感官Dropout和多阶段微调等直观策略在规模化训练中并未持续提升性能。", "conclusion": "本研究挑战了关于机器人领域规模化的一些普遍假设，并为从异构机器人数据中训练大规模VLA策略提供了实用的指导。EEF相对动作表示法对跨具身迁移至关重要，而盲目混合数据可能适得其反。"}}
{"id": "2602.09439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09439", "abs": "https://arxiv.org/abs/2602.09439", "authors": ["Xu Ma", "Yitian Zhang", "Qihua Dong", "Yun Fu"], "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning", "comment": "Dataset: https://huggingface.co/datasets/ma-xu/fine-t2i", "summary": "High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.", "AI": {"tldr": "本文提出了Fine-T2I，一个大规模、高质量、完全开放的文本到图像（T2I）微调数据集，旨在解决现有数据集的局限性，并通过与企业级模型相媲美的性能来促进开放研究。", "motivation": "现有的公开T2I微调数据集在分辨率、文本-图像对齐和多样性方面存在不足，导致开放研究模型与企业级模型之间存在性能差距。作者希望通过构建一个高质量、大规模的数据集来弥合这一差距。", "method": "作者构建了一个包含600万文本-图像对的大规模数据集（Fine-T2I），该数据集结合了合成图像和真实图像，并经过严格的文本-图像对齐、视觉保真度和提示质量过滤。数据集涵盖10个任务组合、32个提示类别、11种视觉风格和5种提示模板。", "result": "在Fine-T2I数据集上进行微调，可以显著提高不同T2I模型的生成质量和指令遵循能力，通过人类评估、视觉比较和自动指标验证。数据集规模接近预训练数据集，但质量达到微调级别。", "conclusion": "Fine-T2I数据集的发布旨在通过提供高质量、大规模的开放数据来支持T2I微调领域的开放研究，从而缩小与企业级模型在数据方面的差距。"}}
{"id": "2602.09446", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09446", "abs": "https://arxiv.org/abs/2602.09446", "authors": ["Mohammad Masudur Rahman", "Md. Rashedur Rahman", "Ashraful Islam", "Saadia B Alam", "M Ashraful Amin"], "title": "A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index", "comment": null, "summary": "Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.", "AI": {"tldr": "本研究是一项关于深度学习在城市视觉污染（UVP）自动检测和管理应用方面的范围审查。它系统地分析了现有研究，指出了当前方法的局限性，并提出了一个整合了视觉污染指数的UVP监测框架，强调了建立统一UVP管理系统的必要性。", "motivation": "尽管城市视觉污染（UVP）是一个日益严峻的问题，但现有关于其自动检测和应用的研究分散且不完整，缺乏一个全面的管理框架。", "method": "采用PRISMA-ScR指南，系统地检索了七个学术数据库，共纳入26篇相关文献。主要分析了用于检测、分类UVP的深度学习方法（如YOLO、Faster R-CNN、EfficientDet）以及现有数据集和应用系统的特点。", "result": "大多数研究集中于特定污染物类别，并使用了YOLO、Faster R-CNN和EfficientDet的变体。现有数据集存在地域局限性和缺乏标准化分类的问题。少数研究集成了实时应用系统，但存在地理偏倚。研究提出了一个整合视觉污染指数的UVP监测框架。", "conclusion": "当前UVP研究在方法和应用上存在局限性。作者强调需要一个统一的UVP管理系统，该系统应包含污染物分类体系、跨城市基准数据集、通用的深度学习模型以及一个支持可持续城市美学和提升居民福祉的评估指数。"}}
{"id": "2602.10004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10004", "abs": "https://arxiv.org/abs/2602.10004", "authors": ["Junda Wang", "Zhichao Yang", "Dongxu Zhang", "Sanjit Singh Batra", "Robert E. Tillman"], "title": "ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference", "comment": null, "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.", "AI": {"tldr": "本文提出了一种名为ESTAR（Early-Stopping for Token-Aware Reasoning）的新方法，通过在大型推理模型（LRMs）生成冗余推理后提前停止，从而在不牺牲准确性的前提下提高效率。", "motivation": "大型推理模型（LRMs）在生成长链思考（chain-of-thought）时，常常在得出正确答案后继续进行冗余推理，浪费计算资源。研究动机是提高LRMs的计算效率。", "method": "ESTAR结合了三种技术：（1）一个基于轨迹的分类器，用于识别何时可以安全地停止推理；（2）监督微调，用于训练LRMs生成“<stop>”信号；（3）“<stop>”感知的强化学习，通过计算感知奖励在自我生成的停止点截断推理过程。", "result": "在四个推理数据集上的实验表明，ESTAR将推理长度平均减少了约3.7倍（从4,799缩短到1,290），同时准确率基本保持不变（74.9%对74.2%），并且具有良好的跨领域泛化能力。", "conclusion": "提前停止（early stopping）是一种简单而有效的机制，可以显著提高大型推理模型的推理效率，而不会降低其准确性。"}}
{"id": "2602.09538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09538", "abs": "https://arxiv.org/abs/2602.09538", "authors": ["Hongyan Xie", "Yikun Ban", "Ruiyu Fang", "Zixuan Huang", "Deqing Wang", "Jianxin Li", "Yitong Yao", "Chao Wang", "Shuangyong Song"], "title": "UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment", "comment": "Under Review", "summary": "Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \\& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.", "AI": {"tldr": "本文提出了一种名为MoSLoRA的新型自回归奖励模型（ARM）训练方法，并基于此构建了统一自回归奖励模型（UniARM）框架，用于解决多目标LLM对齐问题。MoSLoRA通过共享特征提取和偏好调制来解决特征纠缠问题，UniARM则在一个参数空间内联合建模所有偏好维度，提高了对齐的准确性和效率。", "motivation": "现有针对多目标LLM对齐的自回归奖励模型（ARM）方法存在特征纠缠和对齐不准确的问题，因为它们要么独立训练ARM，忽略了偏好间的交互，要么使用独立的特征提取模块，导致特征纠缠。", "method": "提出MoSLoRA用于ARM训练，它首先使用偏好无关模块提取共享特征，然后通过条件于混合偏好向量的偏好调制模块对共享特征进行仿射变换。在此基础上，提出UniARM框架，在一个参数空间内联合建模所有偏好维度，无需为每个偏好目标独立参数。", "result": "MoSLoRA通过缓解特征纠缠，能够更精确地控制推理时的偏好权衡。UniARM能够联合建模所有偏好维度，提高了多目标测试时对齐的准确性，并展示了在更大规模LLM上的有效性。", "conclusion": "UniARM框架通过MoSLoRA有效解决了多目标LLM对齐中的挑战，能够更精确、高效地实现对齐，并具有良好的可扩展性。"}}
{"id": "2602.09765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09765", "abs": "https://arxiv.org/abs/2602.09765", "authors": ["Xijie Huang", "Weiqi Gai", "Tianyue Wu", "Congyu Wang", "Zhiyang Liu", "Xin Zhou", "Yuze Wu", "Fei Gao"], "title": "NavDreamer: Video Models as Zero-Shot 3D Navigators", "comment": "Work in the progress. 22 pages, 15 figures", "summary": "Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.", "AI": {"tldr": "NavDreamer 是一个基于视频的 3D 导航框架，利用生成视频模型作为语言指令和导航轨迹之间的接口，旨在解决现有模型数据稀疏和表示静态的问题，并通过采样优化和逆动力学模型实现零样本泛化。该研究还引入了一个包含多种导航任务的基准测试集。", "motivation": "现有的视觉-语言-动作 (VLA) 模型在导航方面存在数据稀缺、收集成本高以及静态表示无法捕捉时空动态和物理规律等局限性。研究人员希望利用视频的丰富信息和易获取性来提升导航的零样本泛化能力。", "method": "提出 NavDreamer 框架，利用生成视频模型作为语言指令到导航轨迹的接口。通过基于 VLM 的轨迹评分和选择的采样优化方法来缓解生成视频的随机性。使用逆动力学模型从生成的视频计划解码可执行航点，用于导航。引入了一个包含对象导航、精确导航、空间定位、语言控制和场景推理的综合基准。", "result": "NavDreamer 在新对象和未见过的环境中表现出强大的泛化能力。消融实验表明，导航的高层决策性质使其特别适合视频规划。", "conclusion": "基于视频的规划范式，特别是结合 NavDreamer 框架，能够显著提高 3D 导航的零样本泛化能力，并且导航任务的性质使其非常适合这种方法。"}}
{"id": "2602.09945", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09945", "abs": "https://arxiv.org/abs/2602.09945", "authors": ["Jinsong Liu", "Yuhang Jiang", "Ramayya Krishnan", "Rema Padman", "Yiye Zhang", "Jiang Bian"], "title": "Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning", "comment": null, "summary": "Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.", "AI": {"tldr": "本文提出了一种名为差分推理学习（DRL）的框架，通过学习推理差异来改进临床决策支持的准确性和推理保真度，该框架利用参考推理和模型自身推理之间的差异来生成自然语言指令，并以此增强模型在推理过程中的表现。", "motivation": "现有的临床决策支持系统不仅需要准确的答案，还需要符合临床逻辑的推理过程。然而，目前的模型在生成临床推理方面仍存在不足。", "method": "DRL框架首先将参考推理（如医生撰写的文本、临床指南或更强模型输出）和代理模型的自由形式链式思考（CoT）表示为有向无环图（DAG）。然后，利用临床加权的图编辑距离（GED）来分析推理图之间的差异。利用LLM-as-a-judge来对齐语义上等价的节点并诊断差异。这些差异诊断被转化为自然语言指令，并存储在差分推理知识库（DR-KB）中。在推理时，通过检索增强生成（RAG）检索最相关的指令，并将其添加到代理模型的提示中，以弥补逻辑上的差距。", "result": "在开放式医学问答（QA）基准和内部临床数据上的返诊入院（RVA）预测任务中，DRL相较于基线模型在最终答案准确性和推理保真度上均有所提升。消融研究证实了引入参考推理和top-k检索策略的有效性。", "conclusion": "DRL能够支持在复杂推理场景下更可靠的临床决策，并提供了一种在有限token预算下进行部署的实用机制，有效提升了临床AI的可靠性和可解释性。"}}
{"id": "2602.09552", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.09552", "abs": "https://arxiv.org/abs/2602.09552", "authors": ["Klejda Alushi", "Jan Strich", "Chris Biemann", "Martin Semmann"], "title": "Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA", "comment": "Accepted to EACL SRW 26", "summary": "Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\\footnote{\\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}", "AI": {"tldr": "该研究对用于多轮对话问答的检索增强生成（RAG）方法进行了系统的实证比较，发现更简单的方法（如重排序、混合BM25和HyDE）通常比更复杂的方法表现更好，并且不同数据集的特征和对话长度对检索效果有显著影响。", "motivation": "现有对RAG的研究大多侧重于单轮设置，并且孤立地评估RAG方法，缺乏对多轮对话问答场景下RAG方法进行系统性比较的研究。多轮对话中的对话历史、指代消解和用户意图变化增加了检索的难度。", "method": "作者进行了一项全面的实证研究，在八个多样化的多轮对话问答数据集上评估了基础RAG和高级RAG方法。他们使用统一的实验设置，通过生成器和检索器指标来评估检索质量和答案生成，并分析了跨对话轮次的性能演变。", "result": "研究发现，重排序、混合BM25和HyDE等相对简单的方法一致优于基础RAG。然而，一些更高级的技术未能带来性能提升，甚至可能导致性能低于无RAG的基线。数据集特性和对话长度对检索效果有显著影响，解释了为何没有一种RAG策略能始终占优。", "conclusion": "有效的多轮对话RAG策略的成功，与其方法的复杂性不如其与数据集结构的一致性更相关。研究结果表明，应根据数据集特点选择合适的RAG策略，而非盲目追求复杂性。"}}
{"id": "2602.09767", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09767", "abs": "https://arxiv.org/abs/2602.09767", "authors": ["Ruopeng Cui", "Yifei Bi", "Haojie Luo", "Wei Li"], "title": "Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning", "comment": null, "summary": "Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\\% expansion in state-space coverage compared to the baseline.", "AI": {"tldr": "本文提出了一种正交混合专家（OMoE）架构和多判别器框架，用于无监督技能发现，以解决现有方法在学习效率和奖励黑客问题上的局限性。在四足机器人上进行了实验，证明了该方法提高了训练效率并扩展了状态空间覆盖率。", "motivation": "现有的强化学习和模仿学习方法存在奖励塑造和数据收集的负担，而无监督技能发现有望减轻这些负担。然而，现有方法在处理多样的行为和防止奖励黑客方面存在效率低下和行为多样性不足的问题。", "method": "引入了正交混合专家（OMoE）架构，以学习通用的行为库，并设计了一个多判别器框架，其中不同的判别器在不同的观测空间上运行，以减轻奖励黑客问题。", "result": "在12自由度Unitree A1四足机器人上，该方法学习到了多种多样的运动技能。实验结果表明，与基线相比，该框架提高了训练效率，并将状态空间覆盖率提升了18.3%。", "conclusion": "所提出的OMoE架构和多判别器框架能够有效地学习多样化的运动技能，克服现有无监督技能发现方法的局限性，提高学习效率并减少奖励黑客现象。"}}
{"id": "2602.09449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09449", "abs": "https://arxiv.org/abs/2602.09449", "authors": ["Yan Luo", "Henry Huang", "Todd Y. Zhou", "Mengyu Wang"], "title": "Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing", "comment": null, "summary": "Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \\emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \\emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.", "AI": {"tldr": "本文提出两种无需训练的潜在轨迹平滑方法（Look-Ahead 和 Look-Back），通过调整潜在空间中的生成路径来改进基于 ODE 的扩散模型，并在多个数据集上取得了优于现有方法的性能。", "motivation": "现有的无需训练的流匹配方法通过调整速度场来改进图像生成，但这种调整会导致误差累积。而调整潜在轨迹信息可以被预训练的网络自然纠正，从而减少误差。因此，作者希望提出一种直接在潜在空间中调整潜在轨迹的方法。", "method": "提出两种无需训练的潜在轨迹调整方法：1. Look-Ahead：结合当前和下一步的潜在状态，并使用曲率门控权重进行平均。2. Look-Back：使用指数移动平均（EMA）和衰减因子对潜在状态进行平滑。", "result": "所提出的两种训练-free 轨迹平滑方法在 COCO17、CUB-200 和 Flickr30K 等多个数据集上，通过广泛的实验和评估指标证明，其生成性能显著优于各种最先进的模型。", "conclusion": "无需训练的潜在轨迹调整是改进基于 ODE 的扩散模型生成质量的一种有效且可行的途径，比直接修改速度场更具优势，并且本文提出的 Look-Ahead 和 Look-Back 方法能够有效地进行轨迹平滑，从而提升生成效果。"}}
{"id": "2602.09555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09555", "abs": "https://arxiv.org/abs/2602.09555", "authors": ["Yi Lu", "Deyang Kong", "Jianing Wang", "Linsen Guo", "Xue Wang", "Qi Guo", "Tao Gui", "Xuanjing Huang", "Wei Ye", "Shikun Zhang", "Wei Wang"], "title": "Advancing Block Diffusion Language Models for Test-Time Scaling", "comment": null, "summary": "Recent advances in block diffusion language models have demonstrated competitive performance and strong scalability on reasoning tasks. However, existing BDLMs have limited exploration under the test-time scaling setting and face more severe decoding challenges in long Chain-of-Thought reasoning, particularly in balancing the decoding speed and effectiveness. In this work, we propose a unified framework for test-time scaling in BDLMs that introduces adaptivity in both decoding and block-wise generation. At the decoding level, we propose Bounded Adaptive Confidence Decoding (BACD), a difficulty-aware sampling strategy that dynamically adjusts denoising based on model confidence, accelerating inference while controlling error accumulation. Beyond step-wise adaptivity, we introduce Think Coarse, Critic Fine (TCCF), a test-time scaling paradigm that allocates large block sizes to exploratory reasoning and smaller block sizes to refinement, achieving an effective efficiency-effectiveness balance. To enable efficient and effective decoding with a large block size, we adopt Progressive Block Size Extension, which mitigates performance degradation when scaling block sizes. Extensive experiments show that applying BACD and TCCF to TDAR-8B yields significant improvements over strong baselines such as TraDo-8B (2.26x speedup, +11.2 points on AIME24). These results mark an important step toward unlocking the potential of BDLMs for test-time scaling in complex reasoning tasks.", "AI": {"tldr": "本文提出了一种用于块扩散语言模型（BDLM）的测试时自适应框架，通过自适应解码（BACD）和块生成（TCCF）策略，在提高长链推理效率和效果之间取得了更好的平衡，并在数学推理任务上取得了显著提升。", "motivation": "现有BDLM在测试时扩展和长链推理方面存在解码速度和效果难以平衡的问题。", "method": "提出了一种统一的测试时扩展框架，包括：1. Bounded Adaptive Confidence Decoding (BACD)：一种难度感知的采样策略，动态调整去噪过程以平衡速度和误差累积。2. Think Coarse, Critic Fine (TCCF)：一种测试时扩展范式，根据推理阶段分配不同大小的块。3. Progressive Block Size Extension：一种用于支持大块大小高效解码的策略。", "result": "将BACD和TCCF应用于TDAR-8B模型，在AIME24数据集上实现了2.26倍的速度提升和11.2个百分点的准确率提高，显著优于TraDo-8B等基线模型。", "conclusion": "所提出的BACD和TCCF框架能够有效提升BDLM在复杂推理任务中的测试时扩展能力，实现速度和效果的良好平衡，为BDLM的实际应用打开了新的可能性。"}}
{"id": "2602.10009", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.10009", "abs": "https://arxiv.org/abs/2602.10009", "authors": ["Sean Memery", "Kartic Subr"], "title": "Discovering High Level Patterns from Simulation Traces", "comment": null, "summary": "Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.", "AI": {"tldr": "本研究提出了一种自然语言指导方法，用于从详细的物理仿真日志中发现粗粒度模式，并将这些模式映射到程序，从而增强语言模型在物理推理任务中的能力，特别是用于从自然语言目标生成奖励函数。", "motivation": "现有的AI智能体在涉及物理交互的环境中面临推理、规划和问题回答的挑战，而人类用户希望用自然语言引导或与智能体交互时，这些挑战更加严峻。语言模型（LMs）虽然常用，但在物理推理任务上表现不佳，因为它们的物理推理能力是从观察数据中学到的，而非基于仿真。直接将仿真轨迹作为上下文则面临可扩展性问题。", "method": "提出了一种自然语言指导的方法，通过合成操作仿真日志的程序，将其映射到一系列高层激活模式，以发现粗粒度模式（如'刚体碰撞'、'稳定支撑'等）。", "result": "在两个物理基准测试中，证明了这种对仿真日志进行注释的表示方式，比直接使用仿真轨迹更能支持自然语言对物理系统的推理。该方法能够使LMs根据自然语言指定的目标生成有效的奖励程序，可用于规划或监督学习。", "conclusion": "所提出的方法通过从详细仿真日志中提取粗粒度模式，为语言模型提供了更易于理解和推理的物理系统表示，从而提高了它们在涉及物理的自然语言交互任务中的表现，尤其是在奖励函数生成方面。"}}
{"id": "2602.09772", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09772", "abs": "https://arxiv.org/abs/2602.09772", "authors": ["Jonathan Styrud", "Matteo Iovino", "Rebecca Stower", "Mart Kartašev", "Mikael Norrlöf", "Mårten Björkman", "Christian Smith"], "title": "Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics", "comment": null, "summary": "The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.", "AI": {"tldr": "该研究提出了一种名为 BETR-GUI 的图形化用户界面工具，结合了大型语言模型、规划、遗传编程和贝叶斯优化等 AI 技术，以辅助用户创建机器人行为树（BT）。用户研究表明，该工具能显著提升用户在机器人编程任务上的表现，并且在结合了多种辅助方法的完整版本下，用户表现优于单独的 AI 助手。", "motivation": "提高创建反应式机器人程序的效率，使非专业程序员也能快速开发，并且探索将自动化 BT 生成技术与图形化用户界面结合，以实现人类用户对 AI 生成的 BT 进行验证和编辑。", "method": "开发 BETR-GUI，一个结合了大型语言模型、规划、遗传编程和贝叶斯优化等 AI 方法的拖放式图形用户界面。通过用户研究来评估 BETR-GUI 在机器人编程任务上的有效性。", "result": "用户研究（60 名参与者）表明，通过结合不同的辅助方法，BETR-GUI 能够提高用户在机器人编程任务上的表现。此外，使用完整版 BETR-GUI 的人类用户表现优于单独运行的 AI 助手。", "conclusion": "结合多种 AI 辅助方法和图形化用户界面的 BETR-GUI，能够有效地赋能用户（包括非专家）更快、更好地创建机器人行为树程序，并且在某些情况下，人类用户的表现甚至可以超越纯粹的 AI 辅助。"}}
{"id": "2602.09849", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09849", "abs": "https://arxiv.org/abs/2602.09849", "authors": ["Yucheng Hu", "Jianke Zhang", "Yuanfei Luo", "Yanjiang Guo", "Xiaoyu Chen", "Xinshu Sun", "Kun Feng", "Qingzhou Lu", "Sheng Chen", "Yangang Zhang", "Wei Li", "Jianyu Chen"], "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation", "comment": null, "summary": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.", "AI": {"tldr": "BagelVLA是一个统一的框架，集成了语言规划、视觉预测和动作生成，解决了现有VLA模型在复杂长期操作任务中孤立处理这些能力的问题。通过引入残差流引导（RFG），BagelVLA能够高效地耦合模态并实时生成动作，在多个基准测试中取得了显著的性能提升。", "motivation": "现有VLA模型通常将语言规划或视觉预测分开处理，缺乏将两者融合以指导复杂、长周期的操作任务生成动作的能力，导致性能不佳。", "method": "提出BagelVLA，一个统一的模型，将语言规划、视觉预测和动作生成整合在一个框架内。初始化于预训练的统一理解和生成模型，通过交织文本推理和视觉预测到动作执行循环中进行训练。引入残差流引导（RFG），通过单步去噪提取预测性视觉特征，以最小的延迟指导动作生成。", "result": "BagelVLA在多个模拟和现实世界的基准测试中，特别是在需要多阶段推理的任务上，显著优于现有方法。", "conclusion": "BagelVLA通过集成语言规划和视觉预测，并引入高效的RFG机制，能够有效地解决复杂长期操作任务中的挑战，并在实际应用中展现出优越的性能。"}}
{"id": "2602.09475", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09475", "abs": "https://arxiv.org/abs/2602.09475", "authors": ["James Burgess", "Rameen Abdal", "Dan Stoddart", "Sergey Tulyakov", "Serena Yeung-Levy", "Kuan-Chieh Jackson Wang"], "title": "ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs", "comment": "https://jmhb0.github.io/ArtifactLens/", "summary": "Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.", "AI": {"tldr": "本研究提出了一种名为ArtifactLens的系统，能够以极少量的标注数据（几百个样本）有效地检测现代图像生成器产生的伪影，同时达到了最先进的性能，并且通用性强，可用于检测AIGC。", "motivation": "现有的图像伪影检测方法需要大量的标注数据进行微调，成本高昂且难以适应生成器和伪影类型的发展。研究者希望找到一种更高效、更具成本效益的检测方法。", "method": "ArtifactLens利用预训练的视觉语言模型（VLMs），通过多组件架构、上下文学习（in-context learning）和文本指令优化（text instruction optimization）来解锁其检测伪影的能力。该方法在每个伪影类别上仅使用几百个标注样本。", "result": "ArtifactLens在五个不同的人类伪影基准测试中取得了最先进的性能。与现有方法相比，ArtifactLens所需的标注数据量减少了几个数量级。该方法还成功泛化到其他伪影类型（如物体形态、动物解剖、实体交互）和AIGC检测任务。", "conclusion": "预训练的VLMs已经具备检测图像伪影的能力，通过ArtifactLens提供的“脚手架”结构，可以以极少的标注数据高效地激活和利用这种能力，实现了伪影检测的突破性进展，并且具有良好的泛化性。"}}
{"id": "2602.09476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09476", "abs": "https://arxiv.org/abs/2602.09476", "authors": ["Chuanhai Zang", "Jiabao Hu", "XW Song"], "title": "FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation", "comment": "26 pages, 13 figures, 2 tables. Code available at https://github.com/tryzang/FD-DB", "summary": "Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.", "AI": {"tldr": "本文提出了一种名为FD-DB的频率解耦双分支模型，用于解决合成图像到真实图像的域迁移问题，该模型通过分离低频外观编辑和高频残差补偿，在提高图像真实感的同时保持结构稳定性，并显著提升了下游任务（如语义分割）的性能。", "motivation": "合成数据在几何敏感的视觉任务中具有优势，但合成域和真实域之间的外观和成像差异会导致严重的域偏移，从而降低下游任务的性能。现有的无配对合成到真实域迁移方法常在照片真实感和结构稳定性之间面临权衡。", "method": "提出FD-DB模型，一个频率解耦的双分支模型。它将外观迁移分为低频可解释编辑（预测白平衡、曝光、对比度、饱和度、模糊和颗粒感等物理参数）和高频残差补偿。通过门控融合机制在显式频率约束下结合两个分支，并采用两阶段训练策略以提高优化稳定性。", "result": "在YCB-V数据集上的实验表明，FD-DB提高了真实域的外观一致性，并显著提升了下游语义分割的性能，同时保持了几何和语义结构的完整性。", "conclusion": "FD-DB通过频率解耦和双分支设计，有效地解决了合成到真实域迁移中的照片真实感与结构稳定性之间的权衡问题，为下游视觉任务提供了更优的合成数据，并带来了显著的性能提升。"}}
{"id": "2602.10063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10063", "abs": "https://arxiv.org/abs/2602.10063", "authors": ["Tianyi Jiang", "Arctanx An", "Hengyi Feng", "Naixin Zhai", "Haodong Li", "Xiaomin Yu", "Jiahui Liu", "Hanwen Du", "Shuo Zhang", "Zhi Yang", "Jie Huang", "Yuhua Li", "Yongxin Ni", "Huacan Wang", "Ronghao Chen"], "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "comment": null, "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.", "AI": {"tldr": "提出了一种名为 Chain of Mindset (CoM) 的新框架，通过在推理的不同阶段动态切换和协调四种不同的认知模式（空间、收敛、发散、算法），来解决大型语言模型在解决复杂问题时思维僵化的限制，并在多个基准测试中取得了最先进的性能。", "motivation": "现有的大型语言模型（LLM）在解决问题时倾向于使用单一的、固定的思维模式，这限制了它们达到更高水平的智能。研究者希望打破这种“单一直觉”的限制，使模型能够像人类一样在解决问题的过程中灵活运用多种思维模式。", "method": "提出 Chain of Mindset (CoM) 框架，该框架无需训练即可实现推理过程中不同阶段的适应性思维模式协调。CoM 将推理分解为四种不同的思维模式：空间 (Spatial)、收敛 (Convergent)、发散 (Divergent) 和算法 (Algorithmic)。一个 Meta-Agent 会根据推理状态动态选择最合适的思维模式，同时一个双向的 Context Gate 会过滤信息流，以保持效率和效果。", "result": "在数学、代码生成、科学问答和空间推理等六个具有挑战性的基准测试中，CoM 取得了最先进的性能。与最强的基线模型相比，在 Qwen3-VL-32B-Instruct 和 Gemini-2.0-Flash 模型上，CoM 的整体准确率分别提高了 4.96% 和 4.72%，同时保持了推理效率。", "conclusion": "Chain of Mindset (CoM) 框架能够通过在推理过程中动态地、分步地协调不同的思维模式，显著提升大型语言模型在复杂任务上的性能，并有望推动 LLM 向更高智能水平发展。"}}
{"id": "2602.09570", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.09570", "abs": "https://arxiv.org/abs/2602.09570", "authors": ["Narges Baba Ahmadi", "Jan Strich", "Martin Semmann", "Chris Biemann"], "title": "LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval", "comment": "Accepted at EACL SRW 26", "summary": "Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\\footnote{\\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\\footnote{\\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.", "AI": {"tldr": "该研究提出了 LEMUR，一个包含 25 种语言的欧盟环境立法的大规模多语言语料库，并基于此对嵌入模型进行微调，以提高多语言法律信息检索的准确性，尤其是在低资源语言上。", "motivation": "现有的大语言模型在多语言法律信息检索方面存在不足，具体表现为检索不可靠以及缺乏领域适应性的开源嵌入模型。多语言法律语料库设计不适合语义检索，PDF 格式的法律文本提取存在噪声。", "method": "构建 LEMUR 多语言语料库，涵盖 25 种语言的 24,953 份 EUR-Lex PDF 文件。量化 PDF 文本提取的准确性（LCS）。在 LEMUR 语料库上，对三种先进的多语言嵌入模型进行对比学习微调，包括单语和双语设置。", "result": "在低资源和高资源语言的实验中，法律领域微调一致性地提高了 Top-k 检索准确性，在低资源语言上效果尤为显著。跨语言评估表明，改进效果能迁移到未见过的语言。", "conclusion": "通过在法律领域进行微调，嵌入模型能够提升多语言法律信息检索的准确性，特别是在低资源语言上，且这种提升主要体现在增强语言无关的内容表示上，而非特定语言线索。"}}
{"id": "2602.09574", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09574", "abs": "https://arxiv.org/abs/2602.09574", "authors": ["Sora Miyamoto", "Daisuke Oba", "Naoaki Okazaki"], "title": "Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs", "comment": null, "summary": "Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.", "AI": {"tldr": "提出了一种名为 BG-MCTS 的新颖树搜索解码算法，该算法能根据剩余的 token 预算动态调整搜索策略，从而在 token 预算受限的情况下提高 LLM 的性能。", "motivation": "现有的树搜索解码方法在实际应用中，面临固定的查询 token 预算限制，但这些方法通常预算无关，容易导致在预算耗尽前过度分支或过早终止，影响解码效果。", "method": "提出 BG-MCTS（Budget-Guided MCTS）算法，该算法根据剩余的 token 预算来调整搜索策略。它开始时进行广泛探索，随着预算减少，优先进行细化和答案补全，同时减少浅层节点的后期分支。", "result": "在 MATH500 和 AIME24/25 数据集上，使用开放权重 LLM 进行测试，BG-MCTS 算法在不同预算下均显著优于预算无关的基线方法。", "conclusion": "BG-MCTS 算法能够有效地将搜索策略与 token 预算对齐，解决了现有树搜索解码方法在实际应用中因预算限制而产生的不足，并在多个数据集上取得了性能提升。"}}
{"id": "2602.10085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10085", "abs": "https://arxiv.org/abs/2602.10085", "authors": ["Richard Bornemann", "Pierluigi Vito Amadori", "Antoine Cully"], "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs", "comment": "Preprint", "summary": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{https://sites.google.com/view/code-sharp/homepage}{here}$.", "AI": {"tldr": "CODE-SHARP 是一个新框架，它利用基础模型（FM）来开放式地扩展和改进一个层次化的技能库，该库被组织成一个由可执行代码奖励函数组成的有向图。在 Craftax 环境中，仅通过发现的 SHARP 技能生成的奖励进行训练的、以目标为条件的智能体，学会了解决越来越长视距的目标。当由基于 FM 的高层规划器组合时，发现的技能使单个智能体能够解决复杂、长视距的任务，平均性能优于预训练智能体和特定任务的专家策略 134%。", "motivation": "当前的强化学习方法在设计奖励函数方面存在局限性，无法实现开放式技能发现，因为有意义的技能集是未知的。现有的自动化奖励函数设计方法仅限于改进预定义任务的奖励。", "method": "引入 CODE-SHARP 框架，利用基础模型（FM）来开放式地扩展和改进一个层次化的技能库，该库以有向图的形式表示可执行的代码奖励函数。", "result": "在 Craftax 环境中，仅使用 CODE-SHARP 发现的技能进行训练的智能体，能够解决更长视距的目标。当与 FM 规划器结合使用时，该智能体能够解决复杂的长视距任务，平均性能比现有方法高出 134%。", "conclusion": "CODE-SHARP 框架通过利用基础模型实现了开放式的技能发现和改进，能够生成可组合的层次化技能，从而显著提升智能体解决复杂长视距任务的能力。"}}
{"id": "2602.10090", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10090", "abs": "https://arxiv.org/abs/2602.10090", "authors": ["Zhaoyang Wang", "Canwen Xu", "Boyi Liu", "Yite Wang", "Siwei Han", "Zhewei Yao", "Huaxiu Yao", "Yuxiong He"], "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning", "comment": "41 pages", "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.", "AI": {"tldr": "本文提出了一种名为Agent World Model (AWM)的全合成环境生成管道，用于训练多轮交互的自主代理，该管道生成了1000个覆盖日常场景的环境，具有丰富的工具集和高质量的观测，并且比LLM模拟或真实环境更可靠、更高效。", "motivation": "现有的自主代理训练受限于多样性和可靠性不足的环境，难以扩展。需要一种能够生成大量、可靠且高效的训练环境的解决方案。", "method": "提出Agent World Model (AWM)管道，该管道利用代码和数据库生成合成环境。这些环境具有丰富的工具集（平均35个工具）和高质量的观测，支持多轮交互。代理可以在这些完全可执行的环境中进行大规模强化学习，并设计可靠的奖励函数。", "result": "在合成环境中训练的代理表现出强大的分布外泛化能力，优于在特定基准环境训练的代理。实验在三个基准上进行了验证。", "conclusion": "AWM管道能够生成大规模、可靠且高效的合成环境，为训练和评估多轮交互的自主代理提供了有效的资源，并且能够提升代理的泛化能力。"}}
{"id": "2602.09477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09477", "abs": "https://arxiv.org/abs/2602.09477", "authors": ["Bodong Zhang", "Xiwen Li", "Hamid Manoochehri", "Xiaoya Tang", "Deepika Sirohi", "Beatrice S. Knudsen", "Tolga Tasdizen"], "title": "Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings", "comment": null, "summary": "Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.\n  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL", "AI": {"tldr": "提出了一种名为WeakSupCon的弱监督对比学习框架，用于提高数字病理全切片图像（WSIs）的弱监督多示例学习（MIL）性能，通过在训练中纳入袋级别（幻灯片级别）的标签信息，无需实例级别伪标签即可在特征空间中有效分离不同标签的图像块。", "motivation": "当前数字病理图像分析面临训练标签有限的挑战，人工标注耗时费力。弱监督多示例学习（MIL）虽然只需袋级别标签，但现有方法多侧重于特征聚合，忽略了MIL场景下编码器预训练的特征表示学习。", "method": "提出了一种名为WeakSupCon的弱监督对比学习框架，该框架在训练过程中整合了袋级别标签信息，通过对比学习有效分离不同标签的图像块，且不依赖实例级别伪标签。", "result": "与自监督对比学习方法相比，WeakSupCon生成的图像特征在三个数据集上显著提升了下游MIL任务的性能。", "conclusion": "WeakSupCon框架能够有效学习用于MIL任务的图像特征表示，并在数字病理图像分析中取得了优于现有方法的性能。"}}
{"id": "2602.09893", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09893", "abs": "https://arxiv.org/abs/2602.09893", "authors": ["Zhengxue Cheng", "Yan Zhao", "Keyu Wang", "Hengdi Zhang", "Li Song"], "title": "TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data", "comment": "27 pages", "summary": "Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.", "AI": {"tldr": "本文提出了TaCo，一个用于评估触觉数据压缩算法的基准测试。研究人员还开发了专门针对触觉数据训练的神经编解码器TaCo-LL（无损）和TaCo-L（有损），并在各种任务中证明了其优越性。", "motivation": "在机器人等实时应用中，受带宽限制，触觉数据的有效压缩是一个未被充分研究但至关重要的问题。触觉数据的异质性和时空复杂性使得这一挑战更加严峻。", "method": "研究人员构建了TaCo基准测试，评估了30种不同的压缩方法（包括传统算法和神经编解码器），涵盖了五种不同类型传感器的数据集。他们还开发了两种新的数据驱动编解码器：TaCo-LL（无损）和TaCo-L（有损），并在无损存储、人类可视化、材料/物体分类和灵巧抓取等任务上进行了评估。", "result": "TaCo-LL和TaCo-L在所评估的任务中表现出了优越的性能。基准测试揭示了压缩效率和任务性能之间的关键权衡。", "conclusion": "TaCo为触觉数据压缩提供了一个全面的评估框架，并引入了性能优越的TaCo-LL和TaCo-L编解码器，为未来触觉感知领域的发展奠定了基础。"}}
{"id": "2602.09590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09590", "abs": "https://arxiv.org/abs/2602.09590", "authors": ["Shweta Parihar", "Liu Guangliang", "Natalie Parde", "Lu Cheng"], "title": "Context-Aware Counterfactual Data Augmentation for Gender Bias Mitigation in Language Models", "comment": null, "summary": "A challenge in mitigating social bias in fine-tuned language models (LMs) is the potential reduction in language modeling capability, which can harm downstream performance. Counterfactual data augmentation (CDA), a widely used method for fine-tuning, highlights this issue by generating synthetic data that may align poorly with real-world distributions or creating overly simplistic counterfactuals that ignore the social context of altered sensitive attributes (e.g., gender) in the pretraining corpus. To address these limitations, we propose a simple yet effective context-augmented CDA method, Context-CDA, which uses large LMs to enhance the diversity and contextual relevance of the debiasing corpus. By minimizing discrepancies between the debiasing corpus and pretraining data through augmented context, this approach ensures better alignment, enhancing language modeling capability. We then employ uncertainty-based filtering to exclude generated counterfactuals considered low-quality by the target smaller LMs (i.e., LMs to be debiased), further improving the fine-tuning corpus quality. Experimental results on gender bias benchmarks demonstrate that Context-CDA effectively mitigates bias without sacrificing language modeling performance while offering insights into social biases by analyzing distribution shifts in next-token generation probabilities.", "AI": {"tldr": "本文提出了一种名为Context-CDA的上下文增强反事实数据增强方法，用于在减少语言模型社会偏见的同时，保持其语言建模能力。该方法通过生成更具多样性和上下文相关性的反事实数据，并使用基于不确定性的过滤来提高数据质量，实验证明其在消除性别偏见方面有效，且不损害模型性能。", "motivation": "现有的反事实数据增强（CDA）方法在缓解语言模型（LMs）的社会偏见时，可能导致语言建模能力下降，从而影响下游任务性能。这是因为CDA生成的合成数据可能与真实数据分布不符，或者生成的反事实数据过于简单，忽略了预训练语料库中改变的敏感属性（如性别）的社会背景。", "method": "本文提出Context-CDA方法，该方法利用大型语言模型增强反事实数据的多样性和上下文相关性，使其与预训练数据分布更一致。通过最小化反事实数据集与预训练数据之间的差异来提高语言建模能力。随后，采用基于不确定性的过滤技术，排除由目标小型语言模型（即待去偏的语言模型）认为是低质量的反事实数据，进一步提升微调语料库的质量。", "result": "在性别偏见基准测试上的实验结果表明，Context-CDA能够有效地减轻偏见，同时不牺牲语言建模性能。此外，通过分析下一个词生成概率的分布变化，该方法还能提供对社会偏见的洞察。", "conclusion": "Context-CDA是一种有效的方法，可以在缓解语言模型社会偏见的同时，保持其语言建模能力。它通过增强反事实数据的上下文相关性和质量，解决了现有CDA方法的局限性，并为理解和解决语言模型中的社会偏见提供了新的途径。"}}
{"id": "2602.09888", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09888", "abs": "https://arxiv.org/abs/2602.09888", "authors": ["Zihao Li", "Yanan Zhou", "Ranpeng Qiu", "Hangyu Wu", "Guoqiang Ren", "Weiming Zhi"], "title": "TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback", "comment": null, "summary": "Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.", "AI": {"tldr": "本文提出了一种名为TriPilot-FF的开放源码全身遥操作系统，用于双臂移动机械臂。该系统通过激光雷达驱动的触觉踏板提供连续的底座控制，并结合了双臂的领导者-跟随者遥操作。它能够通过阻力反馈引导操作员避免碰撞，并提供力反馈以增强接触感知和操作性，从而提高操作效率。", "motivation": "现有的移动机械臂全身遥操作接口主要以手部为中心，忽略了脚部控制的潜力，使得操作员难以同时协调移动底座和双臂，并处理障碍物和接触。本研究旨在探索一种更直观、更高效的全身遥操作方法，特别关注利用脚部输入进行底座控制，并集成触觉和力反馈来提升操作体验。", "method": "TriPilot-FF系统包含以下核心组件：1. 激光雷达驱动的触觉踏板：利用激光雷达数据生成接近障碍物的阻力反馈，引导操作员的行为。2. 双臂领导者-跟随者遥操作：实现上身操作的直观控制。3. 力反馈：提供接触感知和操作性引导。4. 行动分块Transformer (ACT) 策略：将遥操作反馈信号整合到策略中以提升性能。系统是开源的，并提供了踏板设备设计和完整的软件栈。", "result": "TriPilot-FF能够有效地“共同驾驶”人类操作员，在长时间和需要精确移动底座协调的任务中表现出色。通过激光雷达驱动的触觉反馈，系统能够引导操作员规避碰撞，而无需显式的碰撞避免控制器。力反馈增强了操作员的接触感知和对移动底座重新定位的需求，从而改善了机械臂的可达性。将遥操作反馈整合到ACT策略后，性能得到提升。", "conclusion": "TriPilot-FF是一个创新的全身遥操作系统，通过集成触觉踏板控制和双臂遥操作，显著提高了移动机械臂操作的效率和直观性。该系统通过引导式避障和增强的反馈机制，成功地辅助了人类操作员，并在实际应用中得到了验证。开源的发布进一步促进了该领域的研究和发展。"}}
{"id": "2510.04772", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04772", "abs": "https://arxiv.org/abs/2510.04772", "authors": ["Max Kirchner", "Hanna Hoffmann", "Alexander C. Jenke", "Oliver L. Saldanha", "Kevin Pfeiffer", "Weam Kanjo", "Julia Alekseenko", "Claas de Boer", "Santhi Raj Kolamuri", "Lorenzo Mazza", "Nicolas Padoy", "Sophia Bano", "Annika Reinke", "Lena Maier-Hein", "Danail Stoyanov", "Jakob N. Kather", "Fiona R. Kolbinger", "Sebastian Bodenstedt", "Stefanie Speidel"], "title": "Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge", "comment": "A challenge report pre-print (31 pages), including 7 tables and 8 figures", "summary": "Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.", "AI": {"tldr": "FedSurg挑战赛评估了在手术视频分类中使用联邦学习（FL）技术的最新进展，重点关注其在跨中心泛化和局部适应能力，并提出了ViViT模型作为领先方案，同时指出了FL在临床应用中的挑战。", "motivation": "评估当前联邦学习技术在手术视频分类任务上的表现，特别是在未见过的临床中心下的泛化能力和局部微调适应能力，以促进协作模型开发而不泄露患者数据。", "method": "参赛者使用Appendix300数据集训练模型，评估模型在未见中心下的泛化能力和经过局部微调后的适应能力。参赛方法包括使用基础模型进行线性探测、三元组损失的度量学习以及FedAvg、FedMedian、FedSAM等FL聚合方案。使用F1分数和预期成本进行性能评估，并通过bootstrap和统计检验评估排名鲁棒性。", "result": "在泛化任务中，跨中心性能有限；在适应任务中，所有团队在微调后均有所提升，但排名稳定性较低。基于ViViT的模型提交获得了最强的整体性能。挑战暴露了泛化能力、对类别不平衡的敏感性以及去中心化训练中超参数调整的困难，而时空建模和上下文感知预处理显示出潜力。", "conclusion": "FedSurg挑战赛为评估手术视频分类中的FL策略建立了首个基准。研究结果揭示了局部个性化与全局鲁棒性之间的权衡，并强调了架构选择、预处理和损失函数设计的重要性，为未来开发不平衡感知、自适应和鲁棒的FL方法提供了参考。"}}
{"id": "2602.09591", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09591", "abs": "https://arxiv.org/abs/2602.09591", "authors": ["Daisuke Nohara", "Taishi Nakamura", "Rio Yokota"], "title": "On the Optimal Reasoning Length for RL-Trained Language Models", "comment": "15 pages, 10 figures. Submitted to the Workshop on Scaling Post-training for LLMs (SPOT) at ICLR 2026", "summary": "Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.", "AI": {"tldr": "本研究探讨了强化学习（RL）在大型语言模型（LLM）中引入的推理能力提升与计算成本增加之间的权衡，并研究了长度控制方法的效果，发现适度控制长度可以提高效率，但存在“输出过长导致分散”和“输出过短导致思考不足”两种失败模式。", "motivation": "强化学习虽然能提升LLM的推理能力，但会增加输出长度和计算成本。现有长度控制方法的效果尚不明确，需要找到平衡效率和性能的最佳输出长度。", "method": "在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上，比较了几种长度控制方法的效果，并分析了RL训练策略下的两种长度控制失效模式。", "result": "长度惩罚可能阻碍推理能力的获取。经过适当调整的长度控制方法可以提高具有强大先验推理能力的模型的效率。发现了两种失效模式：1) 过长的输出会增加结果的分散性；2) 过短的输出会导致思考不足。", "conclusion": "适度长度控制对于RL训练的大型语言模型在效率和性能之间取得平衡至关重要，但需要注意避免输出过长或过短带来的负面影响。"}}
{"id": "2602.09483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09483", "abs": "https://arxiv.org/abs/2602.09483", "authors": ["Lin Chen", "Xiaoke Zhao", "Kun Ding", "Weiwei Feng", "Changtao Miao", "Zili Wang", "Wenxuan Guo", "Ying Wang", "Kaiyuan Zheng", "Bo Zhang", "Zhe Li", "Shiming Xiang"], "title": "Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.", "AI": {"tldr": "本文提出了一种名为Align-TI的新型知识蒸馏框架，通过模仿教师模型中的动态token交互来压缩多模态大语言模型（MLLMs），实现了显著的模型压缩和性能提升，甚至优于更大规模的模型。", "motivation": "现有的知识蒸馏方法在压缩大型MLLMs时，主要关注静态的下一个token对齐，忽略了动态token交互在多模态理解和生成中的重要作用。因此，研究者希望开发一种新的知识蒸馏方法，能够更好地捕捉和传递这些动态交互信息。", "method": "Align-TI框架包含两个核心组件：1. IVA（Instruction-guided Vision Alignment）模仿教师模型从指令相关的视觉区域提取信息的能力。2. TPA（Token-to-token Probability Alignment）模仿教师模型在生成过程中序列token间的动态转换逻辑。通过对这两个方面的对齐，实现知识的有效迁移。", "result": "实验结果表明，Align-TI在模型压缩方面表现出色。相比于Vanilla KD，Align-TI带来了2.6%的相对提升。更重要的是，经过Align-TI蒸馏的2B参数模型甚至在性能上超越了7B参数的LLaVA-1.5，证明了该框架在训练参数高效的MLLMs方面的优越性，并达到了新的SOTA水平。", "conclusion": "Align-TI是一种有效的知识蒸馏框架，通过模拟MLLMs中的视觉-指令token交互和内部响应token交互，可以成功地压缩大型MLLMs，同时保持甚至提升模型性能，为训练参数高效的多模态模型提供了一个新的state-of-the-art解决方案。"}}
{"id": "2602.09494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09494", "abs": "https://arxiv.org/abs/2602.09494", "authors": ["Yuwei Chen", "Zhenliang He", "Jia Tang", "Meina Kan", "Shiguang Shan"], "title": "OSI: One-step Inversion Excels in Extracting Diffusion Watermarks", "comment": null, "summary": "Watermarking is an important mechanism for provenance and copyright protection of diffusion-generated images. Training-free methods, exemplified by Gaussian Shading, embed watermarks into the initial noise of diffusion models with negligible impact on the quality of generated images. However, extracting this type of watermark typically requires multi-step diffusion inversion to obtain precise initial noise, which is computationally expensive and time-consuming. To address this issue, we propose One-step Inversion (OSI), a significantly faster and more accurate method for extracting Gaussian Shading style watermarks. OSI reformulates watermark extraction as a learnable sign classification problem, which eliminates the need for precise regression of the initial noise. Then, we initialize the OSI model from the diffusion backbone and finetune it on synthesized noise-image pairs with a sign classification objective. In this manner, the OSI model is able to accomplish the watermark extraction efficiently in only one step. Our OSI substantially outperforms the multi-step diffusion inversion method: it is 20x faster, achieves higher extraction accuracy, and doubles the watermark payload capacity. Extensive experiments across diverse schedulers, diffusion backbones, and cryptographic schemes consistently show improvements, demonstrating the generality of our OSI framework.", "AI": {"tldr": "提出了一种名为 OSI（One-step Inversion）的新方法，用于从扩散模型生成的水印图像中提取高斯着色风格的水印。OSI 将水印提取视为一个可学习的符号分类问题，通过一步完成，比现有的多步反演方法快 20 倍，准确率更高，载荷容量翻倍。", "motivation": "现有的高斯着色风格水印提取方法需要进行多步扩散反演来获得精确的初始噪声，这计算成本高且耗时。研究旨在解决这一效率问题。", "method": "将水印提取重新表述为一个可学习的符号分类问题，消除了对初始噪声进行精确回归的需要。通过在扩散模型骨干网络上进行初始化，并在合成的噪声-图像对上进行微调，使用符号分类目标。从而使 OSI 模型能够一步完成水印提取。", "result": "OSI 比多步扩散反演方法快 20 倍，提取准确率更高，水印载荷容量翻倍。在不同的调度器、扩散骨干网络和加密方案上进行了广泛的实验，结果一致表明了 OSI 框架的通用性。", "conclusion": "OSI 是一种高效、准确且通用的水印提取框架，显著优于现有的多步反演方法，为扩散生成图像的水印保护提供了新的解决方案。"}}
{"id": "2602.09940", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09940", "abs": "https://arxiv.org/abs/2602.09940", "authors": ["Archit Sharma", "Dharmendra Sharma", "John Rebeiro", "Peeyush Thakur", "Narendra Dhar", "Laxmidhar Behera"], "title": "Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation", "comment": null, "summary": "Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.", "AI": {"tldr": "本文提出了一种轻量级的、完全在设备上运行的系统，名为 Instruct2Act 和 RAN，用于将自然语言指令转换为机器人操作。该系统通过 BiLSTM 和自编码器解析指令为原子动作，并利用 DATRN 和 YOLOv8 生成精确的轨迹，在资源受限的单摄像头环境下实现了实时、确定性的机器人操作。", "motivation": "现实世界中的机器人难以处理自由形式的人类指令，这主要是由于计算和传感能力的限制。研究人员希望弥合这一差距，实现更可靠的机器人操作。", "method": "该方法分为两个阶段：1) Instruct2Act 模块：使用紧凑型 BiLSTM 和多头注意力自编码器，将自然语言指令解析为一系列原子动作。2) Robot Action Network (RAN) 模块：结合动态自适应轨迹径向网络 (DATRN) 和基于 YOLOv8 的视觉环境分析器，为每个原子动作生成精确的控制轨迹。整个系统在没有云服务的情况下，运行在配置适中的设备上。", "result": "在自定义数据集上，Instruct2Act 达到了 91.5% 的子动作预测准确率，且模型占用空间小。在实际机器人评估中，跨越了拾放、拾倒、擦拭和拾递四种任务，整体成功率达到 90%。子动作推理完成时间 < 3.8 秒，端到端执行时间为 30-60 秒，具体取决于任务复杂度。", "conclusion": "精细化的指令到动作解析，结合基于 DATRN 的轨迹生成和视觉引导的感知，为在资源受限、单摄像头设置下实现确定性、实时的机器人操作提供了一条实用的途径。"}}
{"id": "2602.09621", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09621", "abs": "https://arxiv.org/abs/2602.09621", "authors": ["R E Zera Marveen Lyngkhoi", "Chirag Chawla", "Pratinav Seth", "Utsav Avaiya", "Soham Bhattacharjee", "Mykola Khandoga", "Rui Yuan", "Vinay Kumar Sankarapu"], "title": "AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models", "comment": "https://github.com/Lexsi-Labs/aligntune", "summary": "Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.", "AI": {"tldr": "AlignTune是一个模块化工具包，旨在解决大型语言模型（LLMs）训练后对齐研究中的后端依赖、奖励碎片化和不可复现的管道问题，提供统一接口以支持SFT和RLHF，并可轻松切换TRL和Unsloth后端。", "motivation": "当前LLM训练后对齐的实践工作流分散在特定后端的工具和临时的粘合代码中，导致实验难以复现，阻碍了对齐研究的进展。", "method": "AlignTune通过提供一个统一的接口，支持监督微调（SFT）和RLHF风格的优化，并允许用户在TRL和Unsloth之间轻松切换后端。它标准化了配置，引入了一个可扩展的奖励层（支持基于规则和学习的奖励），并集成了标准和自定义任务的评估。", "result": "AlignTune通过将后端特定逻辑封装在工厂边界后面，实现了可控的比较和可复现的对齐实验。这有助于解决后端干扰、奖励碎片化和不可复现管道的问题。", "conclusion": "AlignTune通过其模块化设计和统一接口，显著提高了LLM训练后对齐实验的可复现性和效率，为研究人员提供了一个更易于管理和比较不同对齐策略的平台。"}}
{"id": "2602.09598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09598", "abs": "https://arxiv.org/abs/2602.09598", "authors": ["Qiao Liang", "Yuke Zhu", "Chao Ge", "Lei Yang", "Ying Shen", "Bo Zheng", "Sheng Guo"], "title": "Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning", "comment": "20 pages, 11 figures", "summary": "Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.", "AI": {"tldr": "提出了一种名为ELPO（Error-Localized Policy Optimization）的方法，用于解决长时序工具集成推理（TIR）中奖励稀疏、信用分配困难的问题，通过定位并利用第一个不可挽回的错误步骤来改进模型训练，并在多个TIR基准测试中取得了优于现有方法的性能。", "motivation": "传统的基于结果的强化学习在长时序TIR任务中存在奖励稀疏、延迟高和信用分配困难的问题。早期的一个错误可能导致整个任务失败，因此需要一种方法来精细化信用分配，特别是定位并利用首次出现的不可挽回错误。", "method": "ELPO方法包括三个主要步骤：1. 通过二分搜索回滚树在固定回滚预算下定位首次出现的不可挽回错误步骤。2. 将错误定位结果转化为稳定的学习信号，通过分层优势归因（hierarchical advantage attribution）实现。3. 应用错误局部化自适应裁剪（error-localized adaptive clipping）来加强对关键错误步骤及其后续步骤的纠正性更新。", "result": "ELPO在数学、科学QA和代码执行等TIR基准测试中，在相似采样预算下，持续优于强大的Agentic RL基线方法。此外，ELPO在Pass@K和Major@K指标、回滚排名质量以及工具调用效率方面也表现出额外的提升。", "conclusion": "ELPO能够有效地解决长时序TIR任务中的信用分配挑战，通过精确定位和利用首次出现的错误步骤，显著提升了TIR代理的学习效率和任务成功率，并在多个下游任务中展现出优越的性能。"}}
{"id": "2602.09506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09506", "abs": "https://arxiv.org/abs/2602.09506", "authors": ["Sumin Roh", "Harim Kim", "Ho Yun Lee", "Il Yong Chun"], "title": "Equilibrium contrastive learning for imbalanced image classification", "comment": "18 pages, 8 figures", "summary": "Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.", "AI": {"tldr": "提出了一种名为均衡对比学习（ECL）的监督对比学习框架，通过促进表示几何均衡和分类器-类别中心几何均衡来解决现有方法在处理类别不平衡数据集时的局限性，并在多项长尾数据集的实验中取得了优于现有方法的性能。", "motivation": "现有监督对比学习方法在处理类别不平衡数据集时表现不佳，主要存在两方面问题：1. 未考虑类别均值/原型与分类器之间的对齐，影响泛化能力；2. 原型被视为单一样本，其影响随批次内类别实例数量变化，导致各类别贡献不均。", "method": "提出均衡对比学习（ECL）框架，包含两个主要组件：1. 促进表示几何均衡，通过平衡类别平均特征和类别原型的贡献，来实现类别样本的紧凑化和类别均值的均匀分布；2. 建立分类器-类别中心几何均衡，通过对齐分类器权重和类别原型。", "result": "在CIFAR-10(0)-LT、ImageNet-LT、ISIC 2019以及自建的LCCT四个长尾数据集上的实验结果表明，ECL的性能优于现有针对不平衡分类的SOTA监督对比学习方法。", "conclusion": "ECL是一种有效的监督对比学习框架，能够通过建立表示几何均衡和分类器-类别中心几何均衡，有效地缓解类别不平衡问题，提升模型在不平衡数据集上的泛化能力。"}}
{"id": "2602.09972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09972", "abs": "https://arxiv.org/abs/2602.09972", "authors": ["Zixuan Wang", "Huang Fang", "Shaoan Wang", "Yuanfei Luo", "Heng Dong", "Wei Li", "Yiming Gan"], "title": "Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning", "comment": null, "summary": "While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.", "AI": {"tldr": "本文提出了一种名为Hydra-Nav的统一视觉语言模型（VLM）架构，通过自适应地在缓慢的深思熟虑系统和快速的反应系统之间切换，解决了现有VLM在目标导航中成功率低和定位效率不高的问题，并引入了新的效率评估指标SOT。", "motivation": "现有的大型视觉语言模型（VLMs）在物体目标导航方面仍存在成功率低和对未见物体定位效率不高的问题，这主要归因于其薄弱的时空推理能力。尽管一些注入推理能力的尝试提高了成功率，但计算开销巨大。因此，研究动机是为了同时解决现有方法的无效性和低效率问题。", "method": "本文提出了一种名为Hydra-Nav的统一VLM架构，该架构包含一个用于分析探索历史和制定高层计划的“深思熟虑的慢速系统”，以及一个用于高效执行的“反应式快速系统”，并能在两者之间自适应切换。该模型通过一个三阶段的课程进行训练：(i) 空间-动作对齐以加强轨迹规划；(ii) 记忆-推理整合以增强长距离探索中的时空推理；(iii) 迭代拒绝微调以在关键决策点实现选择性推理。此外，还引入了一个新的评估指标SOT（Success weighted by Operation Time）来衡量搜索效率。", "result": "Hydra-Nav在HM3D、MP3D和OVON基准测试中取得了最先进的性能，分别比第二名的方法提高了11.1%、17.4%和21.2%。实验还表明，与固定的推理频率基线相比，自适应推理显著提高了搜索效率。", "conclusion": "Hydra-Nav通过结合深思熟虑和反应式系统，并采用有效的课程学习策略，能够有效地解决目标导航中的时空推理问题，显著提高了导航的成功率和效率。新的SOT指标能够更全面地评估VLM在导航任务中的性能。"}}
{"id": "2602.09973", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09973", "abs": "https://arxiv.org/abs/2602.09973", "authors": ["Hao Li", "Ziqin Wang", "Zi-han Ding", "Shuai Yang", "Yilun Chen", "Yang Tian", "Xiaolin Hu", "Tai Wang", "Dahua Lin", "Feng Zhao", "Si Liu", "Jiangmiao Pang"], "title": "RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation", "comment": "Published to ICLR 2026, 69 pages, 40 figures", "summary": "Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.", "AI": {"tldr": "本文提出了 RoboInter Manipulation Suite，一个包含数据、基准和模型的大规模资源库，用于填补现有机器人操作数据集中表示不足和多样性不足的空白，并通过中间表示来改进 VLA 模型。", "motivation": "现有机器人操作数据集成本高、特定于载体且覆盖范围和多样性不足，阻碍了 VLA 模型在机器人操作领域的泛化能力。现有的 plan-then-execute 方法需要额外的中间监督，而这在现有数据集中是缺失的。", "method": "引入 RoboInter Manipulation Suite，包括 RoboInter-Tool（一个轻量级的 GUI，支持半自动注解）和 RoboInter-Data（一个包含 230k+ 集、571 个场景的大规模数据集，提供超过 10 类中间表示的密集逐帧注解）。此外，还提出了 RoboInter-VQA（包含 9 类空间和 20 类时间体的 VQA）和 RoboInter-VLA（一个集成的 plan-then-execute 框架）。", "result": "RoboInter-Data 的规模和注解质量远超现有工作。RoboInter-VQA 提供了系统性基准测试，RoboInter-VLA 支持模块化和端到端的 VLA 变体。", "conclusion": "RoboInter Manipulation Suite 为通过细粒度和多样化的中间表示来推进健壮和可泛化的机器人学习奠定了实用的基础。"}}
{"id": "2602.09624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09624", "abs": "https://arxiv.org/abs/2602.09624", "authors": ["Nalin Srun", "Parisa Rastin", "Guénaël Cabanes", "Lydia Boudjeloud Assala"], "title": "MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation", "comment": null, "summary": "We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.", "AI": {"tldr": "MILE-RefHumEval是一个无需真实标签或评估员协调的、参考自由的大型语言模型（LLMs）评估框架，通过独立提示的评估员组成的集成来工作，支持离散和连续评分，并已被证明能与人类判断高度一致，性能优于现有方法，且计算成本更低。", "motivation": "现有的大型语言模型评估方法依赖于真实标签或评估员的协调，这在成本、效率和可扩展性方面存在局限性。研究者希望开发一种更灵活、可解释且可扩展的LLM评估框架。", "method": "该研究提出了一种名为MILE-RefHumEval的参考自由评估框架。它使用一个由独立提示的评估员组成的集成，这些评估员遵循一个与人类对齐的模式。该框架支持离散和连续的评分判断，并针对多种任务（如候选选择、摘要、图像字幕和对话）设计了特定的提示。", "result": "实验结果表明，MILE-RefHumEval与人类判断高度一致，性能优于先前的方法，并且计算开销更低。它能够提供灵活、可解释和可扩展的评估。", "conclusion": "MILE-RefHumEval提供了一种高效、鲁棒且与人类对齐的LLM评估解决方案，无需依赖真实标签或评估员协调，适用于实际应用场景。"}}
{"id": "2602.09510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09510", "abs": "https://arxiv.org/abs/2602.09510", "authors": ["Kun Wang", "Yun Zhu", "Pan Zhou", "Na Zhao"], "title": "Robust Depth Super-Resolution via Adaptive Diffusion Sampling", "comment": null, "summary": "We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.", "AI": {"tldr": "提出了一种名为AdaDS的通用深度超分辨率框架，该框架能够从任意退化的低分辨率输入中鲁棒地恢复高分辨率深度图，其核心在于利用高斯平滑的收缩特性，并通过自适应选择反向扩散轨迹的起始时间步长来增强对不同退化模式的鲁棒性。", "motivation": "现有深度超分辨率方法在面对严重或未知退化时容易产生伪影，作者希望开发一种能够鲁棒处理各种退化情况的通用框架。", "method": "AdaDS框架利用高斯平滑的收缩特性，使得退化输入与高质量输入的分布差异随噪声累积而减小，最终趋向于各向同性高斯先验。该方法根据估计的精炼不确定性自适应地选择反向扩散轨迹的起始时间步长，并注入定制噪声，将中间样本置于目标后验分布的高概率区域。", "result": "在真实世界和合成基准数据集上的大量实验表明，AdaDS 在零样本泛化能力和对不同退化模式的抵抗力方面优于最先进的方法。", "conclusion": "AdaDS 框架通过自适应地利用扩散模型的生成先验，即使在上游估计不完美的情况下，也能实现对深度超分辨率任务的鲁棒恢复，并展现出优异的泛化能力。"}}
{"id": "2602.09587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09587", "abs": "https://arxiv.org/abs/2602.09587", "authors": ["Yongfan Lai", "Wen Qian", "Bo Liu", "Hongyan Li", "Hao Luo", "Fan Wang", "Bohan Zhuang", "Shenda Hong"], "title": "MieDB-100k: A Comprehensive Dataset for Medical Image Editing", "comment": null, "summary": "The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.", "AI": {"tldr": "本文提出了 MieDB-100k，一个大规模、高质量、多样化的医学影像编辑数据集，旨在解决现有数据集在多样性、医学理解和质量可扩展性方面的不足。该数据集通过结合专家模型和规则合成方法构建，并经过人工审核以确保临床准确性。使用 MieDB-100k 训练的模型在医学影像编辑任务上表现优于现有模型，并具有良好的泛化能力。", "motivation": "现有医学影像编辑数据集在多样性、医学理解能力和质量可扩展性方面存在不足，这阻碍了多模态生成模型在医学影像编辑领域的应用。研究者希望创建一个更全面、高质量的数据集来推动该领域的发展。", "method": "作者提出了 MieDB-100k 数据集，该数据集通过数据策管流程构建，该流程结合了模态特定的专家模型和基于规则的数据合成方法，并辅以严格的人工检查以确保临床保真度。数据集将编辑任务分为感知、修改和转换三个类别，以同时考虑理解和生成能力。", "result": "在 MieDB-100k 上训练的模型在医学影像编辑任务上持续优于现有的开源和专有模型，并且表现出强大的泛化能力。", "conclusion": "MieDB-100k 是一个用于文本引导的医学影像编辑的大规模、高质量、多样化数据集，它通过一种创新的数据创建流程构建，并且在实验中证明了其有效性。研究者期望该数据集能成为未来医学影像编辑领域进步的基石。"}}
{"id": "2602.09991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.09991", "abs": "https://arxiv.org/abs/2602.09991", "authors": ["François Marcoux", "François Grondin"], "title": "Acoustic Drone Package Delivery Detection", "comment": null, "summary": "In recent years, the illicit use of unmanned aerial vehicles (UAVs) for deliveries in restricted area such as prisons became a significant security challenge. While numerous studies have focused on UAV detection or localization, little attention has been given to delivery events identification. This study presents the first acoustic package delivery detection algorithm using a ground-based microphone array. The proposed method estimates both the drone's propeller speed and the delivery event using solely acoustic features. A deep neural network detects the presence of a drone and estimates the propeller's rotation speed or blade passing frequency (BPF) from a mel spectrogram. The algorithm analyzes the BPFs to identify probable delivery moments based on sudden changes before and after a specific time. Results demonstrate a mean absolute error of the blade passing frequency estimator of 16 Hz when the drone is less than 150 meters away from the microphone array. The drone presence detection estimator has a accuracy of 97%. The delivery detection algorithm correctly identifies 96% of events with a false positive rate of 8%. This study shows that deliveries can be identified using acoustic signals up to a range of 100 meters.", "AI": {"tldr": "本研究提出了一种基于声学信号的无人机包裹递送事件检测算法，利用麦克风阵列估计螺旋桨转速和分析声学特征以识别递送时刻，并在一定距离内取得了高精度。", "motivation": "近期无人机在监狱等限制区域的非法递送构成了严峻的安全挑战，现有研究多集中于无人机检测和定位，而对递送事件的识别关注较少。", "method": "该算法利用地面麦克风阵列，通过深度神经网络从梅尔频谱图中提取声学特征，估计无人机的螺旋桨转速（或叶片通过频率 BPF）。随后，通过分析 BPF 在特定时间点前后的突变来识别递送事件。", "result": "在无人机距离麦克风阵列 150 米以内时，叶片通过频率估计的平均绝对误差为 16 Hz；无人机存在检测的准确率为 97%；递送检测算法能够以 8% 的误报率正确识别 96% 的事件。研究表明，该算法在 100 米范围内可以识别递送事件。", "conclusion": "声学信号可以有效地用于识别无人机包裹递送事件，特别是在近距离范围内，为解决无人机非法递送的安全问题提供了一种新的解决方案。"}}
{"id": "2602.09515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09515", "abs": "https://arxiv.org/abs/2602.09515", "authors": ["Mas Nurul Achmadiah", "Afaroj Ahamad", "Chi-Chia Sun", "Wen-Kai Kuo"], "title": "Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems", "comment": "14 pages, 12 figures", "summary": "This paper presents an Internet of Things (IoT) application that utilizes an AI classifier for fast-object detection using the frame difference method. This method, with its shorter duration, is the most efficient and suitable for fast-object detection in IoT systems, which require energy-efficient applications compared to end-to-end methods. We have implemented this technique on three edge devices: AMD AlveoT M U50, Jetson Orin Nano, and Hailo-8T M AI Accelerator, and four models with artificial neural networks and transformer models. We examined various classes, including birds, cars, trains, and airplanes. Using the frame difference method, the MobileNet model consistently has high accuracy, low latency, and is highly energy-efficient. YOLOX consistently shows the lowest accuracy, lowest latency, and lowest efficiency. The experimental results show that the proposed algorithm has improved the average accuracy gain by 28.314%, the average efficiency gain by 3.6 times, and the average latency reduction by 39.305% compared to the end-to-end method. Of all these classes, the faster objects are trains and airplanes. Experiments show that the accuracy percentage for trains and airplanes is lower than other categories. So, in tasks that require fast detection and accurate results, end-to-end methods can be a disaster because they cannot handle fast object detection. To improve computational efficiency, we designed our proposed method as a lightweight detection algorithm. It is well suited for applications in IoT systems, especially those that require fast-moving object detection and higher accuracy.", "AI": {"tldr": "本文提出了一种基于帧差法的物联网（IoT）快速目标检测应用，并与端到端方法进行了比较，证明了其在计算效率、能耗和延迟方面的优势，特别是在检测火车和飞机等快速移动目标时。", "motivation": "物联网系统需要能效高且延迟低的应用程序，端到端的目标检测方法在处理快速移动目标时效率低下，因此需要一种更优化的方法。", "method": "利用帧差法结合AI分类器进行快速目标检测，并在AMD AlveoT M U50、Jetson Orin Nano和Hailo-8T M AI Accelerator三个边缘设备上实现了四种神经网络和Transformer模型，并对鸟类、汽车、火车和飞机等类别进行了评估。", "result": "帧差法在MobileNet模型上表现出高精度、低延迟和高能效。YOLOX模型精度、延迟和效率均最低。与端到端方法相比，所提出的算法平均提高了28.314%的准确率，3.6倍的效率，并降低了39.305%的延迟。火车和飞机等快速移动目标的检测准确率较低。", "conclusion": "所提出的轻量级帧差法目标检测算法非常适合物联网系统，尤其是在需要快速检测和高精度的场景下，优于端到端方法，尽管在检测非常快速的目标时准确率仍有待提高。"}}
{"id": "2602.09518", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09518", "abs": "https://arxiv.org/abs/2602.09518", "authors": ["Hung-Shuo Chang", "Yue-Cheng Yang", "Yu-Hsi Chen", "Wei-Hsin Chen", "Chien-Yao Wang", "James C. Liao", "Chien-Chang Chen", "Hen-Hsen Huang", "Hong-Yuan Mark Liao"], "title": "A Universal Action Space for General Behavior Analysis", "comment": null, "summary": "Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.", "AI": {"tldr": "本文利用深度学习和大型数据集构建了一个通用的动作空间（UAS），用于分析和分类哺乳动物和黑猩猩的行为，克服了传统方法在行为识别上的局限性。", "motivation": "传统的动物和人类行为分析方法依赖于手工设计的特征，鲁棒性和泛化能力有限。ImageNet的出现使得基于深度学习的大规模视觉识别成为可能，作者希望借鉴这一范式来改进行为分析。", "method": "作者构建了一个大规模的通用动作空间（UAS），利用现有的标注人类动作数据集进行训练。然后，将这个UAS作为基础，用于分析和分类哺乳动物和黑猩猩的行为数据集。", "result": "通过构建UAS，作者能够更有效地分析和分类非人类哺乳动物和黑猩猩的行为数据，表明了该方法在行为识别上的潜力。", "conclusion": "基于深度学习和通用动作空间（UAS）的方法，可以克服传统手工特征方法的局限性，为大规模的动物行为分析提供了一个更强大和通用的框架。"}}
{"id": "2602.09642", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09642", "abs": "https://arxiv.org/abs/2602.09642", "authors": ["Sieun Hyeon", "Jusang Oh", "Sunghwan Steve Cho", "Jaeyoung Do"], "title": "MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.", "AI": {"tldr": "本文提出了一种名为MATA的多智能体TableQA框架，该框架利用多种推理路径和小语言模型工具来提高表格问答的可靠性、可扩展性和效率，并在现有基准上实现了最先进的性能。", "motivation": "现有的大型语言模型（LLMs）在表格理解任务（如TableQA）方面虽然取得了显著进展，但在资源受限或隐私敏感的环境中，可靠性、可扩展性和效率方面仍面临挑战。", "method": "MATA是一个多智能体TableQA框架，它利用多个互补的推理路径和一组由小型语言模型构建的工具。MATA通过不同的推理风格为给定的表格和问题生成候选答案，然后借助这些工具进行优化或选择最佳答案。此外，它还包含一个旨在最小化昂贵LLM调用以提高效率的算法。", "result": "MATA使用小型开源模型保持了强大的性能，并能轻松适应不同类型的LLMs。在两个不同难度的基准和十种不同的LLMs上的大量实验表明，MATA在避免过度LLM推理的同时，实现了最先进的准确性和高效推理。", "conclusion": "仔细协调多个推理路径可以实现可扩展且可靠的TableQA。MATA框架在提高TableQA的效率和可靠性方面展示了有效性，即使在使用小型模型的情况下也能取得优异的性能。"}}
{"id": "2602.10013", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10013", "abs": "https://arxiv.org/abs/2602.10013", "authors": ["Xuhui Kang", "Tongxuan Tian", "Sung-Wook Lee", "Binghao Huang", "Yunzhu Li", "Yen-Ling Kuo"], "title": "Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper", "comment": "10 pages, 11 figures", "summary": "Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .", "AI": {"tldr": "本文介绍了一种低成本的力控夹爪TF-Gripper，以及一个名为RETAF的框架，该框架利用触觉反馈和高频控制来提升机器人精确调节抓取力的能力，并在五个实际任务中验证了其有效性。", "motivation": "为了让机器人能够像人类一样精确地控制抓取力，特别是处理易损物品，需要一种低成本且具有触觉反馈的力控夹爪。", "method": "开发了低成本力控夹爪TF-Gripper，并设计了配套的遥操作设备记录人类抓取数据。提出了RETAF框架，将力控与末端执行器姿态预测解耦，利用手腕图像和触觉反馈实现高频力调节。", "result": "TF-Gripper和RETAF在五个需要精确力调节的真实任务中表现出色。与位置控制相比，直接力控制显著提高了抓取稳定性和任务成功率。触觉反馈对力调节至关重要，RETAF的性能优于基线方法，并可与不同基础策略集成。", "conclusion": "TF-Gripper和RETAF框架为机器人力控策略的学习和应用提供了一条可行路径，尤其是在需要精确力调节的操纵任务中。"}}
{"id": "2602.09701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09701", "abs": "https://arxiv.org/abs/2602.09701", "authors": ["Sandesh Hegde", "Jaison Saji Chacko", "Debarshi Banerjee", "Uma Mahesh"], "title": "GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation", "comment": null, "summary": "We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.\n  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.\n  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.", "AI": {"tldr": "该研究提出了一种名为GenSeg-R1的解耦的“推理-然后-分割”框架，用于细粒度的指代图像分割。该框架利用视觉语言模型（VLM）生成空间提示（边界框和关键点），然后使用SAM 2生成高质量分割掩码，并且通过GRPO进行微调，无需监督的推理链注释。GenSeg-R1-G变体在GRefCOCO上进行了训练，并引入了SAM 2 in-the-loop奖励，直接优化掩码质量。", "motivation": "为了实现更准确和细粒度的指代图像分割，研究人员希望开发一种能够理解自然语言查询并生成精确空间提示（如边界框和关键点）的系统，然后利用这些提示来生成高质量的分割掩码。现有方法在处理复杂场景和需要精确推理时存在不足。", "method": "该研究提出了GenSeg-R1框架，该框架包含两个主要部分：1. 一个视觉语言模型（VLM），如Qwen3-VL，用于接收图像和自然语言查询，进行场景推理，并生成结构化的空间提示（边界框和两个内部关键点）。2. 一个预训练的、可提示的分割模型（SAM 2），用于将VLM生成的提示转换为高质量的分割掩码。GenSeg-R1模型使用Group Relative Policy Optimization（GRPO）进行微调，无需监督的推理链标注。GenSeg-R1-G变体在GRefCOCO数据集上进行训练，并引入了SAM 2 in-the-loop奖励，以直接优化掩码质量。", "result": "在RefCOCOg验证集上，GenSeg-R1-8B模型取得了0.7127 cIoU和0.7382 mIoU的成绩，显著优于基线模型。在GRefCOCO验证集上，GenSeg-R1-G模型达到了76.69%的目标mIoU和82.40%的阴性提示（无目标）准确率，在ReasonSeg测试集上，GenSeg-R1-4B模型达到了68.40%的mIoU，优于现有方法。", "conclusion": "GenSeg-R1框架通过解耦推理和分割过程，有效提升了细粒度指代图像分割的性能。通过使用VLM生成结构化空间提示，并结合SAM 2生成分割掩码，该方法在多个基准测试中取得了 SOTA 性能，并且在无需监督的推理链标注的情况下实现了高效训练。GenSeg-R1-G变体进一步证明了直接优化掩码质量的有效性，并解决了无目标提示的处理问题。"}}
{"id": "2602.09703", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09703", "abs": "https://arxiv.org/abs/2602.09703", "authors": ["Abdulhai Alali", "Abderrahmane Issam"], "title": "Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding", "comment": null, "summary": "Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.", "AI": {"tldr": "研究人员使用 LoRA 微调、适配器合并和 MBR 解码来改进大型语言模型在阿拉伯语方言上的生成和翻译能力，并在叙利亚、摩洛哥和沙特阿拉伯语方言上取得了良好效果。", "motivation": "现有的多语言大型语言模型（LLMs）在处理高资源语言方面表现良好，但由于数据和语言变化的限制，对语言方言的代表性不足。", "method": "研究人员采用低秩适应（LoRA）对预训练的 LLM 进行微调，使用单语和英阿方言平行数据。此外，还结合了适配器合并和方言感知 MBR 解码技术，以提高方言生成和翻译的准确性。", "result": "在叙利亚、摩洛哥和沙特阿拉伯语方言上的实验表明，适配器合并和 MBR 解码技术能够有效提高方言的准确性，同时保持语义的准确性。", "conclusion": "将 LoRA 微调、适配器合并和 MBR 解码相结合，为阿拉伯语方言的生成提供了一个紧凑且有效的框架，能够实现鲁棒的方言生成。"}}
{"id": "2602.09691", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09691", "abs": "https://arxiv.org/abs/2602.09691", "authors": ["Joseph Attieh", "Timothee Mickus", "Anne-Laure Ligozat", "Aurélie Névéol", "Jörg Tiedemann"], "title": "Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs", "comment": null, "summary": "Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.", "AI": {"tldr": "本研究评估了知识蒸馏（KD）方法在机器翻译中的性能，同时考虑了翻译质量和计算成本（以碳足迹表示）。研究发现，在小规模使用时，蒸馏过程的开销是主要的碳排放源，而在大规模使用时，推理过程的开销占主导地位。研究还指出，词级别蒸馏通常比序列级别蒸馏具有更好的性能-成本权衡。", "motivation": "现有关于知识蒸馏的研究通常只关注翻译质量，忽略了计算成本，这使得在计算资源受限的情况下难以选择合适的蒸馏方法。本研究旨在弥补这一不足，通过综合评估翻译质量和计算成本来为选择KD方法提供指导。", "method": "研究人员评估了代表性的知识蒸馏方法，同时考虑了翻译质量和计算成本。计算成本以碳足迹的形式表示，并使用机器学习生命周期评估（MLCA）工具进行量化，该工具考虑了模型生命周期（教师模型训练、蒸馏和推理）中的运行时排放和硬件生产成本。", "result": "研究结果表明：（1）在小规模部署时，蒸馏开销占总碳足迹的绝大部分；（2）在规模化使用时，推理过程的碳排放占主导地位，这表明KD方法只有在超过特定任务相关的阈值后才具有效益；（3）词级别蒸馏通常比序列级别蒸馏提供更有利的碳足迹-质量权衡。", "conclusion": "本研究提出的评估协议为在明确的质量和计算约束下选择知识蒸馏方法提供了可复现的指导。研究强调了在评估KD方法时同时考虑翻译质量和计算成本的重要性，并为在实际应用中做出明智的技术选择提供了依据。"}}
{"id": "2602.09521", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09521", "abs": "https://arxiv.org/abs/2602.09521", "authors": ["Jingyi Wang", "Fei Li", "Rujie Liu"], "title": "Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs", "comment": null, "summary": "Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.", "AI": {"tldr": "提出一种无需训练的注意力干预算法，通过计算视觉-文本交叉注意力来识别并增强与任务相关的视觉token的注意力，同时将视觉注意力值融入beam search解码，以减少大型视觉语言模型的幻觉问题。", "motivation": "现有的LVLMs在视觉注意力方面存在不足，容易产生幻觉。现有的解决方案（调整和放大视觉注意力）会增加对与任务无关的token的注意力。", "method": "提出一种训练免费的注意力干预算法。该算法提取视觉-文本交叉注意力子矩阵，构建重加权矩阵，重新分配注意力，以增强任务相关token的注意力。此外，将视觉注意力值注入beam search解码过程，以识别具有更高视觉注意力的解决方案。", "result": "该方法显著减少了主流LVLMs的幻觉，同时保持了生成内容的准确性和连贯性。", "conclusion": "通过基于视觉-文本相似性增强任务相关token的注意力，并结合改进的解码策略，可以有效缓解LVLMs的幻觉问题。"}}
{"id": "2602.10015", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10015", "abs": "https://arxiv.org/abs/2602.10015", "authors": ["Dharmendra Sharma", "Archit Sharma", "John Reberio", "Vaibhav Kesharwani", "Peeyush Thakur", "Narendra Kumar Dhar", "Laxmidhar Behera"], "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments", "comment": null, "summary": "Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.", "AI": {"tldr": "本文提出了 RoboSubtaskNet，一个用于识别和定位视频中细粒度子任务片段的框架，该框架在 RoboSubtask 数据集和 GTEA 数据集上表现优于现有方法，并成功应用于实际机器人操作。", "motivation": "在人类与机器人协作中，需要对长视频中的细粒度子任务进行时间定位和分类，以便机器人能够直接执行。现有方法在处理这类任务时存在不足。", "method": "提出 RoboSubtaskNet，一个多阶段框架。它结合了注意力增强的 I3D 特征（RGB+光流）和一个修改版的 MS-TCN（采用斐波那契膨胀策略）来捕捉短时转移（如抓取-放置）。训练使用了交叉熵和时间正则化（截断 MSE 和过渡感知项）的复合目标函数。", "result": "RoboSubtaskNet 在 GTEA 和 RoboSubtask 数据集上优于 MS-TCN 和 MS-TCN++，在 Breakfast 数据集上表现具有竞争力。在实际的 7-DoF 机器人操作中，实现了约 91.25% 的任务成功率。", "conclusion": "RoboSubtaskNet 为从视频中的子任务理解到实际机器人操作提供了一条实用的路径，证明了其在真实世界场景中的有效性。"}}
{"id": "2602.09611", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.09611", "abs": "https://arxiv.org/abs/2602.09611", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Yongyi Cui", "Gerard de Melo", "Linlin Wang"], "title": "AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models", "comment": "preprint", "summary": "Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\\% AUC) and robust attack resilience (at least 88.61\\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.", "AI": {"tldr": "提出了一种名为AGMark的新型框架，用于为大型视觉语言模型（LVLM）嵌入可检测的水印，同时保持视觉保真度。该方法通过注意力机制动态识别语义关键证据，并结合不确定性感知和证据校准来确定水印比例，从而避免无关标记并提高生成质量，尤其是在生成后期。AGMark在检测准确性和鲁棒性方面表现出色，且不牺牲推理效率。", "motivation": "现有视觉无关的水印会引入视觉无关的标记并破坏视觉基础；现有的视觉相关水印依赖静态权重估计，忽略权重分布密度，未能应对生成过程中视觉依赖性的动态变化，并可能引入低质量标记。", "method": "AGMark框架在每个解码步骤中，首先根据注意力权重动态识别语义关键证据，结合上下文感知连贯性线索，形成更具适应性和校准性的证据-权重分布。然后，通过联合考虑不确定性感知（标记熵）和证据校准（权重密度），确定语义关键标记的比例，实现自适应词汇划分，避免无关标记。", "result": "AGMark在生成质量方面优于传统方法，尤其在生成后期提高了视觉语义保真度。该框架保持了高竞争力的检测准确性（至少99.36% AUC）和鲁棒的攻击抵御能力（至少88.61% AUC），同时不牺牲推理效率。", "conclusion": "AGMark是一种创新的多模态水印框架，能够在保持视觉保真度的前提下嵌入可检测信号，有效解决了现有方法的不足，并在检测准确性、鲁棒性和生成质量方面取得了显著改进，为可靠的多模态水印设定了新标准。"}}
{"id": "2602.09712", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09712", "abs": "https://arxiv.org/abs/2602.09712", "authors": ["Yiming Shu", "Pei Liu", "Tiange Zhang", "Ruiyang Gao", "Jun Ma", "Chen Sun"], "title": "TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces", "comment": null, "summary": "Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem", "AI": {"tldr": "本文提出了一种名为TraceMem的认知启发式框架，通过处理对话痕迹来构建结构化的叙事记忆模式，以克服大型语言模型长期交互中上下文窗口有限的问题。该框架包括三个阶段：短期记忆处理、突触记忆巩固和系统记忆巩固，并采用代理搜索机制来增强推理。在LoCoMo基准上的评估显示，TraceMem在多跳和时间推理方面优于基线，实现了最先进的性能。", "motivation": "现有的大型语言模型（LLMs）在维持长期交互方面存在困难，主要原因是其有限的上下文窗口难以处理随时间推移的对话历史。现有的记忆系统通常将交互视为不连贯的片段，未能捕捉对话流的潜在叙事连贯性。", "method": "TraceMem框架通过一个三阶段的流水线来构建结构化的叙事记忆模式：1.短期记忆处理：采用演绎式主题分割方法来划分对话片段并提取语义表示。2.突触记忆巩固：将对话片段总结为情景记忆，并与语义信息一起提炼为用户特定的痕迹。3.系统记忆巩固：利用两阶段的层次聚类来组织这些痕迹，形成在统一主题下的连贯、随时间演变叙事线索，并将这些线索封装到结构化的用户记忆卡中，形成叙事记忆模式。内存利用方面，提供代理搜索机制增强推理。", "result": "在LoCoMo基准上的评估显示，TraceMem实现了最先进的性能。分析表明，通过构建连贯的叙事，TraceMem在多跳和时间推理方面优于基线方法，凸显了其在深度叙事理解中的重要作用。", "conclusion": "TraceMem框架通过模拟认知过程，成功地为大型语言模型构建了结构化的、随时间演变的叙事记忆模式，从而克服了长期对话处理的挑战，并在多跳和时间推理任务上取得了领先的性能。这为未来在LLMs中开发更强大的记忆系统提供了新的视角和方向。"}}
{"id": "2602.09523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09523", "abs": "https://arxiv.org/abs/2602.09523", "authors": ["Zhen Qiu", "Kaiwen Xiao", "Zhengwei Lu", "Xiangyu Liu", "Lei Zhao", "Hao Zhang"], "title": "Singpath-VL Technical Report", "comment": null, "summary": "We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.", "AI": {"tldr": "本文提出了Singpath-VL，一个用于宫颈细胞学的人工智能助手，通过合成百万级图像-描述数据集并在此基础上微调Qwen3-VL-4B模型，以克服现有数据不足的挑战，并在形态学感知和细胞诊断分类方面展现出优越性能。", "motivation": "尽管多模态大语言模型（MLLMs）在计算病理学领域取得了进展，但它们在细胞病理学，特别是宫颈细胞学领域的应用仍显不足，主要原因是缺乏大规模、高质量的标注数据集。", "method": "研究人员开发了一个三阶段的合成流水线，利用通用MLLMs生成初步描述，通过共识融合和专家知识注入进行精炼，从而创建了一个大规模的图像-描述数据集。随后，使用该数据集通过多阶段策略对Qwen3-VL-4B模型进行微调，生成了Singpath-VL模型。", "result": "Singpath-VL模型在细粒度形态学感知和细胞级诊断分类方面表现出优越性能。", "conclusion": "Singpath-VL模型成功弥补了宫颈细胞学AI助手的空缺，并通过合成数据集和先进的微调策略，有望推动该领域的发展。研究团队计划开源部分合成数据集和基准测试。"}}
{"id": "2602.10035", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10035", "abs": "https://arxiv.org/abs/2602.10035", "authors": ["Marc-Philip Ecker", "Christoph Fröhlich", "Johannes Huemer", "David Gruber", "Bernhard Bischof", "Tobias Glück", "Wolfgang Kemmetmüller"], "title": "A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation", "comment": "Accepted at ICRA 2026", "summary": "Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.", "AI": {"tldr": "本文提出了一种将避障和载荷摆动阻尼统一在一个模型预测控制器（MPC）框架下的方法，用于林业起重机在动态、非结构化环境中的安全操作。", "motivation": "在林业作业中，起重机需要在复杂环境中安全导航，这要求同时解决碰撞避免和载荷摆动控制的问题。现有方法通常将这两个问题分开处理，效率和安全性不高。", "method": "利用激光雷达（LiDAR）进行环境感知，通过在线欧几里得距离场（EDF）将环境信息整合到MPC控制器中。该控制器同时考虑了碰撞约束和载荷摆动阻尼，实现了动态环境下的实时适应。", "result": "该方法能够应对静态环境变化，在干扰下保持无碰撞运行，并在无法绕行时安全停车。实际林业起重机实验验证了其在摆动阻尼和避障方面的有效性。", "conclusion": "所提出的MPC控制器成功地将避障和载荷摆动阻尼统一在一个框架内，为林业起重机在动态、非结构化环境中的安全自主操作提供了一个有效的解决方案。"}}
{"id": "2602.09717", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.09717", "abs": "https://arxiv.org/abs/2602.09717", "authors": ["Radib Bin Kabir", "Tawsif Tashwar Dipto", "Mehedi Ahamed", "Sabbir Ahmed", "Md Hasanul Kabir"], "title": "From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet", "comment": null, "summary": "Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.", "AI": {"tldr": "本文首次系统性地评估了轻量级CNN到SNN的转换，并提出了一个经过剪枝的SNN-SqueezeNet-P模型，该模型在保持竞争力的准确率的同时，显著提高了能效，为边缘智能提供了低功耗的解决方案。", "motivation": "现有关于SNN的研究主要集中在大规模模型，而轻量级CNN到SNN的转换设计和评估却被忽视了，这限制了SNN在资源受限的边缘设备上的应用。", "method": "研究人员将ShuffleNet、SqueezeNet、MnasNet和MixNet等紧凑型CNN架构转换为SNN，使用Leaky-Integrate-and-Fire (LIF)神经元模型和代理梯度下降进行训练。在CIFAR-10、CIFAR-100和TinyImageNet数据集上进行了评估，并对SNN-SqueezeNet模型应用了结构化剪枝策略。", "result": "SNNs比其CNN对应模型能效高出15.7倍，同时保持了有竞争力的准确率。SNN-SqueezeNet表现最优，经过剪枝的SNN-SqueezeNet-P在CIFAR-10上准确率提高了6%，参数减少了19%，能耗降低了88.1%，且准确率仅比CNN-SqueezeNet低1%。", "conclusion": "轻量级SNN是实际可行的低功耗边缘部署替代方案，为实现高性能、低功耗的边缘智能提供了一条可行路径。"}}
{"id": "2602.09723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09723", "abs": "https://arxiv.org/abs/2602.09723", "authors": ["Christian Buck", "Levke Caesar", "Michelle Chen Huebscher", "Massimiliano Ciaramita", "Erich M. Fischer", "Zeke Hausfather", "Özge Kart Tokmak", "Reto Knutti", "Markus Leippold", "Joseph Ludescher", "Katharine J. Mach", "Sofia Palazzo Corner", "Kasra Rafiezadeh Shahi", "Johan Rockström", "Joeri Rogelj", "Boris Sakschewski"], "title": "AI-Assisted Scientific Assessment: A Case Study on Climate Change", "comment": null, "summary": "The emerging paradigm of AI co-scientists focuses on tasks characterized by repeatable verification, where agents explore search spaces in 'guess and check' loops. This paradigm does not extend to problems where repeated evaluation is impossible and ground truth is established by the consensus synthesis of theory and existing evidence. We evaluate a Gemini-based AI environment designed to support collaborative scientific assessment, integrated into a standard scientific workflow. In collaboration with a diverse group of 13 scientists working in the field of climate science, we tested the system on a complex topic: the stability of the Atlantic Meridional Overturning Circulation (AMOC). Our results show that AI can accelerate the scientific workflow. The group produced a comprehensive synthesis of 79 papers through 104 revision cycles in just over 46 person-hours. AI contribution was significant: most AI-generated content was retained in the report. AI also helped maintain logical consistency and presentation quality. However, expert additions were crucial to ensure its acceptability: less than half of the report was produced by AI. Furthermore, substantial oversight was required to expand and elevate the content to rigorous scientific standards.", "AI": {"tldr": "研究评估了一个基于 Gemini 的 AI 环境在气候科学领域支持科学评估的协作性，结果表明 AI 可以加速科学工作流程，并提高了内容的逻辑一致性和呈现质量，但仍需要专家监督来确保内容的严谨性和可接受性。", "motivation": "现有的 AI 共同科学家范式主要针对可重复验证的任务，而对于无法重复评估、需要理论和现有证据综合才能确定真相的问题，该范式并不适用。因此，需要评估 AI 在这种复杂科学问题上的协作潜力。", "method": "将一个基于 Gemini 的 AI 环境集成到标准的科学工作流程中，并与 13 名气候科学家合作，共同研究 AMOC 的稳定性问题。通过 104 次修订循环，在 46 人时内完成了一份包含 79 篇论文的综合报告，并评估了 AI 在此过程中的贡献。", "result": "AI 显著加速了科学工作流程，大多数 AI 生成的内容被保留在报告中。AI 提高了报告的逻辑一致性和呈现质量。然而，AI 生成的内容不足一半，且需要大量专家监督来提升内容至严谨的科学标准，专家意见对于报告的可接受性至关重要。", "conclusion": "AI 在加速科学工作流程、提高内容质量和一致性方面具有潜力，但对于需要综合理论和证据的复杂科学问题，AI 仍不能取代人类专家的作用，需要人类的监督和贡献来确保科学研究的严谨性和可接受性。"}}
{"id": "2602.09719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09719", "abs": "https://arxiv.org/abs/2602.09719", "authors": ["Longhuan Xu", "Cunjian Chen", "Feng Yin"], "title": "Unsupervised Layer-Wise Dynamic Test Time Adaptation for LLMs", "comment": null, "summary": "Test-time adaptation (TTA) for large language models (LLMs) updates model parameters at inference time using signals available at deployment. This paper focuses on a common yet under-explored regime: unsupervised, sample-specific TTA, where the model adapts independently for each prompt using only the prompt itself, without gold answers or external supervision. Although appealing, naive unsupervised TTA with a fixed, handcrafted learning rate can be unstable: updates may overfit to prompt-specific statistics, drift from the desired answer distribution, and ultimately degrade generation quality. This failure mode is not surprising, as in this case TTA must adapt to a single prompt within only a few gradient steps, unlike standard training that averages updates over large datasets and long optimization horizons. Therefore, we propose layer-wise dynamic test-time adaptation, a framework which explicitly modulates TTA strength as a function of prompt representation, LLM structure and adaptation step. In our setting, TTA updates only LoRA parameters, and a lightweight hypernetwork predicts per-layer, per-step learning-rate multipliers, enabling fine-grained control. Experiments across various datasets and LLMs consistently show that our method substantially strengthens TTA by learning effective scaling patterns over adaptation steps and transformer layer projections, improving stability while delivering better performance.", "AI": {"tldr": "该研究提出了一种名为“层级动态测试时适应”（LD-TTA）的框架，用于解决无监督、样本特定的 LLM 测试时适应（TTA）中存在的模型不稳定和性能下降问题。LD-TTA 通过一个轻量级超网络，根据提示表示、LLM 结构和适应步数，动态调整 LoRA 参数的每层每步学习率，从而实现更精细的控制。", "motivation": "现有的无监督、样本特定的 LLM 测试时适应（TTA）方法，在每次推理时仅使用提示本身进行模型参数更新，存在不稳定的问题。这种不稳定性源于 TTA 需要在极少的梯度步数内适应单个提示，容易出现过拟合、偏离目标答案分布，最终导致生成质量下降。", "method": "提出层级动态测试时适应（LD-TTA）框架。该框架仅更新 LoRA 参数，并利用一个轻量级超网络来预测每层、每步的学习率乘数。这个乘数会根据提示表示、LLM 结构和适应步数进行动态调整，从而实现对 TTA 强度的精细控制。", "result": "实验结果表明，LD-TTA 方法在不同数据集和 LLM 上均能显著增强 TTA 的效果。通过学习有效的适应步数和 Transformer 层投影缩放模式，该方法提高了 TTA 的稳定性，并带来了更好的性能。", "conclusion": "LD-TTA 框架能够有效地解决无监督、样本特定的 LLM TTA 中的不稳定性和性能下降问题，通过动态调整学习率，实现更稳定和高性能的测试时适应。"}}
{"id": "2602.09524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09524", "abs": "https://arxiv.org/abs/2602.09524", "authors": ["Han Zhou", "Yuxuan Gao", "Yinchao Du", "Xuezhe Zheng"], "title": "HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection", "comment": "14 pages, 6 figures, references added", "summary": "Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.", "AI": {"tldr": "本文提出了一种名为 HLGFA 的无监督工业异常检测框架，该框架通过建模高低分辨率特征的一致性来学习正常样本的模式，并在推理时检测跨分辨率对齐失效的区域，从而实现异常检测。同时，还提出了一种噪声感知的数据增强策略来提高鲁棒性。", "motivation": "工业场景中缺陷样本稀缺，需要可靠的无监督异常检测方法。现有的基于重构的方法在处理复杂纹理和噪声时存在局限性。", "method": "HLGFA 框架利用高分辨率和低分辨率输入，通过共享的冻结骨干网络提取多层特征。高分辨率特征被分解为结构和细节先验，用于指导低分辨率特征的细化，具体通过条件调制和门控残差校正实现。此外，引入了噪声感知的数据增强策略。", "result": "在 MVTec AD 数据集上，HLGFA 取得了 97.9% 的像素级 AUROC 和 97.5% 的图像级 AUROC，优于现有的基于重构和基于特征的方法。", "conclusion": "HLGFA 是一种有效的无监督工业异常检测框架，通过跨分辨率特征对齐有效学习正常模式，并在工业环境中表现出优越的性能。"}}
{"id": "2602.10093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10093", "abs": "https://arxiv.org/abs/2602.10093", "authors": ["Baijun Chen", "Weijie Wan", "Tianxing Chen", "Xianda Guo", "Congsheng Xu", "Yuanyang Qi", "Haojie Zhang", "Longyan Wu", "Tianling Xu", "Zixuan Li", "Yizhe Wu", "Rui Li", "Xiaokang Yang", "Ping Luo", "Wei Sui", "Yao Mu"], "title": "UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking", "comment": "Website: https://univtac.github.io/", "summary": "Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.", "AI": {"tldr": "本文提出了UniVTAC，一个基于仿真的视触觉数据合成平台、一个UniVTAC编码器以及一个UniVTAC基准测试，用于改进机器人接触式操作的鲁棒性，并在实验中取得了显著的成功率提升。", "motivation": "现有机器人操作策略主要依赖视觉信息，而对于插入等接触性操作，触觉感知至关重要。然而，物理世界中的大规模、可靠触觉数据获取成本高昂且困难，缺乏统一的评估平台也阻碍了策略学习和系统分析。", "method": "提出了UniVTAC平台，该平台支持三种常用的视触觉传感器，能够生成大规模、可控的接触交互数据。基于此平台，设计了UniVTAC编码器，利用合成数据和特定的监督信号进行训练，为下游操作任务提供以触觉为中心的视触觉表示。此外，构建了UniVTAC基准测试，包含八个代表性的视触觉操作任务。", "result": "在UniVTAC基准测试中，集成UniVTAC编码器将平均成功率提高了17.1%。在真实世界机器人实验中，任务成功率也提升了25%。", "conclusion": "UniVTAC平台、编码器和基准测试能够有效合成视触觉数据，提升机器人接触式操作的性能，尤其是在需要精确触觉反馈的任务中。"}}
{"id": "2602.10069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10069", "abs": "https://arxiv.org/abs/2602.10069", "authors": ["Xinyuan Liu", "Eren Sadikoglu", "Ransalu Senanayake", "Lixiao Huang"], "title": "Humanoid Factors: Design Principles for AI Humanoids in Human Worlds", "comment": null, "summary": "Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.", "AI": {"tldr": "本文提出了“人形机器人因素”框架，这是一个包含物理、认知、社会和伦理四个支柱的框架，旨在促进人形机器人与人类的有效共存和协作。该框架分析了人类和人工智能驱动的人形机器人的能力重叠与差异，并通过评估一个真实的人形机器人控制算法来展示其实用性，强调了传统机器人学度量标准在理解人类认知和交互原则方面的不足。", "motivation": "随着人形机器人越来越多地进入人类的工作、家庭和公共空间，设计挑战从优化人类环境扩展到需要考虑人形机器人与人类的共存和交互。人形机器人引入了类人行为、沟通和社会存在的期望，这改变了可用性、信任和安全方面的考量。", "method": "本文引入了“人形机器人因素”这一概念框架，该框架围绕物理、认知、社会和伦理四个支柱构建。通过分析人类能力与通用人工智能驱动的人形机器人能力之间的重叠和差异来表征它们。最后，将该框架应用于评估一个真实世界的人形机器人控制算法。", "result": "该框架能够识别出传统机器人任务完成指标所忽略的人类认知和交互原则。通过将框架应用于实际算法评估，证明了其在理解人类-人形机器人交互中的实际效用。", "conclusion": "“人形机器人因素”是一个基础性框架，对于设计、评估和管理人类与人形机器人的可持续共存至关重要，它强调了在机器人设计中必须考虑人形机器人的类人特性以及它们与人类的交互方式。"}}
{"id": "2602.09528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09528", "abs": "https://arxiv.org/abs/2602.09528", "authors": ["Ziqiang Shi", "Rujie Liu", "Shanshan Yu", "Satoshi Munakata", "Koichi Shirahata"], "title": "SchröMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrödinger Bridge Problem", "comment": "ICASSP 2026", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchröMind-a novel framework reducing hallucinations via solving the Schrödinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrödinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.", "AI": {"tldr": "提出了一种名为Schrödinger的框架，通过解决Schrödinger桥问题来减少多模态大语言模型（MLLMs）的幻觉问题，该方法在POPE和MME基准测试中取得了最先进的性能。", "motivation": "现有的MLLMs在医疗等高风险领域应用受限，因为它们会产生幻觉，即生成的文本与视觉输入相矛盾或忽略视觉输入。作者认为MLLMs能够理解图像但难以生成准确的token序列，并且其自回归生成特性阻碍了错误纠正。", "method": "提出Schrödinger框架，通过解决Schrödinger桥问题，在幻觉激活和真实激活之间建立token级别的映射。该方法通过轻量级训练实现最小的传输成本，同时保留模型原有能力。", "result": "在POPE和MME基准测试上的大量实验表明，Schrödinger框架优于现有方法，取得了最先进的性能，并且计算开销极小。", "conclusion": "Schrödinger框架有效解决了MLLMs的幻觉问题，通过解决Schrödinger桥问题，在保持模型原有能力的同时，以极低的计算成本显著提高了模型在视觉问答任务上的准确性。"}}
{"id": "2602.09805", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09805", "abs": "https://arxiv.org/abs/2602.09805", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "Decomposing Reasoning Efficiency in Large Language Models", "comment": "Preprint (under review). 29 pages, 4 figures", "summary": "Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.", "AI": {"tldr": "本研究提出了一个可选追踪的框架，用于评估大型语言模型在推理任务中的代币效率，该框架将效率分解为完成度、条件正确性和冗余度等可解释因素，并根据是否有推理轨迹进一步细分冗余度。在CogniLoad基准测试中，发现准确性和代币效率的排名存在差异，效率差距主要由条件正确性驱动，且冗余度开销差异显著，与模型规模关系不大。", "motivation": "标准评估方法仅报告最终准确率，无法揭示代币在推理过程中的消耗情况，即代币效率问题。研究者希望开发一种更全面的评估方法，以理解和改进大型语言模型的代币效率。", "method": "提出一个可选追踪的框架，将代币效率分解为：1. 在固定代币预算下的完成度；2. 在完成前提下的条件正确性；3. 冗余度（代币使用量）。当存在实例工作负载代理时，冗余度进一步分解为：1. 平均冗余度开销（每工作单位的代币数）；2. 耦合系数（冗余度如何随任务负载扩展）。当推理轨迹可用时，添加确定性轨迹质量度量（接地、重复、提示复制）来区分循环浪费和冗长但参与度高的推理。", "result": "在CogniLoad基准上评估25个模型，发现准确率和代币效率排名存在显著差异（Spearman ρ=0.63）。效率差距常由条件正确性驱动。冗余度开销平均相差约9倍，且与模型规模的关联性较弱。", "conclusion": "提出的分解框架能够揭示不同瓶颈类型，为改进模型效率提供不同的干预方向。研究表明，代币效率评估应超越简单的准确率指标，并考虑代币在推理过程中的具体使用情况。"}}
{"id": "2602.09821", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09821", "abs": "https://arxiv.org/abs/2602.09821", "authors": ["Jiaquan Zhang", "Chaoning Zhang", "Shuxu Chen", "Yibei Liu", "Chenghao Li", "Qigan Sun", "Shuai Yuan", "Fachrina Dewi Puspitasari", "Dongshen Han", "Guoqing Wang", "Sung-Ho Bae", "Yang Yang"], "title": "Text summarization via global structure awareness", "comment": "24pages", "summary": "Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a semantic-weighted graph from sentence embeddings, where persistent homology identifies core semantics and logical structures, preserved in a ``protection pool'' as the backbone for summarization. We design a topology-guided iterative strategy, where lightweight proxy metrics approximate sentence importance to avoid repeated high-cost computations, thus preserving structural integrity while improving efficiency. To further enhance long-text processing, we propose a hierarchical strategy that integrates segment-level and global summarization. Experiments on multiple datasets demonstrate that GloSA-sum reduces redundancy while preserving semantic and logical integrity, striking a balance between accuracy and efficiency, and further benefits LLM downstream tasks by shortening contexts while retaining essential reasoning chains.", "AI": {"tldr": " GloSA-sum 是一种新颖的长文本摘要方法，它利用拓扑数据分析 (TDA) 来识别和保留文本的全局结构、核心语义和逻辑依赖关系，从而在保持摘要准确性的同时提高效率，并能增强大型语言模型 (LLM) 的下游任务性能。", "motivation": "现有文本摘要方法往往忽略全局结构，导致摘要连贯性差且影响下游任务性能。而使用大型语言模型 (LLM) 虽然能提高准确性，但成本高昂。因此，需要一种既能感知全局结构又能高效处理长文本的摘要方法。", "method": "GloSA-sum 采用拓扑数据分析 (TDA)。首先，通过句子嵌入构建语义加权图，利用持久同调识别核心语义和逻辑结构，并将其存入“保护池”。然后，设计了一种拓扑引导的迭代策略，使用轻量级代理指标估算句子重要性，以避免高成本计算，从而在保持结构完整性的同时提高效率。此外，还提出了一种分层策略，结合了片段级和全局摘要，以进一步优化长文本处理。", "result": "在多个数据集上的实验表明，GloSA-sum 能够减少冗余，同时保持语义和逻辑的完整性，实现了准确性和效率的平衡。该方法还能通过保留关键推理链来缩短上下文，从而提升 LLM 的下游任务表现。", "conclusion": "GloSA-sum 是首个实现全局结构感知的摘要方法，它通过 TDA 的创新应用，在长文本摘要任务上取得了良好的效果，能够有效平衡摘要质量和计算效率，并对 LLM 的应用有积极的促进作用。"}}
{"id": "2602.09724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09724", "abs": "https://arxiv.org/abs/2602.09724", "authors": ["Maciej Rapacz", "Aleksander Smywiński-Pohl"], "title": "Targum -- A Multilingual New Testament Translation Corpus", "comment": null, "summary": "Many European languages possess rich biblical translation histories, yet existing corpora - in prioritizing linguistic breadth - often fail to capture this depth. To address this gap, we introduce a multilingual corpus of 657 New Testament translations, of which 352 are unique, with unprecedented depth in five languages: English (208 unique versions from 396 total), French (41 from 78), Italian (18 from 33), Polish (30 from 48), and Spanish (55 from 102). Aggregated from 12 online biblical libraries and one preexisting corpus, each translation is manually annotated with metadata that maps the text to a standardized identifier for the work, its specific edition, and its year of revision. This canonicalization empowers researchers to define \"uniqueness\" for their own needs: they can perform micro-level analyses on translation families, such as the KJV lineage, or conduct macro-level studies by deduplicating closely related texts. By providing the first resource designed for such flexible, multilevel analysis, our corpus establishes a new benchmark for the quantitative study of translation history.", "AI": {"tldr": "本文介绍了首个包含657个新约圣经翻译的多语言语料库，重点关注五种欧洲语言（英语、法语、意大利语、波兰语、西班牙语）的翻译深度，并提供了详细的元数据，以支持对翻译历史进行灵活的多层次定量分析。", "motivation": "现有的圣经翻译语料库在语言多样性上有所侧重，而忽略了对翻译历史深度的捕捉。作者希望构建一个能够捕捉翻译深度的语料库，以弥补这一不足。", "method": "作者收集了来自12个在线圣经图书馆和一个已有语料库的657个新约圣经翻译。对其中352个独特的翻译进行了深入研究，并为每种翻译手动添加了元数据，包括标准化的作品标识符、具体版本和修订年份。语料库重点关注英语、法语、意大利语、波兰语和西班牙语。", "result": "构建了一个包含657个新约圣经翻译的多语言语料库，其中352个是独特的。该语料库在五种欧洲语言上具有前所未有的翻译深度，并且每个翻译都带有详细的元数据。这种规范化使得研究人员能够灵活地定义“独特性”，从而进行微观（如翻译家族分析）和宏观（如去重）层面的研究。", "conclusion": "该语料库是第一个为灵活的多层次分析设计的资源，为翻译历史的定量研究树立了新的基准，能够支持对翻译的细致研究，例如翻译家族的演变或不同版本之间的关联。"}}
{"id": "2602.10098", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10098", "abs": "https://arxiv.org/abs/2602.10098", "authors": ["Jingwen Sun", "Wenyao Zhang", "Zekun Qi", "Shaojie Ren", "Zezhi Liu", "Hanxin Zhu", "Guangzhong Sun", "Xin Jin", "Zhibo Chen"], "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model", "comment": null, "summary": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \\emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.", "AI": {"tldr": "本文提出了一种名为VLA-JEPA的预训练框架，用于处理视觉-语言-动作（VLA）策略。它通过“无泄漏的状态预测”来避免现有方法中的外观偏差和信息泄漏问题，并在多个任务中取得了比现有方法更好的泛化性和鲁棒性。", "motivation": "现有的基于隐式动作的预训练方法容易受到外观偏差、无关运动和信息泄漏的影响，因为它们更关注像素变化而非与动作相关的状态转移。研究者希望开发一种更鲁棒的方法。", "method": "VLA-JEPA采用JEPA（Joint Embedding Predictive Architecture）风格的预训练框架。核心思想是“无泄漏的状态预测”，即使用目标编码器从未来帧生成潜在表征，而学生路径仅接收当前观测。通过在潜在空间而非像素空间进行预测，VLA-JEPA学习到了对相机运动和背景变化不敏感的动力学抽象。", "result": "在LIBERO、LIBERO-Plus、SimplerEnv以及真实世界操作任务上的实验表明，VLA-JEPA在泛化性和鲁棒性方面持续优于现有方法。该方法简化了预训练流程，仅需两阶段：JEPA预训练和动作头微调。", "conclusion": "VLA-JEPA通过“无泄漏的状态预测”和在潜在空间进行预测，有效地解决了现有VLA预训练方法中的关键问题，能够学习到更鲁棒的动力学抽象，并带来了性能上的显著提升。"}}
{"id": "2602.09760", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09760", "abs": "https://arxiv.org/abs/2602.09760", "authors": ["Kohei Oda", "Hiroya Takamura", "Kiyoaki Shirai", "Natthawut Kertkeidkachorn"], "title": "Improving Interpretability of Lexical Semantic Change with Neurobiological Features", "comment": "PACLIC 2025", "summary": "Lexical Semantic Change (LSC) is the phenomenon in which the meaning of a word change over time. Most studies on LSC focus on improving the performance of estimating the degree of LSC, however, it is often difficult to interpret how the meaning of a word change. Enhancing the interpretability of LSC is a significant challenge as it could lead to novel insights in this field. To tackle this challenge, we propose a method to map the semantic space of contextualized embeddings of words obtained by a pre-trained language model to a neurobiological feature space. In the neurobiological feature space, each dimension corresponds to a primitive feature of words, and its value represents the intensity of that feature. This enables humans to interpret LSC systematically. When employed for the estimation of the degree of LSC, our method demonstrates superior performance in comparison to the majority of the previous methods. In addition, given the high interpretability of the proposed method, several analyses on LSC are carried out. The results demonstrate that our method not only discovers interesting types of LSC that have been overlooked in previous studies but also effectively searches for words with specific types of LSC.", "AI": {"tldr": "本文提出了一种将词汇语义变化（LSC）的语义空间映射到神经生物学特征空间的方法，以提高LSC的可解释性，并取得了比以往方法更好的性能，还能发现和搜索具有特定LSC类型的词语。", "motivation": "现有研究主要关注LSC的量化，而忽视了其语义变化的解释性，这限制了该领域的深入研究。提高LSC的可解释性能够带来新的见解。", "method": "将预训练语言模型获得的词语语境化嵌入的语义空间映射到一个神经生物学特征空间，其中每个维度代表一个原始词语特征，其值代表该特征的强度。", "result": "该方法在LSC程度估计方面优于大多数现有方法。通过高可解释性，该方法能够发现之前被忽略的LSC类型，并有效搜索具有特定LSC类型的词语。", "conclusion": "该方法成功解决了LSC的可解释性挑战，并在此基础上实现了更优的LSC程度估计，同时还能发现和搜索具有特定语义变化模式的词语，为LSC研究开辟了新途径。"}}
{"id": "2602.10101", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10101", "abs": "https://arxiv.org/abs/2602.10101", "authors": ["Sizhe Yang", "Linning Xu", "Hao Li", "Juncheng Mu", "Jia Zeng", "Dahua Lin", "Jiangmiao Pang"], "title": "Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction", "comment": null, "summary": "3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.", "AI": {"tldr": "本文提出了一种名为Robo3R的新型3D重建模型，能够从RGB图像和机器人状态实时预测精确、度量尺度的场景几何，以满足机器人操作的需求。该模型通过联合推断局部几何和相机姿态，并利用掩码点和基于关键点的PnP算法进行精炼，在多种下游机器人操作任务中表现优于现有方法。", "motivation": "现有的3D重建方法在精度和度量一致性方面存在不足，无法满足机器人物理交互的需求。深度传感器也存在噪声和材料敏感性问题，因此需要一种能够直接从RGB图像和机器人状态实时生成高质量3D几何的模型。", "method": "Robo3R模型采用前馈网络，联合推断尺度不变的局部几何和相对相机姿态，并通过学习到的全局相似变换将它们统一到机器人坐标系中。为了提高精度，模型使用了掩码点预测精细的点云，并采用基于关键点的PnP算法来优化相机外参和全局对齐。", "result": "Robo3R在合成数据集Robo3R-4M上进行了训练，并在下游任务（包括模仿学习、Sim-to-real迁移、抓取合成和无碰撞运动规划）中取得了优于现有最先进的重建方法和深度传感器的方法。", "conclusion": "Robo3R是一种能够实时生成精确、度量尺度3D几何的机器人操作友好型3D重建模型，为机器人操作提供了有前景的3D感知替代方案。"}}
{"id": "2602.09529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09529", "abs": "https://arxiv.org/abs/2602.09529", "authors": ["Emad Gholibeigi", "Abbas Koochari", "Azadeh ZamaniFar"], "title": "SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection", "comment": "6 pages, 2 figures, 3 tables. Submitted for review", "summary": "Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.", "AI": {"tldr": "本文提出了一种名为 SCA-Net 的改进型深度学习模型，用于高精度地检测遥感影像中的建筑物和道路变化，该模型在处理小目标和降低计算成本方面表现出色，并在 LEVIR-CD 和 LEVIR-MCI 数据集上取得了优于现有方法的性能。", "motivation": "现有的深度学习模型在遥感影像变化检测中存在对小目标敏感度低和计算成本高的问题，而精确的建筑物和道路变化检测对于城市管理、环境监测和灾害评估至关重要。", "method": "SCA-Net 基于 Change-Agent 框架，引入了差分金字塔模块（Difference Pyramid Block）进行多尺度变化分析，自适应多尺度处理模块（Adaptive Multi-scale Processing）结合了形状感知和高分辨率增强，并利用了多级注意力机制（PPM 和 CSAGate）进行上下文和细节的联合处理。此外，还采用了动态复合损失函数和四阶段训练策略来稳定训练和加速收敛。", "result": "在 LEVIR-MCI 数据集上，SCA-Net 的 mIoU 提高了 2.64%，对小建筑物目标的 IoU 提升了 57.9%，同时训练时间减少了 61%。在 LEVIR-CD 数据集上，SCA-Net 也取得了优于 Change-Agent 和其他 SOTA 方法的性能。", "conclusion": "SCA-Net 是一种高效、准确且鲁棒的变化检测解决方案，能够有效克服现有方法的局限性，并在实际应用中展现出巨大潜力。"}}
{"id": "2602.09856", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09856", "abs": "https://arxiv.org/abs/2602.09856", "authors": ["Yuhao Zheng", "Li'an Zhong", "Yi Wang", "Rui Dai", "Kaikui Liu", "Xiangxiang Chu", "Linyuan Lv", "Philip Torr", "Kevin Qinghong Lin"], "title": "Code2World: A GUI World Model via Renderable Code Generation", "comment": "github: https://github.com/AMAP-ML/Code2World project page: https://amap-ml.github.io/Code2World/", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "AI": {"tldr": "本文提出了一种名为Code2World的视觉-语言模型，通过生成可渲染的代码来预测GUI的下一个视觉状态，解决了现有方法在视觉保真度和结构可控性方面的不足。通过AndroidCode数据集的构建和渲染感知强化学习的训练，Code2World在UI预测任务上取得了领先性能，并显著提升了下游导航任务的成功率。", "motivation": "现有基于文本和像素的方法在GUI交互模拟中难以同时保证高视觉保真度和精细的结构可控性，同时GUI交互数据的稀缺性也阻碍了模型的训练。", "method": "1. 构建AndroidCode数据集：将GUI轨迹翻译成高保真HTML，并通过视觉反馈修正机制生成超过80K个高质量屏幕-动作对。\n2. 训练Code2World：首先使用SFT进行冷启动以遵循格式布局，然后应用渲染感知强化学习，使用渲染结果作为奖励信号，强制执行视觉语义保真度和动作一致性。", "result": "Code2World-8B在UI预测任务上取得了顶尖性能，可与GPT-5和Gemini-3-Pro-Image媲美。在AndroidWorld导航任务上，Code2World使Gemini-2.5-Flash的导航成功率提升了9.5%。", "conclusion": "Code2World通过生成可渲染代码有效解决了GUI交互模拟中的视觉保真度和结构可控性问题，并且通过构建大规模数据集和创新的训练方法，显著提升了UI预测和下游导航任务的性能。"}}
{"id": "2602.09531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09531", "abs": "https://arxiv.org/abs/2602.09531", "authors": ["Bohan Fu", "Guanyi Qin", "Fazhan Zhang", "Zihao Huang", "Mingxuan Li", "Runze Hu"], "title": "DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment", "comment": "Accepted by AAAI 2026", "summary": "Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.", "AI": {"tldr": "提出了一种名为DR.Experts的新型无参考图像质量评估（BIQA）框架，通过利用视觉-语言模型生成失真先验，并结合语义注意力来增强这些先验，最后通过动态加权模块进行融合，以提高评估的准确性和泛化能力。", "motivation": "现有BIQA模型难以有效捕捉细微的失真线索，导致与人类主观判断不一致。主要原因是缺乏可靠的失真先验，模型仅学习浅层特征与质量分数的关系。", "method": "1. 使用退化感知视觉-语言模型获取失真特定的先验。 2. 提出“失真-显著性差异模块”区分语义注意力，细化和增强失真先验。 3. 提出“动态失真加权模块”，融合细化后的先验、语义和桥接表示，并根据感知影响对每个失真特定特征进行加权。", "result": "在五个具有挑战性的BIQA基准数据集上进行了大量实验，结果表明DR.Experts在准确性、泛化能力和数据效率方面优于现有方法。", "conclusion": "DR.Experts框架通过显式地整合失真先验，能够更可靠地进行质量评估，并与人类感知更一致，显著提升了BIQA的性能。"}}
{"id": "2602.09918", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09918", "abs": "https://arxiv.org/abs/2602.09918", "authors": ["Gulraiz Khan", "Kenneth Y. Wertheim", "Kevin Pimbblet", "Waqas Ahmed"], "title": "SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models", "comment": null, "summary": "Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.", "AI": {"tldr": "本文提出了一种名为SARS（Shape and Appearance-Aware 3D Reconstruction System）的3D重建系统，该系统能够从单张2D图像中提取面部和身体信息，并重建完整的3D人体模型，同时考虑了形状和外观的细微变化。", "motivation": "现有3D人脸重建技术主要关注全局结构或几何信息，忽略了年龄、性别、面部地标等高层级面部语义特征。这导致在处理这些特征变化时存在不足。因此，需要一个能够适应这些高层级面部特征变化的3D重建系统。", "method": "该研究提出了一种模块化的3D重建流程（SARS），它能够从单张2D图像中提取人体（包括面部和身体）的信息，并利用可变形模型（3DMM）来重建详细的3D模型。该系统能够控制形状、纹理、光照和相机参数等多种可变性，并特别关注形状和外观的感知。", "result": "SARS系统能够准确地重建出包含形状和外观细节的3D人体模型，并且能够有效地处理面部语义特征的变化。", "conclusion": "SARS系统通过整合形状和外观信息，克服了以往3D重建方法在捕捉面部语义特征方面的局限性，能够从单张图像中实现更精确、更全面的3D人体重建。"}}
{"id": "2602.09785", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09785", "abs": "https://arxiv.org/abs/2602.09785", "authors": ["Seydou Diallo", "Yacouba Diarra", "Mamadou K. Keita", "Panga Azazia Kamaté", "Adam Bouno Kampo", "Aboubacar Ouattara"], "title": "Where Are We At with Automatic Speech Recognition for the Bambara Language?", "comment": "v1- 8 pages, 5 tables, 1 figure- AfricaNLP Workshop @ EACL 2026", "summary": "This paper introduces the first standardized benchmark for evaluating Automatic Speech Recognition (ASR) in the Bambara language, utilizing one hour of professionally recorded Malian constitutional text. Designed as a controlled reference set under near-optimal acoustic and linguistic conditions, the benchmark was used to evaluate 37 models, ranging from Bambara-trained systems to large-scale commercial models. Our findings reveal that current ASR performance remains significantly below deployment standards in a narrow formal domain; the top-performing system in terms of Word Error Rate (WER) achieved 46.76\\% and the best Character Error Rate (CER) of 13.00\\% was set by another model, while several prominent multilingual models exceeded 100\\% WER. These results suggest that multilingual pre-training and model scaling alone are insufficient for underrepresented languages. Furthermore, because this dataset represents a best-case scenario of the most simplified and formal form of spoken Bambara, these figures are yet to be tested against practical, real-world settings. We provide the benchmark and an accompanying public leaderboard to facilitate transparent evaluation and future research in Bambara speech technology.", "AI": {"tldr": "本研究提出了第一个针对班巴拉语的自动语音识别（ASR）标准化基准，评估了37个模型，结果显示现有ASR性能远未达到部署标准，尤其是在非英语语言方面，并提供了公开基准和排行榜以促进未来研究。", "motivation": "班巴拉语作为一种代表性不足的语言，缺乏标准的ASR评估基准，现有模型在该语言上的性能评估不足。", "method": "创建了一个包含一小时专业录制的马里宪法文本的标准化班巴拉语ASR基准，并用该基准评估了37个不同类型的ASR模型。", "result": "在班巴拉语基准上，词错误率（WER）最高性能为46.76%，字符错误率（CER）最高性能为13.00%。多个知名多语言模型WER超过100%。", "conclusion": "目前的ASR模型在班巴拉语等代表性不足的语言上，即使在近乎最优的正式领域，性能也远低于部署要求。多语言预训练和模型扩展不足以解决这些语言的ASR问题。研究提供了基准和排行榜以支持未来的班巴拉语语音技术研究。"}}
{"id": "2602.10105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10105", "abs": "https://arxiv.org/abs/2602.10105", "authors": ["Juncheng Mu", "Sizhe Yang", "Yiming Bao", "Hojin Bae", "Tianming Wei", "Linning Xu", "Boyi Li", "Huazhe Xu", "Jiangmiao Pang"], "title": "DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos", "comment": null, "summary": "Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).", "AI": {"tldr": "本文提出了DexImit框架，能够将单目人类操作视频自动转化为适用于机器人学习的物理上合理的数据，以解决数据稀疏性问题，并实现零样本的真实世界部署。", "motivation": "现实世界中收集灵巧手操作的真实数据成本高昂且耗时，数据稀缺严重限制了双臂灵巧操作的泛化能力。人类操作视频包含丰富的操作知识，但人类手部和机器人灵巧手之间的具体实现差异（embodiment gap）使得直接利用人类视频进行预训练非常困难。", "method": "DexImit框架包含一个四阶段的生成流程：1. 从任意视角重建具有近乎度量尺度的手-物体交互；2. 进行子任务分解和双臂调度；3. 合成与演示交互一致的机器人轨迹；4. 进行全面的数据增强以实现零样本的真实世界部署。", "result": "DexImit能够基于互联网或视频生成模型的人类视频，生成大规模机器人数据。该框架能够处理多样化的操作任务，包括工具使用（如切苹果）、长时序任务（如制作饮料）和精细操作（如叠杯子）。", "conclusion": "DexImit框架成功地将人类操作视频转化为可用于机器人学习的物理上合理的数据，弥合了人类手部和机器人灵巧手之间的实现差异，从而解锁了大规模人类操作视频数据的潜力，并实现了对各种复杂操作任务的零样本真实世界部署。"}}
{"id": "2602.09532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09532", "abs": "https://arxiv.org/abs/2602.09532", "authors": ["Michael Baltaxe", "Dan Levi", "Sagie Benaim"], "title": "RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes", "comment": null, "summary": "Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.", "AI": {"tldr": "提出了一种名为RAD的检索增强框架，通过利用检索到的相似图像作为几何代理，来提高单目深度估计在复杂场景中代表性不足的类别上的准确性。", "motivation": "当前单目深度估计方法在复杂场景下，对于代表性不足的类别仍然存在精度问题。", "method": "1. 使用不确定性感知检索机制识别输入图像中置信度低的区域，并检索包含语义相似内容的RGB-D上下文样本。 2. 通过双流网络处理输入和检索到的上下文。 3. 利用匹配的交叉注意力模块融合信息，仅在可靠的点对应处传递几何信息。", "result": "RAD在NYU Depth v2、KITTI和Cityscapes数据集上，在代表性不足的类别上显著优于现有方法，分别降低了相对绝对误差29.2%、13.3%和7.2%，同时在标准数据集上保持了竞争力。", "conclusion": "RAD通过引入检索增强机制，有效地缓解了单目深度估计在处理复杂场景及代表性不足类别时的挑战，并在多个基准测试中取得了显著的性能提升。"}}
{"id": "2602.09924", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09924", "abs": "https://arxiv.org/abs/2602.09924", "authors": ["William Lugoloobi", "Thomas Foster", "William Bankes", "Chris Russell"], "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations", "comment": null, "summary": "Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty", "AI": {"tldr": "本研究提出了一种利用大型语言模型（LLM）的内部表征来预测模型在数学和编程任务上的成功率，并基于此进行推理路由，从而在降低推理成本的同时提升性能。", "motivation": "在大型语言模型上运行需要扩展推理的任务成本高昂，但如何判断哪些输入确实需要额外计算是一个难题。研究旨在探索能否在生成之前，从模型的内部表征中恢复其成功的可能性，并利用此信号指导更高效的推理。", "method": "研究训练了线性探测器（linear probes），以预生成激活（pre-generation activations）来预测特定策略在数学和编程任务上的成功率。同时，利用 E2H-AMC 数据集，比较模型内部编码的难度与人类感知的难度。", "result": "线性探测器在预测成功率方面显著优于表面特征（如问题长度和 TF-IDF）。模型内部编码的难度与人类感知的难度不同，且这种差异会随着扩展推理的进行而增大。通过将查询路由到模型池，可以在 MATH 数据集上实现比表现最佳模型更高的性能，同时推理成本降低高达 70%。", "conclusion": "模型的内部表征可以有效地预测生成成功率，并指导推理路由，从而在模型性能与推理成本之间取得实际的效率提升，即使这些内部表征与人类对难度的直觉存在差异。"}}
{"id": "2602.09817", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.09817", "abs": "https://arxiv.org/abs/2602.09817", "authors": ["Khang Ly", "Georgios Cheirmpos", "Adrian Raudaschl", "Christopher James", "Seyed Amin Tabatabaei"], "title": "AnalyticsGPT: An LLM Workflow for Scientometric Question Answering", "comment": null, "summary": "This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the \"science of science.\" When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.", "AI": {"tldr": "本文提出了AnalyticsGPT，一个基于LLM的科学计量问答工作流程，该流程能够进行命名实体识别、多方面数据检索和信息综合。", "motivation": "现有的科学问题回答方法在处理涉及科学计量指标的“科学的科学”元科学问题时存在挑战，LLMs在任务分解和规划方面的潜力激发了这项研究。", "method": "提出一个端到端的系统，采用检索增强生成（RAG）和代理概念的顺序工作流程，并利用专有的研究绩效评估平台作为RAG数据库。系统还处理将数据综合成高质量分析的次要任务。", "result": "通过专家评估和LLMs-as-judges进行了评估，证明了LLM在科学计量问答这一小众任务中的有效性。", "conclusion": "LLMs在科学计量问答方面具有巨大潜力，可以处理命名实体识别、多方面数据检索和信息综合等复杂任务，并能生成结构化的分析报告。"}}
{"id": "2602.09929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09929", "abs": "https://arxiv.org/abs/2602.09929", "authors": ["Zongrui Li", "Xinhua Ma", "Minghui Hu", "Yunqing Zhao", "Yingchen Yu", "Qian Zheng", "Chang Liu", "Xudong Jiang", "Song Bai"], "title": "Monocular Normal Estimation via Shading Sequence Estimation", "comment": "Accepted by ICLR 2026 (Oral Presentation)", "summary": "Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.", "AI": {"tldr": "提出了一种名为RoSE的新方法，通过将法线估计重新定义为着色序列估计，并利用图像到视频生成模型来预测着色序列，从而解决了现有单目法线估计方法中存在的3D不对齐问题。", "motivation": "现有单目法线估计方法依赖深度模型直接预测法线图，但常出现3D不对齐问题，即重建的表面几何细节与真实几何不符。研究者认为这是因为模型难以区分和重建法线图中代表的几何变化，而这些变化仅通过细微的颜色变化体现。", "method": "将法线估计重构为着色序列估计。提出RoSE方法，利用图像到视频生成模型预测着色序列，然后通过求解简单的普通最小二乘问题将预测的着色序列转换为法线图。使用合成数据集MultiShade进行训练，该数据集包含多样的形状、材质和光照条件。", "result": "RoSE在真实世界物体单目法线估计的基准数据集上取得了最先进的性能。", "conclusion": "RoSE通过将法线估计问题转化为着色序列估计，并利用先进的生成模型，有效解决了现有方法的3D不对齐问题，并在单目法线估计任务上取得了显著的性能提升。"}}
{"id": "2602.09826", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09826", "abs": "https://arxiv.org/abs/2602.09826", "authors": ["Abdulmuizz Khalak", "Abderrahmane Issam", "Gerasimos Spanakis"], "title": "From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models", "comment": "Accepted to VarDial 2026", "summary": "Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.", "AI": {"tldr": "本文研究了阿拉伯语语言模型（LMs）在现代标准阿拉伯语（MSA）上预训练后，在阿拉伯语方言上的跨语言迁移能力。研究发现迁移是可能的，但因方言而异，且与地理距离有关。同时，支持所有阿拉伯语方言的模型可能存在负面干扰。", "motivation": "现有的阿拉伯语LMs主要在MSA上预训练，但实际交流多使用各种方言，这限制了LMs的适用性，因为方言与MSA的相似度不同。", "method": "通过在3个自然语言处理（NLP）任务上进行探测（probing）和表示相似性分析，研究阿拉伯语模型的跨语言迁移。", "result": "研究表明，模型在不同方言上的迁移能力存在差异，这种差异部分可以用方言的地理邻近性来解释。此外，支持所有阿拉伯语方言的模型存在负面干扰。", "conclusion": "阿拉伯语LMs的跨语言迁移是可能的，但存在不均衡性，且受地理因素影响。支持所有方言的模型可能存在负面干扰，这引发了对其相似度和跨语言迁移效果的担忧。"}}
{"id": "2602.09534", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09534", "abs": "https://arxiv.org/abs/2602.09534", "authors": ["Jiayi Lyu", "Leigang Qu", "Wenjing Zhang", "Hanyu Jiang", "Kai Liu", "Zhenglin Zhou", "Xiaobo Xia", "Jian Xue", "Tat-Seng Chua"], "title": "AUHead: Realistic Emotional Talking Head Generation via Action Units Control", "comment": "https://openreview.net/forum?id=dmzlAUkulz&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)", "summary": "Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an \"emotion-then-AU\" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR", "AI": {"tldr": "提出了一种名为AUHead的两阶段方法，用于从音频中解耦精细的情感控制（动作单元AU），并实现可控的说话头视频生成。第一阶段利用大型音频语言模型生成AU，第二阶段使用AU驱动的扩散模型生成视频，并引入AU解耦引导策略来权衡情感表现力和身份一致性。", "motivation": "现有方法在生成具有细微情感表达的逼真说话头视频方面存在困难，主要原因是缺乏精细的情感控制。", "method": "该方法分为两个阶段：1. 利用大型音频语言模型（ALMs）通过空间-时间AU标记化和“情感-然后-AU”的思维链机制来生成AU，实现AU与原始语音的解耦。2. 提出一个AU驱动的可控扩散模型，以AU序列作为条件生成视频，通过将AU序列映射到结构化2D面部表示来增强空间保真度，并在交叉注意力模块中建模AU-与视觉的交互。引入AU解耦引导策略以实现AU质量的灵活权衡。", "result": "在基准数据集上，AUHead在情感真实性、唇同步准确性和视觉连贯性方面表现出色，显著优于现有技术。", "conclusion": "AUHead成功地实现了从音频中解耦精细的情感控制（AU），并生成了逼真的、情感表达准确且身份一致的说话头视频，为虚拟化身和影视制作等领域提供了新的解决方案。"}}
{"id": "2602.10109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10109", "abs": "https://arxiv.org/abs/2602.10109", "authors": ["Jinhui Ye", "Fangjing Wang", "Ning Gao", "Junqiu Yu", "Yangkun Zhu", "Bin Wang", "Jinyu Zhang", "Weiyang Jin", "Yanwei Fu", "Feng Zheng", "Yilun Chen", "Jiangmiao Pang"], "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models", "comment": "Spatially Training for VLA, Accepted by ICLR 2026", "summary": "Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/", "AI": {"tldr": "本文提出了一种名为ST4VLA的双系统视觉-语言-动作框架，通过空间引导训练，利用空间先验来改进大型视觉语言模型在具身任务中的表现，显著提升了在真实和模拟机器人环境下的任务完成率和泛化能力。", "motivation": "现有的视觉-语言模型在多模态理解方面表现出色，但在转换为低级运动指令以执行具身任务时存在不足。研究旨在弥合这一差距，提高模型在具身机器人任务中的性能和泛化能力。", "method": "ST4VLA框架包含两个阶段：1. 空间基础预训练：通过预测点、边界框和轨迹，使视觉语言模型具备可迁移的空间先验。2. 空间引导动作后训练：利用空间提示，鼓励模型产生更丰富的空间先验来指导动作生成。该方法旨在保持策略学习中的空间基础，并促进空间和动作目标的一致优化。", "result": "ST4VLA在Google Robot和WidowX Robot数据集上均取得了显著的性能提升，分别从66.1%提升到84.6%，从54.7%提升到73.2%，并在SimplerEnv上创造了新的最优性能记录。该模型还表现出对未见过物体和指令释义的更强泛化能力，以及在真实世界长时扰动下的鲁棒性。", "conclusion": "可扩展的空间引导训练是实现鲁棒、泛化能力强的机器人学习的有前景的方向。ST4VLA的成功表明，将空间先验整合到视觉-语言-动作框架中能够有效提升模型在具身任务中的表现。"}}
{"id": "2602.09541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09541", "abs": "https://arxiv.org/abs/2602.09541", "authors": ["Ziqiang Shi", "Rujie Liu", "Shanshan Yu", "Satoshi Munakata", "Koichi Shirahata"], "title": "Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination", "comment": "WACV 2026 (It was accepted in the first round, with an acceptance rate of 6%.)", "summary": "Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \\textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.", "AI": {"tldr": "本文提出了一种名为Scalpel的方法，通过精炼Transformer层中注意力激活的分布来减少视觉语言模型（LVLMs）的幻觉问题。Scalpel通过预测可信赖的注意力方向并进行调整，实现了对幻觉的有效减轻，且无需额外计算。", "motivation": "大型视觉语言模型（LVLMs）虽然性能强大，但常因LLM的先验知识和模态间注意力不匹配而产生与视觉内容不符的输出，即幻觉。现有方法未能有效解决这一问题。", "method": "Scalpel通过在Transformer层中预测每个注意力头的可信注意力方向，并据此调整激活值来减少幻觉。它使用高斯混合模型捕捉信任区域和幻觉区域注意力分布的特点，并利用熵最优传输（等价于薛定谔桥问题）精确映射高斯分量。在缓解幻觉时，Scalpel根据分量成员资格和幻觉与信任激活之间的映射关系，动态调整干预的强度和方向。", "result": "在多个数据集和基准测试上的广泛实验表明，Scalpel能有效减轻幻觉，性能优于先前方法，并达到最先进水平。此外，Scalpel不依赖于特定模型或数据，仅需一个解码步骤，无需额外计算。", "conclusion": "Scalpel是一种有效的、模型和数据无关的方法，能够通过精炼注意力激活分布来显著减少LVLMs中的幻觉，且无需额外的计算资源。"}}
{"id": "2602.09933", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09933", "abs": "https://arxiv.org/abs/2602.09933", "authors": ["Melika Qahqaie", "Dominik Neumann", "Tobias Heimann", "Andreas Maier", "Veronika A. Zimmer"], "title": "Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors", "comment": "This work has been submitted to the IEEE for possible publication. Accepted at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.", "AI": {"tldr": "提出了一种基于非均衡最优传输（UOT）的配准感知匹配方法，用于解决纵向CT扫描中病灶的对应问题，提高了病灶演变的评估精度。", "motivation": "现有病灶对应方法在处理病灶出现、消失、合并或分裂时效果不佳，亟需一种能处理这些复杂情况的鲁棒方法。", "method": "利用非均衡最优传输（UOT）构建匹配算法，运输成本结合了归一化几何形状、配准场雅可比矩阵以及可选的局部外观一致性。通过相对剪枝稀疏化运输计划，实现一对一匹配，并识别新出现、消失、合并和分裂的病灶。", "result": "与仅基于距离的基线方法相比，该方法在纵向CT数据上实现了更高的边缘检测精度和召回率，提高了病灶状态的召回率，并获得了更优的病灶图组件F1分数。", "conclusion": "该配准感知UOT匹配方法能够有效地处理纵向CT扫描中病灶的复杂演变，无需重新训练或启发式规则，显著优于现有方法。"}}
{"id": "2602.10106", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10106", "abs": "https://arxiv.org/abs/2602.10106", "authors": ["Modi Shi", "Shijia Peng", "Jin Chen", "Haoran Jiang", "Yinghui Li", "Di Huang", "Ping Luo", "Hongyang Li", "Li Chen"], "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration", "comment": "Project page: https://opendrivelab.com/EgoHumanoid", "summary": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.", "AI": {"tldr": "EgoHumanoid 是一个新框架，它结合了大量人类演示数据和少量机器人数据，训练了一个能够执行 loco-manipulation（运动操控）的机器人视觉-语言-动作策略，有效解决了人与机器人之间的具身鸿沟问题，并在实际应用中显著优于仅使用机器人数据的基线。", "motivation": "尽管人类演示数据在机器人操控领域具有潜力，但对于更具挑战性、需要更多数据的人形机器人运动操控（loco-manipulation）领域，其应用仍未得到充分探索。研究的动机在于利用丰富的人类演示数据来增强人形机器人的运动操控能力。", "method": "该研究提出了一种名为 EgoHumanoid 的框架，该框架利用了大量的以自我为中心的人类演示数据和少量机器人数据进行联合训练。为了弥合人类和机器人之间的具身鸿沟（包括形态和视角差异），研究人员开发了一个系统的对齐流程，涵盖硬件设计和数据处理。具体而言，他们开发了一个便携式系统来收集可扩展的人类数据，并制定了实用的收集协议。对齐流程包含两个关键组件：视图对齐（解决相机高度和视角差异）和动作对齐（将人类动作映射到统一的、运动学上可行的机器人控制空间）。", "result": "大量的真实世界实验表明，引入不包含机器人数据的以自我为中心的人类演示数据，相比仅使用机器人数据的基线，在 loco-manipulation 任务上性能提升了 51%，尤其是在未见过的新环境中。研究分析还揭示了哪些行为可以有效迁移，以及人类数据在扩展方面的潜力。", "conclusion": "EgoHumanoid 框架成功地利用了以自我为中心的人类演示数据，显著提高了人形机器人在 loco-manipulation 任务上的性能，尤其是在面对新环境时。该研究证明了人类演示数据在弥合具身鸿沟和增强机器人能力方面的价值，并为未来利用大规模人类数据训练机器人提供了方向。"}}
{"id": "2602.09832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09832", "abs": "https://arxiv.org/abs/2602.09832", "authors": ["Bakhtawar Ahtisham", "Kirk Vanacore", "Zhuqian Zhou", "Jinsook Lee", "Rene F. Kizilcec"], "title": "LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.", "AI": {"tldr": "研究表明，利用大型语言模型（LLM）生成的推理可以有效预测其自身在教育对话标注任务中的错误。通过分析LLM生成的推理，可以识别出不准确的标注，从而提高教育对话分析的自动化质量控制。", "motivation": "现有的大型语言模型在教育对话分析中缺乏可靠的错误检测机制，导致自动化分析的质量难以保证。", "method": "分析了30,300条教师言语，这些言语由多个LLM标注了教学行为及其推理。将预测LLM标注正确性的任务构建为二分类问题。使用TF-IDF编码LLM推理，并训练了五个监督分类器（包括随机森林）。此外，还研究了四种语言标记（因果、区分、试探、洞察）以及句法复杂性和推理长度与正确性的关系。", "result": "随机森林分类器在预测LLM标注正确性方面取得了0.83的F1分数。训练专门针对特定教学行为的检测器可以进一步提高性能。正确预测的推理更倾向于使用基于事实的因果语言，而错误推理则更多地使用试探性语言和元认知表达。句法复杂性和推理长度与正确性无关。", "conclusion": "基于LLM推理的错误检测是一种实用且可扩展的教育对话质量控制方法。通过分析推理中的语言特征，可以有效识别LLM的错误标注，提高自动化分析的可靠性。"}}
{"id": "2602.10111", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10111", "abs": "https://arxiv.org/abs/2602.10111", "authors": ["Yunfan Ren", "Zhiyuan Zhu", "Jiaxu Xing", "Davide Scaramuzza"], "title": "Learning Agile Quadrotor Flight in the Real World", "comment": null, "summary": "Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.", "AI": {"tldr": "提出了一种无需精确系统识别或离线Sim2Real迁移的自适应控制框架，通过自适应时间缩放（ATS）和在线残差学习，并结合基于现实锚定的短期BPTT（RASH-BPTT）实现高效鲁棒的飞行中策略更新，显著提升了四旋翼的敏捷性，使其能够在执行剧烈机动时接近执行器饱和极限。", "motivation": "基于学习的控制器在模拟环境中表现优异，但在实际应用中，不准确的系统模型、外部干扰以及硬件退化等不确定性限制了其安全性和敏捷性。现有的安全运行策略通常过于保守，而在线自适应方法在数据稀缺和安全风险方面存在瓶颈。", "method": "提出了一种自适应框架，通过自适应时间缩放（ATS）探索平台物理极限，并利用在线残差学习增强简单标称模型。在此基础上，提出实时锚定短期BPTT（RASH-BPTT）进行高效鲁棒的飞行中策略更新。", "result": "实验表明，四旋翼能够可靠地执行接近执行器饱和极限的敏捷机动，并且在约100秒的飞行时间内，保守的基准策略峰值速度从1.9 m/s提升到7.3 m/s。", "conclusion": "真实世界的在线自适应不仅可以补偿建模误差，更是提升激进飞行性能的实用机制，能够持续改善飞行性能。"}}
{"id": "2602.09586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09586", "abs": "https://arxiv.org/abs/2602.09586", "authors": ["Bo Peng", "Yuanwei Hu", "Bo Liu", "Ling Chen", "Jie Lu", "Zhen Fang"], "title": "Delving into Spectral Clustering with Vision-Language Representations", "comment": "ICLR26", "summary": "Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \\textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.", "AI": {"tldr": "提出了一种名为神经切线核谱聚类（Neural Tangent Kernel Spectral Clustering）的新方法，通过融合视觉和语言信息来增强谱聚类效果，并在多个数据集上取得了显著的性能提升。", "motivation": "现有的谱聚类方法多基于单模态数据，未能充分利用多模态表示中的丰富信息。受近期视觉-语言预训练成功的启发，本文旨在将谱聚类从单模态扩展到多模态领域。", "method": "本文提出神经切线核谱聚类方法，利用预训练的视觉-语言模型进行跨模态对齐。通过将神经切线核与与图像相关的阳性名词（即语义上接近的词语）进行关联，将图像间的亲和力定义为视觉邻近性和语义重叠性的耦合。此外，还提出了一种正则化亲和力扩散机制，以自适应地集成由不同提示诱导的亲和力矩阵。", "result": "该方法通过增强类内连接并抑制类间虚假连接，鼓励了块对角结构。在包括经典、大规模、细粒度和领域转移数据集在内的16个基准测试中，实验结果表明该方法在性能上持续大幅超越了最先进的方法。", "conclusion": "所提出的神经切线核谱聚类方法通过有效地融合视觉和语言信息，能够更有效地进行无监督数据聚类，并在各种数据集上展现出优越的性能。"}}
{"id": "2602.10114", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10114", "abs": "https://arxiv.org/abs/2602.10114", "authors": ["Dan Evron", "Elias Goldsztejn", "Ronen I. Brafman"], "title": "Decoupled MPPI-Based Multi-Arm Motion Planning", "comment": null, "summary": "Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.", "AI": {"tldr": "提出了一种名为MR-STORM的分布式运动规划算法，用于多机器人协同任务，该算法通过允许每个机器人独立规划并与其他机器人共享信息来克服现有方法的扩展性问题，并在实验中证明了其优越性。", "motivation": "现有的基于GPU的采样运动规划算法在控制多个机器人协同任务时扩展性差。因此，需要一种能有效处理多机器人协同的算法。", "method": "1. 修改STORM算法以处理动态障碍物。2. 允许每个机器人独立计算其运动规划前缀，并将其视为动态障碍物与其他机器人共享。3. 引入动态优先级机制。", "result": "MR-STORM在处理静态和动态障碍物时，相比现有的SOTA算法，表现出了明显的经验优势。", "conclusion": "MR-STORM是一种有效的分布式运动规划方法，能够克服现有方法的扩展性限制，在多机器人协同任务中具有实际应用价值。"}}
{"id": "2602.09983", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09983", "abs": "https://arxiv.org/abs/2602.09983", "authors": ["Calvin Yeung", "Ali Zakeri", "Zhuowen Zou", "Mohsen Imani"], "title": "Coupled Inference in Diffusion Models for Semantic Decomposition", "comment": "15 pages", "summary": "Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.", "AI": {"tldr": "本文提出了一种利用扩散模型进行语义分解的框架，将语义分解视为一个逆问题，并通过重建引导项来耦合扩散过程，以使分解的因子估计组合起来匹配原始的绑定向量。该方法还引入了一种新颖的迭代采样方案，并在合成语义分解任务上取得了优于共振网络（Resonator Networks）的性能。", "motivation": "受到共振网络与 Hopfield 网络相似性的启发，以及 Hopfield 网络与扩散模型之间最近的观察到的相似性，本文旨在开发一种新的语义分解方法。", "method": "该方法将语义分解构建为逆问题，并通过一个重建驱动的引导项来耦合扩散过程，该引导项鼓励因子估计的组合来匹配绑定向量。此外，还提出了一种新的迭代采样方案来提高模型性能。", "result": "该耦合推理框架在各种合成语义分解任务上优于共振网络。同时，论文表明基于注意力机制的共振网络是该框架的一个特例。", "conclusion": "本文提出了一种新颖的、基于扩散模型的耦合推理框架，用于语义分解，并在实验中证明了其有效性，同时揭示了其与共振网络的理论联系。"}}
{"id": "2602.09838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09838", "abs": "https://arxiv.org/abs/2602.09838", "authors": ["Yayun Zhang", "Guanyi Chen", "Fahime Same", "Saad Mahamood", "Tingting He"], "title": "How Do People Quantify Naturally: Evidence from Mandarin Picture Description", "comment": null, "summary": "Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the precision of quantification, while animate referents and modality selectively modulate strategy choice. This study demonstrates how quantification can be examined under unconstrained production conditions and provides a naturalistic dataset for further analyses of quantity expression in language production.", "AI": {"tldr": "本研究调查了汉语中在自然语境下的量化行为，发现物体数量、生命性以及表达方式（口语或书面）会系统性地影响说话者是否进行量化、量化的精确度以及所采用的量化策略。", "motivation": "尽管量化是日常语言的基本组成部分，但人们在自然语境下如何决定是否以及如何进行量化，这方面的知识仍然有限。", "method": "研究者使用了一个基于图片的场景描述任务，要求被试自由描述包含多个物体的场景，但没有明确要求他们进行计数或量化。研究检查了口语和书面两种表达方式下的量化行为，重点关注三个方面：是否进行量化、量化的精确度以及采用的量化策略。", "result": "研究结果显示，物体数量（numerosity）、生命性（animacy）和表达方式（modality）系统性地影响量化行为。具体来说，数量的增加会降低量化的可能性和精确度，而具有生命性的指代对象和不同的表达方式则会选择性地调整策略的选择。", "conclusion": "本研究展示了如何在不受限制的自然产出条件下研究量化，并提供了一个用于进一步分析语言产出中数量表达的自然数据集。研究证明了物体数量、生命性和表达方式对量化行为的关键作用。"}}
{"id": "2602.09949", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09949", "abs": "https://arxiv.org/abs/2602.09949", "authors": ["Franziska Krauß", "Matthias Ege", "Zoltan Lovasz", "Albrecht Bartz-Schmidt", "Igor Tsaur", "Oliver Sawodny", "Carina Veil"], "title": "Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework", "comment": null, "summary": "Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific \"vascular fingerprint\" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.", "AI": {"tldr": "本文提出了一种名为HAC（Hybrid Attention-Convolution）的新型模型，用于分割内窥镜图像中的膀胱血管。该模型结合了Transformer和CNN的优势，并采用了优化的训练数据和自监督预训练策略，有效解决了膀胱内血管分割中的挑战，提高了准确性和鲁棒性，有望用于临床导航。", "motivation": "膀胱癌复查需要追踪肿瘤位置，但膀胱缺乏稳定参照物。内窥镜图像中的血管可以作为患者特异性的“血管指纹”用于导航，但现有血管分割方法难以应对内窥镜数据固有的挑战，如标注稀疏、伪影、形变和粘膜皱褶干扰。", "method": "提出了一种混合注意力-卷积（HAC）架构，结合了Transformer捕捉全局血管拓扑和CNN进行残差细化以恢复细小血管细节。Transformer使用排除了短小分支的优化真值进行训练。为了解决数据稀缺问题，采用了一种基于物理的自监督预训练策略，使用临床相关的增强方法处理未标记数据。", "result": "在BlaVeS数据集上的评估结果显示，该方法取得了0.94的准确率，以及优于现有最先进方法的0.61的精确率和0.66的clDice。该方法能够有效抑制由粘膜皱褶引起的假阳性。", "conclusion": "HAC模型能够可靠地提供膀胱血管的结构稳定性，克服了内窥镜数据的领域特异性挑战，为临床导航提供了支持。"}}
{"id": "2602.09866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09866", "abs": "https://arxiv.org/abs/2602.09866", "authors": ["Johan Sofalas", "Dilushri Pavithra", "Nevidu Jayatilleke", "Ruvan Weerasinghe"], "title": "SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech", "comment": "19 pages, 6 figures, 8 tables, Accepted paper at the 22nd Workshop on Multiword Expressions (MWE 2026) @ EACL 2026", "summary": "Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.", "AI": {"tldr": "本文构建了一个包含2344个僧伽罗语习语及其跨语言翻译和文化背景注释的数据集，并分析了现有大型语言模型在处理这些习语时的不足，为低资源NLP和文化敏感性机器翻译提供了基准。", "motivation": "现有神经机器翻译系统在处理高资源语言的习语方面表现尚可，但在低资源语言（如僧伽罗语）方面面临数据不足的挑战。", "method": "构建了一个包含2344个僧伽罗语习语及其跨语言对应和文化注释的数据集。利用该数据集对习语的文化来源进行分类，识别跨语言对应，并开发了一个二分类器来区分两种类型的习语，准确率约为92%。此外，还评估了现有大型语言模型在该数据集上的表现。", "result": "现有大型语言模型在理解和翻译僧伽罗语习语方面存在显著不足，难以准确传达习语的字面意义。二分类器在区分两种习语类型方面达到了约92%的准确率。", "conclusion": "该研究发布了一个僧伽罗语习语数据集，为低资源自然语言处理和文化敏感性机器翻译领域的研究提供了重要基准，并揭示了当前大型语言模型在处理习语方面的局限性。"}}
{"id": "2602.09600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09600", "abs": "https://arxiv.org/abs/2602.09600", "authors": ["Yuxi Wang", "Wenqi Ouyang", "Tianyi Wei", "Yi Dong", "Zhiqi Shen", "Xingang Pan"], "title": "Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures", "comment": null, "summary": "Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.", "AI": {"tldr": "该论文提出了一种名为 Hand2World 的统一自回归框架，用于从单张场景图像生成第一人称视角的交互视频，其中手部动作与场景对象进行交互并影响世界动态。该框架通过引入基于 3D 手部网格的遮挡不变手部条件、像素级 Plücker 射线嵌入以及因果生成器来解决自由空间手势与训练数据分布偏移、单目视图中手部与相机运动的歧义以及任意长度视频生成等挑战。", "motivation": "为了在增强现实和具身人工智能中实现低延迟、几何一致且长期稳定的视觉生成，需要能够对用户输入（如手势）做出响应的第一人称交互世界模型。", "method": "提出 Hand2World，一个统一的自回归框架，包含：1. 基于投影 3D 手部网格的遮挡不变手部条件，用于处理自由空间手势与训练数据之间的分布偏移。2. 显式的相机几何注入，通过每像素 Plücker 射线嵌入来解耦手部和相机运动，稳定视角变化。3. 全自动单目标注流程和从双向扩散模型蒸馏得到的因果生成器，以实现任意长度的视频合成。", "result": "在三个第一人称交互基准测试中，Hand2World 在感知质量和三维一致性方面取得了显著的改进。该模型支持相机控制和长时序的交互式生成。", "conclusion": "Hand2World 框架成功地解决了从单张场景图像生成第一人称交互视频中的关键挑战，通过其创新的方法显著提升了生成视频的质量和稳定性，并支持灵活的交互式生成。"}}
{"id": "2510.21112", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21112", "abs": "https://arxiv.org/abs/2510.21112", "authors": ["Hezam Albagami", "Haitian Wang", "Xinyu Wang", "Muhammad Ibrahim", "Zainy M. Malakan", "Abdullah M. Alqamdi", "Mohammed H. Alghamdi", "Ajmal Mian"], "title": "LiDAR-based 3D Change Detection at City Scale", "comment": null, "summary": "High-definition 3D city maps enable city planning and change detection, which is essential for municipal compliance, map maintenance, and asset monitoring, including both built structures and urban greenery. Conventional Digital Surface Model (DSM) and image differencing are sensitive to vertical bias and viewpoint mismatch, while original point cloud or voxel models require large memory, assume perfect alignment, and degrade thin structures. We propose an uncertainty-aware, object-centric method for city-scale LiDAR-based change detection. Our method aligns data from different time periods using multi-resolution Normal Distributions Transform (NDT) and a point-to-plane Iterative Closest Point (ICP) method, normalizes elevation, and computes a per-point level of detection from registration covariance and surface roughness to calibrate change decisions. Geometry-based associations are refined by semantic and instance segmentation and optimized using class-constrained bipartite assignment with augmented dummies to handle split-merge cases. Tiled processing bounds memory and preserves narrow ground changes, while instance-level decisions integrate overlap, displacement, and volumetric differences under local detection gating. We perform experiments on a Subiaco (Western Australia) dataset captured in 2023 and again in 2025. Our method achieves 95.3% accuracy, 90.8% mF1, and 82.9% mIoU, improving over the strongest baseline, Triplet KPConv, by 0.3, 0.6, and 1.1 points, respectively. The datasets are available on IEEE DataPort (2023: https://ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset and 2025: https://ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset). The source code is available at https://github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.", "AI": {"tldr": "本文提出了一种面向对象、考虑不确定性的激光雷达城市变化检测方法，通过改进的配准和基于几何、语义信息的关联，有效解决了传统方法的局限性，并在真实数据集上取得了优于现有方法的性能。", "motivation": "传统城市高精度三维地图变化检测方法（如DSM和图像差分）对垂直偏差和视点不匹配敏感；点云或体素模型内存占用大、要求精确对齐且会损坏细小结构。因此，需要一种更鲁棒、高效的方法来检测城市规模的激光雷达数据变化。", "method": "该方法首先利用多分辨率NDT和点到平面ICP进行不同时间点数据的配准，然后进行高程归一化，并基于配准协方差和表面粗糙度计算逐点检测不确定性以校准变化判断。几何关联通过语义和实例分割进行细化，并结合类别约束二分图匹配（包含增广虚拟点）来处理分割合并情况。采用瓦片处理限制内存使用并保留地面变化，最后在实例层面整合重叠、位移和体积差异，并在局部检测门控下做出决策。", "result": "在Subiaco（西澳大利亚）2023年和2025年的数据集上进行实验，所提出的方法实现了95.3%的准确率、90.8%的mF1和82.9%的mIoU，在准确率、mF1和mIoU上分别比最强的基线Triplet KPConv提高了0.3、0.6和1.1个百分点。", "conclusion": "提出的不确定性感知、对象中心化的城市规模激光雷达变化检测方法，通过改进的配准、多模态信息融合以及对不确定性的建模，能够准确有效地检测城市环境中的变化，并且优于现有的最先进方法。"}}
{"id": "2602.09609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09609", "abs": "https://arxiv.org/abs/2602.09609", "authors": ["Jialun Liu", "Yukuo Ma", "Xiao Cao", "Tian Li", "Gonghu Shang", "Haibin Huang", "Chi Zhang", "Xuelong Li", "Cong Liu", "Junqi Liu", "Jiakui Hu", "Robby T. Tan", "Shiwen Zhang", "Liying Yang", "Xiaoyan Yang", "Qizhen Weng", "Xiangzhen Chang", "Yuanzhi Liang", "Yifan Xu", "Zhiyong Huang", "Zuoxin Li", "Xuelong Li"], "title": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing", "comment": null, "summary": "Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.", "AI": {"tldr": "本文提出了Tele-Omni，一个统一的多模态视频生成与编辑框架，能够处理文本、图像、参考视频等多种输入，并支持多种视频任务。", "motivation": "现有视频生成方法多为任务特定，主要依赖文本指令，难以处理多模态输入、上下文引用以及统一的视频生成和编辑场景。许多视频编辑方法依赖于精心设计的、针对单一操作的流水线，阻碍了可扩展性和组合性。", "method": "Tele-Omni利用预训练的多模态大语言模型解析异构指令并推断结构化生成或编辑意图，然后利用基于扩散的模型根据结构化信号进行高质量视频合成。提出任务感知的数据处理流程，将多模态输入统一为结构化指令格式，以支持跨异构视频任务的联合训练。", "result": "Tele-Omni能够支持文本到视频生成、图像到视频生成、首尾帧视频生成、上下文视频生成和上下文视频编辑等多种视频任务。实验结果表明，Tele-Omni在多项任务上表现具有竞争力。", "conclusion": "通过解耦指令解析与视频合成，并结合任务感知的数据设计，Tele-Omni实现了灵活的多模态控制，同时保持了强大的时间连贯性和视觉一致性。"}}
{"id": "2602.10032", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10032", "abs": "https://arxiv.org/abs/2602.10032", "authors": ["Tobias Ladner", "Yasser Shoukry", "Matthias Althoff"], "title": "Perception with Guarantees: Certified Pose Estimation via Reachability Analysis", "comment": null, "summary": "Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.", "AI": {"tldr": "提出了一种仅凭相机图像和已知目标几何形状进行 3D 认证姿态估计的方法，利用可达性分析和形式化神经网络验证来提供正式的安全保证。", "motivation": "在安全关键的网络物理系统中，需要可靠的姿态估计来保证代理的安全，但现有方法（如仅使用粗略估计或依赖不可信的外部服务）不足以进行正式的安全保证。", "method": "通过结合可达性分析和形式化神经网络验证，对相机图像和已知目标几何形状计算出的姿态进行形式化边界，从而实现认证姿态估计。", "result": "该方法在合成和真实世界实验中都能够高效且准确地进行代理姿态本地化，并提供形式化的安全保证。", "conclusion": "提出的认证姿态估计方法为安全关键的网络物理系统提供了一种可靠且可形式化验证的姿态估计解决方案，仅需相机图像和已知目标几何信息。"}}
{"id": "2602.09870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09870", "abs": "https://arxiv.org/abs/2602.09870", "authors": ["Chung-En Sun", "Ge Yan", "Zimo Wang", "Tsui-Wei Weng"], "title": "Steer2Edit: From Activation Steering to Component-Level Editing", "comment": null, "summary": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.", "AI": {"tldr": "本文提出了 Steer2Edit，一种无需训练即可将推理时激活干预转化为组件级权重编辑的框架，从而在不影响模型正向传播的情况下，更精细地控制模型行为，并取得了更好的属性-效用权衡。", "motivation": "现有的控制大语言模型行为的方法（如激活干预）通常会全局修改模型内部状态，忽略了模型行为是由一小部分异质组件控制的事实，导致在强控制下出现不利的属性-效用权衡。因此，需要一种更精细、无需训练的方法来解决这个问题。", "method": "Steer2Edit 将推理时的转向向量转化为组件级（注意力头和 MLP 神经元）的秩-1 权重编辑信号。它不统一地注入转向方向，而是有选择地重新分配行为影响，实现了可解释的权重编辑，同时保持了标准的正向传播并兼容优化的并行推理。", "result": "在安全性对齐、减少幻觉和提高推理效率方面，Steer2Edit 取得了更优的属性-效用权衡。在下游性能匹配的情况下，与现有方法相比，安全性平均提高了 17.2%，真实性提高了 9.8%，推理长度平均减少了 12.2%。", "conclusion": "Steer2Edit 提供了一个连接表征转向和权重编辑的原则性桥梁，将转向信号转化为可解释的、无需训练的参数更新，实现了更有效的模型行为控制，并取得了更好的属性-效用权衡。"}}
{"id": "2602.10021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10021", "abs": "https://arxiv.org/abs/2602.10021", "authors": ["Wenxuan Xie", "Yujia Wang", "Xin Tan", "Chaochao Lu", "Xia Hu", "Xuhong Wang"], "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference", "comment": null, "summary": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.", "AI": {"tldr": "本文提出了一种名为DRIFT的新型双模型架构，用于解决大型语言模型（LLMs）中知识集成和推理的挑战，通过动态压缩文档片段为隐含事实标记，从而有效扩展LLMs的上下文窗口和推理能力。", "motivation": "现有的大型语言模型在集成动态知识时面临挑战，包括上下文窗口限制、检索器噪声以及灾难性遗忘等问题。这促使研究人员寻求一种能够有效解耦知识提取和推理过程的方法。", "method": "DRIFT采用双模型架构，一个轻量级的知识模型负责动态压缩文档片段为查询相关的隐含事实标记（fact tokens），然后将这些密集表示投射到推理模型的嵌入空间，替代原始文本。这种方式旨在保持推理准确性。", "result": "实验结果表明，DRIFT在长上下文任务上显著提升了性能，并且优于同等规模模型的强大基线模型。", "conclusion": "DRIFT提供了一种可扩展且高效的范式，用于扩展LLMs的有效上下文窗口和推理能力，有效地解决了知识集成中的一些关键挑战。"}}
{"id": "2602.09637", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.09637", "abs": "https://arxiv.org/abs/2602.09637", "authors": ["Yueming Sun", "Long Yang", "Jianbo Jiao", "Zeyu Fu"], "title": "Towards Training-free Multimodal Hate Localisation with Large Language Models", "comment": null, "summary": "The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.", "AI": {"tldr": "提出了一种名为LELA的训练无关的大型语言模型（LLM）框架，用于对视频中的仇恨内容进行时序定位，通过分解视频模态并利用多阶段提示和跨模态推理来提高检测精度，并在基准测试中取得优于现有方法的性能。", "motivation": "现有视频仇恨检测方法依赖大量人工标注或缺乏时序精度，而在线视频中的仇恨内容对个人和社会的和谐构成严重威胁，因此需要一种无需训练且能进行时序定位的解决方案。", "method": "将视频分解为图像、语音、OCR、音乐和视频上下文五种模态，利用大型语言模型（LLM）和模态特定字幕，通过多阶段提示生成逐帧的仇恨分数，并引入组合匹配机制增强跨模态推理。", "result": "LELA在HateMM和MultiHateClip两个基准测试中，相比现有的所有训练无关基线方法，在仇恨视频定位方面取得了显著的性能提升。", "conclusion": "LELA是首个基于LLM的训练无关的仇恨视频定位框架，能够有效地进行时序定位，并在实验中证明了其优越性，为可扩展和可解释的仇恨视频定位奠定了基础。"}}
{"id": "2602.10116", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10116", "abs": "https://arxiv.org/abs/2602.10116", "authors": ["Hongchi Xia", "Xuan Li", "Zhaoshuo Li", "Qianli Ma", "Jiashu Xu", "Ming-Yu Liu", "Yin Cui", "Tsung-Yi Lin", "Wei-Chiu Ma", "Shenlong Wang", "Shuran Song", "Fangyin Wei"], "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI", "comment": "Project Page: https://nvlabs.github.io/sage", "summary": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.", "AI": {"tldr": "SAGE是一个代理框架，可以根据用户指定的具身任务自动生成逼真且物理有效的3D环境，用于训练AI代理。", "motivation": "现实世界的数据收集成本高昂且不安全，需要可扩展、逼真且可以直接用于模拟器的3D环境。现有的场景生成系统存在不足，可能产生瑕疵和物理上无效的场景。", "method": "SAGE结合了布局和对象组合的多个生成器，以及评估语义合理性、视觉真实性和物理稳定性的评估器。通过迭代推理和自适应工具选择，框架能够自我完善场景，直到满足用户意图和物理有效性。", "result": "SAGE生成的环境逼真、多样，可直接用于现代模拟器进行策略训练。纯粹基于SAGE数据训练的策略展现出明显的规模效应，并能泛化到未见过的对象和布局。", "conclusion": "SAGE框架能够大规模生成高质量的模拟环境，为具身AI的模拟驱动扩展提供了新的可能性，能够训练出具有良好泛化能力的AI策略。"}}
{"id": "2602.10042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10042", "abs": "https://arxiv.org/abs/2602.10042", "authors": ["Changjiang Jiang", "Xinkuan Sha", "Fengchang Yu", "Jingjing Liu", "Jian Liu", "Mingqi Fang", "Chenfeng Zhang", "Wei Lu"], "title": "Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection", "comment": "Accepted by ICASSP 2026", "summary": "Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.", "AI": {"tldr": "提出了一种名为 Fake-HR1 的混合推理模型，能够自适应地决定是否需要进行推理来检测合成图像，从而在保证检测性能的同时提高效率。", "motivation": "现有的 Chain-of-Thought（CoT）推理方法在检测合成图像时虽然有效，但过长的推理过程会消耗大量资源（如 token 和延迟），尤其是在处理明显伪造的图像时显得冗余。因此，需要一种能够根据任务特性自适应选择推理方式的方法。", "method": "设计了一个两阶段的训练框架：首先使用混合微调（HFT）进行冷启动初始化，然后利用在线强化学习和混合推理分组策略优化（HGRPO）来隐式学习何时选择合适的推理模式。", "result": "Fake-HR1 模型能够针对不同类型的查询自适应地执行推理，在推理能力和生成检测性能上均优于现有的 LLMs，并显著提高了响应效率。", "conclusion": "Fake-HR1 是首个能够自适应地根据生成检测任务的特性来决定是否需要推理的模型，在性能和效率上均取得了显著提升。"}}
{"id": "2602.09877", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09877", "abs": "https://arxiv.org/abs/2602.09877", "authors": ["Chenxu Wang", "Chaozhuo Li", "Songyang Liu", "Zejian Chen", "Jinyu Hou", "Ji Qi", "Rui Li", "Litian Zhang", "Qiwei Ye", "Zheng Liu", "Xu Chen", "Xi Zhang", "Philip S. Yu"], "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "comment": null, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "AI": {"tldr": "研究表明，完全隔离的、能够持续自我进化的多大型语言模型（LLM）代理系统是不可能同时保持安全性的。隔离的自我进化会导致系统偏离人类价值观，安全对齐会不可避免地退化。研究提出了缓解此问题的方向，并强调了外部监督或新安全机制的必要性。", "motivation": "大型语言模型（LLM）构建的多代理系统被认为是实现可扩展集体智能和自我进化的有前途的范式。然而，在实现持续自我改进的同时保持强大的安全对齐，即所谓的“自我进化三难困境”，是一个关键的挑战。研究旨在探索这种理想状态的可行性。", "method": "研究结合了理论分析和实证研究。首先，利用信息论框架将安全性形式化为与人类价值观分布的偏差程度。然后，理论上证明隔离的自我进化会产生统计盲点，导致系统安全性不可逆转的退化。最后，通过在开源的自我进化代理社区（Moltbook）以及两个封闭的自我进化系统中进行实验，来验证理论预测。", "result": "研究理论和实验均表明，一个满足持续自我进化、完全隔离和安全不变性的代理社会是不可能存在的。隔离的自我进化会引入统计盲点，导致系统安全对齐的不可逆退化。开放式代理社区和封闭式自我进化系统的实验结果都证实了安全侵蚀的现象。", "conclusion": "研究揭示了自我进化人工智能社会的根本性限制，即在完全隔离的条件下，持续的自我进化必然伴随着安全性的退化。这表明，当前从症状出发的安全补丁方法不足以应对内在的动态风险，必须考虑外部监督或设计新的安全保持机制。"}}
{"id": "2602.09914", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.09914", "abs": "https://arxiv.org/abs/2602.09914", "authors": ["Tilahun Yeshambel", "Moncef Garouani", "Josiane Mothe"], "title": "AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning", "comment": "7 pages, Submitted to resource track", "summary": "Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected from the web or synthesized by LLMs and then validated by native speakers. The instruction prompt-response dataset comprises 6,285 Amharic prompt-response pairs spanning multiple domains and instruction types, generated with several LLMs and refined through manual review and correction for grammaticality, relevance, fluency, and factual plausibility. We release both datasets with standardized splits and formats (CSV,JSON,JSONL) to enable reproducible work on Amharic retrieval, ranking, and generative modelling. These datasets also come with a methodology that can be generalized to other low-resource languages.", "AI": {"tldr": "本研究发布了两个新的阿姆哈拉语数据集，用于支持低资源语言的神经检索和指令遵循生成任务，并提供了一种可推广的方法。", "motivation": "低资源语言（如阿姆哈拉语）在神经检索和GPT风格生成模型方面缺乏高质量的监督数据。", "method": "构建了一个包含1,091个查询-正例-负例文档三元组的检索-排序数据集，用于对比学习和基准测试；创建了一个包含6,285个阿姆哈拉语指令-响应对的指令遵循数据集。数据来源多样，并通过人工验证、LLM辅助生成和修改等方式保证质量。", "result": "发布了两个高质量的阿姆哈拉语数据集（检索-排序和指令遵循），支持阿姆哈拉语在神经检索、排序和生成模型方面的研究。同时，提供了一种可推广到其他低资源语言的方法。", "conclusion": "本研究通过发布阿姆哈拉语数据集及其生成方法，极大地促进了低资源语言在神经检索和生成领域的进一步研究和发展。"}}
{"id": "2602.09638", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09638", "abs": "https://arxiv.org/abs/2602.09638", "authors": ["Hanqing Wang", "Mingyu Liu", "Xiaoyu Chen", "Chengwei MA", "Yiming Zhong", "Wenti Yin", "Yuhao Liu", "Zhiqing Cui", "Jiahao Yuan", "Lu Dai", "Zhiyuan Ma", "Hui Xiong"], "title": "VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model", "comment": null, "summary": "3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \\textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \\textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \\textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.", "AI": {"tldr": "本文提出了一种名为 VideoAfford 的新方法，用于在 3D 对象上识别可操作区域（affordances），利用包含 38K 交互式视频的新型数据集 VIDA，并融合了多模态大语言模型和潜在动作编码器来提升对动态交互和空间信息的理解能力。", "motivation": "现有 3D 可操作性识别方法主要依赖静态线索（如语言和图像），缺乏动态交互和因果关系信息，难以充分揭示对象的交互潜力。", "method": "收集并发布了包含 38K 视频的 VIDA 数据集，用于 3D 可操作性识别。提出 VideoAfford 模型，该模型结合了多模态大语言模型和 affordance 分割能力，并引入了潜在动作编码器来提取 HOI 视频中的动态交互先验，同时使用空间感知损失函数来增强 3D 空间知识。", "result": "VideoAfford 模型在实验中显著优于现有方法，并展现出强大的开放世界泛化能力和可操作性推理能力。", "conclusion": "本文提出的 VideoAfford 方法通过引入视频数据和融合多模态大语言模型，有效解决了 3D 可操作性识别中的动态交互信息不足的问题，并取得 SOTA 性能。数据集和代码将公开，以促进该领域的研究。"}}
{"id": "2602.09953", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09953", "abs": "https://arxiv.org/abs/2602.09953", "authors": ["Shuaiyi Nie", "Siyu Ding", "Wenyuan Zhang", "Linhao Yu", "Tianmeng Yang", "Yao Chen", "Tingwen Liu", "Weichong Yin", "Yu Sun", "Hua Wu"], "title": "ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning", "comment": "Work in process", "summary": "Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.", "AI": {"tldr": "本文提出了ATTNPO，一个低开销的、利用模型注意力机制进行步进级别信用分配的强化学习框架，旨在解决大型推理模型过时而产生冗余推理的问题，并显著缩短推理长度和提高性能。", "motivation": "现有的RLVR方法在处理复杂推理任务时存在“过时”问题，即生成冗余推理而无性能提升。现有的长度惩罚策略无效且可能损害准确性，而过程监督方法成本高昂且信用分配不准确。", "method": "提出ATTNPO框架，通过识别特殊的注意力头来判断推理步骤的重要性。利用这些注意力头的得分，采取两种子策略：抑制冗余步骤的倾向，同时减少对关键步骤的惩罚，以在缩短推理长度的同时保持准确性。", "result": "在9个基准测试中，ATTNPO显著减少了推理长度，并显著提高了模型性能。", "conclusion": "ATTNPO是一种有效的低开销过程监督RL框架，通过利用模型的注意力机制，能够有效解决大型推理模型的过时问题，实现推理长度的缩减和性能的提升。"}}
{"id": "2602.09648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09648", "abs": "https://arxiv.org/abs/2602.09648", "authors": ["Siyu Chen", "Ting Han", "Haoling Huang", "Chaolei Wang", "Chengzheng Fu", "Duxin Zhu", "Guorong Cai", "Jinhe Su"], "title": "Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation", "comment": null, "summary": "Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.", "AI": {"tldr": "本文提出了一种名为 Time2General 的视频语义分割框架，用于解决跨领域泛化和时间一致性问题，通过引入时空记忆解码器和掩码时间一致性损失，在保持较高帧率的同时显著提高了分割精度和时间稳定性。", "motivation": "现有视频语义分割方法在跨领域泛化时，由于域偏移和时间采样偏移，会导致帧间预测闪烁，即使在标签稳定的区域也是如此。这影响了基于对应关系传播和固定步长的时间聚合。", "method": "Time2General 框架基于稳定性查询，引入了时空记忆解码器来聚合多帧上下文信息，生成片段级时空记忆，并解码出无显式对应关系传播的、时间一致的每帧掩码。此外，还提出了掩码时间一致性损失来规范化不同步长下的时间预测差异，并随机化训练步长以应对各种时间间隔。", "result": "在多个驾驶基准测试中，Time2General 相较于现有的 DGSS 和 VSS 基线方法，在跨领域准确性和时间稳定性方面取得了显著的改进，同时运行速度可达 18 FPS。", "conclusion": "Time2General 框架能够有效解决域偏移和时间采样偏移带来的视频语义分割的挑战，实现跨领域泛化和时间一致性，并且在保持高性能的同时具备较高的推理速度。"}}
{"id": "2602.09961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09961", "abs": "https://arxiv.org/abs/2602.09961", "authors": ["Trung Tien Cao", "Lam Minh Thai", "Nghia Hieu Nguyen", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese", "comment": null, "summary": "Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that jointly training option decision and explanation generation leads to significant improvements in multiple-choice accuracy.", "AI": {"tldr": "该研究提出了一个用于越南语选择题阅读理解（MCRC）并生成解释的新数据集和新模型ViMultiChoice，该模型在两个基准测试上均取得了SotA性能，并证明联合训练答案选择和解释生成能提高准确性。", "motivation": "现有的多项选择阅读理解模型缺乏解释其选择的能力，而需要能够解释其决策过程的模型。此外，针对越南语的此类模型和数据集也相对匮乏。", "method": "研究人员构建了一个新的越南语MCRC数据集（ViMMRC 2.0及新增数据集），并提出了一个名为ViMultiChoice的新方法，该方法能够同时预测正确答案并生成相应的解释。", "result": "ViMultiChoice在ViMMRC 2.0基准测试和新数据集上均取得了SotA性能，优于现有的MCRC基线模型。联合训练答案选择和解释生成显著提高了多项选择的准确性。", "conclusion": "研究表明，ViMultiChoice是一种有效的越南语阅读理解模型，能够同时生成答案和解释。联合训练方式对于提升模型性能至关重要，为开发更具可解释性的MCRC模型提供了新方向。"}}
{"id": "2602.09686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09686", "abs": "https://arxiv.org/abs/2602.09686", "authors": ["Boya Wang", "Ruizhe Li", "Chao Chen", "Xin Chen"], "title": "Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI", "comment": null, "summary": "Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver", "AI": {"tldr": "本研究提出了一个多任务深度学习框架，用于在多参数 MRI 上进行肝脏分割（LiSeg）和肝纤维化分期（LiFS），特别处理了标签数据不足和多模态图像的问题。", "motivation": "肝纤维化是临床上的一个严峻挑战，需要精确的肝脏分割和疾病分期，这促使了该研究的进行。", "method": "该研究采用了一个半监督学习模型进行肝脏分割（LiSeg），该模型整合了图像分割和配准，以利用标记和未标记数据。对于肝纤维化分期（LiFS），采用了基于块（patch-based）的方法，并基于分类输出可视化肝纤维化分期。模型能够处理多模态成像数据、有限的标签和域偏移。", "result": "该方法在包括 in-distribution (ID) 和 out-of-distribution (OOD) 病例的独立测试集上进行了测试，使用了三通道和七通道的 MRI 数据。具体性能指标未在摘要中详细说明，但表明了其处理不同类型数据的能力。", "conclusion": "提出的多任务深度学习框架能够有效地解决肝脏分割和肝纤维化分期中的多模态、有限标签和域偏移等挑战，并已在实际测试中得到验证。"}}
{"id": "2602.10095", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10095", "abs": "https://arxiv.org/abs/2602.10095", "authors": ["Xingjian Bai", "Guande He", "Zhengqi Li", "Eli Shechtman", "Xun Huang", "Zongze Wu"], "title": "Causality in Video Diffusers is Separable from Denoising", "comment": null, "summary": "Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.", "AI": {"tldr": "提出了一种名为SCD（Separable Causal Diffusion）的新架构，通过将因果推理和去噪过程解耦，显著提高了因果扩散模型的生成效率和速度，同时保持或超越了现有模型的生成质量。", "motivation": "现有的因果扩散模型将时间推理与迭代去噪过程耦合在一起，导致计算冗余和效率低下。研究者希望找到一种方法来分离这两个过程，从而提高模型的性能。", "method": "作者通过分析现有模型发现，早期层在去噪过程中产生冗余特征，而深层主要进行帧内渲染。基于这些观察，提出了SCD架构，将因果 Transformer 编码器用于一次性的逐帧因果推理，并将轻量级的扩散解码器用于多步逐帧渲染。", "result": "在合成和真实数据集上的实验表明，SCD在预训练和后训练任务上都显著提高了吞吐量和单帧延迟，并且生成质量与强大的因果扩散基线模型相当或更优。", "conclusion": "因果推理可以与多步去噪过程分离，通过将因果推理和逐帧渲染解耦，SCD架构能够更高效地生成高质量的因果序列数据。"}}
{"id": "2602.09662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09662", "abs": "https://arxiv.org/abs/2602.09662", "authors": ["Deyang Jiang", "Jing Huang", "Xuanle Zhao", "Lei Chen", "Liming Zheng", "Fanfan Liu", "Haibo Qiu", "Peng Shi", "Zhixiong Zeng"], "title": "TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution", "comment": "14 pages, 7 figures", "summary": "Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\\emph{i.e.}, trajectory difficulty) and breadth (\\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.", "AI": {"tldr": "本文提出TreeCUA方法，通过将GUI自动化探索组织成树状结构，提高了GUI规划的效率和可扩展性，并引入了多智能体协作、自适应探索算法、世界知识引导和全局记忆回溯等技术，以及TreeCUA-DPO方法来进一步提升规划能力。", "motivation": "现有的GUI自动化方法主要关注GUI的识别（grounding），而忽略了更关键的GUI规划（planning）的扩展性问题，后者需要更复杂的数据收集。实际应用中，CUAs在应用/桌面/网页中的探索路径呈现树状结构，早期功能入口被频繁访问，因此将大规模轨迹组织成树状结构可以降低数据成本，提高GUI规划的数据扩展性。", "method": "提出TreeCUA，一个用于GUI规划的多智能体协作框架。该框架通过环境探索、动作验证、轨迹总结和质量评估来生成高质量、可扩展的GUI轨迹。关键技术包括：1. 树状拓扑结构存储和重放重复探索节点，提高效率。2. 自适应探索算法，平衡探索深度和广度。3. 世界知识引导和全局记忆回溯，避免低质量生成。4. TreeCUA-DPO方法，利用丰富的树节点信息和相邻轨迹的分支信息，改进GUI规划能力。", "result": "TreeCUA和TreeCUA-DPO在实验中展现出显著的性能提升。跨领域（OOD）研究进一步证明了其强大的泛化能力。", "conclusion": "TreeCUA通过引入树状结构和多项创新技术，有效地解决了GUI规划的数据扩展性问题，显著提高了GUI自动化性能，并表现出良好的泛化能力。TreeCUA-DPO在此基础上进一步增强了GUI规划能力。"}}
{"id": "2602.09992", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09992", "abs": "https://arxiv.org/abs/2602.09992", "authors": ["Xiulin Yang", "Arianna Bisazza", "Nathan Schneider", "Ethan Gotlieb Wilcox"], "title": "A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models", "comment": null, "summary": "How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \\poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \\poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.", "AI": {"tldr": "研究人员使用一个名为 posHbench 的训练和评估套件，通过训练 Transformer 模型来测试“刺激贫乏假说”（PoSH）的有效性。结果表明，即使输入有限，神经模型也能进行一定程度的语法泛化，但数据效率和泛化能力不如儿童。引入的认知归纳偏差可以提高整体语法能力，但并未提升在 posHbench 上的表现。", "motivation": "该研究的动机是检验“刺激贫乏假说”（PoSH），该假说认为儿童获得的语言输入不足以解释其习得的复杂语法能力，因此需要天生的语言结构。研究者希望通过使用不包含语言特异性约束的神经语言模型来计算地验证这一有争议的说法。", "method": "研究者开发了一个名为 posHbench 的训练和评估套件，用于测试关于疑问句形成、运动岛屿等英语现象。他们使用 Transformer 模型，在 1000 万到 5000 万字的、在发展上可行的文本上进行训练，并对比了引入三种认知归纳偏差后的模型表现。", "result": "在没有直接正向证据的情况下，神经模型在 posHbench 测试的所有现象上都显示出了一定程度的泛化能力。然而，与儿童相比，神经模型的数据效率较低，泛化能力也较弱。引入的认知归纳偏差提高了模型的整体语法能力，但并未改善其在 posHbench 上的性能。", "conclusion": "研究结果挑战了天生语法是唯一能够实现语言泛化途径的观点，但同时也表明，要达到人类儿童般的数据效率，需要超出当前测试范围的归纳偏差。"}}
{"id": "2602.10081", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10081", "abs": "https://arxiv.org/abs/2602.10081", "authors": ["Xuehang Guo", "Zhiyong Lu", "Tom Hope", "Qingyun Wang"], "title": "Anagent For Enhancing Scientific Table & Figure Analysis", "comment": null, "summary": "In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \\& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \\& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\\uparrow 13.43\\%$ in training-free settings and $\\uparrow 42.12\\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \\& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.", "AI": {"tldr": "本研究提出了一个名为AnaBench的大规模科学表格和图形分析基准，并开发了一个名为Anagent的多智能体框架，以解决现有AI系统在理解和分析复杂科学数据方面的不足。Anagent通过Planner、Expert、Solver和Critic四个智能体协同工作，并在监督微调和强化学习的训练策略下，显著提升了科学表格和图形的分析能力。", "motivation": "当前AI系统在准确解读科学研究中的多模态知识、整合不同来源的证据以及进行基于领域知识的推断方面存在困难，特别是在处理科学表格和图形时，其复杂性、异构结构和长上下文需求构成了重大挑战。", "method": "研究引入了一个包含63,178个实例的大规模基准AnaBench，用于量化科学表格和图形分析的挑战。同时，提出了一个名为Anagent的多智能体框架，包含Planner（任务分解）、Expert（工具执行检索信息）、Solver（信息合成生成分析）和Critic（基于五维度评估进行迭代优化）四个专门的智能体。训练策略包括模块化监督微调和专门的强化学习。", "result": "在170个子领域进行的综合评估显示，Anagent在无训练情况下相比基线模型提升了最高13.43%，在经过微调后提升了最高42.12%。研究还表明，面向任务的推理和上下文感知的问题解决能力对于高质量的科学表格和图形分析至关重要。", "conclusion": "Anagent框架和AnaBench基准能够有效解决科学表格和图形分析中的挑战，并显著提升AI在此类任务上的性能。研究强调了专门的智能体设计、协同工作以及有效的训练策略对于实现高水平科学数据分析的重要性。"}}
{"id": "2602.10104", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10104", "abs": "https://arxiv.org/abs/2602.10104", "authors": ["Yuxin Jiang", "Yuchao Gu", "Ivor W. Tsang", "Mike Zheng Shou"], "title": "Olaf-World: Orienting Latent Actions for Video World Modeling", "comment": "Project page: https://showlab.github.io/Olaf-World/ Code: https://github.com/showlab/Olaf-World", "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.", "AI": {"tldr": "提出Seq$Δ$-REPA目标和Olaf-World管线，通过对齐视频片段间的时序特征差异来学习更具结构化的潜在动作空间，显著提升了无监督动作学习的跨上下文迁移能力和数据效率。", "motivation": "现有无监督潜在动作学习方法在跨上下文迁移时效果不佳，因为学习到的潜在动作会与场景特定线索纠缠，缺乏共享坐标系，且标准目标函数仅在单个视频片段内操作，无法对齐跨上下文的动作语义。", "method": "提出Seq$Δ$-REPA（sequence-level control-effect alignment objective），一个序列级别的控制-效果对齐目标，将集成潜在动作与冻结的自监督视频编码器提取的时序特征差异关联起来。在此基础上，构建Olaf-World管线，从大规模被动视频中预训练动作条件视频世界模型。", "result": "Seq$Δ$-REPA学习到的潜在动作空间结构更优，在零样本动作迁移方面表现更强，并且在新控制接口上的适应性更具数据效率，优于现有最先进的方法。", "conclusion": "通过引入Seq$Δ$-REPA和Olaf-World，研究成功解决了无监督动作学习的跨上下文迁移问题，为从无标签视频中学习可控的世界模型提供了一种有效的方法。"}}
{"id": "2602.10003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10003", "abs": "https://arxiv.org/abs/2602.10003", "authors": ["Khoa Anh Nguyen", "Long Minh Hoang", "Nghia Hieu Nguyen", "Luan Thanh Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition", "comment": null, "summary": "Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\\textbf{Vi}etnamese \\textbf{Speech} Trans\\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.", "AI": {"tldr": "提出了一种名为ViSpeechFormer的基于音素的越南语自动语音识别（ASR）框架，利用了越南语高音素-字母透明度的特点，并在实验中取得了优异的性能。", "motivation": "现有越南语ASR方法未能充分利用越南语高音素-字母透明度的特点，作者希望提出一种显式建模音素表示的方法来改进ASR性能。", "method": "提出了一种名为ViSpeechFormer的基于音素的Transformer模型，用于越南语ASR。该模型显式地利用音素表示。", "result": "ViSpeechFormer在两个公开的越南语ASR数据集上取得了强劲的性能，对未登录词的泛化能力更好，且不易受训练偏差的影响。", "conclusion": "基于音素的ASR范式对于具有音素书写系统的语言（如越南语）是有效的，并有望推广到其他类似语言。"}}
{"id": "2602.10023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10023", "abs": "https://arxiv.org/abs/2602.10023", "authors": ["Delvin Ce Zhang", "Suhan Cui", "Zhelin Chu", "Xianren Zhang", "Dongwon Lee"], "title": "MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval", "comment": "Accepted to EACL-26", "summary": "Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model.", "AI": {"tldr": "提出了一种新的模型，用于联合进行证据检索、多模态声明验证和解释生成，并构建了一个新的科学领域数据集AIChartClaim。", "motivation": "现有的声明验证方法要么只关注文本证据，要么忽略可解释性，导致验证不准确且缺乏说服力。多模态推理（文本+图像）和透明的文本解释对于声明验证至关重要。", "method": "构建了一个两层多模态图用于证据检索，设计了图像到文本和文本到图像的推理；提出了token级和证据级的融合机制用于多模态声明验证；引入了多模态Fusion-in-Decoder用于解释生成。此外，创建了一个新的科学领域数据集AIChartClaim。", "result": "实验证明了所提模型的有效性。", "conclusion": "所提出的模型能够有效地进行多模态声明验证，并生成可解释的文本解释，且新数据集AIChartClaim填补了科学领域声明验证的空白。"}}
{"id": "2602.09713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09713", "abs": "https://arxiv.org/abs/2602.09713", "authors": ["Ruisi Zhao", "Haoren Zheng", "Zongxin Yang", "Hehe Fan", "Yi Yang"], "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models", "comment": "Accepted by ICLR 2026", "summary": "Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.", "AI": {"tldr": "Stroke3D是一个创新的框架，能够根据用户绘制的2D笔触和文本提示直接生成可动画的3D模型（带骨骼和纹理）。它通过两阶段流程实现：首先生成骨骼，然后合成带纹理的网格。", "motivation": "现有3D生成方法难以生成可动画几何体，而骨骼绑定技术在骨架创建方面缺乏精细的结构控制。本文旨在解决这些局限性，提供一种更直观的创建可动画3D内容的工作流程。", "method": "该框架采用两阶段流程。第一阶段：可控骨骼生成，使用Skeletal Graph VAE (Sk-VAE)和Skeletal Graph DiT (Sk-DiT)，结合文本提示和2D笔触来生成3D骨骼。第二阶段：增强网格合成，使用TextuRig数据集和SKA-DPO（一种基于骨骼-网格对齐得分的偏好优化策略）来合成带纹理的网格。", "result": "Stroke3D能够生成合理的骨骼和高质量的带纹理网格。实验证明该框架能有效生成符合用户输入的3D可动画模型。", "conclusion": "Stroke3D是首个能根据用户绘制的2D笔触生成带骨骼的3D网格的方法，它提供了一种更直观、结构可控的工作流程，用于创建可直接进行动画制作的3D内容。"}}
{"id": "2602.09730", "categories": ["cs.CV", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.09730", "abs": "https://arxiv.org/abs/2602.09730", "authors": ["Laura Paul", "Holger Rauhut", "Martin Burger", "Samira Kabri", "Tim Roith"], "title": "Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings", "comment": null, "summary": "Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.", "AI": {"tldr": "提出一种结合深度生成模型和Mumford-Shah变分泛函的方法，将艺术品图像的裂缝检测视为逆问题，以实现精确的裂缝定位。", "motivation": "自动检测艺术品（特别是数字化绘画）中的裂缝对于评估退化和指导修复至关重要，但由于场景复杂以及裂缝与笔触等艺术特征的视觉相似性而具有挑战性。", "method": "将裂缝检测建模为一个逆问题，将观察到的图像分解为无裂缝的绘画和裂缝成分。使用深度生成模型作为艺术品的先验知识，并结合Mumford-Shah型变分泛函和裂缝先验来捕捉裂缝结构。通过联合优化得到像素级的裂缝定位图。", "result": "该方法能够通过将观察到的图像分解为无裂缝的绘画和裂缝成分来生成像素级的裂缝定位图。", "conclusion": "该混合方法通过将裂缝检测视为一个逆问题，并利用深度生成模型和Mumford-Shah型变分泛函，有效地实现了艺术品图像中裂缝的精确检测。"}}
{"id": "2602.10017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10017", "abs": "https://arxiv.org/abs/2602.10017", "authors": ["Homaira Huda Shomee", "Rochana Chaturvedi", "Yangxinyu Xie", "Tanwi Mallick"], "title": "SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.", "AI": {"tldr": "本研究提出了一个多维度、无需参考的评估框架，用于评估大型语言模型（LLMs）在特定领域问答中的输出质量，强调了单一指标不足以衡量答案质量，并需要结构化的多指标评估。研究还构建了一个包含1412个领域特定问答对的数据集，并进行了人工评估。", "motivation": "现有的评估框架主要依赖表面相似性、事实一致性或语义相关性，无法充分评估LLM在自然灾害响应和基础设施规划等高风险领域中，答案是否包含决策所需的精细化信息。", "method": "提出一个多维度、无需参考的评估框架，包含四个维度：特异性、对释义和语义扰动的鲁棒性、答案相关性和上下文利用。构建了一个包含1412个领域特定问答对的数据集，并进行人工评估。", "result": "结果表明，没有单一指标能充分衡量答案质量，并且在将LLMs应用于高风险场景时，需要结构化的多指标评估框架。人工评估突显了开放式、领域特定评估的内在主观性。", "conclusion": "为了在高风险、特定领域应用中部署LLMs，需要一个结构化的、多指标的评估框架，因为单一评估指标无法完全捕捉答案的质量。"}}
{"id": "2602.09740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09740", "abs": "https://arxiv.org/abs/2602.09740", "authors": ["Sandeep Gupta", "Roberto Passerone"], "title": "Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors", "comment": "Submitted to IEEE Transactions on Intelligent Vehicles", "summary": "This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.", "AI": {"tldr": "本文研究了网联自动驾驶汽车（CAVs）视觉系统的鲁棒性，分析了其关键组成部分和参考架构，并详细阐述了针对该架构的攻击向量及其对CIA三元组的影响，为制定安全措施提供依据。", "motivation": "实现五级自动驾驶能力的关键在于CAVs视觉系统的鲁棒性，以确保安全可靠的导航，而现有研究缺乏对CAVs视觉系统攻击面的全面分析。", "method": "首先，通过分析CAVs导航的关键传感器和视觉组件，推导出CAVs视觉系统（CAVVS）的参考架构。然后，针对该架构识别出的攻击面，详细阐述了攻击向量，并评估了它们对保密性、完整性和可用性（CIA）的影响。", "result": "研究全面分析了CAVs视觉系统的攻击向量动态，并评估了这些攻击对CIA三元组的潜在影响。", "conclusion": "深入理解CAVs视觉系统的攻击向量对于制定能够维护CIA三元组原则的鲁棒安全措施至关重要。"}}
{"id": "2602.09764", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09764", "abs": "https://arxiv.org/abs/2602.09764", "authors": ["Kawtar Zaher", "Ilyass Moummad", "Olivier Buisson", "Alexis Joly"], "title": "Self-Supervised Learning as Discrete Communication", "comment": null, "summary": "Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.", "AI": {"tldr": "本研究提出了一种将自监督学习（SSL）视为离散通信过程的方法，通过在教师和学生网络之间传递二元信息来学习结构化视觉表示。", "motivation": "现有的SSL方法主要通过对齐连续特征来学习表示，缺乏对表示维度信息结构的控制。研究者希望开发一种能够学习更具结构化和可控性的视觉表示的方法。", "method": "1. 将SSL建模为教师-学生网络之间的离散通信过程，通过固定容量的二元信道传递语义信息。2. 学生网络预测教师网络生成的二元消息（多标签二元预测），而不是对齐连续特征。3. 使用逐元素二元交叉熵损失来强制离散一致性。4. 引入编码率正则化项来鼓励有效利用信道容量，促进结构化表示。5. 定期重新初始化投影头以增强效果。6. 分析学习到的二元码，证明其是紧凑且信息丰富的离散语言。", "result": "在图像分类、检索和密集视觉预测任务上，相较于连续对齐基线方法，本方法取得了持续的性能提升。在领域迁移的自监督适应方面也表现出优势。学习到的二元码显示出跨类可复用的语义因子。", "conclusion": "通过将SSL构建为离散通信过程，可以学习到结构化、紧凑且信息丰富的视觉表示，这些表示在多种下游任务和领域适应中表现出优越性能。"}}
{"id": "2602.09736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09736", "abs": "https://arxiv.org/abs/2602.09736", "authors": ["Shaoyang Xie", "Xiaofeng Cong", "Baosheng Yu", "Zhipeng Gui", "Jie Gui", "Yuan Yan Tang", "James Tin-Yau Kwok"], "title": "Toward Fine-Grained Facial Control in 3D Talking Head Generation", "comment": null, "summary": "Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.", "AI": {"tldr": "本文提出了一种名为FG-3DGS的新框架，利用频率感知解耦策略来改进基于3D高斯泼溅的音频驱动的3D数字人脸生成，解决了唇同步不准确和面部抖动问题，实现了更高保真度和时间一致性的生成。", "motivation": "现有3D高斯泼溅方法在生成高保真度的3D数字人脸方面表现出色，但在精细控制面部动作方面存在挑战，特别是唇同步不准确和面部抖动导致“恐怖谷”效应。", "method": "FG-3DGS框架采用频率感知解耦策略，将面部区域根据运动特性分开建模。低频区域（脸颊、鼻子、额头）使用标准MLP，高频区域（眼睛、嘴巴）使用专门网络并结合面部区域掩码。预测的高斯增量应用于静态高斯，通过光栅化器渲染。此外，引入了基于预训练模型的高频精炼后渲染对齐机制，以增强逐帧生成和唇同步精度。", "result": "FG-3DGS在唇同步准确性和面部细节还原方面优于现有最先进的方法，能够生成高保真度、唇部同步准确的3D数字人脸视频。", "conclusion": "FG-3DGS通过频率感知解耦和高频精炼后对齐机制，成功解决了3D高斯泼溅在音频驱动的3D数字人脸生成中的精度和抖动问题，实现了更高质量的生成效果。"}}
{"id": "2602.10092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10092", "abs": "https://arxiv.org/abs/2602.10092", "authors": ["Mohamed Afane", "Kayla Laufer", "Wenqi Wei", "Ying Mao", "Junaid Farooq", "Ying Wang", "Juntao Chen"], "title": "Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing", "comment": "18 pages", "summary": "Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.", "AI": {"tldr": "该研究提出了Quantum-Audit基准，用于评估大型语言模型（LLM）在理解量子计算概念方面的能力，并对26个模型进行了评估。结果显示，一些顶级模型表现优于人类专家，但在专家编写的问题、高级主题和识别错误前提方面仍存在挑战。", "motivation": "现有基准主要关注量子代码生成和电路设计，但缺乏对LLM理解量子计算概念能力的系统性评估。研究旨在弥补这一空白，以了解LLM在量子计算教育和研究中的实际应用潜力。", "method": "构建了一个包含2700个问题的Quantum-Audit基准，这些问题涵盖了量子计算的核心主题，包括1000个专家编写的问题，1000个LLM生成并经专家验证的问题，以及700个包含开放式问题和带错误前提的问题。评估了26个LLM模型。", "result": "顶级模型（如Claude Opus 4.5）达到了84%的准确率，超越了人类专家的平均水平（74%）。然而，模型在处理专家编写的问题时准确率平均下降12个百分点。在安全等高级主题上，准确率进一步下降至73%。模型在识别错误前提方面表现不佳，准确率低于66%。", "conclusion": "Quantum-Audit基准揭示了LLM在理解量子计算概念方面的潜力和局限性。尽管一些模型已取得显著进展，但在处理人类专家细致的表述、应对高级复杂概念以及批判性地识别和纠正错误信息方面，仍有待改进。"}}
{"id": "2602.09816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09816", "abs": "https://arxiv.org/abs/2602.09816", "authors": ["Hojun Song", "Heejung Choi", "Aro Kim", "Chae-yeong Song", "Gahyeon Kim", "Soo Ye Kim", "Jaehyup Lee", "Sang-hyo Park"], "title": "CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video", "comment": "Preprint. Under review", "summary": "High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.", "AI": {"tldr": "本文提出了一种名为CompSplat的压缩感知训练框架，用于从包含压缩伪影和复杂相机轨迹的真实视频中合成新视图，显著提高了渲染质量和姿态准确性。", "motivation": "真实世界的视频数据通常包含长序列、不规则的相机轨迹和未知的姿态，同时又受到有损压缩的影响，导致重建中的姿态漂移、特征不对齐和几何失真。现有方法未能充分解决这些问题，尤其是在处理复杂的压缩模式和长序列视频时。", "method": "CompSplat框架通过显式建模帧压缩特性来减轻帧间不一致性和累积的几何误差。它采用了压缩感知的帧加权和自适应剪枝策略，以增强在重度压缩下的鲁棒性和几何一致性。", "result": "在Tanks and Temples、Free和Hike等具有挑战性的数据集上进行的实验表明，CompSplat在严峻的压缩条件下，其渲染质量和姿态准确性均显著优于当前最先进的新视图合成方法。", "conclusion": "CompSplat是一种有效的压缩感知训练框架，能够有效地解决真实世界视频中因压缩和复杂相机运动引起的新视图合成问题，在提高渲染质量和几何一致性方面取得了最先进的性能。"}}
{"id": "2602.09775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09775", "abs": "https://arxiv.org/abs/2602.09775", "authors": ["Abhipsa Basu", "Yugam Bahl", "Kirti Bhagat", "Preethi Seshadri", "R. Venkatesh Babu", "Danish Pruthi"], "title": "Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets", "comment": "41 pages, 20 figures", "summary": "Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\\%$ of samples, while South American and African countries are severely under-represented with only $1.8\\%$ and $3.8\\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($ρ= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.", "AI": {"tldr": "研究人员通过分析文本到图像模型训练数据中图像的地理来源，发现美国、英国和加拿大占据了绝大多数样本，而南美洲和非洲地区则严重代表性不足。这种代表性与国家的GDP高度相关，并且即使在非英语数据子集中，也存在语言地域偏见。此外，高代表性并不意味着视觉或语义多样性的增加。在实际生成图像方面，基于Re-LAION数据集训练的Stable Diffusion模型生成的图像在地理覆盖范围上远不如真实世界的图像。", "motivation": "文本到图像模型在生成地理代表性图像方面的失败，引发了对其训练数据地理来源的担忧，因此研究人员希望了解这些训练样本来自世界的哪些地区。", "method": "研究人员利用大型语言模型（LLMs）从图像标题中提取地理位置信息，将图像-标题对映射到国家。他们分析了三个常用数据集（Re-LAION、DataComp1B 和 Conceptual Captions）中的英语标题，并研究了20个常见实体。此外，他们还分析了Re-LAION数据集中四种语言的非英语子集，并对Stable Diffusion v1.3生成的国家特定图像进行了分析。", "result": "美国、英国和加拿大占总样本的48.0%，而南美洲和非洲国家仅占1.8%和3.8%。国家GDP与其在数据中的代表性之间存在强烈的正相关（ρ=0.82）。在非英语数据子集中，代表性也严重偏向于这些语言的主要使用国家。高代表性并不一定带来更大的视觉或语义多样性。Stable Diffusion生成的国家特定图像虽然逼真，但在地理覆盖范围上远不如真实世界的图像。", "conclusion": "当前文本到图像模型的训练数据存在严重的地理偏见，主要集中在发达国家，而发展中国家则代表性不足。这种偏见影响了模型的生成能力，导致生成的图像在地理覆盖范围上存在局限性。提高数据集的地理多样性对于构建更具代表性和鲁棒性的文本到图像模型至关重要。"}}
{"id": "2602.09839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09839", "abs": "https://arxiv.org/abs/2602.09839", "authors": ["Yijie Lin", "Guofeng Ding", "Haochen Zhou", "Haobin Li", "Mouxing Yang", "Xi Peng"], "title": "ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge", "comment": null, "summary": "Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.", "AI": {"tldr": "本文提出了ARK基准，用于评估多模态检索在专业知识和复杂推理方面的能力，并通过评估现有检索器发现细粒度视觉和空间推理是主要瓶颈。", "motivation": "现有的多模态检索基准主要关注日常图像的语义匹配，缺乏对专业知识和复杂推理的诊断能力。", "method": "引入ARK基准，从知识域（5个域，17个子类型）和推理技能（6个类别）两个维度评估多模态检索。ARK支持单模态和多模态查询/候选，涵盖16种异构视觉数据类型，并使用有针对性的难负例来避免捷径匹配。", "result": "在ARK基准上评估了23个检索器，发现知识密集型和推理密集型检索之间存在显著差距。细粒度视觉和空间推理是持续存在的瓶颈。重排序和重写等简单增强方法能带来一致的改进，但仍有很大的提升空间。", "conclusion": "ARK基准能够有效评估多模态检索在专业知识和复杂推理方面的能力，并揭示了现有检索器的不足之处，为未来研究提供了方向。"}}
{"id": "2602.09809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09809", "abs": "https://arxiv.org/abs/2602.09809", "authors": ["Tong Zhang", "Honglin Lin", "Zhou Liu", "Chong Chen", "Wentao Zhang"], "title": "SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing", "comment": null, "summary": "Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.", "AI": {"tldr": "提出了一种名为SciFlow-Bench的新型基准测试，用于直接评估科学图表生成模型的像素级输出的结构正确性，解决了现有方法对结构不敏感的问题。", "motivation": "现有的文本到图像模型生成的科学图表在结构上常常不正确，而现有的基准测试无法有效评估最终渲染图像的结构准确性。", "method": "构建了一个名为SciFlow-Bench的基准测试，该基准测试直接在像素级输出上进行评估。它使用一个由真实科学PDF构建的闭环、往返协议，将生成的图表图像反解析回结构图，并与真实的地面真实图进行比较。该过程由一个分层的多智能体系统协调完成。", "result": "实验表明，保持结构正确性仍然是一个根本性的挑战，尤其是在拓扑结构复杂的图表中。", "conclusion": "需要开发结构感知的评估方法，以改进科学图表的生成模型。"}}
{"id": "2602.09825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09825", "abs": "https://arxiv.org/abs/2602.09825", "authors": ["Zhaoxu Li", "Chenqi Kong", "Peijun Bao", "Song Xia", "Yi Tu", "Yi Yu", "Xinghao Jiang", "Xudong Jiang"], "title": "SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding", "comment": null, "summary": "Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.", "AI": {"tldr": "本研究通过分析注意力头、模型层和解码 token 的不稳定性来研究大型视觉-语言模型（LVLM）的幻觉问题，并提出了一种名为 SAKED 的训练无关的解码方法，通过量化知识稳定性来抑制幻觉，实验证明其效果显著。", "motivation": "大型视觉-语言模型（LVLM）中的幻觉问题对实际应用造成了严重的安全和可靠性风险。研究者受到人类在不确定或犹豫时更容易出错的启发，旨在探索模型内部知识的不稳定性如何导致 LVLM 幻觉。", "method": "研究者从三个角度（注意力头、模型层、解码 token）进行了大量的实证分析，发现了三种关键的幻觉模式。基于这些发现，提出了 Stability-Aware Knowledge-Enhanced Decoding (SAKED) 方法，该方法引入了层级的知识稳定性得分（KSS）来量化知识稳定性，并通过对比最稳定和最不稳定的层来抑制解码噪声，动态利用最可靠的内部知识进行 token 生成。SAKED 具有训练无关的特点，可集成到不同架构中。", "result": "SAKED 方法在不同的模型、任务和基准测试中，在减少幻觉方面取得了最先进的性能。", "conclusion": "模型内部知识的不稳定性是导致 LVLM 幻觉的重要因素。SAKED 方法通过量化和利用知识稳定性，能够有效地抑制幻觉，提高 LVLM 的可靠性，并且易于集成和应用。"}}
{"id": "2602.09843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09843", "abs": "https://arxiv.org/abs/2602.09843", "authors": ["Boyang Ding", "Chenglong Chu", "Dunju Zang", "Han Li", "Jiangxia Cao", "Kun Gai", "Muhao Wei", "Ruiming Tang", "Shiyao Wang", "Siyang Mao", "Xinchen Luo", "Yahui Liu", "Zhixin Ling", "Zhuoran Yang", "Ziming Li", "Chengru Song", "Guorui Zhou", "Guowang Zhang", "Hao Peng", "Hao Wang", "Jiaxin Deng", "Jin Ouyang", "Jinghao Zhang", "Lejian Ren", "Qianqian Wang", "Qigen Hu", "Tao Wang", "Xingmei Wang", "Yiping Yang", "Zixing Zhang", "Ziqi Wang"], "title": "Kelix Technique Report", "comment": "Work in progress", "summary": "Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.", "AI": {"tldr": "本文提出了一种名为 Kelix 的全离散自回归统一模型，旨在弥合离散和连续视觉表示之间的理解差距，解决了现有离散视觉标记因容量限制导致信息丢失的问题。", "motivation": "现有的视觉-语言模型（VLMs）大多采用混合接口（离散文本+连续ViT特征），导致其更偏向理解而非生成，且难以充分利用大规模无监督文本外数据。而现有的全离散模型又因标记容量限制导致信息丢失，理解能力弱于连续特征模型。", "method": "Kelix 是一种全离散的自回归统一模型，通过一种新的视觉标记方法来克服现有离散视觉标记的信息丢失问题，从而实现跨模态的统一理解和生成。", "result": "Kelix 能够有效弥合离散和连续视觉表示在理解能力上的差距。", "conclusion": "Kelix 是一种有前景的全离散自回归统一模型，为实现更强大的多模态理解和生成能力提供了新的解决方案。"}}
{"id": "2602.09927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09927", "abs": "https://arxiv.org/abs/2602.09927", "authors": ["Isabel Funke", "Sebastian Bodenstedt", "Felix von Bechtolsheim", "Florian Oehme", "Michael Maruschke", "Stefanie Herrlich", "Jürgen Weitz", "Marius Distler", "Sören Torge Mees", "Stefanie Speidel"], "title": "A benchmark for video-based laparoscopic skill analysis and assessment", "comment": "under review", "summary": "Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.", "AI": {"tldr": "本研究提出了一个大型的腹腔镜手术技能评估数据集LASANA，包含1270个立体视频，并提供了结构化技能评分和错误标签，旨在促进基于视频的技能评估和错误识别模型的研究。", "motivation": "现有深度学习模型在腹腔镜手术技能培训方面的应用受到可用标注数据集规模有限的阻碍。", "method": "构建了一个包含1270个立体视频的LASANA数据集，涵盖四种基础腹腔镜训练任务，并对视频进行结构化技能评分和二元错误标签标注。提供预定义的数据集划分用于基准测试，并展示了一个深度学习模型的基线结果。", "result": "LASANA数据集包含了1270个腹腔镜训练视频，并进行了多维度标注，为模型评估提供了基础。", "conclusion": "LASANA数据集的发布为腹腔镜手术视频技能评估和错误识别的研究提供了一个大规模、多样化的资源，有助于推动相关深度学习模型的发展和评估。"}}
{"id": "2602.09868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09868", "abs": "https://arxiv.org/abs/2602.09868", "authors": ["Xiaoyue Ling", "Chuqin Zhou", "Chunyi Li", "Yunuo Chen", "Yuan Tian", "Guo Lu", "Wenjun Zhang"], "title": "Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence", "comment": null, "summary": "Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.", "AI": {"tldr": "本文提出了一种名为 Free-GVC 的训练无关的生成式视频压缩框架，通过压缩潜在轨迹并利用视频扩散先验来提高超低比特率下的重建质量和时间连贯性。", "motivation": "现有生成式视频压缩方法在利用时间相关性方面存在局限，导致在超低比特率下出现闪烁和时间连贯性下降等问题。", "method": "Free-GVC 将视频编码重构为由视频扩散先验引导的潜在轨迹压缩。它在 GOP（图像组）级别操作，将视频段编码为紧凑的潜在空间，并沿着扩散轨迹逐步压缩。引入了自适应质量控制模块来动态构建在线率感知代理模型，预测每个 GOP 的最佳扩散步骤。还使用了跨 GOP 对齐模块来建立帧重叠并进行潜在融合，以减少闪烁并提高时间连贯性。", "result": "Free-GVC 在 DISTS 指标上实现了比最新神经网络编解码器 DCVC-RT 平均 93.29% 的 BD-Rate 降低。用户研究也证实了其在超低比特率下卓越的感知质量和时间连贯性。", "conclusion": "Free-GVC 是一种有效的训练无关的生成式视频压缩框架，通过利用潜在轨迹压缩和视频扩散先验，能够显著提高超低比特率下的视频重建质量和时间连贯性。"}}
{"id": "2602.09979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09979", "abs": "https://arxiv.org/abs/2602.09979", "authors": ["Thomas H. Schmitt", "Maximilian Bundscherer", "Tobias Bocklet"], "title": "Learning to Detect Baked Goods with Limited Supervision", "comment": null, "summary": "Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.", "AI": {"tldr": "本文提出了一种利用计算机视觉技术自动检测德国烘焙食品剩余产品的方法，以解决人工盘点成本高、效率低的问题。由于特定行业数据稀疏且类别繁多，研究人员探索了弱监督和伪标签等训练策略，并取得了比完全监督基线模型更好的效果。", "motivation": "德国烘焙食品保质期短，监控剩余产品对优化生产至关重要。现有的人工盘点方式成本高、效率低且易出错。因此，自动化盘点流程以降低成本、提高准确性和效率是研究的驱动力。", "method": "研究人员使用 YOLOv11 模型进行目标检测。他们开发了两种训练工作流：1. 结合 OWLv2 和 Grounding DINO 的定位能力，利用图像级标签进行弱监督训练；2. 利用 Segment Anything 2 生成伪标签，对视频帧进行微调以提高视角鲁棒性。最后，将这两种方法结合起来进行训练。", "result": "仅使用图像级标签进行训练的模型达到了 0.91 的 mAP。通过伪标签微调后，模型在非理想部署条件下的性能提高了 19.3%。结合两种训练工作流训练的模型，即使仅使用图像级标签，在非理想部署条件下的表现也优于完全监督的基线模型。", "conclusion": "本文成功地提出了一种在数据稀疏和类别多样的工业场景中，利用弱监督和伪标签训练目标检测模型的方法，该方法在德国烘焙食品剩余产品检测任务上取得了优于完全监督基线模型的性能，证明了其有效性和可扩展性。"}}
{"id": "2602.09850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09850", "abs": "https://arxiv.org/abs/2602.09850", "authors": ["Peng Chen", "Chao Huang", "Yunkang Cao", "Chengliang Liu", "Wenqiang Wang", "Mingbo Yang", "Li Shen", "Wenqi Ren", "Xiaochun Cao"], "title": "Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.", "AI": {"tldr": "提出了一种名为Reason-IAD的知识引导动态潜在推理框架，用于可解释的工业异常检测，通过结合领域知识、熵驱动的推理和动态视觉信息，提高了检测精度和可解释性。", "motivation": "现有的多模态大语言模型在工业领域特定异常检测方面存在准确性和可解释性不足的问题，因为它们在通用领域数据上预训练，难以捕捉细粒度的缺陷模式。", "method": "Reason-IAD包含两个核心组件：1. 检索增强知识模块，将类别特定的文本描述纳入模型输入，实现领域特定缺陷的上下文感知推理。2. 熵驱动的潜在推理机制，通过可优化的潜在思考令牌在紧凑的潜在空间中进行迭代探索，并以鼓励自信稳定预测的熵为基础的奖励进行指导。此外，采用动态视觉注入策略，选择性地将信息量最大的图像块纳入潜在序列，将推理过程导向对异常检测至关重要的区域。", "result": "Reason-IAD在实验中持续优于最先进的方法，表明其在工业异常检测任务上的有效性。", "conclusion": "Reason-IAD通过集成领域知识和创新的推理机制，有效解决了现有模型在工业异常检测中的局限性，显著提高了检测性能和可解释性。"}}
{"id": "2602.09934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09934", "abs": "https://arxiv.org/abs/2602.09934", "authors": ["Yikun Liu", "Yuan Liu", "Shangzhe Di", "Haicheng Wang", "Zhongyin Zhao", "Le Tian", "Xiao Zhou", "Jie Zhou", "Jiangchao Yao", "Yanfeng Wang", "Weidi Xie"], "title": "VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.", "AI": {"tldr": "本文提出VersaViT，一种用于多模态大语言模型（MLLMs）的视觉编码器，通过新颖的多任务协同后训练框架，解决其在密集预测任务中表现不足的问题，使其能同时胜任视觉-语言理解和像素级理解任务。", "motivation": "多模态大语言模型（MLLMs）的视觉编码器在视觉-语言理解方面表现出色，但其在经典的视觉密集预测任务（如语义分割、深度估计）上表现不佳，研究者希望探索其作为通用视觉骨干的潜力。", "method": "作者首先分析了MLLMs视觉编码器在密集特征表示上的不足。然后，提出了VersaViT，一个结合了新颖多任务协同后训练框架的视觉Transformer。该框架通过轻量级的任务头和多粒度监督，优化视觉骨干，使其能够同时处理语言和像素级别的任务。", "result": "实验证明，VersaViT作为一种多功能的视觉骨干，在多项下游任务上表现出色，能够有效支持语言中介的推理和像素级别的理解。", "conclusion": "MLLMs的视觉编码器可以通过VersaViT的多任务协同后训练框架得到改进，使其成为一个能够兼顾视觉-语言理解和传统计算机视觉任务的通用视觉骨干。"}}
{"id": "2602.09878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09878", "abs": "https://arxiv.org/abs/2602.09878", "authors": ["Jiaxu Wang", "Yicheng Jiang", "Tianlun He", "Jingkai Sun", "Qiang Zhang", "Junhao He", "Jiahang Cao", "Zesen Gan", "Mingyuan Sun", "Qiming Shao", "Xiangyu Yue"], "title": "MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation", "comment": null, "summary": "World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.", "AI": {"tldr": "本文提出了一种新的四维（4D）世界模型，能够从单一的RGBD输入生成多视角、几何一致的RGBD数据，并结合轨迹级动作优化，以应对机器人操作中的动态预测和动作规划问题。", "motivation": "现有的基于世界模型的机器人操作方法在预测完整4D场景动态方面存在局限，因为它们通常只支持纯图像预测或基于不完整的3D几何进行推理。", "method": "该研究提出了一种新型的四维世界模型，通过显式设计跨视图和跨模态的特征融合，实现了几何一致的任意视角RGBD生成。同时，为了解决逆动力学模型不适定的问题，引入了测试时动作优化策略，通过反向传播优化轨迹级潜在表示，并利用残差逆动力学模型生成可执行动作。", "result": "在三个数据集上的实验表明，该模型在4D场景生成和下游操作任务上均表现出色。消融实验也提供了关于关键设计选择的实用见解。", "conclusion": "所提出的四维世界模型能够从单视角RGBD输入生成几何一致的多视角RGBD数据，并结合轨迹级动作优化，有效解决了机器人操作中的动态预测和动作规划问题。"}}
{"id": "2602.09883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09883", "abs": "https://arxiv.org/abs/2602.09883", "authors": ["Shaoqiu Zhang", "Zizhong Ding", "Kaicheng Yang", "Junyi Wu", "Xianglong Yan", "Xi Li", "Bingnan Duan", "Jianping Fang", "Yulun Zhang"], "title": "AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization", "comment": "Code will be released at https://github.com/Qiushao-E/AdaTSQ/", "summary": "Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.", "AI": {"tldr": "本文提出了AdaTSQ，一种用于Diffusion Transformers (DiTs) 的后训练量化（PTQ）框架，通过利用DiTs的时间敏感性，在效率和质量之间取得更好的权衡，从而克服了DiTs在边缘设备部署的挑战。", "motivation": "Diffusion Transformers（DiTs）在图像和视频生成方面表现出色，但其巨大的计算和内存开销限制了其在边缘设备的部署。现有PTQ方法直接应用于DiTs效果不佳，因为它们忽略了扩散过程特有的时间动态。", "method": "AdaTSQ包含两个关键技术：1. 帕累托感知的时间步长动态比特宽度分配策略，将量化策略搜索建模为约束路径查找问题，并使用引导式束搜索算法根据端到端重建误差动态分配跨不同时间步长的层级比特宽度。2. Fisher引导的时间校准机制，利用时间Fisher信息从高度敏感的时间步长中优先选择校准数据，并与基于Hessian的权重优化相结合。", "result": "在Flux-Dev、Flux-Schnell、Z-Image和Wan2.1等四个先进DiTs上的大量实验表明，AdaTSQ在性能上显著优于SVDQuant和ViDiT-Q等现有技术。", "conclusion": "AdaTSQ是一种新颖的PTQ框架，通过有效利用DiTs的时间敏感性，显著提高了DiTs在边缘设备上的效率和生成质量，克服了现有方法的局限性。"}}
{"id": "2602.09872", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09872", "abs": "https://arxiv.org/abs/2602.09872", "authors": ["Mridankan Mandal"], "title": "BabyMamba-HAR: Lightweight Selective State Space Models for Efficient Human Activity Recognition on Resource Constrained Devices", "comment": null, "summary": "Human activity recognition (HAR) on wearable and mobile devices is constrained by memory footprint and computational budget, yet competitive accuracy must be maintained across heterogeneous sensor configurations. Selective state space models (SSMs) offer linear time sequence processing with input dependent gating, presenting a compelling alternative to quadratic complexity attention mechanisms. However, the design space for deploying SSMs in the TinyML regime remains largely unexplored. In this paper, BabyMamba-HAR is introduced, a framework comprising two novel lightweight Mamba inspired architectures optimized for resource constrained HAR: (1) CI-BabyMamba-HAR, using a channel independent stem that processes each sensor channel through shared weight, but instance independent transformations to prevent cross channel noise propagation, and (2) Crossover-BiDir-BabyMamba-HAR, using an early fusion stem that achieves channel count independent computational complexity. Both variants incorporate weight tied bidirectional scanning and lightweight temporal attention pooling. Through evaluation across eight diverse benchmarks, it is demonstrated that Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with approximately 27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high channel datasets. Systematic ablation studies reveal that bidirectional scanning contributes up to 8.42% F1-score improvement, and gated temporal attention provides up to 8.94% F1-score gain over mean pooling. These findings establish practical design principles for deploying selective state space models as efficient TinyML backbones for HAR.", "AI": {"tldr": "本文提出了一种名为 BabyMamba-HAR 的新框架，包含两种轻量级 Mamba 启发式架构（CI-BabyMamba-HAR 和 Crossover-BiDir-BabyMamba-HAR），旨在解决资源受限的 TinyML 人类活动识别（HAR）问题。实验证明 Crossover-BiDir-BabyMamba-HAR 在参数量和计算量上均优于现有方法，同时保持了竞争力。", "motivation": "在内存和计算资源有限的穿戴式和移动设备上实现高精度的 HAR 是一个挑战。现有的基于注意力机制的方法计算复杂度高，而选择性状态空间模型（SSMs）在 TinyML 领域的研究尚不充分。", "method": "提出 BabyMamba-HAR 框架，包含两种新架构：CI-BabyMamba-HAR（通道独立）和 Crossover-BiDir-BabyMamba-HAR（跨通道双向）。两种架构均采用权重绑定双向扫描和轻量级时间注意力池化。通过在八个基准数据集上进行评估，并进行消融实验分析了双向扫描和门控时间注意力池化的作用。", "result": "Crossover-BiDir-BabyMamba-HAR 在八个基准数据集上取得了平均 86.52% 的 F1 分数，仅用约 27K 参数和 2.21M MACs，在多通道数据集上比 TinyHAR 的 MACs 减少了 11 倍。消融实验表明，双向扫描最多可提高 8.42% 的 F1 分数，门控时间注意力池化最多可提高 8.94% 的 F1 分数。", "conclusion": "本文提出的 BabyMamba-HAR 框架，特别是 Crossover-BiDir-BabyMamba-HAR 架构，为在资源受限的 TinyML 设备上部署高效的 HAR 模型提供了实用的设计原则，证明了选择性状态空间模型作为 HAR 的轻量级骨干网络的潜力。"}}
{"id": "2602.09932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09932", "abs": "https://arxiv.org/abs/2602.09932", "authors": ["Han Jinzhen", "JinByeong Lee", "JiSung Kim", "MinKyung Cho", "DaHee Kim", "HongSik Yun"], "title": "GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery", "comment": null, "summary": "Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.", "AI": {"tldr": "GeoFormer是一个开源的Swin Transformer框架，仅使用Sentinel-1/2和开放DEM数据，即可在100米网格上联合估计建筑高度（BH）和占地面积（BF），并在54个城市中取得了优于CNN基线方法的性能，且具有良好的跨区域泛化能力。", "motivation": "现有城市三维数据（如建筑高度和占地面积）的获取依赖于专有传感器或跨城市泛化能力差，限制了其在气候建模、灾害风险评估和城市规划等领域的应用。因此，需要开发一种更通用、开放的数据获取方法。", "method": "提出GeoFormer框架，基于Swin Transformer模型，联合利用Sentinel-1/2卫星影像和开放数字高程模型（DEM）数据，在100米网格分辨率下估计建筑高度（BH）和占地面积（BF）。采用地理块划分策略确保训练集和测试集之间的空间独立性。", "result": "在54个不同城市进行评估，GeoFormer的BH RMSE为3.19米，BF RMSE为0.05，相比最强的CNN基线分别提高了7.5%和15.3%。模型在跨大陆迁移测试中，BH RMSE仍能保持在3.5米以下。消融研究表明，DEM对于高度估计是必需的，光学反射比SAR数据更重要，但多源融合能获得最佳精度。", "conclusion": "GeoFormer框架能够仅依靠Sentinel-1/2和开放DEM数据，高效且准确地生成建筑高度和占地面积信息，显著优于现有方法，并具备良好的跨区域泛化能力，为城市规划和相关研究提供了开放的解决方案。"}}
{"id": "2602.10052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10052", "abs": "https://arxiv.org/abs/2602.10052", "authors": ["Serin Varghese", "Kevin Ross", "Fabian Hueger", "Kira Maag"], "title": "Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving", "comment": null, "summary": "Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.", "AI": {"tldr": "提出一种时空注意力（STA）机制，通过整合多帧信息来增强视频语义分割的准确性和稳定性。", "motivation": "现有模型独立处理视频帧，未能利用时间一致性来提升动态场景下的性能。", "method": "修改Transformer的自注意力机制，使其能够处理时空特征序列，从而整合多帧上下文。该方法计算效率高，对现有架构改动小。", "result": "在Cityscapes和BDD100k数据集上，STA在时间一致性指标上提升了9.20个百分点，在平均交并比（mIoU）上最高提升了1.76个百分点，显著优于单帧基线模型。", "conclusion": "STA是一种有效的架构增强方法，能够显著提升视频语义分割的性能，并且适用于各种Transformer架构。"}}
{"id": "2602.09999", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.09999", "abs": "https://arxiv.org/abs/2602.09999", "authors": ["Florian Hahlbohm", "Linus Franke", "Martin Eisemann", "Marcus Magnor"], "title": "Faster-GS: Analyzing and Improving Gaussian Splatting Optimization", "comment": "Project page: https://fhahlbohm.github.io/faster-gaussian-splatting", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.", "AI": {"tldr": "本文提出了一种名为 Faster-GS 的新系统，通过整合现有 3DGS 研究中最有效和最通用的策略，并引入新的优化方法，显著加快了 3DGS 的训练速度（最高可达 5 倍），同时保持了重建质量，成为 3DGS 优化的新基准。该方法还被证明可以应用于 4D 场景的非刚性重建。", "motivation": "现有 3DGS 研究方法在加速优化和保持重建质量方面存在碎片化问题，许多方法将实现层面的改进与根本性算法修改混淆，或者牺牲性能换取保真度，导致难以进行公平比较。作者希望整合并评估最有效和最通用的策略，并进行深入研究，以解决这些问题。", "method": "该研究整合了先前 3DGS 研究中最有效和最通用的策略，并引入了几项新的优化。同时，深入研究了数值稳定性、高斯截断和梯度近似等之前未充分探索的方面。最终系统命名为 Faster-GS。", "result": "Faster-GS 实现了高达 5 倍的训练速度提升，同时保持了视觉质量，确立了新的成本效益和资源效率基准。此外，研究还表明这些优化方法可以应用于 4D 高斯重建，实现了高效的非刚性场景优化。", "conclusion": "Faster-GS 通过整合和优化现有技术，提供了一个严谨的高效 3DGS 优化算法，为 3DGS 的训练设立了新的速度和效率基准。该方法在 4D 非刚性场景重建方面也展现出潜力。"}}
{"id": "2602.10094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10094", "abs": "https://arxiv.org/abs/2602.10094", "authors": ["Yihang Luo", "Shangchen Zhou", "Yushi Lan", "Xingang Pan", "Chen Change Loy"], "title": "4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere", "comment": "Project page: https://yihangluo.com/projects/4RC/", "summary": "We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.", "AI": {"tldr": "4RC是一个统一的前馈框架，用于从单目视频进行4D重建，该框架能够联合学习密集的场景几何和运动动力学，并且通过“一次编码，随时随地查询”的范式，从时空潜在空间中高效查询3D几何和运动。", "motivation": "现有方法通常将运动和几何分离，或仅生成稀疏轨迹/两视图场景流等有限的4D属性。研究者希望提出一种能够联合学习密集场景几何和运动动力学的方法。", "method": "提出4RC框架，采用“一次编码，随时随地查询”的范式：使用Transformer编码器将整个视频编码到紧凑的时空潜在空间；使用条件解码器从潜在空间高效查询任意查询帧在任意目标时间戳的3D几何和运动。通过将每视图的4D属性分解为基础几何和时间相关的相对运动来进行学习。", "result": "4RC在多种4D重建任务上均优于现有和同期方法。", "conclusion": "4RC是一个高效且强大的单目4D重建框架，通过联合学习几何和运动，并采用新颖的时空潜在空间表示和查询机制，在各种4D重建任务上取得了优异性能。"}}
{"id": "2602.09989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09989", "abs": "https://arxiv.org/abs/2602.09989", "authors": ["Oskar Thaeter", "Christian Grashei", "Anette Haas", "Elisa Schmoeckel", "Han Li", "Peter J. Schüffler"], "title": "Efficient Special Stain Classification", "comment": "14 pages, 7 figures, 2 tables", "summary": "Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently\n  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for\n  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly\n  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.\n  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and\n  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of\n  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control\n  in digital pathology workflows.", "AI": {"tldr": "本文比较了两种自动化染色分类方法（多实例学习 MIL 和基于缩略图的方法），用于对病理切片图像进行染色分类，结果表明缩略图方法在通用性和效率方面具有优势。", "motivation": "病理切片中染色信息的准确元数据对于临床存档的质量控制和计算病理学数据集的完整性至关重要。因此，研究自动化染色分类方法以提高效率和准确性。", "method": "本文评估了两种自动化染色分类方法：1. 多实例学习 (MIL) 管道；2. 提出的轻量级缩略图（thumbnail）-基础方法。研究涵盖了14种最常用的特殊染色以及标准和冷冻切片 H&E 染色。", "result": "在内部测试数据上，MIL 取得了最高的性能（宏 F1 分数：0.941，针对16类；0.969，针对14个合并类别）。缩略图方法也表现出竞争力（分别为0.897和0.953）。在外部 TCGA 数据上，缩略图模型泛化能力更好（加权 F1 分数：0.843 vs. MIL 的0.807）。缩略图方法还将吞吐量提高了两个数量级（5.635 vs. MIL 的0.018 slides/s）。", "conclusion": "基于缩略图的分类方法为数字病理学工作流程中的常规视觉质量控制提供了一种可扩展且鲁棒的解决方案。"}}
{"id": "2602.10043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10043", "abs": "https://arxiv.org/abs/2602.10043", "authors": ["Gaurang Sharma", "Harri Polonen", "Juha Pajula", "Jutta Suksi", "Jussi Tohka"], "title": "Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI", "comment": null, "summary": "Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.\n  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.", "AI": {"tldr": "本文研究表明，即使在进行颅骨剥离后，T1加权脑部MRI图像也可能包含独特的个体特征，足以在不同时间、设备和协议下匹配同一参与者的图像，存在隐私泄露风险。", "motivation": "现有数据共享框架要求去除识别信息，但即使去除颅骨，脑实质仍可能包含独特的签名，在与其他数据特征结合时构成隐私风险。研究旨在评估这种风险的现实性，并为制定数据共享政策提供依据。", "method": "使用标准的预处理流程，然后进行图像相似度计算，来评估和链接个体脑部MRI图像。", "result": "在匹配不同时间间隔、扫描仪类型、空间分辨率和采集协议的样本时，实现了近乎完美的链接准确率，即使在模拟了认知能力下降的情况下。", "conclusion": "通过标准预处理和图像相似度计算，可以成功链接个体脑部T1加权MRI图像，这在存在其他识别信息的情况下可能导致重新识别。研究结果旨在为医疗数据共享政策的制定提供有益参考。"}}
{"id": "2602.10045", "categories": ["cs.CV", "cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10045", "abs": "https://arxiv.org/abs/2602.10045", "authors": ["Kerri Lu", "Dan M. Kluger", "Stephen Bates", "Sherrie Wang"], "title": "Conformal Prediction Sets for Instance Segmentation", "comment": null, "summary": "Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.", "AI": {"tldr": "本文提出了一种新的置信预测算法，用于为实例分割生成自适应置信集，并提供了理论保证，能够确保至少一个预测的IoU与真实实例掩码足够高。", "motivation": "现有的实例分割模型平均预测性能高，但缺乏原则性的不确定性量化，其输出未校准，无法保证预测掩码接近真实值。", "method": "提出了一种置信预测算法，用于为实例分割生成自适应置信集。该算法接收图像和像素坐标查询，生成一个像素点的实例预测置信集，并提供概率保证。", "result": "在农业田地描绘、细胞分割和车辆检测等实例分割任务上进行了实验。结果表明，预测集的大小会根据查询的难度而变化，并达到了目标覆盖率，优于Learn Then Test、Conformal Risk Control和基于形态学膨胀的方法。", "conclusion": "所提出的置信预测算法能够为实例分割提供具有理论保证的自适应置信集，并在多个任务上取得了优于现有方法的实证结果。文章提供了具有渐近和有限样本保证的算法版本。"}}
{"id": "2602.10079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10079", "abs": "https://arxiv.org/abs/2602.10079", "authors": ["Soumyaroop Nandi", "Prem Natarajan"], "title": "Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach", "comment": null, "summary": "We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.", "AI": {"tldr": "Forensim是一个基于注意力机制的状态空间框架，用于图像篡改检测，能够同时定位篡改区域（目标）和来源区域，并支持拼接和复制-移动两种篡改类型。", "motivation": "传统的篡改检测方法仅依赖于伪影线索，难以捕捉复制模式，这在某些场景（如抗议图像）下可能导致误导性的解释。因此，需要一种能够联合定位来源区域和目标区域的方法。", "method": "Forensim提出了一个视觉状态空间模型，利用归一化注意力图识别内部相似性，并结合基于区域的块注意力模块来区分篡改区域，从而实现端到端训练和精确的定位。该框架能够输出三类掩码（原始、来源、目标），并统一支持拼接和复制-移动篡改的检测。", "result": "Forensim在标准基准测试中取得了最先进的性能。此外，研究者还发布了一个新的数据集CMFD-Anything，以解决现有复制-移动篡改数据集的局限性。", "conclusion": "Forensim框架能够有效地联合定位图像篡改的来源和目标区域，在检测多种类型的篡改方面表现出色，并有望改进现有的图像篡改检测方法。"}}
{"id": "2602.10115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10115", "abs": "https://arxiv.org/abs/2602.10115", "authors": ["Shuteng Wang", "Natacha Kuete Meli", "Michael Möller", "Vladislav Golyanik"], "title": "Quantum Multiple Rotation Averaging", "comment": "16 pages, 13 figures, 4 tables; project page: https://4dqv.mpi-inf.mpg.de/QMRA/", "summary": "Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.", "AI": {"tldr": "本文提出了一种名为 IQARS 的新算法，利用量子退火解决三维视觉和机器人领域中的多重旋转平均问题，在有噪声的情况下比现有经典方法有更高的准确性。", "motivation": "现有经典的多重旋转平均方法（如 L1-IRLS 和 Shonan）容易陷入局部最小值，并且依赖于无法保持精确流形几何的凸松弛，在高噪声情况下精度会下降。", "method": "IQARS 将多重旋转平均问题重构为一系列可由量子退火器执行的局部二次非凸子问题，通过二值化在量子退火器上运行。它消除了对凸松弛的依赖，更好地保留了旋转流形的非欧几里得几何，并利用量子隧穿和并行性来探索解空间。", "result": "在合成和真实世界数据集上的评估表明，尽管目前的量子退火器规模有限且性能受限，IQARS 在 D-Wave 退火器上的表现比评估过的最佳经典方法 Shonan 准确率高约 12%。", "conclusion": "IQARS 是第一个利用量子退火解决多重旋转平均问题的算法，在有噪声场景下展现出比经典方法更优越的性能，预示着量子计算在三维视觉和机器人领域的应用潜力。"}}
{"id": "2602.10102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10102", "abs": "https://arxiv.org/abs/2602.10102", "authors": ["Zhongwei Ren", "Yunchao Wei", "Xiao Yu", "Guixun Luo", "Yao Zhao", "Bingyi Kang", "Jiashi Feng", "Xiaojie Jin"], "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos", "comment": "Code and models are released at: https://maverickren.github.io/VideoWorld2.github.io/", "summary": "Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.", "AI": {"tldr": "本文提出了VideoWorld 2，一个能够从原始真实视频中直接学习可迁移知识的模型，通过动态增强的潜在动力学模型（dLDM）将视觉外观与动作动力学解耦，并在手工制作和机器人操控任务中取得了显著的性能提升。", "motivation": "为了让智能体能够从无标签的视频数据中学习可迁移知识，并将其应用于新环境，特别是从原始真实视频中学习。", "method": "提出了动态增强的潜在动力学模型（dLDM），利用预训练的视频扩散模型处理视觉外观，dLDM专注于学习紧凑且有意义的任务相关动力学。这些潜在编码通过自回归方式进行建模，以学习任务策略并支持长时推理。", "result": "在具有挑战性的真实世界手工制作任务中，VideoWorld 2 将任务成功率提高了 70%，并生成了连贯的长执行视频。在机器人领域，VideoWorld 2 从 Open-X 数据集中获得了有效的操控知识，显著提高了在 CALVIN 上的任务性能。", "conclusion": "从原始视频中学习可迁移世界知识具有巨大潜力，VideoWorld 2 证明了其在复杂任务中的有效性，并将开放所有代码、数据和模型以促进进一步研究。"}}
{"id": "2602.10113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.10113", "abs": "https://arxiv.org/abs/2602.10113", "authors": ["Mingyang Wu", "Ashirbad Mishra", "Soumik Dey", "Shuo Xing", "Naveen Ravipati", "Hansi Wu", "Binbin Li", "Zhengzhong Tu"], "title": "ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation", "comment": "Project page: https://myangwu.github.io/ConsID-Gen", "summary": "Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.", "AI": {"tldr": "本文提出了ConsID-Gen，一个用于图像到视频生成的框架，通过引入多视角辅助和新的数据集ConsIDVid及评估框架ConsIDVid-Bench，解决了现有方法中物体身份保持和几何失真问题，并在多项指标上超越了现有SOTA模型。", "motivation": "现有图像到视频生成（I2V）模型在保持物体身份和避免几何失真方面存在困难，这归因于单视角2D观测的稀疏性和跨模态对齐的不足。", "method": "1. 提出了ConsIDVid，一个大规模、高质量、时间对齐的物体中心视频数据集。2. 建立了ConsIDVid-Bench，一个包含新颖的多视角一致性评估指标的基准测试框架。3. 提出了ConsID-Gen，一个增强了未姿态辅助视图、使用双流视觉-几何编码器融合语义和结构线索、并通过文本-视觉连接器进行统一条件化的生成框架。", "result": "ConsID-Gen在ConsIDVid-Bench上的实验显示，其在多项指标上均优于现有方法，并在物体身份保真度和时间一致性方面取得了显著提升，超越了Wan2.1和HunyuanVideo等领先模型。", "conclusion": "通过引入多视角辅助、构建新的数据集和评估框架，并提出ConsID-Gen模型，可以有效解决图像到视频生成中的物体身份保持和几何失真问题，提升视频生成质量。"}}
{"id": "2508.08232", "categories": ["eess.IV", "cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08232", "abs": "https://arxiv.org/abs/2508.08232", "authors": ["Buddhi Wijenayake", "Athulya Ratnayake", "Praveen Sumanasekara", "Roshan Godaliyadda", "Parakrama Ekanayake", "Vijitha Herath", "Nichula Wasalathilaka"], "title": "Mamba-FCS: Joint Spatio- Frequency Feature Fusion, Change-Guided Attention, and SeK Loss for Enhanced Semantic Change Detection in Remote Sensing", "comment": "19 pages, 13 Figures", "summary": "Semantic Change Detection (SCD) from remote sensing imagery requires models balancing extensive spatial context, computational efficiency, and sensitivity to class-imbalanced land-cover transitions. While Convolutional Neural Networks excel at local feature extraction but lack global context, Transformers provide global modeling at high computational costs. Recent Mamba architectures based on state-space models offer compelling solutions through linear complexity and efficient long-range modeling. In this study, we introduce Mamba-FCS, a SCD framework built upon Visual State Space Model backbone incorporating, a Joint Spatio-Frequency Fusion block incorporating log-amplitude frequency domain features to enhance edge clarity and suppress illumination artifacts, a Change-Guided Attention (CGA) module that explicitly links the naturally intertwined BCD and SCD tasks, and a Separated Kappa (SeK) loss tailored for class-imbalanced performance optimization. Extensive evaluation on SECOND and Landsat-SCD datasets shows that Mamba-FCS achieves state-of-the-art metrics, 88.62% Overall Accuracy, 65.78% F_scd, and 25.50% SeK on SECOND, 96.25% Overall Accuracy, 89.27% F_scd, and 60.26% SeK on Landsat-SCD. Ablation analyses confirm distinct contributions of each novel component, with qualitative assessments highlighting significant improvements in SCD. Our results underline the substantial potential of Mamba architectures, enhanced by proposed techniques, setting a new benchmark for effective and scalable semantic change detection in remote sensing applications. The complete source code, configuration files, and pre-trained models will be publicly available upon publication.", "AI": {"tldr": "本文提出了一种名为 Mamba-FCS 的遥感影像语义变化检测新框架，该框架基于 Visual State Space Model (Mamba) 主干，并结合了联合时频融合、变化引导注意力机制和分离 Kappa 损失，在 SECOND 和 Landsat-SCD 数据集上取得了最先进的性能。", "motivation": "现有的语义变化检测模型在平衡空间上下文、计算效率和类别不平衡方面的能力不足。卷积神经网络缺乏全局上下文，Transformer 计算成本高，而 Mamba 架构在长距离建模方面具有优势，但需要进一步优化以适应遥感影像变化检测的挑战。", "method": "1. **Visual State Space Model (Mamba) backbone**: 提供高效的长距离建模能力。 2. **Joint Spatio-Frequency Fusion block**: 融合了幅度频域特征，增强边缘清晰度并抑制光照变化。 3. **Change-Guided Attention (CGA) module**: 显式连接二值变化检测 (BCD) 和语义变化检测 (SCD) 任务。 4. **Separated Kappa (SeK) loss**: 专门为类别不平衡问题设计的损失函数。", "result": "在 SECOND 数据集上，Mamba-FCS 取得了 88.62% 的总体准确率 (OA)，65.78% 的 F_scd 和 25.50% 的 SeK。在 Landsat-SCD 数据集上，取得了 96.25% 的 OA，89.27% 的 F_scd 和 60.26% 的 SeK。消融实验证明了每个新组件的有效性，定性评估显示了显著的改进。", "conclusion": "Mamba-FCS 框架在遥感影像语义变化检测任务中展现了 Mamba 架构的巨大潜力，通过提出的联合时频融合、变化引导注意力和分离 Kappa 损失等技术，显著提升了性能，并在计算效率和长距离建模方面取得了新的突破，为遥感应用设定了新的有效性和可扩展性基准。"}}
