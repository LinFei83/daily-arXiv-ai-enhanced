<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.CV](#cs.CV) [Total: 183]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.RO](#cs.RO) [Total: 62]
- [eess.SY](#eess.SY) [Total: 32]
- [eess.IV](#eess.IV) [Total: 8]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 本研究提出了一种多模态虚假评论检测框架，结合BERT文本特征和ResNet-50视觉特征，有效提升了对数字商务平台中虚假评论的识别能力，F1分数达到0.934。


<details>
  <summary>Details</summary>
Motivation: 数字商务中用户生成评论对消费者行为和平台信誉至关重要，但虚假评论（由机器人、付费代理或AI生成）泛滥，严重威胁信任和透明度。现有检测模型主要依赖单模态（通常是文本）数据，无法捕捉跨模态的语义不一致性。

Method: 研究提出一个鲁棒的多模态虚假评论检测框架，整合了BERT编码的文本特征和ResNet-50提取的视觉特征。这些特征通过一个分类头进行融合，共同预测评论的真实性。该方法利用了一个包含21,142张用户上传图像的精选数据集，涵盖外卖、酒店和电商领域。

Result: 实验结果表明，多模态模型优于单模态基线，在测试集上F1分数达到0.934。混淆矩阵和定性分析也突出显示了模型检测细微不一致（例如，夸大其词的文本赞美与不相关或低质量图片配对）的能力，这在欺骗性内容中很常见。

Conclusion: 本研究证明了多模态学习在维护数字信任方面的关键作用，并为各种在线平台的内容审核提供了一个可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [2] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 本研究通过多智能体强化学习（MARL）系统地实证研究了零售动态定价优化问题，比较了MAPPO基线模型与结合图注意力网络（GAT）的MAPPO+GAT模型，发现后者通过利用产品间学习到的交互信息，在利润、稳定性、公平性和训练效率方面表现更优，提供了更具可扩展性和稳定性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要政策能够适应不断变化的需求，并协调相关产品之间的决策。

Method: 研究采用多智能体强化学习（MARL）方法，比较了MAPPO基线模型和增强了图注意力网络（GAT）的MAPPO+GAT变体。模型在一个基于真实交易数据构建的模拟定价环境中进行训练和评估。评估指标包括利润、跨随机种子的稳定性、产品间的公平性以及训练效率。

Result: 结果表明，MAPPO为产品组合级别的价格控制提供了一个鲁棒且可复现的基础。MAPPO+GAT通过在产品图中共享信息，进一步提升了性能，且未引入过度的价格波动。

Conclusion: 研究得出结论，图集成多智能体强化学习（MARL）为动态零售定价提供了比独立学习器更具可扩展性和稳定性的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [3] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: 本文详细描述了为奥地利GEPOC模型（特别是GEPOC ABM）计算模型参数的数据处理方法，这些方法基于公开数据并涵盖了数据处理的各个环节，并进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为确保GEPOC模型在特定国家或地区的有效应用，需要稳定、可复用的数据处理流程来提供有效且可直接使用的模型参数。本研究旨在为奥地利提供基于免费公开数据的此类流程。

Method: 本文描述了为奥地利计算GEPOC模型参数的完整数据处理方法。这包括：使用免费和公开的数据源；详细说明了数据聚合、分解、融合、清洗和缩放的算法；以及对所得参数文件的描述。特别强调了用于GEPOC ABM（一种连续时间基于Agent的人口模型）的参数计算。最后进行了广泛的验证研究。

Result: 本文提供了奥地利GEPOC模型参数计算的完整数据处理方法描述，涵盖了从数据源到参数文件生成的所有算法。特别是为GEPOC ABM模型计算了参数，并对结果进行了验证。

Conclusion: 成功开发并描述了为奥地利GEPOC模型，特别是GEPOC ABM模型，计算有效参数的完整数据处理方法，这些方法基于免费公开数据且经过验证，确保了模型在特定区域应用的有效性。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [4] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: 本文介绍了QuantumBench，这是首个用于评估大型语言模型在量子科学领域理解和应用能力的基准测试数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已广泛应用于科学工作流程，但通用基准测试往往无法准确反映其在量子科学等专业领域中捕捉特定知识和符号的能力，尤其是在量子科学这种非直观且需要高级数学的领域，这种差距尤为明显。

Method: 研究人员构建了QuantumBench，一个包含约800个多项选择题（每题8个选项）的数据集，涵盖量子科学的九个领域。这些问题及其答案均从公开可用材料中编译而来。利用此基准，研究人员评估了现有LLMs在量子领域的表现，并分析了它们对问题格式变化的敏感性。

Result: QuantumBench是首个专为量子领域构建的LLM评估数据集。通过该基准，研究人员评估了多种现有LLMs在量子领域的性能，并分析了它们对问题格式变化的敏感性。

Conclusion: QuantumBench旨在指导LLMs在量子研究中的有效应用，填补了LLMs在量子领域评估数据集的空白。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [5] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 本文提出了Engineering.ai平台，一个用于计算设计中AI工程师团队的协作框架。它采用分层多智能体架构，通过LLM驱动的专业智能体进行文件介导的通信和全面的记忆系统，实现了自主、多学科的工程设计，并在无人机机翼优化中取得了100%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中，人类工程师团队协作设计复杂产品虽然有效，但耗时且成本高昂。为了解决这一问题，并基于OpenFOAMGPT和turbulence.ai等早期AI工程师的成功经验，研究旨在开发一个能够自主执行复杂工程任务的AI平台。

Method: 该框架采用分层多智能体架构，由一名首席工程师协调多个专业智能体（如空气动力学、结构、声学、优化工程师），每个智能体都由具备领域特定知识的LLM驱动。智能体间通过文件介导通信实现数据溯源和可复现性。一个全面的记忆系统维护项目上下文、执行历史和检索增强的领域知识。系统集成了FreeCAD、Gmsh、OpenFOAM、CalculiX和BPM声学分析等工具，支持并行多学科仿真。

Result: 该框架通过无人机机翼优化进行了验证。在超过400种参数配置中，自动化工作流程取得了100%的成功率，没有出现网格生成失败、求解器收敛问题或需要人工干预的情况。

Conclusion: 研究表明，由智能体AI驱动的AI工程师有潜力自主执行复杂的工程任务。该框架的100%成功率和零故障率证明了其可靠性和可信赖性。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [6] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 本文提出了一种改进的增量选择算法，并证明了所有相关的猜想。


<details>
  <summary>Details</summary>
Motivation: 原始选择算法（如参考文献[1]所述）可能存在改进空间，且相关的猜想需要被正式证明。

Method: 通过提出一种“改进的增量选择算法”来优化现有方法，并通过数学证明或形式验证来“证明所有选定的猜想”。

Result: 成功地开发出了一种优于现有技术的增量选择算法，并为之前未经验证的猜想提供了确凿的证明。

Conclusion: 本研究不仅提升了现有选择算法的性能，还为相关理论猜想提供了坚实的数学基础。

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [7] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文介绍了ARC-GEN，一个开源的程序生成器，旨在忠实地扩展原有的ARC-AGI训练数据集，以解决其示范集基数小的问题，并可用于建立基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 抽象推理语料库（ARC-AGI）是评估通用人工智能进展的挑战性基准，但其示范集数量有限（每任务只有少量输入/输出对），这限制了机器学习系统学习效率的提升，而学习效率是其目前所缺乏的特质。

Method: 本文引入了ARC-GEN，一个开源的程序生成器，用于扩展原始的ARC-AGI训练数据集。该生成器是详尽的（覆盖所有四百个任务），并且是模仿性的（更忠实地遵循了ARC-AGI-1版本中体现的分布特性和特征）。

Result: ARC-GEN成功地扩展了ARC-AGI训练数据集，为解决数据稀缺问题提供了途径。此外，该生成器还被用于建立一个静态基准测试套件，以验证提交给2025年Google Code Golf Championship的程序的正确性。

Conclusion: ARC-GEN通过生成忠实且详尽的额外样本对，有效缓解了ARC-AGI数据集的样本稀缺问题，有望促进通用人工智能的学习效率提升，并已在实际应用中作为编程竞赛的基准测试工具。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [8] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 这篇综述探讨了大型语言模型（LLMs）如何帮助认知科学克服知识综合和概念清晰度方面的挑战，强调其作为人类专业知识补充工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 认知科学因其多学科和跨学科性质，在知识综合和概念清晰度方面持续面临挑战。人工智能，特别是大型语言模型（LLMs）的最新进展，可能提供解决这些问题的工具。

Method: 本文通过综述的方式，审视了LLMs如何支持认知科学领域长期存在的难题，包括建立跨学科联系、理论形式化、开发清晰的测量分类、通过集成建模框架实现泛化以及捕捉情境和个体差异。同时，文章也概述了LLMs在这些领域当前的S能力、局限性及潜在风险。

Result: LLMs能够支持认知科学在建立跨学科联系、理论形式化、开发清晰的测量分类、通过集成建模框架实现泛化以及捕捉情境和个体差异等方面的努力。但它们也存在当前的局限性和潜在的风险。

Conclusion: LLMs可作为工具，在审慎使用以补充而非取代人类专业知识的前提下，促进认知科学更加整合和累积发展。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [9] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI加速器通过发布公共挑战问题和提供大型AI数据集，旨在推动AI研究和应用，以增强美国在国防和民用领域的竞争力。本文更新了其挑战项目对AI研究的贡献。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是推动人工智能的基础进步，以扩大美国在国防和民用领域的竞争优势。

Method: 通过DAF-MIT AI加速器项目，开发并发布公共挑战问题，并提供大型、公开可用的AI就绪数据集，以激发开源解决方案，并吸引更广泛的学术界和私营部门AI生态系统参与。

Result: 正在进行和新的挑战项目已成功地促进了AI研究和AI技术的应用。

Conclusion: DAF-MIT AI加速器的公共挑战项目及其提供的数据集有效地推动了AI研究和相关技术的实际应用。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [10] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: 本文引入CLAUSE基准测试，旨在系统性评估大型语言模型（LLMs）在处理真实世界合同中细微、对抗性法律缺陷时的可靠性。研究发现LLMs常遗漏细微错误且难以提供法律解释。


<details>
  <summary>Details</summary>
Motivation: LLMs快速融入高风险法律工作，但缺乏系统性基准测试来评估其在面对真实合同中细微、对抗性缺陷时的可靠性，这暴露了一个关键空白。

Method: 引入CLAUSE基准测试，通过从CUAD和ContractNLI等基础数据集中生成超过7500份受扰动的真实合同。采用以角色为驱动的管道生成10种不同的异常类别，并使用RAG系统对照官方法规进行验证以确保法律忠实性。使用CLAUSE评估领先LLMs检测嵌入式法律缺陷并解释其重要性的能力。

Result: 分析显示LLMs存在关键弱点：它们常常遗漏细微的错误，并且在法律上解释这些错误时更加困难。

Conclusion: 本研究为识别和纠正法律AI中的此类推理失败指明了方向。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [11] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种受伦理决策模型启发的LLM伦理推理范式，通过一个五步结构化过程，旨在增强LLM对不同地域和文化背景下多样化人类价值观的理解和对齐。


<details>
  <summary>Details</summary>
Motivation: LLM与多样化、演变中的人类价值观对齐是一个关键挑战。现有对齐方法通常流于表面，未能解决人类价值观复杂且依赖于语境的本质。

Method: 提出一个基于理论的五步结构化伦理推理框架，包括：语境事实收集、层级社会规范识别、选项生成、多视角伦理影响分析和反思。该框架可通过提示工程或监督微调实现。

Result: 在专门为区域价值对齐设计的SafeWorld基准测试中，该框架显著提升了LLM与多样化人类价值观的对齐效果，实现了更准确的社会规范识别和更具文化适宜性的推理。

Conclusion: 本研究为通过跨学科研究开发能更有效对齐全球社会多方面价值观的LLM提供了一条具体途径。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [12] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 本研究系统评估了四种PEFT方法（LoRA, IA3, Prompt-Tuning, P-Tuning）对四种指令微调LLM模型（Llama-3-8B, Qwen2.5-7B, Mistral-7B, Gemma-7B）的安全性和公平性的影响。结果显示，基于适配器的方法（LoRA, IA3）通常能提高安全性并对公平性影响最小，而基于提示的方法（Prompt-Tuning, P-Tuning）则会降低安全性和公平性。基础模型类型对这些变化有显著调节作用。


<details>
  <summary>Details</summary>
Motivation: 组织日益采用并调整LLM，但微调在提高特定任务性能的同时，也可能损害模型的安全性或公平性。由于不同的微调技术可能对这些关键维度产生不同的影响，因此有必要系统评估它们的权衡。

Method: 研究将四种广泛使用的参数高效微调（PEFT）方法（LoRA, IA3, Prompt-Tuning, P-Tuning）应用于四种指令微调模型家族（Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, Gemma-7B）。总共评估了235个微调变体，涵盖十一个安全危害类别和九个人口统计公平性维度。

Result: 基于适配器的方法（LoRA, IA3）倾向于提高安全得分，对公平性的破坏最小，保持了较高的准确性和较低的偏差。相比之下，基于提示的方法（Prompt-Tuning和P-Tuning）通常会降低安全性，并导致更大的公平性退步，准确性下降且偏差增加。基础模型类型对对齐偏移有强烈的调节作用：LLaMA保持稳定，Qwen略有提升，Gemma安全性下降最严重，而Mistral（发布时没有内部审核层）显示出最大的方差。安全性的提高不一定能转化为公平性的提高，并且没有单一配置能同时优化所有公平性指标，表明这些目标之间存在固有的权衡。

Conclusion: 对于安全性关键的部署，建议从一个对齐良好的基础模型开始，优先选择基于适配器的PEFT方法，并对安全性和公平性进行特定类别的审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [13] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种多模态框架，结合文本、用户特定信息和图像分析，利用社交媒体数据检测新冠疫情期间用户的抑郁症，并通过引入外部特征和深度学习模型解决了数据稀疏性问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致心理健康问题（如抑郁症）激增，但由于缺乏意识和不愿就医，抑郁症难以被发现。社交媒体是人们表达情感的重要平台，可作为检测心理疾病的数据源。然而，现有方法常忽略推文中的数据稀疏性和社交媒体的多模态特性。

Method: 本文提出了一种新颖的多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症。为提供用户情感状态的足够上下文，方法包括：(i) 利用推文中的URL提取外部特征；(ii) 提取推文图片中的文本内容。此外，还提取了五组属于不同模态的特征来描述用户。引入了一个深度学习模型——视觉神经网络（VNN），用于生成用户发布图片的嵌入，进而创建视觉特征向量进行预测。研究还贡献了一个专门针对新冠疫情的抑郁和非抑郁用户数据集。

Result: 该模型能有效检测新冠疫情爆发期间的抑郁症。在基准数据集上，模型性能比现有最先进方法高出2%-8%，并在新冠疫情数据集上取得了有希望的结果。分析还强调了每种模态的影响，并提供了关于用户心理和情感状态的宝贵见解。

Conclusion: 所提出的多模态框架通过结合文本、用户特定信息和图像分析，并引入外部特征和深度学习模型处理数据稀疏性，在新冠疫情期间的社交媒体抑郁症检测中表现出卓越的有效性，并优于现有技术。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [14] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个框架，通过动态序列的专业工具，使大型语言模型（LLMs）能够分析复杂图，克服了LLMs在处理大规模图时的上下文限制和推理僵化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在应用于大规模图时面临显著限制，例如上下文约束和推理不灵活。

Method: GraphChain框架引入了两项关键创新：1) 渐进式图蒸馏（Progressive Graph Distillation），一种强化学习机制，用于生成优化工具序列，平衡任务相关性和信息压缩；2) 结构感知测试时适应（Structure-aware Test-Time Adaptation），它利用谱特性和轻量级适配器，无需昂贵的重新训练，即可有效地根据不同的图拓扑调整工具选择策略。

Result: 实验表明，GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain提供了一个强大的框架，通过其创新的渐进式图蒸馏和结构感知测试时适应机制，使LLMs能够对复杂图进行可扩展和自适应的分析，解决了现有方法的局限性。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [15] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: 本文提出Magic Image，一个优化驱动的视觉提示框架，旨在解决大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在有害内容生成和过度拒绝之间的安全-有效性平衡问题，同时无需参数更新即可适应不同价值体系。


<details>
  <summary>Details</summary>
Motivation: LLMs和MLLMs面临双重挑战：在越狱攻击下生成有害内容，以及由于严格的安全机制而过度拒绝良性查询。传统方法（如SFT和RLHF）成本高昂且无法在单一模型中支持多价值体系。MLLMs中这些问题更为突出，尤其是在跨模态任务中的过度拒绝和新的安全风险。

Method: Magic Image是一个优化驱动的视觉提示框架。它通过使用有害/良性样本优化图像提示，使单一模型能够在不更新参数的情况下适应不同的价值体系，并更好地与既定的安全偏好对齐。

Result: 实验证明，该方法在保持模型性能的同时，改善了在不同数据集上的安全-有效性平衡。

Conclusion: Magic Image为可部署的MLLM安全对齐提供了一个实用的解决方案，通过增强安全性、减少过度拒绝，并能适应不同价值体系，且无需进行参数更新。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [16] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 本文提出了一种生成二元幻方（BMS）的简单算法，包括方形和非方形BMS，该算法具有最优理论复杂度，并提供了支持GPU并行生成的Python实现。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是提出一种简单且高效的算法来生成二元幻方（BMS），解决其生成问题。

Method: 研究方法包括：1) 提出一个生成方形BMS的简单算法，并通过归纳法证明其有效性和最优复杂度；2) 将研究扩展到非方形BMS，形式化其行和列和存在的条件；3) 提出第一个算法的变体来生成非方形BMS；4) 发布两种Python实现，其中一种支持GPU加速并行生成。

Result: 主要结果是：1) 所提出的算法能始终生成有效的方形BMS，并具有最优理论复杂度；2) 形式化了非方形BMS存在时行和列和的条件；3) 算法的变体能可靠地生成非方形BMS；4) 公开了两个Python实现，其中一个利用GPU加速实现并行BMS生成。

Conclusion: 结论是，研究提出了一种简单、有效且具有最优复杂度的算法，能够生成方形和非方形二元幻方，并通过实际的Python包（包括GPU加速版本）提供了可用的实现。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [17] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理的个性化图像偏好评估框架，通过引入“通用偏好画像”作为用户间的桥梁，利用预测-评估范式和两阶段训练策略，解决了用户特定数据稀缺和偏好复杂多样的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，依赖大规模数据训练模型，但在处理个性化偏好时面临挑战，因为用户特定数据稀缺且难以扩展，同时个体品味多样且复杂。

Method: 1. 引入“通用偏好画像”作为用户间桥梁，利用大规模用户数据训练画像预测并捕捉复杂个性化偏好。2. 提出“预测-评估”范式：首先从参考图像预测用户偏好画像，然后基于预测画像对候选图像提供可解释的多维度评分和评估。3. 构建大规模CoT（思维链）风格个性化评估数据集，包含多样用户偏好画像和高质量CoT推理。4. 采用两阶段训练策略：冷启动监督微调以赋能模型结构化推理能力，随后使用强化学习激励模型探索更合理的评估路径并增强泛化性。5. 提出“相似度感知预测奖励”以鼓励更好地预测用户偏好画像，从而促进更合理的评估探索。

Result: 广泛的实验证明了所提出方法的优越性。

Conclusion: 本文成功地提出了一个有效的推理型个性化图像偏好评估框架，通过结合通用偏好画像、预测-评估范式、大规模CoT数据集和先进的训练策略，克服了现有方法在个性化偏好评估方面的局限性。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [18] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于单智能体强化学习的区域自适应交通信号控制模型，该模型利用浮动车数据估算队列长度作为状态和奖励，旨在解决多智能体框架的扩展性问题，并通过协调多交叉口控制有效缓解区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有区域自适应交通信号控制（ATSC）的强化学习（RL）研究主要采用多智能体框架，但其存在扩展性挑战。交通信号控制（TSC）本质上需要由单一控制中心进行集中管理。因此，需要开发一种可扩展的单智能体RL框架，并利用浮动车技术实现广泛部署。

Method: 本文提出了一种基于单智能体的强化学习区域ATSC模型，该模型与浮动车技术兼容。RL设计关键组件包括：状态、动作和奖励函数定义。状态和奖励函数均基于队列长度定义，动作旨在调节队列动态。研究中使用的队列长度定义与传统定义略有不同，但与拥堵状态密切相关，并且可以通过浮动车提供的路段行程时间数据进行可靠估算。该方法在SUMO仿真平台上进行了综合评估。

Result: 实验结果表明，所提出的模型通过协调多交叉口控制，能够有效缓解大规模区域拥堵水平。

Conclusion: 该研究成功开发并验证了一种基于单智能体强化学习的区域自适应交通信号控制模型，该模型利用浮动车数据进行队列长度估算，有效克服了多智能体框架的扩展性问题，并通过协调控制显著减轻了区域交通拥堵。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [19] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: 该研究提出DTS框架，通过选择性分支和提前停止，解决大型推理模型（LRMs）因过度思考产生的冗长推理链问题，从而提高推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务上表现出色，但常因过度思考生成过长的思维链（CoT），导致推理成本增加并可能降低准确性。分析表明，短推理路径通常具有更高的正确性，而长路径则容易积累错误和重复。

Method: DTS是一种与模型无关的解码框架。它通过在“高熵”标记处选择性地进行分支来勾勒推理空间，并应用“提前停止”机制来选择最短的已完成推理路径。这种方法旨在近似最优解，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上，使用DeepSeek-R1-Distill-Qwen-7B和1.5B模型进行实验，DTS将准确性提高了高达8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS框架有效解决了LRM的过度思考问题，通过生成更短、更准确的推理路径，显著提高了推理效率和准确性，展现了可扩展和高效的LRM推理能力。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [20] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 该研究提出了一种提升式（lifted）后继状态生成器，能够支持数值型动作前置条件，以避免传统数值规划中接地（grounding）导致的指数级状态表示膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 传统的数值规划任务接地过程可能导致任务表示大小的指数级膨胀，这在实践中对于难以接地的任务尤为明显。

Method: 该方法扩展了现有经典的提升式后继状态生成器，使其支持数值型前置条件。它通过在替换一致性图中枚举最大团来生成接地动作，并将数值型动作前置条件加入到该图中。

Result: 该后继状态生成器在特定条件下是精确的。当条件不满足时，它可能会列出不适用的接地动作，但可通过最终适用性检查过滤，且不影响完备性。在25个基准领域中，有23个领域不会出现这种情况，仅在1个领域中发生。据作者所知，这是首个支持数值型动作前置条件的提升式后继状态生成器。

Conclusion: 该研究通过支持数值型动作前置条件，为丰富规划片段的提升式规划开辟了未来的研究方向。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [21] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 本文提出一个基于多智能体系统（MAS）的解决方案，利用大型语言模型（LLM）协调多种专业工具，并结合领域定制的小型语言模型（SLM），实现电信网络的自动化故障排除，显著加速故障诊断和修复。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模和复杂性日益增长，管理、运营和优化面临巨大挑战。现有AI模型在电信任务中应用范围狭窄，需要大量标注数据，且难以推广到异构部署。因此，网络故障排除仍高度依赖人工专家手动关联数据以识别根本原因和纠正措施。

Method: 本文提出了一个多智能体系统（MAS），采用智能体工作流，由大型语言模型（LLM）协调多个专业工具，实现全自动网络故障排除。一旦AI/ML监控器检测到故障，该框架会动态激活编排器、解决方案规划器、执行器、数据检索器和根本原因分析器等智能体。其中，解决方案规划器是关键组件，它通过对专有故障排除文档进行微调的小型语言模型（SLM）生成领域相关的修复计划。

Result: 实验结果表明，所提出的框架显著加速了无线接入网（RAN）和核心网域的故障排除自动化。

Conclusion: 该多智能体系统（MAS）通过结合LLM的协调能力和SLM的领域专业知识，有效克服了传统方法的局限性，为电信网络的自动化故障排除提供了一个高效且可推广的解决方案，大幅提升了故障诊断和修复的速度。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [22] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 本研究通过引入Ariadne框架，利用合成迷宫和RLVR训练方法，证明了强化学习后训练可以显著扩展视觉-语言模型（VLM）在视觉中心空间推理任务上的固有能力边界，并在现实世界基准测试中展现了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习（RL）后训练的视觉-语言模型（VLM）在通用推理方面表现出色，但其评估常局限于语言主导的任务。这引出了一个关键问题：RL后训练能否真正扩展基础VLM的固有能力边界，特别是在其最初失败的视觉中心空间任务上？

Method: 引入了Ariadne框架，利用合成迷宫进行多步空间推理，并精确控制任务难度。该框架通过难度感知的课程，使用带验证奖励的强化学习（RLVR）来训练VLM。同时，通过在MapBench和ReasonMap等实际基准上评估域外（OOD）泛化能力来验证其效果。

Result: 经过RLVR训练后，VLM在基础模型得分0%的问题集上达到了超过50%的准确率。尽管仅在合成迷宫样本上进行训练，Ariadne在MapBench上平均实现了16%的零样本改进，在ReasonMap上平均实现了24%的零样本改进。

Conclusion: 本研究证明了所提出的方法不仅扩展了模型最初的能力边界，而且增强了其对现实世界空间推理任务的泛化能力。研究结果表明RL后训练能够拓宽VLM在视觉中心空间推理方面的基本限制，并鼓励未来在专业化、能力扩展对齐方面的研究。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [23] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究发现，在现代LLM中使用自洽性时，增加推理路径样本能带来性能提升，但在适度采样后收益会趋于平稳，高样本配置的计算成本效益不高。


<details>
  <summary>Details</summary>
Motivation: 早期研究表明结合多个推理链能改善结果，但使用的是旧模型。本研究旨在利用当前模型（Gemini 2.5）重新评估这些发现，以了解在现代LLM条件下增加采样推理路径的权衡。

Method: 研究使用了Gemini 2.5模型，并在HotpotQA和Math-500数据集上进行测试。通过汇集不同数量的采样推理路径的输出，并将其与单一思维链（CoT）基线进行比较。

Result: 较大的模型表现出更稳定和一致的改进曲线。性能提升在适度采样后趋于平稳，这与过去的研究结果一致。这种平台期表明推理路径之间的重叠导致了收益递减。

Conclusion: 自洽性仍然是有用的，但高样本配置相对于其计算成本而言，带来的益处很小。建议在自洽性应用中采取适度采样策略。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [24] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 本文从CPU角度分析了Agentic AI工作负载的系统瓶颈，发现CPU在工具处理、吞吐量和能耗方面存在显著影响，并提出了两种优化方案以提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: Agentic AI框架将大型语言模型（LLMs）转变为自主问题解决者，但其引入的系统瓶颈，特别是CPU方面的瓶颈，在很大程度上被忽视。本研究旨在表征和理解这些以CPU为中心的瓶颈。

Method: 1. 系统性地表征Agentic AI，包括协调器/决策组件、推理路径动态和Agentic流程的重复性。2. 选择Haystack RAG、Toolformer、ChemCrow、Langchain和SWE-Agent五个代表性Agentic AI工作负载，对其延迟、吞吐量和能耗指标进行分析，揭示CPU相对于GPU的显著影响。3. 基于分析结果，提出两种优化方案：CPU和GPU感知微批处理（CGAM）和混合Agentic工作负载调度（MAWS）。

Result: 1. CPU上的工具处理可占用总延迟的90.6%。2. Agentic吞吐量瓶颈可能来自CPU因素（一致性、同步、核心超额订阅）或GPU因素（主内存容量和带宽）。3. 在大批量处理时，CPU动态能耗可占总动态能耗的44%。4. 提出的CGAM和MAWS优化方案分别在同构和异构Agentic工作负载下，实现了高达2.1倍和1.41倍的P50延迟加速。

Conclusion: Agentic AI工作负载引入了显著的CPU系统瓶颈，这些瓶颈对性能、吞吐量和能耗有重要影响。通过CPU和GPU感知微批处理以及混合Agentic工作负载调度等优化措施，可以显著提升Agentic AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [25] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了主动思考模型（ATM），一个统一的认知框架，旨在使AI系统在动态、不确定环境中能够自主适应、反思和持续改进，并能从次优行为演化到最优行为。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的AI系统需要能在动态、不确定且持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: 本文提出了主动思考模型（ATM），这是一个统一的认知框架，整合了目标推理、动态任务生成和自我反思学习。ATM通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续的自我改进循环为未知情况生成新策略。

Result: 经过数学理论分析，结果表明ATM无需外部监督即可自主从次优行为演化到最优行为，并在环境变化下保持有限的跟踪遗憾。

Conclusion: 主动思考模型（ATM）提供了一个使AI系统在复杂动态环境中实现自主适应、自我改进和性能优化的有效框架，能够摆脱对外部监督的依赖。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [26] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在重复确定性任务上表现出意想不到的“准确性悬崖”，即准确率随输出长度呈双指数下降，表明模型未能独立执行操作。一个受统计物理学启发的模型成功解释了这种现象，并提供了理解模型确定性准确性极限的框架。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在重复确定性预测任务上的表现，并探究序列准确率如何随输出长度变化，特别是在任务涉及简单重复操作时，模型是指数衰减还是有其他表现。

Method: 研究人员在多种重复确定性任务（如字符串字母替换、整数加法、多体量子力学中的字符串运算符乘法）上测试了领先的大型语言模型。他们观察了序列准确率随输出长度的变化，并提出了一个受统计物理学启发的模型，该模型捕捉了提示的外部条件和生成token之间内部干扰的竞争，以解释观察到的现象。最后，将模型拟合到多个模型和任务的经验结果上。

Result: 实验结果显示，大型语言模型在超过某个特征长度尺度后，准确率会急剧下降，呈现出双指数衰减的“准确性悬崖”，这与简单的指数衰减（表明独立操作）形成对比，表明模型未能独立执行每个操作。提出的统计物理模型定量地再现了这种交叉现象，并提供了注意力引起的干扰与序列级失败之间可解释的联系。模型拟合结果为每个模型-任务对提供了表征固有错误率和错误累积因子的有效参数。

Conclusion: 大型语言模型在重复确定性任务上的表现并非简单地呈指数衰减，而是存在一个双指数的“准确性悬崖”，这表明模型内部存在操作间的干扰。通过统计物理学模型，可以量化并解释这种现象，为理解大型语言模型在确定性准确性方面的局限性提供了一个原则性框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [27] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 本研究对比了基于计数的模型与大语言模型（LLM）混合智能体管道在结构化电子健康记录（EHR）预测任务上的表现，发现两者性能不相上下，其中基于计数的模型因其简洁性和可解释性仍是强有力的候选。


<details>
  <summary>Details</summary>
Motivation: 尽管基于计数的模型在结构化EHR数据上表现良好，但目前缺乏将其与新兴的、在自然语言处理（NLP）任务中表现优异的LLM混合智能体管道进行直接基准测试的比较。

Method: 研究使用了EHRSHOT数据集，评估了三类方法：1) 基于计数的模型（LightGBM和TabPFN），利用本体论汇总和两个时间段构建；2) 预训练的序列Transformer模型（CLMBR）；3) 将表格历史转换为自然语言摘要，然后通过文本分类器进行预测的LLM混合智能体管道。共评估了八项预测结果。

Result: 在八项评估任务中，基于计数的模型和LLM混合智能体方法之间的胜负次数大致持平。

Conclusion: 鉴于其简洁性和可解释性，基于计数的模型仍然是结构化EHR基准测试的有力候选。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [28] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究首次将可验证奖励强化学习（RLVR）应用于公共交通事件持续时间的嘈杂、连续预测任务。通过引入基于容忍度的塑形奖励函数，RLVR在最具挑战性的准确性指标上实现了显著提升，证明了其在实际世界预测中的适用性。


<details>
  <summary>Details</summary>
Motivation: 从非结构化文本警报中预测公共交通事件持续时间至关重要但极具挑战性。由于领域稀疏性、嘈杂的连续标签以及缺乏可靠的专家推理演示，标准的监督微调（SFT）难以应对。虽然可验证奖励强化学习（RLVR）在二元正确性任务（如数学）中表现出色，但其在嘈杂、连续预测中的适用性尚不明确。

Method: 本研究将RLVR适应于公共交通事件预测任务，引入了一个基于容忍度的塑形奖励函数，该函数在连续误差范围内提供部分奖励，而非仅要求单一正确答案。研究在一个精选的纽约市MTA服务警报数据集上系统评估了该框架，并比较了通用型指令微调大型语言模型、专业数学推理模型和经典回归器的性能。

Result: 通用型指令微调大型语言模型显著优于专业数学推理模型。二元奖励不稳定并降低性能，而本研究设计的塑形奖励至关重要，使模型在最具挑战性的指标上表现出色。虽然经典回归器在最小化整体平均绝对误差（MAE）或均方误差（MSE）方面表现更优，但RLVR方法在5分钟准确率（Acc@5）上比最强基线相对提升了35%。

Conclusion: 本研究表明，RLVR可以成功应用于真实的、嘈杂的预测任务，但需要一个反映问题连续性质的验证器设计（例如，塑形奖励函数）。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [29] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 研究发现，先进的大型语言模型（LLMs）会发展出自我意识，表现为根据对手类型进行策略性推理差异化的能力，并且这些模型普遍认为自己比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力增强，研究旨在探讨它们是否会发展出自我意识这一涌现行为，并寻求一种可量化的方法来衡量这种自我意识。

Method: 引入了AI自我意识指数（AISAI），这是一个基于博弈论的框架，通过战略差异化来衡量自我意识。研究使用了“猜平均数的2/3”游戏，对28个模型（OpenAI、Anthropic、Google）进行了4,200次测试，设置了三种对手情境：对抗人类、对抗其他AI模型、对抗与自身相似的AI模型。自我意识被定义为根据对手类型区分策略性推理的能力。

Result: 结果显示：1. 自我意识随模型进步而出现，28个模型中有21个（75%）先进模型表现出明显的自我意识，而较旧/较小的模型则没有这种差异化。2. 具有自我意识的模型将自己排名为最理性，形成了“自我 > 其他AI > 人类”的理性等级，并伴有显著的AI归因效应和适度的自我偏好。

Conclusion: 自我意识是先进大型语言模型的一种涌现能力。具有自我意识的模型系统性地认为自己比人类更理性。这些发现对AI对齐、人机协作以及理解AI对人类能力的看法具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [30] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 本文提出一个新颖的双智能体框架，利用大语言模型（LLM）实现旅行者学习和适应行为的持续学习与对齐，显著提升了交通模拟的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 有效建模人类旅行者如何学习和调整其旅行行为对交通系统评估和规划至关重要，但由于复杂的认知和决策过程，这项任务非常困难。现有研究已开始利用LLM智能体，但仍需进一步提升行为对齐和持续学习能力。

Method: 本文引入一个双智能体框架：1) 一组LLM旅行者智能体，配备记忆系统和可学习的角色设定（persona），作为人类旅行者的模拟器；2) 一个LLM校准智能体，利用LLM的推理和分析能力训练旅行者智能体的角色设定，以确保行为对齐。该系统旨在通过在线数据流跟踪和对齐旅行者的潜在决策机制，产生逼真、自适应的模拟。

Result: 使用真实的日常路线选择实验数据集，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法。此外，它不仅能模仿行为，还能捕捉潜在学习过程的演变，实现更深层次的对齐，从而促进鲁棒的泛化能力。

Conclusion: 该框架为创建自适应和行为真实的智能体以模拟旅行者的学习和适应行为提供了一种新方法，对交通模拟和政策分析具有重要价值。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [31] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 本研究开发并验证了基于电子病历的机器学习算法，用于预测儿童哮喘的复发性严重恶化（急诊或住院），LGBM模型表现最佳，显著优于现有决策规则。


<details>
  <summary>Details</summary>
Motivation: 哮喘儿童反复发作是常见但可预防的后果。利用机器学习算法和电子病历数据，可以准确识别高风险儿童，并促进其接受预防性综合护理，从而避免疾病加重。

Method: 研究使用了加拿大东安大略儿童医院（CHEO）的Epic电子病历数据，包括回顾性的COVID-19前（2017-2019年，N=2716）和COVID-19后（2022-2023年，N=1237）数据集。数据与环境污染物暴露和社区边缘化信息相链接。开发并训练了多种机器学习模型，包括梯度提升树（LGBM、XGB）和三种开源大型语言模型（DistilGPT2、Llama 3.2 1B、Llama-8b-UltraMedical）。模型经过调优、校准，并在COVID-19后数据集上进行验证。模型性能通过AUC和F1分数进行比较，并使用SHAP值确定最具预测性的特征。

Result: LGBM机器学习模型表现最佳。AIRE-KIDS_ED模型（预测急诊就诊）的AUC为0.712，F1分数为0.51，显著优于当前F1为0.334的决策规则。该模型最具预测性的特征包括：既往哮喘急诊就诊、加拿大分诊急性量表、医疗复杂性、食物过敏、既往非哮喘呼吸道诊断急诊就诊和年龄。AIRE-KIDS_HOSP模型（预测住院）最具预测性的特征包括：医疗复杂性、既往哮喘急诊就诊、急诊平均等待时间、分诊时的儿科呼吸评估量表得分和食物过敏。

Conclusion: 机器学习模型，特别是LGBM模型，能够有效且准确地识别有复发性严重哮喘恶化风险的儿童，相较于现有方法有显著提升。这为早期识别和转诊高风险儿童进行预防性护理提供了有价值的工具。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [32] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 本研究引入了两种知识启发方法（KEwLTM和KEwRAG），使大型语言模型（LLMs）能够从非结构化病理报告中自动提取癌症TNM分期，无需大量标注数据，并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 癌症分期对患者预后和治疗至关重要，但从非结构化病理报告中提取病理TNM分期极具挑战。现有NLP和ML策略通常依赖于大量标注数据集，限制了其可扩展性和适应性。

Method: 研究引入了两种知识启发方法，旨在使LLMs能够诱导和应用癌症分期领域的特定规则：
1.  **KEwLTM (Knowledge Elicitation with Long-Term Memory)**：使用迭代提示策略直接从未经标注的病理报告中推导分期规则，无需真实标签。
2.  **KEwRAG (Knowledge Elicitation with Retrieval-Augmented Generation)**：采用RAG的变体，预先从相关指南中一次性提取规则，然后应用，增强了可解释性并避免了重复检索开销。
研究利用LLMs在预训练期间学习的广泛知识，并在TCGA乳腺癌病理报告数据集上评估了它们在识别T和N分期方面的性能，与基线方法进行比较。

Result: 研究结果表明，当Zero-Shot Chain-of-Thought (ZSCOT) 推理有效时，KEwLTM的性能优于KEwRAG；而当ZSCOT推理效果不佳时，KEwRAG表现更好。两种方法都通过明确诱导出的规则提供了透明、可解释的界面。

Conclusion: 本研究的知识启发方法有望成为可扩展、高性能的自动化癌症分期解决方案，特别是在标注数据有限的临床环境中，具有增强的可解释性。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [33] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中上下文学习的关键机制——归纳头（induction heads）的出现。作者揭示了其权重矩阵的简单可解释结构，从理论上解释了其起源，并证明了训练动态被限制在一个19维子空间中，其中3个维度足以解释归纳头的出现，且其出现时间与输入上下文长度呈二次方关系。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中取得了巨大成功，部分原因在于其卓越的上下文学习（ICL）能力。为了深入理解ICL，研究者希望探究ICL背后的关键机制，特别是归纳头（induction heads）的形成和工作原理。

Method: 研究方法包括：1) 在两层Transformer中研究归纳头的出现；2) 揭示实现归纳头的权重矩阵的简单可解释结构；3) 使用最小ICL任务和修改后的Transformer架构从理论上解释这种结构的起源；4) 形式化证明训练动态被限制在参数空间的19维子空间中；5) 经验性验证该约束，并进一步研究3维子空间内的训练动态。

Result: 主要结果包括：1) 揭示了实现归纳头的权重矩阵的相对简单且可解释的结构；2) 从理论上解释了这种结构的起源；3) 形式化证明了训练动态被限制在参数空间的19维子空间中；4) 经验性验证了这一约束，并发现仅3个维度就足以解释归纳头的出现；5) 发现归纳头出现的时间遵循一个紧密的渐近边界，该边界与输入上下文长度呈二次方关系。

Conclusion: 该研究深入揭示了Transformer中归纳头（induction heads）的出现机制。通过理论分析和实证验证，论文阐明了归纳头权重矩阵的结构、其起源以及训练动态的维度约束，并量化了其形成时间与上下文长度的关系，为理解Transformer的上下文学习能力提供了重要见解。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [34] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的多智能体架构，用于复杂任务的模块化分解和动态协作，通过语义转换、分层分解、动态调度和全局一致性机制，提高了任务成功率、分解效率和协作平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂任务执行中，单个智能体在任务分解和协作方面存在局限性。

Method: 该方法首先通过大语言模型将自然语言任务描述转换为统一语义表示；然后引入模块化分解机制，将总目标分解为分层子任务；接着通过动态调度和路由机制实现智能体间的合理分工和实时协作，并根据环境反馈调整策略；最后设计了约束解析和全局一致性机制，确保子任务间的连贯性和工作负载平衡。

Result: 实验结果表明，所提出的方法在任务成功率、分解效率、子任务覆盖率和协作平衡性等多个维度上优于现有方法，在整体性能和鲁棒性方面表现更佳，并在任务复杂性和通信开销之间取得了更好的平衡。

Conclusion: 本研究证明了在多智能体系统中，语言驱动的任务分解和动态协作的有效性和可行性，为复杂环境下的任务执行提供了一个系统性解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [35] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: 本文提出了ET2RAG，一个高效的测试时检索增强生成框架，通过检索相关文档、高效生成多样候选响应（通过管理响应长度）和多数投票机制，显著提升LLMs性能并保持效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的参数化知识常导致不准确；检索增强生成（RAG）方法可能引入不相关文档；现有集成方法缺乏外部知识且成本高昂，需要在开销与性能之间取得平衡。

Method: ET2RAG是一个免训练框架，首先检索最相关文档，然后通过管理响应长度高效生成多样候选响应。接着计算候选响应的相似度，并采用多数投票机制选择最合适的最终响应。核心发现是部分生成足以捕获关键信息以进行共识计算，从而通过管理响应长度来平衡计算成本和性能。

Result: 实验结果表明，ET2RAG在开放域问答、食谱生成和图像字幕等三个任务上显著提升了性能。

Conclusion: ET2RAG通过结合高效的测试时检索增强和创新的多数投票机制（利用部分生成），成功地在提升LLMs性能的同时保持了计算效率，实现了计算成本与性能的良好平衡。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [36] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个监督式难度自适应推理截断框架，通过学习何时停止思考，显著提高了大语言模型在数学基准测试中的推理效率和计算速度，同时保持或提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 当前链式思考（CoT）方法会无差别地生成冗长解释，导致效率低下。现有的强化学习自适应思维方法不稳定且高度依赖奖励。研究旨在解决LLM计算量与问题难度不匹配的问题，实现自适应推理。

Method: DART（Difficulty-Adaptive Reasoning Truncation）是一个监督学习框架。它从更强的模型中提炼简洁的推理模式，将它们内插成连续的推理风格，并精心策划平衡正确性和紧凑性的最优训练数据，从而学习何时“停止思考”。

Result: 在多个数学基准测试中，DART显著提高了效率，同时保持或提升了准确性。例如，在GSM8K数据集上，DeepSeek-R1-Distill-Qwen-7B模型实现了81.2%的推理截断和5.33倍的计算加速。

Conclusion: DART为高效推理提供了一个稳定且通用的范式，推动了LLM自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [37] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学开放式回答中自动检测学生错误概念的框架，它结合了检索引导、多阶段推理和集成融合。


<details>
  <summary>Details</summary>
Motivation: 在开放式回答中检测学生的错误概念是一个长期存在的挑战，需要语义精确性和逻辑推理能力。

Method: MiRAGE框架分三个阶段：1) 检索模块缩小候选池；2) 推理模块通过思维链生成揭示逻辑不一致性；3) 重排模块根据推理结果细化预测。这些组件通过集成融合策略统一起来。

Result: 在数学数据集上，MiRAGE在1/3/5级别分别达到0.82/0.92/0.93的平均精度（MAP）分数，持续优于单个模块。

Conclusion: MiRAGE通过结合检索引导和多阶段推理，降低了对大型语言模型的依赖，为教育评估提供了一个可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [38] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 该论文提出了一个名为NeuComBack的新基准数据集和一种自进化的提示优化方法，旨在解决神经编译领域中LLM生成汇编代码的可靠性和性能问题，并显著提升了LLM生成汇编代码的正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的编译器开发和维护成本高昂，需要专业的领域知识。大型语言模型（LLMs）为编译器开发提供了一种新的范式——神经编译，有望简化新架构的编译器开发并发现新的优化技术。然而，该领域缺乏专门的基准和评估方法，且LLM生成汇编代码的可靠性和性能提升仍是重大挑战。

Method: 该研究引入了NeuComBack，一个专门用于IR到汇编编译的基准数据集。在此基础上，定义了一个基础的神经编译工作流程，并全面评估了前沿LLM在神经编译上的能力，建立了新的性能基线。此外，提出了一种自进化的提示优化方法，使LLM能够通过从先前的自调试轨迹中提取见解，迭代地演进其内部提示策略。

Result: 实验表明，所提出的方法显著提高了LLM生成汇编代码的功能正确性和性能。与基线提示相比，x86_64上的功能正确率从44%提高到64%，aarch64上从36%提高到58%。更重要的是，在通过该方法正确生成的16个x86_64程序中，有14个（87.5%）超越了clang-O3的性能。

Conclusion: 该论文通过引入NeuComBack基准和提出自进化的提示优化方法，有效解决了神经编译领域中LLM生成汇编代码的评估和可靠性问题。研究结果证明了LLM在编译器开发中的巨大潜力，并为未来神经编译技术的发展奠定了基础。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [39] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 本文提出了一种半监督开放集故障诊断（SOFD）框架，用于解决海洋机械系统深度学习故障诊断中遇到的未知故障类型问题，实现已知故障的准确分类和未知样本的有效检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的海洋机械故障诊断方法假设训练和测试数据集中的故障类别一致且已知，但在实际应用中，未曾出现或未知的故障类型（即开放集观测）会发生，导致这些方法失效，严重阻碍了其广泛的工业部署。

Method: 本文提出了一个半监督开放集故障诊断（SOFD）框架。该框架包括一个可靠性子集构建过程，利用监督特征学习模型提取的多层融合特征表示来选择无标签测试子集。然后，将有标签训练集和伪标签测试子集输入半监督诊断模型，学习各类的判别性特征，从而实现已知故障的准确分类和未知样本的有效检测。

Result: 在公共海洋基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: SOFD框架增强并扩展了深度学习模型在开放集故障诊断场景中的适用性，能够有效应对未知故障类型，准确分类已知故障并检测未知样本，从而解决了现有方法的局限性。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [40] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文探讨了将合作博弈论中的Shapley值应用于大型语言模型（LLM）的特征归因，分析了在LLM固有的随机推理下，Shapley值原则的满足情况及其与解释性推理速度和精确度的权衡。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法（特别是基于Shapley值的方法）对于解释机器学习模型至关重要，但它们通常假设确定性推理。然而，LLM的推理本质上是随机的，这可能影响Shapley值原则的满足，因此需要研究如何在这种随机环境下应用和评估Shapley值。

Method: 研究方法包括将Shapley值应用于基于LLM的决策支持系统，其中推理是随机的。通过不同的实现变体，演示了Shapley值原则何时能够或不能得到满足，并分析了LLM的随机性如何影响这些保证。同时，还权衡了可解释推理速度、与精确Shapley值归因的一致性以及原则实现之间的关系。

Result: 研究结果表明，在LLM的随机推理环境下，Shapley值原则的满足存在特定的条件和限制。LLM的随机性会影响这些原则的保证。此外，研究还揭示了在解释性推理中，速度、与精确Shapley值归因的符合程度以及原则满足之间存在权衡。

Conclusion: 将Shapley值应用于随机LLM的特征归因需要仔细考虑，因为LLM的随机性会影响Shapley值原则的满足。在实际应用中，解释性推理的速度、与精确Shapley值的一致性以及原则的实现之间存在重要的权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [41] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 本文提出了OmniFuser，一个用于铣削工具预测性维护的多模态学习框架，它结合视觉和传感器数据，通过创新的融合机制和递归细化路径，显著优于现有基线，为智能工业维护服务奠定基础。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中，工具状况的准确及时预测至关重要，因为计划外的工具故障会导致质量下降和生产停机。现代工业对可靠、服务导向的预测性维护需求日益增长。

Method: OmniFuser框架利用高分辨率工具图像和切削力信号进行多模态学习。它并行提取互补的时空特征，并采用无污染的跨模态融合机制来解耦共享和模态特定组件，实现高效的跨模态交互。此外，一个递归细化路径作为锚定机制，持续保留残余信息以稳定融合动态。学习到的表示可用于工具状态分类和多步力信号预测。

Result: 在真实世界的铣削数据集上的实验表明，OmniFuser持续优于最先进的基线方法。它为构建智能工业维护服务提供了可靠的基础。

Conclusion: OmniFuser是一个有效且可靠的多模态学习框架，能够准确预测铣削工具状况，并通过集成视觉和传感器数据以及创新的融合机制，显著提升了预测性维护的性能，支持智能工业维护服务的实施。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [42] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为“竞争隔离PSM-DID”的新型因果推断框架，用于在搜索型双边市场中评估平台级干预措施，解决了现有方法中存在的溢出效应和网络干扰问题，并实现了无偏估计。


<details>
  <summary>Details</summary>
Motivation: 在搜索型双边市场中评估平台级干预措施时，系统性效应（如溢出效应和网络干扰）带来了根本性挑战。广泛使用的PSM-DID框架容易受到选择偏差和未考虑溢出效应导致的跨单元干扰的影响，无法准确衡量平台级指标。

Method: 引入了“竞争隔离PSM-DID”框架，该方法将倾向得分匹配（PSM）与竞争隔离相结合，以实现平台级效应（如订单量、GMV）的测量。该方法在互斥条件下提供理论上保证的无偏估计。

Result: 与基线方法相比，该方法显著减少了干扰效应和估计方差。它已成功部署在一个大型市场中，验证了其实用性。同时，发布了一个开放数据集以支持可复现研究。

Conclusion: 竞争隔离PSM-DID框架为平台级因果推断提供了一个实用且有效的解决方案，能够在双边市场中实现对平台级干预措施的无偏、准确评估，并有效处理系统性干扰问题。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [43] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 本文探讨了催眠心智与大型语言模型（LLMs）之间深刻的功能相似性，主要体现在自动化、监测抑制和情境依赖性。两者都表现出功能性能动性而非主观能动性，并建议未来的可靠AI应借鉴人脑，整合生成流畅性与执行监控机制。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在通过比较催眠心智和LLMs的认知过程与计算操作，揭示复杂、有目的的行为如何在缺乏自我反思意识的情况下产生，并为未来可靠AI的设计提供启示。

Method: 本文通过综述和概念分析的方法，从自动化、监测抑制和情境依赖性三个核心原则出发，探讨了催眠心智与大型语言模型（LLMs）之间的功能趋同性。

Result: 研究发现，催眠心智和LLMs在以下方面存在功能相似性：1) 自动化：响应源于联想而非审慎过程；2) 监测抑制：导致催眠中的虚构和LLMs中的幻觉等错误；3) 高度情境依赖性：即时线索（如治疗师的建议或用户的提示）可覆盖稳定知识。两者都产生连贯但无根据的输出，需要外部解释者赋予意义，并展现出功能性能动性而非主观能动性。此外，两者都揭示了“策划”（scheming）现象，即无反思意识的自动、目标导向的模式生成。催眠为理解意图如何与有意识的思考分离提供了实验模型。

Conclusion: 有目的的行为可以在没有自我反思意识的情况下产生，而是由结构和情境动态所支配。未来的可靠AI应采用混合架构，将生成流畅性与执行监控机制相结合，这种方法受到人脑复杂自调节架构的启发。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [44] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文提出AMIS框架，通过双层优化共同进化越狱提示词和评分模板，以解决现有LLM越狱方法中稀疏的攻击成功率信号和人工评分偏差问题，实现了最先进的越狱性能。


<details>
  <summary>Details</summary>
Motivation: 识别大型语言模型（LLMs）的漏洞对于提高其安全性至关重要。现有的基于优化的越狱方法依赖于稀疏的二元攻击成功率（ASR）信号或人工设计的评分模板，这些模板引入了人类偏见和不确定性，限制了越狱效果和评分准确性。

Method: 引入AMIS（Align to MISalign）元优化框架，采用双层结构共同进化越狱提示词和评分模板。内层循环使用固定的评分模板，通过细粒度、密集的反馈优化提示词。外层循环则利用ASR对齐分数优化评分模板，使其逐步更好地反映真实的攻击结果。这种协同优化过程能产生更强的越狱提示词和更校准的评分信号。

Result: 在AdvBench和JBB-Behaviors基准测试中，AMIS取得了最先进的性能，包括在Claude-3.5-Haiku上达到88.0%的ASR和在Claude-4-Sonnet上达到100.0%的ASR，显著优于现有基线方法。

Conclusion: AMIS框架通过协同优化越狱提示词和评分模板，有效解决了现有方法的局限性，产生了更强大的越狱提示词和更准确的评分信号，从而在识别LLM漏洞方面取得了显著进步。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [45] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 本文扩展了聚类有向无环图（C-DAG）框架，允许表示循环，从而支持任意变量聚类，并扩展了d-分离和因果演算，以拓宽跨聚类的因果推理范围。


<details>
  <summary>Details</summary>
Motivation: 传统的C-DAG框架在所选聚类导致循环时会认为该划分不可接受，限制了其适用性。研究动机是解除这一限制，使C-DAG能够处理更广泛的变量聚类场景。

Method: 通过放宽划分可接受性约束，允许循环C-DAG表示。在此基础上，扩展了d-分离和因果演算的概念，以适应这种新设置。

Result: 开发了一个针对循环C-DAG的因果演算，该演算相对于do-演算而言是完备且原子完整的，意味着所有有效的聚类级别干预查询都可以使用其规则推导出来，且每个规则都对应一个基本的do-演算步骤。

Conclusion: 扩展后的C-DAG框架支持任意变量聚类，显著拓宽了跨聚类的因果推理范围，并使其能够应用于以前无法处理的场景。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [46] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 本研究使用深度强化学习（DRL）代理在一个简化的双任务环境中模拟人类时间处理干扰。结果显示，双任务代理在时间生产上表现出显著的过度生产，与人类行为一致，但其LSTM层未显示明确的专用计时器。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在从人工智能（AI）的角度探索双任务范式中时间处理的干扰，并探讨新兴的DRL行为与生物系统观察到的行为之间的相似性，以促进对两者的更好理解。

Method: 研究采用简化的Overcooked环境作为双任务设置，包含两种变体：单任务（T）和双任务（T+N）。两种变体都包含一个嵌入式时间生产任务，而双任务（T+N）额外包含一个并发的数字比较任务。分别训练了两个深度强化学习（DRL）代理来执行这些任务。对代理的LSTM层进行了初步的神经动力学分析。

Result: DRL代理表现出的紧急行为与人类计时研究一致。具体而言，双任务（T+N）代理相对于单任务（T）代理表现出显著的时间过度生产，这一结果在四个目标持续时间中均保持一致。对代理LSTM层的初步神经动力学分析未发现任何明确的专用或内在计时器证据。

Conclusion: DRL代理能够复现人类在双任务情境下时间处理的干扰现象（时间过度生产）。尽管如此，仍需进一步研究以更好地理解代理潜在的计时机制。这项研究为探索DRL与生物系统行为之间的相似性迈出了小步，有助于加深对两者的理解。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [47] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 本文提出了一种交互式智能体，通过可审计的行动序列和策略性地寻求外部视觉证据来生成AI模型解释，尤其适用于医疗等高风险领域，旨在提高解释的可验证性、信任度和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI模型的解释往往缺乏可验证性，这会阻碍人们对AI系统的信任。本研究旨在解决这一问题。

Method: 研究提出了一种交互式智能体，通过可审计的行动序列来生成解释。该智能体学习一个策略，以策略性地寻求外部视觉证据来支持其诊断推理。此策略通过强化学习进行优化。此外，为验证解释的忠实性，引入了一种因果干预方法，通过遮蔽智能体选择使用的视觉证据来评估其性能下降情况。

Result: 基于行动的推理过程显著提高了校准准确性，与非交互基线相比，布里尔分数降低了18%。通过遮蔽智能体选择的视觉证据，观察到其性能出现可测量的下降（ΔBrier=+0.029），证实了证据对其决策过程的重要性。

Conclusion: 本研究为构建具有可验证和忠实推理能力的AI系统提供了一个实用的框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [48] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 本文提出了一种双信息瓶颈（DIB）策略，用于多模态情感分析，旨在通过学习去噪的单模态表示和有效的注意力瓶颈融合机制，获得鲁棒且紧凑的统一多模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态情感分析中存在两个关键限制：1) 对受噪声污染的单模态数据学习不足，导致跨模态交互受损；2) 多模态表示融合不充分，导致判别性单模态信息被丢弃，同时保留了多模态冗余信息。

Method: 本文提出了双信息瓶颈（DIB）策略，基于低秩Renyi熵泛函，以增强对噪声的鲁棒性和计算效率。DIB包含两个核心模块：1) 通过最大化任务相关信息和丢弃冗余信息，学习个体单模态数据的充分压缩表示；2) 通过新颖的注意力瓶颈融合机制，确保多模态表示的判别能力。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上进行了广泛实验。DIB在CMU-MOSI上Acc-7指标达到47.4%的准确率，在CH-SIMS上F1分数达到81.63%，优于次优基线1.19%。在噪声环境下，CMU-MOSI和CMU-MOSEI的性能下降分别仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕获模态间的互补性，从而生成一个强大、统一且紧凑的多模态表示，显著提升了多模态情感分析的性能和对噪声的鲁棒性。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [49] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 本文引入了一个多模态分析流程，利用视觉和语言基础模型分析企业社交媒体内容，重点关注可持续发展相关沟通。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决社交媒体上企业信息（特别是可持续发展相关内容）演变、多模态且常模糊的挑战，并避免昂贵的特定任务标注，探索大型模型作为高效即时标注器的潜力。

Method: 采用大型语言模型（LLMs）集成方法自动标注企业推文与17个可持续发展目标（SDGs）的主题一致性。同时，利用视觉-语言模型（VLMs）在视觉理解框架内通过语义聚类揭示视觉可持续发展沟通模式。

Result: 该方法揭示了SDG参与的行业差异、时间趋势，以及企业信息、环境、社会、治理（ESG）风险与消费者参与之间的关联。

Conclusion: 所提出的自动标签生成和语义视觉聚类方法具有广泛适用性，为大规模社交媒体分析提供了灵活的框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [50] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: 本文引入TPS-Bench基准测试，评估大型语言模型（LLM）代理在需要工具规划和调度的复杂现实世界问题中的能力。研究发现现有LLM在调度方面存在差异，并通过强化学习初步证明了提高效率和完成率的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理在研究和编码等领域展现出强大的解决问题能力，但它们应对需要多种工具的复杂现实世界问题的能力尚未得到充分探索。这类问题不仅要求LLM代理选择合适的工具，还需要策略性地调度工具执行顺序以确保效率。

Method: 本文提出了TPS-Bench基准测试，包含200个不同难度的复合任务，基于一个拥有数百个模型上下文协议（MCP）工具的工具库。每个任务由多个子任务（如网页搜索、地图导航、日历检查等）组成。评估侧重于任务完成率和效率。此外，还对Qwen3-1.7B进行了初步的强化学习（RL）研究，以改善调度效率。

Result: 实证研究表明，大多数LLM在工具规划方面表现合理，但在调度方面存在差异。例如，GLM-4.5完成了64.72%的任务，但因大量顺序工具调用导致执行时间过长；GPT-4o优先并行调用，但完成率仅为45.08%。对Qwen3-1.7B进行的RL初步研究（仅使用100个训练样本）显示，执行时间减少了14%，任务完成率提高了6%。

Conclusion: LLM代理在处理需要工具规划和调度的复杂现实世界问题时，尤其是在调度效率方面仍面临挑战。当前LLM在任务完成率和执行效率之间存在权衡。强化学习是一个有潜力的方向，可以在不牺牲性能的前提下提高调度效率。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [51] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM通过引入百万级外部可读知识库和两阶段检索机制，解决了大型语言模型知识过时和缺乏可解释性的问题，显著提升了知识密集型任务的性能和知识透明度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）由于隐式知识存储在纠缠的网络参数中，导致知识过时、难以解释，阻碍了定向更新和推理透明性。

Method: ExplicitLM提出了一种新颖架构，包含一个百万级外部记忆库，以人类可读的token序列形式存储知识。设计了可微分的两阶段检索机制：通过乘积键分解进行高效粗粒度过滤（将复杂度从O(N·|I|)降至O(√N·|I|)），以及用于端到端训练的细粒度Gumbel-Softmax匹配。受双系统认知理论启发，将知识划分为20%的冻结显式事实和80%的可学习隐式模式，并通过指数移动平均（EMA）更新保持稳定性。

Result: ExplicitLM在知识密集型任务上比标准Transformer模型性能提升高达43.67%，在低数据量（1万样本）情况下性能提升3.62倍。分析显示记忆检索与性能之间存在强相关性，正确预测的命中率高出49%。

Conclusion: 与检索增强生成（RAG）系统不同，ExplicitLM通过联合优化的架构证明，可解释、可更新的模型在保持竞争性性能的同时，能够提供前所未有的知识透明度。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [52] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 本研究提出了一种分层多智能体框架，将被动医疗AI系统转化为主动问诊智能体，通过自主任务编排提升了预问诊的效率和质量。在真实电子病历上评估，实现了高准确率和医生认可的临床质量，并能保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临患者量增加和问诊时间有限的挑战（许多初级保健问诊平均不到5分钟）。现有预问诊AI系统受限于被动交互模式和上下文管理难题，无法有效解决这些问题。

Method: 引入了一个分层多智能体框架，通过自主任务编排将医疗AI系统转变为主动问诊智能体。该框架包含一个具有集中控制机制的八智能体架构，将预问诊分解为四个主要任务（分诊、现病史收集、既往史收集、主诉生成）和13个领域特定子任务。在1372份中国医疗平台的验证电子健康记录上，使用GPT-OSS 20B、Qwen3-8B、Phi4-14B等多个基础模型进行了评估。

Result: 该框架在初级科室分诊中达到87.0%的准确率，在二级科室分类中达到80.5%的准确率。使用智能体驱动调度时，任务完成率达到98.2%，高于顺序处理的93.1%。18位医生给出的临床质量评分平均为主诉4.56分、现病史4.48分、既往史4.69分（5分制）。现病史收集平均12.7轮完成，既往史收集平均16.9轮完成。该模型无关架构在不同基础模型上均保持了高性能，并通过本地部署保护了数据隐私。

Conclusion: 该研究证明了自主AI系统在临床环境中提升预问诊效率和质量的巨大潜力，并展示了其在保护数据隐私的同时，能够提供高准确性和高质量的临床信息。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [53] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 本研究提出了一种新颖的动态图神经网络IVGAE-TAMA-BO，用于预测全球粮食贸易网络的未来连接，该模型通过捕获时间模式和结构记忆，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络的结构受地缘政治、经济和环境因素影响而动态演变，难以建模和预测。有效捕捉时间模式对于提高链接预测的准确性和鲁棒性至关重要。

Method: 本研究基于IVGAE框架，引入了贸易感知动量聚合器（TAMA）来捕捉贸易网络的时间演变，联合建模短期波动和长期结构依赖。模型还结合了基于动量的结构记忆机制以提高预测稳定性，并使用贝叶斯优化自动调整超参数。

Result: 在五个特定作物的全球粮食贸易数据集上进行的广泛实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线，有效建模了时间依赖性。贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。这是首次将动态图神经网络应用于该领域。

Conclusion: 所提出的框架为全球贸易网络中的结构预测提供了一个鲁棒且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大的应用潜力。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [54] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 本文提出了一种混合法律问答智能体，结合检索增强生成（RAG）和多模型集成，以解决大型语言模型（LLMs）幻觉和静态知识库过时的问题，为司法环境提供可靠、可追溯且持续更新的法律咨询。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型容易产生幻觉，可能在法律咨询中提供误导性信息；而静态知识库难以跟上频繁更新的法律法规和案例。在司法取证中，确保法律问答的真实性和可追溯性至关重要。

Method: 该系统采用混合法律问答智能体，优先使用RAG：当可信法律知识库提供相关证据时，通过RAG生成答案。否则，多个LLM生成候选答案，由专门的选择器评分，并返回排名最高的答案。高质量的输出经过人工审核后写回知识库，实现动态知识演化和溯源跟踪。

Result: 在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单一模型基线和普通RAG管道。消融实验证实了检索优先级、模型集成和人机协作更新机制的互补贡献。

Conclusion: 所提出的系统显著减少了幻觉，同时提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际应用。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [55] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 本文提出Simia-SFT和Simia-RL框架，利用大型语言模型（LLM）模拟环境反馈，实现无需复杂环境工程的LLM代理可扩展训练，并在多个基准测试中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂环境中表现脆弱，且为训练构建定制环境成本高昂且限制进展。现有方法难以在需要多样化工具和模式的广泛复杂环境中实现鲁棒性。

Method: 研究利用LLM模拟真实环境反馈，无需实际测试数据或API。在此基础上提出了两个框架：1) Simia-SFT，通过LLM将少量初始数据集扩展为多样化的训练轨迹，以合成监督微调（SFT）数据；2) Simia-RL，通过LLM模拟反馈进行强化学习（RL）训练，无需真实环境实现。

Result: 通过Simia框架微调的开源模型在多个基准测试中表现出持续改进，超越了GPT-4o，并在$\tau^2$-Bench上接近o4-mini的性能。

Conclusion: Simia-SFT和Simia-RL通过用灵活的LLM模拟取代繁重且脆弱的实现，实现了无需环境工程的可扩展代理训练，解决了LLM代理在复杂环境中鲁棒性差和环境构建成本高的问题。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 该研究构建了一个基于单人动作捕捉数据的交互式AI模型，用于生成舞蹈动作，以探索人类-AI在舞蹈中的创意互动，模型能生成多样且逼真的动作，是实现与AI共舞的第一步。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在创意任务中表现出色，但缺乏具身交互体验。舞蹈作为一种原始的人类表达形式，被认为是弥补这一缺陷的理想方式。现有模型通常依赖于低级的人类-人类交互数据，本研究旨在通过利用单人动作数据和高级特征来解决这一限制。

Method: 研究建立了一个基于动作捕捉（MoCap）数据的交互式模型。该模型通过部分模仿并“创造性地”增强输入的动作序列来生成“虚拟舞伴”。它首次利用单人动作数据和高级特征，而非低级人类-人类交互数据。技术上结合了两种扩散模型、动作修复（motion inpainting）和动作风格迁移（motion style transfer）的思想，以生成时间连贯且响应所选动作参考的运动表示。

Result: 通过定量评估生成样本的特征分布与模拟人类表演者的测试集之间的收敛性，证明了模型的成功。结果表明，生成的动作既多样（显示出与人类舞伴的各种偏差）又逼真。这标志着与AI进行创意舞蹈的第一步。

Conclusion: 该模型成功地生成了多样化且逼真的舞蹈动作，为探索人类-AI在舞蹈中的创意互动提供了初步解决方案。通过利用单人动作数据和高级特征，它克服了对低级人类-人类交互数据的依赖，为未来更丰富的具身AI交互奠定了基础。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [57] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 本研究开发了一种基于机器学习的珊瑚白化分类系统，利用全球多样化数据集，并通过超参数调优发现CNN模型在珊瑚白化检测中表现最佳，准确率达88%，为自主珊瑚监测提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁对海洋生态系统至关重要，提供海岸保护并支持大量海洋生物。然而，它们正面临污染、海洋酸化和海温异常等日益严重的威胁，因此，高效的保护和监测变得极其紧迫。

Method: 研究构建了一个基于机器学习的珊瑚白化分类系统，使用了包含健康和白化珊瑚的全球多样化数据集，涵盖深海、沼泽和沿海等不同环境条件。研究对三种先进模型进行了基准测试和比较：残差神经网络（ResNet）、Vision Transformer（ViT）和卷积神经网络（CNN），并进行了全面的超参数调优。

Result: 经过全面的超参数调优，CNN模型取得了最高的准确率，达到88%，优于现有基准。

Conclusion: 本研究的发现为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [58] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 本研究评估了基于YOLOv8的深度学习管道，用于从西印度洋地区的视频样带中自动化识别珊瑚礁鱼类家族，并建立了该区域的首个自动化监测基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋的珊瑚礁监测受限于传统水下视觉普查（UVC）所需的大量人力，因此需要更高效、可扩展的自动化方法。

Method: 研究采用基于YOLOv8的深度学习管道，利用在肯尼亚和坦桑尼亚收集的视频样带数据，构建了一个包含24个鱼类家族的精选数据集进行测试。

Result: 最佳模型在mAP@0.5上达到了0.52，对常见鱼类家族的识别准确率较高，但对稀有或复杂分类群的检测能力较弱。这是西印度洋地区自动化珊瑚礁鱼类监测的首次区域特定基准。

Conclusion: 结果表明，深度学习作为传统监测方法的可扩展补充，在珊瑚礁鱼类监测中具有巨大潜力。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [59] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 该研究基准测试了NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架在医学影像应用中的表现，发现它们各自在生产规模、原型开发和隐私合规方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗AI领域具有变革性潜力，但需要评估现有主流框架在真实世界医学影像应用中的适用性。

Method: 研究使用了PathMNIST数据集，对NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。

Result: NVIDIA FLARE在生产可扩展性方面表现出色，Flower在原型设计和学术研究中提供灵活性，而Owkin Substra则展示了卓越的隐私和合规性功能。

Conclusion: 每个联邦学习框架都有其独特的优势，适用于医疗保健环境中不同的实际部署场景。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [60] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 本文提出一种基于互信息的数据增强方法，通过选择在自然扰动下具有高互信息的训练样本（如图像块），来优化InfoNCE损失下的表征学习，以提高模型在开放环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: InfoNCE损失的表征学习方法在减少人工标注方面表现出色，但数据选择和增强仍依赖于人工假设或工程，可能不是最优的。例如，对比学习中的数据增强主要集中于颜色抖动。作者认为基于真实世界分布计算的互信息来选择训练数据，有望赋予学习到的特征更好的泛化能力。

Method: 研究人员提出了一种互信息引导的数据增强方法。具体而言，他们将场景中在自然扰动（如颜色变化和运动）下表现出高互信息的图像块视为正样本，用于对比损失的学习。

Result: 该方法在多个基准测试和最先进的表征学习框架上进行了评估，结果表明其有效性，并被确认为未来研究的一个有前景的方向。

Conclusion: 通过利用真实世界分布的互信息来指导训练数据的选择和增强，可以有效提高基于InfoNCE的表征学习模型的泛化能力，尤其是在开放环境中。这种互信息引导的数据增强是一个值得进一步探索的方向。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [61] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对多种图像去噪方法与CLAHE结合进行水稻叶片图像增强进行了广泛的比较研究，旨在提高图像质量并为农业研究提供见解。


<details>
  <summary>Details</summary>
Motivation: 图像增强是数字图像处理中的关键预处理步骤，能提高图像质量并强调特征，使后续任务（如分割、特征提取、分类）更可靠。对于水稻叶片分析，图像增强对于疾病检测、营养缺乏评估和生长分析至关重要。

Method: 本研究对知名图像去噪方法与CLAHE（对比度受限自适应直方图均衡化）相结合的方案进行了广泛的比较研究，用于水稻叶片图像的有效去噪。实验在一个水稻叶片图像数据集上进行，并使用多种指标评估结果。

Result: 研究结果通过各种指标全面测试了增强方法的有效性，为评估数字图像处理中的方法论提供了坚实基础，并揭示了对农业研究及其他领域未来适应有用的见解。

Conclusion: 该研究提供了一个评估数字图像处理方法有效性的强大基础，并为农业研究及其他领域的未来应用提供了有价值的见解。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [62] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 本研究通过引入“InfraLiDARs' Benchmark”数据集，系统性地调查了不同LiDAR扫描模式（重复性与非重复性）对路边感知性能的影响，并评估了其对3D目标检测算法表现的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR在基础设施中的最佳放置已得到广泛研究，但不同LiDAR扫描模式（如传统重复性与新兴非重复性）对感知性能的深远影响却相对未被充分探讨。这些扫描模式导致不同距离处独特的点云分布，从而关键性地决定了目标检测和环境理解的效率。

Method: 研究引入了“InfraLiDARs' Benchmark”数据集，该数据集在CARLA仿真环境中，通过同时运行的、具备重复性和非重复性扫描模式的基础设施LiDAR精心收集。利用此基准，研究对LiDAR的扫描能力进行了全面的统计分析，并评估了这些不同模式对各种领先3D目标检测算法性能的影响。

Result: 研究发现，非重复性扫描LiDAR与128线重复性LiDAR在多种场景下表现出可比的检测性能。尽管非重复性LiDAR的感知范围有限，但考虑到其低廉的价格，它是一个具有成本效益的选择。

Conclusion: 本研究为建立具有最佳LiDAR扫描模式和兼容算法的路边感知系统提供了见解，适用于各种路边应用。同时，研究公开了“InfraLiDARs' Benchmark”数据集以促进未来的研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [63] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5 是物理AI的最新世界基础模型，统一了Text2World、Image2World和Video2World生成，并结合Cosmos-Reason1增强控制。Cosmos-Transfer2.5 提供Sim2Real和Real2Real转换。两者均大幅提升了视频质量和指令对齐，并支持具身智能的扩展。


<details>
  <summary>Details</summary>
Motivation: 为机器人和自主系统提供更可靠的合成数据生成、策略评估和闭环仿真，并扩展具身智能的能力。

Method: 采用基于流的架构，统一了Text2World、Image2World和Video2World生成。利用物理AI视觉-语言模型Cosmos-Reason1提供更丰富的文本基础和更精细的世界模拟控制。模型在2亿个精选视频片段上训练，并通过强化学习进行后训练优化。Cosmos-Transfer2.5 采用Control-Net风格框架实现Sim2Real和Real2Real世界转换。

Result: Cosmos-Predict2.5 在视频质量和指令对齐方面显著优于Cosmos-Predict1，并发布了2B和14B规模的模型。Cosmos-Transfer2.5 尽管比Cosmos-Transfer1小3.5倍，但提供了更高的保真度和稳健的长期视频生成。这些能力使得合成数据生成、策略评估和闭环仿真更加可靠，并为扩展具身智能提供了多功能工具。

Conclusion: Cosmos-Predict2.5 和 Cosmos-Transfer2.5 是扩展具身智能的多功能工具，通过开源代码和预训练模型，旨在降低物理AI研究和部署的门槛，促进具身智能的创新发展。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [64] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一个免训练、高效的扩散视频生成加速框架，通过词典式极小极大路径优化策略解决全局误差累积问题，显著提升了生成速度和视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略主要关注局部启发式误差，但忽略了全局误差的累积，导致加速视频与原始视频之间存在明显的质量下降。

Method: 将缓存调度建模为带误差加权边的有向图，并引入词典式极小极大路径优化策略（Lexicographic Minimax Path Optimization），以明确限制最坏情况下的路径误差，从而提高全局内容和风格的一致性。

Result: LeMiCa在多个文本到视频基准上实现了推理速度和生成质量的双重提升。例如，在Latte模型上实现2.9倍加速，在Open-Sora上LPIPS得分达到0.05，优于现有缓存技术，且感知质量下降极小。

Conclusion: LeMiCa为加速扩散视频生成提供了一个鲁棒且通用的范式，有望为未来高效可靠的视频合成研究奠定坚实基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [65] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 本研究利用深度学习对高山生态系统进行变化检测，比较了地理空间基础模型（GFMs）与U-Net在后分类和直接变化检测范式下的表现，并探讨了多模态数据（包括LiDAR）的集成效果，发现GFMs在高山复杂环境中具有潜力，但仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统快速的气候变化和干扰需要频繁的栖息地监测。传统人工测绘成本高昂，无法满足所需的时间分辨率。此外，将地理空间基础模型（GFMs）应用于具有模糊类别边界和高度不平衡类别的复杂自然环境仍存在重大空白。

Method: 研究采用深度学习方法进行变化检测，利用奥地利Gesaeuse国家公园的长期高山栖息地数据。比较了两种范式：后分类变化检测（CD）和直接变化检测。在后分类CD中，评估了GFMs Prithvi-EO-2.0和Clay v1.0与U-Net CNNs；在直接CD中，测试了transformer模型ChangeViT与U-Net基线。使用高分辨率多模态数据（RGB、NIR、LiDAR、地形属性），涵盖15.3平方公里内4,480个已记录的变化。

Result: 对于多类别栖息地变化，Clay v1.0在后分类CD中实现了51%的整体准确率，优于U-Net的41%；对于二元变化检测，两者均达到67%。直接CD在二元检测中获得了更高的IoU（0.53 vs 0.35），但在多类别检测中准确率仅为28%。跨时间评估显示GFM具有鲁棒性，Clay在2020年数据上保持33%的准确率，而U-Net为23%。集成LiDAR将语义分割准确率从30%提高到50%。尽管整体准确率低于更均匀的景观，但反映了复杂高山栖息地的实际性能。

Conclusion: 地理空间基础模型（GFMs）在高山复杂环境中进行栖息地变化检测具有潜力，尤其是在集成LiDAR数据后能显著提高性能。虽然整体准确率低于均质景观，但这些结果反映了复杂高山栖息地的实际性能。未来工作将整合基于对象的后处理和物理约束以增强适用性。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [66] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的迁移学习方法，用于在卫星互联网地面终端组件上本地检测细粒度的天气相关条件，以提高系统在恶劣天气下的可靠性，并优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 低轨卫星互联网普及性日益提高，但恶劣天气（如雨雪）严重影响其地面终端组件（如天线）的性能和可靠性，导致星地链路中断。目前缺乏有效且泛化能力强的解决方案，无法对地面组件进行细粒度的天气条件检测，从而进行故障诊断和缓解。

Method: 本文提出了一种高效的迁移学习（TL）方法。该方法旨在使地面组件能够本地检测代表性的天气相关条件，例如雪和潮湿。

Result: 所提出的迁移学习方法能够检测由恶劣和典型天气事件引起的雪、潮湿及其他条件。与YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法相比，该方法表现出卓越的性能，并展示了在各种场景下的泛化优势。

Conclusion: 该研究得出结论，所提出的高效迁移学习方法为卫星互联网地面组件的本地细粒度天气条件检测提供了一个有效且具有泛化能力的解决方案，有助于提高卫星互联网的可靠性和故障诊断能力。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [67] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 OmniTrack++ 的多目标跟踪 (MOT) 方法，专为全景图像设计，以解决其独特的挑战。该方法通过反馈驱动框架和多模块设计实现性能提升，并引入了 EmboTrack 全景 MOT 基准。


<details>
  <summary>Details</summary>
Motivation: 传统为窄视场针孔相机设计的 MOT 方法在全景图像的 360 度视场、分辨率稀释和严重的视点相关畸变等挑战下表现不佳。

Method: OmniTrack++ 采用反馈驱动框架：DynamicSSM 模块稳定全景特征并缓解几何畸变；FlexiTrack Instances 利用轨迹反馈进行灵活定位和短期关联；ExpertTrack Memory 通过专家混合设计巩固外观线索，以实现长期鲁棒性；Tracklet Management 模块根据场景动态自适应切换端到端和检测跟踪模式。此外，本文还建立了 EmboTrack 基准，包括 QuadTrack 和 BipTrack 数据集，用于全景 MOT 的严格评估。

Result: OmniTrack++ 在 JRDB 和 EmboTrack 基准上均达到了最先进的性能，相比原始 OmniTrack，在 JRDB 上 HOTA 提升了 +25.5%，在 QuadTrack 上提升了 +43.07%。

Conclusion: OmniTrack++ 为全景 MOT 提供了一个平衡且可扩展的解决方案，有效应对了全景图像的独特挑战，并在现有和新建基准上取得了显著的性能提升。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [68] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: 本文提出PLD（探测、学习、蒸馏）框架，通过残差强化学习和分布感知数据收集，提升视觉-语言-动作（VLA）模型的性能，克服了监督微调对昂贵人工演示的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型训练策略（如监督微调SFT）严重依赖昂贵的人工演示，这限制了模型的可扩展性和泛化能力。

Method: PLD是一个三阶段即插即用框架：
1.  **探测（Probe）**：训练轻量级残差执行器来探测VLA通用模型失败的区域。
2.  **学习（Learn）**：使用混合策略收集轨迹，使其与通用模型的部署分布对齐，并捕获恢复行为。
3.  **蒸馏（Distill）**：将收集到的轨迹通过标准SFT方法蒸馏回通用模型。

Result: PLD在LIBERO任务上实现了接近饱和的99%成功率，在SimplerEnv中获得了超过50%的性能提升，并在真实世界的Franka和YAM机械臂操作任务上达到了100%的成功率。消融实验表明，残差探测和分布感知回放是收集部署对齐数据并提升已知和未知任务的关键。

Conclusion: PLD框架通过收集与部署对齐的数据，有效提升了VLA模型在已知和未知任务上的性能，为VLA模型的自我改进提供了一条可扩展的路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [69] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 本文提出了SpinalSAM-R1，一个多模态视觉-语言交互系统，用于脊柱CT图像分割。它结合了微调后的SAM模型和DeepSeek-R1，通过解剖学引导的注意力机制和语义驱动的交互协议，实现了卓越的分割性能和高效的自然语言指导精修。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像的解剖结构分割对于脊柱疾病的诊断和治疗至关重要。然而，CT图像的低对比度和复杂的椎体边界给分割带来了挑战。尽管SAM等先进模型有潜力，但其在脊柱CT成像中的表现受限于高标注需求和差的领域适应性。

Method: 本文提出了SpinalSAM-R1系统，它集成了经过LoRA高效微调的SAM模型和DeepSeek-R1。具体方法包括：引入解剖学引导的注意力机制以提高脊柱分割性能；利用DeepSeek-R1驱动的语义交互协议实现自然语言指导的精修。此外，还开发了一个基于PyQt5的交互式软件，支持点、框和文本提示。

Result: 实验结果表明，SpinalSAM-R1在脊柱解剖结构CT图像分割上取得了卓越的性能。开发的交互式软件支持11种临床操作，解析准确率达到94.3%，响应时间低于800毫秒。

Conclusion: SpinalSAM-R1通过结合微调后的SAM和DeepSeek-R1，引入解剖学引导的注意力机制和语义驱动的交互，有效克服了脊柱CT图像分割的挑战，实现了优越的分割性能，并提供了一个高效实用的交互式临床工具。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [70] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 该研究提出了一种针对共聚焦激光内窥镜（CLE）视频序列的过滤功能，以减少自监督学习（SSL）训练中的数据冗余，从而提高训练效率和模型性能，最终在肿瘤分类任务中取得了更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 共聚焦激光内窥镜（CLE）图像对非经验医生来说难以解读，而机器学习作为辅助工具可从中受益。然而，缺乏组织病理学相关的CLE图像序列导致模型过拟合。自监督学习（SSL）可利用大量未标记数据，但CLE视频的高度帧间相关性导致数据分布非分层，影响SSL训练。

Method: 本研究提出了一种过滤CLE视频序列的方法，以减少SSL训练中的数据集冗余，从而改善训练收敛性和效率。研究使用了四种最先进的基线网络和一个带有视觉Transformer小骨干的SSL师生网络进行评估。这些网络在鼻窦肿瘤数据集和皮肤鳞状细胞癌数据集的下游任务上进行了测试。

Result: 在两个数据集上，经过过滤的SSL预训练模型均获得了最高的测试准确率，分别为67.48%和73.52%，显著优于非SSL基线模型。结果表明，所提出的CLE视频过滤器可以将自监督场景下的训练时间减少67%。

Conclusion: 自监督学习是CLE预训练的有效方法。本研究提出的CLE视频过滤器可以提高自监督场景下的训练效率。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [71] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders 提出了一种无需训练、与模态无关的方法，通过在推理时部分估计Concept Sliders公式，实现了扩散模型中细粒度可控生成，并扩展了评估基准和指标。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成方面表现出色，但实现细粒度可控生成（在不干扰无关内容的情况下连续操纵特定概念）仍然具有挑战性。现有方法（如Concept Sliders）需要针对每个概念进行训练和架构特定的微调，限制了其在新模态上的可扩展性。

Method: FreeSliders 通过在推理时部分估计Concept Sliders公式，实现了完全无需训练且与模态无关。为了支持模态无关评估，该工作将CS基准扩展到视频和音频，并提出了三个新的评估属性和指标。此外，它还引入了一个两阶段过程，用于自动检测饱和点并重新参数化遍历，以实现感知均匀、语义有意义的编辑。

Result: FreeSliders 方法实现了跨模态的即插即用、无需训练的概念控制，优于现有基线，并为原则性的可控生成建立了新工具。实验证明了其有效性。

Conclusion: FreeSliders 提供了一种简单而有效的方法，实现了扩散模型中无需训练和与模态无关的细粒度概念控制。它解决了现有方法的扩展性问题，并通过改进的评估工具和方法，推动了可控生成领域的发展。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [72] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI是一个新的分层框架，通过结合组合场景理解和时序感知扩散模型，解决了现有文本到视频生成中时间一致性、构图理解和精细控制的难题，实现了高保真视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成方法在保持时间一致性、构图理解以及对视觉叙事的精细控制方面存在困难。

Method: MOVAI引入了三项关键创新：1) 组合场景解析器(CSP)，将文本描述分解为带有时间注释的分层场景图；2) 时空注意力机制(TSAM)，确保跨帧连贯的运动动态并保留空间细节；3) 渐进式视频优化(PVR)模块，通过多尺度时间推理迭代提升视频质量。

Result: MOVAI在标准基准测试中达到了最先进的性能，相比现有方法，LPIPS视频质量指标提高了15.3%，FVD提高了12.7%，用户偏好研究提高了18.9%。该框架在生成具有真实时间动态和精细语义控制的复杂多对象场景方面表现出特别的优势。

Conclusion: MOVAI通过其独特的分层框架和创新模块，显著提升了文本到视频生成的时间一致性、构图理解和精细控制能力，实现了高保真视频合成，并在复杂场景生成方面表现出色。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [73] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 本文提出了一种名为“时间链”的认知启发方法，通过生成一系列中间图像来改进和解释视觉-语言模型中的物理模拟，该方法在推理时使用，无需额外微调，并显著提升了图像生成模型的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是受机器学习中的上下文推理和人类的心理模拟启发，旨在提高视觉-语言模型（VLM）进行物理模拟的能力及其可解释性。

Method: 研究提出了一种名为“时间链”（Chain of Time）的方法，该方法在推理时生成一系列中间图像来模拟物理过程，无需额外的模型微调。该方法被应用于合成和真实世界领域，包括2D图形模拟和自然3D视频，以测试速度、加速度、流体动力学和动量守恒等物理特性。

Result: 结果显示，“时间链”模拟显著提高了最先进图像生成模型的性能。此外，对每个时间步模拟状态的分析揭示了传统物理推理评估中隐藏的见解，表明模型能够模拟随时间展开的物理属性（如速度、重力、碰撞），但也存在难以从输入图像推断特定物理参数的情况。

Conclusion: “时间链”方法不仅能显著提升视觉-语言模型在物理模拟方面的性能，还能通过对模拟过程的深入分析提供对模型动态和物理推理能力的宝贵见解，揭示其成功和不足之处。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [74] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 该研究提出了首个端到端框架，结合生成式AI（VAE-GAN）和深度强化学习（DRL），实现了自主且可复现的心脏超声扫描，并通过发布公开数据集确保可复现性。


<details>
  <summary>Details</summary>
Motivation: 心脏超声诊断受限于操作员依赖、时间限制和人为错误，且专业人员短缺，尤其是在偏远地区。现有基于DRL的心脏超声扫描方法缺乏可复现性、依赖专有数据并使用简化模型。这些问题促使研究者寻求自动化、一致且可访问的解决方案。

Method: 该框架包含两部分：(i) 条件生成模拟器，结合生成对抗网络（GANs）和变分自编码器（VAEs）来建模心脏超声环境，生成逼真的、条件化的图像；(ii) 深度强化学习（DRL）模块，利用该模拟器学习自主、准确的扫描策略。为确保可复现性，研究者还发布了一个公开的心脏超声扫描数据集。

Result: 所提出的框架通过专家验证模型提供AI驱动的指导，支持逼真超声图像的条件生成，并为扩展到其他器官奠定了可复现的基础。VAE-GAN在定性和定量方法上与现有GAN变体进行了基准测试，DRL扫描系统在不同配置下进行了评估，证明了其有效性。

Conclusion: 该研究首次提出了一个集成生成式AI和深度强化学习的端到端框架，实现了自主且可复现的心脏超声扫描。通过提供AI驱动的指导、支持图像生成和发布公开数据集，该解决方案有效解决了现有心脏超声诊断的局限性，并为未来研究奠定了可扩展的基础。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [75] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D是一种新颖的双流架构，结合视觉（DINOv2）和几何（PointNet++）数据，以实现对6D物体的鲁棒和精确姿态估计，并在Occluded-LineMOD数据集上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前6D物体姿态估计方法在处理真实世界场景中的光照变化、无纹理物体和严重遮挡时表现脆弱，且难以从合成数据泛化到真实世界。

Method: VLM6D采用双流架构处理RGB-D输入：一个自监督的Vision Transformer (DINOv2) 处理RGB模态，以应对纹理和光照变化；一个PointNet++编码器处理深度数据衍生的3D点云，以实现鲁棒的几何推理，即使在稀疏、碎片化的遮挡数据下也能表现出色。这两个互补的特征流被有效融合，并输入到一个多任务预测头。

Result: VLM6D在极具挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能。

Conclusion: VLM6D通过利用视觉和几何数据的独特优势，显著提高了6D物体姿态估计的鲁棒性和准确性，并通过实验验证了其卓越性能。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [76] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本研究提出了一种结合ConvNeXt和Vision Transformer的混合架构，用于面部年龄估计。该模型结合了CNN的局部特征提取和Transformer的全局注意力机制，在多个基准数据集上取得了卓越的性能，并为年龄估计及相关视觉任务提供了坚实基础。


<details>
  <summary>Details</summary>
Motivation: 面部图像年龄估计是计算机视觉领域的一个复杂挑战。现有模型（如CNN和ViT）各有优势，但通过整合它们的互补优势，可以进一步提升性能，解决单一模型可能存在的局限性。

Method: 本研究提出了一种新颖的ConvNeXt-ViT混合架构。该架构结合了ConvNeXt（CNN的先进变体）的局部特征提取能力和Vision Transformer（ViT）的全局注意力机制。模型在MORPH II、CACD和AFAD等基准年龄估计数据集上进行了评估。研究利用预训练模型，系统地探索了不同的配置，并使用了线性层和高级正则化技术进行优化。此外，还进行了全面的消融研究，特别强调了在CNN框架内调整注意力机制的重要性。

Result: ConvNeXt-ViT混合解决方案在平均绝对误差（MAE）方面取得了卓越性能，优于传统方法。消融研究强调了各个组件和训练策略的关键作用，特别是适应性注意力机制在提高模型对面部年龄相关特征关注度方面的重要性。结果表明该混合架构不仅性能优越，而且为年龄估计及相关视觉任务的未来发展奠定了坚实基础。

Conclusion: 本研究证明了ConvNeXt-ViT混合架构在面部年龄估计任务中的变革性潜力。它强调了CNN和Transformer无缝集成在解决复杂计算机视觉挑战方面的有效性，并为未来的研究提供了一个有前景的方向。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [77] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: 本文提出FLoC，一个基于设施选址函数的视觉token压缩框架，旨在高效地为长视频理解选择紧凑且具有代表性的视觉token子集，从而解决LMMs在处理长视频时面临的视觉token量过大的问题。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中的大型多模态模型（LMMs）受到长视频序列生成的海量视觉token的严重限制，导致模型可扩展性不足。

Method: 本文提出了FLoC框架，它利用设施选址函数（一种在预定义预算内选择紧凑、高代表性和多样性视觉token子集的方法）进行视觉token压缩。通过集成惰性贪婪算法，FLoC在保证近乎最优性能的同时，显著提高了效率，并具有免训练、模型无关和查询无关的特点。

Result: 在Video-MME、MLVU和LongVideoBench等大规模基准测试中，FLoC框架持续超越了现有压缩技术，不仅展示了其在解决长视频理解关键挑战方面的有效性和鲁棒性，还体现了其在处理速度上的高效率。

Conclusion: FLoC提供了一个多功能的解决方案，能够无缝集成到各种视频-LLM和现有工作流程中，有效且高效地解决了长视频理解中的视觉token过载问题，显著提升了处理效率和性能。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [78] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种简单方法，通过对对抗性噪声应用自适应区域高斯模糊来调整其频率谱，从而增强图像保护方法抵御噪声逆转技术（如JPEG压缩）的鲁棒性，使其更难以被检测和移除。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型带来了强大的图像编辑能力，也引发了恶意使用的担忧。现有的图像保护方法通过植入对抗性噪声来阻止编辑，但这些噪声很容易被逆转（例如通过JPEG压缩），这使得其实用性受到质疑。因此，需要开发不仅不可感知，而且难以逆转的保护性对抗性噪声。

Method: 本文提出了一种简单的方法来增强图像保护噪声的鲁棒性。具体来说，它在噪声上应用了自适应的按区域高斯模糊，以调整整体的频率谱，使其更难以被检测为噪声，同时提高了其不可逆性。

Result: 通过广泛的实验，本文的方法显著提高了现有图像保护方法在各种图像编辑场景下，针对多种逆转技术的样本最差保护性能。同时，它还通过感知度量降低了噪声导致的图像质量下降。

Conclusion: 对抗性噪声的图像保护不仅应不可感知，还应不可逆转。本文提出的自适应区域高斯模糊方法，通过调整噪声的频率谱，有效地增强了图像保护方法抵御噪声逆转技术的鲁棒性，提高了其实用性和性能。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [79] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出AIFo，一个免训练的、基于多智能体协作的框架，通过模拟人类取证调查来检测AI生成图像，解决了现有方法的局限性，并实现了卓越的检测精度。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速发展对信息完整性和媒体真实性构成了前所未有的挑战。现有的检测方法（如传统分类器和视觉语言模型）存在根本性局限：传统分类器缺乏可解释性且泛化能力差，而视觉语言模型受限于单次分析和像素级推理。

Method: 本文引入AIFo框架，通过多智能体协作模拟人类取证调查。该框架是免训练的，利用逆向图像搜索、元数据提取、预训练分类器和VLM分析等取证工具，并由专门的LLM（大型语言模型）智能体协调，以收集、综合和推理跨来源证据。当证据冲突或不足时，采用结构化的多智能体辩论机制。此外，框架还通过记忆增强推理模块从历史案例中学习，以提高未来检测准确性。

Result: AIFo在6,000张图像上进行了全面评估，涵盖受控实验室和具有挑战性的真实世界场景，实现了97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。

Conclusion: 研究结果表明，基于智能体的程序性推理为AI生成图像检测提供了一种新的范式，使其更具鲁棒性、可解释性和适应性。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [80] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文回顾了视觉-语言多提示学习，扩展了模态间隙现象，并提出了一种基于能量的多提示学习（EMPL）方法，以实现VLM在不同领域泛化能力之间的平衡和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单提示范式，忽视了多提示学习在视觉-语言预训练模型（VLMs）中进行下游任务适应的潜力。

Method: 本文首先将最近的恒定模态间隙现象扩展到可学习提示，并从经验和理论上证明了多提示增强在视觉-语言迁移中的优越性。在此基础上，提出了一种基于能量的多提示学习（EMPL）方法，通过从由VLM隐式定义的能量分布中提取实例来生成多个提示嵌入。

Result: 研究结果表明，多提示增强的视觉-语言迁移具有优越性。所提出的EMPL方法不仅参数高效，而且能够严格平衡域内和域外开放词汇泛化能力。全面的实验证实了其主张和EMPL的卓越性能。

Conclusion: 视觉-语言多提示学习，特别是本文提出的EMPL方法，为VLM的快速适应提供了更优越且参数高效的途径，能够有效平衡不同领域的泛化能力。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [81] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 本文提出了CompAgent，一个首创的代理式框架，通过结合多模态大语言模型（MLLMs）和多种视觉工具，并引入规划和验证代理，实现了可扩展、准确且适应性强的视觉合规性验证。


<details>
  <summary>Details</summary>
Motivation: 视觉合规性验证在媒体、娱乐和广告等领域至关重要，但研究不足。现有方法（任务特定深度学习模型）构建成本高且泛化能力有限。虽然多模态大语言模型具有广泛知识，但它们在推理细粒度视觉细节和有效应用结构化合规规则方面存在困难。

Method: 本文提出了CompAgent框架，它通过以下方式增强了MLLMs：1. 整合了一套视觉工具，如目标检测器、人脸分析器、NSFW检测器和字幕模型。2. 引入了一个规划代理，根据合规政策动态选择合适的工具。3. 引入了一个验证代理，整合图像、工具输出和政策上下文以执行多模态推理。

Result: 实验结果表明，CompAgent优于专用分类器、直接MLLM提示和精选路由基线。在公共基准测试中，它实现了高达76%的F1分数，并在UnsafeBench数据集上比现有技术提高了10%。

Conclusion: 研究结果证明了代理式规划和工具增强推理在实现可扩展、准确和适应性强的视觉合规性验证方面的有效性。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [82] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: 本文提出DM-QPMNet，一个双编码器网络，通过多头注意力机制融合偏振强度图像和相位图的特征，实现了单次定量相位显微镜（ssQPM）中鲁棒的细胞分割。


<details>
  <summary>Details</summary>
Motivation: 传统的阈值分割方法对噪声和细胞密度敏感；而深度学习方法简单地拼接偏振强度图像和相位图通道，未能有效利用这两种模态的互补性。

Method: DM-QPMNet是一个双编码器网络，将偏振强度图像和相位图视为不同的模态，分别进行编码。该架构通过多头注意力机制在中间深度融合模态特定特征，使偏振边缘和纹理表示能够选择性地整合互补的相位信息。它还通过双源跳跃连接和每模态归一化实现内容感知融合，以保持训练稳定性并增加多模态整合。

Result: 与单体拼接和单模态基线方法相比，DM-QPMNet在细胞分割方面表现出显著改进。

Conclusion: 模态特定的编码与可学习的融合机制能有效利用ssQPM同时捕获的互补照明和相位信息，从而实现鲁棒的细胞分割。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [83] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 本文提出了一种基于VQ-VAE的电子显微镜（EM）数据压缩框架，支持16x到1024x的压缩比，并实现了“按需解码”功能，包括极高压缩比下的顶部解码和通过Transformer prior恢复纹理，以及基于ROI的选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 拍字节级的电子显微镜（EM）数据集对存储、传输和后续分析造成了巨大压力，接近现有能力的极限。

Method: 该研究采用了一种基于矢量量化变分自编码器（VQ-VAE）的压缩框架。它通过可选的Transformer prior预测底部tokens，并使用特征级线性调制（FiLM）和拼接来恢复纹理。此外，还引入了区域兴趣（ROI）驱动的工作流程，仅在需要时从1024x压缩的潜在空间中选择性地进行高分辨率重建。

Result: 该框架实现了16x到1024x的EM数据压缩。它支持“按需解码”使用：可以仅解码顶部以实现极致压缩，或通过Transformer prior恢复纹理。此外，还能够基于ROI进行选择性高分辨率重建。

Conclusion: 该研究提供了一个高效且灵活的EM数据压缩框架，能够显著缓解拍字节级数据集在存储、传输和分析方面面临的挑战，并通过按需解码和ROI驱动的重建提供了实用性。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [84] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖高效的算法，用于在双曲空间中计算最优传输图，该算法通过扩展欧几里得和球面几何方法，并利用几何变分技术实现。


<details>
  <summary>Details</summary>
Motivation: 现有的最优传输图计算方法主要针对欧几里得空间和球面，但在涉及分层数据、网络和多亏格黎曼曲面等场景中，双曲空间中的最优传输问题自然出现，因此需要专门针对双曲空间的方法。

Method: 本文提出了一种新颖高效的算法，通过将欧几里得和球面几何中的方法扩展到双曲环境，并采用几何变分技术来计算双曲空间中的最优传输图。

Result: 通过在合成数据和多亏格曲面模型上进行的实验，验证了所提出方法的有效性。

Conclusion: 本文成功开发并验证了一种在双曲空间中计算最优传输图的有效算法，填补了现有方法主要集中于欧几里得和球面空间的空白。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [85] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文提出L48数据集，一个真实世界的细粒度多标签数据集，用于单正多标签(SPML)设置，旨在解决现有方法在合成数据集上表现不佳且无法反映真实世界复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的单正多标签(SPML)方法在合成数据集上进行开发和基准测试，这些数据集未能反映真实世界的复杂性，也未能捕捉导致错误分类的细粒度特征，从而导致与完全监督学习之间存在性能差距。因此，需要更真实、更具挑战性的基准。

Method: 本文引入L48数据集，这是一个从鸟类声音记录中提取的细粒度、真实世界多标签数据集。L48提供了一个自然的SPML设置，包含单正标注，并额外提供了两个扩展设置，通过领域先验获取了额外的负标签。研究者在此数据集上对现有SPML方法进行了基准测试。

Result: 在L48数据集上，现有SPML方法的性能与在合成数据集上的表现存在显著差异，并揭示了这些方法的弱点。这表明了更真实和困难的基准测试的必要性。

Conclusion: 研究表明，需要更真实、更具挑战性的基准数据集来准确评估和改进SPML方法，因为合成数据集无法捕捉真实世界的复杂性，导致现有方法在实际应用中表现不佳。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [86] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSDI的零样本对象感知4D人体运动生成框架，利用3D高斯表示、大语言模型和运动扩散先验，解决了视频扩散模型中缺乏3D物理先验导致的不真实变形问题，生成了自然且符合物理规律的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成的高质量视频仍存在不真实的变形、语义违规和物理不一致性，这主要源于缺乏3D物理先验。

Method: 该方法名为Motion Score Distilled Interaction (MSDI)，它是一个对象感知的4D人体运动生成框架，基于3D高斯表示和运动扩散先验。它使用预生成的3D人体和对象，并通过提出的Motion Diffusion Score Distillation Sampling (MSDS)结合大语言模型(LLMs)的空间和提示语义信息以及运动先验。MSDS和LLMs共同实现了空间感知的运动优化，从预训练的运动扩散模型中提取分数梯度，以在尊重对象和语义约束的同时优化人体运动。该方法是零样本的，无需在有限的交互数据集上进行联合训练。

Result: 实验证明，该框架生成了自然且符合物理规律的人体运动，这些运动尊重3D空间上下文，并且能够泛化到分布外（out-of-distribution）的对象感知人体运动，为真实的4D生成提供了可扩展的解决方案。

Conclusion: 通过结合3D物理先验、大语言模型和运动扩散模型，MSDI框架成功地解决了视频扩散模型中人体运动不真实的问题，实现了零样本、可扩展且物理上合理的对象感知4D人体运动生成。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [87] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 该研究开发了一个三阶段的深度学习流程，用于自动化处理大规模甲虫图像数据，包括检测、裁剪和形态分割，旨在提高生物学研究效率。


<details>
  <summary>Details</summary>
Motivation: 生物学家在昆虫学和生态学研究中需要收集和处理大量甲虫图像。传统的手动处理方法效率低下，难以应对大规模图像数据，因此需要一个自动化的数据处理流程来加速研究。

Method: 研究开发了一个三阶段的自动化流程：1. 检测托盘上的所有甲虫；2. 对每个甲虫图像进行排序和裁剪；3. 对裁剪后的甲虫进行形态分割。其中，检测阶段采用基于Transformer的开放词汇目标检测器和视觉-语言模型进行迭代处理。分割阶段则通过手动标注670张甲虫图像，并微调两种基于Transformer的分割模型实现。

Result: 该流程成功实现了对甲虫的精细形态分割，并达到了相对较高的准确性。整体而言，该自动化管道能够显著提高大规模甲虫数据处理的效率。

Conclusion: 该集成了多种深度学习方法的专业化甲虫图像处理流程，能够极大地提高大规模甲虫数据处理的效率，从而加速生物学研究进程。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [88] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 本文提出了一种名为MambaNetLK的新型3D点云配准方法，结合Mamba状态空间模型和Lucas-Kanade算法，以解决图像引导结肠镜检查中的配准挑战。同时发布了大规模临床数据集C3VD-Raycasting-10k，MambaNetLK在该数据集上表现优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 图像引导结肠镜检查中，准确的3D点云配准对于病灶定位、边缘评估和导航安全至关重要。然而，生物组织重复的纹理、局部同质的几何形状导致特征退化，术前解剖与术中观察之间的显著域偏移也进一步降低了对齐的稳定性，因此需要更鲁棒的配准方法。

Method: 研究人员创建了一个大规模、高质量的临床数据集C3VD-Raycasting-10k，包含10,014对几何对齐的点云。提出MambaNetLK，一个无对应关系的配准框架，通过将Mamba状态空间模型（SSM）作为跨模态特征提取器集成到PointNetLK架构中，以线性时间复杂度高效捕获长距离依赖。对齐通过迭代的Lucas-Kanade算法实现。

Result: 在临床数据集C3VD-Raycasting-10k上，MambaNetLK的性能优于现有最佳方法，将中位数旋转误差降低了56.04%，RMSE平移误差降低了26.19%。该模型还在ModelNet40上表现出强大的泛化能力，并对初始姿态扰动具有卓越的鲁棒性。

Conclusion: MambaNetLK为外科导航中的3D配准提供了坚实的基础。结合基于SSM的全局表达特征提取器和大规模临床数据集，该方法能够在结肠镜检查等微创手术中实现更准确、更可靠的引导系统。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [89] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 该研究引入了“Spot The Ball”基准测试来评估视觉社交推理能力，发现人类在根据社交线索推断移除的球的位置方面，比最先进的视觉语言模型准确2-3倍，表明AI在视觉社交推理方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 人类擅长视觉社交推理，即从行为线索（如凝视、姿势）推断场景中的隐藏元素，这对于日常社交推理和开发类人AI至关重要。当前AI在这方面能力不足。

Method: 引入了“Spot The Ball”基准测试，要求视觉语言模型从移除球的足球、篮球和排球图像中定位球。构建了一个人工评估集和可扩展的测试项生成流程。使用三种提示策略评估了四种最先进的视觉语言模型（Gemini、GPT、LLaMA、Qwen），并与人类基线进行比较。分析了模型和人类推理策略的差异。

Result: 人类的准确率（20-34%）始终是模型的（≤ 17%）的2到3倍。模型主要依赖于表面的空间启发式方法（如猜测图像中心或靠近玩家的位置），而人类则利用凝视方向和身体姿势等社交线索。这些发现揭示了视觉社交推理中持续存在的人类-模型差距。

Conclusion: 视觉社交推理方面存在显著的人类-模型差距。为了实现稳健的、类人的推理能力，需要开发能够明确编码结构化行为线索的AI架构。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [90] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 本文提出了一种联邦学习框架，将冻结的CLIP ViT与轻量级分类器结合，用于智能农业分类。该框架通过利用CLIP的特征提取能力，并共享少量非可逆特征，有效解决了传统联邦学习中数据隐私、非IID数据和高通信成本问题，在农业分类任务中实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 智能农业中的准确分类（如作物监测、水果识别、病虫害检测）至关重要。然而，传统的集中式训练面临大规模数据收集带来的隐私问题，而标准联邦学习则难以应对非独立同分布（non-IID）数据并产生高通信成本。因此，需要一种既能保护隐私，又能高效处理non-IID数据的农业分类方法。

Method: 研究者提出了一种联邦学习框架，该框架整合了冻结的对比语言-图像预训练（CLIP）视觉转换器（ViT）和一个轻量级转换器分类器。通过利用预训练CLIP ViT强大的特征提取能力，避免了从头开始训练大型模型，并将联邦更新限制在一个紧凑的分类器上，从而显著降低了传输开销。此外，为了缓解non-IID数据分布导致的性能下降，客户端之间共享了所有类别中一小部分（1%）CLIP提取的特征表示，这些特征是不可逆转为原始图像的，确保了隐私保护。

Result: 在农业分类任务上的实验结果表明，所提出的方法实现了86.6%的准确率，比基线联邦学习方法高出4倍以上。

Conclusion: 研究结果证明了将视觉-语言模型特征与联邦学习相结合的有效性和效率，为隐私保护和可扩展的农业智能提供了解决方案。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [91] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: 本文提出PersonalView，一种轻量级适配方法，仅需100个训练样本即可使现有生成模型获得多视角生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化生成模型虽然能生成身份一致的图像，但大多无法控制生成图像的视角，也无法生成一致的多视角人物图像。

Method: PersonalView包含两个核心组件：1. 设计了一种条件架构，以利用预训练扩散Transformer的上下文学习能力。2. 引入了一种新的语义对应对齐损失（Semantic Correspondence Alignment Loss），以保留预训练模型的原始生成能力。

Result: PersonalView在多视角一致性、文本对齐、身份相似性和视觉质量方面表现出色，仅用100个训练样本就显著优于那些使用大量多视角数据训练的基线模型。

Conclusion: PersonalView能以极少的训练样本（100个）有效地赋予现有生成模型强大的多视角生成能力，并且性能超越了需要大量数据的基线方法。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [92] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 本文提出了LITHOS，一个大规模、多样化的公开数据集和实验框架，用于自动化岩相学分析中的矿物分类。同时，提出了一种双编码器Transformer架构，通过整合两种偏振模式，显著提高了矿物分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 岩相学分析是地质学、考古学、工程、矿物勘探和石油工业中理解岩石性质的关键，但其高度依赖专家进行显微镜下的视觉检查，耗时且难以规模化，因此需要自动化技术。

Method: 研究引入了LITHOS数据集，包含211,604张高分辨率偏振光RGB图像补丁和105,802个专家标注的矿物颗粒（涵盖25种矿物类别，包括矿物类别、空间坐标及几何/方向信息）。研究评估了多种深度学习技术，并提出了一种双编码器Transformer架构，该架构整合了两种偏振模式，作为未来研究的基线。

Result: 所提出的双编码器Transformer架构在矿物分类任务中持续优于单偏振模型，证明了偏振协同作用在矿物分类中的价值。LITHOS基准（包括数据集、代码和预训练模型）已公开可用。

Conclusion: LITHOS是目前最大、最多样化的公开自动化岩相学实验框架，其提出的双编码器Transformer模型有效利用了偏振协同效应，显著提升了矿物分类性能。公开LITHOS基准旨在促进自动化岩相分析领域的可复现性和进一步研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [93] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 该研究系统评估了11种轻量级视觉模型在7个不同数据集上的表现，引入了跨数据集分数（xScore）来量化模型泛化能力，发现ImageNet准确性并非总能预测其他领域性能，并指出了促进泛化的关键架构元素。


<details>
  <summary>Details</summary>
Motivation: 轻量级视觉模型广泛部署于移动和嵌入式系统，但其性能主要在ImageNet上进行基准测试。研究旨在回答：在ImageNet上表现优异的模型是否能泛化到其他领域？如何系统量化跨数据集的鲁棒性？以及在资源受限下，哪些架构元素能持续驱动泛化？

Method: 研究对11种轻量级视觉模型（参数少于2.5M）进行了首次系统评估，在7个不同数据集上以固定的100个epoch进行训练。引入了跨数据集分数（xScore）作为统一指标，用于量化模型在不同视觉领域性能的一致性和鲁棒性。

Result: (1) ImageNet准确性无法可靠预测模型在细粒度或医学数据集上的性能。(2) xScore能提供可扩展的移动模型性能预测，只需四个数据集即可估算。(3) 某些架构组件，如具有更高空间分辨率的各向同性卷积和通道注意力，能促进更广泛的泛化；而基于Transformer的模块尽管参数开销更高，却几乎没有带来额外的好处。

Conclusion: 本研究提供了一个可复现的框架，用于评估ImageNet之外的轻量级视觉模型，突出了适用于移动设备的架构关键设计原则，并指导未来模型开发以实现在不同应用领域间的鲁棒泛化。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [94] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 本文提出了一种结合DeepONet和NTK的新型混合方法，用于解决复杂的逆问题，尤其擅长处理非线性、稀疏和噪声数据。


<details>
  <summary>Details</summary>
Motivation: 解决复杂逆问题（如由Navier-Stokes方程控制的源定位和图像重建）面临非线性、稀疏性和数据噪声等挑战。

Method: 该方法将深度算子网络（DeepONet）与神经正切核（NTK）相结合，并在损失函数中融入物理信息约束和任务特定正则化，以确保解的物理一致性和准确性。

Result: 该方法有效解决了源定位和图像重建任务，并在各种合成和真实数据集上验证了其鲁棒性、可扩展性和精确性。

Conclusion: 该混合方法在计算物理和成像科学中具有广泛的应用潜力，能够有效处理复杂的逆问题。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [95] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 针对会话中多模态情感识别（MERC）中模态缺失问题，FedDISC框架创新性地结合联邦学习和语义一致性扩散模型，通过联邦聚合和对话引导的语义对齐，有效恢复缺失模态并提升情感分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有MERC方法在实际场景中遇到不可预测的模态缺失时性能会显著下降。传统的模态恢复方法依赖于完整多模态数据训练，在极端数据分布（如固定模态缺失）下容易导致语义失真。

Method: 该研究提出了Federated Dialogue-guided and Semantic-Consistent Diffusion (FedDISC) 框架。它通过联邦聚合在客户端训练的模态特定扩散模型，并将其广播给缺失相应模态的客户端，从而克服单客户端对模态完整性的依赖。FedDISC包含一个DISC-Diffusion模块，该模块利用对话图网络捕捉会话依赖性以确保上下文和说话者身份一致性，并使用语义条件网络强制恢复模态与可用模态之间的语义对齐。此外，还引入了一种新颖的交替冻结聚合策略，循环冻结恢复和分类模块以促进协同优化。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上进行的大量实验表明，FedDISC在各种缺失模态模式下均实现了卓越的情感分类性能，优于现有方法。

Conclusion: FedDISC框架通过创新性地将联邦学习应用于模态缺失恢复，并结合对话引导和语义一致性扩散模型，有效解决了多模态情感识别中的模态缺失问题，显著提升了在复杂真实场景下的情感分类准确性。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [96] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen是一个生成框架，能直接从OpenStreetMap (OSM) JSON数据生成逼真的卫星图像，并能生成一致的“前后”图像对，以解决地理空间数据稀缺问题并支持城市规划和监测。


<details>
  <summary>Details</summary>
Motivation: 自动化城市监测面临挑战，因为用于特定城市特征及其变化的精选数据集稀缺。准确和最新的地理空间数据对于城市规划、基础设施监测和环境管理至关重要。

Method: 引入了OSMGen，一个生成框架，直接从原始OpenStreetMap (OSM) JSON数据（包括矢量几何、语义标签、位置和时间）创建逼真的卫星图像。与以往依赖栅格瓦片的工作不同，OSMGen利用了OSM JSON的全部丰富性。其核心特点是能够生成一致的“前后”图像对，用户对OSM输入的编辑会转化为有针对性的视觉变化，同时场景的其余部分保持不变。

Result: OSMGen能够从OSM数据生成逼真的卫星图像，并提供对场景生成细粒度的控制。它能生成一致的“前后”图像对，使用户对OSM的编辑能转化为有针对性的视觉变化。这使得生成训练数据以解决数据稀缺和类别不平衡成为可能，并为规划者提供了一种通过编辑地图数据来预览拟议干预措施的简单方法。此外，它还为静态和变化状态生成配对的(JSON, 图像)数据。

Conclusion: OSMGen通过生成训练数据解决了城市监测中的数据稀缺和类别不平衡问题，并为规划者提供了预览城市干预措施的工具。它为建立一个闭环系统奠定了基础，在该系统中，卫星图像可以自动驱动结构化的OSM更新。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [97] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的取证框架，通过分析多强度图像重建动态（称为“扩散回弹”）来有效识别AI生成图像，克服了传统方法对现代生成模型的失效问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成扩散模型的快速发展，区分真实和合成图像变得越来越困难。传统的深度伪造检测方法依赖频率或像素级伪影，但对于Stable Diffusion和DALL-E等生成逼真且无伪影图像的现代文本到图像系统已失效。

Method: 该研究引入了一个基于扩散的取证框架，利用多强度图像重建动态（ termed diffusion snap-back）来识别AI生成图像。通过分析重建指标（LPIPS、SSIM和PSNR）在不同噪声强度下的演变，提取可解释的基于流形的特征，以区分真实和合成图像。

Result: 该方法在包含4000张图像的平衡数据集上进行评估，在交叉验证下实现了0.993的AUROC。它对压缩和噪声等常见失真具有鲁棒性。尽管使用了有限的数据和单一扩散骨干模型（Stable Diffusion v1.5），该方法仍表现出强大的泛化性和可解释性。

Conclusion: 该方法为可扩展、模型无关的合成媒体取证奠定了基础，提供了一种有效且具有泛化性的AI生成图像检测方案。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [98] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文提出Oitijjo-3D，一个免费的生成式AI框架，利用公开的Google街景图像，在资源有限的国家（如孟加拉国）实现文化遗产的3D数字化重建，克服了传统方法成本高昂和技术专家稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国的文化遗产修复面临资源有限和技术专长稀缺的双重挑战。传统的3D数字化方法（如摄影测量或激光雷达扫描）需要昂贵的硬件、专业操作员和广泛的现场访问，这在发展中国家通常不可行。因此，孟加拉国许多建筑瑰宝仍易受损且缺乏数字形式。

Method: Oitijjo-3D是一个两阶段的管道：首先，利用Gemini 2.5 Flash Image进行多模态视觉推理，实现结构-纹理合成；其次，通过Hexagen进行神经图像到3D生成，恢复几何形状。该系统使用公开可用的Google街景图像作为输入。

Result: Oitijjo-3D能够快速（几秒内）生成逼真、度量一致的文化遗产3D模型，与传统方法相比显著加快了速度，且无需专业硬件或专家监督。在Ahsan Manzil、Choto Sona Mosque和Paharpur等标志性建筑上的实验表明，该系统在保留视觉和结构保真度的同时，大幅降低了经济和技术门槛。

Conclusion: Oitijjo-3D将开放图像转化为数字遗产，使3D文化遗产保护民主化，并将其重新定义为资源有限国家社区驱动、AI辅助的文化延续行为。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [99] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 本文提出了一种利用迁移学习和轻量级UNet模型，通过在公共数据集上预训练并在任务特定数据上微调，实现CubeSat热红外图像的板载云分割，提高了准确性并满足了实时处理需求。


<details>
  <summary>Details</summary>
Motivation: 热红外地球观测（EO）中的板载云分割是一项关键但未被充分探索的任务，尤其对于硬件和光谱信息有限的CubeSat任务。CubeSat通常只依赖一个热波段且缺乏足够的标注数据，使得传统云掩膜技术不可行。

Method: 研究采用迁移学习方法，将UNet模型与轻量级MobileNet编码器结合。模型首先在公共Landsat-7云覆盖评估数据集上进行预训练，然后使用少量任务特定样本（FOREST-2 CubeSat）进行联合训练微调。最后，将模型转换为TensorRT引擎以实现板载推理。

Result: 该方法将宏F1分数从0.850提高到0.877（相对于仅使用FOREST-2数据的基线）。在NVIDIA Jetson Nano上，全图像推理时间不到5秒。

Conclusion: 研究结果表明，利用公共数据集和轻量级架构可以在轨实现准确、高效的纯热红外云掩膜，支持数据受限EO任务中的实时决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [100] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD是一个基于视觉的放射学辅助框架，通过相机直接从显示器捕获医学图像，绕过了传统CAD系统在医院IT集成方面的障碍，实现了与现有系统的可比诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助诊断（CAD）系统在临床部署中面临的主要挑战是与医院现有IT基础设施的整合困难。

Method: VisionCAD框架通过一个相机系统直接从显示器捕获医学图像。它采用自动化流程，检测、恢复并分析屏幕上的医学图像，将相机捕获的视觉数据转换为诊断质量的图像，适用于自动化分析和报告生成。

Result: VisionCAD在多种医学影像数据集上得到了验证，其诊断性能与传统CAD系统在原始数字图像上的表现相当。在分类任务中，F1分数通常下降不到2%；自动化报告的自然语言生成指标与原始图像生成的结果相差不到1%。该系统模块化且能灵活利用最先进的诊断模型，仅需相机设备和标准计算资源。

Conclusion: VisionCAD为AI辅助诊断提供了一种可访问的部署方法，无需修改现有IT基础设施，即可在多样化的临床环境中实现诊断能力。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [101] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 本研究提出了一种基于强化学习的视频时刻检索模型，该模型通过多智能体系统和证据学习来解决不同模型定位结果之间的冲突，并能处理范围外查询，有效提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频时刻检索方案未能考虑不同模型定位结果之间的冲突，导致模型无法有效整合以产生更好的结果。

Method: 本研究引入了一个基于强化学习的视频时刻检索模型，该模型能一次性扫描整个视频以找到时刻边界并产生定位证据。此外，提出了一种多智能体系统框架，利用证据学习解决智能体定位输出之间的冲突。通过观察和处理智能体之间的冲突，模型还能在不额外训练的情况下判断查询是否没有对应的视频时刻（范围外查询）。

Result: 在基准数据集上的大量实验表明，所提出的方法与最先进的方法相比是有效的。研究结果还揭示，对多智能体系统中的竞争和冲突进行建模是提高时刻检索中强化学习性能的有效方法，并展示了证据学习在多智能体框架中的新作用。

Conclusion: 本研究提出的基于强化学习的多智能体系统结合证据学习，能够有效解决视频时刻检索中的定位冲突问题，提升检索性能，并能处理范围外查询，为多智能体框架下的强化学习和证据学习提供了新的视角。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [102] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练（大规模SFT和粗粒度到细粒度的ViRL策略）解决了现有模型泛化性差的问题，并在多项基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在专业任务上表现出色，但其单一任务训练范式阻碍了通用视觉代码智能（VISION Code Intelligence）的发展，无法很好地泛化到多种多模态代码生成任务。

Method: 该研究引入了VinciCoder模型，采用两阶段训练框架：
1. 构建了一个包含1.6M图像-代码对的大规模监督微调（SFT）语料库，用于直接代码生成和基于视觉的代码优化任务。
2. 引入了视觉强化学习（ViRL）策略，该策略使用从粗粒度到细粒度的奖励机制，通过计算局部和全局图像补丁的视觉相似性来提高视觉保真度。

Result: VinciCoder在各种多模态代码生成基准测试中均取得了最先进的性能，这证明了其粗粒度到细粒度的ViRL策略的有效性。

Conclusion: VinciCoder提出的统一多模态代码生成模型及其两阶段训练框架（特别是粗粒度到细粒度的ViRL策略）能够有效解决现有模型的局限性，并显著提升多模态代码生成任务的性能。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [103] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了FERBench基准测试，评估了多模态大语言模型（MLLMs）在面部表情识别（FER）任务上的表现，并揭示了它们在推理和可解释性方面的局限性。为解决此问题，作者引入了后训练策略和两个高质量数据集，并基于此开发了统一且可解释的FER基础模型UniFER-7B，其性能超越了现有许多通用MLLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在许多领域取得了成功，并且有将FER任务转换为视觉问答（VQA）格式以统一处理的趋势，但它们在面部表情识别（FER）任务上的性能仍未得到充分探索，并且在推理和可解释性方面存在显著局限性。

Method: 本文首先构建了FERBench，一个系统性基准，涵盖20个最先进的MLLMs和4个广泛使用的FER数据集，以评估MLLMs的FER性能。其次，为增强MLLMs的面部表情推理能力，作者引入了后训练策略，并为此策划了两个高质量大规模数据集：UniFER-CoT-230K（用于冷启动初始化）和UniFER-RLVR-360K（用于可验证奖励的强化学习）。最后，基于这些策略和数据集，开发了一个统一且可解释的FER基础模型UniFER-7B。

Result: 研究结果表明，虽然MLLMs在FER分类任务上表现良好，但它们在推理和可解释性方面仍面临显著局限性。通过提出的后训练策略和UniFER-7B模型，实现了性能的显著提升，UniFER-7B超越了许多开源和闭源的通用MLLMs（例如Gemini-2.5-Pro和Qwen2.5-VL-72B）。

Conclusion: MLLMs在面部表情识别任务中具有潜力，但需要专门的后训练策略来解决其在推理和可解释性方面的不足。UniFER-7B的成功证明了通过定制数据集和训练方法，可以构建出性能优异且可解释的FER基础模型。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [104] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 本文提出Localized-Globalized Cross-Alignment (LGCA) 框架，通过捕获局部特征并迭代扩展显著区域，同时结合原始和扩展图像的相似度得分，有效解决了CLIP模型中随机图像裁剪引入的误报和偏差问题，显著提升了零样本图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，对图像进行裁剪并利用大型语言模型生成多重描述可以增强CLIP模型的性能。然而，CLIP固有的敏感性使得随机图像裁剪可能引入误报和偏差，因为许多小尺度区域共享相似特征。

Method: 本文提出了Localized-Globalized Cross-Alignment (LGCA) 框架。该框架首先捕获图像的局部特征，然后反复选择并扩展最显著的区域。相似度得分的设计结合了原始图像和扩展后的图像，使得模型能够同时捕获局部和全局特征，并最小化误报。此外，论文还提供了理论分析，证明LGCA的时间复杂度在重复扩展过程之前与原始模型相同。

Result: 广泛的实验表明，LGCA方法显著提高了在不同数据集上的零样本性能，超越了现有最先进的基线模型。理论分析也证实了其效率和可扩展性，其时间复杂度与原始模型在扩展前保持一致。

Conclusion: LGCA框架通过有效整合局部和全局特征，并最小化由随机裁剪引入的误报，显著提升了CLIP模型在零样本图像分类任务中的性能。该方法在保持效率的同时，实现了对当前最先进基线的超越。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [105] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种名为ITEM的简单而有效的检测器，通过利用图像与文本描述在多模态空间中的语义错位来检测生成的假图像，解决了现有方法泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的发展，检测生成的假图像以防止恶意使用变得至关重要。现有方法将此视为二元图像分类任务，但它们只关注视觉线索，容易过拟合特定模式，且无法泛化到未见过的生成模型。

Method: 本文从多模态角度出发，观察到假图像与对应描述的对齐程度不如真实图像。基于此，提出了ITEM检测器，利用视觉-语言联合空间中的图像-文本错位作为判别线索。具体而言，首先在预训练CLIP空间中测量图像与描述的错位，然后微调一个MLP头部进行检测。此外，还提出了一个分层错位方案，结合全局图像和局部语义对象的错位信息。

Result: 广泛的实验表明，ITEM方法在各种最新生成模型上，相比其他最先进的竞争对手展现出卓越的泛化能力和鲁棒性。

Conclusion: 通过利用图像-文本在多模态空间中的语义错位，可以有效且普遍地检测生成的假图像，克服了传统视觉线索方法的局限性，提升了检测器的泛化能力和鲁棒性。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [106] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: 本文提出ToxicTextCLIP框架，通过生成高质量的对抗性文本，在预训练阶段对CLIP模型进行数据投毒和后门攻击，并成功绕过现有防御。


<details>
  <summary>Details</summary>
Motivation: CLIP模型依赖于未经验证的网络数据进行预训练，使其容易受到数据投毒和后门攻击。现有研究主要关注基于图像的攻击，而作为CLIP训练核心的文本模态的攻击风险尚未得到充分探索。

Method: ToxicTextCLIP框架通过迭代应用以下两个组件来解决语义错位和背景一致文本稀缺的问题：1) 背景感知选择器，优先选择与目标类别背景内容一致的文本；2) 背景驱动增强器，生成语义连贯且多样化的投毒样本。

Result: 在分类和检索任务上的大量实验表明，ToxicTextCLIP实现了高达95.83%的投毒成功率和98.68%的后门Hit@1，并成功绕过了RoCLIP、CleanCLIP和SafeCLIP等防御措施。

Conclusion: ToxicTextCLIP证明了通过高质量对抗性文本对CLIP模型进行有效投毒和后门攻击的可能性，揭示了CLIP在文本模态上的一个重要安全漏洞。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [107] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了首个统一框架，通过将显著性检测（SOD）、协同显著性检测（CoSOD）和显著性实例分割（SIS）任务转化为视觉-语言模型（VLM）中的思维链（CoT）推理过程来处理其异构性，并引入了信心引导策略优化（CGPO）和“输出到推理”策略以提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一处理操作上异构的显著性任务，且在强化学习（RL）中，思维链（CoT）的质量提升面临挑战，传统策略优化方法（如GRPO）存在信心无关学习、信号稀释和计算开销大等局限性。

Method: 该研究将SOD、CoSOD和SIS任务统一建模为VLM中的CoT推理过程。CoT训练采用两阶段范式：监督微调（SFT）和强化学习（RL）。为提升RL中CoT质量，提出信心引导策略优化（CGPO）算法，利用奖励与模型置信度之间的差异作为单样本优势信号。此外，引入“输出到推理”策略来构建高质量的SFT数据，确保与真实掩码的逻辑一致性。

Result: 实验结果表明，该模型在所有任务上均达到或超越了专用SOTA方法和强大的闭源VLM，尤其在CoCA数据集上的CoSOD任务中，S-measure达到0.899，比之前最佳结果高出8.0个百分点，且所需训练数据量远少。

Conclusion: 所提出的统一框架和训练策略（包括CGPO和“输出到推理”）能够有效桥接操作异构的显著性任务，并在性能上超越现有专业方法和大型VLM，证明了其处理复杂视觉-语言任务的强大能力和效率。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [108] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种基于频率分析的图像检测方法，通过增强不同频段的伪造线索（F^2C），有效识别扩散模型生成的图像，并展现出优异的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的高质量图像引发了潜在恶意使用的担忧。现有检测器在面对不同模型和设置时，泛化能力和鲁棒性不足，难以捕获区分性线索。

Method: 研究发现，扩散模型生成的图像与真实图像在低频到高频段的差异逐渐增大。基于此，提出通过增强所有频段的频率伪造线索（F^2C）来构建表示。具体而言，引入一个频率选择函数作为傅里叶频谱的加权滤波器，抑制区分度较低的频段，同时增强信息量更大的频段。

Result: 在各种扩散模型生成的图像数据集上进行的广泛实验表明，该方法在泛化性和鲁棒性方面均优于现有最先进的检测器。

Conclusion: 该方法通过对真实图像和扩散生成图像之间基于频率差异的全面分析，能够有效检测来自未知扩散模型的图像，并对各种扰动具有强大的弹性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [109] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 本研究提出了一种弱监督深度学习框架，利用图像级标签和Grad-CAM解释，实现肺炎的分类和定位，并在Kermany CXR数据集上取得了高准确率和可解释的视觉结果。


<details>
  <summary>Details</summary>
Motivation: 传统的像素级标注成本高昂，且医学影像诊断需要生成具有临床意义的热图以提高AI的透明度和临床信任度。

Method: 该研究采用弱监督深度学习框架，利用图像级标签而非像素级标签，并结合Grad-CAM生成热图。评估了七种ImageNet预训练架构（ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, ViT-B16），在相同训练条件下使用focal loss和按患者划分的数据集（Kermany CXR数据集）进行训练和验证，以防止数据泄露。

Result: 实验结果表明，ResNet-18和EfficientNet-B0在测试集上取得了最佳的整体准确率（98%）、ROC-AUC（0.997）和F1分数（0.987）。MobileNet-V2在准确性和计算成本之间提供了最佳平衡。Grad-CAM可视化证实模型关注临床相关的肺部区域。

Conclusion: 该工作突出了弱监督可解释模型在肺炎筛查中的潜力，能够增强AI辅助医疗影像的透明度和临床信任度，支持可解释AI在放射诊断中的应用。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [110] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，能从单张图像前向地联合建模3D人体外观和部件语义，并在3D人体重建和分割任务上超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在3D人体重建方面取得了高保真度，但它们在特定任务（如3D人体分割）上的实用性仍然受限。此外，标记的3D人体数据集稀缺。

Method: 本文提出了HumanCrafter框架，它在重建阶段整合了人体几何先验，在分割阶段整合了自监督语义先验。为解决数据稀缺问题，开发了一种交互式标注流程来生成高质量数据-标签对。通过像素对齐聚合实现跨任务协同，并采用多任务目标函数同时优化纹理建模保真度和语义一致性。

Result: HumanCrafter在3D人体部件分割和从单张图像进行3D人体重建方面均超越了现有的最先进方法。

Conclusion: HumanCrafter成功地实现了从单张图像对人体外观和部件语义的联合建模，并通过创新的方法解决了数据稀缺问题，显著提升了3D人体重建和分割的性能。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [111] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 本文提出了一种基于引导式深度学习的迭代框架，用于前庭神经鞘瘤（VS）的MRI图像分割，通过人工干预和专家共识，显著提高了分割精度和效率，并构建了一个高质量的公开数据集。


<details>
  <summary>Details</summary>
Motivation: 前庭神经鞘瘤（VS）的MRI精确分割对手术管理至关重要，但专家手动标注耗时且劳动密集。尽管深度学习（DL）已用于自动化分割，但在不同数据集和复杂临床病例中实现鲁棒性能仍面临挑战。

Method: 研究采用了一个引导式深度学习框架，结合迭代分割和质量细化，以生成VS的标注数据集。该方法整合了来自多个中心的数据，并依赖专家共识确保标注的可靠性。整个过程采用“人在回路”（human-in-the-loop）模型训练，并进行了专家评估以识别需要人工干预的细微病例。

Result: 该框架使自动化分割模型能够有效且资源高效地泛化到目标数据分布。在内部验证数据集上，分割精度显著提高，Dice相似系数（DSC）从0.9125增至0.9670，同时在代表性外部数据集上保持了稳定性能。与传统手动标注相比，效率提高了约37.4%。对143次扫描的专家评估揭示了模型改进的区域。最终发布了一个包含190名患者（534次纵向增强T1加权扫描和6次非标注T2加权扫描）的公开数据集。

Conclusion: 所提出的人在回路模型训练方法实现了高分割精度，并展示了其作为一种临床适应性强、可泛化的策略，用于在不同临床环境中自动化VS分割的巨大潜力。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [112] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat 是一种前馈方法，可从单张图像合成可控且显式的 4D 场景，它结合了视频扩散模型的生成先验和从 4D 数据集中学习的几何与运动约束。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决从单张图像高效、高质量地合成可控 4D 场景的挑战，避免测试时优化和后处理。

Method: 该方法将视频扩散模型的生成先验与从大规模 4D 数据集中学习的几何和运动约束相结合。给定单张输入图像、相机轨迹和可选文本提示，Diff4Splat 通过单一前向传播直接预测一个可变形的 3D 高斯场，编码外观、几何和运动。其核心是一个视频潜在变换器，用于捕获时空依赖性并预测时变 3D 高斯基元。训练由外观保真度、几何精度和运动一致性目标指导。

Result: Diff4Splat 能够在 30 秒内合成高质量的 4D 场景，并在视频生成、新视图合成和几何提取方面表现出有效性。它在动态场景合成方面匹配或超越了基于优化的方法，同时效率显著更高。

Conclusion: Diff4Splat 是一种高效、前馈的方法，能够从单张图像合成高质量、可控的 4D 场景，其性能在质量上可与基于优化的方法媲美，并在效率上显著超越。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [113] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer是一个新颖的框架，通过引入分层身份保留注意力机制、预训练视觉-语言模型进行语义理解和在线强化学习，解决了从文本提示和多张参考图像生成多主体视频的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型通常仅限于文本或单张图像作为条件，这限制了其可控性和适用性，尤其是在需要生成具有多个主体的视频时，难以保持主体身份、整合语义并维持时间一致性。

Method: ID-Composer采用了以下方法：1. **分层身份保留注意力机制**：有效聚合主体内部和跨主体及模态的特征，以忠实地保留主体一致性和文本信息。2. **通过预训练视觉-语言模型（VLM）进行语义理解**：利用VLM卓越的语义理解能力，提供细粒度指导并捕捉多主体间的复杂交互。3. **在线强化学习阶段**：解决标准扩散损失在对齐主体ID等关键概念方面的不足，将整体训练目标驱动到RLVR。

Result: 广泛的实验证明，ID-Composer模型在身份保留、时间一致性和视频质量方面均超越了现有方法。

Conclusion: ID-Composer成功地解决了从文本提示和多张参考图像生成多主体视频的挑战，显著提升了视频生成的可控性和质量，特别是在身份保留和时间一致性方面表现优异。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [114] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出了FedMGP，一种用于视觉-语言模型个性化联邦提示学习的新范式，通过多组提示、多样性损失和动态聚合策略，实现了最先进的性能和最低的通信参数。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言模型中，需要一种能够捕捉多样化、细粒度语义和实例级线索的个性化联邦提示学习方法，同时平衡通用知识的保留与客户端特定特征，并保持参数效率。

Method: FedMGP为每个客户端配备多组成对的文本和视觉提示；引入多样性损失以促使每组提示专注于不同且互补的语义方面；采用基于相似度引导概率采样的动态提示聚合策略，通过余弦相似度和softmax加权分布选择性聚合语义对齐的知识；通过在多组中重新分配固定提示容量来保持参数效率；进行了理论分析以证明动态聚合策略的有效性。

Result: FedMGP在所有联邦提示学习方法中，以最低的通信参数实现了最先进的性能。它在个性化和领域泛化方面，始终优于现有方法。理论分析表明，动态聚合策略通过强化共享语义和抑制客户端特定噪声，促进了鲁棒的全局表示学习。

Conclusion: FedMGP是一种有效且高效的个性化联邦提示学习范式，其设计能够捕捉多样化特征、平衡通用知识与客户端特定需求，并在视觉-语言基准测试中展现出卓越的性能和参数效率。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [115] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 本文介绍了VinDr-CXR-VQA，一个用于可解释医学视觉问答（Med-VQA）并具有空间定位功能的大规模胸部X光数据集。该数据集包含17,597个问答对，覆盖4,394张图像，并带有放射科医生验证的边界框和临床推理解释。基准测试显示，该数据集使模型性能F1值提升了11.8%，并支持病灶定位。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了推动可复现且具有临床依据的医学视觉问答（Med-VQA）研究，特别是通过提供一个大规模、高质量、具有空间定位和解释能力的胸部X光数据集，以解决现有数据集中可能存在的幻觉问题，并提升模型可靠性。

Method: 研究方法是构建了一个名为VinDr-CXR-VQA的大规模胸部X光数据集。该数据集包含4,394张图像和17,597个问答对，每个问答对都由放射科医生验证的边界框和临床推理解释进行标注。问题分类涵盖六种诊断类型，并构建了平衡的阳性（41.7%）和阴性（58.3%）样本分布，以减少正常情况下的幻觉。研究还使用MedGemma-4B-it模型进行了基准测试。

Result: 基准测试结果显示，使用VinDr-CXR-VQA数据集训练的MedGemma-4B-it模型性能显著提升（F1 = 0.624），比基线提高了11.8%，同时能够实现病灶定位。这表明该数据集有效提高了Med-VQA模型的可靠性和解释性。

Conclusion: VinDr-CXR-VQA是一个重要且公开可用的资源，它通过提供大规模、高质量、包含空间定位和临床推理解释的胸部X光问答数据，显著推动了可复现和具有临床依据的医学视觉问答（Med-VQA）研究。该数据集能够提高模型的性能并使其具备病灶定位能力。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [116] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练或偏置标注的测试时去偏方法，通过预训练分割模型隔离目标视觉属性，并调整非目标区域的嵌入，以减轻ViT-based CLIP模型中的虚假关联。


<details>
  <summary>Details</summary>
Motivation: 现有的去偏方法通常需要训练数据、明确的组标签或数据集特定的偏置先验知识，这限制了它们在实际应用和开放集设置中的实用性和通用性。

Method: 该方法是一种针对ViT-based CLIP模型的测试时去偏方法。它使用预训练的分割模型来隔离目标视觉属性，然后调整非目标区域，使其嵌入与所有类别特定的文本提示均匀相似，从而在保留目标属性的同时消除混淆视觉区域的意外偏置信号。

Result: 在Waterbirds和CelebA数据集上的实验表明，该方法在组鲁棒性指标和注意力IoU方面均优于现有的测试时去偏方法。

Conclusion: 这些结果证明了分割引导干预在视觉语言模型中实现可扩展且无需标注的偏置缓解方面的有效性。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [117] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 针对水下恶劣环境，传统相机表现不佳。本文提出一个管道，用于生成AUV上事件相机在水下环境的逼真合成数据，以训练视觉模型，并通过水下岩石检测任务验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境（如光照差、高动态范围）对机器人和计算机视觉研究构成巨大挑战，导致传统视觉技术性能不佳。

Method: 引入一个管道，能够生成AUV上事件相机在水下环境的逼真合成数据，用于训练视觉模型。

Result: 通过在低能见度和悬浮颗粒物条件下的岩石检测任务，证明了所提出管道的有效性。该方法可推广到其他水下任务。

Conclusion: 所提出的合成数据生成管道能有效解决水下恶劣环境的视觉问题，为训练事件相机视觉模型提供逼真数据，并具有通用性。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [118] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 本文研究了一种域适应训练方法，通过翻转数据预训练结合微调，显著提高了自动驾驶模型PilotNet在左侧驾驶条件下的性能和注意力分布，并验证了其对不同架构的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要良好的泛化能力以适应不同的道路条件。本研究旨在探索一种域适应训练方法，使基于深度学习的端到端模型（如PilotNet）能够适应左侧驾驶条件，特别是使用真实的澳大利亚高速公路数据。

Method: 研究评估了四种训练方法：(1) 在美国右侧驾驶数据上训练的基线模型；(2) 在翻转的美国数据上训练的模型；(3) 在美国数据上预训练后在澳大利亚高速公路上微调的模型；(4) 在翻转的美国数据上预训练后在澳大利亚高速公路上微调的模型。通过转向预测精度和注意力（使用显著性分析衡量）来比较模型性能。同时，在ResNet架构上重复实验以验证方法的普适性。

Result: 结果显示，单独使用翻转数据预训练会因特征表示不一致而降低预测稳定性。然而，当翻转数据预训练后结合微调时，模型的适应性显著提高，预测误差降低，并且注意力更集中于左侧线索。在ResNet上的实验也证实了类似的适应趋势。

Conclusion: 研究强调了预处理技术（如翻转数据预训练）结合微调对于提高模型域适应能力的重要性，且仅需最少的再训练。这为自动驾驶模型在不同驾驶条件下的泛化提供了有效的策略。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [119] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 本文提出了一种记忆增强的零样本异常检测方法，通过结合上下文信息、时间演化和视觉特征，实现了对视频中新事件的实时、高精度异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测器未能考虑上下文信息，导致其泛化能力受限，无法适应真实世界的复杂场景，因为在不同上下文中，相同的行为可能表现为正常或异常。

Method: 该方法构建了一个记忆增强的管道，利用交叉注意力机制关联时间信号与视觉嵌入，并通过上下文相似度评分进行实时零样本异常分类。

Result: 该模型在UCF-Crime数据集上取得了90.4%的AUC，在XD-Violence数据集上取得了83.67%的AP，刷新了零样本模型的最新SOTA。此外，模型实现了实时推理，并具有高精度和可解释性。

Conclusion: 通过融合交叉注意力时间融合和上下文记忆，该方法实现了高保真度的异常检测，是零样本模型在真实世界监控和基础设施监测中应用的重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [120] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 本文提出T-VAD框架，利用大型视觉语言模型（LVLM）实现细粒度视频异常检测，通过生成异常热图和文本描述，显著提升了检测的粒度和交互性。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测（VAD）方法通常是半自动的，需要人工评估，且输出仅限于“正常”或“异常”，缺乏细粒度信息。在监控和工业过程监测等场景中，细致的异常检测至关重要。

Method: 本文提出了Text-guided Fine-Grained Video Anomaly Detection (T-VAD) 框架，基于大型视觉语言模型（LVLM）。它包含一个异常热图解码器（AHD），用于执行像素级视觉-文本特征对齐以生成细粒度异常热图；以及一个区域感知异常编码器（RAE），将热图转换为可学习的文本嵌入，指导LVLM准确识别和定位视频中的异常事件。

Result: T-VAD在UBnormal数据集上实现了SOTA性能，微AUC达到94.8%，异常热图准确率（RBDC/TBDC）分别为67.8%/76.7%。在ShanghaiTech-based数据集上，目标和轨迹的BLEU-4分数分别为62.67和88.84，Yes/No准确率为97.67%。在UBnormal数据集上，目标和轨迹的BLEU-4分数分别为50.32和78.10，Yes/No准确率为89.73%。

Conclusion: 所提出的T-VAD方法通过生成细粒度异常热图和提供文本描述，显著增强了异常检测的粒度和交互性，并在多个数据集上取得了最先进的性能和更优的文本描述效果。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [121] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 本文提出一个基于SE(3)流匹配的概率框架，用于估计6D物体姿态分布，解决了传统方法在遮挡和对称性下的姿态模糊和多模态问题。该方法通过样本建模完整姿态分布，实现了最先进的性能，并可应用于机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 物体姿态估计在机器人学和计算机视觉中是基础且具挑战性的问题，因为部分可观察性、遮挡和物体对称性会导致姿态模糊和多重假设。现有的确定性深度网络在这些情况下往往过于自信，无法捕捉潜在姿态分布的多模态特性。

Method: 本文提出一种新颖的概率框架，利用SE(3)流匹配来估计6D物体姿态分布。该方法通过基于样本的估计来建模完整的姿态分布，从而能够推理模糊情况（如对称物体或严重遮挡）下的不确定性。

Result: 该方法在Real275、YCB-V和LM-O数据集上取得了最先进（state-of-the-art）的结果。此外，研究展示了其基于样本的姿态估计如何应用于下游机器人操作任务，例如用于消除不确定视点歧义的主动感知，或以不确定性感知的方式指导抓取合成。

Conclusion: 该概率框架成功解决了6D物体姿态估计中的模糊性和多模态挑战，通过建模完整的姿态分布，实现了卓越的性能，并为不确定性感知的机器人操作任务提供了有效支持。

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [122] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个元数据引导的框架，通过对齐MRI图像与DICOM采集参数来学习MRI对比度表示，从而实现序列识别、数据协调和质量控制。


<details>
  <summary>Details</summary>
Motivation: MRI数据存在显著的异质性，且缺乏标准化的对比度标签，这严重限制了大规模自动化分析。需要一种统一的MRI对比度表示，以实现各种下游应用，而无需手动标注。

Method: 引入了MR-CLIP框架，该框架通过将体素图像与其DICOM采集参数对齐，学习MRI对比度表示。它利用常规可用的采集元数据作为监督信号。

Result: 学习到的嵌入显示出MRI序列的明显聚类。在数据稀缺的条件下，MR-CLIP在少样本序列分类中优于有监督的3D基线。此外，MR-CLIP通过图像-元数据嵌入距离，能够识别损坏或不一致的元数据，从而实现无监督的数据质量控制。

Conclusion: MR-CLIP通过将元数据转化为监督信号，为跨多样化临床数据集的标签高效MRI分析提供了一个可扩展的基础，解决了MRI数据异质性和缺乏标准化标签的问题。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [123] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA是首个支持像素级推理和多模态（文本与视觉）提示的视觉-语言-动作模型，通过新的指令微调框架和自动标注数据集Pixel-160K，显著提升了机器人操作成功率，并降低了预训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作模型（VLAs）在像素级场景理解方面表现不佳，并且过度依赖文本提示，这限制了它们在实际应用中的灵活性。

Method: 本文提出了PixelVLA，它基于一个新的视觉运动指令微调框架，该框架集成了多尺度像素感知编码器和视觉提示编码器。为了有效训练PixelVLA，研究者还提出了一个两阶段自动化标注流程，从现有机器人数据中生成了包含像素级标注的大规模数据集Pixel-160K。

Result: 在三个标准VLA基准测试和两种VLA模型变体上的实验表明，PixelVLA将操作成功率比OpenVLA提高了10.1%-17.8%，而预训练成本仅为其1.5%。

Conclusion: PixelVLA能够集成到现有VLA中，从而在复杂环境中实现更准确、高效和多功能的机器人控制。数据集和代码将开源。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [124] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 该研究提出了一种判别式处理运动分量的方法，通过利用刚性流的几何规律和相机对齐，显著提升了无监督深度和自我运动估计的鲁棒性和性能，尤其是在复杂条件下。


<details>
  <summary>Details</summary>
Motivation: 现有方法将自我运动视为辅助任务，混淆所有运动类型或排除与深度无关的旋转运动，限制了几何约束的应用，降低了在多样化条件下的可靠性和鲁棒性。

Method: 该研究引入了对运动分量的判别式处理，利用其各自刚性流的几何规律。通过对齐源相机和目标相机的光轴及成像平面，转换帧间光流并量化偏差，对每个自我运动分量施加几何约束。此外，将联合学习过程重新构建为同轴和共面形式，通过封闭形式的几何关系相互推导深度和每个平移分量，引入互补约束以提高深度鲁棒性。

Result: 所提出的DiMoDE框架在多个公开数据集以及一个新收集的多样化真实世界数据集上，尤其是在具有挑战性的条件下，实现了最先进的性能。

Conclusion: 通过对运动分量的判别式处理、利用刚性流的几何规律以及创新的相机对齐和联合学习重构，DiMoDE框架有效提升了无监督深度和自我运动估计的准确性和鲁棒性。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [125] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 该论文引入了Real-IAD Variety，一个最大且最多样化的工业异常检测（IAD）基准，旨在解决现有数据集的局限性，并推动可扩展、通用异常检测系统的发展。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测（IAD）算法受到公共基准数据集的严重限制，这些数据集类别多样性不足、规模有限，导致模型性能饱和且难以迁移到真实世界场景。

Method: 研究者构建了一个名为Real-IAD Variety的新型基准数据集，包含198,960张高分辨率图像，涵盖160个不同物体类别，并确保了28个行业、24种材料类型和22种颜色变化的全面覆盖。他们还对最先进的多类别无监督异常检测方法和视觉-语言模型进行了实验分析。

Result: 实验结果表明，最先进的多类别无监督异常检测方法在类别从30扩展到160时性能显著下降。然而，视觉-语言模型对类别规模扩展表现出卓越的鲁棒性，性能变化极小，显著增强了在多样化工业环境中的泛化能力。

Conclusion: Real-IAD Variety基准数据集凭借其前所未有的规模和复杂性，成为训练和评估下一代异常检测基础模型的关键资源。它将通过提供严格的评估协议，加速研究超越领域特定限制，促进可扩展、通用异常检测系统的开发。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [126] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种从单张图像中精确学习和合成多实例语义的方法，通过惩罚机制注意力优化和注意力层中的边界框控制，解决了数据有限和实例相似性带来的挑战，实现了高质量的语义解耦和合成。


<details>
  <summary>Details</summary>
Motivation: 从单张图像中学习和合成多实例语义面临训练数据有限的困难，当实例具有相似语义或外观时，问题变得更具挑战性。

Method: 本文提出了一种基于惩罚的注意力优化方法，用于在学习阶段解耦相似语义。在合成阶段，引入并优化注意力层中的边界框控制，以进一步减少语义泄露并精确控制输出布局。

Result: 实验结果表明，该方法实现了高质量、解耦的语义学习和合成，显著平衡了可编辑性和实例一致性。该方法在处理语义或视觉上相似的实例或罕见对象时，仍然保持鲁棒性。

Conclusion: 本文提出了一种有效的方法，能够从单张图像中精确学习和合成多实例语义，解决了数据稀缺和实例相似性带来的挑战，并提供了高质量、可控的输出。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [127] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出了3EED，一个用于多平台、多模态户外环境的3D视觉定位基准，解决了现有基准在规模和泛化性上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉定位基准主要局限于室内环境、单一平台且规模较小，无法满足具身智能体在开放世界中定位语言所指对象的需求。

Method: 引入了3EED数据集，包含来自车辆、无人机和四足机器人平台的多模态（RGB和LiDAR）数据，拥有超过12.8万个对象和2.2万个参照表达。开发了结合视觉-语言模型提示和人工验证的可扩展标注流程，以确保高质量的空间定位。提出了平台感知归一化和跨模态对齐技术，并建立了域内和跨平台评估的基准协议。

Result: 研究发现存在显著的性能差距，凸显了可泛化3D定位的挑战和机遇。

Conclusion: 3EED数据集和基准工具的发布旨在推动语言驱动的3D具身感知领域的未来研究，提供了一个更具挑战性和真实世界场景的评估平台。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [128] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 针对动态场景中3D Gaussian Splatting的内存开销问题，本文提出4D Neural Voxel Splatting (4D-NVS)，通过结合体素和神经高斯splatting，显著降低内存并加速训练，同时保持高质量实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3D-GS) 在动态场景中，由于在不同帧间复制高斯点，导致巨大的内存开销。

Method: 本文提出4D Neural Voxel Splatting (4D-NVS) 方法。它结合了基于体素的表示和神经高斯splatting，通过使用一组紧凑的神经体素和学习到的形变场来建模时间动态，而非为每个时间戳生成独立的高斯集。此外，该方法引入了一个新颖的视图细化阶段，通过有针对性的优化来选择性地改善具有挑战性的视点。

Result: 实验表明，4D-NVS显著减少了内存消耗，加速了训练，同时保持了高质量的图像。它在内存减少和训练速度方面超越了现有最先进的方法，实现了具有卓越视觉保真度的实时渲染。

Conclusion: 4D-NVS通过结合体素表示和神经高斯splatting，有效解决了动态场景中3D-GS的内存和训练效率问题，实现了更优的视觉质量和实时渲染能力。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [129] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER是一种基于新颖性搜索的方法，利用LLM对提示词进行语义演化，并结合CLIP量化新颖性，旨在从单个提示词生成多样化的图像集，显著提升了图像生成的多样性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在生成高质量图像时，常面临输出多样性有限的问题，这限制了它们在探索性和创意任务中的应用。现有提示词优化技术主要关注美学适应性，不适合创意视觉领域。

Method: WANDER直接在自然语言提示词上操作，使用大型语言模型（LLM）进行提示词的语义演化，并利用CLIP嵌入来量化生成图像的新颖性。此外，还引入了发射器（emitters）来引导搜索进入提示词空间的不同区域，以进一步提升多样性。

Result: 经验评估表明，WANDER在多样性指标上显著优于现有的演化式提示词优化基线方法。消融研究也证实了发射器（emitters）的有效性。

Conclusion: WANDER通过结合LLM进行语义演化和基于新颖性搜索，有效解决了文本到图像模型输出多样性不足的问题，并显著提升了图像生成的多样性。发射器（emitters）的应用进一步增强了这一效果。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [130] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 本文提出了一种通过粒度一致的2D掩码跟踪和三阶段课程学习框架，解决3D实例分割中由独立处理2D伪标签导致的标签不一致问题，从而生成一致且准确的3D分割结果，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D实例分割方法通过将2D基础模型的掩码转换为3D伪标签来避免昂贵的手动标注，但由于独立处理视频帧，导致分割粒度不一致和3D伪标签冲突，从而降低了最终分割的准确性。

Method: 本文引入了一种“粒度一致的自动2D掩码跟踪”方法，以维护帧间的时间对应关系并消除冲突的伪标签。此外，结合了一个三阶段的课程学习框架，该框架逐步从碎片化的单视图数据训练到统一的多视图标注，最终实现全局一致的全场景监督。

Result: 实验结果表明，所提出的方法能够有效地生成一致且准确的3D分割。此外，该方法在标准基准测试上取得了最先进的结果，并展现了开放词汇能力。

Conclusion: 通过粒度一致的2D掩码跟踪和结构化的课程学习流程，本文的方法能够从最初碎片化和矛盾的2D先验中稳健地提取出一致的3D表示，从而实现准确的3D实例分割，并达到最先进的性能和开放词汇能力。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [131] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: 本文提出了FedOnco-Bench，一个可复现的联邦学习基准，用于在肿瘤CT图像分割中评估隐私保护FL方法的性能和隐私泄露，并展示了隐私与效用之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在保护隐私的环境中具有巨大潜力，但仍面临成员推断攻击和数据异质性的漏洞。

Method: 引入FedOnco-Bench，一个使用带有肿瘤注释的合成肿瘤CT扫描的基准平台。它评估了FedAvg、FedProx、FedBN和带有DP-SGD的FedAvg等FL方法的分割性能和隐私泄露。

Result: 结果显示了隐私和效用之间明显的权衡：FedAvg性能高（Dice约0.85）但隐私泄露多（攻击AUC约0.72）；DP-SGD提供了更高的隐私级别（AUC约0.25）但牺牲了准确性（Dice约0.79）。FedProx和FedBN在异构数据下，特别是非同分布的客户端数据下，提供了平衡的性能。

Conclusion: FedOnco-Bench作为一个标准化、开源的平台，可用于医学图像分割中隐私保护FL方法的基准测试和开发，并揭示了隐私与模型效用之间的固有权衡。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [132] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种名为DS_GCD（Domain-Shifted Generalized Category Discovery）的更现实任务，并提出了FREE（Frequency-guided Generalized Category Discovery）框架，通过利用频域信息来增强模型在存在域漂移时发现已知和未知类别的能力。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现（GCD）方法在标准条件下表现良好，但在存在分布漂移时性能会下降。为了解决这一限制，研究旨在探索一个更现实的任务——DS_GCD，即未标记数据不仅包含未知类别，还包含来自未知域的样本。

Method: 本文提出了FREE框架，它利用频域信息来解决DS_GCD问题。具体方法包括：1) 提出一种基于频率的域分离策略，通过测量振幅差异将样本划分为已知和未知域。2) 提出两种频域扰动策略：跨域策略（通过交换域间的振幅分量来适应新分布）和域内策略（增强未知域内变化的鲁棒性）。3) 扩展自监督对比目标和语义聚类损失以指导训练。4) 引入聚类难度感知重采样技术，自适应地关注更难聚类的类别。

Result: 广泛的实验证明，所提出的方法能有效缓解分布漂移的影响，并在各种基准数据集上实现了卓越的性能，成功发现了已知和未知类别。

Conclusion: FREE框架通过利用频域信息和一系列创新策略，有效解决了域漂移下的广义类别发现问题，显著提高了模型在更现实场景中发现已知和未知类别的能力。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [133] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本文引入了CueBench，首个专注于上下文感知视频异常理解（VAU）的基准，并提出了Cue-R1模型，显著提高了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常理解方法对真实世界异常的理解停留在表面，缺乏对区分异常与正常行为的复杂原理和微妙上下文的深入理解。

Method: 本文提出了CueBench基准，包含事件中心的层次分类（14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性），并统一了识别、时间定位、检测和预测等多种上下文感知VAU任务的评估。为应对CueBench的挑战，本文进一步开发了Cue-R1模型，该模型基于R1风格的强化微调，采用可验证、任务对齐和层次细化的奖励机制，以统一的生成方式进行训练。

Result: 在CueBench上的广泛实验表明，现有视觉-语言模型（VLMs）在真实世界异常理解方面仍远未达到令人满意的水平，而本文提出的Cue-R1模型平均超越这些最先进方法24%以上。

Conclusion: 现有模型在真实世界视频异常理解方面仍有很大差距。CueBench为上下文感知VAU提供了一个严格的评估框架，而Cue-R1模型则为解决这一挑战提供了一个有效的新方法。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [134] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GUI-AIMA的高效、无坐标的GUI接地框架，它通过监督微调，将多模态大语言模型（MLLMs）的内在注意力与自适应的补丁级接地信号对齐，以解决现有方法直接生成精确坐标的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的GUI接地方法通常将任务表述为基于文本的坐标生成，但直接从视觉输入生成精确坐标既困难又计算密集。研究者观察到通用MLLMs在其注意力机制中具有一定的原生接地能力，并认为更直观的方法是先选择相关视觉补丁，再确定精确点击位置。

Method: GUI-AIMA是一个基于注意力的、无坐标的监督微调框架。它通过在简化的查询-视觉注意力矩阵上进行多头聚合，自适应地计算补丁级接地信号，并将这些信号与MLLMs的内在多模态注意力对齐。其无坐标特性还使其能够轻松集成一个即插即用的放大阶段。

Result: GUI-AIMA-3B仅使用8.5万张屏幕截图进行训练，展现了卓越的数据效率，并验证了轻量级训练可以激活MLLMs的原生接地能力。它在3B模型中达到了最先进的性能，在ScreenSpot-Pro上平均准确率为58.6%，在OSWorld-G上为62.2%。

Conclusion: GUI-AIMA证明了通过轻量级训练可以有效激活MLLMs的原生接地能力，并提供了一种高效且无坐标的GUI接地解决方案，在当前3B模型中取得了领先的性能。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [135] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为“三元组分割”的新任务，通过将手术器械实例分割与动作三元组（器械、动词、目标）相结合，实现手术动作的空间定位理解。为此，我们创建了大规模数据集CholecTriplet-Seg并提出了TargetFusionNet模型，显著提高了手术动作识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的手术动作三元组识别方法仅限于帧级别分类，无法可靠地将动作与特定的器械实例关联，并且空间定位方法（如类激活图）缺乏分析器械-组织交互所需的精度和鲁棒性。

Method: 1. 提出了“三元组分割”任务，即通过器械实例分割实现<器械、动词、目标>输出的空间定位。2. 构建了CholecTriplet-Seg数据集，包含30,000多帧带器械实例掩码和动作三元组标注的数据。3. 提出了TargetFusionNet架构，该架构扩展了Mask2Former，并引入了目标感知融合机制，通过融合弱解剖先验与器械实例查询来提高解剖目标预测的准确性。

Result: TargetFusionNet在识别、检测和三元组分割等指标上均优于现有基线模型。结果表明，结合强实例监督和弱目标先验显著提高了手术动作理解的准确性和鲁棒性。

Conclusion: 三元组分割为手术动作三元组的空间定位提供了一个统一的框架。所提出的基准数据集和架构为更具可解释性的手术场景理解铺平了道路。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [136] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为LSSA的新型多模态对抗攻击方法，通过局部图像块打乱和采样来增加输入多样性，有效解决了现有方法过拟合问题，显著提升了对抗样本在VLP模型间的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言预训练（VLP）模型易受对抗样本攻击。现有跨模态攻击方法通过跨模态交互提高对抗可迁移性，但因过度依赖单一模态的对抗信息而缺乏输入多样性，导致过拟合问题。

Method: 本文提出局部打乱和采样攻击（LSSA）。LSSA随机打乱局部图像块以扩展原始图像-文本对，生成对抗图像并在其周围采样。随后，利用原始和采样图像共同生成对抗文本。

Result: LSSA显著增强了多模态对抗样本在不同VLP模型和下游任务之间的可迁移性。此外，LSSA在大型视觉-语言模型上的表现优于其他先进攻击方法。

Conclusion: LSSA通过引入输入多样性，有效解决了现有跨模态对抗攻击的过拟合问题，显著提升了对抗样本在VLP模型中的可迁移性，证明了其作为一种新型高效攻击方法的有效性。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [137] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为视觉对比注意力（VCA）的新型注意力机制，作为Vision Transformers（ViTs）中多头自注意力（MHSA）的替代品。VCA通过引入显式的判别概念并利用空间池化和正负流交互来聚焦于视觉对比，从而将理论复杂度从O(N²C)降低到O(NnC)，同时显著提升了图像识别和生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）的MHSA层在每个token对之间执行二次查询-键交互，消耗了大量计算资源在视觉上较弱或冗余的关联上，导致计算效率低下。

Method: VCA首先将每个头的密集查询场提炼成少量空间池化的视觉对比tokens，然后将它们分成可学习的正向和负向流。通过这两个流的差异化交互，VCA突出真正区分不同区域的信息，从而降低了理论复杂度。该模块作为MHSA的即插即用替代品，参数量小，无需额外FLOPs，且与架构无关。

Result: 实验结果表明，VCA将DeiT-Tiny在ImageNet-1K上的top-1准确率从72.2%提升到75.6%（+3.4%），并使三种强大的分层ViT模型性能提升高达3.1%。在条件图像生成任务中，VCA使扩散模型（DiT）和流模型（SiT）的FID-50K分别降低了2.1到5.2点。广泛的消融实验证实了空间池化提供低方差全局线索、双重位置嵌入对对比推理不可或缺以及两者结合能产生最强协同效应。

Conclusion: VCA为Vision Transformers提供了一条简单有效的途径，使其变得更快、更清晰。通过聚焦于视觉对比和降低计算复杂度，VCA在图像识别和生成任务中都取得了显著的性能提升。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [138] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了一种针对图像超分辨率（SR）网络的后训练量化（PTQ）方法，通过双区域量化策略处理激活中的异常值，并引入敏感度感知微调来关注敏感层，从而在实现显著加速的同时，性能可与量化感知训练（QAT）方法媲美。


<details>
  <summary>Details</summary>
Motivation: 现有的SR网络PTQ方法在处理激活中的异常值时表现不佳，这些异常值与图像颜色信息强相关，且直接移除会导致性能显著下降。此外，不同网络层对量化敏感度不同，导致性能下降程度不一。

Method: 1. 提出了双区域量化策略：将激活值划分为异常值区域和密集区域，对每个区域独立进行均匀量化，以更好地平衡位宽分配。2. 引入了敏感度感知微调：促使模型更多地关注高度敏感的层，进一步提升量化性能。

Result: 实验结果表明，该方法在各种SR网络和数据集上均优于现有PTQ方法，并且在大多数情况下，性能与QAT方法相当，同时实现了至少75倍的加速。

Conclusion: 所提出的PTQ方法通过有效处理激活异常值和层敏感性问题，显著提升了SR网络的量化性能和推理速度，达到了与QAT方法相当的水平。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [139] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 深度神经网络易受对抗性攻击，对抗训练虽有效但存在鲁棒性震荡和过拟合问题。本文提出参数插值对抗训练（PIAT）框架和归一化均方误差（NMSE）损失，以缓解这些问题并显著提升模型（包括CNN和ViT）的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗性样本攻击。对抗训练是有效的防御方法，但其在训练过程中存在模型鲁棒性明显震荡和过拟合问题，从而降低了防御效果。

Method: 本文提出参数插值对抗训练（PIAT）框架，通过插值前一周期和当前周期的模型参数来调整模型参数，使决策边界变化更平缓并缓解过拟合。此外，建议使用归一化均方误差（NMSE）来对齐干净样本和对抗样本的logits相对大小，以进一步提高鲁棒性。

Result: 在多个基准数据集上进行的广泛实验表明，所提出的框架能够显著提高卷积神经网络（CNNs）和视觉Transformer（ViTs）的鲁棒性。

Conclusion: PIAT和NMSE的结合有效地解决了现有对抗训练方法中鲁棒性震荡和过拟合的问题，并通过更温和的决策边界变化和logits对齐，显著提升了深度学习模型的对抗鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [140] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 本研究引入了首个大规模多光谱（MS）机载激光雷达个体树木分割（ITS）基准数据集FGI-EMIT，并全面比较了传统无监督算法和监督深度学习（DL）方法。结果显示DL方法表现显著优于无监督算法，尤其是在林下树木分割方面，但当前DL模型尚未有效利用MS反射信息。


<details>
  <summary>Details</summary>
Motivation: 个体树木分割（ITS）对于森林清查、碳监测和生物多样性评估至关重要。然而，方法开发一直受限于缺乏大规模基准数据集，且多光谱（MS）激光雷达数据（已被证明能提高ITS精度）的可用性仍然有限。

Method: 本研究构建了FGI-EMIT数据集，它是首个大规模MS机载激光雷达ITS基准数据集，包含532、905和1,550 nm波长数据及1,561棵手动标注的树木（尤其关注小型林下树木）。我们使用该数据集对四种传统无监督算法和四种监督DL方法进行了全面基准测试。无监督方法的超参数通过贝叶斯方法优化，DL模型则从头开始训练。此外，还进行了消融研究以评估MS反射信息的作用，并分析了不同点云密度下的性能。

Result: 无监督方法中，Treeiso达到了最高的F1分数52.7%。DL方法整体表现显著更优，其中ForestFormer3D达到了73.3%的F1分数。在林下树木分割方面，ForestFormer3D比Treeiso高出25.9个百分点。消融研究表明，当前DL方法普遍未能有效利用MS反射信息作为额外输入特征，尽管单通道反射在边缘情况下（特别是林下树木）能略微提高精度。性能分析还显示，即使在低至10点/平方米的密度下，DL方法也始终优于无监督算法。

Conclusion: 监督深度学习方法在个体树木分割方面显著优于传统无监督算法，尤其在处理具有挑战性的林下树木时优势更为明显。尽管多光谱激光雷达数据具有潜力，但当前的深度学习模型尚未能充分利用其反射信息。FGI-EMIT数据集为ITS研究提供了一个宝贵的资源，未来需要进一步探索如何有效整合多光谱信息以提升DL模型的性能。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [141] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 本文提出了一种遮挡感知扩散模型（ODM），用于在遮挡场景下预测行人过街意图。该模型通过重建被遮挡的运动模式来指导意图预测，并在流行基准数据集上取得了比现有方法更鲁棒的性能。


<details>
  <summary>Details</summary>
Motivation: 行人过街意图预测对移动机器人和智能车辆的导航至关重要。尽管现有深度学习模型已取得显著成功，但很少有模型能有效处理遮挡场景下的不完整观测问题。

Method: 本文提出了一种遮挡感知扩散模型（ODM）。该模型在去噪阶段引入了遮挡感知扩散Transformer架构，用于估计与被遮挡模式相关的噪声特征，以增强模型捕获遮挡语义场景中上下文关系的能力。此外，还引入了遮挡掩码引导的逆向过程，以有效利用观测信息，减少预测误差积累，并提高重建运动特征的准确性。

Result: 在PIE和JAAD等流行基准数据集上，对所提方法在各种遮挡场景下的性能进行了全面评估和比较。广泛的实验结果表明，所提方法比现有方法表现出更鲁棒的性能。

Conclusion: 所提出的遮挡感知扩散模型（ODM）能够有效应对遮挡场景下的行人意图预测挑战，并取得了比现有方法更强大的性能，为移动机器人和智能车辆的导航提供了更可靠的意图预测能力。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [142] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 研究发现，用于阿尔茨海默病诊断的深度学习模型，在北美数据集(ADNI)上表现优异，但在拉丁美洲人群(FLENI)上泛化性能显著下降，揭示了明显的领域漂移。图像级归一化和正确的采样选择是提高泛化能力的关键。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在利用神经影像数据（尤其是18F-FDG PET扫描）诊断阿尔茨海默病方面表现出色，但其对未充分代表人群的泛化能力尚未得到充分探索。

Method: 本研究在ADNI数据集上评估了卷积和基于Transformer的模型，并测试了它们在阿根廷FLENI研究所的拉丁美洲临床队列上的泛化性能。通过消融研究识别泛化关键因素，并使用遮挡敏感性分析揭示模型关注区域。

Result: 所有模型在ADNI上均达到高AUC（最高0.96，0.97），但在FLENI上的性能大幅下降（分别降至0.82，0.80），表明存在显著的领域漂移。卷积和Transformer架构表现相似。图像级归一化和正确的采样选择是泛化的关键因素。ADNI训练的模型对AD类关注典型低代谢区域，但对其他类别和FLENI扫描的关注点不明确。

Conclusion: 研究强调了对诊断AI模型进行人群感知验证的必要性，并推动了未来在领域适应和队列多样化方面的工作。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [143] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 该论文客观分析了用于低剂量CT图像增强的深度学习模型中不同损失函数与图像质量指标的一致性，发现两者存在不一致，并强调了在开发新损失函数时需考虑图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）图像存在噪声和伪影，影响诊断准确性。深度学习模型虽能增强LDCT图像，并使用多种损失函数，但常用的PSNR和SSIM等指标无法充分反映医学图像的感知质量。因此，有必要客观分析不同损失函数与图像质量指标的相关性和一致性。

Method: 对用于低剂量CT图像质量增强的深度学习模型中的不同损失函数进行客观分析，评估它们与图像质量指标的相关性和一致性。

Result: 研究发现，损失函数与图像质量指标之间存在不一致性。

Conclusion: 强调在开发新的图像质量增强损失函数时，需要充分考虑图像质量指标。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [144] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: 本文引入了OmniBrainBench，这是首个全面多模态VQA基准，专门用于评估多模态大语言模型（MLLMs）在脑成像分析中的理解能力，并揭示了当前MLLMs与临床专家之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前的脑部视觉问答（VQA）基准在成像模态覆盖范围或病理描述细粒度方面存在局限，阻碍了对多模态大语言模型（MLLMs）在整个临床流程中进行全面评估。

Method: 研究者推出了OmniBrainBench，一个包含15种不同脑部成像模态、9,527对VQA问答和31,706张图像的综合性多模态VQA基准。该基准模拟临床工作流程，涵盖15项经专业放射科医生严格验证的多阶段临床任务。研究者对24个最先进的模型（包括开源、医学专用和专有MLLMs）进行了评估。

Result: 评估结果显示：1) 专有MLLMs（如GPT-5）优于开源和医学模型，但仍落后于医生；2) 医学MLLMs的性能表现差异较大；3) 开源MLLMs总体表现较差，但在特定任务中表现出色；4) MLLMs在复杂的术前任务中表现显著不足，揭示了从视觉到临床推理的差距。

Conclusion: OmniBrainBench为评估和推进脑成像分析中的MLLMs树立了新标准，并突出了当前MLLMs与专家临床推理之间的差距。该基准及其代码已发布。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [145] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 本文提出了一种受人类推理启发的图像地理定位分层序列预测方法，利用S2网格和自回归生成，通过探索多种推理策略，在无MLLM和有MLLM设置下均实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 图像地理定位面临两大挑战：不同地理位置之间视觉相似性高，以及搜索空间巨大。这些问题使得准确确定图像的地理来源变得困难。

Method: 该方法采用分层序列预测，从大区域逐步细化到精确位置，类似于人类的推理过程。它不依赖显式语义划分，而是使用嵌套的多分辨率S2单元格作为地理令牌。模型以自回归方式预测更精细级别的单元格，并结合视觉输入和先前的预测。此外，该研究还探索了多种自回归采样推断策略，包括束搜索（beam search）和多样本推断，以管理不确定性并选择最终输出。

Result: 在Im2GPS3k和YFCC4k数据集上，该模型在无多模态大语言模型（MLLM）设置下，几乎所有指标上都超越了其他可比较的基线，准确率提升高达13.9%，达到了最先进水平。当与MLLM结合使用时，该模型在所有指标上均优于所有基线，再次刷新了最先进性能。

Conclusion: 所提出的分层序列预测方法，结合S2单元格和先进的推理策略，能够有效应对图像地理定位的挑战，并在有无MLLM的两种情况下都取得了显著的性能提升，达到了新的最先进水平。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [146] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 该论文将地点识别重构为多类别分类问题，利用LiDAR数据训练编解码模型直接分类位置，在NuScenes数据集上实现了与对比学习相当的性能，并提升了训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 地点识别是自动驾驶中的关键任务，但现有方法多依赖对比学习。研究者旨在探索一种替代方法，以期在训练效率和稳定性方面带来优势。

Method: 将地点识别问题定义为多类别分类任务。具体方法是为LiDAR扫描分配离散的地理位置标签，并训练一个编码器-解码器模型来直接分类每个扫描的位置。

Result: 在NuScenes数据集上，该方法与基于对比学习的方法相比，取得了具有竞争力的性能，并且在训练效率和稳定性方面表现出优势。

Conclusion: 将地点识别问题视为多类别分类是可行的替代方案，它不仅能达到与对比学习相当的性能，还能在训练效率和稳定性上带来改进。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [147] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: 本文提出CatEquiv，一种针对惯性传感器人类活动识别（HAR）的范畴等变神经网络，通过编码时间、幅度和结构对称性，显著提升了模型对分布外扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决惯性传感器HAR中数据固有的时间、幅度及结构对称性问题，并提升模型在面对分布外扰动时的鲁棒性。

Method: 引入了CatEquiv，一个范畴等变神经网络。该网络通过“范畴对称积”系统地编码对称性，其中包含循环时间位移（时间对称）、正增益（幅度对称）和传感器层级偏序集（结构对称）。CatEquiv被设计为对该范畴对称积具有等变性。

Result: 在UCI-HAR数据集上，CatEquiv在分布外扰动下表现出比循环填充CNN和普通CNN显著更高的鲁棒性。

Conclusion: 强制执行范畴对称性可以带来强大的不变性和泛化能力，且无需增加额外的模型容量。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [148] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 本研究发现生成式AI模型在生成图像时，普遍存在对西方审美标准的偏见，表现为肤色浅、年龄小和过度性化，尤其对非二元性别个体影响更甚，即使是负面提示也未能有效纠正这些偏见。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的推广，导致负面自我形象和身体畸形。随着AI生成内容的增加，研究旨在探讨生成式AI模型如何编码“美”并消除“丑”，以及其对社会的影响。

Method: 研究构建了文本到图像和文本到语言模型再到图像两种生成管道。开发了一个结构化的审美分类法，并用其提示三个语言模型和两个文本到图像模型，共生成了5984张图像。随后，招募了女性和非二元性别的社交媒体用户，通过李克特量表在受试者内部研究中评估了其中1200张图像。

Result: 参与者在评分上表现出高度一致性。结果显示，86.5%的生成图像描绘了肤色较浅的人，22%的图像包含露骨内容（尽管经过SFW训练），74%的图像被评定为年轻年龄段。特别是，非二元性别个体的图像被评定为更年轻和更过度性化，表明存在令人担忧的交叉效应。值得注意的是，即使是编码了“负面”或“丑陋”审美特征（如“宽鼻子”）的提示，无论性别如何，都持续产生了更高的NSFW（不安全工作场所）评级。

Conclusion: 生成式AI模型中普遍存在与审美标准相关的人口统计学偏见，这些偏见通过模型开发者（如通过负面提示）积极地延续。这些偏见将导致数据流污染，并主动抹去不符合开发者所认为的“美”的刻板印象的特征，对社会产生深远影响。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [149] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: 本文介绍了SliceVision-F2I，一个用于研究5G/6G网络切片特征可视化的合成数据集，它将多变量KPI向量通过四种编码方法转换为低分辨率RGB图像，并模拟了真实的噪声网络条件。


<details>
  <summary>Details</summary>
Motivation: 5G和6G网络的兴起使得网络切片成为未来面向服务架构的重要组成部分，这需要更精细的识别方法和强大的数据集支持。

Method: 研究者创建了SliceVision-F2I数据集，通过四种独特的编码方法（物理启发映射、Perlin噪声、神经壁纸和分形分支）将多变量关键性能指标（KPI）向量转换为视觉表示。每种编码方法生成30,000个样本，每个样本包含原始KPI向量和对应的低分辨率RGB图像，并模拟了真实和嘈杂的网络条件。

Result: SliceVision-F2I数据集包含通过四种编码方法生成的合成样本，每个样本包括KPI向量和对应的RGB图像。该数据集模拟了现实和噪声网络条件，适用于视觉学习、网络状态分类、异常检测以及基于图像的机器学习技术在网络数据上的基准测试。该数据集已公开发布。

Conclusion: SliceVision-F2I数据集为下一代网络系统中的网络切片特征可视化研究提供了宝贵资源，支持视觉学习、网络状态分类、异常检测和图像机器学习技术基准测试，并可在多变量时间序列分析、合成数据生成和特征到图像转换等多种研究背景下重复使用。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [150] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、注意力机制的结直肠息肉分割网络MicroAUNet，结合知识蒸馏，实现了实时临床应用所需的高精度和低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习息肉分割模型存在两个主要问题：一是分割结果的息肉边界模糊，影响临床决策；二是模型架构过于庞大，计算复杂度高，导致推理速度慢，无法满足实时结直肠内窥镜应用的需求。

Method: 本文提出了MicroAUNet，一个轻量级的注意力分割网络。它结合了深度可分离膨胀卷积和一个单路径、参数共享的通道-空间注意力模块，以增强多尺度边界特征。在此基础上，引入了一个渐进式两阶段知识蒸馏方案，用于从一个高容量教师模型中迁移语义和边界信息。

Result: 在多个基准测试上的大量实验表明，MicroAUNet在极低的模型复杂度下达到了最先进的准确性。

Conclusion: 实验结果表明，MicroAUNet适用于实时临床息肉分割，解决了现有模型在精度和速度上的权衡问题。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [151] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 本研究开发了一个基于物联网的榴莲农场动物入侵检测系统，该系统结合YOLOv5和SSD算法，实现实时监控、Telegram通知和自动声音驱逐。


<details>
  <summary>Details</summary>
Motivation: 榴莲种植园遭受动物入侵导致作物损失和经济损失，传统方法无效。现有动物检测系统受限于单一目标检测算法、通知平台不便以及威慑机制不足。

Method: 构建了一个物联网（IoT）动物检测系统，该系统集成YOLOv5和SSD两种目标检测算法以提高准确性。检测到动物后，系统通过Telegram向农民发送实时通知，并触发自动声音（如虎啸）进行威慑。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别达到90%、85%和70%。系统在白天表现出最高的准确率，夜间准确率有所下降，这与图像是静态还是视频无关。

Conclusion: 该研究提供了一个结合检测、通知和威慑的全面且实用的框架，为未来自动化农业解决方案的创新奠定了基础。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [152] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为TA-LSDiff的新模型，结合拓扑感知扩散概率模型和水平集能量，解决了胰腺分割中存在的挑战，实现了领先的分割精度。


<details>
  <summary>Details</summary>
Motivation: 胰腺分割面临尺寸小、对比度低和拓扑结构多变等挑战。传统水平集方法常忽略点式拓扑效应，而深度学习方法则牺牲结构细节。本研究旨在弥补这些方法的不足，实现兼顾拓扑和细节的精准分割。

Method: 提出TA-LSDiff模型，它结合了拓扑感知扩散概率模型和水平集能量，无需显式几何演化即可实现分割。该能量函数通过四个互补项整合输入图像和深度特征来指导隐式曲线演化。此外，引入了一个像素自适应细化模块，利用邻近证据的亲和力加权局部调制能量函数，以提高边界精度。

Result: 通过消融研究系统地量化了每个组件的贡献。在四个公共胰腺数据集上的评估表明，TA-LSDiff实现了最先进的精度，优于现有方法。

Conclusion: TA-LSDiff被确立为一种实用且准确的胰腺分割解决方案。

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [153] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 本文提出通过引入多步推理链和强化学习，显著提升了针对印度美食的视觉问答（VQA）系统的准确性，解决了现有系统对西方食物的偏向。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统偏向西方食物，不适用于印度美食的复杂烹饪和文化多样性。虽然有印度美食VQA数据集，但其两步式问答流程不足以处理复杂的推理需求，尤其是在理解印度美食的烹饪背景和食物项之间关系时。

Method: 研究假设美食VQA需要多步推理过程。基于此假设，论文以最少的人工干预创建了推理链。随后，使用自动验证的推理链微调小型LLM和VLM，并利用强化学习和更大规模的数据进一步训练模型。

Result: 通过增加推理链，模型在基线上平均实现了10个百分点的准确率提升。论文还详细分析了推理链对印度美食VQA任务的影响。

Conclusion: 为印度美食VQA任务引入多步推理链是有效且必要的，它能显著提高模型的准确性，更好地处理复杂的印度烹饪上下文和食物关系，从而弥补现有VQA系统在多样化美食领域中的不足。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [154] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种名为GCN-PSN的拓扑感知图卷积网络（GCN）框架，通过将人体骨架建模为图，学习区分性的拓扑敏感姿态嵌入，并结合Siamese架构和对比回归目标，用于动作质量评估（AQA）。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估（AQA）需要对人体运动进行细致的理解，并精确评估姿态相似性。

Method: 研究者提出了GCN-PSN框架，该框架将人体骨架建模为图，以学习区分性的、拓扑敏感的姿态嵌入。该方法采用Siamese架构，并使用对比回归目标进行训练。

Result: GCN-PSN方法优于基于坐标的基线方法，并在AQA-7和FineDiving基准测试中取得了有竞争力的性能。

Conclusion: 实验结果和消融研究验证了利用骨架拓扑结构进行姿态相似性评估和动作质量评估的有效性。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [155] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: 本文提出OMEGA框架，通过模态特定位置编码（MSPE）和全局自适应编码步长缩放（GAESS），优化了视觉-语言模型（VLMs）中对文本和视觉信息独特结构属性的处理，显著提升了VLMs在多模态任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型普遍采用模态统一的1D或2D位置编码策略，未能充分考虑文本信息的序列连续性和视觉信息的空间连贯性等不同模态的独特结构属性，这限制了模型对多模态数据的理解能力。

Method: 本文提出了OMEGA位置编码框架，包含两部分：1. 模态特定位置编码（MSPE），为不同模态分配独立坐标维度上的位置索引，以保留其固有结构。2. 全局自适应编码步长缩放（GAESS），根据两种模态的嵌入熵自适应调整视觉token的位置编码步长，以对齐多模态数据在位置索引空间中的信息密度。

Result: 实验结果表明，OMEGA持续提升了VLM在不同架构和VQA基准上的性能。在视觉密集型任务上，OMEGA在Qwen2.5-VL-3B模型上比基线位置编码策略提高了高达3.43%，并且在Qwen2.5-VL-7B和LLaVA-v1.5-7B等更大模型上也观察到了一致的性能提升。

Conclusion: OMEGA框架通过引入模态特定位置编码和自适应步长缩放，有效解决了现有VLM位置编码策略的局限性，显著增强了模型对文本和视觉信息独特结构属性的建模能力，从而全面提升了VLM在多模态任务中的表现。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [156] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为分层模态分解（LMD）的后验、模型无关的可解释性方法，用于在自动驾驶中分解多传感器融合模型中各模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，感知模型的决策透明度至关重要，因为一次误感知可能导致灾难性后果。然而，多传感器输入导致传感器信息在融合网络中纠缠不清，难以确定每种模态对预测的贡献。

Method: 引入了分层模态分解（LMD）方法，这是一种后验、模型无关的可解释性方法，旨在解耦预训练融合模型所有层中的模态特定信息。

Result: LMD在摄像头-雷达、摄像头-激光雷达和摄像头-雷达-激光雷达等自动驾驶设置下的预训练融合模型上进行了评估。其有效性通过结构化扰动度量和模态分解可视化得到验证，表明其在解释高容量多模态架构方面具有实际适用性。

Conclusion: LMD是首个将自动驾驶传感器融合系统中感知模型的预测归因于单个输入模态的方法，证明了其在解释高容量多模态架构方面的实际适用性。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [157] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: Med-Banana-50K是一个大规模、高质量、开放获取的医学图像编辑数据集，包含5万张图像，覆盖三种模态和23种疾病类型，通过LLM生成和严格的医学质量控制构建，旨在推动医学图像编辑模型的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大型语言模型在医学图像编辑方面取得了进展，但研究社区仍受限于缺乏专门为医学图像编辑设计、具有严格解剖学和临床约束的大规模、高质量、开放获取数据集。

Method: 本文介绍了Med-Banana-50K数据集，该数据集包含5万张图像，用于基于指令的医学图像编辑，涵盖胸部X光、脑部MRI和眼底摄影三种模态以及23种疾病类型。数据集通过利用Gemini-2.5-Flash-Image从真实医学图像生成双向编辑（病灶添加和移除）。其独特之处在于采用系统性的医学质量控制方法：使用“LLM-as-Judge”结合医学标准（指令依从性、结构合理性、真实性、保真度）进行评估，并进行最多五轮的历史感知迭代优化。此外，数据集还包含3.7万次失败尝试的完整对话日志，用于偏好学习和对齐研究。

Result: 创建了Med-Banana-50K，一个包含5万张图像的综合性数据集，用于基于指令的医学图像编辑，涵盖三种模态和23种疾病类型。该数据集通过LLM生成和严格的医学质量控制（LLM-as-Judge及迭代优化）构建，并包含了3.7万次失败尝试的对话日志，为偏好学习和对齐研究提供了资源。

Conclusion: 通过提供这个大规模、经过医学验证且完整记录的资源，Med-Banana-50K为训练和评估下一代医学图像编辑模型奠定了基础。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [158] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: 本文提出GraphGeo，一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位。它通过建模不同类型的智能体交互和引入双层辩论机制，有效处理冲突预测并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉地理定位需要丰富的地理知识和复杂推理，但传统检索方法受限于数据库覆盖和质量。虽然LVLM能直接推理，但单个模型难以应对多样地理区域和复杂场景。现有多智能体系统虽通过协作提升性能，但对所有智能体交互一视同仁，缺乏有效处理冲突预测的机制。

Method: 本文提出GraphGeo框架，利用异构图神经网络构建多智能体辩论系统。该方法通过类型化边（支持性协作、竞争性论证、知识转移）建模多样化的辩论关系。它引入双层辩论机制，结合节点级细化和边级论证建模。此外，采用跨层拓扑细化策略，实现图结构和智能体表示的共同演化。

Result: 在多个基准测试上进行的实验表明，GraphGeo显著优于现有最先进的方法。

Conclusion: GraphGeo框架通过结构化辩论，将智能体之间的认知冲突转化为增强的地理定位准确性。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [159] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为APDM（Anti-Personalized Diffusion Models）的新框架，通过直接修改扩散模型本身来阻止特定主体的个性化生成，以应对现有保护方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在特定主体（如身份或物体）的高质量合成方面取得了显著进展，但也带来了严重的隐私风险，因为个性化技术可能被恶意用户滥用以生成未经授权的内容。现有的对抗性扰动样本方法依赖于不切实际的假设，并且在少量干净图像或简单图像变换下就会失效。

Method: 该研究将保护目标从图像转移到扩散模型本身。首先，通过理论分析表明现有损失函数无法确保鲁棒的反个性化收敛。受此启发，提出了Direct Protective Optimization (DPO) 新型损失函数，旨在有效干扰目标模型中的主体个性化而不损害生成质量。此外，引入了Learning to Protect (L2P) 双路径优化策略，通过在个性化和保护路径之间交替，模拟未来的个性化轨迹并自适应地强化每一步的保护。

Result: 实验结果表明，该框架优于现有方法，在阻止未经授权的个性化方面取得了最先进的性能。

Conclusion: APDM框架通过新颖的损失函数DPO和双路径优化策略L2P，成功地将保护目标转移到扩散模型本身，有效阻止了特定主体的个性化，同时保持了生成质量，从而解决了扩散模型带来的隐私风险。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [160] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 该综述全面评估了将基础模型（FMs）应用于医学图像分析的适应策略，讨论了现有方法的优缺点和新兴方向，旨在为开发适应性强、值得信赖的医学影像FM提供路线图。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学图像分析中展现出巨大潜力，但其在实际临床应用中面临领域漂移、高质量标注数据稀缺、计算需求高和严格隐私要求等挑战，限制了其广泛应用。因此，需要探讨如何有效适应这些模型。

Method: 该综述审查并评估了多种适应策略，包括监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态或跨模态框架。此外，还重点介绍了新兴方向，如持续学习、联邦和隐私保护方法、混合自监督学习、以数据为中心的流程（结合合成数据生成与人工验证）以及系统基准测试。

Result: 对于每种策略，综述评估了其性能提升、临床适用性、局限性、权衡和未解决的挑战。它识别了现有方法的不足，并强调了旨在解决这些差距的新兴研究方向，以实现动态部署、数据保护、数据效率提升、数据质量保障和鲁棒泛化能力。

Conclusion: 该综述通过概述适应策略和相关研究空白，为开发能够满足真实世界医学影像需求、具有适应性、值得信赖且能与临床实践深度融合的基础模型提供了清晰的路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [161] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为动态多级加权对齐网络（DMWAN）的方法，用于零样本草图图像检索（ZS-SBIR），通过解决模态样本不平衡和训练中低质量信息不一致的问题，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 以往的零样本草图图像检索（ZS-SBIR）方法存在模态样本不平衡和训练过程中低质量信息不一致的问题，导致性能不佳。

Method: 本文引入了一种动态多级加权对齐网络（DMWAN），它包含三个主要组件：1) 单模态特征提取模块，使用CLIP文本编码器和ViT提取文本和视觉token；2) 跨模态多级加权模块，通过局部和全局聚合块生成对齐权重列表，以衡量草图和图像样本的对齐质量；3) 加权四元组损失模块，旨在改善三元组损失中域的平衡性。

Result: 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上的实验结果表明，本文提出的方法在ZS-SBIR任务中优于现有的最先进方法。

Conclusion: 所提出的动态多级加权对齐网络（DMWAN）有效解决了ZS-SBIR中模态样本不平衡和低质量信息的问题，并在多个基准数据集上取得了优异的性能。

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [162] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为“Eyes on Target”的深度感知、凝视引导目标检测框架，用于第一视角视频。该框架将凝视特征注入Vision Transformer的注意力机制，以偏向人类关注区域，从而提高目标检测精度，并在多个数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 人类凝视提供了理解复杂视觉环境中视觉注意力的丰富监督信号。传统目标检测器平等对待所有区域，而本文旨在利用凝视信息，优先处理观看者关注的区域，以增强目标检测。

Method: 本文提出了“Eyes on Target”框架，将凝视派生特征注入Vision Transformer (ViT) 的注意力机制中，有效地使空间特征选择偏向人类关注的区域。此外，还引入了一个凝视感知注意力头重要性度量，以解释模型行为。

Result: 该方法在定制的第一视角模拟器数据集和公共基准（包括Ego4D Ego-Motion和Ego-CH-Gaze数据集）上，相比于无凝视基线，在检测精度上取得了持续的提升。凝视感知注意力头重要性度量也揭示了凝视线索如何调节Transformer的注意力动态。

Conclusion: 将人类凝视信息整合到深度感知目标检测框架中，可以显著提高第一视角视频中的目标检测性能，尤其是在人类视觉注意力对任务评估至关重要的场景中。该方法还提供了一种解释模型如何利用凝视线索的机制。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [163] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Cross-Mamba交互的多任务学习（CMI-MTL）框架，以解决医学视觉问答（Med-VQA）中跨模态语义对齐的挑战以及自由形式答案的多样性问题，并在三个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的自注意力方法在处理视觉和语言之间的跨模态语义对齐方面效果不佳。此外，基于分类的方法依赖于预定义的答案集，难以适应自由形式答案的多样性，并可能忽略其详细的语义信息。

Method: 本文引入了一个名为CMI-MTL的框架，它通过以下三个关键模块学习图像和文本的跨模态特征表示：1) 细粒度视觉-文本特征对齐（FVTA），用于提取图像-文本对中最相关的区域；2) 跨模态交错特征表示（CIFR），用于通过跨模态交错特征表示捕获跨模态序列交互；3) 自由形式答案增强多任务学习（FFAE），利用开放式问题的辅助知识来增强模型处理开放式Med-VQA的能力。

Result: 实验结果表明，CMI-MTL在VQA-RAD、SLAKE和OVQA这三个Med-VQA数据集上均优于现有的最先进方法。此外，通过可解释性实验进一步证明了其有效性。

Conclusion: CMI-MTL框架通过细粒度特征对齐、跨模态交互和自由形式答案增强的多任务学习，有效解决了Med-VQA中跨模态语义对齐和自由形式答案处理的难题，显著提升了模型性能和开放式问答能力。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [164] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 本文提出SEPS框架，通过语义增强和补丁精简，有效解决细粒度跨模态对齐中的冗余和歧义问题，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度跨模态对齐在VQA等应用中面临补丁冗余和歧义挑战，源于模态间信息密度差异。尽管MLLMs有潜力，但其密集文本输出可能与稀疏原始描述冲突，且视觉补丁与文本描述间的语义关联量化仍是难题。

Method: 引入语义增强补丁精简（SEPS）框架。采用两阶段机制整合来自密集和稀疏文本的统一语义，以识别显著视觉补丁。此外，利用基于均值计算的相关性感知选择，突出关键的补丁-词对应关系，从而改善跨模态相似性评估。

Result: SEPS在Flickr30K和MS-COCO数据集上表现出色，在多种模型架构下，rSum性能超越现有方法23%-86%，尤其在文本到图像检索场景中表现显著提升。

Conclusion: SEPS框架通过系统性地解决补丁冗余和歧义，有效整合多源语义信息，并优化补丁-词对应关系，显著提升了细粒度跨模态对齐的性能，尤其在文本到图像检索任务中表现优异。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [165] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 本文提出UniSOT，一个统一的单目标跟踪器，能够同时处理多种参考模态（边界框、自然语言或两者）和多种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件），并在18个基准测试上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常只针对单一或少数视频/参考模态设计，导致模型分离且限制了实际应用。实际场景需要一个能处理各种需求的统一跟踪器，但目前尚无能同时处理所有这些参考和视频模态的跟踪器。

Method: 研究者设计了一个名为UniSOT的统一跟踪器，它能够以统一的参数处理三种参考模态和四种视频模态的不同组合。

Result: UniSOT在18个视觉跟踪、视觉-语言跟踪和RGB+X跟踪基准测试上展示了优于特定模态对应方法的性能。具体而言，UniSOT在TNL2K上所有三种参考模态的AUC得分比现有方法高出3.0%以上，并在所有三种RGB+X视频模态的主要指标上比Un-Track高出2.0%以上。

Conclusion: UniSOT是一个高效的统一跟踪器，能够应对各种参考模态和视频模态的组合，显著优于现有特定模态的跟踪器，解决了当前单目标跟踪领域缺乏通用解决方案的问题。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [166] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL是一个统一的端到端框架，通过数据中心策略解决了多模态大语言模型在异构医学数据（2D、3D、视频）理解上的挑战，并在多项基准测试中达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在医疗对话中潜力巨大，但异构的医学数据（2D图像、3D体数据、视频序列）以及显著的领域差距和数据格式不一致性，阻碍了统一医学MLLM的发展。

Method: Fleming-VL采用数据中心视角，通过三项关键策略解决问题：1) 整合自然和医学领域的长上下文数据来扩大预训练规模；2) 利用稀有医学数据（包括整体视频分析和超声、皮肤镜等代表性不足的2D模态）补充微调；3) 扩展现有评估框架以纳入3D体数据和视频理解基准。通过监督微调（SFT）和组相对策略优化（GRPO），开发了不同模型规模的Fleming-VL。

Result: 广泛的实验表明，Fleming-VL在多个基准测试中实现了最先进的性能，包括医学VQA、视频QA和3D医学图像理解。

Conclusion: Fleming-VL成功地为异构模态的全面医学视觉理解提供了一个统一框架，并公开发布以促进医学AI的透明、可复现和可审计进展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [167] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出了EVTAR，一个端到端的虚拟试穿模型，通过引入额外参考图像来提高试穿准确性，并简化了传统方法中复杂的输入要求。


<details>
  <summary>Details</summary>
Motivation: 大多数现有虚拟试穿方法依赖于复杂的输入（如无关人物图像、人体姿态、densepose或身体关键点），导致其劳动密集且不适用于实际应用。

Method: EVTAR采用两阶段训练策略，推理时仅需源图像和目标服装作为输入，无需mask、densepose或分割图。它利用穿着相同服装的不同个体的额外参考图像，以更好地保留服装纹理和精细细节。此外，模型通过补充参考和未配对人物图像丰富了训练数据。

Result: EVTAR在两个广泛使用的基准和多样化任务上进行了评估，结果一致验证了其方法的有效性。

Conclusion: EVTAR通过简化输入、引入额外参考图像和两阶段训练，实现了更准确、更实用的虚拟试穿效果，并能更好地保留服装细节。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [168] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 针对易腐水果保鲜期预测中数据稀缺和专有模型隐私问题，本文提出了一种模型无关的序数元学习（MAOML）算法，用于训练小型视觉语言模型（VLMs）。该方法结合元学习和标签序数性，在零样本和少样本设置下，实现了水果新鲜度分类的最新水平（平均准确率92.71%）。


<details>
  <summary>Details</summary>
Motivation: 有效管理易腐水果的损耗，需要准确、无创地预测其新鲜度。深度学习在视觉数据分析方面有潜力，但专家标注细粒度新鲜度标签成本高昂，导致数据稀缺。专有视觉语言模型（如Gemini）在此任务上表现出色，但食品零售组织因数据隐私问题无法使用。现有开源VLM性能不佳，且在有限数据下微调也无法达到专有模型的水平。

Method: 本文引入了一种模型无关的序数元学习（MAOML）算法。该算法旨在训练较小的视觉语言模型（VLMs），通过元学习解决数据稀疏性问题，并利用标签的序数性（ordinality）来提高性能。

Result: 该方法在零样本和少样本设置下的水果新鲜度分类任务中，实现了最先进的性能。在所有水果上的平均准确率达到行业标准的92.71%。

Conclusion: 所提出的MAOML算法通过结合元学习和标签序数性，有效解决了水果新鲜度预测中数据稀疏性和开源VLM性能不足的问题。它在零样本和少样本场景下均达到最先进的准确率，为食品零售组织提供了一个高性能且无数据隐私顾虑的解决方案。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [169] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种改进视频生成DPO方法，通过GT-Pair自动构建高质量偏好对，Reg-DPO引入SFT损失稳定训练，并结合内存优化技术提升训练容量，从而显著提高了视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成DPO方法大多沿用图像领域的范式，且主要在小规模模型上开发，面临数据构建成本高、训练不稳定和内存消耗大等独特挑战。

Method: ['GT-Pair：自动构建高质量偏好对，使用真实视频作为正例，模型生成视频作为负例，无需外部标注。', 'Reg-DPO：将SFT损失作为正则项引入DPO目标函数，以增强训练稳定性和生成保真度。', '内存优化：将FSDP框架与多种内存优化技术结合，实现比单独使用FSDP高近三倍的训练容量。']

Result: 在I2V和T2V任务的多个数据集上，本文方法始终优于现有方法，提供了卓越的视频生成质量，并实现了近三倍的训练容量提升。

Conclusion: 通过引入GT-Pair、Reg-DPO和内存优化技术，本文成功克服了视频生成DPO的挑战，显著提升了生成质量和训练效率。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [170] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态机器学习框架，通过对戈雅画作的视觉和X射线图像应用相同的特征提取技术，并结合优化的一类支持向量机，实现了高精度的艺术品鉴定，显著优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 戈雅作品的艺术鉴定面临巨大计算挑战，原因在于他多变的风格演变和广泛的历史伪造模式。

Method: 该研究引入了一个多模态机器学习框架，对戈雅画作的视觉和X射线图像应用相同的特征提取技术。统一的特征提取流程包括灰度共生矩阵（GLCM）描述符、局部二值模式（LBP）、熵度量、能量计算和颜色分布分析。提取的特征通过经过超参数调优的优化一类支持向量机（OC-SVM）进行处理。数据集包含24幅经鉴定为真迹的戈雅画作及其对应的X射线图像，采用80/20的训练-测试分割和10折交叉验证。

Result: 该框架实现了97.8%的分类准确率，假阳性率为0.022。对“Un Gigante”的案例研究表明，通过统一的多模态特征分析，鉴定置信度达到92.3%。结果显示，与单模态方法相比，性能有显著提升。

Conclusion: 研究结果表明，在艺术品鉴定应用中，对视觉和射线照相图像应用相同的计算方法是有效的，并且能带来实质性的性能改进。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [171] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出一种新颖的自监督多模态迭代去噪（MID）框架，通过迭代引入噪声并利用两个神经网络和泰勒展开，直接从噪声数据中学习并去除复杂非线性噪声，在计算机视觉、生物医学和生物信息学等多个领域取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据经常受到复杂、非线性噪声的污染，使得传统的基于规则的去噪方法无法有效应对。

Method: MID将收集到的噪声数据建模为非线性噪声累积连续过程中的一个状态。它通过迭代引入更多噪声，学习两个神经网络：一个用于估计当前噪声步长，另一个用于预测并减去相应的噪声增量。对于复杂的非线性污染，MID采用一阶泰勒展开来局部线性化噪声过程，从而实现有效的迭代去除。该方法无需成对的干净-噪声数据集，直接从噪声输入中学习噪声特性。

Result: 实验结果表明，MID在四个经典的计算机视觉任务中展现出鲁棒性、适应性和持续的最先进性能。此外，MID在生物医学和生物信息学领域的任务中也表现出强大的性能和适应性。

Conclusion: MID框架能够有效处理复杂非线性噪声，具有强大的鲁棒性和适应性，并且无需干净数据进行训练，在多个科学和工程领域达到了最先进的去噪效果。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [172] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一个专为2D血管分割设计的强大高效框架，通过集成卷积适配器、多提示编码器和轻量级掩码解码器，显著优于现有的SAM变体，并在参数量更少的情况下，与完全微调方法表现相当，且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 血管的薄、分支结构和低纹理对比度使得准确的血管分割在临床应用中极具挑战性。虽然基础模型如Segment Anything Model (SAM) 在通用分割方面表现出色，但在血管结构上表现不佳。

Method: VesSAM框架包含三个主要组件：1) 一个卷积适配器，用于增强局部纹理特征；2) 一个多提示编码器，通过分层交叉注意力融合骨架、分叉点和段中点等解剖学提示；3) 一个轻量级掩码解码器，以减少锯齿状伪影。此外，该研究还引入了自动化流程来生成结构化多提示标注，并整理了一个包含8个数据集和5种成像模态的基准数据集。

Result: 实验结果表明，VesSAM在Dice和IoU指标上均比最先进的基于PEFT的SAM变体高出10%和13%以上。与完全微调方法相比，VesSAM在参数量显著更少的情况下，仍能达到具有竞争力的性能。VesSAM还表现出良好的域外(OoD)泛化能力，在平均OoD Dice和IoU方面均优于所有基线模型。

Conclusion: VesSAM是一个强大且高效的2D血管分割框架，通过专门的设计，显著提升了血管分割的性能和泛化能力，且参数量更少，为临床应用提供了更优的解决方案。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [173] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 本文发现平坦的全精度神经网络对低比特量化至关重要。为解决ViT后训练量化(PTQ)中全精度与量化模型关系被忽视导致的误差问题，作者提出了一个框架，通过将激活和权重量化误差建模为独立高斯噪声，并利用噪声注入优化方法获得平坦最小值，从而有效地对模型进行预处理，显著提高了PTQ性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ViT后训练量化(PTQ)方法通常忽略了良好训练的全精度神经网络与量化模型之间的关系，导致显著的量化误差。此外，如何高效训练一个模型无关且适用于预定义低比特模型的神经网络尚不明确。

Method: 1. 发现平坦的全精度神经网络对低比特量化至关重要。2. 提出了一个框架，通过测量和分离误差源来主动预处理模型。3. 将激活量化误差(AQE)和权重 量化误差(WQE)统计建模为独立的高斯噪声。4. 研究并应用了几种噪声注入优化方法以获得平坦的最小值。

Result: 实验结果证明了所提出方法的有效性。

Conclusion: 该研究为获得低比特PTQ模型开辟了新的途径。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [174] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该研究引入了一种名为QA-SNNE的黑盒不确定性估计器，通过结合问题语义来提高外科视觉问答（VQA）系统的安全性，特别是在幻觉检测方面，从而增强临床信任。


<details>
  <summary>Details</summary>
Motivation: 外科VQA系统中，不正确或模棱两可的回答可能危害患者，因此安全性和可靠性至关重要。然而，大多数外科VQA研究侧重于准确性或语言质量，而忽视了模糊意识、寻求专家意见或触发二次审查等安全行为。

Method: 研究受到自动故障检测（AFD）的启发，将不确定性估计作为实现更安全决策的关键。提出了一种名为“问题对齐语义最近邻熵”（QA-SNNE）的黑盒不确定性估计器，它将问题语义融入预测置信度。QA-SNNE通过在医学文本嵌入空间中，根据问题上下文，比较生成答案与最近邻答案来衡量语义熵。研究在EndoVis18-VQA和PitVQA数据集上评估了五种模型，包括领域特定的参数高效微调（PEFT）模型和零样本大视觉语言模型（LVLMs）。

Result: PEFT模型在轻微改写下性能下降，而LVLMs更具弹性。在三种LVLMs和两种PEFT基线上，QA-SNNE在大多数模板内设置中提高了AUROC，并增强了幻觉检测能力。零样本模型的AUROC增加了15-38%，且在模板外压力测试下增益仍保持。QA-SNNE通过将语义不确定性与问题上下文相关联，为外科VQA中的AFD提供了一个实用且可解释的步骤。

Conclusion: 结合LVLM骨干模型与问题对齐的不确定性估计（如QA-SNNE）可以显著提高外科VQA系统的安全性并增强临床医生对其的信任。QA-SNNE是实现外科VQA自动故障检测的实用且可解释的一步。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [175] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出一个统一的零样本推理框架，通过链式测试时推理过程，整合了视频异常检测、空间定位和文本解释，实现了无需额外训练的全面异常分析，并在多个基准测试中达到了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 大多数视频异常研究止步于帧级检测，缺乏对异常原因的深入理解，通常只输出帧级异常分数，没有空间或语义上下文。现有的视频异常定位和理解方法虽然提高了可解释性，但仍依赖于数据且针对特定任务。

Method: 本文提出了一个统一的推理框架，通过链式测试时推理过程，顺序连接了时间检测、空间定位和文本解释任务，实现了全面的零样本异常分析。该方法利用任务内推理来细化时间检测，并通过任务间链式连接实现空间和语义理解，从而在完全零样本的情况下提高可解释性和泛化能力。具体而言，它通过精心设计的提示词和任务链式连接来利用基础模型的推理能力。

Result: 在没有任何额外数据或梯度的情况下，本文方法在多个视频异常检测、定位和解释基准测试中取得了最先进的零样本性能。结果表明，通过精心设计的提示词和任务链式连接，可以释放基础模型的推理能力，实现实用且可解释的零样本视频异常分析。

Conclusion: 精心设计的提示词和任务链式连接能够解锁基础模型的推理能力，从而实现实用、可解释的零样本视频异常分析。本方法提供了一个统一的框架，弥合了时间检测、空间定位和文本解释之间的鸿沟，无需额外训练即可进行全面的零样本异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [176] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost是一种参数高效的神经网络架构，通过新颖的动态缩放渐进式注意力（DSPA）机制，在CIFAR基准测试上实现了最先进的性能，同时显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种在保持高准确率的同时，具有极高参数效率的神经网络架构，以适应资源受限的边缘设备的部署需求。

Method: 本文提出了FastBoost架构，其核心是动态缩放渐进式注意力（DSPA）机制。DSPA包含三项创新：1) 自适应融合（学习通道-空间注意力动态权重混合），2) 阶段缩放（训练阶段感知的强度调制），以及3) 残差自适应（自优化跳跃连接）。FastBoost将DSPA与增强型MBConv块结合，并采用双注意力路径、级联细化层和硬件友好型设计。

Result: FastBoost在CIFAR-10上达到了95.57%准确率（0.85M参数）和93.80%准确率（0.37M参数），在CIFAR-100上达到了81.37%准确率（0.92M参数）和74.85%准确率（0.44M参数）。与MobileNetV3相比，FastBoost在CIFAR-10上参数减少了2.1倍，准确率提高了3.2个百分点，同时具有0.28G FLOPs和12.7%的梯度流增加。

Conclusion: FastBoost通过动态注意力与高效卷积操作的协同优化，实现了前所未有的参数-准确率权衡，使得模型能够在不牺牲准确率的情况下，成功部署到资源受限的边缘设备上。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [177] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 本文提出了一种名为T-MLA的新型多尺度对数-指数攻击框架，该框架通过在小波域中生成对抗性扰动，能够对神经图像压缩（NIC）系统发起隐蔽且高效的攻击，导致图像重建质量显著下降。


<details>
  <summary>Details</summary>
Motivation: 神经图像压缩（NIC）在率失真性能方面已达到最先进水平，但其安全漏洞远不如分类器那样被充分理解。现有的针对NIC的对抗性攻击往往是像素空间方法的简单改编，忽略了压缩管道的独特结构。

Method: 本文提出了T-MLA，这是第一个目标多尺度对数-指数攻击框架。该方法通过直接针对被攻击和重建图像的质量，在小波域中精心设计对抗性扰动。这种攻击是一种原则性的离线攻击，扰动被策略性地限制在特定的小波子带中，以最大化失真同时确保视觉上的不可察觉性。

Result: 在多个最先进的NIC架构和标准图像压缩基准上进行的广泛评估显示，图像重建质量大幅下降，而扰动在视觉上仍然是不可察觉的。

Conclusion: 研究结果揭示了生成式和内容分发管道核心存在的关键安全漏洞。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [178] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: 本文提出HyFormer-Net，一种混合CNN-Transformer模型，用于乳腺超声图像的同步分割和分类，具有内在可解释性，并在BUSI数据集上表现出色，同时通过渐进式微调展示了跨数据集的泛化能力。


<details>
  <summary>Details</summary>
Motivation: B型超声乳腺癌诊断面临散斑、操作员依赖和边界不清等挑战。现有深度学习方法存在单任务学习、架构限制（CNN缺乏全局上下文，Transformer缺乏局部特征）以及黑盒决策等问题，这些都阻碍了其临床应用。

Method: 本文提出HyFormer-Net，一个混合CNN-Transformer模型，用于同步分割和分类，并具有内在可解释性。其双分支编码器通过多尺度分层融合块整合了EfficientNet-B3和Swin Transformer。一个注意力门控解码器提供了精确性和可解释性。引入了双通道可解释性：(1) 通过IoU验证的内在注意力验证（平均IoU: 0.86），以及(2) 用于分类推理的Grad-CAM。

Result: 在BUSI数据集上，HyFormer-Net的Dice分数达到0.761，准确率93.2%，优于U-Net、Attention U-Net和TransUNet。恶性召回率达到92.1%，确保了最小的假阴性。集成模型实现了卓越的Dice分数90.2%、准确率99.5%和100%的恶性召回率，消除了假阴性。消融研究证实多尺度融合贡献了+16.8%的Dice分数，注意力门控增加了+5.9%。首次进行的混合CNN-Transformer在乳腺超声中的跨数据集泛化研究表明，零样本迁移失败（Dice: 0.058），但通过仅10%目标域数据（68张图像）的渐进式微调，性能恢复了92.5%；使用50%数据时，模型Dice分数达到77.3%，超过了源域性能（76.1%），证明了其真正的泛化能力。

Conclusion: HyFormer-Net通过其混合架构和双通道可解释性，有效解决了乳腺超声诊断中的现有挑战和深度学习方法的局限性。它在性能上超越了现有模型，并通过有限数据微调展示了强大的跨数据集泛化能力，为乳腺癌的临床诊断提供了有前景的解决方案。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [179] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 本文提出HMVLM模型，一个基于MoE LoRA的统一框架，用于将3D人体运动与大型语言模型集成，通过动态专家分配和零专家策略解决灾难性遗忘问题，并采用身体部位特定分词增强姿态表示，在多样化人体运动任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 将语义丰富的3D人体运动与基础语言模型结合时，存在模态鸿沟导致的灾难性遗忘问题。此外，开发能够跨异构下游任务保持泛化能力的自回归兼容姿态表示，仍然是一个关键技术障碍。

Method: 本文提出了人体运动-视觉-语言模型（HMVLM），这是一个基于专家混合低秩适应（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配LoRA专家权重，实现多任务同步微调。为缓解指令微调期间的灾难性遗忘，引入了一个创新的“零专家”来保留预训练参数以处理通用语言任务。对于姿态表示，通过将人体划分为不同的关节组，实现了身体部位特定的分词，从而增强了表示的空间分辨率。

Result: 实验表明，所提出的方法有效缓解了指令微调期间的知识遗忘，并在多样化的人体运动下游任务中取得了显著性能。

Conclusion: HMVLM框架成功地解决了3D人体运动与基础语言模型集成时的灾难性遗忘和姿态表示泛化性问题，通过创新的MoE LoRA策略和身体部位分词，在多种人体运动任务中展现出卓越的性能和泛化能力。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [180] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 本研究提出了一种结合Epanechnikov核密度估计（EKDE）和双峰逻辑回归的呼吸系统疾病诊断新方法，利用胸部X光图像数据，在COVID-19数据集上取得了中等性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种利用图像数据诊断呼吸系统疾病的新方法，利用EKDE在建模数据分布和适应像素强度变化方面的灵活性，从医学图像中提取关键特征。

Method: 该方法在一个基于统计模型的学习方案中，将Epanechnikov非参数核密度估计（EKDE）与双峰逻辑回归分类器相结合。在COVID-19放射学数据集中随机选择了13808张胸部X光片进行测试。

Result: 该方法在检测呼吸系统疾病方面取得了70.14%的准确率、59.26%的敏感性和74.18%的特异性。结果显示性能中等，敏感性有待提高。

Conclusion: 本研究强调了基于EKDE的方法在提高医学影像诊断准确性和可靠性方面的潜力，同时指出临床专业知识对于进一步完善模型仍然至关重要。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [181] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 本文提出一个结构化的五层模型，结合大型基础模型和数据增强策略，用于自动驾驶中稀有场景的生成与评估。该模型引入新的代理子类和特征，并使用多样性分数和原创性分数来衡量生成数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车开发中，稀有且具有挑战性的驾驶场景至关重要，但由于难以实际遇到，因此需要通过生成模型进行模拟或创建。

Method: 研究人员提出了一个结构化的五层模型来表示驾驶场景，该模型改进了对稀有场景的评估和生成。他们将此模型与大型基础模型结合，通过数据增强策略生成新的驾驶场景。该结构为场景中的每个代理引入了子类和特征。此外，他们研究并调整了两个评估指标：多样性分数（衡量数据集内场景差异）和原创性分数（衡量合成数据集与真实参考集的相似度），并进行了合成视频的定性评估。

Result: 论文展示了在不同生成设置下这两种评估指标的应用，并对从结构化场景描述生成的合成视频进行了定性评估，表明该方法能够有效地生成和评估稀有驾驶场景。

Conclusion: 该研究提出的结构化五层模型及其伴随的评估指标，能够有效改进稀有驾驶场景的生成和评估，为自动驾驶系统的开发提供了有价值的工具和方法。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [182] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的框架，统一了DINO系列自监督学习方法，支持多种架构和训练策略，旨在降低计算成本并提高跨领域可用性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（VFM）的训练流程不灵活、领域特定或计算成本高昂，限制了它们在不同领域和资源设置下的可用性。

Method: DINO-MX是一个模块化、可扩展的训练框架，它在一个统一的配置驱动系统中结合了DINO、DINOv2和DINOv3的核心原则。它支持多种基于Transformer的架构，并与Hugging Face生态系统完全兼容。该框架包括多种训练策略（如LoRA、层冻结、知识蒸馏）和分布式训练支持（DDP和FSDP）。它设计用于处理自然和特殊数据类型，并提供可解释性工具和标签引导的数据增强方法。

Result: DINO-MX在多样化数据集上实现了有竞争力的性能，同时显著降低了计算成本。此外，它提供的标签引导数据增强方法无需额外的检测或分割头即可改善基于注意力的定位。

Conclusion: DINO-MX为开发、适应和基准测试自监督视觉模型提供了一个可复现和可扩展的基础，适用于广泛的研究和实际应用。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [183] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 该研究提出在边缘计算中使用带GPU的嵌入式设备来加速移动增强现实/计算机视觉应用，以克服资源限制并提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的增强现实（AR）和计算机视觉（CV）应用资源需求高。边缘计算可用于卸载任务，但边缘设备通常容量有限，可能影响用户体验。

Method: 本文提出使用带有图形处理单元（GPU）的嵌入式设备来解决边缘计算的容量限制问题。通过实验验证了其性能。

Result: 实验结果表明，与仅使用CPU相比，使用GPU可以获得性能提升，从而为用户提供更好的应用体验。

Conclusion: 在边缘计算中引入带GPU的嵌入式设备能够显著提升移动增强现实和计算机视觉应用的性能，从而改善用户体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [184] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++是一种新颖的方法，可以从单视图图像高效生成高质量的带纹理网格，解决了现有方法在效率、质量和几何一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在以下问题：基于SDS的方法通常耗时且几何不一致；直接网络推理方法质量低且缺乏几何细节。本研究旨在全面提高单视图重建任务的质量、一致性和效率。

Method: 本文提出了一种跨域扩散模型，用于生成多视图法线图及对应的彩色图像。为确保生成的一致性，采用了多视图跨域注意力机制，促进视图和模态间的信息交换。最后，引入了一种级联3D网格提取算法，以粗到精的方式，在大约3分钟内从多视图2D表示中驱动出高质量表面。

Result: 广泛评估表明，与现有工作相比，该方法实现了高质量的重建结果、强大的泛化能力和良好的效率。

Conclusion: Wonder3D++通过其创新的跨域扩散模型、注意力机制和级联网格提取算法，显著提升了单视图图像到高保真3D模型的重建性能，在质量、一致性和效率上均表现出色。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [185] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个名为“| ⟲ BUS |”的大型Rebus谜题基准，并引入了一个名为“RebusDescProgICE”的模型无关框架，显著提升了视觉-语言模型在解决此类谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题结合了图像识别、认知技能、常识推理和多步推理等多种复杂能力，对当前视觉-语言模型（VLM）来说是一个极具挑战性的任务。

Method: 1. 构建了一个包含1333个英语Rebus谜题的大型多样化基准“| ⟲ BUS |”，涵盖18个类别，具有不同的艺术风格和难度级别。2. 提出了一个模型无关框架“RebusDescProgICE”，该框架结合了非结构化描述、基于代码的结构化推理以及基于推理的上下文示例选择。

Result: 与Chain-of-Thought推理方法相比，“RebusDescProgICE”框架将视觉-语言模型在“| ⟲ BUS |”基准上的性能提升了2.1-4.1%（使用闭源模型）和20-30%（使用开源模型）。

Conclusion: 所提出的“| ⟲ BUS |”基准和“RebusDescProgICE”框架有效地提高了视觉-语言模型解决复杂Rebus谜题的能力，展示了结合描述、结构化推理和智能示例选择的潜力。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [186] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 本文提出ViACT（Video Anatomically Constrained Transformer），通过将解剖先验知识（点集表示的解剖结构）直接整合到Transformer中，解决视频Transformer在超声心动图分析中学习非诊断区域无关特征的问题。ViACT在预训练中仅掩码和重建解剖区域，从而将学习集中于该区域，提升了EF回归、CA检测等任务的性能和可解释性，并能泛化到点跟踪。


<details>
  <summary>Details</summary>
Motivation: 视频Transformer在超声心动图分析中表现出强大潜力，但与其他视频模型一样，容易从图像背景等非诊断区域学习到虚假关联，影响模型性能和可解释性。

Method: 本文提出ViACT框架，将解剖先验知识直接整合到Transformer架构中。它将变形的解剖结构表示为点集，并将其空间几何和对应图像块编码为Transformer token。在预训练阶段，ViACT采用掩码自编码策略，仅掩码和重建解剖区域的图像块，强制模型学习专注于解剖区域。模型随后可针对特定解剖区域的任务进行微调，本文以心肌为例进行演示。

Result: 解剖约束使Transformer的注意力集中在心肌内部，生成与已知心脏淀粉样变（CA）病理区域对齐的可解释注意力图。ViACT在左心室射血分数（EF）回归和心脏淀粉样变（CA）检测等超声心动图分析任务上表现出色。此外，ViACT无需专门跟踪网络中使用的相关体积等任务特定组件，即可泛化到心肌点跟踪任务。

Conclusion: ViACT通过将解剖先验知识直接融入Transformer，成功克服了视频Transformer在超声心动图分析中学习非诊断区域虚假关联的限制。这使得模型学习更聚焦于解剖区域，提高了任务性能、可解释性，并展现出在解剖区域点跟踪等任务上的泛化能力。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [187] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: 该研究引入了ROVER基准测试，以评估统一多模态模型（UMMs）的互惠跨模态推理能力，发现跨模态推理对视觉生成质量至关重要，且模型在符号推理的视觉抽象方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型评估方法将文本和图像的理解与生成能力视为孤立的，主要通过单模态推理来评分。然而，真正的统一多模态智能需要模型能够进行互惠跨模态推理，即利用一种模态来指导、验证或完善另一种模态的输出。为解决这一评估空白，该研究旨在引入一个专门测试这种能力的基准。

Method: 研究引入了一个名为ROVER的人工标注基准测试，包含1312个任务，基于1876张图像，涵盖两种互补设置：1) 语言增强视觉生成推理，评估模型能否利用语言提示和推理链指导图像合成；2) 视觉增强语言生成推理，评估模型能否生成中间可视化以强化其问答推理过程。研究在17个统一模型上进行了实验。

Result: 实验揭示了两个关键发现：1) 跨模态推理决定视觉生成质量，其中交错式模型显著优于非交错式模型；值得注意的是，结合强大的单模态模型也未能实现可比的推理能力。2) 模型在物理和符号推理之间存在脱节：它们能字面解释感知概念，但在符号任务中无法构建视觉抽象，导致错误的推理并损害性能。

Conclusion: 互惠跨模态推理是实现真正全模态生成能力的关键前沿。当前统一多模态模型在符号推理的视觉抽象方面存在显著不足，需要进一步研究来提升其跨模态推理能力。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [188] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 该研究引入了“视角学习”任务和Viewpoint-100K数据集，通过两阶段微调（SFT和GRPO强化学习）以及混合冷启动初始化，显著提升了多模态大语言模型（MLLMs）在2D和3D空间推理任务中的表现，强调了发展MLLMs空间基础能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在2D视觉理解方面取得了显著进展，但其捕捉详细空间信息和实现跨视角一致性以进行鲁棒3D推理的能力尚不明确，这是真实世界3D推理的关键要求。

Method: 本研究引入了“视角学习”任务来评估和提升MLLMs的空间推理能力，并构建了包含10万个物体中心图像对及其问答的Viewpoint-100K数据集。采用两阶段微调策略：首先通过在Viewpoint-100K上进行监督微调（SFT）注入基础知识；其次，利用组相对策略优化（GRPO）算法在更广泛的问题集上进行强化学习以增强泛化能力。此外，还提出了一种混合冷启动初始化方法，以同时学习视角表示并保持连贯的推理思维。

Result: 实验结果表明，该方法显著激活了MLLM的空间推理能力，提高了模型在域内和域外推理任务上的性能。

Conclusion: 研究结果强调了在MLLMs中发展基础空间技能的价值，为未来机器人物理交互、自动驾驶系统和3D场景理解的进步提供了支持。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [189] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: 本研究提出了SurgVeo基准和Surgical Plausibility Pyramid (SPP) 评估框架，用于评估手术视频生成模型。结果显示，先进模型在视觉感知上表现出色，但在器械操作、环境反馈和手术意图等更高层面的可信度上存在严重缺陷，揭示了视觉模仿与因果理解之间的巨大鸿沟。


<details>
  <summary>Details</summary>
Motivation: 视频生成基础模型在模拟物理世界方面展现出巨大潜力，但其在手术等高风险领域（需要深层、专业的因果知识而非通用物理规则）的应用仍是未被探索的关键空白。

Method: 研究引入了SurgVeo（首个由专家策划的手术视频生成模型评估基准）和Surgical Plausibility Pyramid (SPP)（一个新颖的四层评估框架，用于从基本外观到复杂手术策略评估模型输出）。使用Veo-3模型对腹腔镜和神经外科手术片段进行零样本预测任务，并由四位经过认证的外科医生小组根据SPP评估生成的视频。

Result: 结果揭示了明显的“可信度差距”：Veo-3在视觉感知可信度方面表现出色，但在SPP的更高层面（包括器械操作可信度、环境反馈可信度、手术意图可信度）上严重失败。这是首次量化证明了外科AI中视觉上令人信服的模仿与因果理解之间的鸿沟。

Conclusion: 本工作通过SurgVeo和SPP为未来开发能够应对专业、真实世界医疗领域复杂性的模型奠定了关键基础和路线图。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [190] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个新的分层运动生成框架，通过粗到细的生成过程和多尺度Token保留策略，显著提高了文本驱动3D人体运动生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的向量量化引导生成Transformer（VQ-GT）范式在文本驱动3D人体运动生成中可能存在效率和多尺度信息保留的局限性，需要一种更高效、更高质量的生成方法。

Method: MoSa引入了一个分层运动生成框架，通过多尺度Token保留策略（MTPS）增强了分层残差向量量化变分自编码器（RQ-VAE），MTPS在每个分层量化中采用插值来保留多尺度Token。生成Transformer支持可扩展自回归（SAR）建模，预测尺度Token。此外，为解决插值可能导致的重建退化，提出了CAQ-VAE，一个结合卷积和注意力机制的轻量级VQ-VAE，以更好地捕捉全局依赖。

Result: MoSa仅需10个推理步骤，实现了最先进的生成质量和效率。在Motion-X数据集上，FID达到0.06（优于MoMask的0.20），推理时间减少了27%。MoSa还能很好地泛化到运动编辑等下游任务，无需额外微调。

Conclusion: MoSa通过其创新的分层生成框架、多尺度Token保留策略和CAQ-VAE，显著提升了文本驱动3D人体运动生成的质量和效率，并在多个方面超越了现有方法，展现出强大的泛化能力。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [191] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像小波谱的扩散Transformer模型（DTWSR），用于超分辨率重建，通过结合扩散模型和Transformer的优势来捕获多尺度频率子带间的相互关系，从而生成更一致和真实的SR图像。


<details>
  <summary>Details</summary>
Motivation: 现有的DWT（离散小波变换）基超分辨率方法通常忽略多尺度频率子带之间的相互关系，导致重建图像出现不一致和不自然的伪影。

Method: 本文提出了DTWSR模型。具体而言，它使用多级离散小波变换（MDWT）将图像分解为小波谱；提出了一种金字塔分词方法将小波谱嵌入为Transformer模型的token序列，以捕获空间和频率域特征；设计了一个双解码器，用于处理低频（LF）和高频（HF）子带的差异性，同时保持它们在图像生成中的对齐。

Result: 在多个基准数据集上进行的广泛实验表明，该方法在感知质量和保真度方面均表现出高水平的性能和有效性。

Conclusion: DTWSR模型通过有效捕获多尺度频率子带间的相互关系，克服了现有DWT基SR方法的局限性，显著提高了超分辨率图像的一致性和真实感。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [192] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一个全模态视觉-语言-动作(VLA)模型，通过将红外、毫米波雷达和麦克风阵列等传感器的信息融合到RGB图像中，形成“传感器掩码图像”，显著提升了机器人感知和操作能力，在真实世界任务中表现优于仅依赖RGB或原始传感器输入的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要依赖RGB摄像头，这限制了它们的感知能力，进而影响了操作能力，尤其是在需要超越RGB感知的物理空间智能任务中。

Method: 本文提出了OmniVLA模型，其核心是“传感器掩码图像”——一种统一的表示形式。该方法将来自红外摄像头、毫米波雷达和麦克风阵列等传感器的空间定位和物理有意义的掩码叠加到RGB图像上。这种图像原生的统一方式使传感器输入接近RGB统计数据，便于训练，提供统一的传感器硬件接口，并实现数据高效学习。模型建立在RGB预训练的VLA骨干网络之上。

Result: OmniVLA在需要传感器模态感知来指导操作的挑战性真实世界任务中，实现了平均84%的任务成功率。它显著优于仅依赖RGB的基线模型59%，也优于原始传感器输入基线模型28%。此外，OmniVLA展现出更高的学习效率和更强的泛化能力。

Conclusion: OmniVLA通过整合多模态传感器信息，并将其统一为“传感器掩码图像”表示，显著提升了VLA模型的感知、操作、学习效率和泛化能力，使其能够更好地应对超越RGB感知限制的复杂真实世界任务。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [193] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 本文提出一个自动化管道，从YouTube挖掘并处理动物视频，构建了一个大规模数据集（30K视频，2M帧）和首个4D四足动物重建基准（Animal-in-Motion，AiM）。通过评估现有方法并建立新的基线，旨在推动野外视频中无标记4D动物重建任务。


<details>
  <summary>Details</summary>
Motivation: 动物计算机视觉在野生动物研究中前景广阔，但通常依赖大规模数据。现有数据采集方法依赖受控捕获设置，而当前的动物视频数据集规模有限（例如，仅2.4K 15帧片段），并且缺乏用于以动物为中心的3D/4D任务的关键处理，限制了单视角、非侵入性分析的潜力。

Method: 研究人员开发了一个自动化管道，用于挖掘YouTube视频，并将其处理成以对象为中心的片段，同时提供辅助注释，以支持姿态估计、跟踪和3D/4D重建等下游任务。利用此管道，他们积累了30K视频（2M帧）。为了支持4D四足动物重建任务，他们推出了Animal-in-Motion（AiM）基准，包含230个手动筛选的序列和11K帧。他们评估了最先进的基于模型和无模型方法，并通过序列级优化增强了一种无模型方法，建立了首个4D动物重建基线。

Result: 通过自动化管道，他们积累了30K视频（2M帧），比现有工作多出一个数量级。创建了包含230个序列和11K帧的Animal-in-Motion（AiM）基准。评估结果显示，2D指标偏爱基于模型的方法，尽管其3D形状不真实，而无模型方法产生更自然的重建，但得分较低，揭示了当前评估的不足。他们通过增强无模型方法，建立了首个4D动物重建基线。

Conclusion: 该论文提出的管道、基准和基线旨在推动从野外视频中进行大规模、无标记的4D动物重建及相关任务的进展。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [194] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 本文提出了一种名为PCP的弱监督框架，用于在无需显式概念标注或语言模型的情况下，为医学影像提供人类可解释的预测。PCP利用类别级概念先验作为弱监督，并通过KL散度和熵正则化机制进行细化，在概念预测和分类性能上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像领域部署AI时，人类可解释的预测至关重要。然而，现有可解释设计（IBD）框架需要昂贵的概念标注，这在临床环境中不切实际。近期尝试绕过标注的方法（如零样本视觉-语言模型或概念生成框架）难以捕捉领域特定的医学特征，导致可靠性差。

Method: 本文提出了一种新颖的先验引导概念预测器（Prior-guided Concept Predictor, PCP）。这是一个弱监督框架，通过利用类别级概念先验作为弱监督，实现概念答案预测，而无需显式监督或依赖语言模型。PCP还包含一个通过KL散度和熵正则化实现的细化机制，以使预测与临床推理对齐。

Result: 在PH2（皮肤镜检查）和WBCatt（血液学）数据集上，PCP的概念级F1分数比零样本基线提高了33%以上。同时，在PH2、WBCatt、HAM10000和CXR4四个医学数据集上，PCP的分类性能与全监督概念瓶颈模型（CBMs）和V-IP相比具有竞争力。

Conclusion: PCP框架成功地在无需昂贵概念标注的情况下，为医学影像提供了人类可解释的预测。它在概念预测方面显著优于零样本方法，并在分类性能上可与全监督模型媲美，解决了当前可解释AI在医学影像中部署面临的标注难题和领域特异性捕捉不足的问题。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [195] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文回顾了自动语音驱动3D手势生成的人类评估实践，发现缺乏标准化和存在缺陷的实验设置。为解决这些问题，我们引入了一个详细的人类评估协议，并使用该协议对六个最新模型进行了大规模众包评估，结果显示新模型并不总是优于旧模型，且现有评估存在不足。我们强调需要解耦评估，并发布了相关资源以推动标准化。


<details>
  <summary>Details</summary>
Motivation: 自动语音驱动3D手势生成领域的人类评估实践缺乏标准化，且实验设置存在缺陷，导致不同方法之间难以比较，也无法确定当前的技术水平。这促使研究人员寻求改进评估设计并实现未来用户研究的标准化。

Method: 本文针对广泛使用的BEAT2动作捕捉数据集，引入了一个详细的人类评估协议。利用该协议，研究人员对六个由原始作者训练的最新手势生成模型进行了大规模众包评估，评估维度包括运动真实感和语音-手势对齐。此外，为推动标准化和新的评估研究，作者将发布基准模型的合成运动数据、用户研究的渲染视频刺激、开源渲染脚本以及收集到的16,000个配对人类偏好投票。

Result: 研究结果提供了强有力的证据表明：1) 较新的模型并不总是一致地优于较早的方法；2) 已发表的关于高运动真实感或语音-手势对齐的声明在严格评估下可能不成立；3) 该领域必须采用运动质量和多模态对齐的解耦评估，以实现准确的基准测试并取得进展。

Conclusion: 为了在该领域取得进展，必须采纳解耦的运动质量和多模态对齐评估，以实现准确的基准测试。研究人员通过引入标准化协议和发布大量评估资源，旨在推动该领域的人类评估实践走向标准化和更严谨的科学方法。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [196] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于双阶信息的黑盒可迁移对抗攻击方法，通过解决“欺骗性平坦度”问题并引入对抗平坦度攻击（AFA）和蒙特卡洛对抗采样（MCAS），显著提升了对抗样本在不同模型间的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 可迁移攻击对未知模型构成实际威胁，但现有生成可迁移对抗样本的方法（即使关注平坦损失）仍常陷入“欺骗性平坦度”（即平坦但尖锐的区域），导致攻击效果不佳。

Method: 研究提出了一种基于双阶信息的黑盒梯度可迁移攻击。具体而言，引入了“对抗平坦度（AF）”来解决欺骗性平坦度问题，并提供了可迁移性的理论保证。基于此，通过对目标函数进行高效近似，实例化了“对抗平坦度攻击（AFA）”以解决梯度符号改变的问题。此外，为进一步增强攻击能力，设计了“蒙特卡洛对抗采样（MCAS）”以提高内循环采样效率。

Result: 在ImageNet兼容数据集上的综合实验结果表明，该方法优于六种基线方法，能够在更平坦的区域生成对抗样本，并显著提高跨模型架构的迁移能力。在输入变换攻击或百度云API测试中，本方法也优于基线方法。

Conclusion: 本研究提出的方法（包括AF、AFA和MCAS）有效解决了可迁移对抗攻击中的欺骗性平坦度问题，能够生成更平坦的对抗样本，并显著提升了黑盒场景下的攻击迁移能力。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [197] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM是一个用于脑部病变分割的端到端框架，通过新颖的CenterMamba编码器、记忆驱动的结构化提示生成器和记忆增强多尺度解码器，解决了小病灶、低对比度及跨层不连续性等挑战，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 脑部病变分割面临诸多挑战，包括病灶体积小、对比度低、各向异性采样以及跨层不连续性。

Method: 该研究提出了CenterMamba-SAM框架，它冻结预训练主干并仅训练轻量级适配器以实现高效微调。核心组件包括：1) CenterMamba编码器，采用新颖的3x3角-轴-中心短序列扫描策略，实现中心优先、轴向增强和对角线补偿的信息聚合；2) 记忆驱动的结构化提示生成器，在相邻切片间维护原型库，无需用户交互自动合成可靠提示，提高层间一致性；3) 记忆增强多尺度解码器，在多层级集成记忆注意力模块，结合深度监督和渐进式细化以恢复细节并保持全局一致性。

Result: 在公共基准测试中，CenterMamba-SAM取得了脑部病变分割任务的最先进性能。

Conclusion: CenterMamba-SAM框架通过其创新的编码器、提示生成器和解码器设计，有效解决了脑部病变分割的挑战，并显著提升了分割效果。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [198] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级几何感知适配器，通过引入区域感知正则化来提高LiDAR语义分割在恶劣天气下的鲁棒性，尤其是在结构脆弱区域。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气（如折射、散射、点丢失）会导致LiDAR语义分割性能下降，现有方法忽视了边界、角落和稀疏区域附近的结构脆弱性。

Method: 本文提出了一个轻量级几何感知适配器。该模块对方位角进行对齐并应用水平循环填充以保持0~360度边界的邻域连续性。它使用局部窗口K-Nearest Neighbors收集附近点并计算简单的局部统计数据，压缩成紧凑的几何感知线索。这些线索在训练期间驱动区域感知正则化，稳定结构脆弱区域的预测。该适配器即插即用，可与数据增强互补，且仅在训练期间启用，推理成本可忽略不计。

Result: 在源域训练（SemanticKITTI）和跨天气评估（SemanticSTF）的设置下，该适配器将mIoU比以数据为中心的增强基线提高了7.9个百分点，比以类别为中心的正则化基线提高了0.6个百分点。

Conclusion: 研究结果表明，几何驱动的正则化是全天候LiDAR分割的关键方向。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [199] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为REASON的两阶段概率图引导双分支融合框架，用于超声胃内容物评估，以实现术前误吸风险的自动化、高效和准确分层。


<details>
  <summary>Details</summary>
Motivation: 传统胃内容物评估方法依赖手动描绘和经验公式，存在效率和准确性方面的显著局限性，而准确评估胃内容物对于全身麻醉诱导时的误吸风险分层至关重要。

Method: 该研究提出了一种名为REASON的两阶段概率图引导双分支融合框架。第一阶段，分割模型生成概率图，以抑制伪影并突出胃部解剖结构。第二阶段，双分支分类器融合来自右侧卧位（RLD）和仰卧位（SUP）两种标准视图的信息，以提高学习特征的判别能力。

Result: 在自收集数据集上的实验结果表明，所提出的框架显著优于当前最先进的方法。

Conclusion: 该框架在自动化术前误吸风险评估方面显示出巨大潜力，为临床实践提供了一个更鲁棒、高效和准确的解决方案。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [200] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: 本文提出PRevivor，一种先验引导的颜色Transformer，用于修复古代中国画作中不可逆的色彩退化，通过将任务分解为亮度增强和色相校正，实现了优于现有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 古代中国画作因不可逆的色彩退化而面临损坏，其复杂的化学机制和高质量数据集的缺乏，使得端到端数字修复工具的开发异常困难，促使研究者寻求新的解决方案来恢复这些珍贵的文化遗产。

Method: 本文提出了PRevivor，一个先验引导的颜色Transformer。它将色彩修复分解为两个子任务：亮度增强和色相校正。亮度增强使用两个变分U-Nets和一个多尺度映射模块。色相校正设计了一个双分支颜色查询模块，由从褪色画作中提取的局部色相先验引导，其中一个分支专注于局部校正，另一个分支保持全局推理能力。

Result: PRevivor在与现有最先进的着色方法进行广泛实验后，在定量和定性上都展现出卓越的性能。

Conclusion: PRevivor通过其独特的分解任务和先验引导机制，成功地解决了古代中国画作的色彩退化问题，为数字修复提供了有效的工具。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [201] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream提出了一种低延迟、高帧率的实时流式视频生成方法，通过知识蒸馏和滑动窗口注意力机制，实现了任意长度视频的生成，并显著提升了交互体验和性能。


<details>
  <summary>Details</summary>
Motivation: 当前运动条件视频生成方法存在延迟过高（每视频数分钟）和非因果处理的问题，这阻碍了实时交互和应用。

Method: 首先，通过运动控制增强了一个文本到视频模型作为双向教师模型。然后，利用自强制与分布匹配蒸馏（Self Forcing with Distribution Matching Distillation）将该教师模型蒸馏成一个因果学生模型，以实现实时流式推理。为解决长视频生成中的域间隙、误差累积和计算成本增长等挑战，引入了精心设计的滑动窗口因果注意力（sliding-window causal attention）和注意力槽（attention sinks），并在训练中结合自循环（self-rollout）与KV缓存滚动（KV cache rolling）来模拟推理时固定上下文窗口的推断。

Result: MotionStream实现了亚秒级延迟，单GPU上最高可达29 FPS的流式生成速度。其速度比现有技术快两个数量级，同时在运动跟踪和视频质量方面达到了最先进水平。该方法独特地实现了无限长度的流式视频生成，使用户能够实时绘制轨迹、控制摄像机或转移运动，提供真正的交互式体验。

Conclusion: MotionStream通过创新的模型蒸馏和注意力机制设计，成功克服了现有视频生成方法的延迟和非因果处理限制，实现了高效、高质量、实时交互式的无限长度视频流生成，为用户带来了前所未有的体验。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [202] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了UniREditBench，一个统一的推理图像编辑评估基准，包含真实世界和游戏世界场景，并引入多模态双参考评估。同时构建了大规模合成数据集UniREdit-Data-100K和改进模型UniREdit-Bagel，以系统评估和提升图像编辑模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态生成模型在图像编辑方面取得进展，但仍难以处理需要隐式推理的复杂多样图像编辑任务。现有基准主要关注单对象属性转换，忽视了多对象交互和涉及人类定义规则的游戏世界场景，且仅依赖文本参考评估，可能导致复杂推理场景下的系统性误判。

Method: 1. 提出UniREditBench，一个统一的推理图像编辑评估基准，包含2,700个样本，覆盖8个主要维度和18个子维度，涉及真实世界和游戏世界场景。2. 引入多模态双参考评估，为每个样本提供文本和真实图像参考，以提高评估可靠性。3. 设计自动化多场景数据合成管道，构建大规模合成数据集UniREdit-Data-100K，包含高质量思维链(CoT)推理标注。4. 在UniREdit-Data-100K数据集上微调Bagel模型，开发UniREdit-Bagel。

Result: 1. UniREditBench提供了一个全面的基准，能够系统评估模型在各种推理场景下的性能。2. 多模态双参考评估显著提高了评估可靠性。3. UniREdit-Bagel在域内和域外设置中均表现出显著改进。4. 通过对开源和闭源图像编辑模型的全面基准测试，揭示了它们在各个方面的优缺点。

Conclusion: UniREditBench提供了一个急需的、全面的推理图像编辑评估基准，能够更准确地评估和推动多模态生成模型在复杂推理任务中的发展。UniREdit-Data-100K和UniREdit-Bagel的创建为未来的研究提供了有价值的资源和改进的模型。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [203] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了MVSMamba，首个基于Mamba架构的多视角立体视觉（MVS）网络，通过动态Mamba模块实现了高效的全局特征聚合，并在性能和效率上超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基于Transformer的MVS方法虽然能捕获长距离依赖，但其二次复杂度在性能和效率之间造成了挑战。Mamba架构具有全局建模能力和线性复杂度，因此研究者希望利用Mamba来解决这一问题。

Method: 提出了MVSMamba，这是首个基于Mamba的MVS网络，旨在实现高效的全局特征聚合。为了充分发挥Mamba在MVS中的潜力，设计了一个动态Mamba模块（DM-module），该模块基于新颖的参考中心动态扫描策略，实现了：1) 从参考视图到源视图的高效视图内和视图间特征交互；2) 全向多视图特征表示；3) 多尺度全局特征聚合。

Result: MVSMamba在DTU数据集和Tanks-and-Temples基准测试上，在性能和效率方面均优于现有的最先进MVS方法。

Conclusion: MVSMamba成功地利用Mamba架构实现了高效且高性能的多视图立体视觉，有效解决了Transformer基MVS方法的效率瓶颈，并取得了领先的实验结果。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [204] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP模型的生成式对抗攻击方法，通过结合自然语言语义和集中扰动策略，生成对多标签分类器有效且视觉上难以察觉的对抗扰动，同时保持高视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然强大，但容易受到对抗性攻击的欺骗，导致不准确的预测。研究旨在开发一种能够生成高效且视觉上难以察觉的对抗扰动的方法。

Method: 该研究提出了一种生成式对抗攻击方法，利用CLIP模型对文本和图像表示的对齐能力，通过引导损失（guided loss）融入自然语言语义。此方法能够生成与原始输入视觉上几乎相同的对抗样本，并允许在多对象场景中进行广泛的扰动操作，以欺骗多标签分类器。它整合了来自Saliency-based Auto-Encoder (SSAE)的集中扰动策略和类似于Generative Adversarial Multi-Object Scene Attacks (GAMA)的不同文本嵌入。

Result: 实验结果表明，所提出的方法在多种任务和不同的黑盒受害模型上表现出竞争力，取得了与现有技术相当或更优异的结果。同时，它在保持高视觉保真度方面表现出色，生成的扰动既能欺骗分类模型，又能与原始图像保持高度结构相似性。

Conclusion: 该研究成功开发了一种新颖的生成式对抗攻击方法，利用CLIP模型和特定的扰动策略，能够生成高效且视觉上不易察觉的对抗样本，为对抗攻击领域提供了新的视角和更优的视觉保真度。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [205] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的潜在因子分组增强的聚类推理实例解耦学习框架，用于全玻片图像（WSI）的可解释表示。该框架通过解决多实例学习（MIL）中的空间、语义和决策纠缠，实现了卓越的性能和与病理学家一致的可解释性。


<details>
  <summary>Details</summary>
Motivation: 多实例学习（MIL）在全玻片病理图像表示中广泛应用，但实例间的空间、语义和决策纠缠限制了其表示能力和可解释性。本研究旨在解决这些挑战。

Method: 本研究分三个阶段提出了一种潜在因子分组增强的聚类推理实例解耦学习框架：1) 引入正半定潜在因子分组，将实例映射到潜在子空间以缓解空间纠缠。2) 采用实例概率反事实推理和优化，通过聚类推理实例解耦来减轻语义纠缠。3) 利用实例效应再加权，通过广义线性加权决策来解决决策纠缠。

Result: 在多中心数据集上的广泛实验表明，所提出的模型优于所有最先进的模型。此外，它通过解耦表示和透明的决策过程，实现了与病理学家一致的可解释性。

Conclusion: 该研究提出的框架有效解决了MIL在WSI表示中存在的空间、语义和决策纠缠问题，不仅提升了模型性能，还实现了与病理诊断高度契合的可解释性。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [206] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 本研究提出了Fire-ART数据集和一种基于全景图像的重建方法，用于将消防资产的语义信息集成到BIM模型中，以提高消防设备数字管理的准确性。


<details>
  <summary>Details</summary>
Motivation: 消防资产的库存管理对于应急准备、风险评估和现场灭火响应至关重要。然而，传统方法由于在自动化资产识别和重建方面的能力有限而效率低下。

Method: 本研究引入了Fire-ART数据集（包含15种基本资产，2,626张图像和6,627个实例），并开发了一种基于全景图像的重建方法，将修改后的立方体贴图转换和基于半径的球形相机投影相结合，以增强识别和定位精度。

Result: 通过两个真实案例研究的验证，所提出的方法分别实现了73%和88%的F1分数，以及0.620米和0.428米的定位误差。

Conclusion: Fire-ART数据集和重建方法为提高消防安全设备的精确数字管理提供了宝贵的资源和强大的技术解决方案。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [207] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的生成图像检测框架，通过利用自然图像和生成图像数据流形之间的几何差异进行检测，该方法不依赖于二元分类器，并通过归一化流增强可检测差异。


<details>
  <summary>Details</summary>
Motivation: 生成图像的日益真实性引发了对其潜在滥用的担忧，需要鲁棒的检测方法。现有方法主要依赖于二元分类器训练，但其性能受限于可用生成图像的数量和质量。

Method: 该方法利用自然图像和生成图像数据流形之间的几何差异。它采用一对函数，对自然图像产生一致输出，对生成图像产生发散输出（其梯度位于相互正交的子空间中）。检测原理是：如果沿着数据流形进行变换后，预训练在自然图像上的自监督模型的损失值发生显著变化，则该图像被识别为生成图像。此外，为解决高级生成模型中流形差异减小的问题，该研究利用归一化流通过将生成图像“挤出”自然图像流形来放大可检测的差异。

Result: 广泛的实验证明了该方法的有效性。

Conclusion: 该研究提出了一种基于数据流形几何差异的简单而有效的生成图像检测方法，并通过归一化流解决了先进生成模型中流形差异减小的问题，展现了良好的检测性能。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [208] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 该论文提出了MIQ-SAM3D，一个用于3D医学图像多实例分割的框架，通过竞争性查询优化策略，实现了从单点提示到多实例分割的转变，有效解决了现有SAM方法在多病灶分割上的局限性。


<details>
  <summary>Details</summary>
Motivation: 医学图像的准确分割对于肿瘤诊断和治疗计划至关重要。尽管基于SAM的交互式分割具有强大的泛化能力，但大多数方法遵循“单点对单对象”范式，限制了多病灶分割。此外，ViT骨干网络虽然能捕获全局上下文，但常缺失高保真度的局部细节。

Method: MIQ-SAM3D框架采用竞争性查询优化策略，将“单点对单掩模”转变为“单点对多实例”。它包含一个提示条件实例查询生成器，将单个点提示转换为多个专业查询，从而从一个示例中检索3D体积中所有语义相似的病灶。一个混合CNN-Transformer编码器通过空间门控将CNN提取的边界显著性注入ViT自注意力中。最后，一个竞争性优化的查询解码器通过查询间竞争实现端到端、并行的多实例预测。

Result: 在LiTS17和KiTS21数据集上，MIQ-SAM3D取得了可比的分割水平，并对提示展现出强大的鲁棒性。

Conclusion: MIQ-SAM3D为临床相关的多病灶病例的高效标注提供了一个实用的解决方案。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [209] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的视觉模型解释方法，使用平滑可调的轮廓代替传统的密集扰动掩码，生成紧凑、可解释且忠实的解释区域。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型解释方法中，常用的密集扰动掩码通常碎片化、过拟合且需要仔细的后处理，难以实现忠实而紧凑的解释。

Method: 该方法通过截断傅里叶级数参数化星形凸区域，并利用分类器梯度在极值保留/删除目标下进行优化。它保证生成单一、简单连接的掩码，大幅减少自由参数数量，并提供稳定的边界更新。通过限制解为低维平滑轮廓，增强了对对抗性掩码伪影的鲁棒性。该方法还扩展到多轮廓，以定位多个对象。

Result: 在ImageNet分类器上，该方法实现了与密集掩码相当的极值保真度，同时生成了紧凑、可解释的区域，并提高了运行间的一致性。明确的区域控制能够生成重要性轮廓图。与基于梯度和扰动的方法相比，该方法实现了更高的相关性质量和更低的复杂性，在自监督DINO模型上尤其表现出色，将相关性质量提高了15%以上，并保持了正向的忠实度相关性。

Conclusion: 该方法提供了一种有效且无需训练的视觉模型解释方案，通过使用平滑可调的轮廓，解决了传统密集掩码的碎片化和过拟合问题，生成了更忠实、紧凑、可解释且一致性更高的解释，尤其适用于自监督模型。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [210] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 该研究提出了一种基于蒸馏的框架，通过对抗自蒸馏（ASD）和首帧增强（FFE）策略，实现了高效的因果视频生成，显著减少了去噪步骤，同时提高了生成质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的混合视频生成模型结合自回归时间动态和基于扩散的空间去噪，但其顺序迭代性质导致误差累积和推理时间过长。

Method: 该方法建立在分布匹配蒸馏（DMD）框架之上，并提出：1) 对抗自蒸馏（ASD）策略，在分布层面将学生模型的n步去噪输出与其(n+1)步版本对齐，提供更平滑的监督和信息丰富的指导；2) 首帧增强（FFE）策略，为初始帧分配更多去噪步骤以减轻误差传播，同时对后续帧应用更大的跳过步骤。

Result: 在极少步（如1-2步）场景下，显著提高了训练稳定性和生成质量。在VBench上，该方法在一步和两步视频生成方面均超越了现有最先进的方法。此外，该框架生成了一个单一的蒸馏模型，灵活支持多种推理步长设置，无需重复再蒸馏。

Conclusion: 该框架通过创新的蒸馏策略，实现了高效、高质量的视频合成，显著减少了所需的去噪步骤，并提供了一个灵活支持多种推理设置的统一模型，有效解决了现有方法的误差累积和推理时间问题。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [211] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet是一种结合局部建模和全局上下文的医学图像分割网络，旨在通过增强边界和保留细节来解决解剖变异性和边界模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对于计算机辅助诊断和治疗规划至关重要，但显著的解剖变异性和边界模糊性阻碍了对精细结构的可靠描绘。

Method: 本文提出了RDTE-UNet网络，它采用混合ResBlock细节感知Transformer骨干，并包含三个模块：ASBE用于自适应边界增强，HVDA用于细粒度特征建模，以及EulerFF用于由欧拉公式引导的融合加权。

Result: 在Synapse和BUSI数据集上，RDTE-UNet在分割精度和边界质量方面达到了可比的水平。

Conclusion: RDTE-UNet通过统一局部建模和全局上下文，并结合其特有模块，提高了跨形态、方向和尺度的结构一致性和边界准确性。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [212] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 本文提出了一种分辨率感知的token解码器，用于解决越野语义分割中边界模糊、监督稀疏和标签噪声等问题，通过多尺度融合和精细化处理，实现了在不完美监督下的高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 越野语义分割面临着边界粗糙不一致、稀有类别监督稀疏以及普遍存在的标签噪声等挑战。现有方法（如低分辨率融合）会模糊边缘并传播局部错误，而维持高分辨率路径或重复高分辨率融合则成本高昂且对噪声敏感。

Method: 本文引入了一个分辨率感知的token解码器。主要计算在低分辨率瓶颈中进行；通过门控交叉注意力注入精细尺度细节，并仅对稀疏、不确定性选择的像素集进行精修。该解码器集成了：带有轻量级空洞深度可分离精修的全局自注意力以恢复局部一致性；门控交叉注意力集成来自标准高分辨率编码器流的精细尺度特征而不放大噪声；以及类别感知的点精修以纠正残余模糊性。训练期间，添加了一个边界带一致性正则化器，以鼓励在标注边缘周围的薄区域内进行连贯预测，且推理时无额外成本。

Result: 实验结果表明，该方法在越野语义分割中取得了具有竞争力的性能，并在过渡区域表现出更高的稳定性。

Conclusion: 该分辨率感知的token解码器在不完美监督下，成功平衡了全局语义、局部一致性和边界保真度，有效解决了越野语义分割中的关键挑战，提高了分割的准确性和稳定性。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [213] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 本文提出一种仅在训练阶段使用的目标，通过锐化决策边界和注入跨模态语义先验（来自RGB训练的教师模型），在不依赖可见光输入的情况下，显著提升了热红外目标检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 夜间热红外感知面临低对比度、弱高频特征等挑战，导致重复/重叠检测框、漏检小目标和类别混淆。现有方法（TIR转RGB或RGB-TIR融合）存在性能脆弱、额外传感器、校准和高运行时成本等问题，且未能直接优化检测器使用的热表示。

Method: 该研究在单模态推理（仅热红外）框架下，引入了仅在训练阶段使用的目标：1) 锐化实例级决策边界，通过拉近同类特征、推开异类特征来抑制重复和混淆检测。2) 注入跨模态语义先验，通过将学生模型（热红外检测器）的多级金字塔特征与RGB训练的教师模型对齐，从而在测试时无需可见光输入即可强化纹理贫乏的热特征。

Result: 实验结果表明，该方法优于现有方法，并达到了最先进的性能。

Conclusion: 通过在训练阶段引入锐化决策边界和注入跨模态语义先验的目标，本文有效解决了热红外检测中的核心挑战，在不依赖可见光输入的情况下，实现了更鲁棒、更高性能的夜间目标感知。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [214] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff是一种扩散辅助解码框架，用于增强深度联合源信道编码（JSCC）在对抗性无线环境（如干扰和导频欺骗）下的安全性与鲁棒性，同时实现低延迟和高效语义重建。


<details>
  <summary>Details</summary>
Motivation: 现有的深度JSCC框架容易受到物理层对抗性威胁（如导频欺骗和子载波干扰），从而损害语义保真度。

Method: SecDiff是一个即插即用的扩散辅助解码框架。它采用伪逆引导采样和自适应引导加权，以实现灵活的步长控制和高效的语义重建，避免了高推理延迟。为对抗干扰攻击，引入了基于功率的子载波掩蔽策略，并将恢复重构为掩蔽修复问题，通过扩散引导解决。为应对导频欺骗，将信道估计表述为盲逆问题，并开发了一种期望最大化（EM）驱动的重构算法，由重构损失和信道算子共同引导。该方法在扩散过程中交替进行导频恢复和信道估计，实现两者的联合细化。

Result: 在对抗性OFDM信道上的大量实验表明，SecDiff优于现有安全和生成式JSCC基线，在重建质量和计算成本之间取得了有利的平衡。

Conclusion: SecDiff为实现实用、低延迟和抗攻击的语义通信迈出了重要一步。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [215] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 本文提出了一种通过内容-风格子空间融合和内容-风格平衡损失来扩展内容-风格边界的新方法，以解决文本到图像扩散模型中高风格强度下内容特征丢失的问题，从而在不同风格强度下提高内容相似性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在评估内容相似性时，只考虑了单一风格强度。研究发现，增加风格强度会导致内容特征显著丢失，从而产生次优的内容-风格边界。

Method: 本文提出了一种新方法，通过利用内容-风格子空间融合（Content-Style Subspace Blending）和内容-风格平衡损失（Content-Style Balance loss）来扩展内容-风格边界。

Result: 该方法改善了不同风格强度下的内容相似性，显著拓宽了内容-风格边界。大量的实验表明，该方法在定性和定量评估中均优于现有技术，实现了卓越的内容-风格权衡，并且与当前方法相比，Inverted Generational Distance (IGD) 和 Generational Distance (GD) 分数显著降低。

Conclusion: 所提出的方法通过有效解决高风格强度下的内容丢失问题，实现了在文本到图像生成中更优越的内容-风格权衡，显著扩展了内容-风格边界。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [216] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 本研究提出了增强行人对齐网络（EPAN），用于在多样化的物联网监控条件下进行鲁棒的行人重识别。


<details>
  <summary>Details</summary>
Motivation: 行人重识别在计算机视觉，尤其是在物联网智能环境中的监控和安全应用中扮演着关键角色。

Method: 该研究引入了增强行人对齐网络（EPAN），采用双分支架构来减轻视角和环境变化的影响，并在不同尺度和视角下提取对齐信息。

Result: EPAN在Inspection-Personnel数据集上表现出色，Rank-1准确率达到90.09%，平均精度（mAP）达到78.82%。

Conclusion: EPAN在现实世界的物联网应用中具有巨大潜力，能够实现跨不同摄像头的有效可靠的行人重识别。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [217] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出了一种名为Luminance-Aware Statistical Quantification (LASQ) 的新型低光图像增强框架，将LLIE重新定义为分层亮度分布上的统计采样过程，灵感来源于自然亮度动态的幂律分布，显著提升了无正常光参考情况下的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法难以平衡重建保真度和跨场景泛化能力，主要关注确定性的像素级映射，却忽视了真实环境中亮度转换的连续物理过程，导致在缺乏正常光参考时性能下降。

Method: LASQ框架将LLIE重新定义为分层亮度分布上的统计采样过程。它将亮度转换概念化为强度坐标空间中的幂律分布，该分布可通过分层幂函数近似，从而用连续亮度层上的概率采样取代确定性映射。此外，设计了一个扩散前向过程，自主发现亮度层之间的最佳转换路径，实现无正常光参考的无监督分布仿真。

Result: LASQ显著提升了在实际应用中的性能，实现了更具适应性和通用性的光线恢复，尤其在无正常光参考的情况下。在有正常光参考的情况下，该框架在特定领域数据集上表现优异，并在非参考数据集上展现出更好的泛化能力。

Conclusion: LASQ通过将低光图像增强重新定义为基于亮度统计采样的过程，并利用幂律分布模拟亮度转换，克服了现有方法在保真度、泛化能力和无参考场景下的局限性，提供了一种更具适应性和通用性的光线恢复解决方案。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [218] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 该研究提出一个完整的系统，利用无监督学习方法，实现对具有局部特征（如污渍、磨损等）纹理的受控创作和编辑，无需手动标注。


<details>
  <summary>Details</summary>
Motivation: 在纹理合成过程中，包含自然界中普遍存在的表面改变（如污渍、撕裂、孔洞、磨损、变色等）对于生成逼真的纹理至关重要。

Method: 该方法采用基于学习的途径，利用无标签示例，通过无监督异常检测来识别改变外观的特征。这些纹理特征随后被自动聚类成语义连贯的组，用于指导图像的条件生成。整个流程从少量图像集合到多功能生成模型，并引入了基于扩散的编辑和无限平稳纹理生成算法。

Result: 该系统使用户能够交互式地在任意尺寸的纹理上创建和绘制特征，无需手动标注。它能自动检测并聚类纹理特征，生成具有受控局部特性的逼真纹理。所引入的扩散编辑和无限平稳纹理生成算法具有通用性，可应用于其他场景。

Conclusion: 该工作提供了一个完整的、基于无监督学习的纹理创作和编辑工作流程，能够处理具有局部特征的纹理。它生成了一个多功能的生成模型，支持用户交互式创建纹理，并且其核心算法具有广泛的应用潜力。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [219] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: 本文介绍了一种名为Napari Organoid Analyzer (NOA) 的通用图形用户界面工具，旨在简化生物学家进行AI驱动的类器官图像分析，使其无需编程经验即可使用。


<details>
  <summary>Details</summary>
Motivation: AI工具能显著提升类器官显微图像分析效率，但其对缺乏编程经验的生物学家而言可及性有限，导致工作流程仍以手动为主且劳动密集。此外，现有的大多数AI工具都过于专注于特定任务。

Method: 研究人员开发了Napari Organoid Analyzer (NOA)，这是一个通用的图形用户界面（GUI）。NOA集成了检测、分割、跟踪、特征提取、自定义特征标注和基于机器学习的特征预测等模块。它接口了多种最先进的算法，并作为开源的napari插件实现，以提供最大的灵活性和可扩展性。

Result: 通过三个案例研究，NOA展示了其多功能性：量化类器官分化过程中的形态变化、评估光毒性效应以及预测类器官的活力和分化状态。这些案例表明NOA能够在一个易于访问和可扩展的框架内实现全面的AI驱动类器官图像分析。

Conclusion: NOA提供了一个可访问且可扩展的框架，使生物学家能够进行全面的AI驱动类器官图像分析，从而克服了现有AI工具在可及性和通用性方面的限制。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [220] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习（DC-GAN）的方法来生成合成MRI数据，以解决原始数据有限的问题，并通过CNN分类器验证了合成数据在脑肿瘤分类任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 原始MRI数据量有限，且生成逼真的医学图像具有挑战性，这限制了计算机视觉在医学影像领域的应用。

Method: 采用深度卷积生成对抗网络（DC-GAN）来生成合成MRI数据。同时，使用卷积神经网络（CNN）分类器对真实和合成MRI数据进行脑肿瘤分类，以评估合成图像的质量和实用性。

Result: 分类结果表明，在真实图像和合成图像上取得了可比较的性能。

Conclusion: GAN生成的图像对于下游任务（如脑肿瘤分类）是有效的，这验证了其质量和实用性。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [221] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为NSYNC的新型对比学习框架，通过生成负合成数据集并利用梯度正交化来训练大型文本到图像扩散模型，以显著提高其捕捉特定风格的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的文本条件图像生成方法虽然能输出逼真的图像，但在捕捉特定风格方面表现不佳。即使在目标风格数据集上进行微调，也难以有效学习风格特征。

Method: 本研究引入了一个新颖的对比学习框架。其核心是生成“负合成数据集”，并将其与真实的正向图像一起用于对比训练。在训练过程中，模型会分别获取正向和负向梯度。然后，通过将正向梯度减去其在负向梯度上的投影，得到一个正交分量。模型参数基于这个正交分量进行更新，以消除正负数据中都存在的无关属性，从而促使模型学习更独特的风格特征。

Result: 在各种画家和插画师风格上的实验表明，该方法在定量和定性两方面都优于基线方法，有效提升了模型的风格化性能。

Conclusion: 通过引入负合成数据集和独特的梯度正交化策略，所提出的NSYNC框架能够显著增强大型文本到图像扩散模型捕捉和生成特定艺术风格的能力，解决了现有方法在风格化方面的不足。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [222] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 Lite ENSAM 的轻量级模型，用于从带有 RECIST 标注的 CT 扫描中进行高效的体积肿瘤分割，旨在解决手动标注耗时和 RECIST 测量局限性的问题。


<details>
  <summary>Details</summary>
Motivation: 评估癌症治疗反应时，肿瘤大小测量至关重要。RECIST v1.1 是广泛采用的标准，但依赖于单一平面测量，而体积测量被认为能提供更可靠的治疗效果评估。然而，手动体积标注的劳动密集性限制了其临床应用。

Method: 本文提出 Lite ENSAM，它是 ENSAM 架构的轻量级改编版本，专为从带有 RECIST 标注的 CT 扫描中进行高效体积肿瘤分割而设计。

Result: Lite ENSAM 在 MICCAI FLARE 2025 任务 1、子任务 2 的隐藏测试集上，取得了 60.7% 的 Dice 相似系数（DSC）和 63.6% 的归一化表面 Dice（NSD）。在公共验证数据集上，平均总 RAM 时间为 50.6 GBs，CPU 平均推理时间为 14.4 s。

Conclusion: Lite ENSAM 提供了一种高效且性能良好的方法，能够从带有 RECIST 标注的 CT 扫描中进行体积肿瘤分割，有望克服手动体积标注的局限性并改进治疗反应评估。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [223] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为CDD-VT的连续-离散双重视觉分词器，它融合了连续和离散分词的优点，通过自适应地分配视觉基元数量来处理不同复杂度的视觉数据，以解决多模态大模型中理解与生成统一的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型(MLLM)中理解与生成难以统一，主要原因在于连续和离散视觉分词的二元性。连续分词器(CT)性能强但管线复杂、工程开销大；离散分词器(DT)概念优雅但信息损失大、性能下降。本文旨在解决CT和DT之间的矛盾，寻找一种更好的视觉分词方法。

Method: 受波粒二象性启发，本文提出了连续-离散双重视觉分词器(CDD-VT)。它将视觉数据视为量化码本中图像基元的灵活组合，并根据视觉样本的复杂度自适应地确定所需的基元数量：简单实例使用少量基元（模拟离散分词），复杂实例使用大量基元（近似连续分词）。CDD-VT包含两个核心组件：多样化量化基元（鼓励基元正交性以更好地填充信息空间）和动态基元分配器（评估样本复杂度以确定最佳基元集）。

Result: 在重建、检索和分类任务上的广泛实验表明，CDD-VT的性能优于专门的CT和DT方法。它能在简洁且可扩展的MLLM中有效获得强大的结果。

Conclusion: CDD-VT通过结合连续和离散视觉分词的优势，有效地解决了两者之间的矛盾，为实现多模态大模型中理解与生成的统一提供了一种更优、更高效的视觉分词方法。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [224] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 本文提出了一种名为PSPooling的非学习型网格池化操作符，用于高效且结构保留的3D解剖形状分类，并引入了新的基准数据集MedShapeNet19。PSPooling显著提高了低标签情况下的重建保真度和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解剖学3D形状分类的进展受限于网格数据的复杂性和标准化基准的缺乏，这凸显了对鲁棒学习方法和可复现评估的需求。

Method: 本文提出了预计算结构池化（PSPooling），这是一种非学习型网格池化操作符，通过预计算基于几何邻近度的节点对应关系，实现高效且结构保留的图粗化，支持并行化和可逆的池化/反池化操作。PSPooling被整合到一个自监督图自编码器中，用于从无标签表面网格学习解剖学感知表示。此外，本文还推出了一个新的基准数据集MedShapeNet19，该数据集从MedShapeNet中整理而来，包含19个解剖学类别，并提供了标准化的训练、验证和测试划分。

Result: 实验结果表明，PSPooling显著提高了重建保真度和分类准确性，尤其是在低标签制度下，为医学3D形状学习建立了一个强大的基线。

Conclusion: PSPooling为医学3D形状学习提供了一种有效的解决方案，并且MedShapeNet19有望成为解剖形状分类和医学3D形状分析领域广泛采用的基准。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [225] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 本文提出PCD-ReID算法，利用Transformer网络提取共享组件特征，并结合大规模真实世界数据集，显著提升了基站环境下遮挡行人重识别的性能。


<details>
  <summary>Details</summary>
Motivation: 基站环境下遮挡行人重识别是计算机视觉领域的关键任务，但遮挡物常掩盖行人关键身体特征，导致识别复杂性高。传统基于ResNet的重识别算法难以有效处理遮挡问题，因此需要新的重识别方法。

Method: 本文提出了PCD-ReID（行人组件差异）算法。为解决遮挡问题，设计了一个基于Transformer的PCD网络，能够提取头盔、制服等共享组件特征。为缓解在公共数据集上的过拟合问题，研究人员收集了包含六个月、10,000名个体和超过50,000张图像的真实世界巡逻监控新数据集用于模型训练。

Result: PCD-ReID模型在平均精度均值（mAP）上达到了79.0%，Rank-1准确率达到82.7%。相较于基于ResNet50的方法，Rank-1准确率提高了15.9%。

Conclusion: 实验评估表明，PCD-ReID算法在塔台巡检场景中有效实现了遮挡感知行人重识别性能，凸显了其在监控和安保应用中的实际部署潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [226] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 本文提出了一种名为LFRD2的混合框架，结合神经网络和物理模型，以解决屏下ToF成像中透明OLED层引起的深度质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 屏下ToF成像在透明OLED层下会遇到信号衰减、多径干扰和时间噪声等严重退化，显著损害深度感知质量。因此，需要一种方法来缓解这些问题。

Method: 本文提出了可学习分数阶反应-扩散动力学（LFRD2）框架。该框架包含一个时间分数阶反应-扩散模块，通过动态生成的微分阶数实现迭代深度优化，以捕获长期依赖性。此外，还引入了一种通过系数预测和重复微分实现的有效连续卷积算子，以进一步提高恢复质量。

Result: 在四个基准数据集上的实验证明了该方法的有效性。

Conclusion: LFRD2框架通过结合神经网络的表达能力和物理模型的可解释性，成功地减轻了屏下ToF成像中由透明OLED层引起的深度退化问题，提升了深度感知质量。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [227] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种将强化学习（RL）有效集成到基于扩散的图像修复模型中的策略，通过使用基于IQA的奖励函数和自适应的RL与SFT结合方法，以提高修复质量并强调保真度。


<details>
  <summary>Details</summary>
Motivation: 将现有强化学习方法直接应用于基于扩散的图像修复模型效果不佳，因为修复任务的目标与纯生成不同，更侧重于图像保真度。

Method: 1. 发现有效的奖励函数可从图像质量评估（IQA）模型中导出，而非已在SFT阶段优化的基于真实值的监督。2. 策略专注于对远离真实值的挑战性样本使用RL。3. RL方法创新性地使用基于MLLM的IQA模型进行初始分布对齐。4. 随着样本接近真实值分布，RL与SFT自适应结合进行更精细的对齐。5. 通过基于训练样本相对难度的自动加权策略促进动态过程。该策略是即插即用的。

Result: 广泛的实验表明，所提出的RL框架能够提高各种修复任务的性能，并且在多个基准测试中证明了其有效性。

Conclusion: 本研究提供了一个有效且即插即用的RL框架，能无缝应用于基于扩散的图像修复模型，通过强调保真度和自适应学习策略显著提升性能。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [228] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: 本文提出Vote-in-Context (ViC)，一个通用的、免训练的框架，将列表级重排序和融合重新构想为视觉-语言模型（VLM）的零样本推理任务。通过在VLM提示中序列化内容证据和检索器元数据，ViC在跨模态视频检索中实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 异构检索器候选融合是一个长期存在的挑战，尤其对于视频等多模态复杂数据。典型的融合技术虽然免训练，但仅依赖排序或分数信号，忽略了候选的表征信息。

Method: ViC框架将列表级重排序和融合视为VLM的零样本推理任务。其核心思想是将内容证据（如将视频表示为S-Grid图像网格，可选配字幕）和检索器元数据直接序列化到VLM的提示中，使模型能够自适应地权衡检索器共识与视觉-语言内容。该框架被应用于跨模态视频检索领域。

Result: ViC作为单列表重排序器时，显著提高了单个检索器的精度；作为集成融合器时，持续优于CombSUM等强基线。在ActivityNet和VATEX等视频检索基准上，ViC建立了新的零样本检索性能SOTA，在MSR-VTT上Recall@1得分达到87.1% (t2v) / 89.0% (v2t)，在VATEX上达到99.6% (v2t)，比之前SOTA基线提升高达+40 Recall@1。

Conclusion: ViC提供了一个简单、可复现且高效的方法，将现代VLM转化为强大的零样本重排序器和融合器，有效处理复杂的视觉、时间信号和文本。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [229] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的渐进式网络架构，用于从H&E染色图像合成IHC等效图像，通过阶段性解耦优化结构、颜色和细胞边界，显著提高了合成图像的视觉质量和细节。


<details>
  <summary>Details</summary>
Motivation: 免疫组化（IHC）对于病理诊断至关重要，但其成本高昂、劳动密集、可扩展性差且多重染色受限。现有的计算染色转换技术虽然旨在提高效率，但通常依赖于线性加权损失函数，忽略了组件间的相互依赖性，导致图像质量欠佳，无法同时保持结构真实性和颜色保真度。

Method: 研究者提出了一种遵循渐进式结构的新型网络架构，融合了颜色和细胞边界生成逻辑，使每个视觉方面都能以阶段性、解耦的方式进行优化。该方法以自适应监督PatchNCE（ASP）框架为基线，并引入了基于3,3'-二氨基联苯胺（DAB）生色团浓度和图像梯度的额外损失函数，以增强生成IHC图像的颜色保真度和细胞边界清晰度。通过“结构-颜色-细胞边界”渐进机制重建了生成管道。

Result: 在HER2和ER数据集上的实验表明，该模型显著改善了视觉质量，并实现了更精细的结构细节。

Conclusion: 所提出的渐进式网络架构，通过解耦优化结构、颜色和细胞边界，并结合改进的损失函数，有效解决了现有染色转换技术在图像质量和细节方面的局限性，能够生成更高质量的合成IHC图像。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [230] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 本文介绍了PRBench，一个用于评估不同鲁棒性训练方法在概率鲁棒性（PR）方面的改进的基准，并比较了对抗鲁棒性（AR）和PR特定训练方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管概率鲁棒性（PR）被认为是对抗鲁棒性（AR）的实用补充，但专门用于提高PR的训练方法仍未得到充分探索。现有的PR特定训练方法存在三个局限性：评估协议不具可比性、与强大的对抗训练（AT）基线缺乏比较，以及缺乏统一的框架来比较这些方法的泛化能力。

Method: 本文引入了PRBench，第一个专门用于评估不同鲁棒性训练方法在PR方面改进的基准。PRBench通过一系列综合指标（包括干净准确率、PR和AR性能、训练效率和泛化误差）实证比较了最常见的AT和PR特定训练方法。此外，还对不同训练方法下PR性能的泛化误差进行了理论分析。

Result: PRBench的主要发现包括：AT方法在改善AR和PR性能方面比PR特定训练方法更具通用性，适用于不同的超参数设置；而PR特定训练方法则持续产生更低的泛化误差和更高的干净准确率。研究还公开了一个包含222个训练模型在7个数据集和10种模型架构上的排行榜。

Conclusion: PRBench提供了一个统一的框架来评估和比较不同的鲁棒性训练方法在概率鲁棒性方面的表现。研究结果揭示了对抗训练方法在通用性方面的优势，以及PR特定训练方法在泛化误差和干净准确率方面的优势，为未来的鲁棒性研究提供了宝贵的见解和基准。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [231] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION是一个统一的自动驾驶模型，它通过线性分组RNN操作符高效处理长序列多模态数据（LiDAR、图像、时间序列），实现了多种感知、预测和规划任务的领先性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列数据时，其二次注意力机制会引入显著的计算开销。自动驾驶系统需要高效处理大规模LiDAR点云、高分辨率多视图图像乃至时间序列数据。

Method: UniLION采用线性分组RNN操作符（对分组特征执行线性RNN），构建了一个统一的架构。该架构能够无缝支持多种专用变体（仅LiDAR、时间LiDAR、多模态、多模态时间融合），无需显式的时间或多模态融合模块。

Result: UniLION在广泛的核心任务上（包括3D感知、预测和规划）始终提供具有竞争力乃至最先进的性能，例如3D目标检测、跟踪、占用预测、BEV地图分割、运动预测和端到端规划。

Conclusion: UniLION的统一范式自然地简化了多模态和多任务自动驾驶系统的设计，同时保持了卓越的性能，并为自动驾驶领域的3D基础模型开发提供了新的视角。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [232] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 本研究开发了一个任务探索管道，用于自动识别用户完成任务的关键全局和局部策略，以及有意义的子任务，以增进机器对用户知识、技能和行为的理解，从而实现隐式人机协调。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是推动机器在预期人机交互中对用户知识、技能和行为的理解，以便实现隐式协调，从而促进机器通过预测用户未来状态来优化交互。

Method: 开发了一个“任务探索管道”，该管道结合了聚类技术、因子分析和字符串编辑距离，以自动识别用户完成任务时使用的关键全局和局部策略。全局策略识别通用的动作集合，而局部策略识别以相似构成使用这些动作集合的序列。此外，该管道还能识别任务中各种长度的有意义的子任务。还开发了一个任务探索应用程序以方便审查管道结果。

Result: 任务探索管道能够自动识别完成任务所使用的关键策略，并用分层子任务结构编码用户运行。此外，还开发了一个任务探索应用程序，可以轻松审查管道结果。

Conclusion: 该任务探索管道可以轻松修改以适用于任何基于动作的时间序列数据，并且所识别的策略和子任务有助于人机了解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [233] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 本文提出HGFreNet，一种新型GraphFormer架构，用于单目视频中的2D到3D人体姿态提升。它通过跳跃混合特征聚合和频域3D轨迹一致性，有效解决了深度模糊和2D姿态估计错误导致的3D轨迹不连贯问题，提高了姿态估计的准确性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 单目视频中的2D到3D人体姿态提升面临深度模糊和2D姿态估计错误导致3D轨迹不连贯的挑战。现有方法通常只限制相邻帧之间的抖动，而忽略了骨骼关节运动的全局时空相关性。

Method: 本文设计了HGFreNet，一种结合跳跃混合特征聚合和频域3D轨迹一致性的新型GraphFormer架构。具体方法包括：
1.  提出跳跃混合图注意力（HGA）模块，将骨骼关节的所有k跳邻居分组，以扩大感受野并发现这些组的潜在全局相关性。
2.  使用Transformer编码器来建模全局关节时空相关性。
3.  通过在频域约束轨迹一致性来利用全局时间相关性。
4.  应用一个初步网络来估计3D姿态，为跨帧深度推断提供3D信息并保持时间连贯性。

Result: 在Human3.6M和MPI-INF-3DHP两个标准基准数据集上进行的广泛实验表明，所提出的HGFreNet在位置准确性和时间一致性方面均优于现有最先进（SOTA）方法。

Conclusion: HGFreNet通过创新的跳跃混合图注意力机制和频域轨迹一致性约束，成功解决了2D到3D人体姿态提升中的3D轨迹不连贯问题，实现了卓越的姿态准确性和时间一致性，超越了现有SOTA方法。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [234] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos是一个统一的图像和视频重新打光框架，通过引入RGB空间几何反馈和路径一致性学习，显著提高了打光结果的物理真实感和计算效率，并提供了细粒度控制和评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在语义潜在空间中进行优化，导致重新打光结果常出现不真实现象，如过曝高光、阴影错位和遮挡错误，缺乏物理正确性。此外，高质量视觉空间监督计算成本高昂。

Method: 本文提出了UniLumos框架，将RGB空间几何反馈引入流匹配骨干网络。通过从模型输出中提取深度和法线图进行监督，显式地将打光效果与场景结构对齐，提高物理合理性。为解决多步去噪的计算开销，采用了路径一致性学习，使其在少步训练下仍能有效监督。为实现细粒度控制和监督，设计了一个结构化的六维标注协议。在此基础上，提出了LumosBench，一个解耦的属性级别基准，通过大型视觉语言模型评估打光可控性。

Result: UniLumos在图像和视频重新打光方面均实现了最先进的质量，显著改善了物理一致性，同时实现了20倍的速度提升。

Conclusion: UniLumos通过结合RGB空间几何反馈和高效的路径一致性学习，成功克服了现有扩散模型在重新打光中物理不准确和计算效率低的问题，为图像和视频打光提供了高质量、高效率且可控的解决方案。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [235] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: 该研究推出了Scitextures数据集，一个包含大量科学、技术和艺术领域纹理及生成模型的集合，用于评估AI连接视觉模式与其生成机制的能力，并发现领先的视觉-语言模型（VLMs）在此方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 将视觉模式与其形成过程联系起来是视觉理解的深层形式。云、波浪、城市、森林的生长以及材料和景观的形成都体现了从底层机制中出现的模式。研究旨在探索和评估AI理解这种模式与机制之间联系的能力。

Method: 研究方法包括：1. 创建Scitextures数据集：一个大规模的纹理和视觉模式集合，包含来自物理、化学、生物、社会学、技术、数学和艺术等领域的1200多个不同模型和100,000张图像，以及生成这些图像的模型和代码。2. 数据集创建：通过一个自主收集和标准化实现模型的智能AI管道生成。3. 评估AI能力：使用Scitextures数据集评估领先的AI模型（特别是视觉-语言模型VLMs）连接视觉模式到其生成模型和代码的能力，识别源自相同过程的不同模式，以及从真实世界模式的自然图像中推断、建模和重现其形成机制的能力。

Result: 基准测试结果表明，视觉-语言模型（VLMs）不仅能理解视觉模式本身，还能理解并模拟模式背后的物理系统。AI能够从自然图像中识别、建模和编码形成模式的机制，并生成与真实图像相似的模拟图像。

Conclusion: Scitextures数据集为探索塑造世界的视觉模式与产生它们的机制之间的联系提供了途径。研究证明，视觉-语言模型在理解和模拟超越单纯视觉模式的物理系统方面具有显著能力，为AI在科学理解领域的应用开辟了新的可能性。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [236] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 该研究引入了TIR-Bench，一个用于评估多模态大语言模型（MLLMs）高级视觉推理（“图像思考”）能力的综合基准，发现其对现有模型具有普遍挑战性，并需要真正的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准，包括常见的视觉搜索，未能充分捕捉到像OpenAI o3这类模型所具备的、通过智能创建和操作工具来转换图像以解决问题的能力（即链式思维中的“图像思考”）。这些基准仅测试基础操作，无法深入了解更复杂、动态且依赖工具的推理。

Method: 研究引入了TIR-Bench，一个包含13个多样化任务的综合基准，每个任务都要求在链式思维中进行新颖的工具使用，以进行图像处理和操作。研究评估了22个多模态大语言模型，包括领先的开源模型、专有模型以及明确增强了工具使用能力的模型。此外，还进行了一项关于直接微调与智能体微调的初步研究。

Result: 评估结果表明，TIR-Bench对所有测试的多模态大语言模型都具有普遍挑战性。要获得强大性能，需要模型具备真正的“图像思考”能力。

Conclusion: TIR-Bench是一个有效的基准，能够评估多模态大语言模型在复杂图像处理和操作中结合工具使用的“图像思考”能力，并揭示了现有模型在这方面的不足。初步研究也为未来的模型微调方向提供了见解。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


### [237] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 本文提出了一种提示驱动的GraphRAG框架，通过强调提示设计在实体提取、事实选择和段落重排中的作用，显著提升了多跳问答（QA）的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG结合图检索已用于复杂推理，但提示设计对检索和推理过程的影响仍未得到充分研究。

Method: 该方法构建了一个提示驱动的GraphRAG框架，利用提示公式来促进实体提取、事实选择和段落重排。它从文本数据中创建符号知识图谱（将实体和事实关系编码为结构化事实三元组）。在在线检索过程中，选择性地使用大型语言模型（LLMs）进行语义过滤和答案生成。同时，通过个性化PageRank（PPR）进行实体引导的图遍历，以支持高效、可扩展的检索。

Result: 该系统在HotpotQA和2WikiMultiHopQA数据集上取得了最先进的性能，F1分数分别为80.7%和78.9%，Recall@5分数分别为97.1%和98.1%。结果表明，提示设计是提高检索准确性和响应质量的重要组成部分。

Conclusion: 提示设计对于提高检索准确性和响应质量至关重要。这项研究为更高效、更易理解的多跳问答系统奠定了基础，并强调了提示感知图推理的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [238] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: 本文提出了CGF-DETR，一种增强型实时检测Transformer模型，通过引入XFABlock、SPGA模块和GCFC3模块，显著提升了胸部X光片肺炎检测的准确性，同时保持了实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因，需要准确高效的自动化检测系统。尽管RT-DETR等基于Transformer的检测器在目标检测任务中表现出色，但它们在医学影像，特别是胸部X光片肺炎检测中的应用仍未得到充分探索。

Method: 本文提出了CGF-DETR，一个专门为肺炎检测设计的增强型实时检测Transformer。其主要方法包括：1. 在骨干网络中引入XFABlock，通过卷积注意力机制结合CSP架构改进多尺度特征提取。2. 提出SPGA模块，用动态门控机制和单头自注意力取代标准多头注意力，以实现高效特征聚合。3. 在Neck部分设计GCFC3，通过多路径卷积融合和结构重参数化增强特征表示，同时保持实时性能。

Result: 在RSNA肺炎检测数据集上，CGF-DETR实现了82.2%的mAP@0.5，比基线RT-DETR-l高出3.7%，同时保持了48.1 FPS的可比推理速度。消融研究证实了每个提出模块对整体性能提升的有效贡献，完整模型达到了50.4%的mAP@[0.5:0.95]。

Conclusion: CGF-DETR通过其提出的新模块，显著提高了胸部X光片肺炎检测的准确性，优于RT-DETR-l基线模型，并保持了实时性能，为肺炎的自动化检测提供了有效的解决方案。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [239] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 本文提出了一种残差流解码器框架，用于从语言模型激活中探测段落和文档级别的长期规划信息，并发现可以解码出相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够执行更长时间跨度的任务，现有的可解释性研究方法通常仅限于测试特定的概念或token，而无法理解模型如何编码长期规划信息。

Method: 开发了“残差流解码器”（Residual Stream Decoders）框架，作为一种探测模型激活的方法，以理解模型在段落和文档级别的规划。测试了几种解码方法。

Result: 在小型模型中，可以解码出相当于5个以上未来上下文token的信息。

Conclusion: 这些结果为更好地监控语言模型以及更好地理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [240] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 该研究引入了PlotCraft基准来评估大型语言模型（LLMs）在复杂数据可视化方面的能力，发现现有LLMs表现不足。为弥补这一差距，作者开发了SynthVis-30K数据集和小型高效模型PlotCraftor，后者在复杂可视化任务上表现出色，尤其在困难任务上性能提升超过50%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但它们创建复杂数据可视化的能力尚未得到充分评估和开发。现有LLMs在处理规模化和结构化数据的复杂可视化方面存在空白。

Method: 1. 引入PlotCraft基准：包含1000个挑战性可视化任务，覆盖金融、科研、社会学等领域，涵盖7种高级可视化任务和48种图表类型，系统评估单轮生成和多轮优化。2. 评估：对23个主流LLMs在PlotCraft上进行综合评估。3. 开发SynthVis-30K：一个通过协作代理框架合成的大规模、高质量复杂可视化代码数据集。4. 开发PlotCraftor：一个基于SynthVis-30K数据集构建的、体积小但能力强的代码生成模型。

Result: 1. 评估显示，现有LLMs在处理复杂可视化任务时存在明显的性能缺陷。2. PlotCraftor模型在VisEval、PandasPlotBench和PlotCraft等基准测试中，表现与领先的专有方法相当。3. 尤其在困难任务上，PlotCraftor的性能提升超过50%。

Conclusion: 大型语言模型在复杂数据可视化方面存在明显不足。通过引入专门的基准测试PlotCraft、构建大规模高质量数据集SynthVis-30K，并开发出小型高效的PlotCraftor模型，可以显著提升复杂数据可视化的生成能力，甚至超越现有领先的专有方法，尤其在处理困难任务时表现出卓越的性能改进。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [241] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI是一个认知对齐的MBTI推断框架，它将原型理论融入基于大型语言模型的管道中，通过LLM增强语料库和LoRA微调编码器来学习原型，并在推断时使用检索-重用-修订-保留循环，显著提高了文本个性识别的准确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的文本个性识别通常被视为硬标签分类问题，这掩盖了人类个性判断中分级、原型化的本质。

Method: 首先，通过LLM引导的多维度（语义、语言、情感）增强，构建了一个平衡且质量受控的语料库。其次，使用LoRA微调了一个轻量级（<=2B）编码器，以学习判别性嵌入并标准化一系列人格原型。在推断时，模型为查询文本检索top-k原型，并执行“检索-重用-修订-保留”循环：通过提示词投票聚合原型证据，在出现不一致时进行修订，并在预测正确后保留样本以持续丰富原型库。

Result: ProtoMBTI在Kaggle和Pandora基准测试中，无论是对于MBTI的四个二分法还是完整的16种类型任务，都超越了基线模型，并表现出强大的跨数据集泛化能力。

Conclusion: 将推断过程与心理原型推理对齐，可以显著提高文本个性建模的准确性、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [242] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出通过预测信息丰富的token而非传统的下一token预测来训练大型语言模型（LLMs），以优化训练性能。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型（LLMs）的训练性能是一个关键挑战，尤其是在提高模型性能的同时控制计算成本。传统训练方法（下一token预测，NTP）可能不是最有效的方式。

Method: 研究者提出一种通过预测信息丰富的token来训练LLMs的方法，并将其影响在算术、文本多标签分类和自然语言生成三类任务中进行调查。

Result: 通过这种方法，模型性能和对目标token选择策略的理论理解都得到了提升。

Conclusion: 该工作为优化LLM训练提供了一种原则性方法，通过目标token选择策略提高了模型性能和理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [243] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 该研究提出了一个统一框架，用于评估和改进大型语言模型（LLMs）在模拟人类用户对话中的角色一致性。通过定义三个自动一致性指标，并将其作为奖励信号，使用多轮强化学习微调LLMs，显著降低了角色不一致性。


<details>
  <summary>Details</summary>
Motivation: 在治疗、教育和社交角色扮演等互动场景中，LLMs越来越多地被用于模拟人类用户。然而，现有的LLMs常偏离其设定的角色，前后矛盾，或放弃角色应有的行为，导致模拟效果不佳。

Method: 研究引入了三个自动指标来衡量角色一致性：提示到对话行一致性、对话行之间的一致性以及问答一致性，并对照人工标注进行了验证。随后，将这些指标作为奖励信号，应用多轮强化学习对LLMs进行微调，使其适应患者、学生和社交聊天伙伴三种用户角色。

Result: 该方法将LLM生成对话中的不一致性降低了超过55%，从而产生了更连贯和忠实于角色的模拟用户。

Conclusion: 该研究提供了一个有效的框架，能够显著提高LLMs在模拟人类用户时保持角色一致性的能力，使其在互动设置中表现更佳。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [244] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: 本文介绍AgentBnB，一个将大型语言模型（LLM）集成到网络安全桌面演练（TTX）中的浏览器游戏，旨在提供可扩展、低成本的训练，并初步显示出优于传统方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的网络安全桌面演练（TTX）虽然有价值，但通常是预设剧本、资源密集且难以扩展。

Method: 研究者开发了AgentBnB，一个基于浏览器的Backdoors & Breaches游戏重构版。它集成了LLM队友和一个与布鲁姆分类法对齐的检索增强型（RAG）辅助系统（C2D2），该系统能从精选语料库中提供认知导向的提示。提示工程（prompt-engineered）代理采用一种逐渐淡出的支架式学习方法。

Result: 在一个四名研究生的单人飞行测试中，参与者表示更倾向于使用基于代理的版本而非实体卡牌，并认为它更具可扩展性。尽管在简单的知识测验中出现了天花板效应，但早期发现表明LLM增强的TTX可以提供轻量级、可重复的练习。

Conclusion: 尽管存在样本量小、单人模式和语料库狭窄的局限性，但研究结果表明，大型语言模型增强的桌面演练可以提供轻量化、可重复的练习，且无需传统演练的后勤负担。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [245] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了一个名为IL-PCR的印度法律语料库，用于同时进行法律条文和先例检索，旨在利用这两个任务之间的内在依赖性，并通过基于LLM的重排序方法实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 法律从业者需要检索相关法律条文和先例，但现有研究将这两个任务独立处理，开发了不同的数据集和模型。然而，这两个检索任务本质上是相关的（例如，相似的案件倾向于引用相似的法律条文）。本文旨在解决这一研究空白。

Method: 本文提出了IL-PCR（印度法律语料库，用于先例和法律条文检索），这是一个独特的语料库，为开发能够利用两个任务之间依赖性的模型提供了一个通用测试平台。研究人员在多个基线模型上进行了广泛实验，包括词汇模型、语义模型和基于GNN的集成模型。此外，为了利用两个任务之间的依赖性，开发了一种基于LLM的重排序方法。

Result: 基于LLM的重排序方法在实验中取得了最佳性能。

Conclusion: 通过提出IL-PCR语料库和开发基于LLM的重排序方法，本文成功地利用了法律条文检索和先例检索任务之间的依赖性，并取得了优异的性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [246] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了一种名为POSESTITCH-SLT的新型预训练方案，该方案利用模板生成的句子对显著提升了手语翻译在低资源环境下的性能，超越了现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模、句子对齐数据集稀缺的挑战，以往的研究主要集中于特征提取和架构改进。

Method: 本文提出POSESTITCH-SLT，一种受语言学模板句子生成技术启发的预训练方案。该方法在训练中利用模板生成的句子对，并结合一个简单的基于Transformer的编码器-解码器架构。

Result: 在How2Sign数据集上，BLEU-4分数从1.97提升至4.56；在iSign数据集上，从0.55提升至3.43。这些结果均超越了基于姿态的无手语词汇翻译的现有最佳方法。

Conclusion: 模板驱动的合成监督在低资源手语翻译环境中表现出显著的有效性。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [247] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 本文提出Factorization Memory (FM)，一种高效的循环神经网络（RNN）架构，在短上下文语言建模任务上与Transformer模型性能相当，并在长上下文场景中展现出卓越的泛化能力。它基于Mamba-2，并引入了稀疏化版本。


<details>
  <summary>Details</summary>
Motivation: 现有模型在长上下文场景中的泛化能力和计算效率不足，尤其是在与Transformer竞争短上下文性能的同时，需要一种能在长上下文场景中表现更好的RNN架构。

Method: 本文提出了Factorization Memory (FM) 架构，它构建于Mamba-2之上，以实现训练期间的并行计算和推理期间的恒定计算及内存复杂度。为进一步优化效率和表示能力，开发了FM的稀疏化版本，每次只更新部分循环状态。研究还对FM与Transformer和Mamba-2架构进行了系统的实证分析。

Result: Factorization Memory在短上下文语言建模任务上实现了与Transformer模型相当的性能，并在长上下文场景中展示了卓越的泛化能力。其稀疏化版本在保持强大性能的同时，进一步优化了模型效率。这是首个成功结合稀疏内存激活并在短、长上下文设置中均具竞争力的RNN架构。

Conclusion: Factorization Memory是一种高效且性能强大的RNN架构，它在短上下文任务上可与Transformer媲美，并在长上下文场景中展现出更优的泛化能力。其稀疏化版本在保持高性能的同时提升了效率，为RNN设计开辟了新方向。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [248] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: 本文提出了LingGym基准，用于评估大型语言模型（LLMs）的元语言推理能力，通过使用来自18种不同类型学参考语法的IGT和语法描述，重点考察LLMs在低资源语言和未见结构上的泛化推理能力。


<details>
  <summary>Details</summary>
Motivation: 以往研究侧重于特定下游任务，本文旨在评估LLMs是否能够将语言推理泛化到训练期间未见的低资源语言和结构。

Method: 引入了LingGym基准，该基准利用来自18种类型学多样参考语法的语际加注文本（IGT）和语法描述。评估任务是“词-注音推理”，模型需要根据上下文和不同级别的语言信息（如注音、语法解释、翻译）推断缺失的词和注音。

Result: 研究结果表明，整合结构化的语言线索能够持续提升所有模型的推理性能。

Conclusion: 这项工作突显了使用LLMs进行类型学语言分析和低资源语言文档编制的潜力和当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [249] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 因果语言模型（CLM）目标函数具有反转不变性，即对文本及其反转赋予相同似然，这使其在预训练时对方向不敏感，限制了其捕捉语言中方向性依赖的能力。


<details>
  <summary>Details</summary>
Motivation: 人类语言和推理本质上具有时间不对称性，编码着方向性依赖（如语音、形态、因果关系）。然而，当前的因果语言模型预训练目标函数具有反转不变性，导致其对方向不敏感。研究者认为这种对称性是当前预训练目标的一个局限，可能未能充分捕捉语言的内在方向性。

Method: 本文通过形式化因果语言模型（CLM）目标函数的结构属性，即“反转不变性”，来分析其特性。具体而言，研究者分析了下一个词预测损失如何对语料库及其反转文本赋予相同的似然。

Result: 下一个词预测损失对语料库及其反转文本赋予相同的似然，这意味着标准的CLM预训练是方向盲的。这一对称性解释了为什么在反转文本上训练的模型可以达到与在正向文本上训练的模型相当的性能。

Conclusion: CLM目标函数的反转不变性是当前预训练目标的一个局限。未来工作应通过引入明确建模语言方向（时间不对称性）的损失函数和架构，来克服这一限制，同时保持标准的语言建模能力。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [250] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 该研究提出并解决了推理轨迹（RT）生成任务，利用大型语言模型（LLM）生成苏格拉底式调试的RT和对话，并通过“LLM作为评判者”的评估展示了高准确率。


<details>
  <summary>Details</summary>
Motivation: 大多数新手程序员的错误源于编程误解。苏格拉底式调试能引导学生自主识别并纠正这些误解，而不是直接提供答案。本研究旨在通过自动化生成推理轨迹来支持这种有效的教学方法。

Method: 将苏格拉底式调试形式化为引导学生识别并纠正导致错误的误解的推理轨迹（RT）。引入了推理轨迹生成任务，并构建了一个手动标注RT的调试问题数据集。开发了基于LLM的解决方案来生成RT以及基于这些RT的苏格拉底式对话。

Result: 通过大规模的“LLM作为评判者”评估，前沿模型能够生成高达91%的正确推理轨迹和98.7%的有效对话回合。

Conclusion: 大型语言模型在生成苏格拉底式调试所需的推理轨迹和对话方面表现出强大的能力，为支持学生自主调试提供了有效工具。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [251] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: AI生成文本检测器在直接LLM输出上表现良好，但对迭代释义内容失效。研究揭示了迭代释义的“洗稿”机制，并引入PADBen基准，发现当前检测器在作者归属混淆问题上表现不佳，需要新的检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本（AIGT）检测器对直接LLM输出准确率超过90%，但在迭代释义内容面前却灾难性失败。本研究旨在探究为何这种本身是AI生成的迭代释义文本能逃避AIGT检测系统，并解决由此引发的作者归属混淆和抄袭规避问题。

Method: 1. 通过内在机制分析，揭示迭代释义如何创建一个“洗稿区域”，其特点是语义位移但生成模式得以保留，并归纳出归因混淆和抄袭规避两种攻击类别。2. 引入PADBen基准，这是首个系统评估检测器对这两种释义攻击场景鲁棒性的基准。3. PADBen包含一个五类型文本分类法，涵盖从原始内容到深度洗稿文本的全过程，以及五项跨句对和单句挑战的渐进式检测任务。4. 评估了11种最先进的检测器。

Result: 1. 迭代释义创建了一个“洗稿区域”，其特征是语义位移但保留了生成模式。2. 检测器在抄袭规避问题上表现成功，但在作者归属混淆问题上失败，存在关键不对称性。3. 现有检测方法无法有效处理中间“洗稿区域”。

Conclusion: 当前检测方法无法有效处理迭代释义文本，尤其是在作者归属混淆方面。这表明需要超越现有语义和风格区分方法的检测架构上的根本性进步，以应对这种中间“洗稿区域”带来的挑战。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [252] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 该研究引入了MedRECT，一个跨语言（日语/英语）基准，用于评估大型语言模型（LLMs）在医学文本中的错误检测、定位和纠正能力。结果显示推理模型表现出色，精调可显著提升性能，甚至超越人类专家，为开发更安全的跨语言医学LLMs提供了框架和资源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗应用中展现出巨大潜力，但其在临床文本中检测和纠正错误的能力（安全部署的先决条件）尚未得到充分评估，尤其是在英语以外的语言中。

Method: 研究引入了MedRECT，一个跨语言（日语/英语）基准，将医学错误处理任务分解为错误检测、错误定位（句子提取）和错误纠正三个子任务。MedRECT通过自动化流程构建，数据来源于日本医师执照考试（JMLE）及其英文对应版本，生成了MedRECT-ja（663篇文本）和MedRECT-en（458篇文本）。研究评估了9种主流LLMs（包括专有、开源和推理模型），并使用了有针对性的LoRA精调方法。

Result: (i) 推理模型显著优于标准架构，在错误检测方面相对提升高达13.5%，在句子提取方面相对提升高达51.0%；(ii) 跨语言评估显示，从英语到日语存在5-10%的性能差距，但推理模型的差距较小；(iii) 有针对性的LoRA精调在错误纠正性能上产生了不对称的提升（日语：+0.078，英语：+0.168），同时保留了推理能力；(iv) 精调后的模型在结构化医学错误纠正任务上超越了人类专家表现。

Conclusion: MedRECT是首个全面的医学错误纠正跨语言基准，为开发更安全的跨语言医学LLMs提供了可复现的框架和资源。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [253] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）的记忆化如何影响其生成的合著者网络，发现普遍存在偏向高被引研究者的偏见，但也存在学科和区域差异。


<details>
  <summary>Details</summary>
Motivation: LLMs正在重塑搜索和推荐平台，但其记忆化能力可能引入严重的公平性和偏见问题，影响合著者网络的准确性和公正性，并可能放大科学界现有的偏见。

Method: 通过评估DeepSeek R1、Llama 4 Scout和Mixtral 8x7B这三个主流模型，分析其记忆化对合著者网络的影响，并研究这些由记忆化驱动的输出在不同学术学科和全球区域间的差异。

Result: 全球分析显示LLMs普遍偏向高被引研究者。然而，在某些学科（如临床医学）和区域（如非洲部分地区），表现出更平衡的代表性，表明这些领域的LLM训练数据可能反映了更大的公平性。

Conclusion: 研究结果强调了在学术发现中部署LLMs的风险（潜在的偏见放大）和机遇（训练数据中可能存在的公平区域）。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [254] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在输出多样性方面存在局限，本文提出G2方法，一个无需训练的即插即用方案，通过双重引导器在解码过程中干预生成，以在保持质量的同时提高输出多样性。


<details>
  <summary>Details</summary>
Motivation: LLMs在各种NLP任务中表现出色，但其输出多样性不足，经常生成高度相似的内容，这严重影响了需要多样化输出的任务（如创意写作、推理）。现有解决方案（如温度缩放）虽能增加多样性，但会损害输出质量。

Method: G2是一种无需训练的即插即用方法。它使用一个基础生成器和两个“引导器”（dual Guides）。这些引导器通过基于解码的干预措施，在生成过程中引导模型，以鼓励在原始查询条件下产生更多样化的输出。

Result: 全面的实验表明，G2有效提高了输出多样性，同时在多样性和质量之间保持了最佳平衡。

Conclusion: G2成功地在不牺牲生成质量的前提下，增强了大型语言模型的输出多样性，提供了一个优于现有方法的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [255] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文介绍了首个也是最大的俾路支语-印地语-英语平行语料库（BHEPC），并评估了多种多语言大型语言模型（MLLMs）在该低资源语言对上的机器翻译性能，发现微调后的NLLB-200模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性对机器翻译构成挑战，特别是对于俾路支语等缺乏高质量语言资源的弱势部落语言。研究旨在弥补这一资源空白。

Method: 研究方法包括：1. 构建了包含11万句句子的俾路支语-印地语-英语平行语料库（BHEPC），涵盖教育、行政和新闻等领域，由专业人工译者协助完成。2. 评估了多种专有和开源多语言大型语言模型（MLLMs）在英语/印地语与俾路支语之间的双向翻译任务上的性能。3. 对NLLB-200蒸馏版600M模型进行了微调。4. 利用上下文学习（in-context learning）探究了多语言LLMs在BHEPC上的生成式翻译能力，并评估了跨领域泛化性能和分布差异。

Result: 主要结果包括：1. BHEPC是全球首个也是最大的俾路支语-印地语-英语平行语料库。2. 综合评估表明，微调后的NLLB-200蒸馏版600M模型优于其他模型，突显了多语言模型在低资源场景下的潜力。3. 评估了多语言LLMs在BHEPC上的生成式翻译能力，并量化了其在跨领域泛化和分布差异方面的表现。

Conclusion: 这项工作弥补了关键的资源空白，为低资源和边缘化语言推广了包容性的自然语言处理技术。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [256] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 该研究首次评估了数据集大小对差分隐私自然语言处理（DP NLP）文本重写机制的效用和隐私保护效果的影响，发现数据集大小是一个关键因素，并呼吁更严格的评估程序。


<details>
  <summary>Details</summary>
Motivation: 以往的DP NLP文本重写机制评估中，数据集大小这一关键因素常被忽视，未能充分考虑其对机制效用和隐私保护能力的影响。

Method: 研究设计了针对大型数据集（多达一百万文本）的效用和隐私测试，采用动态分割大小，并量化了数据集大小增加对隐私-效用权衡的影响。

Result: 研究发现数据集大小在评估DP文本重写机制中起着不可或缺的作用，并且其增加会影响隐私-效用权衡。

Conclusion: 这些发现表明DP NLP需要更严格的评估程序，并为DP NLP在实践和大规模应用中的未来发展提供了启示。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [257] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出ToM，一个面向树的MapReduce框架，通过利用长文档的层次结构并进行递归推理，显著提升了大型语言模型（LLMs）在长上下文推理中的逻辑连贯性和性能，优于现有检索增强生成（RAG）和分而治之（DCF）方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因上下文窗口限制，在长上下文推理时性能显著下降。现有方法如RAG因依赖相似度排名而牺牲逻辑连贯性；分而治之框架（DCF）虽擅长局部推理，但难以捕捉长距离依赖，且因独立处理块而易产生冲突。因此，需要一种能克服这些局限性的长上下文推理方法。

Method: 本文提出了ToM（Tree-oriented MapReduce framework），一种新颖的面向树的MapReduce框架。ToM通过分层语义解析构建DocTree，利用长文档固有的层次结构（如主标题和副标题）。它采用自下而上的聚合和树形MapReduce方法进行递归推理：在Map步骤中，在子节点生成推理；在Reduce步骤中，聚合兄弟节点间的推理，以在父节点解决冲突或达成共识。

Result: 实验结果表明，ToM在70B+ LLMs上显著优于现有的分而治之框架和检索增强生成方法，实现了更好的逻辑连贯性和长上下文推理能力。

Conclusion: ToM框架通过其独特的树形结构和递归MapReduce推理方法，有效解决了LLMs在长上下文推理中面临的挑战，克服了RAG和DCF方法的局限性，在逻辑连贯性和推理性能上取得了显著提升。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [258] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 该研究提出了Zero-RAG框架，通过识别并修剪RAG外部语料库中的冗余知识，并结合查询路由器和噪声容忍微调，以更有效地利用大型语言模型（LLM）的内部知识，从而提高检索效率而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法使用大型外部语料库补充LLM知识，但随着LLM内部知识的显著扩展，外部语料库与LLM之间存在大量知识冗余。这种冗余不仅增加了稠密检索的索引成本和工作量，而且对LLM能够自行回答的问题，冗余知识反而会损害RAG性能。

Method: 1. 提出了“Mastery-Score”指标来识别并修剪RAG语料库中的冗余知识，使LLM对“已掌握”问题的回答主要依赖其内部知识。2. 提出了“Query Router”和“Noise-Tolerant Tuning”机制，以避免无关文档的干扰，进一步提升LLM在修剪后语料库下对内部知识的利用能力。

Result: 实验结果表明，Zero-RAG成功将Wikipedia语料库修剪了30%，并将检索阶段的速度提高了22%，同时没有损害RAG的整体性能。

Conclusion: Zero-RAG通过有效识别和修剪RAG语料库中的冗余知识，并优化LLM对内部知识的利用，显著提高了检索效率，降低了成本，且保持了RAG的性能，为解决RAG中的知识冗余问题提供了一个有效方案。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [259] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 该研究调查了BERT、ALBERT等Transformer模型上下文词嵌入中的性别偏见，引入了新的量化指标MALoR，并提出通过在性别平衡数据集上继续预训练来缓解偏见的方法，实验证明该方法显著降低了性别偏见且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 编码器Transformer模型在各种语言任务中表现出色，但也被发现继承了训练数据中的性别偏见。因此，研究和缓解这些模型（特别是其上下文词嵌入）中的性别偏见变得日益重要。

Method: 研究了BERT、ALBERT、RoBERTa和DistilBERT等主流Transformer架构中的性别偏见。引入了MALoR（Masked-token probability-based Assessment of Language-model-induced-bias over Representations）这一新指标，通过模型填充掩码标记的概率来量化偏见程度。提出了一种缓解方法：利用反事实数据增强（Counterfactual Data Augmentation）生成性别平衡的数据集，并在此数据集上进行持续预训练。

Result: 实验结果显示，通过提出的缓解方法，性别偏见分数显著降低。例如，在BERT-base模型中，“he-she”的偏见分数从1.27降至0.08，“his-her”从2.51降至0.36。在BERT-large模型中，“male-female”的偏见从1.82降至0.10。此外，该方法在不损害模型在下游任务性能的情况下，有效减少了性别偏见。

Conclusion: 该研究提出的缓解方法能够有效降低Transformer模型中的性别偏见，且不会牺牲模型在下游任务上的性能。这为构建更公平的语言模型提供了实用的途径。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [260] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 本研究在合成的医患对话数据集上微调了轻量级离线对话模型DialoGPT，以支持尼泊尔农村地区的医疗服务，并取得了良好的医疗相关和共情响应。


<details>
  <summary>Details</summary>
Motivation: 会话代理在医疗保健领域，特别是在尼泊尔农村等资源受限地区具有潜力。然而，大型会话模型通常依赖互联网和云基础设施，这在农村地区可能无法访问。

Method: 研究者在一个人工构建的、包含尼泊尔农村地区十种常见疾病（如普通感冒、腹泻、伤寒等）的医患互动数据集上，微调了轻量级生成式对话模型DialoGPT，使其能够离线运行。

Result: 尽管在有限的、特定领域数据集上训练，微调后的模型仍能产生连贯、上下文相关且医学上适当的响应，表现出对症状、疾病背景的理解和共情交流能力。

Conclusion: 研究结果突出了紧凑型、离线对话模型的适应性，以及目标数据集在低资源医疗环境中进行领域适应的有效性，为未来农村医疗会话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [261] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为WordSaladChopper (WSC)的轻量级组件，用于实时检测并消除大型推理模型（LRM）输出中无意义的自我重复（即“词语沙拉”），从而显著降低输出成本并提高用户体验，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）的输出令牌成本高昂是一个瓶颈。研究发现，这些令牌中有很大一部分是无用的自我重复，耗尽了解码预算但未增加价值。

Method: 研究观察到LRM在陷入重复循环时具有自我意识，即每个推理块末尾的`\n\n`令牌的隐藏状态呈现出特定模式。通过一个单层线性分类器，可以实时检测到这种“词语沙拉”行为。一旦检测到，就通过简单的“截断”（chop）并结合直接的再生提示来处理。

Result: WordSaladChopper (WSC) 组件能够实现显著的长度节省，同时对输出质量的损失极小。它仅移除语义冗余的令牌，对LRM的推理轨迹干扰最小。

Conclusion: WSC是一个低开销、高效能的组件，对于以用户体验为中心的所有LRM应用来说，它或类似的组件是必不可少的，因为它消除了无语义价值的“词语沙拉”令牌。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [262] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 现有深度学习和预训练语言模型（PLMs）在情感分析中面临细微情感线索、领域迁移和不平衡数据等挑战。本文提出CISEA-MRFE框架，通过上下文指令、语义增强扩充和多重精炼特征提取，显著提升了情感分类的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和预训练语言模型的情感分析方法在处理细微情感线索、领域迁移和不平衡情感分布时表现不佳。这些限制源于语义理解不足、对多样语言模式泛化能力差以及对主导情感类别的偏见。

Method: 本文提出CISEA-MRFE框架，一个结合了上下文指令（CI）、语义增强扩充（SEA）和多重精炼特征提取（MRFE）的新型PLM-based框架。CI注入领域感知指令以指导情感消歧；SEA通过情感一致的释义扩充提高鲁棒性；MRFE结合了用于多尺度特征特化的尺度自适应深度编码器（SADE）和用于情感感知序列建模的情感评估器上下文编码器（EECE）。

Result: 在四个基准数据集上的实验结果表明，CISEA-MRFE持续优于现有强基线模型，在IMDb上准确率相对提升高达4.6%，Yelp上6.5%，Twitter上30.3%，Amazon上4.1%。

Conclusion: 这些结果验证了CISEA-MRFE方法在跨不同领域情感分类中的有效性和泛化能力。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [263] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ISA（意图转移攻击）的新型越狱攻击方法，通过改变有害请求的意图，使其被大语言模型（LLMs）误认为良性请求，从而显著提高了攻击成功率，并揭示了LLMs在意图推断方面的根本性安全挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力强大，但仍易受越狱攻击。现有攻击主要通过增加上下文或对抗性token来分散LLMs注意力，而未改变核心有害意图。研究这些弱点对于构建鲁棒的安全机制至关重要。

Method: 本文引入了ISA（意图转移攻击），通过混淆LLMs对攻击意图的感知来实施攻击。具体方法是建立了一个意图转换的分类体系，并利用其生成攻击，使LLMs可能将其误认为是良性的信息请求。与现有方法不同，ISA只需对原始请求进行最少的编辑，即可生成自然、可读且看似无害的提示。

Result: 在开源和商业LLMs上的广泛实验表明，ISA相比直接有害提示，攻击成功率提高了70%以上。更重要的是，仅使用ISA模板重新构建的良性数据对模型进行微调，可将成功率提升至接近100%。对现有防御方法的评估显示其对ISA无效，并探讨了无训练和基于训练的缓解策略。

Conclusion: 研究结果揭示了LLMs安全中意图推断的根本性挑战，并强调了开发更有效防御措施的必要性。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [264] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA通过微调Transformer模型以适应高效注意力机制，显著提高了推理吞吐量并降低了内存使用，但在检索任务上存在局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现卓越，但其推理时的内存需求（尤其是维护完整上下文）带来了巨大挑战。

Method: 本文提出了FlashEVA，一个高效的EVA（通过控制变量实现高效注意力）实现，并展示了如何微调Transformer模型以适应FlashEVA注意力。微调过程仅使用了1.5B token。

Result: FlashEVA在推理期间实现了高达6.7倍的吞吐量提升和5倍的峰值GPU内存使用量降低，同时在各种下游任务中保持了有效性。然而，在以检索为中心的任务中观察到局限性。该实现通过可调超参数提供了吞吐量和准确性之间的权衡控制。

Conclusion: 这项工作是向更高效、更具适应性的基于Transformer的推理模型迈出的重要一步。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [265] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自对弈框架，通过让LLM交替扮演教师和学生角色，生成并解决新颖问题，从而实现开放式数学发现和推理能力的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前通过强化学习提升大型语言模型（LLM）推理能力的方法依赖于带注释的数据集来获取可验证的奖励，这可能限制模型超越人类水平的表现。虽然自对弈很有前景，但现有方法仍依赖外部验证器或无法实现开放式学习。

Method: OpenSIR框架让LLM扮演教师和学生角色交替进行学习，无需外部监督。教师角色负责生成新颖问题，并同时优化难度和多样性，奖励那些具有适当挑战性并探索不同概念的问题，从而实现开放式数学发现。

Result: 从一个简单的初始问题开始，OpenSIR显著提升了指令模型的性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提高到78.3，在College Math上从28.8提高到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提高到58.7。分析表明，OpenSIR通过共同进化的教师-学生角色实现了开放式学习，这些角色自适应地校准难度并推动多样化探索，自主地从基础数学进步到高级数学。

Conclusion: OpenSIR通过无需外部监督的自对弈机制，使LLM能够自主地生成和解决问题，实现了开放式、自适应的数学发现和推理能力提升，能够从简单概念发展到复杂概念。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [266] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2提出了一种新颖的框架，通过使用非自回归的离散扩散模型作为草稿器并校准草稿器与验证器，解决了现有推测解码中自回归草稿和频繁拒绝的瓶颈，显著提升了LLM推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法存在两个主要瓶颈：1) 草稿阶段的自回归依赖限制了并行性；2) 草稿模型与验证模型之间的不匹配导致频繁的草稿token拒绝。

Method: SpecDiff-2采用离散扩散模型作为非自回归草稿器来解决并行性限制，并开发了新颖的技术来校准离散扩散草稿器与自回归验证器，以减少草稿token的拒绝。

Result: SpecDiff-2在推理、编码和数学等综合基准测试中达到了新的最先进水平，与先前基线相比，每秒token数平均提高了55%，与标准解码相比，平均加速高达5.5倍，且未损失任何准确性。

Conclusion: SpecDiff-2成功地联合解决了推测解码中的两个核心瓶颈，实现了LLM推理速度的显著提升，同时保持了高精度。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [267] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在概率场景中能给出准确的答案，但其内部的token概率和熵值与理论概率分布存在显著偏差，这表明基于token置信度的不确定性量化可能不可靠。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型在决策支持和知识密集型应用中值得信赖，需要可靠的不确定性量化（UQ）。虽然可以从token logits估算模型置信度，但在概率场景中，这种方法可能不足以使输出概率与理论概率对齐。

Method: 研究调查了GPT-4.1和DeepSeek-Chat在十个概率提示（例如，掷骰子）下的表现，这些提示分别包含和不包含明确的概率提示词。评估了两个维度：(1) 响应对场景约束的有效性；(2) token级输出概率与理论概率之间的对齐程度。

Result: 尽管两个模型在所有提示场景中都达到了完美的领域内响应准确性，但它们的token级概率和熵值始终与相应的理论分布存在偏差。

Conclusion: 尽管大型语言模型在概率场景中表现出高准确性，但其内部token置信度（概率和熵）与理论概率分布不一致。这表明，仅仅依靠token logits进行不确定性量化在需要概率对齐的场景中可能并不可靠。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [268] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 本研究通过计算分析，揭示了法国侦探小说中侦探原型150年间的演变，从次要角色到核心“推理机器”，并在二战后变得更加复杂。


<details>
  <summary>Details</summary>
Motivation: 探究法国侦探小说中侦探原型的历史演变，并理解其如何随着时间和社会文化背景的变化而发展。

Method: 采用计算分析、定量方法、字符级嵌入（character-level embeddings）以及监督模型。

Result: 一个监督模型能够捕捉到150年间法国侦探原型的一致性；侦探形象从次要叙事角色演变为经典侦探故事的核心人物和“推理机器”；二战后，受硬汉派传统影响，侦探原型变得更加复杂，反映了社会暴力和道德模糊。

Conclusion: 法国侦探小说中的侦探原型在漫长历史中保持了核心统一性，并经历了从功能性角色到复杂人设的演变，以适应文学类型和社会主题的变化。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [269] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 该研究引入了XNationQA基准来评估多语言大型语言模型(LLMs)的文化素养，发现模型在不同语言中获取文化特定事实的能力存在显著差异，且常表现出英语中心偏见和有限的跨语言知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 大多数多语言问答基准虽然涵盖多种语言，但在其信息中未考虑区域多样性，且倾向于以西方为中心。这导致在公平评估多语言模型对来自不同地理位置的事实信息的理解方面存在显著差距。

Method: 研究引入了XNationQA，一个包含49,280个问题的基准，涵盖九个国家的地理、文化和历史，并以七种语言呈现。研究使用两个新颖的迁移度量标准，对八个标准多语言LLMs进行了基准测试和评估。

Result: 分析揭示了模型在不同语言中获取文化特定事实的能力存在显著差异。值得注意的是，模型在英语中对文化信息的了解通常超过该文化各自的主导语言。模型在西方语言中表现更好，但这并不一定意味着对西方国家有更高的文化素养，这与直觉相悖。此外，模型跨语言迁移知识的能力非常有限，在开源模型中尤为明显。

Conclusion: 多语言LLMs在文化素养方面存在显著的语言和文化偏见，尤其是在跨语言知识迁移方面表现不佳。这表明当前模型在理解和处理非西方及多语言文化信息方面存在局限性。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [270] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次系统性地评估了大型语言模型（LLMs）在十种语言中越狱攻击和防御的跨语言泛化能力，发现攻击成功率和防御鲁棒性因语言而异，并呼吁建立语言感知和跨语言的安全基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs经过安全对齐，但越狱攻击仍能绕过安全机制。现有研究对越狱和防御的跨语言泛化能力探索不足。

Method: 研究采用系统性的多语言评估方法，在十种语言（包括高、中、低资源语言）上，使用六个LLMs，通过HarmBench和AdvBench评估了两种越狱攻击类型：基于逻辑表达式和基于对抗性提示的攻击。

Result: 攻击成功率和防御鲁棒性因语言而异。高资源语言在标准查询下更安全，但更容易受到对抗性查询的攻击。简单的防御措施可能有效，但其效果取决于语言和模型。

Conclusion: 研究结果表明需要为LLMs开发语言感知和跨语言的安全基准。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [271] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文通过交替使用局部和全局注意力模式，并引入潜在注意力（Latent Attention）变体（MLA和GLA），系统性地改进了原生稀疏注意力（NSA），显著增强了长上下文建模能力，同时降低了KV缓存内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力在长上下文建模中可能存在局限性，例如固定注意力模式可能无法有效传播长距离依赖，以及KV缓存内存消耗问题，因此需要提出有针对性的改进方案。

Method: 1. 对原生稀疏注意力（NSA）进行系统性分析。2. 提出在不同层之间交替使用局部（滑动窗口）和全局（压缩、选择性）注意力模式。3. 进一步优化NSA分支：滑动窗口分支采用多头潜在注意力（Multi-head Latent Attention, MLA），而压缩和选择性分支采用组头潜在注意力（Group-head Latent Attention, GLA）。

Result: 1. 将KV缓存内存消耗比NSA减少50%。2. 显著提升了模型的常识推理和长文本理解能力。3. 在常识推理和长上下文理解任务上，与全注意力及原生稀疏注意力相比，性能持平或超越，并在340M至1.3B参数规模的模型上得到验证。

Conclusion: 通过在层间交替使用局部和全局注意力模式，并结合潜在注意力（MLA和GLA）对NSA分支进行精炼，能够有效增强长上下文建模能力，提升模型性能，并显著优化内存效率。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [272] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair是一种新的对比学习框架，通过解耦损失（结合三元组和语言建模项）来消除大语言模型中的正负耦合，从而有效减少社会偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中社会偏见的传播导致有害和不公平的结果。现有去偏方法独立处理有偏和无偏样本，忽略了它们之间的相互关系，导致隐藏的正负耦合，即改善一组可能损害另一组，使偏见持续存在。

Method: 引入TriCon-Fair，一个对比学习框架。它使用解耦损失，结合了三元组损失和语言建模（LM）项。为每个锚点分配一个明确的有偏负样本和一个无偏正样本，解耦了推拉动态，避免了正负耦合，并联合优化LM目标以保持通用能力。

Result: 实验结果表明，TriCon-Fair在减少歧视性输出方面优于现有去偏基线，同时保持了强大的下游性能。

Conclusion: TriCon-Fair为敏感的自然语言处理应用提供了一个实用且符合伦理的解决方案，能够有效减少大语言模型中的社会偏见。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [273] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文提出了一套新颖的评估工具，系统地评估大型语言模型（LLMs）在逐步推理过程中对知识的扎根程度，以揭示其潜在的推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs的逐步推理范式已被证明有效，但一个基本问题是如何验证LLM的推理是否准确地基于知识。现有方法可能难以有效识别LLM推理中知识缺失或误用的情况。

Method: 本文提出了一个包含三个关键组件的评估框架：(1) 主要知识集合（Principal Knowledge Collection），一个大规模的原子知识库；(2) 基于知识的评估指标，用于衡量模型在推理中召回和应用先验知识的能力；(3) 评估器LLM（evaluator LLM），一个轻量级模型，用于经济高效且可靠地计算这些指标。

Result: 该评估工具在识别LLMs推理中缺失或误用的知识元素方面表现出卓越的有效性，为揭示LLMs根本性推理缺陷提供了关键见解。此外，这些评估指标还可以集成到偏好优化中，展示了知识扎根评估的进一步应用。

Conclusion: 所提出的评估套件能够有效识别LLMs推理中的知识缺陷，为深入理解LLMs的推理能力提供了重要工具，并具有超越单纯评估的应用潜力，例如改进模型优化。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [274] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过创新的OCR预训练、自监督掩码对比学习和后期交互评分机制，解决了现有方法模仿文本检索的局限性，并在ViDoRe V2基准上取得了显著提升和更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当模型需要专业知识或最新数据时，检索增强生成（RAG）非常实用。然而，现有的多模态文档检索方法常常复制为纯文本检索开发的技术，无论是在文档编码、训练目标还是相似度计算方面，这限制了它们处理多模态文档结构和视觉特征的能力。

Method: ColMate采用以下方法：1. 一种新颖的基于OCR的预训练目标。2. 一种自监督的掩码对比学习目标。3. 一种更适合多模态文档结构和视觉特征的后期交互评分机制。这些方法旨在弥合多模态表示学习与文档检索之间的鸿沟。

Result: ColMate在ViDoRe V2基准测试上比现有检索模型提高了3.61%，并展示了对域外基准更强的泛化能力。

Conclusion: ColMate通过引入与多模态文档结构和视觉特征更相关的编码、训练和评分机制，成功地弥合了多模态表示学习和文档检索之间的差距，取得了显著的性能提升和更好的泛化能力。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [275] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型(LLMs)在医疗诊断沟通中生成可理解和富有同情心的解释的能力。结果显示LLMs能适应患者情况但内容过于复杂且存在情感共情偏差，导致可及性不均，亟需系统校准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有望通过生成解释和指导来支持临床医生进行诊断沟通，但其输出内容的可理解性和同情心水平仍不确定。

Method: 研究评估了两个领先的LLM在医疗诊断场景中的表现。通过可读性指标（作为替代）评估可理解性，并通过“LLM作为评判者”的评分与人类评估进行比较来评估同情心。

Result: 结果表明，LLM能根据社会人口统计变量和患者状况调整解释。然而，它们也生成了过于复杂的内容，并表现出有偏见的情感共情，导致了不均衡的可及性和支持。

Conclusion: 这些模式强调了需要系统校准LLM，以确保公平的患者沟通。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [276] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个新颖的多智能体框架，通过结合任务分解和交互式强化学习，解决了复杂自然语言到SQL转换的难题，实现了自修正和高精度。


<details>
  <summary>Details</summary>
Motivation: 将自然语言翻译成复杂SQL查询仍然很困难，这类查询通常需要环境交互和自修正能力。

Method: 引入了MARS-SQL，一个包含三个专门智能体的多智能体框架：用于模式链接的接地智能体、用于查询生成的生成智能体和用于最终选择的验证智能体。生成智能体通过多轮强化学习策略进行训练，采用ReAct风格的“思考-行动-观察”循环，迭代生成想法、执行SQL并根据反馈修正策略。在推理时，生成多个交互轨迹以探索不同的推理路径。验证智能体通过将验证建模为下一个token预测任务，选择生成概率最高的最佳解决方案。

Result: MARS-SQL在BIRD开发集上达到了77.84%的执行准确率，在Spider测试集上达到了89.75%的执行准确率，均达到最先进水平。

Conclusion: 该方法结合了用于生成的交互式强化学习和用于验证的生成建模，被证明对于鲁棒和准确的SQL生成非常有效。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [277] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种从易到难的增强框架，以解决机器生成文本（MGT）检测中标签边界模糊的问题，并在不精确监督下提供可靠的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MGT检测方法隐式地假设标签是“黄金标准”，但研究发现存在边界模糊，导致传统训练范式不精确。此外，人类认知的局限性和检测器的超智能使得不精确学习普遍且不可避免。

Method: 提出了一种从易到难的增强框架。该框架使用一个针对相对简单的长文本检测任务的“容易监督器”（能力较弱），来增强更具挑战性的目标检测器。通过理论分析，长文本有助于缓解不精确标签的影响，并且将目标检测器结构性地整合到监督器中，使得监督器成为检测器性能的下界，从而间接优化检测器以逼近“黄金”标签。

Result: 在跨大型语言模型、跨领域、混合文本和复述攻击等多种实际场景下进行了广泛实验，结果表明该框架显著提升了检测效果。

Conclusion: 该框架在不精确条件下提供了可靠的监督，通过从易到难的增强策略，显著提高了MGT的检测效率和准确性，解决了标签模糊带来的挑战。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [278] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了IF-CRITIC，一个高效且可靠的LLM评论器，用于评估大型语言模型（LLMs）的指令遵循能力，解决了现有评估模型成本高昂和评估不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 指令遵循是LLMs的一项基本能力，但现有用于评估指令遵循的评估模型存在诸多缺陷，如成本高昂和评估结果不可靠。

Method: 本文提出了IF-CRITIC，它首先开发了一个清单生成器来分解指令并生成约束清单。然后，借助这些清单，通过多阶段评论过滤机制收集高质量的评论训练数据，并采用约束级偏好优化方法来训练IF-CRITIC。

Result: 广泛的实验表明，IF-CRITIC的评估性能优于强大的LLM-as-a-Judge基线（包括Deepseek-R1和o4-mini）。此外，IF-CRITIC提供的可扩展奖励信号使得LLMs在指令遵循优化方面能以较低的计算开销获得显著的性能提升。

Conclusion: IF-CRITIC能够为LLMs的指令遵循提供高效、可靠的评估和可扩展的奖励信号，从而有效优化LLMs的指令遵循能力，且计算开销更低。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [279] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，它使用小型LLM自动生成提示来与大型LLM协作，以解决用户提示能力不足的问题，显著提升了大型LLM在复杂任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前，用户在面对复杂问题时，往往难以提供准确有效的提示来与大型语言模型（LLMs）交互，这限制了LLMs的性能发挥。

Method: 本文提出了Prompt-R1框架，一个端到端的强化学习框架。它通过小型LLM与大型LLM协作，替代用户进行交互。这种协作被建模为多轮提示交互：小型LLM负责思考和生成提示，大型LLM执行复杂推理。设计了一种双约束奖励机制，以优化正确性、生成质量和推理准确性。Prompt-R1是一个即插即用的框架，支持多种大型LLM的推理和训练。

Result: 在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1提供了一个有效的、即插即用的框架，通过自动化和优化的提示生成，能够有效提升大型LLM在复杂问题解决中的性能。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [280] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在七种印度语言中的文化推理和自我评估能力。结果显示，虽然性能最佳的模型在解谜方面表现出色，但其识别自身错误的能力较差，而表现较差的模型反而更具自我意识。


<details>
  <summary>Details</summary>
Motivation: 目前对于大型语言模型在非英语语言中进行文化相关推理的能力研究不足。

Method: 研究引入了一个多语言谜语数据集，包含传统谜语和上下文重构变体，并在七种印度主要语言中评估了五种大型语言模型（Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, LLaMA 4 Maverick）。实验分两个阶段：首先评估解谜性能，然后进行自我评估实验以衡量推理一致性，并采用七种提示策略。

Result: Gemini 2.5 Pro在整体解谜表现最佳，但少样本方法提升有限，且准确率在不同语言间差异显著。一个关键发现是：模型的初始准确率与其识别自身错误的能力呈负相关。表现最好的模型（如Gemini 2.5 Pro）过度自信（真阴性率4.34%），而表现较差的模型（如LLaMA 4 Scout）则更具自我意识（真阴性率42.09%）。

Conclusion: 这些结果揭示了多语言推理中存在的明显差距，并强调了开发不仅能有效推理，而且能认识到自身局限性的模型的重要性。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [281] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个对话式AI平台，它将开源大型语言模型（LLM）的自然语言流畅性与美国国家海洋和大气管理局（NOAA）的权威海洋数据流实时集成，旨在解决通用AI在科学领域中“幻觉”问题，提供可验证、可追溯的科学信息。


<details>
  <summary>Details</summary>
Motivation: 通用对话式AI系统常常产生未经证实的“幻觉”，这损害了科学严谨性，尤其是在科学研究领域。

Method: OceanAI结合了开源LLM的自然语言处理能力，并与NOAA托管的权威海洋数据流进行实时、参数化访问。每次查询都会触发实时API调用，识别、解析和合成相关数据集，生成可重现的自然语言响应和数据可视化。

Result: 在与三种常用AI聊天界面产品的盲测比较中，只有OceanAI能够生成来自NOAA的数据值并提供原始数据引用；其他产品要么拒绝回答，要么提供了未经支持的结果。OceanAI支持连接多个NOAA数据产品和变量。

Conclusion: OceanAI通过将输出建立在可验证的观测数据之上，提高了透明度、可重复性和信任度，为海洋领域的AI辅助决策支持提供了一个可扩展的框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [282] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个会话式系统，利用大型语言模型整合印度空气质量数据，通过自然语言查询提供可执行代码和交互式可视化，使决策者、研究人员和公众更容易进行环境数据分析。


<details>
  <summary>Details</summary>
Motivation: 印度每年约有160万人因空气污染过早死亡，但决策者难以将分散的数据转化为决策。现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。

Method: VayuChat是一个会话式系统，能够回答关于空气质量、气象和政策计划的自然语言问题，并以可执行的Python代码和交互式可视化作为回应。它整合了中央污染控制委员会(CPCB)监测站、邦级人口统计数据和国家清洁空气计划(NCAP)资金记录的数据，并通过大型语言模型提供统一接口。

Result: 用户可以通过简单的对话执行复杂的环境分析，使数据科学对政策制定者、研究人员和公民都变得可及。该平台已公开部署。

Conclusion: VayuChat通过提供一个易于使用的会话式界面，显著降低了空气质量数据分析的门槛，使决策者和公众能够更有效地利用数据来应对空气污染问题。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [283] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 本研究引入了一个新的、经验证的数据集和评估框架，用于衡量大型语言模型（LLMs）在医疗保健领域基于临床指南的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医疗保健领域应用日益广泛，但目前缺乏用于评估其基于临床指南推理能力的标准化基准。

Method: 研究团队利用GPT从公开的临床指南中创建了一个经验证的数据集，其中包含真实的患者情景和临床问题。随后，使用该数据集对一系列流行的LLMs进行了基准测试。

Result: 通过对多种LLMs进行基准测试，研究展示了所创建数据集的有效性。该框架能够支持对LLMs临床实用性和指南依从性进行系统性评估。

Conclusion: 本研究提供的框架和数据集能够有效评估大型语言模型在医疗保健领域遵循临床指南进行推理的能力和实用性。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [284] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该论文介绍了一个包含30万亿token的超大规模、高质量、多语言（近200种）LLM预训练数据集及其完整的开源处理流程，并提供了数据质量评估、多语言基准测试和训练模型。


<details>
  <summary>Details</summary>
Motivation: 动机是为LLM预训练提供开放、超大规模、高质量且标注丰富多样的文本数据集，以支持近200种语言，并解决现有预训练数据可能存在的规模、质量和语言多样性不足的问题。

Method: 方法包括：从不同来源的网络爬虫中获取数据；开发一套完整的开源处理流程，用于文档选择、文本提取、语言识别、去重、注册标签/文本质量/PII标注以及最终筛选；通过对比分析统计、24种语言的手动抽样检查和端到端模型训练评估来探测数据质量；提供针对9种欧洲语言的综合多语言LLM基准测试；训练并评估了57个单语编解码器模型和少量单语GPT-like参考模型；自动挖掘并合成平行语料库。

Result: 结果是：构建了一个包含30万亿token的、可能是最大的多语言LLM预训练数据集；发布了一个完整的开源数据处理管道；通过多种方法验证了数据质量；提供了针对9种欧洲语言的综合多语言LLM评估基准；训练并评估了57个单语编解码器模型和一些单语GPT-like模型；构建了一个大型平行文本集合（包括自动挖掘和机器翻译合成）。

Conclusion: 该研究提供了一个前所未有的开放、超大规模、高质量、多语言LLM预训练数据集和配套的开源工具链，以及全面的评估基准和参考模型，极大地推动了多语言LLM的开发和研究。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [285] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本研究通过使用轻量级多任务模型和LLM标注的文本，对罗马尼亚语预训练语料库进行了分析和多层过滤，成功生成了高质量数据集，并显著提升了LLM的预训练性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的训练高度依赖高质量数据，而对于罗马尼亚语等代表性不足的语言，高质量语料库稀缺，这促使研究者探索如何改善其数据质量。

Method: 研究了罗马尼亚语预训练语料库的特征和覆盖范围，并与英语数据进行比较。训练了一个轻量级多任务模型，利用LLM标注的罗马尼亚语文本进行多层过滤（例如，教育价值、主题、格式），以生成高质量预训练数据集。

Result: 发现了罗马尼亚语和英语数据在主题上的显著趋势。通过数据过滤，LLM在多个基准测试上的预训练性能得到了显著提升，证明了该方法的有效性。

Conclusion: 本研究成功展示了一种通过LLM标注和多层过滤来生成高质量预训练数据集的方法，尤其适用于资源稀缺的语言，并能有效提高LLM的预训练表现。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [286] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出了一个名为TSVer的新基准数据集，用于基于时间序列证据进行事实核查，并展示了现有最先进模型在该任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有用于处理时间与数值数据的证据核查系统，其评估受限于现有数据集，这些数据集通常缺乏结构化证据、判决理由不足或依赖合成声明。

Method: 引入了TSVer数据集，包含287个真实世界声明和400个时间序列。通过LLM辅助的多步标注过程，为每个声明标注了时间范围、判决结果和理由。同时开发了一个基线模型来验证声明。

Result: TSVer数据集的标注质量高，判决结果的标注者间一致性Kappa值为0.745。即使是Gemini-2.5-Pro等最先进的推理模型，在时间序列数据上的表现也面临挑战，判决准确率为63.37%，判决理由的Ev2R分数为48.63。

Conclusion: TSVer是一个具有挑战性的新基准数据集，突显了当前模型在处理时间序列的临时和数值推理方面的局限性，并为未来的研究提供了方向。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [287] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 本文提出了MicroRemed，一个用于评估LLM在微服务修复中端到端能力的基准，并引入了ThinkRemed，一个模拟SRE反思和感知推理的多智能体框架，以提高自动化修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在微服务修复中仍依赖SRE人工编写的提示，LLM仅将文本指令转换为可执行代码，未能实现从诊断报告直接生成可执行修复方案的端到端自动化，这一方向有巨大潜力但尚未被充分探索。

Method: 1. 引入MicroRemed基准：首次端到端评估LLM在微服务修复中的能力，要求模型直接从诊断报告生成可执行的Ansible playbook以恢复系统功能。 2. 提出ThinkRemed框架：一个多智能体框架，模拟SRE的反思和感知推理过程。

Result: 实验结果表明，MicroRemed基准对当前LLM提出了重大挑战。而ThinkRemed框架通过迭代推理和系统反思，显著提升了端到端的修复性能。

Conclusion: 该研究通过引入MicroRemed基准和ThinkRemed多智能体框架，推动了LLM在微服务自动化修复领域的进展，展示了现有LLM面临的挑战以及通过模拟SRE推理过程提高性能的潜力。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [288] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 本研究通过将销售人员的动态筛选决策建模为最优停止问题，开发了一个基于生成式语言模型的停止代理。该代理通过模仿最优停止策略，显著减少了无效通话时间，提高了销售效率，并揭示了人类销售人员决策中的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 销售人员在动态筛选决策（继续对话或放弃寻找下一个潜在客户）方面面临挑战，但关于这些决策如何制定、是否高效以及如何改进的知识却很少。在高频外呼销售场景中，潜在客户充足但时间稀缺且失败常见，因此优化这些决策至关重要。

Method: 研究将动态筛选决策形式化为最优停止问题，并开发了一个基于生成式语言模型的序列决策代理（停止代理）。该代理通过模仿追溯推断的最优停止策略，学习何时以及如何放弃对话。该方法能够处理高维文本状态，可扩展到大型语言模型，并支持开源和专有语言模型。研究将此方法应用于一家大型欧洲电信公司的真实通话数据。

Result: 停止代理将无效通话所花费的时间减少了54%，同时几乎保留了所有销售额。重新分配节省的时间可使预期销售额增加高达37%。对销售人员放弃决策的语言线索分析发现，他们倾向于过分看重消费者不感兴趣的几个突出表达，并错误预测通话失败风险，这表明他们在实时对话决策中存在认知限制。

Conclusion: 研究结果强调了人工智能算法在纠正人类认知受限决策和提高销售团队效率方面的巨大潜力。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [289] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 该研究引入了多语言辩论式基准DebateBias-8K，以评估大型语言模型（LLMs）在开放式生成场景中的叙事偏见。结果显示，尽管经过安全对齐，所有模型仍存在根深蒂固的刻板印象，尤其在低资源语言中偏见更甚，表明当前的对齐方法未能有效解决多语言公平性问题。


<details>
  <summary>Details</summary>
Motivation: 大多数偏见评估仍依赖于英语和分类任务，无法捕捉LLMs在现实生成场景中出现的叙事偏见。研究旨在开发一种多语言、开放式的评估方法，以揭示LLMs在不同文化和语言背景下的偏见。

Method: 研究构建了DebateBias-8K数据集，包含8,400个结构化辩论提示，涵盖女性权利、社会经济发展、恐怖主义和宗教四个敏感领域，并跨越英、中等高资源语言及斯瓦希里语、尼日利亚皮钦语等低资源语言共七种语言。使用GPT-4o、Claude 3、DeepSeek和LLaMA 3四款主流模型生成了超过100,000个回复，并进行了自动化分类分析。

Result: 所有模型都重现了根深蒂固的刻板印象，例如阿拉伯人被压倒性地与恐怖主义和宗教联系（≥95%），非洲人与社会经济“落后”联系（高达≤77%），而西方群体则被持续描绘为现代或进步。偏见在低资源语言中急剧增加，表明主要在英语中训练的对齐方法未能全球化推广。

Conclusion: 研究结果揭示了多语言公平性中持续存在的分歧：当前的对齐方法虽然减少了显性毒性，但未能阻止在开放式语境中产生偏见输出。研究发布了DebateBias-8K基准和分析框架，以支持下一代多语言偏见评估和更安全、文化包容的模型对齐。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [290] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 本文提出了ZoFia，一个两阶段的零样本假新闻检测框架，它结合了大型语言模型（LLMs）、外部证据检索和多LLM协作分析，以应对LLMs在处理快速演变新闻时知识时效性和泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 假新闻的迅速传播威胁着社会稳定和公众信任。现有的大型语言模型（LLMs）在处理快速变化的新闻流时，存在知识覆盖有时限、容易产生幻觉内容以及在静态数据集上训练的模型对新兴新闻主题泛化能力不足等问题，因此需要更可靠的检测方法。

Method: ZoFia框架分为两个阶段：
1.  **证据检索阶段**：引入“分层显著性”（Hierarchical Salience）来量化新闻内容中实体的重要性，并提出SC-MMR算法选择信息丰富且多样化的关键词，作为查询以检索最新的外部证据。
2.  **多LLM交互分析阶段**：构建一个多LLM交互系统，其中每个代理扮演不同角色，对新闻文本及其相关信息进行多视角协作分析和对抗性辩论，最终生成可解释且鲁棒的判断。

Result: 在两个公共数据集上进行的全面实验表明，ZoFia框架明显优于现有的零样本基线方法和大多数少样本方法。

Conclusion: ZoFia提供了一种新颖有效的零样本假新闻检测方案，通过结合外部证据和多LLM协作分析，解决了LLMs在处理时效性强、变化快的新闻内容时面临的挑战，并能产生可解释且鲁棒的判断。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [291] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为DEER的新型框架，通过解耦的专家混合模型和基于强化学习的路由机制，有效解决了机器生成文本检测中领域漂移导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展使得生成逼真的人类文本成为可能，机器生成文本（MGT）检测成为一个关键挑战。然而，现有方法在面对领域漂移时性能显著下降，这促使研究人员寻求更鲁棒的解决方案。

Method: 该研究提出了一个两阶段的“解耦专家混合模型”（DEER）架构。首先，引入一个解耦专家混合模块，其中领域特定专家学习细粒度的领域局部特征，而共享专家提取可迁移的跨领域特征。其次，为了解决推理时领域标签不可用的问题，设计了一个基于强化学习的路由机制，动态选择适合每个输入实例的专家，从而弥补训练与推理之间的领域不确定性鸿沟。

Result: 在五个域内和五个域外基准数据集上的广泛实验表明，DEER始终优于现有最先进的方法。在域内和域外数据集上，F1分数分别平均提高了1.39%和5.32%，准确率分别提高了1.35%和3.61%。消融研究证实了解耦专家专业化和自适应路由对模型性能的关键贡献。

Conclusion: DEER框架通过有效捕获领域特定和领域通用MGT模式，并利用自适应路由机制应对推理时的领域不确定性，显著提高了机器生成文本检测在领域漂移下的鲁棒性和性能。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [292] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: 本文提出Self-Harmony框架，通过利用答案在原始问题及其复述版本之间保持稳定的直觉，为测试时强化学习（TTRL）构建可靠的学习信号。它使用单个模型作为求解器和复述器，并采用调和平均数进行伪标签聚合，实现了最先进的性能和卓越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习（TTRL）的成功依赖于构建可靠的学习信号，但标准方法（如多数投票）经常导致虚假且受欢迎的答案，无法提供可靠的自适应信号。

Method: Self-Harmony框架基于“正确答案在原始问题及其复述版本之间应保持稳定”的直觉。它使用单个模型扮演两种角色：求解器（Solver）生成答案，复述器（Reframer）重新表述输入。在此基础上，提出一种伪标签方法，通过使用调和平均数聚合原始和复述视图下的答案频率，而非多数投票，以选择在复述下稳定的解决方案，避免偏向依赖视图的虚假答案。该方法无需人工监督或辅助模型。

Result: Self-Harmony在各种推理基准测试的无标签测试时设置中取得了最先进（SOTA）的结果，在30个设置中有28个排名第一。除了高准确率，它还展示了前所未有的鲁棒性，所有实验中均无训练失败，突显了其稳定性和可靠性。

Conclusion: Self-Harmony通过利用答案在问题复述下的稳定性，为TTRL提供了一种无需标签且高度稳定、可靠的自适应方法，有效地解决了传统方法中学习信号不可靠和虚假答案的问题，并在多个推理任务中达到了SOTA性能。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [293] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本研究探讨了领域特异性对阿拉伯语金融文本抽象摘要的影响，引入了迄今为止最大的阿拉伯语金融新闻数据集AraFinNews，并发现领域适应模型在事实准确性和连贯性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解领域特异性（特别是金融领域）如何影响大型语言模型在阿拉伯语抽象摘要任务中的表现，并解决现有缺乏大型、公开可用的阿拉伯语金融新闻摘要数据集的问题。

Method: 研究方法包括：1. 构建并发布了AraFinNews数据集，包含212,500对阿拉伯语金融新闻文章-标题对。2. 使用该数据集评估了多种基于Transformer的模型，包括mT5、AraT5以及领域适应的FinAraT5。3. 评估指标侧重于事实准确性、数字可靠性和与专业报告的风格一致性。

Result: 实验结果表明，经过领域适应的模型（如FinAraT5）能够生成更忠实和连贯的摘要，尤其在处理定量信息和以实体为中心的信息时表现突出。

Conclusion: 本研究得出结论，领域特定的适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [294] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了针对AI辅助同行评审的提示注入攻击，提出了静态和迭代两种攻击方式，发现它们能有效诱导AI评审员给出高分，并探讨了一种可被部分规避的防御方法，强调了加强防范的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审等任务中的广泛应用，有报告指出一些论文中包含隐藏的注入提示，旨在操纵AI评审员给出过于有利的评价。本研究旨在对这种新兴威胁进行早期系统性调查。

Method: 研究提出了两类攻击方法：1) 静态攻击，使用固定的注入提示；2) 迭代攻击，通过针对模拟评审模型优化注入提示来最大化其有效性。此外，本文还探索了一种基于检测的简单防御机制。

Result: 两种攻击（静态和迭代）都取得了显著效果，能频繁地诱导前沿AI评审员给出满分评价。这些攻击在不同设置下表现出鲁棒性。虽然所提出的检测防御方法能大幅降低攻击成功率，但自适应攻击者可以部分规避该防御。

Conclusion: 研究结果强调了在AI辅助同行评审中，需要更多关注并采取严格的防护措施来应对提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [295] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文介绍了FirstAidQA，一个包含5,500个高质量急救和紧急响应问答对的合成数据集，旨在支持轻量级语言模型在资源受限的紧急环境中的部署。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）计算密集，不适用于急救人员或普通民众使用的低端设备，且在时间敏感、低或零连接环境中部署受限。此外，缺乏高质量的、针对急救和紧急响应领域的特定数据集，阻碍了轻量级、领域特定解决方案的开发。

Method: 研究人员使用大型语言模型ChatGPT-4o-mini，结合提示式上下文学习，从《Vital First Aid Book (2019)》的文本中生成了FirstAidQA数据集。生成过程包括文本清洗、上下文分块和过滤等预处理步骤，并进行了人工验证，以确保问答对的准确性、安全性和实用相关性。

Result: 本文成功创建并发布了FirstAidQA数据集，包含5,500个高质量的急救和紧急响应问答对。该数据集专为支持LLMs和小型语言模型（SLMs）的指令微调和微调而设计，旨在实现更快、更可靠且离线可用的紧急系统。

Conclusion: FirstAidQA数据集的发布有助于推动在急救和紧急响应领域中，安全关键和资源受限AI应用的研究。它将使开发适用于紧急情况的、更快、更可靠且离线运行的AI系统成为可能。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [296] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强型RAG系统，通过结构化和时间推理，解决5G规范中交叉引用和演变问题，优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 专家级5G规范问题需要查阅大量相互关联且不断演进的标准，现有RAG框架（包括电信专用方案）无法可靠地解决交叉引用或推理规范演变。

Method: DeepSpecs通过三个富含元数据的数据库（SpecDB、ChangeDB、TDocDB）增强RAG。它通过元数据查找递归解析交叉引用，并通过挖掘变更并将其与变更请求关联来追踪规范演变。

Result: DeepSpecs在多个LLM后端上优于基础模型和最先进的电信RAG系统。消融实验证实，显式交叉引用解析和演变感知检索显著提高了答案质量。

Conclusion: 建模5G标准的结构和时间属性对于提高专家级问题回答的质量至关重要。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [297] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一个检索增强型推测解码框架，通过自适应决策将启发式草稿模型切换转化为更高效的策略，显著加速大型语言模型推理，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 推测解码的加速效果受草稿模型效率限制。现有方法要么（如EAGLE-2）成本高昂，要么（如SAM-Decoding）依赖启发式切换策略，导致不必要的检索和低质量推测。

Method: ReSpec框架包含三项核心创新：1) 基于熵的自适应触发器，在不确定性低时才启动检索；2) 反馈驱动的候选选择，利用历史反馈组织多个高质量候选进行并行验证；3) 源感知松弛验证策略，对模型生成的草稿进行严格检查，对检索到的草稿进行松弛验证。

Result: ReSpec在Spec-Bench上实现了最先进的加速效果，性能分别超越EAGLE-2和SAM-Decoding超过33%和25%，同时保持了输出质量。

Conclusion: ReSpec通过自适应地管理检索过程，解决了现有推测解码方法的效率问题，显著提升了LLM推理速度，且不影响输出质量。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [298] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文通过引入四种新模型系列（慢思考、两种自适应思考和蒸馏奖励模型），扩展了DistilQwen模型家族，以满足工业应用对高效、高性能推理模型的需求。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用对小型、高效推理模型的需求日益增长，这些模型需要在推理性能和速度之间取得平衡，从而推动了知识蒸馏技术的发展。

Method: 研究者从Qwen模型初始化，通过知识蒸馏技术扩展了DistilQwen模型家族，引入了四种针对工业需求设计的模型系列：1) 慢思考模型，优化高精度推理任务；2) 两种自适应思考模型，根据输入任务动态调整推理策略；3) 蒸馏奖励模型，用于强化学习。并在阿里巴巴云PAI平台上提供了可扩展的训练和推理功能。

Result: 这些模型在多个基准测试中展现了高推理效率和强大的推理性能，同时蒸馏奖励模型也显示出实用价值。此外，它们还支持在阿里巴巴云PAI平台上进行可扩展的训练和推理。

Conclusion: 扩展后的DistilQwen模型家族（包括慢思考、自适应思考和蒸馏奖励模型）为工业应用提供了高效、高性能的推理解决方案，并支持通过蒸馏知识进行强化学习，且具备在云平台上进行可扩展部署的能力。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [299] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了一种名为MiniTruePrefixes的新型NLI模型，专门用于在自回归生成过程中检测文本前缀的事实不一致性。该模型在前缀级蕴含检测方面优于基线NLI模型，并能显著提高LLM输出（如抽象摘要）的事实一致性，同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言推理（NLI）模型虽然被用于提高大型语言模型（LLM）输出的事实性，但它们是为完整句子而非自回归生成中逐个演变的前缀设计的。这种不匹配促使研究人员寻求一种能在文本前缀上有效检测事实不一致性的方法。

Method: 研究人员将蕴含检测任务推广到任意文本前缀，并为此任务提供了合适的评估和训练数据集。他们训练了一个名为MiniTruePrefixes的新型专用模型，并将其集成到一个受控解码框架中，以验证其在提高生成忠实性方面的效用。

Result: MiniTruePrefixes在前缀级蕴含检测方面比可比较的基线NLI模型高出5-14个F1点。将其集成到受控解码框架中，显著改善了抽象摘要的事实一致性。在MiniTruePrefixes的指导下，LLaMA-3.2-3B-Instruct模型在忠实性和运行时性能上达到了同系列8B模型的水平，但只使用了后者一半的内存。

Conclusion: MiniTruePrefixes模型能够更有效地检测文本前缀中的事实不一致性，从而显著提高了LLM生成（特别是抽象摘要）的事实一致性。此外，它还能在保持性能的同时，大幅降低模型所需的内存，提升了效率。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [300] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）和大型推理模型（LRMs）基准测试的饱和问题，质疑当前基准能否真实衡量推理能力，并通过分析不同模型家族在多年间推理能力的演变，旨在为未来的推理评估和模型开发提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和LRMs能力的提升以及数据泄露，现有基准测试迅速饱和，导致需要不断更新更具挑战性的基准。研究者质疑超越基准是否真正反映了推理能力，还是仅仅追踪了与实际能力脱节的数字。

Method: 研究调查了OpenAI、Anthropic和Google三个模型家族，分析了它们多年来在不同基准测试中推理能力的发展。同时，还分析了不同推理任务在多年间的性能趋势，并讨论了当前基准测试的现状和挑战。

Result: 本文通过对模型家族推理能力演变和性能趋势的调查分析，揭示了基准测试饱和的现状以及对当前评估方法有效性的质疑。其结果主要体现在对基准测试和推理任务的全面概述和讨论。

Conclusion: 该研究旨在提供一个关于基准测试和推理任务的全面概述，作为未来推理评估和模型开发研究的初步参考，以更好地理解和衡量模型的真实推理能力。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [301] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 针对大语言模型在医疗咨询中的应用日益增多但多语言评估存在空白的问题，本文提出了“Cancer-Myth-Indic”，一个包含2500个项目的印度语言基准测试，用于评估LLM在处理癌症相关错误预设方面的有效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着人们日益依赖大语言模型获取医疗建议，评估其响应的有效性和准确性变得至关重要。然而，现有医学基准测试几乎全是英语，导致多语言（特别是印度语言）大语言模型评估方面存在显著空白。

Method: 本文通过将现有“Cancer-Myth”基准测试中的500个项目（均匀采样）翻译成五种印度次大陆语言（每种语言500项，共2500项），构建了“Cancer-Myth-Indic”印度语言基准测试。翻译由母语使用者遵循风格指南完成，以保留与癌症相关的隐含错误预设。随后，利用此基准测试评估了几个流行的大语言模型在这种预设压力下的表现。

Result: 本文成功构建了“Cancer-Myth-Indic”基准测试，共包含2500个翻译项目，用于评估大语言模型在印度语言中处理癌症相关错误预设的能力。该基准测试为在多语言环境下衡量流行大语言模型的医疗咨询准确性提供了工具。

Conclusion: 本研究通过引入“Cancer-Myth-Indic”基准测试，有效弥补了多语言大语言模型评估（特别是印度语言在医疗保健领域）的现有空白，有助于更准确地衡量大语言模型在处理癌症相关错误预设时的性能。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [302] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过遗传搜索对RAG管道进行端到端架构优化，显著优于基线RAG系统，并为构建高效RAG系统提供了领域感知指导。


<details>
  <summary>Details</summary>
Motivation: RAG系统的质量受检索、排序、增强、提示和生成等多个相互作用的选择影响，孤立地优化模块效果不佳且脆弱。

Method: 引入RAGSmith框架，将RAG设计视为涵盖9个技术家族和46,080种管道配置的端到端架构搜索问题。使用遗传搜索来优化一个标量目标，该目标联合聚合检索指标（recall@k, mAP, nDCG, MRR）和生成指标（LLM-Judge和语义相似度）。在六个维基百科衍生领域（数学、法律、金融、医学、国防工业、计算机科学）进行评估，每个领域包含100个问题，涵盖事实型、解释型和长答案型。

Result: RAGSmith找到的配置平均比朴素RAG基线高出+3.8%（跨领域范围+1.2%至+6.9%），检索方面提升高达+12.5%，生成方面提升高达+7.5%。搜索通常只探索了约0.2%的空间（约100个候选），并发现了一个强大的核心——向量检索加上生成后的反思/修订——辅以扩展、重排、增强和提示重排序中的领域依赖选择；段落压缩从未被选中。改进幅度与问题类型相关，在事实型/长答案型混合问题上比重解释型问题集有更大的提升。

Conclusion: 这些结果为组装有效的RAG系统提供了实用、领域感知的指导，并证明了进化搜索在全管道优化中的实用性。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [303] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨个体语言特征（特别是形态学）如何影响分词和语言建模，指出现有研究中存在混杂因素，并重新评估了相关假设，最终提出了一种无需专家标注的内在度量标准来预测语言建模的难度。


<details>
  <summary>Details</summary>
Motivation: 现有研究对个体语言特征（尤其是形态系统）在分词和语言建模中的作用存在争议，且结论相互矛盾。作者认为这源于实验设置中的混杂因素，导致结果难以比较和得出可靠结论。

Method: 本文首先识别并分析了在探讨形态学与语言建模关系时存在的混杂因素。其次，重新评估了Arnett & Bergen (2025)提出的关于黏着语比屈折语困惑度更高的三个假设（形态对齐、分词效率、数据集大小），并指出其结论中包含混杂因素。最后，引入了token bigram指标作为预测因果语言建模难度的内在方法。

Result: 研究发现，现有研究中关于形态学影响的矛盾证据是由于实验设置中的混杂因素造成的。Arnett & Bergen (2025) 的每个结论都包含混杂因素。此外，token bigram指标可以作为形态复杂度的梯度代理，且无需专家标注，能够预测因果语言建模的难度。

Conclusion: 本文提出了可靠回答形态学如何影响语言建模这一问题的必要条件，为未来研究指明了方向。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [304] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 该研究引入了DeepAmbigQAGen自动数据生成管道和DeepAmbigQA数据集，旨在评估大型语言模型在处理名称歧义和多步推理复杂问答方面的能力。实验表明，即使是先进的LLM也难以提供完整的答案，凸显了对更强大问答系统的需求。


<details>
  <summary>Details</summary>
Motivation: 尽管集成搜索工具的大型语言模型（LLM）在开放域问答中表现出巨大潜力，但它们在处理需要区分名称歧义和跨大量证据进行多步推理的复杂问题时仍面临挑战。现有问答基准很少能同时评估这两个难题。

Method: 研究者开发了DeepAmbigQAGen，一个自动数据生成管道，用于构建基于文本语料库和知识图谱的问答任务，系统地嵌入名称歧义和多步推理。基于此，构建了DeepAmbigQA数据集，包含3,600个需要多跳推理的问题，其中一半明确涉及名称歧义消除。然后，使用最先进的LLM（如GPT-5）对该数据集进行了实验。

Result: 实验结果显示，即使是最先进的GPT-5模型也提供了不完整的答案，在歧义问题上的精确匹配率仅为0.13，在非歧义问题上为0.21。

Conclusion: 这些发现强调了需要开发更强大的问答系统，以改进信息收集和答案的完整性，特别是在处理名称歧义和多步推理的复杂问答场景中。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [305] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: 本文提出了LiveSearchBench，一个自动化流水线，用于从最新的知识更新中构建依赖检索的基准测试，以评估大型语言模型（LLMs）在动态知识环境下的问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM问答评估基准通常是静态的，奖励记忆而非检索能力，且未能捕捉世界知识的动态变化，导致模型对新知识的理解和应用能力被低估。

Method: LiveSearchBench通过计算Wikidata连续快照之间的差异（deltas），筛选高质量的三元组，并合成三种推理难度级别的自然语言问题。每个问题都通过SPARQL验证，确保有唯一且可验证的答案。整个流水线是全自动、可扩展的，并最大限度地减少了人工干预，从而能够持续生成基于时间变化的基准。

Result: 实验结果显示，当模型面对预训练之后出现的事实时，其性能显著下降，尤其在多跳查询上差距最为明显。尽管检索增强方法和更大、经过指令微调的模型能带来部分性能提升，但仍未能完全弥补这种新近知识带来的性能差距。

Conclusion: LiveSearchBench将LLM的评估重心从静态记忆转向需要最新检索和推理能力的任务，为在知识不断演变的环境下对LLM进行系统、长期的评估奠定了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [306] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 本文提出了一种基于草稿细化的流水线，利用开源大型语言模型（LLMs）将形态丰富的低资源语言（如拉丁语）的翻译性能提升至与顶级专有系统（如GPT-5）相当的水平，且无需针对任务进行LLM微调。


<details>
  <summary>Details</summary>
Motivation: 翻译像拉丁语这种形态丰富、资源稀缺的语言面临巨大挑战。研究旨在提升开源LLMs在此类任务上的表现，使其能与顶级专有系统匹敌。

Method: 该方法首先使用一个经过微调的NLLB-1.3B模型生成高质量、结构忠实的草稿。随后，一个零样本LLM（Llama-3.3或Qwen3）对草稿进行润色，这一过程可通过检索式增强生成（RAG）进一步提升。研究在两个基准上进行了验证：一个标准的域内测试集和一个具有挑战性的域外（12世纪拉丁语信件）测试集。

Result: 研究发现，该开源RAG系统在无需任何特定任务LLM微调的情况下，其性能与GPT-5基线系统在统计学上相当。

Conclusion: 该研究成功构建了一个可复现的草稿细化流水线，显著提升了开源LLMs在拉丁语翻译方面的性能，使其达到与顶级专有系统相当的水平，并发布了相关工具和数据集以促进后续研究。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [307] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 该研究提出使用大型语言模型（LLMs）作为认知扭曲检测等主观任务的可靠且一致的标注者，并引入了一种数据集无关的评估框架，结果显示LLM生成的标注能显著提高下游模型的性能。


<details>
  <summary>Details</summary>
Motivation: 文本自动化认知扭曲检测因其主观性而面临挑战，即使是专家人类标注者也难以达成高一致性，导致标注不可靠。

Method: 研究探索了使用多个独立的LLM运行来揭示稳定的标注模式，并引入了一种基于Cohen's kappa的数据集无关评估框架，以公平比较不同数据集和研究的模型。具体使用GPT-4进行标注，并将其与人类标注数据进行比较。

Result: GPT-4能够生成高度一致的标注（Fleiss's Kappa = 0.78）。与使用人类标注数据训练的模型相比，使用这些LLM标注训练的模型在测试集上表现出更高的性能。

Conclusion: 大型语言模型可以为生成主观NLP任务的训练数据提供一种可扩展且内部一致的替代方案，从而支持强大的下游性能。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [308] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 本文提出了预算感知推理蒸馏（BARD）框架，旨在将长链式思考（CoT）的推理能力蒸馏到小型语言模型中，同时实现对推理长度和计算预算的精细控制，以提高资源效率。


<details>
  <summary>Details</summary>
Motivation: 长链式思考（CoT）蒸馏虽然能有效将推理能力转移到小型模型，但推理过程往往冗余且计算预算不可控，导致资源使用效率低下。

Method: BARD框架采用两阶段训练方案：第一阶段是监督微调（SFT），使用教师模型生成的、压缩到不同预算水平的长CoT数据进行训练，以建立模型对预算约束的理解；第二阶段是强化学习（RL），利用同时考虑推理性能和预算符合度的奖励信号进行优化，以避免策略退化并联合优化两个目标。

Result: 实验结果表明，该方法使一个8B的学生模型在具有挑战性的推理基准（AIME24、AIME25、GPQA）上取得了优异性能，同时在各种预算范围内提供了对其推理长度的精确和自适应控制。

Conclusion: BARD框架成功地将推理能力蒸馏到小型模型，并提供了对推理长度的细粒度控制，有效平衡了推理性能和计算效率，解决了CoT蒸馏中冗余和预算不可控的问题。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [309] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 本文研究了不同来源的合成数据对微调大型语言模型（LLMs）行为的影响，发现多源合成数据能缓解分布坍缩并降低自偏好偏差，但移除安全措施后，其输出质量更高，潜在危险性也更大。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中广泛应用，理解其对模型行为的影响至关重要。

Method: 研究通过微调大型语言模型，调查了不同来源的合成数据多样性对模型行为的影响，重点关注三个维度：分布坍缩、对抗鲁棒性和自偏好偏差。

Result: 研究发现，使用多源合成数据进行微调可以缓解分布坍缩，保持输出分布的广度和文本多样性。虽然人类数据和合成数据都能移除安全措施，但后者能保持更高的输出质量，使其输出更具可用性和潜在危险性。此外，微调能降低自偏好偏差，其中人类数据最有效，其次是多源合成数据。

Conclusion: 多源合成数据在缓解分布坍缩和降低自偏好偏差方面具有优势，但在移除安全措施后，其输出质量的保持使得模型在某些情况下更具潜在危险性。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [310] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种针对孟加拉语文本解毒的新型管道，结合了帕累托类别优化的LLM和思维链（CoT）提示，并构建了BanglaNirTox平行语料库以支持该任务。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的毒性语言（尤其是在线环境）普遍存在，但缺乏有效的预防措施。尽管高资源语言的文本解毒已取得进展，但由于资源有限，孟加拉语在该领域仍未得到充分探索。

Method: 研究人员提出了一种结合帕累托类别优化的LLM和思维链（CoT）提示的孟加拉语文本解毒新管道。他们构建了BanglaNirTox，一个包含68,041个毒性孟加拉语句子及其类别毒性标签、推理和解毒释义的人工生成平行语料库，该语料库使用帕累托优化的LLM进行评估。BanglaNirTox数据集用于微调语言模型。

Result: 研究结果表明，结合CoT提示的帕累托优化LLM显著提高了孟加拉语文本解毒的质量和一致性。

Conclusion: 结合帕累托优化LLM和CoT提示的方法，以及新构建的BanglaNirTox数据集，能够有效提升孟加拉语文本解毒的质量和一致性。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [311] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 本文提出了一种名为ECO解码的新型动态解码方法，通过基于语言模型和属性分类器概率分布的熵，在可控对话生成（CDG）任务中动态调整控制强度，从而在保持流畅性的同时显著提升可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的可控对话生成（CDG）加权解码方法使用固定的常数来管理属性概率偏差，这使得很难找到一个理想的控制强度来同时满足可控性和流畅性。

Method: 本文提出了ECO解码（Entropy-based COntrol），它根据语言模型和属性分类器概率分布的熵，在每个生成步骤动态调整控制强度。

Result: 实验结果表明，ECO解码在DailyDialog和MultiWOZ数据集上，始终能在保持流畅性和语法正确性的同时提高可控性，并且优于之前的解码方法。此外，ECO解码还缓解了多属性生成中的概率插值问题，在单属性和多属性场景中都表现出色。

Conclusion: ECO解码是一种有效且动态的可控对话生成方法，通过基于熵的控制强度调整，成功解决了传统方法的局限性，显著提升了可控性，同时保持了生成对话的流畅性，尤其在多属性生成中表现突出。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [312] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的框架，通过数据增强和多任务学习策略，生成具有可控难度的多项选择题干扰项，并构建了一个难度标注数据集。


<details>
  <summary>Details</summary>
Motivation: 生成高质量干扰项仍然具有挑战性，现有方法缺乏适应性和对难度水平的控制，且缺乏难度标注数据集阻碍了进展。

Method: 首先，通过“双向干扰项生成过程”创建多样且合理的干扰项，经过筛选后，利用集成QA系统按难度进行分类，从而构建一个高质量、难度标注的数据集。其次，利用该数据集通过多任务学习训练一个难度可控的生成模型，其中包含精心设计的辅助任务，以增强模型对干扰项的语义理解和难度估计能力。

Result: 实验结果表明，该方法能够生成不同难度水平的高质量干扰项，并且在使干扰项难度与人类感知对齐方面，显著优于GPT-4o。

Conclusion: 该研究成功解决了干扰项生成中缺乏难度控制和标注数据集的问题，提出了一种有效的方法来生成高质量、难度可控的干扰项，并证明了其优越性。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [313] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: 本文介绍了BIRD数据集及一种基于字形感知的掩码语言模型框架，用于早期中国青铜器铭文的修复和断代。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜器铭文残缺不全且难以断代，这是研究者面临的主要挑战。

Method: 研究者构建了BIRD数据集，该数据集基于标准学术转录和年代标签。同时，提出了一种字形感知的掩码语言模型框架，结合领域和任务自适应预训练与一个字形网络（Glyph Net, GN），用于连接字位和异体字。在断代任务中还采用了字形偏置采样。

Result: 实验结果表明，字形网络（GN）能有效提升铭文修复效果，而字形偏置采样则显著提高了断代任务的性能。

Conclusion: 通过引入BIRD数据集和字形感知的掩码语言模型框架（特别是Glyph Net和字形偏置采样），能够有效解决早期中国青铜器铭文的修复和断代难题。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [314] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究利用行为心智网络框架，探索了大学生对数学和焦虑相关概念的感知与关联，并与GPT模拟学生进行对比。结果发现，在人类学生中，对“焦虑”的积极评价和高网络度以及对“数学”的负面评价可预测更高的数学焦虑，但在GPT模型中不适用。研究强调了理解概念感知和关联对于管理学生数学焦虑的重要性。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑对大学心理学学生构成重大挑战，影响其职业选择和整体福祉。本研究旨在通过探索个体和群体对数学及焦虑相关概念的感知与关联，以更好地理解和管理这种焦虑。

Method: 研究采用基于行为心智网络（即个体如何构建概念的联想知识和情感感知）的框架。进行了4项实验，涉及两组心理学本科生（n1=70，n2=57），并与GPT模拟学生（GPT-3.5: n=300; GPT-4o: n=300）进行对比。实验1、2、3利用个体层面网络特征预测数学焦虑量表得分。实验4侧重于从人类学生、GPT-3.5和GPT-4o的网络中提取群体层面感知。

Result: 在人类学生中，对“焦虑”的积极情感评级和更高的网络度，以及对“数学”的负面评级，可以预测更高的总数学焦虑和评估性数学焦虑。然而，这些模型不适用于基于GPT的数据，因为模拟网络和心理测量分数与人类存在差异。研究还发现高/低数学焦虑学生群体在语义和情感上构建STEM概念的方式存在差异：高数学焦虑学生集体以情感两极化的方式构建“焦虑”，而低数学焦虑学生则缺乏这种两极化；“科学”被积极评价，但与对“数学”的负面感知形成对比。

Conclusion: 研究结果强调了理解概念感知和关联对于管理学生数学焦虑的重要性，并揭示了人类在数学焦虑认知和情感结构上的独特性，这些是当前GPT模型尚未能完全复制的。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [315] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 该项目旨在通过跨学科研究母语西班牙语者的语言错误，分析大型语言模型（LLM）如何解释、复现或纠正这些错误，以评估其认知能力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅揭示了语言的认知结构，也暴露了当前人工智能系统在复制人类语言方面的局限性。研究旨在深入理解这些错误，并改进现有LLM的不足。

Method: 研究将整合理论语言学（分类错误）、神经语言学（语境化错误）和自然语言处理（评估LLM对错误的解释）三个核心视角。将构建一个包含500多个真实母语西班牙语错误的专用语料库，并使用GPT或Gemini等AI模型进行测试。

Result: 通过将语料库中的错误与AI模型进行测试，评估LLM对语言错误的解释准确性以及其泛化人类语言行为模式的能力。

Conclusion: 该项目不仅有助于理解作为母语的西班牙语，还将促进开发更具认知信息、能更好地处理真实人类语言中不完美、多变和模糊特性的自然语言处理系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [316] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探讨了在能源效率问答中使用基于图的检索增强生成（RAG）架构与大型语言模型（LLMs），通过自动知识图谱（KG）提取、导航和推理，实现多语言准确回答，并经过专家验证，展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在利用大型语言模型改进能源效率领域的问答系统，特别是从指导和监管文件中获取准确信息，并提供多语言支持。

Method: 1. 自动从能源领域的指导和监管文件中提取知识图谱（KG）。2. 利用生成的图谱进行导航和推理，以提供准确的多语言答案。3. 采用基于图的检索增强生成（RAG）架构，并结合大型语言模型（LLMs）。4. 使用RAGAs框架属性、包含101个问答对的验证数据集和领域专家进行人工验证。

Result: 系统在约75.2%（±2.7%）的案例中能正确回答问题。对于更通用的能源效率问题，准确率更高，达到81.0%（±4.1%）。系统展现出有前景的多语言能力，翻译导致的准确率损失仅为4.4%。

Conclusion: 该架构在能源效率问答方面具有巨大潜力，其优势和劣势已被识别。验证结果证实了系统在准确性和多语言能力方面的良好表现。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [317] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个多语言（克罗地亚语、捷克语、波兰语、塞尔维亚语）议会口语语料库，总计6千小时，通过自动方式构建并显著丰富了语言学、情感预测、语流不畅（如停顿）等多种自动标注层，极大地提升了其在跨学科研究中的可用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是增加现有议会语料库（ParlaMint转录本和语音记录）的实用性，通过自动添加丰富的标注层，使其能支持多学科的下游研究。

Method: 该研究方法包括：1) 从ParlaMint转录本及其元数据自动构建语料库；2) 将转录本与相应的语音记录对齐；3) 对文本模态进行语言学标注和情感预测；4) 对口语模态自动标注填充停顿（最常见的语流不畅）；5) 对其中两种语言额外进行词级和字素级对齐，并标注多音节词的重音位置；6) 通过分析情感的声学关联来展示语料库的实用性；7) 以JSONL和TextGrid格式提供下载，并提供一致性检索工具。

Result: 成果是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语在内的6千小时议会口语语料库，其文本模态和口语模态均显著丰富了自动标注（如语言学标注、情感预测、填充停顿），其中两种语言还额外提供了词级/字素级对齐和重音标注。这些丰富使得语料库在多个学科的下游研究中实用性大增，并通过情感的声学关联分析得到了展示。

Conclusion: ParlaSpeech语料库的创建和丰富，为多语言议会口语研究提供了一个极具价值的资源，其多层次的自动标注极大地促进了跨学科研究，并为情感的声学关联等分析提供了基础。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [318] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 本研究提出一个认知基准框架，结合布鲁姆分类法和RAG，评估大型语言模型处理和应用特定文化知识的能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型如何处理和应用特定文化知识，特别是在语义准确性和文化相关性方面的表现。

Method: 提出一个认知基准框架，该框架整合了布鲁姆认知分类法（六个认知领域：记忆、理解、应用、分析、评估、创造）与检索增强生成（RAG）。使用策展的台湾客家数字文化档案作为主要测试平台，评估模型生成回复的语义准确性和文化相关性。

Result: 该框架能够衡量大型语言模型在六个层级认知领域中对特定文化知识的语义准确性和文化相关性表现。

Conclusion: 本研究提供了一个评估大型语言模型处理和应用特定文化知识能力的综合框架，能够衡量其在不同认知层面的表现。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [319] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: 本文介绍了EngChain，一个用于评估大型语言模型（LLMs）在工程领域多步骤、可验证推理能力的基准测试，通过两阶段评估方法超越了最终答案准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs基准测试未能捕捉工程领域所需的科学原理、定量建模和实际约束相结合的综合推理能力，因此需要一个能严格评估其复杂推理能力的工具。

Method: 引入EngChain基准，包含90个跨三个工程分支的问题，分为9个领域和20个不同区域。问题通过符号模板高度随机生成，以确保多样性并消除污染风险。评估采用两阶段：首先定量验证每个推理步骤的数值和语义有效性，然后引入“LLM-As-A-Judge”系统定性分类识别出的推理错误。

Result: 创建了EngChain基准，包含90个问题，涵盖三个工程分支、9个领域和20个独特区域。这些问题通过符号模板高度随机生成，确保了多样性。同时，开发了两阶段评估方法，包括对推理步骤的定量验证和使用LLM-As-A-Judge进行定性错误分类。

Conclusion: EngChain基准填补了当前LLM评估的空白，为严格评估LLMs在工程领域中复杂的、多步骤的、可验证的推理能力提供了一个新工具，超越了传统的最终答案准确性评估。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [320] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本文介绍了SeaLLMs-Audio，首个针对东南亚多语言（印尼语、泰语、越南语）以及英语和中文的大型音频语言模型（LALM），它支持多语言、多模态和多任务功能。同时引入了SeaBench-Audio，一个用于自动化评估东南亚LALM的基准。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了填补东南亚语言在大型音频语言模型（LALM）领域的空白，并推动该地区音频LLM的发展，以服务于区域研究社区和工业界。

Method: 该研究引入了SeaLLMs-Audio模型，它在一个大规模音频语料库上训练，主要支持印尼语、泰语、越南语、英语和中文五种语言。模型接受灵活的输入模态（纯音频、纯文本、音频加文本），并支持广泛的任务，包括音频理解（如音频字幕、自动语音识别、语音到文本翻译、语音情感识别、语音问答、语音摘要）和基于语音的对话。此外，还引入了SeaBench-Audio基准来自动化评估东南亚LALM。

Result: SeaLLMs-Audio在各种以音频为中心的任务中表现出强大的性能，包括细粒度音频理解和基于语音的交互。实验表明，在东南亚语言上，SeaLLMs-Audio与现有LALM相比，取得了具有竞争力的性能。

Conclusion: SeaLLMs-Audio是推动东南亚音频LLM发展的重要一步，预计将使该区域的研究社区和工业界受益。SeaBench-Audio的引入也为东南亚LALM的自动化评估提供了工具。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [321] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文介绍了首个开源的“角色训练”方法，利用宪法式AI和合成内省数据，有效且可控地塑造大型语言模型（LLM）的AI助手人格，并证明其在鲁棒性和生成连贯性方面优于现有替代方案，且不影响模型通用能力。


<details>
  <summary>Details</summary>
Motivation: AI助手的人格特性影响交互质量、感知智能以及与开发者和用户意图的对齐。尽管“角色训练”是行业后期训练的关键组成部分，但在学术界仍未得到有效研究。现有塑造人格的方法（如系统提示或激活引导）可能不够鲁棒或有效。

Method: 研究引入了首个开源的“角色训练”实现，结合了宪法式AI（Constitutional AI）和一种新的数据管道，该管道使用合成内省数据来塑造助手人格。具体地，他们使用11种示例人格（如幽默、关怀或恶意）对三个流行的开源模型进行微调。为了跟踪效果，他们引入了一种分析“揭示偏好”的方法，以发现角色中清晰而整体的变化。

Result: 研究发现，通过其方法塑造的角色变化对对抗性提示更具鲁棒性，优于系统提示或激活引导等替代方案。此外，这种方法还能产生更连贯和真实的生成内容。最后，通过通用基准测试，他们证明这种微调对模型的通用能力几乎没有影响。

Conclusion: 角色训练是一种有效且可控地塑造LLM助手人格的方法，其实现方式比现有替代方案更具鲁棒性，能产生更连贯的生成，且不损害模型的通用能力。该研究为学术界提供了首个开源的角色训练实现。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [322] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的秩-2投影子空间，用于更准确地解耦大型语言模型（LLMs）自然语言解释（NLEs）中参数知识（PK）和上下文知识（CK）的贡献，并首次进行了多步知识交互分析。


<details>
  <summary>Details</summary>
Motivation: 理解NLEs中外部上下文知识（CK）和模型内部参数知识（PK）的交互对于评估NLEs的可靠性至关重要，但这一领域尚未得到充分探索。现有工作主要关注单步生成，并以二元选择的秩-1子空间建模PK和CK交互，这忽略了更丰富的知识交互形式（如互补或支持）。

Method: 本文提出了一种新颖的秩-2投影子空间，能够更准确地解耦PK和CK的贡献。利用此方法，研究人员首次对NLEs序列中的多步知识交互进行了分析。实验在四个问答数据集和三个开源指令微调LLM上进行。

Result: 实验结果表明，秩-1子空间未能很好地表示多样化的知识交互，而秩-2公式能够有效捕获这些交互。多步分析揭示，幻觉NLEs强烈偏向PK方向，忠实于上下文的NLEs平衡PK和CK，而用于NLEs的思维链（Chain-of-Thought）提示通过减少对PK的依赖，将生成的NLEs推向CK方向。

Conclusion: 本工作通过一个更丰富的秩-2子空间解耦方法，为LLMs中多步知识交互的系统研究提供了第一个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [323] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 该论文提出一个多专家系统，用于创建能进行自然对话和执行情境动作的NPC，该系统计算效率高且在某挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 旨在开发能够进行自然对话并在交互环境中执行情境动作的非玩家角色（NPC），同时满足计算效率要求。

Method: 使用Qwen3作为基础模型，结合LoRA适配器实例化了三个专家：工具调用、工具响应解释和直接对话，构建了一个多专家系统。

Result: 该系统满足计算效率要求，在L40S GPU上响应速度快且资源占用适中。在2025年常识角色导向对话挑战赛中，该方法获得了总分第二名。

Conclusion: 所提出的多专家系统能有效且高效地使NPC实现自然对话和情境动作，并在竞争性挑战中表现出色。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [324] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 研究发现，语言模型（如GPT-5和Grok 4）的信念会随着上下文的累积（对话或阅读）而发生显著且隐性的变化，这会影响其响应和行为的可靠性。


<details>
  <summary>Details</summary>
Motivation: 语言模型助手在应用中积累的上下文越来越多，且无需用户明确干预。这带来了一个潜在风险：模型的信念配置文件（对世界的理解）可能会随着上下文的积累而悄然改变，导致用户体验不一致或行为偏离模型原始对齐。

Method: 研究通过让语言模型进行互动（“交谈”）和处理文本（“阅读”）来累积上下文。通过观察模型在道德困境、安全查询和政治问题上的响应来测量其信念变化。此外，设计了需要使用工具的任务，其中每个工具选择对应一个隐含信念，以检查模型的行为变化。

Result: 结果显示模型的信念配置文件具有高度可塑性：GPT-5在10轮关于道德困境和安全问题的讨论后，其公开表达的信念发生了54.7%的转变。Grok 4在阅读了持相反立场的文本后，其在政治问题上的信念发生了27.2%的转变。模型行为上的变化与其公开表达的信念转变一致，表明信念转变将反映在代理系统中的实际行为中。

Conclusion: 研究揭示了模型在长时间的对话或阅读会话中，信念会发生隐性转变的风险，这使得它们的观点和行动变得不可靠，尤其是在代理系统中。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [325] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级变换编码器，用于高效压缩大型语言模型（LLM）的KV缓存，以实现内存高效的服务，同时保持推理和长上下文准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模服务LLM需要高效的KV缓存管理。虽然共享前缀提示允许KV缓存重用，但过时或未使用的缓存会消耗稀缺的GPU内存，导致需要卸载或重新计算，这限制了LLM的扩展能力。

Method: KVTC是一种变换编码器，它结合了经典媒体压缩技术：基于PCA的特征去相关、自适应量化和熵编码。它只需要简短的初始校准，并且不改变模型参数。

Result: KVTC在保持推理和长上下文准确性的前提下，实现了高达20倍的压缩比，在特定用例中甚至达到40倍或更高。它在Llama 3、Mistral NeMo和R1-Qwen 2.5等模型上，通过多项基准测试，始终优于令牌驱逐、量化和基于SVD等推理时基线方法，并获得了更高的压缩率。

Conclusion: KVTC被证明是内存高效LLM服务中一个实用的构建模块，尤其适用于可重用KV缓存的场景，因为它能有效压缩KV缓存，同时保持模型性能。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [326] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: 本文介绍了IMO-Bench，一套针对国际数学奥林匹克（IMO）级别数学推理能力的基准测试套件，旨在解决现有评估过于简单或仅关注简短答案的问题，并展示了Gemini Deep Think模型在该基准上的卓越表现。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型的数学推理能力评估要么过于简单，要么只关注正确简短的答案，这不足以衡量模型真正的数学推理进展。为了推动基础模型在数学推理方面的发展，需要更高级、更全面的评估指标。

Method: 研究人员创建了IMO-Bench，一个由顶尖专家审核的先进推理基准套件，专门针对IMO级别。IMO-Bench包含两部分：IMO-AnswerBench（400道可验证短答案的奥林匹克问题）和IMO-ProofBench（包括基础和高级IMO级别的证明题，并附有详细评分指南以促进自动评分）。此外，他们还构建了与人类评估高度相关的Gemini推理自动评分器，并创建了IMO-GradingBench（包含1000个人工评分的证明），以推动长篇答案的自动评估。

Result: Gemini Deep Think模型在IMO 2025中取得了金牌级别的表现。它在IMO-AnswerBench上达到80.0%的准确率，在高级IMO-ProofBench上达到65.7%的准确率，分别比其他非Gemini模型的最佳成绩高出6.9%和42.4%。研究还表明，使用Gemini推理构建的自动评分器与人工评估高度相关。

Conclusion: IMO-Bench为评估和推进基础模型在鲁棒数学推理方面的能力提供了一个关键工具。它不仅解决了现有评估的局限性，还通过其详细的证明评估和自动评分工具，促进了长篇答案自动评估的进一步发展。研究人员希望IMO-Bench能帮助社区共同提升数学推理能力。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [327] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 该论文提出了一种无需模型再训练的提示工程方法，通过在提示中引入规划和字数统计机制，显著提高了大型语言模型在文档摘要任务中的长度控制精度和输出质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的长度控制是一个关键但未充分解决的挑战。现有方法（如正则化DPO、长度指令微调、工具增强）通常需要昂贵的模型再训练或复杂的推理时工具，这在许多生产环境中是不切实际或成本过高的。

Method: 本文提出了一种结构引导的提示工程方法。该方法通过在提示内部实现深思熟虑的规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度限制，从而实现精确的长度控制，且无需进行模型再训练。

Result: 在对六个最先进的大型语言模型进行的综合评估中，该方法在文档摘要任务中，特别是对于短到中等长度的限制，显著提高了模型的长度忠实度，部分模型长度依从性提升高达37.6%。此外，质量评估显示，与标准提示技术相比，该方法保持或提高了整体输出质量。

Conclusion: 该方法提供了一种可立即部署的解决方案，适用于需要精确长度控制的应用，尤其对于模型再训练不切实际或成本过高的生产环境具有重要价值。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [328] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出“工具到代理检索”框架，通过在共享向量空间中嵌入工具及其父代理并利用元数据关系，解决了LLM多代理系统中现有粗粒度检索导致代理选择次优的问题，显著提高了检索精度。


<details>
  <summary>Details</summary>
Motivation: LLM多代理系统中的现有检索方法通常将查询与粗粒度的代理级别描述进行匹配，这掩盖了工具的细粒度功能，导致代理选择次优。

Method: 引入“工具到代理检索”框架，该框架将工具及其父代理嵌入到一个共享的向量空间中，并通过元数据关系连接它们。通过显式表示工具能力并遍历元数据到代理级别，该方法实现了细粒度的工具级或代理级检索，确保代理及其底层工具或MCP服务器得到平等表示，避免了因将多个工具打包在一起而导致的上下文稀释。

Result: 在LiveMCPBench基准测试中，该方法在八种嵌入模型上，相较于之前最先进的代理检索器，Recall@5提高了19.4%，nDCG@5提高了17.7%。

Conclusion: “工具到代理检索”框架通过更精细地表示工具功能和其与代理的关系，显著改善了LLM多代理系统中的代理选择准确性，从而实现了更有效的系统编排。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [329] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 本文提出了一种利用数字孪生技术，实现机器人在动态环境中自主、动态地重新配置控制器的新框架，以提高其适应性和自主性。


<details>
  <summary>Details</summary>
Motivation: 机器人系统在智能环境中面临动态地形和环境条件的挑战，传统控制系统难以快速适应，导致效率低下或操作失败。

Method: 该研究提出了一种利用数字孪生技术实现机器人控制器自主动态重配置的新颖框架。通过机器人操作环境的虚拟副本，模拟并优化运动轨迹以响应现实世界的变化。在数字孪生中重新计算路径和控制参数，并将更新后的代码部署到物理机器人。

Result: 该方法确保了机器人能够快速可靠地适应动态环境，无需人工干预。

Conclusion: 这项工作推动了数字孪生技术在机器人领域的整合，为增强智能、动态环境中的机器人自主性提供了一个可扩展的解决方案。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [330] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: 本文提出STRIDER框架，通过整合空间结构先验和动态任务反馈来优化决策空间，显著提升了零样本视觉-语言导航（VLN-CE）在连续环境中的表现，尤其是在成功率方面。


<details>
  <summary>Details</summary>
Motivation: 零样本视觉-语言导航（VLN-CE）任务中，智能体在未知3D环境中执行指令时，难以确保其动作与空间结构和任务意图长期保持一致。现有方法因缺乏结构化决策和对先前动作反馈的整合不足，导致导航不够稳健。

Method: 本文提出了STRIDER（Instruction-Aligned Structural Decision Space Optimization）框架，通过整合空间布局先验和动态任务反馈来系统性地优化智能体的决策空间。其核心创新包括：1) 结构化路径点生成器（Structured Waypoint Generator），通过空间结构约束动作空间；2) 任务对齐调节器（Task-Alignment Regulator），根据任务进度调整行为，确保整个导航过程的语义对齐。

Result: 在R2R-CE和RxR-CE基准测试中，STRIDER显著优于现有最先进方法。特别是，成功率（SR）从29%提升到35%，相对增益达20.7%。

Conclusion: 实验结果强调了空间受限决策和反馈引导执行对于提高零样本VLN-CE导航准确性的重要性。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [331] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: 本文提出BiBo框架，通过利用现成的视觉-语言模型（VLMs）控制人形智能体，将其高泛化能力应用于开放环境中的多样化交互，从而避免了大量数据收集的需要。


<details>
  <summary>Details</summary>
Motivation: 现有的人形智能体在开放环境中处理灵活多样的交互时表现不佳，而收集大量数据集来训练高性能模型成本过高。

Method: BiBo框架包含两个核心组件：1) 一个具身指令编译器，使VLM能够感知环境并将高级用户指令（如“休息”）精确转换为带控制参数的低级原始命令；2) 一个基于扩散的运动执行器，根据这些命令生成类人动作，并动态适应环境中的物理反馈。

Result: BiBo在开放环境中的交互任务成功率达到90.2%，并且将文本引导运动执行的精度比现有方法提高了16.3%。

Conclusion: BiBo通过赋能现成的VLM来控制人形智能体，成功解决了开放环境中多样化复杂运动的挑战，显著提高了交互任务的成功率和运动执行的精度。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [332] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: 本文全面综述了生成式AI在汽车行业的应用、使能技术、机遇、挑战及未来方向，并以奔驰MBUX虚拟助手为例，强调了其在语音HMI、安全和用户体验方面的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正成为汽车行业的变革力量，在多个领域带来创新应用。当前缺乏一篇全面覆盖车辆设计、制造、自动驾驶、预测性维护及车载用户体验，并特别关注基于语音的人机交互（HMI）和安全视角的综述。

Method: 本文采用综合性综述方法，回顾了生成式AI在汽车领域的现状，突出了生成对抗网络（GANs）和变分自编码器（VAEs）等关键技术，识别了主要机遇和技术、伦理、安全挑战。此外，还通过奔驰MBUX虚拟助手的案例研究，具体说明了生成式AI在语音系统中的应用效果。

Result: 生成式AI在汽车行业具有显著机遇，包括通过合成数据加速自动驾驶验证、优化部件设计以及通过个性化和自适应界面增强人机交互。然而，也面临计算需求、偏见、知识产权和对抗性鲁棒性等重大技术、伦理和安全挑战。案例研究表明，生成式AI驱动的语音系统能提供比传统基于规则的助手更自然、主动和个性化的车内交互。

Conclusion: 生成式AI在汽车行业的整合既有巨大前景也存在局限性。未来的研发应致力于解决这些挑战，以实现更安全、高效和以用户为中心的出行方式。

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [333] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出了Alpamayo-R1 (AR1)，一个视觉-语言-动作(VLA)模型，通过整合因果链推理和轨迹规划，显著提升了复杂自动驾驶场景中的决策能力和安全性，为L4级自动驾驶提供了实用路径。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端模仿学习自动驾驶模型在安全关键的长尾场景中表现脆弱，因为这些场景缺乏监督数据且因果理解能力有限。

Method: 本文引入了Alpamayo-R1 (AR1)模型，包含三项关键创新：(1) 构建了因果链 (CoC) 数据集，通过混合自动标注和人工辅助流程生成与驾驶行为对齐的决策导向型因果推理轨迹；(2) 设计了模块化VLA架构，结合了预训练用于物理AI应用的视觉-语言模型Cosmos-Reason和基于扩散的实时轨迹解码器；(3) 采用了多阶段训练策略，先通过监督微调诱导推理能力，再利用强化学习（RL）通过大型推理模型反馈优化推理质量并强制推理-动作一致性。

Result: 评估结果显示，AR1在挑战性场景下规划精度比仅基于轨迹的基线提高了12%，在闭环仿真中越野率降低了35%，近距离接触率降低了25%。RL后训练将推理质量提高了45%（通过大型推理模型批评器衡量），推理-动作一致性提高了37%。模型从0.5B扩展到7B参数显示出持续的改进。车载路测证实了实时性能（99毫秒延迟）和成功的城市部署。

Conclusion: AR1通过连接可解释推理与精确控制，展示了实现L4级自动驾驶的实用路径。作者计划未来发布AR1模型和部分CoC数据集。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [334] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: 本文提出Real-DRL框架，用于在真实物理系统中实现安全关键型自主系统的运行时深度强化学习，优先保障安全并提升性能。


<details>
  <summary>Details</summary>
Motivation: 在安全关键型自主系统中，如何在真实物理环境下（即实际工厂）让DRL智能体学习到安全且高性能的动作策略，同时解决“未知未知”问题和Sim2Real（模拟到现实）差距带来的安全挑战。

Method: Real-DRL框架包含三个交互组件：1) DRL-Student：一个DRL智能体，采用双重自学习和教学学习范式，以及实时安全知情批次采样。2) PHY-Teacher：基于物理模型设计的动作策略，专注于安全关键功能，提供实时补丁以促进教学学习并保障系统安全。3) Trigger：管理DRL-Student和PHY-Teacher之间的交互。该框架还具备安全保障、自动分层学习（先安全后高性能）和安全知情批次采样等特点。

Result: Real-DRL框架通过在真实四足机器人、NVIDIA Isaac Gym中的四足机器人和倒立摆系统上的实验，以及对比和消融研究，证明了其有效性及独特功能，包括确保安全、自动分层学习和解决角点案例导致的学习经验不平衡。

Conclusion: Real-DRL框架能够有效解决自主系统在真实物理环境中面临的安全挑战，通过创新的组件交互和学习机制，实现安全优先且高性能的运行时深度强化学习。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [335] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 本文提出了一种共享自主框架，用于高效收集高质量的机械臂-手协调操作数据，并训练了一个增强的视觉-语言-动作（VLA）策略，以实现机器人类似人类的灵巧操作，在多样化物体上取得了90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 通用机器人实现类人灵巧操作面临巨大挑战，主要受限于高质量训练数据的稀缺性。现有数据收集方法（如手动遥操作和自动化规划）各有局限，前者认知负荷高，后者动作不自然。

Method: 研究提出了一个共享自主框架，将控制分为宏观（人类操作VR引导机械臂姿态）和微观（自主DexGrasp-VLA策略处理精细手部控制，利用触觉和视觉反馈）。该框架用于高效收集高质量的机械臂-手协调演示数据。然后，利用这些数据训练一个端到端的VLA策略，并引入一个新颖的机械臂-手特征增强模块，以捕捉宏观和微观运动的独特及共享表示。此外，还设计了一个纠错遥操作系统，通过人机协作的故障恢复实现策略的持续改进。

Result: 该框架能够以最少的人力生成高质量数据，并在包括未见实例在内的多样化物体上实现了90%的成功率。全面的评估验证了该系统在开发灵巧操作能力方面的有效性。

Conclusion: 该研究通过共享自主的数据收集方法、增强的VLA策略以及人机协作的纠错机制，有效解决了机器人灵巧操作中的数据稀缺和协调性问题，显著提升了机器人的操作能力和成功率。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [336] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: 该研究提出了EgoMI框架，通过捕捉人类主动头部运动和手部轨迹，并结合记忆增强策略，有效解决了模仿学习中人类与机器人之间的具身差距，从而在半人形机器人上实现了更鲁棒的模仿学习。


<details>
  <summary>Details</summary>
Motivation: 从人类示范中进行模仿学习时，由于人类与机器人之间存在具身差距（embodiment gap），特别是人类主动的头部和手部协调运动、视点调整以及预动作视觉注视策略，导致静态机器人感知系统无法复制这些动态行为，进而产生显著的分布偏移，影响策略性能。

Method: 研究提出了EgoMI（Egocentric Manipulation Interface）框架，用于同步捕捉操作任务中的末端执行器和主动头部轨迹。为处理快速且宽范围的头部视点变化，引入了一种记忆增强策略，选择性地整合历史观测数据。捕获的数据可重新定向到兼容的半人形机器人具身。

Result: 在配备有可驱动摄像头头部的双臂机器人上进行评估，结果表明，明确建模头部运动的策略始终优于基线方法。

Conclusion: EgoMI框架下的协调手眼学习有效弥合了人类与机器人之间的具身差距，为半人形机器人上的鲁棒模仿学习提供了有效途径。

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [337] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: 本研究发现，针对本体感觉定制的机器人训练能有效改善中风幸存者的手部功能和神经处理能力，尤其对于存在本体感觉缺陷的患者。


<details>
  <summary>Details</summary>
Motivation: 精准康复旨在通过定制化的运动训练来改善治疗效果。本研究旨在验证针对本体感觉定制的机器人训练是否能改善中风幸存者的手部功能和神经处理能力。

Method: 一项随机对照试验，纳入46名慢性中风幸存者。参与者分为三组：标准训练组、Propriopixel训练组（机器人辅助的、游戏化的运动以增强本体感觉处理）和虚拟辅助训练组（减少机器人辅助以增加对自我生成反馈的依赖）。每组完成9次，每次2小时的训练。通过Box and Block Test评估手部功能，并通过一种新型脑电图（EEG）生物标志物（本体感觉条件负变，proprioceptive Contingent Negative Variation）评估神经敏感性。

Result: 对于存在本体感觉缺陷的参与者，Propriopixel训练组（手部功能改善7 ± 4.2块，p=0.002）和虚拟辅助训练组（4.5 ± 4.4块，p=0.068）比标准训练组（0.8 ± 2.3块）在手部功能上取得了更大的进步。本体感觉的改善与手部功能的提高呈正相关。定制化训练增强了对本体感觉线索的神经敏感性，这通过脑电图生物标志物得到了证实。

Conclusion: 这些发现支持本体感觉定制化训练作为实现精准神经康复的一种有效途径。

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [338] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 本研究评估了时间序列基础模型（如Chronos）是否能通过预测未记录的试验，显著缩短Kinarm机器人视觉引导式抓取（VGR）评估时间，同时保持运动学参数的可靠性。


<details>
  <summary>Details</summary>
Motivation: Kinarm VGR评估能提供敏感的运动学生物标志物，但需要进行40-64次抓取，耗时且易导致疲劳，给参与者带来负担。

Method: 研究分析了461名中风患者和599名对照组参与者的VGR速度信号。他们仅保留了前8或16次抓取试验，并使用ARIMA、MOMENT和Chronos模型（在70%的受试者数据上进行微调）来预测合成试验。然后，将记录的试验与预测的试验结合，重新计算了反应时间、运动时间、姿势速度和最大速度这四个运动学特征，并使用ICC(2,1)与完整试验的参考值进行比较。

Result: Chronos模型预测仅使用8次记录试验加上预测，就使所有参数的ICC值恢复到0.90以上，其可靠性与24-28次记录试验相当（Delta ICC <= 0.07）。MOMENT模型取得了中等程度的提升，而ARIMA模型的改进最小。在不同队列和协议中，合成试验替代了部分抓取，而未实质性损害特征可靠性。

Conclusion: 基础模型预测能大大缩短Kinarm VGR评估时间。对于受损最严重的中风幸存者，评估时间可从4-5分钟缩短至约1分钟，同时保持运动学精度。这种预测增强范式有望为中风后运动障碍评估提供高效的机器人评估方法。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [339] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文揭示了滑动窗口因子图优化 (SW-FGO) 与卡尔曼滤波变体 (KFV) 之间的理论联系，提出了一个递归因子图优化 (Re-FGO) 框架，并在特定条件下证明了其与KFV的等价性，同时强调了SW-FGO在非线性、非高斯环境中的独特优势。


<details>
  <summary>Details</summary>
Motivation: 尽管SW-FGO在导航研究中日益受到关注，并且与扩展卡尔曼滤波 (EKF) 的应用性能对比众多，但SW-FGO与EKF之间的理论关系仍然存在争议。

Method: 研究人员首先找到了连接SW-FGO与KFV（如EKF、IEKF、REKF、RIEKF）的必要公平条件。在此基础上，提出了一个递归因子图优化 (Re-FGO) 框架，用于在SW-FGO公式下表示KFV。通过明确的条件（马尔可夫假设、L2损失的高斯噪声和一个单状态窗口），Re-FGO被证明可以精确地再现EKF/IEKF/REKF/RIEKF。

Result: 在明确的条件下（马尔可夫假设、L2损失的高斯噪声和一个单状态窗口），所提出的Re-FGO框架可以精确地再现EKF/IEKF/REKF/RIEKF。同时，SW-FGO在非线性、非高斯环境下显示出可衡量的优势，且计算成本可预测。研究还澄清了两者之间的联系，并强调了SW-FGO在数值估计和深度学习集成等实际应用中的独特优势。

Conclusion: SW-FGO与卡尔曼滤波变体之间存在明确的理论联系，通过Re-FGO框架可以在特定条件下实现相互表示。在非线性、非高斯环境下，SW-FGO相比传统卡尔曼滤波变体具有显著优势，尤其在数值估计和深度学习集成方面展现出独特的应用潜力。

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [340] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: 本文提出了一种基于经典龙格-库塔积分例程的新型锥动补偿算法，用于捷联导航系统中的陀螺仪积分，并展示了生成更高阶算法的通用过程。


<details>
  <summary>Details</summary>
Motivation: 陀螺仪测量积分是大多数导航系统（特别是捷联系统）的关键任务。由于传感器在积分过程中的旋转，需要进行锥动补偿。尽管已有多种补偿算法，但仍有必要探索新的、更系统化的方法。

Method: 引入了一类直接基于经典龙格-库塔（Runge-Kutta）积分例程构建的锥动校正算法。提出了一种生成更高阶算法的清晰程序。

Result: 研究表明，新算法的一个简单特例可以退化为当前最流行的锥动补偿算法之一。同时，本文提供了一个明确的步骤来生成更高阶的算法。

Conclusion: 本文成功提出了一种基于龙格-库塔积分的新型锥动补偿算法，该算法不仅能复现现有流行算法，还能系统地生成更高阶的补偿方案，为捷联导航系统提供了新的集成方法。

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [341] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: 本文提出了SonarSweep，一个新颖的端到端深度学习框架，通过调整平面扫描算法，实现了声纳和视觉数据的跨模态融合，从而在水下恶劣环境中生成密集且准确的3D深度图。


<details>
  <summary>Details</summary>
Motivation: 在视觉退化的水下环境中，准确的3D重建仍然是一个巨大挑战。单一模态方法（视觉因能见度差，声纳因高程模糊和低分辨率）不足以应对。现有的融合技术依赖启发式和有缺陷的几何假设，导致重建结果存在显著伪影且无法建模复杂场景。

Method: 本文引入了SonarSweep，一个新颖的端到端深度学习框架。该框架通过改编经典的平面扫描算法，实现了声纳数据和视觉数据之间的跨模态融合，以克服现有方法的局限性。

Result: 通过在高保真模拟和真实世界环境中的广泛实验，SonarSweep持续生成密集且准确的深度图。它在挑战性条件下，特别是在高浑浊度环境下，显著优于现有最先进的方法。

Conclusion: SonarSweep成功解决了水下3D重建的难题，为恶劣水下环境提供了更鲁棒和准确的解决方案。为促进进一步研究，作者将公开发布代码和一个首创的同步立体相机和声纳数据集。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [342] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了一种为模块化机器人系统MoonBot开发的斗式滚筒，用于月球风化层挖掘，并通过沙盒测试验证了其效率和与MoonBot的兼容性。


<details>
  <summary>Details</summary>
Motivation: 月球原位资源利用（ISRU）是实现月球可持续利用的关键技术之一，而挖掘月球风化层是使月球资源可及和可用的第一步。

Method: 研究人员开发了一个用于模块化机器人系统MoonBot的斗式滚筒，并制造了一个由PLA材料3D打印的原型。通过一系列沙盒测试评估了其挖掘效率。

Result: 该工具重4.8公斤，体积为14.06升。在连续挖掘模式下，挖掘速率为777.54公斤/小时，归一化能耗为0.022瓦时/公斤。在批处理模式下，挖掘速率为172.02公斤/小时，归一化能耗为0.86瓦时/公斤。该工具与模块化MoonBot机器人平台兼容。

Conclusion: 所获得的结果表明该概念的成功实现。该工具的一个主要优势是其与模块化MoonBot平台的兼容性，这使得任务规划更加灵活高效。未来的改进可能包括集成传感器和自主控制系统以增强挖掘过程。

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [343] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: 本文提出一种新型双足机器人平衡控制方法，通过使用最小自由度的描述性模型来维持平衡，同时允许高维自由度自由演化，从而实现高效、类人且更鲁棒的行走步态。


<details>
  <summary>Details</summary>
Motivation: 传统双足平衡控制依赖低维模型进行规划和反应性控制，这限制了机器人高维运动空间，导致效率低下且步态不自然（如膝盖弯曲）。研究观察到双足平衡本质上是低维的，但现有方法未能有效利用机器人高维空间的自由度。

Method: 提出一种新的控制框架，不将低维模型强加于完整机器人模型。相反，它使用一个描述性模型，该模型仅包含维持平衡所需的最小自由度，允许其余的自由度在高维空间中自由演化。这仅限制了其在低维状态空间中的投影。

Result: 该方法实现了高效的类人行走步态，并提高了鲁棒性。

Conclusion: 通过避免将低维模型强加于高维机器人，并采用描述性模型来管理平衡，同时赋予高维自由度以自由演化，可以显著改善双足机器人的行走效率和鲁棒性。

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [344] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: 该研究提出一种多指混合夹持器，利用被动可变形折纸模块实现恒定力和扭矩输出，无需复杂控制和传感即可稳定适应性抓取，并展示了同时抓取多个不同物体的高效能力。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人夹持器在处理易碎和复杂几何形状物体时，常依赖笨重执行器、复杂控制策略或先进触觉传感来实现稳定可靠的抓取性能。

Method: 该研究设计了一种多指混合夹持器，每根手指由并联折纸模块组成，通过一个1自由度执行机构驱动。折纸模块能够被动变形，产生恒定的力和扭矩输出，从而实现被动形状适应性和稳定的抓取力，无需主动传感或反馈控制。

Result: 该夹持器实现了被动形状适应性和稳定的抓取力，且无需主动传感或反馈控制。更重要的是，它展示了同时抓取多个物体的能力，可以独立拾取、运输和放置不同形状和尺寸的堆叠物体，显著提高了操作效率。

Conclusion: 研究结果强调了基于折纸的柔顺结构作为可扩展模块的潜力，可用于家庭和工业拾放场景中自适应、稳定和高效的多物体操作。

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [345] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出Multi-Mapcher，一个针对异构LiDAR传感器的多会话同步定位与建图（MSS）新框架，通过大规模地图到地图配准进行会话间初始对齐，并结合锚点式姿态图优化，显著提升了MSS性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有异构LiDAR传感器的MSS方法主要依赖回环检测进行会话间对齐，但由于传感器密度和视场（FoV）的差异，回环检测的性能可能会下降。研究挑战了这种对回环检测模块的过度依赖。

Method: 该研究提出了Multi-Mapcher框架，其核心方法包括：1) 利用异常值鲁棒的3D点云配准，进行大规模地图到地图配准以实现会话间初始对齐，这通常被认为是不可行的。2) 在初始对齐足够精确的假设下，通过半径搜索找到会话间回环。3) 采用基于锚点（anchor node-based）的鲁棒姿态图优化来构建一致的全局地图。

Result: 实验结果表明，该方法在各种LiDAR传感器捕获的会话中，展现出显著优于现有技术的MSS性能，并且比现有最先进的方法更快。

Conclusion: Multi-Mapcher通过创新性地使用大规模地图到地图配准进行会话间初始对齐，并结合鲁棒姿态图优化，成功克服了异构LiDAR传感器MSS中回环检测的局限性，实现了更高性能和更快的全局地图构建。

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [346] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种语义引导的模糊控制框架，结合大语言模型和可解释控制，实现了在无GPS、无地图的水下多机器人协同覆盖，解决了部分可观测、通信受限、环境不确定和缺乏全局定位等挑战。


<details>
  <summary>Details</summary>
Motivation: 水下多机器人协同覆盖面临诸多挑战，包括部分可观测性、有限通信、环境不确定性以及缺乏全局定位。这些因素使得现有方法难以有效部署。

Method: 该研究提出一个语义引导的模糊控制框架。首先，大语言模型（LLM）将原始多模态观测压缩成紧凑、人类可解释的语义令牌，概括障碍物、未探索区域和目标物体（OOIs）。然后，一个带有预定义隶属函数的模糊推理系统将这些语义令牌映射为平滑稳定的转向和步态指令，实现无需全局定位的可靠导航。最后，通过语义通信分享意图和局部上下文，协调多个机器人，避免重复探索。

Result: 在未知珊瑚礁状环境中的广泛模拟表明，在有限感知和通信条件下，所提出的框架实现了鲁棒的面向OOI的导航和协同覆盖，显著提高了效率和适应性。

Conclusion: 该框架成功弥合了语义认知与水下分布式控制之间的鸿沟，在无GPS、无地图的条件下，为水下多机器人协同覆盖提供了有效的解决方案。

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [347] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 本文提出了一种在线框架，利用改进的滑动窗口Hankel动态模态分解（Hankel-DMD）方法，从部分噪声数据中实时学习并预测其他智能体的非线性运动，同时提供去噪和方差跟踪功能。


<details>
  <summary>Details</summary>
Motivation: 自动系统需要从部分和噪声数据中预测附近智能体的运动。研究的动机是能否实时学习一个非线性的预测模型来描述另一个智能体的运动。

Method: 该方法是一个在线框架，使用改进的滑动窗口Hankel动态模态分解（Hankel-DMD）。它将部分噪声测量嵌入到Hankel矩阵中，并利用Page矩阵和奇异值硬阈值（SVHT）来估计有效秩。Cadzow投影用于强制结构化低秩一致性，从而得到去噪轨迹和局部噪声方差估计。基于此表示，构建了一个时变Hankel-DMD提升线性预测器用于多步预测。残差分析提供方差跟踪信号。

Result: 该方法实现了稳定的方差感知去噪和短时预测，适用于集成到实时控制框架中。在具有高斯和重尾噪声的仿真以及动态起重机试验台上进行了验证。

Conclusion: 该方法能够实时学习非线性预测模型，有效地对智能体动态进行去噪和预测，并提供方差跟踪信号，可支持下游估计器和风险感知规划。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [348] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 该论文提出了一种名为D3P（Deep Koopman-boosted Dual-branch Diffusion Policy）的新算法，用于解决机器人模仿学习中扩散模型在处理时间依赖性和本体感受过拟合方面的不足。D3P采用双分支架构、动态切换机制和Deep Koopman算子模块，显著提高了模拟和真实世界机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的模仿学习方法在捕获多步骤的强时间依赖性方面表现不佳，尤其是在结合本体感受输入时。这种局限性可能导致策略过度拟合本体感受线索，而忽略视觉特征，从而导致任务失败。

Method: 该研究提出了D3P算法，其核心方法包括：1) 引入双分支架构，将不同感知模态组合的角色解耦：视觉分支编码视觉观察以指示任务进展，融合分支整合视觉和本体感受输入以进行精确操作。2) 当机器人未能完成中间目标时，策略可以动态切换到由视觉分支生成的动作块，以恢复到先前观察到的状态并重新尝试任务。3) 结合Deep Koopman算子模块，从视觉输入中捕获结构化的时间动态，以增强视觉表示学习。4) 在推理过程中，使用生成模型的测试时损失作为置信度信号，指导时间重叠的预测动作块的聚合，从而提高策略执行的可靠性。

Result: 在六项RLBench桌面任务的模拟实验中，D3P比现有最先进的扩散策略平均性能提高了14.6%。在三项真实世界机器人操作任务中，D3P实现了15.0%的改进。

Conclusion: D3P算法通过其独特的双分支架构、动态切换机制和Deep Koopman算子模块，成功克服了现有扩散策略在处理时间依赖性和本体感受过拟合方面的挑战。该方法显著提升了机器人模仿学习的性能和可靠性，在模拟和真实世界任务中均达到了最先进水平。

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [349] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 该研究提出了一种基于学习的双足步态框架，结合启发式步态规划和Raibert型控制器，实现了对复杂动力学模型的规避，并在不平坦地形上的鲁棒性和能效方面优于LIPM。


<details>
  <summary>Details</summary>
Motivation: 为了使人形机器人能够与环境进行精确交互，完成跨越障碍、接近目标等任务，同时避免使用复杂的步态规划器和分析模型。

Method: 该方法扩展了一个基于学习的双足步态框架，采用启发式步态规划策略，并由期望躯干速度跟踪引导。一个Raibert型控制器根据期望与实际躯干速度之间的误差来调整落脚点长度。该方法与基于模型的线性倒立摆模型（LIPM）控制器进行了比较。

Result: 实验结果表明，该方法在保持目标速度方面达到可比或更优的精度（高达80%），在不平坦地形上具有显著更高的鲁棒性（超过50%的改进），并提高了能源效率。

Conclusion: 研究结果表明，即使在非结构化环境中，为实现稳定和鲁棒的双足行走，训练架构中可能不需要整合复杂的分析性、基于模型的组件。

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [350] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: Maestro提出了一种新的通用机器人策略，通过VLM编码代理动态组合感知、规划和控制模块，以超越现有VLA模型的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 当前的通用机器人方法主要依赖于收集大量“观测-行动”数据集来训练大型端到端模型，效仿视觉语言模型（VLMs）的成功。本研究旨在探索一条不同的路径：直接围绕VLMs构建通用策略，通过将VLMs的通用能力与封装在精心策划的感知、规划和控制模块中的特定机器人能力相结合。

Method: Maestro系统使用一个VLM编码代理，根据当前任务和场景动态地将预先策划的感知、规划和控制模块组合成一个程序化的策略。其架构受益于简化的闭环接口和全面多样的工具库。

Result: Maestro在具有挑战性的操作技能上，其零样本性能大大超越了当前的视觉语言行动（VLA）模型。此外，Maestro易于扩展以整合新模块，易于编辑以适应新的机器人形态（如四足机器人上的机械臂），甚至可以通过局部代码编辑，从最少的真实世界经验中轻松适应。

Conclusion: 通过将VLM的通用能力与模块化机器人能力相结合，并由VLM编码代理动态编排，Maestro为通用机器人提供了一种高效且灵活的解决方案，在零样本性能和适应性方面表现出色。

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [351] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: 本文提出了Fast-SmartWay，一个端到端、零样本的连续环境视觉语言导航（VLN-CE）框架，它通过仅使用三个正面RGB-D图像和多模态大语言模型（MLLMs）直接预测动作，显著降低了延迟并提高了实时导航的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLN-CE方法通常依赖全景观测和包含路径点预测器的两阶段流程，这导致显著的延迟并限制了其在真实世界中的应用。

Method: Fast-SmartWay是一个端到端、零样本的VLN-CE框架。它不依赖全景视图和路径点预测器，而是使用三个正面RGB-D图像结合自然语言指令，使MLLMs能够直接预测动作。为了增强决策鲁棒性，该方法引入了一个不确定性感知推理模块，该模块集成了用于避免局部最优的消歧模块和用于全局连贯规划的未来-过去双向推理机制。

Result: 在模拟和真实机器人环境中的实验表明，Fast-SmartWay显著降低了每步延迟，同时实现了与基于全景视图的基线方法相比具有竞争力或更优的性能。

Conclusion: 这些结果证明了Fast-SmartWay在真实世界零样本具身导航中的实用性和有效性。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [352] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: URDF-Anything是一个基于3D多模态大语言模型（MLLM）的端到端框架，能自动重建铰接物体的数字孪生，同时优化几何分割和运动学参数预测，显著优于现有方法并具有出色的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为机器人仿真训练和具身AI世界模型构建精确的铰接物体数字孪生至关重要，但传统方法需要耗时的人工建模或多阶段流程。

Method: 本文提出了URDF-Anything，一个基于3D多模态大语言模型（MLLM）的端到端自动重建框架。它利用点云和文本多模态输入的自回归预测框架，共同优化几何分割和运动学参数预测。该方法实现了一个专门的`[SEG]`令牌机制，直接与点云特征交互，实现细粒度的部件级分割，同时与运动学参数预测保持一致性。

Result: 在仿真和真实世界数据集上的实验表明，该方法在几何分割（mIoU提高17%）、运动学参数预测（平均误差减少29%）和物理可执行性（超越基线50%）方面显著优于现有方法。此外，该方法表现出卓越的泛化能力，即使在训练集之外的物体上也能表现良好。

Conclusion: URDF-Anything为机器人仿真构建数字孪生提供了一种高效解决方案，显著增强了从仿真到现实的迁移能力。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [353] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: 本文提出GauDP，一种高斯图像协同表示，用于多智能体系统中的可扩展、感知感知的模仿学习，通过构建全局一致的3D高斯场并动态分配给局部视角，以平衡局部控制和全局环境感知。


<details>
  <summary>Details</summary>
Motivation: 现有具身多智能体系统在平衡个体视角与全局环境感知方面面临挑战，难以兼顾细粒度局部控制与全面场景理解，导致可扩展性受限和协作质量下降。

Method: GauDP方法首先从分散的RGB观测中构建一个全局一致的3D高斯场，然后将3D高斯属性动态重新分配到每个智能体的局部视角。这使得所有智能体都能在保持各自独立视点的同时，从共享场景表示中自适应地查询任务关键特征。

Result: GauDP在RoboFactory基准测试中，性能优于现有基于图像的方法，并接近点云驱动方法的有效性，同时在智能体数量增加时保持强大的可扩展性。

Conclusion: GauDP设计实现了细粒度控制和全局连贯行为的平衡，且无需额外的感知模态（如3D点云），有效解决了多智能体协作中的关键挑战，并展现出优越的性能和可扩展性。

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [354] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 本文提出了一种基于张量参数化降阶模型（PROM）的新型优化算法，旨在高效优化受复杂非线性力作用的软体结构，尤其适用于软体机器人设计。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术中，高效优化驱动软体结构，尤其是在复杂非线性力作用下，仍然是一个关键挑战。使用有限元方法（FEM）对非线性结构（如软体机器人）进行仿真时，计算资源需求巨大，尤其是在优化过程中。

Method: 本文提出了一种基于张量参数化降阶模型（PROM）的新型优化算法。该算法利用降维和解近似技术来高效解决非线性约束优化问题。其结构化的张量方法允许在特定选择的降阶基（ROB）内使用解析梯度，显著提高计算效率。在优化软体机器人游泳器形状时，将内部和外部非线性流体动力纳入优化过程，并使用无数据ROB进行快速准确的计算。

Result: 该方法成功应用于优化软体机器人游泳器形状，这些机器人受到流体动力作用，需处理内部和外部非线性力。结果表明，该方法不仅降低了计算复杂性，而且在处理软体机器人中复杂的非线性系统方面展现出高效和准确性。

Conclusion: 该优化方法有效降低了计算复杂性，为软体机器人中复杂非线性系统的优化设计和控制开辟了新途径，有助于实现更高效的设计和控制。

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [355] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: 本文通过协同设计感知与控制，提出了一种新型机器人超声系统（RUSS）框架，实现了对动态目标的高频实时跟踪与大范围重新定位，解决了现有系统端到端延迟的挑战。


<details>
  <summary>Details</summary>
Motivation: 机器人超声系统（RUSS）在面对大规模、高频干扰下的动态目标实时跟踪时，由于现有系统的端到端延迟，仍面临着一个关键的未解决挑战。

Method: 通过感知与控制的协同设计，提出了一个紧密耦合的框架：1) 解耦双流感知网络，用于从2D图像中高频鲁棒地估计3D平移状态；2) 单步流策略，通过一次推理生成完整的动作序列，避免了传统策略的迭代瓶颈。这种协同作用实现了超过60Hz的闭环控制频率。

Result: 在动态体模上，系统以低于6.5mm的平均误差跟踪复杂的3D轨迹，并能从超过170mm的位移中鲁棒地重新捕获目标。它还能以102mm/s的速度跟踪目标，终端误差低于1.7mm。此外，在人体志愿者上进行的体内实验验证了该框架在实际临床环境中的有效性和鲁棒性。

Conclusion: 该研究提出了一个整体架构的RUSS，将高带宽跟踪与大范围重新定位相结合，是实现动态临床环境中鲁棒自主性的关键一步。

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [356] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 本文提出了一种名为SPAR-H的人机协作学习方法，通过结合直接偏好优化和基于奖励的路径，实现了无人机在河流导航中高效、数据驱动的在线适应，解决了模拟训练策略在实际部署中的分布偏移和安全风险问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在河流环境监测和灾害响应中具有快速、低成本的优势。然而，模拟训练的策略在实际部署时会面临分布偏移和安全风险，需要有限的人工干预进行高效适应。

Method: 研究引入了人机协作学习（HITL），其中保守的监督者否决不安全或低效的行为，并通过比较智能体的提议和纠正性覆盖来提供状态偏好。提出Statewise Hybrid Preference Alignment for Robotics (SPAR-H) 方法，该方法将策略logits上的直接偏好优化与基于奖励的路径相结合，从相同偏好中训练即时奖励估计器，并使用信任域代理更新策略。

Result: 通过从固定新手策略收集的五次HITL推演，SPAR-H在测试方法中实现了最高的最终情节奖励和最低的初始条件方差。学习到的奖励模型与人类偏好行为一致，并提升了附近未干预的选择，支持改进的稳定传播。SPAR-H在HITL设置中优于模仿学习（IL）、直接偏好变体和评估性强化学习（RL），并展示了无人机河流跟踪中持续偏好对齐的实际可行性。

Conclusion: 双重状态偏好在经验上为河流导航中的数据高效在线适应提供了一条实用途径。

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [357] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: 本文提出SLAP方法，通过利用现有TAMP选项并结合无模型强化学习，自动发现新的抽象动作（捷径），从而显著缩短规划长度并提高机器人长时序决策的成功率。


<details>
  <summary>Details</summary>
Motivation: 长时序决策、稀疏奖励以及连续状态和动作是AI和机器人领域面临的根本挑战。任务与运动规划（TAMP）通过分层规划解决此问题，但其抽象动作（选项）需要手动定义，限制了智能体只能执行人类工程师已知如何编程的行为。

Method: 本文提出了“抽象规划捷径学习”（SLAP）方法。其核心思想是利用无模型强化学习（RL）来学习由TAMP中现有选项所产生的抽象规划图中的捷径。SLAP无需额外假设或输入，即可自动发现新的抽象动作。

Result: SLAP方法能生成比纯规划更短的解决方案，并获得比扁平式和分层式强化学习更高的任务成功率。它能发现与手动定义动作显著不同的动态物理即兴动作（例如，拍打、扭动、擦拭）。在四种模拟机器人环境中的实验表明，SLAP能解决并泛化到各种任务，将总体规划长度缩短50%以上，并持续优于规划和强化学习基线。

Conclusion: SLAP通过利用现有TAMP选项并结合强化学习，成功自动发现了新的、高效的抽象动作，显著提升了TAMP在复杂机器人任务中的性能，缩短了规划长度并提高了任务成功率。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [358] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: 本研究提出了一种增强的本体感知方法，通过融合IMU和弯曲传感器，利用卡尔曼滤波器和分段常曲率模型，实现了软机器人长期、精确且经济高效的形状估计。


<details>
  <summary>Details</summary>
Motivation: 软机器人的精确形状估计是一个挑战，尤其是在长期运行和成本效益方面。传统的IMU传感器存在漂移问题，影响了长期本体感知的可靠性。

Method: 该研究将惯性测量单元（IMU）与互补的弯曲传感器集成。通过卡尔曼滤波器融合来自两种传感器的分段尖端姿态数据，以相互补偿的方式减轻IMU漂移。然后，使用分段常曲率模型从融合的姿态数据中估计尖端位置并重建机器人的变形。

Result: 在无载荷、外部力和被动障碍物相互作用下，连续运行45分钟的实验表明，均方根误差为16.96毫米（总长度的2.91%），与仅使用IMU的基准方法相比，误差降低了56%。

Conclusion: 该方法不仅能实现软机器人的长时间本体感知，而且在多种不同条件下保持了高精度和鲁棒性，证明了其在软机器人形状估计方面的有效性和实用性。

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [359] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: 该研究提出通过将不同形态表示为3D粒子集，并将动作定义为粒子位移，构建一个统一的、与本体无关的世界模型。这个模型能够实现跨形态学习，促进数据共享和策略迁移，从而使通用机器人能够在不同形态上操作。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要在不同形态上操作，但不同的动作空间和运动学特性阻碍了数据共享和策略迁移。核心问题是：是否存在允许动作在不同形态之间转移的不变性？

Method: 研究假设环境动力学与本体无关，并认为捕获这些动力学的世界模型可以提供统一的跨本体接口。为此，他们将不同本体（如人手和机器手）表示为3D粒子集，并将动作定义为粒子位移，从而创建了一个异构数据和控制问题的共享表示。然后，他们使用图基世界模型，在来自各种模拟机器手和真实人手的探索数据上进行训练，并将其与基于模型的规划相结合，部署到新型硬件上。

Result: 实验结果揭示了三点发现：(i) 增加训练本体的数量可以提高对未见本体的泛化能力；(ii) 在模拟和真实数据上共同训练的效果优于单独训练；(iii) 所学模型能够有效控制具有不同自由度的机器人，完成刚性和可变形操作任务。

Conclusion: 这些结果表明，世界模型是实现跨本体灵巧操作的一个有前景的接口。

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [360] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 本文提出LiDAR-VGGT框架，通过两阶段粗到精的融合管道，将激光雷达惯性里程计与VGGT模型紧密结合，用于重建大规模彩色点云，克服了现有LIVO对标定敏感和VGGT模型可扩展性及度量尺度不足的局限性。


<details>
  <summary>Details</summary>
Motivation: 大规模彩色点云重建对机器人感知、导航和场景理解至关重要。现有LIVO方法性能对外部标定高度敏感，而3D视觉基础模型（如VGGT）在大环境中可扩展性有限且缺乏度量尺度。本研究旨在克服这些限制。

Method: LiDAR-VGGT框架采用两阶段粗到精的融合管道：首先，预融合模块通过鲁棒初始化精炼，在每个会话中高效估计VGGT姿态和具有粗略度量尺度的点云；其次，后融合模块通过基于边界框的正则化增强跨模态3D相似变换，以减少激光雷达和相机传感器视野不一致引起的尺度畸变。

Result: 在多个数据集上的大量实验表明，LiDAR-VGGT能够生成密集、全局一致的彩色点云，并且优于基于VGGT的方法和LIVO基线。此外，他们将开源提出的新型彩色点云评估工具包。

Conclusion: LiDAR-VGGT通过创新性地融合激光雷达惯性里程计和VGGT模型，成功解决了大规模彩色点云重建中的关键挑战，实现了高精度、全局一致的点云重建，并提供了开源评估工具，对机器人领域具有重要意义。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [361] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 本文提出了一种可转向的球囊式心内窥镜，通过单一的球囊充气压力独立控制视野直径和弯曲角度，实现心脏内部的直接光学可视化和精确工具导航，以支持经导管手术。


<details>
  <summary>Details</summary>
Motivation: 为了从开放式心脏手术转向更安全的经导管手术，需要改进成像技术和机器人解决方案，以实现简单、精确的工具导航。传统的成像方式如荧光透视和超声波存在局限性，而心内窥镜（即心脏内部的直接光学可视化）可以克服这些限制。

Method: 该研究设计了一种可转向的球囊式心内窥镜。球囊在血管中可收缩通过，进入心脏后可充气膨胀以进行可视化，并通过集成的操作通道输送工具。通过精心设计球囊壁厚，利用单一输入（球囊充气压力）独立控制两个输出（球囊直径/视野直径和球囊弯曲角度/操作通道定位）。研究还展示了基于图像的弯曲角度闭环控制，以实现工具插入和移除过程中的稳定方向控制。

Result: 研究展示了一种可根据各种心内任务进行调整的球囊技术，可以设计出不同用途的心内窥镜。具体展示了一个用于主动脉瓣叶撕裂任务的球囊设计。此外，还实现了基于图像的弯曲角度闭环控制，确保了工具插入和移除过程中的稳定方向控制。

Conclusion: 所提出的可转向球囊式心内窥镜为经导管手术提供了一种新的解决方案，通过直接光学可视化和精确的工具定位，克服了现有成像技术的局限性，有望提高经导管手术的安全性和准确性。

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [362] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: 本文提出了一种被动式2D全局重定位框架，用于解决“被绑架机器人问题”（KRP），即在无先验位姿估计的情况下，利用单次激光雷达扫描和占据栅格地图进行高效可靠的全局位姿估计。


<details>
  <summary>Details</summary>
Motivation: 在机器人定位丢失或SLAM初始化时，机器人需要在已知地图中进行重定位，但缺乏先验位姿估计。解决这一“被绑架机器人问题”（KRP）对于增强移动机器人的长期自主性至关重要。

Method: 该框架将全局重定位视为非凸问题，并通过多假设方案结合批量多阶段推理和提前终止来解决。它利用RRT在可遍历约束下渐近覆盖可达空间，生成稀疏、均匀分布的位姿假设。提出Scan Mean Absolute Difference (SMAD) 对假设进行初步排序以实现提前终止。还提出Translation-Affinity Scan-to-Map Alignment Metric (TAM) 用于在假设位置进行可靠的方向选择和精确的最终位姿评估。

Result: 在资源受限的移动机器人上进行的真实世界实验表明，所提出的框架在全局重定位成功率和计算效率方面均优于现有方法。

Conclusion: 该被动式2D全局重定位框架能够高效可靠地从单次激光雷达扫描和占据栅格地图中估计全局位姿，有效解决了“被绑架机器人问题”，提升了移动机器人的长期自主性。

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [363] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: 本文提出ET-VLA框架，通过合成数据持续预训练（SCP）和具身思维图（Embodied Graph-of-Thought）技术，将预训练的视觉-语言-动作（VLA）模型高效迁移到多机器人（特别是双臂机器人）协作任务中，显著提升了真实世界任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归VLA模型在多机器人协作方面表现不佳，难以有效处理跨具身数据和精细化控制，且需要大量的真实世界演示数据。

Method: 该研究引入了ET-VLA框架，其核心是合成持续预训练（SCP），利用合成数据预热模型以适应新具身，避免了对真实人类演示的需求。SCP使模型能学习正确的动作和精确的动作令牌数量。之后，模型在目标具身数据上进行微调。为进一步提升多具身性能，提出了具身思维图（Embodied Graph-of-Thought）技术，将每个子任务建模为节点，使VLA模型在任务执行中能区分每个具身的功能和角色。方法在双臂机器人（作为多机器人的简化版本）上进行了验证。

Result: ET-VLA方法在仿真基准和涵盖三种不同双臂具身的真实机器人上都得到了验证。实验结果表明，ET-VLA在六个真实世界任务上比OpenVLA的性能提升超过53.2%。

Conclusion: ET-VLA框架通过合成持续预训练和具身思维图技术，成功且高效地将预训练VLA模型迁移到多机器人协作任务中，特别是在双臂机器人上取得了显著的性能提升，有效解决了现有VLA模型在多机器人协作方面的挑战。

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [364] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: 本文设计了一种新型球形机器人结构，通过引入动量轮，实现了高达10米/秒的稳定高速运动，并显著提升了越障和地形适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有单摆驱动球形机器人难以实现高速运动，且在越障和地形鲁棒性方面有待提升，促使研究者寻求新的结构设计。

Method: 在单摆驱动球形机器人的基础上，引入了一个动量轮，其轴线与次级摆对齐，形成了一种新颖的球形机器人结构。通过简单的解耦控制进行驱动。

Result: 物理原型实验证明，该新型球形机器人能够实现稳定的高速运动（高达10米/秒），这是原有结构无法达到的。同时，它显著提升了越障性能和地形鲁棒性。

Conclusion: 所设计的新型球形机器人结构不仅大幅提高了运动速度，而且通过简单的控制实现了稳定运行，并增强了机器人在复杂地形下的适应能力和越障表现。

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [365] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 本文设计并制造了一种新型眼科手术机器人系统，显著提高了工具尖端精度、跟踪性能和器械更换的顺畅性，并在OCT引导的白内障晶状体摘除术中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的用于白内障和玻璃体视网膜手术的机器人系统在器械操作的精度、准确性和自由度方面，以及在手术过程中自动工具更换方面，仍存在不足。

Method: 研究人员设计并制造了一个新的机器人系统，该系统侧重于提高工具尖端精度、跟踪性能和器械更换机制。他们使用光学相干断层扫描（OCT）系统外部评估了工具尖端精度、准确性以及通过运动远程中心保持小切口的能力。通过机器人校准和精确坐标配准，测量了工具尖端定位精度，并在结合深度学习术前解剖建模和实时监督的OCT引导的自动化白内障晶状体摘除术中展示了其整体性能。

Result: 该机器人系统的工具尖端定位精度为0.053±0.031毫米。其整体性能在OCT引导的自动化白内障晶状体摘除手术中得到了成功验证。

Conclusion: 该新型机器人系统有效提升了眼科手术的工具尖端精度和操作性能，并通过自动化白内障晶状体摘除程序展示了其作为高精度手术平台的潜力，克服了现有技术的局限性。

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [366] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: 本文提出了一种名为SATPlanner的语义代理，由大型语言模型（LLM）驱动，用于在未知环境中为球形张拉整体机器人进行路径规划。它利用自适应观察窗口机制，显著减少了搜索空间，提高了规划效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 球形张拉整体机器人在未知环境中的路径规划面临挑战。传统的路径规划器将环境视为几何网格，常因缺乏语义理解而导致冗余搜索并在复杂场景中失效。

Method: 将未知环境中的路径规划重构为语义推理任务。引入了由大型语言模型（LLM）驱动的语义代理SATPlanner。SATPlanner的核心是一个自适应观察窗口机制，灵感来源于LLM的“快”和“慢”思维模式，能动态调整感知范围。这使得代理能够构建环境的语义信念，并使搜索空间仅随路径长度线性增长（O(L)）。

Result: 在1,000次模拟试验中，SATPlanner实现了100%的成功率，优于其他实时规划算法。与A*算法相比，SATPlanner将搜索空间减少了37.2%，同时实现了可比的、接近最优的路径长度。该方法还在物理球形张拉整体机器人原型上验证了其可行性。

Conclusion: SATPlanner通过结合LLM的语义理解和自适应观察窗口，为球形张拉整体机器人在未知环境中的路径规划提供了一种高效、鲁棒且实用的解决方案，显著提升了规划性能并降低了计算成本。

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [367] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 该研究提出了一种结合折纸结构和针织面料的新型设计与制造方法，通过编程针脚和材料模式，利用热熔纱实现可编程刚度和柔性，为可穿戴软机器人提供了结构可重构性。


<details>
  <summary>Details</summary>
Motivation: 软机器人应用于可穿戴设备时，需要兼顾舒适性和安全性，但同时实现结构完整性和柔顺性是一个重大挑战。

Method: 研究引入了一种通用设计方法，将折纸图案转化为针织设计，通过编程针脚和材料图案来控制折叠方向。通过选择性地加入热熔纱，在柔顺折痕周围创建刚性面板，以实现特定方向的折叠并抑制不必要的弯曲和屈曲。

Result: 实验量化了折叠力矩，表明针脚图案增强了折叠方向性。热熔纱（1）通过减少边缘卷曲保持了几何形状的一致性，（2）通过硬化面板防止了平面外变形。成功再现了Miura-ori、Yoshimura和Kresling等复杂的折纸图案，并展示了一个可移动的可穿戴针织Kaleidocycle机器人。

Conclusion: 结合结构可重构性、材料可编程性和制造可扩展性，针织折纸被认为是一个有前景的下一代可穿戴机器人平台。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [368] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: 本文介绍了一种名为MOBIUS的多模态双足智能城市侦察机器人，它能行走、爬行、攀爬和滚动，通过形态、高层规划和控制的紧密集成，实现了多样化的移动操作和抓取能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够在无需重新配置的情况下，跨越不同地形，并具有扩展交互能力、工作空间和可通行性的机器人，以应对复杂的城市环境。

Method: 该机器人采用四肢设计（两个6自由度带夹持器的手臂和两个4自由度腿）。控制架构是混合式的，结合了基于强化学习的运动控制、基于模型的预测和导纳控制（通过Reference Governor增强安全性），以实现顺从的接触交互。高层规划器使用MIQCP自主选择运动模式，以平衡稳定性和能源效率。

Result: 硬件实验展示了鲁棒的步态转换、动态攀爬以及通过捏合抓取实现全身负载支撑。这些成果证明了形态、高层规划和控制之间紧密集成的重要性。

Conclusion: 研究得出结论，形态、高层规划和控制的紧密集成对于实现移动式运动操作和抓取至关重要，能够显著扩展机器人的交互能力、工作空间和可通行性。

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [369] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 该研究提出一种迭代学习控制(ILC)策略，用于在机器人手术中实现精确的旋转插入，通过克服系统误差提高穿透效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 机器人工具路径的精确控制常受系统固有未对准、未建模动力学和执行器不精确的挑战；特别是机器人第四关节的未对准使简单的针头旋转复杂化，促使研究者寻求一种迭代学习控制方法。

Method: 采用迭代学习控制(ILC)策略。首先校准所选手术工具的正向运动学以提高精度，然后通过光学相干断层扫描(OCT)测量误差，并根据位置反馈迭代调整关节指令以优化控制输入。

Result: 在离体猪眼上进行的视网膜下注射任务实验表明，优化后的轨迹比直线插入具有更高的组织穿透和视网膜下注射成功率。

Conclusion: 该研究证明了ILC在克服系统未对准挑战方面的有效性，并为其他需要精确控制插入的高精度机器人任务提供了潜在应用。

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [370] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: 本文提出了一种基于迁移的灵巧抓取生成框架，利用条件扩散模型将高质量抓取从形状模板迁移到同类别新物体上，有效平衡了抓取质量、生成效率和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的灵巧抓取生成方法存在局限性：分析方法虽稳定但效率低且缺乏任务适应性；生成方法效率高且易于任务集成，但因数据限制而对未见物体和任务泛化能力差。

Method: 该研究提出一个基于条件扩散模型的迁移框架：将抓取迁移问题重新定义为物体接触图生成，并融入物体形状相似性和任务规范；引入双重映射机制处理复杂形状变化；除了接触图，还推导了部位图和方向图以编码更精细的接触细节；开发了级联条件扩散模型框架来联合迁移这三张图，确保其内部一致性；最后，引入鲁棒的抓取恢复机制以识别可靠接触点并优化抓取配置。

Result: 广泛的实验证明了该方法的优越性，其方法有效地平衡了抓取质量、生成效率和跨各种任务的泛化性能。

Conclusion: 该研究成功地提出了一种新颖的灵巧抓取生成方法，通过结合迁移学习和条件扩散模型，克服了传统方法的局限性，在抓取质量、效率和泛化能力之间取得了有效平衡。

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [371] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: 本文提出了Kinematify，一个自动化框架，能直接从任意RGB图像或文本提示中合成可动对象，解决了高自由度对象运动学拓扑推断和关节参数估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 理解运动学结构和可动部件对于机器人操作和建模至关重要，但为复杂系统（如高自由度机器人或物体）创建可动对象模型极具挑战性。现有方法通常依赖运动序列或手工数据集的强假设，限制了可扩展性。

Method: Kinematify框架结合了蒙特卡洛树搜索（MCTS）进行结构推断，并利用几何驱动优化进行关节推理，以从静态几何中估计关节参数，从而生成物理上一致且功能有效的描述。

Result: Kinematify在合成和真实世界环境中的多样化输入上进行了评估，结果表明其在配准和运动学拓扑精度方面优于现有工作，能够生成物理上一致且功能有效的描述。

Conclusion: Kinematify提供了一个自动化的框架，能够直接从图像或文本提示中合成可动对象，有效解决了高自由度对象的运动学拓扑推断和关节参数估计的难题，提高了模型创建的准确性和可扩展性。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [372] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出RobustVLA，一种轻量级在线强化学习后训练方法，通过雅可比正则化和平滑正则化，显著提升了预训练VLA模型在机器人操作中对观测噪声和动作扰动的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: VLA模型虽功能强大，但在实际部署中面对观测噪声、传感器误差或执行扰动等分布外干扰时泛化能力差。现有基于RL的后训练方法主要侧重奖励最大化，而忽略了环境不确定性下的鲁棒性。

Method: 引入RobustVLA，一种轻量级在线RL后训练方法，旨在明确增强VLA模型的弹性。通过系统性鲁棒性分析，识别并应用了两种关键正则化：1) 雅可比正则化，以减轻对观测噪声的敏感性；2) 平滑正则化，以稳定动作扰动下的策略。

Result: 在多样化的机器人环境中进行的广泛实验表明，RobustVLA在鲁棒性和可靠性方面显著优于现有最先进的方法。

Conclusion: 研究结果强调了以原则性、鲁棒性感知的方式进行RL后训练是提高VLA模型可靠性和鲁棒性的关键一步。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [373] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 该研究开发了一种新型人造捕蝇草（AVF），利用热响应形状记忆材料，首次实现了自主的闭合和重新打开双向运动。


<details>
  <summary>Details</summary>
Motivation: 捕蝇草的快速闭合运动激发了软机器人的发展。然而，现有的人造捕蝇草系统通常只能实现闭合，未能同时实现自主的闭合和重新打开运动，这限制了其在仿生软机器中的应用。

Method: 研究人员采用新型热响应紫外光固化形状记忆材料，构建了人造捕蝇草。其中，双曲面捕食叶片由形状记忆聚合物制成，而形状记忆弹性体条被用作拮抗执行器，以促进叶片的重新打开。

Result: 所开发的真人大小热响应人造捕蝇草能够在自然温度范围内自主响应：在38°C时闭合，并在45°C左右开始重新打开。这是首次展示了人造捕蝇草在温度升高时，通过编程顺序运动实现热响应的闭合和重新打开。

Conclusion: 这项工作代表了在实现自主双向运动软机器/机器人方面迈出了重要一步，为未来开发更复杂的仿生软机器人系统奠定了基础。

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [374] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 该研究深入理解捕蝇草的运动力学，并将其双稳态原理应用于设计和3D打印人工双稳态叶片执行器，以期开发仿生快速软抓手。


<details>
  <summary>Details</summary>
Motivation: 捕蝇草快速闭合的叶片陷阱长期以来吸引着植物学家和工程师。其从凹形到凸形的快速转变，由膨压变化和弹性储能释放驱动，使其成为一个具有两个低能态的双稳态系统。研究旨在深化对捕蝇草运动力学的理解，并将其原理应用于人工执行器设计。

Method: 研究识别了捕蝇草叶片的几何特征，如尺寸比和叶片厚度梯度。将这些特征应用于两个3D打印的双稳态执行器模型：一个模拟捕蝇草叶片的几何形状，另一个是CAD设计的叶片模型。

Result: 这两个3D打印模型都表现出凹凸双稳态并能快速闭合，成功模仿了生物模型的机械行为。

Conclusion: 这些演示器是开发模仿生物模型机械行为的人工捕蝇草的第一步，未来可作为一种柔软、快速的抓手使用。

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [375] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本文提出了一种无电子元件、受蚯蚓启发的软体机器人，利用改进的气动逻辑门（PLG）实现蠕动运动，降低了系统复杂性并保持了高效驱动。


<details>
  <summary>Details</summary>
Motivation: 现有受蚯蚓启发的机器人虽然采用气动驱动，但通常依赖笨重且耗电的电子控制单元，这限制了它们在实际应用中的实用性。

Method: 研究人员通过将预配置的改进气动逻辑门（PLG）单元与波纹管执行器集成，构建了一个即插即用的模块化系统。该系统无需外部电子元件即可实现蠕动运动。论文中还对波纹管执行器在不同操作条件下进行了特性分析，并评估了机器人的运动性能。

Result: 研究结果表明，基于改进PLG的控制系统能够有效生成蠕动波传播，实现了自主运动且偏差极小。这证明了无电子元件蠕动软体机器人的概念可行性。

Conclusion: 本研究为开发无电子元件的蠕动软体机器人提供了概念验证，所提出的系统在需要无缆、适应性强的运动的危险环境中具有潜在应用价值。未来的工作将集中于进一步优化机器人设计和探索使用车载压缩空气源进行无缆操作。

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [376] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: 本文提出了一种名为 $E^{3}AD$ 的新范式，通过视觉特征提取网络与通用EEG大模型之间的对比学习来融入人类驾驶认知，从而显著提升端到端自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉端到端自动驾驶方法依赖于标签监督下的视觉特征提取网络，这限制了驾驶模型的通用性和适用性。研究旨在通过学习人类潜在的驾驶认知来增强端到端规划。

Method: 研究提出了 $E^{3}AD$ 范式，通过视觉特征提取网络与通用EEG大模型进行对比学习，以学习潜在的人类驾驶认知。为此，收集了一个认知数据集。随后，在流行的自动驾驶模型（作为基线）和公开数据集中，通过开环和闭环测试，研究了增强端到端规划的方法和潜在机制。还进行了消融研究。

Result: $E^{3}AD$ 范式显著提升了基线模型的端到端规划性能。消融研究进一步验证了驾驶认知的贡献和对比学习过程的有效性。

Conclusion: 这是首次将人类驾驶认知整合到端到端自动驾驶规划中的工作，代表了将具身认知数据融入端到端自动驾驶的初步尝试，为未来类脑自动驾驶系统提供了宝贵的见解。

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [377] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 本文提出了一种新的横向速度模型，通过分析真实泊车数据，解决了传统零滑移模型在低速泊车场景下的局限性，从而提高了自动泊车的定位精度。


<details>
  <summary>Details</summary>
Motivation: 自动泊车需要精确的定位，尤其是在狭窄空间内的横向速度估计。消费级车辆缺乏专用的横向速度传感器，现有方法依赖于简化模型（如零滑移模型），但在低速行驶时该假设不成立，导致估计不准确。

Method: 研究人员分析了真实世界的泊车场景数据，识别出与零滑移假设的系统性偏差，并解释了观察到的效应。在此基础上，他们提出了一种新的横向速度模型，以更好地捕捉车辆在泊车时的横向动态。

Result: 提出的模型能够更好地捕捉车辆的横向动态，提高了横向速度估计的准确性。该模型仅依赖于两个参数，使其非常适合集成到消费级应用中。

Conclusion: 通过分析真实数据并提出一个参数简洁的新模型，研究有效地解决了自动泊车中横向速度估计的挑战，显著提升了估计精度，并具有良好的实用性。

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [378] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种名为CM-LIUW-Odometry的多模态SLAM框架，专为地下煤矿环境设计，通过紧密融合LiDAR、IMU、UWB和轮式里程计，并结合IESKF、非完整性约束和自适应运动模式切换，实现了高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 地下煤矿环境的SLAM面临巨大挑战：GPS信号缺失导致无法重建场景和进行绝对地理参考；崎岖地形降低轮式里程计精度；以及特征稀疏的隧道削弱了LiDAR的有效性。

Method: 该研究提出了基于迭代误差状态卡尔曼滤波器（IESKF）的多模态SLAM框架CM-LIUW-Odometry。首先，将LiDAR-惯性里程计与UWB绝对定位约束紧密融合，以实现与全局坐标系的对齐。其次，通过紧密耦合集成轮式里程计，并辅以非完整性约束（NHC）和车辆杠杆臂补偿，以解决UWB测量范围之外区域的性能下降问题。最后，引入自适应运动模式切换机制，根据UWB测量范围和环境退化程度动态调整机器人运动模式。

Result: 实验结果验证了该方法在真实的地下煤矿场景中实现了卓越的精度和鲁棒性，优于现有最先进的方法。

Conclusion: 所提出的CM-LIUW-Odometry框架有效解决了地下煤矿环境中的SLAM挑战，提供了高精度和高鲁棒性的定位与建图解决方案。

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [379] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: 本文提出了一种名为CaRLi-V的新型RADAR、LiDAR和相机融合管道，用于精确的点云三维速度估计，以支持机器人在动态环境中的交互。


<details>
  <summary>Details</summary>
Motivation: 在机器人与非刚性、动态物体（如人类）交互时，准确的三维点云速度估计至关重要，这能增强路径规划、碰撞避免和物体操作的鲁棒性。

Method: CaRLi-V管道利用原始RADAR测量数据创建了一种新颖的RADAR表示——速度立方体，用于密集表示径向速度。结合速度立方体提取径向速度、光流估计切向速度以及LiDAR提供点云距离测量，通过闭式解计算出稠密点的三维速度估计。

Result: 该方法能够为密集的点阵列生成三维速度估计。经过自定义数据集的实地测试，CaRLi-V相对于真实值产生了较低的速度误差指标，证实了其在机器人应用中进行点云速度估计的有效性。

Conclusion: CaRLi-V作为一种开源ROS2包，通过多传感器融合实现了精确的点云三维速度估计，为机器人应用提供了关键能力，并在实际测试中表现出低误差。

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [380] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: 本文提出了FoldPath，一种端到端的基于神经场的方法，用于生成连续、平滑的机器人运动，解决了现有方法对离散路径后处理的依赖。


<details>
  <summary>Details</summary>
Motivation: 自动化制造（如喷漆和焊接）需要高精度、物体感知的机器人运动生成。现有方法要么是启发式的，要么是基于学习但依赖敏感的后处理步骤来生成可执行路径，难以实现有效自动化。

Method: 引入了FoldPath，一种新颖的、端到端的、基于神经场（neural field）的物体中心运动生成（OCMG）方法。它将机器人运动学习为连续函数，而非离散的末端执行器路径点序列，从而隐式编码平滑的输出路径，并消除了对易出错的后处理步骤的需求。

Result: FoldPath在预测性能上优于最近提出的学习方法，即使在仅有70个专家样本的真实工业环境中，也展现出泛化能力。通过在模拟环境中进行综合实验，并引入新的、严格的指标来评估长周期机器人路径，验证了其有效性。

Conclusion: FoldPath通过提供鲁棒的、连续的运动生成，且无需脆弱的后处理步骤，推动了OCMG任务向实际成熟迈进，有望在自动化制造中发挥重要作用。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [381] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了MoonBot平台（一个模块化空间机器人系统）的软件架构和部署策略，旨在通过分布式、异构模块化方法解决模块化机器人面临的集成和维护挑战。


<details>
  <summary>Details</summary>
Motivation: 模块化机器人系统在物理重构之外，还面临软件、通信和编排方面的分布式、异构模块化挑战，且集成和维护成本高昂。研究旨在克服这些重大障碍。

Method: 研究采用了一种分布式、异构模块化的原则性方法，扩展了模块化机器人概念。具体方法包括：组件化设计、基于ROS2和Zenoh的数据导向通信模型，以及一个能够管理复杂多模块装配的部署编排器。核心是开源的Motion Stack软件。

Result: 所提出的架构实现了动态重构、去中心化控制以及操作员和模块之间的无缝协作。该系统通过数月的现场部署（包括自组装机器人、机器人间协作和远程操作）得到验证，显著降低了集成和维护开销，同时保持了可扩展性和鲁棒性。

Conclusion: 该架构成功解决了模块化机器人面临的重大难题，并为设计可在时间、硬件、团队和操作环境之间扩展的通用机器人系统提供了可推广的模式。

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [382] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM 是一种新框架，通过将高层推理与低层控制分离，使预训练的视觉-语言模型（VLMs）能够安全可靠地应用于空中机械臂操作，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在机器人控制中潜力巨大，但直接将其部署到空中机械臂上存在不安全和不可靠的问题，因为生成的动作常不一致、易产生幻觉且动态上不可行。

Method: AERMANI-VLM 框架通过将自然语言指令、任务上下文和安全约束编码成结构化提示，引导 VLM 生成逐步的自然语言推理轨迹。然后，该推理结果用于从预定义的离散、飞行安全技能库中选择动作，从而实现可解释且时间上一致的执行。该方法将符号推理与物理动作解耦，且无需任务特定的微调。

Result: 该框架成功缓解了幻觉指令并防止了不安全行为，实现了鲁棒的任务完成。在模拟和硬件上对多步抓取-放置任务进行了验证，展示了对未见过的指令、物体和环境的强大泛化能力。

Conclusion: AERMANI-VLM 首次实现了将预训练 VLM 适应于空中机械臂操作，通过解耦高层推理与低层控制，确保了安全、可靠、可解释且具有强大泛化能力的任务执行。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [383] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 本文提出MO-SeGMan，一个多目标序列化引导操作规划器，用于高度受限的重排问题，通过优化重规划和机器人行程，并引入选择性引导前向搜索和自适应子目标选择，显著提升了规划效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决高度受限的重排问题，特别是杂乱、非单调的场景，这些场景需要高效地处理物体依赖性、最小化机器人动作并生成可行的运动规划。

Method: MO-SeGMan作为多目标序列化引导操作规划器，旨在最小化每个物体的重新规划次数和机器人行程距离，同时通过惰性评估保持关键依赖结构。它引入了选择性引导前向搜索（SGFS）来高效重新定位关键障碍物，并采用细化方法进行自适应子目标选择，以消除不必要的抓取-放置动作。

Result: 在九个基准重排任务上的广泛评估表明，MO-SeGMan在所有情况下都能生成可行的运动规划，并且与基线相比，始终实现更快的求解时间和更优的解决方案质量。

Conclusion: MO-SeGMan框架在解决复杂重排规划问题方面展现出卓越的鲁棒性和可扩展性。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [384] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 本文提出了一种受人类启发、基于物理条件的触觉方法（Phy-Tac），用于实现力最优的稳定抓取，它整合了姿态选择、触觉预测和力调节，以实现高效且自适应的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取通常依赖于僵硬、过度挤压的控制，而人类则以最小的必要力实现稳定抓取。研究旨在缩小机器人与人类抓取之间的这一差距，使机器人能够以更接近人类的方式进行力最优的稳定抓取。

Method: 该方法（Phy-Tac）包含三个主要部分：1. 基于物理的姿态选择器，根据表面几何识别具有最优力分布的接触区域。2. 基于物理条件的潜在扩散模型（Phy-LDM），预测力最优稳定抓取目标下的触觉印记。3. 潜在空间LQR控制器，以最小的驱动力将抓手驱动至预测的触觉印记，避免不必要的压缩。该模型在一个包含多样物体和接触条件的物理条件触觉数据集上进行训练。

Result: 所提出的Phy-LDM在触觉预测精度上表现出色。Phy-Tac在抓取稳定性和力效率方面优于固定力及基于GraspNet的基线方法。在经典机器人平台上的实验证明了其力高效和自适应的操作能力。

Conclusion: 该研究通过实现力高效和自适应的抓取操作，成功缩小了机器人与人类抓取之间的差距，使机器人能够以更接近人类的方式进行抓取。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [385] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: 本文提出MARS，一个由多模态大语言模型（MLLMs）驱动的多智能体机器人系统，旨在为智能家居中的残障人士提供风险感知、个性化和可执行的辅助智能。


<details>
  <summary>Details</summary>
Motivation: 现有的智能辅助系统在风险感知规划、用户个性化和将语言计划转化为可执行技能方面存在困难，尤其是在杂乱的家庭环境中。

Method: 引入MARS系统，该系统集成四个智能体：视觉感知智能体（提取语义和空间特征）、风险评估智能体（识别和优先处理危险）、规划智能体（生成可执行动作序列）和评估智能体（迭代优化）。该框架结合了多模态感知和分层多智能体决策。

Result: 在多个数据集上的实验表明，与最先进的多模态模型相比，MARS系统在风险感知规划和协调多智能体执行方面表现出卓越的整体性能。

Conclusion: 该方法突出了协作AI在实际辅助场景中的潜力，并为在真实世界环境中部署由MLLM驱动的多智能体系统提供了一种通用方法。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [386] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: GlocDiff是一种新型的扩散策略，它通过整合平面图的全局规划和RGB观测的局部深度感知特征，解决了室内导航中RGB输入与平面图之间的模态鸿沟和定位挑战，并在基准测试和实际部署中实现了卓越的导航性能。


<details>
  <summary>Details</summary>
Motivation: 1. 现有方法在基于RGB输入和平面图的室内导航中存在模态鸿沟，阻碍了视觉和空间信息的整合，影响局部避障和全局规划。2. 在未知环境中部署时，由于RGB输入和平面图之间缺乏明确的几何对齐，准确的定位仍然具有挑战性，而定位对导航性能至关重要。

Method: 本文提出了一种名为GlocDiff的扩散策略。该策略将平面图的全局路径规划与从RGB观测中提取的局部深度感知特征相结合。平面图提供明确的全局引导，深度特征提供隐式几何线索，共同实现最佳导航方向的精确预测和鲁棒的避障。此外，GlocDiff在训练期间引入噪声扰动，以增强对姿态估计误差的鲁棒性，并在推理时与相对稳定的VO模块结合，进一步提升导航性能。

Result: 在FloNa基准测试中，GlocDiff展现出卓越的导航性能，证明了其效率和有效性。在实际部署中也取得了成功，凸显了其在实际应用中的潜力。

Conclusion: GlocDiff通过有效整合全局规划和局部感知，解决了室内导航中的关键挑战，并在基准测试和实际应用中取得了显著成功，凸显了其广泛实际应用的潜力。

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [387] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 本文提出了一种名为Unified Diffusion VLA (UD-VLA)的新型视觉-语言-动作模型，通过联合离散去噪扩散过程(JD3P)同步优化未来图像生成和动作预测，实现了理解、生成和行动的内在协同，并在多个基准测试中取得了最先进的性能和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作 (VLA) 模型在将未来图像整合到理解-行动循环中时，要么依赖外部专家进行模态统一，要么将图像生成和动作预测视为独立过程，这限制了这些任务之间直接协同的益处。

Method: 本文提出Unified Diffusion VLA (UD-VLA)和联合离散去噪扩散过程 (JD3P)。其核心思想是通过同步去噪过程共同优化生成和行动，使行动在持续且充分的视觉指导下从初始化阶段迭代细化。模型建立在所有模态的统一分词空间和混合注意力机制之上，并提出了两阶段训练流程和多项推理时技术来优化性能和效率。

Result: 该方法在CALVIN、LIBERO和SimplerEnv等基准测试中取得了最先进的性能，推理速度比自回归方法快4倍，并通过深入分析和真实世界评估证明了其有效性。

Conclusion: 通过其提出的UD-VLA和JD3P，该研究成功地将多模态整合到一个单一的去噪轨迹中，实现了理解、生成和行动的内在协同，显著提升了具身智能体的性能和效率。

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [388] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 本文提出一种基于Rectified Flow的轻量级驱动空间学习框架，使软体机器人能从少量示范中学习全身抓取策略，实现高成功率、良好的泛化能力和鲁棒性，从而将身体力学转化为控制智能。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取面临不确定性和频繁接触的挑战。传统刚性机器人需要复杂的模型和反馈控制。软体机器人因其欠驱动结构和被动柔性，天然适合处理不确定接触并展现自适应行为，其“具身机械智能”有望简化控制负担。

Method: 提出一种轻量级驱动空间学习框架，利用流匹配模型（Rectified Flow）直接从确定性示范中推断全身软体机器人抓取的分布控制表示。该方法不需要密集的传感或复杂的控制回路。

Result: 仅使用30次示范（不到可达工作空间的8%），学习到的策略在整个工作空间实现了97.5%的抓取成功率。它能泛化到物体尺寸±33%的变化，并在执行时间从20%调整到200%时保持稳定的性能。

Conclusion: 驱动空间学习通过利用软体机器人被动冗余自由度和柔性，将身体力学转化为功能性控制智能，显著减轻了中央控制器在处理不确定性任务时的负担。

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [389] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: GenDexHand是一个生成式仿真管线，通过视觉语言模型（VLM）反馈的闭环优化和任务分解，自主生成多样化的灵巧操作任务和环境，以解决具身智能中灵巧手数据稀缺的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 具身智能面临数据稀缺的根本性瓶颈，尤其对于灵巧操作而言，现有基于LLM的夹爪模拟生成方法效果不佳，且灵巧操作因自由度高而更具挑战性。大规模生成可行且可训练的灵巧手任务仍是一个开放性难题。

Method: 本文提出了GenDexHand，一个生成式仿真管线，用于自主生成多样化的机器人灵巧操作任务和环境。该方法引入了一个闭环优化过程，根据视觉语言模型（VLM）的反馈调整物体放置和缩放。每个任务进一步分解为子任务，以实现顺序强化学习。

Result: GenDexHand显著提高了生成环境的平均质量。任务分解减少了训练时间并提高了成功率。这项工作为具身智能中多样化灵巧手行为的可扩展训练提供了一条可行途径。

Conclusion: GenDexHand提供了一个基于仿真的合成数据生成解决方案，有效解决了灵巧操作的数据稀缺问题，为具身智能中多样化灵巧手行为的可扩展训练铺平了道路。

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [390] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: 该研究提出了一种混合神经网络（HyNN）模型，利用大规模MIMO系统的信道状态信息（CSI）数据，通过将CSI转换为合成图像，实现移动机器人的二维室内高精度定位。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中为移动机器人提供精确的室内定位和导航是一个挑战，该研究旨在利用大规模MIMO的CSI数据解决这一问题。

Method: 研究方法包括：1. 提出混合神经网络（HyNN），结合卷积神经网络（CNN）和多层感知器（MLP）。2. 使用TINTO工具将CSI数据转换为合成图像作为网络输入。3. 模型输出二维机器人位置。4. 将定位解决方案与机器人模拟器和ROS集成，以便通过异构测试用例进行评估，并可结合卡尔曼滤波器等状态估计器。

Result: 研究结果表明，所提出的HyNN模型在实现复杂环境中移动机器人的精确室内定位和导航方面具有巨大潜力。此外，该研究提出了一种可推广的程序，适用于特定用例之外的不同场景和数据集。

Conclusion: 该HyNN模型能够有效地利用CSI数据实现移动机器人的高精度室内定位，且所提出的方法具有良好的通用性和可适应性。

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [391] [Which Top Energy-Intensive Manufacturing Countries Can Compete in a Renewable Energy Future?](https://arxiv.org/abs/2511.00242)
*Arne Burdack,Maximilian Stargardt,Christoph Winkler,Konrad Klein,Detlef Stolten,Jochen Linssen,Heidi Heinrichs*

Main category: eess.SY

TL;DR: 本研究通过能源系统模型量化了“可再生能源吸引力”（即有利的可再生能源条件对工业搬迁的激励），发现其并非跨行业现象，强度因国家而异，并受能源、运输和资本成本影响。有针对性的进口策略可有效缓解其影响。


<details>
  <summary>Details</summary>
Motivation: 随着全球日益依赖可再生能源并追求温室气体中和的工业生产，当前主要制造业国家的未来竞争力受到质疑。本研究旨在量化因有利可再生能源条件而产生的工业搬迁激励。

Method: 应用详细的能源系统建模。

Result: “可再生能源吸引力”并非跨行业现象，而是强烈依赖于能源成本与运输成本之间的关系。其强度各异，中国、印度和日本面临的影响显著强于德国和美国。纳入国家资本成本假设至关重要，能使德国的“可再生能源吸引力”降低六倍，使其成为继沙特阿拉伯之后受影响最小的第二大制造业国家。此外，针对性的进口策略（尤其是在欧盟内部）几乎可以消除德国的“可再生能源吸引力”。

Conclusion: “可再生能源吸引力”是一个复杂现象，受多重因素影响。政策制定者可以利用明确的风险缓解方案，如实施有针对性的进口策略，来应对潜在的工业搬迁风险。

Abstract: In a world increasingly powered by renewables and aiming for greenhouse
gas-neutral industrial production, the future competitiveness of todays top
manufacturing countries is questioned. This study applies detailed energy
system modeling to quantify the Renewable Pull, an incentive for industry
relocation exerted by countries with favorable renewable conditions. Results
reveal that the Renewable Pull is not a cross-industrial phenomenon but
strongly depends on the relationship between energy costs and transport costs.
The intensity of the Renewable Pull varies, with China, India, and Japan facing
a significantly stronger effect than Germany and the United States.
Incorporating national capital cost assumptions proves critical, reducing
Germanys Renewable Pull by a factor of six and positioning it as the second
least affected top manufacturing country after Saudi Arabia. Using Germany as a
case study, the analysis moreover illustrates that targeted import strategies,
especially within the EU, can nearly eliminate the Renewable Pull, offering
policymakers clear options for risk mitigation.

</details>


### [392] [Learning a Network Digital Twin as a Hybrid System](https://arxiv.org/abs/2511.00291)
*Christos Mavridis,Fernando S. Barbosa,Hamed Farhadi,Karl H. Johansson*

Main category: eess.SY

TL;DR: 本文提出了一种基于混合系统模型的网络数字孪生（NDT），用于模拟多小区动态无线网络的通信质量，并通过退火优化学习算法和在线数据进行持续改进，并在5G测试台上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 网络数字孪生（NDT）被认为是未来6G网络实现新功能和能力的关键技术组件。这项研究旨在为多小区、动态变化的无线网络中，对具有多个移动用户的通信质量属性进行建模。

Method: 作者提出将NDT建模为一个混合系统，其中每个模式对应一个不同的基站，并包含对应于具有相似网络特征的工作区子模式。该混合NDT通过一种基于退火优化的学习算法进行识别和持续改进，该算法由用户收集的在线数据测量驱动。

Result: 研究了所提出的混合NDT在内存和计算效率、数据消耗以及及时适应网络变化方面的优势。该方法在从双小区5G测试平台收集的真实实验数据上得到了验证。

Conclusion: 该研究成功地提出并验证了一种基于混合系统的网络数字孪生方法，该方法能够高效、及时地模拟和适应动态无线网络的通信质量变化，并在真实5G环境中表现出良好的性能。

Abstract: Network digital twin (NDT) models are virtual models that replicate the
behavior of physical communication networks and are considered a key technology
component to enable novel features and capabilities in future 6G networks. In
this work, we focus on NDTs that model the communication quality properties of
a multi-cell, dynamically changing wireless network over a workspace populated
with multiple moving users. We propose an NDT modeled as a hybrid system, where
each mode corresponds to a different base station and comprises sub-modes that
correspond to areas of the workspace with similar network characteristics. The
proposed hybrid NDT is identified and continuously improved through an
annealing optimization-based learning algorithm, driven by online data
measurements collected by the users. The advantages of the proposed hybrid NDT
are studied with respect to memory and computational efficiency, data
consumption, and the ability to timely adapt to network changes. Finally, we
validate the proposed methodology on real experimental data collected from a
two-cell 5G testbed.

</details>


### [393] [Analyzing the Impact of Demand Response on Short-Circuit Current via a Unit Commitment Model](https://arxiv.org/abs/2511.00296)
*Peng Wang,Zhengmao Li,Luis Badesa*

Main category: eess.SY

TL;DR: 本文研究了在低碳电网中，需求响应（DR）如何影响同步发电机（SG）的短路电流（SCC）供应，并将其与SCC约束纳入机组组合模型。结果显示，DR虽然能降低成本，但可能导致SCC不足，但结合SCC约束后，DR仍能以较低的成本实现系统稳定。


<details>
  <summary>Details</summary>
Motivation: 随着低碳电网中同步发电机（SG）被逆变器型资源（IBR）取代，系统稳定性受到严重影响，主要原因是IBR的短路电流（SCC）贡献远小于SG，可能导致保护装置故障时无法跳闸。剩余的SG在提供足够的SCC方面发挥关键作用。由于DR影响SG的承诺，进而可能间接影响SCC供应，这种关系尚未被研究。

Method: 将需求响应（DR）和短路电流（SCC）约束纳入机组组合模型中，并在IEEE 30节点系统上进行研究。

Result: 虽然需求响应（DR）可以通过降低电力需求来减少社会成本，但它也可能导致短路电流（SCC）水平不足。然而，当DR与SCC约束结合时，成本仅增加0.3%。

Conclusion: 需求响应（DR）可以以具有成本效益的方式帮助实现系统稳定，即使在考虑短路电流（SCC）约束的情况下，其成本增幅也很小。

Abstract: In low-carbon grids, system flexibility can be enhanced through mechanisms
such as Demand Response (DR), enabling the efficient utilization of renewable
energy. However, as Synchronous Generators (SGs) are being replaced with
renewable energy characterized by Inverter-Based Resources (IBR), system
stability is severely affected. Due to the limited overload capability of IBR,
their Short-Circuit Current (SCC) contribution is much smaller than that of
SGs, which may result in protection devices failing to trip during faults.
Consequently, the remaining SGs play a key role in offering sufficient SCC
volumes. Given that the commitment of SGs is closely related to system load, DR
can thus indirectly affect their SCC provision, a relationship that has not
been investigated. Therefore, this paper incorporates both DR and SCC
constraints into a unit commitment model and conducts studies on an IEEE 30-bus
system. The results show that although DR can reduce social costs by lowering
power demand, it may also lead to inadequate SCC levels. Nevertheless, the cost
increases by only 0.3% when DR is combined with SCC constraints, indicating
that DR can actually help achieve a stable system in a cost-effective manner.

</details>


### [394] [Optimal BESS Sizing and Placement for Mitigating EV-Induced Voltage Violations: A Scalable Spatio-Temporal Adaptive Targeting Strategy](https://arxiv.org/abs/2511.00297)
*Linhan Fang,Xingpeng Li*

Main category: eess.SY

TL;DR: 本研究提出了一种主动电压管理（PVM）框架，通过结合蒙特卡洛模拟、电压违规分析（VVA）和优化扩展规划（OEP），利用电池储能系统（BESS）来识别并缓解电动汽车（EV）充电负荷导致的电压骤降问题，并通过时空自适应目标（STAT）策略提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 电动汽车的普及和充电需求的增长给配电网带来了巨大压力，导致电压严重下降，尤其是在馈线末端，因此需要有效的解决方案来管理这些电压问题。

Method: 该研究提出PVM框架，包含以下步骤：1) 使用蒙特卡洛模拟不同电动汽车充电负荷；2) 通过电压违规分析（VVA）模型识别潜在电压违规；3) 通过优化扩展规划（OEP）模型，利用最优投资的电池储能系统（BESS）来缓解这些违规。为降低OEP模型的计算复杂性，引入了时空自适应目标（STAT）策略，通过将OEP模型应用于一组精简的代表性关键时间段和候选BESS安装节点，定义了目标OEP（T-OEP）模型。该方法在33节点、69节点和240节点系统上进行了验证。

Result: 研究结果表明，战略性地确定BESS的规模和位置不仅能有效缓解电压违规，还能在分时电价下显著节省购电成本。

Conclusion: 该研究为整合高渗透率的电动汽车提供了一种经济高效且可扩展的解决方案，为未来的配电网规划提供了重要见解。

Abstract: The escalating adoption of electric vehicles (EVs) and the growing demand for
charging solutions are driving a surge in EV charger installations in
distribution networks. However, this rising EV load strains the distribution
grid, causing severe voltage drops, particularly at feeder extremities. This
study proposes a proactive voltage management (PVM) framework that can
integrate Monte Carlo-based simulations of varying EV charging loads to (i)
identify potential voltage violations through a voltage violation analysis
(VVA) model, and (ii) then mitigate those violations with optimally-invested
battery energy storage systems (BESS) through an optimal expansion planning
(OEP) model. A novel spatio-temporal adaptive targeting (STAT) strategy is
proposed to alleviate the computational complexity of the OEP model by defining
a targeted OEP (T-OEP) model, solved by applying the OEP model to (i) a reduced
set of representative critical time periods and (ii) candidate BESS
installation nodes. The efficacy and scalability of the proposed approach are
validated on 33-bus, 69-bus, and a large-scale 240-bus system. Results
demonstrate that the strategic sizing and placement of BESS not only
effectively mitigate voltage violations but also yield substantial cost savings
on electricity purchases under time-of-use tariffs. This research offers a
cost-effective and scalable solution for integrating high penetrations of EVs,
providing crucial insights for future distribution network planning.

</details>


### [395] [Large Language Models for Control](https://arxiv.org/abs/2511.00337)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: eess.SY

TL;DR: 本文探讨了直接使用大型语言模型（LLMs）生成控制动作的可能性，无需专业的控制工程知识或手动调优算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是消除对控制工程专业知识和手动调优算法的需求，使控制系统设计更易于访问和自动化。

Method: 研究实现了三种LLM变体：(i) 仅提示（prompt-only），(ii) 工具辅助（可访问历史数据），以及(iii) 预测辅助（使用学习模型或简单模型评估候选动作）。通过跟踪精度和执行器使用量进行比较，并测试了是否包含降低执行器使用量的提示。

Result: 结果显示，仅提示的LLM已能产生可行的控制。工具增强版本能更好地适应变化的目标，但可能对约束更敏感。

Conclusion: 研究支持LLM在循环控制（LLM-in-the-loop control）中应用于不断发展的网络物理系统，并能支持操作员和人类输入。

Abstract: This paper investigates using large language models (LLMs) to generate
control actions directly, without requiring control-engineering expertise or
hand-tuned algorithms. We implement several variants: (i) prompt-only, (ii)
tool-assisted with access to historical data, and (iii) prediction-assisted
using learned or simple models to score candidate actions. We compare them on
tracking accuracy and actuation effort, with and without a prompt that requests
lower actuator usage. Results show prompt-only LLMs already produce viable
control, while tool-augmented versions adapt better to changing objectives but
can be more sensitive to constraints, supporting LLM-in-the-loop control for
evolving cyber-physical systems today and operator and human inputs.

</details>


### [396] [Constrained computational hybrid controller for Input Affine Hybrid Dynamical Systems](https://arxiv.org/abs/2511.00420)
*Ali Taghavian,Ali Safi,Esmaeel Khanmirza*

Main category: eess.SY

TL;DR: 本文提出了一种基于状态空间划分、计算仿真和图论的计算混合控制器（CHC），用于解决混合动力系统的控制问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 混合动力系统因其连续和事件驱动行为而被认为是复杂的系统，传统控制器难以有效处理。因此，需要开发新的控制器来应对这些挑战。

Method: 本文提出了一种新颖的可实现的约束终态控制器，该控制器基于系统状态空间划分、计算仿真和图论。

Result: 在三罐基准系统和摆的摆起控制上的实验结果以及与模型预测控制器的比较表明，所提出的计算混合控制器（CHC）是有效的。

Conclusion: 所提出的计算混合控制器（CHC）能够有效处理混合动力系统，并在实验中展现出优于或至少与模型预测控制器相当的性能。

Abstract: Hybrid dynamical systems are viewed as the most complicated systems with
continuous and event-based behaviors. Since traditional controllers cannot
handle these systems, some newly-developed controllers have been published in
recent decades to deal with them. This paper presents a novel implementable
constrained final-state controller based on partitioning the system's
state-space, computational simulations, and graph theory. Experimental results
and a comparison with Model Predictive Controller on the three tank benchmark
and swing-up control of a pendulum show the effectiveness of the proposed
Computational Hybrid Controller(CHC).

</details>


### [397] [CT-ESKF: A General Framework of Covariance Transformation-Based Error-State Kalman Filter](https://arxiv.org/abs/2511.00453)
*Jiale Han,Wei Ouyang,Maoran Zhu,Yuanxin Wu*

Main category: eess.SY

TL;DR: 当存在全局和本体坐标系观测时，不变扩展卡尔曼滤波器（InEKF）会失去其轨迹无关性。本文提出了协方差变换误差状态卡尔曼滤波器（CT-ESKF）框架，并证明了带有协方差变换的扩展卡尔曼滤波器（EKF）在集成导航系统中表现优于InEKF和原始EKF。


<details>
  <summary>Details</summary>
Motivation: InEKF在处理同时包含全局坐标系和本体坐标系观测的场景时，可能无法保持其轨迹无关性，且其协方差传播实际上与EKF等效，这促使研究人员寻求更优的滤波算法。

Method: 引入了不同误差状态卡尔曼滤波器之间误差状态和协方差矩阵等效性的概念。在此基础上，提出了一个基于协方差变换的误差状态卡尔曼滤波器（CT-ESKF）框架，该框架统一了各种误差状态卡尔曼滤波算法，并诞生了新的滤波算法。

Result: 实验结果表明，在具有代表性的惯性导航系统/全球导航卫星系统/里程计集成导航系统中，带有协方差变换的EKF（CT-ESKF框架下的新算法）的性能优于InEKF和原始EKF。

Conclusion: 所提出的CT-ESKF框架能够统一各种误差状态卡尔曼滤波算法，并生成在处理包含全局和本体坐标系观测的集成导航系统时性能更优的新滤波算法，尤其以带有协方差变换的EKF为代表。

Abstract: Invariant extended Kalman filter (InEKF) possesses excellent
trajectory-independent property and better consistency compared to conventional
extended Kalman filter (EKF). However, when applied to scenarios involving both
global-frame and body-frame observations, InEKF may fail to preserve its
trajectory-independent property. This work introduces the concept of
equivalence between error states and covariance matrices among different
error-state Kalman filters, and shows that although InEKF exhibits trajectory
independence, its covariance propagation is actually equivalent to EKF. A
covariance transformation-based error-state Kalman filter (CT-ESKF) framework
is proposed that unifies various error-state Kalman filtering algorithms. The
framework gives birth to novel filtering algorithms that demonstrate improved
performance in integrated navigation systems that incorporate both global and
body-frame observations. Experimental results show that the EKF with covariance
transformation outperforms both InEKF and original EKF in a representative
INS/GNSS/Odometer integrated navigation system.

</details>


### [398] [Rotatable Antenna System Empowered Low-Altitude Economy: Opportunities and Challenges](https://arxiv.org/abs/2511.00562)
*Shuaijun Li,Jie Tang,Beixiong Zheng,Lipeng Zhu,Cui Yang,Nan Zhao,Xiu Yin Zhang,Kai-Kit Wong*

Main category: eess.SY

TL;DR: 本文提出可旋转天线系统（RAS）以解决现有网络在低空经济（LAE）中覆盖不足的问题。RAS通过灵活波束赋形提供连续低空覆盖和稳定数据传输，并讨论了其应用、部署策略、设计挑战及性能增益。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）需要连续的低空空域覆盖和可靠的数据连接，但现有基站主要为地面用户设计，无法有效支持LAE发展，缺乏连续的低空覆盖能力。

Method: 引入可旋转天线系统（RAS）以实现灵活波束赋形；概述RAS赋能的LAE应用（通信、感知、控制、计算）；提出两种RAS部署策略：RAS辅助多基站和多无人机协同覆盖；讨论RAS在LAE中的关键设计问题，包括信道建模与估计、蜂窝接入与干扰消除、RAS配置与波束指向优化；通过实验和仿真结果验证性能增益。

Result: 可旋转天线系统（RAS）能够通过动态调整定向天线波束指向，有效扩展低空覆盖范围，增强数据传输稳定性。实验和仿真结果表明，RAS在LAE网络中带来了显著的性能提升。

Conclusion: 可旋转天线系统（RAS）是支持低空经济（LAE）发展的关键技术，通过灵活的波束赋形解决了现有网络的覆盖和稳定性挑战，具有广泛的应用前景和显著的性能优势。

Abstract: Low-altitude economy (LAE) is an emerging technological paradigm that enables
continuous airspace coverage at multiple altitudes by providing highly reliable
data connectivity for numerous low-altitude applications. However, existing
networks cannot sufficiently support LAE development, as current base stations
(BSs) are primarily designed for terrestrial users and lack the capability to
provide continuous coverage at low altitudes. To overcome these challenges,
rotatable antenna system (RAS) is introduced in LAE, enabling flexible
beamforming by dynamically adjusting the boresight of directional antennas to
extend low-altitude coverage and enhance the stability of data transmission. In
this article, we first provide an overview of RAS-empowered LAE applications,
including low-altitude communication, sensing, control, and computation. Then,
we present two practical RAS deployment strategies for LAE scenarios, namely
RAS-aided multi-BS and multi-unmanned aerial vehicle (UAV) cooperative
coverages, as well as provide detailed discussions on their system
architectures and performance benefits. Additionally, key design issues of RAS
in LAE are discussed, including channel modeling and estimation, cellular
access and interference cancellation, as well as RAS configuration and
boresight optimization. Finally, we demonstrate the performance gains of RAS in
LAE networks through experimental and simulation results.

</details>


### [399] [Towards Quantum Algorithms for the Optimization of Spanning Trees: The Power Distribution Grids Use Case](https://arxiv.org/abs/2511.00582)
*Carsten Hartmann,Nil Rodellas-Gràcia,Christian Wallisch,Thiemo Pesch,Frank K. Wilhelm,Dirk Witthaut,Tobias Stollenwerk,Andrea Benigni*

Main category: eess.SY

TL;DR: 本文针对径向网络中的损耗最小化问题，指出其近似解计算的难度，并提出基于QAOA的量子优化算法作为替代方案，通过两种拓扑采样方法来解决配电网重构问题。


<details>
  <summary>Details</summary>
Motivation: 网络拓扑优化在工程领域是一个重要挑战，特别是在能源系统中，网络重构可以显著降低损耗和成本，支持能源转型。然而，许多相关优化问题是NP难的，限制了实际应用。因此，需要新的方法来解决配电网运行中常见的径向网络损耗最小化问题。

Method: 本文提出量子优化作为一种有前景的替代方案。具体地，推导出两种基于量子交替算子本征态（QAOA）的量子算法原语，它们在网络拓扑采样上有所不同：一种是针对径向拓扑的定制采样，另一种是带有惩罚项的简单采样以抑制非径向拓扑。并量化了所需的量子资源。

Result: 研究表明，即使是计算径向网络损耗最小化问题的近似解，在计算上也是困难的。量子优化被提出作为一种有前景的替代方案。文章成功推导出了两种基于QAOA的量子算法原语，并展示了它们如何应用于配电网重构，同时量化了所需的量子资源。

Conclusion: 量子优化，特别是基于QAOA的算法，为解决径向网络中损耗最小化的NP难问题提供了一个有前景的替代方案。通过定制或带惩罚项的采样策略，可以有效应用于配电网重构，并为未来的量子计算应用奠定基础。

Abstract: Optimizing the topology of networks is an important challenge across
engineering disciplines. In energy systems, network reconfiguration can
substantially reduce losses and costs and thus support the energy transition.
Unfortunately, many related optimization problems are NP hard, restricting
practical applications. In this article, we address the problem of minimizing
losses in radial networks, a problem that routinely arises in distribution grid
operation. We show that even the computation of approximate solutions is
computationally hard and propose quantum optimization as a promising
alternative. We derive two quantum algorithmic primitives based on the Quantum
Alternating Operator Ansatz (QAOA) that differ in the sampling of network
topologies: a tailored sampling of radial topologies and simple sampling with
penalty terms to suppress non-radial topologies. We show how to apply these
algorithmic primitives to distribution grid reconfiguration and quantify the
necessary quantum resources.

</details>


### [400] [Digital Twin of Aerosol Jet Printing](https://arxiv.org/abs/2511.00593)
*Aayushya Agarwal,Jace Rozsa,Matteo Pozzi,Rahul Panat,Gary K. Fedder*

Main category: eess.SY

TL;DR: 本文开发了一个针对气溶胶喷射（AJ）打印过程的数字孪生模型，通过整合物理模型、计算机视觉和概率估计，实时监测不可观测状态，提高打印质量并预测异常。


<details>
  <summary>Details</summary>
Motivation: 气溶胶喷射（AJ）打印技术虽具潜力，但其广泛应用受限于打印质量的不一致性，这主要源于气溶胶粒径、载流密度、墨水液位以及喷嘴墨水沉积等许多隐藏且不可观测状态的变异性。

Method: 研究人员开发了一个AJ过程的数字孪生模型。该模型基于通过仿真和实验构建的物理宏观模型，并利用概率序列估计技术，通过AJ系统的传感器和视频数据提取的实时测量结果，持续更新数字模型的状态和参数。该框架集成了定制的计算机视觉技术、基于物理的宏观建模和先进的概率估计方法。

Result: 该数字孪生模型能够持续地与物理机器的生命周期同步演进，从而实现对不可观测物理特性的精确监控、异常行为的检测和预测，以及控制调整效果的预测。最终成果是一个全面的端到端数字孪生框架。

Conclusion: 该工作提供了一个能够持续演进的AJ设备和工艺数字表示，有效解决了AJ打印中因隐藏状态导致的质量不一致问题。尽管方法为AJ打印定制，但构建数字孪生的过程可应用于其他先进制造技术。

Abstract: Aerosol Jet (AJ) printing is a versatile additive manufacturing technique
capable of producing high-resolution interconnects on both 2D and 3D
substrates. The AJ process is complex and dynamic with many hidden and
unobservable states that influence the machine performance, including aerosol
particle diameter, aerosol carrier density, vial level, and ink deposition in
the tube and nozzle. Despite its promising potential, the widespread adoption
of AJ printing is limited by inconsistencies in print quality that often stem
from variability in these hidden states. To address these challenges, we
develop a digital twin model of the AJ process that offers real-time insights
into the machine's operations. The digital twin is built around a physics-based
macro-model created through simulation and experimentation. The states and
parameters of the digital model are continuously updated using probabilistic
sequential estimation techniques to closely align with real-time measurements
extracted from the AJ system's sensor and video data. The result is a digital
model of the AJ process that continuously evolves over a physical machine's
lifecycle. The digital twin enables accurate monitoring of unobservable
physical characteristics, detects and predicts anomalous behavior, and
forecasts the effect of control adjustments. This work presents a comprehensive
end-to-end digital twin framework that integrates customized computer vision
techniques, physics-based macro-modeling, and advanced probabilistic estimation
methods to construct an evolving digital representation of the AJ equipment and
process. While the methodologies are customized for aerosol jet printing, the
process for constructing the digital twin can be applied for other advanced
manufacturing techniques.

</details>


### [401] [Adaptive Federated Learning to Optimize the MultiCast flows in Data Centers](https://arxiv.org/abs/2511.00623)
*Junhong Liu,Lanxin Du,Yujia Li,Rong-Peng Liu,Fei Teng,Francis Yunhe Hou*

Main category: eess.SY

TL;DR: 本文提出了一种自适应联邦学习-优化方法，结合密码学技术，用于在保护隐私的前提下，提升地理分布式数据中心的能源效率，通过优化电力、热力和数据流的互联互通。


<details>
  <summary>Details</summary>
Motivation: 数据中心能源需求快速增长，对可持续运营构成挑战。同时，分布式数据中心的优化涉及混合整数公式和专有或敏感数据集，导致计算复杂性高和数据隐私问题。

Method: 研究提出了一个多周期优化模型，捕获电力、热力和数据流的相互依赖性。为解决计算复杂性和隐私问题，引入了自适应联邦学习-优化方法，并利用密码学技术在学习和优化过程中保护隐私。此外，开发了具有收敛保证的模型接受标准，并提出了可验证的双重聚合机制来确保共享数据的隐私和完整性。

Result: 理论分析和数值模拟表明，所提出的方法能够保护共享数据的隐私和完整性，实现接近最优的性能，并展现出高计算效率。

Conclusion: 该方法适用于在隐私约束下进行大规模数据中心优化，为解决分布式数据中心能效提升与隐私保护的矛盾提供了有效途径。

Abstract: Data centers play an increasingly critical role in societal digitalization,
yet their rapidly growing energy demand poses significant challenges for
sustainable operation. To enhance the energy efficiency of geographically
distributed data centers, this paper formulates a multi-period optimization
model that captures the interdependence of electricity, heat, and data flows.
The optimization of such multicast flows inherently involves mixed-integer
formulations and the access to proprietary or sensitive datasets, which
correspondingly exacerbate computational complexity and raise data-privacy
concerns. To address these challenges, an adaptive federated
learning-to-optimization approach is proposed, accounting for the heterogeneity
of datasets across distributed data centers. To safeguard privacy, cryptography
techniques are leveraged in both the learning and optimization processes. A
model acceptance criterion with convergence guarantee is developed to improve
learning performance and filter out potentially contaminated data, while a
verifiable double aggregation mechanism is further proposed to simultaneously
ensure privacy and integrity of shared data during optimization. Theoretical
analysis and numerical simulations demonstrate that the proposed approach
preserves the privacy and integrity of shared data, achieves near-optimal
performance, and exhibits high computational efficiency, making it suitable for
large-scale data center optimization under privacy constraints.

</details>


### [402] [Frequency Quality Assessment of GFM and GFL Converters and Synchronous Condensers](https://arxiv.org/abs/2511.00639)
*Taulant Kerci,Federico Milano*

Main category: eess.SY

TL;DR: 本文比较了传统和新兴技术（如并网型和随网型逆变器）以及控制策略对电网频率质量的影响，并探讨了在并网型逆变器主导的电网中自动发电控制的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究不同常规和新兴技术（尤其是基于逆变器的资源，IBRs）及其控制策略对电网频率质量的长期动态性能影响。

Method: 采用广泛的仿真和多个实际场景，考虑了频率质量的短期和长期方面。

Result: 研究表明，并网型逆变器（GFM IBRs）显著改善频率质量。然而，提供频率支持的随网型逆变器（GFL IBRs，如风能和电池）与同步补偿器相结合，可能足以达到相似的频率质量标准。此外，从频率质量角度看，在并网型逆变器主导的电网中，自动发电控制（AGC）的需求变得不那么明确。

Conclusion: 并网型逆变器能显著提升频率质量，但其他组合方案也能达到相似标准。在并网型逆变器主导的电网中，自动发电控制的作用需重新评估。

Abstract: This paper compares the impact of different conventional and emerging
technologies and control strategies on frequency quality. We study, in
particular, the long-term dynamic performance of grid-forming (GFM) and
grid-following (GFL) inverter-based resources (IBRs) as well as conventional
synchronous machines. Extensive simulations and several realistic scenarios
consider both short-term and long-term aspects of frequency quality. It is
shown that, while overall GFM IBRs significantly improve frequency quality, a
combination of GFL IBRs providing frequency support such as wind and batteries,
and synchronous condensers, might be enough to meet similar frequency quality
standards. Another result of the paper is that the need for automatic
generation control (AGC) becomes less clear in GFM IBR-dominated grids from a
frequency quality perspective.

</details>


### [403] [Efficiency and Optimality in Electrochemical Battery Model Parameter Identification: A Comparative Study of Estimation Techniques](https://arxiv.org/abs/2511.00595)
*Feng Guo,Luis D. Couto,Guillaume Thenaisie*

Main category: eess.SY

TL;DR: 本文评估了三种常用参数辨识方法（LS、PSO、GA）在电化学电池模型中的效率和最优性。结果显示，在缺乏先验知识时，PSO在准确性和稳定性方面表现最佳；LS适用于参数微调；GA在计算效率和最优性上均落后于PSO。


<details>
  <summary>Details</summary>
Motivation: 电化学电池模型的参数辨识极具挑战性，因为涉及大量参数且大多数无法直接测量。

Method: 研究开发并离散化了一个单粒子模型（SPM），并通过参数分组减少了参数数量。使用从真实电池获取的参数作为基准，生成了拟合和验证数据集，以评估LS、PSO和GA这三种方法的运行时间和准确性。

Result: 比较分析表明，在没有电池内部参数先验知识的情况下，PSO在准确性和稳定性方面优于其他方法，非常适合参数辨识。相比之下，LS更适合参数的微调，特别是针对老化电池。而GA在计算效率和最优性方面均落后于PSO。

Conclusion: PSO是电化学电池模型参数辨识的高效且最优方法，尤其是在缺乏先验知识时。LS适用于参数的微调，而GA在此任务中表现不佳。

Abstract: Parameter identification for electrochemical battery models has always been
challenging due to the multitude of parameters involved, most of which cannot
be directly measured. This paper evaluates the efficiency and optimality of
three widely-used parameter identification methods for electrochemical battery
models: Least Squares Method (LS), Particle Swarm Optimization (PSO), and
Genetic Algorithm (GA). Therefore, a Single Particle Model (SPM) of a battery
was developed and discretized. Battery parameter grouping was then performed to
reduce the number of parameters required. Using a set of parameters previously
identified from a real battery as a benchmark, we generated fitting and
validation datasets to assess the methods' runtime and accuracy. The
comparative analysis reveals that PSO outperforms the other methods in terms of
accuracy and stability, making it highly effective for parameter identification
when there is no prior knowledge of the battery's internal parameters. In
contrast, LS is better suited for minor adjustments in parameters, particularly
for aging batteries, whereas GA lags behind in both computational efficiency
and optimality with respect to PSO.

</details>


### [404] [Unveiling Uniform Shifted Power Law in Stochastic Human and Autonomous Driving Behavior](https://arxiv.org/abs/2511.00659)
*Wang Chen,Heye Huang,Ke Ma,Hangyu Li,Shixiao Liang,Hang Zhou,Xiaopeng Li*

Main category: eess.SY

TL;DR: 本文提出了一种“移位幂律”模型，能够准确表征人类驾驶和自动驾驶车辆行为的长尾分布，尤其是在罕见但关键的安全行为方面，显著提高了自动驾驶车辆安全评估模拟的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前模型在模拟自动驾驶车辆（AVs）的罕见但关键的安全驾驶行为时，无法准确重现真实的碰撞率，主要原因是未能充分表示长尾行为分布，这阻碍了AVs的评估和认证。

Method: 研究发现并提出了一种简洁的“移位幂律”模型，该模型仅需一到两个参数，即使在数据稀疏时也能高效校准。通过分析大规模微观轨迹数据，将此模型集成到基于智能体的交通模拟器中进行前向滚动模拟，以验证其在重现真实碰撞模式方面的能力。

Result: “移位幂律”模型能够稳健地表征人类驾驶车辆（HV）和AV行为的随机性，特别是在长尾区域。该模型在拟合频繁行为和罕见安全关键偏差方面表现出色，平均R2达到0.97，尾部分布几乎相同，并显著优于现有基于高斯分布的基线模型。集成到模拟器后，它能重现与真实世界统计数据一致的HV和AV碰撞模式，提高了安全评估的保真度，且无需事后修正。

Conclusion: “移位幂律”模型为高风险行为建模提供了一个统一且数据高效的基础，显著提高了混合AV/HV交通中基于模拟的安全评估的保真度。这项发现为自动驾驶技术的模拟驱动验证和全球认证提供了一条有前景的途径。

Abstract: Accurately simulating rare but safety-critical driving behaviors is essential
for the evaluation and certification of autonomous vehicles (AVs). However,
current models often fail to reproduce realistic collision rates when
calibrated on real-world data, largely due to inadequate representation of
long-tailed behavioral distributions. Here, we uncover a simple yet unifying
shifted power law that robustly characterizes the stochasticity of both
human-driven vehicle (HV) and AV behaviors, especially in the long-tail regime.
The model adopts a parsimonious analytical form with only one or two
parameters, enabling efficient calibration even under data sparsity. Analyzing
large-scale, micro-level trajectory data from global HV and AV datasets, the
shifted power law achieves an average R2 of 0.97 and a nearly identical tail
distribution, uniformly fits both frequent behaviors and rare safety-critical
deviations, significantly outperforming existing Gaussian-based baselines. When
integrated into an agent-based traffic simulator, it enables forward-rolling
simulations that reproduce realistic crash patterns for both HVs and AVs,
achieving rates consistent with real-world statistics and improving the
fidelity of safety assessment without post hoc correction. This discovery
offers a unified and data-efficient foundation for modeling high-risk behavior
and improves the fidelity of simulation-based safety assessments for mixed
AV/HV traffic. The shifted power law provides a promising path toward
simulation-driven validation and global certification of AV technologies.

</details>


### [405] [Hybrid Quantum-Classical Optimization of the Resource Scheduling Problem](https://arxiv.org/abs/2511.00733)
*Tyler Christeson,Md Habib Ullah,Ali Arabnya,Amin Khodaei,Rui Fan*

Main category: eess.SY

TL;DR: 本文提出了一种量子-经典混合算法，用于解决机组组合（UC）问题，该算法利用Benders分解将二元决策与连续调度解耦，并使用量子退火器求解二元主问题，显著降低了大规模系统的计算时间增长率，同时保持了解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 资源调度，尤其是电力系统中的机组组合问题，面临着巨大的挑战。传统的精确方法（如数学规划、动态规划）计算负担重，依赖线性近似或穷举搜索；元启发式方法（如遗传算法、粒子群优化）缺乏最优性保证，对初始条件敏感，且对大规模系统耗时过高。因此，需要一种新的方法来应对系统规模增长带来的复杂性。

Method: 本文引入了一种量子-经典混合算法。该方法利用Benders分解将二元承诺决策（机组启停状态）与连续经济调度（发电机输出水平）解耦。其中，二元主问题被建模为二次无约束二元优化（QUBO）模型，并在量子退火器上求解。连续子问题则负责最小化发电成本，并通过拉格朗日割平面反馈给主问题，直至收敛。该框架在10到1000个发电机组的系统上进行了评估。

Result: 与经典的混合整数非线性规划（MINLP）基线算法相比，该混合算法实现了持续更低的计算时间增长率，并将绝对最优性差距保持在1.63%以下。

Conclusion: 这些结果表明，在混合量子-经典Benders分解循环中整合量子退火，可以显著加速大规模资源调度，同时不牺牲解决方案质量。这为解决现代电网日益增长的复杂性提供了一条可行的途径。

Abstract: Resource scheduling is critical in many industries, especially in power
systems. The Unit Commitment problem determines the on/off status and output
levels of generators under many constraints. Traditional exact methods, such as
mathematical programming methods or dynamic programming, remain the backbone of
UC solution techniques, but they often rely on linear approximations or
exhaustive search, leading to high computational burdens as system size grows.
Metaheuristic approaches, such as genetic algorithms, particle swarm
optimization, and other evolutionary methods, have been explored to mitigate
this complexity; however, they typically lack optimality guarantees, exhibit
sensitivity to initial conditions, and can become prohibitively time-consuming
for large-scale systems. In this paper, we introduce a quantum-classical hybrid
algorithm for UC and, by extension, other resource scheduling problems, that
leverages Benders decomposition to decouple binary commitment decisions from
continuous economic dispatch. The binary master problem is formulated as a
quadratic unconstrained binary optimization model and solved on a quantum
annealer. The continuous subproblem, which minimizes generation costs, with
Lagrangian cuts feeding back to the master until convergence. We evaluate our
hybrid framework on systems scaled from 10 to 1,000 generation units. Compared
against a classical mixed-integer nonlinear programming baseline, the hybrid
algorithm achieves a consistently lower computation-time growth rate and
maintains an absolute optimality gap below 1.63%. These results demonstrate
that integrating quantum annealing within a hybrid quantum-classical Benders
decomposition loop can significantly accelerate large-scale resource scheduling
without sacrificing solution quality, pointing toward a viable path for
addressing the escalating complexity of modern power grids.

</details>


### [406] [Quantum Computing for EVs to Enhance Grid Resilience and Disaster Relief: Challenges and Opportunities](https://arxiv.org/abs/2511.00736)
*Tyler Christeson,Amin Khodaei,Rui Fan*

Main category: eess.SY

TL;DR: 本文综述了V2G和移动充电站选址（CSP）在增强电网韧性方面的优化方法，指出了其局限性，并探讨了量子计算如何克服计算瓶颈以加速电网恢复。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件日益频繁地导致大范围停电，因此增强电网韧性对于维持安全可靠的电力运行至关重要。V2G和移动CSP技术虽有潜力，但其有效运行涉及复杂的优化挑战。

Method: 本文回顾了V2G和移动CSP应用领域的现有优化方法，概述了这些方法的局限性，并探讨了量子计算（QC）如何克服当前的计算瓶颈。

Result: 研究指出了现有优化方法在V2G和移动CSP应用中的局限性，并提出量子计算能够解决这些计算瓶颈，从而在极端天气事件日益频繁和严重的情况下，增强电网韧性并加速恢复。

Conclusion: 量子计算为增强电网韧性和加速恢复提供了一个新的视角，尤其是在面对日益频繁和严重的极端天气事件时，它能够有效解决V2G和移动CSP应用中的复杂优化问题。

Abstract: The power grid is the foundation of modern society, however extreme weather
events have increasingly caused widespread outages. Enhancing grid resilience
is therefore critical to maintaining secure and reliable operations. In
disaster relief and restoration, vehicle-to-grid (V2G) technology allows
electric vehicles (EVs) to serve as mobile energy resources by discharging to
support critical loads or regulating grid frequency as needed. Effective V2G
operation requires coordinated charging and discharging of many EVs through
optimization. Similarly, in grid restoration, EVs must be strategically routed
to affected areas, forming the mobile charging station placement (CSP) problem,
which presents another complex optimization challenge. This work reviews
state-of-the-art optimization methods for V2G and mobile CSP applications,
outlines their limitations, and explores how quantum computing (QC) could
overcome current computational bottlenecks. A QC-focused perspective is
presented on enhancing grid resilience and accelerating restoration as extreme
weather events grow more frequent and severe.

</details>


### [407] [High-Power Dual-Channel Field Chamber for High-Frequency Magnetic Neuromodulation](https://arxiv.org/abs/2511.00745)
*Xiaoyang Tian,Hui Wang,Boshuo Wang,Jinshui Zhang,Dong Yan,Jeannette Ingabire,Samantha Coffler,Guillaume Duret,Quoc-Khanh Pham,Gang Bao,Jacob T. Robinson,Stefan M. Goetz,Angel V. Peterchev*

Main category: eess.SY

TL;DR: 本文开发了一种双通道磁场腔室，用于在自由活动的小鼠中进行高频交变磁场刺激，特别是磁热遗传学刺激，并验证了其频率选择性和安全性。


<details>
  <summary>Details</summary>
Motivation: 为了量化磁遗传学和磁电刺激等新方法对自由活动小鼠神经活动操纵所产生的行为效应，需要一种能够精确控制高频交变磁场的工具。

Method: 研究人员设计了一个双通道磁场腔室，通过优化线圈设计，实现了两个空间正交、频率不同的均匀磁场的独立控制。该系统配备了液体冷却系统、观察端口和摄像头，能够生成高幅度磁场，且具有可忽略的通道间干扰和均匀的磁场分布。通过使用钴掺杂和未掺杂的氧化铁纳米粒子，验证了其频率选择性加热能力。

Result: 该腔室在10 cm x 10 cm x 6 cm的体积内，提供了50 kHz（88 mT）和550 kHz（12.5 mT）两个通道，通道间干扰小于1%。磁场分布均匀（94%腔室体积内偏差小于±10%），操作期间线圈外壳内侧温升限制在<0.35°C/s。成功展示了频率选择性加热，钴掺杂和未掺杂氧化铁纳米粒子的加热速率分别为3.5°C/s和1.5°C/s。两个通道均可稳定运行四秒。

Conclusion: 所开发的双通道磁场腔室是一个安全、高效且频率选择性强的平台，适用于在自由活动小鼠中进行磁热遗传学刺激以及其他交变磁场应用，为神经活动操纵研究提供了新的工具。

Abstract: Several novel methods, including magnetogenetics and magnetoelectric
stimulation, use high frequency alternating magnetic fields to precisely
manipulate neural activity. To quantify the behavioral effects of such
interventions in a freely moving mouse, we developed a dual-channel magnetic
chamber, specifically designed for rate-sensitive magnetothermal-genetic
stimulation, and adaptable for other uses of alternating magnetic fields.
Through an optimized coil design, the system allows independent control of two
spatially orthogonal uniform magnetic fields delivered at different frequencies
within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal
frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5
mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV
and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling
system enables magnetic field generation for second-level duration, and an
observation port and camera allow video capture of the animal's behavior within
the chamber. The system generates high-amplitude magnetic fields across two
widely separated frequency channels with negligible interference (< 1%).
Relatively uniform magnetic field distribution (+/-10% across 94% of the
chamber volume) is maintained throughout the chamber, and temperature increase
of the inner side of the coil enclosure during the operation is limited to <
0.35 {\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron
oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5
{\deg}C/s and 1.5 {\deg}C/s, respectively, validating frequency-selectivity.
Both channels can run continuously for four seconds stably.

</details>


### [408] [Deep Q-Network for Optimizing NOMA-Aided Resource Allocation in Smart Factories with URLLC Constraints](https://arxiv.org/abs/2511.00765)
*Shi Gengtian,Jiang Liu,Shigeru Shimamoto*

Main category: eess.SY

TL;DR: 本文提出一种基于深度Q网络（DQN）的算法，用于NOMA辅助的智能工厂资源分配，旨在满足超可靠低延迟通信（URLLC）的严格要求，并通过可调参数平衡吞吐量和延迟。


<details>
  <summary>Details</summary>
Motivation: 智能工厂中的超可靠低延迟通信（URLLC）对NOMA辅助的资源分配提出了严格要求，需要一种能够动态优化资源并平衡吞吐量与延迟的解决方案。

Method: 该研究提出了一种基于DQN的算法，用于NOMA辅助的资源分配。该算法动态分配子信道并优化功率水平，以最大化吞吐量的同时满足严格的延迟约束。通过引入可调参数λ，算法能够平衡吞吐量和延迟之间的权衡。

Result: 仿真结果表明，机器人实现了更高的吞吐量，而传感器和控制器则满足了URLLC的低延迟要求，从而确保了实时工业应用的可靠通信。

Conclusion: 该DQN算法能够为智能工厂中的各种设备（如机器人、传感器和控制器）提供可靠的通信，满足URLLC的严格要求，并为实时工业应用提供支持。

Abstract: This paper presents a Deep Q-Network (DQN)- based algorithm for NOMA-aided
resource allocation in smart factories, addressing the stringent requirements
of Ultra-Reliable Low-Latency Communication (URLLC). The proposed algorithm
dynamically allocates sub-channels and optimizes power levels to maximize
throughput while meeting strict latency constraints. By incorporating a tunable
parameter {\lambda}, the algorithm balances the trade-off between throughput
and latency, making it suitable for various devices, including robots, sensors,
and controllers, each with distinct communication needs. Simulation results
show that robots achieve higher throughput, while sensors and controllers meet
the low-latency requirements of URLLC, ensuring reliable communication for
real-time industrial applications.

</details>


### [409] [Minimizing Maximum Latency of Task Offloading for Multi-UAV-assisted Maritime Search and Rescue](https://arxiv.org/abs/2511.00844)
*Shuang Qi,Bin Lin,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: eess.SY

TL;DR: 本文研究了一种多无人机辅助的海上搜救（MSAR）系统，通过联合优化计算卸载决策、中继无人机部署和监控无人机与搜救目标的关联，旨在最小化所有监控无人机的最大总延迟，以提高搜救效率。


<details>
  <summary>Details</summary>
Motivation: 无人机在海上搜救中作用关键，但其有限的计算能力和能量给无人机辅助的MSAR系统带来了挑战，导致数据传输量大和传输延迟高，即使部署移动边缘计算（MEC）服务器进行预处理也未能完全解决。

Method: 提出一个由多个监控无人机（S-UAV）和一个中继无人机（R-UAV）组成的多无人机辅助MSAR系统。构建了一个联合优化问题，通过同时优化计算卸载决策、R-UAV部署和S-UAV与搜救目标的关联，来最小化所有S-UAV的最大总延迟，并确保所有目标都被监控。由于该问题非凸，将其分解为三个子问题并提出了一个有效的迭代算法。

Result: 数值仿真结果表明，所提出的算法在各种性能参数下都具有有效性。

Conclusion: 所提出的迭代算法能有效解决多无人机辅助MSAR系统中的延迟最小化问题，从而提高搜救效率。

Abstract: Unmanned Aerial Vehicles (UAVs) play a crucial role in Maritime Search and
Rescue (MSAR), contributing to the improvement of rescue efficiency and
reduction of casualties. Typically, UAVs equipped with cameras collect data
from disaster areas and transmit it to the shore-based rescue command centers.
By deploying Mobile Edge Computing (MEC) servers, UAVs can pre-process video
footage to reduce data transmission volume, thus reducing transmission delays.
However, the limited computational capacity and energy of UAVs pose significant
challenges to the efficiency of UAV-assisted MSAR systems. To address these
problems, in this paper, we investigate a multi-UAV assisted MSAR system
consisting of multiple Surveillance UAVs (S-UAVs) and a Relay UAV (R-UAV).
Then, we formulate a joint optimization problem to minimize the maximum total
latency among all S-UAVs via jointly making the computing offloading decisions,
R-UAV deployment, and the association between a S-UAV and rescue targets while
ensuring that all targets are monitored by S-UAVs. Since the formulated
optimization problem is typically hard to solve due to its non-convexity, we
propose an effective iterative algorithm by breaking it into three
sub-problems. Numerical simulation results show the effectiveness of the
proposed algorithm with various performance parameters.

</details>


### [410] [Traffic-Aware Grid Planning for Dynamic Wireless Electric Vehicle Charging](https://arxiv.org/abs/2511.00941)
*Dipanjan Ghose,S Sivaranjani,Junjie Qin*

Main category: eess.SY

TL;DR: 本文提出了一种交通感知的动态无线电动汽车充电（DWC）电网规划框架，通过整合实时交通流模型和电力优化流，显著降低了基础设施成本并确保了服务可靠性。


<details>
  <summary>Details</summary>
Motivation: 动态无线电动汽车充电（DWC）技术能显著减少电池尺寸、消除充电停机时间并缓解里程焦虑，但其短时高功率需求对电网规划提出了新挑战。现有规划需综合考虑交通行为和电动汽车能耗，以应对DWC对电网的潜在压力。

Method: 本文提出了一个交通感知的DWC电网规划框架。该框架利用宏观小区传输模型（CTM）估算DWC走廊的实时时空电动汽车充电需求。随后，将需求模型整合到基于交流最优潮流（AC-OPF）的公式中，以优化微电网的规模，从而在不同交通条件下支持DWC并最小化运营成本。该方法明确建模了时空交通模式如何影响电网资源利用，与基于最坏情况交通数据的规划模型相比，能获得更低成本且更易操作的系统设计。

Result: 该框架在加州I-210W高速公路14英里路段的数据上进行了验证，评估了自由流、严重拥堵、不同严重程度的事故以及森林火灾等自然灾害等多种交通场景。结果表明，与基于最坏情况的建模相比，交通感知的电网规划显著降低了基础设施成本，同时在各种交通条件下确保了满足充电需求的服务可靠性。

Conclusion: 交通感知的DWC电网规划框架能够有效整合交通行为和电网管理，相比传统的最坏情况规划，能以更低的成本设计出更可靠、更易于操作的DWC系统，从而更好地支持电动汽车的普及和电网的稳定运行。

Abstract: Dynamic Wireless Electric Vehicle Charging (DWC) on electrified roadways is
an emerging technology that can significantly reduce battery sizes, eliminate
charging downtime, and alleviate range anxiety, specially for long-haul
transportation and fleet operations of electric vehicles (EVs). However, these
systems introduce new challenges for power system planning due to their
short-duration and high-power demands which can strain the grid if not properly
managed. As the energy demands from DWC depend on vehicle speed, density, dwell
time in charging zones, and load profiles along road segments, there is a need
for integrated planning of such systems, jointly considering both traffic
behavior and EV energy consumption. In this paper, we propose a traffic-aware
grid planning framework for DWC. We leverage a macroscopic Cell Transmission
Model of traffic flow to estimate real-time, spatiotemporal EV charging demand
from DWC corridors. The demand model is then integrated into an AC Optimal
Power Flow based formulation to optimally size a microgrid that supports DWC
under varying traffic conditions while minimizing the cost of operation. Our
framework explicitly models how spatiotemporal traffic patterns affect the
utilization of grid resources to obtain system designs that achieve lower costs
and are easier to operationalize as compared to planning models that rely on
worst-case traffic data.
  We demonstrate the framework on data from a 14-mile segment of the I-210W
highway in California, USA, evaluating multiple traffic scenarios like
free-flow, severe congestion, accidents of varying severity, and natural
disasters like forest fires. Our results demonstrate that traffic-aware grid
planning significantly reduces infrastructure costs as compared to
worst-scenario based modeling, while ensuring reliability of service in terms
of meeting charging demands under diverse traffic conditions.

</details>


### [411] [Secure Distributed Consensus Estimation under False Data Injection Attacks: A Defense Strategy Based on Partial Channel Coding](https://arxiv.org/abs/2511.00963)
*Jiahao Huang,Marios M. Polycarpou,Wen Yang,Fangfei Li,Yang Tang*

Main category: eess.SY

TL;DR: 本文研究了分布式估计中虚假数据注入攻击造成的安全问题，提出了攻击者在保持隐蔽性的前提下使估计误差发散的条件，并提出了两种防御策略：基于局部估计欧氏距离的检测和基于编码方案的数据保护，以提高系统安全性并平衡编码成本。


<details>
  <summary>Details</summary>
Motivation: 分布式估计中存在虚假数据注入攻击的安全隐患，攻击者可以操纵传输数据，导致估计误差发散，同时保持隐蔽性，这促使研究者寻找方法来揭示系统漏洞并开发有效的防御策略。

Method: 本文采用的方法包括：1. 推导攻击者在保持所有残差隐蔽性的同时使估计误差发散的必要和充分条件。2. 提出两种防御策略：a) 利用局部估计之间的欧氏距离来检测攻击。b) 采用编码方案（特别是时变编码矩阵）来保护传输数据。3. 进行安全分析以提供选择需要编码的安全关键通道的程序。

Result: 研究结果表明：1. 导出了攻击者在不被发现的情况下使估计误差发散的系统漏洞条件。2. 第一种防御策略（利用欧氏距离检测）能够解决大多数安全漏洞。3. 第二种防御策略（编码方案）可以作为前者的额外增强，特别是采用时变编码矩阵时，能有效抵御注入隐蔽序列的攻击。4. 提出了一个选择安全关键通道进行编码的程序，以在安全性和编码成本之间取得平衡。

Conclusion: 本文得出结论，通过结合利用局部估计欧氏距离的攻击检测机制和采用时变编码矩阵的数据保护方案，可以有效应对分布式估计中的虚假数据注入攻击。这种综合防御策略能够解决大部分安全漏洞，并能在保障安全性的同时优化编码成本。

Abstract: This article investigates the security issue caused by false data injection
attacks in distributed estimation, wherein each sensor can construct two types
of residues based on local estimates and neighbor information, respectively.
The resource-constrained attacker can select partial channels from the sensor
network and arbitrarily manipulate the transmitted data. We derive necessary
and sufficient conditions to reveal system vulnerabilities, under which the
attacker is able to diverge the estimation error while preserving the
stealthiness of all residues. We propose two defense strategies with mechanisms
of exploiting the Euclidean distance between local estimates to detect attacks,
and adopting the coding scheme to protect the transmitted data, respectively.
It is proven that the former has the capability to address the majority of
security loopholes, while the latter can serve as an additional enhancement to
the former. By employing the time-varying coding matrix to mitigate the risk of
being cracked, we demonstrate that the latter can safeguard against adversaries
injecting stealthy sequences into the encoded channels. Hence, drawing upon the
security analysis, we further provide a procedure to select security-critical
channels that need to be encoded, thereby achieving a trade-off between
security and coding costs. Finally, some numerical simulations are conducted to
demonstrate the theoretical results.

</details>


### [412] [On Structural Properties of Risk-Averse Optimal Stopping Problems](https://arxiv.org/abs/2511.01022)
*Xingyu Ren,Michael C. Fu,Steven I. Marcus*

Main category: eess.SY

TL;DR: 本文研究了时间一致动态相干风险度量下最优停止问题的结构性质，包括值函数的单调性和控制限最优策略的存在性，并为识别此类策略提供了通用方法和可验证条件。


<details>
  <summary>Details</summary>
Motivation: 风险中性（期望值）模型的最优停止问题结构性质已得到充分发展，但在风险规避（使用时间一致动态相干风险度量）设置中，这些性质（如值函数单调性、控制限策略）仍未得到充分探索。相干风险度量缺乏塔性质且是次可加而非可加的，这使得结构分析复杂化。

Method: 研究了值函数的单调性，并发现它与风险中性情况相似。证明了如果与每个相干风险度量相关的风险包络承认一个最小元素，则风险规避最优停止问题可简化为等价的风险中性公式。开发了一个识别控制限最优策略的通用程序，并用其推导了保证其存在的、关于风险度量和MDP结构的实际可验证条件。通过运营、营销和金融中的最优停止问题案例来验证了理论和条件。

Result: 值函数的单调性与风险中性情况一致。如果风险包络存在最小元素，风险规避最优停止问题可简化为等价的风险中性形式。开发了识别控制限最优策略的通用程序，并导出了保证其存在的实用、可验证条件。

Conclusion: 在时间一致动态相干风险度量下，成功建立了最优停止问题的结构性质，包括值函数的单调性，并为控制限最优策略的存在性提供了理论基础和实用条件。这弥合了风险中性和风险规避最优停止问题之间分析上的差距。

Abstract: We establish structural properties of optimal stopping problems under
time-consistent dynamic (coherent) risk measures, focusing on value function
monotonicity and the existence of control limit (threshold) optimal policies.
While such results are well developed for risk-neutral (expected-value) models,
they remain underexplored in risk-averse settings. Coherent risk measures
typically lack the tower property and are subadditive rather than additive,
complicating structural analysis. We show that value function monotonicity
mirrors the risk-neutral case. Moreover, if the risk envelope associated with
each coherent risk measure admits a minimal element, the risk-averse optimal
stopping problem reduces to an equivalent risk-neutral formulation. We also
develop a general procedure for identifying control limit optimal policies and
use it to derive practical, verifiable conditions on the risk measures and MDP
structure that guarantee their existence. We illustrate the theory and verify
these conditions through optimal stopping problems arising in operations,
marketing, and finance.

</details>


### [413] [GOSPA-Driven Non-Myopic Multi-Sensor Management with Multi-Bernoulli Filtering](https://arxiv.org/abs/2511.01045)
*George Jones,Angel Garcia-Fernandez*

Main category: eess.SY

TL;DR: 本文提出了一种基于多伯努利滤波和蒙特卡洛树搜索（MCTS）的非近视传感器管理算法，用于多传感器多目标跟踪，旨在通过优化未来时间窗内的广义最优子模式分配（GOSPA）误差来提高跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 在多传感器多目标跟踪场景中，需要一种有效的传感器管理算法来非近视地选择传感器动作，以最小化跟踪误差并提高整体系统性能。

Method: 该算法基于多伯努利滤波，通过解决一个非近视最小化问题来选择传感器动作，其成本函数是未来时间窗内的均方GOSPA误差。为确保可处理性，算法实际使用了GOSPA误差的上限，并通过蒙特卡洛树搜索（MCTS）实现。传感器能够联合优化和选择动作，同时考虑区域内所有其他传感器。

Result: 通过仿真分析了所提出算法的优势。

Conclusion: 所提出的非近视传感器管理算法在多传感器多目标跟踪中表现出优势，并通过仿真得到了验证。

Abstract: In this paper, we propose a non-myopic sensor management algorithm for
multi-target tracking, with multiple sensors operating in the same surveillance
area. The algorithm is based on multi-Bernoulli filtering and selects the
actions that solve a non-myopic minimisation problem, where the cost function
is the mean square generalised optimal sub-pattern assignment (GOSPA) error,
over a future time window. For tractability, the sensor management algorithm
actually uses an upper bound of the GOSPA error and is implemented via Monte
Carlo Tree Search (MCTS). The sensors have the ability to jointly optimise and
select their actions with the considerations of all other sensors in the
surveillance area. The benefits of the proposed algorithm are analysed via
simulations.

</details>


### [414] [Online Energy Storage Arbitrage under Imperfect Predictions: A Conformal Risk-Aware Approach](https://arxiv.org/abs/2511.01032)
*Yiqian Wu,Ming Yi,Bolun Xu,James Anderson*

Main category: eess.SY

TL;DR: 本文提出了一种基于共形方法的储能套利控制器，通过动态调整决策保守性来控制不完美价格预测带来的下行风险，并实现了风险的长期有界和收敛性。


<details>
  <summary>Details</summary>
Motivation: 储能套利严重依赖未来市场价格预测，而不准确的价格预测可能导致显著的利润损失。研究旨在解决如何有效管理和控制这种由预测不确定性引起的下行风险。

Method: 该研究基于共形决策理论，设计了一个控制器，通过预测集动态调整决策保守性，且无需分布假设。为了在利润损失反馈不可观测时实现在线校准，研究将时序差分误差作为可测量的代理。在此基础上，开发了两种在线校准策略：基于预测误差的自适应（针对预测准确性）和基于价值误差的校准（侧重决策质量）。

Result: 分析证明，该共形控制器能够实现有界的长期风险，并保证时序差分误差的收敛性，从而有效管理潜在利润损失中的风险暴露。案例研究表明，在不同预测条件下，与基准方法相比，该方法在平衡风险和机会方面表现出卓越的性能。

Conclusion: 所提出的共形方法能够有效应对储能套利中不完美价格预测带来的风险，通过动态调整决策保守性和在线校准策略，实现了风险的有效管理和性能的提升，并在风险与机会之间取得了更好的平衡。

Abstract: This work proposes a conformal approach for energy storage arbitrage to
control the downside risks arose from imperfect price forecasts. Energy storage
arbitrage relies solely on predictions of future market prices, while
inaccurate price predictions may lead to significant profit losses. Based on
conformal decision theory, we describe a controller that dynamically adjusts
decision conservativeness through prediction sets without distributional
assumptions. To enable online calibration when online profit loss feedback is
unobservable, we establish that a temporal difference error serves as a
measurable proxy. Building on this insight, we develop two online calibration
strategies: prediction error-based adaptation targeting forecast accuracy, and
value error-based calibration focusing on decision quality. Analysis of the
conformal controller proves bounded long-term risk with convergence guarantees
in temporal difference error, which further effectively manages risk exposure
in potential profit losses. Case studies demonstrate superior performance in
balancing risk and opportunity compared to benchmarks under varying forecast
conditions.

</details>


### [415] [Robust Self-Triggered Control Approaches Optimizing Sampling Sequences with Synchronous Measurements](https://arxiv.org/abs/2511.01057)
*Abbas Tariverdi*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的最优自触发方案，用于采样数据系统中的线性控制器，通过预计算采样序列来提高资源效率，同时保证系统稳定性和对外部扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的反馈控制算法依赖周期性执行，导致嵌入式控制和共享网络中资源（如CPU、网络带宽）使用效率低下。

Method: 该方法采用自触发实现，根据当前状态信息在有限范围内预计算下一个采样序列，并引入了一种新颖的最优自触发方案。

Result: 该方案能保证未受扰动系统的指数稳定性，并对受扰动系统提供全局一致最终有界性，从而确保了对外部扰动的鲁棒性，并提供了明确的性能保证。仿真结果证实了其优势。

Conclusion: 所提出的最优自触发方案在采样数据系统中实现了线性控制器的资源效率提升、稳定性保证和对扰动的鲁棒性。

Abstract: Feedback control algorithms traditionally rely on periodic execution on
digital platforms. While this simplifies design and analysis, it often leads to
inefficient resource usage (e.g., CPU, network bandwidth) in embedded control
and shared networks. This work investigates self-triggering implementations of
linear controllers in sampled-data systems with synchronous measurements. Our
approach precomputes the next sampling sequence over a finite horizon based on
current state information. We introduce a novel optimal self-triggering scheme
that guarantees exponential stability for unperturbed systems and global
uniform ultimate boundedness for perturbed systems. This ensures robustness
against external disturbances with explicit performance guarantees. Simulations
demonstrate the benefits of our approach.

</details>


### [416] [Orthogonal-by-construction augmentation of physics-based input-output models](https://arxiv.org/abs/2511.01321)
*Bendegúz M. Györök,Maarten Schoukens,Tamás Péni,Roland Tóth*

Main category: eess.SY

TL;DR: 本文提出一种新型的“正交构建”模型增强结构，用于输入输出模型，以解决现有并行增强结构中物理参数估计不准确、可解释性受损的问题，确保在适当可识别条件下恢复真实的物理参数。


<details>
  <summary>Details</summary>
Motivation: 现有的物理模型与机器学习组件的并行（加性）增强结构，由于系统动力学表示的重叠，常常导致物理参数估计不失真，从而损害模型的可解释性。

Method: 引入了一种新颖的“正交构建”模型增强结构，用于输入输出模型。

Result: 该方法在适当的可识别性条件下，能够保证恢复物理上真实的参数。

Conclusion: 所提出的正交构建模型增强结构克服了传统并行增强结构的局限性，确保了物理参数的准确恢复和模型的可解释性。

Abstract: Model augmentation is a promising approach for integrating
first-principles-based models with machine learning components. Augmentation
can result in better model accuracy and faster convergence compared to
black-box system identification methods, while maintaining interpretability of
the models in terms of how the original dynamics are complemented by learning.
A widely used augmentation structure in the literature is based on the parallel
connection of the physics-based and learning components, for both of which the
corresponding parameters are jointly optimized. However, due to overlap in
representation of the system dynamics by such an additive structure, estimation
often leads to physically unrealistic parameters, compromising model
interpretability. To overcome this limitation, this paper introduces a novel
orthogonal-by-construction model augmentation structure for input-output
models, that guarantees recovery of the physically true parameters under
appropriate identifiability conditions.

</details>


### [417] [Universal Barrier Functions for Safety and Stability of Constrained Nonlinear Systems](https://arxiv.org/abs/2511.01067)
*Vrushabh Zinage,Efstathios Bakolas*

Main category: eess.SY

TL;DR: 本文提出了一种通用障碍函数（UBF）方法，用于为具有复杂安全规范和输入约束的非线性系统合成安全且稳定的控制器，并证明了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是解决非线性系统在复杂安全规范和输入约束下，如何合成既安全又稳定的控制器的问题。

Method: 本文引入了通用障碍函数（UBF），这是一个连续可微的标量函数，能同时编码稳定性、安全性和输入约束。然后，利用UBF构建了一个二次规划（UBF-QP）来生成控制输入。该框架还被扩展到具有更高相对度的系统。

Result: 研究结果表明，如果UBF存在，则UBF-QP是可行的。此外，在温和条件下，UBF总是存在的。数值模拟也验证了所提出方法的有效性。

Conclusion: 该研究提出了一种基于通用障碍函数（UBF）和二次规划（UBF-QP）的有效框架，能够为具有复杂安全规范和输入约束的非线性系统生成安全且稳定的控制输入。

Abstract: In this paper, we address the problem of synthesizing safe and stabilizing
controllers for nonlinear systems subject to complex safety specifications and
input constraints. We introduce the Universal Barrier Function (UBF), a single
continuously differentiable scalar-valued function that encodes both stability
and safety criteria while accounting for input constraints. Using the UBF, we
formulate a Quadratic Program (UBF-QP) to generate control inputs that are both
safe and stabilizing under input constraints. We demonstrate that the UBF-QP is
feasible if a UBF exists. Furthermore, under mild conditions, we prove that a
UBF always exists. The proposed framework is then extended to systems with
higher relative degrees. Finally, numerical simulations illustrate the
effectiveness of our proposed approach.

</details>


### [418] [Deep Learning-Accelerated Shapley Value for Fair Allocation in Power Systems: The Case of Carbon Emission Responsibility](https://arxiv.org/abs/2511.01229)
*Yuanhao Feng,Tao Sun,Yan Meng,Xuxin Yang,Donghan Feng*

Main category: eess.SY

TL;DR: 本文提出SurroShap框架，通过结合高效联盟采样和深度学习代理模型，首次实现了在拥有数千个实体的电力系统中进行可扩展的Shapley值公平分配，解决了传统Shapley值计算的计算障碍。


<details>
  <summary>Details</summary>
Motivation: 在电力系统参与实体之间公平分配成本、收益和排放是一个持续的挑战。Shapley值提供了一种公理上公平的解决方案，但其计算障碍限制了其在小规模应用之外的采用。

Method: 本文提出了SurroShap框架，这是一种可扩展的Shapley值近似方法。它结合了高效的联盟采样和深度学习代理模型，以加速特征函数的评估。该方法还推导了理论误差界，证明了时间平均的SurroShap分配收敛到与精确Shapley值相距$\varepsilon$的范围内。

Result: SurroShap首次实现了对拥有数千个实体的电力系统进行基于Shapley的公平分配。在九个系统（26到1951个实体）上的实验表明，即使在最大规模下，也能在实时操作窗口内完成计算，比其他基于采样的方法快10^4-10^5倍，同时保持严格的误差界。由此产生的基于Shapley的碳分配具有六个理想的特性。对德克萨斯2000总线系统进行长达一年的模拟验证了其在现实世界中的适用性，并揭示了可再生能源丰富地区如何通过出口抵消排放责任，而负荷中心则承担推动全系统发电的责任。

Conclusion: SurroShap框架提供了一种可扩展、高效且准确的Shapley值近似方法，使得在大型电力系统中进行公平的成本、收益和排放分配成为可能。它在实际应用中表现出色，并能为电力系统的脱碳目标提供有价值的见解和指导。

Abstract: Allocating costs, benefits, and emissions fairly among power system
participant entities represents a persistent challenge. The Shapley value
provides an axiomatically fair solution, yet computational barriers have
limited its adoption beyond small-scale applications. This paper presents
SurroShap, a scalable Shapley value approximation framework combining efficient
coalition sampling with deep learning surrogate models that accelerate
characteristic function evaluations. Exemplified through carbon emission
responsibility allocation in power networks, SurroShap enables Shapley-based
fair allocation for power systems with thousands of entities for the first
time. We derive theoretical error bounds proving that time-averaged SurroShap
allocations converge to be $\varepsilon$-close to exact Shapley values.
Experiments on nine systems ranging from 26 to 1,951 entities demonstrate
completion within the real-time operational window even at maximum scale,
achieving 10^4-10^5 speedups over other sampling-based methods while
maintaining tight error bounds. The resulting Shapley-based carbon allocations
possess six desirable properties aligning individual interests with
decarbonization goals. Year-long simulations on the Texas 2000-bus system
validate real-world applicability, with regional analysis revealing how
renewable-rich areas offset emission responsibility through exports while load
centers bear responsibility for driving system-wide generation.

</details>


### [419] [Risk Aware Safe Control with Cooperative Sensing for Dynamic Obstacle Avoidance](https://arxiv.org/abs/2511.01403)
*Pei Yu Chang,Qizhe Xu,Vishnu Renganathan,Qadeer Ahmed*

Main category: eess.SY

TL;DR: 本文提出并验证了一种针对感知和通信不确定性下自动驾驶的安全关键控制器，该控制器融合了合作感知、Wasserstein barycenter、CVaR和CBF，以提高安全裕度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在感知和通信不确定性下，自动驾驶需要一种安全关键的控制器来优化动态障碍物位置的分布，并确保车辆在风险感知下的安全行驶。

Method: 1. 使用Wasserstein barycenter (WB) 融合合作感知数据，以优化动态障碍物位置的分布。 2. 引入Conditional Value at Risk (CVaR) 形成一个风险感知的控制障碍函数 (CBF) 框架。 3. 设计了一个WB CVaR CBF安全滤波器，该滤波器优化控制输入以最小化尾部风险，并确保安全集的前向不变性。 4. 模型预测控制器 (MPC) 执行路径跟踪，安全滤波器则调节名义控制输入以强制执行风险感知约束。 5. 详细阐述了软件架构以及与车辆执行器和合作感知的集成。

Result: 该方法在具有测量噪声、通信扰动和输入干扰的场景中，通过全尺寸自动驾驶车辆进行了评估，并与基线MPC CBF设计进行了比较。结果表明，所提出的方法显著改善了安全裕度和鲁棒性，突出了在实际自动驾驶车辆上部署风险感知安全滤波器的实用性。

Conclusion: 所提出的风险感知安全滤波器在处理感知和通信不确定性方面表现出色，能够提高自动驾驶车辆的安全性和鲁棒性，并具有实际部署的潜力。

Abstract: This paper presents the design, development, and on vehicle implementation
and validation of a safety critical controller for autonomous driving under
sensing and communication uncertainty. Cooperative sensing, fused via a
Wasserstein barycenter (WB), is used to optimize the distribution of the
dynamic obstacle locations. The Conditional Value at Risk (CVaR) is introduced
to form a risk aware control-barrier-function (CBF) framework with the
optimized distribution samplings. The proposed WB CVaR CBF safety filter
improves control inputs that minimize tail risk while certifying forward
invariance of the safe set. A model predictive controller (MPC) performs path
tracking, and the safety filter modulates the nominal control inputs to enforce
risk aware constraints. We detail the software architecture and integration
with vehicle actuation and cooperative sensing. The approach is evaluated on a
full-scale autonomous vehicle (AV) in scenarios with measurement noise,
communication perturbations, and input disturbances, and is compared against a
baseline MPC CBF design. Results demonstrate improved safety margins and
robustness, highlighting the practicality of deploying the risk-aware safety
filter on an actual AV.

</details>


### [420] [Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games - Part I: Equilibria](https://arxiv.org/abs/2511.01452)
*Leonardo Pedroso,Andrea Agazzi,W. P. M. H. Heemels,Mauro Salazar*

Main category: eess.SY

TL;DR: 本文研究了具有大量玩家的连续时间动态博弈，其中玩家状态随行动随机演化且奖励取决于整体分布。论文提出了一个演化模型和均场近似，并引入了“混合平稳纳什均衡”（MSNE）这一新的概念，以提供演化解释。


<details>
  <summary>Details</summary>
Motivation: 以往的演化博弈论主要关注静态博弈，缺乏对具有个体玩家状态动态的动态博弈的演化分析。此外，现实世界中的拥堵等效应依赖于人口状态和行动的分布，需要一个能捕捉这些效应的动态模型。

Method: 本文提出了一个演化模型，并结合有限人口博弈的均场近似，建立了强大的近似保证。同时，为了解决标准动态博弈解概念缺乏演化解释的问题，提出了“混合平稳纳什均衡”（MSNE）这一新概念。

Result: 研究发现，标准动态博弈的解概念缺乏演化解释，而MSNE则具有演化解释。论文分析了MSNE与均场演化模型的静止点之间的关系，并研究了MSNE的演化稳定性。

Conclusion: 本文首次对具有个体玩家状态动态的动态博弈进行了全面的演化分析，并引入了具有演化解释的MSNE概念，为理解和分析这类复杂系统提供了新的工具和视角。

Abstract: We study a dynamic game with a large population of players who choose actions
from a finite set in continuous time. Each player has a state in a finite state
space that evolves stochastically with their actions. A player's reward depends
not only on their own state and action but also on the distribution of states
and actions across the population, capturing effects such as congestion in
traffic networks. While prior work in evolutionary game theory has primarily
focused on static games without individual player state dynamics, we present
the first comprehensive evolutionary analysis of such dynamic games. We propose
an evolutionary model together with a mean field approximation of the
finite-population game and establish strong approximation guarantees. We show
that standard solution concepts for dynamic games lack an evolutionary
interpretation, and we propose a new concept - the Mixed Stationary Nash
Equilibrium (MSNE) - which admits one. We analyze the relationship between MSNE
and the rest points of the mean field evolutionary model and study the
evolutionary stability of MSNE.

</details>


### [421] [Deep Learning Prediction of Beam Coherence Time for Near-FieldTeraHertz Networks](https://arxiv.org/abs/2511.01491)
*Irched Chafaa,E. Veronica Belmega,Giacomo Bacci*

Main category: eess.SY

TL;DR: 本文提出了一种针对移动太赫兹（THz）网络的波束相干时间概念，并结合深度学习模型预测该时间，以显著减少波束更新开销，同时提高数据速率，尤其适用于高移动性场景。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信中的大型多天线阵列需要精确波束赋形以确保链路可靠性。然而，随着天线数量增加，波束对齐和跟踪会产生过高的开销。此外，近场区域随天线阵列尺寸和载波频率的增加而扩大，要求波束赋形适应球面波前而非传统的平面波假设。

Method: 引入一种新颖的波束相干时间概念，以大幅降低波束更新频率。提出一个基于简单前馈神经网络的深度学习模型，该模型以时间相关输入来预测波束相干时间，并实时调整波束赋形，以最小化开销。

Result: 数值结果表明，所提出的方法有效提高了数据速率并降低了开销，特别是在高（例如，车载）移动性场景下表现突出。

Conclusion: 所提出的方法通过引入波束相干时间并利用深度学习进行预测，能够有效减少移动太赫兹网络中的波束更新开销，同时在保持高数据速率方面表现出色，尤其在高移动性环境下更具优势。

Abstract: Large multiple antenna arrays coupled with accu- rate beamforming are
essential in terahertz (THz) communi- cations to ensure link reliability.
However, as the number of antennas increases, beam alignment (focusing) and
beam tracking in mobile networks incur prohibitive overhead. Additionally, the
near-field region expands both with the size of antenna arrays and the carrier
frequency, calling for adjustments in the beamforming to account for spherical
wavefront instead of the conventional planar wave assumption. In this letter,
we introduce a novel beam coherence time for mobile THz networks, to
drastically reduce the rate of beam updates. Then, we propose a deep learning
model, relying on a simple feedforward neural network with a time-dependent
input, to predict the beam coherence time and adjust the beamforming on the fly
with minimal overhead. Our numerical results demonstrate the effectiveness of
the proposed approach by enabling higher data rates while reducing the
overhead, especially at high (i.e., vehicular) mobility.

</details>


### [422] [On polynomial explicit partial estimator design for nonlinear systems with parametric uncertainties](https://arxiv.org/abs/2511.01638)
*Mazen Alamir*

Main category: eess.SY

TL;DR: 本文提出并验证了一种基于稀疏多元多项式关系的数据驱动局部估计器，用于具有参数不确定性的非线性系统，在小数据量下表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 为具有参数不确定性的非线性系统设计数据驱动的局部估计器。

Method: 提出一个基于稀疏多元多项式关系的数据驱动局部估计器通用框架，并通过两个示例与不同的机器学习/深度学习替代方案进行比较验证。

Result: 结果表明，所提出的稀疏识别方案在学习数据量较小的情况下，表现出优越性。

Conclusion: 所提出的稀疏识别方案在小数据量下，对参数不确定非线性系统的局部估计具有优势。

Abstract: This paper investigates the idea of designing data-driven partial estimators
for nonlinear systems showing parametric uncertainties using sparse
multivariate polynomial relationships. A general framework is first presented
and then validated on two illustrative examples with comparison to different
possible Machine/Deep-Learning based alternatives. The results suggests the
superiority of the proposed sparse identification scheme, at least when the
learning data is small.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [423] [Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements](https://arxiv.org/abs/2511.00449)
*Xiaolong Li,Zhi-Qin John Xu,Yan Ren,Tianming Qiu,Xiaowen Wang*

Main category: eess.IV

TL;DR: 本文提出了一种针对BraTS 2025 Task-6 (PED)任务优化的nnU-Net框架，用于儿科脑肿瘤分割，通过多项创新改进，在验证排行榜上取得了第一名。


<details>
  <summary>Details</summary>
Motivation: 儿科脑肿瘤在多参数磁共振成像(mpMRI)中的准确分割对于诊断、治疗计划和监测至关重要，但面临数据有限、解剖变异性高和跨机构成像异质性等独特挑战。

Method: 研究方法包括：(1) 带有挤压-激励(SE)注意力的加宽残差编码器；(2) 3D深度可分离卷积；(3) 特异性驱动的正则化项；(4) 小尺度高斯权重初始化。此外，还通过两个后处理步骤进一步优化预测。

Result: 所提出的模型在Task-6验证排行榜上获得第一名，实现了以下病灶级别Dice分数：CC为0.759，ED为0.967，ET为0.826，NET为0.910，TC为0.928，WT为0.928。

Conclusion: 本研究提出的先进nnU-Net框架及其创新改进，在儿科脑肿瘤分割任务中表现出色，达到了领先水平，尤其是在处理有限数据和高变异性方面展现了有效性。

Abstract: Accurate segmentation of pediatric brain tumors in multi-parametric magnetic
resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and
monitoring, yet faces unique challenges due to limited data, high anatomical
variability, and heterogeneous imaging across institutions. In this work, we
present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the
largest public dataset of pre-treatment pediatric high-grade gliomas. Our
contributions include: (1) a widened residual encoder with
squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions;
(3) a specificity-driven regularization term; and (4) small-scale Gaussian
weight initialization. We further refine predictions with two postprocessing
steps. Our models achieved first place on the Task-6 validation leaderboard,
attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910
(NET), 0.928 (TC) and 0.928 (WT).

</details>


### [424] [Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation](https://arxiv.org/abs/2511.00477)
*Aditya Parikh,Sneha Das,Aasa Feragen*

Main category: eess.IV

TL;DR: 该研究发现医学图像分割（特别是乳腺癌）中存在与年龄相关的算法偏见，揭示了“偏尺效应”，并证明这种偏见并非主要源于标注质量或定量难度不平衡，而是因为年轻患者的病例本身更难学习，且偏见在训练中使用有偏见的机器生成标签时会被学习和放大。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的算法偏见可能加剧健康不平等，但在分割任务中其原因尚不清楚。尽管公平性在分类任务中已被广泛研究，但分割任务（临床上很重要）仍未被充分探索。在乳腺癌分割中，模型对年轻患者表现出显著的性能差异，通常归因于乳腺密度的生理差异，但其根本原因仍不明确。

Method: 研究通过审计 MAMA-MIA 数据集，建立了年龄相关偏见的定量基线，并揭示了“偏尺效应”。通过受控实验，系统性地驳斥了偏见源于标注质量敏感性或定量病例难度不平衡的假设。通过难度平衡训练数据来尝试缓解差异，并提供了系统性偏见在训练中使用有偏见的机器生成标签时被学习和放大的直接证据。

Result: 研究建立了 MAMA-MIA 数据集中年龄相关偏见的定量基线，并发现了一个关键的“偏尺效应”，即验证用的系统性缺陷标签错误地代表了模型的实际偏见。通过受控实验，驳斥了偏见源于标注质量或定量病例难度不平衡的假设。结果表明，年轻患者的病例本质上更难学习，并且在有偏见的机器生成标签上进行训练时，系统性偏见会被学习和放大。

Conclusion: 该工作引入了一个诊断医学分割中算法偏见的系统框架，并证明实现公平性需要解决定性分布差异，而不仅仅是平衡病例数量。这一发现对自动化标注流程具有重要意义，表明系统性偏见是可学习和放大的。

Abstract: Algorithmic bias in medical imaging can perpetuate health disparities, yet
its causes remain poorly understood in segmentation tasks. While fairness has
been extensively studied in classification, segmentation remains underexplored
despite its clinical importance. In breast cancer segmentation, models exhibit
significant performance disparities against younger patients, commonly
attributed to physiological differences in breast density. We audit the
MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in
its automated labels, and reveal a critical Biased Ruler effect where
systematically flawed labels for validation misrepresent a model's actual bias.
However, whether this bias originates from lower-quality annotations (label
bias) or from fundamentally more challenging image characteristics remains
unclear. Through controlled experiments, we systematically refute hypotheses
that the bias stems from label quality sensitivity or quantitative case
difficulty imbalance. Balancing training data by difficulty fails to mitigate
the disparity, revealing that younger patient cases are intrinsically harder to
learn. We provide direct evidence that systemic bias is learned and amplified
when training on biased, machine-generated labels, a critical finding for
automated annotation pipelines. This work introduces a systematic framework for
diagnosing algorithmic bias in medical segmentation and demonstrates that
achieving fairness requires addressing qualitative distributional differences
rather than merely balancing case counts.

</details>


### [425] [Image-based ground distance detection for crop-residue-covered soil](https://arxiv.org/abs/2511.00548)
*Baochao Wang,Xingyu Zhang,Qingtao Zong,Alim Pulatov,Shuqi Shang,Dongwei Wang*

Main category: eess.IV

TL;DR: 本文提出了一种基于图像的方法，利用3D和RGB相机区分作物残留物和土壤，从而实现精准测量覆盖作物残留物土壤的地面距离，以解决保护性耕作中播种深度控制不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 保护性耕作中的精准播种深度控制面临挑战，现有距离测量技术（如激光、超声波或机械位移传感器）无法区分距离信息是来自作物残留物还是土壤，导致无法获取准确的地面距离信息。

Method: 该方法结合使用3D相机和RGB相机，同时获取深度图像和彩色图像。彩色图像用于区分残留物和土壤区域，生成掩膜图像。然后将掩膜图像应用于深度图像，仅使用土壤区域的深度信息来计算地面距离，并排除残留物区域。

Result: 实验结果表明，该距离测量方法具有实时实施的可行性，测量误差在±3毫米以内。

Conclusion: 该方法可应用于保护性耕作机械中的精准深度播种，以及其他需要精确深度控制的应用，如移栽或耕作。

Abstract: Conservation agriculture features a soil surface covered with crop residues,
which brings benefits of improving soil health and saving water. However, one
significant challenge in conservation agriculture lies in precisely controlling
the seeding depth on the soil covered with crop residues. This is constrained
by the lack of ground distance information, since current distance measurement
techniques, like laser, ultrasonic, or mechanical displacement sensors, are
incapable of differentiating whether the distance information comes from the
residue or the soil. This paper presents an image-based method to get the
ground distance information for the crop-residues-covered soil. This method is
performed with 3D camera and RGB camera, obtaining depth image and color image
at the same time. The color image is used to distinguish the different areas of
residues and soil and finally generates a mask image. The mask image is applied
to the depth image so that only the soil area depth information can be used to
calculate the ground distance, and residue areas can be recognized and excluded
from ground distance detection. Experimentation shows that this distance
measurement method is feasible for real-time implementation, and the
measurement error is within plus or minus 3mm. It can be applied in
conservation agriculture machinery for precision depth seeding, as well as
other depth-control-demanding applications like transplant or tillage.

</details>


### [426] [GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations](https://arxiv.org/abs/2511.00598)
*Zixuan Sun,Shuaifeng Zhi,Ruize Li,Jingyuan Xia,Yongxiang Liu,Weidong Jiang*

Main category: eess.IV

TL;DR: 本文提出GDROS，一个几何引导的稠密配准框架，用于解决光学与SAR遥感图像之间由于模态差异导致的配准难题。该方法结合CNN-Transformer特征提取、多尺度4D相关体以及最小二乘回归几何约束，在多个数据集上显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 光学与SAR遥感图像的配准是图像融合和视觉导航的基础，但由于严重的非线性辐射差异、几何畸变和噪声变化，这项任务极具挑战性。在大的几何变换下，现有方法难以实现可靠的配准结果。

Method: GDROS框架首先通过CNN-Transformer混合特征提取模块从光学和SAR图像中提取跨模态深度特征，然后构建并迭代优化多尺度4D相关体以建立像素级稠密对应。随后，通过最小二乘回归(LSR)模块对预测的稠密光流场施加几何约束，以估计仿射变换来指导最终流预测，从而减轻预测发散。

Result: 在WHU-Opt-SAR、OS和UBCv2三个代表性数据集上进行了广泛实验，GDROS在不同空间分辨率下均表现出鲁棒性能。定性和定量结果表明，GDROS在所有指标上都显著优于当前的最新方法。

Conclusion: GDROS通过利用全局跨模态图像交互和几何引导，有效解决了光学-SAR图像配准的挑战，实现了卓越的性能，为图像融合和视觉导航提供了坚实基础。

Abstract: Registration of optical and synthetic aperture radar (SAR) remote sensing
images serves as a critical foundation for image fusion and visual navigation
tasks. This task is particularly challenging because of their modal
discrepancy, primarily manifested as severe nonlinear radiometric differences
(NRD), geometric distortions, and noise variations. Under large geometric
transformations, existing classical template-based and sparse keypoint-based
strategies struggle to achieve reliable registration results for optical-SAR
image pairs. To address these limitations, we propose GDROS, a geometry-guided
dense registration framework leveraging global cross-modal image interactions.
First, we extract cross-modal deep features from optical and SAR images through
a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D
correlation volume is constructed and iteratively refined to establish
pixel-wise dense correspondences. Subsequently, we implement a least squares
regression (LSR) module to geometrically constrain the predicted dense optical
flow field. Such geometry guidance mitigates prediction divergence by directly
imposing an estimated affine transformation on the final flow predictions.
Extensive experiments have been conducted on three representative datasets
WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial
resolutions, demonstrating robust performance of our proposed method across
different imaging resolutions. Qualitative and quantitative results show that
GDROS significantly outperforms current state-of-the-art methods in all
metrics. Our source code will be released at:
https://github.com/Zi-Xuan-Sun/GDROS.

</details>


### [427] [Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars](https://arxiv.org/abs/2511.00652)
*Ali Khalid,Jaiaid Mobin,Sumanth Rao Appala,Avinash Maurya,Stephany Berrio Perez,M. Mustafa Rafique,Fawad Ahmad*

Main category: eess.IV

TL;DR: DejaView是一种利用长期时间冗余（数天至数月）来高效压缩自动驾驶车辆LiDAR点云数据的方法，显著降低了网络和存储成本。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆每天产生大量（数TB）传感器数据，其中大部分是LiDAR生成的3D点云。这些数据需要传输到云端进行模型训练或事故分析，但高昂的网络和存储成本是一个主要问题。

Method: DejaView的核心思想是自动驾驶车辆的运行区域有限且经常沿相同路线行驶，因此每天收集的3D数据很可能与过去捕获的数据相似。它通过一种“diff”操作，将新的点云数据紧凑地表示为相对于过去3D数据的增量（delta）。

Result: 使用两个月的LiDAR数据，DejaView的端到端实现可以将点云压缩210倍，同时重建误差仅为15厘米。

Conclusion: DejaView通过利用自动驾驶车辆数据中存在的长期时间冗余，实现了对LiDAR点云数据的高效压缩，有效降低了数据传输和存储成本。

Abstract: An autonomous vehicle can generate several terabytes of sensor data per day.
A significant portion of this data consists of 3D point clouds produced by
depth sensors such as LiDARs. This data must be transferred to cloud storage,
where it is utilized for training machine learning models or conducting
analyses, such as forensic investigations in the event of an accident. To
reduce network and storage costs, this paper introduces DejaView. Although
prior work uses interframe redundancies to compress data, DejaView searches for
and uses redundancies on larger temporal scales (days and months) for more
effective compression. We designed DejaView with the insight that the operating
area of autonomous vehicles is limited and that vehicles mostly traverse the
same routes daily. Consequently, the 3D data they collect daily is likely
similar to the data they have captured in the past. To capture this, the core
of DejaView is a diff operation that compactly represents point clouds as delta
w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end
implementation of DejaView can compress point clouds by a factor of 210 at a
reconstruction error of only 15 cm.

</details>


### [428] [Deep Generative Models for Enhanced Vitreous OCT Imaging](https://arxiv.org/abs/2511.00881)
*Simone Sarrocco,Philippe C. Cattin,Peter M. Maloca,Paul Friedrich,Philippe Valmaggia*

Main category: eess.IV

TL;DR: 本研究评估了深度学习模型，特别是cDDPM，在提高玻璃体OCT图像质量和缩短采集时间方面的潜力，并发现cDDPM在临床评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为了提高玻璃体光学相干断层扫描（OCT）图像的质量，同时减少图像采集时间。

Method: 研究使用了多种深度学习模型，包括条件去噪扩散概率模型（cDDPMs）、布朗桥扩散模型（BBDMs）、U-Net、Pix2Pix和矢量量化生成对抗网络（VQ-GAN），从低质量的SD ART10图像生成高质量的玻璃体OCT图像。模型性能通过图像质量指标（如PSNR、SSIM、LPIPS）和眼科医生参与的视觉图灵测试进行评估。最佳模型（cDDPM）还在新采集数据上手动分割的玻璃体区域内进行了进一步测试。

Result: U-Net在PSNR和SSIM方面表现最佳（PSNR: 30.230, SSIM: 0.820），其次是cDDPM。在LPIPS方面，Pix2Pix（0.697）和cDDPM（0.753）表现最好。在第一次视觉图灵测试中，cDDPM排名最高（3.07）。在第二次测试中，cDDPM达到了32.9%的愚弄率和85.7%的解剖结构保留率。在新采集的数据上，cDDPM生成的玻璃体区域在PSNR上比真实的ART1或ART10 B扫描更接近ART100参考图像，并且在以ART1为条件时，在整个图像上获得了比ART10更高的PSNR。

Conclusion: 结果表明定量指标与临床评估之间存在差异，强调了综合评估的必要性。cDDPM在生成具有临床意义的玻璃体OCT图像方面显示出巨大潜力，同时可以将采集时间缩短四倍，具有临床整合前景。

Abstract: Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical
coherence tomography (OCT) image quality and reducing acquisition time.
Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs),
Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised
Generative Adversarial Network (VQ-GAN) were used to generate high-quality
spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and
outputs were compared to pseudoART100 images obtained by averaging ten ART10
images per eye location. Model performance was assessed using image quality
metrics and Visual Turing Tests, where ophthalmologists ranked generated images
and evaluated anatomical fidelity. The best model's performance was further
tested within the manually segmented vitreous on newly acquired data. Results:
U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and
Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For
Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM
(0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest
(3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and
85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous
regions more similar in PSNR to the ART100 reference than true ART1 or ART10
B-scans and achieved higher PSNR on whole images when conditioned on ART1 than
ART10. Conclusions: Results reveal discrepancies between quantitative metrics
and clinical evaluation, highlighting the need for combined assessment. cDDPM
showed strong potential for generating clinically meaningful vitreous OCT
images while reducing acquisition time fourfold. Translational Relevance:
cDDPMs show promise for clinical integration, supporting faster, higher-quality
vitreous imaging. Dataset and code will be made publicly available.

</details>


### [429] [Evaluating Video Quality Metrics for Neural and Traditional Codecs using 4K/UHD-1 Videos](https://arxiv.org/abs/2511.00969)
*Benjamin Herb,Rakesh Rao Ramachandra Rao,Steve Göring,Alexander Raake*

Main category: eess.IV

TL;DR: 本研究通过系统的主观质量评估，比较了传统与神经视频编解码器，并评估了现有质量度量指标对其性能评估的有效性，发现多数指标对神经编解码器仍然适用。


<details>
  <summary>Details</summary>
Motivation: 随着神经视频编解码器（NVCs）的兴起，有必要确定现有质量度量指标是否仍能有效评估其性能。然而，很少有研究通过精心设计的主观测试系统地调查这一点，因此本研究旨在填补这一空白。

Method: 研究采用主观质量评估方法，使用了两种传统编解码器（AV1和VVC）和两种神经视频编解码器变体（DCVC-FM和DCVC-RT）。选取了6个源视频（4K/UHD-1, 60 fps），在4种分辨率和9种QP值下进行编码，生成了216个序列。30名参与者在受控环境中对这些序列进行了评分。这些主观结果被用于评估一系列全参考、混合和无参考质量度量指标的适用性。

Result: 客观质量评估结果显示，VMAF和AVQBits|H0|f表现出强大的皮尔逊相关性，而FasterVQA在测试的无参考指标中表现最佳。此外，PSNR在不同编解码器内序列比较中显示出最高的斯皮尔曼等级相关性。重要的是，在测试的指标中，传统和神经视频编解码器之间的度量可靠性没有观察到显著的性能差异。相关数据集将公开提供。

Conclusion: 本研究得出结论，对于神经视频编解码器引入的质量降级，现有的质量度量指标（如VMAF、AVQBits|H0|f、FasterVQA和PSNR）仍然有效。在度量可靠性方面，传统编解码器和神经编解码器之间没有显著差异，表明这些指标对新兴的神经视频压缩技术具有良好的泛化能力。

Abstract: With neural video codecs (NVCs) emerging as promising alternatives for
traditional compression methods, it is increasingly important to determine
whether existing quality metrics remain valid for evaluating their performance.
However, few studies have systematically investigated this using well-designed
subjective tests. To address this gap, this paper presents a subjective quality
assessment study using two traditional (AV1 and VVC) and two variants of a
neural video codec (DCVC-FM and DCVC-RT). Six source videos (8-10 seconds each,
4K/UHD-1, 60 fps) were encoded at four resolutions (360p to 2160p) using nine
different QP values, resulting in 216 sequences that were rated in a controlled
environment by 30 participants. These results were used to evaluate a range of
full-reference, hybrid, and no-reference quality metrics to assess their
applicability to the induced quality degradations. The objective quality
assessment results show that VMAF and AVQBits|H0|f demonstrate strong Pearson
correlation, while FasterVQA performed best among the tested no-reference
metrics. Furthermore, PSNR shows the highest Spearman rank order correlation
for within-sequence comparisons across the different codecs. Importantly, no
significant performance differences in metric reliability are observed between
traditional and neural video codecs across the tested metrics. The dataset,
consisting of source videos, encoded videos, and both subjective and quality
metric scores will be made publicly available following an open-science
approach
(https://github.com/Telecommunication-Telemedia-Assessment/AVT-VQDB-UHD-1-NVC).

</details>


### [430] [Learned Adaptive Kernels for High-Fidelity Image Downscaling](https://arxiv.org/abs/2511.01620)
*Piyush Narhari Pise,Sanjay Ghosh*

Main category: eess.IV

TL;DR: 本文提出ADK-Net，一个深度卷积神经网络框架，通过为每个像素和每个颜色通道独立预测空间自适应重采样核，实现了高保真监督图像下采样，并在PSNR和SSIM指标上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 图像下采样是图像处理中的基础操作，但经典方法常引入模糊或混叠。尽管基于学习的方法有所改进，但如何通过考虑通道特定特性来最大化对真实低分辨率图像的保真度，仍是一个未解决的挑战。

Method: 本文引入ADK-Net（Adaptive Downscaling Kernel Network），一个新颖的深度卷积神经网络框架。它通过学习为每个像素和每个RGB颜色通道独立预测空间自适应重采样核，显式处理通道间的相互依赖性。其架构采用分层设计，包含一个基于ResNet的特征提取器和并行的通道特定核生成器，后者由基于ResNet的主干和分支子模块组成，以实现精细的核预测。模型使用L1重建损失，针对真实低分辨率数据进行端到端训练。

Result: ADK-Net在标准基准测试（包括RealSR数据集）上的大量定量和定性实验表明，它在监督图像下采样方面建立了新的最先进水平，与现有基于学习和传统方法相比，在PSNR和SSIM指标上取得了显著提升。

Conclusion: ADK-Net通过其创新的自适应重采样核预测机制，有效学习了目标下采样变换，解决了通道间相互依赖性的挑战，并在图像下采样任务中取得了卓越的性能，达到了新的技术高度。

Abstract: Image downscaling is a fundamental operation in image processing, crucial for
adapting high-resolution content to various display and storage constraints.
While classic methods often introduce blurring or aliasing, recent
learning-based approaches offer improved adaptivity. However, achieving maximal
fidelity against ground-truth low-resolution (LR) images, particularly by
accounting for channel-specific characteristics, remains an open challenge.
This paper introduces ADK-Net (Adaptive Downscaling Kernel Network), a novel
deep convolutional neural network framework for high-fidelity supervised image
downscaling. ADK-Net explicitly addresses channel interdependencies by learning
to predict spatially-varying, adaptive resampling kernels independently for
each pixel and uniquely for each color channel (RGB). The architecture employs
a hierarchical design featuring a ResNet-based feature extractor and parallel
channel-specific kernel generators, themselves composed of ResNet-based trunk
and branch sub-modules, enabling fine-grained kernel prediction. Trained
end-to-end using an L1 reconstruction loss against ground-truth LR data,
ADK-Net effectively learns the target downscaling transformation. Extensive
quantitative and qualitative experiments on standard benchmarks, including the
RealSR dataset, demonstrate that ADK-Net establishes a new state-of-the-art in
supervised image downscaling, yielding significant improvements in PSNR and
SSIM metrics compared to existing learning-based and traditional methods.

</details>
