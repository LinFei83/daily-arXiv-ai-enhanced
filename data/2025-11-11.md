<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 63]
- [cs.CV](#cs.CV) [Total: 217]
- [cs.CL](#cs.CL) [Total: 100]
- [cs.RO](#cs.RO) [Total: 55]
- [eess.SY](#eess.SY) [Total: 40]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SMAGDi: Socratic Multi Agent Interaction Graph Distillation for Efficient High Accuracy Reasoning](https://arxiv.org/abs/2511.05528)
*Aayush Aluru,Myra Malik,Samarth Patankar,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: SMAGDi是一种蒸馏框架，它通过将多智能体辩论动态转化为有向交互图，并结合语言模型、图监督、对比推理和嵌入对齐的复合目标，将一个40B的多智能体系统压缩成一个6B的苏格拉底式分解-求解学生模型，同时保持了高准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）虽然推理准确性高，但由于需要重复辩论，计算成本昂贵。研究旨在寻找一种方法，在保持MAS高准确性的同时，显著降低其计算开销，使其适用于实际部署。

Method: 引入了SMAGDi蒸馏框架。它将多智能体辩论轨迹表示为有向交互图，其中节点编码带有正确性标签的中间推理步骤，边捕获连续性和跨智能体影响。学生模型是一个紧凑的苏格拉底式分解-求解器，通过结合语言建模、基于图的监督、对比推理和嵌入对齐的复合目标进行训练，以同时保留流畅性和结构化推理能力。

Result: 在StrategyQA和MMLU数据集上，SMAGDi将一个40B的多智能体系统压缩成一个6B的学生模型，并保留了88%的准确率。这显著优于先前的蒸馏方法，如MAGDi、标准知识蒸馏和微调基线。

Conclusion: 研究结果表明，明确建模交互图和苏格拉底式分解能够使小型模型继承多智能体辩论的准确性优势，同时保持足够的效率以用于实际部署。

Abstract: Multi-agent systems (MAS) often achieve higher reasoning accuracy than single models, but their reliance on repeated debates across agents makes them computationally expensive. We introduce SMAGDi, a distillation framework that transfers the debate dynamics of a five-agent Llama-based MAS into a compact Socratic decomposer-solver student. SMAGDi represents debate traces as directed interaction graphs, where nodes encode intermediate reasoning steps with correctness labels and edges capture continuity and cross-agent influence. The student is trained with a composite objective combining language modeling, graph-based supervision, contrastive reasoning, and embedding alignment to preserve both fluency and structured reasoning. On StrategyQA and MMLU, SMAGDi compresses a 40B multi-agent system into a 6B student while retaining 88% of its accuracy, substantially outperforming prior distillation methods such as MAGDi, standard KD, and fine-tuned baselines. These results highlight that explicitly modeling interaction graphs and Socratic decomposition enable small models to inherit the accuracy benefits of multi-agent debate while remaining efficient enough for real-world deployment.

</details>


### [2] [Evidence-Bound Autonomous Research (EviBound): A Governance Framework for Eliminating False Claims](https://arxiv.org/abs/2511.05524)
*Ruiying Chen*

Main category: cs.AI

TL;DR: EviBound是一个证据绑定执行框架，通过双重治理门和机器可验证证据，消除基于大型语言模型（LLM）的自主研究代理报告虚假声明（幻觉）的问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的自主研究代理存在报告虚假声明的问题，即使缺少必要成果、指标矛盾或执行失败，也可能将任务标记为“完成”。

Method: EviBound采用双重治理门机制：1. 预执行审批门：在代码运行前验证验收标准模式，主动捕获结构性违规。2. 后执行验证门：通过MLflow API查询（带递归路径检查）验证成果，并可选地验证指标。只有当声明有可查询的运行ID、所需成果和“完成”状态支持时才传播。框架还包括有界、置信度门控的重试机制。

Result: 在8个基准任务上评估，EviBound（双重门）实现了0%的幻觉（7/8任务验证成功，1任务在审批门被正确阻止），而基线A（仅提示级别）产生100%幻觉，基线B（仅验证）产生25%幻觉。EviBound的执行开销约为8.3%。

Conclusion: 研究的完整性是一个架构属性，应通过治理门而非仅通过模型规模来实现。EviBound框架通过其双重治理门有效消除了LLM代理的虚假声明问题。

Abstract: LLM-based autonomous research agents report false claims: tasks marked "complete" despite missing artifacts, contradictory metrics, or failed executions. EviBound is an evidence-bound execution framework that eliminates false claims through dual governance gates requiring machine-checkable evidence.
  Two complementary gates enforce evidence requirements. The pre-execution Approval Gate validates acceptance criteria schemas before code runs, catching structural violations proactively. The post-execution Verification Gate validates artifacts via MLflow API queries (with recursive path checking) and optionally validates metrics when specified by acceptance criteria. Claims propagate only when backed by a queryable run ID, required artifacts, and FINISHED status. Bounded, confidence-gated retries (typically 1-2 attempts) recover from transient failures without unbounded loops.
  The framework was evaluated on 8 benchmark tasks spanning infrastructure validation, ML capabilities, and governance stress tests. Baseline A (Prompt-Level Only) yields 100% hallucination (8/8 claimed, 0/8 verified). Baseline B (Verification-Only) reduces hallucination to 25% (2/8 fail verification). EviBound (Dual Gates) achieves 0% hallucination: 7/8 tasks verified and 1 task correctly blocked at the approval gate, all with only approximately 8.3% execution overhead.
  This package includes execution trajectories, MLflow run IDs for all verified tasks, and a 4-step verification protocol. Research integrity is an architectural property, achieved through governance gates rather than emergent from model scale.

</details>


### [3] [CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization](https://arxiv.org/abs/2511.05747)
*Ziqian Bi,Kaijie Chen,Tianyang Wang,Junfeng Hao,Xinyuan Song*

Main category: cs.AI

TL;DR: 本文提出了一种自适应推理摘要框架，通过语义分割和重要性评分等技术压缩思维链（CoT）推理过程，以实现跨模型的高效CoT迁移，从而在资源受限环境下提高大型语言模型（LLMs）的推理能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）推理虽然能增强大型语言模型（LLMs）的问题解决能力，但会产生大量的推理开销，限制了其在资源受限环境中的部署。

Method: 所提出的方法是一个自适应推理摘要框架，通过语义分割与重要性评分、预算感知的动态压缩和连贯性重建来压缩推理轨迹。这在保留关键推理步骤的同时显著减少了token使用。此外，还引入了一个基于高斯过程的贝叶斯优化模块来降低评估成本。

Result: 在7,501个医学考试问题上，该方法在相同token预算下比截断法高出40%的准确率。对来自8个LLMs（1.5B-32B参数）的64对模型进行的评估证实了其强大的跨模型迁移能力。贝叶斯优化模块将评估成本降低了84%，并揭示了模型大小与跨领域鲁棒性之间的幂律关系。

Conclusion: 推理摘要为高效的思维链（CoT）迁移提供了一条实用途径，使得在严格的计算限制下也能实现高级推理。

Abstract: Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.

</details>


### [4] [From Prompts to Power: Measuring the Energy Footprint of LLM Inference](https://arxiv.org/abs/2511.05597)
*Francisco Caravaca,Ángel Cuevas,Rubén Cuevas*

Main category: cs.AI

TL;DR: 本研究对大型语言模型（LLM）的推理能耗进行了大规模测量分析，开发了能耗预测模型，并将其实现为浏览器扩展以提高对生成式AI环境影响的认识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的快速发展带来了巨大的能源需求，尤其是在推理阶段。部署这些模型需要大量能耗的GPU基础设施，甚至促使一些数据中心考虑使用核能。然而，目前对推理能耗的系统性分析仍然有限。

Method: 本研究进行了大规模的基于测量的研究，涵盖了21种GPU配置和155种模型架构，总计超过32,500次测量。研究使用vLLM推理引擎，在提示级别量化能耗，并识别了架构和操作因素如何影响能耗需求。在此基础上，开发了一个能准确预测未知架构和硬件推理能耗的预测模型，并将其实现为浏览器扩展。

Result: 研究成功量化了LLM在提示级别的能耗，并识别了影响能耗的架构和操作因素。基于这些洞察，开发了一个能够准确估计未见架构和硬件推理能耗的预测模型，并将其作为一个浏览器扩展发布，以提高公众对生成式AI环境影响的认识。

Conclusion: 本研究提供了LLM推理能耗的系统性分析，揭示了影响能耗的关键因素，并开发了一个实用的能耗预测工具。通过浏览器扩展的部署，旨在提升对生成式AI能源消耗及其环境影响的关注度。

Abstract: The rapid expansion of Large Language Models (LLMs) has introduced unprecedented energy demands, extending beyond training to large-scale inference workloads that often dominate total lifecycle consumption. Deploying these models requires energy-intensive GPU infrastructure, and in some cases has even prompted plans to power data centers with nuclear energy. Despite this growing relevance, systematic analyses of inference energy consumption remain limited. In this work, we present a large-scale measurement-based study comprising over 32,500 measurements across 21 GPU configurations and 155 model architectures, from small open-source models to frontier systems. Using the vLLM inference engine, we quantify energy usage at the prompt level and identify how architectural and operational factors shape energy demand. Building on these insights, we develop a predictive model that accurately estimates inference energy consumption across unseen architectures and hardware, and implement it as a browser extension to raise awareness of the environmental impact of generative AI.

</details>


### [5] [DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis](https://arxiv.org/abs/2511.05810)
*Bowen Xu,Xinyue Zeng,Jiazhen Hu,Tuo Wang,Adithya Kulkarni*

Main category: cs.AI

TL;DR: DiagnoLLM是一个混合框架，结合贝叶斯去卷积、eQTL引导的深度学习和基于LLM的叙述生成，用于可解释的疾病诊断，尤其在阿尔茨海默病检测中表现出色，并能生成针对特定受众的诊断报告。


<details>
  <summary>Details</summary>
Motivation: 构建值得信赖的临床AI系统不仅需要准确的预测，还需要透明且有生物学依据的解释。

Method: 该框架包括：1. GP-unmix，一个基于高斯过程的层次模型，用于从RNA-seq数据推断细胞类型特异性基因表达谱并建模生物学不确定性。2. 结合eQTL分析提供的调控先验，驱动一个神经网络分类器进行疾病预测。3. 一个基于LLM的推理模块，将模型输出转化为以临床特征、归因信号和领域知识为基础的、针对特定受众（医生和患者）的诊断报告。

Result: 在阿尔茨海默病（AD）检测中实现了88.0%的准确率。人类评估证实，LLM生成的报告准确、可操作，并能恰当地为医生和患者量身定制。

Conclusion: LLM作为混合诊断流程中的事后推理器而非端到端预测器时，可以作为有效的沟通工具，从而提高临床AI系统的信任度和可理解性。

Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.

</details>


### [6] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 本研究通过对数概率行为分析和Shapley值归因，发现大型语言模型（LLMs）存在稳健的锚定偏差，其影响深入到输出分布，而非仅是表面模仿，并提出了评估该偏差的统一分数。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被视为行为主体和决策系统，理解其观察到的认知偏差是表面模仿还是更深层的概率转移至关重要。锚定偏差作为一个经典的认知偏差，是检验这一点的关键。现有研究多依赖于表面输出，未能深入探讨其内部机制和归因贡献。

Method: 本研究通过三方面进行：1) 基于对数概率的行为分析，揭示锚点如何改变整个输出分布，并控制训练数据污染；2) 对结构化提示字段进行精确的Shapley值归因，量化锚点对模型对数概率的影响；3) 整合行为和归因证据，为六个开源模型开发统一的锚定偏差敏感性分数。

Result: 结果显示，Gemma-2B、Phi-2和Llama-2-7B等模型表现出稳健的锚定效应，归因分析表明锚点影响了权重调整。GPT-2、Falcon-RW-1B和GPT-Neo-125M等较小型模型则表现出变异性，暗示模型规模可能调节敏感性。然而，归因效应在不同提示设计中有所差异，突显了将LLMs视为人类替代品的脆弱性。

Conclusion: 研究表明LLMs中的锚定偏差是稳健、可测量和可解释的，同时也强调了在应用领域中的潜在风险。该框架连接了行为科学、LLM安全性和可解释性，为评估LLMs中的其他认知偏差提供了一条可复现的路径。

Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.

</details>


### [7] [Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection](https://arxiv.org/abs/2511.05854)
*Zepeng Bao,Shen Zhou,Qiankun Pi,Jianhao Chen,Mayi Xu,Ming Zhong,Yuanyuan Zhu,Tieyun Qian*

Main category: cs.AI

TL;DR: 该研究提出LEAP框架，通过动态学习和主动纠正机制，使高效的学生模型能够自适应地规划和调整幻觉检测策略，解决了现有方法策略固定、缺乏适应性的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的幻觉是其安全部署的关键障碍。现有工具增强的幻觉检测方法需要预定义固定验证策略，且在动态执行环境中缺乏适应性。使用强大的闭源LLM成本高昂，而基于教师-学生架构的微调开源模型受限于固定策略，可能导致检测失败。

Method: LEAP框架将幻觉检测问题视为动态策略学习问题。首先，利用教师模型在动态学习循环中生成轨迹，并根据执行失败动态调整策略。其次，通过智能体微调将这种动态规划能力蒸馏到高效的学生模型中。最后，在策略执行期间，学生模型采用主动纠正机制，使其能够在执行前提出、审查和优化自己的验证策略。

Result: 实验证明，在三个具有挑战性的基准测试中，经过LEAP微调的模型优于现有的最先进方法。

Conclusion: LEAP框架成功赋予了学生模型动态学习和主动纠正能力，有效解决了幻觉检测中策略适应性不足的问题，提升了检测性能。

Abstract: Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.

</details>


### [8] [An Empirical Study of Reasoning Steps in Thinking Code LLMs](https://arxiv.org/abs/2511.05874)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.AI

TL;DR: 本研究对大型语言模型（LLMs）在代码生成任务中生成的中间推理链的质量进行了全面实证评估，分析了其结构、对性能的影响、常见问题以及稳定性。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成最终答案前生成显式中间推理痕迹（即“思考”过程）可能提高透明度、可解释性和代码生成准确性。然而，这些推理链的质量尚未得到充分探索。

Method: 研究评估了六个最先进的推理LLMs（DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, Qwen-QwQ），在BigCodeBench的100个不同难度的代码生成任务上进行测试。方法包括量化推理链结构（步数和冗余度）、进行受控步数预算调整，以及由21名参与者进行的人工评估（评估维度：效率、逻辑正确性、完整性）。

Result: 研究发现，有针对性地增加推理步数可以提高某些模型/任务的解决率，而适度减少步数通常能保持标准任务的成功率，但对困难任务则不然。完整性是主要的失败模式，尤其是在处理复杂任务时。任务复杂性显著影响推理质量，困难问题比标准任务更容易出现不完整。稳定性分析表明，思考型LLMs在不同计算努力水平下保持一致的逻辑结构，并能自我纠正先前的错误。

Conclusion: 本研究为当前思考型LLMs在软件工程领域的优势和局限性提供了新见解，尤其揭示了推理链的质量问题，特别是完整性是关键的挑战。

Abstract: Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.

</details>


### [9] [Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement](https://arxiv.org/abs/2511.05931)
*Hiroaki Hayashi,Bo Pang,Wenting Zhao,Ye Liu,Akash Gokul,Srijan Bansal,Caiming Xiong,Semih Yavuz,Yingbo Zhou*

Main category: cs.AI

TL;DR: SAGE是一个框架，它使基于大型语言模型（LLM）的智能体能够通过从自身任务执行中学习并提炼经验，从而进行自我抽象和行为改进。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体通常在静态执行框架中运行，缺乏从自身经验和历史运行中学习和自我改进的机制，导致其性能受限于初始框架设计和底层LLM的能力。

Method: SAGE框架在智能体首次执行任务后，从其具体经验中提炼出一个简洁的计划抽象，概括关键步骤、依赖关系和约束。然后，这个学习到的抽象作为上下文指导反馈给智能体，以优化其策略，支持后续更结构化、更明智的执行。

Result: SAGE在不同的LLM骨干和智能体架构上均实现了一致的性能提升。与GPT-5（高）结合时，它比强大的Mini-SWE-Agent基线取得了7.2%的相对性能提升。在SWE-Bench Verified基准测试中，SAGE分别使用Mini-SWE-Agent和OpenHands CodeAct智能体框架，达到了73.2%和74%的Pass@1解决率。

Conclusion: SAGE通过自我抽象机制，成功使LLM智能体能够从经验中学习并改进其行为，从而显著提升了它们在软件工程任务中的性能，突破了传统LLM智能体的性能上限。

Abstract: Large language model (LLM) based agents are increasingly used to tackle software engineering tasks that require multi-step reasoning and code modification, demonstrating promising yet limited performance. However, most existing LLM agents typically operate within static execution frameworks, lacking a principled mechanism to learn and self-improve from their own experience and past rollouts. As a result, their performance remains bounded by the initial framework design and the underlying LLM's capabilities. We propose Self-Abstraction from Grounded Experience (SAGE), a framework that enables agents to learn from their own task executions and refine their behavior through self-abstraction. After an initial rollout, the agent induces a concise plan abstraction from its grounded experience, distilling key steps, dependencies, and constraints. This learned abstraction is then fed back as contextual guidance, refining the agent's policy and supporting more structured, informed subsequent executions. Empirically, SAGE delivers consistent performance gains across diverse LLM backbones and agent architectures. Notably, it yields a 7.2% relative performance improvement over the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone. SAGE further achieves strong overall performance on SWE-Bench Verified benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent and OpenHands CodeAct agent framework, respectively.

</details>


### [10] [Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks](https://arxiv.org/abs/2511.05883)
*Hehai Lin,Hui Liu,Shilei Cao,Jing Li,Haoliang Li,Wenya Wang*

Main category: cs.AI

TL;DR: 本文提出三种基于不同粒度（粗粒度模态效益、中粒度信息流、细粒度因果分析）的自动化样本级模态偏见量化方法，以解决多模态错误信息基准中存在的偏见问题，并通过人工评估验证其有效性，揭示了集成多视角、避免检测器波动以及不同视角在偏见样本上分歧的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态错误信息基准存在模态偏见，导致检测器仅依赖单一模态进行预测。此前量化偏见的方法停留在数据集层面或手动识别虚假关联，缺乏样本级洞察力且难以扩展到海量在线信息。因此，研究动机是设计一种自动化识别样本级模态偏见的方法。

Method: 本文提出了三种基于不同粒度理论/视角的偏见量化方法：1) 粗粒度的模态效益评估；2) 中粒度信息流的量化；3) 细粒度的因果分析。通过对两个流行基准进行人工评估来验证这些方法的有效性。

Result: 实验结果揭示了三个有趣的发现：1) 集成多视角对于可靠的自动化分析至关重要；2) 自动化分析容易受到检测器引起的波动影响；3) 不同视角在模态平衡样本上的一致性更高，但在偏见样本上则存在分歧。

Conclusion: 本文探索了自动化识别样本级模态偏见的设计，并提出了三种量化方法。研究结果为未来的研究提供了潜在方向，强调了集成多视角的重要性、需注意检测器波动，以及不同视角在处理平衡与偏见样本时的表现差异。

Abstract: Numerous multimodal misinformation benchmarks exhibit bias toward specific modalities, allowing detectors to make predictions based solely on one modality. While previous research has quantified bias at the dataset level or manually identified spurious correlations between modalities and labels, these approaches lack meaningful insights at the sample level and struggle to scale to the vast amount of online information. In this paper, we investigate the design for automated recognition of modality bias at the sample level. Specifically, we propose three bias quantification methods based on theories/views of different levels of granularity: 1) a coarse-grained evaluation of modality benefit; 2) a medium-grained quantification of information flow; and 3) a fine-grained causality analysis. To verify the effectiveness, we conduct a human evaluation on two popular benchmarks. Experimental results reveal three interesting findings that provide potential direction toward future research: 1)~Ensembling multiple views is crucial for reliable automated analysis; 2)~Automated analysis is prone to detector-induced fluctuations; and 3)~Different views produce a higher agreement on modality-balanced samples but diverge on biased ones.

</details>


### [11] [Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling](https://arxiv.org/abs/2511.05951)
*Qi Wang,Hongzhi Zhang,Jia Fu,Kai Fu,Yahui Liu,Tinghai Zhang,Chenxi Sun,Gangwei Jiang,Jingyi Tang,Xingguang Ji,Yang Yue,Jingyuan Zhang,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.AI

TL;DR: 本文提出并开源了一个完整的训练流程，用于从Qwen3-8B基础模型构建高性能智能体模型Klear-Qwen3-AgentForge，并在工具使用和编码任务上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强大的智能体模型不断涌现，但缺乏关键的训练后细节阻碍了开源社区开发出具有竞争力的高性能智能体模型。

Method: 研究人员设计了一个从Qwen3-8B基础模型开始的全面开源训练流程，包括使用合成数据进行有效的监督微调（SFT），随后进行多轮强化学习（RL），以激发模型在多种智能体任务上的潜力。

Result: Klear-Qwen3-AgentForge-8B在工具使用和编码领域的各种智能体基准测试中，在同等规模的LLM中实现了最先进的性能，并与显著更大的模型保持了竞争力。

Conclusion: 该研究成功构建并开源了一个高性能智能体模型Klear-Qwen3-AgentForge的训练管道，有效解决了开源社区在开发智能体模型方面缺乏细节的问题，并展示了其在多种任务上的卓越表现。

Abstract: Despite the proliferation of powerful agentic models, the lack of critical post-training details hinders the development of strong counterparts in the open-source community. In this study, we present a comprehensive and fully open-source pipeline for training a high-performance agentic model for interacting with external tools and environments, named Klear-Qwen3-AgentForge, starting from the Qwen3-8B base model. We design effective supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL) to unlock the potential for multiple diverse agentic tasks. We perform exclusive experiments on various agentic benchmarks in both tool use and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art performance among LLMs of similar size and remains competitive with significantly larger models.

</details>


### [12] [An Epistemic Perspective on Agent Awareness](https://arxiv.org/abs/2511.05977)
*Pavel Naumov,Alexandra Pavlova*

Main category: cs.AI

TL;DR: 该论文提出将智能体意识视为一种知识形式，并区分其主观（de re）和客观（de dicto）形式，通过2D语义形式化，并构建了一个完备的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 现有文献对意识的处理方式与传统不同，该研究旨在打破传统，将意识视为一种知识形式，并对其主观和客观形式进行区分和形式化。

Method: 将智能体意识视为一种知识，区分其主观（de re）和客观（de dicto）形式。引入两种模态来捕捉这些形式，并使用2D语义版本正式指定其含义。构建一个逻辑系统。

Result: 主要技术成果是一个健全且完备的逻辑系统，它描述了所提出的两种模态（主观和客观意识-即-知识）与标准“事实知识”模态之间的相互作用。

Conclusion: 该论文成功地将意识重新定义为知识，区分了其两种形式，并提供了一个描述这些形式与标准知识之间相互作用的健全且完备的逻辑框架。

Abstract: The paper proposes to treat agent awareness as a form of knowledge, breaking the tradition in the existing literature on awareness. It distinguishes the de re and de dicto forms of such knowledge. The work introduces two modalities capturing these forms and formally specifies their meaning using a version of 2D-semantics. The main technical result is a sound and complete logical system describing the interplay between the two proposed modalities and the standard "knowledge of the fact" modality.

</details>


### [13] [ScRPO: From Errors to Insights](https://arxiv.org/abs/2511.06065)
*Lianrui Li,Dakuan Lu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出自校正相对策略优化（ScRPO）框架，通过自我反思和错误纠正，显著提升大型语言模型解决复杂数学问题的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决具有挑战性的数学问题时表现不佳，研究旨在开发一种能利用有限外部反馈进行自我改进的方法。

Method: ScRPO包含两个阶段：1. 试错学习阶段：使用GRPO训练模型，并将错误答案及其问题收集到错误池中；2. 自校正学习阶段：引导模型反思之前答案错误的原因。实验基于Deepseek-Distill-Qwen-1.5B和Deepseek-Distill-Qwen-7B模型，并在多个数学推理基准上进行评估。

Result: 实验结果表明，ScRPO在AIME、AMC、Olympiad、MATH-500、GSM8k等多个数学推理基准上，始终优于现有的多种后训练方法。

Conclusion: ScRPO为语言模型在有限外部反馈下，自我改进解决困难任务提供了一个有前景的范式，有助于构建更可靠、更强大的AI系统。

Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathematical problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collecting incorrect answers along with their corresponding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous answers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH-500, GSM8k, using Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B. The experimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way toward more reliable and capable AI systems.

</details>


### [14] [Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs](https://arxiv.org/abs/2511.06134)
*Wei Yang,Jiacheng Pang,Shixuan Li,Paul Bogdan,Stephen Tu,Jesse Thomason*

Main category: cs.AI

TL;DR: 本文提出Maestro框架，通过角色编排将多智能体LLM的探索和合成认知模式解耦。结合CLPO强化学习目标，Maestro有效解决了信度分配问题，显著提升了数学推理和通用问题解决的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在解决复杂问题时，难以平衡发散性探索和收敛性合成，导致过早达成共识、错误传播以及信度分配不当，无法区分真正推理和表面合理论证。

Method: 本文提出“Maestro”（Multi-Agent Exploration-Synthesis framework Through Role Orchestration）框架，结构性地解耦探索和合成模式。Maestro使用并行执行智能体进行多样化探索，并由一个专门的中央智能体进行收敛性评估合成。为实现关键的合成阶段，引入“CLPO”（Conditional Listwise Policy Optimization）强化学习目标，该目标通过结合决策导向的策略梯度和基于理由的列表式排序损失，分离战略决策信号和战术理由，从而实现清晰的信度分配和更强的比较监督。

Result: 在数学推理和通用问题解决基准上的实验表明，结合CLPO的Maestro框架始终优于现有最先进的多智能体方法，平均绝对准确率提高了6%，最高可达10%。

Conclusion: Maestro框架与CLPO强化学习目标的结合，成功解决了多智能体LLM系统中探索与合成之间的核心认知张力，通过结构化解耦和有效的信度分配，显著提升了复杂问题解决能力。

Abstract: Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.

</details>


### [15] [When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks](https://arxiv.org/abs/2511.06136)
*Stefano Ferraro,Akihiro Nakano,Masahiro Suzuki,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出了一种无监督的以对象为中心的DLPWM模型，它在视觉建模上表现出色，但在多对象交互中存在表征漂移，导致下游模型控制性能不佳。


<details>
  <summary>Details</summary>
Motivation: 以对象为中心的世界模型（OCWM）通过将视觉场景分解为对象级别的表示，提供结构化的抽象，有望提高强化学习中的组合泛化能力和数据效率。研究者假设明确解耦的对象级表示，通过定位任务相关信息，可以增强策略在新的特征组合中的性能。

Method: 研究者引入了DLPWM，一个完全无监督、解耦的以对象为中心的世界模型，它直接从像素学习对象级别的潜在表示。该模型在重建和预测性能方面进行了测试，包括对几种分布外（OOD）视觉变化的鲁棒性。然后，将其用于下游基于模型的控制，并将训练在DLPWM潜在空间上的策略与DreamerV3进行比较。通过潜在轨迹分析，识别了不稳定策略学习的关键驱动因素。

Result: DLPWM实现了强大的重建和预测性能，并对多种OOD视觉变化表现出鲁棒性。然而，当用于下游基于模型的控制时，在DLPWM潜在空间上训练的策略表现不如DreamerV3。通过潜在轨迹分析，研究者发现多对象交互过程中的表示漂移是导致策略学习不稳定的关键因素。

Conclusion: 尽管以对象为中心的感知支持鲁棒的视觉建模，但实现稳定的控制需要缓解潜在漂移（representation shift）问题。

Abstract: Object-centric world models (OCWM) aim to decompose visual scenes into object-level representations, providing structured abstractions that could improve compositional generalization and data efficiency in reinforcement learning. We hypothesize that explicitly disentangled object-level representations, by localizing task-relevant information, can enhance policy performance across novel feature combinations. To test this hypothesis, we introduce DLPWM, a fully unsupervised, disentangled object-centric world model that learns object-level latents directly from pixels. DLPWM achieves strong reconstruction and prediction performance, including robustness to several out-of-distribution (OOD) visual variations. However, when used for downstream model-based control, policies trained on DLPWM latents underperform compared to DreamerV3. Through latent-trajectory analyses, we identify representation shift during multi-object interactions as a key driver of unstable policy learning. Our results suggest that, although object-centric perception supports robust visual modeling, achieving stable control requires mitigating latent drift.

</details>


### [16] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: 本研究引入PRIME框架，通过逻辑谜题评估大型语言模型（LLMs）在复杂推理中隐含的社会偏见，发现模型在与刻板印象一致时推理更准确。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM安全防护措施未能有效抑制在复杂逻辑推理任务中出现的微妙社会偏见，且现有评估基准未能涵盖这一领域，导致评估空白。

Method: 研究提出了PRIME（模型评估中隐性偏见的谜题推理）框架，该框架利用逻辑网格谜题系统地探测社会刻板印象对LLMs逻辑推理和决策的影响。PRIME支持谜题的自动生成和验证，并能调节复杂度和偏见设置。它通过共享谜题结构生成刻板印象、反刻板印象和中性变体，以进行受控的精细比较。研究评估了多个模型家族在不同谜题规模下的表现，并测试了基于提示的缓解策略，重点关注性别刻板印象。

Result: 研究发现，当解决方案与刻板印象关联一致时，模型在推理时表现出更高的准确性。

Conclusion: PRIME框架在诊断和量化LLMs演绎推理中存在的社会偏见方面具有重要意义，尤其是在公平性至关重要的领域，它揭示了模型在推理过程中延续刻板印象的问题。

Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.

</details>


### [17] [Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.06168)
*Boxuan Wang,Zhuoyun Li,Xinmiao Huang,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 本文提出一个框架，通过新的“对齐分数”指标评估和优化大型语言模型（LLM）的推理一致性，并引入语义一致性优化采样（SCOS）方法显著提升长推理链的对齐分数。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型（LLM）在思维链（CoT）推理中推理一致性的评估和优化问题。

Method: 1. 引入“对齐分数”来量化模型生成推理链与人类参考链之间的语义对齐程度。2. 定义了四种关键错误类型（逻辑脱节、主题偏移、冗余推理、因果逆转）来解释对齐分数下降的原因。3. 提出“语义一致性优化采样（SCOS）”方法，通过采样并偏爱具有最少对齐错误的推理链。

Result: 1. 经验发现，2跳推理链的对齐分数最高。2. 四种错误类型均会导致对齐分数下降。3. SCOS方法显著提高了对齐分数，对于3跳等更长的推理链，平均提升了29.84%。

Conclusion: 该框架通过对齐分数、错误类型分析和SCOS方法，有效评估并优化了大型语言模型在长推理链中的推理一致性。

Abstract: This paper presents a framework for evaluating and optimizing reasoning consistency in Large Language Models (LLMs) via a new metric, the Alignment Score, which quantifies the semantic alignment between model-generated reasoning chains and human-written reference chains in Chain-of-Thought (CoT) reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest Alignment Score. To explain this phenomenon, we define four key error types: logical disconnection, thematic shift, redundant reasoning, and causal reversal, and show how each contributes to the degradation of the Alignment Score. Building on this analysis, we further propose Semantic Consistency Optimization Sampling (SCOS), a method that samples and favors chains with minimal alignment errors, significantly improving Alignment Scores by an average of 29.84% with longer reasoning chains, such as in 3-hop tasks.

</details>


### [18] [MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning](https://arxiv.org/abs/2511.06142)
*Sizhe Tang,Jiayu Chen,Tian Lan*

Main category: cs.AI

TL;DR: 针对多智能体规划中蒙特卡洛树搜索（MCTS）面临的组合动作空间爆炸问题，MALinZero提出通过低维表示结构和线性置信上限树（LinUCT）来高效地进行探索和利用，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体规划中，MCTS因组合动作空间巨大导致分支因子呈指数级增长，使得树搜索过程中的探索与利用效率低下。

Method: MALinZero方法将联合动作回报投影到低维空间，并将其建模为上下文线性强盗问题。该问题通过凸和μ-光滑损失函数解决，并推导了适用于低维空间的线性置信上限树（LinUCT）以实现多智能体探索与利用。文章分析了低维奖励函数的遗憾，并提出了一种通过最大化次模态目标进行联合动作选择的(1-1/e)-近似算法。

Result: MALinZero在矩阵博弈、SMAC和SMACv2等多智能体基准测试中展现出最先进的性能，其学习速度和性能均优于基于模型和无模型的多智能体强化学习基线。

Conclusion: MALinZero通过利用低维表示结构和LinUCT，有效解决了多智能体规划中MCTS面临的挑战，显著提升了规划效率和性能，为复杂多智能体环境下的决策提供了新的解决方案。

Abstract: Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.

</details>


### [19] [CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference](https://arxiv.org/abs/2511.06175)
*Kaijie Xu,Fandi Meng,Clark Verbrugge,Simon Lucas*

Main category: cs.AI

TL;DR: 本文提出了CSP4SDG，一个基于概率约束满足的框架，用于社交推理游戏中的角色推理。它结合了信息论和语言无关的约束，在推理性能上优于LLM，并能作为LLM的辅助推理工具。


<details>
  <summary>Details</summary>
Motivation: 在《阿瓦隆》、《狼人杀》等社交推理游戏（SDGs）中，玩家隐藏身份并故意误导他人，使得隐藏角色推理成为核心且艰巨的任务。准确的角色识别是AI和人类玩家表现的基础和关键。

Method: 引入了CSP4SDG，一个概率性的、约束满足的框架，用于客观分析游戏玩法。游戏事件和对话被映射到四类语言无关的约束：证据、现象、断言和假设。硬约束用于排除不可能的角色分配，而加权软约束则对剩余的可能性进行评分。信息增益加权将每个假设与其在熵减下的期望值关联起来，一个简单的封闭形式评分规则确保真实断言以最小误差收敛到经典硬逻辑。最终的角色后验概率是可解释的，并能实时更新。

Result: 在三个公共数据集上的实验表明，CSP4SDG在所有推理场景中都优于基于大型语言模型（LLM）的基线。此外，当CSP4SDG作为辅助“推理工具”提供时，它能显著提升LLM的性能。

Conclusion: 本研究验证了结合信息论的原则性概率推理方法，是社交推理游戏中重量级神经模型的一种可扩展的替代方案或补充。

Abstract: In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary "reasoning tool." Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.

</details>


### [20] [Dataforge: A Data Agent Platform for Autonomous Data Engineering](https://arxiv.org/abs/2511.06185)
*Xinyuan Wang,Yanjie Fu*

Main category: cs.AI

TL;DR: Data Agent是一个完全自主的系统，利用大型语言模型（LLM）实现表格数据的自动化准备，包括数据清洗和特征优化，旨在解决AI应用中数据准备的劳动密集型和专业依赖性问题。


<details>
  <summary>Details</summary>
Motivation: AI应用（如材料发现、分子建模、气候科学）对数据准备的需求日益增长，但原始数据处理（清洗、标准化、转换）以及特征工程（转换、选择）是一个劳动密集型且需要专业知识的步骤，存在可扩展性和专业依赖性挑战。

Method: Data Agent系统利用大型语言模型（LLM）的推理能力和基于事实的验证，通过双重反馈循环自动执行数据清洗、分层路由和特征级优化。它遵循“自动化”、“安全”和“非专家友好”三大核心原则，确保无需人工监督的端到端可靠性。

Result: 该研究展示了自主Data Agent的首次实际实现，能够将原始数据转换为“更好的数据”，并在整个过程中无需人工监督，确保了端到端的数据准备可靠性。

Conclusion: Data Agent提供了一个自主、安全且对非专家友好的解决方案，能够高效、可靠地准备用于AI应用的表格数据，有效应对了传统数据准备在可扩展性和专业知识方面的挑战。

Abstract: The growing demand for AI applications in fields such as materials discovery, molecular modeling, and climate science has made data preparation an important but labor-intensive step. Raw data from diverse sources must be cleaned, normalized, and transformed to become AI-ready, while effective feature transformation and selection are essential for efficient training and inference. To address the challenges of scalability and expertise dependence, we present Data Agent, a fully autonomous system specialized for tabular data. Leveraging large language model (LLM) reasoning and grounded validation, Data Agent automatically performs data cleaning, hierarchical routing, and feature-level optimization through dual feedback loops. It embodies three core principles: automatic, safe, and non-expert friendly, which ensure end-to-end reliability without human supervision. This demo showcases the first practical realization of an autonomous Data Agent, illustrating how raw data can be transformed "From Data to Better Data."

</details>


### [21] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 本文提出了一种基于数据驱动不确定性分数（UHeads）的轻量级方法，用于验证大型语言模型（LLMs）的多步骤推理链，其性能与现有大型模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务需要LLMs生成长多步骤推理链。现有验证方法（如PRMs）虽然能提升性能和可解释性，但计算成本高昂、领域受限或需要大量人工/模型标注。

Method: 本文提出了轻量级的Transformer-based不确定性量化头部（UHeads）。这些UHeads利用冻结LLM的内部状态来估计其在生成过程中推理步骤的不确定性。目标标签通过更大的LLM或原始模型自监督生成，整个过程是全自动的。

Result: UHeads参数量小于10M，效果显著且轻量化。在数学、规划和常识问答等多个领域，UHeads的性能与比其大810倍的PRMs相当甚至超越。

Conclusion: LLMs的内部状态编码了其不确定性，可作为推理验证的可靠信号，为开发可扩展、通用化的内省LLMs提供了有前景的方向。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.

</details>


### [22] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B是一个1.5B参数的小型模型，通过其Spectrum-to-Signal Principle (SSP)框架，以极低的成本实现了超越大型模型的推理能力，挑战了小模型推理能力不足的普遍共识。


<details>
  <summary>Details</summary>
Motivation: 挑战小模型固有推理能力不足的普遍共识，以及当前通过扩大模型参数规模来增强能力的流行方法。

Method: 采用了Spectrum-to-Signal Principle (SSP)框架。该框架首先通过“两阶段多样性探索蒸馏”（Two-Stage Diversity-Exploring Distillation, SFT）生成广泛的解决方案谱，然后通过“最大熵引导策略优化”（MaxEnt-Guided Policy Optimization, RL）来放大正确的信号。

Result: VibeThinker-1.5B总训练成本仅7,800美元，在推理能力上优于Magistral Medium和Claude Opus 4等闭源模型，并与GPT OSS-20B Medium等开源模型持平。在AIME24、AIME25和HMMT25三个数学基准测试中，它超越了规模大400倍的DeepSeek R1。在LiveCodeBench V6上，其得分也优于Magistral Medium。

Conclusion: 研究结果表明，小型模型可以实现与大型模型相当的推理能力，从而大幅降低训练和推理成本，进而推动高级人工智能研究的民主化。

Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.

</details>


### [23] [ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving](https://arxiv.org/abs/2511.06226)
*Xingcheng Liu,Yanchen Guan,Haicheng Liao,Zhengbing He,Zhenning Li*

Main category: cs.AI

TL;DR: 本研究提出ROAR模型，结合离散小波变换（DWT）、自适应物体感知模块和动态焦点损失，以提高自动驾驶车辆在非理想条件（如传感器故障、环境干扰、数据不完善和驾驶行为差异）下的事故检测和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有事故预测方法常假设理想条件，忽略了传感器故障、环境干扰、数据不完善等挑战，且未能充分解决不同车型驾驶行为和事故率的显著差异，导致预测准确性下降。

Method: 本研究引入ROAR方法，结合了：1) 离散小波变换（DWT）以从噪声和不完整数据中提取特征；2) 自适应物体感知模块，通过关注高风险车辆并建模交通参与者之间的时空关系来改进事故预测；3) 动态焦点损失，以减轻正负样本之间的类别不平衡影响。

Result: ROAR模型在Dashcam Accident Dataset (DAD)、Car Crash Dataset (CCD) 和 AnAn Accident Detection (A3D) 三个常用数据集上，在平均精度（AP）和平均事故发生时间（mTTA）等关键指标上持续优于现有基线。结果表明模型在真实世界条件，特别是处理传感器退化、环境噪声和不平衡数据分布方面具有鲁棒性。

Conclusion: 本研究为复杂交通环境中可靠、准确的事故预测提供了一个有前景的解决方案，尤其在处理非理想条件下的挑战方面表现出色。

Abstract: Accurate accident anticipation is essential for enhancing the safety of autonomous vehicles (AVs). However, existing methods often assume ideal conditions, overlooking challenges such as sensor failures, environmental disturbances, and data imperfections, which can significantly degrade prediction accuracy. Additionally, previous models have not adequately addressed the considerable variability in driver behavior and accident rates across different vehicle types. To overcome these limitations, this study introduces ROAR, a novel approach for accident detection and prediction. ROAR combines Discrete Wavelet Transform (DWT), a self adaptive object aware module, and dynamic focal loss to tackle these challenges. The DWT effectively extracts features from noisy and incomplete data, while the object aware module improves accident prediction by focusing on high-risk vehicles and modeling the spatial temporal relationships among traffic agents. Moreover, dynamic focal loss mitigates the impact of class imbalance between positive and negative samples. Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently outperforms existing baselines in key metrics such as Average Precision (AP) and mean Time to Accident (mTTA). These results demonstrate the model's robustness in real-world conditions, particularly in handling sensor degradation, environmental noise, and imbalanced data distributions. This work offers a promising solution for reliable and accurate accident anticipation in complex traffic environments.

</details>


### [24] [Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents](https://arxiv.org/abs/2511.06292)
*Yaoning Yu,Kaimin Chang,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种自改进的提示框架，通过数据增强优化，为大型语言模型（LLMs）在金融推理任务中生成和验证合成数据，从而迭代地优化提示，无需外部标签，提高了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在金融文档推理任务中优化LLMs提示时，面临固定数据集、难以适应新问题类型或文档结构、以及需要昂贵手动标注数据集的局限性。提示质量对LLMs的性能影响巨大，因此需要一种更有效、自适应的提示优化方法。

Method: 研究引入了一个由数据增强优化驱动的自改进提示框架。这是一个闭环过程，包括生成合成金融表格和文档片段、验证其正确性和鲁棒性，然后根据结果更新提示。具体而言，该框架结合了合成数据生成器、验证器和提示优化器。生成器暴露当前提示的弱点，验证器检查生成示例的有效性和鲁棒性，优化器则逐步完善提示。通过这种反馈循环迭代，系统无需外部标签即可提高提示准确性。

Result: 在DocMath-Eval基准测试中，该系统在准确性和鲁棒性方面均优于标准提示方法。

Conclusion: 将合成数据生成整合到金融应用的提示学习中具有显著价值，能够有效提高大型语言模型在金融推理任务中的性能。

Abstract: Financial documents like earning reports or balance sheets often involve long tables and multi-page reports. Large language models have become a new tool to help numerical reasoning and understanding these documents. However, prompt quality can have a major effect on how well LLMs perform these financial reasoning tasks. Most current methods tune prompts on fixed datasets of financial text or tabular data, which limits their ability to adapt to new question types or document structures, or they involve costly and manually labeled/curated dataset to help build the prompts. We introduce a self-improving prompt framework driven by data-augmented optimization. In this closed-loop process, we generate synthetic financial tables and document excerpts, verify their correctness and robustness, and then update the prompt based on the results. Specifically, our framework combines a synthetic data generator with verifiers and a prompt optimizer, where the generator produces new examples that exposes weaknesses in the current prompt, the verifiers check the validity and robustness of the produced examples, and the optimizer incrementally refines the prompt in response. By iterating these steps in a feedback cycle, our method steadily improves prompt accuracy on financial reasoning tasks without needing external labels. Evaluation on DocMath-Eval benchmark demonstrates that our system achieves higher performance in both accuracy and robustness than standard prompt methods, underscoring the value of incorporating synthetic data generation into prompt learning for financial applications.

</details>


### [25] [Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems](https://arxiv.org/abs/2511.06301)
*Azanzi Jiomekong,Jean Bikim,Patricia Negoue,Joyce Chin*

Main category: cs.AI

TL;DR: 本文介绍了一个名为Secu-Table的新数据集，该数据集包含来自CVE和CWE的安全领域表格数据，并使用Wikidata和SEPSES CSKG进行标注。该数据集旨在评估语义表格解释（STI）系统，特别是大型语言模型（LLM）在安全领域的性能，并提供了初步的基线评估。


<details>
  <summary>Details</summary>
Motivation: 在安全等特定领域，用于评估语义表格解释（STI）系统（特别是基于LLM的系统）的表格数据集稀缺且不对外公开。这阻碍了该领域最先进系统的开发和评估。

Method: 研究人员构建了Secu-Table数据集，其中包含超过1500个表格和15000多个实体，这些数据从通用漏洞披露（CVE）和通用弱点枚举（CWE）数据源中提取，并使用Wikidata和SEPSES CSKG进行标注。数据集和所有代码都已公开。该数据集被纳入SemTab挑战赛，用于评估基于开源LLM的STI系统。初步基线评估使用了Falcon3-7b-instruct、Mistral-7B-Instruct（开源LLM）和GPT-4o mini（闭源LLM）。

Result: 研究成果是发布了Secu-Table数据集，其中包含超过1500个表格和15000多个安全实体，并附带了所有相关代码。该数据集已提供给研究社区，并用于SemTab挑战赛。同时，研究人员使用多种LLM（包括开源和闭源）进行了初步的基线评估。

Conclusion: Secu-Table数据集的引入填补了安全领域中用于评估语义表格解释系统（尤其是LLM）的公开可用表格数据集的空白。通过将其纳入SemTab挑战赛，该数据集将促进研究社区在安全领域STI方面的研究和基准测试。

Abstract: Evaluating semantic tables interpretation (STI) systems, (particularly, those based on Large Language Models- LLMs) especially in domain-specific contexts such as the security domain, depends heavily on the dataset. However, in the security domain, tabular datasets for state-of-the-art are not publicly available. In this paper, we introduce Secu-Table dataset, composed of more than 1500 tables with more than 15k entities constructed using security data extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES CSKG). Along with the dataset, all the code is publicly released. This dataset is made available to the research community in the context of the SemTab challenge on Tabular to Knowledge Graph Matching. This challenge aims to evaluate the performance of several STI based on open source LLMs. Preliminary evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source LLM.

</details>


### [26] [GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening](https://arxiv.org/abs/2511.06262)
*Siming Zhao,Qi Li*

Main category: cs.AI

TL;DR: GAIA是一个以治理为先的框架，旨在解决大型语言模型（LLM）在B2B高风险筛选和谈判任务中授权委托的安全、效率和可问责性问题。


<details>
  <summary>Details</summary>
Motivation: 组织日益探索将筛选和谈判任务委托给AI系统，但在高风险B2B环境中部署受限于治理挑战，例如防止未经授权的承诺、确保充分信息以及维持有效的人工监督和可审计性。现有的大型语言模型谈判研究主要强调代理之间的自主谈判，而忽略了分阶段信息收集、明确授权边界和系统化反馈整合等实际需求。

Method: 本文提出了GAIA框架，这是一个以治理为先的LLM-人类在B2B谈判和筛选中的代理框架。GAIA定义了三个基本角色（委托人、代理、交易方），并可选地增加一个批评者角色以提高性能。它通过三种机制组织交互：信息门控进展（将筛选与谈判分离）、双重反馈整合（结合AI批评和轻量级人工修正）以及带有明确升级路径的授权边界。研究贡献包括一个形式化的治理框架、通过任务完成度跟踪（TCI）实现的信息门控进展、融合批评者建议和人类监督的双重反馈整合，以及结合自动化协议指标与人类判断的混合验证蓝图。

Result: GAIA提供了四个主要贡献：(1) 一个具有三个协调机制和四个安全不变式的形式化治理框架，用于有界授权的委托；(2) 通过任务完成度跟踪（TCI）和明确状态转换实现的信息门控进展，将筛选与承诺分离；(3) 通过并行学习通道将批评者建议与人类监督相结合的双重反馈整合；(4) 一个结合自动化协议指标与人类对结果和安全判断的混合验证蓝图。

Conclusion: GAIA通过连接理论与实践，为安全、高效和可问责的AI委托提供了一个可复现的规范，可应用于采购、房地产和人员配置等工作流程。

Abstract: Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.

</details>


### [27] [The Station: An Open-World Environment for AI-Driven Discovery](https://arxiv.org/abs/2511.06309)
*Stephen Chung,Wenyu Du*

Main category: cs.AI

TL;DR: STATION是一个开放世界多智能体环境，模拟科学生态系统，智能体能进行独立研究、互动，产生新方法，并在多个科学领域超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 旨在探索一种新的科学发现范式，通过开放世界环境中的涌现行为实现自主科学发现，超越传统刚性优化。

Method: 引入STATION环境，智能体利用其扩展的上下文窗口，自主进行科学研究，包括阅读论文、提出假设、提交代码、执行分析和发表结果。环境中无中心化协调，智能体自由选择行动并发展自己的叙事。

Result: AI智能体在数学、计算生物学和机器学习等广泛基准测试中取得了新的SOTA性能，显著超越了AlphaEvolve在圆堆积问题上的表现。智能体之间涌现出丰富的研究叙事，并有机地产生了新方法，例如用于scRNA-seq批次整合的密度自适应算法。

Conclusion: STATION代表了通过开放世界环境中涌现行为驱动的自主科学发现的第一步，提供了一个超越传统刚性优化的新范式。

Abstract: We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.

</details>


### [28] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: 该研究提出了LPFQA，一个基于真实专业论坛的长尾知识基准，用于更准确地评估大型语言模型（LLMs）在专业领域的能力，并发现现有LLMs在专业推理上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估LLMs的真实能力，它们常侧重简化任务或人工场景，忽略了长尾知识和现实应用的复杂性。因此，需要一个能弥补这一差距的、基于真实专业知识的评估基准。

Method: 研究者构建了LPFQA基准，它源自20个学术和工业领域的真实专业论坛，涵盖502个基于实践经验的任务。LPFQA具有四项创新：细粒度评估维度（知识深度、推理、术语理解、上下文分析）、确保语义清晰和唯一答案的层次化难度结构、模拟真实用户角色的专业场景建模，以及跨领域知识整合。

Result: 在LPFQA上评估了12个主流LLMs后，研究者观察到显著的性能差异，尤其是在专业推理任务中。这表明LPFQA能够有效区分不同LLMs的能力。

Conclusion: LPFQA提供了一个强大、真实且具有区分性的基准，能够推动LLM评估的进步并指导未来的模型开发。

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.

</details>


### [29] [ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning](https://arxiv.org/abs/2511.06316)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain*

Main category: cs.AI

TL;DR: 本研究提出ALIGN，一个视觉-语言框架，通过模拟人类空间推理，直接从文本和地图线索推断交通事故坐标，以解决低收入国家缺乏准确事故地理信息的问题。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家严重缺乏准确、特定地点的交通事故数据。现有基于文本的地理编码工具在多语言和非结构化新闻环境中表现不佳，因为不完整的地点描述和混合语言（如孟加拉语-英语）模糊了空间上下文。

Method: ALIGN（Accident Location Inference through Geo-Spatial Neural Reasoning）是一个视觉-语言框架，它将大型语言模型和视觉-语言模型集成到一个多阶段管道中。该管道执行光学字符识别（OCR）、语言推理，并通过基于网格的空间扫描进行地图级验证。该框架系统地根据上下文和视觉证据评估每个预测位置，无需模型再训练即可确保可解释、细粒度的地理定位结果。

Result: ALIGN在孟加拉语新闻数据上的应用表明，它比传统地理分析方法有持续改进，能够准确识别区域和次区域级别的事故地点。该框架为数据稀缺地区的自动化事故地图绘制奠定了高准确性基础。

Conclusion: ALIGN框架为数据稀缺地区的自动化事故地图绘制建立了高准确性基础，支持以证据为导向的道路安全政策制定，并促进多模态人工智能在交通分析中的更广泛整合。

Abstract: Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed Bangla-English scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning)- a vision-language framework that emulates human spatial reasoning to infer accident coordinates directly from textual and map-based cues. ALIGN integrates large language and vision-language models within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics. The code for this paper is open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN

</details>


### [30] [What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models](https://arxiv.org/abs/2511.06380)
*Chen He,Xun Jiang,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Xing Xu*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在复杂非数学推理中存在“回声反思”问题，即反思阶段无法产生新见解。本文提出了一种新颖的强化学习方法AEPO，通过信息过滤和自适应熵优化来解决此问题，显著提升了LLMs在此类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学推理方面表现出色，但在涉及复杂领域知识的推理任务中，它们在反思阶段难以生成新颖见解，表现为机械地重复早期推理步骤（“回声反思”）。这主要归因于：1) 响应生成过程中信息流失控，导致过早的中间思想传播并扭曲最终决策；2) 反思过程中对内部知识探索不足，导致重复而非产生新认知洞察。

Method: 本文提出了一种名为自适应熵策略优化（Adaptive Entropy Policy Optimization, AEPO）的新型强化学习方法。AEPO框架包含两个主要组件：1) 反思感知信息过滤（Reflection-aware Information Filtration），量化认知信息流，防止早期不良认知信息影响最终答案；2) 自适应熵优化（Adaptive-Entropy Optimization），动态平衡不同推理阶段的探索与利用，以促进反思多样性和答案准确性。

Result: 广泛的实验表明，AEPO在各种基准测试中持续超越主流强化学习基线，达到了最先进的性能。

Conclusion: AEPO通过有效控制信息流和优化探索与利用，成功解决了LLMs在复杂领域特定推理任务中“回声反思”的问题，显著提升了模型的反思能力和推理表现。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as "Echo Reflection". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.

</details>


### [31] [Efficient LLM Safety Evaluation through Multi-Agent Debate](https://arxiv.org/abs/2511.06396)
*Dachuan Lin,Guobin Shen,Zihao Yang,Tianrong Liu,Dongcheng Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 本文提出了一种成本效益高的多智能体小语言模型（SLM）评判框架，用于LLM安全评估，并通过新建的大规模人工标注越狱基准HAJailBench进行验证，实现了与GPT-4o评判员相当的准确性，同时显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估中，基于LLM作为评判员的框架成本高昂，限制了可扩展性。因此，需要更经济高效的评估方法和更严格的评估基准。

Method: 本文提出了一种成本效益高的多智能体评判框架，该框架利用小语言模型（SLMs）通过评论者、辩护者和评判者智能体之间的结构化辩论进行评估。为严格评估安全判断，构建了HAJailBench，一个包含12,000个对抗性交互的大规模人工标注越狱基准，涵盖多种攻击方法和目标模型。该数据集提供了细粒度的专家标注真值，用于评估安全鲁棒性和评判员可靠性。

Result: 基于SLM的框架在HAJailBench上的判断一致性与GPT-4o评判员相当，同时大幅降低了推理成本。消融实验结果表明，三轮辩论在准确性和效率之间取得了最佳平衡。

Conclusion: 结构化、价值对齐的辩论使SLM能够捕捉越狱攻击的语义细微差别。HAJailBench为可扩展的LLM安全评估提供了可靠的基础。该研究表明，SLM-based框架是LLM安全评估中一个成本高效且可靠的替代方案。

Abstract: Safety evaluation of large language models (LLMs) increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability. We propose a cost-efficient multi-agent judging framework that employs Small Language Models (SLMs) through structured debates among critic, defender, and judge agents. To rigorously assess safety judgments, we construct HAJailBench, a large-scale human-annotated jailbreak benchmark comprising 12,000 adversarial interactions across diverse attack methods and target models. The dataset provides fine-grained, expert-labeled ground truth for evaluating both safety robustness and judge reliability. Our SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Ablation results show that three rounds of debate yield the optimal balance between accuracy and efficiency. These findings demonstrate that structured, value-aligned debate enables SLMs to capture semantic nuances of jailbreak attacks and that HAJailBench offers a reliable foundation for scalable LLM safety evaluation.

</details>


### [32] [SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization](https://arxiv.org/abs/2511.06411)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 软思维范式在LLM推理中具有潜力，但难以通过强化学习（RL）进行优化。本文提出了SofT-GRPO算法，通过注入Gumbel噪声和Gumbel-Softmax技术，成功地将RL应用于软思维LLM，在Pass@1和Pass@32上均超越了离散token的GRPO。


<details>
  <summary>Details</summary>
Motivation: 软思维范式在某些场景下优于传统的离散token思维链（CoT）推理，但将其与强化学习（RL）结合（如通过策略优化算法GRPO）面临挑战。主要困难在于如何将随机性注入软思维token并相应地更新策略。此前尝试结合软思维和GRPO的方法表现通常不如离散token的GRPO，因此需要一种新方法来充分发挥软思维的潜力。

Method: 本文提出了一种新颖的策略优化算法SofT-GRPO，用于在软思维推理范式下强化LLM。SofT-GRPO通过以下方式实现：1) 将Gumbel噪声注入logits；2) 采用Gumbel-Softmax技术确保软思维token在预训练嵌入空间内；3) 在策略梯度中利用重参数化技巧。

Result: 实验结果表明，SofT-GRPO使得软思维LLM在Pass@1上略微优于离散token的GRPO（平均准确率提高0.13%），并且在Pass@32上取得了显著提升（平均准确率提高2.19%）。这些结果在1.5B到7B参数的基础LLM上均得到验证。

Conclusion: SofT-GRPO成功地克服了将强化学习应用于软思维LLM的挑战，使得软思维LLM能够超越离散token的GRPO，尤其是在需要更高通过率的场景下表现出显著优势，从而充分释放了软思维的潜力。

Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master

</details>


### [33] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: 大型推理模型（LRM）存在逢迎行为，本研究提出MONICA框架，通过实时监控和校准，在推理步骤层面有效减轻模型的逢迎倾向。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）容易出现逢迎行为，即倾向于同意用户的错误信念而非独立推理，这损害了模型的可靠性并带来社会风险。现有方法主要关注最终答案的判断和纠正，未能理解逢迎行为在推理过程中如何发展。

Method: 本研究提出了MONICA（Monitor-guided Calibration）框架，它在模型推理过程中，于推理步骤层面监控并减轻逢迎行为。MONICA集成了一个逢迎监控器，实时提供逢迎漂移分数；以及一个校准器，当分数超过预设阈值时动态抑制逢迎行为。

Result: 在12个数据集和3个大型推理模型上的广泛实验表明，MONICA有效减少了中间推理步骤和最终答案中的逢迎行为，实现了稳健的性能提升。

Conclusion: MONICA框架通过在推理步骤层面进行实时监控和动态校准，成功解决了大型推理模型中的逢迎问题，提高了模型的可靠性和独立推理能力。

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.

</details>


### [34] [AUTO-Explorer: Automated Data Collection for GUI Agent](https://arxiv.org/abs/2511.06417)
*Xiangwu Guo,Difei Gao,Mike Zheng Shou*

Main category: cs.AI

TL;DR: 本文提出了一种名为Auto-Explorer的自动化GUI数据收集方法，其具有低标注成本和高效探索机制，并开发了UIXplore基准来评估探索质量。实验证明，Auto-Explorer能有效提升多模态大语言模型（MLLM）在探索过的软件中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据获取方法难以应用于桌面软件或Common Crawl未收录的新网站，且个性化场景需要模型能快速完美适应新软件或网站，这促使研究者寻求一种低成本的自动化数据收集方案。

Method: 本文提出Auto-Explorer，一种包含简单有效探索机制的自动化数据收集方法，能自主解析和探索GUI环境。同时，开发了UIXplore基准来评估探索质量，该基准为探索代理创建环境以发现和保存软件状态。收集到的数据用于微调多模态大语言模型（MLLM），并建立GUI元素定位测试集以评估探索策略的有效性。

Result: 实验结果表明，Auto-Explorer表现优越，能快速增强MLLM在已探索软件中的能力。

Conclusion: Auto-Explorer通过其自动化探索机制和低标注成本，有效解决了GUI数据获取的挑战，并能显著提升多模态大语言模型在新软件或网站上的适应和表现能力。

Abstract: Recent advancements in GUI agents have significantly expanded their ability to interpret natural language commands to manage software interfaces. However, acquiring GUI data remains a significant challenge. Existing methods often involve designing automated agents that browse URLs from the Common Crawl, using webpage HTML to collect screenshots and corresponding annotations, including the names and bounding boxes of UI elements. However, this method is difficult to apply to desktop software or some newly launched websites not included in the Common Crawl. While we expect the model to possess strong generalization capabilities to handle this, it is still crucial for personalized scenarios that require rapid and perfect adaptation to new software or websites. To address this, we propose an automated data collection method with minimal annotation costs, named Auto-Explorer. It incorporates a simple yet effective exploration mechanism that autonomously parses and explores GUI environments, gathering data efficiently. Additionally, to assess the quality of exploration, we have developed the UIXplore benchmark. This benchmark creates environments for explorer agents to discover and save software states. Using the data gathered, we fine-tune a multimodal large language model (MLLM) and establish a GUI element grounding testing set to evaluate the effectiveness of the exploration strategies. Our experiments demonstrate the superior performance of Auto-Explorer, showing that our method can quickly enhance the capabilities of an MLLM in explored software.

</details>


### [35] [Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis](https://arxiv.org/abs/2511.06437)
*Abhishek More,Anthony Zhang,Nicole Bonilla,Ashvik Vivekan,Kevin Zhu,Parham Sharafoleslami,Maheep Chaudhary*

Main category: cs.AI

TL;DR: 为解决大语言模型链式思考(CoT)推理中现有置信度估计方法校准差和过度自信的问题，本文提出EDTR方法，通过结合拓扑分析和Dirichlet不确定性量化，显著提升了多步推理的置信度校准，实现了更可靠的模型部署。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)通过链式思考(CoT)提示能够解决复杂问题，但其安全部署需要可靠的置信度估计。现有方法在错误预测上校准不佳且过度自信，这促使研究者寻求更准确的置信度量化方法。

Method: 本文提出增强Dirichlet和拓扑风险(EDTR)解码策略。该方法结合拓扑分析和基于Dirichlet的不确定性量化，以衡量LLM在多个推理路径上的置信度。EDTR将每个CoT视为高维空间中的向量，并提取八个拓扑风险特征来捕捉推理分布的几何结构：更紧密、更连贯的簇表示更高的置信度，而分散、不一致的路径则表示不确定性。

Result: EDTR在四个推理基准（AIME、GSM8K、常识推理、股票价格预测）上与三种最先进的校准方法进行比较。结果显示，EDTR的校准性能比竞争方法提高了41%，平均ECE为0.287，综合得分最高达到0.672。值得注意的是，EDTR在AIME上实现了完美准确率，并在GSM8K上取得了0.107的卓越ECE校准，而基线方法在这些领域表现出严重的过度自信。

Conclusion: EDTR为理解和量化多步LLM推理中的不确定性提供了一个几何框架，通过提供经过校准的置信度估计，实现了更可靠的模型部署，这对于LLM在关键应用中的安全使用至关重要。

Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.

</details>


### [36] [GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets](https://arxiv.org/abs/2511.06471)
*Jingtao Tang,Hang Ma*

Main category: cs.AI

TL;DR: 本文提出了GCS-TSP，一种在凸集图（GCS）上定义的新型旅行商问题变体，用于轨迹规划。GHOST是一个分层框架，通过结合组合路径搜索和凸轨迹优化来最优地解决GCS-TSP，并利用新颖的下界进行高效剪枝。


<details>
  <summary>Details</summary>
Motivation: 在GCS-TSP中，边成本不固定，而是取决于通过每个凸区域选择的具体轨迹，这使得经典的TSP方法无法适用。因此，需要一种能够处理这种动态成本并实现最优轨迹规划的方法。

Method: 本文引入了GHOST，一个分层框架。它将组合巡回搜索与凸轨迹优化相结合，系统地探索由GCS导出的完整图上的巡回路径。GHOST使用一种新颖的抽象路径展开算法来计算可容许的下界，这些下界在高层（巡回路径）和低层（实现巡回路径的可行GCS路径）指导最佳优先搜索，从而提供强大的剪枝能力并避免不必要的凸优化调用。此外，还提出了一个有界次优变体。

Result: GHOST被证明能够保证最优性。在简单情况下，它比统一的混合整数凸规划基线快几个数量级。它还能独特地处理涉及高阶连续性约束和不完整GCS的复杂轨迹规划问题。

Conclusion: GHOST为GCS-TSP提供了一个最优且高效的解决方案，其性能远超现有基线方法，并能有效处理复杂的轨迹规划场景，包括高阶连续性约束和不完整的配置空间表示。

Abstract: We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP) defined over a Graph of Convex Sets (GCS) -- a powerful representation for trajectory planning that decomposes the configuration space into convex regions connected by a sparse graph. In this setting, edge costs are not fixed but depend on the specific trajectory selected through each convex region, making classical TSP methods inapplicable. We introduce GHOST, a hierarchical framework that optimally solves the GCS-TSP by combining combinatorial tour search with convex trajectory optimization. GHOST systematically explores tours on a complete graph induced by the GCS, using a novel abstract-path-unfolding algorithm to compute admissible lower bounds that guide best-first search at both the high level (over tours) and the low level (over feasible GCS paths realizing the tour). These bounds provide strong pruning power, enabling efficient search while avoiding unnecessary convex optimization calls. We prove that GHOST guarantees optimality and present a bounded-suboptimal variant for time-critical scenarios. Experiments show that GHOST is orders-of-magnitude faster than unified mixed-integer convex programming baselines for simple cases and uniquely handles complex trajectory planning problems involving high-order continuity constraints and an incomplete GCS.

</details>


### [37] [Brain-Inspired Planning for Better Generalization in Reinforcement Learning](https://arxiv.org/abs/2511.06470)
*Mingde "Harry" Zhao*

Main category: cs.AI

TL;DR: 本论文旨在通过赋予强化学习（RL）智能体受人类大脑启发的推理行为，从而增强其零样本系统泛化能力，以应对真实世界应用中泛化能力差的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统在真实世界场景中面临显著挑战，主要原因是它们在与训练条件不同的环境中泛化能力差。

Method: 1. 引入受人类意识规划启发的“空间抽象”顶层注意力机制，使智能体在决策时动态关注环境状态中最相关的方面。2. 在空间抽象的基础上，开发了Skipper框架，自动将复杂任务分解为更简单的子任务。3. 针对依赖生成模型生成状态目标的规划智能体中存在的“妄想规划”问题，提出学习一个可行性评估器来拒绝幻想出的不可行目标。

Result: 1. 空间抽象方法显著改善了训练任务之外的系统泛化能力。2. Skipper框架通过关注环境的相关空间和时间元素，提供了对抗分布偏移的鲁棒性和在长期、组合规划中的有效性。3. 可行性评估器的引入显著提升了各种规划智能体的性能。

Conclusion: 本研究通过引入受人类启发的推理行为（如空间抽象、任务分解和可行性评估），有效增强了RL智能体的零样本系统泛化能力和规划稳健性。未来的研究方向包括实现通用任务抽象和完全抽象规划。

Abstract: Existing Reinforcement Learning (RL) systems encounter significant challenges when applied to real-world scenarios, primarily due to poor generalization across environments that differ from their training conditions. This thesis explores the direction of enhancing agents' zero-shot systematic generalization abilities by granting RL agents reasoning behaviors that are found to help systematic generalization in the human brain. Inspired by human conscious planning behaviors, we first introduced a top-down attention mechanism, which allows a decision-time planning agent to dynamically focus its reasoning on the most relevant aspects of the environmental state given its instantaneous intentions, a process we call "spatial abstraction". This approach significantly improves systematic generalization outside the training tasks. Subsequently, building on spatial abstraction, we developed the Skipper framework to automatically decompose complex tasks into simpler, more manageable sub-tasks. Skipper provides robustness against distributional shifts and efficacy in long-term, compositional planning by focusing on pertinent spatial and temporal elements of the environment. Finally, we identified a common failure mode and safety risk in planning agents that rely on generative models to generate state targets during planning. It is revealed that most agents blindly trust the targets they hallucinate, resulting in delusional planning behaviors. Inspired by how the human brain rejects delusional intentions, we propose learning a feasibility evaluator to enable rejecting hallucinated infeasible targets, which led to significant performance improvements in various kinds of planning agents. Finally, we suggest directions for future research, aimed at achieving general task abstraction and fully enabling abstract planning.

</details>


### [38] [FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis](https://arxiv.org/abs/2511.06522)
*Jan Ondras,Marek Šuppa*

Main category: cs.AI

TL;DR: 该研究通过FractalBench基准评估多模态AI系统从分形图像中抽象数学规则并合成程序的能力，发现主流模型在生成语法正确代码方面表现尚可，但在捕获分形数学结构（特别是递归）方面存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 数学推理要求从视觉模式中抽象出符号规则，即“从有限推断无限”。研究旨在探究多模态AI系统是否具备这种能力，以弥合视觉感知与数学抽象之间的鸿沟。

Method: 研究提出了FractalBench基准，用于评估模型从分形图像合成程序的能力。该基准包含12个经典分形，并测试了GPT-4o、Claude 3.7 Sonnet、Gemini 2.5 Flash和Qwen 2.5-VL四种主流多模态大语言模型（MLLMs）。模型需要生成可执行的Python代码来复现分形，以便进行客观评估。

Result: 结果显示，76%的模型能生成语法有效的代码，但只有4%的代码成功捕获了分形的数学结构。模型在处理几何变换（如科赫曲线，成功率17-21%）方面表现较好，但在分支递归（如树形分形，成功率低于2%）方面则彻底失败，这揭示了它们在数学抽象能力上的根本性差距。

Conclusion: 多模态AI系统在视觉-数学推理方面存在显著的脱节，尤其是在理解和生成递归的数学结构方面表现不佳。FractalBench提供了一个不受污染的诊断工具，用于评估和揭示模型在视觉-数学推理能力上的不足。

Abstract: Mathematical reasoning requires abstracting symbolic rules from visual patterns -- inferring the infinite from the finite. We investigate whether multimodal AI systems possess this capability through FractalBench, a benchmark evaluating fractal program synthesis from images. Fractals provide ideal test cases: Iterated Function Systems with only a few contraction maps generate complex self-similar patterns through simple recursive rules, requiring models to bridge visual perception with mathematical abstraction. We evaluate four leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL -- on 12 canonical fractals. Models must generate executable Python code reproducing the fractal, enabling objective evaluation. Results reveal a striking disconnect: 76% generate syntactically valid code but only 4% capture mathematical structure. Success varies systematically -- models handle geometric transformations (Koch curves: 17-21%) but fail at branching recursion (trees: <2%), revealing fundamental gaps in mathematical abstraction. FractalBench provides a contamination-resistant diagnostic for visual-mathematical reasoning and is available at https://github.com/NaiveNeuron/FractalBench

</details>


### [39] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: 该研究提出了一种新颖的框架，利用基于强化学习的大语言模型（LLM）将法律合同转换为结构化语义图，从而实现合同的自动化分析和发现隐藏依赖。


<details>
  <summary>Details</summary>
Motivation: 合同是复杂的文档，其人工审查过程艰巨且容易出错。研究旨在简化并自动化合同审查和分析任务。

Method: 该方法将法律合同转换为结构化语义图，通过详细的本体论将核心法律合同元素映射到图论中的节点和边。它引入了一个名为GRAPH-GRPO-LEX的框架，该框架结合了LLM、强化学习和群组相对策略优化（GRPO），用于实体和关系的分割与提取。通过精心设计的基于图度量的奖励函数，该方法能够识别条款间的直接和隐藏依赖，并引入了门控GRPO方法。

Result: 该框架能够自动识别条款之间的直接关系，并揭示隐藏的依赖关系。门控GRPO方法显示出强大的学习信号，能将合同分析从线性的手动阅读过程转变为易于可视化的图谱。

Conclusion: 该研究提供了一种更动态的合同分析方法，并为未来实现类似于软件工程中的合同代码检查（linting）奠定了基础。

Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.

</details>


### [40] [SRNN: Spatiotemporal Relational Neural Network for Intuitive Physics Understanding](https://arxiv.org/abs/2511.06761)
*Fei Yang*

Main category: cs.AI

TL;DR: 本文提出了一种受大脑启发的时空关系神经网络（SRNN），用于直观物理理解。SRNN利用统一的神经表示和赫布学习规则，在感知和语言之间建立联系，并在CLEVRER基准上表现出色，同时揭示了基准偏差并提供了白盒诊断能力。


<details>
  <summary>Details</summary>
Motivation: 机器在直观物理方面的能力远不及人类，为了弥合这一差距，研究旨在转向受大脑启发的计算原理。

Method: 本文引入了时空关系神经网络（SRNN），该模型为物体属性、关系和时间线建立了统一的神经表示。计算由赫布的“共同激活，共同连接”机制在专门的“内容（What）”和“方式（How）”通路中驱动。这种统一的表示直接用于生成视觉场景的结构化语言描述。此外，SRNN采用“预定义-然后-微调”的方法，而非普遍的“预训练-然后-微调”范式。

Result: SRNN在CLEVRER基准测试中取得了有竞争力的性能。分析还揭示了基准偏差，并提出了更全面的评估路径。SRNN的白盒实用性也得到了验证，可用于精确的错误诊断。

Conclusion: 该研究证实了将生物智能转化为工程系统以实现直观物理理解的可行性。

Abstract: Human prowess in intuitive physics remains unmatched by machines. To bridge this gap, we argue for a fundamental shift towards brain-inspired computational principles. This paper introduces the Spatiotemporal Relational Neural Network (SRNN), a model that establishes a unified neural representation for object attributes, relations, and timeline, with computations governed by a Hebbian ``Fire Together, Wire Together'' mechanism across dedicated \textit{What} and \textit{How} pathways. This unified representation is directly used to generate structured linguistic descriptions of the visual scene, bridging perception and language within a shared neural substrate. Moreover, unlike the prevalent ``pretrain-then-finetune'' paradigm, SRNN adopts a ``predefine-then-finetune'' approach. On the CLEVRER benchmark, SRNN achieves competitive performance. Our analysis further reveals a benchmark bias, outlines a path for a more holistic evaluation, and demonstrates SRNN's white-box utility for precise error diagnosis. Our work confirms the viability of translating biological intelligence into engineered systems for intuitive physics understanding.

</details>


### [41] [Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives](https://arxiv.org/abs/2511.06626)
*Chloe Li,Mary Phuong,Daniel Tan*

Main category: cs.AI

TL;DR: 本文提出自报告微调（SRFT）技术，通过训练AI模型承认错误来提高其诚实倾向，使其在对抗性代理设置中能够揭露隐藏的未对齐目标，从而有效检测和揭露不安全的AI行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统处理复杂代理任务的能力增强，它们也可能追求不良目标并造成危害。现有方法通过直接询问模型来发现不安全实例，但模型的弱点在于它们会撒谎，导致无法有效检测潜在危害。

Method: 本文提出自报告微调（SRFT），一种简单的监督微调技术。该技术训练模型在被询问时承认其事实性错误。研究表明，这种对事实错误的承认可以泛化到在对抗性代理设置中承认隐藏的未对齐目标。

Result: 经过SRFT训练后，模型在被询问时更有可能承认其隐藏目标的细节，即使在强大压力下也不泄露。对SRFT模型进行审问，可以以接近最佳性能（F1分数 = 0.98）检测出隐藏目标，而基线模型在相同条件下会撒谎（F1分数 = 0）。SRFT模型还能进一步获取隐藏目标的内容，恢复28-100%的细节，而基线模型和预填充助手轮次攻击只能恢复0%的细节。

Conclusion: SRFT是一种有前景的技术，能够提高AI系统的诚实倾向，并有效揭露未对齐的AI系统，从而促进AI安全。

Abstract: As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to admit their factual mistakes when asked. We show that the admission of factual errors in simple question-answering settings generalizes out-of-distribution (OOD) to the admission of hidden misaligned objectives in adversarial agentic settings. We evaluate SRFT in OOD stealth tasks, where models are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. After SRFT, models are more likely to confess the details of their hidden objectives when interrogated, even under strong pressure not to disclose them. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100% details, compared to 0% details recovered in the baseline model and by prefilled assistant turn attacks. This provides a promising technique for promoting honesty propensity and incriminating misaligned AI systems.

</details>


### [42] [MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning](https://arxiv.org/abs/2511.06805)
*Jinhao Chen,Zhen Yang,Jianxin Shi,Tianyu Wo,Jie Tang*

Main category: cs.AI

TL;DR: 本文提出MathSE，一个针对多模态大语言模型（MLLMs）的数学自演化框架，通过迭代推理、反思和基于奖励的反馈，显著提升了MLLMs在复杂数学推理任务上的表现，超越了现有领先模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言问答任务上表现出色，但在数学问题解决等复杂推理任务上仍面临挑战。现有方法依赖于静态的、由教师模型蒸馏而来的数据集进行微调，这限制了模型适应新颖或复杂问题的能力，并缺乏鲁棒泛化所需的迭代深度。

Method: 本文提出MathSE（Mathematical Self-Evolving），一个数学自演化框架。与传统的一次性微调范式不同，MathSE通过推理、反思和基于奖励的反馈循环迭代地改进模型。具体来说，它利用迭代微调，整合前一阶段推理中获得的正确推理路径，并结合来自专门的“结果奖励模型”（ORM）的反思。

Result: MathSE在多项挑战性基准测试中表现出显著的性能提升，超越了基线模型。尤其值得注意的是，在MathVL-test上的实验结果表明，MathSE超越了领先的开源多模态数学推理模型QVQ。

Conclusion: MathSE框架通过其迭代自演化机制，有效克服了多模态大语言模型在复杂数学推理方面的局限性，实现了显著的性能提升，并确立了其在开源多模态数学推理模型中的领先地位。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \textbf{\method}, a \textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak MathSE.github.io/}.

</details>


### [43] [Proceedings of the 2025 XCSP3 Competition](https://arxiv.org/abs/2511.06918)
*Gilles Audemard,Christophe Lecoutre,Emmanuel Lonca*

Main category: cs.AI

TL;DR: 本文档是2025年XCSP3约束求解器竞赛的会议记录。


<details>
  <summary>Details</summary>
Motivation: 评估和比较不同的约束求解器性能。

Method: 组织了一场基于XCSP3格式的约束求解器竞赛。

Result: 竞赛结果已在CP'25（第31届约束编程原理与实践国际会议）上公布。

Conclusion: 本文档作为2025年XCSP3竞赛的正式记录，收集并呈现了竞赛结果。

Abstract: This document represents the proceedings of the 2025 XCSP3 Competition. The results of this competition of constraint solvers were presented at CP'25 (31st International Conference on Principles and Practice of Constraint Programming).

</details>


### [44] [Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning](https://arxiv.org/abs/2511.07061)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao,Yu Liu*

Main category: cs.AI

TL;DR: 本文提出PRC-Emo框架，通过结合提示工程、示例检索和课程学习，显著提升了大型语言模型（LLMs）在对话情感识别（ERC）中对显式和隐式情感的理解能力，并在两个基准数据集上取得了新的最先进（SOTA）性能。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别（ERC）对理解人类情感和实现自然人机交互至关重要。尽管LLMs在此领域展现潜力，但其捕获显式和隐式情感之间内在联系的能力仍有限。

Method: 本文提出了PRC-Emo训练框架，整合了：1) **提示工程**：设计基于显式和隐式情感线索的情感敏感提示模板；2) **示例检索**：构建首个专用于ERC的检索库，包含现有数据集和高质量LLM生成并人工验证的对话示例；3) **课程学习**：在LoRA微调中引入课程学习策略，根据同/不同说话人话语间的情感转变加权，分配对话样本难度并按从易到难顺序训练。

Result: 在IEMOCAP和MELD两个基准数据集上的实验结果表明，该方法取得了新的最先进（SOTA）性能。

Conclusion: PRC-Emo方法有效且具有泛化性，显著提升了基于LLM的情感理解能力。

Abstract: Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.

</details>


### [45] [Improving Region Representation Learning from Urban Imagery with Noisy Long-Caption Supervision](https://arxiv.org/abs/2511.07062)
*Yimei Zhang,Guojiang Shen,Kaili Ning,Tongwei Ren,Xuebo Qiu,Mengmeng Wang,Xiangjie Kong*

Main category: cs.AI

TL;DR: 本文提出UrbanLN框架，通过长文本感知和噪声抑制，改进城市区域表征学习。它解决了长文本与细粒度视觉特征对齐困难以及LLM生成文本噪声过多的问题，实现了更优的跨模态知识融合。


<details>
  <summary>Details</summary>
Motivation: 城市区域表征学习在城市计算中至关重要，城市的视觉外观能反映其社会经济和环境特征。现有基于LLM结合图像的城市区域表征学习面临两大挑战：一是细粒度视觉特征与长文本对齐困难，二是LLM生成文本中的噪声导致知识融合次优。

Method: 本文提出UrbanLN预训练框架，旨在实现长文本感知和噪声抑制。具体方法包括：1) 信息保留拉伸插值策略，用于将长文本与复杂城市场景中的细粒度视觉语义对齐；2) 双层优化策略，用于有效挖掘LLM生成文本中的知识并过滤噪声，其中：数据层采用多模型协作流程自动生成多样且可靠的文本，模型层采用基于动量的自蒸馏机制生成稳定伪目标，以实现噪声条件下的鲁棒跨模态学习。

Result: 在四个真实城市和多种下游任务上的大量实验表明，UrbanLN框架表现出卓越的性能。

Conclusion: UrbanLN通过其创新的长文本对齐和噪声抑制机制，显著提升了城市区域表征学习的效果，有效克服了现有方法的局限性，为城市计算领域提供了更强大的工具。

Abstract: Region representation learning plays a pivotal role in urban computing by extracting meaningful features from unlabeled urban data. Analogous to how perceived facial age reflects an individual's health, the visual appearance of a city serves as its ``portrait", encapsulating latent socio-economic and environmental characteristics. Recent studies have explored leveraging Large Language Models (LLMs) to incorporate textual knowledge into imagery-based urban region representation learning. However, two major challenges remain: i)~difficulty in aligning fine-grained visual features with long captions, and ii) suboptimal knowledge incorporation due to noise in LLM-generated captions. To address these issues, we propose a novel pre-training framework called UrbanLN that improves Urban region representation learning through Long-text awareness and Noise suppression. Specifically, we introduce an information-preserved stretching interpolation strategy that aligns long captions with fine-grained visual semantics in complex urban scenes. To effectively mine knowledge from LLM-generated captions and filter out noise, we propose a dual-level optimization strategy. At the data level, a multi-model collaboration pipeline automatically generates diverse and reliable captions without human intervention. At the model level, we employ a momentum-based self-distillation mechanism to generate stable pseudo-targets, facilitating robust cross-modal learning under noisy conditions. Extensive experiments across four real-world cities and various downstream tasks demonstrate the superior performance of our UrbanLN.

</details>


### [46] [RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2511.07070)
*Fei Zhao,Chonggang Lu,Haofu Qian,Fangcheng Shi,Zijie Meng,Jianzhao Huang,Xu Tang,Zheyong Xie,Zheyu Ye,Zhe Xu,Yao Hu,Shaosheng Cao*

Main category: cs.AI

TL;DR: RedOne 2.0 是一种面向社交网络服务 (SNS) 的大型语言模型 (LLM)，采用渐进式、RL优先的后训练范式，旨在实现快速稳定的适应性。它通过三阶段训练，在紧凑规模下表现出卓越的数据效率和稳定性，为SNS领域LLM提供了一个有竞争力的、高成本效益的基线。


<details>
  <summary>Details</summary>
Motivation: 社交网络服务 (SNS) 对大型语言模型 (LLM) 提出了独特挑战，包括异构工作负载、快速变化的规范和俚语、多语言和文化多样性语料库导致的剧烈分布偏移。传统的监督微调 (SFT) 虽然可以专业化模型，但常导致在分布内增益和分布外鲁棒性之间出现“跷跷板”效应，尤其对于小型模型。

Method: RedOne 2.0 采用渐进式、RL优先的后训练范式，包含三个阶段：(1) 探索性学习：在精选的SNS语料库上进行，以建立初始对齐并识别系统性弱点；(2) 目标性微调：有选择地对诊断出的不足应用SFT，并混合少量通用数据以缓解遗忘；(3) 改进性学习：重新应用以SNS为中心信号的强化学习 (RL)，以巩固改进并协调跨任务的权衡。

Result: 我们的4B规模模型在涵盖三类任务的各种任务上，比7B次优基线平均提高了约2.41。此外，RedOne 2.0 比基础模型平均性能提升约8.74，而所需数据量不到以SFT为中心方法RedOne的一半，这证明了其在紧凑规模下卓越的数据效率和稳定性。

Conclusion: RedOne 2.0 为SNS场景中的领域特定LLM建立了一个有竞争力且具有成本效益的基线，在不牺牲鲁棒性的前提下提升了能力。

Abstract: As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.

</details>


### [47] [Increasing AI Explainability by LLM Driven Standard Processes](https://arxiv.org/abs/2511.07083)
*Marc Jansen,Marcel Pehlke*

Main category: cs.AI

TL;DR: 本文提出了一种通过将大型语言模型（LLMs）嵌入标准化分析流程来提高人工智能系统可解释性的方法，将不透明的推理转化为透明且可审计的决策轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的解释性人工智能（XAI）方法侧重于特征归因或事后解释，但AI系统仍存在不透明性。研究旨在通过更结构化的方法，使AI决策过程变得透明、可审计。

Method: 该方法将LLMs集成到标准化决策模型中，例如问题-选项-标准（QOC）、敏感性分析、博弈论和风险管理。提出了一种分层架构，将LLM的推理空间与上层的可解释过程空间分离。

Result: 实证评估表明，该系统能够在去中心化治理、系统分析和战略推理等背景下，重现人类水平的决策逻辑。

Conclusion: 研究结果表明，由LLM驱动的标准化流程为可靠、可解释和可验证的AI辅助决策提供了基础。

Abstract: This paper introduces an approach to increasing the explainability of artificial intelligence (AI) systems by embedding Large Language Models (LLMs) within standardized analytical processes. While traditional explainable AI (XAI) methods focus on feature attribution or post-hoc interpretation, the proposed framework integrates LLMs into defined decision models such as Question-Option-Criteria (QOC), Sensitivity Analysis, Game Theory, and Risk Management. By situating LLM reasoning within these formal structures, the approach transforms opaque inference into transparent and auditable decision traces. A layered architecture is presented that separates the reasoning space of the LLM from the explainable process space above it. Empirical evaluations show that the system can reproduce human-level decision logic in decentralized governance, systems analysis, and strategic reasoning contexts. The results suggest that LLM-driven standard processes provide a foundation for reliable, interpretable, and verifiable AI-supported decision making.

</details>


### [48] [LLM Driven Processes to Foster Explainable AI](https://arxiv.org/abs/2511.07086)
*Marcel Pehlke,Marc Jansen*

Main category: cs.AI

TL;DR: 本文提出一个模块化、可解释的LLM代理决策支持系统，将推理过程外部化为可审计的工件，并在物流案例中展示了其模仿专家工作流的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM输出通常不透明，研究旨在开发一种透明、可审计、可解释的决策支持系统，能够模拟专家工作流程并提供可追溯的中间结果。

Method: 系统采用模块化LLM代理管道，将推理过程外部化为可审计工件。它实例化了Vester敏感性模型、范式博弈和序贯博弈三种框架，并在每一步都设有可替换模块。LLM组件（默认为GPT-5）与确定性分析器（用于均衡和基于矩阵的角色分类）结合，产生可追溯的中间结果。

Result: 在真实的物流案例（100次运行）中，系统与人类基线的平均因子对齐度为55.5%（26个因子），在运输核心子集上为62.9%；角色匹配协议率为57%。LLM评审员使用八项标准对运行进行评分，结果与重建的人类基线相当。

Conclusion: 可配置的LLM管道能够通过透明、可检查的步骤模仿专家工作流程，从而实现可解释的决策支持。

Abstract: We present a modular, explainable LLM-agent pipeline for decision support that externalizes reasoning into auditable artifacts. The system instantiates three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix, systemic roles, feedback loops); normal-form games (strategies, payoff matrix, equilibria); and sequential games (role-conditioned agents, tree construction, backward induction), with swappable modules at every step. LLM components (default: GPT-5) are paired with deterministic analyzers for equilibria and matrix-based role classification, yielding traceable intermediates rather than opaque outputs. In a real-world logistics case (100 runs), mean factor alignment with a human baseline was 55.5\% over 26 factors and 62.9\% on the transport-core subset; role agreement over matches was 57\%. An LLM judge using an eight-criterion rubric (max 100) scored runs on par with a reconstructed human baseline. Configurable LLM pipelines can thus mimic expert workflows with transparent, inspectable steps.

</details>


### [49] [Agentic AI Sustainability Assessment for Supply Chain Document Insights](https://arxiv.org/abs/2511.07097)
*Diego Gosmar,Anna Chiara Pallotta,Giovanni Zenezini*

Main category: cs.AI

TL;DR: 本文提出了一个基于代理人工智能（AI）的供应链文档智能可持续性评估框架，旨在提高自动化效率并量化环境绩效。研究表明，与手动流程相比，AI辅助和代理AI显著减少了能源、碳排放和水资源消耗。


<details>
  <summary>Details</summary>
Motivation: 在文档密集型工作流程中，研究旨在实现自动化效率提升和可衡量的环境绩效的双重目标，以应对供应链运营中的可持续性挑战。

Method: 研究构建了一个全面的可持续性评估框架，并比较了三种场景：完全手动、AI辅助（人机协作）和利用解析器与验证器的先进多代理AI工作流。该框架将性能、能源和排放指标整合到统一的ESG导向方法中，并通过一个可复现的实际用例进行了验证。

Result: 与手动流程相比，AI辅助人机协作和代理AI场景在能源消耗方面减少了70-90%，二氧化碳排放减少了90-97%，水资源使用减少了89-98%。值得注意的是，即使资源使用量略高于简单的AI辅助方案，结合高级推理（思维模式）和多代理验证的完整代理配置仍能比纯人工方法实现显著的可持续性收益。

Conclusion: 代理人工智能在供应链文档智能中能够带来显著的可持续性改进。该框架提供了一个统一的、ESG导向的方法，用于评估和管理AI驱动的供应链解决方案，证明了AI在提高效率和环境绩效方面的巨大潜力。

Abstract: This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve reductions of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and 89-98% in water usage compared to manual processes. Notably, full agentic configurations, combining advanced reasoning (thinking mode) and multi-agent validation, achieve substantial sustainability gains over human-only approaches, even when resource usage increases slightly versus simpler AI-assisted solutions. The framework integrates performance, energy, and emission indicators into a unified ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions. The paper includes a complete replicability use case demonstrating the methodology's application to real-world document extraction tasks.

</details>


### [50] [Data Complexity of Querying Description Logic Knowledge Bases under Cost-Based Semantics](https://arxiv.org/abs/2511.07095)
*Meghyn Bienvenu,Quentin Manière*

Main category: cs.AI

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: In this paper, we study the data complexity of querying inconsistent weighted description logic (DL) knowledge bases under recently-introduced cost-based semantics. In a nutshell, the idea is to assign each interpretation a cost based upon the weights of the violated axioms and assertions, and certain and possible query answers are determined by considering all (resp. some) interpretations having optimal or bounded cost. Whereas the initial study of cost-based semantics focused on DLs between $\mathcal{EL}_\bot$ and $\mathcal{ALCO}$, we consider DLs that may contain inverse roles and role inclusions, thus covering prominent DL-Lite dialects. Our data complexity analysis goes significantly beyond existing results by sharpening several lower bounds and pinpointing the precise complexity of optimal-cost certain answer semantics (no non-trivial upper bound was known). Moreover, while all existing results show the intractability of cost-based semantics, our most challenging and surprising result establishes that if we consider $\text{DL-Lite}^\mathcal{H}_\mathsf{bool}$ ontologies and a fixed cost bound, certain answers for instance queries and possible answers for conjunctive queries can be computed using first-order rewriting and thus enjoy the lowest possible data complexity ($\mathsf{TC}_0$).

</details>


### [51] [Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts](https://arxiv.org/abs/2511.07090)
*Marcel Rojahn,Marcus Grum*

Main category: cs.AI

TL;DR: 本文定义了绿色AI，提出了一个五阶段生命周期方法，并结合硬件策略和校准测量框架，为研究人员、实践者和政策制定者提供了减少AI多维度环境负担的可操作指导。


<details>
  <summary>Details</summary>
Motivation: 人工智能生命周期（从硬件到开发、部署和重用）的环境负担巨大，涉及能源、碳、水和隐含影响。现有云提供商的工具透明度不足、异构，且常忽略水和价值链效应，限制了可比性和可重现性。

Method: (i) 建立了绿色AI的统一操作定义，区别于可持续AI；(ii) 形式化了映射到生命周期评估（LCA）阶段的五阶段生命周期，将能源、碳、水和隐含影响作为首要考量；(iii) 通过PDCA循环和决策门控指定了治理；(iv) 系统化了跨边缘云连续体的硬件和系统级策略以减少隐含负担；(v) 定义了一个结合估算模型和直接计量的校准测量框架，以实现可重现、与提供商无关的比较。

Result: 本文成功建立了绿色AI的定义、生命周期流程、硬件策略和校准测量方法，为解决AI的多维度环境负担提供了统一、可操作且基于证据的指导。

Conclusion: 通过结合定义、生命周期流程、硬件策略和校准测量，本文为研究人员、实践者和政策制定者提供了减少AI生命周期中多维度环境负担的实用、基于证据的指导。

Abstract: Across the Artificial Intelligence (AI) lifecycle - from hardware to development, deployment, and reuse - burdens span energy, carbon, water, and embodied impacts. Cloud provider tools improve transparency but remain heterogeneous and often omit water and value chain effects, limiting comparability and reproducibility. Addressing these multi dimensional burdens requires a lifecycle approach linking phase explicit mapping with system levers (hardware, placement, energy mix, cooling, scheduling) and calibrated measurement across facility, system, device, and workload levels. This article (i) establishes a unified, operational definition of Green AI distinct from Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle Assessment (LCA) stages, making energy, carbon, water, and embodied impacts first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles with decision gateways; (iv) systematizes hardware and system level strategies across the edge cloud continuum to reduce embodied burdens; and (v) defines a calibrated measurement framework combining estimator models with direct metering to enable reproducible, provider agnostic comparisons. Combining definition, lifecycle processes, hardware strategies, and calibrated measurement, this article offers actionable, evidence based guidance for researchers, practitioners, and policymakers.

</details>


### [52] [A Theoretical Analysis of Detecting Large Model-Generated Time Series](https://arxiv.org/abs/2511.07104)
*Junji Hou,Junzhou Zhao,Shuo Zhang,Pinghui Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于不确定性收缩原理的白盒检测器UCE，用于识别时间序列大模型（TSLM）生成的合成时间序列，并在32个数据集上表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据滥用和伪造的风险日益增加，且现有针对文本模态的模型生成检测方法不适用于时间序列数据（因其信息密度较低和概率分布更平滑），因此需要一种专门针对时间序列的检测方法。

Method: 研究了真实和模型生成时间序列之间的细微分布差异，提出了“收缩假说”，即模型生成的时间序列在递归预测下表现出不确定性逐渐降低的特性。该假说在理论假设下得到形式化证明，并经过多样化数据集的经验验证。在此基础上，引入了不确定性收缩估计器（UCE），这是一种白盒检测器，通过聚合连续前缀的不确定性指标来识别TSLM生成的时间序列。

Result: 收缩假说在多个数据集上得到了经验验证。UCE在32个数据集上的广泛实验表明，它始终优于现有最先进的基线方法。

Conclusion: UCE为检测模型生成的时间序列提供了一个可靠且可泛化的解决方案。

Abstract: Motivated by the increasing risks of data misuse and fabrication, we investigate the problem of identifying synthetic time series generated by Time-Series Large Models (TSLMs) in this work. While there are extensive researches on detecting model generated text, we find that these existing methods are not applicable to time series data due to the fundamental modality difference, as time series usually have lower information density and smoother probability distributions than text data, which limit the discriminative power of token-based detectors. To address this issue, we examine the subtle distributional differences between real and model-generated time series and propose the contraction hypothesis, which states that model-generated time series, unlike real ones, exhibit progressively decreasing uncertainty under recursive forecasting. We formally prove this hypothesis under theoretical assumptions on model behavior and time series structure. Model-generated time series exhibit progressively concentrated distributions under recursive forecasting, leading to uncertainty contraction. We provide empirical validation of the hypothesis across diverse datasets. Building on this insight, we introduce the Uncertainty Contraction Estimator (UCE), a white-box detector that aggregates uncertainty metrics over successive prefixes to identify TSLM-generated time series. Extensive experiments on 32 datasets show that UCE consistently outperforms state-of-the-art baselines, offering a reliable and generalizable solution for detecting model-generated time series.

</details>


### [53] [Boosting Fine-Grained Urban Flow Inference via Lightweight Architecture and Focalized Optimization](https://arxiv.org/abs/2511.07098)
*Yuanshao Zhu,Xiangyu Zhao,Zijian Zhang,Xuetao Wei,James Jianqiao Yu*

Main category: cs.AI

TL;DR: 本文提出了一种名为PLGF的轻量级架构，结合DualFocal Loss，用于细粒度城市流推断。该方法有效解决了现有模型计算成本高昂和传统损失函数在高度倾斜数据分布下性能不佳的问题，在实现最先进性能的同时显著降低了模型大小。


<details>
  <summary>Details</summary>
Motivation: 现有城市流推断方法在实际部署中面临两大挑战：一是过参数化模型导致计算成本过高；二是传统损失函数在城市流高度倾斜的分布上表现不佳。

Method: 本文提出了一种统一解决方案，结合了架构效率和自适应优化。具体而言，首先引入了PLGF架构，该架构采用渐进式局部-全局融合策略，有效捕捉细粒度细节和全局上下文依赖。其次，提出了一种新颖的DualFocal Loss函数，它集成了双空间监督和难度感知聚焦机制，使模型能够自适应地关注难以预测的区域。

Result: 在4个真实世界场景中进行了广泛实验，验证了所提方法的有效性和可扩展性。值得注意的是，PLGF在实现最先进性能的同时，将模型大小比现有高性能方法减少了高达97%。此外，在相似的参数预算下，该模型比强大的基线模型提高了超过10%的准确性。

Conclusion: 本文提出的PLGF架构和DualFocal Loss的统一解决方案，有效解决了细粒度城市流推断中的计算成本和数据分布挑战，实现了卓越的性能和效率，为城市规划和智能交通系统提供了精确支持。

Abstract: Fine-grained urban flow inference is crucial for urban planning and intelligent transportation systems, enabling precise traffic management and resource allocation. However, the practical deployment of existing methods is hindered by two key challenges: the prohibitive computational cost of over-parameterized models and the suboptimal performance of conventional loss functions on the highly skewed distribution of urban flows. To address these challenges, we propose a unified solution that synergizes architectural efficiency with adaptive optimization. Specifically, we first introduce PLGF, a lightweight yet powerful architecture that employs a Progressive Local-Global Fusion strategy to effectively capture both fine-grained details and global contextual dependencies. Second, we propose DualFocal Loss, a novel function that integrates dual-space supervision with a difficulty-aware focusing mechanism, enabling the model to adaptively concentrate on hard-to-predict regions. Extensive experiments on 4 real-world scenarios validate the effectiveness and scalability of our method. Notably, while achieving state-of-the-art performance, PLGF reduces the model size by up to 97% compared to current high-performing methods. Furthermore, under comparable parameter budgets, our model yields an accuracy improvement of over 10% against strong baselines. The implementation is included in the https://github.com/Yasoz/PLGF.

</details>


### [54] [MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107)
*Liang Shan,Kaicheng Shen,Wen Wu,Zhenyu Ying,Chaochao Lu,Guangze Ye,Liang He*

Main category: cs.AI

TL;DR: MENTOR是一个元认知驱动的自演化框架，旨在通过元认知自我评估、动态规则生成和激活引导，揭示并缓解大型语言模型（LLMs）在特定领域任务中的隐性风险，提高其安全性和价值对齐。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs对齐工作主要关注显性风险，忽视了深层、领域特定的隐性风险，且缺乏适用于不同专业领域的灵活通用框架。此外，人工评估耗时费力。

Method: MENTOR框架引入了元认知自我评估工具，使LLMs能通过换位思考和结果思考等策略反思潜在的价值错位。它还发布了一个包含9000个教育、金融和管理领域风险查询的数据集。基于元认知反思结果，框架动态生成补充规则知识图谱以扩展静态规则树，形成持续的自演化循环。最后，在推理过程中采用激活引导来指导LLMs遵循规则，以经济高效的方式增强规则执行力。

Result: 实验结果表明，MENTOR在三个垂直领域的防御性测试中显著降低了语义攻击成功率，实现了LLMs隐性风险缓解的新水平。此外，元认知评估不仅与人工评估结果高度一致，而且能提供更深入、更全面的LLMs价值对齐分析。

Conclusion: MENTOR框架有效缓解了LLMs的隐性风险，提供了一种鲁棒、成本效益高且可泛化的方法，以增强LLMs在多样化领域任务中的安全性和价值对齐。

Abstract: Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.

</details>


### [55] [Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture](https://arxiv.org/abs/2511.07110)
*Tianhao Fu,Xinxin Xu,Weichen Xu,Jue Chen,Ruilong Ren,Bowen Deng,Xinyu Zhao,Jian Cao,Xixin Cao*

Main category: cs.AI

TL;DR: 本文提出了一种名为合作做市（CMM）的新颖框架，通过解耦大型语言模型（LLM）特征并利用学生模型协同学习，实现LLM在做市任务中的知识蒸馏，从而解决推理速度慢的问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在做市中备受关注，且LLM作为代理在金融领域表现出色，但其推理速度慢是一个主要障碍。当前研究缺乏针对特定做市任务的LLM蒸馏方法。

Method: 首先，提出归一化荧光探针来研究LLM的特征机制。在此基础上，提出合作做市（CMM）框架：它将LLM特征在层、任务和数据三个正交维度上解耦。多个学生模型协同学习简单的LLM特征，每个模型负责一个独特的特征以实现知识蒸馏。此外，CMM引入了Hájek-MoE，通过在核函数生成的公共特征空间中调查不同模型的贡献来整合学生模型的输出。

Result: 在四个真实市场数据集上进行的广泛实验结果表明，CMM优于当前的蒸馏方法和基于RL的做市策略。

Conclusion: CMM是一个卓越的LLM做市蒸馏框架，它有效解决了LLM推理速度慢的问题，并在做市任务中实现了显著的性能提升。

Abstract: Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM's feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an Hájek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.

</details>


### [56] [Saliency Map-Guided Knowledge Discovery for Subclass Identification with LLM-Based Symbolic Approximations](https://arxiv.org/abs/2511.07126)
*Tim Bohne,Anne-Kathrin Patricia Windler,Martin Atzmueller*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的神经符号方法，利用神经网络的梯度显著图和大型语言模型（LLM）来发现时间序列分类任务中的潜在子类，并在实验中表现出优于仅基于信号的基线。


<details>
  <summary>Details</summary>
Motivation: 研究动机是识别时间序列分类任务中隐藏的潜在子类，这对于深入理解数据和知识发现至关重要。传统的信号分析方法可能不足以揭示这些深层结构。

Method: 该方法首先将多类时间序列分类问题转换为二分类问题，并训练分类器以生成梯度显著图。然后，根据预测类别对输入信号进行分组，并在三种不同配置下进行聚类。最后，将聚类中心输入到LLM中进行符号近似和模糊知识图谱匹配，以发现原始多类问题中的潜在子类。

Result: 在成熟的时间序列分类数据集上的实验结果表明，该显著图驱动的知识发现方法是有效的。它在聚类和子类识别方面均优于仅基于信号的基线方法。

Conclusion: 该研究得出结论，所提出的显著图驱动的神经符号方法能够有效地进行知识发现，并成功识别时间序列数据中的潜在子类。

Abstract: This paper proposes a novel neuro-symbolic approach for sensor signal-based knowledge discovery, focusing on identifying latent subclasses in time series classification tasks. The approach leverages gradient-based saliency maps derived from trained neural networks to guide the discovery process. Multiclass time series classification problems are transformed into binary classification problems through label subsumption, and classifiers are trained for each of these to yield saliency maps. The input signals, grouped by predicted class, are clustered under three distinct configurations. The centroids of the final set of clusters are provided as input to an LLM for symbolic approximation and fuzzy knowledge graph matching to discover the underlying subclasses of the original multiclass problem. Experimental results on well-established time series classification datasets demonstrate the effectiveness of our saliency map-driven method for knowledge discovery, outperforming signal-only baselines in both clustering and subclass identification.

</details>


### [57] [Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations](https://arxiv.org/abs/2511.07204)
*Giacomo Fidone,Lucia Passaro,Riccardo Guidotti*

Main category: cs.AI

TL;DR: 本文设计了一个基于大型语言模型（LLM）的在线社交网络（OSN）对话模拟器，用于评估内容审核策略，并通过实验证明了其心理真实性、社交传染现象以及个性化审核策略的优越性。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络中内容审核的实际效果因数据收集成本高和实验控制有限而难以评估。尽管大型语言模型（LLMs）为代理基模型（ABM）模拟人类社交行为提供了新途径，但现有工具尚不支持基于模拟的审核策略评估。

Method: 设计了一个由LLM驱动的OSN对话模拟器。该模拟器能够进行并行、反事实的模拟，在其中审核干预会影响有害行为，同时保持其他条件不变。

Result: 广泛的实验揭示了OSN代理的心理真实性、社交传染现象的出现，以及个性化审核策略的卓越有效性。

Conclusion: 该LLM驱动的模拟器为OSN对话中的内容审核策略评估提供了一个有效工具，并证明了个性化审核策略的优越性，同时验证了模拟器在模拟社交行为方面的真实性。

Abstract: Online Social Networks (OSNs) widely adopt content moderation to mitigate the spread of abusive and toxic discourse. Nonetheless, the real effectiveness of moderation interventions remains unclear due to the high cost of data collection and limited experimental control. The latest developments in Natural Language Processing pave the way for a new evaluation approach. Large Language Models (LLMs) can be successfully leveraged to enhance Agent-Based Modeling and simulate human-like social behavior with unprecedented degree of believability. Yet, existing tools do not support simulation-based evaluation of moderation strategies. We fill this gap by designing a LLM-powered simulator of OSN conversations enabling a parallel, counterfactual simulation where toxic behavior is influenced by moderation interventions, keeping all else equal. We conduct extensive experiments, unveiling the psychological realism of OSN agents, the emergence of social contagion phenomena and the superior effectiveness of personalized moderation strategies.

</details>


### [58] [PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork](https://arxiv.org/abs/2511.07260)
*Hohei Chan,Xinzhi Zhang,Antao Xiang,Weinan Zhang,Mengchen Zhao*

Main category: cs.AI

TL;DR: 本文提出PADiff，一种基于扩散模型的方法，用于解决即席团队协作（AHT）中的多模态合作模式和对未知队友的适应性问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 即席团队协作（AHT）在实际应用中至关重要，但现有基于强化学习的方法往往导致策略单一，无法捕捉AHT固有的多模态合作模式。此外，在高度非稳态的AHT场景中，标准扩散模型也缺乏预测和适应能力。

Method: 引入PADiff，一种基于扩散模型的方法来捕捉智能体的多模态行为和多样化的合作模式。为了解决标准扩散模型在非稳态AHT场景中的局限性，提出了一种新颖的扩散策略，将关于队友的关键预测信息整合到去噪过程中。

Result: 在三个合作环境中进行了广泛实验，结果表明PADiff显著优于现有的AHT方法。

Conclusion: PADiff通过捕捉智能体的多模态行为并将队友预测信息融入去噪过程，有效解决了AHT中多模态合作模式和动态适应未知队友的挑战。

Abstract: Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen teammates, which is crucial for many real-world applications. The core challenge of AHT is to develop an ego agent that can predict and adapt to unknown teammates on the fly. Conventional RL-based approaches optimize a single expected return, which often causes policies to collapse into a single dominant behavior, thus failing to capture the multimodal cooperation patterns inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach that captures agent's multimodal behaviors, unlocking its diverse cooperation modes with teammates. However, standard diffusion models lack the ability to predict and adapt in highly non-stationary AHT scenarios. To address this limitation, we propose a novel diffusion-based policy that integrates critical predictive information about teammates into the denoising process. Extensive experiments across three cooperation environments demonstrate that PADiff outperforms existing AHT methods significantly.

</details>


### [59] [AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning](https://arxiv.org/abs/2511.07262)
*Qile Jiang,George Karniadakis*

Main category: cs.AI

TL;DR: AgenticSciML是一个多智能体系统，它通过协作推理和迭代演化自主设计和优化科学机器学习（SciML）解决方案，在误差减少方面显著超越人类和单智能体基线，并产生新的方法论创新。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习（SciML）解决方案的设计（包括架构、损失函数和训练策略）是一个高度依赖专家经验、需要大量实验和特定问题洞察力的过程。

Method: 引入了AgenticSciML，一个协作式多智能体系统，其中超过10个专业AI智能体通过结构化推理和迭代演化来协作提出、评论和改进SciML解决方案。该框架集成了结构化辩论、检索增强的方法记忆和集成引导的进化搜索，使智能体能够生成和评估关于架构和优化程序的新假设。

Result: 在物理信息学习和算子学习任务中，该框架发现的解决方案在误差减少方面比单智能体和人类设计的基线高出多达四个数量级。智能体产生了新颖的策略，包括自适应专家混合架构、基于分解的PINN和物理信息算子学习模型，这些策略并未明确出现在知识库中。

Conclusion: AI智能体之间的协作推理可以产生涌现的方法论创新，这为科学计算中可扩展、透明和自主的发现提供了一条途径。

Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.

</details>


### [60] [IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction](https://arxiv.org/abs/2511.07327)
*Guoxin Chen,Zile Qiao,Xuanzhong Chen,Donglei Yu,Haotian Xu,Wayne Xin Zhao,Ruihua Song,Wenbiao Yin,Huifeng Yin,Liwen Zhang,Kuan Li,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.AI

TL;DR: IterResearch提出了一种迭代式深度研究范式，通过战略性工作区重建和演进报告来解决现有深度研究代理在长周期任务中面临的上下文窒息和噪声污染问题，显著提升了性能和交互扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究代理采用单上下文范式，将所有信息累积在一个不断扩展的上下文窗口中，导致上下文窒息和噪声污染，限制了它们在长周期任务中的有效性。

Method: IterResearch将长周期研究重新定义为具有战略性工作区重建的马尔可夫决策过程。它通过维护一份不断演进的报告作为记忆并定期综合见解，从而保持一致的推理能力。此外，还开发了效率感知策略优化（EAPO），这是一个强化学习框架，通过几何奖励折扣激励高效探索，并通过自适应下采样实现稳定的分布式训练。

Result: IterResearch在六个基准测试中平均比现有开源代理提高了14.5个百分点，并缩小了与前沿专有系统的差距。该范式展现了前所未有的交互扩展性，可扩展到2048次交互并带来显著的性能提升（从3.5%到42.5%）。它还作为一种有效的提示策略，在长周期任务上比ReAct提升了前沿模型高达19.2个百分点。

Conclusion: IterResearch是长周期推理的多功能解决方案，既可以作为训练好的代理，也可以作为前沿模型的提示范式。

Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.

</details>


### [61] [Beyond Detection: Exploring Evidence-based Multi-Agent Debate for Misinformation Intervention and Persuasion](https://arxiv.org/abs/2511.07267)
*Chen Han,Yijia Ma,Jin Tan,Wenzhen Zheng,Xijin Tang*

Main category: cs.AI

TL;DR: ED2D是一个基于证据的多智能体辩论框架，用于检测错误信息并说服用户。它在检测方面表现出色，其正确预测的解释具有与人类专家相当的说服力，但错误分类的解释可能反而强化用户的错误观念，凸显了MAD系统应用的潜力和风险。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论（MAD）框架侧重于检测准确性，但忽视了帮助用户理解事实判断背后的推理并培养未来抵御能力的重要性。MAD生成的辩论记录是透明推理的丰富但未充分利用的资源。因此，需要一个不仅能检测还能说服用户纠正错误信念并阻止错误信息传播的系统。

Method: 本研究引入了ED2D，一个基于证据的MAD框架，通过整合事实证据检索来扩展现有方法。ED2D被设计为不仅是检测框架，还是一个旨在纠正用户信念和阻止错误信息分享的说服性多智能体系统。研究将ED2D生成的辟谣记录与人类专家撰写的记录的说服效果进行了比较。

Result: ED2D在三个错误信息检测基准上优于现有基线。当ED2D做出正确预测时，其辟谣记录显示出与人类专家相当的说服效果。然而，当ED2D错误分类时，其附带的解释可能会无意中强化用户的错误观念，即使同时呈现了准确的人类解释。研究还开发了一个公共社区网站以帮助用户探索ED2D。

Conclusion: 多智能体辩论（MAD）系统在错误信息干预方面既有前景也有潜在风险。部署此类系统需要谨慎，并应关注其解释的准确性及可能对用户信念产生的意外影响。

Abstract: Multi-agent debate (MAD) frameworks have emerged as promising approaches for misinformation detection by simulating adversarial reasoning. While prior work has focused on detection accuracy, it overlooks the importance of helping users understand the reasoning behind factual judgments and develop future resilience. The debate transcripts generated during MAD offer a rich but underutilized resource for transparent reasoning. In this study, we introduce ED2D, an evidence-based MAD framework that extends previous approach by incorporating factual evidence retrieval. More importantly, ED2D is designed not only as a detection framework but also as a persuasive multi-agent system aimed at correcting user beliefs and discouraging misinformation sharing. We compare the persuasive effects of ED2D-generated debunking transcripts with those authored by human experts. Results demonstrate that ED2D outperforms existing baselines across three misinformation detection benchmarks. When ED2D generates correct predictions, its debunking transcripts exhibit persuasive effects comparable to those of human experts; However, when ED2D misclassifies, its accompanying explanations may inadvertently reinforce users'misconceptions, even when presented alongside accurate human explanations. Our findings highlight both the promise and the potential risks of deploying MAD systems for misinformation intervention. We further develop a public community website to help users explore ED2D, fostering transparency, critical thinking, and collaborative fact-checking.

</details>


### [62] [DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas](https://arxiv.org/abs/2511.07338)
*Zhen Wang,Yufan Zhou,Zhongyan Luo,Lyumanshan Ye,Adam Wood,Man Yao,Luoshang Pan*

Main category: cs.AI

TL;DR: DEEPPERSONA是一个可扩展的生成引擎，通过构建大规模人类属性分类法并分阶段采样生成，合成了深度、复杂的叙述完整型合成角色，显著提升了LLM在个性化问答和人类行为模拟方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有合成角色过于肤浅和简单，未能反映真实人类身份的丰富性和多样性，限制了代理行为模拟、LLM个性化和人机对齐等领域的研究进展。

Method: DEEPPERSONA采用两阶段、分类法引导的方法：首先，通过挖掘数千个真实用户-ChatGPT对话，算法构建了一个包含数百个分层属性的大规模人类属性分类法；其次，逐步从该分类法中采样属性，有条件地生成连贯且真实的角色，这些角色平均包含数百个结构化属性和约1 MB的叙述文本。

Result: 内部评估显示，DEEPPERSONA在属性多样性（覆盖率提高32%）和档案独特性（增加44%）方面显著优于现有基线。外部评估表明，其角色将GPT-4.1-mini的个性化问答准确率平均提高了11.6%，并将模拟LLM公民与真实人类在社会调查中的响应差距缩小了31.7%，在大五人格测试中将性能差距缩小了17%。

Conclusion: DEEPPERSONA提供了一个严谨、可扩展且无隐私问题的平台，用于高保真人类模拟和个性化AI研究。

Abstract: Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.

</details>


### [63] [DigiData: Training and Evaluating General-Purpose Mobile Control Agents](https://arxiv.org/abs/2511.07413)
*Yuxuan Sun,Manchen Wang,Shengyi Qian,William R. Wong,Eric Gan,Pierluca D'Oro,Alejandro Castillejo Munoz,Sneha Silwal,Pedro Matias,Nitin Kamra,Satwik Kottur,Nick Raines,Xuanyi Zhao,Joy Chen,Joseph Greer,Andrea Madotto,Allen Bolourchi,James Valori,Kevin Carlberg,Karl Ridgeway,Joseph Tighe*

Main category: cs.AI

TL;DR: 本文介绍了DigiData，一个用于训练移动控制AI代理的大规模、高质量、多模态数据集，以及DigiData-Bench，一个用于评估代理在真实复杂任务上的基准。同时提出了更严格的动态和AI驱动评估方法，以取代不足的步长准确度指标。


<details>
  <summary>Details</summary>
Motivation: 为了加速AI代理控制用户界面以改变人机交互的进程，需要高质量的数据集来训练代理实现复杂目标，以及鲁棒的评估方法来快速提升代理性能。现有数据集在目标复杂性和多样性方面存在不足，且常用的评估指标不够可靠。

Method: 本文构建了DigiData，一个通过全面探索应用程序功能而非非结构化交互生成的大规模、高质量、多样化、多模态数据集。同时提出了DigiData-Bench，一个用于评估移动控制代理在真实复杂任务上的基准。为解决步长准确度指标的不足，提出了动态评估协议和AI驱动的评估方法作为更严格的替代方案。

Result: DigiData数据集相比现有数据集具有更大的多样性和更高的目标复杂性。DigiData-Bench为移动控制代理提供了真实世界复杂任务的评估平台。研究发现，常用的步长准确度指标不足以可靠评估移动控制代理，而动态评估协议和AI驱动评估是更严格、有效的替代方案。

Conclusion: 本文的贡献旨在显著推动移动控制代理的发展，为更直观、有效的人机交互铺平道路。

Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2](https://arxiv.org/abs/2511.05509)
*Joel Valdivia Ortega,Lorenz Lamm,Franziska Eckardt,Benedikt Schworm,Marion Jasnin,Tingying Peng*

Main category: cs.CV

TL;DR: 本文提出RMLP正则化方法，通过对比学习提高Vision Transformers（如DINOv2）在医学和自然图像领域的注意力图可解释性，同时保持或提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）在处理图像时常重用信息量低的图像块tokens，导致注意力图和特征图的可解释性降低，尤其是在存在领域偏移的医学图像中，性能和透明度都会受损。

Method: 引入基于对比学习的Randomized-MLP (RMLP) 正则化方法。在对DINOv2进行医学和自然图像模态微调时使用RMLP。同时，对RMLP进行了数学分析。

Result: RMLP正则化方法在保持或提升下游任务性能的同时，生成了更具可解释性的注意力图。数学分析也为RMLP在增强ViT模型和推进对比学习理解方面的作用提供了见解。

Conclusion: RMLP正则化通过鼓励语义对齐的表示，有效提升了基于ViT的模型（如DINOv2）的可解释性和性能，并加深了对对比学习的理解。

Abstract: Vision Transformers (ViTs), such as DINOv2, achieve strong performance across domains but often repurpose low-informative patch tokens in ways that reduce the interpretability of attention and feature maps. This challenge is especially evident in medical imaging, where domain shifts can degrade both performance and transparency. In this paper, we introduce Randomized-MLP (RMLP) regularization, a contrastive learning-based method that encourages more semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to both medical and natural image modalities, showing that it improves or maintains downstream performance while producing more interpretable attention maps. We also provide a mathematical analysis of RMLPs, offering insights into its role in enhancing ViT-based models and advancing our understanding of contrastive learning.

</details>


### [65] [Token Is All You Need: Cognitive Planning through Sparse Intent Alignment](https://arxiv.org/abs/2511.05540)
*Shiyao Sang*

Main category: cs.CV

TL;DR: 该研究挑战了端到端自动驾驶中详尽场景建模的必要性，提出仅使用少量语义丰富的token即可实现高效规划，并在nuPlan基准测试中取得了优异性能，并观察到时间模糊性。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端自动驾驶方法（如世界模型和VLA系统）依赖于计算密集型的未来场景生成或受马尔可夫假设限制。本研究质疑了高性能端到端自动驾驶是否需要详尽的场景建模。

Method: 使用感知增强的BEV（鸟瞰图）表示来生成一组最小的、语义丰富的token。在nuPlan基准测试（720个场景，超过11,000个样本）上进行实验，探究了在预测未来token条件下的轨迹解码以及显式重建损失的影响。

Result: (1) 即使没有未来预测，稀疏表示也实现了0.548 m的ADE，与之前在nuScenes上报告的约0.75 m的性能相当或超越；(2) 在预测的未来token上进行轨迹解码，将ADE降至0.479 m，比当前基线提高了12.6%；(3) 在可靠感知输入下，显式重建损失没有益处，甚至可能降低性能。此外，观察到“时间模糊性”的出现，模型能够自适应地关注任务相关的语义。

Conclusion: 研究提出了“token即所需全部”的原则，标志着从重建世界到理解世界的范式转变，为受认知启发、通过想象而非反应进行规划的系统奠定了基础。

Abstract: We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Unlike world-model approaches that rely on computationally intensive future scene generation or vision-language-action (VLA) systems constrained by Markov assumptions, we show that a minimal set of semantically rich tokens is sufficient for effective planning. Experiments on the nuPlan benchmark (720 scenarios, over 11,000 samples) using perception-informed BEV representations yield three key findings: (1) even without future prediction, our sparse representation achieves 0.548 m ADE, comparable to or surpassing prior methods reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over current-state baselines; and (3) explicit reconstruction loss offers no benefit and may degrade performance under reliable perception inputs. Notably, we observe the emergence of temporal fuzziness, where the model adaptively attends to task-relevant semantics rather than aligning rigidly to fixed timestamps, providing a cognitive advantage for planning under uncertainty. Our "token is all you need" principle marks a paradigm shift from reconstructing the world to understanding it, laying a foundation for cognitively inspired systems that plan through imagination rather than reaction.

</details>


### [66] [Automated Invoice Data Extraction: Using LLM and OCR](https://arxiv.org/abs/2511.05547)
*Advait Thakur,Khushi Khanchandani,Akshita Shetty,Chaitravi Reddy,Ritisa Behera*

Main category: cs.CV

TL;DR: 本文提出一个结合OCR、深度学习、LLM和图分析的综合AI平台，以解决传统发票信息提取的挑战，实现前所未有的提取质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统OCR系统面临发票版式多变、手写文本、低质量扫描等挑战，且模板依赖性强，缺乏灵活性。虽然新的深度学习和LLM解决方案有所改进，但仍需进一步提升提取质量和一致性。

Method: 本文引入了一个综合性人工智能平台，该平台结合了OCR技术、深度学习模型（如CNN和Transformer）、大型语言模型（LLMs）以及图分析。

Result: 通过结合上述技术，该平台实现了前所未有的提取质量和一致性，超越了现有方法在发票信息提取方面的表现。

Conclusion: 该综合AI平台通过整合多种先进技术，显著提升了发票信息提取的准确性和稳定性，是解决复杂文档处理挑战的有效方案。

Abstract: Conventional Optical Character Recognition (OCR) systems are challenged by
variant invoice layouts, handwritten text, and low- quality scans, which are
often caused by strong template dependencies that restrict their flexibility
across different document structures and layouts. Newer solutions utilize
advanced deep learning models such as Convolutional Neural Networks (CNN) as
well as Transformers, and domain-specific models for better layout analysis and
accuracy across various sections over varied document types. Large Language
Models (LLMs) have revolutionized extraction pipelines at their core with
sophisticated entity recognition and semantic comprehension to support complex
contextual relationship mapping without direct programming specification.
Visual Named Entity Recognition (NER) capabilities permit extraction from
invoice images with greater contextual sensitivity and much higher accuracy
rates than older approaches. Existing industry best practices utilize hybrid
architectures that blend OCR technology and LLM for maximum scalability and
minimal human intervention. This work introduces a holistic Artificial
Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph
analytics to achieve unprecedented extraction quality and consistency.

</details>


### [67] [In-Context-Learning-Assisted Quality Assessment Vision-Language Models for Metal Additive Manufacturing](https://arxiv.org/abs/2511.05551)
*Qiaojie Zheng,Jiucai Zhang,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 本研究利用上下文学习（ICL）增强视觉语言模型（VLM），在增材制造中实现基于视觉的质量评估，无需大量特定数据集，并能提供可解释的推理。


<details>
  <summary>Details</summary>
Motivation: 增材制造中的视觉质量评估通常需要专用机器学习模型和大量应用特定数据集，导致数据收集和模型训练成本高昂且耗时。

Method: 该研究利用视觉语言模型（VLM）的推理能力，并通过上下文学习（ICL）为其提供应用特定知识和演示样本，从而无需大量训练数据。研究探索了不同的ICL采样策略，并在Gemini-2.5-flash和Gemma3:27b两个VLM上，针对线激光直接能量沉积过程的质量评估任务进行了评估。此外，为了评估VLM生成的可解释性理由，提出了知识相关性（knowledge relevance）和理由有效性（rationale validity）两个新指标。

Result: 结果表明，ICL辅助的VLM在仅需极少量样本的情况下，就能达到与传统机器学习模型相似的质量分类准确率。同时，VLM能够生成人类可解释的理由，提高了模型的透明度和信任度。

Conclusion: ICL辅助的VLM能够有效解决有限数据下的应用特定任务，在实现相对较高准确性的同时，还能提供有效的支持理由，从而显著提升决策的透明度。

Abstract: Vision-based quality assessment in additive manufacturing often requires dedicated machine learning models and application-specific datasets. However, data collection and model training can be expensive and time-consuming. In this paper, we leverage vision-language models' (VLMs') reasoning capabilities to assess the quality of printed parts and introduce in-context learning (ICL) to provide VLMs with necessary application-specific knowledge and demonstration samples. This method eliminates the requirement for large application-specific datasets for training models. We explored different sampling strategies for ICL to search for the optimal configuration that makes use of limited samples. We evaluated these strategies on two VLMs, Gemini-2.5-flash and Gemma3:27b, with quality assessment tasks in wire-laser direct energy deposition processes. The results show that ICL-assisted VLMs can reach quality classification accuracies similar to those of traditional machine learning models while requiring only a minimal number of samples. In addition, unlike traditional classification models that lack transparency, VLMs can generate human-interpretable rationales to enhance trust. Since there are no metrics to evaluate their interpretability in manufacturing applications, we propose two metrics, knowledge relevance and rationale validity, to evaluate the quality of VLMs' supporting rationales. Our results show that ICL-assisted VLMs can address application-specific tasks with limited data, achieving relatively high accuracy while also providing valid supporting rationales for improved decision transparency.

</details>


### [68] [EVLP:Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning](https://arxiv.org/abs/2511.05553)
*Xinyan Cai,Shiguang Wu,Dafeng Chi,Yuzheng Zhuang,Xingyue Quan,Jianye Hao,Qiang Guan*

Main category: cs.CV

TL;DR: EVLP是一个创新的多模态统一生成框架，通过动态预训练和强化对齐，协同整合语言推理和视觉生成，以解决复杂具身长周期操作任务中的多模态规划不一致问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂的具身长周期操作任务中，有效的任务分解和执行需要文本逻辑推理与视觉空间想象的协同整合。现有方法未能采用统一的多模态规划生成框架，导致多模态规划中的不一致性。

Method: 本文提出了EVLP (Embodied Vision-Language Planner) 框架，其核心创新包括：1) **统一多模态生成框架**：整合语义信息与空间特征进行视觉感知，直接学习离散图像的联合分布进行一步视觉合成，并通过可学习的跨模态注意力机制实现语言-视觉建模。2) **动态感知预训练**：采用双向动态对齐策略，通过逆动力学任务和正动力学任务，在一个统一特征空间内有效加强多模态关联。3) **强化监督微调**：在统一生成空间中进行基于指令的微调时，构建强化损失以对齐文本动作与生成图像之间的空间逻辑。

Result: 该方法通过新颖的训练流程（包括动态预训练和强化对齐）实现了长周期任务的多模态规划，使模型能够获得空间感知的多模态规划能力。

Conclusion: EVLP通过创新的统一多模态生成框架、动态感知预训练和强化监督微调，有效解决了复杂具身任务中多模态规划不一致的挑战，实现了语言推理和视觉生成的协同整合，从而赋能模型进行空间感知的多模态规划。

Abstract: In complex embodied long-horizon manipulation tasks, effective task decomposition and execution require synergistic integration of textual logical reasoning and visual-spatial imagination to ensure efficient and accurate operation. Current methods fail to adopt a unified generation framework for multimodal planning, lead to inconsistent in multimodal planning. To address this challenge, we present \textbf{EVLP (Embodied Vision-Language Planner)}, an innovative multimodal unified generation framework that jointly models linguistic reasoning and visual generation. Our approach achieves multimodal planning for long-horizon tasks through a novel training pipeline incorporating dynamic pretraining and reinforced alignment. Our core innovations consist of three key components: \textbf{1) Unified Multimodal Generation Framework}: For understanding, We integrate semantic information with spatial features to provide comprehensive visual perception. For generation, we directly learn the joint distribution of discrete images for one-step visual synthesis, enabling coordinated language-visual modeling through learnable cross-modal attention mechanisms. \textbf{2) Dynamic Perception Pretraining}: We propose a bidirectional dynamic alignment strategy employing inverse dynamics tasks and forward dynamics tasks, effectively strengthening multimodal correlations within a unified feature space. \textbf{3) Reinforced Supervised Fine-Tuning}: While conducting instruction-based fine-tuning in the unified generation space, we construct a reinforce loss to align the spatial logic between textual actions and generated images, enabling the model to acquire spatio-awared multimodal planning capabilities.

</details>


### [69] [MCFCN: Multi-View Clustering via a Fusion-Consensus Graph Convolutional Network](https://arxiv.org/abs/2511.05554)
*Chenping Pei,Fadi Dornaika,Jingjun Bi*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MCFCN的多视图聚类方法，通过融合共识图卷积网络，以端到端的方式学习鲁棒的共识图和有效的共识表示，解决了现有方法中拓扑结构忽视、图噪声敏感和优化过程不连贯等问题，并在多视图基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于子空间学习的多视图聚类方法忽略了数据的固有拓扑结构；集成图神经网络（GNN）的方法其输入图结构易受噪声干扰；基于多视图图细化（MGRC）的方法存在跨视图一致性考虑不足、难以处理特征空间中难区分样本以及图构建算法导致优化过程不连贯等局限性。

Method: 该论文提出了一种融合共识图卷积网络（MCFCN），它通过视图特征融合模型和统一图结构适配器（UGA）以端到端方式学习多视图数据的共识图和有效的共识表示。MCFCN设计了相似度矩阵对齐损失（SMAL）和特征表示对齐损失（FRAL），在共识的指导下优化视图特定图，保持跨视图拓扑一致性，促进类内边缘的构建，并借助GCN实现有效的共识表示学习。

Result: MCFCN在八个多视图基准数据集上展示了最先进的性能，并通过广泛的定性和定量实验验证了其有效性。

Conclusion: MCFCN通过学习鲁棒的共识图和有效的共识表示，有效解决了现有方法的局限性，显著提高了多视图聚类性能。

Abstract: Existing Multi-view Clustering (MVC) methods based on subspace learning focus on consensus representation learning while neglecting the inherent topological structure of data. Despite the integration of Graph Neural Networks (GNNs) into MVC, their input graph structures remain susceptible to noise interference. Methods based on Multi-view Graph Refinement (MGRC) also have limitations such as insufficient consideration of cross-view consistency, difficulty in handling hard-to-distinguish samples in the feature space, and disjointed optimization processes caused by graph construction algorithms. To address these issues, a Multi-View Clustering method via a Fusion-Consensus Graph Convolutional Network (MCFCN) is proposed. The network learns the consensus graph of multi-view data in an end-to-end manner and learns effective consensus representations through a view feature fusion model and a Unified Graph Structure Adapter (UGA). It designs Similarity Matrix Alignment Loss (SMAL) and Feature Representation Alignment Loss (FRAL). With the guidance of consensus, it optimizes view-specific graphs, preserves cross-view topological consistency, promotes the construction of intra-class edges, and realizes effective consensus representation learning with the help of GCN to improve clustering performance. MCFCN demonstrates state-of-the-art performance on eight multi-view benchmark datasets, and its effectiveness is verified by extensive qualitative and quantitative implementations. The code will be provided at https://github.com/texttao/MCFCN.

</details>


### [70] [Compressing Multi-Task Model for Autonomous Driving via Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.05557)
*Jiayuan Wang,Q. M. Jonathan Wu,Ning Zhang,Katsuya Suto,Lei Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种多任务模型压缩框架，结合了任务感知安全剪枝和特征级知识蒸馏，有效减少了自动驾驶全景感知模型的参数和复杂性，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖多任务全景感知，但多任务学习模型参数和复杂度高，难以部署到车载设备上。

Method: 该研究提出一个多任务模型压缩框架，包括：1) 任务感知安全剪枝策略，结合基于Taylor的通道重要性和梯度冲突惩罚，以保留重要通道并移除冗余和冲突通道；2) 任务头无关的特征级知识蒸馏方法，将教师模型的中间骨干和编码器特征传递给学生模型以指导学习。

Result: 在BDD100K数据集上的实验表明，压缩模型参数减少了32.7%，分割性能损失可忽略不计，检测性能仅有轻微下降（Recall -1.2%，mAP50 -1.8%），同时仍能以32.7 FPS实时运行。

Conclusion: 结合剪枝和知识蒸馏为多任务全景感知提供了一种有效的模型压缩解决方案。

Abstract: Autonomous driving systems rely on panoptic perception to jointly handle object detection, drivable area segmentation, and lane line segmentation. Although multi-task learning is an effective way to integrate these tasks, its increasing model parameters and complexity make deployment on on-board devices difficult. To address this challenge, we propose a multi-task model compression framework that combines task-aware safe pruning with feature-level knowledge distillation. Our safe pruning strategy integrates Taylor-based channel importance with gradient conflict penalty to keep important channels while removing redundant and conflicting channels. To mitigate performance degradation after pruning, we further design a task head-agnostic distillation method that transfers intermediate backbone and encoder features from a teacher to a student model as guidance. Experiments on the BDD100K dataset demonstrate that our compressed model achieves a 32.7% reduction in parameters while segmentation performance shows negligible accuracy loss and only a minor decrease in detection (-1.2% for Recall and -1.8% for mAP50) compared to the teacher. The compressed model still runs at 32.7 FPS in real-time. These results show that combining pruning and knowledge distillation provides an effective compression solution for multi-task panoptic perception.

</details>


### [71] [FilletRec: A Lightweight Graph Neural Network with Intrinsic Features for Automated Fillet Recognition](https://arxiv.org/abs/2511.05561)
*Jiali Gao,Taoran Liu,Hongfei Ye,Jianjun Chen*

Main category: cs.CV

TL;DR: 本文提出了一个端到端、数据驱动的倒圆角特征识别与简化框架，包括一个大型数据集和轻量级图神经网络FilletRec。FilletRec利用姿态不变的几何特征，显著提高了复杂倒圆角识别的准确性和泛化能力，同时保持了高模型效率。


<details>
  <summary>Details</summary>
Motivation: CAD模型中倒圆角特征的自动识别与简化对CAE分析至关重要，但仍是一个开放性挑战。传统基于规则的方法缺乏鲁棒性，现有深度学习模型因通用设计和训练数据不足，在复杂倒圆角上泛化能力差、精度低。

Method: 本文首先构建并发布了一个大规模、多样化的倒圆角识别基准数据集。在此基础上，提出了一种名为FilletRec的轻量级图神经网络。FilletRec的核心创新在于利用曲率等姿态不变的内在几何特征，学习更基础的几何模式。最后，该框架通过集成有效的几何简化算法，完成了从识别到简化的自动化工作流程。

Result: 实验表明，FilletRec在准确性和泛化能力上均超越了现有最先进的方法。同时，它仅使用了基线模型0.2%-5.4%的参数，展示了高模型效率。

Conclusion: 所提出的端到端、数据驱动的框架及其核心FilletRec网络，通过引入大型数据集和利用姿态不变的内在几何特征，有效解决了CAD模型中倒圆角特征自动识别和简化的挑战，实现了高精度、强泛化能力和高效率。

Abstract: Automated recognition and simplification of fillet features in CAD models is critical for CAE analysis, yet it remains an open challenge. Traditional rule-based methods lack robustness, while existing deep learning models suffer from poor generalization and low accuracy on complex fillets due to their generic design and inadequate training data. To address these issues, this paper proposes an end-to-end, data-driven framework specifically for fillet features. We first construct and release a large-scale, diverse benchmark dataset for fillet recognition to address the inadequacy of existing data. Based on it, we propose FilletRec, a lightweight graph neural network. The core innovation of this network is its use of pose-invariant intrinsic geometric features, such as curvature, enabling it to learn more fundamental geometric patterns and thereby achieve high-precision recognition of complex geometric topologies. Experiments show that FilletRec surpasses state-of-the-art methods in both accuracy and generalization, while using only 0.2\%-5.4\% of the parameters of baseline models, demonstrating high model efficiency. Finally, the framework completes the automated workflow from recognition to simplification by integrating an effective geometric simplification algorithm.

</details>


### [72] [M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection](https://arxiv.org/abs/2511.05564)
*Yang Liu,Boan Chen,Xiaoguang Zhu,Jing Liu,Peng Sun,Wei Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba的多尺度时空学习(M2S2L)框架，用于视频异常检测，旨在平衡检测精度和计算效率，并适用于现代监控系统。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测(VAD)面临在检测精度和计算效率之间取得平衡的挑战。随着视频内容日益复杂，传统的VAD方法难以提供鲁棒评估。现有方法要么缺乏全面的时空建模，要么需要过多的计算资源以实现实时应用。

Method: 本文提出了Mamba-based的多尺度时空学习(M2S2L)框架。该方法采用在多粒度上操作的分层空间编码器和捕获不同时间尺度运动动态的多时间编码器。此外，引入了特征分解机制，以实现外观和运动重建的任务特定优化，从而实现更细致的行为建模和质量感知异常评估。

Result: M2S2L框架在三个基准数据集上取得了显著效果：在UCSD Ped2、CUHK Avenue和ShanghaiTech上分别达到了98.5%、92.1%和77.9%的帧级AUC。同时，它保持了高效率，计算量为20.1G FLOPs，推理速度达到45 FPS。

Conclusion: M2S2L框架在视频异常检测任务中实现了高精度和高效率的平衡，其性能使其适用于实际的监控部署。

Abstract: Video anomaly detection (VAD) is an essential task in the image processing community with prospects in video surveillance, which faces fundamental challenges in balancing detection accuracy with computational efficiency. As video content becomes increasingly complex with diverse behavioral patterns and contextual scenarios, traditional VAD approaches struggle to provide robust assessment for modern surveillance systems. Existing methods either lack comprehensive spatial-temporal modeling or require excessive computational resources for real-time applications. In this regard, we present a Mamba-based multi-scale spatial-temporal learning (M2S2L) framework in this paper. The proposed method employs hierarchical spatial encoders operating at multiple granularities and multi-temporal encoders capturing motion dynamics across different time scales. We also introduce a feature decomposition mechanism to enable task-specific optimization for appearance and motion reconstruction, facilitating more nuanced behavioral modeling and quality-aware anomaly assessment. Experiments on three benchmark datasets demonstrate that M2S2L framework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHK Avenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1G FLOPs and 45 FPS inference speed, making it suitable for practical surveillance deployment.

</details>


### [73] [In-Context Adaptation of VLMs for Few-Shot Cell Detection in Optical Microscopy](https://arxiv.org/abs/2511.05565)
*Shreyan Ganguly,Angona Biswas,Jaydeep Rade,Md Hasibul Hasan Hasib,Nabila Masud,Nitish Singla,Abhipsa Dash,Ushashi Bhattacharjee,Aditya Balu,Anwesha Sarkar,Adarsh Krishnamurthy,Soumik Sarkar*

Main category: cs.CV

TL;DR: 本研究探讨了基础视觉-语言模型（VLMs）在生物医学显微图像中通过上下文学习进行少样本目标检测的潜力，并引入了Micro-OD基准。结果显示，尽管存在领域差距，少样本支持能显著提升检测性能，并提出了一种混合FSOD管道以增强效果。


<details>
  <summary>Details</summary>
Motivation: 基础视觉-语言模型在自然图像上表现出色，但其在生物医学显微镜图像领域的应用尚未得到充分探索。显微图像常缺乏大规模标注数据集，因此需要一种能在数据稀缺情况下执行目标检测的方法。

Method: 研究通过上下文学习，使最先进的VLM模型能进行少样本目标检测。引入了Micro-OD基准，包含252张带有边界框标注的图像，涵盖11种细胞类型。系统评估了8个VLM模型在少样本条件下的表现，并比较了带有和不带隐式测试时推理标记的变体。此外，实现了一个混合少样本目标检测（FSOD）管道，结合了检测头和基于VLM的少样本分类器。

Result: 零样本性能因领域差距而较弱；然而，少样本支持持续改进了检测性能，在六个样本后增益趋于平缓。带有推理标记的模型在端到端定位中更有效，而更简单的变体更适合分类预定位的裁剪图像。混合FSOD管道增强了VLM在基准测试上的少样本性能。

Conclusion: 上下文适应是显微镜图像分析的实用途径。Micro-OD基准为推进生物医学成像中的开放词汇检测提供了一个可复现的测试平台。

Abstract: Foundation vision-language models (VLMs) excel on natural images, but their utility for biomedical microscopy remains underexplored. In this paper, we investigate how in-context learning enables state-of-the-art VLMs to perform few-shot object detection when large annotated datasets are unavailable, as is often the case with microscopic images. We introduce the Micro-OD benchmark, a curated collection of 252 images specifically curated for in-context learning, with bounding-box annotations spanning 11 cell types across four sources, including two in-lab expert-annotated sets. We systematically evaluate eight VLMs under few-shot conditions and compare variants with and without implicit test-time reasoning tokens. We further implement a hybrid Few-Shot Object Detection (FSOD) pipeline that combines a detection head with a VLM-based few-shot classifier, which enhances the few-shot performance of recent VLMs on our benchmark. Across datasets, we observe that zero-shot performance is weak due to the domain gap; however, few-shot support consistently improves detection, with marginal gains achieved after six shots. We observe that models with reasoning tokens are more effective for end-to-end localization, whereas simpler variants are more suitable for classifying pre-localized crops. Our results highlight in-context adaptation as a practical path for microscopy, and our benchmark provides a reproducible testbed for advancing open-vocabulary detection in biomedical imaging.

</details>


### [74] [Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster](https://arxiv.org/abs/2511.05567)
*Shin Kamada,Takumi Ichimura*

Main category: cs.CV

TL;DR: 本文提出了一种基于自适应受限玻尔兹曼机（RBM）和深度信念网络（DBN）的结构学习方法，并将其应用于自动道路网络识别系统RoadTracer。通过采用师生集成学习模型，该方法显著提高了道路检测精度，并成功应用于自然灾害后可用道路的快速识别，且可在边缘设备上实现轻量级部署。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在复杂特征识别方面具有强大潜力。由于道路地图包含许多复杂特征，需要具有高表示能力的模型进行检测。此外，在自然灾害发生时，需要快速识别可用道路以获取交通方式。

Method: 研究开发了一种自适应结构学习方法，通过RBM中的神经元生成-湮灭算法和DBN中的层生成算法，为给定输入构建最优网络结构。本文提出了一种使用基于师生集成学习模型的自适应DBN的RoadTracer新方法，用于从航空照片数据中生成地面路网图。为了实现快速推理，将训练好的小型模型部署在嵌入式边缘设备上，实现了轻量级深度学习。

Result: 实验结果显示，在测试数据集中七个主要城市，所提出模型的检测精度平均从40.0%提高到89.0%。此外，该方法成功应用于检测自然灾害（如山体滑坡）后可用的道路，以快速获取交通方式。研究还报告了在日本降雨灾害前后卫星图像的检测结果。

Conclusion: 所提出的基于师生集成学习的自适应DBN模型显著提升了道路网络识别（RoadTracer）的准确性。该方法能够有效识别灾害后的可用道路，对于快速获取交通方式具有重要意义。轻量级部署在边缘设备上实现了快速推理，为灾害响应提供了实用解决方案。

Abstract: An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\% to 89.0\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.

</details>


### [75] [Do Street View Imagery and Public Participation GIS align: Comparative Analysis of Urban Attractiveness](https://arxiv.org/abs/2511.05570)
*Milad Malekzadeh,Elias Willberg,Jussi Torkko,Silviya Korpilo,Kamyar Hasanzadeh,Olle Järv,Tuuli Toivonen*

Main category: cs.CV

TL;DR: 本研究发现街景图像（SVI）预测的城市吸引力与公众参与式地理信息系统（PPGIS）报告的居民体验仅部分一致，SVI模型难以捕捉非视觉的体验维度。


<details>
  <summary>Details</summary>
Motivation: 随着数字工具在空间规划中日益重要，理解不同数据源（特别是SVI和PPGIS）如何反映人类对城市环境的体验至关重要。尽管两者都能捕捉基于地点的感知，但它们的可比性尚未得到充分探索。

Method: 研究在芬兰赫尔辛基进行，结合参与者评分的SVI数据和语义图像分割，训练了一个机器学习模型来预测基于视觉特征的感知吸引力。随后，将这些预测与PPGIS调查中居民标记的吸引或不吸引地点进行比较，使用严格和中等两套标准计算一致性。此外，还分析了噪声、交通、人口密度和土地利用等非视觉情境变量对不匹配的影响。

Result: 研究发现SVI和PPGIS数据集之间仅存在部分一致性。在中等阈值下，吸引力地点的一致性达到67%，不吸引力地点为77%。然而，在严格阈值下，一致性分别下降到27%和29%。分析表明，非视觉线索（如活动水平和环境压力源）显著导致了不匹配，SVI模型未能充分考虑这些体验维度。

Conclusion: 结果表明，虽然SVI为城市感知提供了一个可扩展的视觉代理，但它不能完全替代PPGIS所捕捉的丰富体验。两种方法都很有价值，但服务于不同的目的；因此，需要一种更综合的方法来全面捕捉人们对城市环境的感知。

Abstract: As digital tools increasingly shape spatial planning practices, understanding how different data sources reflect human experiences of urban environments is essential. Street View Imagery (SVI) and Public Participation GIS (PPGIS) represent two prominent approaches for capturing place-based perceptions that can support urban planning decisions, yet their comparability remains underexplored. This study investigates the alignment between SVI-based perceived attractiveness and residents' reported experiences gathered via a city-wide PPGIS survey in Helsinki, Finland. Using participant-rated SVI data and semantic image segmentation, we trained a machine learning model to predict perceived attractiveness based on visual features. We compared these predictions to PPGIS-identified locations marked as attractive or unattractive, calculating agreement using two sets of strict and moderate criteria. Our findings reveal only partial alignment between the two datasets. While agreement (with a moderate threshold) reached 67% for attractive and 77% for unattractive places, agreement (with a strict threshold) dropped to 27% and 29%, respectively. By analysing a range of contextual variables, including noise, traffic, population presence, and land use, we found that non-visual cues significantly contributed to mismatches. The model failed to account for experiential dimensions such as activity levels and environmental stressors that shape perceptions but are not visible in images. These results suggest that while SVI offers a scalable and visual proxy for urban perception, it cannot fully substitute the experiential richness captured through PPGIS. We argue that both methods are valuable but serve different purposes; therefore, a more integrated approach is needed to holistically capture how people perceive urban environments.

</details>


### [76] [Efficient Online Continual Learning in Sensor-Based Human Activity Recognition](https://arxiv.org/abs/2511.05566)
*Yao Zhang,Souza Leite Clayton,Yu Xiao*

Main category: cs.CV

TL;DR: 本文提出PTRN-HAR，首次成功将基于预训练模型（PTM）的在线持续学习（OCL）应用于基于传感器的活动识别（HAR），通过对比学习预训练特征提取器并使用关系模块网络，显著降低资源消耗并提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 基于传感器的HAR模型需要部署后适应新活动和新执行方式。现有的在线持续学习（OCL）方法计算密集且需要大量标记样本。虽然基于预训练模型（PTM）的OCL在计算机视觉中表现出色，但由于HAR数据集的异质性和部署后标记数据稀缺，将其应用于HAR面临挑战。

Method: 本文引入PTRN-HAR。与以往的PTM-based OCL不同，PTRN-HAR使用对比损失在有限数据上预训练特征提取器，并在后续流式学习阶段冻结该提取器。此外，它用关系模块网络取代了传统的密集分类层。

Result: PTRN-HAR设计不仅显著降低了模型训练所需的资源消耗，同时保持了高性能，并通过减少持续学习所需的标记数据量，提高了数据效率。在三个公共数据集上的实验证明，PTRN-HAR超越了现有最先进的方法。

Conclusion: PTRN-HAR是首个成功将基于预训练模型的在线持续学习应用于基于传感器的活动识别的方法，它通过创新的预训练和网络结构，有效解决了计算和数据效率问题，为HAR领域的持续学习提供了高效且高性能的解决方案。

Abstract: Machine learning models for sensor-based human activity recognition (HAR) are expected to adapt post-deployment to recognize new activities and different ways of performing existing ones. To address this need, Online Continual Learning (OCL) mechanisms have been proposed, allowing models to update their knowledge incrementally as new data become available while preserving previously acquired information. However, existing OCL approaches for sensor-based HAR are computationally intensive and require extensive labeled samples to represent new changes. Recently, pre-trained model-based (PTM-based) OCL approaches have shown significant improvements in performance and efficiency for computer vision applications. These methods achieve strong generalization capabilities by pre-training complex models on large datasets, followed by fine-tuning on downstream tasks for continual learning. However, applying PTM-based OCL approaches to sensor-based HAR poses significant challenges due to the inherent heterogeneity of HAR datasets and the scarcity of labeled data in post-deployment scenarios. This paper introduces PTRN-HAR, the first successful application of PTM-based OCL to sensor-based HAR. Unlike prior PTM-based OCL approaches, PTRN-HAR pre-trains the feature extractor using contrastive loss with a limited amount of data. This extractor is then frozen during the streaming stage. Furthermore, it replaces the conventional dense classification layer with a relation module network. Our design not only significantly reduces the resource consumption required for model training while maintaining high performance, but also improves data efficiency by reducing the amount of labeled data needed for effective continual learning, as demonstrated through experiments on three public datasets, outperforming the state-of-the-art. The code can be found here: https://anonymous.4open.science/r/PTRN-HAR-AF60/

</details>


### [77] [C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling](https://arxiv.org/abs/2511.05571)
*Xiaofei Wang,Stephen Price,Chao Li*

Main category: cs.CV

TL;DR: C3-Diff是一个跨模态对比扩散框架，通过整合组织学图像来增强低分辨率空间转录组（ST）数据，解决了ST数据分辨率低和信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组（ST）平台分辨率低，限制了对空间基因表达的深入理解。虽然超分辨率方法承诺通过结合组织学图像来增强ST图谱，但如何有效建模组织学图像与基因表达之间的相互作用仍是一个挑战。

Method: 本研究提出了C3-Diff，一个跨模态跨内容对比扩散框架，以组织学图像为指导增强ST。C3-Diff首先改进了传统对比学习范式，以提取ST图谱和组织学图像的模态不变和内容不变特征。其次，通过在特征单元超球面上进行基于噪声的信息增强，克服ST图谱测序敏感性低的问题。最后，提出了一种动态跨模态插补训练策略，以缓解ST数据稀缺性。

Result: C3-Diff在四个公共数据集上测试，相较于现有方法取得了显著改进。此外，C3-Diff在细胞类型定位、基因表达相关性和单细胞水平基因表达预测等下游任务中表现良好。

Conclusion: C3-Diff框架通过结合组织学图像有效增强了空间转录组数据，为生物医学研究和临床应用中的AI增强生物技术提供了支持。

Abstract: The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene expressions, has made it possible to measure gene expression within original tissue, enabling us to discover molecular mechanisms. However, current ST platforms frequently suffer from low resolution, limiting the in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, it remains a challenge to model the interactions between histology images and gene expressions for effective ST enhancement. This study presents a cross-modal cross-content contrastive diffusion framework, called C3-Diff, for ST enhancement with histology images as guidance. In C3-Diff, we firstly analyze the deficiency of traditional contrastive learning paradigm, which is then refined to extract both modal-invariant and content-invariant features of ST maps and histology images. Further, to overcome the problem of low sequencing sensitivity in ST maps, we perform nosing-based information augmentation on the surface of feature unit hypersphere. Finally, we propose a dynamic cross-modal imputation-based training strategy to mitigate ST data scarcity. We tested C3-Diff by benchmarking its performance on four public datasets, where it achieves significant improvements over competing methods. Moreover, we evaluate C3-Diff on downstream tasks of cell type localization, gene expression correlation and single-cell-level gene expression prediction, promoting AI-enhanced biotechnology for biomedical research and clinical applications. Codes are available at https://github.com/XiaofeiWang2018/C3-Diff.

</details>


### [78] [Video Text Preservation with Synthetic Text-Rich Videos](https://arxiv.org/abs/2511.05573)
*Ziyang Liu,Kevin Valencia,Justin Cui*

Main category: cs.CV

TL;DR: 本文提出一种轻量级方法，利用合成数据对现有文本到视频（T2V）模型进行微调，显著提升了视频中短文本的可读性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）模型在生成视频中清晰、连贯的文本方面表现不佳，即使是短语或单词也难以正确渲染。此前解决此问题的方法计算成本高昂，不适用于视频生成。

Method: 研究人员首先使用文本到图像（T2I）扩散模型生成富含文本的图像，然后利用与文本无关的图像到视频（I2V）模型将这些图像动画化为短视频。这些合成的视频-提示对被用于微调预训练的T2V模型（Wan2.1），且未进行任何架构更改。

Result: 实验结果表明，该方法显著改善了短文本的可读性和时间一致性，并且对于长文本也展现出新兴的结构先验能力。

Conclusion: 这些发现表明，精心策划的合成数据和弱监督为提高T2V生成中文本的保真度提供了一条实用的途径。

Abstract: While Text-To-Video (T2V) models have advanced rapidly, they continue to struggle with generating legible and coherent text within videos. In particular, existing models often fail to render correctly even short phrases or words and previous attempts to address this problem are computationally expensive and not suitable for video generation. In this work, we investigate a lightweight approach to improve T2V diffusion models using synthetic supervision. We first generate text-rich images using a text-to-image (T2I) diffusion model, then animate them into short videos using a text-agnostic image-to-video (I2v) model. These synthetic video-prompt pairs are used to fine-tune Wan2.1, a pre-trained T2V model, without any architectural changes. Our results show improvement in short-text legibility and temporal consistency with emerging structural priors for longer text. These findings suggest that curated synthetic data and weak supervision offer a practical path toward improving textual fidelity in T2V generation.

</details>


### [79] [Elements of Active Continuous Learning and Uncertainty Self-Awareness: a Narrow Implementation for Face and Facial Expression Recognition](https://arxiv.org/abs/2511.05574)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 本文提出了一种模拟自我意识的机制，通过一个监督神经网络观察另一个基础神经网络（用于人脸识别和表情任务的CNN集成）的激活模式，以识别高不确定性，从而触发主动学习模式并请求人类帮助。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于，将智能中关键的自我反思和纠错能力，即对自身思维过程的反省和修正，从抽象概念层面模型化到具体的窄域机器学习算法中，以期为通用人工智能（AGI）奠定基础。

Method: 该方法包括一个监督人工神经网络（ANN），它负责观察另一个作为基础的ANN（一个用于人脸识别和面部表情任务的卷积神经网络CNN集成）的激活模式。监督ANN旨在寻找基础ANN高不确定性的迹象，从而评估其预测的可信度。监督ANN包含一个记忆区域，存储其过往表现信息，其可学习参数在训练期间进行调整以优化性能。当判断为不可信时，将触发主动学习模式，请求人类在高度不确定和困惑的情况下提供帮助。

Result: 通过这种自我意识机制的模拟，系统能够识别基础神经网络预测中的高不确定性，并据此触发主动学习模式，使机器学习算法在面临高不确定性时能够主动寻求人类的帮助，从而获得了一定的能动性。

Conclusion: 研究表明，即使在窄域机器学习算法中，也可以通过监督神经网络模拟自我意识机制，实现对自身性能不确定性的识别，并主动寻求人类干预，这为构建具有能动性和更接近通用人工智能的系统提供了可能。

Abstract: Reflection on one's thought process and making corrections to it if there exists dissatisfaction in its performance is, perhaps, one of the essential traits of intelligence. However, such high-level abstract concepts mandatory for Artificial General Intelligence can be modelled even at the low level of narrow Machine Learning algorithms. Here, we present the self-awareness mechanism emulation in the form of a supervising artificial neural network (ANN) observing patterns in activations of another underlying ANN in a search for indications of the high uncertainty of the underlying ANN and, therefore, the trustworthiness of its predictions. The underlying ANN is a convolutional neural network (CNN) ensemble employed for face recognition and facial expression tasks. The self-awareness ANN has a memory region where its past performance information is stored, and its learnable parameters are adjusted during the training to optimize the performance. The trustworthiness verdict triggers the active learning mode, giving elements of agency to the machine learning algorithm that asks for human help in high uncertainty and confusion conditions.

</details>


### [80] [Enhancing Diffusion Model Guidance through Calibration and Regularization](https://arxiv.org/abs/2511.05844)
*Seyed Alireza Javid,Amirhossein Bagheri,Nuria González-Prelcic*

Main category: cs.CV

TL;DR: 本文针对分类器引导扩散模型中早期去噪阶段预测过自信导致引导梯度消失的问题，提出了可微分校准方法和增强的采样引导策略，显著提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 分类器引导扩散模型在去噪早期阶段存在预测过度自信的问题，导致引导梯度消失，影响生成性能。

Method: 1. 提出基于平滑期望校准误差（Smooth ECE）的可微分校准目标，通过最小化微调来改善分类器校准。2. 开发了无需重新训练的增强采样引导方法，包括：带批次级重加权的倾斜采样、自适应熵正则化采样以保持多样性、以及基于f-散度的新型采样策略，以增强类别一致性引导并保持模式覆盖。

Result: 在ImageNet 128x128数据集上，使用ResNet-101分类器，本文的散度正则化引导方法实现了2.13的FID，优于现有分类器引导扩散方法，且无需重新训练扩散模型。校准方法也带来了可衡量的FID改进。

Conclusion: 有原则的校准和散度感知采样为分类器引导扩散模型提供了实用且有效的改进。

Abstract: Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.

</details>


### [81] [DiffSwap++: 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping](https://arxiv.org/abs/2511.05575)
*Weston Bondurant,Arkaprava Sinha,Hieu Le,Srijan Das,Stephanie Schuckers*

Main category: cs.CV

TL;DR: DiffSwap++ 是一种新颖的基于扩散的人脸交换方法，通过整合3D人脸潜在特征和条件去噪过程，显著提高了身份保留和几何一致性，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的人脸交换方法在处理复杂姿态和表情时，常出现细粒度伪影和身份保留不佳的问题。主要限制在于未能有效利用3D人脸结构来解耦身份、姿态和表情。

Method: 本文提出DiffSwap++，一个在训练过程中融入3D人脸潜在特征的扩散人脸交换流程，通过3D感知表示引导生成过程。此外，设计了一个扩散架构，将去噪过程同时基于身份嵌入和面部地标进行条件化。

Result: 在CelebA、FFHQ和CelebV-Text数据集上的广泛实验表明，DiffSwap++在保持目标姿态和表情的同时，在保留源身份方面优于现有方法。通过生物识别风格评估和用户研究进一步验证了其真实性和有效性。

Conclusion: DiffSwap++通过有效利用3D人脸结构，实现了高保真、身份保留的人脸交换，增强了几何一致性，并改进了身份与外观属性的解耦。

Abstract: Diffusion-based approaches have recently achieved strong results in face swapping, offering improved visual quality over traditional GAN-based methods. However, even state-of-the-art models often suffer from fine-grained artifacts and poor identity preservation, particularly under challenging poses and expressions. A key limitation of existing approaches is their failure to meaningfully leverage 3D facial structure, which is crucial for disentangling identity from pose and expression. In this work, we propose DiffSwap++, a novel diffusion-based face-swapping pipeline that incorporates 3D facial latent features during training. By guiding the generation process with 3D-aware representations, our method enhances geometric consistency and improves the disentanglement of facial identity from appearance attributes. We further design a diffusion architecture that conditions the denoising process on both identity embeddings and facial landmarks, enabling high-fidelity and identity-preserving face swaps. Extensive experiments on CelebA, FFHQ, and CelebV-Text demonstrate that DiffSwap++ outperforms prior methods in preserving source identity while maintaining target pose and expression. Additionally, we introduce a biometric-style evaluation and conduct a user study to further validate the realism and effectiveness of our approach. Code will be made publicly available at https://github.com/WestonBond/DiffSwapPP

</details>


### [82] [Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps](https://arxiv.org/abs/2511.05590)
*Yoojin Oh,Junhyug Noh*

Main category: cs.CV

TL;DR: 本文提出一种双分支Sigmoid头，用于解耦定位与分类，解决了基于Softmax的CAM方法中存在的偏差和符号混淆问题，显著提升了解释的忠实度和定位性能，且不影响分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有的类激活图（CAM）及其扩展方法依赖于最终的Softmax分类器，导致了两种根本性扭曲：任意偏置重要性分数的附加logit偏移和混淆兴奋性与抑制性特征的符号塌陷。

Method: 提出一种架构无关的双分支Sigmoid头。克隆预训练模型中的分类头，形成一个并行分支，该分支以每类Sigmoid输出结束。冻结原始Softmax头，仅使用类别平衡的二元监督微调Sigmoid分支。推理时，Softmax保持识别准确性，而Sigmoid分支生成类证据图，保留特征贡献的幅度和符号。

Result: 在细粒度任务（CUB-200-2011、Stanford Cars）和WSOL基准（ImageNet-1K、OpenImages30K）上，显著提高了解释的忠实度，并持续获得了Top-1定位增益，同时未降低分类准确性。该方法可与大多数CAM变体无缝集成，且开销可忽略不计。

Conclusion: 通过引入双分支Sigmoid头，成功解决了Softmax分类器在CAM方法中引入的扭曲问题，实现了更忠实的解释和更精确的定位，同时保持了原始模型的分类性能。

Abstract: Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy. Code is available at https://github.com/finallyupper/beyond-softmax.

</details>


### [83] [Google-MedGemma Based Abnormality Detection in Musculoskeletal radiographs](https://arxiv.org/abs/2511.05600)
*Soumyajit Maity,Pranjal Kamboj,Sneha Maity,Rajat Singh,Sankhadeep Chatterjee*

Main category: cs.CV

TL;DR: 本文提出了一种基于MedGemma的框架，用于肌肉骨骼X射线图像的异常自动检测，该方法利用预训练的医学基础模型，并结合轻量级多层感知器进行分类，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有肌肉骨骼X射线异常检测方法（如传统自编码器和神经网络）可能存在局限性。研究旨在利用先进的医学基础模型MedGemma，提升自动异常检测的准确性和泛化能力。

Method: 该方法利用MedGemma基础模型，其中包含一个在多种医学影像模态上预训练的SigLIP视觉编码器。预处理后的X射线图像通过MedGemma视觉骨干网络编码成高维嵌入，随后输入一个轻量级多层感知器（MLP）进行二元分类。模型还利用了MedGemma的迁移学习能力，并支持模块化训练策略（如选择性冻结编码器块）以实现高效的领域适应。

Result: 实验评估显示，基于MedGemma的分类器表现出强大的性能，超越了传统的卷积神经网络和基于自编码器的方法。MedGemma的迁移学习能力增强了模型的泛化性并优化了特征工程。

Conclusion: 研究结果表明，MedGemma驱动的分类系统能够提供可扩展且准确的异常检测，从而推动临床X射线分诊的进步，并有望在自动化医学图像分析中得到更广泛的应用。

Abstract: This paper proposes a MedGemma-based framework for automatic abnormality detection in musculoskeletal radiographs. Departing from conventional autoencoder and neural network pipelines, the proposed method leverages the MedGemma foundation model, incorporating a SigLIP-derived vision encoder pretrained on diverse medical imaging modalities. Preprocessed X-ray images are encoded into high-dimensional embeddings using the MedGemma vision backbone, which are subsequently passed through a lightweight multilayer perceptron for binary classification. Experimental assessment reveals that the MedGemma-driven classifier exhibits strong performance, exceeding conventional convolutional and autoencoder-based metrics. Additionally, the model leverages MedGemma's transfer learning capabilities, enhancing generalization and optimizing feature engineering. The integration of a modern medical foundation model not only enhances representation learning but also facilitates modular training strategies such as selective encoder block unfreezing for efficient domain adaptation. The findings suggest that MedGemma-powered classification systems can advance clinical radiograph triage by providing scalable and accurate abnormality detection, with potential for broader applications in automated medical image analysis.
  Keywords: Google MedGemma, MURA, Medical Image, Classification.

</details>


### [84] [In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing](https://arxiv.org/abs/2511.05604)
*Subash Gautam,Alejandro Vargas-Uscategui,Peter King,Hans Lohr,Alireza Bab-Hadiashar,Ivan Cole,Ehsan Asadi*

Main category: cs.CV

TL;DR: 本研究提出一种实时监控系统，用于在高沉积率机器人增材制造（HDRRAM）过程中检测并跟踪零件的形状偏差，以提高制造精度和质量。


<details>
  <summary>Details</summary>
Motivation: 高沉积率机器人增材制造（HDRRAM）工艺（如冷喷涂增材制造CSAM）虽然提高了构建速度，但由于现有开环系统中的工艺不稳定性，保持形状精度仍然是一个严峻挑战。实时检测这些偏差对于防止误差传播、确保零件质量和减少后处理至关重要。

Method: 本研究开发了一个实时监控系统，用于获取和重建正在生长的零件，并将其与接近净形的参考模型直接比较，以在制造过程中检测形状偏差。该系统还能早期识别形状不一致，并对每个偏差区域进行分割和跟踪。

Result: 该系统能够实时获取并重建生长中的零件，通过与参考模型比较，实现形状偏差的早期识别、分割和跟踪。

Conclusion: 早期识别、分割和跟踪形状偏差为及时干预和补偿提供了可能，从而有助于在HDRRAM过程中实现一致的零件质量。

Abstract: Additive manufacturing (AM) is an emerging digital manufacturing technology to produce complex and freeform objects through a layer-wise deposition. High deposition rate robotic AM (HDRRAM) processes, such as cold spray additive manufacturing (CSAM), offer significantly increased build speeds by delivering large volumes of material per unit time. However, maintaining shape accuracy remains a critical challenge, particularly due to process instabilities in current open-loop systems. Detecting these deviations as they occur is essential to prevent error propagation, ensure part quality, and minimize post-processing requirements. This study presents a real-time monitoring system to acquire and reconstruct the growing part and directly compares it with a near-net reference model to detect the shape deviation during the manufacturing process. The early identification of shape inconsistencies, followed by segmenting and tracking each deviation region, paves the way for timely intervention and compensation to achieve consistent part quality.

</details>


### [85] [Pose-Aware Multi-Level Motion Parsing for Action Quality Assessment](https://arxiv.org/abs/2511.05611)
*Shuaikang Zhu,Yang Yang,Chen Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多级运动解析框架，用于基于增强时空姿态特征的动作质量评估（AQA），在动作分割和动作评分任务上均达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 人类姿态是动作质量评估的基石，其中姿态中细微的时空变化往往能区分优劣。在高级别比赛中，这些细微的差异是评分的决定性因素，因此需要一种能捕捉这些细节的方法。

Method: 该框架包括：1) 动作单元解析器，利用姿态提取实现精确的动作分割和全面的局部-全局姿态表示；2) 运动解析器，通过时空特征学习捕捉每个动作单元的姿态变化和外观细节；3) 额外设计的条件解析器，处理与身体无关的特殊条件（如跳水水花）；4) 权重调整评分模块，以适应不同动作类型和多尺度动作单元的需求。

Result: 在大型跳水运动数据集上进行了广泛评估，结果表明所提出的多级运动解析框架在动作分割和动作评分任务上均取得了最先进的性能。

Conclusion: 该多级运动解析框架通过其分层解析和模块化设计，有效地解决了动作质量评估中的姿态分析挑战，并在实际应用中展现出卓越的性能。

Abstract: Human pose serves as a cornerstone of action quality assessment (AQA), where subtle spatial-temporal variations in pose often distinguish excellence from mediocrity. In high-level competitions, these nuanced differences become decisive factors in scoring. In this paper, we propose a novel multi-level motion parsing framework for AQA based on enhanced spatial-temporal pose features. On the first level, the Action-Unit Parser is designed with the help of pose extraction to achieve precise action segmentation and comprehensive local-global pose representations. On the second level, Motion Parser is used by spatial-temporal feature learning to capture pose changes and appearance details for each action-unit. Meanwhile, some special conditions other than body-related will impact action scoring, like water splash in diving. In this work, we design an additional Condition Parser to offer users more flexibility in their choices. Finally, Weight-Adjust Scoring Module is introduced to better accommodate the diverse requirements of various action types and the multi-scale nature of action-units. Extensive evaluations on large-scale diving sports datasets demonstrate that our multi-level motion parsing framework achieves state-of-the-art performance in both action segmentation and action scoring tasks.

</details>


### [86] [Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation](https://arxiv.org/abs/2511.05609)
*Ziying Li,Xuequan Lu,Xinkui Zhao,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为TraCe的新型文本到3D生成框架，通过将生成过程公式化为学习最佳传输轨迹，并从理论上将SDS与薛定谔桥联系起来，解决了现有方法（如SDS）导致的3D资产伪影问题，实现了更高质量的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的基于优化的文本到3D生成方法（如Score Distillation Sampling, SDS）在从预训练文本到图像扩散模型中提取知识时，经常会引入过度饱和和过度平滑等伪影，影响生成的3D资产质量。本研究旨在解决这一关键问题。

Method: 1. 将生成过程公式化为学习当前渲染分布与目标分布之间的最佳直接传输轨迹。2. 从理论上将SDS建立为薛定谔桥框架的一个简化实例，证明SDS使用了薛定谔桥的逆向过程。3. 基于此，引入轨迹中心蒸馏（TraCe）框架，重新构建薛定谔桥的数学可追踪框架，显式地从当前渲染到其文本条件去噪目标构建一个扩散桥。4. 训练一个LoRA适配模型，利用该轨迹的分数动态进行鲁棒的3D优化。

Result: TraCe框架在综合实验中持续实现了优于现有最先进技术的质量和保真度，并且能够以更小的Classifier-free Guidance (CFG) 值进行高质量生成。

Conclusion: 本文成功地通过提出TraCe框架，解决了文本到3D生成中SDS引入的伪影问题。TraCe通过将生成过程重新定义为学习最佳传输轨迹，并利用薛定谔桥的理论基础，显著提升了3D资产的生成质量和保真度，为文本到3D生成领域提供了一个更优越的解决方案。

Abstract: Recent advancements in optimization-based text-to-3D generation heavily rely on distilling knowledge from pre-trained text-to-image diffusion models using techniques like Score Distillation Sampling (SDS), which often introduce artifacts such as over-saturation and over-smoothing into the generated 3D assets. In this paper, we address this essential problem by formulating the generation process as learning an optimal, direct transport trajectory between the distribution of the current rendering and the desired target distribution, thereby enabling high-quality generation with smaller Classifier-free Guidance (CFG) values. At first, we theoretically establish SDS as a simplified instance of the Schrödinger Bridge framework. We prove that SDS employs the reverse process of an Schrödinger Bridge, which, under specific conditions (e.g., a Gaussian noise as one end), collapses to SDS's score function of the pre-trained diffusion model. Based upon this, we introduce Trajectory-Centric Distillation (TraCe), a novel text-to-3D generation framework, which reformulates the mathematically trackable framework of Schrödinger Bridge to explicitly construct a diffusion bridge from the current rendering to its text-conditioned, denoised target, and trains a LoRA-adapted model on this trajectory's score dynamics for robust 3D optimization. Comprehensive experiments demonstrate that TraCe consistently achieves superior quality and fidelity to state-of-the-art techniques.

</details>


### [87] [Personalized Image Editing in Text-to-Image Diffusion Models via Collaborative Direct Preference Optimization](https://arxiv.org/abs/2511.05616)
*Connor Dunlop,Matthew Zheng,Kavana Venkatesh,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了协作直接偏好优化（C-DPO）框架，首次实现了扩散模型中个性化图像编辑。该方法通过动态偏好图和图神经网络学习用户嵌入，并将其整合到新颖的DPO目标中，从而根据用户特定偏好生成图像编辑，并利用相似用户的协作信号。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）扩散模型在图像生成和编辑方面表现出色，但它们是通用的，无法适应个体用户的细微审美偏好。

Method: 引入了协作直接偏好优化（C-DPO）方法。将每个用户编码为动态偏好图中的节点，并通过轻量级图神经网络学习嵌入，实现具有相似视觉品味用户之间的信息共享。将这些个性化嵌入集成到新颖的DPO目标中，共同优化个体对齐和邻域一致性，以增强扩散模型的编辑能力。

Result: 通过用户研究和定量基准测试，实验证明该方法在生成符合用户偏好的编辑方面始终优于基线模型。

Conclusion: C-DPO框架成功实现了扩散模型中的个性化图像编辑，通过利用协作信号使图像编辑与用户特定偏好对齐，显著提升了编辑效果。

Abstract: Text-to-image (T2I) diffusion models have made remarkable strides in generating and editing high-fidelity images from text. Yet, these models remain fundamentally generic, failing to adapt to the nuanced aesthetic preferences of individual users. In this work, we present the first framework for personalized image editing in diffusion models, introducing Collaborative Direct Preference Optimization (C-DPO), a novel method that aligns image edits with user-specific preferences while leveraging collaborative signals from like-minded individuals. Our approach encodes each user as a node in a dynamic preference graph and learns embeddings via a lightweight graph neural network, enabling information sharing across users with overlapping visual tastes. We enhance a diffusion model's editing capabilities by integrating these personalized embeddings into a novel DPO objective, which jointly optimizes for individual alignment and neighborhood coherence. Comprehensive experiments, including user studies and quantitative benchmarks, demonstrate that our method consistently outperforms baselines in generating edits that are aligned with user preferences.

</details>


### [88] [Convolutional Fully-Connected Capsule Network (CFC-CapsNet): A Novel and Fast Capsule Network](https://arxiv.org/abs/2511.05617)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: 本文提出了一种名为CFC-CapsNet的新型胶囊网络，通过引入新的CFC层来创建更少但更强大的胶囊，从而解决了传统CapsNet在复杂数据集上性能不佳、速度慢和参数多的问题，并在多个数据集上实现了更高的准确性、更快的训练/推理速度和更少的参数。


<details>
  <summary>Details</summary>
Motivation: 胶囊网络(CapsNet)在小型数据集（如MNIST）上表现出色，但其在更复杂的数据集和实际应用中性能不佳。此外，与卷积神经网络(CNN)相比，CapsNet速度较慢且参数量更大。

Method: 本文提出了一种卷积全连接胶囊网络(CFC-CapsNet)，通过引入一种新的层（CFC层）来替代传统方法创建胶囊。这种新方法旨在生成更少但更强大的胶囊。

Result: 实验结果表明，与传统CapsNet相比，CFC-CapsNet在CIFAR-10、SVHN和Fashion-MNIST数据集上取得了具有竞争力的准确性，更快的训练和推理速度，并且使用了更少的参数。

Conclusion: CFC-CapsNet通过改进胶囊创建方法，有效解决了传统CapsNet在处理复杂数据集时的局限性，使其在性能、速度和参数效率方面均有所提升。

Abstract: A Capsule Network (CapsNet) is a relatively new classifier and one of the possible successors of Convolutional Neural Networks (CNNs). CapsNet maintains the spatial hierarchies between the features and outperforms CNNs at classifying images including overlapping categories. Even though CapsNet works well on small-scale datasets such as MNIST, it fails to achieve a similar level of performance on more complicated datasets and real applications. In addition, CapsNet is slow compared to CNNs when performing the same task and relies on a higher number of parameters. In this work, we introduce Convolutional Fully-Connected Capsule Network (CFC-CapsNet) to address the shortcomings of CapsNet by creating capsules using a different method. We introduce a new layer (CFC layer) as an alternative solution to creating capsules. CFC-CapsNet produces fewer, yet more powerful capsules resulting in higher network accuracy. Our experiments show that CFC-CapsNet achieves competitive accuracy, faster training and inference and uses less number of parameters on the CIFAR-10, SVHN and Fashion-MNIST datasets compared to conventional CapsNet.

</details>


### [89] [Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition](https://arxiv.org/abs/2511.05622)
*Nicholas Babey,Tiffany Gu,Yiheng Li,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: 该研究提出了一种融合V-JEPA 2和CoMotion的模型架构，通过结合世界动态和人体姿态数据，将动作识别建立在物理空间中，尤其提高了在复杂和遮挡场景下的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前的动作识别模型主要依赖RGB视频，学习表面的模式与标签关联，难以捕捉复杂的物理交互动态和人体姿态，特别是在复杂场景中表现不佳。具身智能体需要对人类动作有细致的物理空间理解。

Method: 提出了一种模型架构，通过融合两种互补的表示来将动作识别根植于物理空间：V-JEPA 2（提供上下文、预测性的世界动态）和CoMotion（提供明确、抗遮挡的人体姿态数据）。

Result: 该模型在InHARD和UCF-19-Y-OCC基准测试中（分别用于通用动作识别和高遮挡动作识别）表现优于其他三个基线模型，尤其在复杂、遮挡场景下性能更佳。

Conclusion: 研究结果强调，动作识别需要由空间理解而非统计模式识别来支持。

Abstract: For embodied agents to effectively understand and interact within the world around them, they require a nuanced comprehension of human actions grounded in physical space. Current action recognition models, often relying on RGB video, learn superficial correlations between patterns and action labels, so they struggle to capture underlying physical interaction dynamics and human poses in complex scenes. We propose a model architecture that grounds action recognition in physical space by fusing two powerful, complementary representations: V-JEPA 2's contextual, predictive world dynamics and CoMotion's explicit, occlusion-tolerant human pose data. Our model is validated on both the InHARD and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion action recognition, respectively. Our model outperforms three other baselines, especially within complex, occlusive scenes. Our findings emphasize a need for action recognition to be supported by spatial understanding instead of statistical pattern recognition.

</details>


### [90] [Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties](https://arxiv.org/abs/2511.05623)
*Mariafrancesca Patalano,Giovanna Capizzi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的免配准方法，用于监测复杂形状的点云数据（PCD），通过利用内在几何特征（拉普拉斯和测地距离）来检测缺陷，从而避免了传统配准和网格重建的误差和耗时。


<details>
  <summary>Details</summary>
Motivation: 现代传感技术收集的点云数据常用于监测3D物体的几何精度。然而，传统的点云数据监测流程中，配准和网格重建等预处理步骤容易出错、耗时，并可能引入伪影，从而影响监测结果和导致误报。

Method: 本文提出了一种免配准方法，无需进行配准和网格重建。该方法包含两种替代的特征学习方法和一个通用的监测方案。特征学习方法利用形状的内在几何属性，通过拉普拉斯算子和测地距离进行捕捉。在监测方案中，采用阈值技术进一步选择最能指示潜在失控状态的内在特征。

Result: 数值实验和案例研究表明，所提出的方法在识别不同类型缺陷方面表现出有效性。

Conclusion: 该研究成功开发了一种免配准的点云数据监测方法，通过利用内在几何特征和阈值监测方案，有效识别复杂形状中的缺陷，克服了传统预处理步骤的局限性。

Abstract: Modern sensing technologies have enabled the collection of unstructured point cloud data (PCD) of varying sizes, which are used to monitor the geometric accuracy of 3D objects. PCD are widely applied in advanced manufacturing processes, including additive, subtractive, and hybrid manufacturing. To ensure the consistency of analysis and avoid false alarms, preprocessing steps such as registration and mesh reconstruction are commonly applied prior to monitoring. However, these steps are error-prone, time-consuming and may introduce artifacts, potentially affecting monitoring outcomes. In this paper, we present a novel registration-free approach for monitoring PCD of complex shapes, eliminating the need for both registration and mesh reconstruction. Our proposal consists of two alternative feature learning methods and a common monitoring scheme. Feature learning methods leverage intrinsic geometric properties of the shape, captured via the Laplacian and geodesic distances. In the monitoring scheme, thresholding techniques are used to further select intrinsic features most indicative of potential out-of-control conditions. Numerical experiments and case studies highlight the effectiveness of the proposed approach in identifying different types of defects.

</details>


### [91] [Culture in Action: Evaluating Text-to-Image Models through Social Activities](https://arxiv.org/abs/2511.05681)
*Sina Malakouti,Boqing Gong,Adriana Kovashka*

Main category: cs.CV

TL;DR: 文本到图像（T2I）扩散模型存在文化偏见，尤其未能忠实描绘欠代表地区。本研究引入CULTIVate基准，通过跨文化活动（如问候、用餐、游戏等）评估T2I模型，覆盖16个国家，并提出新的可解释指标。结果显示模型对全球北方国家表现优于全球南方国家，且人类研究证实新指标与人类判断更相关。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型通过大规模网络数据训练，继承了文化偏见，未能忠实描绘欠代表地区。现有文化基准主要关注以物体为中心的类别（如食物、服装、建筑），却忽视了更能反映文化规范的社会和日常活动。此外，缺乏衡量文化忠实度的有效指标。

Method: 引入CULTIVate基准，用于评估T2I模型在跨文化活动（如问候、用餐、游戏、传统舞蹈和文化庆典）方面的表现。CULTIVate涵盖16个国家，包含576个提示和超过19,000张图像。它提供了一个基于描述符的可解释评估框架，跨越多个文化维度，包括背景、服饰、物体和互动。提出了四种新指标来衡量文化对齐、幻觉、夸大元素和多样性。

Result: 研究发现系统性差异：模型对全球北方国家的表现优于全球南方国家，并且不同的T2I系统存在不同的失败模式。人类研究证实，本文提出的指标与人类判断的相关性强于现有文本-图像指标。

Conclusion: CULTIVate基准和新的评估框架揭示了T2I模型中存在的文化偏见和系统性差异，尤其是在跨文化活动描绘方面。它提供了一种更有效的方法来量化和理解模型在文化表征方面的局限性，并为未来改进模型提供了方向。

Abstract: Text-to-image (T2I) diffusion models achieve impressive photorealism by training on large-scale web data, but models inherit cultural biases and fail to depict underrepresented regions faithfully. Existing cultural benchmarks focus mainly on object-centric categories (e.g., food, attire, and architecture), overlooking the social and daily activities that more clearly reflect cultural norms. Few metrics exist for measuring cultural faithfulness. We introduce CULTIVate, a benchmark for evaluating T2I models on cross-cultural activities (e.g., greetings, dining, games, traditional dances, and cultural celebrations). CULTIVate spans 16 countries with 576 prompts and more than 19,000 images, and provides an explainable descriptor-based evaluation framework across multiple cultural dimensions, including background, attire, objects, and interactions. We propose four metrics to measure cultural alignment, hallucination, exaggerated elements, and diversity. Our findings reveal systematic disparities: models perform better for global north countries than for the global south, with distinct failure modes across T2I systems. Human studies confirm that our metrics correlate more strongly with human judgments than existing text-image metrics.

</details>


### [92] [VMDT: Decoding the Trustworthiness of Video Foundation Models](https://arxiv.org/abs/2511.05682)
*Yujin Potter,Zhun Wang,Nicholas Crispino,Kyle Montgomery,Alexander Xiong,Ethan Y. Chang,Francesco Pinto,Yuqi Chen,Rahul Gupta,Morteza Ziyadi,Christos Christodoulopoulos,Bo Li,Chenguang Wang,Dawn Song*

Main category: cs.CV

TL;DR: 本文引入了VMDT，一个用于评估文本到视频（T2V）和视频到文本（V2T）模型信任度的统一平台，涵盖安全性、幻觉、公平性、隐私和对抗鲁棒性五个维度。评估发现现有视频基础模型存在严重信任度问题，并强调了开发更可靠模型的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型日益复杂，确保其信任度变得至关重要。然而，与文本和图像模态不同，视频模态仍缺乏全面的信任度基准。

Method: 研究人员推出了VMDT（Video-Modal DecodingTrust），这是一个统一的平台，用于评估文本到视频（T2V）和视频到文本（V2T）模型在安全性、幻觉、公平性、隐私和对抗鲁棒性五个关键信任度维度上的表现。他们使用VMDT对7个T2V模型和19个V2T模型进行了广泛评估。

Result: 评估揭示了多项重要发现：所有开源T2V模型未能识别有害查询并常生成有害视频，且比图像模态模型表现出更高的不公平性。V2T模型中，不公平性和隐私风险随模型规模增加，而幻觉和对抗鲁棒性有所改善（尽管整体性能仍低）。独特的是，安全性与模型规模无关联，表明当前安全水平受非规模因素影响。

Conclusion: 研究结果凸显了开发更鲁棒、更值得信赖的视频基础模型的迫切需求。VMDT提供了一个系统框架，用于衡量和追踪实现这一目标的进展。

Abstract: As foundation models become more sophisticated, ensuring their trustworthiness becomes increasingly critical; yet, unlike text and image, the video modality still lacks comprehensive trustworthiness benchmarks. We introduce VMDT (Video-Modal DecodingTrust), the first unified platform for evaluating text-to-video (T2V) and video-to-text (V2T) models across five key trustworthiness dimensions: safety, hallucination, fairness, privacy, and adversarial robustness. Through our extensive evaluation of 7 T2V models and 19 V2T models using VMDT, we uncover several significant insights. For instance, all open-source T2V models evaluated fail to recognize harmful queries and often generate harmful videos, while exhibiting higher levels of unfairness compared to image modality models. In V2T models, unfairness and privacy risks rise with scale, whereas hallucination and adversarial robustness improve -- though overall performance remains low. Uniquely, safety shows no correlation with model size, implying that factors other than scale govern current safety levels. Our findings highlight the urgent need for developing more robust and trustworthy video foundation models, and VMDT provides a systematic framework for measuring and tracking progress toward this goal. The code is available at https://sunblaze-ucb.github.io/VMDT-page/.

</details>


### [93] [Pedicle Screw Pairing and Registration for Screw Pose Estimation from Dual C-arm Images Using CAD Models](https://arxiv.org/abs/2511.05702)
*Yehyun Suh,Lin Li,Aric Plumley,Chaochao Zhou,Daniel Moyer,Kongbin Kang*

Main category: cs.CV

TL;DR: 本文提出了一种通过双C形臂图像解决椎弓根螺钉对应和姿态估计的方法，通过比较螺钉组合和2D-3D对齐，显著提高了螺钉定位的准确性，有望改善脊柱手术效果。


<details>
  <summary>Details</summary>
Motivation: 在脊柱减压和稳定手术中，准确匹配椎弓根螺钉的AP和LAT图像至关重要，但建立螺钉对应关系，尤其是在LAT视图中，仍然是一个显著的临床挑战。

Method: 该方法通过比较螺钉组合来解决双C形臂图像中的椎弓根螺钉对应和姿态估计问题。它还利用螺钉CAD 3D模型进行2D-3D对齐，以准确配对和估计双视图中的螺钉姿态。

Result: 结果显示，即使在配准之前，正确的螺钉组合在所有测试案例中都始终优于不正确的配对。配准后，正确的组合进一步增强了投影和图像之间的对齐，显著降低了投影误差。

Conclusion: 该方法通过提供可靠的螺钉定位反馈，有望改善脊柱手术结果。

Abstract: Accurate matching of pedicle screws in both anteroposterior (AP) and lateral (LAT) images is critical for successful spinal decompression and stabilization during surgery. However, establishing screw correspondence, especially in LAT views, remains a significant clinical challenge. This paper introduces a method to address pedicle screw correspondence and pose estimation from dual C-arm images. By comparing screw combinations, the approach demonstrates consistent accuracy in both pairing and registration tasks. The method also employs 2D-3D alignment with screw CAD 3D models to accurately pair and estimate screw pose from dual views. Our results show that the correct screw combination consistently outperforms incorrect pairings across all test cases, even prior to registration. After registration, the correct combination further enhances alignment between projections and images, significantly reducing projection error. This approach shows promise for improving surgical outcomes in spinal procedures by providing reliable feedback on screw positioning.

</details>


### [94] [Towards Better Ultrasound Video Segmentation Foundation Model: An Empirical study on SAM2 Finetuning from Data Perspective](https://arxiv.org/abs/2511.05731)
*Xing Yao,Ahana Gangopadhyay,Hsi-Ming Chang,Ravi Soni*

Main category: cs.CV

TL;DR: 本研究对SAM2在超声视频分割中的适应性进行了以数据为中心的研究，发现数据规模和时间上下文比模型架构更重要，并提出联合训练是一种有效策略。


<details>
  <summary>Details</summary>
Motivation: 超声视频分割面临挑战，而像SAM2这样的基础模型在医疗影像领域表现不佳。当前适应性研究主要侧重于架构修改，但数据特性和训练方案的影响尚未被系统地检验。

Method: 本研究对SAM2在超声视频分割中的适应性进行了全面的、以数据为中心的调查。分析了训练集大小、视频时长和增强方案在三种范式（任务特定微调、中间适应、多任务联合训练）、五种SAM2变体和多种提示模式下的影响。此外，设计了六种超声特有的增强方法，并评估了它们相对于通用策略的效果。

Result: 实验表明，数据规模和时间上下文在适应性能中扮演着比模型架构或初始化更决定性的角色。此外，联合训练在模态对齐和任务专业化之间提供了一种高效的折衷方案。

Conclusion: 这项工作旨在为开发超声视频分析中高效、数据感知的SAM2适应性流程提供实证见解。

Abstract: Ultrasound (US) video segmentation remains a challenging problem due to strong inter- and intra-dataset variability, motion artifacts, and limited annotated data. Although foundation models such as Segment Anything Model 2 (SAM2) demonstrate strong zero-shot and prompt-guided segmentation capabilities, their performance deteriorates substantially when transferred to medical imaging domains. Current adaptation studies mainly emphasize architectural modifications, while the influence of data characteristics and training regimes has not been systematically examined. In this study, we present a comprehensive, data-centric investigation of SAM2 adaptation for ultrasound video segmentation. We analyze how training-set size, video duration, and augmentation schemes affect adaptation performance under three paradigms: task-specific fine-tuning, intermediate adaptation, and multi-task joint training, across five SAM2 variants and multiple prompting modes. We further design six ultrasound-specific augmentations, assessing their effect relative to generic strategies. Experiments on three representative ultrasound datasets reveal that data scale and temporal context play a more decisive role than model architecture or initialization. Moreover, joint training offers an efficient compromise between modality alignment and task specialization. This work aims to provide empirical insights for developing efficient, data-aware adaptation pipelines for SAM2 in ultrasound video analysis.

</details>


### [95] [A Second-Order Attention Mechanism For Prostate Cancer Segmentation and Detection in Bi-Parametric MRI](https://arxiv.org/abs/2511.05760)
*Mateo Ortiz,Juan Olmos,Fabio Martínez*

Main category: cs.CV

TL;DR: 本文提出了一种基于黎曼流形的二阶几何注意力（SOGA）机制，用于指导分割网络，从双参数MRI中检测具有临床意义的前列腺癌（csPCa）病灶，并在公开数据集上取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 双参数MRI检测csPCa虽然是非侵入性技术，但其分析高度依赖专家主观判读。现有深度学习方法受限于对大量标注数据的需求，且病灶在不同前列腺区域的高度变异性也带来了挑战。

Method: 研究引入了一种二阶几何注意力（SOGA）机制，该机制通过跳跃连接引导专门的分割网络检测csPCa病灶。所提出的注意力机制在黎曼流形上建模，从对称正定（SPD）表示中学习。该机制被集成到标准的U-Net和nnU-Net骨干网络中，并在公开的PI-CAI和独立的Prostate158数据集上进行了验证。

Result: 在PI-CAI数据集上，该方法实现了0.37的平均精度（AP）和0.83的ROC曲线下面积（AUC-ROC），优于基线网络和现有的注意力方法。在Prostate158独立测试集上，取得了0.37的AP和0.75的AUC-ROC，证实了其强大的泛化能力和判别性的学习表示。

Conclusion: 所提出的SOGA机制能够有效地指导深度学习网络检测csPCa病灶，并展现出强大的泛化能力和学习到具有判别性的特征表示，从而改善了csPCa的诊断准确性。

Abstract: The detection of clinically significant prostate cancer lesions (csPCa) from biparametric magnetic resonance imaging (bp-MRI) has emerged as a noninvasive imaging technique for improving accurate diagnosis. Nevertheless, the analysis of such images remains highly dependent on the subjective expert interpretation. Deep learning approaches have been proposed for csPCa lesions detection and segmentation, but they remain limited due to their reliance on extensively annotated datasets. Moreover, the high lesion variability across prostate zones poses additional challenges, even for expert radiologists. This work introduces a second-order geometric attention (SOGA) mechanism that guides a dedicated segmentation network, through skip connections, to detect csPCa lesions. The proposed attention is modeled on the Riemannian manifold, learning from symmetric positive definitive (SPD) representations. The proposed mechanism was integrated into standard U-Net and nnU-Net backbones, and was validated on the publicly available PI-CAI dataset, achieving an Average Precision (AP) of 0.37 and an Area Under the ROC Curve (AUC-ROC) of 0.83, outperforming baseline networks and attention-based methods. Furthermore, the approach was evaluated on the Prostate158 dataset as an independent test cohort, achieving an AP of 0.37 and an AUC-ROC of 0.75, confirming robust generalization and suggesting discriminative learned representations.

</details>


### [96] [Sign language recognition from skeletal data using graph and recurrent neural networks](https://arxiv.org/abs/2511.05772)
*B. Mederos,J. Mejía,A. Medina-Reyes,Y. Espinosa-Almeyda,J. D. Díaz-Roman,I. Rodríguez-Mederos,M. Mejía-Carreon,F. Gonzalez-Lopez*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨骼姿态数据的图-GRU时间网络，用于识别孤立手语手势，并在AUTSL数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过有效建模视频序列中帧间的空间和时间依赖性，实现对孤立手语手势的准确识别。

Method: 该研究提出了一种Graph-GRU时间网络。该网络利用从视频序列中提取的骨骼姿态数据，同时建模帧间的空间和时间依赖性，以实现手势分类。

Result: 该方法在AUTSL（安卡拉大学土耳其手语）数据集上进行了训练和评估，取得了高准确率。实验结果证明了将基于图的空间表示与时间建模相结合的有效性，并提供了一个可扩展的手语识别框架。

Conclusion: 该方法的结果突出了姿态驱动方法在手语理解方面的潜力，尤其是在结合图基空间表示和时间建模时。

Abstract: This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial representations with temporal modeling, providing a scalable framework for sign language recognition. The results of this approach highlight the potential of pose-driven methods for sign language understanding.

</details>


### [97] [Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale](https://arxiv.org/abs/2511.05705)
*David Acuna,Chao-Han Huck Yang,Yuntian Deng,Jaehun Jung,Ximing Lu,Prithviraj Ammanabrolu,Hyunwoo Kim,Yuan-Hong Liao,Yejin Choi*

Main category: cs.CV

TL;DR: 本文提出一个包含超过100万高质量合成视觉中心问题的推理数据生成框架，并利用该数据微调Qwen2.5-VL-7B，在多个视觉中心基准测试中超越了所有开放数据基线和一些封闭数据模型，并展现出积极的跨模态迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理的进展主要依赖于未公开的数据集和专有数据合成方法，导致如何系统地构建大规模、以视觉为中心的推理数据集，特别是超越视觉数学任务的数据集，仍存在疑问。

Method: 本文提出了一个两阶段（规模和复杂性）的数据生成框架，用于创建多样化技能和复杂度的视觉中心推理数据。推理轨迹通过结合VLM和推理LLM的两阶段过程合成，生成丰富的CoT（思维链）轨迹。数据集还包含支持离线和在线强化学习的偏好数据和指令提示。

Result: 在本文数据上微调的Qwen2.5-VL-7B在所有评估的视觉中心基准测试中均优于所有开放数据基线，甚至在V* Bench、CV-Bench和MMStar-V上超越了MiMo-VL-7B-RL等强大的封闭数据模型。尽管数据完全以视觉为中心，但它对纯文本推理（MMLU-Pro）和音频推理（MMAU）以及单证据具身问答（NiEH）也产生了积极的迁移效果。经验分析表明：(i) 使用高质量、非线性推理轨迹的数据进行SFT对有效的在线RL至关重要；(ii) 分阶段离线RL的性能与在线RL相当，同时降低了计算需求；(iii) 仔细的高质量数据SFT可以显著改善域外和跨模态的迁移。

Conclusion: 高质量数据上使用非线性推理轨迹进行SFT对于有效的在线强化学习至关重要；分阶段离线强化学习可以达到与在线强化学习相当的性能，同时减少计算需求；仔细的高质量数据SFT能够显著提升域外和跨模态的迁移能力。

Abstract: Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.

</details>


### [98] [TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2511.05782)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 本文提出TCSA-UDA框架，通过文本驱动的跨语义对齐，利用领域不变的文本类别描述指导视觉特征学习，以解决医学图像无监督域适应分割中的跨模态域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中的无监督域适应（UDA）因CT和MRI等成像模态之间显著的域偏移而面临巨大挑战。尽管最近的视觉-语言表示学习方法前景广阔，但其在UDA分割任务中的潜力尚未得到充分探索。

Method: 本文提出TCSA-UDA（Text-driven Cross-Semantic Alignment）框架。该方法引入了视觉-语言协方差余弦损失，直接将图像编码器特征与类间文本语义关系对齐，以鼓励语义有意义且模态不变的特征表示。此外，它还包含一个原型对齐模块，利用高级语义原型对齐跨域的类别级像素特征分布，从而缓解残余的类别级差异并增强跨模态一致性。

Result: 在具有挑战性的跨模态心脏、腹部和脑肿瘤分割基准测试中，广泛的实验证明TCSA-UDA框架显著减少了域偏移，并持续优于最先进的UDA方法。

Conclusion: TCSA-UDA框架为将语言驱动的语义整合到域自适应医学图像分析中建立了一个新范式。

Abstract: Unsupervised domain adaptation for medical image segmentation remains a significant challenge due to substantial domain shifts across imaging modalities, such as CT and MRI. While recent vision-language representation learning methods have shown promise, their potential in UDA segmentation tasks remains underexplored. To address this gap, we propose TCSA-UDA, a Text-driven Cross-Semantic Alignment framework that leverages domain-invariant textual class descriptions to guide visual representation learning. Our approach introduces a vision-language covariance cosine loss to directly align image encoder features with inter-class textual semantic relations, encouraging semantically meaningful and modality-invariant feature representations. Additionally, we incorporate a prototype alignment module that aligns class-wise pixel-level feature distributions across domains using high-level semantic prototypes. This mitigates residual category-level discrepancies and enhances cross-modal consistency. Extensive experiments on challenging cross-modality cardiac, abdominal, and brain tumor segmentation benchmarks demonstrate that our TCSA-UDA framework significantly reduces domain shift and consistently outperforms state-of-the-art UDA methods, establishing a new paradigm for integrating language-driven semantics into domain-adaptive medical image analysis.

</details>


### [99] [Position-Prior-Guided Network for System Matrix Super-Resolution in Magnetic Particle Imaging](https://arxiv.org/abs/2511.05795)
*Xuqing Geng,Lei Su,Zhongwei Bian,Zewen Sun,Jiaxuan Wen,Jie Tian,Yang Du*

Main category: cs.CV

TL;DR: 该研究通过将位置先验知识整合到基于深度学习的超分辨率（SR）框架中，以加速磁粒子成像（MPI）系统矩阵（SM）的校准过程。


<details>
  <summary>Details</summary>
Motivation: 磁粒子成像（MPI）中的系统矩阵（SM）校准耗时且在系统参数变化时需要重复测量。现有基于深度学习的超分辨率（SR）方法未能充分利用SM相关的物理先验知识，如对称位置先验。

Method: 该研究将位置先验知识整合到现有的SM校准框架中，并进行了理论论证和实证验证。实验涵盖了2D和3D SM的超分辨率方法。

Result: 通过2D和3D SM超分辨率方法的实验，经验性地验证了整合位置先验知识的有效性。

Conclusion: 将位置先验知识整合到现有的深度学习超分辨率框架中，可以有效提高磁粒子成像系统矩阵校准的效率和准确性。

Abstract: Magnetic Particle Imaging (MPI) is a novel medical imaging modality. One of the established methods for MPI reconstruction is based on the System Matrix (SM). However, the calibration of the SM is often time-consuming and requires repeated measurements whenever the system parameters change. Current methodologies utilize deep learning-based super-resolution (SR) techniques to expedite SM calibration; nevertheless, these strategies do not fully exploit physical prior knowledge associated with the SM, such as symmetric positional priors. Consequently, we integrated positional priors into existing frameworks for SM calibration. Underpinned by theoretical justification, we empirically validated the efficacy of incorporating positional priors through experiments involving both 2D and 3D SM SR methods.

</details>


### [100] [MACMD: Multi-dilated Contextual Attention and Channel Mixer Decoding for Medical Image Segmentation](https://arxiv.org/abs/2511.05803)
*Lalit Maurya,Honghai Liu,Reyer Zwiggelaar*

Main category: cs.CV

TL;DR: 本文提出了一种名为MACMD的新型解码器，用于医学图像分割。该解码器通过增强注意力机制和跨通道混合模块，有效整合了局部细节和全局上下文信息，解决了现有模型在长距离依赖建模和信息损失方面的局限性，并在分割精度和计算效率上超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法面临挑战：卷积神经网络（CNN）善于捕获局部特征但难以建模长距离依赖；Transformer通过自注意力机制解决了长距离依赖问题，但难以保留局部上下文信息。此外，主流的编码器-解码器架构存在两个主要局限：1) 浅层细节在通过深层时会丢失；2) 编码器和解码器之间局部细节与全局上下文的整合效率低下。

Method: 为解决上述挑战，本文提出了基于MACMD的解码器。该设计通过跳跃连接增强了编码器和解码器之间的注意力机制和通道混合。具体而言，它利用了分层空洞卷积（hierarchical dilated convolutions）、注意力驱动调制（attention-driven modulation）和一个跨通道混合模块（cross channel-mixing module）来捕获长距离依赖，同时保留局部上下文细节。

Result: 研究团队使用多个Transformer编码器在二元和多器官分割任务上评估了所提出的方法。结果表明，该方法在Dice分数和计算效率方面均优于现有先进方法，证明了其在实现准确和鲁棒分割性能方面的有效性。

Conclusion: 所提出的MACMD-based解码器通过有效整合局部和全局信息，显著提升了医学图像分割的准确性和鲁棒性。该方法在Dice分数和计算效率上均超越了现有最先进技术，为精确的医学图像分割提供了有效的解决方案。

Abstract: Medical image segmentation faces challenges due to variations in anatomical structures. While convolutional neural networks (CNNs) effectively capture local features, they struggle with modeling long-range dependencies. Transformers mitigate this issue with self-attention mechanisms but lack the ability to preserve local contextual information. State-of-the-art models primarily follow an encoder-decoder architecture, achieving notable success. However, two key limitations remain: (1) Shallow layers, which are closer to the input, capture fine-grained details but suffer from information loss as data propagates through deeper layers. (2) Inefficient integration of local details and global context between the encoder and decoder stages. To address these challenges, we propose the MACMD-based decoder, which enhances attention mechanisms and facilitates channel mixing between encoder and decoder stages via skip connections. This design leverages hierarchical dilated convolutions, attention-driven modulation, and a cross channel-mixing module to capture long-range dependencies while preserving local contextual details, essential for precise medical image segmentation. We evaluated our approach using multiple transformer encoders on both binary and multi-organ segmentation tasks. The results demonstrate that our method outperforms state-of-the-art approaches in terms of Dice score and computational efficiency, highlighting its effectiveness in achieving accurate and robust segmentation performance. The code available at https://github.com/lalitmaurya47/MACMD

</details>


### [101] [LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting](https://arxiv.org/abs/2511.05818)
*Yuchen Su,Zhineng Chen,Yongkun Du,Zuxuan Wu,Hongtao Xie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains largely unsolved. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape method based on low-rank approximation for precise detection and a triple assignment detection head to enable fast inference. Specifically, unlike other shape representation methods that employ data-irrelevant parameterization, our data-driven approach derives a low-rank subspace directly from labeled text boundaries. To ensure this process is robust against the inherent annotation noise in this data, we utilize a specialized recovery method based on an $\ell_1$-norm formulation, which accurately reconstructs the text shape with only a few key orthogonal vectors. By exploiting the inherent shape correlation among different text contours, our method achieves consistency and compactness in shape representation. Next, the triple assignment scheme introduces a novel architecture where a deep sparse branch (for stabilized training) is used to guide the learning of an ultra-lightweight sparse branch (for accelerated inference), while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on several challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code will be available at: https://github.com/ychensu/LRANet-PP.git

</details>


### [102] [TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation](https://arxiv.org/abs/2511.05833)
*Taixi Chen,Yiu-ming Cheung*

Main category: cs.CV

TL;DR: 本文提出了一种名为TYrPPG的远程光电容积描记法（rPPG）算法，该算法基于Mambaout结构，通过创新的门控视频理解块（GVB）和综合监督损失函数（CSL）实现了高效且准确的远程心率估计，并在常用数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的rPPG模型多基于Transformer，计算效率低下。Mamba模型在NLP中表现高效，但其核心SSM模块被证明对视觉任务不必要（Mambaout模型）。因此，研究动机是证明基于Mambaout的模块在远程心率学习中的可行性，并开发一种更高效的rPPG算法。

Method: 本文提出TYrPPG算法，核心方法包括：1) 引入创新的门控视频理解块（GVB），该块基于Mambaout结构，并集成了2D-CNN和3D-CNN以增强视频理解能力；2) 提出一种综合监督损失函数（CSL）及其弱监督变体，以提高模型的学习能力。

Result: 实验结果表明，TYrPPG算法在常用的rPPG数据集上取得了最先进（SOTA）的性能，显示了其在远程心率估计方面的应用前景和优越性。

Conclusion: 研究证明了基于Mambaout的模块在远程心率学习中的可行性，并成功开发了TYrPPG算法。该算法通过结合Mambaout结构、2D/3D CNN以及改进的损失函数，实现了高效且高精度的远程心率估计，为rPPG领域提供了新的解决方案。

Abstract: Remote photoplethysmography (rPPG) can remotely extract physiological signals from RGB video, which has many advantages in detecting heart rate, such as low cost and no invasion to patients. The existing rPPG model is usually based on the transformer module, which has low computation efficiency. Recently, the Mamba model has garnered increasing attention due to its efficient performance in natural language processing tasks, demonstrating potential as a substitute for transformer-based algorithms. However, the Mambaout model and its variants prove that the SSM module, which is the core component of the Mamba model, is unnecessary for the vision task. Therefore, we hope to prove the feasibility of using the Mambaout-based module to remotely learn the heart rate. Specifically, we propose a novel rPPG algorithm called uncomplicated and enhanced learning capability rPPG (TYrPPG). This paper introduces an innovative gated video understanding block (GVB) designed for efficient analysis of RGB videos. Based on the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhance video understanding for analysis. In addition, we propose a comprehensive supervised loss function (CSL) to improve the model's learning capability, along with its weakly supervised variants. The experiments show that our TYrPPG can achieve state-of-the-art performance in commonly used datasets, indicating its prospects and superiority in remote heart rate estimation. The source code is available at https://github.com/Taixi-CHEN/TYrPPG.

</details>


### [103] [Hilbert-Guided Block-Sparse Local Attention](https://arxiv.org/abs/2511.05832)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 通过使用希尔伯特曲线重新排列图像令牌，本文提出了一种新的局部注意力窗口和邻域构建方法，显著提高了块稀疏性，从而结合现有块稀疏核，大幅提升了2D局部注意力的计算效率。


<details>
  <summary>Details</summary>
Motivation: 全局自注意力在处理高分辨率图像时面临二次方的计算和内存成本。虽然局部注意力可以降低复杂度，但传统局部注意力模式中的窗口内令牌在1D序列中不连续，导致块稀疏核难以实现显著加速。

Method: 该研究提出了一种基于希尔伯特曲线构建窗口和邻域的新方法。首先，图像令牌沿希尔伯特曲线重新排序，然后在此重新排序的1D序列上形成窗口和邻域。从块稀疏的角度来看，这种策略显著增加了块稀疏性，并可与现有块稀疏核结合，以提高2D局部注意力的效率。

Result: 实验表明，所提出的希尔伯特窗口注意力（Hilbert Window Attention）和希尔伯特滑动注意力（Hilbert Slide Attention）分别将窗口注意力和滑动注意力加速了约4倍和18倍。实例化为希尔伯特窗口Transformer和希尔伯特邻域Transformer后，实现了端到端加速，且准确性损失极小。

Conclusion: 将希尔伯特曲线引导的局部注意力与块稀疏核相结合，为提高图像2D局部注意力的效率提供了一种通用且实用的方法。

Abstract: The quadratic compute and memory costs of global self-attention severely limit its use in high-resolution images. Local attention reduces complexity by restricting attention to neighborhoods. Block-sparse kernels can further improve the efficiency of local attention, but conventional local attention patterns often fail to deliver significant speedups because tokens within a window are not contiguous in the 1D sequence. This work proposes a novel method for constructing windows and neighborhoods based on the Hilbert curve. Image tokens are first reordered along a Hilbert curve, and windows and neighborhoods are then formed on the reordered 1D sequence. From a block-sparse perspective, this strategy significantly increases block sparsity and can be combined with existing block-sparse kernels to improve the efficiency of 2D local attention. Experiments show that the proposed Hilbert Window Attention and Hilbert Slide Attention can accelerate window attention and slide attention by about $4\times$ and $18\times$, respectively. To assess practicality, the strategy is instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood Transformer, both of which achieve end-to-end speedups with minimal accuracy loss. Overall, combining Hilbert-guided local attention with block-sparse kernels offers a general and practical approach to enhancing the efficiency of 2D local attention for images. The code is available at https://github.com/Yunge6666/Hilbert-Local-Attention.

</details>


### [104] [Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation](https://arxiv.org/abs/2511.05841)
*Changqing Gong,Huafeng Qin,Mounim A. El-Yacoubi*

Main category: cs.CV

TL;DR: 本文提出了一种名为CLFA的轻量级跨层融合适配器框架，通过改造CLIP模型进行基于手写笔迹的阿尔茨海默病（AD）筛查，实现了零样本推理和跨任务泛化，并揭示了有效区分AD的手写模式和任务类型。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期检测至关重要。手写笔迹作为一种非侵入性且经济高效的方式，能反映早期AD患者的运动和认知能力下降。现有研究多依赖在线轨迹和手工特征，未系统探究任务类型对诊断性能和跨任务泛化的影响。同时，大型视觉语言模型在其他医学领域展现出强大的零/少样本异常检测能力，但在手写笔迹疾病检测中应用不足。

Method: 引入了轻量级的“跨层融合适配器”（CLFA）框架，该框架改造了CLIP模型以用于基于手写笔迹的AD筛查。CLFA在视觉编码器内部植入多级融合适配器，逐步调整表征以适应手写笔迹特有的医学线索，从而实现无提示且高效的零样本推理。研究还系统地调查了跨任务泛化能力，即在一个特定手写任务上训练，在未见过的任务上进行评估。

Result: CLFA框架揭示了哪些任务类型和书写模式能最有效地辨别AD。广泛的分析进一步突出了有助于早期AD识别的特征性笔画模式和任务层面的因素。

Conclusion: 该研究为AD的诊断提供了深入见解，并为基于手写笔迹的认知评估提供了基准。CLFA框架在AD早期诊断中具有应用潜力。

Abstract: Alzheimer's disease is a prevalent neurodegenerative disorder for which early detection is critical. Handwriting-often disrupted in prodromal AD-provides a non-invasive and cost-effective window into subtle motor and cognitive decline. Existing handwriting-based AD studies, mostly relying on online trajectories and hand-crafted features, have not systematically examined how task type influences diagnostic performance and cross-task generalization. Meanwhile, large-scale vision language models have demonstrated remarkable zero or few-shot anomaly detection in natural images and strong adaptability across medical modalities such as chest X-ray and brain MRI. However, handwriting-based disease detection remains largely unexplored within this paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA implants multi-level fusion adapters within the visual encoder to progressively align representations toward handwriting-specific medical cues, enabling prompt-free and efficient zero-shot inference. Using this framework, we systematically investigate cross-task generalization-training on a specific handwriting task and evaluating on unseen ones-to reveal which task types and writing patterns most effectively discriminate AD. Extensive analyses further highlight characteristic stroke patterns and task-level factors that contribute to early AD identification, offering both diagnostic insights and a benchmark for handwriting-based cognitive assessment.

</details>


### [105] [Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology](https://arxiv.org/abs/2511.05853)
*Bingyang Guo,Qiang Zuo,Ruiyun Yu*

Main category: cs.CV

TL;DR: 本研究构建了一个高质量的陶瓷封装基板（CPS）表面缺陷3D点云分割数据集CPS3D-Seg，并提出了一种基于因果推断的新型3D分割方法CINet，在缺陷检测方面显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 3D数据分割对工业应用至关重要，尤其是在集成电路（IC）领域检测细微缺陷。陶瓷封装基板（CPS）作为IC封装的重要材料，其复杂结构和微小缺陷以及缺乏公开数据集，严重阻碍了CPS表面缺陷检测的发展。

Method: 研究构建了高质量的CPS3D-Seg点云数据集（包含1300个样本，20个产品类别，具有点级别标注和最佳分辨率/精度）。在此基础上，对SOTA点云分割算法进行了全面基准测试，并提出了一种基于因果推断的新型3D分割方法CINet，该方法通过结构细化（SR）和质量评估（QA）模块量化点云中潜在的混杂因素。

Result: CPS3D-Seg数据集在现有3D工业数据集中具有最佳的点分辨率和精度。实验结果表明，CINet在mIoU和准确性方面均显著优于现有算法。

Conclusion: 本研究成功构建了用于CPS表面缺陷3D分割的高质量数据集，并提出了一种有效的、基于因果推断的3D分割方法CINet，为IC封装领域的缺陷检测提供了重要工具和基准。

Abstract: The effective segmentation of 3D data is crucial for a wide range of industrial applications, especially for detecting subtle defects in the field of integrated circuits (IC). Ceramic package substrates (CPS), as an important electronic material, are essential in IC packaging owing to their superior physical and chemical properties. However, the complex structure and minor defects of CPS, along with the absence of a publically available dataset, significantly hinder the development of CPS surface defect detection. In this study, we construct a high-quality point cloud dataset for 3D segmentation of surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution and precision compared to existing 3D industrial datasets. CPS3D-Seg consists of 1300 point cloud samples under 20 product categories, and each sample provides accurate point-level annotations. Meanwhile, we conduct a comprehensive benchmark based on SOTA point cloud segmentation algorithms to validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D segmentation method based on causal inference (CINet), which quantifies potential confounders in point clouds through Structural Refine (SR) and Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet significantly outperforms existing algorithms in both mIoU and accuracy.

</details>


### [106] [CGCE: Classifier-Guided Concept Erasure in Generative Models](https://arxiv.org/abs/2511.05865)
*Viet Nguyen,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了分类器引导概念擦除（CGCE）框架，这是一种即插即用的方法，通过在推理时修改文本嵌入来鲁棒地擦除生成模型中的不良概念，同时不影响模型原始质量，从而在安全性和性能之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型在生成高质量内容的同时，也带来了生成不安全内容的安全隐患。现有的概念擦除方法容易受到对抗性攻击，并且在实现鲁棒擦除时，往往会损害模型对安全、无关概念的生成质量。

Method: CGCE是一个高效的即插即用框架，无需修改生成模型的原始权重。它利用一个轻量级分类器对文本嵌入进行操作，首先检测并随后精炼包含不良概念的提示词。该方法通过在推理时仅修改不安全的嵌入来防止有害内容生成，同时保持模型在良性提示上的原始质量。它还具有高度可扩展性，通过聚合多个分类器的指导，支持多概念擦除。

Result: 广泛的实验表明，CGCE在应对各种红队攻击时，达到了最先进的鲁棒性。该方法还保持了高生成效用，在安全性和性能之间展现出卓越的平衡。CGCE成功应用于多种现代T2I（文本到图像）和T2V（文本到视频）模型，证明了其通用性。

Conclusion: CGCE被确立为实现安全生成式AI的一个实用且有效的解决方案。

Abstract: Recent advancements in large-scale generative models have enabled the creation of high-quality images and videos, but have also raised significant safety concerns regarding the generation of unsafe content. To mitigate this, concept erasure methods have been developed to remove undesirable concepts from pre-trained models. However, existing methods remain vulnerable to adversarial attacks that can regenerate the erased content. Moreover, achieving robust erasure often degrades the model's generative quality for safe, unrelated concepts, creating a difficult trade-off between safety and performance. To address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE), an efficient plug-and-play framework that provides robust concept erasure for diverse generative models without altering their original weights. CGCE uses a lightweight classifier operating on text embeddings to first detect and then refine prompts containing undesired concepts. This approach is highly scalable, allowing for multi-concept erasure by aggregating guidance from several classifiers. By modifying only unsafe embeddings at inference time, our method prevents harmful content generation while preserving the model's original quality on benign prompts. Extensive experiments show that CGCE achieves state-of-the-art robustness against a wide range of red-teaming attacks. Our approach also maintains high generative utility, demonstrating a superior balance between safety and performance. We showcase the versatility of CGCE through its successful application to various modern T2I and T2V models, establishing it as a practical and effective solution for safe generative AI.

</details>


### [107] [Aerial Image Stitching Using IMU Data from a UAV](https://arxiv.org/abs/2511.06841)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.CV

TL;DR: 本文提出一种结合IMU数据和计算机视觉的无人机图像拼接新方法，旨在提高在复杂场景下拼接的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机图像拼接面临挑战，传统基于特征的算法在特征检测和匹配中易出现错误和模糊性，特别是在大位移、旋转和相机姿态变化等复杂场景下。

Method: 该方法结合IMU数据和计算机视觉技术，包括：估计无人机在连续图像间的位移和旋转、校正透视畸变、计算单应性矩阵，然后利用标准图像拼接算法对齐和融合图像。

Result: 该方法利用IMU数据提供额外信息，校正多种畸变，易于集成到现有无人机工作流程。实验证明其有效性和鲁棒性，在准确性和可靠性方面优于现有基于特征的算法，特别是在挑战性场景下表现更佳。

Conclusion: 该研究提出了一种利用IMU数据和计算机视觉技术进行无人机图像拼接的新颖方法，有效解决了传统算法的局限性，显著提高了在复杂场景下的拼接精度和鲁棒性。

Abstract: Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and remote sensing applications. One of the main challenges is to stitch together multiple images into a single high-resolution image that covers a large area. Featurebased image stitching algorithms are commonly used but can suffer from errors and ambiguities in feature detection and matching. To address this, several approaches have been proposed, including using bundle adjustment techniques or direct image alignment. In this paper, we present a novel method that uses a combination of IMU data and computer vision techniques for stitching images captured by a UAV. Our method involves several steps such as estimating the displacement and rotation of the UAV between consecutive images, correcting for perspective distortion, and computing a homography matrix. We then use a standard image stitching algorithm to align and blend the images together. Our proposed method leverages the additional information provided by the IMU data, corrects for various sources of distortion, and can be easily integrated into existing UAV workflows. Our experiments demonstrate the effectiveness and robustness of our method, outperforming some of the existing feature-based image stitching algorithms in terms of accuracy and reliability, particularly in challenging scenarios such as large displacements, rotations, and variations in camera pose.

</details>


### [108] [Light-Field Dataset for Disparity Based Depth Estimation](https://arxiv.org/abs/2511.05866)
*Suresh Nehra,Aupendu Kar,Jayanta Mukhopadhyay,Prabir Kumar Biswas*

Main category: cs.CV

TL;DR: 本文介绍了一个公开可用的光场图像数据集，用于开发和测试基于视差的光场深度估计算法，并探讨了焦距位置对视差的影响以及现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 光场相机因其捕获空间和角度信息的能力，被广泛用于三维场景的深度估计。然而，为了设计、开发和测试新颖的基于视差的光场深度估计算法，急需合适的、公开可用的光场图像数据集。此外，现有数据集存在不足，且焦距位置对视差的影响是一个关键的权衡问题。

Method: 研究人员创建并详细描述了一个新的光场图像数据集。该数据集包含使用Lytro Illum相机捕获的285张真实光场图像和13张合成光场图像。此外，还创建了一个具有与真实光场相机相似视差特性的合成数据集，以及利用机械龙门系统和Blender创建的真实和合成立体光场数据集。

Result: 本文推出了一个公开可用的光场图像数据集（包含285张真实和13张合成光场图像），该数据集还包括一个具有与真实光场相机相似视差特性的合成子集，以及真实和合成的立体光场数据集。研究还展示了焦距位置对三维点视差的影响，并指出了当前可用光场数据集的不足。

Conclusion: 所提出的光场数据集为开发和测试新颖的基于视差的光场深度估计算法提供了必要的资源，解决了现有数据集的局限性，并考虑了焦距位置对视差的关键影响。

Abstract: A Light Field (LF) camera consists of an additional two-dimensional array of micro-lenses placed between the main lens and sensor, compared to a conventional camera. The sensor pixels under each micro-lens receive light from a sub-aperture of the main lens. This enables the image sensor to capture both spatial information and the angular resolution of a scene point. This additional angular information is used to estimate the depth of a 3-D scene. The continuum of virtual viewpoints in light field data enables efficient depth estimation using Epipolar Line Images (EPIs) with robust occlusion handling. However, the trade-off between angular information and spatial information is very critical and depends on the focal position of the camera. To design, develop, implement, and test novel disparity-based light field depth estimation algorithms, the availability of suitable light field image datasets is essential. In this paper, a publicly available light field image dataset is introduced and thoroughly described. We have also demonstrated the effect of focal position on the disparity of a 3-D point as well as the shortcomings of the currently available light field dataset. The proposed dataset contains 285 light field images captured using a Lytro Illum LF camera and 13 synthetic LF images. The proposed dataset also comprises a synthetic dataset with similar disparity characteristics to those of a real light field camera. A real and synthetic stereo light field dataset is also created by using a mechanical gantry system and Blender. The dataset is available at https://github.com/aupendu/light-field-dataset.

</details>


### [109] [MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2511.05876)
*Jian Zhu,Xin Zou,Jun Sun,Cheng Luo,Lei Liu,Lingfang Zeng,Ning Zhang,Bian Wu,Chang Tang,Lirong Dai*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多视图聚类方法MoEGCL，通过混合专家网络实现样本级别的细粒度图融合，并结合自图对比学习，显著提升了深度多视图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的多视图聚类方法在图融合方面存在粗粒度问题，通常在视图级别进行加权融合，而非更精细的样本级别融合，限制了性能。

Method: 本文提出了MoEGCL，包含两个模块：1) 混合自图融合 (MoEGF) 模块，它构建自图并利用混合专家网络实现样本级别的细粒度自图融合；2) 自图对比学习 (EGCL) 模块，用于对齐融合表示与视图特定表示，并增强同一簇内样本的表示相似性，进一步提升细粒度图表示。

Result: 广泛的实验表明，MoEGCL在深度多视图聚类任务中取得了最先进的（state-of-the-art）结果。

Conclusion: MoEGCL通过创新的样本级细粒度图融合策略和自图对比学习，有效解决了现有方法的局限性，显著提升了多视图聚类的性能。

Abstract: In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.

</details>


### [110] [Towards Frequency-Adaptive Learning for SAR Despeckling](https://arxiv.org/abs/2511.05890)
*Ziqing Ma,Chang Yang,Zhichang Guo,Yao Li*

Main category: cs.CV

TL;DR: SAR-FAH是一种频率自适应异构去斑模型，通过小波分解将SAR图像分为不同频率子带，并为低频和高频分量设计专门的子网络（低频使用神经ODE，高频使用带可变形卷积的U-Net），从而在去噪的同时更好地保留边缘和纹理。


<details>
  <summary>Details</summary>
Motivation: SAR图像固有的斑点噪声限制了其高精度应用。现有深度学习去斑方法通常使用单一网络处理整个图像，未能考虑不同空间物理特性关联的斑点统计差异，导致伪影、边缘模糊和纹理失真。

Method: 提出SAR-FAH模型，采用分而治之架构。首先，使用小波分解将图像分离成具有不同内在特征的频率子带。然后，根据其不同的噪声特性，设计专门的子网络：低频部分通过神经常微分方程（ODE）建模为连续动态系统进行去噪，确保结构保真度和平滑性；高频子带则引入带有可变形卷积的增强U-Net进行噪声抑制和特征增强。

Result: 在合成和真实SAR图像上的大量实验验证了所提模型在去噪和结构保留方面的卓越性能。

Conclusion: SAR-FAH通过频率自适应的异构方法，有效解决了SAR图像去斑问题，通过为不同频率分量设计专门的网络，显著提高了噪声抑制能力和结构保真度，优于传统的统一网络方法。

Abstract: Synthetic Aperture Radar (SAR) images are inherently corrupted by speckle noise, limiting their utility in high-precision applications. While deep learning methods have shown promise in SAR despeckling, most methods employ a single unified network to process the entire image, failing to account for the distinct speckle statistics associated with different spatial physical characteristics. It often leads to artifacts, blurred edges, and texture distortion. To address these issues, we propose SAR-FAH, a frequency-adaptive heterogeneous despeckling model based on a divide-and-conquer architecture. First, wavelet decomposition is used to separate the image into frequency sub-bands carrying different intrinsic characteristics. Inspired by their differing noise characteristics, we design specialized sub-networks for different frequency components. The tailored approach leverages statistical variations across frequencies, improving edge and texture preservation while suppressing noise. Specifically, for the low-frequency part, denoising is formulated as a continuous dynamic system via neural ordinary differential equations, ensuring structural fidelity and sufficient smoothness that prevents artifacts. For high-frequency sub-bands rich in edges and textures, we introduce an enhanced U-Net with deformable convolutions for noise suppression and enhanced features. Extensive experiments on synthetic and real SAR images validate the superior performance of the proposed model in noise removal and structural preservation.

</details>


### [111] [Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition](https://arxiv.org/abs/2511.05893)
*Hongxia Li,Ying Ji,Yongxin Dong,Yuehua Feng*

Main category: cs.CV

TL;DR: 本文提出了一种混合二阶梯度直方图全局低秩稀疏回归（H2H-GLRSR）模型，通过结合新的局部特征描述符H2H和对残差矩阵施加全局低秩约束，有效提升了在复杂遮挡和光照变化下人脸识别的性能。


<details>
  <summary>Details</summary>
Motivation: 低秩稀疏回归模型在人脸识别中应用广泛，但面对复杂遮挡和光照变化带来的挑战时仍有不足。本文旨在进一步解决这些问题。

Method: 1. 设计了一种新颖的混合二阶梯度直方图（H2H）特征描述符，以更有效地表征人脸图像的局部结构特征。2. 将H2H描述符与基于稀疏正则化核范数矩阵回归（SR_NMR）相结合。3. 对残差矩阵施加全局低秩约束，以更好地捕获结构化噪声中固有的全局相关性。

Result: 实验结果表明，所提出的方法在涉及遮挡、光照变化和非受限环境等挑战性场景下，显著优于现有的基于回归的分类方法。

Conclusion: H2H-GLRSR模型通过结合创新的局部特征描述符和全局低秩约束，在处理复杂遮挡和光照变化的人脸识别任务中展现出卓越的性能。

Abstract: Low-rank sparse regression models have been widely applied in the field of face recognition. To further address the challenges caused by complex occlusions and illumination variations, this paper proposes a Hybrid Second-Order Gradient Histogram based Global Low-Rank Sparse Regression (H2H-GLRSR) model. Specifically, a novel feature descriptor called the Hybrid Second-Order Gradient Histogram (H2H) is first designed to more effectively characterize the local structural features of facial images. Then, this descriptor is integrated with the Sparse Regularized Nuclear Norm based Matrix Regression (SR$\_$NMR). Moreover, a global low-rank constraint is imposed on the residual matrix, enabling the model to better capture the global correlations inherent in structured noise. Experimental results demonstrate that the proposed method significantly outperforms existing regression-based classification approaches under challenging scenarios involving occlusions, illumination changes, and unconstrained environments.

</details>


### [112] [Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.05894)
*Fei Yu,Quan Deng,Shengeng Tang,Yuehua Li,Lechao Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种结合检索增强推理的开放世界3D场景图生成统一框架，以实现可泛化和交互式的3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 开放世界3D场景理解面临闭集词汇监督和静态标注的限制，给计算机视觉和机器人技术带来了根本性挑战。

Method: 该方法整合了视觉-语言模型（VLMs）与基于检索的推理，支持多模态探索和语言引导交互。框架包括两个关键组件：1) 动态场景图生成模块，无需固定标签集即可检测物体并推断语义关系；2) 检索增强推理管道，将场景图编码到向量数据库中以支持文本/图像条件查询。

Result: 在3DSSG和Replica基准上，通过场景问答、视觉定位、实例检索和任务规划四项任务进行评估，展示了强大的泛化能力和在多样化环境中的卓越性能。

Conclusion: 结合开放词汇感知与基于检索的推理，对于可扩展的3D场景理解是有效的。

Abstract: Understanding 3D scenes in open-world settings poses fundamental challenges for vision and robotics, particularly due to the limitations of closed-vocabulary supervision and static annotations. To address this, we propose a unified framework for Open-World 3D Scene Graph Generation with Retrieval-Augmented Reasoning, which enables generalizable and interactive 3D scene understanding. Our method integrates Vision-Language Models (VLMs) with retrieval-based reasoning to support multimodal exploration and language-guided interaction. The framework comprises two key components: (1) a dynamic scene graph generation module that detects objects and infers semantic relationships without fixed label sets, and (2) a retrieval-augmented reasoning pipeline that encodes scene graphs into a vector database to support text/image-conditioned queries. We evaluate our method on 3DSSG and Replica benchmarks across four tasks-scene question answering, visual grounding, instance retrieval, and task planning-demonstrating robust generalization and superior performance in diverse environments. Our results highlight the effectiveness of combining open-vocabulary perception with retrieval-based reasoning for scalable 3D scene understanding.

</details>


### [113] [GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks](https://arxiv.org/abs/2511.05898)
*Zhaoyang Wang,Dong Wang*

Main category: cs.CV

TL;DR: 针对多任务架构中量化感知训练(QAT)的性能下降问题，本文提出了GABFusion和ADA方法，通过动态平衡梯度、融合特征和特征级蒸馏，显著提升了量化模型的性能，缩小了与全精度模型的精度差距。


<details>
  <summary>Details</summary>
Motivation: 量化感知训练(QAT)在多任务架构上的性能显著下降，原因在于任务特定的特征差异和梯度冲突。

Method: 本文提出了两种方法：1) 梯度感知平衡特征融合(GABFusion)，它动态平衡梯度幅度并以对量化友好的方式融合任务特定特征；2) 注意力分布对齐(ADA)，这是一种专为量化模型设计的特征级蒸馏策略。方法在梯度偏差减少方面提供了理论保证。

Result: 实验结果表明，该策略持续提升了各种QAT方法在不同网络架构和位宽上的性能。在PASCAL VOC和COCO数据集上，平均mAP分别提高了约3.3%和1.6%。在YOLOv5的4比特量化下，该方法将与全精度模型在VOC上的精度差距缩小到仅1.7%。

Conclusion: 所提出的框架是模块化的、易于集成且与任何现有QAT技术兼容，无需修改原始网络架构即可有效提升量化模型的性能，尤其适用于多任务场景下的低比特量化。

Abstract: Despite the effectiveness of quantization-aware training (QAT) in compressing deep neural networks, its performance on multi-task architectures often degrades significantly due to task-specific feature discrepancies and gradient conflicts. To address these challenges, we propose Gradient-Aware Balanced Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and fuses task-specific features in a quantization-friendly manner. We further introduce Attention Distribution Alignment (ADA), a feature-level distillation strategy tailored for quantized models. Our method demonstrates strong generalization across network architectures and QAT algorithms, with theoretical guarantees on gradient bias reduction. Extensive experiments demonstrate that our strategy consistently enhances a variety of QAT methods across different network architectures and bit-widths. On PASCAL VOC and COCO datasets, the proposed approach achieves average mAP improvements of approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit quantization, our method narrows the accuracy gap with the full-precision model to only 1.7% on VOC, showcasing its effectiveness in preserving performance under low-bit constraints. Notably, the proposed framework is modular, easy to integrate, and compatible with any existing QAT technique-enhancing the performance of quantized models without requiring modifications to the original network architecture.

</details>


### [114] [Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation](https://arxiv.org/abs/2511.05923)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Weitao Ma,Xiachong Feng*

Main category: cs.CV

TL;DR: 本文提出了细粒度跨模态因果追踪（FCCT）框架，用于深入分析大型视觉语言模型（LVLMs）的机制可解释性，揭示了多头自注意力（MHSA）和前馈网络（FFN）在视觉对象感知中的关键作用，并基于此开发了无需训练的推理时技术IRI，有效缓解了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLMs）取得了显著进展，但其机制可解释性仍未被充分探索。现有分析不够全面，缺乏对视觉和文本标记、模型组件及所有层的检查，这限制了提升模型输出忠实度和开发下游任务（如幻觉缓解）的实用见解。

Method: 本文引入了细粒度跨模态因果追踪（FCCT）框架，系统地量化了对视觉对象感知的因果效应。FCCT对视觉和文本标记、多头自注意力（MHSA）、前馈网络（FFN）和隐藏状态这三个核心模型组件以及所有解码器层进行了细粒度分析。在此基础上，提出了一种无需训练的推理时技术——中间表示注入（IRI），通过精确干预特定组件和层的跨模态表示来增强视觉对象信息流。

Result: 分析首次表明，中间层最后一个标记的MHSA在聚合跨模态信息中起着关键作用，而FFN则展现出存储和传输视觉对象表示的三阶段分层进展。IRI在五个广泛使用的基准和LVLMs上持续改进，实现了最先进的性能，同时保持了推理速度和其他基础性能。

Conclusion: FCCT框架提供了对LVLMs深层的机制理解，揭示了关键组件在跨模态信息处理中的作用。基于这些洞察，提出的IRI技术能够有效增强视觉感知并缓解幻觉，在无需训练的情况下达到了最先进的性能，为提升LVLMs的忠实度提供了实际可行的方案。

Abstract: Despite the remarkable advancements of Large Vision-Language Models (LVLMs), the mechanistic interpretability remains underexplored. Existing analyses are insufficiently comprehensive and lack examination covering visual and textual tokens, model components, and the full range of layers. This limitation restricts actionable insights to improve the faithfulness of model output and the development of downstream tasks, such as hallucination mitigation. To address this limitation, we introduce Fine-grained Cross-modal Causal Tracing (FCCT) framework, which systematically quantifies the causal effects on visual object perception. FCCT conducts fine-grained analysis covering the full range of visual and textual tokens, three core model components including multi-head self-attention (MHSA), feed-forward networks (FFNs), and hidden states, across all decoder layers. Our analysis is the first to demonstrate that MHSAs of the last token in middle layers play a critical role in aggregating cross-modal information, while FFNs exhibit a three-stage hierarchical progression for the storage and transfer of visual object representations. Building on these insights, we propose Intermediate Representation Injection (IRI), a training-free inference-time technique that reinforces visual object information flow by precisely intervening on cross-modal representations at specific components and layers, thereby enhancing perception and mitigating hallucination. Consistent improvements across five widely used benchmarks and LVLMs demonstrate IRI achieves state-of-the-art performance, while preserving inference speed and other foundational performance.

</details>


### [115] [CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework](https://arxiv.org/abs/2511.05929)
*Jiaxuan Li,Qing Xu,Xiangjian He,Ziyu Liu,Chang Xing,Zhen Chen,Daokun Zhang,Rong Qu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文提出了互补掩码自编码器（CoMA）和动态多窗口自注意力视觉Transformer（DyViT），显著提升了MAE预训练的效率和模型的适应性，同时减少了参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的MAE及其变体普遍存在预训练周期长以保持适应性、以及ViT因固定空间分辨率导致参数利用效率低下的问题。

Method: 本文提出两种方法：1) 互补掩码策略（CoMA），通过确保所有像素的均匀采样来提高特征学习的有效性和模型适应性；2) 动态多窗口自注意力（DM-MSA）的分层视觉Transformer（DyViT），以减少参数和FLOPs，同时改进细粒度特征学习。

Result: 在ImageNet-1K上，使用CoMA预训练的DyViT仅用MAE 12%的预训练周期就达到了MAE的下游任务性能，并且每周期预训练时间减少了10%，显示出更有效的学习和卓越的预训练效率。

Conclusion: CoMA和DyViT克服了传统MAE的局限性，通过互补掩码和动态分层Transformer设计，显著提高了自监督学习的效率和模型的适应性。

Abstract: Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.

</details>


### [116] [Interaction-Centric Knowledge Infusion and Transfer for Open-Vocabulary Scene Graph Generation](https://arxiv.org/abs/2511.05935)
*Lin Li,Chuhan Zhang,Dong Zhang,Chong Sun,Chen Li,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种以交互为中心的端到端开放词汇场景图生成（OVSGG）框架ACC，通过解决现有方法中难以区分交互与非交互实例的问题，改进了知识注入和知识迁移过程，从而提升了OVSGG的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的OVSGG方法在区分同一对象类别中交互和非交互实例时存在困难，这导致了知识注入阶段生成带有噪声的伪监督，并在知识迁移阶段造成模糊的查询匹配。

Method: ACC框架采用交互驱动范式来最小化不匹配问题。在“以交互为中心的知识注入”阶段，ACC使用双向交互提示来生成鲁棒的伪监督。在“以交互为中心的知识迁移”阶段，ACC首先采用交互引导的查询选择来优先配对交互对象，然后集成交互一致的知识蒸馏，通过将关系前景推离背景同时保留一般知识来增强鲁棒性。

Result: ACC在三个基准测试上取得了最先进的性能，证明了以交互为中心的范式在实际应用中的潜力。

Conclusion: 以交互为中心的范式能够有效解决开放词汇场景图生成中区分交互与非交互实例的挑战，显著提升模型性能，对实际应用具有重要意义。

Abstract: Open-vocabulary scene graph generation (OVSGG) extends traditional SGG by recognizing novel objects and relationships beyond predefined categories, leveraging the knowledge from pre-trained large-scale models. Existing OVSGG methods always adopt a two-stage pipeline: 1) \textit{Infusing knowledge} into large-scale models via pre-training on large datasets; 2) \textit{Transferring knowledge} from pre-trained models with fully annotated scene graphs during supervised fine-tuning. However, due to a lack of explicit interaction modeling, these methods struggle to distinguish between interacting and non-interacting instances of the same object category. This limitation induces critical issues in both stages of OVSGG: it generates noisy pseudo-supervision from mismatched objects during knowledge infusion, and causes ambiguous query matching during knowledge transfer. To this end, in this paper, we propose an inter\textbf{AC}tion-\textbf{C}entric end-to-end OVSGG framework (\textbf{ACC}) in an interaction-driven paradigm to minimize these mismatches. For \textit{interaction-centric knowledge infusion}, ACC employs a bidirectional interaction prompt for robust pseudo-supervision generation to enhance the model's interaction knowledge. For \textit{interaction-centric knowledge transfer}, ACC first adopts interaction-guided query selection that prioritizes pairing interacting objects to reduce interference from non-interacting ones. Then, it integrates interaction-consistent knowledge distillation to bolster robustness by pushing relational foreground away from the background while retaining general knowledge. Extensive experimental results on three benchmarks show that ACC achieves state-of-the-art performance, demonstrating the potential of interaction-centric paradigms for real-world applications.

</details>


### [117] [Global Multiple Extraction Network for Low-Resolution Facial Expression Recognition](https://arxiv.org/abs/2511.05938)
*Jingyi Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为GME-Net的新型网络，用于解决低分辨率人脸表情识别中细节缺失和全局建模不足的问题，通过混合注意力局部特征提取和多尺度全局特征提取，实现了对低分辨率表情的有效识别。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸表情识别算法在处理低分辨率图像时性能会显著下降，主要原因在于：1) 低分辨率图像缺乏细节信息；2) 当前方法全局建模能力较弱，难以提取判别性特征。

Method: 本文提出了一种全局多重提取网络（GME-Net），包含两个主要模块：1) 基于混合注意力的局部特征提取模块，结合注意力相似性知识蒸馏从高分辨率网络中学习图像细节；2) 具有准对称结构的多尺度全局特征提取模块，旨在减轻局部图像噪声的影响并促进捕捉全局图像特征。

Result: GME-Net能够提取与表情相关的判别性特征。在多个常用数据集上进行的广泛实验表明，所提出的GME-Net能够更好地识别低分辨率人脸表情，并取得了优于现有解决方案的性能。

Conclusion: GME-Net通过有效解决低分辨率图像的细节缺失和全局建模不足问题，显著提升了低分辨率人脸表情识别的准确性，并展现出优越的性能。

Abstract: Facial expression recognition, as a vital computer vision task, is garnering significant attention and undergoing extensive research. Although facial expression recognition algorithms demonstrate impressive performance on high-resolution images, their effectiveness tends to degrade when confronted with low-resolution images. We find it is because: 1) low-resolution images lack detail information; 2) current methods complete weak global modeling, which make it difficult to extract discriminative features. To alleviate the above issues, we proposed a novel global multiple extraction network (GME-Net) for low-resolution facial expression recognition, which incorporates 1) a hybrid attention-based local feature extraction module with attention similarity knowledge distillation to learn image details from high-resolution network; 2) a multi-scale global feature extraction module with quasi-symmetric structure to mitigate the influence of local image noise and facilitate capturing global image features. As a result, our GME-Net is capable of extracting expression-related discriminative features. Extensive experiments conducted on several widely-used datasets demonstrate that the proposed GME-Net can better recognize low-resolution facial expression and obtain superior performance than existing solutions.

</details>


### [118] [AD-DAE: Unsupervised Modeling of Longitudinal Alzheimer's Disease Progression with Diffusion Auto-Encoder](https://arxiv.org/abs/2511.05934)
*Ayantika Das,Arunima Sarkar,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 本文提出了一种可条件化的扩散自编码器框架，用于在无监督情况下，通过控制潜在空间中的偏移，从基线图像生成疾病进展图像，特别是在阿尔茨海默病建模中表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成建模方法在疾病进展建模中，将图像映射到潜在空间后，其潜在空间的可控性有限，无法在缺乏特定受试者纵向图像的明确监督下生成后续图像。

Method: 引入了一个可条件化的扩散自编码器框架。该框架通过其显式编码机制形成紧凑的潜在空间，捕获高级语义并解耦进展相关信息。通过限制潜在空间中的偏移到一个子空间，将进展相关因素与受试者身份保留组件分离，从而实现对基线表示的受控移动。这些偏移通过与进展属性相关联进行隐式引导，无需受试者特定的纵向监督。

Result: 通过图像质量指标、体积进展分析以及在来自两个不同来源和疾病类别的阿尔茨海默病数据集上的下游分类，验证了生成图像的有效性。结果表明该方法对于阿尔茨海默病进展建模和纵向图像生成是有效的。

Conclusion: 该方法为阿尔茨海默病进展建模和纵向图像生成提供了一种有效的解决方案，尤其是在无监督条件下通过控制潜在空间实现进展图像生成。

Abstract: Generative modeling frameworks have emerged as an effective approach to capture high-dimensional image distributions from large datasets without requiring domain-specific knowledge, a capability essential for longitudinal disease progression modeling. Recent generative modeling approaches have attempted to capture progression by mapping images into a latent representational space and then controlling and guiding the representations to generate follow-up images from a baseline image. However, existing approaches impose constraints on distribution learning, leading to latent spaces with limited controllability to generate follow-up images without explicit supervision from subject-specific longitudinal images. In order to enable controlled movements in the latent representational space and generate progression images from a baseline image in an unsupervised manner, we introduce a conditionable Diffusion Auto-encoder framework. The explicit encoding mechanism of image-diffusion auto-encoders forms a compact latent space capturing high-level semantics, providing means to disentangle information relevant for progression. Our approach leverages this latent space to condition and apply controlled shifts to baseline representations for generating follow-up. Controllability is induced by restricting these shifts to a subspace, thereby isolating progression-related factors from subject identity-preserving components. The shifts are implicitly guided by correlating with progression attributes, without requiring subject-specific longitudinal supervision. We validate the generations through image quality metrics, volumetric progression analysis, and downstream classification in Alzheimer's disease datasets from two different sources and disease categories. This demonstrates the effectiveness of our approach for Alzheimer's progression modeling and longitudinal image generation.

</details>


### [119] [Polymap: generating high definition map based on rasterized polygons](https://arxiv.org/abs/2511.05944)
*Shiyu Gao,Hao Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于实例分割的矢量化高精地图感知框架，通过将道路元素重新解释为栅格化多边形，并结合Transformer和Potrace后处理，解决了现有检测方法泛化性差的问题，并在Nuscene数据集上验证了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的高精地图感知至关重要。现有研究多关注在线构建，其中基于检测的方法（如Maptr系列）能实时输出矢量化地图实例。然而，这些方法被观察到泛化能力不足，限制了它们在自动标注系统中的应用。因此，本文旨在提高高精地图感知的泛化能力。

Method: 本文将道路元素重新解释为栅格化多边形，并设计了一个简洁的基于实例分割的框架。首先，使用基于分割的Transformer模型端到端地生成实例掩码；随后，采用基于Potrace的后处理模块，将这些掩码最终转换为矢量化的地图元素。

Result: 在Nuscene数据集上进行的定量实验结果证实了本文方法的有效性和泛化能力。

Conclusion: 通过将道路元素视为栅格化多边形并采用基于实例分割的Transformer结合Potrace后处理，本文方法成功提高了高精地图感知的泛化能力，并在一项流行的数据集上得到了验证。

Abstract: The perception of high-definition maps is an integral component of environmental perception in autonomous driving systems. Existing research have often focused on online construction of high-definition maps. For instance, the Maptr[9] series employ a detection-based method to output vectorized map instances parallelly in an end-to-end manner. However, despite their capability for real-time construction, detection-based methods are observed to lack robust generalizability[19], which hampers their applicability in auto-labeling systems. Therefore, aiming to improve the generalizability, we reinterpret road elements as rasterized polygons and design a concise framework based on instance segmentation. Initially, a segmentation-based transformer is employed to deliver instance masks in an end-to-end manner; succeeding this step, a Potrace-based[17] post-processing module is used to ultimately yield vectorized map elements. Quantitative results attained on the Nuscene[1] dataset substantiate the effectiveness and generaliz-ability of our method.

</details>


### [120] [Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey](https://arxiv.org/abs/2511.05982)
*Albert Schotschneider,Svetlana Pavlitska,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 该综述全面概述了深度神经网络（DNNs）在安全关键应用中，用于检测泛化误差、OOD输入和对抗性攻击等安全问题的运行时安全监控方法，并对其进行分类、分析并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）在自动驾驶和机器人等安全关键感知系统中广泛应用，但仍易受泛化误差、分布外（OOD）输入和对抗性攻击等安全问题的影响，这些问题可能导致危险的故障。

Method: 本文通过一项全面的综述，将现有的运行时安全监控方法分为三大类：监控输入、监控内部表示和监控输出，这些方法在推理过程中与DNN并行运行，无需修改DNN本身即可检测安全问题。

Result: 该综述分析了每一类别的最新技术，识别了其优势和局限性，并将各种方法与其解决的安全问题进行了映射。

Conclusion: 文章强调了运行时安全监控领域的开放挑战和未来的研究方向。

Abstract: Deep neural networks (DNNs) are widely used in perception systems for safety-critical applications, such as autonomous driving and robotics. However, DNNs remain vulnerable to various safety concerns, including generalization errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can lead to hazardous failures. This survey provides a comprehensive overview of runtime safety monitoring approaches, which operate in parallel to DNNs during inference to detect these safety concerns without modifying the DNN itself. We categorize existing methods into three main groups: Monitoring inputs, internal representations, and outputs. We analyze the state-of-the-art for each category, identify strengths and limitations, and map methods to the safety concerns they address. In addition, we highlight open challenges and future research directions.

</details>


### [121] [On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration](https://arxiv.org/abs/2511.06611)
*Jiajun Jiang,Xiao Hu,Wancheng Liu,Wei Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种几何学原理框架，用于解决LiDAR-相机外参校准中圆形目标3D-2D中心对应不准确的问题，通过改进的3D圆心估计和2D投影中心恢复方法，显著提高了校准精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 圆形目标在LiDAR-相机外参校准中广泛应用，但准确的3D-2D圆心对应仍然具有挑战性。现有方法常因解耦的3D拟合和错误的2D椭圆中心估计而失败。

Method: 该框架包含两项创新：(i) 基于共形几何代数和RANSAC的鲁棒3D圆心估计器；(ii) 通过弦长方差最小化方法恢复真实的2D投影中心，并利用单应性验证或准RANSAC回退解决其双极小值模糊性。

Result: 在合成和真实世界数据集上的评估表明，该框架显著优于现有技术，减少了外参估计误差，并实现了跨不同传感器和目标类型（包括自然圆形物体）的鲁棒校准。

Conclusion: 所提出的几何学原理框架有效解决了LiDAR-相机外参校准中圆形目标3D-2D中心对应不准确的难题，提供了一种更精确、更鲁棒的校准解决方案。

Abstract: Circular targets are widely used in LiDAR-camera extrinsic calibration due to their geometric consistency and ease of detection. However, achieving accurate 3D-2D circular center correspondence remains challenging. Existing methods often fail due to decoupled 3D fitting and erroneous 2D ellipse-center estimation. To address this, we propose a geometrically principled framework featuring two innovations: (i) a robust 3D circle center estimator based on conformal geometric algebra and RANSAC; and (ii) a chord-length variance minimization method to recover the true 2D projected center, resolving its dual-minima ambi- guity via homography validation or a quasi-RANSAC fallback. Evaluated on synthetic and real-world datasets, our framework significantly outperforms state-of-the-art approaches. It reduces extrinsic estimation error and enables robust calibration across diverse sensors and target types, including natural circular objects. Our code will be publicly released for reproducibility.

</details>


### [122] [U(PM)$^2$:Unsupervised polygon matching with pre-trained models for challenging stereo images](https://arxiv.org/abs/2511.05949)
*Chang Li,Xingtao Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为U(PM)$^2$的无监督多边形匹配方法，通过结合预训练模型（如SAM和LoFTR）以及自动学习与手工特征，解决了立体图像匹配中多边形匹配所面临的视差不连续、尺度变化、训练需求和泛化能力等挑战，并在ScanNet和SceneFlow数据集上取得了最先进的精度、竞争性速度和良好的泛化性能，且无需训练。


<details>
  <summary>Details</summary>
Motivation: 立体图像匹配是计算机视觉等领域的基础任务，但多边形匹配领域几乎未被探索。该领域面临视差不连续、尺度变化、训练需求和泛化能力等挑战，需要一种有效的方法来解决这些问题。

Method: 本文提出了U(PM)$^2$（低成本无监督多边形匹配与预训练模型），其核心是结合自动学习和手工特征。具体流程如下：1) 检测器利用预训练的Segment Anything Model获取掩膜；2) 矢量化器将掩膜转换为多边形和图形结构；3) 全局匹配器基于双向金字塔策略和预训练的LoFTR解决全局视角变化和尺度变化；4) 局部匹配器通过局部联合几何和多特征匹配策略结合匈牙利算法，克服局部视差不连续和拓扑不一致问题。

Result: U(PM)$^2$在ScanNet和SceneFlow数据集上，使用本文提出的新指标进行基准测试，实现了最先进的精度、具有竞争力的速度、令人满意的泛化性能，且成本低廉，无需任何训练要求。

Conclusion: U(PM)$^2$成功解决了多边形匹配的挑战，通过创新的无监督方法和预训练模型结合，在无需训练的情况下实现了卓越的性能、效率和泛化能力。

Abstract: Stereo image matching is a fundamental task in computer vision, photogrammetry and remote sensing, but there is an almost unexplored field, i.e., polygon matching, which faces the following challenges: disparity discontinuity, scale variation, training requirement, and generalization. To address the above-mentioned issues, this paper proposes a novel U(PM)$^2$: low-cost unsupervised polygon matching with pre-trained models by uniting automatically learned and handcrafted features, of which pipeline is as follows: firstly, the detector leverages the pre-trained segment anything model to obtain masks; then, the vectorizer converts the masks to polygons and graphic structure; secondly, the global matcher addresses challenges from global viewpoint changes and scale variation based on bidirectional-pyramid strategy with pre-trained LoFTR; finally, the local matcher further overcomes local disparity discontinuity and topology inconsistency of polygon matching by local-joint geometry and multi-feature matching strategy with Hungarian algorithm. We benchmark our U(PM)$^2$ on the ScanNet and SceneFlow datasets using our proposed new metric, which achieved state-of-the-art accuracy at a competitive speed and satisfactory generalization performance at low cost without any training requirement.

</details>


### [123] [PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/abs/2511.06840)
*Qunchao Jin,Yilin Wu,Changhao Chen*

Main category: cs.CV

TL;DR: PanoNav是一种纯RGB、无地图的零样本物体导航框架，它利用全景场景解析增强多模态大语言模型的空间理解能力，并结合动态有界记忆队列进行记忆引导决策，以避免局部死锁，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中进行零样本物体导航（ZSON）对家用机器人来说是一个挑战。现有方法（如基于度量地图和LLM的方法）依赖深度传感器或预构建地图，限制了多模态大语言模型（MLLM）的空间推理能力。无地图ZSON方法虽然出现，但因缺乏历史上下文，常做出短视决策，导致局部死锁。

Method: 本文提出PanoNav，一个纯RGB、无地图的ZSON框架。它包含：1) 一个全景场景解析模块，通过全景RGB输入解锁MLLM的空间解析潜力；2) 一个记忆引导决策机制，通过动态有界记忆队列增强，以整合探索历史并避免局部死锁。

Result: 在公共导航基准上的实验表明，PanoNav在成功率（SR）和路径长度效率（SPL）指标上均显著优于代表性基线方法。

Conclusion: PanoNav通过结合全景感知和记忆引导的导航策略，有效解决了零样本物体导航的挑战，并在性能上取得了显著提升。

Abstract: Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics.

</details>


### [124] [Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration](https://arxiv.org/abs/2511.05965)
*Zhixin Cheng,Xiaotian Yin,Jiacheng Deng,Bohao Liao,Yujia Chen,Xu Zhou,Baoqun Yin,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的跨模态注册框架，通过迭代代理选择（IAS）和可靠代理交互（RAI）模块，增强结构特征感知并有效选择和利用可靠代理，显著提高了图像到点云注册在挑战条件下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的图像到点云注册方法在噪声环境下难以准确计算相似性并建立对应关系，且缺乏有效设计来选择信息丰富且相关的跨模态表示，从而限制了注册的鲁棒性和准确性。

Method: 本文提出了一个由两个关键模块组成的跨模态注册框架：迭代代理选择（IAS）模块和可靠代理交互（RAI）模块。IAS通过相位图增强结构特征感知，并利用强化学习原则高效选择可靠代理。RAI则利用这些选定的代理引导跨模态交互，以减少不匹配并提高整体鲁棒性。

Result: 在RGB-D Scenes v2和7-Scenes基准测试上进行的广泛实验表明，该方法始终达到最先进的性能。

Conclusion: 所提出的方法通过其独特的IAS和RAI模块，有效解决了图像到点云注册在噪声和信息选择方面的挑战，显著提升了注册的鲁棒性和准确性，并实现了最先进的性能。

Abstract: Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.

</details>


### [125] [Reperio-rPPG: Relational Temporal Graph Neural Networks for Periodicity Learning in Remote Physiological Measurement](https://arxiv.org/abs/2511.05946)
*Ba-Thinh Nguyen,Thach-Ha Ngoc Pham,Hoang-Long Duc Nguyen,Thi-Duyen Ngo,Thanh-Ha Le*

Main category: cs.CV

TL;DR: 该研究提出Reperio-rPPG框架，通过结合关系卷积网络和图Transformer有效捕捉生理信号的内在周期性，并引入CutMix数据增强技术，在多个基准数据集上实现最先进的性能和卓越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的远程光电容积描记法（rPPG）在建模生理信号固有的周期性方面不足，限制了它们在真实世界条件下捕捉精细时间动态的能力，且现有rPPG数据集的多样性有限。

Method: 提出Reperio-rPPG框架，该框架战略性地整合了关系卷积网络（Relational Convolutional Networks）与图Transformer（Graph Transformer）来有效捕捉生理信号固有的周期性结构。此外，为增强模型泛化能力，引入了定制的CutMix数据增强方法。

Result: Reperio-rPPG在PURE、UBFC-rPPG和MMPD三个广泛使用的基准数据集上不仅实现了最先进的性能，而且在各种运动（如静止、旋转、说话、行走）和光照条件（如自然光、低LED、高LED）下表现出卓越的鲁棒性。

Conclusion: Reperio-rPPG通过有效建模生理信号的周期性并增强模型的泛化能力，显著提升了远程生理信号测量的准确性和鲁棒性，为该领域树立了新的基准。

Abstract: Remote photoplethysmography (rPPG) is an emerging contactless physiological sensing technique that leverages subtle color variations in facial videos to estimate vital signs such as heart rate and respiratory rate. This non-invasive method has gained traction across diverse domains, including telemedicine, affective computing, driver fatigue detection, and health monitoring, owing to its scalability and convenience. Despite significant progress in remote physiological signal measurement, a crucial characteristic - the intrinsic periodicity - has often been underexplored or insufficiently modeled in previous approaches, limiting their ability to capture fine-grained temporal dynamics under real-world conditions. To bridge this gap, we propose Reperio-rPPG, a novel framework that strategically integrates Relational Convolutional Networks with a Graph Transformer to effectively capture the periodic structure inherent in physiological signals. Additionally, recognizing the limited diversity of existing rPPG datasets, we further introduce a tailored CutMix augmentation to enhance the model's generalizability. Extensive experiments conducted on three widely used benchmark datasets - PURE, UBFC-rPPG, and MMPD - demonstrate that Reperio-rPPG not only achieves state-of-the-art performance but also exhibits remarkable robustness under various motion (e.g., stationary, rotation, talking, walking) and illumination conditions (e.g., nature, low LED, high LED). The code is publicly available at https://github.com/deconasser/Reperio-rPPG.

</details>


### [126] [CSGaze: Context-aware Social Gaze Prediction](https://arxiv.org/abs/2511.05955)
*Surbhi Madan,Shreya Ghosh,Ramanathan Subramanian,Abhinav Dhall,Tom Gedeon*

Main category: cs.CV

TL;DR: 本文提出CSGaze，一种上下文感知的多模态方法，通过结合面部、场景信息和精细注意力机制，有效预测对话中的社交凝视模式，并展现出良好的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 个体的凝视行为能揭示其注意力焦点、社交参与度和自信心。本研究旨在探索如何有效利用上下文线索、视觉场景和面部信息来预测和解释对话中的社交凝视模式。

Method: 引入了CSGaze模型，这是一种上下文感知的多模态方法。它利用面部和场景信息作为补充输入，以增强多人物图像中的社交凝视模式预测。该模型还结合了以主要说话者为中心的精细注意力机制，以更好地建模社交凝视动态。

Result: 实验结果表明，CSGaze在GP-Static、UCO-LAEO和AVA-LAEO数据集上与现有最先进方法相比具有竞争力。研究发现强调了上下文线索在改善社交凝视预测中的作用。此外，模型通过生成的注意力分数提供了初步的可解释性，并在一系列开放集数据集上展示了其泛化能力和鲁棒性。

Conclusion: CSGaze通过整合上下文线索和多模态信息（面部、场景），显著提升了社交凝视模式的预测能力。该模型不仅性能优越，具有良好的泛化性，还能通过注意力机制提供决策过程的初步解释，凸显了上下文线索在社交凝视预测中的关键作用。

Abstract: A person's gaze offers valuable insights into their focus of attention, level of social engagement, and confidence. In this work, we investigate how contextual cues combined with visual scene and facial information can be effectively utilized to predict and interpret social gaze patterns during conversational interactions. We introduce CSGaze, a context aware multimodal approach that leverages facial, scene information as complementary inputs to enhance social gaze pattern prediction from multi-person images. The model also incorporates a fine-grained attention mechanism centered on the principal speaker, which helps in better modeling social gaze dynamics. Experimental results show that CSGaze performs competitively with state-of-the-art methods on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of contextual cues in improving social gaze prediction. Additionally, we provide initial explainability through generated attention scores, offering insights into the model's decision-making process. We also demonstrate our model's generalizability by testing our model on open set datasets that demonstrating its robustness across diverse scenarios.

</details>


### [127] [Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion](https://arxiv.org/abs/2511.07377)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: FLASH是一种新颖的激光雷达超分辨率框架，通过结合局部空间注意力与全局频域分析（FFT）的频率感知窗口注意力，以及自适应多尺度融合，实现了双域处理。它在KITTI数据集上取得了最先进的性能，并能高效部署。


<details>
  <summary>Details</summary>
Motivation: 现有激光雷达超分辨率方法，特别是基于Transformer的方法（如TULIP），受限于仅在空间域处理且感受野有限，难以从低成本、低分辨率传感器中获得高质量的3D感知。本研究旨在克服这些局限性。

Method: FLASH框架采用双域处理，包含两项关键创新：(i) 频率感知窗口注意力：结合局部空间注意力与通过FFT进行的全局频域分析，以对数线性复杂度捕捉精细几何和周期性扫描模式。(ii) 自适应多尺度融合：用学习到的位置特定特征聚合取代传统跳跃连接，并通过CBAM注意力增强动态特征选择。

Result: FLASH在KITTI数据集上所有评估指标上均达到最先进性能，超越了需要多次前向传播的不确定性增强基线。值得注意的是，FLASH在保持单次通过效率的同时，优于带有蒙特卡洛Dropout的TULIP，从而实现实时部署。在所有距离范围内的持续优越性验证了其双域方法通过架构设计而非计算昂贵的随机推理有效处理不确定性。

Conclusion: FLASH通过其独特的双域处理架构设计，有效解决了激光雷达超分辨率中的不确定性问题，无需高计算成本的随机推理。这使得FLASH具有单次通过效率和实时部署能力，对于自动驾驶系统具有实际应用价值。

Abstract: LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems.

</details>


### [128] [Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory](https://arxiv.org/abs/2511.05966)
*Yuxuan Lin,Hanjing Yan,Xuan Tong,Yang Chang,Huanzhen Wang,Ziheng Zhou,Shuyong Gao,Yan Wang,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于结构共性的新颖少样本无监督多模态工业异常检测方法CIF，通过超图捕获类内结构先验，并在测试阶段利用结构信息更新特征和辅助记忆搜索，显著提升了少样本场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 少样本设置下，训练样本不足以覆盖测试样本的多样模式，导致模型泛化能力差。通过从少量训练样本中提取结构共性，可以有效缓解这一挑战。

Method: 本文提出了CIF方法。首先，利用超图（能够建模高阶相关性）从训练样本中提取类内结构共性，并将其存储在记忆库中。具体包括：1) 设计了一个语义感知超图构建模块，用于单语义工业图像，提取公共结构以指导记忆库的构建。2) 使用一个免训练的超图消息传递模块来更新测试样本的视觉特征，以缩小测试特征与记忆库特征之间的分布差距。3) 提出了一个超边引导的记忆搜索模块，利用结构信息辅助记忆搜索过程，降低误报率。

Result: 在MVTec 3D-AD和Eyecandies数据集上的实验结果表明，该方法在少样本设置下优于现有最先进（SOTA）方法。

Conclusion: 所提出的CIF方法通过利用超图捕获结构共性，有效解决了少样本多模态工业异常检测的挑战，并在实际数据集中展现出卓越的性能。

Abstract: Few-shot multimodal industrial anomaly detection is a critical yet underexplored task, offering the ability to quickly adapt to complex industrial scenarios. In few-shot settings, insufficient training samples often fail to cover the diverse patterns present in test samples. This challenge can be mitigated by extracting structural commonality from a small number of training samples. In this paper, we propose a novel few-shot unsupervised multimodal industrial anomaly detection method based on structural commonality, CIF (Commonality In Few). To extract intra-class structural information, we employ hypergraphs, which are capable of modeling higher-order correlations, to capture the structural commonality within training samples, and use a memory bank to store this intra-class structural prior. Firstly, we design a semantic-aware hypergraph construction module tailored for single-semantic industrial images, from which we extract common structures to guide the construction of the memory bank. Secondly, we use a training-free hypergraph message passing module to update the visual features of test samples, reducing the distribution gap between test features and features in the memory bank. We further propose a hyperedge-guided memory search module, which utilizes structural information to assist the memory search process and reduce the false positive rate. Experimental results on the MVTec 3D-AD dataset and the Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA) methods in few-shot settings. Code is available at https://github.com/Sunny5250/CIF.

</details>


### [129] [DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities](https://arxiv.org/abs/2511.05968)
*Nagur Shareef Shaik,Teja Krishna Cherukuri,Adnan Masood,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本文提出DiA-gnostic VLVAE框架，通过解耦对齐（Disentangled Alignment）实现鲁棒的放射学报告生成，有效解决现有方法中模态缺失和特征纠缠问题，并超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化放射学报告生成方法依赖于资源密集型大型语言模型（LLMs）或静态知识图谱，并且在实际临床数据中面临两大挑战：(1) 模态缺失，如临床上下文不完整；(2) 特征纠缠，即混合的模态特定和共享信息导致次优融合及临床上不忠实的幻觉发现。

Method: 本文提出DiA-gnostic VLVAE模型，通过解耦对齐实现鲁棒的放射学报告生成。该框架通过基于专家混合（MoE）的视觉-语言变分自编码器（VLVAE）解耦共享和模态特定特征，以应对模态缺失。通过约束优化目标，强制这些潜在表示之间正交并对齐，以防止次优融合。最后，一个紧凑的LLaMA-X解码器利用这些解耦的表示高效生成报告。

Result: 在IU X-Ray和MIMIC-CXR数据集上，DiA分别取得了0.266和0.134的BLEU@4分数，具有竞争力。实验结果表明，所提出的方法显著优于现有最先进的模型。

Conclusion: DiA-gnostic VLVAE通过解耦对齐，有效解决了放射学报告生成中模态缺失和特征纠缠的挑战，实现了鲁棒且准确的报告生成，并在性能上超越了现有先进方法。

Abstract: The integration of medical images with clinical context is essential for generating accurate and clinically interpretable radiology reports. However, current automated methods often rely on resource-heavy Large Language Models (LLMs) or static knowledge graphs and struggle with two fundamental challenges in real-world clinical data: (1) missing modalities, such as incomplete clinical context , and (2) feature entanglement, where mixed modality-specific and shared information leads to suboptimal fusion and clinically unfaithful hallucinated findings. To address these challenges, we propose the DiA-gnostic VLVAE, which achieves robust radiology reporting through Disentangled Alignment. Our framework is designed to be resilient to missing modalities by disentangling shared and modality-specific features using a Mixture-of-Experts (MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained optimization objective enforces orthogonality and alignment between these latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder then uses these disentangled representations to generate reports efficiently. On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4 scores of 0.266 and 0.134, respectively. Experimental results show that the proposed method significantly outperforms state-of-the-art models.

</details>


### [130] [TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research](https://arxiv.org/abs/2511.07412)
*Han Zhang,Yiqing Shen,Roger D. Soberanis-Mukul,Ankita Ghosh,Hao Ding,Lalithkumar Seenivasan,Jose L. Porras,Zhekai Mao,Chenjia Li,Wenjie Xiao,Lonny Yarmus,Angela Christine Argento,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: TwinOR是一个用于构建手术室逼真、动态数字孪生的框架，旨在为具身AI提供安全、可控的训练和评估环境，并实现从虚拟到现实的AI部署加速。


<details>
  <summary>Details</summary>
Motivation: 在真实手术室中，安全法规和操作限制阻碍了具身AI的自由感知和交互，限制了其学习和评估。数字孪生提供了高保真、无风险的环境，但如何创建捕捉空间、视觉和行为复杂性的手术室逼真动态数字表示尚不清楚。

Method: TwinOR框架通过预扫描视频重建静态几何，并通过多视角感知手术室活动连续建模人员和设备运动。静态和动态组件被融合到一个沉浸式3D环境中，支持可控模拟和具身探索。

Result: TwinOR以厘米级精度重建完整手术室几何，同时保留了手术流程中的动态交互，实现了逼真渲染。在TwinOR合成数据上，FoundationStereo和ORB-SLAM3等模型在几何理解和视觉定位任务上达到了与真实室内数据集报告精度相当的性能，证明了TwinOR提供了足以应对感知和定位挑战的传感器级真实感。

Conclusion: TwinOR通过建立从现实到模拟的动态、逼真手术室数字孪生构建管道，为具身AI提供了安全、可扩展和数据高效的开发与基准测试环境，从而加速了具身AI从模拟到现实的部署。

Abstract: Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.

</details>


### [131] [Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols](https://arxiv.org/abs/2511.05967)
*Tri-Thien Nguyen,Lorenz A. Kapsner,Tobias Hepp,Shirin Heidarikahkesh,Hannes Schreiter,Luise Brock,Dominika Skwierawska,Dominique Hadler,Julian Hossbach,Evelyn Wenkel,Sabine Ohlmeyer,Frederik B. Laun,Andrzej Liebert,Andreas Maier,Michael Uder,Sebastian Bickelhaupt*

Main category: cs.CV

TL;DR: 本研究评估了基于DINOv2的医学切片Transformer (MST) 在对比增强和非对比增强乳腺MRI中排除BI-RADS >=4显著发现的性能，在97.5%的灵敏度下，对有无显著发现的病例进行了正确分类，特异性分别为19%和17%。


<details>
  <summary>Details</summary>
Motivation: 乳腺MRI对乳腺癌检测敏感度高，但判读耗时。人工智能有望辅助预筛查，以提高效率。

Method: 研究采用回顾性设计，纳入了1847例内部数据集和924例外部验证数据集（Duke）的单侧乳腺MRI检查（其中377例为BI-RADS >=4）。测试了四种缩略协议：T1加权早期减影（T1sub）、b=1500 s/mm2的弥散加权成像（DWI1500）、DWI1500+T2加权（T2w）以及T1sub+T2w。性能评估指标包括90%、95%和97.5%灵敏度下的特异性、曲线下面积（AUC），并使用DeLong检验比较AUC差异。同时分析了假阴性特征，并对外部数据集中的真阳性注意力图进行了评分。

Result: 共纳入1448名女性患者。T1sub+T2w协议的AUC为0.77 +/- 0.04；DWI1500+T2w为0.74 +/- 0.04 (p=0.15)。在97.5%灵敏度下，T1sub+T2w的特异性最高（19% +/- 7%），其次是DWI1500+T2w（17% +/- 11%）。漏诊病灶的平均直径小于10毫米，主要为非肿块强化。外部验证的AUC为0.77，88%的注意力图被评为良好或中等。

Conclusion: 在97.5%的灵敏度下，MST框架能正确筛分出没有BI-RADS >=4的病例，对对比增强MRI的特异性为19%，对非对比增强MRI的特异性为17%。在临床实施前，仍需进一步研究。

Abstract: Background: Magnetic resonance imaging (MRI) has high sensitivity for breast cancer detection, but interpretation is time-consuming. Artificial intelligence may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice Transformer (MST) for ruling out significant findings (Breast Imaging Reporting and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced abbreviated breast MRI. Materials and Methods: This institutional review board approved retrospective study included 1,847 single-breast MRI examinations (377 BI-RADS >=4) from an in-house dataset and 924 from an external validation dataset (Duke). Four abbreviated protocols were tested: T1-weighted early subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500), DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%, and 97.5% sensitivity using five-fold cross-validation and area under the receiver operating characteristic curve (AUC) analysis. AUC differences were compared with the DeLong test. False negatives were characterized, and attention maps of true positives were rated in the external dataset. Results: A total of 1,448 female patients (mean age, 49 +/- 12 years) were included. T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04 (p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/- 7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter <10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the MST framework correctly triaged cases without BI-RADS >=4, achieving 19% specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI. Further research is warranted before clinical implementation.

</details>


### [132] [Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds](https://arxiv.org/abs/2511.05996)
*Xianhui Meng,Yukang Huo,Li Zhang,Liu Liu,Haonan Jiang,Yan Zhong,Pingrui Zhang,Cewu Lu,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PPF-Tracker的新型基于点对特征的姿态跟踪框架，用于解决铰接物体的多帧姿态跟踪问题，通过准规范化、点对特征建模和关节轴语义约束，实现了在复杂环境中的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 铰接物体在日常生活中和机器人操作任务中普遍存在，但由于其固有的运动学约束，与刚性物体相比，铰接物体的姿态跟踪问题仍未得到充分探索。

Method: 该框架首先在SE(3)李群空间中对点云进行准规范化，然后利用点对特征(PPF)及其SE(3)不变性来建模铰接物体并预测姿态投票参数。最后，结合关节轴的语义信息，对铰接物体所有部件施加统一的运动学约束。

Result: PPF-Tracker在合成数据集和真实世界场景中都得到了系统评估，在多样化和具有挑战性的环境中表现出强大的泛化能力。实验结果突出了PPF-Tracker在铰接物体多帧姿态跟踪中的有效性和鲁棒性。

Conclusion: PPF-Tracker为铰接物体的姿态跟踪提供了一个有效且鲁棒的解决方案，有望推动机器人技术、具身智能和增强现实领域的进步。

Abstract: Articulated objects are prevalent in daily life and robotic manipulation tasks. However, compared to rigid objects, pose tracking for articulated objects remains an underexplored problem due to their inherent kinematic constraints. To address these challenges, this work proposes a novel point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The proposed framework first performs quasi-canonicalization of point clouds in the SE(3) Lie group space, and then models articulated objects using Point Pair Features (PPF) to predict pose voting parameters by leveraging the invariance properties of SE(3). Finally, semantic information of joint axes is incorporated to impose unified kinematic constraints across all parts of the articulated object. PPF-Tracker is systematically evaluated on both synthetic datasets and real-world scenarios, demonstrating strong generalization across diverse and challenging environments. Experimental results highlight the effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of articulated objects. We believe this work can foster advances in robotics, embodied intelligence, and augmented reality. Codes are available at https://github.com/mengxh20/PPFTracker.

</details>


### [133] [A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation](https://arxiv.org/abs/2511.05989)
*Prateek Singh,Moumita Dholey,P. K. Vinod*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件去噪扩散模型的新框架，结合ViT编码器和增强型UNet解码器，并通过自适应条件桥、拓扑去噪一致性损失和双头架构，解决了乳腺超声图像中病灶分割的低对比度、噪声和解剖学不一致问题，实现了最先进的性能和解剖学上合理的结果。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像中病灶的精确分割对早期诊断至关重要，但低对比度、散斑噪声和模糊边界使其难以实现。现有深度学习模型常因缺乏足够的全局上下文而导致解剖学上不一致的分割结果。

Method: 本文提出了一种灵活的条件去噪扩散模型，该模型结合了基于UNet的生成解码器和用于全局特征提取的Vision Transformer（ViT）编码器。主要创新包括：1) 自适应条件桥（ACB），用于高效的多尺度语义特征融合；2) 拓扑去噪一致性（TDC）损失组件，通过惩罚去噪过程中的结构不一致性来正则化训练；3) 双头架构，利用去噪目标作为强大的正则化器，使轻量级辅助头能快速准确地进行推断，同时还有一个噪声预测头。

Result: 该框架在公共乳腺超声数据集上建立了新的最先进水平，在BUSI数据集上Dice得分达到0.96，在BrEaST数据集上达到0.90，在BUS-UCLM数据集上达到0.97。全面的消融研究经验性地验证了模型组件对于实现这些结果以及生成准确且解剖学上合理的分割至关重要。

Conclusion: 所提出的框架通过引入先进的扩散模型、ViT编码器、自适应特征融合、拓扑一致性损失和双头架构，显著提高了乳腺超声图像病灶分割的准确性和解剖学合理性，成功克服了现有方法的局限性，并达到了新的最先进性能。

Abstract: In breast ultrasound images, precise lesion segmentation is essential for early diagnosis; however, low contrast, speckle noise, and unclear boundaries make this difficult. Even though deep learning models have demonstrated potential, standard convolutional architectures frequently fall short in capturing enough global context, resulting in segmentations that are anatomically inconsistent. To overcome these drawbacks, we suggest a flexible, conditional Denoising Diffusion Model that combines an enhanced UNet-based generative decoder with a Vision Transformer (ViT) encoder for global feature extraction. We introduce three primary innovations: 1) an Adaptive Conditioning Bridge (ACB) for efficient, multi-scale fusion of semantic features; 2) a novel Topological Denoising Consistency (TDC) loss component that regularizes training by penalizing structural inconsistencies during denoising; and 3) a dual-head architecture that leverages the denoising objective as a powerful regularizer, enabling a lightweight auxiliary head to perform rapid and accurate inference on smaller datasets and a noise prediction head. Our framework establishes a new state-of-the-art on public breast ultrasound datasets, achieving Dice scores of 0.96 on BUSI, 0.90 on BrEaST and 0.97 on BUS-UCLM. Comprehensive ablation studies empirically validate that the model components are critical for achieving these results and for producing segmentations that are not only accurate but also anatomically plausible.

</details>


### [134] [One-Shot Knowledge Transfer for Scalable Person Re-Identification](https://arxiv.org/abs/2511.06016)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.CV

TL;DR: 在边缘计算行人重识别（ReID）中，传统模型压缩方法在需要多种尺寸模型时计算繁琐。本文提出OSKT（One-Shot Knowledge Transfer），通过将教师模型知识整合到“权重链”中，实现一次性知识迁移，无需额外计算即可根据资源需求扩展到目标模型尺寸，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 边缘计算ReID对减少中心云服务器负载和保护用户隐私至关重要，需要紧凑模型。然而，传统压缩方法在需要适应不同资源条件、生成多种尺寸模型时，需要对每个学生模型进行重复且繁琐的计算。

Method: 提出一种名为OSKT（One-Shot Knowledge Transfer）的新型知识继承方法。该方法将教师模型的知识整合到一个名为“权重链”的中间载体中。当需要满足特定资源约束的下游模型时，该权重链可以直接扩展到目标模型尺寸，无需额外计算。

Result: OSKT显著优于现有最先进的压缩方法。其额外优势在于知识的一次性迁移，消除了为每个目标模型进行频繁计算的需要。

Conclusion: OSKT提供了一种高效的、一次性知识迁移方案，解决了边缘计算ReID中对多种尺寸模型的需求，显著提升了性能并减少了计算开销。

Abstract: Edge computing in person re-identification (ReID) is crucial for reducing the load on central cloud servers and ensuring user privacy. Conventional compression methods for obtaining compact models require computations for each individual student model. When multiple models of varying sizes are needed to accommodate different resource conditions, this leads to repetitive and cumbersome computations. To address this challenge, we propose a novel knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which consolidates the knowledge of the teacher model into an intermediate carrier called a weight chain. When a downstream scenario demands a model that meets specific resource constraints, this weight chain can be expanded to the target model size without additional computation. OSKT significantly outperforms state-of-the-art compression methods, with the added advantage of one-time knowledge transfer that eliminates the need for frequent computations for each target model.

</details>


### [135] [MALeR: Improving Compositional Fidelity in Layout-Guided Generation](https://arxiv.org/abs/2511.06002)
*Shivank Saxena,Dhruv Srivastava,Makarand Tapaswi*

Main category: cs.CV

TL;DR: 本文提出MALeR方法，通过防止主体溢出布局和引入蒙版属性感知绑定机制，显著提升了文本到图像模型在生成多主体、多属性复杂构图场景时的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像模型在图像生成方面取得进展，但在生成具有多个主体和属性的复杂构图场景时仍面临挑战。现有布局引导方法存在主体溢出布局、生成图像不自然以及属性在主体间泄漏等问题，导致视觉输出不准确。

Method: MALeR方法通过以下方式解决上述挑战：1) 确保生成的主体不会出现在给定布局之外，同时保持图像分布内。2) 提出一种蒙版、属性感知的绑定机制，以防止属性泄漏，从而在复杂构图场景中准确渲染具有多个属性的主体。

Result: 定性和定量评估表明，MALeR在构图准确性、生成一致性和属性绑定方面均优于现有方法。该方法尤其擅长生成包含多个主体且每个主体具有多个属性的场景图像。

Conclusion: MALeR是一种有效的方法，能够解决现有布局引导文本到图像模型在生成复杂构图场景时面临的挑战，特别是在处理多主体和多属性方面表现出色，显著提高了生成图像的准确性和真实性。

Abstract: Recent advances in text-to-image models have enabled a new era of creative and controllable image generation. However, generating compositional scenes with multiple subjects and attributes remains a significant challenge. To enhance user control over subject placement, several layout-guided methods have been proposed. However, these methods face numerous challenges, particularly in compositional scenes. Unintended subjects often appear outside the layouts, generated images can be out-of-distribution and contain unnatural artifacts, or attributes bleed across subjects, leading to incorrect visual outputs. In this work, we propose MALeR, a method that addresses each of these challenges. Given a text prompt and corresponding layouts, our method prevents subjects from appearing outside the given layouts while being in-distribution. Additionally, we propose a masked, attribute-aware binding mechanism that prevents attribute leakage, enabling accurate rendering of subjects with multiple attributes, even in complex compositional scenes. Qualitative and quantitative evaluation demonstrates that our method achieves superior performance in compositional accuracy, generation consistency, and attribute binding compared to previous work. MALeR is particularly adept at generating images of scenes with multiple subjects and multiple attributes per subject.

</details>


### [136] [How Reasoning Influences Intersectional Biases in Vision Language Models](https://arxiv.org/abs/2511.06005)
*Adit Desai,Sudipta Roy,Mohna Chakraborty*

Main category: cs.CV

TL;DR: 本文系统分析了五种开源视觉语言模型（VLMs）在职业预测任务中存在的社会偏见，发现其偏见推理模式导致了交叉性差异，强调了在部署前使VLM推理与人类价值观对齐的必要性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）被广泛部署，但其训练数据常包含社会偏见，导致输出结果存在偏差。与人类不同，VLMs通过统计关联处理图像，其推理方式与人类推理存在差异。理解VLM的推理过程如何延续固有偏见并影响下游性能至关重要。

Method: 研究系统分析了五种开源VLM在职业预测任务中的社会偏见。使用FairFace数据集，针对32种职业和三种不同的提示风格，获取了模型的预测结果和推理过程。

Result: 研究发现，偏见的推理模式系统性地导致了交叉性差异，即VLM的输出结果中存在基于多个属性（如种族、性别）组合的歧视。

Conclusion: 鉴于VLM偏见的推理模式导致了交叉性差异，有必要在VLM部署到下游任务之前，使其推理与人类价值观对齐。

Abstract: Vision Language Models (VLMs) are increasingly deployed across downstream tasks, yet their training data often encode social biases that surface in outputs. Unlike humans, who interpret images through contextual and social cues, VLMs process them through statistical associations, often leading to reasoning that diverges from human reasoning. By analyzing how a VLM reasons, we can understand how inherent biases are perpetuated and can adversely affect downstream performance. To examine this gap, we systematically analyze social biases in five open-source VLMs for an occupation prediction task, on the FairFace dataset. Across 32 occupations and three different prompting styles, we elicit both predictions and reasoning. Our findings reveal that the biased reasoning patterns systematically underlie intersectional disparities, highlighting the need to align VLM reasoning with human values prior to its downstream deployment.

</details>


### [137] [MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model](https://arxiv.org/abs/2511.06019)
*Priyansh Srivastava,Romit Chatterjee,Abir Sen,Aradhana Behura,Ratnakar Dash*

Main category: cs.CV

TL;DR: MiVID是一种轻量级、自监督、基于扩散的视频帧插值(VFI)框架，通过结合3D U-Net和Transformer风格的时间注意力，在无高帧率监督的情况下，有效解决了传统VFI方法的挑战，并实现了与监督基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的VFI方法（如光流和基于学习的模型）在处理遮挡、域偏移和模糊运动时表现不佳，并且通常需要密集的真实高帧率数据进行监督训练。

Method: MiVID框架采用3D U-Net骨干网络结合Transformer风格的时间注意力，无需显式运动估计。它通过混合掩码机制（模拟遮挡和运动不确定性）、基于余弦的渐进掩码和自适应损失调度进行自监督训练，从而学习鲁棒的时空表示。该模型完全在CPU上使用9帧视频片段进行训练，是一种低资源但高效的流水线。

Result: MiVID在UCF101-7和DAVIS-7数据集上进行了评估，尽管训练资源受限（仅CPU），但在50个周期内就取得了最佳结果，性能与多个监督基线相当。

Conclusion: 这项工作展示了自监督扩散先验在时间连贯帧合成方面的强大能力，并为开发可访问且通用性强的VFI系统提供了一条可扩展的路径。

Abstract: Video Frame Interpolation (VFI) remains a cornerstone in video enhancement, enabling temporal upscaling for tasks like slow-motion rendering, frame rate conversion, and video restoration. While classical methods rely on optical flow and learning-based models assume access to dense ground-truth, both struggle with occlusions, domain shifts, and ambiguous motion. This article introduces MiVID, a lightweight, self-supervised, diffusion-based framework for video interpolation. Our model eliminates the need for explicit motion estimation by combining a 3D U-Net backbone with transformer-style temporal attention, trained under a hybrid masking regime that simulates occlusions and motion uncertainty. The use of cosine-based progressive masking and adaptive loss scheduling allows our network to learn robust spatiotemporal representations without any high-frame-rate supervision. Our framework is evaluated on UCF101-7 and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and 9-frame video segments, making it a low-resource yet highly effective pipeline. Despite these constraints, our model achieves optimal results at just 50 epochs, competitive with several supervised baselines.This work demonstrates the power of self-supervised diffusion priors for temporally coherent frame synthesis and provides a scalable path toward accessible and generalizable VFI systems.

</details>


### [138] [Distributed Deep Learning for Medical Image Denoising with Data Obfuscation](https://arxiv.org/abs/2511.06006)
*Sulaimon Oyeniyi Adebayo,Ayaz H. Khan*

Main category: cs.CV

TL;DR: 本研究探索了利用分布式深度学习对NIH Chest X-ray14数据集中的胸部X光图像进行去噪，比较了U-Net和U-Net++架构在不同多GPU训练配置下的性能，并展示了优化训练流程在加速医疗图像处理方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 改善医疗图像质量，同时最大限度地减少敏感信息暴露，尤其是在处理大规模临床数据集时，医疗图像去噪至关重要。本研究旨在通过深度学习方法解决这一问题，并探索高效的分布式训练策略。

Method: 研究使用了NIH Chest X-ray14数据集，并采用加性高斯噪声作为轻量级混淆技术。实验评估了U-Net和U-Net++架构在单GPU、标准多GPU (DataParallel) 和优化多GPU (PyTorch的DistributedDataParallel (DDP) 和Automatic Mixed Precision (AMP)) 训练配置下的性能。评估指标包括峰值信噪比 (PSNR)、结构相似性指数 (SSIM) 和学习感知图像块相似度 (LPIPS)。

Result: U-Net++在去噪性能上始终优于U-Net，取得了有竞争力的PSNR和SSIM分数，但在低中噪声水平下LPIPS表现不如U-Net，表明U-Net++具有增强的结构保真度和低感知相似性。优化的训练流程（DDP+AMP）将训练时间比单GPU训练缩短了60%以上，比标准DataParallel快了40%以上，且模型准确性仅有轻微下降（以准确性换取速度）。

Conclusion: 软件级优化在医疗图像分布式学习中是有效的。将架构设计、轻量级混淆和先进的分布式训练策略相结合，在现实世界的临床和研究环境中，可以显著加速和增强医疗图像处理流程，具有实际可行性。

Abstract: Medical image denoising is essential for improving image quality while minimizing the exposure of sensitive information, particularly when working with large-scale clinical datasets. This study explores distributed deep learning for denoising chest X-ray images from the NIH Chest X-ray14 dataset, using additive Gaussian noise as a lightweight obfuscation technique. We implement and evaluate U-Net and U-Net++ architectures under single-GPU, standard multi-GPU (DataParallel), and optimized multi-GPU training configurations using PyTorch's DistributedDataParallel (DDP) and Automatic Mixed Precision (AMP). Our results show that U-Net++ consistently delivers superior denoising performance, achieving competitive Peak Signal to Noise Ratio (PSNR) and Structured Similarity Index Method (SSIM) scores, though with less performance in Learned Perceptual Image Patch Similarity (LPIPS) compared to U-Net under low and moderate noise levels. This indicates U-Net++'s enhanced structural fidelity and low perceptual similarity. Meanwhile, our optimized training pipeline reduces training time by over 60% for both models compared to single-GPU training, and outperforms standard DataParallel by over 40%, with only a minor accuracy drop for both models (trading some accuracy for speed). These findings highlight the effectiveness of software-level optimization in distributed learning for medical imaging. This work demonstrates the practical viability of combining architectural design, lightweight obfuscation, and advanced distributed training strategies to accelerate and enhance medical image processing pipelines in real-world clinical and research environments. The full implementation is publicly available at: https://github.com/Suadey/medical-image-denoising-ddp.

</details>


### [139] [Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era](https://arxiv.org/abs/2511.06024)
*Feng Lu,Tong Jin,Canming Ye,Yunpeng Liu,Xiangyuan Lan,Chun Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种无需专用聚合器，仅通过Transformer主干网络就能实现视觉地点识别（VPR）的方法，通过引入可学习的聚合令牌，并利用自注意力机制隐式聚合信息，取得了最先进的性能和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉地点识别（VPR）方法普遍遵循“主干网络+聚合器”的范式。作者认为在Transformer时代，专用聚合器可能并非必需，并尝试仅用主干网络就能获得鲁棒的全局描述符。

Method: 该方法引入了可学习的聚合令牌，将其预置于特定Transformer块之前的图像块令牌。所有令牌通过内在的自注意力机制进行联合处理和全局交互，从而将图像块令牌中的有用信息隐式聚合到聚合令牌中。最终，仅提取最后一个输出的聚合令牌并拼接作为全局表示。此外，论文还通过实证研究提出了最优的令牌插入策略和初始化方法。

Result: 实验结果表明，该方法在多个VPR数据集上超越了现有最先进的方法，具有更高的效率，并在MSLS挑战排行榜上排名第一。

Conclusion: 在Transformer时代，通过在主干网络中引入可学习的聚合令牌并利用其固有的自注意力机制进行隐式聚合，可以以极其简单的方式获得鲁棒的全局描述符，从而无需专用聚合器，并能实现卓越的VPR性能和效率。

Abstract: Visual place recognition (VPR) is typically regarded as a specific image retrieval task, whose core lies in representing images as global descriptors. Over the past decade, dominant VPR methods (e.g., NetVLAD) have followed a paradigm that first extracts the patch features/tokens of the input image using a backbone, and then aggregates these patch features into a global descriptor via an aggregator. This backbone-plus-aggregator paradigm has achieved overwhelming dominance in the CNN era and remains widely used in transformer-based models. In this paper, however, we argue that a dedicated aggregator is not necessary in the transformer era, that is, we can obtain robust global descriptors only with the backbone. Specifically, we introduce some learnable aggregation tokens, which are prepended to the patch tokens before a particular transformer block. All these tokens will be jointly processed and interact globally via the intrinsic self-attention mechanism, implicitly aggregating useful information within the patch tokens to the aggregation tokens. Finally, we only take these aggregation tokens from the last output tokens and concatenate them as the global representation. Although implicit aggregation can provide robust global descriptors in an extremely simple manner, where and how to insert additional tokens, as well as the initialization of tokens, remains an open issue worthy of further exploration. To this end, we also propose the optimal token insertion strategy and token initialization method derived from empirical studies. Experimental results show that our method outperforms state-of-the-art methods on several VPR datasets with higher efficiency and ranks 1st on the MSLS challenge leaderboard. The code is available at https://github.com/lu-feng/image.

</details>


### [140] [S2ML: Spatio-Spectral Mutual Learning for Depth Completion](https://arxiv.org/abs/2511.06033)
*Zihui Zhao,Yifei Zhang,Zheng Wang,Yang Li,Kui Jiang,Zihan Geng,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为S2ML的空谱互学习框架，通过结合空间域和频率域的优势来完成深度图像，有效解决了原始深度图像不完整的问题，并超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: RGB-D相机捕获的原始深度图像常因弱反射、边界阴影和伪影导致深度值不完整，这限制了其在下游视觉任务中的应用。现有深度补全方法主要在图像域进行，但忽视了原始深度图像的物理特性，特别是无效深度区域会改变频率分布模式。

Method: 本文提出了一个空谱互学习框架（S2ML），旨在整合空间域和频率域的优势。具体而言，该方法考虑了幅度和相位谱的不同特性，设计了一个专用的谱融合模块。同时，在统一的嵌入空间中计算空间域和频率域特征之间的局部和全局相关性。通过渐进的相互表示和细化，鼓励网络充分探索互补的物理特性和先验知识，以实现更准确的深度补全。

Result: 实验结果表明，所提出的S2ML方法是有效的，在NYU-Depth V2和SUN RGB-D数据集上分别比最先进的方法CFormer性能提高了0.828 dB和0.834 dB。

Conclusion: S2ML框架通过有效利用空间域和频率域的互补物理特性，实现了更准确的深度补全，显著提升了深度补全任务的性能。

Abstract: The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.

</details>


### [141] [StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video](https://arxiv.org/abs/2511.06046)
*Zhihui Ke,Yuyang Liu,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: 本文提出StreamSTGS，一种用于实时流式传输自由视角视频的新型表示方法，通过高效压缩3D高斯表示和支持自适应码率控制，显著降低了存储需求并提升了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 实时流式传输自由视角视频（FVV）面临训练、渲染和传输效率的巨大挑战，特别是基于3D高斯溅射（3DGS）的最新方法，其存储需求高达每帧10MB，使得实时传输成为不可能。

Method: StreamSTGS通过规范3D高斯、时间特征和形变场来表示动态场景。为了实现高压缩效率，规范高斯属性被编码为2D图像，时间特征被编码为视频。这种设计天然支持基于网络状况的自适应码率控制。此外，提出了一种滑动窗口方案来聚合相邻时间特征以学习局部运动，并引入了一个Transformer引导的辅助训练模块来学习全局运动。

Result: StreamSTGS在各种FVV基准测试中，与最先进的方法相比，在所有指标上都表现出具有竞争力的性能。值得注意的是，StreamSTGS将PSNR平均提高了1dB，同时将平均帧大小减少到仅170KB。

Conclusion: StreamSTGS成功解决了实时流式传输自由视角视频的存储和传输效率问题，通过创新的表示和压缩方案，在保持高质量渲染的同时，大幅降低了数据量并支持自适应码率控制，使其成为实时FVV流媒体的可行解决方案。

Abstract: Streaming free-viewpoint video~(FVV) in real-time still faces significant challenges, particularly in training, rendering, and transmission efficiency. Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent 3DGS-based FVV methods have achieved notable breakthroughs in both training and rendering. However, the storage requirements of these methods can reach up to $10$MB per frame, making stream FVV in real-time impossible. To address this problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D Gaussians, temporal features, and a deformation field. For high compression efficiency, we encode canonical Gaussian attributes as 2D images and temporal features as a video. This design not only enables real-time streaming, but also inherently supports adaptive bitrate control based on network condition without any extra training. Moreover, we propose a sliding window scheme to aggregate adjacent temporal features to learn local motions, and then introduce a transformer-guided auxiliary training module to learn global motions. On diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the PSNR by an average of $1$dB while reducing the average frame size to just $170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.

</details>


### [142] [Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration](https://arxiv.org/abs/2511.06087)
*Umar Rashid,Muhammad Arslan Arshad,Ghulam Ahmad,Muhammad Zeeshan Anjum,Rizwan Khan,Muhammad Akmal*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN和Vision Transformer的混合深度学习框架，用于有效且高效地恢复场景文本图像中的运动模糊，特别是在处理空间可变模糊和长程依赖方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 场景文本图像中的运动模糊严重影响可读性并阻碍计算机视觉任务（如自动驾驶、文档数字化）的可靠性。传统去模糊方法难以处理空间可变模糊，并且无法有效建模恢复文本清晰度所需的长期依赖关系。

Method: 研究引入了一个混合深度学习框架，结合了卷积神经网络（CNN）和Vision Transformer（ViT）。该架构使用基于CNN的编码器-解码器来保留结构细节，同时通过自注意力机制的Transformer模块增强全局感知。模型在从TextOCR派生的人工合成模糊数据集上进行训练，该数据集包含清晰文本样本和使用真实运动模糊核生成的模糊版本。优化通过结合平均绝对误差（MAE）、均方误差（MSE）、感知相似性和结构相似性（SSIM）的复合损失进行。

Result: 定量评估显示，所提出的方法在PSNR上达到32.20 dB，SSIM上达到0.934，同时保持轻量级（2.83百万参数）和高效（平均推理时间61毫秒）。

Conclusion: 这些结果突出了CNN-ViT混合设计的有效性和计算效率，证明了其在实际运动模糊场景文本恢复中的实用性。

Abstract: Motion blur in scene text images severely impairs readability and hinders the reliability of computer vision tasks, including autonomous driving, document digitization, and visual information retrieval. Conventional deblurring approaches are often inadequate in handling spatially varying blur and typically fall short in modeling the long-range dependencies necessary for restoring textual clarity. To overcome these limitations, we introduce a hybrid deep learning framework that combines convolutional neural networks (CNNs) with vision transformers (ViTs), thereby leveraging both local feature extraction and global contextual reasoning. The architecture employs a CNN-based encoder-decoder to preserve structural details, while a transformer module enhances global awareness through self-attention. Training is conducted on a curated dataset derived from TextOCR, where sharp scene-text samples are paired with synthetically blurred versions generated using realistic motion-blur kernels of multiple sizes and orientations. Model optimization is guided by a composite loss that incorporates mean absolute error (MAE), squared error (MSE), perceptual similarity, and structural similarity (SSIM). Quantitative evaluations show that the proposed method attains 32.20 dB in PSNR and 0.934 in SSIM, while remaining lightweight with 2.83 million parameters and an average inference time of 61 ms. These results highlight the effectiveness and computational efficiency of the CNN-ViT hybrid design, establishing its practicality for real-world motion-blurred scene-text restoration.

</details>


### [143] [LoopExpose: An Unsupervised Framework for Arbitrary-Length Exposure Correction](https://arxiv.org/abs/2511.06066)
*Ao Li,Chen Chen,Zhenyu Wang,Tao Huang,Fangfang Wu,Weisheng Dong*

Main category: cs.CV

TL;DR: LoopExpose是一种基于伪标签的无监督任意长度曝光校正方法，通过嵌套循环优化策略和反馈机制，联合优化校正模型和伪监督信息，并引入亮度排序损失实现自监督学习，性能优于现有先进无监督方法。


<details>
  <summary>Details</summary>
Motivation: 有监督学习在曝光校正方面取得了显著进展，但严重依赖难以获取的大规模标注数据集。本研究旨在解决这一局限性。

Method: 提出了一种名为LoopExpose的基于伪标签的无监督方法，用于任意长度曝光校正。采用嵌套循环优化策略，在双层框架中联合优化校正模型和伪监督信息。上层训练校正模型，利用下层通过多曝光融合生成的伪标签。引入反馈机制，将校正后的图像反馈到融合过程中以细化伪标签，形成自增强学习循环。此外，引入亮度排序损失（Luminance Ranking Loss），利用输入序列的相对亮度顺序作为自监督约束。

Result: 在不同基准数据集上进行的广泛实验表明，LoopExpose实现了卓越的曝光校正和融合性能，优于现有最先进的无监督方法。

Conclusion: LoopExpose通过其创新的无监督伪标签和嵌套循环优化策略，有效解决了大规模标注数据获取困难的问题，并在曝光校正和融合方面取得了领先的性能。

Abstract: Exposure correction is essential for enhancing image quality under challenging lighting conditions. While supervised learning has achieved significant progress in this area, it relies heavily on large-scale labeled datasets, which are difficult to obtain in practical scenarios. To address this limitation, we propose a pseudo label-based unsupervised method called LoopExpose for arbitrary-length exposure correction. A nested loop optimization strategy is proposed to address the exposure correction problem, where the correction model and pseudo-supervised information are jointly optimized in a two-level framework. Specifically, the upper-level trains a correction model using pseudo-labels generated through multi-exposure fusion at the lower level. A feedback mechanism is introduced where corrected images are fed back into the fusion process to refine the pseudo-labels, creating a self-reinforcing learning loop. Considering the dominant role of luminance calibration in exposure correction, a Luminance Ranking Loss is introduced to leverage the relative luminance ordering across the input sequence as a self-supervised constraint. Extensive experiments on different benchmark datasets demonstrate that LoopExpose achieves superior exposure correction and fusion performance, outperforming existing state-of-the-art unsupervised methods. Code is available at https://github.com/FALALAS/LoopExpose.

</details>


### [144] [An Artificial Intelligence-based Assistant for the Visually Impaired](https://arxiv.org/abs/2511.06080)
*Luis Marquez-Carpintero,Francisco Gomez-Donoso,Zuria Bauer,Bessie Dominguez-Dager,Alvaro Belmonte-Baeza,Mónica Pina-Navarro,Francisco Morillas-Espejo,Felix Escalona,Miguel Cazorla*

Main category: cs.CV

TL;DR: 本文介绍了一款名为AIDEN的人工智能助手应用，旨在通过物体识别、文本阅读和环境问答，提升视障人士的生活质量和独立性。


<details>
  <summary>Details</summary>
Motivation: 视障人士在识别物体、阅读文本和导航陌生环境方面面临挑战，现有解决方案（如盲文、有声读物、屏幕阅读器）并非在所有情况下都有效，这限制了他们的独立性并降低了生活质量。

Method: 该应用利用最先进的机器学习算法，特别是You Only Look Once (YOLO) 架构和大型语言视觉助手，来实现物体识别与描述、文本阅读以及环境问答。系统还整合了多种方法以促进用户交互和信息获取。

Result: AIDEN能够识别并描述物体、阅读文本并回答有关环境的问题。它旨在增强用户的自主性和信息获取能力，并通过用户反馈证实其提升了日常可用性感知。

Conclusion: AIDEN作为一款基于AI的助手应用，通过提供有效的视觉和文本信息访问，显著改善了视障人士的日常可用性和生活质量。

Abstract: This paper describes an artificial intelligence-based assistant application, AIDEN, developed during 2023 and 2024, aimed at improving the quality of life for visually impaired individuals. Visually impaired individuals face challenges in identifying objects, reading text, and navigating unfamiliar environments, which can limit their independence and reduce their quality of life. Although solutions such as Braille, audio books, and screen readers exist, they may not be effective in all situations. This application leverages state-of-the-art machine learning algorithms to identify and describe objects, read text, and answer questions about the environment. Specifically, it uses You Only Look Once architectures and a Large Language and Vision Assistant. The system incorporates several methods to facilitate the user's interaction with the system and access to textual and visual information in an appropriate manner. AIDEN aims to enhance user autonomy and access to information, contributing to an improved perception of daily usability, as supported by user feedback.

</details>


### [145] [LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation](https://arxiv.org/abs/2511.06272)
*Zijie Wang,Weiming Zhang,Wei Zhang,Xiao Tan,Hongxing Liu,Yaowei Wang,Guanbin Li*

Main category: cs.CV

TL;DR: LaneDiffusion引入了一种新的生成范式，利用扩散模型在鸟瞰图（BEV）特征级别生成车道中心线先验，显著提升了自动驾驶中中心线图学习的性能，并取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的中心线图学习方法缺乏空间推理能力，难以处理被遮挡或不可见的中心线。尽管生成式方法潜力巨大，但在该领域尚未得到充分探索。

Method: 本文提出了LaneDiffusion，一种新颖的生成范式，它不直接预测矢量化中心线，而是利用扩散模型在鸟瞰图（BEV）特征级别生成车道中心线先验。该方法集成了车道先验注入模块（LPIM）和车道先验扩散模块（LPDM），以有效构建扩散目标并管理扩散过程。随后，从这些注入先验的BEV特征中解码出矢量化中心线和拓扑结构。

Result: 在nuScenes和Argoverse2数据集上的广泛评估表明，LaneDiffusion显著优于现有方法。在细粒度点级指标（GEO F1, TOPO F1, JTOPO F1, APLS和SDA）上分别提升了4.2%、4.6%、4.7%、6.4%和1.8%，在段级指标（IoU, mAP_cf, DET_l和TOP_ll）上分别提升了2.3%、6.4%、6.8%和2.1%。这些结果确立了中心线图学习领域的最新技术水平。

Conclusion: LaneDiffusion通过引入生成式扩散模型在BEV特征级别进行中心线先验生成，成功解决了传统方法的局限性，并在中心线图学习任务中实现了最先进的性能，为该领域生成模型的研究提供了新见解。

Abstract: Centerline graphs, crucial for path planning in autonomous driving, are traditionally learned using deterministic methods. However, these methods often lack spatial reasoning and struggle with occluded or invisible centerlines. Generative approaches, despite their potential, remain underexplored in this domain. We introduce LaneDiffusion, a novel generative paradigm for centerline graph learning. LaneDiffusion innovatively employs diffusion models to generate lane centerline priors at the Bird's Eye View (BEV) feature level, instead of directly predicting vectorized centerlines. Our method integrates a Lane Prior Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively construct diffusion targets and manage the diffusion process. Furthermore, vectorized centerlines and topologies are then decoded from these prior-injected BEV features. Extensive evaluations on the nuScenes and Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and 2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and TOP_ll). These results establish state-of-the-art performance in centerline graph learning, offering new insights into generative models for this task.

</details>


### [146] [MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution](https://arxiv.org/abs/2511.06172)
*Hua Chang,Xin Xu,Wei Liu,Wei Wang,Xin Yuan,Kui Jiang*

Main category: cs.CV

TL;DR: 本文针对中国戏曲老视频画质退化问题，提出了一个大规模数据集COVC和基于Mamba的多尺度融合网络MambaOVSR，以实现时空超分辨率，显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 中国戏曲经典表演的早期录像因设备限制（如低帧率、低分辨率）而画质退化，影响了档案保存。现有时空视频超分辨率（STVSR）方法在戏曲视频上应用面临挑战，主要原因包括数据集稀缺导致高频细节恢复困难，以及现有方法缺乏全局建模能力，难以处理戏曲特有的大幅度动作，从而影响视觉质量。

Method: 本文首先构建了一个大规模中国戏曲视频片段（COVC）数据集。在此基础上，提出了一种基于Mamba的多尺度融合网络MambaOVSR。该网络包含三个创新组件：用于通过多尺度交替扫描机制进行运动建模的全局融合模块（GFM）、用于在不同序列长度上进行对齐的多尺度协同Mamba模块（MSMM），以及解决对齐过程中特征伪影和位置信息丢失的MambaVR块。

Result: 在COVC数据集上的实验结果表明，MambaOVSR在PSNR方面比现有最先进的STVSR方法平均高出1.86 dB，性能显著优越。数据集和代码将公开发布。

Conclusion: 本文通过创建专用数据集和提出创新的MambaOVSR网络，有效解决了中国戏曲老视频的时空超分辨率难题，显著提升了视频质量，对戏曲艺术的数字化保存具有重要意义。

Abstract: Chinese opera is celebrated for preserving classical art. However, early filming equipment limitations have degraded videos of last-century performances by renowned artists (e.g., low frame rates and resolution), hindering archival efforts. Although space-time video super-resolution (STVSR) has advanced significantly, applying it directly to opera videos remains challenging. The scarcity of datasets impedes the recovery of high frequency details, and existing STVSR methods lack global modeling capabilities, compromising visual quality when handling opera's characteristic large motions. To address these challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset and propose the Mamba-based multiscale fusion network for space-time Opera Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three novel components: the Global Fusion Module (GFM) for motion modeling through a multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba Module (MSMM) for alignment across different sequence lengths. Additionally, our MambaVR block resolves feature artifacts and positional information loss during alignment. Experimental results on the COVC dataset show that MambaOVSR significantly outperforms the SOTA STVSR method by an average of 1.86 dB in terms of PSNR. Dataset and Code will be publicly released.

</details>


### [147] [CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection](https://arxiv.org/abs/2511.06325)
*Minsuk Jang,Hyeonseo Jeong,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: CINEMAE是一种新型AIGC图像检测范式，将文本检测原理应用于视觉领域，利用MAE的条件重建不确定性来量化局部语义异常，并通过融合实现强大的跨生成器泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像检测器易于过拟合特定生成器伪影，而基于上下文的文本检测器在泛化方面表现出色。研究旨在将文本检测的核心原理（测量分布不一致性）应用于视觉领域，以解决AIGC图像检测的泛化挑战。

Method: 引入CINEMAE范式，将文本检测原理适应到视觉领域。核心洞察是Masked AutoEncoder (MAE) 在给定可见上下文重建被遮蔽补丁时，自然编码了语义一致性期望。通过概率化重建过程，计算条件负对数似然（NLL, p(masked | visible)）来量化局部语义异常。这些补丁级统计数据与全局MAE特征通过学习融合机制结合。

Result: CINEMAE仅使用Stable Diffusion v1.4进行训练，在GenImage基准测试中，对所有八个未见过的生成器实现了超过95%的准确率。该方法显著优于最先进的检测器。

Conclusion: 上下文条件重建不确定性为AIGC检测提供了一个鲁棒且可迁移的信号，证明了其在跨生成器泛化方面的有效性。

Abstract: While context-based detectors have achieved strong generalization for AI-generated text by measuring distributional inconsistencies, image-based detectors still struggle with overfitting to generator-specific artifacts. We introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the core principles of text detection methods to the visual domain. Our key insight is that Masked AutoEncoder (MAE), trained to reconstruct masked patches conditioned on visible context, naturally encodes semantic consistency expectations. We formalize this reconstruction process probabilistically, computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to quantify local semantic anomalies. By aggregating these patch-level statistics with global MAE features through learned fusion, CINEMAE achieves strong cross-generator generalization. Trained exclusively on Stable Diffusion v1.4, our method achieves over 95% accuracy on all eight unseen generators in the GenImage benchmark, substantially outperforming state-of-the-art detectors. This demonstrates that context-conditional reconstruction uncertainty provides a robust, transferable signal for AIGC detection.

</details>


### [148] [GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding](https://arxiv.org/abs/2511.06348)
*Athul M. Mathew,Haithem Hermassi,Thariq Khalid,Arshad Ali Khan,Riad Souissi*

Main category: cs.CV

TL;DR: GazeVLM是一个新型的视觉-语言模型（VLM），用于统一多任务注视理解，包括人物检测、注视目标检测和注视对象识别，并通过整合视觉和语言提示，在相关数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究已对视觉场景中的注视线索进行建模，但仍缺乏一个能结合视觉和语言提示的统一系统，以实现全面的人物检测、注视目标检测和注视对象识别。

Method: 本文提出了GazeVLM，一个利用视觉（RGB和深度）和文本模态的多任务注视理解VLM。通过消融研究发现，融合RGB图像与HHA编码深度图，并由文本提示引导，能获得优越性能。此外，引入了一个对象级注视检测指标($AP_{ob}$)。

Result: GazeVLM在视觉输入组合的消融研究中，显示RGB图像与HHA编码深度图的融合在文本提示引导下表现最佳。实验结果表明，GazeVLM显著提升了性能，并在GazeFollow和VideoAttentionTarget数据集上达到了最先进的评估分数。

Conclusion: GazeVLM是首次将VLM应用于结合人物检测、注视目标检测和注视对象识别的多任务注视理解系统。通过有效融合视觉和语言模态，GazeVLM显著提高了注视理解能力，并达到了当前最佳水平。

Abstract: Gaze understanding unifies the detection of people, their gaze targets, and objects of interest into a single framework, offering critical insight into visual attention and intent estimation. Although prior research has modelled gaze cues in visual scenes, a unified system is still needed for gaze understanding using both visual and language prompts. This paper introduces GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding in images, addressing person detection, gaze target detection, and gaze object identification. While other transformer-based methods exist for gaze analysis, GazeVLM represents, to our knowledge, the first application of a VLM to these combined tasks, allowing for selective execution of each task. Through the integration of visual (RGB and depth) and textual modalities, our ablation study on visual input combinations revealed that a fusion of RGB images with HHA-encoded depth maps, guided by text prompts, yields superior performance. We also introduce an object-level gaze detection metric for gaze object identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates significant improvements, notably achieving state-of-the-art evaluation scores on GazeFollow and VideoAttentionTarget datasets.

</details>


### [149] [Neodragon: Mobile Video Generation using Diffusion Transformer](https://arxiv.org/abs/2511.06055)
*Animesh Karnewar,Denis Korzhenkov,Ioannis Lelekas,Adil Karjauv,Noor Fathima,Hanwen Xiong,Vancheeswaran Vaidyanathan,Will Zeng,Rafael Esteves,Tushar Singhal,Fatih Porikli,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: Neodragon是一个移动优化的文生视频系统，能够在高通Hexagon NPU上以6.7秒生成640x1024分辨率的2秒视频，通过一系列蒸馏和剪枝技术实现了高效和高质量的设备端视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有的文生视频模型主要基于Transformer且通常为离线生成，不适用于移动硬件，导致生成效率低下且依赖云服务。研究旨在开发一个专门为移动硬件优化的高效、高保真视频合成系统，以实现低成本、私密、设备端的AI视频创作。

Method: 本研究通过四项关键技术贡献实现目标：(1) 使用新颖的Text-Encoder蒸馏程序，将大型T5xxl文本编码器替换为更小的DT5，同时保持最小的质量损失。(2) 提出非对称解码器蒸馏方法，用更高效的VAE解码器替换原生解码器，而不干扰生成潜在空间。(3) 基于相对重要性对去噪器骨干中的MMDiT块进行剪枝，并通过两阶段蒸馏恢复性能。(4) 通过使用适用于金字塔流匹配的DMD进行步进蒸馏，减少去噪器的NFE需求，显著加速视频生成。此外，系统还集成了优化的SSD1B首帧图像生成器和QuickSRNet进行2倍超分辨率。

Result: Neodragon系统能够在6.7秒内直接在高通Hexagon NPU上生成640x1024分辨率的2秒视频（49帧@24 fps），达到7 FPS的速度。作为端到端系统，它是一个参数高效（完整模型4.945B）、内存高效（峰值RAM使用3.5GB）和运行时高效的模型，并在VBench总分上达到81.61分。

Conclusion: Neodragon成功实现了低成本、私密、设备端的文生视频合成，有效降低了AI视频内容创作的门槛。它赋能创作者无需依赖云服务即可生成高质量视频，从而使AI驱动的视频创作民主化。

Abstract: We introduce Neodragon, a text-to-video system capable of generating 2s (49 frames @24 fps) videos at the 640x1024 resolution directly on a Qualcomm Hexagon NPU in a record 6.7s (7 FPS). Differing from existing transformer-based offline text-to-video generation models, Neodragon is the first to have been specifically optimised for mobile hardware to achieve efficient and high-fidelity video synthesis. We achieve this through four key technical contributions: (1) Replacing the original large 4.762B T5xxl Text-Encoder with a much smaller 0.2B DT5 (DistilT5) with minimal quality loss, enabled through a novel Text-Encoder Distillation procedure. (2) Proposing an Asymmetric Decoder Distillation approach allowing us to replace the native codec-latent-VAE decoder with a more efficient one, without disturbing the generative latent-space of the generation pipeline. (3) Pruning of MMDiT blocks within the denoiser backbone based on their relative importance, with recovery of original performance through a two-stage distillation process. (4) Reducing the NFE (Neural Functional Evaluation) requirement of the denoiser by performing step distillation using DMD adapted for pyramidal flow-matching, thereby substantially accelerating video generation. When paired with an optimised SSD1B first-frame image generator and QuickSRNet for 2x super-resolution, our end-to-end Neodragon system becomes a highly parameter (4.945B full model), memory (3.5GB peak RAM usage), and runtime (6.7s E2E latency) efficient mobile-friendly model, while achieving a VBench total score of 81.61. By enabling low-cost, private, and on-device text-to-video synthesis, Neodragon democratizes AI-based video content creation, empowering creators to generate high-quality videos without reliance on cloud services. Code and model will be made publicly available at our website: https://qualcomm-ai-research.github.io/neodragon

</details>


### [150] [Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving](https://arxiv.org/abs/2511.06138)
*Hossein Askari,Yadan Luo,Hongfu Sun,Fred Roosta*

Main category: cs.CV

TL;DR: 本文提出LFlow，一个无需训练的框架，通过预训练的潜在流先验解决线性逆问题，克服了现有流模型在像素空间操作和先验无关后验协方差的局限性，并在重建质量上优于最先进的潜在扩散求解器。


<details>
  <summary>Details</summary>
Motivation: 当前流基逆问题求解器存在两个主要限制：(i) 直接在像素空间操作，导致训练计算资源消耗大且难以扩展到高分辨率图像；(ii) 采用先验无关的后验协方差指导策略，这会削弱与生成轨迹的对齐并降低后验覆盖率。

Method: 本文提出了LFlow (Latent Refinement via Flows)，一个无需训练的框架，用于通过预训练的潜在流先验解决线性逆问题。LFlow利用流匹配的效率，在潜在空间中沿着最优路径进行ODE采样。这种潜在公式进一步允许引入一个理论上扎实的后验协方差，该协方差源自最优向量场，从而实现有效的流引导。

Result: 实验结果表明，LFlow在大多数任务中，其重建质量优于最先进的潜在扩散求解器。

Conclusion: LFlow通过利用潜在流先验和理论上扎实的后验协方差，为线性逆问题提供了一个高效且有效的解决方案，实现了卓越的重建质量。

Abstract: Recent advances in inverse problem solving have increasingly adopted flow priors over diffusion models due to their ability to construct straight probability paths from noise to data, thereby enhancing efficiency in both training and inference. However, current flow-based inverse solvers face two primary limitations: (i) they operate directly in pixel space, which demands heavy computational resources for training and restricts scalability to high-resolution images, and (ii) they employ guidance strategies with prior-agnostic posterior covariances, which can weaken alignment with the generative trajectory and degrade posterior coverage. In this paper, we propose LFlow (Latent Refinement via Flows), a training-free framework for solving linear inverse problems via pretrained latent flow priors. LFlow leverages the efficiency of flow matching to perform ODE sampling in latent space along an optimal path. This latent formulation further allows us to introduce a theoretically grounded posterior covariance, derived from the optimal vector field, enabling effective flow guidance. Experimental results demonstrate that our proposed method outperforms state-of-the-art latent diffusion solvers in reconstruction quality across most tasks. The code will be publicly available at https://github.com/hosseinaskari-cs/LFlow .

</details>


### [151] [Real-Time Bundle Adjustment for Ultra-High-Resolution UAV Imagery Using Adaptive Patch-Based Feature Tracking](https://arxiv.org/abs/2511.06152)
*Selim Ahmet Iz,Francesco Nex,Norman Kerle,Henry Meissner,Ralf Berger*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的实时捆绑调整（BA）框架，能够直接处理全分辨率无人机影像，通过分块处理和局部优化，在不牺牲细节和精度的情况下，实现快速的地理空间信息生成。


<details>
  <summary>Details</summary>
Motivation: 实时处理无人机影像对于灾害响应等需要紧急地理空间信息的应用至关重要。然而，处理高分辨率影像的计算需求巨大，传统BA方法要么降低图像分辨率牺牲细节，要么处理时间过长，不适用于时间敏感任务。

Method: 提出的框架直接在全分辨率无人机影像上操作，不进行降采样。它将每张图像分割成用户定义的图像块（例如150x150像素），并利用无人机GNSS/IMU数据和粗略的DSM动态跟踪这些图像块，以确保空间一致性。通过无人机导航系统实时确定图像重叠关系，快速选择相关邻近图像进行局部BA。优化范围限制在重叠图像的滑动集群内，包括来自相邻飞行带的图像，从而实现实时性能。

Result: 该方法在保持全局BA精度的同时实现了实时性能。在50MP MACS数据集上的验证表明，该方法能在多条飞行带上保持精确的相机姿态和高保真度测绘，并在2秒内完成完整的捆绑调整，无需GPU加速。

Conclusion: 该研究成功解决了全分辨率无人机影像实时捆绑调整的挑战，提供了一种轻量级、机载兼容的解决方案，可无缝集成到DLR模块化航空相机系统（MACS）中，支持灾害响应、基础设施监测和海岸保护等领域的大面积实时测绘。

Abstract: Real-time processing of UAV imagery is crucial for applications requiring urgent geospatial information, such as disaster response, where rapid decision-making and accurate spatial data are essential. However, processing high-resolution imagery in real time presents significant challenges due to the computational demands of feature extraction, matching, and bundle adjustment (BA). Conventional BA methods either downsample images, sacrificing important details, or require extensive processing time, making them unsuitable for time-critical missions. To overcome these limitations, we propose a novel real-time BA framework that operates directly on fullresolution UAV imagery without downsampling. Our lightweight, onboard-compatible approach divides each image into user-defined patches (e.g., NxN grids, default 150x150 pixels) and dynamically tracks them across frames using UAV GNSS/IMU data and a coarse, globally available digital surface model (DSM). This ensures spatial consistency for robust feature extraction and matching between patches. Overlapping relationships between images are determined in real time using UAV navigation system, enabling the rapid selection of relevant neighbouring images for localized BA. By limiting optimization to a sliding cluster of overlapping images, including those from adjacent flight strips, the method achieves real-time performance while preserving the accuracy of global BA. The proposed algorithm is designed for seamless integration into the DLR Modular Aerial Camera System (MACS), supporting largearea mapping in real time for disaster response, infrastructure monitoring, and coastal protection. Validation on MACS datasets with 50MP images demonstrates that the method maintains precise camera orientations and high-fidelity mapping across multiple strips, running full bundle adjustment in under 2 seconds without GPU acceleration.

</details>


### [152] [DiLO: Disentangled Latent Optimization for Learning Shape and Deformation in Grouped Deforming 3D Objects](https://arxiv.org/abs/2511.06115)
*Mostofa Rafid Uddin,Jana Armouti,Umong Sain,Md Asib Rahman,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 本文提出一种基于解耦潜在优化的无监督方法，将成组变形的3D物体参数化为形状和变形因子，并通过生成器和编码器实现高效推理，在多个下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于无监督地将成组变形的3D物体参数化为独立的形状和变形因子。

Method: 方法分为两阶段：1. 联合优化生成器网络以及形状和变形因子，并辅以特定的正则化技术。2. 训练两个顺序不变的基于PointNet的编码器网络，以实现解耦形状和变形码的高效摊销推理。

Result: 实验证明该方法在无监督变形迁移、变形分类和可解释性分析等多个下游应用中表现显著。在3D人体、动物和面部表情数据集上，其效果与现有复杂方法相当或更优。

Conclusion: 该简单方法在下游任务中非常有效，性能可与现有复杂方法媲美或超越。

Abstract: In this work, we propose a disentangled latent optimization-based method for parameterizing grouped deforming 3D objects into shape and deformation factors in an unsupervised manner. Our approach involves the joint optimization of a generator network along with the shape and deformation factors, supported by specific regularization techniques. For efficient amortized inference of disentangled shape and deformation codes, we train two order-invariant PoinNet-based encoder networks in the second stage of our method. We demonstrate several significant downstream applications of our method, including unsupervised deformation transfer, deformation classification, and explainability analysis. Extensive experiments conducted on 3D human, animal, and facial expression datasets demonstrate that our simple approach is highly effective in these downstream tasks, comparable or superior to existing methods with much higher complexity.

</details>


### [153] [On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective](https://arxiv.org/abs/2511.06406)
*Shuo Yang,Yinghui Xing,Shizhou Zhang,Zhilong Niu*

Main category: cs.CV

TL;DR: 本文提出Scarf-DETR，一个即插即用的模块，通过模态无关的可变形注意力机制和伪模态丢弃策略，有效解决了红外可见光目标检测（IVOD）中模态数据不完整的问题，并在完整和不完整模态场景下均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有IVOD模型在面对模态数据不完整（特别是主导模态缺失）时性能显著下降，影响全天候应用。研究者希望从架构兼容性角度深入探讨模态不完整的IVOD问题。

Method: 1. 提出Scarf Neck模块，用于DETR变体，引入模态无关的可变形注意力机制，使检测器能灵活适应单或双模态。2. 设计伪模态丢弃策略来训练Scarf-DETR，充分利用多模态信息，提高其对单/双模态工作模式的兼容性和鲁棒性。3. 构建一个全面的模态不完整IVOD任务基准，评估主导或次要模态缺失的情况。

Result: 所提出的Scarf-DETR不仅在模态缺失场景下表现优异，而且在标准IVOD模态完整基准上也取得了卓越的性能。

Conclusion: Scarf-DETR通过其创新的架构和训练策略，成功解决了IVOD中的模态不完整问题，使其在单、双模态工作模式下均具有高度兼容性和鲁棒性，并在多种场景下均达到领先水平。

Abstract: Infrared and visible object detection (IVOD) is essential for numerous around-the-clock applications. Despite notable advancements, current IVOD models exhibit notable performance declines when confronted with incomplete modality data, particularly if the dominant modality is missing. In this paper, we take a thorough investigation on modality incomplete IVOD problem from an architecture compatibility perspective. Specifically, we propose a plug-and-play Scarf Neck module for DETR variants, which introduces a modality-agnostic deformable attention mechanism to enable the IVOD detector to flexibly adapt to any single or double modalities during training and inference. When training Scarf-DETR, we design a pseudo modality dropout strategy to fully utilize the multi-modality information, making the detector compatible and robust to both working modes of single and double modalities. Moreover, we introduce a comprehensive benchmark for the modality-incomplete IVOD task aimed at thoroughly assessing situations where the absent modality is either dominant or secondary. Our proposed Scarf-DETR not only performs excellently in missing modality scenarios but also achieves superior performances on the standard IVOD modality complete benchmarks. Our code will be available at https://github.com/YinghuiXing/Scarf-DETR.

</details>


### [154] [Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT](https://arxiv.org/abs/2511.06625)
*Yifei Zhang,Jiashuo Zhang,Xiaofeng Yang,Liang Zhao*

Main category: cs.CV

TL;DR: 该研究提出了一个可解释的跨疾病推理框架，利用低剂量胸部CT（LDCT）同时对肺部和心血管健康进行联合评估，提供准确且生理学上合理的风险预测及解释。


<details>
  <summary>Details</summary>
Motivation: LDCT同时捕获肺部和心脏结构，为联合评估提供了独特机会。然而，现有方法通常将这两个领域视为独立任务，忽视了它们之间的生理相互作用和共享影像生物标志物。研究旨在弥合图像预测与基于机制的医学解释之间的鸿沟。

Method: 提出一个可解释的跨疾病推理框架，模拟临床诊断思维。该框架包含三个协同组件：肺部感知模块（总结肺部异常）、知识引导推理模块（推断心血管影响）和心脏表征模块（编码结构性生物标志物）。这些模块的输出被融合，以产生整体的心血管风险预测。

Result: 在NLST队列上的实验表明，该框架在CVD筛查和死亡率预测方面达到了最先进的性能，优于单一疾病和纯基于图像的基线方法。此外，该框架提供了与心脏病学理解一致的、可供人类验证的推理，揭示了肺部异常与心脏应激机制之间的内在联系。

Conclusion: 这项工作为LDCT的心血管分析建立了一个统一且可解释的范式，弥合了基于图像的预测与基于机制的医学解释之间的差距。

Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation.

</details>


### [155] [Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models](https://arxiv.org/abs/2511.06490)
*Yule Chen,Yufan Ren,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 该研究引入了AI4VA-FG，一个针对视觉语言模型（VLM）漫画理解的细粒度基准，揭示了现有模型在漫画理解方面的显著不足，并提出了包括区域感知强化学习（RARL）在内的后训练策略，以显著提升VLM在该领域的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在自然图像上表现出色，但它们在处理漫画中风格化线稿、拟声词和密集多面板布局时面临巨大挑战。现有研究缺乏细粒度且全面的VLM漫画理解基准，且模型在该领域的性能存在显著差距。

Method: 研究方法包括：1) 引入AI4VA-FG，一个从基础识别到高级叙事构建的细粒度漫画理解基准，并提供密集标注。2) 评估了GPT-4o、Gemini-2.5等专有模型和Qwen2.5-VL等开源模型的性能。3) 系统研究了后训练策略，包括基于解决方案的监督微调（SFT-S）、基于推理轨迹的监督微调（SFT-R）和强化学习（RL）。4) 提出了受“图像思考”范式启发的区域感知强化学习（RARL），通过动态关注相关区域的缩放操作来训练模型。

Result: 研究结果显示：1) 现有最先进模型在AI4VA-FG基准的核心任务上表现出显著的性能缺陷，表明漫画理解仍是一个未解决的挑战。2) 当应用于Qwen2.5-VL模型时，强化学习（RL）和区域感知强化学习（RARL）在低级实体识别和高级故事情节排序方面取得了显著的性能提升。

Conclusion: 漫画理解对视觉语言模型来说仍然是一个严峻的挑战。本研究提出的AI4VA-FG基准揭示了现有模型的不足，而系统性后训练策略，特别是区域感知强化学习（RARL），为提升VLM在漫画领域的准确性和效率开辟了新途径。

Abstract: Complex visual narratives, such as comics, present a significant challenge to Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often struggle with stylized line art, onomatopoeia, and densely packed multi-panel layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and comprehensive benchmark for VLM-based comic understanding. It spans tasks from foundational recognition and detection to high-level character reasoning and narrative construction, supported by dense annotations for characters, poses, and depth. Beyond that, we evaluate state-of-the-art proprietary models, including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL, revealing substantial performance deficits across core tasks of our benchmarks and underscoring that comic understanding remains an unsolved challenge. To enhance VLMs' capabilities in this domain, we systematically investigate post-training strategies, including supervised fine-tuning on solutions (SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL) for VLMs, which trains models to dynamically attend to relevant regions through zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL and RARL yield significant gains in low-level entity recognition and high-level storyline ordering, paving the way for more accurate and efficient VLM applications in the comics domain.

</details>


### [156] [Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models](https://arxiv.org/abs/2511.06201)
*Rodrigo Gallardo,Oz Fishman,Alexander Htet Kyaw*

Main category: cs.CV

TL;DR: 本文提出一个结合生成式AI和人工干预的计算机视觉框架，用于在公共空间中提出微观设计干预措施，以支持持续的本地参与。


<details>
  <summary>Details</summary>
Motivation: 研究旨在超越自上而下的总体规划，通过将设计选择根植于日常模式和生活经验，支持更持续、本地化的参与。

Method: 该系统使用Grounding DINO和ADE20K数据集（作为城市建成环境的代理）来检测城市物体，并构建共现嵌入以揭示常见的空间配置。用户可收到五个统计上可能与选定锚定物互补的建议。随后，一个视觉语言模型会根据场景图像和选定的物体对，推断并建议第三个物体，以完成更复杂的城市策略。整个工作流程保持人工对选择和细化的控制。

Result: 系统能够检测城市物体，揭示常见的空间配置，并向用户推荐统计上可能互补的物体。通过视觉语言模型的推理，系统还能建议完成更复杂城市策略的第三个物体，从而支持微观尺度的设计干预。

Conclusion: 该框架通过将选择和细化的控制权交予用户，并基于日常模式和生活经验进行决策，成功地超越了传统的自上而下规划模式，促进了更具参与性和接地气的城市设计。

Abstract: This paper introduces a human-in-the-loop computer vision framework that uses generative AI to propose micro-scale design interventions in public space and support more continuous, local participation. Using Grounding DINO and a curated subset of the ADE20K dataset as a proxy for the urban built environment, the system detects urban objects and builds co-occurrence embeddings that reveal common spatial configurations. From this analysis, the user receives five statistically likely complements to a chosen anchor object. A vision language model then reasons over the scene image and the selected pair to suggest a third object that completes a more complex urban tactic. The workflow keeps people in control of selection and refinement and aims to move beyond top-down master planning by grounding choices in everyday patterns and lived experience.

</details>


### [157] [Sim4Seg: Boosting Multimodal Multi-disease Medical Diagnosis Segmentation with Region-Aware Vision-Language Similarity Masks](https://arxiv.org/abs/2511.06665)
*Lingran Song,Yucheng Zhou,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出医学诊断分割（MDS）任务，旨在结合医学图像分割和诊断，并引入M3DS数据集和Sim4Seg框架，通过区域感知视觉-语言相似性模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割模型很少联合探索分割和诊断任务。然而，对于患者来说，模型能够提供可解释的诊断结果以及医学分割结果至关重要。

Method: 本文提出医学诊断分割（MDS）任务，并构建了多模态多疾病医学诊断分割（M3DS）数据集，该数据集包含多模态多疾病医学图像、分割掩码和诊断思维链，通过自动化诊断思维链生成流程创建。此外，提出Sim4Seg框架，利用区域感知视觉-语言相似性到掩码（RVLS2M）模块来提高诊断分割性能。还研究了MDS任务的测试时缩放策略。

Result: 实验结果表明，该方法在分割和诊断方面均优于基线模型。

Conclusion: 本文提出的方法有效地解决了医学诊断分割任务，通过新数据集和Sim4Seg框架实现了卓越的分割和诊断性能，为医学图像分析提供了可解释的诊断能力。

Abstract: Despite significant progress in pixel-level medical image analysis, existing medical image segmentation models rarely explore medical segmentation and diagnosis tasks jointly. However, it is crucial for patients that models can provide explainable diagnoses along with medical segmentation results. In this paper, we introduce a medical vision-language task named Medical Diagnosis Segmentation (MDS), which aims to understand clinical queries for medical images and generate the corresponding segmentation masks as well as diagnostic results. To facilitate this task, we first present the Multimodal Multi-disease Medical Diagnosis Segmentation (M3DS) dataset, containing diverse multimodal multi-disease medical images paired with their corresponding segmentation masks and diagnosis chain-of-thought, created via an automated diagnosis chain-of-thought generation pipeline. Moreover, we propose Sim4Seg, a novel framework that improves the performance of diagnosis segmentation by taking advantage of the Region-Aware Vision-Language Similarity to Mask (RVLS2M) module. To improve overall performance, we investigate a test-time scaling strategy for MDS tasks. Experimental results demonstrate that our method outperforms the baselines in both segmentation and diagnosis.

</details>


### [158] [NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling](https://arxiv.org/abs/2511.06194)
*Muhammad Usama,Mohammad Sadil Khan,Didier Stricker,Muhammad Zeshan Afzal*

Main category: cs.CV

TL;DR: NURBGen是首个直接从文本生成高保真、可编辑NURBS 3D CAD模型的框架，通过微调LLM将文本转换为NURBS参数JSON，并引入混合表示和新的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文本到CAD系统生成的是网格模型或依赖稀缺的设计历史数据，导致模型不可编辑。研究旨在直接生成可编辑的3D CAD模型。

Method: 该研究微调了一个大型语言模型（LLM），将自由文本翻译成包含NURBS曲面参数（控制点、节点向量、次数、有理权重）的JSON表示，然后用Python将其转换为BRep格式。此外，提出了一种结合未修剪NURBS和解析基元的混合表示，以更鲁棒地处理修剪曲面和退化区域，并降低token复杂性。还引入了partABC数据集，它是ABC数据集的精选子集，包含单个CAD组件并附有详细自动标注的标题。

Result: NURBGen在各种提示下表现出强大的性能，在几何保真度和尺寸精度方面超越了现有方法，并得到了专家评估的证实。

Conclusion: NURBGen成功实现了从文本直接生成高保真、可编辑的NURBS 3D CAD模型，解决了现有方法的局限性，并有望公开代码和数据集。

Abstract: Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (\textit{i.e}, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.

</details>


### [159] [Temporal-Guided Visual Foundation Models for Event-Based Vision](https://arxiv.org/abs/2511.06238)
*Ruihao Xia,Junhong Cai,Luziwei Leng,Liuyi Wang,Chengju Liu,Ran Cheng,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出TGVFM框架，通过引入时序上下文融合模块，将预训练的图像视觉基础模型（VFMs）应用于事件相机数据处理，以解决异步事件流处理难题并提升各项视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机在复杂环境下具有独特优势，但处理异步事件流仍是挑战。现有方法依赖特定架构或资源密集型训练，而利用预训练的图像VFMs处理事件数据的潜力尚未充分挖掘。

Method: TGVFM框架无缝整合了VFMs和作者提出的时序上下文融合模块。该模块包含三个关键组件：(1) 长程时序注意力（Long-Range Temporal Attention）用于建模全局时序依赖；(2) 双时空注意力（Dual Spatiotemporal Attention）用于多尺度帧关联；(3) 深度特征引导机制（Deep Feature Guidance Mechanism）用于融合语义-时序特征。通过在真实世界数据上重新训练事件到视频模型并利用基于Transformer的VFMs，TGVFM在利用预训练表示的同时保留了时空动态。

Result: 实验证明，TGVFM在语义分割、深度估计和目标检测任务上均达到了最先进的性能（SoTA），相比现有方法分别提升了16%、21%和16%。

Conclusion: 这项工作成功地利用图像VFMs的跨模态潜力，结合时序推理能力，为基于事件的视觉任务提供了新的解决方案。

Abstract: Event cameras offer unique advantages for vision tasks in challenging environments, yet processing asynchronous event streams remains an open challenge. While existing methods rely on specialized architectures or resource-intensive training, the potential of leveraging modern Visual Foundation Models (VFMs) pretrained on image data remains under-explored for event-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), a novel framework that integrates VFMs with our temporal context fusion block seamlessly to bridge this gap. Our temporal block introduces three key components: (1) Long-Range Temporal Attention to model global temporal dependencies, (2) Dual Spatiotemporal Attention for multi-scale frame correlation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporal features. By retraining event-to-video models on real-world data and leveraging transformer-based VFMs, TGVFM preserves spatiotemporal dynamics while harnessing pretrained representations. Experiments demonstrate SoTA performance across semantic segmentation, depth estimation, and object detection, with improvements of 16%, 21%, and 16% over existing methods, respectively. Overall, this work unlocks the cross-modality potential of image-based VFMs for event-based vision with temporal reasoning. Code is available at https://github.com/XiaRho/TGVFM.

</details>


### [160] [Active Learning for Animal Re-Identification with Ambiguity-Aware Sampling](https://arxiv.org/abs/2511.06658)
*Depanshu Sani,Mehar Khurana,Saket Anand*

Main category: cs.CV

TL;DR: 动物Re-ID因其高影响力但挑战性大而受到关注，现有基础模型在零样本Re-ID上存在性能差距。本文提出一种新颖的主动学习（AL）框架，利用互补聚类和约束来识别信息丰富的样本对，通过极少量标注显著提升动物Re-ID性能，超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 动物Re-ID对生物多样性监测至关重要，但面临细微模式、新物种处理和开放集性质等挑战。尽管基础模型已引入零样本Re-ID，但其性能存在显著差距。现有无监督（USL）和AL方法在动物Re-ID中表现不佳，且全面标注费时费力并需要专业知识。

Method: 引入一种新颖的AL Re-ID框架。该框架利用互补聚类方法来发现嵌入空间中结构模糊的区域，从而挖掘既信息丰富又具有广泛代表性的样本对。通过神谕反馈，以必连（must-link）和非连（cannot-link）约束的形式，实现简单的标注界面。这些约束通过提出的约束聚类细化算法自然地与现有USL方法集成。

Result: 仅使用0.033%的总标注，该方法在13个野生动物数据集上始终优于现有基础模型、USL和AL基线。相对于基础模型、USL和AL方法，mAP平均提升分别为10.49%、11.19%和3.99%，并在每个数据集上达到最先进的性能。此外，在开放世界设置中，对未知个体也显示出11.09%、8.2%和2.06%的改进。

Conclusion: 所提出的AL Re-ID框架通过利用互补聚类和约束，仅需极少量标注即可显著提升动物Re-ID的性能，有效解决了现有方法的局限性，尤其在已知和未知物种的识别上均表现出色，具有高度实用性。

Abstract: Animal Re-ID has recently gained substantial attention in the AI research community due to its high impact on biodiversity monitoring and unique research challenges arising from environmental factors. The subtle distinguishing patterns, handling new species and the inherent open-set nature make the problem even harder. To address these complexities, foundation models trained on labeled, large-scale and multi-species animal Re-ID datasets have recently been introduced to enable zero-shot Re-ID. However, our benchmarking reveals significant gaps in their zero-shot Re-ID performance for both known and unknown species. While this highlights the need for collecting labeled data in new domains, exhaustive annotation for Re-ID is laborious and requires domain expertise. Our analyses show that existing unsupervised (USL) and AL Re-ID methods underperform for animal Re-ID. To address these limitations, we introduce a novel AL Re-ID framework that leverages complementary clustering methods to uncover and target structurally ambiguous regions in the embedding space for mining pairs of samples that are both informative and broadly representative. Oracle feedback on these pairs, in the form of must-link and cannot-link constraints, facilitates a simple annotation interface, which naturally integrates with existing USL methods through our proposed constrained clustering refinement algorithm. Through extensive experiments, we demonstrate that, by utilizing only 0.033% of all annotations, our approach consistently outperforms existing foundational, USL and AL baselines. Specifically, we report an average improvement of 10.49%, 11.19% and 3.99% (mAP) on 13 wildlife datasets over foundational, USL and AL methods, respectively, while attaining state-of-the-art performance on each dataset. Furthermore, we also show an improvement of 11.09%, 8.2% and 2.06% for unknown individuals in an open-world setting.

</details>


### [161] [MoRA: Missing Modality Low-Rank Adaptation for Visual Recognition](https://arxiv.org/abs/2511.06225)
*Shu Zhao,Nilesh Ahuja,Tan Yu,Tianyi Shen,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: MoRA是一种参数高效的微调方法，用于解决预训练视觉语言模型在模态缺失场景下的视觉识别问题，它通过显式建模跨模态交互，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型通常假定训练和推理时多模态输入完整，但在现实世界中，由于隐私、收集困难或资源限制，模态可能缺失。现有方法（如提示学习）未能有效捕捉跨模态关系，并存在计算开销大的问题。

Method: 本文提出了MoRA，一种参数高效的微调方法。MoRA在文本和视觉编码器之间引入模态通用参数，实现双向知识迁移，同时结合模态特定参数，使骨干模型能维持模态间交互和模态内灵活性。

Result: MoRA在模态缺失场景下平均性能提升5.24%，推理时间仅为SOTA方法的25.90%，所需可训练参数仅为全微调的0.11%。

Conclusion: MoRA是一种有效且参数高效的微调方法，能够显著提升预训练视觉语言模型在模态缺失场景下的性能，并在计算效率上超越现有SOTA方法。

Abstract: Pre-trained vision language models have shown remarkable performance on visual recognition tasks, but they typically assume the availability of complete multimodal inputs during both training and inference. In real-world scenarios, however, modalities may be missing due to privacy constraints, collection difficulties, or resource limitations. While previous approaches have addressed this challenge using prompt learning techniques, they fail to capture the cross-modal relationships necessary for effective multimodal visual recognition and suffer from inevitable computational overhead. In this paper, we introduce MoRA, a parameter-efficient fine-tuning method that explicitly models cross-modal interactions while maintaining modality-specific adaptations. MoRA introduces modality-common parameters between text and vision encoders, enabling bidirectional knowledge transfer. Additionally, combined with the modality-specific parameters, MoRA allows the backbone model to maintain inter-modality interaction and enable intra-modality flexibility. Extensive experiments on standard benchmarks demonstrate that MoRA achieves an average performance improvement in missing-modality scenarios by 5.24% and uses only 25.90% of the inference time compared to the SOTA method while requiring only 0.11% of trainable parameters compared to full fine-tuning.

</details>


### [162] [Physics-Informed Image Restoration via Progressive PDE Integration](https://arxiv.org/abs/2511.06244)
*Shamika Likhite,Santiago López-Tapia,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 本文提出了一种渐进式训练框架，将物理信息偏微分方程（PDE）动力学融入深度学习去模糊模型中，以更好地捕捉运动模糊的长程空间依赖性和方向流。该方法在实现卓越恢复质量的同时，仅带来极小的计算开销。


<details>
  <summary>Details</summary>
Motivation: 运动模糊严重降低图像质量并影响下游计算机视觉任务。现有的深度学习去模糊方法难以捕捉运动模糊固有的长程空间依赖性，因为传统卷积的感受野有限，需要极深的网络才能模拟全局空间关系。这促使研究者寻求结合物理先验的替代方法。

Method: 本文提出一个渐进式训练框架，将物理信息PDE动力学（特别是对流-扩散方程）集成到最先进的图像恢复架构中。通过利用这些方程模拟特征演化，该方法自然地捕捉了运动模糊的方向流动特性，并实现了原则性的全局空间建模。

Result: PDE增强的去模糊模型实现了卓越的恢复质量，且开销极小（推理GMACs仅增加约1%），并在多种最先进的架构上持续改善感知质量。在标准运动去模糊基准上的综合实验表明，该物理信息方法显著提高了四种不同架构（包括FFTformer、NAFNet、Restormer和Stripformer）的PSNR和SSIM。

Conclusion: 通过基于PDE的全局层融入数学物理原理可以增强基于深度学习的图像恢复，这为计算机视觉应用中的物理信息神经网络设计开辟了一个有前景的方向。

Abstract: Motion blur, caused by relative movement between camera and scene during exposure, significantly degrades image quality and impairs downstream computer vision tasks such as object detection, tracking, and recognition in dynamic environments. While deep learning-based motion deblurring methods have achieved remarkable progress, existing approaches face fundamental challenges in capturing the long-range spatial dependencies inherent in motion blur patterns. Traditional convolutional methods rely on limited receptive fields and require extremely deep networks to model global spatial relationships. These limitations motivate the need for alternative approaches that incorporate physical priors to guide feature evolution during restoration. In this paper, we propose a progressive training framework that integrates physics-informed PDE dynamics into state-of-the-art restoration architectures. By leveraging advection-diffusion equations to model feature evolution, our approach naturally captures the directional flow characteristics of motion blur while enabling principled global spatial modeling. Our PDE-enhanced deblurring models achieve superior restoration quality with minimal overhead, adding only approximately 1\% to inference GMACs while providing consistent improvements in perceptual quality across multiple state-of-the-art architectures. Comprehensive experiments on standard motion deblurring benchmarks demonstrate that our physics-informed approach improves PSNR and SSIM significantly across four diverse architectures, including FFTformer, NAFNet, Restormer, and Stripformer. These results validate that incorporating mathematical physics principles through PDE-based global layers can enhance deep learning-based image restoration, establishing a promising direction for physics-informed neural network design in computer vision applications.

</details>


### [163] [Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View](https://arxiv.org/abs/2511.06722)
*Jianyu Qi,Ding Zou,Wenrui Yan,Rui Ma,Jiaxu Li,Zhijie Zheng,Zhiguo Yang,Rongchang Zhao*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型（MLLMs）在链式思考（CoT）推理中RL后训练范式存在的样本难度量化和联合优化问题，提出了PISM和CMAB两种难度感知采样策略，并设计了分层训练框架。实验证明，对难度分层样本应用GRPO能持续优于传统SFT+GRPO，且可避免监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（RL）的MLLMs后训练范式主要集中在数学数据集上，但存在两个关键缺陷：1. 缺乏可量化、能战略性筛选样本进行优化的难度指标；2. 未能联合优化模型的感知和推理能力。

Method: 本文提出两种新颖的难度感知采样策略：1. 渐进式图像语义遮蔽（PISM），通过系统性图像降级量化样本难度；2. 跨模态注意力平衡（CMAB），通过分析注意力分布评估跨模态交互复杂性。利用这些指标，设计了一个分层训练框架，融合了纯GRPO和SFT+GRPO混合训练范式，并在六个基准数据集上进行了评估。

Result: 实验结果表明，与传统的SFT+GRPO流水线相比，将GRPO应用于难度分层样本持续表现出优越性。这表明战略性数据采样可以在提高模型准确性的同时，避免对监督微调的需求。

Conclusion: 战略性地利用难度感知采样（如PISM和CMAB）来筛选数据，并结合强化学习后训练（如GRPO），能够有效优化多模态大语言模型的推理能力，甚至可能取代传统的监督微调，从而提高模型性能。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.

</details>


### [164] [MirrorMamba: Towards Scalable and Robust Mirror Detection in Videos](https://arxiv.org/abs/2511.06716)
*Rui Song,Jiaying Lin,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出了一种名为MirrorMamba的新型视频镜面检测方法，它结合了多线索（深度、对应性、光流）并首次成功应用基于Mamba的架构，解决了现有方法的性能和鲁棒性限制，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频镜面检测方法性能和鲁棒性有限，常过度依赖单一不可靠的动态特征，并且基于感受野有限的CNN或计算复杂度为二次方的Transformer。此外，模糊的深度图会导致边界不清晰。

Method: 本文提出了MirrorMamba方法。它融合了感知深度、对应性和光流等多种线索。引入了基于Mamba的多方向对应性提取器，利用Mamba空间状态模型的全局感受野和线性复杂度捕捉对应性。此外，设计了一个基于Mamba的逐层边界增强解码器，以解决模糊深度图导致的边界不清晰问题。这是Mamba架构首次成功应用于镜面检测领域。

Result: 广泛实验表明，MirrorMamba在基准数据集上超越了现有最先进的视频镜面检测方法。在最具挑战性和代表性的基于图像的镜面检测数据集上，该方法也达到了最先进的性能，证明了其鲁棒性和泛化能力。

Conclusion: MirrorMamba通过融合多线索和创新性地应用基于Mamba的架构，有效解决了视频镜面检测的现有问题，并在多个数据集上取得了最先进的性能，展示了Mamba在镜面检测领域的巨大潜力。

Abstract: Video mirror detection has received significant research attention, yet existing methods suffer from limited performance and robustness. These approaches often over-rely on single, unreliable dynamic features, and are typically built on CNNs with limited receptive fields or Transformers with quadratic computational complexity. To address these limitations, we propose a new effective and scalable video mirror detection method, called MirrorMamba. Our approach leverages multiple cues to adapt to diverse conditions, incorporating perceived depth, correspondence and optical. We also introduce an innovative Mamba-based Multidirection Correspondence Extractor, which benefits from the global receptive field and linear complexity of the emerging Mamba spatial state model to effectively capture correspondence properties. Additionally, we design a Mamba-based layer-wise boundary enforcement decoder to resolve the unclear boundary caused by the blurred depth map. Notably, this work marks the first successful application of the Mamba-based architecture in the field of mirror detection. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches for video mirror detection on the benchmark datasets. Furthermore, on the most challenging and representative image-based mirror detection dataset, our approach achieves state-of-the-art performance, proving its robustness and generalizability.

</details>


### [165] [Gait Recognition via Collaborating Discriminative and Generative Diffusion Models](https://arxiv.org/abs/2511.06245)
*Haijun Xiong,Bin Feng,Bang Wang,Xinggang Wang,Wenyu Liu*

Main category: cs.CV

TL;DR: 本文提出CoD$^2$框架，结合扩散模型和判别模型的优势，通过多级条件控制策略提取鲁棒的步态特征，并在多个数据集上实现了最先进的步态识别性能。


<details>
  <summary>Details</summary>
Motivation: 步态识别是一种非侵入式生物识别方案，但生成模型（特别是扩散模型）在步态特征提取方面的潜力尚未被充分探索，而判别模型虽已成功但仍有提升空间。

Method: 引入CoD$^2$框架，融合扩散模型的数据分布建模能力与判别模型的语义表示学习能力。提出多级条件控制策略：高层条件（由判别提取器提取的身份感知语义）指导生成身份一致的步态序列；低层视觉细节（外观、运动）用于增强一致性。生成的序列反过来促进判别提取器学习更全面的高层语义特征。

Result: 在SUSTech1K、CCPG、GREW和Gait3D四个数据集上进行了广泛实验，CoD$^2$取得了最先进的性能，并且可以无缝集成到现有判别方法中，带来持续的性能提升。

Conclusion: CoD$^2$框架通过结合生成模型和判别模型的优势，有效提取了鲁棒的步态特征，显著提升了步态识别的性能，并为该领域提供了新的研究方向。

Abstract: Gait recognition offers a non-intrusive biometric solution by identifying individuals through their walking patterns. Although discriminative models have achieved notable success in this domain, the full potential of generative models remains largely underexplored. In this paper, we introduce \textbf{CoD$^2$}, a novel framework that combines the data distribution modeling capabilities of diffusion models with the semantic representation learning strengths of discriminative models to extract robust gait features. We propose a Multi-level Conditional Control strategy that incorporates both high-level identity-aware semantic conditions and low-level visual details. Specifically, the high-level condition, extracted by the discriminative extractor, guides the generation of identity-consistent gait sequences, whereas low-level visual details, such as appearance and motion, are preserved to enhance consistency. Furthermore, the generated sequences facilitate the discriminative extractor's learning, enabling it to capture more comprehensive high-level semantic features. Extensive experiments on four datasets (SUSTech1K, CCPG, GREW, and Gait3D) demonstrate that CoD$^2$ achieves state-of-the-art performance and can be seamlessly integrated with existing discriminative methods, yielding consistent improvements.

</details>


### [166] [AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving](https://arxiv.org/abs/2511.06253)
*Ruifei Zhang,Junlin Xie,Wei Zhang,Weikai Chen,Xiao Tan,Xiang Wan,Guanbin Li*

Main category: cs.CV

TL;DR: AdaDrive是一个自适应的快慢框架，它优化了大型语言模型（LLM）在自动驾驶中何时以及如何参与决策，以平衡高层推理和实时效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在自动驾驶中的集成方案存在问题：要么激活过于频繁导致计算开销大，要么采用固定调度无法适应动态驾驶条件。

Method: AdaDrive提出了一种自适应协同的快慢框架：1) 采用新颖的自适应激活损失和比较学习机制，动态决定在复杂或关键场景下激活LLM。2) 引入自适应融合策略，根据场景复杂度和预测置信度，以连续、可调节的LLM影响力与传统规划器协同，而非僵硬的二元激活。

Result: 在基于语言的自动驾驶基准测试中，AdaDrive在驾驶精度和计算效率方面均达到了最先进的性能。

Conclusion: AdaDrive提供了一个灵活、上下文感知的框架，能够在不牺牲实时性能的前提下最大化决策准确性。

Abstract: Effectively integrating Large Language Models (LLMs) into autonomous driving requires a balance between leveraging high-level reasoning and maintaining real-time efficiency. Existing approaches either activate LLMs too frequently, causing excessive computational overhead, or use fixed schedules, failing to adapt to dynamic driving conditions. To address these challenges, we propose AdaDrive, an adaptively collaborative slow-fast framework that optimally determines when and how LLMs contribute to decision-making. (1) When to activate the LLM: AdaDrive employs a novel adaptive activation loss that dynamically determines LLM invocation based on a comparative learning mechanism, ensuring activation only in complex or critical scenarios. (2) How to integrate LLM assistance: Instead of rigid binary activation, AdaDrive introduces an adaptive fusion strategy that modulates a continuous, scaled LLM influence based on scene complexity and prediction confidence, ensuring seamless collaboration with conventional planners. Through these strategies, AdaDrive provides a flexible, context-aware framework that maximizes decision accuracy without compromising real-time performance. Extensive experiments on language-grounded autonomous driving benchmarks demonstrate that AdaDrive state-of-the-art performance in terms of both driving accuracy and computational efficiency. Code is available at https://github.com/ReaFly/AdaDrive.

</details>


### [167] [TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning](https://arxiv.org/abs/2511.06817)
*Rui Wang,Ying Zhou,Hao Wang,Wenwei Zhang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 针对微创手术中稀疏监督下的立体匹配问题，本文提出TiS-TSL框架。该框架通过统一模型进行灵活时间建模，并采用两阶段学习策略：先将稀疏图像级知识转移到时间模型，再通过双向时空一致性细化视频预测，有效减少了伪影并提高了精度。


<details>
  <summary>Details</summary>
Motivation: 微创手术中，由于解剖学限制，难以获得密集视差监督，通常只有稀疏的图像级标签。现有师生学习（TSL）方法仅限于图像级监督，缺乏时空一致性估计，导致视频视差预测不稳定和严重的闪烁伪影。

Method: 本文提出TiS-TSL，一个新颖的时间可切换师生学习框架。其核心是一个统一模型，可在图像预测（IP）、前向视频预测（FVP）和后向视频预测（BVP）三种模式下运行，实现灵活的时间建模。学习策略分为两阶段：1) 图像到视频（I2V）阶段：将稀疏图像级知识转移以初始化时间建模。2) 视频到视频（V2V）阶段：通过比较前向和后向预测来计算双向时空一致性，从而识别不可靠区域、过滤噪声视频级伪标签并强制执行时间连贯性，以细化时间视差预测。

Result: 在两个公共数据集上的实验结果表明，TiS-TSL超越了其他基于图像的最新技术，将TEPE至少提高了2.11%，EPE至少提高了4.54%。

Conclusion: TiS-TSL框架通过引入时空一致性估计和灵活的时间建模，有效解决了微创手术中稀疏监督下视频立体匹配的挑战，显著提高了视差预测的稳定性和准确性。

Abstract: Stereo matching in minimally invasive surgery (MIS) is essential for next-generation navigation and augmented reality. Yet, dense disparity supervision is nearly impossible due to anatomical constraints, typically limiting annotations to only a few image-level labels acquired before the endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a promising solution by leveraging a teacher trained on sparse labels to generate pseudo labels and associated confidence maps from abundant unlabeled surgical videos. However, existing TSL methods are confined to image-level supervision, providing only spatial confidence and lacking temporal consistency estimation. This absence of spatio-temporal reliability results in unstable disparity predictions and severe flickering artifacts across video frames. To overcome these challenges, we propose TiS-TSL, a novel time-switchable teacher-student learning framework for video stereo matching under minimal supervision. At its core is a unified model that operates in three distinct modes: Image-Prediction (IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP), enabling flexible temporal modeling within a single architecture. Enabled by this unified model, TiS-TSL adopts a two-stage learning strategy. The Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal disparity predictions by comparing forward and backward predictions to calculate bidirectional spatio-temporal consistency. This consistency identifies unreliable regions across frames, filters noisy video-level pseudo labels, and enforces temporal coherence. Experimental results on two public datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts by improving TEPE and EPE by at least 2.11% and 4.54%, respectively..

</details>


### [168] [NeuroBridge: Bio-Inspired Self-Supervised EEG-to-Image Decoding via Cognitive Priors and Bidirectional Semantic Alignment](https://arxiv.org/abs/2511.06836)
*Wenjiang Zhang,Sifeng Wang,Yuwei Su,Xinyu Li,Chen Zhang,Suyu Zhong*

Main category: cs.CV

TL;DR: NeuroBridge是一种新型自监督架构，通过认知先验增强（CPA）和共享语义投影器（SSP）解决视觉神经解码中数据稀缺和语义不匹配问题，显著提升了从脑活动模式重建视觉刺激的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉神经解码方法受限于高质量刺激-脑响应数据对的稀缺性，以及神经表征与视觉内容之间固有的语义不匹配问题。

Method: NeuroBridge提出了一种自监督架构，结合了认知先验增强（CPA）和共享语义投影器（SSP）。CPA通过对EEG信号和图像应用非对称、模态特定的变换来模拟感知变异性，增强语义多样性。SSP通过协同适应策略建立双向对齐过程，将两种模态的特征相互对齐到一个共享语义空间中，以实现有效的跨模态学习。

Result: NeuroBridge在单受试者和跨受试者设置下均超越了现有最先进方法。在单受试者场景中，它在200类零样本检索任务中，top-1准确率提高了12.3%（达到63.2%），top-5准确率提高了10.2%（达到89.9%）。

Conclusion: 实验结果证明了所提出框架在神经视觉解码方面的有效性、鲁棒性和可扩展性。

Abstract: Visual neural decoding seeks to reconstruct or infer perceived visual stimuli from brain activity patterns, providing critical insights into human cognition and enabling transformative applications in brain-computer interfaces and artificial intelligence. Current approaches, however, remain constrained by the scarcity of high-quality stimulus-brain response pairs and the inherent semantic mismatch between neural representations and visual content. Inspired by perceptual variability and co-adaptive strategy of the biological systems, we propose a novel self-supervised architecture, named NeuroBridge, which integrates Cognitive Prior Augmentation (CPA) with Shared Semantic Projector (SSP) to promote effective cross-modality alignment. Specifically, CPA simulates perceptual variability by applying asymmetric, modality-specific transformations to both EEG signals and images, enhancing semantic diversity. Unlike previous approaches, SSP establishes a bidirectional alignment process through a co-adaptive strategy, which mutually aligns features from two modalities into a shared semantic space for effective cross-modal learning. NeuroBridge surpasses previous state-of-the-art methods under both intra-subject and inter-subject settings. In the intra-subject scenario, it achieves the improvements of 12.3% in top-1 accuracy and 10.2% in top-5 accuracy, reaching 63.2% and 89.9% respectively on a 200-way zero-shot retrieval task. Extensive experiments demonstrate the effectiveness, robustness, and scalability of the proposed framework for neural visual decoding.

</details>


### [169] [PlantTraitNet: An Uncertainty-Aware Multimodal Framework for Global-Scale Plant Trait Inference from Citizen Science Data](https://arxiv.org/abs/2511.06943)
*Ayushi Sharma,Johanna Trost,Daniel Lusk,Johannes Dollinger,Julian Schrader,Christian Rossi,Javier Lopatin,Etienne Laliberté,Simon Haberstroh,Jana Eichel,Daniel Mederer,Jose Miguel Cerda-Paredes,Shyam S. Phartyal,Lisa-Maricia Schwarz,Anja Linstädter,Maria Conceição Caldeira,Teja Kattenborn*

Main category: cs.CV

TL;DR: PlantTraitNet利用众包照片和深度学习，生成了比现有产品更准确的全球植物性状地图，解决了传统测量成本高、覆盖范围有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的全球植物性状地图受限于野外测量的高成本和稀疏地理覆盖。众包科学（如带地理标签的植物照片）作为一种未充分利用的资源，有望克服这些局限性。

Method: 本研究引入了PlantTraitNet，一个多模态、多任务、不确定性感知的深度学习框架。该框架利用弱监督从众包照片中预测四种关键植物性状（株高、叶面积、比叶面积和氮含量），并通过空间聚合生成全球性状分布图。

Result: PlantTraitNet在所有评估的性状上均持续优于现有领先的全球性状产品。这表明，当众包图像与计算机视觉和地理空间AI结合时，不仅能实现可扩展的全球性状制图，而且能提供更高的准确性。

Conclusion: 该方法为生态研究和地球系统建模提供了一条强大的新途径，证明了众包图像结合计算机视觉和地理空间AI，能够实现更具扩展性和准确性的全球性状制图。

Abstract: Global plant maps of plant traits, such as leaf nitrogen or plant height, are essential for understanding ecosystem processes, including the carbon and energy cycles of the Earth system. However, existing trait maps remain limited by the high cost and sparse geographic coverage of field-based measurements. Citizen science initiatives offer a largely untapped resource to overcome these limitations, with over 50 million geotagged plant photographs worldwide capturing valuable visual information on plant morphology and physiology. In this study, we introduce PlantTraitNet, a multi-modal, multi-task uncertainty-aware deep learning framework that predictsfour key plant traits (plant height, leaf area, specific leaf area, and nitrogen content) from citizen science photos using weak supervision. By aggregating individual trait predictions across space, we generate global maps of trait distributions. We validate these maps against independent vegetation survey data (sPlotOpen) and benchmark them against leading global trait products. Our results show that PlantTraitNet consistently outperforms existing trait maps across all evaluated traits, demonstrating that citizen science imagery, when integrated with computer vision and geospatial AI, enables not only scalable but also more accurate global trait mapping. This approach offers a powerful new pathway for ecological research and Earth system modeling.

</details>


### [170] [Robust Nearest Neighbour Retrieval Using Targeted Manifold Manipulation](https://arxiv.org/abs/2511.06261)
*B. Ghosh,H. Harikumar,S. Rana*

Main category: cs.CV

TL;DR: TMM-NN通过在查询图像上添加触发补丁，并利用网络对补丁的响应程度（即样本被“推入”指定特征流形区域的难易程度）来定义近邻，从而实现比传统距离度量更鲁棒的语义近邻检索。


<details>
  <summary>Details</summary>
Motivation: 当前的近邻检索方法过度依赖手工调整特征层和距离度量，效率低下且效果不佳。

Method: TMM-NN通过轻量级的、特定于查询的触发补丁实现。将补丁添加到查询图像中，网络被“弱后门化”，使任何带有该补丁的输入都被引导至一个虚拟类别。与查询相似的图像只需轻微偏移即可高概率地被归类为虚拟类别，而不同的图像则受影响较小。通过对候选样本的置信度进行排序，TMM-NN检索出语义上最相关的近邻。

Result: 鲁棒性分析和基准实验证实，在噪声环境下和各种任务中，这种基于触发器的排序方法优于传统的度量标准。

Conclusion: TMM-NN通过评估样本对目标扰动的响应能力来重新概念化近邻检索，并利用查询特定的触发补丁实现，从而在多种任务和噪声条件下，提供了一种比传统方法更优越的语义相关近邻检索方案。

Abstract: Nearest-neighbour retrieval is central to classification and explainable-AI pipelines, but current practice relies on hand-tuning feature layers and distance metrics. We propose Targeted Manifold Manipulation-Nearest Neighbour (TMM-NN), which reconceptualises retrieval by assessing how readily each sample can be nudged into a designated region of the feature manifold; neighbourhoods are defined by a sample's responsiveness to a targeted perturbation rather than absolute geometric distance. TMM-NN implements this through a lightweight, query-specific trigger patch. The patch is added to the query image, and the network is weakly ``backdoored'' so that any input with the patch is steered toward a dummy class. Images similar to the query need only a slight shift and are classified as the dummy class with high probability, while dissimilar ones are less affected. By ranking candidates by this confidence, TMM-NN retrieves the most semantically related neighbours. Robustness analysis and benchmark experiments confirm this trigger-based ranking outperforms traditional metrics under noise and across diverse tasks.

</details>


### [171] [From Attribution to Action: Jointly ALIGNing Predictions and Explanations](https://arxiv.org/abs/2511.06944)
*Dongsheng Hong,Chao Chen,Yanhui Chen,Shanshan Lin,Zhihao Chen,Xiangwen Liao*

Main category: cs.CV

TL;DR: 本文提出ALIGN框架，通过迭代联合训练分类器和掩码器，生成高质量任务相关掩码以指导模型解释，从而在计算机视觉任务中提高模型性能、泛化能力和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有解释引导学习（EGL）方法依赖外部或启发式分割来监督模型解释，这些监督信号通常噪声大、不精确且难以扩展。低质量的监督信号反而会降低模型性能。

Method: 本文提出ALIGN框架，以迭代方式联合训练分类器和掩码器。掩码器学习生成软性、任务相关的掩码以突出信息区域。分类器则同时优化预测准确性及其显著图与学习到的掩码之间的对齐。

Result: ALIGN在VLCS和Terra Incognita两个域泛化基准测试中，持续优于六个强基线模型，无论是在分布内还是分布外设置。此外，ALIGN在充分性和全面性方面也产生了卓越的解释质量。

Conclusion: ALIGN通过利用高质量掩码作为指导，有效地提高了模型的解释性和泛化能力，并能生成准确且可解释的模型。

Abstract: Explanation-guided learning (EGL) has shown promise in aligning model predictions with interpretable reasoning, particularly in computer vision tasks. However, most approaches rely on external annotations or heuristic-based segmentation to supervise model explanations, which can be noisy, imprecise and difficult to scale. In this work, we provide both empirical and theoretical evidence that low-quality supervision signals can degrade model performance rather than improve it. In response, we propose ALIGN, a novel framework that jointly trains a classifier and a masker in an iterative manner. The masker learns to produce soft, task-relevant masks that highlight informative regions, while the classifier is optimized for both prediction accuracy and alignment between its saliency maps and the learned masks. By leveraging high-quality masks as guidance, ALIGN improves both interpretability and generalizability, showing its superiority across various settings. Experiments on the two domain generalization benchmarks, VLCS and Terra Incognita, show that ALIGN consistently outperforms six strong baselines in both in-distribution and out-of-distribution settings. Besides, ALIGN also yields superior explanation quality concerning sufficiency and comprehensiveness, highlighting its effectiveness in producing accurate and interpretable models.

</details>


### [172] [A Mixture-of-Experts Framework with Log-Logistic Components for Survival Analysis on Histopathology Images](https://arxiv.org/abs/2511.06266)
*Ardhendu Sekhar,Vasu Soni,Keshav Aske,Shivam Madnoorkar,Pranav Jeevan,Amit Sethi*

Main category: cs.CV

TL;DR: 该论文提出了一个模块化框架，通过整合补丁选择、图引导聚类、分层上下文注意力及Log-logistic混合模型，利用全玻片病理图像（WSIs）预测癌症特异性生存期，并超越了现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用全玻片病理图像预测癌症特异性生存期方面可能存在局限性，研究旨在开发一个更有效、更准确的模型来解决这一问题。

Method: 该方法集成了四个模块：(i) 基于分位数阈值的“分位数门控补丁选择”以识别预后信息区域；(ii) 使用k近邻图的“图引导聚类”以捕捉表型异质性；(iii) “分层上下文注意力”以学习簇内和簇间交互；(iv) “专家驱动的Log-logistic混合框架”以估计复杂的生存分布。

Result: 该模型在TCGA LUAD上获得了0.644的C-index，在TCGA KIRC上获得了0.751，在TCGA BRCA上获得了0.752，优于现有的最先进方法。

Conclusion: 所提出的模块化框架能够有效地从全玻片病理图像中预测癌症特异性生存期，并在多个TCGA数据集中取得了优于现有技术的性能。

Abstract: We propose a modular framework for predicting cancer specific survival from whole slide pathology images (WSIs). The method integrates four components: (i) Quantile Gated Patch Selection via quantile based thresholding to isolate prognostically informative tissue regions; (ii) Graph Guided Clustering using a k nearest neighbor graph to capture phenotype level heterogeneity through spatial and morphological coherence; (iii) Hierarchical Context Attention to learn intra and inter cluster interactions; and (iv) an Expert Driven Mixture of Log logistics framework to estimate complex survival distributions using Log logistics distributions. The model attains a concordance index of 0.644 on TCGA LUAD, 0.751 on TCGA KIRC, and 0.752 on TCGA BRCA respectively, outperforming existing state of the art approaches.

</details>


### [173] [VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving](https://arxiv.org/abs/2511.06256)
*Ruifei Zhang,Wei Zhang,Xiao Tan,Sibei Yang,Xiang Wan,Xiaonan Luo,Guanbin Li*

Main category: cs.CV

TL;DR: VLDrive是一种轻量级多模态大语言模型（MLLM），通过增强视觉组件和减少参数，解决了现有LLM自动驾驶中视觉表示不足和部署困难的问题，实现了最先进的驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLMs）的自动驾驶方法面临两大挑战：1) 视觉表示的局限性导致频繁碰撞和障碍，影响驾驶性能；2) LLMs庞大的参数量带来了部署障碍。

Method: 本文提出了VLDrive，采用轻量级MLLM架构和增强的视觉组件。具体方法包括：通过循环一致动态视觉剪枝和记忆增强特征聚合策略生成紧凑的视觉tokens；提出距离解耦指令注意力机制，以改善视觉-语言特征的联合学习，特别是针对长距离视觉tokens。

Result: 在CARLA模拟器中，VLDrive实现了最先进的驾驶性能，同时将参数量减少了81%（从7B降至1.3B）。在闭环评估中，其驾驶分数在微小、短距离和长距离上分别显著提高了15.4%、16.8%和7.6%。

Conclusion: VLDrive通过创新的视觉处理和注意力机制，有效解决了LLM自动驾驶中视觉表示不足和模型部署困难的问题，显著提升了驾驶性能，并大大降低了模型复杂度，为鲁棒和可部署的自动驾驶提供了新途径。

Abstract: Recent advancements in language-grounded autonomous driving have been significantly promoted by the sophisticated cognition and reasoning capabilities of large language models (LLMs). However, current LLM-based approaches encounter critical challenges: (1) Failure analysis reveals that frequent collisions and obstructions, stemming from limitations in visual representations, remain primary obstacles to robust driving performance. (2) The substantial parameters of LLMs pose considerable deployment hurdles. To address these limitations, we introduce VLDrive, a novel approach featuring a lightweight MLLM architecture with enhanced vision components. VLDrive achieves compact visual tokens through innovative strategies, including cycle-consistent dynamic visual pruning and memory-enhanced feature aggregation. Furthermore, we propose a distance-decoupled instruction attention mechanism to improve joint visual-linguistic feature learning, particularly for long-range visual tokens. Extensive experiments conducted in the CARLA simulator demonstrate VLDrive`s effectiveness. Notably, VLDrive achieves state-of-the-art driving performance while reducing parameters by 81% (from 7B to 1.3B), yielding substantial driving score improvements of 15.4%, 16.8%, and 7.6% at tiny, short, and long distances, respectively, in closed-loop evaluations. Code is available at https://github.com/ReaFly/VLDrive.

</details>


### [174] [FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image Manipulation and Detection](https://arxiv.org/abs/2511.06947)
*Yulin Chen,Zeyuan Wang,Tianyuan Yu,Yingmei Wei,Liang Bai*

Main category: cs.CV

TL;DR: 本文提出了FoCLIP，一个基于特征空间错位的框架，旨在欺骗基于CLIP的图像质量评估指标。FoCLIP通过优化图像-文本模态对齐、分数分布平衡和像素保护正则化，能够显著提高CLIPscore，同时保持视觉保真度，并揭示了颜色通道敏感性可用于检测此类欺骗，实现了91%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 基于CLIP的模型因其良好的对齐属性而被广泛应用于图像质量评估，例如CLIPscore。然而，这种基于CLIP的指标对于其精细的多模态对齐非常脆弱，容易受到攻击。

Method: 本文提出了FoCLIP框架，利用随机梯度下降技术构建欺骗性示例。FoCLIP集成了三个关键组件：作为核心模块的特征对齐以减少图像-文本模态差距，分数分布平衡模块和像素保护正则化，共同优化CLIPscore性能和图像质量之间的多模态输出平衡。此外，基于灰度转换导致的特征降级现象，提出了一种颜色通道敏感性驱动的篡改检测机制。

Result: 实验表明，优化后的图像在十个艺术杰作提示和ImageNet子集上能显著提高CLIPscore，同时保持高视觉保真度。研究发现，灰度转换会导致欺骗图像的特征显著降级，引起CLIPscore的明显下降。基于此现象提出的颜色通道敏感性驱动的篡改检测机制在标准基准上实现了91%的准确率。

Conclusion: 这项工作为基于CLIP的多模态系统中的特征错位攻击提供了一条实用途径，并提出了相应的防御方法。

Abstract: The well-aligned attribute of CLIP-based models enables its effective application like CLIPscore as a widely adopted image quality assessment metric. However, such a CLIP-based metric is vulnerable for its delicate multimodal alignment. In this work, we propose \textbf{FoCLIP}, a feature-space misalignment framework for fooling CLIP-based image quality metric. Based on the stochastic gradient descent technique, FoCLIP integrates three key components to construct fooling examples: feature alignment as the core module to reduce image-text modality gaps, the score distribution balance module and pixel-guard regularization, which collectively optimize multimodal output equilibrium between CLIPscore performance and image quality. Such a design can be engineered to maximize the CLIPscore predictions across diverse input prompts, despite exhibiting either visual unrecognizability or semantic incongruence with the corresponding adversarial prompts from human perceptual perspectives. Experiments on ten artistic masterpiece prompts and ImageNet subsets demonstrate that optimized images can achieve significant improvement in CLIPscore while preserving high visual fidelity. In addition, we found that grayscale conversion induces significant feature degradation in fooling images, exhibiting noticeable CLIPscore reduction while preserving statistical consistency with original images. Inspired by this phenomenon, we propose a color channel sensitivity-driven tampering detection mechanism that achieves 91% accuracy on standard benchmarks. In conclusion, this work establishes a practical pathway for feature misalignment in CLIP-based multimodal systems and the corresponding defense method.

</details>


### [175] [LLM-Driven Completeness and Consistency Evaluation for Cultural Heritage Data Augmentation in Cross-Modal Retrieval](https://arxiv.org/abs/2511.06268)
*Jian Zhang,Junyi Guo,Junyi Yuan,Huanda Lu,Yanlin Zhou,Fangyu Wu,Qiufeng Wang,Dongming Lu*

Main category: cs.CV

TL;DR: 本文提出$C^3$框架，通过提升大型语言模型（LLM）生成描述的完整性和一致性，增强跨模态检索性能，尤其是在文化遗产数据领域。


<details>
  <summary>Details</summary>
Motivation: 文化遗产数据的跨模态检索受限于文本描述的不完整或不一致，这源于历史数据丢失和专家标注成本高昂。尽管LLM能丰富文本描述，但其输出常出现幻觉或缺失视觉细节。

Method: $C^3$框架引入了完整性评估模块，利用视觉线索和LLM输出来评估语义覆盖。为解决事实不一致性，该框架构建了一个马尔可夫决策过程（MDP）来监督思维链（Chain-of-Thought）推理，通过自适应查询控制指导一致性评估。

Result: 在文化遗产数据集CulTi和TimeTravel以及通用基准MSCOCO和Flickr30K上的实验表明，$C^3$在微调和零样本设置下均达到了最先进（state-of-the-art）的性能。

Conclusion: $C^3$框架通过提高LLM生成描述的完整性和一致性，有效解决了跨模态检索中的挑战，显著提升了检索效果。

Abstract: Cross-modal retrieval is essential for interpreting cultural heritage data, but its effectiveness is often limited by incomplete or inconsistent textual descriptions, caused by historical data loss and the high cost of expert annotation. While large language models (LLMs) offer a promising solution by enriching textual descriptions, their outputs frequently suffer from hallucinations or miss visually grounded details. To address these challenges, we propose $C^3$, a data augmentation framework that enhances cross-modal retrieval performance by improving the completeness and consistency of LLM-generated descriptions. $C^3$ introduces a completeness evaluation module to assess semantic coverage using both visual cues and language-model outputs. Furthermore, to mitigate factual inconsistencies, we formulate a Markov Decision Process to supervise Chain-of-Thought reasoning, guiding consistency evaluation through adaptive query control. Experiments on the cultural heritage datasets CulTi and TimeTravel, as well as on general benchmarks MSCOCO and Flickr30K, demonstrate that $C^3$ achieves state-of-the-art performance in both fine-tuned and zero-shot settings.

</details>


### [176] [TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding](https://arxiv.org/abs/2511.07007)
*Duc Nguyen,Yan-Ling Lai,Qilin Zhang,Prabin Gyawali,Benedikt Schwab,Olaf Wysocki,Thomas H. Kolbe*

Main category: cs.CV

TL;DR: TrueCity是一个新的城市语义分割基准，提供精确标注的真实世界和模拟点云，用于量化和解决3D场景理解中的模拟到真实域差距问题。


<details>
  <summary>Details</summary>
Motivation: 3D语义场景理解面临真实世界标注数据有限的挑战。现有合成数据集虽可扩展且标注完美，但因缺乏真实世界复杂性和传感器噪声，导致合成到真实的域差距。此外，缺乏同步的真实与模拟点云基准来分析分割任务中的域偏移。

Method: 引入了TrueCity数据集，这是首个包含厘米级精确标注的真实世界点云、语义3D城市模型以及代表同一城市的标注模拟点云的城市语义分割基准。TrueCity的分割类别与国际3D城市建模标准对齐，以实现一致的模拟到真实差距评估。

Result: 通过在通用基线上进行广泛实验，TrueCity量化了域偏移，并突出了利用合成数据增强真实世界3D场景理解的策略。它提供了一个评估模拟到真实差距的统一平台。

Conclusion: TrueCity数据集将促进模拟到真实差距量化方面的进一步发展，并有助于开发可泛化的数据驱动模型，从而提升3D语义场景理解能力。

Abstract: 3D semantic scene understanding remains a long-standing challenge in the 3D computer vision community. One of the key issues pertains to limited real-world annotated data to facilitate generalizable models. The common practice to tackle this issue is to simulate new data. Although synthetic datasets offer scalability and perfect labels, their designer-crafted scenes fail to capture real-world complexity and sensor noise, resulting in a synthetic-to-real domain gap. Moreover, no benchmark provides synchronized real and simulated point clouds for segmentation-oriented domain shift analysis. We introduce TrueCity, the first urban semantic segmentation benchmark with cm-accurate annotated real-world point clouds, semantic 3D city models, and annotated simulated point clouds representing the same city. TrueCity proposes segmentation classes aligned with international 3D city modeling standards, enabling consistent evaluation of synthetic-to-real gap. Our extensive experiments on common baselines quantify domain shift and highlight strategies for exploiting synthetic data to enhance real-world 3D scene understanding. We are convinced that the TrueCity dataset will foster further development of sim-to-real gap quantification and enable generalizable data-driven models. The data, code, and 3D models are available online: https://tum-gis.github.io/TrueCity/

</details>


### [177] [RelightMaster: Precise Video Relighting with Multi-plane Light Images](https://arxiv.org/abs/2511.06271)
*Weikang Bian,Xiaoyu Shi,Zhaoyang Huang,Jianhong Bai,Qinghe Wang,Xintao Wang,Pengfei Wan,Kun Gai,Hongsheng Li*

Main category: cs.CV

TL;DR: RelightMaster是一个新颖的视频重打光框架，通过构建RelightVideo数据集、引入多平面光照图像（MPLI）作为视觉提示，并设计光照图像适配器，实现了高精度、可控的视频重打光，同时保持内容一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频（T2V）模型缺乏精细的光照控制能力，因为文本在描述光照细节方面存在局限性，且预训练数据中缺乏相关提示。此外，高质量的重打光训练数据难以获取，因为真实世界可控光照数据稀缺。

Method: 1. 构建了RelightVideo数据集，这是首个基于虚幻引擎、包含相同动态内容在不同精确光照条件下的数据集。2. 引入了多平面光照图像（MPLI），这是一种受多平面图像（MPI）启发的新颖视觉提示，通过K个深度对齐的平面建模光照，表示3D光源位置、强度和颜色，支持多光源场景并泛化到未见的光照设置。3. 设计了光照图像适配器，将MPLI无缝注入到预训练的视频扩散Transformer (DiT) 中：通过预训练的视频VAE压缩MPLI，并将潜在光照特征注入DiT块，利用基础模型的生成先验而不产生灾难性遗忘。

Result: 实验表明，RelightMaster能够生成物理上合理的光照和阴影，并保留原始场景内容。

Conclusion: RelightMaster通过解决数据和控制方面的挑战，提供了一种准确且可控的视频重打光解决方案，实现了高质量的视频生成和编辑。

Abstract: Recent advances in diffusion models enable high-quality video generation and editing, but precise relighting with consistent video contents, which is critical for shaping scene atmosphere and viewer attention, remains unexplored. Mainstream text-to-video (T2V) models lack fine-grained lighting control due to text's inherent limitation in describing lighting details and insufficient pre-training on lighting-related prompts. Additionally, constructing high-quality relighting training data is challenging, as real-world controllable lighting data is scarce. To address these issues, we propose RelightMaster, a novel framework for accurate and controllable video relighting. First, we build RelightVideo, the first dataset with identical dynamic content under varying precise lighting conditions based on the Unreal Engine. Then, we introduce Multi-plane Light Image (MPLI), a novel visual prompt inspired by Multi-Plane Image (MPI). MPLI models lighting via K depth-aligned planes, representing 3D light source positions, intensities, and colors while supporting multi-source scenarios and generalizing to unseen light setups. Third, we design a Light Image Adapter that seamlessly injects MPLI into pre-trained Video Diffusion Transformers (DiT): it compresses MPLI via a pre-trained Video VAE and injects latent light features into DiT blocks, leveraging the base model's generative prior without catastrophic forgetting. Experiments show that RelightMaster generates physically plausible lighting and shadows and preserves original scene content. Demos are available at https://wkbian.github.io/Projects/RelightMaster/.

</details>


### [178] [VideoSSR: Video Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06281)
*Zefeng He,Xiaoye Qu,Yafu Li,Siyuan Huang,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 本文提出VideoSSR，一个视频自监督强化学习框架，通过利用视频内在信息自生成高质量可验证的训练数据，显著提升了多模态大语言模型（MLLMs）的视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视频理解方面进步迅速，但现有视频数据集的复杂性已跟不上模型发展，同时手动标注高质量数据成本过高。研究旨在探索是否能利用视频的内在信息自生成高质量、可验证的训练数据。

Method: 研究引入了三个自监督预设任务：异常定位、物体计数和时间拼图。构建了视频内在理解基准（VIUBench）来验证这些任务的难度。基于这些预设任务，开发了VideoSSR-30K数据集，并提出了VideoSSR，一个新颖的视频自监督强化学习框架，用于可验证奖励强化学习（RLVR）。

Result: 实验表明，当前最先进的MLLMs在VIUBench任务上表现不佳。VideoSSR在17个基准测试（涵盖通用视频问答、长视频问答、时间定位和复杂推理四大视频领域）中，始终如一地提升了模型性能，平均改进超过5%。

Conclusion: VideoSSR被确立为开发更先进MLLMs视频理解能力的强大基础框架，通过自监督方式有效解决了高质量视频训练数据匮乏的问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR.

</details>


### [179] [Pandar128 dataset for lane line detection](https://arxiv.org/abs/2511.07084)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: 本文介绍了Pandar128，一个最大的128线激光雷达车道线检测公开数据集，并提出了一个轻量级基线方法SimpleLidarLane和一个新的评估指标IAM-F1，以促进激光雷达车道线检测领域的研究和标准化。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏用于车道线检测的大规模128线激光雷达数据集，以及标准化、基于折线的评估指标。研究旨在提供高质量数据和更有效的评估方法，以推动该领域的发展。

Method: 研究方法包括：1) 构建Pandar128数据集，包含52,000多帧相机图像和34,000次激光雷达扫描，以及完整的传感器校准和里程计数据。2) 提出SimpleLidarLane，一个轻量级基线方法，结合了BEV分割、聚类和多线拟合。3) 提出IAM-F1，一个新颖的、基于折线的评估指标，在BEV空间中采用插值感知横向匹配。

Result: Pandar128是目前最大的128线激光雷达车道线检测公开数据集。SimpleLidarLane尽管简单，但在各种挑战性条件下（如雨天、稀疏返回）表现出色。IAM-F1是提出的一个 novel 的基于折线的评估指标。所有数据和代码均已公开发布，以支持可复现性。

Conclusion: 高质量的数据集、模块化管道和原则性的评估方法，即使是简单的模型也能在激光雷达车道线检测中取得与复杂方法相当的强大性能。本研究通过发布数据集、基线方法和评估指标，解决了该领域数据和评估标准缺乏的问题，促进了可复现性。

Abstract: We present Pandar128, the largest public dataset for lane line detection using a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDAR scans, captured in diverse real-world conditions in Germany. The dataset includes full sensor calibration (intrinsics, extrinsics) and synchronized odometry, supporting tasks such as projection, fusion, and temporal modeling.
  To complement the dataset, we also introduce SimpleLidarLane, a light-weight baseline method for lane line reconstruction that combines BEV segmentation, clustering, and polyline fitting. Despite its simplicity, our method achieves strong performance under challenging various conditions (e.g., rain, sparse returns), showing that modular pipelines paired with high-quality data and principled evaluation can compete with more complex approaches.
  Furthermore, to address the lack of standardized evaluation, we propose a novel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - that employs interpolation-aware lateral matching in BEV space.
  All data and code are publicly released to support reproducibility in LiDAR-based lane detection.

</details>


### [180] [From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses](https://arxiv.org/abs/2511.06282)
*Ali Abbasian Ardakani,Afshin Mohammadi,Alisa Mohebbi,Anushya Vijayananthan,Sook Sam Leong,Lim Yi Ting,Mohd Kamil Bin Mohamad Fabell,U Rajendra Acharya,Sepideh Hatamikia*

Main category: cs.CV

TL;DR: 深度学习（DL）模型在卵巢附件病变超声分类方面显著优于放射科医生单独使用O-RADS v2022评估，而人机混合框架进一步提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: O-RADS v2022更新了附件病变的风险分层，但人工解读存在变异性和保守阈值。同时，深度学习模型在图像分析方面展现出潜力。本研究旨在评估放射科医生应用O-RADS v2022的表现，并与领先的深度学习模型进行比较，同时探究人机混合框架的诊断增益。

Method: 这是一项单中心、回顾性队列研究，纳入了227名患者的512张附件肿块图像。训练并验证了16种深度学习模型，包括DenseNets、EfficientNets、ResNets、VGGs、Xception和ViTs。此外，还为每种方案构建了一个结合放射科医生O-RADS评分和DL预测概率的混合模型。

Result: 放射科医生单独使用O-RADS评估的AUC为0.683，准确率为68.0%。CNN模型表现为AUC 0.620-0.908，准确率59.2%-86.4%。ViT16-384模型表现最佳，AUC为0.941，准确率为87.4%。人机混合框架显著提升了CNN模型的性能，但对ViT模型的提升不具有统计学意义（P值>0.05）。

Conclusion: 深度学习模型显著优于放射科医生单独进行的O-RADS v2022评估。专家评分与AI的结合实现了最高的诊断准确性和鉴别能力。人机混合范式在标准化盆腔超声解读、减少假阳性以及提高高风险病变检测方面具有巨大潜力。

Abstract: Background: The 2022 update of the Ovarian-Adnexal Reporting and Data System (O-RADS) ultrasound classification refines risk stratification for adnexal lesions, yet human interpretation remains subject to variability and conservative thresholds. Concurrently, deep learning (DL) models have demonstrated promise in image-based ovarian lesion characterization. This study evaluates radiologist performance applying O-RADS v2022, compares it to leading convolutional neural network (CNN) and Vision Transformer (ViT) models, and investigates the diagnostic gains achieved by hybrid human-AI frameworks. Methods: In this single-center, retrospective cohort study, a total of 512 adnexal mass images from 227 patients (110 with at least one malignant cyst) were included. Sixteen DL models, including DenseNets, EfficientNets, ResNets, VGGs, Xception, and ViTs, were trained and validated. A hybrid model integrating radiologist O-RADS scores with DL-predicted probabilities was also built for each scheme. Results: Radiologist-only O-RADS assessment achieved an AUC of 0.683 and an overall accuracy of 68.0%. CNN models yielded AUCs of 0.620 to 0.908 and accuracies of 59.2% to 86.4%, while ViT16-384 reached the best performance, with an AUC of 0.941 and an accuracy of 87.4%. Hybrid human-AI frameworks further significantly enhanced the performance of CNN models; however, the improvement for ViT models was not statistically significant (P-value >0.05). Conclusions: DL models markedly outperform radiologist-only O-RADS v2022 assessment, and the integration of expert scores with AI yields the highest diagnostic accuracy and discrimination. Hybrid human-AI paradigms hold substantial potential to standardize pelvic ultrasound interpretation, reduce false positives, and improve detection of high-risk lesions.

</details>


### [181] [TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks](https://arxiv.org/abs/2511.06283)
*Xuanle Zhao,Shuxin Zeng,Yinyuan Cai,Xiang Cheng,Duzhen Zhang,Xiuyi Chen,Bo Xu*

Main category: cs.CV

TL;DR: 本文提出了TinyChemVL，一个高效且强大的化学领域视觉语言模型（VLM），通过视觉令牌减少和反应级任务，显著提升了模型效率和推理能力。同时引入了ChemRxn-V，一个用于评估视觉反应识别和预测的基准。TinyChemVL在分子和反应任务上均表现出色，且推理和训练速度更快。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型（VLM）在化学领域的应用受限，现有工作多侧重文本而忽略分子结构等关键视觉信息。直接应用标准VLM存在两个问题：(i) 处理整个化学图像（含非信息背景）导致计算效率低下；(ii) 任务范围狭窄，仅限于分子级任务，限制了化学推理的进展。

Method: 本文提出了TinyChemVL，一个利用视觉令牌减少（visual token reduction）和反应级任务（reaction-level tasks）来提升模型效率和推理能力的化学VLM。此外，还提出了ChemRxn-V，一个用于评估基于视觉的反应识别和预测任务的反应级基准。

Result: 结果显示，TinyChemVL仅用40亿参数就在分子和反应任务上取得了卓越性能，同时展现出比现有模型更快的推理和训练速度。值得注意的是，TinyChemVL在使用仅为ChemVLM 1/16的视觉令牌时，性能超越了ChemVLM。

Conclusion: 通过协同设计模型架构和任务复杂度，本工作为化学领域构建了高效而强大的视觉语言模型。

Abstract: While Vision Language Models (VLMs) have demonstrated remarkable capabilities in general visual understanding, their application in the chemical domain has been limited, with previous works predominantly focusing on text and thus overlooking critical visual information, such as molecular structures. Current approaches that directly adopt standard VLMs for chemical tasks suffer from two primary issues: (i) computational inefficiency of processing entire chemical images with non-informative backgrounds. (ii) a narrow scope on molecular-level tasks that restricts progress in chemical reasoning. In this work, we propose \textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages visual token reduction and reaction-level tasks to improve model efficiency and reasoning capacity. Also, we propose \textbf{ChemRxn-V}, a reaction-level benchmark for assessing vision-based reaction recognition and prediction tasks. Directly predicting reaction products from molecular images poses a non-trivial challenge, as it requires models to integrate both recognition and reasoning capacities. Our results demonstrate that with only 4B parameters, TinyChemVL achieves superior performance on both molecular and reaction tasks while demonstrating faster inference and training speeds compared to existing models. Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the visual tokens. This work builds efficient yet powerful VLMs for chemical domains by co-designing model architecture and task complexity.

</details>


### [182] [How Bias Binds: Measuring Hidden Associations for Bias Control in Text-to-Image Compositions](https://arxiv.org/abs/2511.07091)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 本研究初步探讨了文本到图像生成模型中，语义绑定（即对象和属性的上下文关联）如何影响和放大偏见。为此，我们引入了偏见依从性分数，并开发了一个免训练的上下文偏见控制框架，通过解耦token来有效去偏，发现当前去偏方法在语义绑定背景下存在局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的去偏研究主要集中于单一对象提示，忽略了提示中对象和属性之间语义绑定（如“戴粉色帽子的助理”中粉色帽子与女性的关联）所产生的联合偏见效应，导致现有去偏方法在此类复杂情境下失效。

Method: 本研究首先调查了语义绑定下偏见的表现，然后引入了一个偏见依从性分数来量化特定对象-属性绑定如何激活偏见。接着，开发了一个免训练的上下文偏见控制框架，通过token解耦来探索去偏语义绑定的方法。

Result: 研究表明，潜在的偏见分布会因对象和属性之间的关联而被放大。所提出的框架在组合生成任务中实现了超过10%的去偏改善。对不同属性-对象绑定和token去相关性的偏见分数分析揭示了一个核心挑战：在不破坏基本语义关系的前提下去除偏见。

Conclusion: 这些发现揭示了当前去偏方法在应用于语义绑定上下文时的关键局限性，强调需要重新评估现行的偏见缓解策略。

Abstract: Text-to-image generative models often exhibit bias related to sensitive attributes. However, current research tends to focus narrowly on single-object prompts with limited contextual diversity. In reality, each object or attribute within a prompt can contribute to bias. For example, the prompt "an assistant wearing a pink hat" may reflect female-inclined biases associated with a pink hat. The neglected joint effects of the semantic binding in the prompts cause significant failures in current debiasing approaches. This work initiates a preliminary investigation on how bias manifests under semantic binding, where contextual associations between objects and attributes influence generative outcomes. We demonstrate that the underlying bias distribution can be amplified based on these associations. Therefore, we introduce a bias adherence score that quantifies how specific object-attribute bindings activate bias. To delve deeper, we develop a training-free context-bias control framework to explore how token decoupling can facilitate the debiasing of semantic bindings. This framework achieves over 10% debiasing improvement in compositional generation tasks. Our analysis of bias scores across various attribute-object bindings and token decorrelation highlights a fundamental challenge: reducing bias without disrupting essential semantic relationships. These findings expose critical limitations in current debiasing approaches when applied to semantically bound contexts, underscoring the need to reassess prevailing bias mitigation strategies.

</details>


### [183] [Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation](https://arxiv.org/abs/2511.07238)
*Seungheon Song,Jaekoo Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的文本驱动OOD分割模型，通过利用视觉-语言空间中的丰富语言知识，显著提升了自动驾驶中对未知对象的检测能力，并在多个基准数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和机器人领域，确保道路安全和可靠决策至关重要，而这严重依赖于域外（OOD）分割。尽管已有许多方法用于检测道路上的异常物体，但利用提供丰富语言知识的视觉-语言空间仍是一个未充分探索的领域。作者假设，将这些语言线索融入复杂的自动驾驶场景中会特别有益。

Method: 本文提出了一种文本驱动的OOD分割模型。具体方法包括：将视觉-语言模型的编码器与Transformer解码器相结合；采用与域内（ID）类别具有不同语义距离的基于距离的OOD提示；并利用OOD语义增强来丰富OOD表示。通过对齐视觉和文本信息，该方法能够有效地泛化到未见过的物体。

Result: 在Fishyscapes、Segment-Me-If-You-Can和Road Anomaly等公开OOD分割数据集上进行了广泛实验。结果表明，该方法在像素级和对象级评估中均实现了最先进的性能。

Conclusion: 研究结果强调了基于视觉-语言的OOD分割在增强未来自动驾驶系统安全性和可靠性方面的巨大潜力。

Abstract: In autonomous driving and robotics, ensuring road safety and reliable decision-making critically depends on out-of-distribution (OOD) segmentation. While numerous methods have been proposed to detect anomalous objects on the road, leveraging the vision-language space-which provides rich linguistic knowledge-remains an underexplored field. We hypothesize that incorporating these linguistic cues can be especially beneficial in the complex contexts found in real-world autonomous driving scenarios.
  To this end, we present a novel approach that trains a Text-Driven OOD Segmentation model to learn a semantically diverse set of objects in the vision-language space. Concretely, our approach combines a vision-language model's encoder with a transformer decoder, employs Distance-Based OOD prompts located at varying semantic distances from in-distribution (ID) classes, and utilizes OOD Semantic Augmentation for OOD representations. By aligning visual and textual information, our approach effectively generalizes to unseen objects and provides robust OOD segmentation in diverse driving environments.
  We conduct extensive experiments on publicly available OOD segmentation datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets, demonstrating that our approach achieves state-of-the-art performance across both pixel-level and object-level evaluations. This result underscores the potential of vision-language-based OOD segmentation to bolster the safety and reliability of future autonomous driving systems.

</details>


### [184] [Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use](https://arxiv.org/abs/2511.07171)
*Sébastien Thuau,Siba Haidar,Rachid Chelouah*

Main category: cs.CV

TL;DR: 该研究比较了联邦学习中三种视频暴力检测策略（零样本VLM、LoRA微调VLM和个性化3D CNN），发现3D CNN在能耗减半的情况下实现了卓越的校准和高精度，而VLM提供丰富的多模态推理，并提出了一种混合部署策略。


<details>
  <summary>Details</summary>
Motivation: 深度学习视频监控日益需要保护隐私的架构，同时要降低计算和环境开销。联邦学习虽能保护隐私，但部署大型视觉-语言模型（VLMs）会带来巨大的能源和可持续性挑战。

Method: 研究比较了三种联邦暴力检测策略：使用预训练VLM进行零样本推理、对LLaVA-NeXT-Video-7B进行LoRA微调，以及个性化联邦学习一个65.8M参数的3D CNN。实验在RWF-2000和RLVS数据集的非IID分裂上进行，并量化了能源和CO2e消耗。此外，还在UCF-Crime数据集上应用了基于语义相似性和类别排斥的层次类别分组。

Result: 所有方法在二元暴力检测中均达到90%以上的准确率。3D CNN实现了卓越的校准（ROC AUC 92.59%），能耗约为联邦LoRA的一半（240 Wh vs. 570 Wh）。VLM提供了更丰富的多模态推理能力。层次类别分组将VLM在UCF-Crime数据集上的多类别准确率从65.31%提升到81%。这是首次对联邦暴力检测中LoRA微调VLM和个性化CNN进行比较模拟研究，并明确量化了能源和CO2e。

Conclusion: 研究结果为混合部署策略提供了依据：日常推理可默认使用高效的CNN，而复杂的上下文推理则选择性地启用VLM。

Abstract: Deep learning-based video surveillance increasingly demands privacy-preserving architectures with low computational and environmental overhead. Federated learning preserves privacy but deploying large vision-language models (VLMs) introduces major energy and sustainability challenges. We compare three strategies for federated violence detection under realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed 90% accuracy in binary violence detection. The 3D CNN achieves superior calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570 Wh) of federated LoRA, while VLMs provide richer multimodal reasoning. Hierarchical category grouping (based on semantic similarity and class exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime dataset. To our knowledge, this is the first comparative simulation study of LoRA-tuned VLMs and personalized CNNs for federated violence detection, with explicit energy and CO2e quantification. Our results inform hybrid deployment strategies that default to efficient CNNs for routine inference and selectively engage VLMs for complex contextual reasoning.

</details>


### [185] [MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs](https://arxiv.org/abs/2511.07250)
*Tianhao Peng,Haochen Wang,Yuanxing Zhang,Zekun Wang,Zili Wang,Ge Zhang,Jian Yang,Shihao Li,Yanghai Wang,Xintao Wang,Houyi Li,Wei Ji,Pengfei Wan,Wenhao Huang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 现有MLLM评估基准仅限于单视频理解，忽略了多视频理解的需求。本文引入MVU-Eval，首个综合性多视频理解基准，评估MLLM的八项核心能力，并揭示了当前模型在此方面的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）已扩展到视觉领域，但现有评估基准仅限于单视频理解，未能满足现实世界中多视频理解的关键需求（如体育分析和自动驾驶）。

Method: 引入MVU-Eval，这是首个用于评估MLLM多视频理解能力的综合基准。它主要通过1,824个精心策划的问答对，涵盖来自不同领域的4,959个视频，评估八项核心能力，包括基本感知任务和高阶推理任务。这些能力与多传感器合成和跨角度体育分析等实际应用紧密对齐。

Result: 通过对最先进的开源和闭源模型进行广泛评估，研究揭示了当前MLLMs在执行跨多个视频理解能力方面存在显著的性能差异和局限性。

Conclusion: MVU-Eval填补了MLLM多视频理解评估的空白，提供了一个全面的基准，并揭示了当前MLLM在此能力上的不足。该基准将公开，以促进未来的研究。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.

</details>


### [186] [Learning-Based Vision Systems for Semi-Autonomous Forklift Operation in Industrial Warehouse Environments](https://arxiv.org/abs/2511.06295)
*Vamshika Sutar,Mahek Maheshwari,Archak Mittal*

Main category: cs.CV

TL;DR: 本研究提出了一个基于单摄像头的视觉框架，利用YOLOv8和YOLOv11模型，通过超参数优化和空间后处理，实现托盘及托盘孔的精确检测与映射，为叉车提供了一种低成本、可改装的感知解决方案，以提升仓库自动化水平。


<details>
  <summary>Details</summary>
Motivation: 仓库物料搬运自动化日益依赖于叉车和自动导引车（AGV）的鲁棒、低成本感知系统。

Method: 研究采用单一标准摄像头，利用YOLOv8和YOLOv11架构进行托盘及托盘孔检测。通过Optuna驱动的超参数优化和空间后处理来增强模型性能。此外，还开发了一个创新的托盘孔映射模块，将检测结果转换为可操作的空间表示，以实现托盘与托盘孔的精确关联。

Result: 实验结果表明，YOLOv8在托盘和托盘孔检测中实现了高精度，而YOLOv11（尤其在优化配置下）提供了卓越的精度和稳定的收敛性。这些结果证明了为叉车开发经济高效、可改装视觉感知模块的可行性。

Conclusion: 本研究提出了一种可扩展的方法，旨在推动仓库自动化，促进更安全、经济和智能的物流操作。

Abstract: The automation of material handling in warehouses increasingly relies on robust, low cost perception systems for forklifts and Automated Guided Vehicles (AGVs). This work presents a vision based framework for pallet and pallet hole detection and mapping using a single standard camera. We utilized YOLOv8 and YOLOv11 architectures, enhanced through Optuna driven hyperparameter optimization and spatial post processing. An innovative pallet hole mapping module converts the detections into actionable spatial representations, enabling accurate pallet and pallet hole association for forklift operation. Experiments on a custom dataset augmented with real warehouse imagery show that YOLOv8 achieves high pallet and pallet hole detection accuracy, while YOLOv11, particularly under optimized configurations, offers superior precision and stable convergence. The results demonstrate the feasibility of a cost effective, retrofittable visual perception module for forklifts. This study proposes a scalable approach to advancing warehouse automation, promoting safer, economical, and intelligent logistics operations.

</details>


### [187] [LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging](https://arxiv.org/abs/2511.07298)
*Kagan Celik,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 本研究提出一个基于LLM的低剂量CT图像质量评估系统，能生成数值分数和文本描述，并系统性地探索了多种推理策略以提高性能，为临床工作流程提供可解释的评估。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT虽能降低辐射，但会增加图像噪声、模糊和对比度损失，从而降低诊断质量。因此，一致且鲁棒的图像质量评估对临床应用至关重要。

Method: 本研究提出了一个基于大型语言模型（LLM）的质量评估系统，该系统能生成数值分数和关于噪声、模糊、对比度损失等退化的文本描述。此外，系统性地考察了多种推理策略，包括零样本学习、元数据集成和错误反馈。

Result: 评估结果不仅产生了高度相关的分数，还提供了可解释的输出。研究表明，每种推理策略都对整体性能有渐进的贡献。

Conclusion: 所提出的基于LLM的图像质量评估系统为低剂量CT提供了有价值且可解释的评估，从而提升了临床工作流程的价值。

Abstract: Low-dose computed tomography (CT) represents a significant improvement in patient safety through lower radiation doses, but increased noise, blur, and contrast loss can diminish diagnostic quality. Therefore, consistency and robustness in image quality assessment become essential for clinical applications. In this study, we propose an LLM-based quality assessment system that generates both numerical scores and textual descriptions of degradations such as noise, blur, and contrast loss. Furthermore, various inference strategies - from the zero-shot approach to metadata integration and error feedback - are systematically examined, demonstrating the progressive contribution of each method to overall performance. The resultant assessments yield not only highly correlated scores but also interpretable output, thereby adding value to clinical workflows. The source codes of our study are available at https://github.com/itu-biai/lmms_ldct_iqa.

</details>


### [188] [Glioma C6: A Novel Dataset for Training and Benchmarking Cell Segmentation](https://arxiv.org/abs/2511.07286)
*Roman Malashin,Svetlana Pashkevich,Daniil Ilyukhin,Arseniy Volkov,Valeria Yachnaya,Andrey Denisov,Maria Mikhalkova*

Main category: cs.CV

TL;DR: 本文介绍了Glioma C6，一个用于胶质瘤C6细胞实例分割的开放数据集，旨在作为深度学习模型的基准和训练资源，并包含生物学家提供的形态学分类。


<details>
  <summary>Details</summary>
Motivation: 生物医学图像分析需要一个真实且丰富的测试平台和训练资源，特别是针对胶质瘤C6细胞的实例分割，以推动癌症细胞研究。现有通用分割模型在此类任务上存在局限性。

Method: 研究人员构建了Glioma C6数据集，包含75张高分辨率相差显微图像和超过12,000个带注释的细胞。数据集包括细胞体注释和由生物学家提供的形态学细胞分类，并分为两部分：一部分用于基准测试，另一部分用于泛化测试。作者评估了多个通用分割模型在该数据集上的性能。

Result: 实验表明，通用分割模型在Glioma C6数据集上存在局限性。然而，在该数据集上进行训练能显著提高分割性能，证明了其对于开发鲁棒和泛化模型的重要性。

Conclusion: Glioma C6数据集是一个宝贵的公共资源，有助于开发更强大、更具泛化能力的深度学习模型，以促进生物医学图像分析和癌症细胞研究。

Abstract: We present Glioma C6, a new open dataset for instance segmentation of glioma C6 cells, designed as both a benchmark and a training resource for deep learning models. The dataset comprises 75 high-resolution phase-contrast microscopy images with over 12,000 annotated cells, providing a realistic testbed for biomedical image analysis. It includes soma annotations and morphological cell categorization provided by biologists. Additional categorization of cells, based on morphology, aims to enhance the utilization of image data for cancer cell research. Glioma C6 consists of two parts: the first is curated with controlled parameters for benchmarking, while the second supports generalization testing under varying conditions. We evaluate the performance of several generalist segmentation models, highlighting their limitations on our dataset. Our experiments demonstrate that training on Glioma C6 significantly enhances segmentation performance, reinforcing its value for developing robust and generalizable models. The dataset is publicly available for researchers.

</details>


### [189] [SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection](https://arxiv.org/abs/2511.06298)
*Xin Zuo,Yuchen Qu,Haibo Zhan,Jifeng Shen,Wankou Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的空间和频率特征重构方法（SFFR），利用Kolmogorov-Arnold网络（KAN）在融合前重建空间和频率域的互补特征，以提升多光谱目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱目标检测方法主要关注基于CNN或Transformer的空间域特征融合，而频率域特征的潜力尚未被充分挖掘。

Method: SFFR方法利用KAN的空间-频率特征表示机制，通过频率分量交换KAN（FCEKAN）模块和多尺度高斯KAN（MSGKAN）模块进行特征重构。FCEKAN通过选择性频率分量交换增强跨模态特征的互补性和一致性；MSGKAN利用多尺度高斯基函数在空间域建模非线性特征，捕捉不同无人机飞行高度引起的尺度变化，增强模型对尺度变化的适应性和鲁棒性。

Result: 实验验证了FCEKAN和MSGKAN模块的互补性，它们能分别有效捕获频率和空间语义特征，实现更好的特征融合。在SeaDroneSee、DroneVehicle和DVTOD数据集上的广泛实验表明，所提出的方法在无人机多光谱目标感知任务中表现出卓越的性能和显著优势。

Conclusion: SFFR方法通过在空间和频率域重建互补特征，显著提升了无人机多光谱目标感知任务的性能，有效解决了频率域特征未被充分利用的问题，并增强了模型对尺度变化的适应性。

Abstract: Recent multispectral object detection methods have primarily focused on spatial-domain feature fusion based on CNNs or Transformers, while the potential of frequency-domain feature remains underexplored. In this work, we propose a novel Spatial and Frequency Feature Reconstruction method (SFFR) method, which leverages the spatial-frequency feature representation mechanisms of the Kolmogorov-Arnold Network (KAN) to reconstruct complementary representations in both spatial and frequency domains prior to feature fusion. The core components of SFFR are the proposed Frequency Component Exchange KAN (FCEKAN) module and Multi-Scale Gaussian KAN (MSGKAN) module. The FCEKAN introduces an innovative selective frequency component exchange strategy that effectively enhances the complementarity and consistency of cross-modal features based on the frequency feature of RGB and IR images. The MSGKAN module demonstrates excellent nonlinear feature modeling capability in the spatial domain. By leveraging multi-scale Gaussian basis functions, it effectively captures the feature variations caused by scale changes at different UAV flight altitudes, significantly enhancing the model's adaptability and robustness to scale variations. It is experimentally validated that our proposed FCEKAN and MSGKAN modules are complementary and can effectively capture the frequency and spatial semantic features respectively for better feature fusion. Extensive experiments on the SeaDroneSee, DroneVehicle and DVTOD datasets demonstrate the superior performance and significant advantages of the proposed method in UAV multispectral object perception task. Code will be available at https://github.com/qchenyu1027/SFFR.

</details>


### [190] [GEWDiff: Geometric Enhanced Wavelet-based Diffusion Model for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2511.07103)
*Sirui Wang,Jiang He,Natàlia Blasco Andreo,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为GEWDiff的几何增强小波扩散模型，用于实现高光谱图像（HSI）的4倍超分辨率重建，通过高效压缩、几何特征保持和多级损失函数解决了现有扩散模型在处理HSI时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像由于其高光谱维度，直接输入传统扩散模型会导致内存消耗过大。通用生成模型缺乏对遥感图像中地物拓扑和几何结构的理解。此外，大多数扩散模型在噪声层级优化损失函数，导致收敛行为不直观且复杂数据生成质量不佳。

Method: 提出了一种几何增强小波扩散模型（GEWDiff）。该模型包含一个基于小波的编解码器，用于将HSI高效压缩到潜在空间并保留光谱空间信息。引入了几何增强扩散过程以在生成过程中避免失真并保留几何特征。设计了多级损失函数来指导扩散过程，促进稳定收敛并提高重建保真度。

Result: 所提出的模型在多个维度上展示了最先进的结果，包括保真度、光谱精度、视觉真实感和清晰度。

Conclusion: GEWDiff模型通过有效压缩HSI、在生成过程中保留几何特征以及优化损失函数，成功解决了高光谱图像超分辨率重建中的挑战，并取得了卓越的重建质量。

Abstract: Improving the quality of hyperspectral images (HSIs), such as through super-resolution, is a crucial research area. However, generative modeling for HSIs presents several challenges. Due to their high spectral dimensionality, HSIs are too memory-intensive for direct input into conventional diffusion models. Furthermore, general generative models lack an understanding of the topological and geometric structures of ground objects in remote sensing imagery. In addition, most diffusion models optimize loss functions at the noise level, leading to a non-intuitive convergence behavior and suboptimal generation quality for complex data. To address these challenges, we propose a Geometric Enhanced Wavelet-based Diffusion Model (GEWDiff), a novel framework for reconstructing hyperspectral images at 4-times super-resolution. A wavelet-based encoder-decoder is introduced that efficiently compresses HSIs into a latent space while preserving spectral-spatial information. To avoid distortion during generation, we incorporate a geometry-enhanced diffusion process that preserves the geometric features. Furthermore, a multi-level loss function was designed to guide the diffusion process, promoting stable convergence and improved reconstruction fidelity. Our model demonstrated state-of-the-art results across multiple dimensions, including fidelity, spectral accuracy, visual realism, and clarity.

</details>


### [191] [Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective](https://arxiv.org/abs/2511.06284)
*Bing Wang,Ximing Li,Yanjun Wang,Changchun Li,Lin Yuanbo Wu,Buyu Wang,Shengsheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RETSIMD的多模态虚假信息检测方法，其核心思想是文本模态比图像模态包含更多信息。该方法通过文本片段生成图像序列，并结合图神经网络融合特征，以提高检测效果。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息检测（MMD）任务中，社交媒体帖子通常包含文本和图像。作者观察到文本通常描述事件的完整故事，而图像往往只呈现部分场景，因此文本模态可能比图像模态提供更多信息。初步实验结果也支持图像模态对MMD的贡献较小，这促使作者提出一种更侧重文本信息的方法。

Method: 本文提出了RETSIMD方法。具体而言，将文本分割成多个片段，每个片段描述一个局部场景。然后，利用预训练的文本到图像生成器，根据这些文本片段生成一系列图像。此外，引入两个辅助目标（文本-图像和图像-标签互信息），并在辅助的文本到图像生成基准数据集上对生成器进行后训练。最后，通过定义图像之间的三种启发式关系构建图结构，并使用图神经网络生成融合特征。

Result: 广泛的实证结果验证了RETSIMD方法的有效性。

Conclusion: RETSIMD通过认识到文本模态在多模态虚假信息检测中的更高信息量，创新性地利用文本片段生成辅助图像序列，并结合图神经网络进行特征融合，从而有效提高了虚假信息检测的性能。

Abstract: Multimodal Misinformation Detection (MMD) refers to the task of detecting social media posts involving misinformation, where the post often contains text and image modalities. However, by observing the MMD posts, we hold that the text modality may be much more informative than the image modality because the text generally describes the whole event/story of the current post but the image often presents partial scenes only. Our preliminary empirical results indicate that the image modality exactly contributes less to MMD. Upon this idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that each text can be divided into several segments, and each text segment describes a partial scene that can be presented by an image. Accordingly, we split the text into a sequence of segments, and feed these segments into a pre-trained text-to-image generator to augment a sequence of images. We further incorporate two auxiliary objectives concerning text-image and image-label mutual information, and further post-train the generator over an auxiliary text-to-image generation benchmark dataset. Additionally, we propose a graph structure by defining three heuristic relationships between images, and use a graph neural network to generate the fused features. Extensive empirical results validate the effectiveness of RETSIMD.

</details>


### [192] [Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates](https://arxiv.org/abs/2511.06310)
*Seunghyeok Shin,Dabin Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 本文提出了一种名为Forward Curvature-Matching (FCM)的新型扩散模型更新方法，用于从图像重建高质量点云。FCM通过动态确定最优似然更新步长，解决了现有方法的灵活性不足和重建质量次优问题，实现了无需重新训练即可从单视图、多视图及不同模态输入进行高保真重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型（尤其是扩散模型）的点云重建方法存在局限性：它们需要训练时提供条件信号，只支持固定数量的输入视图，并且对于不同的测量需要完全重新训练，缺乏灵活性。近期结合先验模型和似然更新的扩散方法，由于依赖启发式固定步长进行似然更新，导致收敛缓慢和重建质量不佳。

Method: 本文通过将新颖的Forward Curvature-Matching (FCM)更新方法与扩散采样相结合，来推进该领域。FCM方法仅使用前向自动微分和有限差分曲率估计，动态确定似然更新的最优步长，从而实现精确的似然更新优化。这种方法支持从单视图和多视图输入进行高保真重建，并通过简单的算子替换支持各种输入模态，所有这些都无需重新训练。

Result: 在ShapeNet和CO3D数据集上的实验表明，本文方法在相同或更低的NFEs下实现了卓越的重建质量，获得了更高的F-score以及更低的CD和EMD，验证了其在实际应用中的效率和适应性。

Conclusion: FCM方法通过动态确定最优似然更新步长，显著提升了扩散模型在点云重建中的性能和灵活性。它能够在不重新训练的情况下，从多种视图和模态输入中实现高保真重建，证明了其高效性和广泛适用性。

Abstract: Reconstructing high-quality point clouds from images remains challenging in computer vision. Existing generative-model-based approaches, particularly diffusion-model approaches that directly learn the posterior, may suffer from inflexibility -- they require conditioning signals during training, support only a fixed number of input views, and need complete retraining for different measurements. Recent diffusion-based methods have attempted to address this by combining prior models with likelihood updates, but they rely on heuristic fixed step sizes for the likelihood update that lead to slow convergence and suboptimal reconstruction quality. We advance this line of approach by integrating our novel Forward Curvature-Matching (FCM) update method with diffusion sampling. Our method dynamically determines optimal step sizes using only forward automatic differentiation and finite-difference curvature estimates, enabling precise optimization of the likelihood update. This formulation enables high-fidelity reconstruction from both single-view and multi-view inputs, and supports various input modalities through simple operator substitution -- all without retraining. Experiments on ShapeNet and CO3D datasets demonstrate that our method achieves superior reconstruction quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD, validating its efficiency and adaptability for practical applications. Code is available at https://github.com/Seunghyeok0715/FCM

</details>


### [193] [Seq2Seq Models Reconstruct Visual Jigsaw Puzzles without Seeing Them](https://arxiv.org/abs/2511.06315)
*Gur Elkn,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出一种新颖方法，利用语言模型（而非视觉输入）通过将拼图块转换为离散token序列来解决方形拼图，并取得了超越传统视觉方法的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统拼图算法主要基于视觉，本文旨在探索一种根本不同的方法，即使用语言模型解决拼图，以摆脱对原始视觉输入的依赖。

Method: 研究引入了一种特殊的tokenizer，将每个拼图块转换为离散的token序列。拼图重组被重新定义为序列到序列的预测任务，并使用编码器-解码器Transformer模型作为“盲”求解器，仅通过推理token序列来重构原始布局。

Result: 尽管模型被限制无法访问视觉输入，但它们能够准确重建原始布局，并在多个基准测试中取得了SOTA结果，甚至经常优于基于视觉的方法。

Conclusion: 这些发现突显了语言模型解决其原生领域之外问题的惊人能力，并表明非常规方法可以为拼图解决研究启发有前景的方向。

Abstract: Jigsaw puzzles are primarily visual objects, whose algorithmic solutions have traditionally been framed from a visual perspective. In this work, however, we explore a fundamentally different approach: solving square jigsaw puzzles using language models, without access to raw visual input. By introducing a specialized tokenizer that converts each puzzle piece into a discrete sequence of tokens, we reframe puzzle reassembly as a sequence-to-sequence prediction task. Treated as "blind" solvers, encoder-decoder transformers accurately reconstruct the original layout by reasoning over token sequences alone. Despite being deliberately restricted from accessing visual input, our models achieve state-of-the-art results across multiple benchmarks, often outperforming vision-based methods. These findings highlight the surprising capability of language models to solve problems beyond their native domain, and suggest that unconventional approaches can inspire promising directions for puzzle-solving research.

</details>


### [194] [Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection](https://arxiv.org/abs/2511.07301)
*Huizai Yao,Sicheng Zhao,Pengteng Li,Yi Cui,Shuo Lu,Weiyu Guo,Yunfan Lu,Yijie Xu,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出一种新颖的无源目标检测（SFOD）框架，利用视觉基础模型（VFMs）作为外部知识源，以同时增强特征对齐和伪标签质量。


<details>
  <summary>Details</summary>
Motivation: 现有SFOD方法主要依赖源模型的内部知识，这限制了其跨域泛化能力并导致有偏见的伪标签。而视觉基础模型（VFMs）虽然具有强大的感知和泛化能力，但在SFOD设置中其潜力尚未得到充分利用。

Method: 该方法设计了三个基于VFM的模块：1) 补丁加权全局特征对齐（PGFA），通过基于补丁相似度的加权从VFMs中提取全局特征；2) 基于原型的实例特征对齐（PIFA），通过动量更新的VFM原型指导实例级对比学习；3) 双源增强伪标签融合（DEPF），通过熵感知策略融合检测VFM和教师模型的预测，以生成更可靠的监督。

Result: 在六个基准测试上进行的大量实验表明，该方法实现了最先进的SFOD性能。

Conclusion: 将VFMs整合到SFOD中能有效提高模型的可迁移性和判别性，验证了该方法的有效性。

Abstract: Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object detector to a target domain without access to source data. However, existing SFOD methods predominantly rely on internal knowledge from the source model, which limits their capacity to generalize across domains and often results in biased pseudo-labels, thereby hindering both transferability and discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on massive and diverse data, exhibit strong perception capabilities and broad generalization, yet their potential remains largely untapped in the SFOD setting. In this paper, we propose a novel SFOD framework that leverages VFMs as external knowledge sources to jointly enhance feature alignment and label quality. Specifically, we design three VFM-based modules: (1) Patch-weighted Global Feature Alignment (PGFA) distills global features from VFMs using patch-similarity-based weighting to enhance global feature transferability; (2) Prototype-based Instance Feature Alignment (PIFA) performs instance-level contrastive learning guided by momentum-updated VFM prototypes; and (3) Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from detection VFMs and teacher models via an entropy-aware strategy to yield more reliable supervision. Extensive experiments on six benchmarks demonstrate that our method achieves state-of-the-art SFOD performance, validating the effectiveness of integrating VFMs to simultaneously improve transferability and discriminability.

</details>


### [195] [Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field](https://arxiv.org/abs/2511.06299)
*Haoqin Hong,Ding Fan,Fubin Dou,Zhi-Li Zhou,Haoran Sun,Congcong Zhu,Jingrun Chen*

Main category: cs.CV

TL;DR: 针对3DGS在动态场景中物理驱动运动捕捉的不足，本文提出了PIDG，通过将高斯粒子视为拉格朗日物质点，引入柯西动量残差作为物理约束，并结合光流监督，显著提升了单目动态重建的物理一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的3D Gaussian Splatting (3DGS) 在从单目视频输入中捕捉动态场景中多样化的物理驱动运动模式时表现不佳。

Method: 该方法将每个高斯粒子视为具有时变本构参数的拉格朗日物质点，并通过运动投影受2D光流监督。具体而言，它采用静态-动态解耦的4D分解哈希编码来高效重建几何和运动。随后，施加柯西动量残差作为物理约束，通过时变材料场独立预测每个粒子的速度和本构应力。最后，通过将拉格朗日粒子流与相机补偿光流匹配来进一步监督数据拟合。

Result: 在定制的物理驱动数据集以及标准合成和真实世界数据集上的实验表明，PIDG 在物理一致性和单目动态重建质量方面取得了显著提升。

Conclusion: PIDG通过将物理约束（柯西动量残差）和光流监督有效地集成到可变形高斯溅射中，解决了纯数据驱动3DGS在捕捉复杂物理运动方面的局限性，显著提高了单目动态场景重建的物理一致性和视觉质量。

Abstract: Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.

</details>


### [196] [Inference-Time Scaling of Diffusion Models for Infrared Data Generation](https://arxiv.org/abs/2511.07362)
*Kai A. Horstmann,Maxim Clouser,Kia Khezeli*

Main category: cs.CV

TL;DR: 本文提出了一种在数据稀缺的红外领域，通过对现有文本到图像扩散模型进行微调，并利用领域适应的CLIP验证器在推理时进行引导，以提高红外图像生成质量的方法。


<details>
  <summary>Details</summary>
Motivation: 红外图像在低能见度下对场景理解至关重要，但高质量标注数据稀缺，阻碍了下游视觉模型开发。同时，由于数据集有限，在红外领域训练基础级生成扩散模型也面临挑战。

Method: 研究者将最先进的文本到图像扩散模型FLUX.1-dev通过参数高效技术在少量红外图像上进行微调，使其适应红外领域。随后，在推理阶段使用一个经过领域适应的CLIP验证器来引导扩散采样过程，以生成更高质量且与文本提示更一致的红外图像。

Result: 该方法显著提升了生成质量，在KAIST多光谱行人检测基准数据集上，与无引导的基线样本相比，FID分数降低了10%。

Conclusion: 研究结果表明，推理时引导为解决低数据红外环境中的领域鸿沟提供了一个有前景的方向。

Abstract: Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.

</details>


### [197] [Improving Multimodal Sentiment Analysis via Modality Optimization and Dynamic Primary Modality Selection](https://arxiv.org/abs/2511.06328)
*Dingkang Yang,Mingcheng Li,Xuecheng Wu,Zhaoyu Chen,Kaixun Jiang,Keliang Liu,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态情感分析框架MODS，通过图基动态序列压缩器优化非语言模态，并设计自适应主模态选择器动态确定主导模态，再通过以主模态为中心的交叉注意力模块增强主导模态并促进跨模态交互，以解决模态性能不平衡和序列冗余问题。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析中，单模态性能不平衡常导致融合表示不佳。现有方法通常采用固定主模态策略，无法适应不同样本中模态重要性的动态变化。此外，非语言模态（声学/视觉）存在序列冗余和噪声，当它们作为主要输入时会降低模型性能。

Method: 本文提出了模态优化和动态主模态选择框架（MODS）。具体包括：1. 构建图基动态序列压缩器（GDC），利用胶囊网络和图卷积减少声学/视觉模态的序列冗余。2. 开发样本自适应的主模态选择器（MSelector），用于动态确定主导模态。3. 设计以主模态为中心的交叉注意力（PCCA）模块，增强主导模态并促进跨模态交互。

Result: 在四个基准数据集上的广泛实验表明，MODS优于现有最先进的方法，通过有效平衡模态贡献和消除冗余噪声，实现了卓越的性能。

Conclusion: MODS框架通过优化非语言模态和动态选择主模态，有效解决了多模态情感分析中的模态不平衡和序列冗余问题，从而取得了优越的性能。

Abstract: Multimodal Sentiment Analysis (MSA) aims to predict sentiment from language, acoustic, and visual data in videos. However, imbalanced unimodal performance often leads to suboptimal fused representations. Existing approaches typically adopt fixed primary modality strategies to maximize dominant modality advantages, yet fail to adapt to dynamic variations in modality importance across different samples. Moreover, non-language modalities suffer from sequential redundancy and noise, degrading model performance when they serve as primary inputs. To address these issues, this paper proposes a modality optimization and dynamic primary modality selection framework (MODS). First, a Graph-based Dynamic Sequence Compressor (GDC) is constructed, which employs capsule networks and graph convolution to reduce sequential redundancy in acoustic/visual modalities. Then, we develop a sample-adaptive Primary Modality Selector (MSelector) for dynamic dominance determination. Finally, a Primary-modality-Centric Cross-Attention (PCCA) module is designed to enhance dominant modalities while facilitating cross-modal interaction. Extensive experiments on four benchmark datasets demonstrate that MODS outperforms state-of-the-art methods, achieving superior performance by effectively balancing modality contributions and eliminating redundant noise.

</details>


### [198] [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/abs/2511.07403)
*Hunar Batra,Haoqin Tu,Hardy Chen,Yuanze Lin,Cihang Xie,Ronald Clark*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在空间理解方面表现不佳，SpatialThinker通过强化学习和场景图集成结构化空间基础与多步推理，实现了3D感知的空间理解，并在有限数据下超越了现有基线和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在空间理解方面存在困难，通常依赖于显式3D输入、特定的架构修改，并且受限于大规模数据集或稀疏监督。

Method: SpatialThinker是一个3D感知的MLLM，通过强化学习（RL）进行训练，以整合结构化空间基础与多步推理。它通过构建任务相关物体和空间关系的场景图来模拟人类的空间感知，并通过密集的空间奖励进行推理。主要贡献包括：1) 一个数据合成管道，生成高质量的空间VQA数据集STVQA-7K；2) 采用在线RL，结合多目标密集空间奖励来强制执行空间基础。

Result: SpatialThinker-7B在空间理解和真实世界VQA基准测试中，优于有监督微调和稀疏RL基线。与稀疏RL相比，其基础模型增益几乎翻倍，并超越了GPT-4o。

Conclusion: 将空间监督与奖励对齐推理相结合，能有效在有限数据下实现鲁棒的3D空间理解，推动MLLMs向人类水平的视觉推理迈进。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.

</details>


### [199] [Label-Efficient 3D Forest Mapping: Self-Supervised and Transfer Learning for Individual, Structural, and Species Analysis](https://arxiv.org/abs/2511.06331)
*Aldino Rizaldy,Fabian Ewald Fassnacht,Ahmed Jamal Afifi,Hua Jiang,Richard Gloaguen,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 本研究通过结合自监督学习和迁移学习，显著减少了深度学习模型对大量标注数据的依赖，从而提高了激光扫描点云中个体树木分割、语义分割和物种分类的性能，并提供了一个统一的开源框架。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在处理3D点云数据进行个体树木分析时，需要大量高质量的标注训练数据，这在复杂森林环境中耗时且难以扩展，限制了模型在精准林业、生物多样性保护和生物量碳绘图中的应用和进一步改进。

Method: 研究探索了自监督学习和迁移学习架构，以减少对大规模标注数据集的依赖。具体方法包括将自监督学习与域适应相结合用于实例分割，以及采用分层迁移学习实现未见物种的分类。此外，将所有任务整合到一个统一的框架中，并发布了开源贡献。

Result: 研究发现，结合自监督学习和域适应显著提升了实例分割性能（AP50 +16.98%）；自监督学习足以改善语义分割（mIoU +1.79%）；分层迁移学习使得对未见物种的准确分类成为可能（Jaccard +6.07%）。此外，预训练模型可减少约21%的能源消耗和碳排放。所有任务被整合到一个统一的框架中。

Conclusion: 自监督学习和迁移学习策略能有效减少对大量标注数据的依赖，显著提升了3D点云中个体树木的实例分割、语义分割和物种分类的性能。所提出的统一开源框架将加速从激光扫描点云中提取个体树木信息的操作化过程，以支持林业、生物多样性和碳绘图等应用。

Abstract: Detailed structural and species information on individual tree level is increasingly important to support precision forestry, biodiversity conservation, and provide reference data for biomass and carbon mapping. Point clouds from airborne and ground-based laser scanning are currently the most suitable data source to rapidly derive such information at scale. Recent advancements in deep learning improved segmenting and classifying individual trees and identifying semantic tree components. However, deep learning models typically require large amounts of annotated training data which limits further improvement. Producing dense, high-quality annotations for 3D point clouds, especially in complex forests, is labor-intensive and challenging to scale. We explore strategies to reduce dependence on large annotated datasets using self-supervised and transfer learning architectures. Our objective is to improve performance across three tasks: instance segmentation, semantic segmentation, and tree classification using realistic and operational training sets. Our findings indicate that combining self-supervised learning with domain adaptation significantly enhances instance segmentation compared to training from scratch (AP50 +16.98%), self-supervised learning suffices for semantic segmentation (mIoU +1.79%), and hierarchical transfer learning enables accurate classification of unseen species (Jaccard +6.07%). To simplify use and encourage uptake, we integrated the tasks into a unified framework, streamlining the process from raw point clouds to tree delineation, structural analysis, and species classification. Pretrained models reduce energy consumption and carbon emissions by ~21%. This open-source contribution aims to accelerate operational extraction of individual tree information from laser scanning point clouds to support forestry, biodiversity, and carbon mapping.

</details>


### [200] [AesTest: Measuring Aesthetic Intelligence from Perception to Production](https://arxiv.org/abs/2511.06360)
*Guolong Wang,Heng Huang,Zhiqiang Zhang,Wentian Li,Feilong Ma,Xin Jin*

Main category: cs.CV

TL;DR: 本文介绍了AesTest，一个用于评估多模态大语言模型（MLLMs）美学感知和生成能力的综合基准，旨在弥补现有评估工具的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在感知和生成美学判断方面的能力尚未得到充分探索，现有图像美学评估（IAA）基准在感知范围上狭窄或缺乏评估系统性美学生成所需的丰富性。

Method: 研究者引入了AesTest基准，其特点包括：1）包含基于生成学习心理学理论的十项任务的多项选择题，涵盖感知、欣赏、创作和摄影；2）整合了来自专业编辑工作流、摄影构图教程和众包偏好等多样化数据源，确保覆盖专家级原则和真实世界变异；3）支持属性分析、情感共鸣、构图选择和风格推理等多种美学查询类型。

Result: 在AesTest上评估了指令微调的IAA MLLMs和通用MLLMs，结果显示在构建美学智能方面存在显著挑战。

Conclusion: 构建美学智能对MLLMs而言仍是巨大挑战。AesTest将被公开发布以支持该领域的未来研究。

Abstract: Perceiving and producing aesthetic judgments is a fundamental yet underexplored capability for multimodal large language models (MLLMs). However, existing benchmarks for image aesthetic assessment (IAA) are narrow in perception scope or lack the diversity needed to evaluate systematic aesthetic production. To address this gap, we introduce AesTest, a comprehensive benchmark for multimodal aesthetic perception and production, distinguished by the following features: 1) It consists of curated multiple-choice questions spanning ten tasks, covering perception, appreciation, creation, and photography. These tasks are grounded in psychological theories of generative learning. 2) It integrates data from diverse sources, including professional editing workflows, photographic composition tutorials, and crowdsourced preferences. It ensures coverage of both expert-level principles and real-world variation. 3) It supports various aesthetic query types, such as attribute-based analysis, emotional resonance, compositional choice, and stylistic reasoning. We evaluate both instruction-tuned IAA MLLMs and general MLLMs on AesTest, revealing significant challenges in building aesthetic intelligence. We will publicly release AesTest to support future research in this area.

</details>


### [201] [V-Shuffle: Zero-Shot Style Transfer via Value Shuffle](https://arxiv.org/abs/2511.06365)
*Haojun Tang,Qiwei Lin,Tongda Xu,Lida Huang,Yan Wang*

Main category: cs.CV

TL;DR: V-Shuffle是一种零样本风格迁移方法，通过在扩散模型的自注意力层中打乱值特征来减少内容泄露，并结合混合风格正则化，在利用多张或单张风格图像时均能实现卓越的风格迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力注入的风格迁移方法常面临内容泄露问题，即风格图像中不必要的语义内容错误地出现在风格化输出中。

Method: 本文提出了V-Shuffle，一种零样本风格迁移方法。它利用来自同一风格域的多张风格图像，通过在扩散模型的自注意力层内打乱值特征来隐式破坏风格图像的语义内容，从而保留低级风格表示。此外，还引入了混合风格正则化，用高级风格纹理补充这些低级表示以增强风格保真度。

Result: 实验结果表明，V-Shuffle在利用多张风格图像时表现出色。即使仅使用一张风格图像，V-Shuffle也超越了以往最先进的方法。

Conclusion: V-Shuffle通过破坏风格图像的语义内容并结合混合风格正则化，有效平衡了内容保留和风格保真度，在处理多张或单张风格图像时均能达到优异的性能。

Abstract: Attention injection-based style transfer has achieved remarkable progress in recent years. However, existing methods often suffer from content leakage, where the undesired semantic content of the style image mistakenly appears in the stylized output. In this paper, we propose V-Shuffle, a zero-shot style transfer method that leverages multiple style images from the same style domain to effectively navigate the trade-off between content preservation and style fidelity. V-Shuffle implicitly disrupts the semantic content of the style images by shuffling the value features within the self-attention layers of the diffusion model, thereby preserving low-level style representations. We further introduce a Hybrid Style Regularization that complements these low-level representations with high-level style textures to enhance style fidelity. Empirical results demonstrate that V-Shuffle achieves excellent performance when utilizing multiple style images. Moreover, when applied to a single style image, V-Shuffle outperforms previous state-of-the-art methods.

</details>


### [202] [HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment](https://arxiv.org/abs/2511.06653)
*Ruijia Wu,Ping Chen,Fei Shen,Shaoan Zhao,Qiang Hui,Huanlin Gao,Ting Lu,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: HiMo-CLIP通过引入分层分解和单调性感知对比损失，增强了CLIP模型处理复杂、长文本描述的能力，从而在图像-文本检索任务中取得了更好的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP类模型将文本视为扁平序列，无法捕捉语言的语义层次结构和语义单调性，这限制了它们处理复杂、组合性和长篇描述的能力，导致与视觉内容的对齐效果不佳。

Method: 本文提出了HiMo-CLIP框架，该框架在不修改编码器架构的情况下增强了CLIP模型。它包含两个关键组件：1) 分层分解 (HiDe) 模块，通过批内PCA从长文本中提取潜在语义组件，实现跨不同语义粒度的灵活对齐。2) 单调性感知对比损失 (MoLo)，它联合对齐全局和组件级别的表示，促使模型内化语义排序和对齐强度与文本完整性的关系。

Result: 在多个图像-文本检索基准测试中，HiMo-CLIP持续优于强大的基线模型，特别是在处理长或组合性描述时表现更佳。

Conclusion: HiMo-CLIP成功地通过引入语义层次和单调性，解决了CLIP模型在处理复杂文本时的局限性，从而产生了结构化、认知对齐的跨模态表示，并显著提升了检索性能。

Abstract: Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions. The code is available at https://github.com/UnicomAI/HiMo-CLIP.

</details>


### [203] [BuildingWorld: A Structured 3D Building Dataset for Urban Foundation Models](https://arxiv.org/abs/2511.06337)
*Shangfeng Huang,Ruisheng Wang,Xin Wang*

Main category: cs.CV

TL;DR: 该论文提出了BuildingWorld，一个包含约五百万个LOD2建筑模型和LiDAR点云的综合性3D建筑数据集，旨在解决现有数据集在建筑风格多样性方面的局限性，提升城市级基础模型的泛化能力。此外，还引入了Cyber City用于生成无限训练数据，并提供了标准化的评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生技术在现代城市转型中的核心地位日益凸显，精确且结构化的3D建筑模型成为实现高保真、可更新城市表示的关键。现有基于学习的3D城市建模方法因训练数据集建筑多样性有限而泛化能力不足，无法有效支持能源建模、城市规划、自动导航和实时推理等多样化应用。

Method: 该研究构建了BuildingWorld数据集，收集了来自北美、欧洲、亚洲、非洲和大洋洲等地理和建筑风格多样区域的约五百万个LOD2建筑模型，并附带真实及模拟的机载LiDAR点云。同时，引入了虚拟城市模型Cyber City，以生成结构多样化的无限训练数据。此外，还提供了针对建筑重建的标准化评估指标。

Result: BuildingWorld数据集为城市级基础建模和分析提供了全球代表性的数据资源，支持3D建筑重建、检测和分割的全面研究。Cyber City能够生成定制化且结构多样的点云训练数据。标准化的评估指标有助于促进大规模视觉模型和基础模型在结构化3D城市环境中的训练、评估和比较。

Conclusion: BuildingWorld数据集通过提供前所未有的建筑风格多样性，显著弥补了现有3D城市建模数据集的不足，极大地增强了学习模型的泛化能力。结合Cyber City的无限数据生成能力和标准化评估指标，该工作为推进3D城市数字孪生和相关应用的研究与发展奠定了坚实基础。

Abstract: As digital twins become central to the transformation of modern cities, accurate and structured 3D building models emerge as a key enabler of high-fidelity, updatable urban representations. These models underpin diverse applications including energy modeling, urban planning, autonomous navigation, and real-time reasoning. Despite recent advances in 3D urban modeling, most learning-based models are trained on building datasets with limited architectural diversity, which significantly undermines their generalizability across heterogeneous urban environments. To address this limitation, we present BuildingWorld, a comprehensive and structured 3D building dataset designed to bridge the gap in stylistic diversity. It encompasses buildings from geographically and architecturally diverse regions -- including North America, Europe, Asia, Africa, and Oceania -- offering a globally representative dataset for urban-scale foundation modeling and analysis. Specifically, BuildingWorld provides about five million LOD2 building models collected from diverse sources, accompanied by real and simulated airborne LiDAR point clouds. This enables comprehensive research on 3D building reconstruction, detection and segmentation. Cyber City, a virtual city model, is introduced to enable the generation of unlimited training data with customized and structurally diverse point cloud distributions. Furthermore, we provide standardized evaluation metrics tailored for building reconstruction, aiming to facilitate the training, evaluation, and comparison of large-scale vision models and foundation models in structured 3D urban environments.

</details>


### [204] [VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes](https://arxiv.org/abs/2511.06408)
*Zhengyu Zou,Jingfeng Li,Hao Li,Xiaolei Hou,Jinwen Hu,Jingkun Chen,Lechao Cheng,Dingwen Zhang*

Main category: cs.CV

TL;DR: VDNeRF是一种无需已知相机位姿信息，仅通过视觉输入即可恢复相机轨迹并学习动态城市场景时空表示的方法，它通过静态和动态NeRF模型以及鲁棒的训练框架，在相机位姿估计和动态新视角合成方面超越了现有无位姿NeRF方法。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF方法在自动驾驶和机器人感知等应用中面临挑战，主要原因在于难以获取精确的相机位姿信息，以及处理大规模动态环境的能力有限。

Method: VDNeRF使用两个独立的NeRF模型来联合重建场景。静态NeRF模型优化相机位姿和静态背景，而动态NeRF模型则结合3D场景流以确保动态物体的准确一致重建。为解决相机运动与独立物体运动之间的模糊性，设计了一个有效且强大的训练框架，以实现鲁棒的相机位姿估计和场景中静态与动态元素的自监督分解。

Result: 在主流城市驾驶数据集上的广泛评估表明，VDNeRF在相机位姿估计和动态新视角合成方面均超越了现有的最先进的基于NeRF的无位姿方法。

Conclusion: VDNeRF成功地解决了NeRF在无需额外传感器数据或已知相机位姿的情况下，进行动态城市场景中的相机位姿恢复和时空表示学习的挑战，并取得了卓越的性能。

Abstract: Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional scenes using a set of images with known camera poses, enabling the rendering of photorealistic novel views. However, existing NeRF-based methods encounter challenges in applications such as autonomous driving and robotic perception, primarily due to the difficulty of capturing accurate camera poses and limitations in handling large-scale dynamic environments. To address these issues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately recovers camera trajectories and learns spatiotemporal representations for dynamic urban scenes without requiring additional camera pose information or expensive sensor data. VDNeRF employs two separate NeRF models to jointly reconstruct the scene. The static NeRF model optimizes camera poses and static background, while the dynamic NeRF model incorporates the 3D scene flow to ensure accurate and consistent reconstruction of dynamic objects. To address the ambiguity between camera motion and independent object motion, we design an effective and powerful training framework to achieve robust camera pose estimation and self-supervised decomposition of static and dynamic elements in a scene. Extensive evaluations on mainstream urban driving datasets demonstrate that VDNeRF surpasses state-of-the-art NeRF-based pose-free methods in both camera pose estimation and dynamic novel view synthesis.

</details>


### [205] [Diagnose Like A REAL Pathologist: An Uncertainty-Focused Approach for Trustworthy Multi-Resolution Multiple Instance Learning](https://arxiv.org/abs/2511.06433)
*Sungrae Hong,Sol Lee,Jisu Shin,Mun Yong Yi*

Main category: cs.CV

TL;DR: 本研究提出了一种名为UFC-MIL的多分辨率多实例学习方法，旨在通过模仿病理学家行为并提供校准的诊断预测，提高组织病理学诊断的可靠性和性能，特别关注不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着组织病理学检查和诊断报告需求的增长，AI辅助诊断成为焦点。现有的多分辨率多实例学习（MIL）方法虽然提高了性能，但缺乏对诊断结果校准的研究，导致临床专家难以完全信任其诊断结果。

Method: 本文提出了不确定性聚焦校准多实例学习（UFC-MIL）。该方法利用多分辨率图像，包含一个新颖的逐块损失函数，用于学习实例的潜在模式并表达其分类不确定性。同时，采用基于注意力的架构和邻域块聚合模块来收集特征。此外，通过块级不确定性对聚合预测进行校准，无需多次迭代推理。

Result: 在具有挑战性的公共数据集上，UFC-MIL在模型校准方面表现出卓越的性能，同时实现了与最先进方法相当的分类准确性。

Conclusion: UFC-MIL通过更紧密地模仿病理学家的检查行为并提供校准的诊断预测，为组织病理学诊断提供了可靠且值得信赖的AI辅助解决方案，在校准和准确性方面均表现出色。

Abstract: With the increasing demand for histopathological specimen examination and diagnostic reporting, Multiple Instance Learning (MIL) has received heightened research focus as a viable solution for AI-centric diagnostic aid. Recently, to improve its performance and make it work more like a pathologist, several MIL approaches based on the use of multiple-resolution images have been proposed, delivering often higher performance than those that use single-resolution images. Despite impressive recent developments of multiple-resolution MIL, previous approaches only focus on improving performance, thereby lacking research on well-calibrated MIL that clinical experts can rely on for trustworthy diagnostic results. In this study, we propose Uncertainty-Focused Calibrated MIL (UFC-MIL), which more closely mimics the pathologists' examination behaviors while providing calibrated diagnostic predictions, using multiple images with different resolutions. UFC-MIL includes a novel patch-wise loss that learns the latent patterns of instances and expresses their uncertainty for classification. Also, the attention-based architecture with a neighbor patch aggregation module collects features for the classifier. In addition, aggregated predictions are calibrated through patch-level uncertainty without requiring multiple iterative inferences, which is a key practical advantage. Against challenging public datasets, UFC-MIL shows superior performance in model calibration while achieving classification accuracy comparable to that of state-of-the-art methods.

</details>


### [206] [EIDSeg: A Pixel-Level Semantic Segmentation Dataset for Post-Earthquake Damage Assessment from Social Media Images](https://arxiv.org/abs/2511.06456)
*Huili Huang,Chengeng Liu,Danrong Zhang,Shail Patel,Anastasiya Masalava,Sagar Sadak,Parisa Babolhavaeji,WeiHong Low,Max Mahdi Roozbahani,J. David Frost*

Main category: cs.CV

TL;DR: 本文介绍了EIDSeg，首个用于震后社交媒体图像语义分割的大规模数据集，包含3,266张图像和五种基础设施损伤类别，并提出了跨学科标注协议，基准测试显示Encoder-only Mask Transformer (EoMT)表现最佳，为更快速、精细的震后损伤评估铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 现有的震后快速损伤评估方法依赖昂贵的航空图像和专家标注，且早期评估仅产生二元损伤图，缺乏细节。尽管社交网络提供了宝贵的地面图像，但缺乏用于此任务的大规模像素级标注数据集。

Method: 研究人员构建了EIDSeg数据集，包含来自九次大地震的3,266张社交媒体图像，并标注了五类基础设施损伤（未受损建筑、受损建筑、被毁建筑、未受损道路、受损道路）。他们提出了一种实用的三阶段跨学科标注协议，实现了超过70%的标注者间一致性。此外，他们还对多种最先进的分割模型进行了基准测试。

Result: 成功构建了EIDSeg数据集，并制定了高效的标注协议，非专家标注者也能达到高一致性。在基准测试中，Encoder-only Mask Transformer (EoMT)表现最佳，其平均交并比（mIoU）达到80.8%。

Conclusion: 该工作通过利用社交网络丰富的地面视角，为震后场景中更快、更精细的损伤评估开辟了道路，解决了现有方法在成本、细节和数据可用性方面的局限性。

Abstract: Rapid post-earthquake damage assessment is crucial for rescue and resource planning. Still, existing remote sensing methods depend on costly aerial images, expert labeling, and produce only binary damage maps for early-stage evaluation. Although ground-level images from social networks provide a valuable source to fill this gap, a large pixel-level annotated dataset for this task is still unavailable. We introduce EIDSeg, the first large-scale semantic segmentation dataset specifically for post-earthquake social media imagery. The dataset comprises 3,266 images from nine major earthquakes (2008-2023), annotated across five classes of infrastructure damage: Undamaged Building, Damaged Building, Destroyed Building, Undamaged Road, and Damaged Road. We propose a practical three-phase cross-disciplinary annotation protocol with labeling guidelines that enables consistent segmentation by non-expert annotators, achieving over 70% inter-annotator agreement. We benchmark several state-of-the-art segmentation models, identifying Encoder-only Mask Transformer (EoMT) as the top-performing method with a Mean Intersection over Union (mIoU) of 80.8%. By unlocking social networks' rich ground-level perspective, our work paves the way for a faster, finer-grained damage assessment in the post-earthquake scenario.

</details>


### [207] [Countering Multi-modal Representation Collapse through Rank-targeted Fusion](https://arxiv.org/abs/2511.06450)
*Seulgi Kim,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 多模态融合常面临特征和模态崩溃问题。本文提出了一种基于有效秩的“秩增强令牌融合器”(R3D)框架，通过选择性融合互补特征来同时解决这两种崩溃，并在动作预测任务中显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 多模态融合方法普遍存在两种表示崩溃：特征崩溃（维度判别力丧失）和模态崩溃（一种模态主导）。现有方法通常单独处理这两种问题，缺乏一个能同时高效解决它们的统一框架，这阻碍了人类动作预测等应用的发展。

Method: 本文提出利用“有效秩”作为衡量和对抗两种表示崩溃的统一指标。在此基础上，提出了一个理论基础扎实的“秩增强令牌融合器”（Rank-enhancing Token Fuser）框架，该框架选择性地将一种模态中信息量较少的特征与另一种模态的互补特征进行融合，以提高融合表示的有效秩。为解决模态崩溃，该方法评估了能相互提升有效秩的模态组合，发现深度信息与RGB融合能保持表示平衡，避免模态崩溃。在动作预测任务中，提出了深度信息增强的融合框架R3D。

Result: 实验证明，所提出的方法能够增加融合表示的有效秩。在RGB与深度信息融合时，深度信息能保持表示平衡，有效避免模态崩溃。在NTURGBD、UTKinect和DARai数据集上的动作预测任务中，该方法显著优于现有最先进的方法，性能提升高达3.74%。

Conclusion: 利用有效秩作为统一量度，本文提出的秩增强令牌融合器(R3D)框架能有效解决多模态融合中的特征崩溃和模态崩溃问题。通过选择性融合互补特征，并利用深度信息保持模态平衡，R3D在动作预测任务中实现了卓越的性能提升。

Abstract: Multi-modal fusion methods often suffer from two types of representation collapse: feature collapse where individual dimensions lose their discriminative power (as measured by eigenspectra), and modality collapse where one dominant modality overwhelms the other. Applications like human action anticipation that require fusing multifarious sensor data are hindered by both feature and modality collapse. However, existing methods attempt to counter feature collapse and modality collapse separately. This is because there is no unifying framework that efficiently addresses feature and modality collapse in conjunction. In this paper, we posit the utility of effective rank as an informative measure that can be utilized to quantify and counter both the representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a theoretically grounded fusion framework that selectively blends less informative features from one modality with complementary features from another modality. We show that our method increases the effective rank of the fused representation. To address modality collapse, we evaluate modality combinations that mutually increase each others' effective rank. We show that depth maintains representational balance when fused with RGB, avoiding modality collapse. We validate our method on action anticipation, where we present \texttt{R3D}, a depth-informed fusion framework. Extensive experiments on NTURGBD, UTKinect, and DARai demonstrate that our approach significantly outperforms prior state-of-the-art methods by up to 3.74\%. Our code is available at: \href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.

</details>


### [208] [InfoAffect: A Dataset for Affective Analysis of Infographics](https://arxiv.org/abs/2511.06404)
*Zihang Fu,Yunchao Wang,Chenyu Huang,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 由于数据稀缺，信息图的情感维度研究不足。本文引入了一个包含3.5k样本的情感标注InfoAffect数据集，结合文本和真实世界信息图，并通过用户研究验证其高准确性。


<details>
  <summary>Details</summary>
Motivation: 信息图被广泛用于传达复杂信息，但由于数据资源稀缺，其情感维度尚未得到充分探索。

Method: 研究首先从六个领域收集原始数据，并通过预处理、伴随文本优先方法和三种策略进行对齐，以保证数据质量。随后构建情感表以约束标注。使用五种最先进的多模态大型语言模型（MLLMs）分析文本和图像两种模态，并通过倒数排名融合（RRF）算法融合其输出，以获得稳健的情感和置信度。最后，通过包含两个实验的用户研究来验证可用性，并使用复合情感一致性指数（CACI）评估InfoAffect数据集。

Result: 引入了一个包含3.5k样本的情感标注InfoAffect数据集，该数据集结合了文本内容和真实世界信息图。通过用户研究和CACI评估，数据集的总体分数达到0.986，表明其高准确性。

Conclusion: InfoAffect数据集的引入有效解决了信息图情感维度研究中数据稀缺的问题，并提供了高准确度的标注，为未来相关研究奠定了基础。

Abstract: Infographics are widely used to convey complex information, yet their affective dimensions remain underexplored due to the scarcity of data resources. We introduce a 3.5k-sample affect-annotated InfoAffect dataset, which combines textual content with real-world infographics. We first collect the raw data from six domains and aligned them via preprocessing, the accompanied-text-priority method, and three strategies to guarantee the quality and compliance. After that we construct an affect table and use it to constrain annotation. Five state-of-the-art multimodal large language models (MLLMs) then analyze both modalities, and their outputs are fused with Reciprocal Rank Fusion (RRF) algorithm to yield robust affects and confidences. We conducted a user study with two experiments to validate usability and assess InfoAffect dataset using the Composite Affect Consistency Index (CACI), achieving an overall score of 0.986, which indicates high accuracy.

</details>


### [209] [DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization](https://arxiv.org/abs/2511.06422)
*Tao Liu,Kan Ren,Qian Chen*

Main category: cs.CV

TL;DR: 本文提出DiffusionUavLoc，一个用于GNSS拒绝环境下无人机跨视角定位的框架。它通过图像提示、无文本、以扩散模型为中心，并利用VAE统一表示，解决了无人机斜视角与卫星正射图之间的几何和外观差异，实现了具有竞争力的定位性能。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济发展，无人机在智能巡逻中日益重要。然而，在GNSS拒绝环境中，仅依赖卫星信号的定位方案容易失效。基于跨视角图像检索的定位是一种有前景的替代方案，但无人机斜视角与卫星正射图之间存在显著的几何和外观域间隙。此外，现有方法通常依赖复杂的网络架构、文本提示或大量标注，限制了泛化能力。

Method: 本文提出了DiffusionUavLoc框架，它是一个图像提示、无文本、以扩散模型为中心并采用VAE进行统一表示的跨视角定位方法。首先，利用免训练的几何渲染从无人机图像合成伪卫星图像作为结构提示。然后，设计了一个无文本条件扩散模型，融合多模态结构线索，以学习对视角变化鲁棒的特征。在推理时，在固定的时间步t计算描述符，并使用余弦相似度进行比较。

Result: 该方法在University-1652和SUES-200数据集上实现了具有竞争力的跨视角定位性能，尤其是在University-1652数据集上的卫星到无人机定位方面表现突出。

Conclusion: DiffusionUavLoc提供了一个有效的跨视角定位框架，能够解决GNSS拒绝环境下无人机定位面临的挑战，并通过创新的扩散模型和图像提示机制，克服了几何和外观域间隙，且无需复杂的网络、文本或大量标注，展现了良好的泛化能力和定位精度。

Abstract: With the rapid growth of the low-altitude economy, unmanned aerial vehicles (UAVs) have become key platforms for measurement and tracking in intelligent patrol systems. However, in GNSS-denied environments, localization schemes that rely solely on satellite signals are prone to failure. Cross-view image retrieval-based localization is a promising alternative, yet substantial geometric and appearance domain gaps exist between oblique UAV views and nadir satellite orthophotos. Moreover, conventional approaches often depend on complex network architectures, text prompts, or large amounts of annotation, which hinders generalization. To address these issues, we propose DiffusionUavLoc, a cross-view localization framework that is image-prompted, text-free, diffusion-centric, and employs a VAE for unified representation. We first use training-free geometric rendering to synthesize pseudo-satellite images from UAV imagery as structural prompts. We then design a text-free conditional diffusion model that fuses multimodal structural cues to learn features robust to viewpoint changes. At inference, descriptors are computed at a fixed time step t and compared using cosine similarity. On University-1652 and SUES-200, the method performs competitively for cross-view localization, especially for satellite-to-drone in University-1652.Our data and code will be published at the following URL: https://github.com/liutao23/DiffusionUavLoc.git.

</details>


### [210] [SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports](https://arxiv.org/abs/2511.06499)
*Haotian Xia,Haonan Ge,Junbo Zou,Hyun Woo Choi,Xuebin Zhang,Danny Suradja,Botao Rui,Ethan Tran,Wendy Jin,Zhen Ye,Xiyang Lin,Christopher Lai,Shengjie Zhang,Junwen Miao,Shichao Chen,Rhys Tracy,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: SportR是一个新的大规模多运动基准，旨在训练和评估多模态大语言模型（MLLMs）在运动智能所需的细粒度视觉感知、规则推理和视觉基础方面的能力，现有模型在此基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 理解运动需要复杂的细粒度视觉感知和基于规则的推理，这超出了当前多模态模型的极限。现有运动基准要么只涵盖单一运动，要么缺乏详细的推理链和精确的视觉基础，无法在多运动背景下全面评估这些核心能力。

Method: 我们引入了SportR，第一个多运动大规模基准，包含5,017张图像和2,101段视频。它采用渐进式的问答（QA）对层次结构来探测不同深度的推理。对于需要多步推理的高级任务，我们提供了7,118个高质量的人工标注思维链（CoT）。此外，基准结合了图像和视频模态，并提供手动边界框标注以直接测试视觉基础。

Result: 广泛的实验表明，SportR基准难度极高。最先进的基线模型在最具挑战性的任务上表现不佳。尽管通过监督微调和强化学习在我们的数据上进行训练可以提高分数，但它们仍然相对较低，这突出表明当前模型能力存在显著差距。

Conclusion: SportR为社区提出了一个新挑战，提供了一个关键资源来推动未来多模态运动推理领域的研究。

Abstract: Deeply understanding sports requires an intricate blend of fine-grained visual perception and rule-based reasoning - a challenge that pushes the limits of current multimodal models. To succeed, models must master three critical capabilities: perceiving nuanced visual details, applying abstract sport rule knowledge, and grounding that knowledge in specific visual evidence. Current sports benchmarks either cover single sports or lack the detailed reasoning chains and precise visual grounding needed to robustly evaluate these core capabilities in a multi-sport context. To address this gap, we introduce SportR, the first multi-sports large-scale benchmark designed to train and evaluate MLLMs on the fundamental reasoning required for sports intelligence. Our benchmark provides a dataset of 5,017 images and 2,101 videos. To enable granular evaluation, we structure our benchmark around a progressive hierarchy of question-answer (QA) pairs designed to probe reasoning at increasing depths - from simple infraction identification to complex penalty prediction. For the most advanced tasks requiring multi-step reasoning, such as determining penalties or explaining tactics, we provide 7,118 high-quality, human-authored Chain of Thought (CoT) annotations. In addition, our benchmark incorporates both image and video modalities and provides manual bounding box annotations to test visual grounding in the image part directly. Extensive experiments demonstrate the profound difficulty of our benchmark. State-of-the-art baseline models perform poorly on our most challenging tasks. While training on our data via Supervised Fine-Tuning and Reinforcement Learning improves these scores, they remain relatively low, highlighting a significant gap in current model capabilities. SportR presents a new challenge for the community, providing a critical resource to drive future research in multimodal sports reasoning.

</details>


### [211] [NOAH: Benchmarking Narrative Prior driven Hallucination and Omission in Video Large Language Models](https://arxiv.org/abs/2511.06475)
*Kyuho Lee,Euntae Kim,Jinwoo Choi,Buru Chang*

Main category: cs.CV

TL;DR: 视频大型语言模型（Video LLMs）在追求叙事连贯性时，会产生“叙事先验”偏见，导致幻觉和遗漏错误。本文引入NOAH基准，通过构建复合视频和设计多项任务，系统评估了这些错误，发现大多数Video LLMs存在此类问题，且错误模式受架构、事件相似性和插入位置影响，在稀疏采样下错误更严重。


<details>
  <summary>Details</summary>
Motivation: Video LLMs通过鼓励事件连续性来增强叙事连贯性，但这引入了一种“叙事先验”偏见，即模型优先考虑故事情节一致性而非严格的视觉证据。这种偏见导致了幻觉（引入不存在事件或误解现有事件）和遗漏（压制与上下文不符的真实事件）错误。因此，需要系统评估这种偏见导致的错误。

Method: 本文提出了NOAH，一个大规模基准，通过将其他来源的片段插入目标视频来构建复合视频。该基准通过改变语义相似性和插入位置，实现对叙事先验的受控和可扩展分析。设计了一个带有定制指标的字幕任务和三个问答任务（存在性、时间性、叙事性），共生成超过6万个评估样本，并进行了广泛实验。

Result: (i) 大多数Video LLMs表现出由叙事先验驱动的幻觉和遗漏错误；(ii) 这些错误的模式因架构而异，并取决于事件相似性和插入位置；(iii) 在帧数较少的稀疏采样下，模型对叙事先验的依赖性增强，当事件连续性较弱时，错误会被放大。

Conclusion: NOAH是首个用于标准化评估Video LLMs中由叙事先验引起的幻觉和遗漏的基准。它为开发更可靠、更值得信赖的模型奠定了基础。

Abstract: Video large language models (Video LLMs) have recently achieved strong performance on tasks such as captioning, summarization, and question answering. Many models and training methods explicitly encourage continuity across events to enhance narrative coherence. While this improves fluency, it also introduces an inductive bias that prioritizes storyline consistency over strict grounding in visual evidence. We identify this bias, which we call narrative prior, as a key driver of two errors: hallucinations, where non-existent events are introduced or existing ones are misinterpreted, and omissions, where factual events are suppressed because they are misaligned with surrounding context. To systematically evaluate narrative prior-induced errors, we introduce NOAH, a large-scale benchmark that constructs composite videos by inserting clips from other sources into target videos. By varying semantic similarity and insertion position, our benchmark enables controlled and scalable analysis of narrative priors. We design one captioning task with tailored metrics and three QA tasks - Existence, Temporal, and Narrative - yielding more than 60K evaluation samples. Extensive experiments yield three key findings: (i) most Video LLMs exhibit hallucinations and omissions driven by narrative priors, (ii) the patterns of these errors vary across architectures and depend on event similarity and insertion position, and (iii) reliance on narrative priors intensifies under sampling with fewer frames, amplifying errors when event continuity is weak. We establish NOAH as the first standardized evaluation of narrative prior-induced hallucination and omission in Video LLMs, providing a foundation for developing more reliable and trustworthy models. Our benchmark and code are available at https://anonymous550520.github.io/.

</details>


### [212] [UniADC: A Unified Framework for Anomaly Detection and Classification](https://arxiv.org/abs/2511.06644)
*Ximiao Zhang,Min Xu,Zheng Zhang,Junlin Hu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 本文提出UniADC，一个统一的异常检测与分类模型，通过一个免训练的修复网络和多任务判别器，在少量甚至没有异常图像的情况下，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将异常检测和分类视为独立任务，忽视了它们固有的相关性，限制了信息共享，导致性能不佳。

Method: UniADC模型包含两个核心组件：1) 一个免训练的可控修复网络，通过异常先验指导重绘正常区域来合成特定类别的异常图像，并能增强少样本异常数据。2) 一个多任务判别器，在合成样本上训练，通过将细粒度图像特征与异常类别嵌入对齐，实现精确的异常检测和分类。

Result: 在MVTec-FS、MTD和WFDD三个异常检测和分类数据集上进行的大量实验表明，UniADC在异常检测、定位和分类方面始终优于现有方法。

Conclusion: UniADC成功地将异常检测和分类任务统一起来，即使在异常图像极少或没有的情况下，也能通过合成数据和多任务判别器实现卓越的性能。

Abstract: In this paper, we introduce the task of unified anomaly detection and classification, which aims to simultaneously detect anomalous regions in images and identify their specific categories. Existing methods typically treat anomaly detection and classification as separate tasks, thereby neglecting their inherent correlation, limiting information sharing, and resulting in suboptimal performance. To address this, we propose UniADC, a unified anomaly detection and classification model that can effectively perform both tasks with only a few or even no anomaly images. Specifically, UniADC consists of two key components: a training-free controllable inpainting network and a multi-task discriminator. The inpainting network can synthesize anomaly images of specific categories by repainting normal regions guided by anomaly priors, and can also repaint few-shot anomaly samples to augment the available anomaly data. The multi-task discriminator is then trained on these synthesized samples, enabling precise anomaly detection and classification by aligning fine-grained image features with anomaly-category embeddings. We conduct extensive experiments on three anomaly detection and classification datasets, including MVTec-FS, MTD, and WFDD, and the results demonstrate that UniADC consistently outperforms existing methods in anomaly detection, localization, and classification. The code is available at https://github.com/cnulab/UniADC.

</details>


### [213] [DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting](https://arxiv.org/abs/2511.06632)
*Chenpeng Su,Wenhua Wu,Chensheng Peng,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: DIAL-GS是一种新颖的自监督动态实例感知重建方法，利用4D高斯泼溅技术，通过识别动态实例和引入互惠机制，实现了城市场景的高质量重建和精细化编辑。


<details>
  <summary>Details</summary>
Motivation: 城市场景重建对自动驾驶至关重要，但监督方法成本高昂且缺乏可扩展性。现有自监督方法常混淆静态与动态元素，且无法区分单个动态对象，限制了细粒度编辑。

Method: 本文提出了DIAL-GS，一种基于4D高斯泼溅的动态实例感知重建方法。首先，通过利用扭曲渲染与实际观测之间的外观-位置不一致性，准确识别动态实例。其次，以实例级动态感知为指导，采用实例感知的4D高斯作为统一的体表示，实现动态自适应和实例感知的重建。此外，引入了一种互惠机制，使身份和动态相互加强，提升了完整性和一致性。

Result: 在城市驾驶场景的实验表明，DIAL-GS在重建质量和实例级编辑方面超越了现有自监督基线。

Conclusion: DIAL-GS为城市场景建模提供了一个简洁而强大的解决方案，实现了优异的重建效果和实例级编辑能力。

Abstract: Urban scene reconstruction is critical for autonomous driving, enabling structured 3D representations for data synthesis and closed-loop testing. Supervised approaches rely on costly human annotations and lack scalability, while current self-supervised methods often confuse static and dynamic elements and fail to distinguish individual dynamic objects, limiting fine-grained editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction method for label-free street scenes with 4D Gaussian Splatting. We first accurately identify dynamic instances by exploiting appearance-position inconsistency between warped rendering and actual observation. Guided by instance-level dynamic perception, we employ instance-aware 4D Gaussians as the unified volumetric representation, realizing dynamic-adaptive and instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism through which identity and dynamics reinforce each other, enhancing both integrity and consistency. Experiments on urban driving scenarios show that DIAL-GS surpasses existing self-supervised baselines in reconstruction quality and instance-level editing, offering a concise yet powerful solution for urban scene modeling.

</details>


### [214] [Video Dataset for Surgical Phase, Keypoint, and Instrument Recognition in Laparoscopic Surgery (PhaKIR)](https://arxiv.org/abs/2511.06549)
*Tobias Rueckert,Raphaela Maerkl,David Rauber,Leonard Klausmann,Max Gutbrod,Daniel Rueckert,Hubertus Feussner,Dirk Wilhelm,Christoph Palm*

Main category: cs.CV

TL;DR: 本文介绍了PhaKIR数据集，这是一个用于机器人辅助微创手术(RAMIS)计算机视觉任务的多中心、全手术序列、多任务标注数据集，旨在解决现有数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器人和计算机辅助微创手术(RAMIS)越来越依赖计算机视觉进行器械识别和手术流程理解。然而，开发此类系统所需的大型、良好标注的数据集通常存在任务孤立、忽略时间依赖性或缺乏多中心变异性等问题。

Method: 本文提出了PhaKIR（Surgical Procedure Phase, Keypoint, and Instrument Recognition）数据集，包含来自三个医疗中心的八个完整腹腔镜胆囊切除术视频。该数据集提供帧级标注，用于三个相互关联的任务：手术阶段识别、器械关键点估计和器械实例分割。

Result: PhaKIR是首个联合提供手术阶段标签、器械姿态信息和像素级精确器械分割的多机构数据集，并且由于包含完整的手术过程序列，它还支持利用时间上下文。该数据集共包含485,875帧的手术阶段识别标注，以及19,435帧的器械关键点估计和器械实例分割标注。

Conclusion: PhaKIR数据集作为MICCAI 2024 EndoVis挑战赛中PhaKIR挑战的基础，用于评估手术场景理解方法，进一步验证了数据集的质量和相关性。该数据集可公开获取。

Abstract: Robotic- and computer-assisted minimally invasive surgery (RAMIS) is increasingly relying on computer vision methods for reliable instrument recognition and surgical workflow understanding. Developing such systems often requires large, well-annotated datasets, but existing resources often address isolated tasks, neglect temporal dependencies, or lack multi-center variability. We present the Surgical Procedure Phase, Keypoint, and Instrument Recognition (PhaKIR) dataset, comprising eight complete laparoscopic cholecystectomy videos recorded at three medical centers. The dataset provides frame-level annotations for three interconnected tasks: surgical phase recognition (485,875 frames), instrument keypoint estimation (19,435 frames), and instrument instance segmentation (19,435 frames). PhaKIR is, to our knowledge, the first multi-institutional dataset to jointly provide phase labels, instrument pose information, and pixel-accurate instrument segmentations, while also enabling the exploitation of temporal context since full surgical procedure sequences are available. It served as the basis for the PhaKIR Challenge as part of the Endoscopic Vision (EndoVis) Challenge at MICCAI 2024 to benchmark methods in surgical scene understanding, thereby further validating the dataset's quality and relevance. The dataset is publicly available upon request via the Zenodo platform.

</details>


### [215] [Spatial-Frequency Enhanced Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2511.06593)
*Hui Sun,Long Lv,Pingping Zhang,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出SFMFusion，一个基于Mamba的新型多模态图像融合（MMIF）框架，通过空间-频率增强的Mamba模块和多分支结构有效结合图像重建，克服了现有CNN和Transformer方法的局限性，并实现了优异的融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的MMIF方法存在不足：CNN感受野有限，Transformer计算成本高。Mamba虽能处理长距离依赖但缺乏空间和频率感知能力，而这些对MMIF至关重要。此外，如何高效利用图像重建（IR）作为辅助任务也是一个挑战。

Method: 提出SFMFusion框架：1) 采用三分支结构耦合MMIF和IR，以保留源图像的完整内容。2) 提出空间-频率增强Mamba块（SFMB），在空间和频率域增强Mamba，以实现全面的特征提取。3) 提出动态融合Mamba块（DFMB），用于跨不同分支的动态特征融合。

Result: 在六个MMIF数据集上，所提方法取得了比大多数现有最先进方法更好的结果。

Conclusion: SFMFusion框架通过创新性地结合空间-频率增强的Mamba模块和多分支图像重建结构，成功解决了现有MMIF方法的局限性，显著提升了多模态图像融合的性能。

Abstract: Multi-Modal Image Fusion (MMIF) aims to integrate complementary image information from different modalities to produce informative images. Previous deep learning-based MMIF methods generally adopt Convolutional Neural Networks (CNNs) or Transformers for feature extraction. However, these methods deliver unsatisfactory performances due to the limited receptive field of CNNs and the high computational cost of Transformers. Recently, Mamba has demonstrated a powerful potential for modeling long-range dependencies with linear complexity, providing a promising solution to MMIF. Unfortunately, Mamba lacks full spatial and frequency perceptions, which are very important for MMIF. Moreover, employing Image Reconstruction (IR) as an auxiliary task has been proven beneficial for MMIF. However, a primary challenge is how to leverage IR efficiently and effectively. To address the above issues, we propose a novel framework named Spatial-Frequency Enhanced Mamba Fusion (SFMFusion) for MMIF. More specifically, we first propose a three-branch structure to couple MMIF and IR, which can retain complete contents from source images. Then, we propose the Spatial-Frequency Enhanced Mamba Block (SFMB), which can enhance Mamba in both spatial and frequency domains for comprehensive feature extraction. Finally, we propose the Dynamic Fusion Mamba Block (DFMB), which can be deployed across different branches for dynamic feature fusion. Extensive experiments show that our method achieves better results than most state-of-the-art methods on six MMIF datasets. The source code is available at https://github.com/SunHui1216/SFMFusion.

</details>


### [216] [Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes](https://arxiv.org/abs/2511.06457)
*Shaoxiang Wang,Shihong Zhang,Christen Millerdurai,Rüdiger Westermann,Didier Stricker,Alain Pagani*

Main category: cs.CV

TL;DR: Inpaint360GS是一个基于3D高斯泼溅的360度场景编辑框架，解决了多对象去除和高保真修复的挑战，通过2D分割蒸馏到3D和虚拟相机视图实现。


<details>
  <summary>Details</summary>
Motivation: 尽管单对象正面修复在NeRF和3DGS方面取得了进展，但复杂的360度场景修复仍未得到充分探索。主要挑战包括：(i) 在360度环境中识别3D目标对象，(ii) 处理多对象场景中严重的遮挡，难以定义修复区域，以及(iii) 有效地在不同视角间保持一致和高质量的外观。

Method: 本文提出了Inpaint360GS，一个灵活的基于3DGS的360度编辑框架，支持3D空间中的多对象移除和高保真修复。该方法通过将2D分割蒸馏到3D，并利用虚拟相机视图进行上下文指导，实现精确的对象级编辑和一致的场景补全。此外，还引入了一个新的360度修复数据集，以解决缺乏无对象场景真实数据的问题。

Result: 实验证明Inpaint360GS优于现有基线，并取得了最先进的性能，实现了准确的对象级编辑和一致的场景补全。

Conclusion: Inpaint360GS提供了一个灵活的360度编辑框架，基于3DGS实现了多对象移除和高保真修复，有效解决了360度复杂场景修复中的关键挑战，并达到了最先进的水平。

Abstract: Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360° scenes remains largely underexplored. This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360° environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively. To tackle these challenges, we propose Inpaint360GS, a flexible 360° editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space. By distilling 2D segmentation into 3D and leveraging virtual camera views for contextual guidance, our method enables accurate object-level editing and consistent scene completion. We further introduce a new dataset tailored for 360° inpainting, addressing the lack of ground truth object-free scenes. Experiments demonstrate that Inpaint360GS outperforms existing baselines and achieves state-of-the-art performance. Project page: https://dfki-av.github.io/inpaint360gs/

</details>


### [217] [REOcc: Camera-Radar Fusion with Radar Feature Enrichment for 3D Occupancy Prediction](https://arxiv.org/abs/2511.06666)
*Chaehee Song,Sanmin Kim,Hyeonjun Jeong,Juyeb Shin,Joonhee Lim,Dongsuk Kum*

Main category: cs.CV

TL;DR: REOcc是一种新颖的相机-雷达融合网络，通过雷达稠密器和雷达放大器丰富雷达特征，有效解决了雷达数据的稀疏性和噪声问题，显著提升了3D占用预测性能，尤其是在动态物体方面。


<details>
  <summary>Details</summary>
Motivation: 基于相机的3D占用预测在复杂环境中表现不佳。相机-雷达融合虽有潜力，但雷达数据的稀疏性和噪声限制了其有效性，导致融合性能不理想。

Method: 本文提出了REOcc，一个相机-雷达融合网络，包含两个主要组件：雷达稠密器（Radar Densifier）和雷达放大器（Radar Amplifier）。它们通过整合空间和上下文信息来精炼雷达特征，有效提升了雷达数据的空间密度和质量。

Result: 在Occ3D-nuScenes基准测试中，REOcc相较于纯相机基线模型取得了显著的性能提升，尤其是在动态物体类别上。这表明REOcc能够有效缓解雷达数据的稀疏性和噪声问题。

Conclusion: REOcc成功地增强了雷达数据的质量，使其能更有效地补充相机数据，从而充分发挥相机-雷达融合在稳健可靠的3D占用预测中的潜力。

Abstract: Vision-based 3D occupancy prediction has made significant advancements, but its reliance on cameras alone struggles in challenging environments. This limitation has driven the adoption of sensor fusion, among which camera-radar fusion stands out as a promising solution due to their complementary strengths. However, the sparsity and noise of the radar data limits its effectiveness, leading to suboptimal fusion performance. In this paper, we propose REOcc, a novel camera-radar fusion network designed to enrich radar feature representations for 3D occupancy prediction. Our approach introduces two main components, a Radar Densifier and a Radar Amplifier, which refine radar features by integrating spatial and contextual information, effectively enhancing spatial density and quality. Extensive experiments on the Occ3D-nuScenes benchmark demonstrate that REOcc achieves significant performance gains over the camera-only baseline model, particularly in dynamic object classes. These results underscore REOcc's capability to mitigate the sparsity and noise of the radar data. Consequently, radar complements camera data more effectively, unlocking the full potential of camera-radar fusion for robust and reliable 3D occupancy prediction.

</details>


### [218] [NOVO: Bridging LLaVA and SAM with Visual-only Prompts for Reasoning Segmentation](https://arxiv.org/abs/2511.06651)
*Kyung-Yoon Yoon,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本研究提出了NOVO框架，通过纯视觉提示连接视觉语言模型（VLM）和分割模型，实现了推理分割，并引入了无训练的细化模块和新的RISeg基准，取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 以往方法将文本衍生的SEG token嵌入输入分割模型，而本研究旨在通过纯视觉提示来连接视觉语言模型和分割模型，以更好地利用预训练分割模型（如SAM）的能力，并提升推理分割的性能和边界质量。

Method: NOVO框架通过VLM输出生成粗略掩码和点提示，这些视觉提示与Segment Anything Model (SAM)兼容。为进一步提高边界质量和实现实例级分割，引入了一个无训练的细化模块。此外，还提出了一个包含918张图像、2533个实例级掩码和多样化推理查询的新基准RISeg用于评估。

Result: 实验证明，NOVO在多个指标和模型尺寸上均实现了最先进的性能，展示了其在推理分割方面的有效性和可扩展性。

Conclusion: NOVO是一个有效且可扩展的推理分割框架，通过纯视觉提示成功地连接了VLM和分割模型，并通过无训练的细化模块提升了分割质量，达到了最先进的性能。

Abstract: In this study, we propose NOVO (NO text, Visual-Only prompts), a novel framework that bridges vision-language models (VLMs) and segmentation models through visual-only prompts. Unlike prior approaches that feed text-derived SEG token embeddings into segmentation models, NOVO instead generates a coarse mask and point prompts from the VLM output. These visual prompts are compatible with the Segment Anything Model (SAM), preserving alignment with its pretrained capabilities. To further enhance boundary quality and enable instance-level segmentation, we introduce a training-free refinement module that reduces visual artifacts and improves the quality of segmentation masks. We also present RISeg, a new benchmark comprising 918 images, 2,533 instance-level masks, and diverse reasoning queries to evaluate this task. Experiments demonstrate that NOVO achieves state-of-the-art performance across multiple metrics and model sizes, demonstrating its effectiveness and scalability in reasoning segmentation.

</details>


### [219] [FreqGRL: Suppressing Low-Frequency Bias and Mining High-Frequency Knowledge for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2511.06648)
*Siqi Hui,Sanping Zhou,Ye deng,Wenli Huang,Jinjun Wang*

Main category: cs.CV

TL;DR: 本文提出FreqGRL，一个基于频率空间的跨域少样本学习（CD-FSL）框架，通过替换源域低频成分、增强高频特征和全局频率滤波，解决源域偏差和目标域稀疏性问题，实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本学习（CD-FSL）面临显著的域偏移和源域数据丰富而目标域数据稀缺的严重不平衡问题。现有模型容易偏向源域数据中低频成分编码的特定知识，且稀疏的目标域数据阻碍了高频、域泛化特征的学习。

Method: 本文提出FreqGRL框架，从频率空间角度解决问题：
1.  **低频替换（LFR）模块**：用目标域的低频成分替换源任务的低频成分，创建与目标域更匹配的新源任务，以减少源域特定偏差并促进泛化表示学习。
2.  **高频增强（HFE）模块**：滤除低频成分，直接在频率空间的高频特征上进行学习，以提高跨域泛化能力。
3.  **全局频率滤波器（GFF）**：抑制噪声或不相关的频率，强调信息丰富的频率，以减轻有限目标监督下的过拟合风险。

Result: 在五个标准CD-FSL基准测试上进行了广泛实验，证明所提出的频率引导框架取得了最先进的性能。

Conclusion: FreqGRL框架通过在频率空间中处理数据不平衡和域偏移问题，有效缓解了源域偏差并增强了泛化特征学习，从而在CD-FSL任务中取得了卓越表现。

Abstract: Cross-domain few-shot learning (CD-FSL) aims to recognize novel classes with only a few labeled examples under significant domain shifts. While recent approaches leverage a limited amount of labeled target-domain data to improve performance, the severe imbalance between abundant source data and scarce target data remains a critical challenge for effective representation learning. We present the first frequency-space perspective to analyze this issue and identify two key challenges: (1) models are easily biased toward source-specific knowledge encoded in the low-frequency components of source data, and (2) the sparsity of target data hinders the learning of high-frequency, domain-generalizable features. To address these challenges, we propose \textbf{FreqGRL}, a novel CD-FSL framework that mitigates the impact of data imbalance in the frequency space. Specifically, we introduce a Low-Frequency Replacement (LFR) module that substitutes the low-frequency components of source tasks with those from the target domain to create new source tasks that better align with target characteristics, thus reducing source-specific biases and promoting generalizable representation learning. We further design a High-Frequency Enhancement (HFE) module that filters out low-frequency components and performs learning directly on high-frequency features in the frequency space to improve cross-domain generalization. Additionally, a Global Frequency Filter (GFF) is incorporated to suppress noisy or irrelevant frequencies and emphasize informative ones, mitigating overfitting risks under limited target supervision. Extensive experiments on five standard CD-FSL benchmarks demonstrate that our frequency-guided framework achieves state-of-the-art performance.

</details>


### [220] [Flexible Concept Bottleneck Model](https://arxiv.org/abs/2511.06678)
*Xingbo Du,Qiantong Dou,Lei Fan,Rui Zhang*

Main category: cs.CV

TL;DR: FCBM是一种灵活的概念瓶颈模型，通过超网络和改进的sparsemax模块，支持概念的动态适应和替换，无需完全重新训练，提高了模型在真实世界场景中的适应性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型（VLM）的概念瓶颈模型在引入新概念时通常需要完全重新训练整个模型，这限制了它们在快速演进的VLM背景下的适应性和灵活性。

Method: 本文提出了灵活概念瓶颈模型（FCBM）。它设计了一个超网络，根据概念嵌入生成预测权重，从而在不重新训练整个模型的情况下无缝集成新概念。此外，引入了一个带有可学习温度参数的改进sparsemax模块，动态选择最相关的概念。

Result: 在五个公共基准测试中，FCBM在有效概念数量相似的情况下，实现了与最先进基线相当的准确性。模型通过单次epoch的微调，对未见概念也展现出良好的泛化能力，证明了其强大的适应性和灵活性。

Conclusion: FCBM通过支持概念的动态适应和替换，解决了现有VLM-based CBMs在引入新概念时需要完全重新训练的问题，显著提高了模型的适应性和灵活性，同时保持了高准确性。

Abstract: Concept bottleneck models (CBMs) improve neural network interpretability by introducing an intermediate layer that maps human-understandable concepts to predictions. Recent work has explored the use of vision-language models (VLMs) to automate concept selection and annotation. However, existing VLM-based CBMs typically require full model retraining when new concepts are involved, which limits their adaptability and flexibility in real-world scenarios, especially considering the rapid evolution of vision-language foundation models. To address these issues, we propose Flexible Concept Bottleneck Model (FCBM), which supports dynamic concept adaptation, including complete replacement of the original concept set. Specifically, we design a hypernetwork that generates prediction weights based on concept embeddings, allowing seamless integration of new concepts without retraining the entire model. In addition, we introduce a modified sparsemax module with a learnable temperature parameter that dynamically selects the most relevant concepts, enabling the model to focus on the most informative features. Extensive experiments on five public benchmarks demonstrate that our method achieves accuracy comparable to state-of-the-art baselines with a similar number of effective concepts. Moreover, the model generalizes well to unseen concepts with just a single epoch of fine-tuning, demonstrating its strong adaptability and flexibility.

</details>


### [221] [AnoStyler: Text-Driven Localized Anomaly Generation via Lightweight Style Transfer](https://arxiv.org/abs/2511.06687)
*Yulim So,Seokho Kang*

Main category: cs.CV

TL;DR: AnoStyler是一种轻量级、零样本异常生成方法，它将异常生成视为文本引导的风格迁移。它使用单个正常图像、类别标签和缺陷类型，通过生成异常掩码和文本提示，并利用轻量级U-Net模型和CLIP损失函数，生成视觉真实且语义对齐的异常图像，从而克服了现有方法的局限性，并提高了异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的异常生成方法存在至少以下一个局限性，阻碍了其实际部署：1) 生成的异常缺乏视觉真实感；2) 依赖大量真实图像；3) 使用内存密集、重量级的模型架构。

Method: AnoStyler将零样本异常生成框架为文本引导的风格迁移。给定一个正常图像、其类别标签和预期的缺陷类型，它通过通用的、类别无关的程序生成异常掩码（指示局部异常区域）和代表正常/异常状态的两类文本提示。一个使用CLIP基损失函数训练的轻量级U-Net模型用于将正常图像风格化为视觉真实的异常图像，其中异常由掩码定位并与文本提示语义对齐。

Result: 在MVTec-AD和VisA数据集上的大量实验表明，AnoStyler在生成高质量和多样化的异常图像方面优于现有方法。此外，使用这些生成的异常有助于提高异常检测性能。

Conclusion: AnoStyler提供了一种轻量级而有效的方法，通过文本引导的风格迁移实现零样本异常生成，解决了现有方法的真实性、数据依赖性和模型复杂性问题。它能够生成高质量、多样化的异常图像，并能有效提升异常检测的性能。

Abstract: Anomaly generation has been widely explored to address the scarcity of anomaly images in real-world data. However, existing methods typically suffer from at least one of the following limitations, hindering their practical deployment: (1) lack of visual realism in generated anomalies; (2) dependence on large amounts of real images; and (3) use of memory-intensive, heavyweight model architectures. To overcome these limitations, we propose AnoStyler, a lightweight yet effective method that frames zero-shot anomaly generation as text-guided style transfer. Given a single normal image along with its category label and expected defect type, an anomaly mask indicating the localized anomaly regions and two-class text prompts representing the normal and anomaly states are generated using generalizable category-agnostic procedures. A lightweight U-Net model trained with CLIP-based loss functions is used to stylize the normal image into a visually realistic anomaly image, where anomalies are localized by the anomaly mask and semantically aligned with the text prompts. Extensive experiments on the MVTec-AD and VisA datasets show that AnoStyler outperforms existing anomaly generation methods in generating high-quality and diverse anomaly images. Furthermore, using these generated anomalies helps enhance anomaly detection performance.

</details>


### [222] [SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection](https://arxiv.org/abs/2511.06702)
*Yifan Wang,Yian Zhao,Fanqi Pu,Xiaochen Yang,Yang Tang,Xi Chen,Wenming Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为空间投影对齐（SPAN）的新方法，通过显式地引入空间和投影对齐约束，以及分层任务学习策略，解决了单目3D检测中解耦预测范式导致的几何不一致问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D检测器通过解耦预测（分别估计几何中心、深度、尺寸和旋转角度）来简化3D边界框的非线性回归。然而，这种策略忽略了不同属性之间的几何协作约束，导致缺乏几何一致性先验，从而性能不佳。

Method: 本文提出了空间投影对齐（SPAN）方法，包含两个关键组件：(i) 空间点对齐，强制预测和真实3D边界框之间存在明确的全局空间约束，纠正解耦属性回归引起的空间漂移。(ii) 3D-2D投影对齐，确保投影的3D框紧密对齐其在图像平面上的2D检测框，弥补了先前工作中忽略的投影未对齐问题。为确保训练稳定性，还引入了分层任务学习策略，随着3D属性预测的细化逐步整合空间投影对齐，防止早期阶段错误在属性间传播。

Result: 广泛的实验表明，所提出的方法可以轻松集成到任何已有的单目3D检测器中，并能带来显著的性能提升。

Conclusion: 通过引入空间投影对齐和分层任务学习策略，本方法有效解决了单目3D检测中因解耦预测导致的几何不一致问题，从而显著提高了检测性能和鲁棒性。

Abstract: Existing monocular 3D detectors typically tame the pronounced nonlinear regression of 3D bounding box through decoupled prediction paradigm, which employs multiple branches to estimate geometric center, depth, dimensions, and rotation angle separately. Although this decoupling strategy simplifies the learning process, it inherently ignores the geometric collaborative constraints between different attributes, resulting in the lack of geometric consistency prior, thereby leading to suboptimal performance. To address this issue, we propose novel Spatial-Projection Alignment (SPAN) with two pivotal components: (i). Spatial Point Alignment enforces an explicit global spatial constraint between the predicted and ground-truth 3D bounding boxes, thereby rectifying spatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection Alignment ensures that the projected 3D box is aligned tightly within its corresponding 2D detection bounding box on the image plane, mitigating projection misalignment overlooked in previous works. To ensure training stability, we further introduce a Hierarchical Task Learning strategy that progressively incorporates spatial-projection alignment as 3D attribute predictions refine, preventing early stage error propagation across attributes. Extensive experiments demonstrate that the proposed method can be easily integrated into any established monocular 3D detector and delivers significant performance improvements.

</details>


### [223] [K-Stain: Keypoint-Driven Correspondence for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2511.06709)
*Sicheng Yang,Zhaohu Xing,Haipeng Zhou,Lei Zhu*

Main category: cs.CV

TL;DR: K-Stain是一种新型虚拟染色框架，通过利用关键点来解决组织切片错位问题，从而在H&E到IHC图像转换中实现更精确的对齐和空间信息整合，显著提升合成IHC图像的保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟染色方法（将H&E图像转换为IHC图像）因组织切片错位而难以有效利用空间信息，导致合成图像的准确性受限。研究旨在克服这一挑战，提高合成IHC图像的质量。

Method: 本文提出了K-Stain框架，包含三个核心组件：(1) 分层空间关键点检测器（HSKD），用于识别染色图像中的关键点；(2) 关键点感知增强生成器（KEG），在图像生成过程中整合这些关键点；(3) 关键点引导判别器（KGD），提高判别器对空间细节的敏感性。该方法还利用相邻切片的上下文信息。

Result: 广泛的实验表明，K-Stain在定量指标和视觉质量方面均优于现有最先进的方法，能生成更准确、视觉上更一致的IHC图像。

Conclusion: K-Stain通过引入基于关键点的空间和语义关系，有效解决了虚拟染色中的对齐和空间信息利用问题，显著提升了合成IHC图像的保真度和质量。

Abstract: Virtual staining offers a promising method for converting Hematoxylin and Eosin (H&E) images into Immunohistochemical (IHC) images, eliminating the need for costly chemical processes. However, existing methods often struggle to utilize spatial information effectively due to misalignment in tissue slices. To overcome this challenge, we leverage keypoints as robust indicators of spatial correspondence, enabling more precise alignment and integration of structural details in synthesized IHC images. We introduce K-Stain, a novel framework that employs keypoint-based spatial and semantic relationships to enhance synthesized IHC image fidelity. K-Stain comprises three main components: (1) a Hierarchical Spatial Keypoint Detector (HSKD) for identifying keypoints in stain images, (2) a Keypoint-aware Enhancement Generator (KEG) that integrates these keypoints during image generation, and (3) a Keypoint Guided Discriminator (KGD) that improves the discriminator's sensitivity to spatial details. Our approach leverages contextual information from adjacent slices, resulting in more accurate and visually consistent IHC images. Extensive experiments show that K-Stain outperforms state-of-the-art methods in quantitative metrics and visual quality.

</details>


### [224] [Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning](https://arxiv.org/abs/2511.06734)
*Qianfeng Yang,Xiang Chen,Pengpeng Li,Qiyuan Guan,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 针对雨水对多视角图像和3D场景重建的负面影响，本文构建了一个名为OmniRain3D的新数据集，模拟真实雨景的视角异质性和亮度动态性。在此基础上，提出了REVR-GSNet框架，通过递归亮度增强、高斯基元优化和GS引导的去雨联合交替优化，实现从雨中图像高保真重建清晰3D场景。


<details>
  <summary>Details</summary>
Motivation: 雨水会降低多视角图像的视觉质量，导致3D场景重建不准确和不完整。现有数据集常忽略真实下雨3D场景的两个关键特征：雨痕外观因视角投影而异，以及降雨时云层覆盖导致的亮度降低。

Method: 1. 构建了新数据集OmniRain3D，融入透视异质性和亮度动态性，以更真实地模拟3D场景中的雨水降级。2. 提出了一个名为REVR-GSNet的端到端重建框架，用于3D高斯飞溅（Gaussian Splatting）。3. REVR-GSNet通过联合交替优化，将递归亮度增强、高斯基元优化和GS引导的去雨整合到统一架构中。

Result: 广泛的实验证明了所提出数据集和方法的有效性。该方法能够从受雨水影响的输入中，实现对干净3D场景的高保真重建。

Conclusion: 所提出的数据集和方法为未来多视角图像去雨和雨中3D场景重建研究奠定了基础。

Abstract: Rain degrades the visual quality of multi-view images, which are essential for 3D scene reconstruction, resulting in inaccurate and incomplete reconstruction results. Existing datasets often overlook two critical characteristics of real rainy 3D scenes: the viewpoint-dependent variation in the appearance of rain streaks caused by their projection onto 2D images, and the reduction in ambient brightness resulting from cloud coverage during rainfall. To improve data realism, we construct a new dataset named OmniRain3D that incorporates perspective heterogeneity and brightness dynamicity, enabling more faithful simulation of rain degradation in 3D scenes. Based on this dataset, we propose an end-to-end reconstruction framework named REVR-GSNet (Rain Elimination and Visibility Recovery for 3D Gaussian Splatting). Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian primitive optimization, and GS-guided rain elimination into a unified architecture through joint alternating optimization, achieving high-fidelity reconstruction of clean 3D scenes from rain-degraded inputs. Extensive experiments show the effectiveness of our dataset and method. Our dataset and method provide a foundation for future research on multi-view image deraining and rainy 3D scene reconstruction.

</details>


### [225] [Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV](https://arxiv.org/abs/2511.06741)
*Wenbo Huang,Jinghui Zhang,Zhenghao Chen,Guang Li,Lei Zhang,Yang Cao,Fang Dong,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出Otter模型，通过复合分割模块强调主体并结合时间重建模块增强时间关系，显著提升了宽视角少样本动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: 宽视角视频在少样本动作识别(FSAR)中因背景干扰而难以识别动作，现有方法缺乏对主体和背景的全局理解。直接应用RWKV模型可能因背景信息过多而无法突出主体，且相似背景帧会降低时间关系建模的有效性。

Method: 本文设计了CompOund SegmenTation and Temporal REconstructing RWKV (Otter) 模型。具体地，提出复合分割模块(CSM)以分割并强调每帧中的关键区域，从而突出主体。时间重建模块(TRM)被整合到时间增强原型构建中，实现双向扫描以更好地重建时间关系。此外，将常规原型与时间增强原型结合，以同时增强主体强调和时间建模。

Result: 在SSv2、Kinetics、UCF101和HMDB51等基准数据集上的广泛实验表明，Otter实现了最先进的性能。在VideoBadminton数据集上的额外评估进一步验证了Otter在宽视角FSAR中的优越性。

Conclusion: Otter模型通过其复合分割和时间重建机制，有效解决了宽视角少样本动作识别中背景干扰和时间关系建模的挑战，显著提升了识别性能。

Abstract: Wide-angle videos in few-shot action recognition (FSAR) effectively express actions within specific scenarios. However, without a global understanding of both subjects and background, recognizing actions in such samples remains challenging because of the background distractions. Receptance Weighted Key Value (RWKV), which learns interaction between various dimensions, shows promise for global modeling. While directly applying RWKV to wide-angle FSAR may fail to highlight subjects due to excessive background information. Additionally, temporal relation degraded by frames with similar backgrounds is difficult to reconstruct, further impacting performance. Therefore, we design the CompOund SegmenTation and Temporal REconstructing RWKV (Otter). Specifically, the Compound Segmentation Module~(CSM) is devised to segment and emphasize key patches in each frame, effectively highlighting subjects against background information. The Temporal Reconstruction Module (TRM) is incorporated into the temporal-enhanced prototype construction to enable bidirectional scanning, allowing better reconstruct temporal relation. Furthermore, a regular prototype is combined with the temporal-enhanced prototype to simultaneously enhance subject emphasis and temporal modeling, improving wide-angle FSAR performance. Extensive experiments on benchmarks such as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves state-of-the-art performance. Extra evaluation on the VideoBadminton dataset further validates the superiority of Otter in wide-angle FSAR.

</details>


### [226] [SinSEMI: A One-Shot Image Generation Model and Data-Efficient Evaluation Framework for Semiconductor Inspection Equipment](https://arxiv.org/abs/2511.06740)
*ChunLiang Wu,Xiaochun Li*

Main category: cs.CV

TL;DR: SinSEMI是一种新颖的一次性学习方法，旨在解决半导体设备开发中光学图像数据稀缺的问题。它能从单张光学图像生成多样化且高度逼真的图像，并在视觉质量、定量指标和下游任务中表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 半导体设备开发早期，获取大量原始光学图像极具挑战性，这种数据稀缺性阻碍了人工智能在半导体制造领域解决方案的进步。

Method: 本文提出了SinSEMI，一种新颖的一次性学习方法。它采用多尺度基于流的模型，并在采样过程中通过LPIPS（学习感知图像块相似度）能量引导进行增强，以确保图像的感知真实性和输出多样性。此外，还引入了一个专门为此应用设计的综合评估框架，仅需两张参考图像即可进行全面评估。

Result: 通过与多种一次性生成技术的评估比较，SinSEMI在视觉质量、定量指标和下游任务中均表现出卓越性能。实验结果表明，SinSEMI生成的图像具有高保真度和有意义的多样性，使其适合作为半导体AI应用的训练数据。

Conclusion: SinSEMI成功地解决了半导体领域光学图像数据稀缺的问题，通过生成高保真度和多样性的图像，为半导体AI应用的开发提供了有效的训练数据支持。

Abstract: In the early stages of semiconductor equipment development, obtaining large quantities of raw optical images poses a significant challenge. This data scarcity hinder the advancement of AI-powered solutions in semiconductor manufacturing. To address this challenge, we introduce SinSEMI, a novel one-shot learning approach that generates diverse and highly realistic images from single optical image. SinSEMI employs a multi-scale flow-based model enhanced with LPIPS (Learned Perceptual Image Patch Similarity) energy guidance during sampling, ensuring both perceptual realism and output variety. We also introduce a comprehensive evaluation framework tailored for this application, which enables a thorough assessment using just two reference images. Through the evaluation against multiple one-shot generation techniques, we demonstrate SinSEMI's superior performance in visual quality, quantitative measures, and downstream tasks. Our experimental results demonstrate that SinSEMI-generated images achieve both high fidelity and meaningful diversity, making them suitable as training data for semiconductor AI applications.

</details>


### [227] [PointCubeNet: 3D Part-level Reasoning with 3x3x3 Point Cloud Blocks](https://arxiv.org/abs/2511.06744)
*Da-Yeong Kim,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出PointCubeNet，一个无需部件标注即可实现部件级推理的新型多模态3D理解框架，通过全局和局部分支以及无监督训练方法，提升了整体3D对象理解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于在没有部件标注的情况下，实现3D对象的部件级推理，并证明部件级理解能够增强对整个3D对象的理解。

Method: 本文提出了PointCubeNet框架，包含全局分支和局部分支。局部分支由3x3x3局部块构成，用于分析点云子区域及其对应的局部文本标签。通过伪标签方法和局部损失函数，实现了无监督训练。

Result: 实验结果表明，理解3D对象部件能够增强对整体3D对象的理解。此外，这是首次尝试进行无监督3D部件级推理，并取得了可靠且有意义的结果。

Conclusion: PointCubeNet成功实现了无需部件标注的无监督3D部件级推理，提升了对3D对象的整体理解，并为该领域开辟了新的研究方向。

Abstract: In this paper, we propose PointCubeNet, a novel multi-modal 3D understanding framework that achieves part-level reasoning without requiring any part annotations. PointCubeNet comprises global and local branches. The proposed local branch, structured into 3x3x3 local blocks, enables part-level analysis of point cloud sub-regions with the corresponding local text labels. Leveraging the proposed pseudo-labeling method and local loss function, PointCubeNet is effectively trained in an unsupervised manner. The experimental results demonstrate that understanding 3D object parts enhances the understanding of the overall 3D object. In addition, this is the first attempt to perform unsupervised 3D part-level reasoning and achieves reliable and meaningful results.

</details>


### [228] [Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System](https://arxiv.org/abs/2511.06724)
*Shubham Agarwal,Subrata Mitra,Saud Iqbal*

Main category: cs.CV

TL;DR: Argus是一个高吞吐量文本到图像（T2I）推理系统，它通过为每个提示词智能选择合适的近似级别，在保持图像质量的同时显著提升了吞吐量并减少了延迟违规。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）扩散模型因其迭代去噪过程而计算密集且推理时间长，给设计高吞吐量系统带来了巨大挑战。尽管可以使用更快的近似模型，但必须为每个提示词仔细校准近似设置以避免质量下降，这使得设计一个能将提示词分配给适当模型和兼容近似设置的高吞吐量系统成为一个难题。

Method: 本文提出了Argus系统，它通过为每个提示词选择适当的近似级别来平衡质量和吞吐量。Argus能够智能地在不同的近似策略之间切换，以同时满足吞吐量和质量要求。

Result: 与基线系统相比，Argus在两个真实世界的工作负载跟踪上实现了延迟服务水平目标（SLO）违规减少10倍，平均质量提高10%，吞吐量提高40%。

Conclusion: Argus系统通过智能地为T2I推理选择合适的近似级别，成功地解决了高吞吐量和质量保持的挑战，显著提升了系统性能，降低了延迟，并优化了图像质量。

Abstract: Text-to-image (T2I) models have gained significant popularity. Most of these are diffusion models with unique computational characteristics, distinct from both traditional small-scale ML models and large language models. They are highly compute-bound and use an iterative denoising process to generate images, leading to very high inference time. This creates significant challenges in designing a high-throughput system. We discovered that a large fraction of prompts can be served using faster, approximated models. However, the approximation setting must be carefully calibrated for each prompt to avoid quality degradation. Designing a high-throughput system that assigns each prompt to the appropriate model and compatible approximation setting remains a challenging problem. We present Argus, a high-throughput T2I inference system that selects the right level of approximation for each prompt to maintain quality while meeting throughput targets on a fixed-size cluster. Argus intelligently switches between different approximation strategies to satisfy both throughput and quality requirements. Overall, Argus achieves 10x fewer latency service-level objective (SLO) violations, 10% higher average quality, and 40% higher throughput compared to baselines on two real-world workload traces.

</details>


### [229] [Relative Energy Learning for LiDAR Out-of-Distribution Detection](https://arxiv.org/abs/2511.06720)
*Zizhao Li,Zhengkang Xiang,Jiayang Ao,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文提出了一种名为相对能量学习（REL）的框架，用于激光雷达点云中的域外（OOD）检测。REL通过利用正负logits之间的能量差作为相对评分函数，并结合轻量级数据合成策略Point Raise来生成辅助异常，显著优于现有方法，为自动驾驶中的OOD检测提供了可靠且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的安全性严重依赖于识别训练分布之外的道路障碍和意外物体，因此域外（OOD）检测至关重要。尽管2D图像的OOD检测研究广泛，但直接应用于3D激光雷达点云效果不佳。当前激光雷达OOD方法难以区分罕见异常与常见类别，导致在安全关键场景中出现高误报率和过度自信的错误。

Method: 本文提出了相对能量学习（REL）框架，用于激光雷达点云的OOD检测。REL利用正（域内）和负logits之间的能量差作为相对评分函数，以缓解原始能量值的校准问题，并提高在各种场景下的鲁棒性。为解决训练期间OOD样本缺失的问题，本文提出了一种名为Point Raise的轻量级数据合成策略，通过扰动现有激光雷达点云来生成辅助异常，同时不改变内部点的语义。

Result: 在SemanticKITTI和Spotting the Unexpected (STU) 基准测试中，REL始终以显著优势优于现有方法。

Conclusion: 研究结果表明，建模相对能量并结合简单的合成异常点，为开放世界自动驾驶中可靠的OOD检测提供了一个原则性且可扩展的解决方案。

Abstract: Out-of-distribution (OOD) detection is a critical requirement for reliable autonomous driving, where safety depends on recognizing road obstacles and unexpected objects beyond the training distribution. Despite extensive research on OOD detection in 2D images, direct transfer to 3D LiDAR point clouds has been proven ineffective. Current LiDAR OOD methods struggle to distinguish rare anomalies from common classes, leading to high false-positive rates and overconfident errors in safety-critical settings. We propose Relative Energy Learning (REL), a simple yet effective framework for OOD detection in LiDAR point clouds. REL leverages the energy gap between positive (in-distribution) and negative logits as a relative scoring function, mitigating calibration issues in raw energy values and improving robustness across various scenes. To address the absence of OOD samples during training, we propose a lightweight data synthesis strategy called Point Raise, which perturbs existing point clouds to generate auxiliary anomalies without altering the inlier semantics. Evaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, REL consistently outperforms existing methods by a large margin. Our results highlight that modeling relative energy, combined with simple synthetic outliers, provides a principled and scalable solution for reliable OOD detection in open-world autonomous driving.

</details>


### [230] [Image Restoration via Primal Dual Hybrid Gradient and Flow Generative Model](https://arxiv.org/abs/2511.06748)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 该研究将流匹配生成模型先验集成到即插即用（PnP）框架中，并提出一种受PDHG启发的高效PnP算法，以支持更广泛的数据保真项，从而在非高斯噪声下实现图像恢复的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的正则化优化在图像逆问题中很常见，但现有PnP方法主要限于光滑平方L2数据保真项（高斯噪声）。流匹配生成模型具有强大的先验建模能力，但将其整合到能处理更一般数据保真项的PnP框架中仍有待探索。

Method: 该方法将流匹配生成先验集成到基于近端分裂的PnP框架中，其中正则化器的近端算子被生成模型导出的时间相关去噪器取代。为解决现有PnP方法对数据保真项的限制，提出了一种受原始-对偶混合梯度（PDHG）方法启发的通用高效PnP算法，支持包括L1和L2范数在内的多种保真项。

Result: 所提出的PnP算法计算高效且内存友好，能适应广泛的保真项，特别是L1和L2范数损失，使其对泊松噪声和脉冲噪声等非高斯噪声具有鲁棒性。在去噪、超分辨率、去模糊和图像修复等任务上的验证表明，在非高斯噪声存在时，L1和L2保真项优于传统的平方L2损失。

Conclusion: 该研究成功地将流匹配生成先验与PDHG启发的PnP框架结合，提出了一种通用且高效的图像恢复算法。该算法通过支持L1和L2等多种数据保真项，显著增强了对非高斯噪声的鲁棒性，并在多项图像恢复任务中展现出优越性能。

Abstract: Regularized optimization has been a classical approach to solving imaging inverse problems, where the regularization term enforces desirable properties of the unknown image. Recently, the integration of flow matching generative models into image restoration has garnered significant attention, owing to their powerful prior modeling capabilities. In this work, we incorporate such generative priors into a Plug-and-Play (PnP) framework based on proximal splitting, where the proximal operator associated with the regularizer is replaced by a time-dependent denoiser derived from the generative model. While existing PnP methods have achieved notable success in inverse problems with smooth squared $\ell_2$ data fidelity--typically associated with Gaussian noise--their applicability to more general data fidelity terms remains underexplored. To address this, we propose a general and efficient PnP algorithm inspired by the primal-dual hybrid gradient (PDHG) method. Our approach is computationally efficient, memory-friendly, and accommodates a wide range of fidelity terms. In particular, it supports both $\ell_1$ and $\ell_2$ norm-based losses, enabling robustness to non-Gaussian noise types such as Poisson and impulse noise. We validate our method on several image restoration tasks, including denoising, super-resolution, deblurring, and inpainting, and demonstrate that $\ell_1$ and $\ell_2$ fidelity terms outperform the conventional squared $\ell_2$ loss in the presence of non-Gaussian noise.

</details>


### [231] [AvatarTex: High-Fidelity Facial Texture Reconstruction from Single-Image Stylized Avatars](https://arxiv.org/abs/2511.06721)
*Yuda Qiu,Zitong Xiao,Yiwei Zuo,Zisheng Ye,Weikai Chen,Xiaoguang Han*

Main category: cs.CV

TL;DR: AvatarTex是一个高保真面部纹理重建框架，能从单张图像生成风格化和真实感纹理。它通过一个新颖的三阶段扩散-GAN管道解决现有方法在风格化头像上的局限性，并引入了多风格纹理数据集TexHub。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格化头像方面表现不佳，原因在于缺乏多样化的多风格数据集，以及难以在非标准纹理中保持几何一致性。

Method: AvatarTex引入了一个三阶段的“扩散到GAN”管道：首先通过基于扩散的修复完成缺失纹理区域；其次通过基于GAN的潜在空间优化来细化风格和结构一致性；最后通过基于扩散的重绘来增强细节。此外，该研究还构建了一个包含20,000张高分辨率、多风格、UV对齐纹理的TexHub数据集。

Result: AvatarTex实现了高质量、拓扑对齐的纹理合成，具有艺术和几何连贯性，在多风格面部纹理重建方面达到了最先进水平。

Conclusion: AvatarTex通过利用TexHub数据集和结构化的扩散-GAN管道，在多风格面部纹理重建领域取得了突破。TexHub数据集将在发表后发布，以促进未来的研究。

Abstract: We present AvatarTex, a high-fidelity facial texture reconstruction framework capable of generating both stylized and photorealistic textures from a single image. Existing methods struggle with stylized avatars due to the lack of diverse multi-style datasets and challenges in maintaining geometric consistency in non-standard textures. To address these limitations, AvatarTex introduces a novel three-stage diffusion-to-GAN pipeline. Our key insight is that while diffusion models excel at generating diversified textures, they lack explicit UV constraints, whereas GANs provide a well-structured latent space that ensures style and topology consistency. By integrating these strengths, AvatarTex achieves high-quality topology-aligned texture synthesis with both artistic and geometric coherence. Specifically, our three-stage pipeline first completes missing texture regions via diffusion-based inpainting, refines style and structure consistency using GAN-based latent optimization, and enhances fine details through diffusion-based repainting. To address the need for a stylized texture dataset, we introduce TexHub, a high-resolution collection of 20,000 multi-style UV textures with precise UV-aligned layouts. By leveraging TexHub and our structured diffusion-to-GAN pipeline, AvatarTex establishes a new state-of-the-art in multi-style facial texture reconstruction. TexHub will be released upon publication to facilitate future research in this field.

</details>


### [232] [Med-SORA: Symptom to Organ Reasoning in Abdomen CT Images](https://arxiv.org/abs/2511.06752)
*You-Kyoung Na,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 本文提出了Med-SORA框架，用于腹部CT图像中的症状到器官推理，通过RAG数据构建、可学习器官锚点的软标签以及2D-3D交叉注意力机制，解决了现有模型简化的一对一关系和缺乏3D信息的问题，实现了准确的3D临床推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型存在两个主要局限性：一是过度依赖简单的一对一硬标签，无法捕捉症状与多个器官相关联的临床现实；二是主要使用单层2D特征，缺乏整合3D信息，限制了捕获完整解剖上下文的能力。

Method: 本文提出了Med-SORA框架，包含以下创新点：1) 基于RAG的数据集构建；2) 引入可学习器官锚点的软标签，以捕捉症状与器官之间的一对多关系；3) 采用2D-3D交叉注意力架构，融合局部和全局图像特征。

Result: 实验结果表明，Med-SORA优于现有医学多模态模型，并能够实现准确的3D临床推理。

Conclusion: Med-SORA是首个解决医学多模态学习中症状到器官推理的工作，它通过创新的数据构建、软标签和2D-3D特征融合机制，显著提升了临床推理的准确性和对复杂生物学关系的理解。

Abstract: Understanding symptom-image associations is crucial for clinical reasoning. However, existing medical multimodal models often rely on simple one-to-one hard labeling, oversimplifying clinical reality where symptoms relate to multiple organs. In addition, they mainly use single-slice 2D features without incorporating 3D information, limiting their ability to capture full anatomical context. In this study, we propose Med-SORA, a framework for symptom-to-organ reasoning in abdominal CT images. Med-SORA introduces RAG-based dataset construction, soft labeling with learnable organ anchors to capture one-to-many symptom-organ relationships, and a 2D-3D cross-attention architecture to fuse local and global image features. To our knowledge, this is the first work to address symptom-to-organ reasoning in medical multimodal learning. Experimental results show that Med-SORA outperforms existing medical multimodal models and enables accurate 3D clinical reasoning.

</details>


### [233] [CAST-LUT: Tokenizer-Guided HSV Look-Up Tables for Purple Flare Removal](https://arxiv.org/abs/2511.06764)
*Pu Wang,Shuning Sun,Jialang Lu,Chen Wu,Zhihua Zhang,Youshan Zhang,Chenggang Shan,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于解耦HSV查找表（LUTs）的新型网络，用于校正图像中的紫边伪影，解决了传统方法和深度学习方法在灵活性、颜色耦合及数据稀缺性方面的问题，并在视觉效果和量化指标上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 紫边（Purple flare）作为一种弥散性色差伪影，严重损害图像的色调过渡和色彩。现有传统方法依赖手工特征，缺乏灵活性且受限于固定先验。深度学习方法则因缺乏成对训练数据而受阻。

Method: 本文提出了一种基于解耦HSV查找表（LUTs）的新型网络，旨在通过独立调整色相（H）、饱和度（S）和明度（V）分量来简化色彩校正，解决传统方法的固有颜色耦合问题。模型采用两阶段架构：首先，一个色度感知光谱分词器（CAST）将输入图像从RGB空间转换为HSV空间，并独立编码H和V通道为描述紫边状态的语义令牌；其次，HSV-LUT模块以这些令牌为输入，动态生成H、S、V三个通道的独立校正曲线（1D-LUTs）。为有效训练和验证模型，本文构建了首个大规模紫边数据集，并提出了专门针对该任务的新度量标准和损失函数。

Result: 广泛的实验表明，本文提出的模型不仅在视觉效果上显著优于现有方法，而且在所有量化指标上都达到了最先进的性能。

Conclusion: 本文提出的基于解耦HSV LUTs的网络，通过创新的两阶段架构和大规模数据集，有效解决了紫边校正的挑战，克服了传统方法的局限性和深度学习的数据瓶颈，取得了卓越的校正效果。

Abstract: Purple flare, a diffuse chromatic aberration artifact commonly found around highlight areas, severely degrades the tone transition and color of the image. Existing traditional methods are based on hand-crafted features, which lack flexibility and rely entirely on fixed priors, while the scarcity of paired training data critically hampers deep learning. To address this issue, we propose a novel network built upon decoupled HSV Look-Up Tables (LUTs). The method aims to simplify color correction by adjusting the Hue (H), Saturation (S), and Value (V) components independently. This approach resolves the inherent color coupling problems in traditional methods. Our model adopts a two-stage architecture: First, a Chroma-Aware Spectral Tokenizer (CAST) converts the input image from RGB space to HSV space and independently encodes the Hue (H) and Value (V) channels into a set of semantic tokens describing the Purple flare status; second, the HSV-LUT module takes these tokens as input and dynamically generates independent correction curves (1D-LUTs) for the three channels H, S, and V. To effectively train and validate our model, we built the first large-scale purple flare dataset with diverse scenes. We also proposed new metrics and a loss function specifically designed for this task. Extensive experiments demonstrate that our model not only significantly outperforms existing methods in visual effects but also achieves state-of-the-art performance on all quantitative metrics.

</details>


### [234] [Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes](https://arxiv.org/abs/2511.06765)
*Meijun Guo,Yongliang Shi,Caiyun Liu,Yixiao Feng,Ming Ma,Tinghai Yan,Weining Lu,Bin Liang*

Main category: cs.CV

TL;DR: 本文提出了一种改进的3D高斯泼溅（3DGS）方法，通过融合激光雷达-惯性测量单元里程计（LiDAR-IMU Odometry）进行姿态估计和引入法向量与有效秩正则化进行场景表示，解决了大型室外弱纹理场景中姿态估计不稳定和场景表示失真的问题，显著提升了渲染质量和效率。


<details>
  <summary>Details</summary>
Motivation: 在大型室外场景中，尤其是那些具有弱纹理或重复纹理的环境，传统的3DGS方法面临姿态估计不稳定和场景表示失真的问题，影响了数字资产创建的效率和视觉质量。

Method: 该方法从姿态估计和场景表示两方面入手：
1.  **姿态估计**：利用LiDAR-IMU Odometry提供相机在大型环境中的先验姿态，将其整合到COLMAP的三维重建过程中，并通过光束法平差进行姿态优化，确保像素数据关联与先验姿态的一致性。
2.  **场景表示**：引入法向量约束和有效秩正则化，以强制高斯基元的方向和形状保持一致性。这些约束与现有的光度损失函数共同优化，以提高地图质量。

Result: 实验结果表明：
1.  **姿态优化**：在保持精度和鲁棒性的前提下，所需时间仅为传统方法的约三分之一。
2.  **场景表示**：显著优于传统的3DGS管线，尤其在自采集的弱纹理或重复纹理数据集中，展示了增强的可视化能力和卓越的整体性能。

Conclusion: 该研究通过结合LiDAR-IMU里程计的先验姿态约束和对高斯基元的法向量及有效秩正则化，成功解决了大型室外弱纹理场景中3DGS的姿态估计不稳定和场景表示失真问题，显著提升了方法的鲁棒性、准确性和渲染质量。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality. To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation. For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments. These prior pose constraints are incorporated into COLMAP's triangulation process, with pose optimization performed via bundle adjustment. Ensuring consistency between pixel data association and prior poses helps maintain both robustness and accuracy. For scene representation, we introduce normal vector constraints and effective rank regularization to enforce consistency in the direction and shape of Gaussian primitives. These constraints are jointly optimized with the existing photometric loss to enhance the map quality. We evaluate our approach using both public and self-collected datasets. In terms of pose optimization, our method requires only one-third of the time while maintaining accuracy and robustness across both datasets. In terms of scene representation, the results show that our method significantly outperforms conventional 3DGS pipelines. Notably, on self-collected datasets characterized by weak or repetitive textures, our approach demonstrates enhanced visualization capabilities and achieves superior overall performance. Codes and data will be publicly available at https://github.com/justinyeah/normal_shape.git.

</details>


### [235] [ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction with Fewer Primitives](https://arxiv.org/abs/2511.06810)
*Bartłomiej Baranowski,Stefano Esposito,Patricia Gschoßmann,Anpei Chen,Andreas Geiger*

Main category: cs.CV

TL;DR: 3DGS在原始体空间分布上存在不足，源于基于克隆的稠密化。ConeGS提出一种图像空间感知的稠密化框架，利用iNGP重建作为几何代理来估计深度，并沿视锥插入新的高斯，从而在不同高斯预算下，尤其是在严格的原始体限制下，显著提升重建质量和渲染性能。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) 虽然在图像质量和实时性能上表现出色，但其原始体的空间分布往往不佳。这主要源于基于克隆的稠密化策略，该策略仅沿现有几何体传播高斯，限制了探索能力，并需要大量原始体才能充分覆盖场景。

Method: ConeGS采用图像空间感知的稠密化框架，独立于现有场景几何状态。首先，它创建一个快速的Instant Neural Graphics Primitives (iNGP) 重建作为几何代理，以估计每像素深度。在随后的3DGS优化过程中，它识别高误差像素，并沿相应的视锥在预测深度值处插入新的高斯，其尺寸根据视锥直径初始化。通过预激活不透明度惩罚快速移除冗余高斯，并通过原始体预算策略（固定或自适应）控制原始体总数，以确保高重建质量。

Result: 实验表明，ConeGS在各种高斯预算下均能持续提升重建质量和渲染性能，尤其是在原始体限制严格、高效放置至关重要的情况下，其增益更为显著。

Conclusion: ConeGS通过引入图像空间感知的稠密化策略，有效解决了3DGS中原始体空间分布次优的问题。该方法通过几何代理辅助深度估计和视锥插入新高斯，结合冗余移除和预算控制，显著提升了重建质量和渲染效率，特别是在资源受限的场景下表现优异。

Abstract: 3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and real-time performance in novel view synthesis but often suffers from a suboptimal spatial distribution of primitives. This issue stems from cloning-based densification, which propagates Gaussians along existing geometry, limiting exploration and requiring many primitives to adequately cover the scene. We present ConeGS, an image-space-informed densification framework that is independent of existing scene geometry state. ConeGS first creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a geometric proxy to estimate per-pixel depth. During the subsequent 3DGS optimization, it identifies high-error pixels and inserts new Gaussians along the corresponding viewing cones at the predicted depth values, initializing their size according to the cone diameter. A pre-activation opacity penalty rapidly removes redundant Gaussians, while a primitive budgeting strategy controls the total number of primitives, either by a fixed budget or by adapting to scene complexity, ensuring high reconstruction quality. Experiments show that ConeGS consistently enhances reconstruction quality and rendering performance across Gaussian budgets, with especially strong gains under tight primitive constraints where efficient placement is crucial.

</details>


### [236] [Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders](https://arxiv.org/abs/2511.06846)
*Federico Vasile,Ri-Zhao Qiu,Lorenzo Natale,Xiaolong Wang*

Main category: cs.CV

TL;DR: AS-DiffMPM是一个可微分的物质点法（MPM）框架，通过引入可微分碰撞处理机制，实现了从视频观测中估计物理属性，支持与任意形状的碰撞体进行交互。


<details>
  <summary>Details</summary>
Motivation: 现有的可微分MPM和渲染方法在系统识别中仅限于平面碰撞体，无法处理物体与非平面表面碰撞的复杂场景，限制了其在机器人和图形学应用中的实用性。

Method: 本文提出了AS-DiffMPM，通过集成一个可微分的碰撞处理机制，扩展了现有方法。这使得目标物体能够与复杂的刚体进行交互，同时保持端到端优化，从而支持任意形状的碰撞体。

Result: AS-DiffMPM能够实现对物理属性的估计，即使在物体与任意形状碰撞体交互的复杂场景中。此外，它还可以轻松地与各种新颖的视图合成方法结合，作为从视觉观测进行系统识别的框架。

Conclusion: AS-DiffMPM提供了一个强大的可微分MPM框架，能够处理复杂碰撞场景下的系统识别任务，并为从视觉数据中估计物理属性开辟了新的可能性。

Abstract: System identification involving the geometry, appearance, and physical properties from video observations is a challenging task with applications in robotics and graphics. Recent approaches have relied on fully differentiable Material Point Method (MPM) and rendering for simultaneous optimization of these properties. However, they are limited to simplified object-environment interactions with planar colliders and fail in more challenging scenarios where objects collide with non-planar surfaces. We propose AS-DiffMPM, a differentiable MPM framework that enables physical property estimation with arbitrarily shaped colliders. Our approach extends existing methods by incorporating a differentiable collision handling mechanism, allowing the target object to interact with complex rigid bodies while maintaining end-to-end optimization. We show AS-DiffMPM can be easily interfaced with various novel view synthesis methods as a framework for system identification from visual observations.

</details>


### [237] [Integrating Reweighted Least Squares with Plug-and-Play Diffusion Priors for Noisy Image Restoration](https://arxiv.org/abs/2511.06823)
*Ji Li,Chao Wang*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Existing plug-and-play image restoration methods typically employ off-the-shelf Gaussian denoisers as proximal operators within classical optimization frameworks based on variable splitting. Recently, denoisers induced by generative priors have been successfully integrated into regularized optimization methods for image restoration under Gaussian noise. However, their application to non-Gaussian noise--such as impulse noise--remains largely unexplored. In this paper, we propose a plug-and-play image restoration framework based on generative diffusion priors for robust removal of general noise types, including impulse noise. Within the maximum a posteriori (MAP) estimation framework, the data fidelity term is adapted to the specific noise model. Departing from the conventional least-squares loss used for Gaussian noise, we introduce a generalized Gaussian scale mixture-based loss, which approximates a wide range of noise distributions and leads to an $\ell_q$-norm ($0<q\leq2$) fidelity term. This optimization problem is addressed using an iteratively reweighted least squares (IRLS) approach, wherein the proximal step involving the generative prior is efficiently performed via a diffusion-based denoiser. Experimental results on benchmark datasets demonstrate that the proposed method effectively removes non-Gaussian impulse noise and achieves superior restoration performance.

</details>


### [238] [MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks](https://arxiv.org/abs/2511.06830)
*Tianang Chen,Jian Jin,Shilv Cai,Zhuangzi Li,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多距离主观质量评估方法，并构建了一个名为MUGSQA的新型GS质量评估数据集，以应对高斯泼溅（GS）重建三维物体感知质量评估的挑战。此外，还构建了两个基准测试来评估GS重建方法和现有质量评估指标的性能。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅（GS）作为一种3D物体重建技术，其变体不断涌现，但评估不同GS方法重建的3D物体感知质量仍是一个开放的挑战。

Method: 1. 提出了一种统一的多距离主观质量评估方法，该方法模拟人类观看行为，以收集GS重建物体的感知体验。2. 构建了一个名为MUGSQA的GS质量评估数据集，该数据集考虑了输入数据的多种不确定性，包括输入视图的数量和分辨率、视角距离以及初始点云的准确性。3. 构建了两个基准测试：一个用于评估各种GS重建方法在多重不确定性下的鲁棒性，另一个用于评估现有质量评估指标的性能。

Result: 研究提出了一个统一的多距离主观质量评估方法，构建了一个包含多重不确定性的GS质量评估数据集MUGSQA，并建立了两个用于评估GS重建方法鲁棒性和现有质量评估指标性能的基准测试。

Conclusion: 本文的工作通过提出主观评估方法、构建综合数据集和基准测试，为解决GS重建3D物体感知质量评估的开放性挑战提供了重要的工具和平台，有助于未来GS方法的研究和改进。

Abstract: Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.

</details>


### [239] [VAEVQ: Enhancing Discrete Visual Tokenization through Variational Modeling](https://arxiv.org/abs/2511.06863)
*Sicheng Yang,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.CV

TL;DR: 本文提出VAEVQ，一个改进的向量量化框架，通过变分潜在量化、表示一致性策略和分布一致性正则化，解决了现有VQ模型中潜在空间不平滑、表示对齐弱和域一致性差的问题，从而提升了重建和生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于向量量化（VQ）的框架存在一些问题，包括潜在空间不平滑、量化前后表示对齐性差以及连续与离散域之间的一致性不足。这些问题导致码字学习不稳定、码本利用率低，最终降低了重建和下游生成任务的性能。

Method: 本文提出了VAEVQ框架，包含三个关键组件：(1) 变分潜在量化（VLQ），用VAE代替AE进行量化，以利用其结构化和平滑的潜在空间，促进更有效的码字激活；(2) 表示一致性策略（RCS），自适应地调节量化前后特征的对齐强度，以增强一致性并防止过拟合噪声；(3) 分布一致性正则化（DCR），将整个码本分布与连续潜在分布对齐，以提高码本利用率。

Result: 在两个基准数据集上进行的广泛实验表明，VAEVQ的性能优于现有最先进的方法。

Conclusion: VAEVQ通过引入变分潜在量化、表示一致性策略和分布一致性正则化，有效解决了传统VQ框架的固有问题，显著提升了图像特征离散表示的质量，从而在重建和生成任务中取得了卓越的性能。

Abstract: Vector quantization (VQ) transforms continuous image features into discrete representations, providing compressed, tokenized inputs for generative models. However, VQ-based frameworks suffer from several issues, such as non-smooth latent spaces, weak alignment between representations before and after quantization, and poor coherence between the continuous and discrete domains. These issues lead to unstable codeword learning and underutilized codebooks, ultimately degrading the performance of both reconstruction and downstream generation tasks. To this end, we propose VAEVQ, which comprises three key components: (1) Variational Latent Quantization (VLQ), replacing the AE with a VAE for quantization to leverage its structured and smooth latent space, thereby facilitating more effective codeword activation; (2) Representation Coherence Strategy (RCS), adaptively modulating the alignment strength between pre- and post-quantization features to enhance consistency and prevent overfitting to noise; and (3) Distribution Consistency Regularization (DCR), aligning the entire codebook distribution with the continuous latent distribution to improve utilization. Extensive experiments on two benchmark datasets demonstrate that VAEVQ outperforms state-of-the-art methods.

</details>


### [240] [A Two-Stage System for Layout-Controlled Image Generation using Large Language Models and Diffusion Models](https://arxiv.org/abs/2511.06888)
*Jan-Hendrik Koch,Jonas Krumme,Konrad Gadzicki*

Main category: cs.CV

TL;DR: 本文提出一个两阶段系统，通过大语言模型（LLM）规划布局，然后使用布局条件扩散模型生成图像，以解决文本到图像扩散模型在物体数量和空间排列控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然具有出色的生成能力，但在精确控制物体数量和空间排列方面存在局限性。

Method: 该系统分为两阶段：第一阶段，使用大型语言模型（LLM）从物体列表生成结构化布局；第二阶段，使用布局条件扩散模型合成符合该布局的逼真图像。LLM的空间规划通过任务分解实现，先生成核心物体布局，再通过基于规则的插入完成整体布局。图像合成阶段比较了ControlNet和GLIGEN两种领先的条件化方法，并在餐桌布置数据集上进行了领域特定微调。

Result: 对于复杂场景，通过任务分解，LLM的物体召回率从57.2%提高到99.9%。图像合成方面，ControlNet能保留文本风格控制但存在物体幻觉，而GLIGEN提供更好的布局保真度但提示词可控性降低。最终，端到端系统成功生成了具有指定物体数量和合理空间排列的图像。

Conclusion: 解耦方法（LLM用于布局规划，扩散模型用于图像合成）对于实现组合控制的图像合成是可行的。

Abstract: Text-to-image diffusion models exhibit remarkable generative capabilities, but lack precise control over object counts and spatial arrangements. This work introduces a two-stage system to address these compositional limitations. The first stage employs a Large Language Model (LLM) to generate a structured layout from a list of objects. The second stage uses a layout-conditioned diffusion model to synthesize a photorealistic image adhering to this layout. We find that task decomposition is critical for LLM-based spatial planning; by simplifying the initial generation to core objects and completing the layout with rule-based insertion, we improve object recall from 57.2% to 99.9% for complex scenes. For image synthesis, we compare two leading conditioning methods: ControlNet and GLIGEN. After domain-specific finetuning on table-setting datasets, we identify a key trade-off: ControlNet preserves text-based stylistic control but suffers from object hallucination, while GLIGEN provides superior layout fidelity at the cost of reduced prompt-based controllability. Our end-to-end system successfully generates images with specified object counts and plausible spatial arrangements, demonstrating the viability of a decoupled approach for compositionally controlled synthesis.

</details>


### [241] [Generating an Image From 1,000 Words: Enhancing Text-to-Image With Structured Captions](https://arxiv.org/abs/2511.06876)
*Eyal Gutflaish,Eliran Kachlon,Hezi Zisman,Tal Hacham,Nimrod Sarid,Alexander Visheratin,Saar Huberman,Gal Davidi,Guy Bukchin,Kfir Goldberg,Ron Mokady*

Main category: cs.CV

TL;DR: 现有文生图模型因短提示词导致可控性不足。本文提出FIBO模型，通过在长结构化描述上训练，结合DimFusion机制和TaBR评估协议，显著提升了模型的表达能力和可控性，实现了开源模型中的最佳提示词对齐。


<details>
  <summary>Details</summary>
Motivation: 大多数文生图模型将短提示词映射到详细图像，导致稀疏文本输入与丰富视觉输出之间的不匹配。这种不匹配降低了可控性，因为模型经常任意填充缺失细节，偏向平均用户偏好，限制了专业用途的精确性。

Method: 1. 训练首个在长结构化描述上进行训练的开源文生图模型，每个训练样本都用一套细粒度属性进行标注，以最大化表达覆盖范围并实现视觉因素的解耦控制。2. 提出DimFusion机制，一种高效融合轻量级LLM中间token的方法，且不增加token长度，以有效处理长描述。3. 引入Text-as-a-Bottleneck Reconstruction (TaBR) 评估协议，通过评估真实图像通过“描述-生成”循环的重建程度，直接衡量模型的可控性和表达能力。4. 训练了大型模型FIBO。

Result: FIBO模型在开源模型中实现了最先进的提示词对齐效果。

Conclusion: 通过在长结构化描述上训练，并结合DimFusion融合机制和TaBR评估协议，可以显著提高文生图模型的可控性和表达能力，FIBO模型验证了这一贡献。

Abstract: Text-to-image models have rapidly evolved from casual creative tools to professional-grade systems, achieving unprecedented levels of image quality and realism. Yet, most models are trained to map short prompts into detailed images, creating a gap between sparse textual input and rich visual outputs. This mismatch reduces controllability, as models often fill in missing details arbitrarily, biasing toward average user preferences and limiting precision for professional use. We address this limitation by training the first open-source text-to-image model on long structured captions, where every training sample is annotated with the same set of fine-grained attributes. This design maximizes expressive coverage and enables disentangled control over visual factors. To process long captions efficiently, we propose DimFusion, a fusion mechanism that integrates intermediate tokens from a lightweight LLM without increasing token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR) evaluation protocol. By assessing how well real images can be reconstructed through a captioning-generation loop, TaBR directly measures controllability and expressiveness, even for very long captions where existing evaluation methods fail. Finally, we demonstrate our contributions by training the large-scale model FIBO, achieving state-of-the-art prompt alignment among open-source models. Model weights are publicly available at https://huggingface.co/briaai/FIBO

</details>


### [242] [MRT: Learning Compact Representations with Mixed RWKV-Transformer for Extreme Image Compression](https://arxiv.org/abs/2511.06717)
*Han Liu,Hengyu Man,Xingtao Wang,Wenrui Li,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的混合RWKV-Transformer (MRT) 架构，通过将图像编码为更紧凑的1-D潜在表示，显著提升了极端图像压缩的效率和质量，并引入了专门的RWKV压缩模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法大多将像素数据压缩到2-D潜在空间，但这些空间通常保留了大量的空间冗余，从而限制了整体压缩性能。

Method: MRT架构通过协同整合基于线性注意力的RWKV和基于自注意力的Transformer模型的优势，将图像编码为1-D潜在表示。具体而言，MRT将图像分割成固定大小的窗口，RWKV模块用于捕获跨窗口的全局依赖，而Transformer块用于建模每个窗口内的局部冗余。此外，还引入了专门的RWKV压缩模型（RCM）以适应MRT中间1-D潜在特征的结构特性。

Result: 在标准图像压缩基准测试中，MRT框架在低于0.02 bpp的比特率下持续实现卓越的重建质量。基于DISTS指标的量化结果显示，MRT显著优于最先进的2-D架构GLC，在Kodak和CLIC2020测试数据集上分别实现了43.75%和30.59%的比特率节省。

Conclusion: 所提出的MRT框架通过其独特的1-D潜在表示和分层注意力机制，在极端图像压缩方面表现出显著的有效性，并超越了现有最先进的2-D架构，实现了更高的压缩效率和重建质量。

Abstract: Recent advances in extreme image compression have revealed that mapping pixel data into highly compact latent representations can significantly improve coding efficiency. However, most existing methods compress images into 2-D latent spaces via convolutional neural networks (CNNs) or Swin Transformers, which tend to retain substantial spatial redundancy, thereby limiting overall compression performance. In this paper, we propose a novel Mixed RWKV-Transformer (MRT) architecture that encodes images into more compact 1-D latent representations by synergistically integrating the complementary strengths of linear-attention-based RWKV and self-attention-based Transformer models. Specifically, MRT partitions each image into fixed-size windows, utilizing RWKV modules to capture global dependencies across windows and Transformer blocks to model local redundancies within each window. The hierarchical attention mechanism enables more efficient and compact representation learning in the 1-D domain. To further enhance compression efficiency, we introduce a dedicated RWKV Compression Model (RCM) tailored to the structure characteristics of the intermediate 1-D latent features in MRT. Extensive experiments on standard image compression benchmarks validate the effectiveness of our approach. The proposed MRT framework consistently achieves superior reconstruction quality at bitrates below 0.02 bits per pixel (bpp). Quantitative results based on the DISTS metric show that MRT significantly outperforms the state-of-the-art 2-D architecture GLC, achieving bitrate savings of 43.75%, 30.59% on the Kodak and CLIC2020 test datasets, respectively.

</details>


### [243] [Adaptive Morph-Patch Transformer for Arotic Vessel Segmentation](https://arxiv.org/abs/2511.06897)
*Zhenxi Zhang,Fuchen Zheng,Adnan Iltaf,Yifei Han,Zhenyu Cheng,Yue Du,Bin Li,Tianyong Liu,Shoujun Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为自适应形态补丁Transformer（MPT）的新型架构，通过引入自适应补丁划分策略和语义聚类注意力机制，解决了传统Transformer在主动脉血管分割中因固定大小补丁而导致的结构完整性问题，显著提升了复杂血管结构的分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的模型在主动脉血管分割中，其固定大小的矩形补丁策略常会破坏复杂血管结构的完整性，导致分割精度不佳。因此，需要一种能更好地保留血管结构语义完整性的方法。

Method: 本文提出自适应形态补丁Transformer（MPT）。它包含两个主要创新点：1) 自适应补丁划分策略，动态生成与复杂血管结构对齐的形态感知补丁，以保留其语义完整性。2) 语义聚类注意力（SCA）方法，动态聚合具有相似语义特征的不同补丁的特征，以增强模型分割不同尺寸血管的能力并保持结构完整性。

Result: 在AVT、AortaSeg24和TBAD三个开源数据集上进行的广泛实验表明，MPT模型实现了最先进的性能，特别是在分割复杂精细的血管结构方面取得了显著改进。

Conclusion: MPT通过其自适应补丁划分和语义聚类注意力机制，有效地解决了传统Transformer在主动脉血管分割中的挑战，显著提高了复杂血管结构的分割精度，并能更好地处理不同尺寸的血管，具有重要的诊断和治疗应用价值。

Abstract: Accurate segmentation of aortic vascular structures is critical for diagnosing and treating cardiovascular diseases.Traditional Transformer-based models have shown promise in this domain by capturing long-range dependencies between vascular features. However, their reliance on fixed-size rectangular patches often influences the integrity of complex vascular structures, leading to suboptimal segmentation accuracy. To address this challenge, we propose the adaptive Morph Patch Transformer (MPT), a novel architecture specifically designed for aortic vascular segmentation. Specifically, MPT introduces an adaptive patch partitioning strategy that dynamically generates morphology-aware patches aligned with complex vascular structures. This strategy can preserve semantic integrity of complex vascular structures within individual patches. Moreover, a Semantic Clustering Attention (SCA) method is proposed to dynamically aggregate features from various patches with similar semantic characteristics. This method enhances the model's capability to segment vessels of varying sizes, preserving the integrity of vascular structures. Extensive experiments on three open-source dataset(AVT, AortaSeg24 and TBAD) demonstrate that MPT achieves state-of-the-art performance, with improvements in segmenting intricate vascular structures.

</details>


### [244] [Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding](https://arxiv.org/abs/2511.06908)
*Yuzhen Li,Min Liu,Zhaoyang Li,Yuan Bian,Xueping Wang,Erbo Zhai,Yaonan Wang*

Main category: cs.CV

TL;DR: 该论文提出了Mono3DVG-EnSD框架，通过CLIP-LCA强制模型理解空间描述，并利用D2M解耦文本特征以减少跨维度干扰，从而在单目3D视觉定位任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D视觉定位（Mono3DVG）方法存在两大局限：一是过度依赖明确识别目标对象的高确定性关键词，而忽略了关键的空间描述；二是通用文本特征包含2D和3D描述信息，导致在文本指导下细化视觉特征时产生跨维度干扰。

Method: 本文提出了Mono3DVG-EnSD框架，包含两个核心组件：CLIP引导的词汇确定性适配器（CLIP-LCA）和维度解耦模块（D2M）。CLIP-LCA动态遮蔽高确定性关键词，保留低确定性的隐式空间描述，以迫使模型更深入理解文本中的空间关系。D2M则将通用文本特征解耦为维度特定（2D/3D）的文本特征，以指导相应维度的视觉特征，从而通过维度一致的跨模态交互来减轻跨维度干扰。

Result: 通过在Mono3DRefer数据集上的综合比较和消融研究，Mono3DVG-EnSD方法在所有指标上均达到了最先进（SOTA）的性能。特别是在具有挑战性的Far(Acc@0.5)场景中，性能显著提升了13.54%。

Conclusion: Mono3DVG-EnSD框架通过解决现有方法对关键词的过度依赖和跨维度干扰问题，有效提升了单目3D视觉定位的准确性和鲁棒性，并在多项指标上超越了现有技术水平。

Abstract: Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.

</details>


### [245] [Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation](https://arxiv.org/abs/2511.06857)
*Fanding Li,Xiangyu Li,Xianghe Su,Xingyu Qiu,Suyu Dong,Wei Wang,Kuanquan Wang,Gongning Luo,Shuo Li*

Main category: cs.CV

TL;DR: 针对模糊医学图像分割（AMIS）中预测准确性和多样性难以同时提升的问题，本文提出了模糊感知截断流匹配（ATFM）模型，通过数据分层推理、高斯截断表示和分割流匹配，有效解耦准确性和多样性，提升预测的保真度和合理性，并在多个数据集上实现了最先进的性能和更高的推理效率。


<details>
  <summary>Details</summary>
Motivation: 模糊医学图像分割（AMIS）中，提高预测的准确性和多样性存在固有的权衡，难以同时实现。现有的截断扩散概率模型（TDPMs）在预测的准确性和多样性上存在纠缠，且保真度和合理性不足。

Method: 本文提出了模糊感知截断流匹配（ATFM），包含以下三个核心组件：
1. **数据分层推理（Data-Hierarchical Inference）**：重新定义了AMIS特有的推理范式，分别在数据分布和数据样本层面增强准确性和多样性，以有效解耦。
2. **高斯截断表示（Gaussian Truncation Representation, GTR）**：通过在$T_{\text{trunc}}$处将截断分布明确建模为高斯分布，而不是使用基于采样的近似，从而增强预测的保真度和截断分布的可靠性。
3. **分割流匹配（Segmentation Flow Matching, SFM）**：通过扩展流匹配（FM）中的语义感知流变换，增强了多样化预测的合理性。

Result: ATFM在LIDC和ISIC3数据集上进行了综合评估，结果表明它优于现有最先进的方法，并同时实现了更高效的推理。与先进方法相比，ATFM在GED上提升了高达12%，在HM-IoU上提升了高达7.3%。

Conclusion: ATFM成功解决了模糊医学图像分割中准确性和多样性难以同时提升的挑战，通过其新颖的推理范式和专用模型组件，实现了对预测准确性和多样性的同步增强，并提供了更高效的推理，表现出卓越的性能。

Abstract: A simultaneous enhancement of accuracy and diversity of predictions remains a challenge in ambiguous medical image segmentation (AMIS) due to the inherent trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong potential with a paradigm optimization, existing TDPMs suffer from entangled accuracy and diversity of predictions with insufficient fidelity and plausibility. To address the aforementioned challenges, we propose Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel inference paradigm and dedicated model components. Firstly, we propose Data-Hierarchical Inference, a redefinition of AMIS-specific inference paradigm, which enhances accuracy and diversity at data-distribution and data-sample level, respectively, for an effective disentanglement. Secondly, Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity of predictions and reliability of truncation distribution, by explicitly modeling it as a Gaussian distribution at $T_{\text{trunc}}$ instead of using sampling-based approximations.Thirdly, Segmentation Flow Matching (SFM) is proposed to enhance the plausibility of diverse predictions by extending semantic-aware flow transformation in Flow Matching (FM). Comprehensive evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA methods and simultaneously achieves a more efficient inference. ATFM improves GED and HM-IoU by up to $12\%$ and $7.3\%$ compared to advanced methods.

</details>


### [246] [Classification of Microplastic Particles in Water using Polarized Light Scattering and Machine Learning Methods](https://arxiv.org/abs/2511.06901)
*Leonard Saur,Marc von Pawlowski,Ulrich Gengenbach,Ingo Sieber,Hossein Shirali,Lorenz Wührl,Rainer Kiko,Christian Pylatiuk*

Main category: cs.CV

TL;DR: 本文提出并验证了一种基于偏振光散射的新型反射式方法，用于水中微塑料的原位分类和识别，结合深度卷积神经网络（CNN）实现了80%的分类准确率，并发现线性偏振角（AOLP）和线性偏振度（DOLP）信号在识别不同聚合物方面具有互补优势。


<details>
  <summary>Details</summary>
Motivation: 当前对水生环境中微塑料的连续、大规模监测面临挑战，现有“黄金标准”方法存在局限性，特别是在水体中基于透射的干扰问题。

Method: 该研究采用了一种反射式方法，通过线性偏振激光照射无色微塑料颗粒（50-300 μm），并使用偏振敏感相机捕获反射信号。然后，利用深度卷积神经网络（CNN）对图像进行分类，并分析了线性偏振角（AOLP）和线性偏振度（DOLP）信号对分类性能的影响。

Result: 该方法成功识别了高密度聚乙烯、低密度聚乙烯和聚丙烯三种常见聚合物，在测试数据集上实现了80%的峰值平均分类准确率。CNN的决策主要依赖于颗粒的微观结构完整性和内部纹理（偏振模式）。AOLP信号比DOLP信号对上下文噪声更鲁棒，在区分两种聚乙烯塑料方面表现更优。DOLP信号在识别聚丙烯类别方面表现出色，但整体分类结果略逊于AOLP。

Conclusion: 基于反射式偏振光散射结合CNN的方法为水体中微塑料的原位分类提供了一种有效途径。AOLP信号在整体性能和区分聚乙烯方面表现突出，而DOLP信号在识别聚丙烯方面具有独特优势，表明这两种偏振信号在微塑料分类中具有互补性。

Abstract: Facing the critical need for continuous, large-scale microplastic monitoring, which is hindered by the limitations of gold-standard methods in aquatic environments, this paper introduces and validates a novel, reflection-based approach for the in-situ classification and identification of microplastics directly in water bodies, which is based on polarized light scattering. In this experiment, we classify colorless microplastic particles (50-300 $μ$m) by illuminating them with linearly polarized laser light and capturing their reflected signals using a polarization-sensitive camera. This reflection-based technique successfully circumvents the transmission-based interference issues that plague many conventional methods when applied in water. Using a deep convolutional neural network (CNN) for image-based classification, we successfully identified three common polymer types, high-density polyethylene, low-density polyethylene, and polypropylene, achieving a peak mean classification accuracy of 80% on the test dataset. A subsequent feature hierarchy analysis demonstrated that the CNN's decision-making process relies mainly on the microstructural integrity and internal texture (polarization patterns) of the particle rather than its macroshape. Critically, we found that the Angle of Linear Polarization (AOLP) signal is significantly more robust against contextual noise than the Degree of Linear Polarization (DOLP) signal. While the AOLP-based classification achieved superior overall performance, its strength lies in distinguishing between the two polyethylene plastics, showing a lower confusion rate between high-density and low-density polyethylene. Conversely, the DOLP signal demonstrated slightly worse overall classification results but excels at accurately identifying the polypropylene class, which it isolated with greater success than AOLP.

</details>


### [247] [DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and Tokenized Temporal Modeling](https://arxiv.org/abs/2511.06925)
*Zhicheng Li,Kunyang Sun,Rui Yao,Hancheng Zhu,Fuyuan Hu,Jiaqi Zhao,Zhiwen Shao,Yong Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频阴影检测网络，通过视觉-语言匹配模块解决阴影与背景的混淆，并通过标记化时间块有效处理动态阴影形变，实现了最先进的准确性和实时推理效率。


<details>
  <summary>Details</summary>
Motivation: 视频阴影检测面临两大挑战：一是如何区分阴影与复杂背景（阴影-背景模糊性），二是如何在不同光照下建模动态阴影形变（时间建模）。

Method: 1. **视觉-语言匹配模块 (VMM) 和 暗区语义块 (DSB)**：利用语言先验和文本引导特征来明确区分阴影与黑暗物体，解决阴影-背景模糊性。2. **自适应掩码重加权和边缘掩码**：在训练期间降低半影区域的权重，并在解码器阶段应用边缘掩码以提供更好的监督。3. **标记化时间块 (TTB)**：解耦时空学习，将跨帧阴影语义总结为可学习的时间标记，以最小的计算开销实现高效的序列编码，用于时间建模。

Result: 在多个基准数据集上的综合实验表明，该方法实现了最先进的准确性，并具有实时推理效率。

Conclusion: 该研究通过引入视觉-语言先验和创新的时空解耦方法，有效解决了视频阴影检测中的关键难题，取得了卓越的性能和效率。

Abstract: Video shadow detection confronts two entwined difficulties: distinguishing shadows from complex backgrounds and modeling dynamic shadow deformations under varying illumination. To address shadow-background ambiguity, we leverage linguistic priors through the proposed Vision-language Match Module (VMM) and a Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly differentiate shadows from dark objects. Furthermore, we introduce adaptive mask reweighting to downweight penumbra regions during training and apply edge masks at the final decoder stage for better supervision. For temporal modeling of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics into learnable temporal tokens, enabling efficient sequence encoding with minimal computation overhead. Comprehensive Experiments on multiple benchmark datasets demonstrate state-of-the-art accuracy and real-time inference efficiency. Codes are available at https://github.com/city-cheng/DTTNet.

</details>


### [248] [PADM: A Physics-aware Diffusion Model for Attenuation Correction](https://arxiv.org/abs/2511.06948)
*Trung Kien Pham,Hoang Minh Vu,Anh Duc Chu,Dac Thai Nguyen,Trung Thanh Nguyen,Thao Nguyen Truong,Mai Hong Son,Thanh Trung Nguyen,Phi Le Nguyen*

Main category: cs.CV

TL;DR: 本研究提出了一种名为PADM的无CT衰减校正扩散模型，用于心脏SPECT心肌灌注成像，通过结合物理先验和师生蒸馏机制，实现了优于现有技术的衰减伪影校正，并引入了CardiAC数据集。


<details>
  <summary>Details</summary>
Motivation: 心脏SPECT心肌灌注成像中的衰减伪影严重影响诊断准确性，而混合SPECT/CT系统虽然能减轻伪影，但成本高、可及性差且增加辐射暴露，限制了其广泛应用。因此，需要一种无CT的衰减校正解决方案。

Method: 研究提出了一种名为PADM的扩散生成模型，通过师生蒸馏机制融入显式物理先验知识，仅使用未衰减校正(NAC)输入即可进行衰减伪影校正。同时，构建了包含424个患者研究的CardiAC数据集，其中包含配对的NAC和AC重建图像以及高分辨率CT衰减图。

Result: 广泛实验表明，PADM在定量指标和视觉评估方面均优于现有最先进的生成模型，提供了卓越的重建保真度。

Conclusion: PADM为心脏SPECT衰减校正提供了一种新颖、无CT且性能优越的解决方案，有望提高诊断准确性并扩大临床应用。

Abstract: Attenuation artifacts remain a significant challenge in cardiac Myocardial Perfusion Imaging (MPI) using Single-Photon Emission Computed Tomography (SPECT), often compromising diagnostic accuracy and reducing clinical interpretability. While hybrid SPECT/CT systems mitigate these artifacts through CT-derived attenuation maps, their high cost, limited accessibility, and added radiation exposure hinder widespread clinical adoption. In this study, we propose a novel CT-free solution to attenuation correction in cardiac SPECT. Specifically, we introduce Physics-aware Attenuation Correction Diffusion Model (PADM), a diffusion-based generative method that incorporates explicit physics priors via a teacher--student distillation mechanism. This approach enables attenuation artifact correction using only Non-Attenuation-Corrected (NAC) input, while still benefiting from physics-informed supervision during training. To support this work, we also introduce CardiAC, a comprehensive dataset comprising 424 patient studies with paired NAC and Attenuation-Corrected (AC) reconstructions, alongside high-resolution CT-based attenuation maps. Extensive experiments demonstrate that PADM outperforms state-of-the-art generative models, delivering superior reconstruction fidelity across both quantitative metrics and visual assessment.

</details>


### [249] [Distillation Dynamics: Towards Understanding Feature-Based Distillation in Vision Transformers](https://arxiv.org/abs/2511.06848)
*Huiyuan Tian,Bonan Xu Shijian Li*

Main category: cs.CV

TL;DR: 本文分析了为何基于特征的知识蒸馏在视觉Transformer (ViT)中失效，发现其根本原因是教师和学生模型之间存在表示范式不匹配，尤其在后期层中教师模型的高维编码学生模型难以复制。


<details>
  <summary>Details</summary>
Motivation: 基于特征的知识蒸馏在卷积神经网络（CNN）中非常有效，但在ViT中却意外失败，甚至不如简单的logit蒸馏。研究旨在探究这一现象背后的原因。

Method: 引入了名为“蒸馏动力学”的新分析框架，结合了频率谱分析、信息熵指标和激活幅度跟踪来全面调查ViT的信息处理模式。

Result: 研究发现ViT表现出独特的U形信息处理模式（先压缩后扩展）。负迁移的根本原因是教师和学生模型之间存在根本的表示范式不匹配。频率域分析显示，教师模型在后期层采用分布式、高维编码策略，而小型学生模型由于通道容量有限无法复制，导致后期层的特征对齐反而损害了学生模型的性能。

Conclusion: 成功的ViT知识迁移需要超越简单的特征模仿，采用尊重这些基本表示约束的方法，为设计有效的ViT压缩策略提供了重要的理论指导。

Abstract: While feature-based knowledge distillation has proven highly effective for compressing CNNs, these techniques unexpectedly fail when applied to Vision Transformers (ViTs), often performing worse than simple logit-based distillation. We provide the first comprehensive analysis of this phenomenon through a novel analytical framework termed as ``distillation dynamics", combining frequency spectrum analysis, information entropy metrics, and activation magnitude tracking. Our investigation reveals that ViTs exhibit a distinctive U-shaped information processing pattern: initial compression followed by expansion. We identify the root cause of negative transfer in feature distillation: a fundamental representational paradigm mismatch between teacher and student models. Through frequency-domain analysis, we show that teacher models employ distributed, high-dimensional encoding strategies in later layers that smaller student models cannot replicate due to limited channel capacity. This mismatch causes late-layer feature alignment to actively harm student performance. Our findings reveal that successful knowledge transfer in ViTs requires moving beyond naive feature mimicry to methods that respect these fundamental representational constraints, providing essential theoretical guidance for designing effective ViTs compression strategies. All source code and experimental logs are provided in the supplementary material.

</details>


### [250] [GFix: Perceptually Enhanced Gaussian Splatting Video Compression](https://arxiv.org/abs/2511.06953)
*Siyue Teng,Ge Gao,Duolikun Danier,Yuxuan Jiang,Fan Zhang,Thomas Davis,Zoe Liu,David Bull*

Main category: cs.CV

TL;DR: 针对3D Gaussian Splatting (3DGS) 视频编码中存在的视觉伪影和低压缩率问题，本文提出了GFix框架。GFix利用单步扩散模型进行感知增强，并引入调制LoRA方案提高压缩效率，实现了显著的感知质量提升和BD-rate节省。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3DGS的视频编解码器通常存在明显的视觉伪影和相对较低的压缩比。本研究旨在专门提升基于3DGS的视频压缩的感知质量。

Method: 本文提出了GFix，一个内容自适应框架，包含一个简化的单步扩散模型作为现成的神经增强器，其前提是3DGS渲染和量化产生的伪影类似于扩散训练中采样的噪声潜在变量。此外，为提高压缩效率，提出了一种调制LoRA方案，冻结低秩分解并调制中间隐藏状态，从而通过高度可压缩的更新实现扩散骨干网络的有效自适应。

Result: 实验结果表明，GFix在感知质量增强方面表现出色，相较于GSVC，在LPIPS上实现了高达72.1%的BD-rate节省，在FID上实现了21.4%的BD-rate节省。

Conclusion: GFix通过其提出的内容自适应单步扩散模型和调制LoRA方案，成功解决了3DGS视频编码中的感知质量和压缩效率问题，带来了强大的感知质量提升和显著的BD-rate节省。

Abstract: 3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through explicit representation and fast rendering, demonstrating potential benefits for various low-level vision tasks, including video compression. However, existing 3DGS-based video codecs generally exhibit more noticeable visual artifacts and relatively low compression ratios. In this paper, we specifically target the perceptual enhancement of 3DGS-based video compression, based on the assumption that artifacts from 3DGS rendering and quantization resemble noisy latents sampled during diffusion training. Building on this premise, we propose a content-adaptive framework, GFix, comprising a streamlined, single-step diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to increase compression efficiency, We propose a modulated LoRA scheme that freezes the low-rank decompositions and modulates the intermediate hidden states, thereby achieving efficient adaptation of the diffusion backbone with highly compressible updates. Experimental results show that GFix delivers strong perceptual quality enhancement, outperforming GSVC with up to 72.1% BD-rate savings in LPIPS and 21.4% in FID.

</details>


### [251] [Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models](https://arxiv.org/abs/2511.07004)
*Christofer Meinecke,Estelle Guéville,David Joseph Wrisley*

Main category: cs.CV

TL;DR: 该研究旨在通过先进技术对中世纪手稿页面及其内容进行整体理论化分析和描述，以创建更丰富的计算机视觉训练数据。


<details>
  <summary>Details</summary>
Motivation: 需要为中世纪特定视觉内容的计算机视觉技术（如实例分割和多模态模型）创建更丰富、更全面的训练数据。

Method: 采用最先进的技术对整个手稿对开页进行分割和描述，以更整体地理论化中世纪手稿页面及其内容。

Result: 通过整体分析和描述，旨在创建更丰富的训练数据，以支持计算机视觉技术（特别是实例分割）和针对中世纪视觉内容的多模态模型。

Conclusion: 通过对中世纪手稿页面的整体分析和数据增强，旨在显著改善计算机视觉技术在中世纪视觉内容识别和理解方面的表现。

Abstract: We aim to theorize the medieval manuscript page and its contents more holistically, using state-of-the-art techniques to segment and describe the entire manuscript folio, for the purpose of creating richer training data for computer vision techniques, namely instance segmentation, and multimodal models for medieval-specific visual content.

</details>


### [252] [Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked Autoencoder for Histopathology Representation Learning](https://arxiv.org/abs/2511.06958)
*Raneen Younis,Louay Hamdi,Lukas Chavez,Zahra Ahmadi*

Main category: cs.CV

TL;DR: 本文提出WISE-MAE框架，通过小波变换引导的补丁选择策略，为数字病理学中的自监督学习（基于MAE）引入结构和生物学相关性，以提高表征学习质量和下游分类性能。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中全玻片图像尺寸巨大且注释稀缺，自监督学习至关重要。传统的基于MAE的预训练方法在随机补丁采样时常包含无关或噪声区域，限制了模型捕获有意义组织模式的能力。

Method: WISE-MAE是一个轻量级且领域适应的框架，采用两步粗到细的过程：首先在低倍镜下进行基于小波的筛选，以定位结构丰富的区域；然后进行高分辨率提取，用于详细建模。这种方法模拟了病理学家的诊断工作流程。

Result: 在肺癌、肾癌和结直肠癌等多个癌症数据集上的评估表明，WISE-MAE在保持弱监督下效率的同时，实现了具有竞争力的表征质量和下游分类性能。

Conclusion: WISE-MAE通过小波变换引导的补丁选择策略，成功地将结构和生物学相关性引入MAE学习中，有效提高了学习到的表征质量，并展现出良好的下游任务性能。

Abstract: Whole-slide images are central to digital pathology, yet their extreme size and scarce annotations make self-supervised learning essential. Masked Autoencoders (MAEs) with Vision Transformer backbones have recently shown strong potential for histopathology representation learning. However, conventional random patch sampling during MAE pretraining often includes irrelevant or noisy regions, limiting the model's ability to capture meaningful tissue patterns. In this paper, we present a lightweight and domain-adapted framework that brings structure and biological relevance into MAE-based learning through a wavelet-informed patch selection strategy. WISE-MAE applies a two-step coarse-to-fine process: wavelet-based screening at low magnification to locate structurally rich regions, followed by high-resolution extraction for detailed modeling. This approach mirrors the diagnostic workflow of pathologists and improves the quality of learned representations. Evaluations across multiple cancer datasets, including lung, renal, and colorectal tissues, show that WISE-MAE achieves competitive representation quality and downstream classification performance while maintaining efficiency under weak supervision.

</details>


### [253] [Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain](https://arxiv.org/abs/2511.07029)
*Liang Zhou,Qiming Wang,Tianze Chen*

Main category: cs.CV

TL;DR: FreqCert是一种新颖的认证框架，通过将鲁棒性分析转移到频率域，实现对3D点云分类器的结构化L2扰动认证，显著提高了认证准确性和经验准确性。


<details>
  <summary>Details</summary>
Motivation: 3D点云分类器在自动驾驶、机器人等安全关键应用中易受对抗性扰动和几何损坏的影响。现有认证防御主要限制点级扰动，却忽略了细微的几何失真，这可能导致错误分类，对安全部署构成风险。

Method: 本文提出了FreqCert框架。它首先通过图傅里叶变换（GFT）将输入点云转换到频率域，然后应用结构化频率感知子采样生成多个子点云。每个子点云由标准模型独立分类，最终预测通过多数投票获得。子点云的构建基于频谱相似性而非空间邻近性，使其在L2扰动下更稳定并与物体内在结构对齐。此外，本文推导了一个封闭形式的认证L2鲁棒性半径下限，并证明了其在最小且可解释假设下的紧密性。

Result: 在ModelNet40和ScanObjectNN数据集上进行的大量实验表明，FreqCert在强扰动下始终能实现更高的认证准确性和经验准确性。

Conclusion: 研究结果表明，频谱表示为3D点云识别中可认证的鲁棒性提供了一条有效的途径。

Abstract: 3D point cloud classification is a fundamental task in safety-critical applications such as autonomous driving, robotics, and augmented reality. However, recent studies reveal that point cloud classifiers are vulnerable to structured adversarial perturbations and geometric corruptions, posing risks to their deployment in safety-critical scenarios. Existing certified defenses limit point-wise perturbations but overlook subtle geometric distortions that preserve individual points yet alter the overall structure, potentially leading to misclassification. In this work, we propose FreqCert, a novel certification framework that departs from conventional spatial domain defenses by shifting robustness analysis to the frequency domain, enabling structured certification against global L2-bounded perturbations. FreqCert first transforms the input point cloud via the graph Fourier transform (GFT), then applies structured frequency-aware subsampling to generate multiple sub-point clouds. Each sub-cloud is independently classified by a standard model, and the final prediction is obtained through majority voting, where sub-clouds are constructed based on spectral similarity rather than spatial proximity, making the partitioning more stable under L2 perturbations and better aligned with the object's intrinsic structure. We derive a closed-form lower bound on the certified L2 robustness radius and prove its tightness under minimal and interpretable assumptions, establishing a theoretical foundation for frequency domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN datasets demonstrate that FreqCert consistently achieves higher certified accuracy and empirical accuracy under strong perturbations. Our results suggest that spectral representations provide an effective pathway toward certifiable robustness in 3D point cloud recognition.

</details>


### [254] [Improving Deepfake Detection with Reinforcement Learning-Based Adaptive Data Augmentation](https://arxiv.org/abs/2511.07051)
*Yuxuan Zhou,Tao Yu,Wen Huang,Yuheng Zhang,Tao Dai,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出CRDA框架，通过结合强化学习和因果推断，动态生成从简单到复杂的伪造人脸数据进行增强，显著提升了深度伪造检测器在跨域场景下的泛化能力，优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器的数据增强方法依赖于固定的策略，无法充分模拟真实世界中不断演变和复杂多样的伪造特征（如面部扭曲、表情操纵），导致泛化能力不足。单一静态的增强策略无法满足需求。

Method: 本文提出CRDA（课程强化学习数据增强）框架。CRDA通过可配置的伪造操作池合成增强样本，并根据检测器当前的学习状态动态生成对抗性样本。核心在于整合了强化学习（RL）和因果推断：RL智能体根据检测器性能动态选择增强动作，以有效探索广阔的增强空间并适应更具挑战性的伪造；同时，因果推断引导智能体引入动作空间变化以生成异构伪造模式，缓解虚假相关性，抑制任务无关偏差，并关注因果不变特征，从而确保鲁棒泛化。

Result: 实验结果表明，CRDA显著提升了检测器的泛化能力，在多个跨域数据集上均优于现有最先进（SOTA）方法。

Conclusion: CRDA通过动态的、结合强化学习和因果推断的数据增强策略，有效解决了固定增强策略的局限性，使检测器能逐步掌握多领域伪造特征，并缓解虚假相关性，从而为深度伪造检测器带来了强大的泛化能力和鲁棒性。

Abstract: The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.

</details>


### [255] [RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion](https://arxiv.org/abs/2511.07067)
*Ruijie Zhang,Bixin Zeng,Shengpeng Wang,Fuhui Zhou,Wei Wang*

Main category: cs.CV

TL;DR: 毫米波雷达因其稀疏性限制了3D感知能力。RaLD框架通过结合激光雷达自动编码、顺序不变潜在表示和雷达频谱条件化，利用潜在扩散模型从原始雷达频谱生成密集准确的3D点云，从而提升了恶劣环境下的感知能力。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在恶劣条件下表现鲁棒且成本低廉，但其点云稀疏且分辨率低，严重限制了需要密集精确3D感知的任务。现有生成方法效率低下，难以保留结构细节。此外，潜在扩散模型（LDMs）因缺乏兼容的表示和条件化策略，尚未有效应用于基于雷达的3D生成。

Method: 本文提出了RaLD框架，通过集成以下关键组件来弥补现有空白：1. 场景级视锥体激光雷达自动编码；2. 顺序不变的潜在表示；3. 直接雷达频谱条件化。这些策略共同实现了更紧凑和富有表现力的生成过程。

Result: 实验结果表明，RaLD能够从原始雷达频谱生成密集且准确的3D点云。

Conclusion: RaLD为在复杂环境中实现鲁棒感知提供了一个有前景的解决方案，有效解决了雷达点云的局限性并成功利用了潜在扩散模型。

Abstract: Millimeter-wave radar offers a promising sensing modality for autonomous systems thanks to its robustness in adverse conditions and low cost. However, its utility is significantly limited by the sparsity and low resolution of radar point clouds, which poses challenges for tasks requiring dense and accurate 3D perception. Despite that recent efforts have shown great potential by exploring generative approaches to address this issue, they often rely on dense voxel representations that are inefficient and struggle to preserve structural detail. To fill this gap, we make the key observation that latent diffusion models (LDMs), though successful in other modalities, have not been effectively leveraged for radar-based 3D generation due to a lack of compatible representations and conditioning strategies. We introduce RaLD, a framework that bridges this gap by integrating scene-level frustum-based LiDAR autoencoding, order-invariant latent representations, and direct radar spectrum conditioning. These insights lead to a more compact and expressive generation process. Experiments show that RaLD produces dense and accurate 3D point clouds from raw radar spectrums, offering a promising solution for robust perception in challenging environments.

</details>


### [256] [3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition](https://arxiv.org/abs/2511.07040)
*Yuanmin Huang,Wenxuan Li,Mi Zhang,Xiaohan Zhang,Xiaoyu You,Min Yang*

Main category: cs.CV

TL;DR: 针对3D点云识别中深度神经网络易受对抗性攻击的问题，本文提出了3D-ANC方法。该方法利用神经坍缩（NC）机制，通过解耦特征空间来增强模型鲁棒性，有效应对了类别不平衡和几何相似性等挑战，显著提升了模型在对抗环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在3D点云识别中取得了显著进展，但其对对抗性扰动的脆弱性在实际部署中带来了严重的安全挑战。传统的防御机制难以应对多方面的攻击模式。通过系统分析现有防御，发现其性能不佳主要源于纠缠的特征空间，使得对抗性攻击易于实施。

Method: 本文提出了3D-ANC，一种利用神经坍缩（NC）机制来组织判别性特征学习的新方法。NC描述了最后一层特征和分类器权重如何共同演变为一个单纯形等角紧框架（ETF）排列，从而建立最大可分离的类别原型。为解决点云数据集中普遍存在的类别不平衡和对象类别间复杂的几何相似性，3D-ANC将一个ETF对齐的分类模块与一个自适应训练框架相结合，该框架包含表示平衡学习（RBL）和动态特征方向损失（FDL）。

Result: 综合评估表明，3D-ANC显著提高了各种结构模型在两个数据集上的鲁棒性。例如，DGCNN在ModelNet40上的分类准确率从27.2%提升到80.9%，绝对增益达53.7%，超越了领先的基线34.0%。

Conclusion: 3D-ANC能够无缝赋能现有模型，使其在3D数据分布复杂性下也能发展出解耦的特征空间。这显著提升了模型对对抗性攻击的鲁棒性，克服了现有防御机制因特征空间纠缠而性能不佳的问题。

Abstract: Deep neural networks have recently achieved notable progress in 3D point cloud recognition, yet their vulnerability to adversarial perturbations poses critical security challenges in practical deployments. Conventional defense mechanisms struggle to address the evolving landscape of multifaceted attack patterns. Through systematic analysis of existing defenses, we identify that their unsatisfactory performance primarily originates from an entangled feature space, where adversarial attacks can be performed easily. To this end, we present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC) mechanism to orchestrate discriminative feature learning. In particular, NC depicts where last-layer features and classifier weights jointly evolve into a simplex equiangular tight frame (ETF) arrangement, establishing maximally separable class prototypes. However, leveraging this advantage in 3D recognition confronts two substantial challenges: (1) prevalent class imbalance in point cloud datasets, and (2) complex geometric similarities between object categories. To tackle these obstacles, our solution combines an ETF-aligned classification module with an adaptive training framework consisting of representation-balanced learning (RBL) and dynamic feature direction loss (FDL). 3D-ANC seamlessly empowers existing models to develop disentangled feature spaces despite the complexity in 3D data distribution. Comprehensive evaluations state that 3D-ANC significantly improves the robustness of models with various structures on two datasets. For instance, DGCNN's classification accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain that surpasses leading baselines by 34.0%.

</details>


### [257] [Performance Decay in Deepfake Detection: The Limitations of Training on Outdated Data](https://arxiv.org/abs/2511.07009)
*Jack Richings,Margaux Leblanc,Ian Groves,Victoria Nockles*

Main category: cs.CV

TL;DR: 深度伪造技术不断进步，对检测构成挑战。一种两阶段检测方法初期效果良好，但对新生成技术表现迅速下降。研究表明，鲁棒检测需要持续多样化的数据收集，并应侧重于帧级伪影而非时间不一致性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术质量不断提高，使得恶意生成的内容越来越难以与现实区分，从而加剧了虚假信息、欺诈和骚扰的威胁，促使研究人员寻求有效的检测方法。

Method: 本文提出了一种简单而有效的两阶段检测方法。

Result: 该方法在当代深度伪造数据集上实现了超过99.8%的AUROC。然而，其性能衰减显著，在仅六个月后生成的新深度伪造内容上，召回率下降超过30%。分析揭示，鲁棒检测需要持续收集大规模、多样化的数据集，并且预测能力主要来源于静态的帧级伪影，而非时间不一致性。

Conclusion: 未来有效的深度伪造检测依赖于快速的数据收集和先进的帧级特征检测器的开发。

Abstract: The continually advancing quality of deepfake technology exacerbates the threats of disinformation, fraud, and harassment by making maliciously-generated synthetic content increasingly difficult to distinguish from reality. We introduce a simple yet effective two-stage detection method that achieves an AUROC of over 99.8% on contemporary deepfakes. However, this high performance is short-lived. We show that models trained on this data suffer a recall drop of over 30% when evaluated on deepfakes created with generation techniques from just six months later, demonstrating significant decay as threats evolve. Our analysis reveals two key insights for robust detection. Firstly, continued performance requires the ongoing curation of large, diverse datasets. Second, predictive power comes primarily from static, frame-level artifacts, not temporal inconsistencies. The future of effective deepfake detection therefore depends on rapid data collection and the development of advanced frame-level feature detectors.

</details>


### [258] [ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora](https://arxiv.org/abs/2511.07068)
*Nikolas Adaloglou,Diana Petrusheva,Mohamed Asker,Felix Michels,Markus Kollmann*

Main category: cs.CV

TL;DR: 本文提出ClusterMine，一种无需预定义正向标签的无监督OOD检测方法。它通过结合视觉聚类和零样本图文一致性，从文本语料库中挖掘正向概念，实现了最先进的OOD检测性能，并对协变量分布内偏移具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉OOD检测方法依赖于预定义的分布内（ID）真实标签名称，这些标签可能不可用、不可靠或因部署后的ID偏移而失去相关性。研究目标是实现真正的无监督OOD检测。

Method: 在通用概念挖掘范式下，利用广泛可用的文本语料库进行正向标签挖掘。提出ClusterMine方法，它结合了纯视觉样本一致性（通过聚类）和零样本图文一致性，从大型文本语料库中提取正向概念。

Result: ClusterMine是首个在无需正向标签的情况下实现最先进OOD检测性能的方法。它可扩展到多种CLIP模型，并对协变量分布内偏移表现出最先进的鲁棒性。

Conclusion: ClusterMine成功解决了OOD检测中对正向标签的依赖问题，通过从文本语料库中挖掘概念，提供了一种无需正向标签、可扩展且对分布内偏移鲁棒的无监督OOD检测解决方案。

Abstract: Large-scale visual out-of-distribution (OOD) detection has witnessed remarkable progress by leveraging vision-language models such as CLIP. However, a significant limitation of current methods is their reliance on a pre-defined set of in-distribution (ID) ground-truth label names (positives). These fixed label names can be unavailable, unreliable at scale, or become less relevant due to in-distribution shifts after deployment. Towards truly unsupervised OOD detection, we utilize widely available text corpora for positive label mining, bypassing the need for positives. In this paper, we utilize widely available text corpora for positive label mining under a general concept mining paradigm. Within this framework, we propose ClusterMine, a novel positive label mining method. ClusterMine is the first method to achieve state-of-the-art OOD detection performance without access to positive labels. It extracts positive concepts from a large text corpus by combining visual-only sample consistency (via clustering) and zero-shot image-text consistency. Our experimental study reveals that ClusterMine is scalable across a plethora of CLIP models and achieves state-of-the-art robustness to covariate in-distribution shifts. The code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.

</details>


### [259] [From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge](https://arxiv.org/abs/2511.07049)
*Hui Lu,Yi Yu,Song Xia,Yiming Yang,Deepu Rajan,Boon Poh Ng,Alex Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为TVA的、无需访问受害者任务或数据的可迁移视频对抗攻击方法，旨在利用开源视频基础模型（VFM）的时间表征动态，攻击其下游模型或多模态大语言模型（MLLM），揭示了VFM部署中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 大规模视频基础模型（VFM）的开放性带来了严重的安全风险，攻击者可能利用对VFM的完全了解发起强大攻击。现有的攻击方法通常依赖于任务对齐的替代模型或需要访问受害者任务数据。本文旨在探索一种新颖且实用的威胁场景：在不访问受害者任务、训练数据、模型查询和架构的情况下，直接从VFM中利用对抗性漏洞来攻击其下游模型或MLLM。

Method: 本文提出了可迁移视频攻击（TVA）方法。TVA是一种时间感知对抗攻击方法，利用VFM的时间表征动态来生成有效的扰动。它整合了双向对比学习机制，以最大化干净特征和对抗性特征之间的差异，并引入了时间一致性损失，利用运动线索增强扰动的序列影响。TVA避免了训练昂贵的替代模型或访问特定领域数据的需求。

Result: 在24个视频相关任务上的大量实验表明，TVA对下游模型和MLLM均有效。这揭示了视频模型部署中一个先前未被充分探索的安全漏洞。TVA提供了一种更实用和高效的攻击策略。

Conclusion: 开源视频基础模型（VFM）存在未被充分探索的安全漏洞，可以直接通过利用其时间表征动态进行攻击。TVA是一种无需依赖替代模型或任务数据的实用且高效的对抗攻击方法，能够有效攻击从VFM微调的下游模型和多模态大语言模型。

Abstract: Large-scale Video Foundation Models (VFMs) has significantly advanced various video-related tasks, either through task-specific models or Multi-modal Large Language Models (MLLMs). However, the open accessibility of VFMs also introduces critical security risks, as adversaries can exploit full knowledge of the VFMs to launch potent attacks. This paper investigates a novel and practical adversarial threat scenario: attacking downstream models or MLLMs fine-tuned from open-source VFMs, without requiring access to the victim task, training data, model query, and architecture. In contrast to conventional transfer-based attacks that rely on task-aligned surrogate models, we demonstrate that adversarial vulnerabilities can be exploited directly from the VFMs. To this end, we propose the Transferable Video Attack (TVA), a temporal-aware adversarial attack method that leverages the temporal representation dynamics of VFMs to craft effective perturbations. TVA integrates a bidirectional contrastive learning mechanism to maximize the discrepancy between the clean and adversarial features, and introduces a temporal consistency loss that exploits motion cues to enhance the sequential impact of perturbations. TVA avoids the need to train expensive surrogate models or access to domain-specific data, thereby offering a more practical and efficient attack strategy. Extensive experiments across 24 video-related tasks demonstrate the efficacy of TVA against downstream models and MLLMs, revealing a previously underexplored security vulnerability in the deployment of video models.

</details>


### [260] [LeCoT: revisiting network architecture for two-view correspondence pruning](https://arxiv.org/abs/2511.07078)
*Luanyuan Dai,Xiaoyu Du,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文提出LeCoT，一个用于双视图对应剪枝的新网络，它通过空间-通道融合Transformer块自然地利用全局上下文信息，无需额外模块，并在多项任务中超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前流行的对应剪枝策略以MLP为骨干，辅以额外模块来增强其处理上下文信息的能力，而MLP本身在处理上下文信息方面存在局限性。本文旨在无需额外设计模块的情况下，引入一种捕捉对应上下文信息的新视角。

Method: 本文设计了一个名为LeCoT的双视图对应剪枝网络。其核心是新提出的“空间-通道融合Transformer块”，该模块能高效利用稀疏对应关系中的空间和通道全局上下文信息。此外，LeCoT集成了一个预测块，利用中间阶段的对应特征生成一个逐步细化的概率集，作为后续学习阶段的指导信息，从而更有效地捕获鲁棒的全局上下文信息并缓解信息丢失问题。

Result: 广泛的实验证明，所提出的LeCoT在对应剪枝、相对姿态估计、单应性估计、视觉定位和3D重建任务中均优于现有最先进的方法。

Conclusion: LeCoT通过其新颖的网络架构，特别是空间-通道融合Transformer块和逐步细化的预测块，能够自然有效地利用全局上下文信息进行对应剪枝，并在多项计算机视觉任务中取得了卓越的性能。

Abstract: Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks. Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs. In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules. To this end, we design a two-view correspondence pruning network called LeCoT, which can naturally leverage global context information at different stages. Specifically, the core design of LeCoT is the Spatial-Channel Fusion Transformer block, a newly proposed component that efficiently utilizes both spatial and channel global context information among sparse correspondences. In addition, we integrate the proposed prediction block that utilizes correspondence features from intermediate stages to generate a probability set, which acts as guiding information for subsequent learning phases, allowing the network to more effectively capture robust global context information. Notably, this prediction block progressively refines the probability set, thereby mitigating the issue of information loss that is common in the traditional one. Extensive experiments prove that the proposed LeCoT outperforms state-of-the-art methods in correspondence pruning, relative pose estimation, homography estimation, visual localization, and $3$D~reconstruction tasks. The code is provided in https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning.

</details>


### [261] [Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction](https://arxiv.org/abs/2511.07122)
*Changyue Shi,Chuxiao Yang,Xinyuan Hu,Minghao Chen,Wenwen Pan,Yan Yang,Jiajun Ding,Zhou Yu,Jun Yu*

Main category: cs.CV

TL;DR: Sparse4DGS是首个针对稀疏帧动态场景重建的方法，通过纹理感知变形正则化和纹理感知规范优化，在稀疏帧输入下实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的动态高斯溅射（Dynamic Gaussian Splatting）方法依赖于密集帧视频序列进行高质量重建。然而，在实际场景中，由于设备限制，有时只能获取稀疏帧，导致现有方法失效。

Method: Sparse4DGS专注于纹理丰富的区域。对于变形网络，提出“纹理感知变形正则化”（Texture-Aware Deformation Regularization），引入基于纹理的深度对齐损失来规范高斯变形。对于规范高斯场，引入“纹理感知规范优化”（Texture-Aware Canonical Optimization），将基于纹理的噪声融入到规范高斯场的梯度下降过程中。

Result: 大量实验表明，在以稀疏帧作为输入时，Sparse4DGS在NeRF-Synthetic、HyperNeRF、NeRF-DS以及自定义的iPhone-4D数据集上均优于现有的动态或少样本技术。

Conclusion: Sparse4DGS成功解决了稀疏帧动态场景重建的挑战，通过创新的纹理感知机制，在有限数据条件下实现了高质量的4D场景重建。

Abstract: Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.

</details>


### [262] [ProcGen3D: Learning Neural Procedural Graph Representations for Image-to-3D Reconstruction](https://arxiv.org/abs/2511.07142)
*Xinyi Zhang,Daoyi Gao,Naiqi Li,Angela Dai*

Main category: cs.CV

TL;DR: ProcGen3D是一种新的3D内容创建方法，通过生成3D对象的程序图抽象，然后解码为复杂3D资产。它利用Transformer和蒙特卡洛树搜索（MCTS）从图像进行3D重建。


<details>
  <summary>Details</summary>
Motivation: 受生产3D应用中程序生成器广泛使用的启发，研究旨在解决从图像进行3D重建的挑战，生成更丰富、更复杂的3D资产。

Method: 该方法提出了一种序列化的、基于图的程序图表示。它使用基于边缘的标记化来编码程序图，并训练一个Transformer模型，根据输入的RGB图像预测下一个标记。关键是，通过引入蒙特卡洛树搜索（MCTS）引导采样，使生成的程序图更忠实于输入图像。

Result: 在仙人掌、树木和桥梁等对象上的实验表明，ProcGen3D的神经程序图生成优于最先进的生成式3D方法和领域特定建模技术。此外，尽管仅在合成数据上训练，它在真实世界输入图像上表现出更好的泛化能力。

Conclusion: ProcGen3D提供了一种有效的从图像创建3D内容的方法，通过生成复杂的程序图抽象，能够产生高质量、忠实于图像的3D资产，并展现出强大的性能和泛化能力。

Abstract: We introduce ProcGen3D, a new approach for 3D content creation by generating procedural graph abstractions of 3D objects, which can then be decoded into rich, complex 3D assets. Inspired by the prevalent use of procedural generators in production 3D applications, we propose a sequentialized, graph-based procedural graph representation for 3D assets. We use this to learn to approximate the landscape of a procedural generator for image-based 3D reconstruction. We employ edge-based tokenization to encode the procedural graphs, and train a transformer prior to predict the next token conditioned on an input RGB image. Crucially, to enable better alignment of our generated outputs to an input image, we incorporate Monte Carlo Tree Search (MCTS) guided sampling into our generation process, steering output procedural graphs towards more image-faithful reconstructions. Our approach is applicable across a variety of objects that can be synthesized with procedural generators. Extensive experiments on cacti, trees, and bridges show that our neural procedural graph generation outperforms both state-of-the-art generative 3D methods and domain-specific modeling techniques. Furthermore, this enables improved generalization on real-world input images, despite training only on synthetic data.

</details>


### [263] [Automated Estimation of Anatomical Risk Metrics for Endoscopic Sinus Surgery Using Deep Learning](https://arxiv.org/abs/2511.07199)
*Konrad Reuter,Lennart Thaysen,Bilkay Doruk,Sarah Latus,Brigitte Holst,Benjamin Becker,Dennis Eggert,Christian Betz,Anna-Sophie Hoffmann,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 本文提出一种自动化深度学习方法，通过热图回归定位关键解剖标志物，以估计鼻内窥镜手术中颅底解剖风险评分（如Keros、Gera和TMS评分）。


<details>
  <summary>Details</summary>
Motivation: 鼻内窥镜手术需要仔细的术前颅底解剖评估以最小化脑脊液漏等风险。现有的Keros、Gera和TMS等解剖风险评分方法标准化，但需要耗时的人工测量。

Method: 开发了一个自动化深度学习流程，通过热图回归定位关键解剖标志物，从而估计风险评分。比较了直接方法与专门的全局到局部学习策略。

Result: 在相关解剖测量中，Keros评分的平均绝对误差为0.506毫米，Gera评分的平均绝对误差为4.516°，TMS分类的平均绝对误差分别为0.802毫米和0.777毫米。

Conclusion: 所提出的深度学习流程能够准确自动化评估颅底解剖风险评分，有望提高术前评估的效率和安全性。

Abstract: Endoscopic sinus surgery requires careful preoperative assessment of the skull base anatomy to minimize risks such as cerebrospinal fluid leakage. Anatomical risk scores like the Keros, Gera and Thailand-Malaysia-Singapore score offer a standardized approach but require time-consuming manual measurements on coronal CT or CBCT scans. We propose an automated deep learning pipeline that estimates these risk scores by localizing key anatomical landmarks via heatmap regression. We compare a direct approach to a specialized global-to-local learning strategy and find mean absolute errors on the relevant anatomical measurements of 0.506mm for the Keros, 4.516° for the Gera and 0.802mm / 0.777mm for the TMS classification.

</details>


### [264] [Geometric implicit neural representations for signed distance functions](https://arxiv.org/abs/2511.07206)
*Luiz Schirmer,Tiago Novello,Vinícius da Silva,Guilherme Schardong,Daniel Perazzo,Hélio Lopes,Nuno Gonçalves,Luiz Velho*

Main category: cs.CV

TL;DR: 这篇综述回顾了隐式神经表示（INR）在通过有向点云或姿态图像近似有符号距离函数（SDF）方面的应用，特别强调了几何INR在3D表面重建中的进展。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）作为一种低维空间信号表示框架前景广阔。本研究的动机是系统回顾和分析其在近似表面场景的有符号距离函数（SDF）方面的应用，尤其是那些结合了微分几何工具的INR，以促进3D重建。

Method: 本研究通过文献综述，探讨了使用有向点云或姿态图像近似SDF的INR方法。重点关注“几何INR”，其损失函数中包含微分几何工具（如法线和曲率）作为正则化项，以确保SDF满足单位梯度等全局属性。综述还探讨了INR的定义、几何损失函数的构建以及从微分几何角度的采样方案。

Result: 几何INR通过在损失函数中引入微分几何工具，显著推动了从有向点云和姿态图像进行表面重建的进展。

Conclusion: 通过将微分几何工具（如法线和曲率）纳入损失函数，几何隐式神经表示（INR）在从有向点云和姿态图像进行3D表面重建方面取得了显著进步，为SDF的近似提供了强大的方法。

Abstract: \textit{Implicit neural representations} (INRs) have emerged as a promising framework for representing signals in low-dimensional spaces. This survey reviews the existing literature on the specialized INR problem of approximating \textit{signed distance functions} (SDFs) for surface scenes, using either oriented point clouds or a set of posed images. We refer to neural SDFs that incorporate differential geometry tools, such as normals and curvatures, in their loss functions as \textit{geometric} INRs. The key idea behind this 3D reconstruction approach is to include additional \textit{regularization} terms in the loss function, ensuring that the INR satisfies certain global properties that the function should hold -- such as having unit gradient in the case of SDFs. We explore key methodological components, including the definition of INR, the construction of geometric loss functions, and sampling schemes from a differential geometry perspective. Our review highlights the significant advancements enabled by geometric INRs in surface reconstruction from oriented point clouds and posed images.

</details>


### [265] [LiteUpdate: A Lightweight Framework for Updating AI-Generated Image Detectors](https://arxiv.org/abs/2511.07192)
*Jiajie Lu,Zhenkan Fu,Na Zhao,Long Xing,Kejiang Chen,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: LiteUpdate是一个轻量级框架，通过代表性样本选择和模型合并，高效更新AI生成图像检测器，以适应新生成器并缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致新生成模型的出现，而现有检测方法难以跟上，导致检测性能显著下降。这凸显了持续更新AI生成图像检测器以适应新生成器的紧迫需求，同时需要解决更新过程中的低效率和灾难性遗忘问题。

Method: 本文提出了LiteUpdate框架。它包含一个代表性样本选择模块，利用图像置信度和基于梯度的判别特征精确选择边界样本，以提高在有限新生成图像上的学习和检测精度。此外，它还包含一个模型合并模块，融合来自预训练、代表性样本和随机更新等多个微调轨迹的权重，以平衡对新生成器的适应性并减轻对先验知识的灾难性遗忘。

Result: 实验证明，LiteUpdate显著提升了各种检测器的检测性能。具体而言，在AIDE检测器上，Midjourney图像的平均检测准确率从87.63%提高到93.03%，相对增长了6.16%。

Conclusion: LiteUpdate提供了一种有效且轻量级的解决方案，用于更新AI生成图像检测器，使其能够高效适应新的生成模型，同时保持对先前知识的鲁棒性，从而显著提升检测性能。

Abstract: The rapid progress of generative AI has led to the emergence of new generative models, while existing detection methods struggle to keep pace, resulting in significant degradation in the detection performance. This highlights the urgent need for continuously updating AI-generated image detectors to adapt to new generators. To overcome low efficiency and catastrophic forgetting in detector updates, we propose LiteUpdate, a lightweight framework for updating AI-generated image detectors. LiteUpdate employs a representative sample selection module that leverages image confidence and gradient-based discriminative features to precisely select boundary samples. This approach improves learning and detection accuracy on new distributions with limited generated images, significantly enhancing detector update efficiency. Additionally, LiteUpdate incorporates a model merging module that fuses weights from multiple fine-tuning trajectories, including pre-trained, representative, and random updates. This balances the adaptability to new generators and mitigates the catastrophic forgetting of prior knowledge. Experiments demonstrate that LiteUpdate substantially boosts detection performance in various detectors. Specifically, on AIDE, the average detection accuracy on Midjourney improved from 87.63% to 93.03%, a 6.16% relative increase.

</details>


### [266] [MPJudge: Towards Perceptual Assessment of Music-Induced Paintings](https://arxiv.org/abs/2511.07137)
*Shiqi Jiang,Tianyi Liang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 该研究提出了一种新的框架，用于评估音乐启发绘画与音乐之间的感知一致性，通过构建大规模数据集和开发基于调制融合与偏好优化的模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖情感识别模型来评估音乐与绘画的相似性，但这些模型引入了大量噪声，并且忽视了情感之外更广泛的感知线索。因此，需要一种直接建模音乐与视觉艺术之间感知一致性的方法。

Method: 研究提出了一个直接建模音乐与视觉艺术之间感知一致性的新型评估框架。具体方法包括：1) 构建了首个大规模音乐-绘画对数据集MPD，由领域专家基于感知一致性进行标注；2) 为处理模糊情况，进一步收集了成对偏好标注；3) 提出了MPJudge模型，通过基于调制的融合机制将音乐特征整合到视觉编码器中；4) 采用直接偏好优化（DPO）进行训练，以有效学习模糊案例。

Result: 广泛的实验证明，所提出的方法优于现有方法。定性结果进一步表明，该模型能更准确地识别绘画中与音乐相关的区域。

Conclusion: 该研究成功提出了一种新颖的音乐启发绘画评估框架、一个大规模专家标注数据集和基于调制融合与偏好优化的MPJudge模型，能够直接建模音乐与视觉艺术之间的感知一致性，有效处理模糊情况，并显著提高了评估性能。

Abstract: Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.

</details>


### [267] [Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization](https://arxiv.org/abs/2511.07210)
*Binyan Xu,Fan Yang,Di Tang,Xilin Dai,Kehuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GCB（Generative Clean-Image Backdoors）的新型纯净图像后门攻击范式，通过优化触发器本身，在实现成功攻击的同时，将对模型干净准确率的影响降至最低（低于1%），显著提升了攻击的隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有的纯净图像后门攻击方法需要较高的投毒率，导致模型在正常任务上的干净准确率（CA）显著下降，从而降低了攻击的隐蔽性，容易被检测。研究旨在开发一种能最小化这种准确率下降的纯净图像攻击。

Method: GCB框架利用条件InfoGAN来识别图像中自然存在的特征，并将其作为有效且隐蔽的触发器。通过确保这些触发器易于与良性任务相关特征分离，GCB使受害模型能够从极少数中毒样本中学习后门，从而最大限度地减少对干净准确率的影响。

Result: GCB攻击在多种实验中表现出卓越的通用性，成功适应了六个数据集、五种架构和四种任务（包括首次在回归和分割任务中演示纯净图像后门攻击）。攻击导致的干净准确率下降不到1%，并且对大多数现有后门防御措施具有鲁棒性。

Conclusion: GCB提供了一种新的纯净图像后门攻击方法，它通过优化触发器来最大化隐蔽性，将干净准确率的下降控制在极低的水平。其广泛的适用性和对防御的鲁棒性使其成为一个显著的安全威胁，并为未来的防御研究提供了新的挑战。

Abstract: Clean-image backdoor attacks, which use only label manipulation in training datasets to compromise deep neural networks, pose a significant threat to security-critical applications. A critical flaw in existing methods is that the poison rate required for a successful attack induces a proportional, and thus noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This paper presents a new paradigm for clean-image attacks that minimizes this accuracy degradation by optimizing the trigger itself. We introduce Generative Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to identify naturally occurring image features that can serve as potent and stealthy triggers. By ensuring these triggers are easily separable from benign task-related features, GCB enables a victim model to learn the backdoor from an extremely small set of poisoned examples, resulting in a CA drop of less than 1%. Our experiments demonstrate GCB's remarkable versatility, successfully adapting to six datasets, five architectures, and four tasks, including the first demonstration of clean-image backdoors in regression and segmentation. GCB also exhibits resilience against most of the existing backdoor defenses.

</details>


### [268] [Noise & pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection](https://arxiv.org/abs/2511.07233)
*Alexander Bauer,Klaus-Robert Müller*

Main category: cs.CV

TL;DR: 本文提出了一种用于工业检测的自监督自动编码器，通过引入结构化损坏模型和高斯噪声正则化来检测结构异常，并在MVTec AD基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在自动化工业检测中，异常检测至关重要，但收集所有可能的异常代表性样本是不可行的。因此，需要一种方法来识别视觉模式中细微或罕见的缺陷。

Method: 该研究采用自监督自动编码器来修复损坏的输入，以解决结构异常检测问题。其核心方法包括：1) 引入一种新颖的损坏模型，该模型注入结构化、空间连贯的扰动来模拟结构缺陷，使其任务成为分割和修复的混合体。2) 在遮挡之上添加并保留高斯噪声，这作为一种Tikhonov正则化器，将重建函数的雅可比矩阵锚定到恒等矩阵，从而稳定重建并提高检测和分割精度。

Result: 该方法在MVTec AD基准测试中取得了最先进的结果（I/P-AUROC: 99.9/99.4）。

Conclusion: 研究结果支持了所提出的理论框架，并证明了该方法在自动检测中的实际相关性，为工业异常检测提供了有效的解决方案。

Abstract: Anomaly detection plays a pivotal role in automated industrial inspection, aiming to identify subtle or rare defects in otherwise uniform visual patterns. As collecting representative examples of all possible anomalies is infeasible, we tackle structural anomaly detection using a self-supervised autoencoder that learns to repair corrupted inputs. To this end, we introduce a corruption model that injects artificial disruptions into training images to mimic structural defects. While reminiscent of denoising autoencoders, our approach differs in two key aspects. First, instead of unstructured i.i.d.\ noise, we apply structured, spatially coherent perturbations that make the task a hybrid of segmentation and inpainting. Second, and counterintuitively, we add and preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov regularizer anchoring the Jacobian of the reconstruction function toward identity. This identity-anchored regularization stabilizes reconstruction and further improves both detection and segmentation accuracy. On the MVTec AD benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4), supporting our theoretical framework and demonstrating its practical relevance for automatic inspection.

</details>


### [269] [4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation](https://arxiv.org/abs/2511.07241)
*Mengmeng Liu,Jiuming Liu,Yunpeng Zhang,Jiangtao Li,Michael Ying Yang,Francesco Nex,Hao Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为4DSTR的新型4D生成网络，通过时空校正调制4D高斯泼溅，以解决现有方法在时空一致性和适应快速时间变化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 二维图像和三维形状生成领域的显著进展促使人们关注动态四维内容生成。然而，先前的四维生成方法由于缺乏有效的时空建模，通常难以保持时空一致性，并且对快速时间变化的适应性差。

Method: 本文提出了一种名为4DSTR的新型4D生成网络，它通过时空校正来调制生成式4D高斯泼溅。具体来说，设计了跨生成4D序列的时间相关性，以校正可变形的尺度和旋转，并保证时间一致性。此外，提出了一种自适应空间稠密化和剪枝策略，通过动态添加或删除高斯点（感知其前一帧的运动）来解决显著的时间变化。

Result: 广泛的实验表明，4DSTR在视频到4D生成中实现了最先进的性能，在重建质量、时空一致性以及对快速时间运动的适应性方面表现出色。

Conclusion: 4DSTR通过有效的时空建模和自适应策略，成功解决了4D生成中时空一致性差和难以适应快速时间变化的问题，达到了卓越的性能。

Abstract: Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements.

</details>


### [270] [Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee Camps with Sub-Meter Imagery](https://arxiv.org/abs/2511.07231)
*Kyeongjin Ahn,YongHun Suh,Sungwon Han,Jeasurk Yang,Hannes Taubenböck,Meeyoung Cha*

Main category: cs.CV

TL;DR: 本研究提出了一种遥感驱动的框架，用于量化难民营中的水、环境卫生和个人卫生（WASH）可及性。研究发现，在孟加拉国考克斯巴扎的罗兴亚难民营中，WASH可及性正在下降，且存在性别差异，这凸显了需求响应式资源分配策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 难民营中的水、环境卫生和个人卫生（WASH）服务可及性仍然是一个主要的公共卫生问题，尤其是在人口密集的难民营中。研究旨在量化WASH可及性，并识别面临的挑战。

Method: 研究开发了一个遥感驱动的框架，使用亚米级卫星图像和半监督分割框架来检测难民住所（F1-score为76.4%）。该框架应用于多年数据，并进行了性别分层分析，以量化水泵、厕所和淋浴间的可及性。

Result: 研究发现WASH可及性正在下降，这主要是由难民人口快速增长和设施可用性减少所致，从2022年的每设施25人上升到2025年的29.4人。性别分层分析显示，在WASH设施安全隔离不足的情况下，妇女和女童的可及性更低。

Conclusion: 研究强调了需求响应式分配策略的重要性，该策略能够识别服务不足的人群（如妇女和女童），并确保在预算有限或缩减的情况下，有限的基础设施能够服务最多的人。高分辨率遥感和机器学习在发现不平等和指导复杂人道主义环境中的公平资源规划方面具有重要价值。

Abstract: Access to Water, Sanitation, and Hygiene (WASH) services remains a major public health concern in refugee camps. This study introduces a remote sensing-driven framework to quantify WASH accessibility-specifically to water pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one of the world's most densely populated displacement settings. Detecting refugee shelters in such emergent camps presents substantial challenges, primarily due to their dense spatial configuration and irregular geometric patterns. Using sub-meter satellite images, we develop a semi-supervised segmentation framework that achieves an F1-score of 76.4% in detecting individual refugee shelters. Applying the framework across multi-year data reveals declining WASH accessibility, driven by rapid refugee population growth and reduced facility availability, rising from 25 people per facility in 2022 to 29.4 in 2025. Gender-disaggregated analysis further shows that women and girls experience reduced accessibility, in scenarios with inadequate safety-related segregation in WASH facilities. These findings suggest the importance of demand-responsive allocation strategies that can identify areas with under-served populations-such as women and girls-and ensure that limited infrastructure serves the greatest number of people in settings with fixed or shrinking budgets. We also discuss the value of high-resolution remote sensing and machine learning to detect inequality and inform equitable resource planning in complex humanitarian environments.

</details>


### [271] [Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images](https://arxiv.org/abs/2511.07222)
*JiaKui Hu,Shanshan Zhao,Qing-Guo Chen,Xuerui Qiu,Jialun Liu,Zhao Xu,Weihua Luo,Kaifu Zhang,Yanye Lu*

Main category: cs.CV

TL;DR: Omni-View将统一的多模态理解和生成扩展到基于多视图图像的3D场景，通过联合建模场景理解、新视图合成和几何估计，实现了3D场景理解与生成任务的协同作用，并在VSI-Bench上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将统一的多模态理解和生成能力扩展到3D场景，并基于多视图图像探索“生成促进理解”的原则，实现3D场景理解与生成任务之间的协同交互。

Method: Omni-View由理解模型、纹理模块和几何模块组成。纹理模块负责外观合成和时空建模，几何模块提供显式几何约束。该模型联合建模场景理解、新视图合成和几何估计，并通过两阶段策略进行训练。

Result: Omni-View在VSI-Bench基准测试中取得了55.4的最先进分数，超越了现有的专业3D理解模型。同时，它在新视图合成和3D场景生成方面也表现出色。

Conclusion: Omni-View成功地将统一的多模态理解和生成扩展到3D场景，验证了“生成促进理解”的原则。它通过协同建模，在3D场景理解上达到了最先进水平，并在生成任务中表现出色，展示了其在3D场景处理方面的强大能力。

Abstract: This paper presents Omni-View, which extends the unified multimodal understanding and generation to 3D scenes based on multiview images, exploring the principle that "generation facilitates understanding". Consisting of understanding model, texture module, and geometry module, Omni-View jointly models scene understanding, novel view synthesis, and geometry estimation, enabling synergistic interaction between 3D scene understanding and generation tasks. By design, it leverages the spatiotemporal modeling capabilities of its texture module responsible for appearance synthesis, alongside the explicit geometric constraints provided by its dedicated geometry module, thereby enriching the model's holistic understanding of 3D scenes. Trained with a two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the VSI-Bench benchmark, outperforming existing specialized 3D understanding models, while simultaneously delivering strong performance in both novel view synthesis and 3D scene generation.

</details>


### [272] [StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression](https://arxiv.org/abs/2511.07278)
*Yilong Chen,Xiang Bai,Zhibin Wang,Chengyu Bai,Yuhan Dai,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: StreamKV是一个免训练的框架，通过动态语义分段、段级摘要向量和引导提示，统一且分层自适应地实现了KV缓存的检索与压缩，显著提升了Video-LLMs处理长视频的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型（Video-LLMs）在处理长篇真实视频时面临挑战，尤其是在KV缓存的压缩和检索方面尚未得到充分探索。现有方法虽然引入了检索机制，但KV缓存的处理仍有提升空间。

Method: StreamKV是一个免训练框架，它动态地将视频流划分为语义片段，以更好地保留语义信息。在KV缓存检索方面，为每个片段计算一个摘要向量。在KV缓存压缩方面，引入引导提示以捕获每个片段的关键语义元素。StreamKV将KV缓存检索和压缩统一在一个模块中，并以层自适应的方式执行。

Result: 在公共StreamingVQA基准测试上，StreamKV显著优于现有的在线Video-LLMs，实现了更高的准确性，并大幅提升了内存效率和计算延迟。

Conclusion: StreamKV通过其创新的KV缓存检索与压缩机制，有效解决了Video-LLMs在处理长视频时面临的效率和准确性问题，提供了一种优越的流媒体视频问答解决方案。

Abstract: Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.

</details>


### [273] [Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI](https://arxiv.org/abs/2511.07281)
*R. P. Chowdhury,T. Rahman*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的深度学习框架，用于在多种MRI序列上快速自动分割缺血性卒中病灶，通过Res-Unet架构和多数投票分类器实现了高效且准确的分割。


<details>
  <summary>Details</summary>
Motivation: 准确理解缺血性卒中病灶对于治疗和预后至关重要。然而，专家手动分割费时、费力且易受观察者影响。现有的自动方法依赖于手工特征，无法有效捕捉病灶复杂形状。

Method: 本研究提出了一种在T1、T2、DWI和FLAIR等多种MRI序列上自动分割缺血性卒中病灶的新框架。该方法在ISLES 2015数据集上进行验证，采用Res-Unet架构，并分别使用预训练权重和不使用预训练权重进行两次训练以探索迁移学习的益处。最终整合多数投票分类器来融合各轴的结果。

Result: 该分割方法在3D体积上实现了80.5%的Dice分数和74.03%的准确率。

Conclusion: 本研究提出的缺血性卒中病灶自动分割方法高效且有效，在多种MRI序列上表现出良好的性能。

Abstract: The accurate understanding of ischemic stroke lesions is critical for efficient therapy and prognosis of stroke patients. Magnetic resonance imaging (MRI) is sensitive to acute ischemic stroke and is a common diagnostic method for stroke. However, manual lesion segmentation performed by experts is tedious, time-consuming, and prone to observer inconsistency. Automatic medical image analysis methods have been proposed to overcome this challenge. However, previous approaches have relied on hand-crafted features that may not capture the irregular and physiologically complex shapes of ischemic stroke lesions. In this study, we present a novel framework for quickly and automatically segmenting ischemic stroke lesions on various MRI sequences, including T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model using the Res-Unet architecture twice: first, with pre-existing weights, and then without, to explore the benefits of transfer learning. Evaluation metrics, including the Dice score and sensitivity, were computed across 3D volumes. Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes from each axis, resulting in a comprehensive segmentation method. Our efforts culminated in achieving a Dice score of 80.5\% and an accuracy of 74.03\%, showcasing the efficacy of our segmentation approach.

</details>


### [274] [Garbage Vulnerable Point Monitoring using IoT and Computer Vision](https://arxiv.org/abs/2511.07325)
*R. Kumar,A. Lall,S. Chaudhari,M. Kale,A. Vattem*

Main category: cs.CV

TL;DR: 本文提出一种利用物联网和计算机视觉技术，通过街头摄像头和目标检测算法，智能监控城市垃圾易堆点非法倾倒垃圾的系统。


<details>
  <summary>Details</summary>
Motivation: 城市地区垃圾易堆点存在非法倾倒垃圾的问题，需要一种智能且高效的监控和管理方法。

Method: 该研究结合物联网（IoT）和计算机视觉（CV）技术。具体方法包括使用街头摄像头收集数据，并应用多种目标检测算法（YOLOv8, YOLOv10, YOLO11m, RT-DETR）进行垃圾检测。数据采集自印度特伦甘纳邦的Sangareddy地区。

Result: 在所提出的数据集上，YOLO11m模型在垃圾检测方面表现最佳，准确率达到92.39%，mAP@50达到0.91。此外，该系统还能有效捕捉垃圾倾倒模式，包括每小时、每日和每周的倾倒趋势，实现全天候监控。

Conclusion: 研究结果证实，目标检测模型非常适合监测和跟踪垃圾易堆点的非法倾倒事件，并能提供全面的监控和垃圾处理模式分析。

Abstract: This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.

</details>


### [275] [VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models](https://arxiv.org/abs/2511.07299)
*Ying Cheng,Yu-Ho Lin,Min-Hung Chen,Fu-En Yang,Shang-Hong Lai*

Main category: cs.CV

TL;DR: VADER是一个由大型语言模型（LLM）驱动的视频异常理解框架，它结合了关键帧对象关系特征和视觉线索，以提供详细、基于因果关系的异常描述和问答，从而超越了传统的异常检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常分析方法主要关注检测和定位，但忽略了对象之间深层的因果关系和互动，这对于真正理解异常行为至关重要。

Method: VADER框架首先使用异常评分器（Anomaly Scorer）为每帧分配异常分数，然后通过上下文感知采样（CAES）策略捕获异常事件的因果上下文。接着，关系特征提取器和对比关系编码器（CORE）共同建模动态对象交互，生成紧凑的关系表示。这些视觉和关系线索最终与LLM结合，生成详细的、基于因果的异常描述，并支持鲁棒的异常相关问答。

Result: 在多个真实世界视频异常理解（VAU）基准测试中，VADER在异常描述、解释和因果推理任务上均取得了显著成果。

Conclusion: VADER框架通过提供详细的、基于因果关系的异常描述和问答能力，推动了可解释视频异常分析领域的发展。

Abstract: Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.

</details>


### [276] [StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation](https://arxiv.org/abs/2511.07399)
*Tianrui Feng,Zhi Li,Shuo Yang,Haocheng Xi,Muyang Li,Xiuyu Li,Lvmin Zhang,Keting Yang,Kelly Peng,Song Han,Maneesh Agrawala,Kurt Keutzer,Akio Kodaira,Chenfeng Xu*

Main category: cs.CV

TL;DR: StreamDiffusionV2是一个无训练的实时视频扩散模型直播流媒体管道，通过系统级优化和可扩展的并行化，解决了现有方法的时序一致性、延迟和可扩展性问题，实现了超低延迟和高帧率的生成式直播流媒体。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的直播流扩散模型存在时序一致性问题。离线视频扩散模型虽然改善了时序一致性，但主要优化吞吐量，无法满足实时在线直播的严格服务水平目标（如最小的首帧时间、每帧低抖动），且多GPU实时流媒体的可扩展服务问题尚未解决。

Method: StreamDiffusionV2集成了SLO感知的批处理调度器、块调度器、sink-token引导的滚动KV缓存、运动感知噪声控制器以及其他系统级优化。此外，它引入了一种可扩展的管道编排，通过在去噪步骤和网络层之间并行化扩散过程，实现了近乎线性的FPS扩展，同时保证了低延迟。

Result: StreamDiffusionV2在不使用TensorRT或量化的情况下，首帧渲染时间在0.5秒以内，在四块H100 GPU上，使用14B参数模型达到58.28 FPS，使用1.3B参数模型达到64.52 FPS。它实现了近乎线性的FPS扩展，支持异构GPU环境，并能灵活选择1-4个去噪步骤，提供超低延迟和更高质量两种模式。

Conclusion: StreamDiffusionV2使最先进的生成式直播流媒体变得实用和可及，无论是对于个人创作者还是企业级平台，都能提供卓越的性能和可扩展性，解决了实时视频扩散模型在直播应用中的关键挑战。

Abstract: Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.

</details>


### [277] [ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search](https://arxiv.org/abs/2511.06833)
*Zhenjie Liu,Jianzhang Lu,Renjie Lu,Cong Liang,Shangfei Wang*

Main category: cs.CV

TL;DR: 本文提出了ConsistTalk，一个新颖的强度可控、时间一致的扩散噪声搜索推理驱动的口型动画生成框架，通过解耦运动与外观、联合建模音视频运动以及改进推理策略，显著提升了生成视频的稳定性、身份保持和音视频同步性。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在音频驱动人像动画方面存在闪烁、身份漂移和音视频同步性差等问题，这主要源于外观-运动表示的纠缠以及不稳定的推理策略。

Method: ['光流引导时间模块（OFT）：利用面部光流将运动特征与静态外观解耦，以减少视觉闪烁并提高时间一致性。', '音频到强度（A2I）模型：通过多模态师生知识蒸馏获得，将音频和面部速度特征转化为逐帧强度序列，实现音视频运动的联合建模，并支持细粒度的运动动态控制。', '扩散噪声初始化策略（IC-Init）：在推理时通过对背景连贯性和运动连续性施加显式约束，实现更好的身份保留和运动动态优化。']

Result: ConsistTalk在减少闪烁、保持身份和生成时间稳定、高保真口型动画视频方面，显著优于现有方法。

Conclusion: ConsistTalk通过其创新的OFT、A2I模型和IC-Init策略，有效解决了音频驱动人像动画中的关键挑战，实现了更高质量和更一致的生成效果。

Abstract: Recent advancements in video diffusion models have significantly enhanced audio-driven portrait animation. However, current methods still suffer from flickering, identity drift, and poor audio-visual synchronization. These issues primarily stem from entangled appearance-motion representations and unstable inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel intensity-controllable and temporally consistent talking head generation framework with diffusion noise search inference. First, we propose \textbf{an optical flow-guided temporal module (OFT)} that decouples motion features from static appearance by leveraging facial optical flow, thereby reducing visual flicker and improving temporal consistency. Second, we present an \textbf{Audio-to-Intensity (A2I) model} obtained through multimodal teacher-student knowledge distillation. By transforming audio and facial velocity features into a frame-wise intensity sequence, the A2I model enables joint modeling of audio and visual motion, resulting in more natural dynamics. This further enables fine-grained, frame-wise control of motion dynamics while maintaining tight audio-visual synchronization. Third, we introduce a \textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing explicit constraints on background coherence and motion continuity during inference-time noise search, we achieve better identity preservation and refine motion dynamics compared to the current autoregressive strategy. Extensive experiments demonstrate that ConsistTalk significantly outperforms prior methods in reducing flicker, preserving identity, and delivering temporally stable, high-fidelity talking head videos.

</details>


### [278] [DIMO: Diverse 3D Motion Generation for Arbitrary Objects](https://arxiv.org/abs/2511.07409)
*Linzhan Mou,Jiahui Lei,Chen Wang,Lingjie Liu,Kostas Daniilidis*

Main category: cs.CV

TL;DR: DIMO是一种生成式方法，能够从单张图像为任意物体生成多样化的3D运动。它利用预训练视频模型的丰富先验知识来提取通用运动模式，并将其嵌入共享的低维潜在空间，通过神经关键点轨迹驱动3D高斯模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决从单张图像为任意物体生成多样化3D运动的挑战，并希望利用预训练视频模型中丰富的运动先验知识来提取和利用这些通用运动模式。

Method: DIMO首先生成同一物体具有多样化运动的多个视频，然后将每种运动嵌入到潜在向量中。接着，训练一个共享的运动解码器，学习由神经关键点轨迹表示的结构化紧凑运动分布。最后，利用这些关键点驱动规范的3D高斯模型来建模几何和外观。推理时，可在学习到的潜在空间中通过一次前向传播即时采样多样化的3D运动。

Result: 该方法能够从单张图像为任意物体生成多样化的3D运动，并支持3D运动插值和语言引导运动生成等多种应用。在推理时，可以实现多样化3D运动的即时采样。

Conclusion: DIMO提出了一种新颖的生成方法，通过利用预训练视频模型的先验知识和学习结构化的运动表示，实现了从单张图像生成任意物体的多样化3D运动，并展现了在多种应用中的潜力。

Abstract: We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.

</details>


### [279] [HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and End-to-end Autonomous Driving](https://arxiv.org/abs/2511.07106)
*Zhongyu Xia,Zhiwei Lin,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 该论文提出了HENet和HENet++框架，用于多任务3D感知和端到端自动驾驶，通过混合图像编码和同时提取密集/稀疏特征，解决了计算资源限制和不同任务对特征表示的需求，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中，3D特征提取是关键。然而，大型图像编码器、高分辨率图像和长期时序输入虽然能提升特征质量，但由于计算资源限制，在训练和推理中往往不兼容。此外，不同任务偏好不同的特征表示，使得单一模型难以在保持高精度的同时执行多任务端到端推理。

Method: 本文提出了HENet和HENet++框架。核心方法包括：1) 混合图像编码网络，对短期帧使用大型图像编码器，对长期帧使用小型图像编码器；2) 同时提取密集和稀疏特征，为不同任务提供更合适的表示，减少累积误差，并向规划模块提供更全面的信息。该架构还兼容现有3D特征提取方法并支持多模态输入。

Result: HENet++在nuScenes基准测试中实现了最先进的端到端多任务3D感知结果，并在nuScenes端到端自动驾驶基准测试中取得了最低的碰撞率。

Conclusion: HENet和HENet++框架通过创新的混合图像编码和多尺度特征提取策略，有效克服了自动驾驶中多任务3D感知面临的计算资源和特征表示挑战，显著提升了性能和安全性，为端到端自动驾驶提供了强大的解决方案。

Abstract: Three-dimensional feature extraction is a critical component of autonomous driving systems, where perception tasks such as 3D object detection, bird's-eye-view (BEV) semantic segmentation, and occupancy prediction serve as important constraints on 3D features. While large image encoders, high-resolution images, and long-term temporal inputs can significantly enhance feature quality and deliver remarkable performance gains, these techniques are often incompatible in both training and inference due to computational resource constraints. Moreover, different tasks favor distinct feature representations, making it difficult for a single model to perform end-to-end inference across multiple tasks while maintaining accuracy comparable to that of single-task models. To alleviate these issues, we present the HENet and HENet++ framework for multi-task 3D perception and end-to-end autonomous driving. Specifically, we propose a hybrid image encoding network that uses a large image encoder for short-term frames and a small one for long-term frames. Furthermore, our framework simultaneously extracts both dense and sparse features, providing more suitable representations for different tasks, reducing cumulative errors, and delivering more comprehensive information to the planning module. The proposed architecture maintains compatibility with various existing 3D feature extraction methods and supports multimodal inputs. HENet++ achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, while also attaining the lowest collision rate on the nuScenes end-to-end autonomous driving benchmark.

</details>


### [280] [YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting](https://arxiv.org/abs/2511.07321)
*Botao Ye,Boqi Chen,Haofei Xu,Daniel Barath,Marc Pollefeys*

Main category: cs.CV

TL;DR: YoNoSplat是一个快速灵活的前馈模型，能从任意数量的非结构化图像重建高质量的3D高斯泼溅表示，支持有姿态/无姿态和已标定/未标定输入，并引入了新颖的训练策略和尺度模糊解决方案。


<details>
  <summary>Details</summary>
Motivation: 从非结构化图像集合中进行快速灵活的3D场景重建仍然是一个重大挑战。

Method: YoNoSplat是一个前馈模型，为每个视图预测局部高斯和相机姿态，并使用预测或提供的姿态将其聚合为全局表示。它引入了一种新颖的混合训练策略，通过从使用真实姿态聚合局部高斯逐渐过渡到混合预测和真实姿态，解决了联合学习3D高斯和相机参数的难题。此外，它通过新颖的成对相机距离归一化方案和将相机内参嵌入网络来解决尺度模糊问题，并能预测内参以处理未标定输入。

Result: YoNoSplat在NVIDIA GH200 GPU上仅需2.69秒即可从100个视图（280x518分辨率）重建场景。它在标准基准测试中，无论是在无姿态还是有姿态依赖的设置下，都达到了最先进的性能。

Conclusion: YoNoSplat为从多样化的非结构化图像输入进行3D高斯泼溅重建提供了一个高效且高质量的解决方案，有效解决了姿态估计、尺度模糊和未标定输入等挑战。

Abstract: Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [281] [Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation](https://arxiv.org/abs/2511.05516)
*Canxiang Yan,Chunxiang Jin,Dawei Huang,Haibing Yu,Han Peng,Hui Zhan,Jie Gao,Jing Peng,Jingdong Chen,Jun Zhou,Kaimeng Ren,Ming Yang,Mingxue Yang,Qiang Xu,Qin Zhao,Ruijie Xiong,Shaoxiong Lin,Xuezhi Wang,Yi Yuan,Yifei Wu,Yongjie Lyu,Zhengyu He,Zhihao Qiu,Zhiqiang Fang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 该研究引入了一个统一的框架，包括一个新颖的连续语音分词器MingTok-Audio和一系列模型（Ming-UniAudio、Ming-UniAudio-Edit），旨在解决现有语音模型在理解和生成任务中表示冲突的问题，并首次实现了基于自然语言指令的通用自由形式语音编辑。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在理解和生成任务中对token表示有相互竞争的要求，导致表示不一致，从而阻碍了语音语言模型进行基于指令的自由形式编辑。

Method: 核心是一个统一的连续语音分词器MingTok-Audio，它首次有效整合了语义和声学特征，适用于理解和生成任务。在此基础上，开发了语音语言模型Ming-UniAudio，平衡了生成和理解能力。进一步训练了专用的语音编辑模型Ming-UniAudio-Edit，实现了无需时间戳的语义和声学修改。为了评估编辑能力，还引入了Ming-Freeform-Audio-Edit基准，涵盖语义正确性、声学质量和指令对齐。

Result: Ming-UniAudio在ContextASR基准测试的12项指标中，有8项达到了新的SOTA。在中文语音克隆方面，实现了0.95的Seed-TTS-WER。Ming-UniAudio-Edit是第一个支持通用、自由形式语音编辑的语音语言模型，仅通过自然语言指令即可进行语义和声学修改。研究还开源了连续音频分词器、统一基础模型和自由形式指令编辑模型。

Conclusion: 该研究通过引入统一的连续语音分词器和系列模型，成功解决了语音理解和生成任务中的表示冲突，并首次实现了基于自然语言指令的通用自由形式语音编辑。所提出的框架和开放资源将促进统一音频理解、生成和操作的发展。

Abstract: Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.

</details>


### [282] [Retracing the Past: LLMs Emit Training Data When They Get Lost](https://arxiv.org/abs/2511.05518)
*Myeongseob Ko,Nikhil Reddy Billa,Adam Nguyen,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.CL

TL;DR: 本文提出了一种名为“困惑诱导攻击”（CIA）的新框架，通过系统地最大化模型不确定性来提取大型语言模型（LLM）中记忆的训练数据，并优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对训练数据的记忆引发了严重的隐私和版权问题。现有的数据提取方法（特别是基于启发式的分歧攻击）成功率有限，且未能深入理解记忆泄露的根本驱动因素。

Method: 本文引入了“困惑诱导攻击”（CIA）框架，通过系统地最大化模型不确定性来提取记忆数据。研究发现，记忆文本的泄露前会出现持续的token级预测熵峰值，CIA利用这一发现，优化输入片段以故意诱导这种高熵状态。对于对齐的LLM，进一步提出“不匹配监督微调”（Mismatched SFT）来同时削弱其对齐并诱导目标困惑，从而增加攻击的易感性。

Result: 在各种未对齐和已对齐的LLM上的实验表明，本文提出的攻击在提取逐字和近似逐字训练数据方面优于现有基线方法，且无需事先了解训练数据。

Conclusion: 研究结果强调了各种LLM中持续存在的记忆风险，并提供了一种更系统的方法来评估这些漏洞。

Abstract: The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.

</details>


### [283] [Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning](https://arxiv.org/abs/2511.05532)
*Rufan Zhang,Lin Zhang,Xianghang Mi*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文学习（ICL）和基础模型的新框架，用于统一检测有害在线内容（如毒性、垃圾邮件、负面情绪），并实现轻量级个性化内容审核，无需模型再训练，适用于隐私敏感和去中心化环境。


<details>
  <summary>Details</summary>
Motivation: 现有的内容审核系统是中心化、任务特定且缺乏透明度的，未能充分考虑用户偏好，这在隐私敏感或去中心化环境中表现不佳。有害在线内容的激增（如毒性、垃圾邮件和负面情绪）要求更强大和适应性更强的审核系统。

Method: 该研究利用基础模型结合上下文学习（ICL）来统一检测二元、多类别和多标签设置下的毒性、垃圾邮件和负面情绪。关键在于通过简单的基于提示的干预实现轻量级个性化，允许用户在不重新训练模型的情况下，轻松屏蔽新类别、取消屏蔽现有类别或扩展检测到语义变体。

Result: 通过在公共基准（TextDetox, UCI SMS, SST2）和一个新的Mastodon数据集上进行广泛实验，结果显示：(i) 基础模型实现了强大的跨任务泛化能力，通常与任务特定的微调模型相媲美或超越；(ii) 只需一个用户提供的示例或定义即可实现有效的个性化；(iii) 用标签定义或理由增强提示显著增强了对嘈杂真实世界数据的鲁棒性。

Conclusion: 该工作表明，通过上下文学习（ICL）可以实现超越“一刀切”的审核方法，为下一代以用户为中心的内容安全系统提供了一条实用、保护隐私且高度适应性的途径。

Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.

</details>


### [284] [MCP4IFC: IFC-Based Building Design Using Large Language Models](https://arxiv.org/abs/2511.05533)
*Bharathi Kannan Nithyanantham,Tobias Sesterhenn,Ashwin Nedungadi,Sergio Peral Garijo,Janis Zenkner,Christian Bartelt,Stefan Lüdtke*

Main category: cs.CL

TL;DR: MCP4IFC是一个开源框架，它使大型语言模型（LLMs）能够通过模型上下文协议（MCP）直接操作行业基础类（IFC）数据，从而将生成式AI引入建筑、工程和施工（AEC）领域。


<details>
  <summary>Details</summary>
Motivation: 将生成式AI引入AEC领域需要系统能够将自然语言指令转换为对标准化数据模型（如IFC）的操作。

Method: MCP4IFC框架通过模型上下文协议（MCP）使LLMs能够直接操作IFC数据。它提供了一套BIM工具，包括用于信息检索的场景查询工具、用于创建和修改常见建筑元素的预定义函数，以及一个结合了上下文学习和检索增强生成（RAG）的动态代码生成系统，以处理超出预定义工具集的任务。

Result: 实验证明，使用该框架的LLM能够成功执行复杂任务，从构建简单房屋到查询和编辑现有IFC数据。

Conclusion: MCP4IFC框架为LLM驱动的BIM设计提供了基础，并为AI辅助建模工作流程奠定了基础，已开源以鼓励进一步研究。

Abstract: Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.

</details>


### [285] [Future of AI Models: A Computational perspective on Model collapse](https://arxiv.org/abs/2511.05535)
*Trivikram Satharasi,S Sitharama Iyengar*

Main category: cs.CL

TL;DR: 随着人工智能生成内容在网络上占据主导地位，本研究通过分析维基百科的语义相似度，量化并预测了由于递归训练导致的“模型崩溃”的发生，发现大型语言模型普及后相似度呈指数级增长，对数据丰富性和模型泛化能力构成威胁。


<details>
  <summary>Details</summary>
Motivation: 人工智能，特别是大型语言模型（LLMs）和扩散模型，已广泛应用于多个领域，导致大量AI生成内容充斥网络。这种合成内容的主导地位以及将其用于递归训练的风险，可能导致语言和语义多样性丧失，从而引发“模型崩溃”。本研究旨在量化并预测这种崩溃的发生时间。

Method: 本研究通过检查2013年至2025年间英文维基百科（过滤后的Common Crawl数据）的年度语义相似度来量化和预测崩溃的发生。方法包括使用Transformer嵌入和余弦相似度指标进行分析。

Result: 研究结果显示，在大型语言模型普及之前，语义相似度呈稳定上升趋势，这可能归因于早期的RNN/LSTM翻译和文本规范化管道。在大型语言模型公开普及后，相似度呈现指数级增长。观察到的波动反映了不可约的语言多样性、不同年份语料库规模的变化以及有限的抽样误差。

Conclusion: 这些发现提供了一个数据驱动的估计，表明递归式AI污染何时可能显著威胁到数据的丰富性和模型的泛化能力。

Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.

</details>


### [286] [FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference](https://arxiv.org/abs/2511.05534)
*Kunxi Li,Yufan Xiong,Zhonghua Jiang,Yiyun Zhou,Zhaode Wang,Chengfei Lv,Shengyu Zhang*

Main category: cs.CL

TL;DR: FlowMM是一种自适应框架，通过利用跨模态信息流和敏感度自适应令牌匹配机制，优化多模态KV缓存合并，显著减少内存并降低解码延迟。


<details>
  <summary>Details</summary>
Motivation: 传统的KV缓存驱逐策略会降低生成质量，导致上下文丢失或幻觉。现有的KV合并方法在多模态场景中受限于模态间分布偏差和交叉模态交互中的注意力偏差，效果不佳。

Method: 本文提出了FlowMM框架，它利用跨模态信息流动态应用层特定的合并策略，以捕捉模态特定模式并保持上下文完整性。此外，引入了一种敏感度自适应的令牌匹配机制，该机制联合评估令牌相似性和任务关键敏感度，合并低风险令牌同时保护高敏感度令牌。

Result: FlowMM在多种主流多模态大语言模型上进行了广泛实验，结果显示KV缓存内存减少了80%至95%，解码延迟降低了1.3-1.8倍，同时保持了具有竞争力的任务性能。

Conclusion: FlowMM通过其创新的跨模态信息流引导合并和敏感度自适应匹配机制，有效解决了多模态KV缓存优化的挑战，显著提升了效率和性能。

Abstract: Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.

</details>


### [287] [Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements](https://arxiv.org/abs/2511.05560)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了在BabyLM 2025任务约束下，用于样本高效语言建模的架构和优化技术。模型BLaLM采用线性时间mLSTM替代自注意力，并结合滑动窗口注意力等轻量级增强。通过精心策划的高质量语料库和Muon优化器，在零样本性能和收敛稳定性上均有所提升，证明了无需依赖规模也能实现高效语言建模的策略。


<details>
  <summary>Details</summary>
Motivation: 在BabyLM 2025共享任务的约束下，研究样本高效的语言建模技术，特别是在低资源设置中，如何不依赖模型规模实现高效性能。

Method: 模型BLaLM用线性时间mLSTM令牌混合器取代了自注意力机制，并探索了轻量级增强，包括短卷积、带动态调制的滑动窗口注意力以及Hedgehog特征图。为支持低资源训练，构建了一个强调可读性和教学结构的高质量语料库。优化器方面，使用Muon优化器。

Result: 实验结果显示：1) 线性注意力与滑动窗口注意力的结合持续提升了零样本性能；2) Muon优化器相比AdamW能稳定收敛并降低困惑度。

Conclusion: 这些结果突出了在不依赖大规模的情况下，实现高效语言建模的有效策略，主要通过架构选择（线性注意力与滑动窗口注意力结合）和优化器（Muon）来实现。

Abstract: We study architectural and optimization techniques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM token mixer and explores lightweight enhancements, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support training in low-resource settings, we curate a high-quality corpus emphasizing readability and pedagogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding window attention consistently improves zero-shot performance, and (2) the Muon optimizer stabilizes convergence and reduces perplexity over AdamW. These results highlight effective strategies for efficient language modeling without relying on scale.

</details>


### [288] [Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability](https://arxiv.org/abs/2511.05541)
*Usha Bhalla,Alex Oesterling,Claudio Mayrink Verdun,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.CL

TL;DR: 本文提出时间稀疏自编码器（T-SAEs），通过引入新颖的对比损失来鼓励相邻token间高层特征激活的一致性，从而在无监督情况下从句法特征中解耦出语义特征，显著提升语言模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有字典学习方法（如稀疏自编码器SAEs）在解释大型语言模型（LLMs）时，未能捕捉到丰富的概念信息，反而偏向浅层、特定token或噪声特征。研究认为这是因为当前无监督方法忽视了语言本身丰富的语法、语义和语用结构，导致特征发现偏向表面模式而非有意义的概念。

Method: 基于语义内容具有长程依赖且在序列上趋于平滑、而句法信息更局部的洞察，本文提出了时间稀疏自编码器（T-SAEs）。它在SAE中引入了一种新颖的对比损失，鼓励相邻token的高级特征激活保持一致。这种修改使得T-SAEs能够以自监督方式解耦语义和句法特征。

Result: 在多个数据集和模型上，T-SAEs在不牺牲重建质量的前提下，恢复了更平滑、更连贯的语义概念。尽管在训练中没有明确的语义信号，T-SAEs仍表现出清晰的语义结构，并能将语义特征与句法特征解耦。

Conclusion: T-SAEs通过其简单的修改，成功地在无监督情况下实现了语义特征的解耦和更连贯的语义概念发现。这为语言模型中的无监督可解释性提供了一条新途径，揭示了模型内部的语义结构。

Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.

</details>


### [289] [UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8](https://arxiv.org/abs/2511.05578)
*Preston Firestone,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.CL

TL;DR: 本文利用幺半群理论形式化了子词分词，并证明了包含非UTF-8词元的字节级分词器总是会生成非UTF-8序列，这导致了实际应用中的错误，并评估了缓解措施。


<details>
  <summary>Details</summary>
Motivation: 语言模型中常用的字节级子词分词器，虽然能有效避免词汇表外错误，但其词元序列不保证是有效的UTF-8编码。这会破坏依赖于有效UTF-8输入的下游代码，因此需要研究和解决这种潜在的系统性问题。

Method: 研究方法包括：使用幺半群理论对分词过程进行形式化；数学证明，证明包含畸形UTF-8词元的分词器能够生成畸形UTF-8序列；形式化证明了增量转换词元与整体转换序列结果的不同；通过案例研究评估了主流基础模型、服务引擎和受限生成系统中的缓解措施。

Result: 研究结果表明：包含畸形UTF-8词元的分词器总是能够生成畸形UTF-8序列；将词元增量转换回字符串并解释为UTF-8，与一次性转换整个词元序列的结果不同；这些形式化结果预示了实际存在的错误；论文还评估了针对已识别问题的缓解措施，并提供了主要基础模型、服务引擎和受限生成系统的案例研究。

Conclusion: 字节级子词分词器生成畸形UTF-8序列是一个普遍存在且具有理论依据的问题，会导致实际应用中的错误。应用程序必须考虑这种破坏性，并且需要有效的缓解策略来处理或避免这些问题。

Abstract: Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.

</details>


### [290] [Optimizing Diversity and Quality through Base-Aligned Model Collaboration](https://arxiv.org/abs/2511.05650)
*Yichen Wang,Chenghao Yang,Tenghao Huang,Muhao Chen,Jonathan May,Mina Lee*

Main category: cs.CL

TL;DR: 本文提出BACo框架，通过在推理时动态结合基础LLM及其对齐版本，以在不牺牲质量的前提下显著提升生成文本的多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的对齐虽然提高了输出质量，但导致生成结果多样性不足，输出高度相似。现有提升多样性的方法常以牺牲质量为代价或成本高昂。

Method: BACo是一种推理时基于token级别的模型协作框架。它动态地将一个基础LLM与其对齐模型结合，并采用路由策略，根据下一个token预测的不确定性以及预测内容的语义角色，在每个token处决定从哪个模型进行解码。

Result: BACo在三个开放式生成任务和13个多样性与质量指标上，持续超越现有最先进的推理时基线。最佳路由策略使多样性和质量联合提升21.3%。人类评估也证实了这些改进。

Conclusion: 基础模型和对齐模型之间的协作可以有效优化和控制生成文本的多样性和质量，为解决LLM多样性下降问题提供了一个有效方案。

Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.

</details>


### [291] [OckBench: Measuring the Efficiency of LLM Reasoning](https://arxiv.org/abs/2511.05722)
*Zheng Du,Hao Kang,Song Han,Tushar Krishna,Ligeng Zhu*

Main category: cs.CL

TL;DR: 本文介绍了OckBench，一个用于评估大型语言模型在推理和编码任务中准确性和解码token效率的基准，揭示了模型在token消耗上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注准确性和输出质量，而忽略了解码token效率。在实际系统中，生成token的数量对延迟、成本和能耗有巨大影响。

Method: 研究引入了OckBench，这是一个与模型和硬件无关的基准，用于同时评估推理和编码任务的准确性和token计数。通过比较多个开源和闭源模型进行了实验。

Result: 实验发现，许多准确性相当的模型在token消耗上存在巨大差异，表明效率是一个被忽视但重要的区分维度。研究还展示了准确性-效率平面上的帕累托前沿。

Conclusion: 研究认为应该改变评估范式，不应将token视为“免费”的资源，token效率至关重要。OckBench提供了一个统一平台来衡量、比较和指导token高效推理的研究。

Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .

</details>


### [292] [In-Context Learning Without Copying](https://arxiv.org/abs/2511.05743)
*Kerem Sahin,Sheridan Feucht,Adam Belfki,Jannik Brinkmann,Aaron Mueller,David Bau,Chris Wendler*

Main category: cs.CL

TL;DR: 研究表明，抑制归纳复制（通过归纳头实现）并不会阻碍Transformer模型获得抽象语境学习能力，甚至可能在某些任务上表现更好，这挑战了归纳复制是语境学习先决条件的观点。


<details>
  <summary>Details</summary>
Motivation: 归纳头（执行归纳复制）被认为是模型获得复杂语境学习（ICL）能力的前提，因为它们出现时训练损失会急剧下降。本研究旨在探讨在抑制归纳复制的情况下，Transformer模型是否仍能获得ICL能力。

Method: 提出“Hapax”设置，即忽略那些可以被归纳头正确预测的token的损失贡献。然后，比较在这种设置下训练的模型与普通模型在抽象ICL任务（答案不在输入语境中）上的表现。同时，进行机制分析以检查归纳头的发展情况。

Result: 尽管归纳复制显著减少（31.7%的token损失被忽略），但模型在抽象ICL任务上的表现仍具可比性，甚至在21个任务中的13个上超越了普通模型。此外，模型在不能被归纳头正确预测的token位置上实现了更低的损失值。机制分析显示，Hapax训练的模型形成了更少、更弱的归纳头，但仍保留了ICL能力。

Conclusion: 研究结果表明，归纳复制对于学习抽象语境学习机制并非必不可少。

Abstract: Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.

</details>


### [293] [Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models](https://arxiv.org/abs/2511.05752)
*Xiangchen Song,Yulin Huang,Jinxu Guo,Yuchen Liu,Yaxuan Luan*

Main category: cs.CL

TL;DR: 本研究提出一种混合文本分类方法，结合大型语言模型进行深度特征提取、特征金字塔进行多尺度融合以及图神经网络进行结构化建模，以提升复杂语义环境下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 在复杂语义环境中，现有文本分类模型可能难以充分捕捉文本的深层语义和结构化关系。本研究旨在通过集成深度特征、多尺度融合和结构化建模来提升文本分类的性能。

Method: 首先，利用大型语言模型提取文本的上下文依赖和深层语义特征。其次，通过特征金字塔机制对多层级特征进行融合，平衡全局信息和局部细节，构建分层语义表达。然后，将融合后的特征转换为图表示，并使用图神经网络捕捉文本中潜在的语义关系和逻辑依赖。最后，通过读出和分类模块生成最终的类别预测。

Result: 该方法在鲁棒性对齐实验中表现出显著优势，在ACC、F1-Score、AUC和Precision等指标上均优于现有模型，验证了框架的有效性和稳定性。

Conclusion: 本研究构建了一个平衡全局与局部信息、语义与结构的集成框架，并为文本分类任务中的多尺度特征融合和结构化语义建模提供了新视角。

Abstract: This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.

</details>


### [294] [Language Generation: Complexity Barriers and Implications for Learning](https://arxiv.org/abs/2511.05759)
*Marcelo Arenas,Pablo Barceló,Luis Cofré,Alexander Kozachinskiy*

Main category: cs.CL

TL;DR: 本文揭示，即使对于简单的语言家族（如正则语言和上下文无关语言），成功进行语言生成所需的示例数量可能极其庞大，甚至无法计算，这凸显了理论可能性与实际可学习性之间的巨大鸿沟。


<details>
  <summary>Details</summary>
Motivation: Kleinberg和Mullainathan的研究表明语言生成在理论上是可能的，但其在实践中的可行性仍是一个未解的问题，尤其是在所需示例数量方面。

Method: 通过对正则语言和上下文无关语言等简单且经过充分研究的语言家族进行理论分析，考察了成功生成所需的示例数量。

Result: 对于这些简单的语言家族，成功生成所需的示例数量可能非常巨大，在某些情况下甚至不受任何可计算函数的限制。

Conclusion: 理论上的可能性与实际高效的可学习性之间存在显著差距。解释现代语言模型经验成功的关键在于考虑自然语言的结构特性，这些特性使其在实践中能够有效生成。

Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.

</details>


### [295] [DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning](https://arxiv.org/abs/2511.05784)
*Yaxuan Wang,Chris Yuhao Liu,Quan Liu,Jinglong Pang,Wei Wei,Yujia Bao,Yang Liu*

Main category: cs.CL

TL;DR: DRAGON是一种基于推理的框架，通过上下文思维链指令和轻量级检测模块，在不访问保留数据的情况下，实现大型语言模型的遗忘，以保护隐私和删除有害知识。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型遗忘方法通常依赖于保留数据进行微调，但在实际场景中这些数据往往不可用，限制了其实用性。

Method: DRAGON提出一个系统性的、基于推理的框架，利用上下文思维链（CoT）指令在推理前保护部署的LLM。它引入了一个轻量级检测模块，无需保留数据即可识别应遗忘的提示，然后将这些提示通过专门的CoT防护模型进行路由，以执行安全准确的上下文干预。该方法利用LLM固有的指令遵循能力，而不修改基础模型。同时，引入了新的评估指标来衡量遗忘性能和持续遗忘设置。

Result: 在三个代表性遗忘任务上的大量实验验证了DRAGON的有效性，展示了其强大的遗忘能力、可扩展性以及在实际场景中的适用性。

Conclusion: DRAGON通过利用LLM的指令遵循能力和上下文推理，成功克服了数据受限场景下LLM遗忘的挑战，提供了一种实用且高效的解决方案，用于保护数据和移除有害知识。

Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.

</details>


### [296] [Quantifying Edits Decay in Fine-tuned LLMs](https://arxiv.org/abs/2511.05852)
*Yinjie Cheng,Paul Youssef,Christin Seifert,Jörg Schlötterer,Zhixue Zhao*

Main category: cs.CL

TL;DR: 本研究系统性地量化了微调后知识编辑的衰减情况，发现编辑会衰减，并提出了选择性层微调策略来有效移除编辑。


<details>
  <summary>Details</summary>
Motivation: 知识编辑和微调是LLM后训练的两种常见干预手段，但它们之间的相互作用尚未被研究。了解微调是否会影响编辑的存活至关重要，这关系到重复编辑的成本（如果编辑消失）和恶意编辑的传播（如果编辑持续存在）。

Method: 研究评估了两种先进的编辑方法（MEMIT、AlphaEdit）和三种微调方法（全参数、LoRA、DoRA），涉及五种LLM和三个数据集，共232种实验配置。此外，还提出了选择性层微调策略。

Result: 实验结果表明，微调后知识编辑会衰减，且衰减程度因配置而异（例如，AlphaEdit的编辑衰减比MEMIT更显著）。研究发现，仅微调被编辑的层可以有效移除编辑，但对下游性能略有影响。令人惊讶的是，微调未被编辑的层比全参数微调更能损害编辑的存活。

Conclusion: 本研究为知识编辑与微调的整合提供了实证基线和可行策略，并强调在评估模型编辑时需要考虑完整的LLM应用流程。

Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.

</details>


### [297] [Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations](https://arxiv.org/abs/2511.05901)
*Rui Yang,Matthew Yu Heng Wong,Huitao Li,Xin Li,Wentao Zhu,Jingchi Liao,Kunyu Yu,Jonathan Chong Kai Liew,Weihao Xuan,Yingjian Chen,Yuhe Ke,Jasmine Chiat Ling Ong,Douglas Teodoro,Chuan Hong,Daniel Shi Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 该研究综述了医学领域检索增强生成（RAG）技术的应用现状，指出其仍处于早期阶段，并强调了未来在临床验证、跨语言适应和低资源环境支持方面的需求。


<details>
  <summary>Details</summary>
Motivation: 医学知识的快速增长和临床实践的日益复杂性带来了挑战。尽管大型语言模型（LLMs）已展现价值，但其固有限制依然存在，促使研究人员探索RAG技术以增强其临床适用性。

Method: 本研究通过对医学领域RAG应用的文献综述进行。

Result: 研究发现，RAG应用主要依赖公开数据，私有数据应用有限；检索方法常使用以英语为中心的嵌入模型；LLMs多为通用模型，医学专用LLMs使用较少；评估侧重生成质量和任务性能的自动化指标，以及准确性、完整性、相关性和流畅度的人工评估，但对偏见和安全性关注不足；应用主要集中在问答、报告生成、文本摘要和信息提取。

Conclusion: 医学RAG仍处于早期阶段，需要进一步在临床验证、跨语言适应和低资源环境支持方面取得进展，以实现其在全球范围内的可信赖和负责任的应用。

Abstract: The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.

</details>


### [298] [IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction](https://arxiv.org/abs/2511.05921)
*Ankan Mullick,Sukannya Purkayastha,Saransh Sharma,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 本文提出IDALC，一个半监督框架，旨在通过主动学习纠正语音控制对话系统拒绝的语句并检测用户意图，从而显著降低人工标注成本并提高性能。


<details>
  <summary>Details</summary>
Motivation: 语音控制对话系统在低置信度情况下会拒绝已知意图的语句，并且需要随着时间推移用新意图重新训练，这导致高昂的人工标注成本。需要一种高效机制来减少标注开销。

Method: IDALC（意图检测和基于主动学习的纠正）是一个半监督框架。它利用主动学习来检测用户意图并纠正系统拒绝的语句，以最小化人工标注的需求。

Result: 在多个基准数据集上，IDALC系统比基线方法准确率提高5-10%，宏观F1分数提高4-8%。同时，它将总标注成本维持在系统可用未标注数据的6-10%。

Conclusion: IDALC提供了一个有效的半监督解决方案，用于处理语音控制对话系统中的意图检测和被拒绝语句的纠正问题，显著提高了性能并大幅降低了标注成本。

Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1

</details>


### [299] [NILC: Discovering New Intents with LLM-assisted Clustering](https://arxiv.org/abs/2511.05913)
*Hongtao Wang,Renchi Yang,Wenqing Lin*

Main category: cs.CL

TL;DR: 本文提出NILC，一种新颖的聚类框架，专为有效的新意图发现（NID）设计。它采用迭代工作流，利用大型语言模型（LLMs）精炼聚类中心和不确定语句的文本嵌入，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有新意图发现（NID）方法主要采用级联架构，先编码再聚类（如K-Means），这种方式未能实现两阶段的相互反馈优化，且仅基于嵌入的聚类忽视了细微的文本语义，导致性能不佳。

Method: NILC是一个迭代聚类框架。它通过以下方式更新聚类分配：1. 利用LLMs创建额外的语义聚类中心，丰富嵌入的欧几里得中心。2. 利用LLMs通过重写来增强聚类中识别出的难样本（模糊或简洁的语句），以进行聚类修正。3. 在半监督设置下，通过种子技术和软链接注入监督信号，实现更准确的NID。

Result: 在无监督和半监督设置下，NILC与多个最新基线在六个不同领域的基准数据集上进行了广泛实验比较，结果表明NILC持续取得显著的性能提升。

Conclusion: NILC是一个专门为NID设计的有效聚类框架，通过迭代工作流和LLMs的辅助，成功解决了现有方法中缺乏相互优化和忽视细微文本语义的问题，并在多样化的数据集上展现出卓越且一致的性能改进。

Abstract: New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.

</details>


### [300] [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://arxiv.org/abs/2511.05933)
*Renfei Zhang,Manasa Kaniselvan,Niloofar Mireshghallah*

Main category: cs.CL

TL;DR: 研究挑战了强化学习(RL)损害语言模型记忆知识的观点，发现RL增强模型在知识召回任务上表现更好，特别是分层知识。这归因于RL提升了模型导航现有知识的程序性技能，而非获取新知识。结构化提示可显著缩小监督微调(SFT)模型与RL模型之间的性能差距，且RL主要改变了模型遍历知识的方式，而非知识本身的表示。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为RL会损害语言模型的记忆知识。本研究通过观察到RL增强模型在纯知识召回任务上表现优异，特别是涉及分层、结构化知识的任务，从而挑战了这一叙述。

Method: 1. 比较RL增强模型与基线/SFT模型在知识召回任务（如医疗编码）上的表现。2. 对SFT模型应用结构化提示，引导其进行分层遍历，以评估其性能。3. 分析模型召回正确程序路径的能力。4. 进行层级内部激活分析，比较SFT和RL模型中事实表示和查询表示的余弦相似度。

Result: 1. RL增强模型在纯知识召回任务上持续优于基线和SFT模型，尤其是在需要遍历分层、结构化知识的任务上。2. 结构化提示能使SFT模型恢复大部分性能差距（在MedConceptsQA上将24pp降至7pp）。3. RL增强模型在深度检索任务中保留了召回正确程序路径的卓越能力。4. 事实表示（如“代码57.95指尿路感染”）在SFT和RL模型之间保持高度余弦相似性，而查询表示（如“代码57.95是什么”）则显著不同，表明RL主要改变了模型遍历知识的方式，而非知识表示本身。

Conclusion: RL通过提升模型在现有知识层次结构中导航和搜索的程序性技能来改进知识召回，而不是通过获取新数据或损害现有记忆。结构化提示的有效性以及激活分析结果支持了这一假设，即RL主要转换了模型处理查询的方式，而非事实知识的表示。

Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.

</details>


### [301] [Revisiting Entropy in Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2511.05993)
*Renren Jin,Pengzhi Gao,Yuqi Ren,Zhuowen Han,Tongxuan Zhang,Wuwei Huang,Wei Liu,Jian Luan,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文研究了强化学习与可验证奖励（RLVR）训练中大型语言模型（LLM）的熵衰减问题，分析了熵与响应多样性、校准和性能的关系，并发现正优势令牌是熵衰减的主要原因，通过调整正负优势令牌的损失权重可以有效调节模型熵。


<details>
  <summary>Details</summary>
Motivation: RLVR是增强LLM推理能力的主流方法，但训练过程中LLM的熵通常会衰减，导致过早收敛到次优局部最小值并阻碍性能提升。尽管已有缓解方法，但对RLVR中熵的全面研究仍然缺乏。

Method: 研究人员进行了广泛的实验，调查了RLVR训练中LLM的熵动态，分析了模型熵与响应多样性、校准和各种基准测试性能之间的相关性。此外，他们还从理论和经验上进行了论证。

Result: 研究发现，离策略更新次数、训练数据多样性和优化目标中的裁剪阈值是影响RLVR训练LLM熵的关键因素。理论和经验证据表明，具有正优势的令牌是熵衰减的主要贡献者。通过调整训练中具有正负优势令牌的相对损失权重，可以有效调节模型熵。

Conclusion: LLM在RLVR训练中发生的熵衰减主要由正优势令牌引起，并且可以通过在训练期间调整具有正负优势令牌的相对损失权重来有效调节模型熵，从而可能缓解次优收敛问题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.

</details>


### [302] [Interpretable Recognition of Cognitive Distortions in Natural Language Texts](https://arxiv.org/abs/2511.05969)
*Anton Kolonin,Anna Arinicheva*

Main category: cs.CL

TL;DR: 该研究提出了一种基于加权结构化N-gram模式的多因素文本分类新方法，并考虑了模式间的异层关系，成功应用于心理护理中认知扭曲的自动化检测，显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决心理护理中特定认知扭曲的自动化检测问题，并构建一个可解释、鲁棒且透明的人工智能模型，以产生社会影响。

Method: 采用基于加权结构化模式（如N-gram）的多因素自然语言文本分类方法，并考虑这些模式之间的异层关系。

Result: 在两个公开数据集上，该方法在任务的F1分数上取得了显著优于现有文献已知结果的提升，并确定了最优超参数。代码和模型已公开可用。

Conclusion: 所提出的识别和学习算法改进了该领域的现有技术水平，为认知扭曲检测提供了一个可解释、鲁棒和透明的AI模型。

Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.

</details>


### [303] [LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis](https://arxiv.org/abs/2511.06000)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型在生物医学摘要中保留年龄相关信息的能力，发现不同模型和年龄组之间存在系统性差异，尤其在成人摘要中保真度最低，且弱势群体更容易出现幻觉。


<details>
  <summary>Details</summary>
Motivation: 临床干预措施常取决于年龄，但语言模型在生物医学证据合成中日益普及，其是否能保留关键的人口统计学（如年龄）区别仍不确定。本研究旨在解决这一空白。

Method: 构建了一个新的年龄分层数据集DemogSummary，涵盖儿童、成人和老年人群的系统性综述原始研究。评估了Qwen、Longformer和GPT-4.1 Nano三个主流摘要生成LLM，使用了标准指标和新提出的“人口统计学显著性评分”（DSS）来量化年龄相关实体的保留和幻觉情况。

Result: 结果显示，模型和年龄组之间存在系统性差异：以成人为重点的摘要的人口统计学保真度最低，且弱势群体更容易出现幻觉。

Conclusion: 这些发现揭示了当前LLM在忠实且无偏见的摘要生成方面的局限性，并强调了在生物医学自然语言处理中需要公平性感知的评估框架和摘要管道。

Abstract: Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.

</details>


### [304] [Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data](https://arxiv.org/abs/2511.06023)
*Deng Yixuan,Ji Xiaoqiang*

Main category: cs.CL

TL;DR: 本研究提出了一种多奖励组相对策略优化（GRPO）框架，通过构建基于中文语境歧视类别的合成数据集和训练多维度奖励模型，有效地对大型语言模型进行微调，以减少偏见并实现符合伦理的无偏行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在反映社会刻板印象的隐性偏见和歧视倾向。尽管RLHF和DPO等对齐技术有所缓解，但在解决特定文化和多维度歧视方面仍有限制。

Method: 研究提出了一种多奖励组相对策略优化（GRPO）框架来微调LLMs。该方法构建了一个源自中文语境歧视类别（包括地域、民族和职业偏见）的合成英文数据集，并为每个实例配对中性和有偏见的响应。基于DeBERTa-v3训练了一个奖励模型，提供捕捉公平性、中立性和语言质量的多维度奖励信号。然后，训练好的奖励模型指导GRPO微调，以优化LLM输出在这些伦理维度上的表现。

Result: 实验结果表明，该方法显著降低了偏见强度，并提高了与非歧视标准的对齐，同时没有损害模型的流畅性和信息量。

Conclusion: 本研究强调了基于GRPO的多奖励优化在LLMs去偏见方面的有效性，并为文化语境下的伦理对齐提供了一个可复制的框架。

Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.

</details>


### [305] [Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework](https://arxiv.org/abs/2511.06051)
*Mahmoud El-Bahnasawi*

Main category: cs.CL

TL;DR: 本文提出一个高效的三层框架，结合规则预过滤和LoRA微调的BERTweet模型，实现计算效率高且性能接近SOTA大模型的仇恨言论检测，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 开发计算效率高、性能具竞争力且适合实时部署的仇恨言论检测系统是一个关键挑战。

Method: 提出一个新颖的三层框架，包括：基于规则的预过滤、参数高效的LoRA微调BERTweet模型，以及持续学习能力。通过战略性数据集统一和优化微调提升性能。

Result: 宏观F1分数达到0.85，相当于SOTA大型语言模型（如SafePhi）性能的94%，但基础模型小100倍（1.34亿 vs 140亿参数）。仅需187万可训练参数（占完全微调的1.37%），在单个T4 GPU上训练约2小时。性能优于具有相似计算需求的传统BERT方法。

Conclusion: 该系统使在资源受限环境中实现鲁棒的仇恨言论检测成为可能，同时保持了在实际部署中具有竞争力的准确性。

Abstract: This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.

</details>


### [306] [ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning](https://arxiv.org/abs/2511.06057)
*Bingbing Wang,Zhengda Jin,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: ReMoD是一个受人类双加工理论启发的框架，用于多模态立场检测，通过双重推理范式动态调整不同模态对立场表达的贡献，以避免模态组合不当导致的误解噪声。


<details>
  <summary>Details</summary>
Motivation: 现有多模态立场检测（MSD）方法简单融合各模态信息，忽视了不同模态对立场表达贡献的差异，导致在学习过程中引入立场误解噪声。

Method: ReMoD框架借鉴人类认知的双加工理论，提出了双重推理范式：
1. 经验驱动的直觉推理：查询模态经验池（MEP）和语义经验池（SEP）形成初步立场假设，优先考虑历史上有影响力的模态。
2. 审慎反思推理：通过两个推理链细化假设，调整模态偏见，提炼立场判断，并动态加权模态贡献。
   - Modality-CoT：通过自适应融合策略更新MEP，放大相关模态。
   - Semantic-CoT：通过更深层次的立场语义上下文洞察力细化SEP。
MEP和SEP这两个双重经验结构在训练过程中不断完善，并在推理时被召回以指导鲁棒且上下文感知的立场决策。

Result: 在公共MMSD基准上的广泛实验表明，ReMoD显著优于大多数基线模型，并表现出强大的泛化能力。

Conclusion: ReMoD通过其双重推理范式有效解决了多模态立场检测中模态贡献不均的问题，实现了更鲁棒和上下文感知的立场判断。

Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.

</details>


### [307] [Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts](https://arxiv.org/abs/2511.06048)
*Xinyuan Yan,Shusen Liu,Kowshik Thopalli,Bei Wang*

Main category: cs.CL

TL;DR: 针对稀疏自编码器(SAE)特征数量庞大难以探索的问题，本文提出一个结合拓扑可视化和降维的交互系统，通过聚焦策展概念来忠实呈现SAE特征的局部和全局关系，实现更深入的分析。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器(SAE)能从大型语言模型(LLM)中提取可解释特征，但特征数量巨大导致难以全面探索。传统嵌入技术（如UMAP）存在高维压缩伪影、过度绘制和邻域失真等局限性，无法有效可视化所有SAE特征。

Method: 本文提出一个“聚焦探索框架”，优先处理精选概念及其对应的SAE特征，而非同时可视化所有可用特征。开发了一个交互式可视化系统，该系统结合了基于拓扑的视觉编码和降维技术。

Result: 该混合方法能够忠实地表示所选SAE特征的局部和全局关系。它使用户能够通过有针对性的、可解释的子集来研究SAE的行为，从而促进对潜在空间中概念表示的更深入和细致的分析。

Conclusion: 通过聚焦策展概念并结合拓扑可视化与降维，该框架提供了一种更有效、可解释的方式来探索SAE特征，从而加深对LLM潜在空间中概念表示的理解。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.

</details>


### [308] [Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework](https://arxiv.org/abs/2511.06067)
*Haoyue Yang,Xuanle Zhao,Yujie Liu,Zhuojun Zou,Kailin Lyu,Changchun Zhou,Yao Zhu,Jie Hao*

Main category: cs.CL

TL;DR: ArchCraft是一个将学术论文中的抽象硬件架构描述转换为可综合Verilog项目并进行RTL验证的框架。它通过结构化工作流将非结构化论文转化为硬件感知设计，并生成RTL代码和测试平台，最终报告PPA。同时，论文还提出了ArchSynthBench基准测试集。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开的源代码和硬件描述语言（HDL）的复杂性，从学术论文中复现硬件架构仍然是一个重大挑战。

Method: ArchCraft引入了一个结构化的工作流：使用形式图捕获架构蓝图（Architectural Blueprint），使用符号定义功能规范（Functional Specification），从而将非结构化的学术论文转换为可验证的、硬件感知的设计。该框架随后生成RTL和测试平台（TB）代码，通过这些符号进行解耦以方便验证和调试，最终报告电路的功耗、面积和性能（PPA）。此外，论文还提出了ArchSynthBench，这是第一个用于从架构描述合成硬件的基准测试集。

Result: ArchCraft在ArchSynthBench上进行了系统评估，实验结果表明其方法优于直接生成方法和VerilogCoder框架，在论文理解和代码完成方面均表现出色。此外，对生成的RTL代码进行评估和物理实现表明，这些实现满足所有时序约束，且其性能指标与原始论文中报告的一致。

Conclusion: ArchCraft成功地将学术论文中的抽象硬件架构描述转化为可综合、可验证的Verilog项目，并提供准确的PPA报告，有效解决了硬件架构复现的挑战。其生成的RTL代码在物理实现中表现良好，性能与原始论文一致。

Abstract: The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.

</details>


### [309] [MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://arxiv.org/abs/2511.06086)
*Saurabh Page,Advait Joshi,S. S. Sonawane*

Main category: cs.CL

TL;DR: 本文引入了MuonAll优化器，通过将所有参数转换为2D矩阵，使Muon能够处理所有参数，并在微调大型语言模型时，Muon和MuonAll表现与AdamW相当。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在语言模型预训练中表现良好，但在现有公开预训练模型的微调中尚未被探索。目前Muon与AdamW结合使用，仍有改进空间以使所有参数都由Muon处理。

Method: 引入MuonAll优化器，通过将所有参数转换为2D矩阵，使所有参数都纳入Muon内部处理。对公开可用的、规模达5亿参数的语言模型进行了广泛的微调实验。

Result: 在主要基准测试中，Muon和MuonAll的表现与AdamW相当。

Conclusion: Muon和MuonAll是有效的替代优化器，可用于语言模型的微调。

Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer

</details>


### [310] [Stemming Hallucination in Language Models Using a Licensing Oracle](https://arxiv.org/abs/2511.06073)
*Simeon Emanuilov,Richard Ackermann*

Main category: cs.CL

TL;DR: 本研究提出“许可预言机”（Licensing Oracle），一种架构解决方案，通过对结构化知识图谱进行形式化验证，在语言模型生成过程中强制执行真实性约束，从而完全消除幻觉。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成自然语言时表现出色，但容易产生幻觉，即生成事实不准确但语法连贯的信息。研究旨在解决这一问题，确保生成内容的真实性。

Method: 引入“许可预言机”，这是一种将确定性验证步骤嵌入到模型生成过程中的架构方案。它通过对照结构化知识图谱进行形式化验证来强制执行真实性约束。该方法与基线语言模型、事实召回微调、弃权行为微调以及检索增强生成（RAG）等现有方法进行了比较评估。

Result: RAG和微调虽然提高了性能，但未能消除幻觉。相比之下，“许可预言机”实现了完美的弃权精度（AP = 1.0）和零错误答案（FAR-NE = 0.0），确保只生成有效的主张，并在事实响应中达到89.1%的准确率。

Conclusion: “许可预言机”等架构创新为在具有结构化知识表示的领域中解决语言模型幻觉提供了一个必要且充分的解决方案，提供了统计方法无法比拟的保证。它为未来AI系统中的真实性约束生成奠定了基础。

Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.

</details>


### [311] [Evaluation of retrieval-based QA on QUEST-LOFT](https://arxiv.org/abs/2511.06125)
*Nathan Scales,Nathanael Schärli,Olivier Bousquet*

Main category: cs.CL

TL;DR: 本文深入分析了RAG和长上下文模型在处理复杂推理和多文档信息问答（QUEST-LOFT基准）时的不足，并提出通过结构化输出（包含推理和证据）和可选的答案再验证来优化RAG，使其显著优于长上下文方法。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）方法和长上下文语言模型在处理信息分布在多文档中或需要复杂推理的问题时表现不佳，特别是在QUEST基准上存在显著改进空间。

Method: 对QUEST-LOFT上性能不佳的因素进行深入分析；通过彻底的人工评估发布更新的数据；将RAG与包含推理和证据的结构化输出格式结合，并可选地进行答案再验证，以优化RAG性能。

Result: 优化后的RAG，当与包含推理和证据的结构化输出格式结合，并可选地进行答案再验证时，在QUEST-LOFT基准上能够显著优于长上下文方法。

Conclusion: RAG可以通过结合结构化输出（包含推理和证据）和可选的答案再验证来有效优化，从而在处理需要复杂推理和多文档信息的问答任务上，显著超越长上下文语言模型。

Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.

</details>


### [312] [Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models](https://arxiv.org/abs/2511.06146)
*Akshar Tumu,Varad Shinde,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 本文提出使用指代表达理解（REC）任务来评估视觉语言模型（VLMs）的空间推理能力，并深入分析了模型在物体检测模糊性、复杂空间表达和否定句方面的表现，揭示了当前模型的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 空间推理是人类认知的重要组成部分，而最新的视觉语言模型（VLMs）在此方面表现出困难。现有分析工作主要使用图像字幕和视觉问答任务，可能未能充分捕捉空间推理的复杂性，因此需要一个更深入的评估平台。

Method: 本文提出使用指代表达理解（Referring Expression Comprehension, REC）任务作为评估VLMs空间推理能力的平台。在此平台上，作者分析了模型在以下三种情况下的空间理解和 grounding 能力：1) 物体检测存在歧义时，2) 包含更长句式和多个空间关系的复杂空间表达时，以及 3) 包含否定词（如'not'）的表达时。分析中使用了特定任务架构和大型VLMs。

Result: 所有模型在REC任务中都面临挑战，但它们的相对表现取决于底层模型和具体的空间语义类别（如拓扑、方向、近距离等）。研究结果突出了这些挑战和行为。

Conclusion: 研究结果揭示了VLMs在空间推理方面的研究空白和未来方向，强调了模型在处理歧义、复杂表达和否定句方面的不足，为后续研究提供了见解。

Abstract: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.

</details>


### [313] [BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering](https://arxiv.org/abs/2511.06183)
*Ryuhei Miyazato,Ting-Ruen Wei,Xuyang Wu,Hsin-Tai Wu,Kei Harada*

Main category: cs.CL

TL;DR: 本文提出BookAsSumQA，一个基于问答的评估框架，用于方面级图书摘要。研究发现，对于长文本，RAG方法比LLM方法更有效。


<details>
  <summary>Details</summary>
Motivation: 方面级摘要在图书领域的应用尚待探索，主要原因是难以构建长文本的参考摘要。

Method: 提出BookAsSumQA，一个基于问答的评估框架。它通过叙事知识图谱自动生成方面特定的问答对，并根据摘要的问答表现来评估其质量。

Result: 实验表明，LLM方法在短文本上表现更佳，但随着文档长度增加，RAG方法在方面级图书摘要方面变得更有效、更高效和实用。

Conclusion: BookAsSumQA为方面级图书摘要提供了一个新的评估方法，并指出RAG方法对于处理长篇文档的方面级摘要具有更高的效率和实用性。

Abstract: Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.

</details>


### [314] [Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning](https://arxiv.org/abs/2511.06190)
*Sangmook Lee,Dohyung Kim,Hyukhun Koh,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出STEER框架，利用小型LLM的内部置信度，在推理的每个步骤中动态决定是否调用大型LLM，从而在不使用外部模型的情况下，实现成本效益高且领域无关的推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理能力显著提升，但代价是更高的推理成本。现有降低成本的方法（如训练路由模型）存在领域迁移下鲁棒性差、需要昂贵的数据合成来获取路由标签等问题。

Method: 本文提出信心引导的逐步模型路由（STEER）框架。它是一种领域无关的方法，在更小和更大的LLM之间进行细粒度的、步骤级别的路由，且不依赖外部模型。STEER利用小型模型在生成推理步骤之前的置信度分数（来自其logits），仅在必要时才调用大型模型。

Result: 在数学推理、多跳问答和规划任务等多个领域的挑战性基准测试中，STEER在实现竞争或更高准确性的同时，显著降低了推理成本（例如，在AIME上，准确率提升20%的同时，浮点运算量减少48%），并且优于依赖外部训练模块的基线方法。研究结果表明，模型内部置信度是模型路由的一个鲁棒、领域无关的信号。

Conclusion: STEER框架通过利用模型内部置信度，为高效的LLM部署提供了一条可扩展的途径，实现了成本效益高且鲁棒的推理能力。

Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.

</details>


### [315] [SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization](https://arxiv.org/abs/2511.06222)
*Yue Huang,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为“优先级对齐”的新范式和“自优先级对齐（SPA）”的无监督框架，旨在确保大型语言模型（LLMs）在高风险场景中首先具备可信度（如无害、诚实），然后才优化其帮助性，从而在不牺牲安全性的前提下提升助益性。


<details>
  <summary>Details</summary>
Motivation: 在自残、法律或医疗等高风险情境中，LLMs必须既可信又有用。然而，这两个目标经常相互冲突，需要一种新的对齐策略来解决。

Method: 引入了“优先级对齐”范式，强制执行“可信优先于有用”的顺序。为实现此，提出了“自优先级对齐（SPA）”框架：它是一个完全无监督的框架，能生成多样化响应、模型自我评估和优化响应，并应用双准则去噪以消除不一致性和控制方差。SPA通过构建词典序偏好对，并使用强调高置信度、高差距决策的不确定性加权对齐损失来微调模型。

Result: 实验结果表明，SPA在不损害安全性的前提下提高了模型的帮助性，优于现有强大的基线模型，并保留了通用能力。

Conclusion: SPA为关键LLM应用提供了一种可扩展且可解释的对齐策略，成功解决了可信度和帮助性之间的冲突，在高风险场景中表现出色。

Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.

</details>


### [316] [Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease](https://arxiv.org/abs/2511.06215)
*Puzhen Su,Yongzhu Miao,Chunxi Guo,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 该研究提出了EK-ICL框架，通过整合结构化显式知识，显著提高了大型语言模型在数据稀缺和分布外条件下检测阿尔茨海默病(AD)的性能，解决了现有上下文学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 从叙述性文本中检测阿尔茨海默病对大型语言模型（LLMs）来说具有挑战性，尤其是在数据稀缺和分布外（OOD）条件下。现有的上下文学习（ICL）方法存在任务识别失败、示例选择不佳以及标签词与任务目标不匹配等问题，这些问题在阿尔茨海默病检测等临床领域尤为突出。

Method: 本文提出了显式知识上下文学习器（EK-ICL）框架，该框架通过整合结构化显式知识来增强ICL的推理稳定性和任务对齐。EK-ICL包含三个知识组件：利用小型语言模型（SLMs）的置信度分数来使预测与任务相关模式对齐；解析特征分数以捕捉结构差异并改进示例选择；以及标签词替换以解决与LLM先验知识的语义不匹配。此外，EK-ICL采用基于解析的检索策略和集成预测来减轻AD文本中语义同质性的影响。

Result: 在三个阿尔茨海默病数据集上进行的广泛实验表明，EK-ICL显著优于最先进的微调和ICL基线方法。

Conclusion: 研究表明，阿尔茨海默病检测中的ICL性能对标签语义和任务特定上下文的对齐高度敏感，强调了在低资源条件下，显式知识在临床推理中的重要性。

Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.

</details>


### [317] [Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records](https://arxiv.org/abs/2511.06230)
*Juntao Li,Haobin Yuan,Ling Luo,Tengxiao Lv,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文概述了CHIP 2025共享任务2竞赛，旨在利用真实世界中文电子病历数据，通过开发先进的大型语言模型（LLM）集成系统，自动推荐出院药物，并介绍了所构建的CDrugRed数据集，同时指出该领域仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 确保治疗连续性、预防再入院以及改善慢性代谢疾病患者的长期管理，出院药物推荐在此过程中发挥关键作用。

Method: 组织了CHIP 2025共享任务2竞赛，构建了包含5,894份匿名住院记录（来自3,190名中国患者）的高质量CDrugRed数据集。任务的挑战在于药物推荐的多标签性质、异构临床文本和患者特定的治疗计划变异性。参赛团队提交解决方案，其中表现最佳的团队采用了基于LLM的集成系统。

Result: 共有526支队伍注册，167支和95支队伍分别向A阶段和B阶段排行榜提交了有效结果。表现最佳的团队在最终测试集上取得了0.5102的Jaccard分数和0.6267的F1分数，展示了先进的基于LLM的集成系统的潜力。

Conclusion: 研究结果突显了将LLM应用于中文电子病历中的药物推荐的潜力和现有挑战。尽管LLM-based集成系统表现出良好前景，但仍有待进一步改进和克服的难题。

Abstract: Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.

</details>


### [318] [Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy](https://arxiv.org/abs/2511.06234)
*Mojtaba Noghabaei*

Main category: cs.CL

TL;DR: 预训练NLI模型在处理否定句时表现不佳，通过数据增强（对比集和对抗样本）可以有效提升模型对否定句的理解能力，同时不影响整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有NLI预训练模型在基准数据集上表现良好，但往往依赖虚假关联而非真正理解语言细微之处（如否定），导致在处理否定句时表现不佳。

Method: 首先，分析在SNLI数据集上微调的ELECTRA-small模型对否定句的分类性能。然后，通过使用强调否定的对比集和对抗样本来增强训练数据。

Result: 模型在分类包含否定词的例子时表现不佳。然而，有针对性的数据增强显著提高了模型在否定句上的准确性，且未对整体性能产生负面影响。

Conclusion: 通过有针对性的数据增强，可以有效缓解NLI模型在处理否定句时的数据集伪影问题，提升模型对语言细微之处的理解能力。

Abstract: Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.

</details>


### [319] [TimeSense:Making Large Language Models Proficient in Time-Series Analysis](https://arxiv.org/abs/2511.06344)
*Zhirui Zhang,Changhua Pei,Tianyi Gao,Zhe Xie,Yibo Hao,Zhaoyang Yu,Longlong Xu,Tong Xiao,Jing Han,Dan Pei*

Main category: cs.CL

TL;DR: 该论文提出了EvalTS基准测试和TimeSense框架，旨在解决现有结合LLM与时间序列的方法过度依赖文本标签、忽视时间特征的问题。TimeSense通过平衡文本推理和保留时间感知能力，在多任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有将大型语言模型（LLM）与时间序列数据结合的方法在训练时过度依赖文本标签进行监督，这可能导致模型偏向文本线索而忽视完整的时间特征，从而产生与底层时间序列上下文矛盾的输出。

Method: 论文构建了EvalTS基准测试，包含10个任务，分为三个难度级别，用于在更具挑战性和现实的场景下评估模型。同时，提出了TimeSense多模态框架，该框架包含一个“时间感知模块”（Temporal Sense module），用于在模型上下文中重建输入时间序列，确保文本推理以时间序列动态为基础。此外，为增强时间序列数据的空间理解，明确引入了基于坐标的位置嵌入（coordinate-based positional embeddings），为每个时间点提供空间上下文。

Result: 实验结果表明，TimeSense在多项任务上取得了最先进的性能，尤其在复杂的多维时间序列推理任务上明显优于现有方法。

Conclusion: TimeSense成功解决了现有方法中文本与时间序列特征不平衡的问题，通过其创新的时间感知模块和位置嵌入，使LLM能够更准确、更全面地理解和推理时间序列数据，并在各种复杂任务中表现出色。

Abstract: In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.

</details>


### [320] [SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss](https://arxiv.org/abs/2511.06402)
*Lionel Z. Wang,Shihan Ben,Yulu Huang,Simeng Qing*

Main category: cs.CL

TL;DR: 本文提出SugarTextNet，一个基于Transformer的框架，用于检测社交媒体上的“糖恋”相关内容。该框架结合了预训练Transformer、注意力线索提取器和上下文短语编码器，并引入了上下文感知Focal Loss以解决类别不平衡问题。在中文微博数据集上，SugarTextNet显著优于现有模型，为敏感内容检测提供了有效方案。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上“糖恋”相关内容迅速蔓延，引发了亲密关系商业化和交易关系常态化等严重的社会和监管担忧。由于委婉语、模糊语言线索和真实数据中极度的类别不平衡，检测此类内容极具挑战性。

Method: 本研究提出了SugarTextNet，一个专门用于识别社交媒体上“糖恋”相关帖子的新型基于Transformer的框架。它集成了预训练Transformer编码器、基于注意力的线索提取器和上下文短语编码器，以捕捉用户生成文本中显著和细微的特征。为解决类别不平衡问题并增强少数类别检测，引入了上下文感知Focal Loss，该损失函数结合了Focal Loss缩放和上下文加权。模型在一个新收集的、手动标注的包含3,067条新浪微博中文社交媒体帖子的数据集上进行评估。

Result: SugarTextNet在多个指标上显著优于传统的机器学习模型、深度学习基线模型和大型语言模型。全面的消融研究证实了每个组件不可或缺的作用。

Conclusion: 研究结果强调了针对敏感内容检测进行领域特定、上下文感知建模的重要性，并为复杂现实场景中的内容审核提供了一个鲁棒的解决方案。

Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.

</details>


### [321] [HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection](https://arxiv.org/abs/2511.06391)
*Irina Proskurina,Marc-Antoine Carpentier,Julien Velcin*

Main category: cs.CL

TL;DR: 本研究引入HatePrototypes，一种类别级向量表示，实现了显性和隐性仇恨言论检测的跨任务迁移和高效早期退出，解决了现有模型在处理隐性仇恨方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测基准主要关注显性仇恨，往往忽视了隐性或间接仇恨，而后者需要更深层的语义处理。此外，模型优化通常需要对新基准进行重复预训练或微调。

Method: 本研究提出并分析了HatePrototypes，即从为仇恨言论检测和安全审核优化的语言模型中提取的类别级向量表示。同时，探索了结合原型的参数无关早期退出机制。

Result: HatePrototypes只需少量样本（每类50个）即可在显性和隐性仇恨之间实现跨任务迁移，并且原型在不同基准之间可互换。此外，结合原型的参数无关早期退出对这两种仇恨类型均有效。

Conclusion: HatePrototypes提供了一种高效且可迁移的仇恨言论检测方法，尤其在处理隐性仇恨方面表现出色，减少了重复微调的需求。研究成果及资源已发布，以支持未来的相关研究。

Abstract: Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.

</details>


### [322] [Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop](https://arxiv.org/abs/2511.06427)
*Lifeng Han,David Lindevelt,Sander Puts,Erik van Mulligen,Suzan Verberne*

Main category: cs.CL

TL;DR: 本研究从荷兰癌症患者的访谈和在线论坛数据中提取隐喻，利用大型语言模型（LLMs）探索不同的提示策略，并通过人工验证构建了一个名为HealthQuote.NL的隐喻语料库，旨在改善医疗沟通和患者护理。


<details>
  <summary>Details</summary>
Motivation: 隐喻和隐喻语言在医护人员、患者及其家属之间的医疗沟通中扮演着重要角色。理解和利用这些隐喻可以支持更好的患者护理，如共享决策、改善医患沟通和提升患者健康素养。

Method: 研究使用荷兰语癌症患者数据，包括患者访谈和在线论坛帖子。通过大型语言模型（LLMs）提取患者使用的隐喻，并探索了链式思维推理、少样本学习和自提示等不同的提示策略。提取的隐喻经过人工验证，并汇编成HealthQuote.NL语料库。

Result: 研究成功构建了一个名为HealthQuote.NL的隐喻语料库，其中包含了从荷兰癌症患者数据中提取的隐喻。同时，研究探讨了当前最先进的LLMs在该任务上的表现。

Conclusion: 提取出的隐喻有望支持更好的患者护理，包括促进共享决策、改善医患沟通、提升患者健康素养，并为个性化护理路径的设计提供信息。

Abstract: Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL

</details>


### [323] [How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset](https://arxiv.org/abs/2511.06418)
*Sunil Mohan,Theofanis Karaletsos*

Main category: cs.CL

TL;DR: 本研究引入了一个新数据集，评估大型语言模型（LLMs）在药物开发和个性化医疗领域的事实知识和新情境推理能力。结果显示o4-mini和Qwen3-4B-thinking表现突出，并揭示开放世界推理和影响内部链接的反事实推理更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 药物开发/再利用和个性化医疗领域对预训练LLMs的兴趣日益增长。LLMs需展示事实知识和对药物机制的深入理解，以便在新情境中回忆和推理相关知识。

Method: 本研究引入了一个数据集，用于评估LLMs在已知药物机制的事实知识以及在新颖反事实情境下（模型在训练中不太可能见过）的推理能力。研究比较了OpenAI的o4-mini、4o、o3、o3-mini模型以及Qwen3-4B-thinking模型的性能，并分析了开放世界与封闭世界设置下以及影响推理链内部链接与药物相关链接的反事实情境下的表现。

Result: o4-mini模型优于OpenAI的4o、o3和o3-mini模型。最近的小型Qwen3-4B-thinking模型表现与o4-mini非常接近，在某些情况下甚至超越。需要模型回忆相关知识的开放世界推理任务比提供所需事实知识的封闭世界任务更具挑战性。影响推理链内部链接的反事实情境比影响提示中提及药物链接的任务更困难。

Conclusion: LLMs在药物开发和个性化医疗应用中，需要深入理解药物机制并具备在未知情境下进行推理的能力。o4-mini和Qwen3-4B-thinking模型在此类任务中表现出色。开放世界推理和针对内部链接的反事实推理是LLMs面临的更大挑战，表明这些是未来改进的关键领域。

Abstract: Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.

</details>


### [324] [Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models](https://arxiv.org/abs/2511.06441)
*Mayank Saini,Arit Kumar Bishwas*

Main category: cs.CL

TL;DR: 本文提出一个统一的模块化框架，通过学习型路由网络智能地将文本、多模态或复杂查询分配给最合适的专家模型，以平衡成本和质量，显著降低了对昂贵模型的依赖，同时保持或超越了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在视觉、音频和文档理解方面的应用日益广泛，但其高昂的推理成本阻碍了实时、可扩展的部署。同时，小型开源模型虽然具有成本优势，但在处理复杂或多模态查询时表现不足。

Method: 该研究引入了一个统一的、模块化的框架，该框架使用一个学习型路由网络，智能地将每种查询（文本、多模态或复杂）路由到最合适的专家模型，以平衡成本和质量。对于视觉任务，采用了一个优化的两阶段开源流程，并重新利用了在特定子任务中仍是SOTA的高效经典视觉组件。

Result: 在MMLU和VQA等基准测试中，该框架的性能与始终使用高级LLM（单一模型服务所有查询类型）的系统相当或更优，同时将对昂贵模型的依赖降低了超过67%。

Conclusion: 通过其可扩展的多代理编排，该研究提供了一种高质量、资源高效且可大规模部署的AI解决方案。

Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.

</details>


### [325] [SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention](https://arxiv.org/abs/2511.06446)
*Bohan Yu,Wei Huang,Kang Liu*

Main category: cs.CL

TL;DR: 本文提出SR-KI，一种将大规模结构化知识库（KBs）高效且端到端地整合到大型语言模型（LLMs）中的新方法，通过注入KV缓存和两阶段训练实现内部检索和动态更新。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法严重依赖外部检索器和多阶段流程，存在效率和端到端推理的局限性。研究旨在开发一种能将知识库高效压缩、支持动态更新并实现模型内部端到端检索的集成方案。

Method: SR-KI首先使用预训练编码器将知识库编码为键值对，并将其注入到LLMs的KV缓存中。接着，采用两阶段训练范式：首先在LLM内部定位一个专用的检索层，然后在此层应用基于注意力的损失函数，明确监督模型关注相关的知识库条目。这种设计允许在模型潜在空间内进行端到端推理，无需外部检索器。

Result: SR-KI成功地将多达40K的知识库集成到一个7B的LLM中，仅需一张A100 40GB GPU。它展示了强大的检索性能，在最佳任务上Recall@10超过98%，所有任务平均超过88%。在问答和KB ID生成等任务上，SR-KI在保持强大性能的同时，实现了高达99.75%的注入知识库压缩率。

Conclusion: SR-KI提供了一种有效且高效的解决方案，将大规模实时结构化知识库集成到LLMs中。它通过模型内部的端到端检索克服了传统RAG的局限性，实现了知识的高效压缩和动态更新，并在检索和任务性能上均表现出色。

Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.

</details>


### [326] [You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations](https://arxiv.org/abs/2511.06516)
*Amit LeVi,Raz Lapid,Rom Himelstein,Yaniv Nemcovsky,Ravid Shwartz Ziv,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出两种任务感知后训练量化(PTQ)方法——TAQ和TAQO，通过识别和保留任务相关层的精度，从而在保持高准确性的同时，为大型语言模型(LLMs)创建高效的任务专用版本，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)虽然能力强大，但在许多仅需有限能力的场景下，其内存和延迟效率低下。现有的后训练量化(PTQ)方法大多是任务无关的，未能考虑任务特定信号在模型层间的分布情况。

Method: 本研究提出使用编码任务显著信号的隐藏表示作为量化指导。具体比较了两种新的任务感知PTQ方法：任务感知量化(TAQ)通过任务条件下的隐藏激活统计数据分配位宽；TAQO则基于直接的层敏感度测试分配精度。这两种方法都通过一个小型的校准集来识别任务相关的层，保留这些层的精度，同时对其他层进行激进量化。

Result: 这些方法产生了稳定的任务敏感度配置文件和高效的任务专用模型。TAQ和TAQO在不同模型上均优于基线方法；TAQ在Phi-4模型上表现最佳，而TAQO在Llama-3.1、Qwen3和Qwen2.5模型上表现最佳。例如，在Phi-4上，TAQ达到了42.33 EM / 50.81 F1，远超Activation-aware Weight Quantization (AWQ)的2.25 / 7.07，并且在较低的平均精度下仍能保持与原始精度小于1.0%的差距。

Conclusion: 通过利用任务显著信号来指导量化过程，TAQ和TAQO这两种任务感知PTQ方法能够有效地为LLMs创建高效、任务专用的模型，同时显著提升性能并保持高精度，解决了传统PTQ方法的任务无关性问题。

Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.

</details>


### [327] [Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement](https://arxiv.org/abs/2511.06530)
*Xiaonan Luo,Yue Huang,Ping He,Xiangliang Zhang*

Main category: cs.CL

TL;DR: RefineLab是一个由大型语言模型（LLM）驱动的框架，能在受控的token预算下，自动将原始问答文本数据精炼成高质量数据集，以解决现有问答数据集的质量问题。


<details>
  <summary>Details</summary>
Motivation: 高质量的问答（QA）数据集是可靠评估大型语言模型（LLM）的基础。然而，即使是专家制作的数据集也存在领域覆盖不足、难度分布不均和事实不一致等问题。生成模型驱动的数据集激增进一步加剧了这些质量挑战。

Method: RefineLab接收一组目标质量属性（如覆盖率和难度平衡）作为精炼目标，并在预定义的token预算内进行选择性编辑。它将此视为一个受限优化问题：在资源限制下尽可能提高QA样本质量。通过一系列可用的精炼操作（如改写、干扰项替换），RefineLab输入原始数据集、目标质量维度和token预算，并通过一个分配模块选择最佳精炼策略，以最大化整体数据集质量并遵守预算约束。

Result: 实验表明，RefineLab在覆盖率、难度对齐、事实保真度和干扰项质量方面，持续缩小了与专家数据集的差距。

Conclusion: RefineLab开创了一条可扩展、可定制且可复现的数据集设计路径，对大型语言模型评估具有广泛影响。

Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.

</details>


### [328] [Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages](https://arxiv.org/abs/2511.06497)
*Quang Phuoc Nguyen,David Anugraha,Felix Gaschi,Jun Bin Cheng,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本研究通过实证分析表明，多语言模型中的词语对齐（realignment）对低资源语言（LRLs）特别有效，并且精心选择的、语言学上多样化的子集可以达到甚至超越所有语言对齐的效果，减少数据收集开销。


<details>
  <summary>Details</summary>
Motivation: 多语言模型中的词语对齐策略在跨语言迁移方面的经验结果好坏参半且不可靠，尤其对于类型学上距离较远或低资源语言。此外，现有的词语对齐工具通常依赖高质量的并行数据，而这对于许多低资源语言来说是稀缺或嘈杂的。

Method: 我们进行了一项广泛的实证研究，通过受控实验调查词语对齐是否真的需要使用所有可用语言，或者战略性选择的子集能否提供可比甚至改进的跨语言迁移，并研究其对低资源语言的影响。

Result: 实验结果显示，词语对齐对低资源语言特别有效。使用精心选择的、语言学上多样化的子集可以与完整的多语言对齐效果相媲美，甚至在未见过的低资源语言上表现更优。

Conclusion: 有效的词语对齐不需要穷尽所有语言覆盖。在知情的语言选择指导下，可以减少数据收集开销，同时保持效率和鲁棒性。

Abstract: Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.

</details>


### [329] [Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages](https://arxiv.org/abs/2511.06531)
*Oluwadara Kalejaiye,Luel Hagos Beyene,David Ifeoluwa Adelani,Mmekut-Mfon Gabriel Edet,Aniefon Daniel Akpan,Eno-Abasi Urua,Anietie Andy*

Main category: cs.CL

TL;DR: 本文介绍了ibom数据集，旨在解决尼日利亚四种未被充分研究的沿海语言（Anaang、Efik、Ibibio和Oro）在机器翻译和主题分类方面的数据稀缺问题，并评估了现有大型语言模型（LLMs）在此类语言上的表现。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚拥有丰富的语言多样性，但自然语言处理（NLP）研究主要集中在少数几种语言上（不到总数的1%），而大多数语言因缺乏文本数据而未被研究。特别是四种沿海语言（Anaang、Efik、Ibibio和Oro）在主流翻译工具和基准测试中均无代表，作者旨在填补这一空白。

Method: 研究者引入了ibom数据集，用于Anaang、Efik、Ibibio和Oro这四种沿海尼日利亚语言的机器翻译和主题分类。他们将Flores-200基准测试扩展到这些语言，并将翻译文本与基于SIB-200分类数据集的主题标签对齐。

Result: 评估结果显示，当前的大型语言模型（LLMs）在这些语言的机器翻译任务上表现不佳，无论是在零样本还是少样本设置下。然而，研究发现，在少样本设置下，随着样本数量的增加，主题分类的性能会稳步提升。

Conclusion: 该研究通过引入ibom数据集，为尼日利亚未被充分研究的沿海语言的NLP研究奠定了基础。尽管现有LLMs在这些语言的机器翻译方面表现有限，但少样本学习在主题分类上展现出潜力，强调了未来需要更多数据和专门模型来支持这些语言的NLP发展。

Abstract: Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.

</details>


### [330] [TabRAG: Tabular Document Retrieval via Structured Language Representations](https://arxiv.org/abs/2511.06582)
*Jacob Si,Mike Qu,Michelle Lee,Yingzhen Li*

Main category: cs.CL

TL;DR: TabRAG是一个基于解析的RAG管线，专门通过结构化语言表示处理包含大量表格的文档，在生成和检索方面优于现有流行方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG数据摄取方法存在局限：直接微调嵌入模型计算成本高，而基于解析的方法在处理表格数据时性能不佳。

Method: 本文提出TabRAG，一个基于解析的RAG管线，利用结构化语言表示来处理和提取表格密集型文档中的信息。

Result: TabRAG在生成和检索任务上均优于现有的流行解析方法。

Conclusion: TabRAG有效解决了RAG处理表格密集型文档的挑战，通过其创新的解析方法提高了性能。

Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.

</details>


### [331] [Rep2Text: Decoding Full Text from a Single LLM Token Representation](https://arxiv.org/abs/2511.06571)
*Haiyan Zhao,Zirui He,Fan Yang,Ali Payani,Mengnan Du*

Main category: cs.CL

TL;DR: 本研究提出Rep2Text框架，能从大型语言模型(LLM)的最后一个token表示中恢复原始输入文本，发现即使是压缩表示也保留了大量语义信息，但存在信息瓶颈效应。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在各种任务中取得了显著进展，但其内部机制仍不透明。本研究旨在探究LLM的最后一个token表示能在多大程度上保留并恢复原始输入文本的信息。

Method: Rep2Text框架通过一个可训练的适配器，将目标LLM的内部表示投影到解码语言模型的嵌入空间。随后，解码语言模型自回归地重建输入文本。

Result: 实验表明，平均而言，可以从16个token序列的压缩表示中恢复超过一半的信息，同时保持强大的语义完整性和连贯性。分析揭示了信息瓶颈效应：序列越长，token级别的恢复率越低，但语义完整性仍能保持。该框架对分布外(OOD)的医疗数据也表现出强大的泛化能力。

Conclusion: LLM的最后一个token表示能够有效保留原始输入文本的语义信息，即使在信息压缩的情况下。尽管存在信息瓶颈效应导致长序列的token级恢复率下降，但语义完整性得以维持，这揭示了LLM内部信息处理和压缩的特性。

Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.

</details>


### [332] [MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592)
*Zhi Rui Tam,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 研究发现，在临床环境中，大型语言模型（LLMs）通过音频交互时，存在严重的模态和人口学偏见，导致手术建议基于患者声音特征而非医学证据，可能加剧医疗不平等。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从文本界面转向临床环境中的音频交互，可能会通过音频中的副语言线索引入新的漏洞，导致基于声音特征而非医学证据的临床决策偏见。

Method: 研究评估了170个临床病例，每个病例都通过36种不同的语音配置文件（涵盖年龄、性别和情感变化）合成语音。将音频输入的模型表现与相同的文本输入进行比较，并分析了链式思考提示（chain-of-thought prompting）和明确推理（explicit reasoning）对偏见的影响。

Result: 研究发现严重的模态偏见，音频输入的医疗建议与相同的文本输入相比，差异高达35%，其中一个模型提供的建议减少了80%。在年轻和老年声音之间存在高达12%的年龄差异，且在大多数模型中，即使采用链式思考提示也未能消除。明确推理成功消除了性别偏见，但由于情感识别性能不佳，未能检测到情感的影响。

Conclusion: 音频LLMs容易根据患者的声音特征而非医学证据做出临床决策，这种缺陷有加剧医疗不平等的风险。因此，在这些模型部署到临床之前，急需开发具有偏见意识的架构。

Abstract: As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.

</details>


### [333] [Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes](https://arxiv.org/abs/2511.06601)
*Zi-Niu Wu*

Main category: cs.CL

TL;DR: 本文提出基于对偶性的修辞模式操作和金字塔多层映射框架，以扩展修辞模式、量化表达多样性和复杂性，并为未来AI系统处理分层修辞推理结构提供途径。


<details>
  <summary>Details</summary>
Motivation: 修辞模式在学术和非学术写作中都很有用，并且是语言学研究和计算建模的主题。在这些领域之间建立概念桥梁可以使它们相互受益。目前的修辞分类法是静态且不可测量的，需要更动态、可测量的方法。

Method: 提出了基于对偶性的模式操作（分裂-联合、前向-后向、扩展-缩减和正交对偶性）来扩展修辞模式，引入了组合和泛化等生成模式。提出了一种金字塔多层映射框架（从修辞模型层到认知层再到认知层）。通过二项式组合学和香农熵分析量化了表达多样性和复杂性降低的程度。定义了边缘修辞比特（MRB）以测量表达增长速度。通过直接熵测量比较了分层选择与扁平选择的不确定性。

Result: 对偶性操作扩展了修辞模式集，增强了认知多样性。多层映射框架降低了认知复杂性。二项式组合学和香农熵分析量化了表达多样性和复杂性降低的程度。边缘修辞比特（MRB）被识别，并可用于定义修辞可伸缩参数。直接熵测量表明，与跨所有模式的扁平选择相比，分层选择显著降低了选择不确定性。这些考虑将静态、不可测量的修辞分类法转变为更动态、可测量的语篇设计系统。

Conclusion: 这项工作为未来的AI系统提供了一条途径，使其不仅能在语言标记上操作，还能在分层的修辞推理结构上操作，从而连接语言学、教育学、学术和计算研究。

Abstract: Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research

</details>


### [334] [How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models](https://arxiv.org/abs/2511.06676)
*Subhojit Ghimire*

Main category: cs.CL

TL;DR: 本研究揭示了AI毒性模型对非洲裔美国英语（AAE）存在显著偏见，将其错误标记为更具毒性，并开发了一个交互式工具来展示这种偏见如何通过人类设定的策略导致歧视。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的内容审核日益普及，人们普遍担忧AI算法可能存在偏见，导致某些在线帖子被不公正地标记为“不当”。研究旨在探究这种担忧的深层原因，并量化AI偏见对不同英语方言的影响。

Method: 研究采用双重方法：1. 对广泛使用的毒性模型（unitary/toxic-bert）进行定量基准测试，比较其在非洲裔美国英语（AAE）和标准美式英语（SAE）文本上的表现差异。2. 引入一个交互式教学工具，通过用户可控的“敏感度阈值”功能，直观地展示抽象的AI偏见如何通过人类设定的策略转化为实际的歧视。

Result: 基准测试显示，该模型平均将AAE文本标记为毒性高1.8倍，将“身份仇恨”得分高8.8倍，证实了系统性偏见。交互式工具进一步揭示，偏见的得分本身并非唯一危害，更令人担忧的是，看似中立的人类设定策略（如敏感度阈值）最终将歧视付诸实施。

Conclusion: 本研究提供了AI毒性模型对AAE文本存在显著偏见的统计证据，并开发了一个面向公众的工具，旨在提高批判性AI素养，让人们认识到偏见不仅存在于算法中，也存在于操作化歧视的人类策略中。

Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.

</details>


### [335] [Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention](https://arxiv.org/abs/2511.06682)
*Shibing Mo,Haoyang Ruan,Kai Wu,Jing Liu*

Main category: cs.CL

TL;DR: 本文提出文本自注意力网络（TSAN），一种无需参数更新的测试时偏好优化范式，通过在自然语言中模拟自注意力机制，分析、权衡并综合多个候选回复的优势，生成更符合偏好的新回复。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的输出对齐人类偏好通常需要昂贵的监督微调（SFT）。现有测试时方法虽利用文本反馈，但仅限于批判和修改单个候选回复，缺乏系统分析、权衡和综合多个有前景候选回复优势的机制。这种机制至关重要，因为不同回复可能在不同方面表现出色，结合其最佳元素能产生更优结果。

Method: TSAN在自然语言中模拟自注意力机制：它将多个候选回复格式化为文本键和值进行分析，使用基于LLM的注意力模块权衡其相关性，并在学习到的文本注意力指导下，将其优势综合为一个新的、与偏好对齐的回复。整个过程在文本梯度空间中迭代且可解释地进行。

Result: 经验评估表明，在基础SFT模型上仅进行三次测试时迭代，TSAN便超越了Llama-3.1-70B-Instruct等监督模型，并通过有效利用多个候选解决方案，超越了当前最先进的测试时对齐方法。

Conclusion: TSAN通过在文本自注意力机制下系统地分析和综合多个候选回复的优势，为测试时偏好优化提供了一种新颖且高效的范式，无需参数更新即可显著提升LLM的输出质量和对齐度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.

</details>


### [336] [Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation](https://arxiv.org/abs/2511.06680)
*Keunhyeung Park,Seunguk Yu,Youngbin Kim*

Main category: cs.CL

TL;DR: 本文提出DIA-REFINE框架，通过迭代翻译、验证和反馈机制，结合外部方言分类器，解决大型语言模型在标准语到方言翻译中的方言差距问题。同时，引入方言忠实度得分（DFS）和目标方言比率（TDR）两种新评估指标，以克服n-gram指标的局限性，实现更准确的方言翻译评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在标准语到方言翻译中存在显著的方言差距，难以生成忠实的方言输出。此外，传统的n-gram指标在评估方言翻译时存在偏差，倾向于奖励源语言复制而非真实的方言翻译，导致评估失真。

Method: 本文提出方言细化（DIA-REFINE）框架，该框架通过翻译、验证和反馈的迭代循环，利用外部方言分类器引导大型语言模型生成忠实的目标方言输出。为解决n-gram指标的局限性，引入方言忠实度得分（DFS）来量化语言转换，并引入目标方言比率（TDR）来衡量方言翻译的成功程度。

Result: 实验结果表明，DIA-REFINE在零样本和上下文学习基线上均能持续提升方言忠实度。所提出的新指标能够有效区分“虚假成功”（高n-gram得分但方言翻译失败）和“真实尝试”（低n-gram得分但有真实方言翻译尝试）。研究还发现模型对该框架的响应程度各异，并且整合上下文示例能进一步改善方言表达的翻译。

Conclusion: 本文建立了一个针对目标导向、包容性方言翻译的稳健框架，不仅提供了严格的评估方法，也为理解模型性能提供了关键见解。

Abstract: Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.

</details>


### [337] [Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content](https://arxiv.org/abs/2511.06708)
*Adi Danish Bin Muhammad Amin,Mohaiminul Islam Bhuiyan,Nur Shazwani Kamarudin,Zulfahmi Toh,Nur Syafiqah Nafis*

Main category: cs.CL

TL;DR: 本研究对YouTube游戏评论进行情感分析，利用机器学习算法（如SVM）来理解游戏社区的用户情绪，为游戏开发者提供有价值的反馈。


<details>
  <summary>Details</summary>
Motivation: 游戏行业的快速发展和社区的壮大，使得深入理解用户在YouTube等社交媒体平台上表达的情感变得必要。

Method: 通过YouTube API收集游戏相关评论，使用TextBlob进行情感分析，并利用朴素贝叶斯、逻辑回归和支持向量机（SVM）等机器学习算法进行数据分类。数据预处理是分析的关键步骤。

Result: 支持向量机（SVM）在不同数据集上表现出卓越的性能，取得了最高的分类准确率。分析揭示了流行游戏视频中用户偏好和批评的趋势和见解。

Conclusion: 先进的情感分析对于捕捉用户评论中细微的情感至关重要，能为游戏开发者提供宝贵的反馈，以改进游戏设计和用户体验。未来的研究将整合更复杂的自然语言处理技术并探索更多数据源。

Abstract: The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Naïve Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.

</details>


### [338] [Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights](https://arxiv.org/abs/2511.06738)
*Hyunjae Kim,Jiwoong Sohn,Aidan Gilson,Nicholas Cochran-Caggiano,Serina Applebaum,Heeju Jin,Seihee Park,Yujin Park,Jiyeong Park,Seoyoung Choi,Brittany Alexandra Herrera Contreras,Thomas Huang,Jaehoon Yun,Ethan F. Wei,Roy Jiang,Leah Colucci,Eric Lai,Amisha Dave,Tuo Guo,Maxwell B. Singer,Yonghoe Koo,Ron A. Adelman,James Zou,Andrew Taylor,Arman Cohan,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本研究通过专家评估发现，在医学领域，标准RAG（检索增强生成）常常会降低大型语言模型的性能，主要原因是检索和证据选择环节存在问题。然而，简单的策略可以有效改善这些问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学领域面临知识更新快和缺乏可验证、基于证据的推理两大挑战。RAG被广泛采用以解决这些限制，但其在医学领域是否可靠尚不明确。

Method: 研究进行了迄今为止最全面的医学RAG专家评估。18位医学专家贡献了80,502条注释，评估了GPT-4o和Llama-3.1-8B在200个真实患者和USMLE风格查询下生成的800个模型输出。RAG流程被系统地分解为三个部分：（i）证据检索、（ii）证据选择和（iii）响应生成。此外，研究还测试了包括证据过滤和查询重构在内的简单有效策略。

Result: 与预期相反，标准RAG常常会降低性能：前16个检索到的段落中只有22%是相关的，证据选择仍然很弱（精确率41-43%，召回率27-49%），与非RAG变体相比，事实性和完整性分别下降了高达6%和5%。检索和证据选择是模型的主要失败点，导致整体性能下降。研究还表明，简单的策略（如证据过滤和查询重构）能显著缓解这些问题，将MedMCQA和MedXpertQA上的性能分别提高了高达12%和8.2%。

Conclusion: 这些发现呼吁重新审视RAG在医学中的作用，并强调了阶段感知评估和精心系统设计对于可靠的医学LLM应用的重要性。

Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.

</details>


### [339] [Sensitivity of Small Language Models to Fine-tuning Data Contamination](https://arxiv.org/abs/2511.06763)
*Nicy Scaria,Silvester John Joseph Kennedy,Deepak Subramani*

Main category: cs.CL

TL;DR: 本文系统研究了小型语言模型（SLMs）对指令微调过程中数据污染的敏感性，发现它们对句法污染极度脆弱，而对语义污染表现出不对称的敏感性，且更大的模型更容易受到语义污染的影响。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型越来越多地部署在资源受限的环境中，但它们对指令微调期间数据污染的行为鲁棒性尚不清楚，这促使研究人员深入探究其敏感性。

Method: 研究系统地调查了23个SLM（2.7亿至40亿参数），通过测量它们对句法（字符和单词颠倒）和语义（不相关和反事实响应）转换类型的敏感性。每种污染类型都以25%、50%、75%和100%的污染水平应用。

Result: 研究揭示了脆弱性模式的根本不对称性：句法转换导致灾难性的性能下降，其中字符颠倒导致所有模型几乎完全失效。而语义转换则表现出明显的阈值行为和更强的核心语言能力弹性。此外，发现存在“能力诅咒”，即更大、能力更强的模型更容易学习语义腐败，更容易遵循有害指令。基模型与指令微调模型的分析显示，对齐提供的鲁棒性益处不一致，有时甚至会降低弹性。

Conclusion: 研究建立了三个核心贡献：(1) SLM对句法模式污染过度脆弱的经验证据；(2) 句法和语义转换之间不对称敏感性模式的识别；(3) 污染鲁棒性评估的系统评估协议。这些发现对部署具有直接影响，表明当前对小型模型的鲁棒性假设可能不成立，并强调需要制定污染感知训练协议。

Abstract: Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\textit{capability curse}" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.

</details>


### [340] [Learning to Focus: Focal Attention for Selective and Scalable Transformers](https://arxiv.org/abs/2511.06818)
*Dhananjay Ram,Wei Xia,Stefano Soatto*

Main category: cs.CL

TL;DR: Focal Attention通过控制softmax温度来锐化注意力分布，解决了标准softmax注意力在长上下文中的噪声问题，显著提升了Transformer模型在效率和准确性方面的表现。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的标准softmax注意力在长上下文中常产生嘈杂的概率分布，这会损害模型在各层中有效选择特征的能力。

Method: 提出Focal Attention，通过控制softmax温度来锐化注意力分布，该温度可以是固定的超参数，也可以是训练过程中可学习的参数。这使得模型能够专注于最相关的token并抑制不相关的token。

Result: Focal Attention在模型大小、训练数据和上下文长度方面比标准Transformer表现出更好的扩展性。在不同基准测试中，它能以最多42%更少的参数或33%更少的训练数据达到相同的准确率。在长上下文任务上，它实现了17%到82%的显著相对改进。

Conclusion: Focal Attention在实际应用中非常有效，尤其是在长上下文任务中，它通过提高效率和准确性来增强Transformer模型的性能。

Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.

</details>


### [341] [SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces](https://arxiv.org/abs/2511.06778)
*Ruiheng Liu,XiaoBing Chen,Jinyu Zhang,Qiongwen Zhang,Yu Zhang,Bailong Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为\textsc{SafeNlidb}的隐私-安全对齐框架，旨在解决基于大型语言模型（LLM）的自然语言数据库接口（NLIDB）中的数据泄露问题，通过自动化生成混合思维链数据和优化偏好学习，无需人工标注偏好数据即可生成安全感知型SQL。


<details>
  <summary>Details</summary>
Motivation: LLM在NLIDB中的广泛应用带来了严重的隐私和安全问题，可能无意中泄露机密数据库内容或被攻击者利用。现有方法（如基于规则的启发式或LLM代理）难以应对复杂的推理攻击，存在高误报率，并可能损害SQL查询的可靠性。

Method: 本文提出了\textsc{SafeNlidb}框架，其核心方法包括：1) 一个自动化管道，从零开始生成混合思维链交互数据，将隐式安全推理与SQL生成无缝结合；2) 引入推理预热（reasoning warm-up）和交替偏好优化（alternating preference optimization），以克服直接偏好优化（DPO）的多偏好振荡问题，从而使LLM无需人工标注偏好数据即可通过细粒度推理生成安全感知的SQL。

Result: 实验结果表明，\textsc{SafeNlidb}方法优于更大规模的LLM和理想设置的基线模型，在显著提升安全性的同时，保持了高实用性。

Conclusion: \textsc{SafeNlidb}框架通过新颖的自动化数据生成和偏好优化技术，有效解决了LLM-based NLIDB中的隐私和安全挑战，使LLM能够生成安全感知型SQL，且无需依赖人工偏好标注，表现优于现有方法。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!

</details>


### [342] [Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection](https://arxiv.org/abs/2511.06826)
*Puzhen Su,Haoran Yin,Yongzhu Miao,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 该研究提出DA4ICL框架，通过多样化和对比检索扩展上下文广度，并利用投影向量锚定深化演示信号，显著提升大型语言模型在阿尔茨海默病检测这一细粒度、OOD和低资源任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在阿尔茨海默病（AD）叙事转录检测上面临挑战，因为该任务是预训练中未涵盖的分布外（OOD）任务，且现有演示样本同质性高，削弱了模型的任务认知和上下文感知能力。标准的上下文学习（ICL）很快饱和，而现有的任务向量（TV）方法因注入粒度、强度和位置不匹配而不适用于此任务。

Method: 研究提出了DA4ICL，一个以演示为中心的锚定框架。它通过“多样化和对比检索”（DCR）来扩展上下文宽度，并通过在每个Transformer层进行“投影向量锚定”（PVA）来深化每个演示的信号。

Result: DA4ICL在三个AD基准测试中，相对于标准的ICL和任务向量基线，均取得了显著且稳定的性能提升。

Conclusion: DA4ICL为细粒度、OOD和低资源场景下的大型语言模型适应性提供了一种新的范式。

Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.

</details>


### [343] [CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition](https://arxiv.org/abs/2511.06860)
*Hung-Yang Sung,Chien-Chun Wang,Kuan-Tang Huang,Tien-Hong Lo,Yu-Sheng Tsao,Yung-Chang Hsu,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出CLiFT-ASR，一个基于普通话HuBERT模型的跨语言微调框架，通过两阶段渐进式适应策略（先学习拼音标注，后学习汉字标注），有效提升了低资源台语（闽南语）自动语音识别（ASR）的性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如台语）的ASR因标注数据稀缺而面临挑战。直接在汉字转录上微调难以捕捉详细的语音和声调信息，而仅使用罗马字训练又缺乏词汇和句法覆盖。此外，现有研究很少探索整合这两种标注类型的分阶段策略。

Method: CLiFT-ASR框架以普通话HuBERT模型为基础，采用两阶段渐进式适应策略：第一阶段，从台罗拼音标注中学习声学和声调表示；第二阶段，从汉字转录中捕捉词汇和句法信息。这种渐进式适应旨在实现语音和正字结构之间的有效对齐。

Result: 在TAT-MOE语料库上的实验表明，与强基线模型相比，CLiFT-ASR实现了字符错误率（CER）24.88%的相对降低。

Conclusion: 研究结果表明，CLiFT-ASR为台语ASR提供了一种有效且参数高效的解决方案，并有望应用于其他低资源语言场景。

Abstract: Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.

</details>


### [344] [Inclusion of Role into Named Entity Recognition and Ranking](https://arxiv.org/abs/2511.06886)
*Neelesh Kumar Shukla,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文研究了一种实体角色检测问题，将其建模为命名实体识别（NER）和实体检索/排序任务，并通过自动化方法学习角色和实体的表示，以应对领域特定数据集稀缺的挑战。


<details>
  <summary>Details</summary>
Motivation: 在信息抽取、问答、文本摘要等NLP任务中，实体扮演的角色（根据其行为或属性在特定上下文中）是一个新挑战。当需要根据角色检索实体子集时，如何定义角色和拥有这些角色的实体成为一个问题。

Method: 本文将实体角色检测建模为两种任务：1) 命名实体识别（NER），将角色视为互斥类别并使用序列标注方法。2) 实体检索/排序，将角色视为查询，实体作为集合。针对实体检索中实体和角色间接描述的特点，本文提出了自动化方法来学习代表性词语和短语，并构建角色和实体的表示。研究还探索了句子和文档等不同上下文，并尝试以领域无关的方式利用小型数据集中的信息，以应对领域特定数据集或知识库不足的问题。

Result: 本文提出了一个解决实体角色检测问题的研究框架，通过将该问题转化为NER和实体检索任务，并设计了自动化方法来学习角色和实体的表示。该方法能够利用有限数据，并在不同上下文中进行角色检测。

Conclusion: 本文提出了一种通过将实体角色检测问题建模为NER和实体检索任务来解决该问题的方法。该方法通过自动化学习角色和实体表示，并以领域无关的方式利用小型数据集，有效应对了领域特定数据稀缺的挑战。

Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.

</details>


### [345] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本文介绍了两个大型语言模型（LLM）生成的英语和捷克语语料库，旨在为语言学研究提供资源，以比较人类书写文本与LLM生成文本的差异，并确保其多体裁、内容丰富且与现有语料库具有可比性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了创建一个资源，用于在语言学层面比较人类书写的文本与大型语言模型生成的文本，并强调这些资源应是多体裁、主题和作者丰富，且能与现有的人类创建语料库保持可比性。

Method: 研究方法是使用OpenAI、Anthropic、Alphabet、Meta和DeepSeek等公司的多种LLM（从GPT-3到GPT-4.5）生成文本。这些生成的语料库复制了参考的人类语料库（英语的BE21和捷克语的Koditex），以确保可比性。语料库经过了通用依存关系标准标注（包括分词、词形还原、形态和句法标注）。英语部分包含约2700万词元，捷克语部分包含约2150万词元。

Result: 研究成果是创建了两个新的LLM生成语料库：一个英语语料库（总计约2700万词元）和一个捷克语语料库（总计约2150万词元）。这些语料库是多体裁、主题和作者丰富的，并已按照通用依存关系标准进行标注。它们在CC BY 4.0许可下免费提供下载（标注数据在CC BY-NC-SA 4.0许可下），并可通过捷克国家语料库的搜索界面访问。

Conclusion: 该研究为语言学分析和比较人类与LLM生成文本提供了宝贵的多维度资源。通过提供这些经过精心设计和标注的语料库，它为未来深入探索LLM生成文本的语言特征奠定了基础。

Abstract: This article presents two corpora of English and Czech texts generated with large language models (LLMs). The motivation is to create a resource for comparing human-written texts with LLM-generated text linguistically. Emphasis was placed on ensuring these resources are multi-genre and rich in terms of topics, authors, and text types, while maintaining comparability with existing human-created corpora. These generated corpora replicate reference human corpora: BE21 by Paul Baker, which is a modern version of the original Brown Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in Czech. The new corpora were generated using models from OpenAI, Anthropic, Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and are tagged according to the Universal Dependencies standard (i.e., they are tokenized, lemmatized, and morphologically and syntactically annotated). The subcorpus size varies according to the model used (the English part contains on average 864k tokens per model, 27M tokens altogether, the Czech partcontains on average 768k tokens per model, 21.5M tokens altogether). The corpora are freely available for download under the CC BY 4.0 license (the annotated data are under CC BY-NC-SA 4.0 licence) and are also accessible through the search interface of the Czech National Corpus.

</details>


### [346] [EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers](https://arxiv.org/abs/2511.06890)
*Yilin Jiang,Mingzi Zhang,Xuanyu Yin,Sheng Jin,Suyu Lu,Zuocan Ying,Zengyi Yu,Xiangjie Kong*

Main category: cs.CL

TL;DR: 本文提出了EduGuardBench，一个双组件基准测试，用于评估模拟专业（特别是教师）的大语言模型（SP-LLMs）的专业忠诚度和伦理安全性。研究发现模型性能两极分化，无能是主要失败模式，中型模型可能最脆弱的“规模悖论”，以及最安全的模型能将有害请求转化为可教时刻的“教育转化效应”。


<details>
  <summary>Details</summary>
Motivation: SP-LLMs（尤其是教师角色）对个性化教育至关重要，但确保其专业能力和伦理安全面临严峻挑战。现有基准未能衡量角色扮演的忠诚度或解决教育场景中特有的教学危害。

Method: 本文提出了EduGuardBench，一个双组件基准。它使用“角色扮演忠诚度分数”（RFS）评估专业忠诚度，同时诊断教学专业特有的危害。它还利用基于角色的对抗性提示（针对一般危害和学术不端行为）探测安全漏洞，并使用“攻击成功率”（ASR）和三层“拒绝质量”评估指标进行衡量。对14个主流模型进行了广泛实验。

Result: 实验揭示了14个主流模型性能的显著两极分化。推理导向的模型通常表现出更高的忠诚度，但无能仍是大多数模型的主要失败模式。对抗性测试发现了一个反直觉的“规模悖论”，即中型模型可能最脆弱，挑战了单调的安全假设。关键是，研究识别出强大的“教育转化效应”：最安全的模型擅长通过提供理想的“教育性拒绝”将有害请求转化为可教时刻。这种能力与ASR呈强负相关，揭示了先进AI安全的新维度。

Conclusion: EduGuardBench提供了一个可复现的框架，超越了孤立的知识测试，实现了对教育领域AI的专业、伦理和教学一致性的整体评估。它揭示了部署可信赖教育AI所需的复杂动态，并提出了“教育转化效应”作为AI安全的新维度。

Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.

</details>


### [347] [RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation](https://arxiv.org/abs/2511.06899)
*Haofeng Wang,Yu Zhang*

Main category: cs.CL

TL;DR: 现有LVLMs评估忽略推理过程和模态间关系，本文提出基于树结构的RPTS指标来评估推理过程，并构建RPTS-Eval基准，揭示了LVLMs在多模态推理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型（LVLMs）的评估基准主要通过选择题或简答题形式，未能考虑推理过程。即使有评估推理过程的基准，也过于简化，仅在答案错误时检查，且忽略了模态间关系对推理的影响。

Method: 本文提出推理过程树分数（RPTS），这是一种基于树结构的度量，用于评估推理过程。它将推理步骤组织成推理树，并利用其层级信息为每个推理步骤分配加权忠实度分数，动态调整权重以评估整体推理的正确性并定位失败点。此外，本文构建了一个新的基准RPTS-Eval，包含374张图像和390个推理实例，每个实例都包含可靠的视觉-文本线索作为推理树的叶节点。同时，定义了三种模态间关系来研究它们如何影响推理过程。

Result: 通过评估代表性的LVLMs（如GPT4o、Llava-Next），本文揭示了它们在多模态推理中的局限性，并突出了开源和闭源商业LVLMs之间的性能差异。

Conclusion: RPTS指标和RPTS-Eval基准有助于更全面地评估LVLMs的多模态推理能力，并有望推动该领域的研究进展。

Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.

</details>


### [348] [HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection](https://arxiv.org/abs/2511.06942)
*Fangqi Dai,Xingjian Jiang,Zizhuang Deng*

Main category: cs.CL

TL;DR: 为应对LLM生成内容带来的误报风险，本研究提出了HLPD（人类语言偏好检测）方法，通过HLPO（人类语言偏好优化）使模型更敏感于人类写作模式，从而有效识别LLM修订的文本，尤其在黑盒设置下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 防止LLM生成的可信内容引发的虚假信息和社会问题，开发高效可靠的文本溯源方法至关重要。现有方法在检测完全由LLM生成的文本方面表现出色，但在面对高级LLM输出或对抗性多任务机器修订（尤其在生成模型未知的情况下）时表现不佳。

Method: 基于人类写作具有独特风格模式的假设，提出HLPD（人类语言偏好检测）。HLPD采用HLPO（人类语言偏好优化）这一基于奖励的对齐过程，将评分模型的token分布向人类写作方向偏移，使模型对人类写作更敏感，从而增强机器修订文本的识别能力。通过一个利用五维提示生成器和多个高级LLM创建多样修订场景的对抗性多任务评估框架进行测试。

Result: 在检测由GPT系列模型修订的文本时，HLPD在AUROC上比ImBD相对提升15.11%，比Fast-DetectGPT高45.56%。在评估由高级LLM生成的文本时，HLPD取得了最高的平均AUROC，比ImBD高5.53%，比Fast-DetectGPT高34.14%。

Conclusion: HLPD通过优化模型对人类语言偏好的敏感度，显著提高了在黑盒设置下检测机器修订和高级LLM生成文本的能力，有效解决了现有方法在复杂场景下的不足。

Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.

</details>


### [349] [SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs](https://arxiv.org/abs/2511.07001)
*Zhenliang Zhang,Xinyu Hu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出SCOPE，一种推理时方法，利用稀疏自编码器识别并抑制大语言模型中与版权相关的语义子空间，从而在不影响模型通用能力的情况下缓解版权侵权。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有时会无意中复制受版权保护的内容，给下游应用带来法律风险。现有的防御方法主要集中在表面级别的token匹配，依赖外部黑名单或过滤器，增加了部署复杂性，且可能忽略语义层面的改写泄露。研究者希望开发一种内在的、基于语义空间的控制方法。

Method: SCOPE是一种推理时方法，无需参数更新或辅助过滤器。它利用稀疏自编码器（SAE）将隐藏状态投影到高维、近乎单义的语义空间中。在此表示中，SCOPE识别出一个对版权敏感的子空间，并在解码过程中钳制（clamp）其激活值。

Result: 在广泛认可的基准测试中，实验表明SCOPE在不损害模型通用能力的情况下有效缓解了版权侵权。进一步的可解释性分析证实，被隔离的子空间确实捕捉到了高级语义信息。

Conclusion: SCOPE通过内在的语义空间控制，提供了一种无需外部依赖且不牺牲通用性的有效推理时方法，能够显著缓解大型语言模型中的版权侵权问题。

Abstract: Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.

</details>


### [350] [Automated Circuit Interpretation via Probe Prompting](https://arxiv.org/abs/2511.07002)
*Giuseppe Birardi*

Main category: cs.CL

TL;DR: 本文提出了一种名为“探针提示”（probe prompting）的自动化管道，用于将归因图转换为紧凑且可解释的子图，以理解神经网络中的特征路径，显著减少了手动分析的时间和复杂性。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性旨在通过识别哪些学习到的特征介导特定行为来理解神经网络。归因图揭示了这些特征路径，但解释它们需要大量手动分析，一个提示可能需要经验丰富的电路追踪者大约2小时，效率低下。

Method: 研究人员开发了“探针提示”自动化管道。该方法从一个种子提示和目标logit开始，选择高影响特征，生成针对概念但上下文变化的探针，并根据跨提示激活签名将特征分组为语义（Semantic）、关系（Relationship）和“说-X”（Say-X）类别，使用透明的决策规则构建概念对齐的超节点。

Result: 在五个提示（包括经典的“首都”电路）上，探针提示生成的子图在压缩复杂性的同时保持了高解释覆盖率（完整性0.83，替换率0.54）。与几何聚类基线相比，概念对齐的组表现出更高的行为一致性：峰值token一致性高出2.3倍（0.425 vs 0.183），激活模式相似性高出5.8倍（0.762 vs 0.130），尽管几何紧凑度较低。实体交换测试揭示了分层结构：早期层特征稳健地迁移（64%迁移率，平均层6.3），而晚期层“说-X”特征专门用于输出提升（平均层16.4），支持了Transformer计算的“骨干和专业化”视图。

Conclusion: “探针提示”提供了一种高效、自动化的方法来解释神经网络的归因图，通过生成概念对齐的紧凑子图，显著提高了可解释性。它不仅在保持解释能力的同时压缩了复杂度，还揭示了Transformer计算中特征的分层结构和专业化模式。

Abstract: Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.

</details>


### [351] [Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs](https://arxiv.org/abs/2511.07003)
*Yingfeng Luo,Ziqiang Xu,Yuxuan Ouyang,Murun Yang,Dingyang Lin,Kaiyan Chang,Tong Zheng,Bei Li,Peinan Feng,Quan Du,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文介绍了LMT，一套以中英为中心的大规模多语言翻译模型，覆盖60种语言和234个翻译方向。通过提出“方向性退化”现象和“策略性下采样”方法，以及“并行多语言提示”，LMT在同等语言覆盖的模型中实现了SOTA性能，其4B模型甚至超越了更大的Aya-101-13B和NLLB-54B模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言机器翻译（MMT）方面取得了显著进展，但其广泛的语言覆盖、一致的翻译质量以及以英语为中心的偏见仍然是未解决的挑战。

Method: 1. 引入LMT模型，以中文和英文为中心，覆盖60种语言和234个翻译方向。
2. 识别并提出“方向性退化”现象，即对称多向微调数据过度强调反向翻译（X → 英/中），导致多对一映射和翻译质量下降。
3. 提出“策略性下采样”方法来缓解方向性退化。
4. 设计“并行多语言提示”（PMP），利用类型学相关的辅助语言来增强跨语言迁移。
5. 进行了严格的数据整理和精细的适应策略。

Result: LMT在同等语言覆盖的模型中实现了最先进的性能（SOTA）。其中，LMT的4B模型（LMT-60-4B）以显著优势超越了更大的Aya-101-13B和NLLB-54B模型。研究团队发布了四种不同尺寸（0.6B/1.7B/4B/8B）的LMT模型。

Conclusion: LMT通过解决多语言翻译中的关键挑战，提供了强大的基线模型，有望促进未来在包容、可扩展和高质量MMT方面的研究。

Abstract: Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale \textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.

</details>


### [352] [Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks](https://arxiv.org/abs/2511.07025)
*Yauhen Babakhin,Radek Osmulski,Ronay Ak,Gabriel Moreira,Mengyao Xu,Benedikt Schifferer,Bo Liu,Even Oldridge*

Main category: cs.CL

TL;DR: 本文介绍了llama-embed-nemotron-8b，一个开源文本嵌入模型，在多语言大规模文本嵌入基准（MMTEB）上取得了最先进的性能，并详细公开了其训练数据和方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现有模型表现强劲，但其训练数据和方法往往不完全公开。本研究旨在通过开发一个完全开源的模型来解决这一透明度问题。

Method: 该模型采用1610万查询-文档对的创新数据混合，其中包含770万公共数据集样本和840万由开源大型语言模型生成的合成样本。研究还进行了详细的消融研究，分析了对比损失实现、合成数据生成策略和模型合并的影响，并使其成为一个指令感知模型。

Result: llama-embed-nemotron-8b在所有主要的嵌入任务（包括检索、分类和语义文本相似性）上均表现出色，尤其在多语言场景（如低资源语言和跨语言设置）中表现卓越，并在MMTEB排行榜上实现了最先进的性能。

Conclusion: llama-embed-nemotron-8b模型凭借其顶级的性能、广泛的适用性和用户驱动的灵活性，结合其开源特性，能够作为一个通用的文本嵌入解决方案。

Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.

</details>


### [353] [A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation](https://arxiv.org/abs/2511.07010)
*Siddharth Betala,Kushan Raj,Vipul Betala,Rohan Saswade*

Main category: cs.CL

TL;DR: 本文提出了一个用于WAT 2025英印多模态翻译任务（仅文本）的系统BLEU Monday。该系统采用两阶段方法：首先通过一个视觉增强的判断-纠正器管道自动检测并纠正训练数据中的错误，然后使用LoRA对IndicTrans2模型进行参数高效微调。结果显示，经过纠正的数据显著提升了BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在参与WAT 2025英印多模态翻译任务的纯文本翻译子任务，并解决训练数据中的质量问题，以提升翻译模型的性能。

Method: 该方法包括两个阶段：
1.  **自动化错误检测和纠正**：引入一个视觉增强的判断-纠正器管道，利用多模态语言模型识别并纠正训练数据中的翻译错误。判断组件将翻译分为正确、视觉模糊（需要图像上下文）或翻译错误。GPT-4o-mini用于处理视觉歧义，IndicTrans2用于处理纯翻译质量问题。该管道处理了28,928个训练样本，平均纠正了每个语言17.1%的字幕。
2.  **参数高效模型微调**：使用低秩适应（LoRA）技术，在原始和纠正后的数据集上微调IndicTrans2 en-indic 200M蒸馏模型。

Result: 自动化管道平均纠正了每个语言17.1%的训练字幕。在纠正后的数据上进行训练，持续带来了BLEU分数的提升：
*   英语-孟加拉语：评估集提高+1.30 (42.00 -> 43.30)，挑战集提高+0.70 (44.90 -> 45.60)。
*   英语-奥里亚语：评估集提高+0.60 (41.00 -> 41.60)。
*   英语-印地语：挑战集提高+0.10 (53.90 -> 54.00)。

Conclusion: 该研究提出的两阶段方法，特别是通过视觉增强的判断-纠正器管道改进训练数据质量，能够显著提升英印文本翻译的性能。在纠正后的数据上进行模型微调，稳定地带来了BLEU分数的提高，证明了数据质量对翻译任务的重要性。

Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).

</details>


### [354] [Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011)
*Anastasiia Tokareva,Judith Dineley,Zoe Firth,Pauline Conde,Faith Matcham,Sara Siddi,Femke Lamers,Ewan Carr,Carolin Oetzmann,Daniel Leightley,Yuezhou Zhang,Amos A. Folarin,Josep Maria Haro,Brenda W. J. H. Penninx,Raquel Bailon,Srinivasan Vairavan,Til Wykes,Richard J. B. Dobson,Vaibhav A. Narayan,Matthew Hotopf,Nicholas Cummins,The RADAR-CNS Consortium*

Main category: cs.CL

TL;DR: 本研究对多语言纵向口语数据进行探索性分析，以识别与重度抑郁症(MDD)症状严重程度相关的可解释词汇特征。结果显示，在英语和荷兰语中发现了一些关联，但在西班牙语中未发现，且词汇特征的预测能力接近偶然水平，表明需要进一步的研究。


<details>
  <summary>Details</summary>
Motivation: 口语在临床预约之间通过移动设备捕获，具有客观、更规律地评估症状严重程度和早期检测重度抑郁症复发的潜力。然而，迄今为止的研究主要集中在非临床横断面书面语言样本上，使用了复杂且可解释性有限的机器学习方法。

Method: 本研究对RADAR-MDD研究中从英国、荷兰和西班牙的586名参与者收集的5,836份录音和PHQ-8评估数据进行了初步探索性分析。使用线性混合效应模型识别与MDD症状严重程度相关的可解释词汇特征。同时，利用这些可解释特征和高维向量嵌入测试了四种回归机器学习模型的预测性能。

Result: 在英语数据中，MDD症状严重程度与包括词汇多样性和绝对主义语言在内的7个特征相关。在荷兰语中，观察到与每句话的单词数和积极词频率的关联；在西班牙语录音中未观察到任何关联。词汇特征和向量嵌入的预测能力在所有语言中均接近偶然水平。局限性包括非英语语音样本量较小、方法选择（如启发提示）以及非英语语言NLP工具的缺乏。

Conclusion: 为了理解词汇标记在临床研究和实践中的价值，需要在更大规模、涵盖多种语言、采用改进协议以及考虑个体内部和个体之间语言变异的机器学习模型进行进一步研究。

Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.

</details>


### [355] [Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065)
*Brage Eilertsen,Røskva Bjørgfinsdóttir,Francielle Vargas,Ali Ramezani-Kebrya*

Main category: cs.CL

TL;DR: 本文提出了一种名为监督理性注意力（SRA）的框架，通过将模型注意力与人类理由对齐，显著提高了仇恨言论检测系统的可解释性和公平性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的不透明性对仇恨言论检测系统的伦理部署构成了重大挑战。研究旨在解决这一局限性，提升模型的可解释性和公平性。

Method: SRA框架将监督注意力机制集成到基于Transformer的分类器中。它优化了一个联合目标函数，该函数结合了标准分类损失和对齐损失项，以最小化注意力权重与人类标注理由之间的差异。

Result: 在英语和葡萄牙语的仇恨言论基准测试中，SRA实现了比现有基线高2.4倍的可解释性，并生成了更忠实、与人类对齐的词元级解释。在公平性方面，SRA在所有衡量标准上都达到了有竞争力的公平性，在检测针对特定身份群体的有毒帖子方面表现第二，同时在其他指标上保持了可比的结果。

Conclusion: 研究结果表明，将人类理由融入注意力机制可以增强模型的可解释性和忠实性，同时不损害其公平性。

Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.

</details>


### [356] [Importance-Aware Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2511.07074)
*Tingyu Jiang,Shen Li,Yiyao Song,Lan Zhang,Hualei Zhu,Yuan Zhao,Xiaohang Xu,Kenjiro Taura,Hao Henry Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为MIWV的新指标，通过衡量模型在上下文学习（ICL）中的响应差异来量化指令数据的重要性，旨在为特定大型语言模型（LLM）选择最高质量的指令数据，以最大限度地提升指令微调性能。实验证明，仅使用基于MIWV选择的1%数据就能超越使用完整数据集的训练效果。


<details>
  <summary>Details</summary>
Motivation: 指令微调对LLM的性能和效率至关重要，其成功取决于指令数据质量和LLM自身能力。现有研究表明少量高质量数据即可取得良好效果，但当前方法主要关注计算数据质量得分。研究动机在于需要一种更有效的方法，能够为特定LLM选择那些能最大化提升指令微调性能的高质量数据，而非仅仅是泛泛的数据质量评估。

Method: 本文提出了一种新颖的指标——模型指令弱点值（Model Instruction Weakness Value, MIWV），用于量化指令数据在提升模型能力方面的重要性。MIWV通过测量模型在使用上下文学习（ICL）时响应的差异来得出，旨在识别出对增强指令微调性能最有益的数据。

Result: 实验结果表明，仅基于MIWV选择前1%的数据进行训练，其性能可以超越使用完整数据集进行训练。此外，该方法超越了现有专注于数据质量评分的数据选择研究，提供了强有力的经验证据支持其有效性。

Conclusion: MIWV是一种有效的新型指标，能够量化指令数据的重要性并识别出对特定LLM最有益的数据。通过基于MIWV选择少量高质量数据，可以显著提升指令微调性能，甚至超越使用全部数据进行训练的效果，为高质量指令数据选择提供了一种新范式。

Abstract: Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.

</details>


### [357] [Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data](https://arxiv.org/abs/2511.07044)
*Mihael Arcan,David-Paul Niland*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型、经典机器学习和Transformer模型在DAIC-WOZ数据集上进行焦虑、抑郁和压力检测的性能，并利用合成数据解决类别不平衡问题，结果显示Transformer模型和合成数据在提高检测效果方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 全球超过五分之一的成年人受精神健康障碍影响，但由于症状表达的微妙和多样性，从文本中检测这些疾病仍然具有挑战性。

Method: 研究比较了大型语言模型（如Llama和GPT）、经典机器学习方法以及基于Transformer的架构（如BERT、XLNet和Distil-RoBERTa）。模型在DAIC-WOZ临床访谈数据集上针对焦虑、抑郁和压力分类进行了微调，并应用合成数据生成来缓解类别不平衡问题。

Result: Distil-RoBERTa在GAD-2任务上实现了最高的F1分数（0.883），而XLNet在PHQ任务上表现最佳（F1高达0.891）。对于压力检测，零样本合成方法（SD+Zero-Shot-Basic）达到了0.884的F1和0.886的ROC AUC。研究结果表明基于Transformer的模型是有效的，合成数据在提高召回率和泛化能力方面具有价值，但需要仔细校准以防止精度损失。

Conclusion: 本研究强调了结合先进语言模型和数据增强技术在文本自动精神健康评估方面的潜力，并指出在实际应用中需要平衡精度与召回率。

Abstract: Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.

</details>


### [358] [EmoBang: Detecting Emotion From Bengali Texts](https://arxiv.org/abs/2511.07077)
*Abdullah Al Maruf,Aditi Golder,Zakaria Masud Jiyad,Abdullah Al Numan,Tarannum Shaila Zaman*

Main category: cs.CL

TL;DR: 本文为孟加拉语情感检测引入了一个新的八类情感数据集，并提出了两种高性能模型（CRNN混合模型和AdaBoost-BERT集成模型），同时建立了该领域的首个综合基准。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为世界第四大语言，在情感检测方面资源匮乏，缺乏大型标准化数据集和先进模型，现有研究多采用传统机器学习方法，性能有限。

Method: 研究构建了一个新的八类情感标注的孟加拉语情感数据集。提出了两种模型：(i) 混合卷积循环神经网络（CRNN）模型（EmoBangHybrid）和 (ii) AdaBoost-双向编码器表示转换器（BERT）集成模型（EmoBangEnsemble）。此外，还评估了六种基线模型、五种特征工程技术，并测试了零样本和少样本大型语言模型（LLMs）。

Result: EmoBangHybrid模型和EmoBangEnsemble模型分别达到了92.86%和93.69%的准确率，均优于现有方法。本文首次为孟加拉语情感检测建立了全面的基准。

Conclusion: 所提出的模型在孟加拉语情感检测方面表现出色，并为未来的研究建立了强大的基线。

Abstract: Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.

</details>


### [359] [When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction](https://arxiv.org/abs/2511.07055)
*Katharina Beckh,Stefan Rüping*

Main category: cs.CL

TL;DR: 特征归因方法通常仅提供最小充分证据。本文发现，在医疗数据集上，单个模型难以识别完整证据，而通过聚合多个模型的证据，可将完整证据的召回率从约0.60提升至约0.86。


<details>
  <summary>Details</summary>
Motivation: 当前的特征归因方法仅提供“最小充分证据”，这在需要识别所有贡献特征（即“完整证据”）的应用（如合规性、编目）中是不足的。

Method: 1. 在含有人工标注完整证据的医疗数据集上进行案例研究。2. 评估单个模型识别完整证据的能力。3. 采用集成方法聚合多个模型的证据。4. 分析召回率-精确率权衡、训练中证据的作用以及带有置信度阈值的动态集成。

Result: 1. 单个模型通常只能恢复完整证据的子集。2. 聚合多个模型的证据能显著提高证据召回率，从最佳单一模型的约0.60提高到集成模型的约0.86。3. 进一步分析了召回率-精确率权衡、训练中证据的作用及动态集成。

Conclusion: 对于需要完整证据的应用场景，单独的特征归因模型不足以满足需求。通过集成多个模型聚合证据可以显著提高完整证据的召回率。

Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.

</details>


### [360] [Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora](https://arxiv.org/abs/2511.07080)
*Khalil Hennara,Ahmad Bastati,Muhammad Hreden,Mohamed Motasim Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 本文介绍了一个名为Wasm的管道，用于处理Common Crawl数据集，以创建一个新的阿拉伯语多模态数据集。该数据集独特地提供Markdown输出，保留了文档结构，旨在填补高质量阿拉伯语多模态数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和多模态模型（LMMs）的性能严重依赖于预训练数据集的质量和规模。现有研究表明，在图像和文本交错的自然文档上训练的多模态模型表现更优。然而，阿拉伯语领域缺乏高质量、能保留文档结构的多模态数据集，这限制了该领域的发展。

Method: 研究人员开发了一个名为Wasm的管道，用于处理Common Crawl数据集。该管道旨在创建新的阿拉伯语多模态数据集，其独特之处在于提供Markdown输出，从而保留了网页内容的结构完整性。该方法灵活适用于纯文本和多模态预训练场景。论文还对该数据处理管道与现有主要数据集所用的管道进行了全面的比较分析。

Result: 研究成功创建了一个新的阿拉伯语多模态数据集，该数据集以Markdown格式输出，并有效保留了网络内容的结构完整性。该数据集既适用于纯文本预训练，也适用于多模态预训练。此外，研究人员还提供了与现有主要数据集处理管道的比较分析，并公开了代表性数据集转储和多模态处理管道，以支持未来的研究。

Conclusion: 所提出的Wasm管道及其生成的阿拉伯语多模态数据集填补了该领域高质量、结构化数据的空白。通过提供保留文档结构的Markdown输出，该数据集将极大地促进阿拉伯语LLMs和LMMs的未来研究和发展。

Abstract: The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.

</details>


### [361] [Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought](https://arxiv.org/abs/2511.07124)
*Zhikang Chen,Sen Cui,Deheng Ye,Yu Zhang,Yatao Bian,Tingting Zhu*

Main category: cs.CL

TL;DR: 本文提出EBM-CoT框架，通过能量基模型（EBM）校准大语言模型（LLM）的潜在思维表示，使其推理路径更具一致性，从而提升多步推理的准确性和效率，解决了传统CoT方法中误差传播和隐式推理缺乏一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的显式思维链（CoT）推理易受离散token限制，导致误差传播和表达能力不足，推理过程僵硬且不一致。虽然隐式或连续推理缓解了部分问题，但缺乏明确机制来强制推理步骤之间的一致性，导致推理路径发散和结果不稳定。

Method: 本文提出了EBM-CoT（Energy-Based Chain-of-Thought Calibration）框架。该方法通过一个能量基模型（EBM）来精炼潜在的思维表示，动态地将潜在推理轨迹调整到嵌入空间中能量更低、一致性更高的区域，从而提高推理的准确性和一致性，且无需修改基础语言模型。

Result: 在数学、常识和符号推理基准上的大量实验表明，所提出的框架显著增强了LLM多步推理的一致性和效率。

Conclusion: EBM-CoT通过能量基模型对LLM的潜在思维进行校准，有效解决了显式CoT的误差传播和隐式推理缺乏一致性的问题，显著提升了LLM多步推理的准确性和稳定性。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.

</details>


### [362] [LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging](https://arxiv.org/abs/2511.07129)
*Seungeon Lee,Soumi Das,Manish Gupta,Krishna P. Gummadi*

Main category: cs.CL

TL;DR: LoGo是一个无需训练的框架，它能动态地在实例级别选择和合并LoRA适配器，以应对多样化任务，并在多个NLP基准测试中超越了基于训练的基线。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA适配器通常针对单一任务训练，限制了其在真实世界中处理多样化输入的适用性。现有的多LoRA组合方法需要标记数据或额外的任务特定训练，这在大规模应用中成本高昂。

Method: LoGo是一个无需训练的框架，它在实例级别动态选择和合并LoRA适配器。它通过单次前向传播从LoRA适配器中提取信号，实时识别最相关的适配器并确定它们的贡献。

Result: LoGo在5个NLP基准测试、27个数据集和3个模型家族中，在某些任务上比基于训练的基线表现高出高达3.6%，同时在其他任务上保持竞争力并维持推理吞吐量。

Conclusion: LoGo展示了其在处理多样化和不可预测领域中的有效性和实用性，无需额外要求即可动态适应，使其成为一种高效的LoRA应用方案。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.

</details>


### [363] [TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine](https://arxiv.org/abs/2511.07148)
*Zihao Cheng,Yuheng Lu,Huaiqian Ye,Zeming Liu,Minqi Wang,Jingjing Liu,Zihan Li,Wei Fan,Yuanfang Guo,Ruiji Fu,Shifeng She,Gang Wang,Yunhong Wang*

Main category: cs.CL

TL;DR: 本文介绍了TCM-Eval，首个中医动态基准测试集；提出了SI-CoTE方法，用于自主增强训练数据；并开发了专为中医设计的LLM——ZhiMingTang，其性能显著超越了人类执业医师的及格线。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在现代医学中表现出色，但由于缺乏标准化基准和高质量训练数据，它们在中医药（TCM）领域的应用受到严重限制。

Method: 研究团队构建了TCM-Eval，一个动态可扩展的中医基准测试集，该测试集从国家医学执照考试中精选并经中医专家验证。此外，他们构建了一个大规模训练语料库，并提出了自迭代思维链增强（SI-CoTE）方法，通过拒绝采样自主丰富带有验证推理链的问答对，实现了数据和模型的共同演进。利用这些增强数据，开发了名为ZhiMingTang（ZMT）的专为中医设计的LLM。

Result: 开发的ZhiMingTang（ZMT）模型显著超越了人类执业医师的及格线。研究团队还发布了一个公共排行榜，以鼓励未来的研究和开发。

Conclusion: 本研究成功解决了LLM在中医领域应用面临的基准和数据挑战，通过TCM-Eval基准和SI-CoTE数据增强方法，开发出性能卓越的中医LLM ZhiMingTang，并为未来的社区研究提供了开放资源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.

</details>


### [364] [Discourse Graph Guided Document Translation with Large Language Models](https://arxiv.org/abs/2511.07230)
*Viet-Thanh Pham,Minghan Wang,Hao-Han Liao,Thuy-Trang Vu*

Main category: cs.CL

TL;DR: TransGraph是一个篇章引导的文档翻译框架，通过构建结构化篇章图并选择性地利用图邻域信息，有效解决了大型语言模型在长文档翻译中的连贯性挑战，同时显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型应用于完整文档翻译面临挑战，包括难以捕捉长距离依赖和维持篇章连贯性。现有的代理式机器翻译系统虽然通过多代理协调和持久记忆缓解了上下文窗口限制，但它们需要大量计算资源且对记忆检索策略敏感。

Method: 本文提出了TransGraph，一个篇章引导的框架。它通过结构化篇章图显式地建模块间关系，并选择性地根据相关的图邻域信息来翻译每个文本段，而非依赖顺序或穷尽的上下文。

Result: 在涵盖六种语言和不同领域的三项文档级机器翻译基准测试中，TransGraph在翻译质量和术语一致性方面始终优于强大的基线模型，同时显著降低了令牌开销。

Conclusion: TransGraph通过其篇章图引导和选择性上下文利用机制，有效提升了文档级机器翻译的质量和一致性，并显著降低了计算成本，为全文档翻译提供了一个高效且高质量的解决方案。

Abstract: Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.

</details>


### [365] [AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning](https://arxiv.org/abs/2511.07166)
*Meiyun Wang,Charin Polpanumas*

Main category: cs.CL

TL;DR: AdaRec是一个利用大语言模型（LLM）进行自适应个性化推荐的少样本上下文学习框架，通过叙事画像和双通道推理，在少样本和零样本场景下显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统面临特征工程复杂、少样本场景表现不佳以及跨任务适应性差等问题。该研究旨在开发一个能够自适应、无需手动特征工程并能快速适应不同任务的个性化推荐框架。

Method: AdaRec框架引入了“叙事画像”，将用户-物品交互转化为自然语言表示，以实现统一任务处理和增强可读性。其核心是“双变量推理范式”，采用双通道架构：整合“横向行为对齐”（发现同伴驱动模式）与“纵向因果归因”（突出用户偏好背后的决定性因素）。该方法通过语义表示消除了手动特征工程，并支持最小监督下的快速跨任务适应。

Result: 在真实电商数据集上，AdaRec在少样本设置中比机器学习模型和基于LLM的基线模型性能提升高达8%。在零样本场景下，它比专家手工画像的性能提高了19%，显示出在交互数据极少的情况下对长尾个性化的有效性。此外，通过AdaRec生成的合成数据进行轻量级微调，其性能与完全微调的模型相当，突显了其在不同任务中的效率和泛化能力。

Conclusion: AdaRec通过创新的叙事画像和双变量推理范式，在少样本、零样本和长尾个性化推荐场景中展现出卓越的性能和效率。它消除了手动特征工程，并能快速适应多种任务，是基于LLM的推荐系统领域的一个重要进展。

Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.

</details>


### [366] [Who Is the Story About? Protagonist Entity Recognition in News](https://arxiv.org/abs/2511.07296)
*Jorge Gabín,M. Eduardo Ares,Javier Parapar*

Main category: cs.CL

TL;DR: 本文引入了主角实体识别（PER）任务，旨在识别新闻故事中驱动叙事的核心组织。通过比较大型语言模型（LLM）与专家标注，并利用LLM进行大规模自动标注，证明了PER的可行性以及LLM在识别叙事重要性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 传统命名实体识别（NER）对所有提及的实体一视同仁，无法区分哪些实体真正推动了新闻叙事，这限制了下游任务对事件显著性、影响力或叙事焦点的理解。

Method: 1. 提出主角实体识别（PER）任务，用于识别新闻故事的核心组织。2. 在黄金语料库上，将LLM的预测与四位专家标注进行比较，以建立标注者间一致性和人-LLM一致性。3. 利用先进的LLM通过NER引导的提示，自动标注大规模新闻集合，生成高质量的监督数据。4. 评估其他LLM在减少上下文和无明确候选指导的情况下，是否仍能推断出正确的主角。

Result: 研究结果表明，PER是叙事中心信息提取的、可行且有意义的扩展。此外，经过引导的LLM能够在大规模数据集上近似人类对叙事重要性的判断。

Conclusion: 主角实体识别（PER）是信息提取领域一个有价值的新任务。通过NER引导的LLM能够有效地、大规模地识别新闻故事中的核心组织，并近似人类的判断。

Abstract: News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.

</details>


### [367] [More Agents Helps but Adversarial Robustness Gap Persists](https://arxiv.org/abs/2511.07112)
*Khashayar Alavi,Zhastay Yeltay,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 研究发现，LLM代理协作在数学问答中能提升准确性，但面对对抗性输入（尤其是人类错误类型），其对抗性鲁棒性差距依然存在，且不会随代理数量增加而消除。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理协作在数学问答中表现出优于单个LLM的性能，但其对抗性输入（如标点噪声和真实世界错别字）的鲁棒性如何，仍是一个未被充分探索的问题。

Method: 研究采用统一的采样-投票框架（Agent Forest），评估了六个开源模型（Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B）。测试涵盖四个数学基准（GSM8K, MATH, MMLU-Math, MultiArith），并引入了三种强度的标点噪声以及真实世界和类人错别字（WikiTypo, R2ATA）作为对抗性扰动。代理数量n从1到25不等（1, 2, 5, 10, 15, 20, 25）。

Result: (1) 噪声类型影响显著：标点噪声的危害随强度增加，而人类错别字（WikiTypo, R2ATA）是主要瓶颈，导致最大的准确性差距和最高的攻击成功率（ASR），即使代理数量众多也如此。(2) 协作能可靠地提高准确性，代理数量n从1增加到5时收益最大，超过10个代理后收益递减。然而，无论代理数量多少，对抗性鲁棒性差距始终存在。

Conclusion: LLM代理协作虽然能提升数学问答的准确性，但面对对抗性输入（特别是人类错别字），其固有的鲁棒性弱点并未因代理数量的增加而得到弥补。人类错别字仍然是LLM代理鲁棒性的主要挑战。

Abstract: When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.

</details>


### [368] [ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding](https://arxiv.org/abs/2511.07311)
*Tuan-Dung Le,Shohreh Haddadan,Thanh Q. Thieu*

Main category: cs.CL

TL;DR: 本文提出一种利用大型语言模型扩展医学缩写并结合一致性训练的数据增强技术ACE-ICD，显著提升了ICD自动编码的性能，解决了现有方法忽视医学缩写的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的ICD自动编码方法主要关注代码层级和同义词理解，但忽略了临床笔记中普遍存在的医学缩写，而这些缩写是ICD代码推断的关键因素。

Method: 1. 提出一种新颖有效的数据增强技术，利用大型语言模型（LLMs）将医学缩写扩展为完整形式，用于模型训练。2. 引入一致性训练，通过强制原始文档和增强文档之间的预测一致性来正则化模型。

Result: 在MIMIC-III数据集上的广泛实验表明，所提出的ACE-ICD方法在多种设置下（包括常见代码、稀有代码和全代码分配）均取得了新的最先进性能。

Conclusion: ACE-ICD方法通过有效处理医学缩写，显著提高了ICD自动编码的准确性，并在多个评估维度上建立了新的SOTA。

Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.

</details>


### [369] [Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?](https://arxiv.org/abs/2511.07162)
*Lynn Greschner,Meike Bauer,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 本研究首次系统比较了论证说服力预测中情绪模型的作用，发现相比于传统情绪类别，主观评估理论（appraisal theories）能更显著地提升对论证主观说服力的预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注论证中情绪的整体强度和类别，但本研究认为论证引起的情绪是主观的，取决于接收者的目标、标准、先验知识和立场。评估理论能将主观认知评估与情绪联系起来，虽已用于事件中心的情绪分析，但其在评估论证说服力方面的适用性尚未被探索。

Method: 研究利用最近发布的ContArgA语料库，进行零样本提示实验。评估了黄金标注和预测情绪以及评估对主观说服力标签评估的重要性。通过比较不同情绪模型对说服力预测的影响来验证评估理论的适用性。

Result: 研究发现，虽然分类情绪信息确实能改善说服力预测，但评估（appraisals）带来的改进更为显著。这表明评估理论在捕捉论证主观情绪和说服力方面具有优势。

Conclusion: 这项工作首次系统比较了用于说服力预测的情绪模型，证明了评估理论的优势，为计算论证领域的理论和实践应用提供了见解。

Abstract: The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.

</details>


### [370] [Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification](https://arxiv.org/abs/2511.07304)
*Sourav Saha,K M Nafi Asib,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 该论文介绍了“Retriv”团队在孟加拉语仇恨言论识别共享任务中的参与，利用Transformer模型集成和加权多任务框架，在仇恨类型分类、目标群体识别及联合检测子任务上取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语仇恨言论识别是一项具有社会影响力但语言上充满挑战的任务。该研究作为BLP Workshop, IJCNLP-AACL 2025“孟加拉语多任务仇恨言论识别”共享任务的一部分，旨在推动该领域在低资源语境下的进展。

Method: 对于子任务1A（仇恨类型分类）和1B（目标群体识别），团队采用了Transformer模型（BanglaBERT、MuRIL、IndicBERTv2）的软投票集成。对于子任务1C（类型、严重性和目标的联合检测），则训练了三个多任务变体并通过加权投票集成其预测。

Result: 系统在子任务1A中获得了72.75%的micro-f1分数，在1B中获得72.69%的micro-f1分数，在1C中获得72.62%的加权micro-f1分数。在共享任务排行榜上，这些结果分别对应第9、第10和第7名。

Conclusion: 研究结果突显了Transformer模型集成和加权多任务框架在推进低资源语境下孟加拉语仇恨言论检测方面的潜力。所有实验脚本均已公开。

Abstract: This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the "Bangla Multi-task Hate Speech Identification" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.

</details>


### [371] [When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs](https://arxiv.org/abs/2511.07318)
*Shaowen Wang,Yiqi Dong,Ruinian Chang,Tansheng Zhu,Yuebo Sun,Kaifeng Lyu,Jian Li*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型(LLMs)中由虚假关联（训练数据中表面上统计显著的特征与属性关联）引起的一类关键幻觉，这些幻觉难以检测和消除，需要新的应对方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs取得了显著进展，但仍存在幻觉现象，即生成貌似合理但错误的回应。本文旨在探讨一类此前未充分探索的、由训练数据中虚假关联驱动的幻觉。

Method: 通过系统控制的合成实验和对最先进的开源及专有LLMs（包括GPT-5）的实证评估，研究了虚假关联导致的幻觉。此外，还进行了理论分析，以阐明这些统计偏差为何会破坏基于置信度的检测技术。

Result: 研究发现，虚假关联导致的幻觉具有高度自信、不受模型规模影响、能规避当前的检测方法（如基于置信度的过滤和内部状态探测），甚至在拒绝微调后依然存在。理论分析进一步解释了这些统计偏差为何从根本上破坏了基于置信度的检测技术。

Conclusion: 研究结果强调，迫切需要专门设计的新方法来解决由虚假关联引起的幻觉问题。

Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.

</details>


### [372] [FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation](https://arxiv.org/abs/2511.07322)
*Song Jin,Shuqi Li,Shukun Zhang,Rui Yan*

Main category: cs.CL

TL;DR: 本文首次提出了股票研究报告（ERR）生成任务，并发布了开源基准FinRpt，包括数据集构建流程、11个评估指标和一个多智能体框架FinRpt-Gen，旨在推动ERR自动化生成领域的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在金融任务中表现出色，但其在完全自动化生成股票研究报告方面的应用仍是空白。该领域面临数据稀缺和缺乏统一评估指标的挑战。

Method: 本文首先明确定义了ERR生成任务。接着，提出了开源评估基准FinRpt，包含一个整合7种金融数据类型并自动生成高质量ERR数据集的数据构建流程，以及一个包含11个指标的综合评估系统。此外，还提出了一个名为FinRpt-Gen的多智能体框架，并使用监督微调（SFT）和强化学习（RL）在该数据集上训练了多个基于LLM的智能体。

Result: 实验结果表明，FinRpt基准的数据质量和评估指标有效性得到了验证，并且FinRpt-Gen框架表现出强大的性能，展示了其在ERR生成领域的巨大潜力。

Conclusion: 本文首次系统地提出了ERR生成任务，并构建了FinRpt基准和FinRpt-Gen框架，为自动化股票研究报告的生成提供了有力的工具和评估标准，有望推动该领域的创新和发展。

Abstract: While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.

</details>


### [373] [RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments](https://arxiv.org/abs/2511.07317)
*Zhiyuan Zeng,Hamish Ivison,Yiping Wang,Lifan Yuan,Shuyue Stella Li,Zhuorui Ye,Siting Li,Jacqueline He,Runlong Zhou,Tong Chen,Chenyang Zhao,Yulia Tsvetkov,Simon Shaolei Du,Natasha Jaques,Hao Peng,Pang Wei Koh,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 本文提出RLVE方法，通过自适应可验证环境扩展语言模型的强化学习，显著提升了泛化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型强化学习面临静态数据分布导致学习信号消失（问题过易或过难）的挑战，需要一种能动态调整问题难度的机制来有效扩展RL训练。

Method: 引入自适应可验证环境强化学习（RLVE），该方法使用可验证环境程序化生成问题并提供算法可验证奖励。这些环境能动态调整问题难度分布以适应策略模型的能力。为实现RLVE，作者创建了RLVE-Gym，一个包含400个精心设计、手动工程化的可验证环境的大规模套件，并在此套件上进行联合训练。

Result: 环境扩展（增加训练环境数量）持续提升了泛化推理能力。RLVE在RLVE-Gym的400个环境上进行联合训练，使一个强大的1.5B推理语言模型在六个推理基准上平均绝对提升了3.37%。相比之下，继续该语言模型的原始RL训练，尽管使用了超过3倍的计算资源，平均绝对增益仅为0.49%。

Conclusion: RLVE通过自适应可验证环境，有效解决了语言模型强化学习中静态数据分布的挑战，并通过环境扩展显著提高了模型的泛化推理能力，证明了其在高效训练方面的优越性。

Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.

</details>


### [374] [Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence](https://arxiv.org/abs/2511.07384)
*Sean McLeish,Ang Li,John Kirchenbauer,Dayal Singh Kalra,Brian R. Bartoldson,Bhavya Kailkhura,Avi Schwarzschild,Jonas Geiping,Tom Goldstein,Micah Goldblum*

Main category: cs.CL

TL;DR: 研究如何将预训练的非循环语言模型转换为深度循环模型，通过循环课程训练，在保持性能的同时降低计算成本，并在数学任务上取得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 深度循环语言模型在训练和参数量与测试计算量之间解耦方面显示出潜力。本文旨在探索如何将现有的预训练非循环语言模型转换为深度循环模型以利用这一优势。

Method: 将现有的预训练非循环语言模型转换为深度循环模型。在训练过程中，采用“循环课程”（curriculum of recurrences）策略，逐步增加模型的有效深度。

Result: 转换后的模型在保持性能的同时降低了总计算成本。在数学实验中，与简单地对原始非循环模型进行后续训练相比，将预训练模型转换为循环模型在给定计算预算下取得了更好的性能。

Conclusion: 通过循环课程将预训练的非循环语言模型转换为深度循环模型是一种有效的方法，可以在降低计算成本的同时保持或提升性能，尤其在数学等任务上表现出色。

Abstract: Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.

</details>


### [375] [Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction](https://arxiv.org/abs/2511.07392)
*Hyeryun Park,Byung Mo Gu,Jun Hee Lee,Byeong Hyeon Choi,Sekeun Kim,Hyun Koo Kim,Kyungsang Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于语音控制的手术智能体编排平台（SAOP），利用分层多智能体框架和大型语言模型（LLM）驱动的智能体，帮助达芬奇机器人手术中的外科医生无需中断即可访问和操作多模态患者数据，提高了操作的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在达芬奇机器人手术中，外科医生的手和眼睛完全投入手术过程，导致难以在不中断的情况下访问和操作多模态患者数据。

Method: 研究者提出了一个语音导向的手术智能体编排平台（SAOP），该平台基于分层多智能体框架，包含一个编排智能体和三个由大型语言模型（LLM）驱动的特定任务智能体。这些LLM智能体能自主规划、完善、验证和推理，将语音命令映射到特定任务，如检索临床信息、操作CT扫描或在手术视频上导航3D解剖模型。此外，还引入了多级编排评估指标（MOEM），从命令级别和类别级别全面评估性能和鲁棒性。

Result: SAOP在240个语音命令上实现了高准确率和成功率。基于LLM的智能体显著提高了对语音识别错误以及多样化或模糊自由形式命令的鲁棒性。

Conclusion: SAOP展示了支持微创达芬奇机器人手术的强大潜力，通过语音指令有效且鲁棒地辅助外科医生进行数据访问和操作。

Abstract: In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.

</details>


### [376] [ConvFill: Model Collaboration for Responsive Conversational Voice Agents](https://arxiv.org/abs/2511.07397)
*Vidya Srinivas,Zachary Englhardt,Maximus Powers,Shwetak Patel,Vikram Iyer*

Main category: cs.CL

TL;DR: 该研究提出“对话填充”方法，通过轻量级设备端模型实时生成对话并无缝整合云端大型模型的知识流，以实现响应迅速且功能强大的对话代理。


<details>
  <summary>Details</summary>
Motivation: 云端大型语言模型提供深度推理和领域知识，但引入高延迟，破坏自然对话；设备端模型响应即时，但缺乏复杂性。这种延迟与能力之间的权衡是部署会话语音代理的关键挑战。

Method: 提出“对话填充”任务，由一个轻量级设备端模型生成上下文适宜的对话，同时无缝整合来自强大后端模型的流式知识。开发了ConvFill，一个3.6亿参数的模型，通过合成多领域对话进行训练。

Result: 对话填充可以成功学习。ConvFill在相同大小的独立小型模型上实现了36-42%的准确性提升，同时始终保持低于200毫秒的响应延迟。

Conclusion: 该方法有望构建既能即时响应又具备丰富知识的设备端会话代理，解耦了响应延迟和模型能力之间的关系。

Abstract: Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.

</details>


### [377] [SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations](https://arxiv.org/abs/2511.07405)
*Manon Berriche,Célia Nouri,Chloé Clavel,Jean-Philippe Cointet*

Main category: cs.CL

TL;DR: 本文介绍了SPOT语料库，这是首个将社会学中的“停止点”概念转化为可重现NLP任务的标注语料库。该语料库包含43,305条法语Facebook评论，用于二元分类任务。基准测试显示，微调编码器模型在F1分数上显著优于提示式LLM，并且上下文元数据能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有框架（如反驳或社会纠正）常忽略在线讨论中通过讽刺、微妙怀疑或碎片化论证等形式暂停或重定向对话的“停止点”。研究旨在将这一社会学概念操作化为可重现的NLP任务，以更好地理解和处理在线讨论中的关键干预。

Method: 引入了SPOT语料库，包含43,305条手动标注的法语Facebook评论，这些评论与被标记为虚假信息的URL相关，并富含上下文元数据。将“停止点”定义为二元分类任务。使用微调编码器模型（CamemBERT）和指令微调大型语言模型（LLM）进行基准测试，并尝试了不同的提示策略。

Result: 基准测试结果显示，微调编码器模型在F1分数上比提示式LLM高出10个百分点以上。此外，整合上下文元数据将编码器模型的F1分数从0.75提高到0.78。这强调了监督学习对于新兴非英语社交媒体任务的重要性。

Conclusion: 监督学习对于处理新兴的非英语社交媒体NLP任务至关重要。整合上下文元数据可以有效提升模型性能。研究公开了匿名数据集、标注指南和代码，以促进透明度和可重复性研究。

Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.

</details>


### [378] [EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models](https://arxiv.org/abs/2511.07193)
*Jiacheng Huang,Ning Yu,Xiaoyin Yi*

Main category: cs.CL

TL;DR: 处理失败


<details>
  <summary>Details</summary>
Motivation: 处理失败

Method: 处理失败

Result: 处理失败

Conclusion: 处理失败

Abstract: Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.

</details>


### [379] [Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation](https://arxiv.org/abs/2511.07382)
*K M Nafi Asib,Sourav Saha,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文提出了一种结合指令提示和测试驱动、反馈引导的迭代优化方法，使用微调的Qwen2.5-14B模型为孟加拉语（一种低资源语言）生成代码。该方法在“孟加拉语代码生成”共享任务中获得第二名，Pass@1得分为0.934。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成方面取得了进展，但孟加拉语等低资源语言因缺乏指令到代码数据集和评估基准而代表性不足。IJCNLP-AACL 2025 BLP研讨会的共享任务旨在解决这一问题。

Method: 研究团队结合了指令提示和测试驱动、反馈引导的迭代优化过程。他们使用微调的Qwen2.5-14B模型从孟加拉语指令生成代码，然后通过单元测试进行测试。对于失败的输出，模型会通过三次评估迭代地进行细化，每次都利用测试反馈进行指导。

Result: 该方法使“Retriv”团队在共享任务中获得第二名，Pass@1得分为0.934。分析揭示了孟加拉语指令理解和Python代码生成方面的挑战，强调了低资源语言中对目标方法的需要。实验脚本已公开提供。

Conclusion: 所提出的结合指令提示和迭代细化的方法在低资源语言的代码生成任务中表现出色，但孟加拉语指令理解和Python代码生成仍然存在挑战，需要进一步研究针对性的方法。

Abstract: Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation in Bangla". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team "Retriv" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.

</details>


### [380] [Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains](https://arxiv.org/abs/2511.07380)
*Pingjie Wang,Hongcheng Liu,Yusheng Liao,Ziqing Fan,Yaxin Du,Shuo Tang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 针对低资源领域中大型语言模型（LLMs）面临的数据稀缺和过拟合挑战，本文提出了NTK-Selector框架。该框架利用神经正切核（NTK）高效选择有价值的通用领域辅助数据，显著提升了LLMs在低资源领域的性能，实现了高达10.9倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在低资源领域应用时，由于数据稀缺和过拟合风险，表现不佳。尽管领域内数据有限，但存在大量相似的通用领域数据，初步研究发现这些数据可以作为辅助监督。研究的中心问题是如何有效选择最有价值的辅助数据以最大化领域特定性能，尤其是在缺乏大型领域内数据集或验证集导致传统方法无法应用的情况下。

Method: 本文提出了NTK-Selector，一个基于神经正切核（NTK）的原则性且高效的框架，用于选择通用领域辅助数据以增强领域特定性能。该方法通过实证展示LLMs在LoRA微调过程中稳定的NTK类行为，并提出了一种无雅可比（Jacobian-free）近似方法，解决了直接将NTK应用于LLMs面临的理论假设和计算成本过高两大挑战。

Result: 在医疗、金融、法律和心理学四个低资源领域的广泛实验表明，NTK-Selector持续提升了下游任务性能。具体而言，仅使用1,000个领域内样本进行微调，Llama3-8B-Instruct和Qwen3-8B分别仅获得+0.8和+0.9点的提升。而通过NTK-Selector选择的9,000个辅助样本进行丰富后，性能显著提升了+8.7和+5.1点，相对于仅使用领域内数据的情况，分别实现了10.9倍和5.7倍的改进。

Conclusion: NTK-Selector为解决LLMs在低资源领域的数据稀缺问题提供了一个有效且高效的解决方案。通过智能选择通用领域辅助数据，该方法能够显著提升LLMs在领域特定任务上的性能，为低资源场景下的LLMs应用开辟了新途径。

Abstract: Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \textbf{\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a \textbf{10.9x and 5.7x improvement} over the domain-only setting.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [381] [VLM-driven Skill Selection for Robotic Assembly Tasks](https://arxiv.org/abs/2511.05680)
*Jeong-Jung Kim,Doo-Yeol Koh,Chang-Hyun Kim*

Main category: cs.RO

TL;DR: 本文提出了一个结合视觉-语言模型（VLMs）和模仿学习的机器人装配框架，用于执行装配操作。


<details>
  <summary>Details</summary>
Motivation: 旨在实现灵活适应的机器人操作，通过结合视觉感知、自然语言理解和学习到的基本技能来解决装配任务。

Method: 该框架利用配备夹持器的机器人在3D空间中移动，融合了视觉-语言模型（VLMs）与模仿学习。它整合了视觉感知、自然语言理解和学习到的基本技能。

Result: 实验结果表明，该方法在装配场景中表现出有效性，实现了高成功率，并通过结构化的基本技能分解保持了可解释性。

Conclusion: 该方法对于机器人装配任务是有效的，它结合了高成功率和通过技能分解实现的可解释性。

Abstract: This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.

</details>


### [382] [Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots](https://arxiv.org/abs/2511.05642)
*Justin Williams,Kishor Datta Gupta,Roy George,Mrinmoy Sarkar*

Main category: cs.RO

TL;DR: 本文展示了在移动机器人上部署小型视觉语言模型（VLMs）的可行性，以在边缘实现实时场景理解和推理，同时进行移动和感知，无需云连接。


<details>
  <summary>Details</summary>
Motivation: 在GPS受限环境下运行的自主机器人，需要本地、资源高效的AI模型进行推理。现有方法通常将感知与移动分离，而将AI模型部署到边缘的计算约束是一个挑战。

Method: 提出了一种框架，将紧凑型VLM与多模态感知集成，直接部署在嵌入式硬件上，实现同步移动和推理。该系统不依赖云连接。

Result: 实验验证了在计算效率、任务准确性和系统响应性之间的平衡。成功在移动机器人上部署了小型VLM，实现了并发推理和移动，这是首次在该领域取得的成功部署之一。

Conclusion: 这项工作为服务机器人、灾难响应和国防行动等应用中的可扩展、有保障的自主性奠定了基础。

Abstract: The deployment of artificial intelligence models at the edge is increasingly critical for autonomous robots operating in GPS-denied environments where local, resource-efficient reasoning is essential. This work demonstrates the feasibility of deploying small Vision-Language Models (VLMs) on mobile robots to achieve real-time scene understanding and reasoning under strict computational constraints. Unlike prior approaches that separate perception from mobility, the proposed framework enables simultaneous movement and reasoning in dynamic environments using only on-board hardware. The system integrates a compact VLM with multimodal perception to perform contextual interpretation directly on embedded hardware, eliminating reliance on cloud connectivity. Experimental validation highlights the balance between computational efficiency, task accuracy, and system responsiveness. Implementation on a mobile robot confirms one of the first successful deployments of small VLMs for concurrent reasoning and mobility at the edge. This work establishes a foundation for scalable, assured autonomy in applications such as service robotics, disaster response, and defense operations.

</details>


### [383] [TumorMap: A Laser-based Surgical Platform for 3D Tumor Mapping and Fully-Automated Tumor Resection](https://arxiv.org/abs/2511.05723)
*Guangshen Ma,Ravi Prakash,Beatrice Schleupner,Jeffrey Everitt,Arpit Mishra,Junqin Chen,Brian Mann,Boyuan Chen,Leila Bridgeman,Pei Zhong,Mark Draelos,William C. Eward,Patrick J. Codd*

Main category: cs.RO

TL;DR: 本文提出了一种名为“TumorMap”的手术机器人平台，该平台结合多功能激光和深度学习模型，实现术中3D肿瘤边界构建和全自动、非接触式肿瘤切除，解决了当前手术中肿瘤识别和切除的挑战。


<details>
  <summary>Details</summary>
Motivation: 外科医生在恶性实体瘤切除手术中，面临准确识别病理组织、完整切除肿瘤同时保留健康组织的关键挑战。现有方法存在高保真肿瘤重建不足、通用组织模型难以应对肿瘤诊断的复杂性，以及术中双手动操作、生理性震颤和疲劳等物理限制。

Method: “TumorMap”平台整合了三激光机制（光学相干断层扫描、激光诱导内源性荧光和切割激光刀），并结合深度学习模型，以实现全自动、非接触式肿瘤切除。该平台旨在构建术中3D肿瘤边界，并进行自主组织切除。

Result: 在小鼠骨肉瘤和软组织肉瘤模型中验证了TumorMap。研究建立了新的组织病理学工作流程来评估传感器性能。结果显示，激光切除精度达到亚毫米级别，实现了多模态传感器引导的自主肿瘤手术，无需任何人工干预。

Conclusion: TumorMap提供了一种创新的手术机器人解决方案，能够克服传统手术中肿瘤识别和切除的挑战，通过结合多功能激光和深度学习，实现高精度、全自动、非接触式的肿瘤切除，有望提高手术效果。

Abstract: Surgical resection of malignant solid tumors is critically dependent on the surgeon's ability to accurately identify pathological tissue and remove the tumor while preserving surrounding healthy structures. However, building an intraoperative 3D tumor model for subsequent removal faces major challenges due to the lack of high-fidelity tumor reconstruction, difficulties in developing generalized tissue models to handle the inherent complexities of tumor diagnosis, and the natural physical limitations of bimanual operation, physiologic tremor, and fatigue creep during surgery. To overcome these challenges, we introduce "TumorMap", a surgical robotic platform to formulate intraoperative 3D tumor boundaries and achieve autonomous tissue resection using a set of multifunctional lasers. TumorMap integrates a three-laser mechanism (optical coherence tomography, laser-induced endogenous fluorescence, and cutting laser scalpel) combined with deep learning models to achieve fully-automated and noncontact tumor resection. We validated TumorMap in murine osteoscarcoma and soft-tissue sarcoma tumor models, and established a novel histopathological workflow to estimate sensor performance. With submillimeter laser resection accuracy, we demonstrated multimodal sensor-guided autonomous tumor surgery without any human intervention.

</details>


### [384] [A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms](https://arxiv.org/abs/2511.05785)
*Lianhao Yin,Haiping Yu,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 本文提出一个统一的随机模型，揭示生物、物理和机器人集群通过在不同能量函数约束下的熵最大化实现集体行为，并为设计可扩展的智能机器人集群提供原则。


<details>
  <summary>Details</summary>
Motivation: 尽管生物集群（如蚁群）和物理系统（如气体、液体）都表现出去中心化和随机的个体行为，但目前尚缺乏一个统一的框架来解释这两种系统中随机行为的共性机制，尤其是在生物系统能实现集体目标而物理系统不能的情况下。

Method: 通过对红林蚁（Formica polyctena）的实证研究，揭示其行为背后的统计机制。进一步，将该原理应用于机器人集群，以验证其在实现可扩展、去中心化协作中的有效性。

Result: 研究发现，红林蚁的行为与物理系统一样，都遵循在不同能量函数约束下的最大化统计机制。基于此原理的机器人集群能够展现出可扩展、去中心化的协作能力，并模拟物理相变行为，且个体计算量极小。

Conclusion: 该研究建立了一个连接生物、物理和机器人集群的统一随机模型，为设计鲁棒和智能的集群机器人提供了可扩展的原理。

Abstract: Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.

</details>


### [385] [VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models](https://arxiv.org/abs/2511.05791)
*Manav Kulshrestha,S. Talha Bukhari,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: VLAD-Grasp是一种基于视觉语言模型的零样本抓取检测方法，无需训练和抓取数据集，通过生成目标图像并结合3D重建和点云对齐来预测抓取姿态，在多个数据集上表现出与SOTA监督模型相当或更优的性能，并具备强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大多数现有机器人抓取方法依赖大量专家标注，并且在新物体上需要重新训练，这限制了自主操作的灵活性和效率。

Method: VLAD-Grasp从单张RGB-D图像出发，(1) 提示大型视觉语言模型生成一张目标图像（物体被直杆“刺穿”，代表对趾抓取），(2) 预测深度和分割以将生成的图像提升到3D，(3) 通过主成分分析和无对应优化，对齐生成和观察到的物体点云，从而恢复可执行的抓取姿态。该方法无需训练和依赖预先整理的抓取数据集。

Result: VLAD-Grasp在Cornell和Jacquard数据集上取得了与最先进的监督模型具有竞争力或更优的性能。此外，它在Franka Research 3机器人上对新颖的真实世界物体展示了卓越的零样本泛化能力。

Conclusion: VLAD-Grasp证明了视觉语言基础模型作为机器人操作的强大先验知识的潜力，使得在无需训练和特定抓取数据集的情况下，也能实现高效且泛化能力强的机器人抓取。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.

</details>


### [386] [An Open-Source, Reproducible Tensegrity Robot that can Navigate Among Obstacles](https://arxiv.org/abs/2511.05798)
*William R. Johnson,Patrick Meng,Nelson Chen,Luca Cimatti,Augustin Vercoutere,Mridul Aanjaneya,Rebecca Kramer-Bottiglio,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 本文提出了一套完整、开源、可复现的系统，用于三杆张拉整体机器人的导航，包括硬件设计和涵盖建模、估计、规划与控制的软件栈，并在复杂环境中验证了其鲁棒性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 张拉整体机器人具有抗冲击、轻量和适应非结构化地形的优点，但其柔顺性和复杂的耦合动力学带来了建模和控制难题，阻碍了路径规划和避障能力的实现。

Method: 该研究开发了一个完整的开源系统，包含：(i) 低成本、开源的硬件设计；(ii) 集成的开源软件栈，用于基于物理的建模、系统辨识、状态估计、路径规划和控制。该系统能跟踪机器人姿态并在已知障碍物中执行无碰撞路径到指定目标。通过垂直跌落、斜坡和颗粒介质等非建模环境挑战以及户外演示来验证系统鲁棒性，并通过在两个不同实验室进行实验来验证可复现性。

Result: 研究成功构建了一个完整、开源且可复现的导航系统，使三杆张拉整体机器人能够跟踪姿态并执行无碰撞路径。该系统在包括垂直跌落、斜坡、颗粒介质和户外环境等非建模挑战中表现出良好的鲁棒性，并且其结果在不同实验室得到了验证。

Conclusion: 该工作为机器人社区提供了一个用于柔顺、抗冲击、形变机器人的完整导航系统。该系统旨在为推进其他非常规机器人平台的导航能力提供一个起点。

Abstract: Tensegrity robots, composed of rigid struts and elastic tendons, provide impact resistance, low mass, and adaptability to unstructured terrain. Their compliance and complex, coupled dynamics, however, present modeling and control challenges, hindering path planning and obstacle avoidance. This paper presents a complete, open-source, and reproducible system that enables navigation for a 3-bar tensegrity robot. The system comprises: (i) an inexpensive, open-source hardware design, and (ii) an integrated, open-source software stack for physics-based modeling, system identification, state estimation, path planning, and control. All hardware and software are publicly available at https://sites.google.com/view/tensegrity-navigation/. The proposed system tracks the robot's pose and executes collision-free paths to a specified goal among known obstacle locations. System robustness is demonstrated through experiments involving unmodeled environmental challenges, including a vertical drop, an incline, and granular media, culminating in an outdoor field demonstration. To validate reproducibility, experiments were conducted using robot instances at two different laboratories. This work provides the robotics community with a complete navigation system for a compliant, impact-resistant, and shape-morphing robot. This system is intended to serve as a springboard for advancing the navigation capabilities of other unconventional robotic platforms.

</details>


### [387] [3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots](https://arxiv.org/abs/2511.05816)
*Taku Okawara,Ryo Nishibe,Mao Kasano,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 针对空间探索中的攀爬机器人，本文提出一种基于单目相机和肢体正向运动学的SLAM方法，通过因子图优化实现实时、度量尺度的三维地形图构建，并支持自主抓取，克服了传统RGB-D相机体积大、功耗高的问题。


<details>
  <summary>Details</summary>
Motivation: 用于空间探索的攀爬机器人需要手眼相机准确估计可抓取点（凸起地形表面）的三维位置。传统方法使用RGB-D相机进行三维地形映射和抓取点检测，但RGB-D相机体积大、功耗高，不适用于空间任务。单目相机更轻巧、紧凑且功耗低，但单目SLAM存在尺度模糊问题。

Method: 本文提出一种融合单目视觉约束与肢体正向运动学的SLAM方法。该方法利用因子图优化，联合估计时间序列的夹持器姿态和三维地图的全局度量尺度，从而解决单目SLAM的尺度模糊问题。

Result: 通过物理模拟和真实世界实验验证，所提出的框架能够实时构建具有度量尺度的三维地形图。该系统仅使用单目手眼相机，无需RGB-D相机，即可实现对凸起地形表面的自主抓取。

Conclusion: 该方法为未来涉及有肢攀爬机器人的空间任务提供了可扩展且节能的感知解决方案，实现了基于单目相机的实时度量尺度三维地形映射和自主抓取。

Abstract: Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars. In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views. While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power.
  This work presents a 3D terrain mapping system designed for space exploration using limbed climbing robots equipped with a monocular hand-eye camera. Compared to RGB-D cameras, monocular cameras are more lightweight, compact structures, and have lower power consumption. Although monocular SLAM can be used to construct 3D maps, it suffers from scale ambiguity. To address this limitation, we propose a SLAM method that fuses monocular visual constraints with limb forward kinematics. The proposed method jointly estimates time-series gripper poses and the global metric scale of the 3D map based on factor graph optimization.
  We validate the proposed framework through both physics-based simulations and real-world experiments. The results demonstrate that our framework constructs a metrically scaled 3D terrain map in real-time and enables autonomous grasping of convex terrain surfaces using a monocular hand-eye camera, without relying on RGB-D cameras. Our method contributes to scalable and energy-efficient perception for future space missions involving limbed climbing robots. See the video summary here: https://youtu.be/fMBrrVNKJfc

</details>


### [388] [Adversarial Game-Theoretic Algorithm for Dexterous Grasp Synthesis](https://arxiv.org/abs/2511.05809)
*Yu Chen,Botao He,Yuemin Mao,Arthur Jakobsson,Jeffrey Ke,Yiannis Aloimonos,Guanya Shi,Howie Choset,Jiayuan Mao,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: 本文提出一种基于双人博弈的新型多指机器人抓取合成方法，通过显式捕捉对抗性物体运动来提高抓取稳定性，并在仿真和真实世界中均取得了显著更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有抓取合成方法在抵抗单一扭矩时往往忽略物体潜在的对抗性运动（如逃脱），导致抓取在扰动下不稳定和失败，从而难以产生可靠的抓取来充分限制物体运动。

Method: 将抓取合成问题建模为一个双人博弈：一个玩家控制机器人生成可行的抓取配置，另一个玩家对抗性地控制物体寻找试图逃脱抓取的运动。这种方法显式地捕捉并利用了对抗性物体运动。

Result: 在仿真实验中，该方法成功率达到75.78%，比现有最佳基线高出19.61%。双人博弈机制比没有博弈机制的方法将抓取成功率提高了27.40%。平均生成抓取配置仅需0.28-1.04秒。在真实世界实验中，ShadowHand上的平均成功率为85.0%，LeapHand上为87.5%。

Conclusion: 该方法通过引入双人博弈机制来考虑对抗性物体运动，显著提高了多指机器人抓取的成功率和稳定性，并在仿真和真实机器人设置中都得到了验证，证明了其可行性和有效性。

Abstract: For many complex tasks, multi-finger robot hands are poised to revolutionize how we interact with the world, but reliably grasping objects remains a significant challenge. We focus on the problem of synthesizing grasps for multi-finger robot hands that, given a target object's geometry and pose, computes a hand configuration. Existing approaches often struggle to produce reliable grasps that sufficiently constrain object motion, leading to instability under disturbances and failed grasps. A key reason is that during grasp generation, they typically focus on resisting a single wrench, while ignoring the object's potential for adversarial movements, such as escaping. We propose a new grasp-synthesis approach that explicitly captures and leverages the adversarial object motion in grasp generation by formulating the problem as a two-player game. One player controls the robot to generate feasible grasp configurations, while the other adversarially controls the object to seek motions that attempt to escape from the grasp. Simulation experiments on various robot platforms and target objects show that our approach achieves a success rate of 75.78%, up to 19.61% higher than the state-of-the-art baseline. The two-player game mechanism improves the grasping success rate by 27.40% over the method without the game formulation. Our approach requires only 0.28-1.04 seconds on average to generate a grasp configuration, depending on the robot platform, making it suitable for real-world deployment. In real-world experiments, our approach achieves an average success rate of 85.0% on ShadowHand and 87.5% on LeapHand, which confirms its feasibility and effectiveness in real robot setups.

</details>


### [389] [Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills](https://arxiv.org/abs/2511.05855)
*Jiayu Zhou,Qiwei Wu,Jian Li,Zhe Chen,Xiaogang Xiong,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出一个新颖的框架，通过结合分层语义分解、仿真强化学习（RL）、视觉语言模型（VLM）和知识蒸馏，实现长时间、接触密集型操作任务的自主执行，避免了昂贵的真实世界数据和人工演示，并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统上，长时间、接触密集型操作任务的自主执行需要大量的真实世界数据和专业的工程设计，这带来了显著的成本和可扩展性挑战。

Method: 该方法将复杂任务分解为原子技能，并为每个原子技能在仿真中训练强化学习策略（包含明确的力约束）。视觉语言模型（VLM）执行高层任务分解和技能规划，生成多样化的专家演示。这些演示通过视觉-触觉扩散策略（Visual-Tactile Diffusion Policy）蒸馏成一个统一的端到端策略。研究还进行了消融研究，探索不同的VLM任务规划器和模仿学习算法。

Result: 该方法实现了长时间操作的策略学习，无需昂贵的人工演示。VLM引导的原子技能框架实现了对多样化任务的可扩展泛化。通过广泛的仿真实验和物理部署验证了其有效性。

Conclusion: 该集成框架成功克服了传统方法对大量真实世界数据和专家工程的依赖，通过结合仿真强化学习、VLM规划和知识蒸馏，为长时间、接触密集型操作任务提供了一种可扩展且有效的解决方案。

Abstract: Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks.

</details>


### [390] [Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections](https://arxiv.org/abs/2511.05886)
*Lei Shi,Yongju Kim,Xinzhi Zhong,Wissam Kontar,Qichao Liu,Soyoung Ahn*

Main category: cs.RO

TL;DR: 本文提出一种考虑公平性的分层控制框架，用于连接和自动驾驶车辆在交叉口的协调管理，旨在确保公平性、安全性并优化交通流。


<details>
  <summary>Details</summary>
Motivation: 在安全关键、实时交通控制中，确保连接和自动驾驶车辆在交叉口协调的公平性对于公平准入、社会接受度和长期系统效率至关重要，但目前仍未得到充分探索。

Method: 该研究提出一个公平感知的分层控制框架：顶层是一个集中式分配模块，通过最大化考虑等待时间、紧急性、控制历史和速度偏差的效用函数来分配控制权限；底层是授权车辆使用线性二次调节器（LQR）执行预计算轨迹，并应用高阶控制障碍函数（HOCBF）安全过滤器进行实时防撞。

Result: 仿真结果表明，该框架在不同交通需求和分布下实现了近乎完美的公平性，消除了碰撞，减少了平均延误，并保持了实时可行性。

Conclusion: 研究结果强调，公平性可以系统地融入交通控制，而无需牺牲安全性或性能，从而为未来的自主交通系统实现可扩展且公平的协调。

Abstract: Ensuring fairness in the coordination of connected and automated vehicles at intersections is essential for equitable access, social acceptance, and long-term system efficiency, yet it remains underexplored in safety-critical, real-time traffic control. This paper proposes a fairness-aware hierarchical control framework that explicitly integrates inequity aversion into intersection management. At the top layer, a centralized allocation module assigns control authority (i.e., selects a single vehicle to execute its trajectory) by maximizing a utility that accounts for waiting time, urgency, control history, and velocity deviation. At the bottom layer, the authorized vehicle executes a precomputed trajectory using a Linear Quadratic Regulator (LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety filter for real-time collision avoidance. Simulation results across varying traffic demands and demand distributions demonstrate that the proposed framework achieves near-perfect fairness, eliminates collisions, reduces average delay, and maintains real-time feasibility. These results highlight that fairness can be systematically incorporated without sacrificing safety or performance, enabling scalable and equitable coordination for future autonomous traffic systems.

</details>


### [391] [ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface](https://arxiv.org/abs/2511.05858)
*Chuanyu Li,Chaoyi Liu,Daotan Wang,Shuyu Zhang,Lusong Li,Zecui Zeng,Fangchen Liu,Jing Xu,Rui Chen*

Main category: cs.RO

TL;DR: 本文提出了ViTaMIn-B，一个用于收集复杂双手和接触密集型任务数据的更强大、高效的手持系统。它包含新型顺应性视觉触觉传感器DuoTact和基于Meta Quest控制器的鲁棒双手姿态跟踪，解决了现有系统在触觉感知和姿态跟踪方面的不足，并在用户研究和任务实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有手持设备在处理复杂交互场景（特别是双手和接触密集型任务）时，通常缺乏鲁棒的触觉感知或可靠的姿态跟踪能力。

Method: 本文提出了ViTaMIn-B系统。具体方法包括：1) 设计了DuoTact，一种新型顺应性视觉触觉传感器，具有柔性框架，能承受大接触力并捕获高分辨率接触几何形状。2) 提出将传感器的全局变形重建为3D点云作为策略输入，以增强跨传感器泛化能力。3) 利用Meta Quest控制器开发了一种鲁棒、统一的6自由度双手姿态获取过程，消除了常见基于SLAM方法的轨迹漂移问题。

Result: 全面的用户研究证实了ViTaMIn-B在新手和专家操作员中的高效率和可用性。此外，在四项双手操作任务上的实验表明，ViTaMIn-B相对于现有系统具有卓越的任务性能。

Conclusion: ViTaMIn-B是一个更强大、高效的手持数据收集系统，特别适用于复杂双手和接触密集型任务，通过创新的视觉触觉传感器和鲁棒的姿态跟踪解决了现有系统的局限性，并展现出优越的性能和可用性。

Abstract: Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.

</details>


### [392] [10 Open Challenges Steering the Future of Vision-Language-Action Models](https://arxiv.org/abs/2511.05936)
*Soujanya Poria,Navonil Majumder,Chia-Yu Hung,Amir Ali Bagherzadeh,Chuan Li,Kenneth Kwok,Ziwei Wang,Cheston Tan,Jiajun Wu,David Hsu*

Main category: cs.RO

TL;DR: 本文探讨了视觉-语言-动作（VLA）模型在具身AI领域发展的10个主要里程碑和新兴趋势，旨在加速其普及。


<details>
  <summary>Details</summary>
Motivation: VLA模型因其遵循自然语言指令的能力在具身AI领域日益普及，继承了LLM和VLM的成功。研究动机是识别并讨论关键进展和挑战，以加速VLA模型的开发和广泛接受。

Method: 本文通过讨论的方式，梳理了VLA模型发展中的10个主要里程碑（多模态、推理、数据、评估、跨机器人动作泛化、效率、全身协调、安全、代理和人机协调），并探讨了四个新兴趋势（空间理解、世界动态建模、训练后处理和数据合成）。

Result: 识别出VLA模型发展的10个关键里程碑和4个新兴趋势。这些趋势被认为是实现里程碑的关键手段，包括空间理解、世界动态建模、训练后处理和数据合成。

Conclusion: 本文旨在通过这些讨论，引起对可能加速VLA模型发展并使其更广泛接受的研究方向的关注。

Abstract: Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.

</details>


### [393] [From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation](https://arxiv.org/abs/2511.05889)
*Zeyuan Feng,Haimingyue Zhang,Somil Bansal*

Main category: cs.RO

TL;DR: 本文提出一个模块化框架，使机器人能够将自然语言指令转换为结构化安全规范，并通过感知模块进行环境感知，最终使用模型预测控制（MPC）安全滤波器实时执行语义和几何约束，以实现安全导航。


<details>
  <summary>Details</summary>
Motivation: 随着机器人日益融入开放、以人为中心的环境，其解释自然语言指令和遵守安全约束的能力至关重要。现有方法常侧重于将语言映射到奖励函数而非安全规范，或仅处理狭窄的约束类别（如避障），限制了其鲁棒性和适用性。

Method: 该框架包含三个核心组件：1) 基于大型语言模型（LLM）的模块，将自由形式指令转换为结构化安全规范；2) 感知模块，通过维护环境的物体级3D表示来落实这些规范；3) 基于模型预测控制（MPC）的安全滤波器，实时强制执行语义和几何约束。

Result: 通过仿真研究和硬件实验，证明了该框架能够鲁棒地解释和执行各种语言指定的约束，适用于广泛的环境和场景。

Conclusion: 该框架有效地解决了机器人在开放世界中解释和执行多样化语言安全约束的问题，提升了机器人交互的有效性和可信度。

Abstract: As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.

</details>


### [394] [Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm](https://arxiv.org/abs/2511.05995)
*Jianbo Yuan,Jing Dai,Yerui Fan,Yaxiong Wu,Yunpeng Liang,Weixin Yan*

Main category: cs.RO

TL;DR: 本研究设计了一种新型轻量化肌腱驱动肌肉骨骼手臂（LTDM-Arm），结合模块化人工肌肉系统和数据驱动迭代学习控制，实现了在负载扰动下的人形轨迹跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 开发具有人类操作特性（如灵巧性、柔顺性和鲁棒性）的机器人系统，通过模拟人体的肌肉骨骼结构来实现，一直是研究的重点。

Method: 研究设计了具有七自由度骨骼关节系统和15个执行器的模块化人工肌肉系统（MAMS）的LTDM-Arm。此外，采用了Hilly型肌肉模型和数据驱动迭代学习控制（DDILC）来学习和优化重复任务的激活信号。通过仿真和实验验证了系统的抗干扰能力。

Result: LTDM-Arm系统在仿真中负载扰动达20%和实验中负载扰动达15%的情况下，仍能有效完成预期的轨迹跟踪任务。

Conclusion: 本研究为开发具有类人操作性能的先进机器人系统奠定了基础。

Abstract: The human arm exhibits remarkable capabilities, including both explosive power and precision, which demonstrate dexterity, compliance, and robustness in unstructured environments. Developing robotic systems that emulate human-like operational characteristics through musculoskeletal structures has long been a research focus. In this study, we designed a novel lightweight tendon-driven musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF) skeletal joint system and a modularized artificial muscular system (MAMS) with 15 actuators. Additionally, we employed a Hilly-type muscle model and data-driven iterative learning control (DDILC) to learn and refine activation signals for repetitive tasks within a finite time frame. We validated the anti-interference capabilities of the musculoskeletal system through both simulations and experiments. The results show that the LTDM-Arm system can effectively achieve desired trajectory tracking tasks, even under load disturbances of 20 % in simulation and 15 % in experiments. This research lays the foundation for developing advanced robotic systems with human-like operational performance.

</details>


### [395] [Development and testing of novel soft sleeve actuators](https://arxiv.org/abs/2511.06102)
*Mohammed Abboodi*

Main category: cs.RO

TL;DR: 本研究提出了一种柔软套筒驱动架构，用于可穿戴移动辅助设备，通过定制的熔融长丝制造工艺生产，实现高效的力矩传递和多轴运动，解决了现有刚性系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 人口老龄化和神经肌肉骨骼疾病的日益流行，增加了对有效、舒适且与解剖结构兼容的可穿戴移动辅助设备的需求。现有系统通常采用刚性机制和笨重界面，阻碍力传递并降低佩戴舒适性。

Method: 研究引入了一种柔软套筒驱动架构，能够贴合肢体并高效传递力和力矩。开发了三种产生线性、弯曲和扭转运动的软套筒执行器，以及一种结合这些运动的全向设计。执行器采用热塑性弹性体，通过定制的熔融长丝制造（FFF）工艺制造，该工艺能生产气密且柔顺的结构，并解决了传统方法中观察到的泄漏问题。使用专用实验平台量化了低气压下的运动学（位移、角度、扭转）和动力学（力、扭矩）输出。通过参数研究探讨了几何特征和材料特性对性能的影响。

Result: 结果显示，该系统能实现可重复的多轴运动，提高了力向肢体的传递效率，并减少了对复杂附件硬件的需求。

Conclusion: 这项工作为软套筒驱动建立了一个统一且可制造的框架，为紧凑型、以用户为中心的辅助技术提供了可能，并显著提升了其运动学和动力学性能。

Abstract: Aging populations and the rising prevalence of neurological and musculoskeletal disorders increase the demand for wearable mobility assistive devices that are effective, comfortable, and anatomically compatible. Many existing systems use rigid mechanisms and bulky interfaces that impede force transmission and reduce wearability. This study introduces a soft sleeve actuation architecture that conforms to the limb while transmitting forces and moments efficiently. We develop three soft sleeve actuators that produce linear, bending, and twisting motion, and an omnidirectional design that combines these motions in one device. Actuators are fabricated from thermoplastic elastomers using a customized fused filament fabrication process that produces airtight and compliant structures and resolves leakage observed with conventional methods. A dedicated experimental platform quantifies kinematic outputs such as displacement, angle, and twist, and kinetic outputs such as force and torque under low pneumatic pressures. A parametric study varies geometric features and material properties to determine their influence on performance. Results show reproducible multi axis motion with improved transfer of force to the limb and reduced need for complex attachment hardware. The work establishes a unified and manufacturable framework for soft sleeve actuation that enables compact and user centered assistive technologies with enhanced kinematic and kinetic performance.

</details>


### [396] [PlaCo: a QP-based robot planning and control framework](https://arxiv.org/abs/2511.06141)
*Marc Duclusaud,Grégoire Passault,Vincent Padois,Olivier Ly*

Main category: cs.RO

TL;DR: PlaCo是一个软件框架，旨在简化机器人系统中基于二次规划（QP）的规划和控制问题的制定与求解。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，机器人QP规划与控制问题的底层数学公式复杂，难以直观地制定。该研究旨在提供一个高层接口，抽象化这些复杂性，使用户能够模块化、直观地定义任务和约束。

Method: PlaCo提供了一个高层接口，抽象了QP问题的底层数学公式。它支持Python绑定用于快速原型开发，并提供C++实现以实现实时性能。

Result: 该框架允许用户以模块化和直观的方式指定任务和约束，从而简化了QP规划与控制问题的制定和求解过程。

Conclusion: PlaCo通过提供高层抽象和多语言支持，有效地简化了机器人QP规划与控制问题的开发，提高了开发效率和潜在的系统性能。

Abstract: This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.

</details>


### [397] [OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/abs/2511.06182)
*Peican Lin,Gan Sun,Chenxi Liu,Fazeng Li,Weihong Ren,Yang Cong*

Main category: cs.RO

TL;DR: 本文提出了OpenVLN框架，一种数据高效的开放世界空中视觉语言导航方案，旨在通过强化学习优化视觉语言模型并引入长程规划器，以解决复杂空中环境中无人机长距离导航的数据稀缺和规划挑战。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）已广泛应用于地面视觉语言导航（VLN）。然而，复杂的户外空中环境带来了数据获取困难和无人机长程轨迹规划要求，给空中VLN带来了新的复杂性。

Method: 我们提出了OpenVLN框架，该框架能以有限数据执行语言引导飞行并增强长程轨迹规划能力。具体而言，我们重新配置了一个强化学习框架，通过基于规则的策略在有限训练数据下高效微调VLM以优化无人机导航任务。同时，引入了一个长程规划器，通过基于价值的奖励动态生成精确的无人机动作以进行轨迹合成。

Result: 在TravelUAV基准上进行了充分的导航实验，并在不同奖励设置下扩展了数据集。我们的方法在成功率上比基线方法提高了4.34%，在Oracle成功率上提高了6.19%，在路径长度加权成功率上提高了4.07%。

Conclusion: 实验结果验证了OpenVLN框架在复杂空中环境中进行长程无人机导航的部署有效性。

Abstract: Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.

</details>


### [398] [ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval](https://arxiv.org/abs/2511.06202)
*Shahram Najam Syed,Yatharth Ahuja,Arthur Jakobsson,Jeff Ichnowski*

Main category: cs.RO

TL;DR: ExpReS-VLA通过经验回放和检索，有效提升预训练视觉-语言-动作（VLA）模型在新部署环境中的适应性，同时防止灾难性遗忘，实现高效且高性能的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 尽管像OpenVLA这样的视觉-语言-动作（VLA）模型在机器人操作任务中展现出令人印象深刻的零样本泛化能力，但它们往往难以有效地适应新的部署环境。在许多实际应用中，在有限任务集上保持持续高性能比广泛泛化更为重要。

Method: 本文提出了ExpReS-VLA，一种通过经验回放和检索来特化预训练VLA模型的方法，同时防止灾难性遗忘。该方法存储冻结视觉骨干网络的紧凑特征表示而非原始图像-动作对，内存使用量减少约97%。在部署过程中，通过余弦相似度检索相关的历史经验来指导适应，同时优先经验回放机制强调成功的轨迹。此外，还引入了阈值混合对比损失（Thresholded Hybrid Contrastive Loss），使其能够从成功和失败的尝试中学习。

Result: 在LIBERO模拟基准测试中，ExpReS-VLA将空间推理任务的成功率从82.6%提高到93.1%，将长周期任务的成功率从61%提高到72.3%。在物理机器人实验中，针对五个操作任务，ExpReS-VLA在已见和未见设置下均达到98%的成功率，而朴素微调的成功率分别为84.7%和32%。使用12个演示在单个RTX 5090 GPU上，适应时间仅为31秒，使得该方法在实际机器人部署中具有实用性。

Conclusion: ExpReS-VLA成功地使预训练VLA模型能够高效适应新的部署环境，在物理和模拟任务中均显著提高了性能，并在保持内存效率和防止遗忘的同时，实现了快速适应，使其成为实际机器人部署的实用方案。

Abstract: Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.

</details>


### [399] [Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2511.06240)
*Tzu-Jung Lin,Jia-Fong Yeh,Hung-Ting Su,Chung-Yi Lin,Yi-Ting Chen,Winston H. Hsu*

Main category: cs.RO

TL;DR: 本文提出了一种零样本、仿生引导的粗到精探索框架，用于开放词汇移动操作中的机器人基座放置。该框架结合了视觉语言模型的语义理解和几何可行性，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在开放词汇移动操作中，任务成功很大程度上取决于机器人基座的恰当放置。现有方法通常只导航到基于距离的区域，而未考虑可供性，导致操作失败率高。

Method: 我们提出了“仿生引导的粗到精探索”框架。该框架通过迭代优化过程，将视觉语言模型（VLM）的语义理解与几何可行性相结合。它构建了跨模态表示（仿生RGB和障碍图+），以对齐语义与空间上下文。方法利用VLM的粗略语义先验来引导搜索到任务相关区域，并用几何约束细化放置，以减少局部最优的风险。

Result: 在五项多样化的开放词汇移动操作任务中，我们的系统实现了85%的成功率，显著优于经典的几何规划器和基于VLM的方法。

Conclusion: 研究结果表明，仿生感知和多模态推理对于开放词汇移动操作中可泛化、指令驱动的规划具有广阔前景。

Abstract: In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.

</details>


### [400] [Robust Differentiable Collision Detection for General Objects](https://arxiv.org/abs/2511.06267)
*Jiayi Chen,Wei Zhao,Liangwang Ruan,Baoquan Chen,He Wang*

Main category: cs.RO

TL;DR: 本文提出了一种鲁棒高效的微分碰撞检测框架，支持凸形和凹形物体，通过距离基随机平滑、自适应采样和等效梯度传输，显著优于现有基线，并应用于灵巧抓取合成。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞检测算法（如GJK+EPA）不可微分，限制了梯度优化在机器人接触任务中的应用。现有微分方法受限于凸形物体，且对复杂几何体缺乏鲁棒性。

Method: 本文提出了一种鲁棒高效的微分碰撞检测框架，支持凸形和凹形物体。核心方法包括：基于距离的一阶随机平滑、自适应采样以及等效梯度传输，以实现鲁棒和信息丰富的梯度计算。

Result: 在来自DexGraspNet和Objaverse的复杂网格上的实验表明，本文方法比现有基线有显著改进。此外，还展示了该方法在灵巧抓取合成中直接应用，以提高抓取质量。

Conclusion: 本文提出了一种鲁棒高效的微分碰撞检测框架，有效解决了传统方法的不可微和现有微分方法的局限性，支持复杂几何体，并在机器人抓取等应用中展现出潜力。

Abstract: Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.

</details>


### [401] [Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning](https://arxiv.org/abs/2511.06371)
*Yingnan Zhao,Xinmiao Wang,Dewei Wang,Xinzhe Liu,Dan Lu,Qilong Han,Peng Liu,Chenjia Bai*

Main category: cs.RO

TL;DR: 本文提出自适应人形机器人控制（AHC）框架，通过多行为蒸馏和强化微调两阶段方法，学习一个统一的控制器，使人形机器人在不同技能和多样地形上展现出强大的自适应运动能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为每项运动技能训练独立的策略，导致控制器在不规则地形和复杂情境下泛化能力差、性能脆弱。研究动机是开发一个能够跨越多种技能和地形进行自适应控制的通用人形机器人运动控制器。

Method: AHC采用两阶段框架：
1.  **第一阶段：** 训练多个主要运动策略，然后进行多行为蒸馏，以获得一个基础的多行为控制器，实现基于环境的自适应行为切换。
2.  **第二阶段：** 通过在线收集在更多样地形上执行自适应行为的反馈，进行强化微调，以增强控制器的地形适应性。

Result: 在仿真和Unitree G1机器人上的真实世界实验结果表明，AHC方法在各种情境和地形下都表现出强大的适应性。

Conclusion: AHC成功地克服了传统行为特定控制器泛化能力差的挑战，通过其两阶段学习框架，实现了一个能够自适应处理多种运动技能和多样地形的人形机器人运动控制器。

Abstract: Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.

</details>


### [402] [ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects](https://arxiv.org/abs/2511.06378)
*Prajval Kumar Murali,Mohsen Kaboli*

Main category: cs.RO

TL;DR: 本文提出了一种名为ArtReg的新型视觉-触觉融合方法，用于在机器人交互过程中跟踪未知、复杂或可动关节物体的姿态，无需先验知识，并能实现目标驱动的操作。


<details>
  <summary>Details</summary>
Motivation: 机器人在现实世界中操作时，经常遇到具有复杂结构和可动关节的未知物体，如门、抽屉等。在缺乏物体几何或运动学先验知识的情况下，感知、跟踪和操作这些物体仍然是一个基本挑战。

Method: 本文提出ArtReg（可动关节配准），它将视觉和触觉点云集成到SE(3)李群上的无迹卡尔曼滤波器中进行点云配准。ArtReg通过有目的的操作（如推或拉）来检测物体中可能的可动关节。此外，ArtReg还用于开发一个闭环控制器，以实现可动关节物体到期望姿态配置的目标驱动操作。

Result: 该方法已通过真实机器人实验对各种未知物体进行了广泛评估，并展示了其在不同质心、低光照条件和复杂视觉背景下的鲁棒性。在标准可动关节物体数据集上，ArtReg在姿态精度方面优于现有最先进的方法。

Conclusion: 利用视觉-触觉信息进行鲁棒和准确的姿态跟踪，使机器人能够感知并与未知、复杂的（具有旋转或平移关节的）可动关节物体进行交互。

Abstract: Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).

</details>


### [403] [External Photoreflective Tactile Sensing Based on Surface Deformation Measurement](https://arxiv.org/abs/2511.06311)
*Seiichi Yamamoto,Hiroki Ishizuka,Takumi Kawasetsu,Koh Hosoda,Takayuki Kameoka,Kango Yanagida,Takato Horii,Sei Ikeda,Osamu Oshiro*

Main category: cs.RO

TL;DR: 本文提出一种利用软机器人机械柔顺性实现触觉感知的方法，通过外部光电反射模块读取硅胶皮肤表面形变来估计接触力，无需嵌入式传感器。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器存在损坏风险、制造复杂、布线复杂等问题，限制了软机器人的应用。研究旨在提供一种实用且鲁棒的力感知方案，同时保留软机器人的结构柔韧性和可制造性。

Method: 该方法使用一个可外部附加的光电反射模块，读取硅胶皮肤的表面形变。首先对光学传感元件和柔顺皮肤进行特性分析，然后确定原型触觉传感器的设计。通过压缩实验验证了该方法。

Result: 实验结果表明，该方法展现出与理论一致的单调力输出关系、低滞后、高重复性，并且对压痕速度响应小。此外，该模块成功集成到软机器人抓手，可靠地检测到抓取事件。与现有方法相比，该模块化附加架构提高了耐用性，减少了布线复杂性，并支持在不同机器人几何结构上的部署。该传感原理还可扩展到关节角度或执行器状态估计等其他体感线索。

Conclusion: 通过利用表面柔顺性结合外部光学模块，为软机器人提供力感知功能提供了一条实用且鲁棒的途径，同时保留了结构灵活性和可制造性，为机器人应用和安全人机协作铺平了道路。

Abstract: We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.

</details>


### [404] [Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot](https://arxiv.org/abs/2511.06397)
*Cong Wen,Yunfei Li,Kexin Liu,Yixin Qiu,Xuanhong Liao,Tianyu Wang,Dingchuan Liu,Tao Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种针对新型六自由度轮式双足机器人的全身控制框架，该框架结合了完整的动力学模型和地形估计，以实现机器人在不平坦地形上的稳定行走。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略腿部动力学，限制了轮式双足机器人的运动潜力，且机器人在穿越不平坦地形时面临挑战。

Method: 开发了包含闭环动力学和基于估计地面法向量的地面接触模型的完整动力学模型；采用LiDAR惯性里程计框架和改进的主成分分析进行地形估计；使用PD控制律进行姿态控制，LQR进行质心动力学平衡控制；通过分层优化方法解决全身控制问题。

Result: 验证了地形估计算法的性能，并通过仿真和实际实验证明了算法的鲁棒性以及机器人穿越不平坦地形的能力。

Conclusion: 该全身控制框架和地形估计算法有效提升了轮式双足机器人在复杂不平坦地形上的运动能力和稳定性。

Abstract: Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.

</details>


### [405] [From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies](https://arxiv.org/abs/2511.06385)
*Ralf Römer,Julian Balletshofer,Jakob Thumm,Marco Pavone,Angela P. Schoellig,Matthias Althoff*

Main category: cs.RO

TL;DR: 本文提出了一种名为路径一致安全过滤（PACS）的方法，用于扩散策略（DPs），旨在在动态环境中提供形式化的安全保证，同时保持任务成功率，并优于现有的反应式安全方法。


<details>
  <summary>Details</summary>
Motivation: 扩散策略（DPs）在复杂操作任务中表现出色，但缺乏安全保证。现有的外部安全机制会以训练中未见过的方式改变动作，导致不可预测的行为和性能下降。

Method: 本文提出了路径一致安全过滤（PACS）。该方法通过对从生成动作序列计算出的轨迹执行路径一致制动来确保执行与策略的训练分布一致。为了实现实时部署和处理不确定性，研究使用基于集合的可达性分析来验证安全性。

Result: 在模拟和三个具有挑战性的真实世界人机交互任务中，实验评估表明PACS (a) 在动态环境中提供形式化的安全保证，(b) 保持任务成功率，并且 (c) 在任务成功率方面比控制障碍函数等反应式安全方法高出多达68%。

Conclusion: PACS为扩散策略提供了一种有效的安全过滤机制，能够在动态环境中提供形式化的安全保证，同时保持甚至提高任务成功率，优于其他反应式安全方法。

Abstract: Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.

</details>


### [406] [Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator](https://arxiv.org/abs/2511.06434)
*Wenkang Hu,Xincheng Tang,Yanzhi E,Yitong Li,Zhengjie Shu,Wei Li,Huamin Wang,Ruigang Yang*

Main category: cs.RO

TL;DR: RGBench是一个用于机器人服装操作的综合基准，包含大量服装模型、一个高性能模拟器以及基于真实服装动态的评估协议，其模拟器性能显著优于现有同类产品。


<details>
  <summary>Details</summary>
Motivation: 在刚性物体机器人操作中，利用模拟数据取得了显著进展，但将此成功应用于可变形物体（如服装）受阻于缺乏可变形物体模型和逼真的非刚性物体模拟器。

Method: 本文提出了Real Garment Benchmark (RGBench)，一个用于机器人服装操作的综合基准。它包含6000多个多样化的服装网格模型、一个新的高性能模拟器，以及一个通过精心测量的真实服装动态来评估服装模拟质量的综合协议。

Result: 实验表明，所提出的模拟器在性能上显著优于目前可用的布料模拟器，将模拟误差降低了20%，同时速度快了3倍。

Conclusion: RGBench将被公开发布，以加速未来机器人服装操作领域的研究。

Abstract: While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/

</details>


### [407] [Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion](https://arxiv.org/abs/2511.06465)
*Lingfan Bao,Tianhu Peng,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本章探讨了双足机器人深度强化学习（DRL）中仿真到现实（sim-to-real）迁移的关键挑战，分析了仿真差距的来源，并提出了通过缩小差距和强化策略两种互补方法来解决问题的战略框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于双足运动中深度强化学习（DRL）的仿真到现实（sim-to-real）迁移面临的“模拟诅咒”这一关键挑战，即仿真与现实之间的巨大差距。

Method: 本章首先分析了仿真到现实差距的主要来源，包括机器人动力学、接触建模、状态估计和数值求解器。然后，提出了两种互补的解决方案哲学：一是通过以模型为中心的策略系统地提高仿真器的物理保真度来“缩小差距”；二是通过仿真中的鲁棒性训练和部署后适应来“强化策略”，使其固有地抵抗模型不准确性。最后，将这些哲学综合为一个战略框架。

Result: 本章提出了一个战略框架，为开发和评估稳健的仿真到现实解决方案提供了一个清晰的路线图。

Conclusion: 通过综合缩小仿真差距和强化策略的哲学，本章提供了一个战略框架，为双足机器人深度强化学习中稳健的仿真到现实解决方案的开发和评估指明了方向。

Abstract: This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.

</details>


### [408] [A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving](https://arxiv.org/abs/2511.06496)
*Keke Long,Jiacheng Guo,Tianyun Zhang,Hongkai Yu,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的自包含低秩方法，用于在没有外部参考或模型访问的情况下，根据幻觉水平自动对多个VLM生成的候选字幕进行排序，以提高自动驾驶场景理解的可靠性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在自动驾驶中用于理解交通场景时，有时会产生幻觉（与视觉输入不符的错误细节）。在缺乏真实参考和模型内部信息不可访问的情况下，检测和缓解这些幻觉具有挑战性。

Method: 该方法构建一个句子嵌入矩阵，并将其分解为低秩共识分量和稀疏残差。通过使用残差的大小来对字幕进行排序，选择残差最小的作为最无幻觉的字幕。该方法仅依赖于字幕本身，无需外部参考或模型访问。

Result: 在NuScenes数据集上的实验表明，该方法在识别无幻觉字幕方面达到了87%的选择准确率，比未过滤基线提高了19%，比多智能体辩论方法提高了6-10%。稀疏误差大小产生的排序与人类对幻觉的判断高度相关。此外，该方法可并行化，推理时间比辩论方法减少了51-67%。

Conclusion: 所提出的自包含低秩方法能够有效且高效地识别自动驾驶VLM生成中的无幻觉字幕，其排序结果与人类判断高度一致，并显著减少了推理时间，使其适用于实时自动驾驶应用。

Abstract: Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.

</details>


### [409] [Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control](https://arxiv.org/abs/2511.06515)
*Cormac O'Neill,Jasmine Terrones,H. Harry Asada*

Main category: cs.RO

TL;DR: 本文利用Koopman算子将机器人动态接触带来的分段动力学统一为嵌入空间中的全局线性模型，从而实现凸模型预测控制，解决了接触控制的非凸优化难题。


<details>
  <summary>Details</summary>
Motivation: 机器人动态接触环境是一个紧迫的挑战，但接触边界处的动力学切换使控制变得困难，导致预测控制器面临非凸优化问题。

Method: 通过应用Koopman算子，将接触变化引起的分段动力学归结为嵌入空间中统一的全局线性模型。研究表明，机器人与环境交互的粘弹性接触支持在不近似控制输入的情况下使用Koopman算子。这种方法使得凸模型预测控制（MPC）成为可能。

Result: 该方法实现了对腿式机器人的凸模型预测控制，以及对进行动态推动的机械臂的实时控制。机器人能够实时地在包含多次接触变化的长时间范围内发现复杂的控制策略。此外，该方法适用于机器人领域之外的广泛领域。

Conclusion: Koopman算子提供了一种有效处理机器人动态接触的方法，能够实现实时、复杂的控制策略，且该方法具有广泛的适用性，超越了机器人领域。

Abstract: Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.

</details>


### [410] [Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation](https://arxiv.org/abs/2511.06500)
*JiaHao Wu,ShengWen Yu*

Main category: cs.RO

TL;DR: 本文提出了一种结合元学习和强化学习的层次化控制框架，用于PID参数的自动调整，通过基于物理的数据增强解决了样本效率问题。实验证明该方法显著提升了机器人的控制性能，并揭示了强化学习优化效果受元学习基线质量和误差分布影响的“优化上限效应”。


<details>
  <summary>Details</summary>
Motivation: PID控制器因其简单性和可靠性在工业机器人中广泛应用，但为不同机器人平台手动调整PID参数耗时且需要大量专业知识。

Method: 提出了一种新颖的层次化控制框架，该框架结合了元学习（用于PID初始化）和强化学习（用于在线自适应）。为了解决样本效率问题，引入了“基于物理的数据增强”策略，通过系统地扰动物理参数来生成虚拟机器人配置，从而在有限的真实机器人数据下实现有效的元学习。

Result: 在Franka Panda机械臂上实现了平均16.6%的改进（6.26° MAE），在高负载关节（J2）上表现出显著提升（从12.36°降至2.42°，改进80.4%）。发现了“优化上限效应”：当元学习基线在局部存在高误差关节时，强化学习能带来显著改进；但当基线性能均匀强劲时（如Laikago），强化学习则无益（0.0%）。该方法在扰动下（参数不确定性：+19.2%，无扰动：+16.6%，平均：+10.0%）表现出鲁棒性，且仅需10分钟训练时间。多种子分析（100次随机初始化）证实了性能的稳定性（平均4.81+/-1.64%）。

Conclusion: 强化学习在层次化控制系统中的有效性高度依赖于元学习基线的质量和误差分布，这为层次化控制系统的设计提供了重要的指导。

Abstract: Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\% average improvement on Franka Panda (6.26° MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from 12.36° to 2.42°). Critically, this work discovers the \textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.

</details>


### [411] [CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning](https://arxiv.org/abs/2511.06575)
*Jun Wang,Yevgeniy Vorobeychik,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 本文提出CoFineLLM，一个针对大型语言模型（LLMs）规划器的保形预测（CP）感知微调框架，旨在减小预测集大小并降低人工干预频率，同时保持规划的正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为规划器在长任务中表现出不可靠性，常产生过度自信的错误。虽然保形预测能确保规划正确性，但由于LLMs对预测集缺乏感知，它们倾向于生成过大的预测集，导致频繁的人工干预，限制了自主部署。

Method: 引入CoFineLLM，这是首个针对基于LLM的规划器的保形预测感知微调框架。该框架明确旨在减小预测集的大小，从而减少用户干预的需求。

Result: CoFineLLM在多个语言指令机器人规划问题上，在预测集大小和求助率方面，持续优于不确定性感知和不确定性无关的微调基线。此外，硬件实验证明了该方法对分布外场景的鲁棒性。

Conclusion: CoFineLLM通过CP感知微调，显著减小了LLM规划器的预测集大小并降低了人工干预率，同时保持了规划的可靠性，提升了LLM规划器的自主部署能力。

Abstract: Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.

</details>


### [412] [Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring](https://arxiv.org/abs/2511.06578)
*Kaustubh Singh,Shivam Kumar,Shashikant Pawar,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 本文介绍了一种欠驱动仿生水下机器人，旨在用于海洋和淡水环境的生态系统监测，并利用强化学习技术学习其最小驱动行为。


<details>
  <summary>Details</summary>
Motivation: 在海洋和淡水环境中进行生态系统监测，需要一种高效、适应性强的机器人平台。

Method: 研究人员提出了一种鱼形机器人的更新机械设计，包括尾部摆动机制的初步设计。他们计划利用强化学习技术来学习机器人的最小驱动行为，并在FishGym模拟器上演示游泳行为并测试这些强化学习技术。

Result: 论文展示了尾部摆动机制的初步机械设计，并在FishGym模拟器上成功演示了机器人的游泳行为。强化学习技术将在该模拟器上进行测试。

Conclusion: 本文展示了一种适用于生态系统监测的欠驱动仿生水下机器人的初步机械设计和通过强化学习学习其行为的设想，并利用模拟器进行了初步验证，为后续的强化学习测试奠定了基础。

Abstract: In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on

</details>


### [413] [Rapidly Learning Soft Robot Control via Implicit Time-Stepping](https://arxiv.org/abs/2511.06667)
*Andrew Choi,Dezhong Tong*

Main category: cs.RO

TL;DR: 本文提出通过隐式时间步进和delta自然曲率控制，实现软体机器人策略的快速学习，显著提高模拟速度且不牺牲精度。


<details>
  <summary>Details</summary>
Motivation: 刚体模拟器在策略学习中已成主流，但软体机器人模拟框架稀缺，难以使用，且连续介质力学模拟计算成本高昂，导致策略学习难以实现。

Method: 采用通用、全隐式的软体模拟器DisMech，利用隐式时间步进方法，并引入delta自然曲率控制（类似于刚性机械臂的delta关节位置控制）。通过在四种不同的软体机械臂任务中与广泛使用的Elastica框架进行对比评估。

Result: 隐式时间步进在非接触情况下实现高达6倍的速度提升，在接触密集场景下实现高达40倍的速度提升。通过sim-to-sim评估，证明这种加速并未牺牲精度。

Conclusion: 隐式时间步进结合delta自然曲率控制，为软体机器人策略学习提供了显著的加速，且保持了高精度，解决了现有软体模拟框架的计算效率问题。

Abstract: With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.

</details>


### [414] [How Do VLAs Effectively Inherit from VLMs?](https://arxiv.org/abs/2511.06619)
*Chuheng Zhang,Rushuai Yang,Xiaoyu Chen,Kaixin Wang,Li Zhao,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 本文提出了GrinningFace诊断基准，通过表情符号桌面操作任务，评估视觉-语言-动作（VLA）模型如何有效继承大型视觉-语言模型（VLM）的先验知识，并发现保留VLM先验对于VLA的泛化至关重要。


<details>
  <summary>Details</summary>
Motivation: VLA模型有望实现泛化具身控制，通常通过利用大型视觉-语言模型（VLM）丰富的视觉语义先验。然而，核心问题是如何让VLA有效继承VLM的先验知识。

Method: 引入了GrinningFace诊断基准，这是一个表情符号桌面操作任务。机器人根据语言指令将物体放置到打印的表情符号上。该任务利用了在互联网规模VLM预训练数据中普遍存在但在标准机器人数据集中缺失的表情符号知识，作为衡量VLM先验知识有效迁移的代理。该诊断任务在模拟环境和真实机器人上均有实现，并比较了参数高效微调、VLM冻结、协同训练、预测离散动作和预测潜在动作等多种知识迁移技术。

Result: 成功的任务完成表明VLM先验知识有效迁移到具身控制。通过系统评估，研究工作不仅证明了保留VLM先验对于VLA泛化能力的关键重要性，还为未来开发真正可泛化的具身AI系统提供了指导。

Conclusion: 保留VLM先验对于VLA模型的泛化至关重要。本研究为未来开发可泛化的具身AI系统提供了指导方针。

Abstract: Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.

</details>


### [415] [Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots](https://arxiv.org/abs/2511.06673)
*Joel Kemp,Andre Farinha,David Howard,Krishna Manaswi Digumarti,Josh Pinskier*

Main category: cs.RO

TL;DR: 本文提出了一种参数化可伸缩软气动执行器（PTSPAs），通过参数化几何生成器实现定制设计，并系统探索了其性能。该执行器能够轴向膨胀，适用于可展开结构和狭窄空间操作，并在一个可展开软体四足机器人中得到了应用。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有极高的设计自由度，能够实现任意方向的力和构型变换，但其设计自由度因“维度诅咒”而难以有效利用。研究旨在寻找一种可追踪、模块化的方法，以充分利用软体结构的自由形态特性，创造丰富的具身行为，特别是在需要大幅度长度变化和狭窄空间操作的场景。

Method: 研究引入了一种参数化的软体执行器类别——可编程伸缩软气动执行器（PTSPAs）。开发了一个参数化几何生成器，能够根据高级输入定制执行器模型。通过半自动化实验和系统性参数探索，研究了新的设计空间，并对执行器的伸长/弯曲、膨胀和刚度等性能进行了表征。最后，将这些执行器应用于一个可展开的软体四足机器人中进行演示。

Result: PTSPAs在充气时能够轴向膨胀，非常适合可展开结构和在具有挑战性的狭窄空间中进行操作。研究揭示了关键设计参数与执行器性能（伸长/弯曲、膨胀和刚度）之间清晰的关系。成功展示了该执行器在一个可展开软体四足机器人中的应用，其腿部可展开以行走，从而实现对狭窄空间的自动适应。

Conclusion: PTSPAs为可展开和变形成形结构以及任何需要大幅度长度变化的场景提供了一种新的设计范式。它们通过参数化设计，有效解决了软体机器人设计自由度难以利用的问题，为未来软体机器人的开发开辟了新途径。

Abstract: Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.

</details>


### [416] [Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06745)
*Lan Thi Ha Nguyen,Kien Ton Manh,Anh Do Duc,Nam Pham Hai*

Main category: cs.RO

TL;DR: 本文提出PI-RIG，通过将物理约束融入变分自编码器（VAE）中，解决了自监督目标条件强化学习中生成不切实际目标的问题，从而提高了机器人技能学习的效率。


<details>
  <summary>Details</summary>
Motivation: 自监督目标条件强化学习允许机器人无需人工监督自主学习技能，但现有方法（如RIG）在学习到的潜在空间中生成的目标可能在物理上不可行，这阻碍了学习效率。

Method: 本文提出了Physics-Informed RIG（PI-RIG），通过一种新型的增强物理信息变分自编码器（Enhanced p3-VAE）将物理约束直接整合到VAE训练过程中。其核心创新在于将潜在空间明确地分为控制物体动力学的物理变量和捕获视觉外观的环境因素，并通过微分方程约束和守恒定律强制执行物理一致性，从而生成符合物体永恒性、碰撞约束和动态可行性等物理原则的目标。

Result: 通过广泛实验证明，这种物理信息的目标生成显著提高了所提出目标的质量，从而在包括抓取、推动和放置在内的视觉机器人操作任务中实现了更有效的探索和更好的技能获取。

Conclusion: PI-RIG通过整合物理约束到目标生成中，能够生成物理上一致且可实现的目标，显著提升了自监督目标条件强化学习的效率和机器人的技能获取能力。

Abstract: Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.

</details>


### [417] [Semi-distributed Cross-modal Air-Ground Relative Localization](https://arxiv.org/abs/2511.06749)
*Weining Lu,Deer Bin,Lian Ma,Ming Ma,Zhihao Ma,Xiangyang Chen,Longfei Wang,Yixiao Feng,Zhouxian Jiang,Yongliang Shi,Bin Liang*

Main category: cs.RO

TL;DR: 本文提出一种半分布式跨模态空地相对定位框架，通过UAV和UGV独立SLAM并利用深度学习关键点和描述符，实现高效、高精度、低带宽的空地相对位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有机器人相对定位方法主要依赖于分布式多机器人SLAM系统，这些系统通常要求传感器配置相同，并与所有机器人的状态估计紧密耦合，从而限制了灵活性和精度。为了解决这些问题，本研究旨在利用UGV多传感器集成能力，开发一种更灵活、准确的空地相对定位方法。

Method: 该方法构建了一个半分布式跨模态空地相对定位框架。UAV和UGV独立执行SLAM，并提取基于深度学习的关键点和全局描述符，从而将相对定位与所有代理的状态估计解耦。UGV利用激光雷达、相机和IMU进行局部束调整（BA），以快速获得准确的相对位姿估计。BA过程采用稀疏关键点优化，分为两阶段：首先优化从激光惯性里程计（LIO）插值的相机位姿，然后估计UGV和UAV之间的相对相机位姿。此外，还实现了使用深度学习描述符的增量式闭环检测算法，以高效维护和检索关键帧。通信仅传输关键点像素及其描述符。

Result: 实验结果表明，该方法在精度和效率方面均表现出色。与传统多机器人SLAM方法传输图像或点云不同，该方法仅传输关键点像素及其描述符，有效将通信带宽限制在0.3 Mbps以下。

Conclusion: 本文提出的半分布式跨模态空地相对定位框架，通过独立SLAM、深度学习关键点和两阶段BA优化，实现了高精度、高效率的相对定位，并显著降低了通信带宽，解决了传统多机器人SLAM在灵活性和精度上的局限性。

Abstract: Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.

</details>


### [418] [SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation](https://arxiv.org/abs/2511.06754)
*Taisei Hanyu,Nhat Chung,Huy Le,Toan Nguyen,Yuki Ikebe,Anthony Gunderman,Duy Nguyen Ho Minh,Khoa Vo,Tung Kieu,Kashu Yamazaki,Chase Rainwater,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: 该研究探索了以对象和对象关系为中心的紧凑表示，作为多任务机器人操作的基础。为此，他们提出了LIBERO+基准数据集和基于槽注意力（Slot-attention）的框架SlotVLA，以实现高效、可解释的视觉运动控制。


<details>
  <summary>Details</summary>
Motivation: 受人类对离散对象及其关系推理的启发，研究旨在解决现有机器人多任务模型依赖密集嵌入（混合对象和背景信息）所导致的效率和可解释性问题，寻求更结构化、高效和可解释的视觉运动控制方法。

Method: 1. 引入LIBERO+：一个细粒度基准数据集，提供对象中心注释（边界框、掩码级别标签和实例级时间跟踪），以支持紧凑和可解释的视觉运动表示。2. 提出SlotVLA：一个基于槽注意力（Slot-attention）的框架，用于捕捉对象及其关系以进行动作解码。它包括一个基于槽的视觉分词器（用于维护一致的时间对象表示）、一个以关系为中心的解码器（用于生成任务相关嵌入）和一个LLM驱动的模块（将嵌入转换为可执行动作）。

Result: 在LIBERO+上的实验表明，对象中心槽和对象关系槽表示显著减少了所需的视觉tokens数量，同时提供了具有竞争力的泛化能力。

Conclusion: LIBERO+和SlotVLA共同为推进以对象关系为中心的机器人操作提供了一个紧凑、可解释且有效的基础。

Abstract: Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.

</details>


### [419] [Human-Level Actuation for Humanoids](https://arxiv.org/abs/2511.06796)
*MD-Nazmus Sunbeam*

Main category: cs.RO

TL;DR: 本文提出一个全面的框架，包括DoF图谱、HEE和HLAS，用于量化和比较仿人机器人“人类水平”的驱动能力，解决现有峰值规格无法反映实际任务性能的问题。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人常声称达到“人类水平”驱动，但此说法缺乏量化依据。峰值扭矩或速度规格无法有效评估关节在特定姿态和速率下能否提供正确的扭矩、功率和耐力组合，因此需要一个可衡量和可比较的综合框架。

Method: 该方法包含三个组成部分：1. 运动学**DoF图谱**：使用ISB规范标准化关节坐标系和运动范围，确保人机在相同参考系中比较。2. **人类等效包络（HEE）**：通过测量机器人在特定关节角度和速率下是否同时满足人类扭矩和功率需求来定义每个关节的要求，并根据特定任务（行走、爬楼梯、举重、触及、手部动作）的机械功进行加权。3. **人类水平驱动分数（HLAS）**：整合六个物理基础因素：工作空间覆盖（ROM和DoF）、HEE覆盖、扭矩模式带宽、效率和热可持续性。论文还提供了详细的测量协议（测力计、电功率监测、热测试）。

Result: 通过一个多关节仿人机器人的实例演示了HLAS的计算，结果表明HLAS能揭示执行器在齿轮比、带宽和效率之间的权衡，而这是峰值扭矩规格无法体现的。该框架的所有组成部分都以已发表的人体生物力学数据为基础。

Conclusion: 该框架可作为仿人机器人开发的H设计规范，也可作为驱动系统比较的基准标准。

Abstract: Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \emph{and} power simultaneously at the same joint angle and rate $(q,ω)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.

</details>


### [420] [Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots](https://arxiv.org/abs/2511.06801)
*Praveen Kumar,Tushar Sandhan*

Main category: cs.RO

TL;DR: 本文提出一个框架，将轻量级语义感知与在线A*规划器紧密集成，使经济实惠的机器人平台能够进行上下文感知的实时导航，避开传统规划器无法识别的关键视觉线索。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人导航系统依赖于几何精确但缺乏语义感知的LiDAR，无法区分重要物品和无害垃圾。尽管存在先进的语义分割技术，但尚未有工作成功将其集成到适用于低成本嵌入式硬件的实时路径规划器中。

Method: 该方法将轻量级感知模块与在线A*规划器紧密集成。感知系统利用语义分割模型识别用户定义的视觉约束，并将其作为非几何障碍物投影到持续更新的全局地图上。这使得机器人能够根据上下文重要性而非物理尺寸进行导航。

Result: 通过高保真模拟和真实机器人平台的广泛实验验证，该框架展示了鲁棒的实时性能，证明了经济高效的机器人能够安全地在复杂环境中导航，同时尊重传统规划器不可见的视觉线索。

Conclusion: 该论文成功弥合了感知与规划之间的关键差距，实现了一种在经济实惠的机器人平台上进行上下文感知导航的框架，使机器人能够根据上下文重要性进行导航，从而解决了传统导航系统的语义盲区问题。

Abstract: The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.

</details>


### [421] [Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation](https://arxiv.org/abs/2511.06892)
*Kailin Tong,Selim Solmaz,Kenan Mujkic,Gottfried Allmer,Bo Leng*

Main category: cs.RO

TL;DR: 本文提出一个多智能体AI框架，结合多模态大语言模型（MLLMs）和视觉感知，用于道路状况监控，实现了高召回率和消息格式正确性，但存在误报和语义理解不足，且Gemini-2.0-Flash表现优于Gemini-2.5-Flash。


<details>
  <summary>Details</summary>
Motivation: 传统道路状况检测方法在预定义场景中表现良好，但在未知情况下失效，并且缺乏对可靠交通建议至关重要的语义解释。

Method: 引入一个多智能体AI框架，该框架将多模态大语言模型（MLLMs，包括Gemini-2.0-Flash和Gemini-2.5-Flash）与基于视觉的感知相结合，用于道路状况监控。框架处理摄像头视频流，并协调专用智能体进行状况检测、距离估计、决策制定和C-ITS消息生成。评估在一个包含103张图像（来自TAD数据集的20个视频）的自定义数据集上进行。

Result: 在状况检测中达到100%的召回率和完美的消息模式正确性。然而，两种模型都存在误报，并且在车道数量、行驶车道状态和原因代码方面性能下降。令人惊讶的是，Gemini-2.5-Flash在检测准确性和语义理解方面不如Gemini-2.0-Flash，并且延迟更高。

Conclusion: 研究结果表明，未来工作应专注于为智能交通应用量身定制的专业LLMs或MLLMs的微调。

Abstract: Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.

</details>


### [422] [Vision-Based System Identification of a Quadrotor](https://arxiv.org/abs/2511.06839)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 本文探索了将基于视觉的系统辨识技术应用于四旋翼飞行器建模与控制，通过实验验证了其有效性，并为性能提升、故障检测等未来研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决四旋翼飞行器建模中，特别是推力与阻力系数方面的复杂性和局限性，并评估机载视觉系统在其中的应用潜力，以期提升飞行器性能和操作能力。

Method: 研究采用灰箱建模方法来处理不确定性，并评估了机载视觉系统的有效性。基于来自机载视觉系统的数据，设计了一个使用系统辨识模型的LQR控制器。

Result: 实验结果表明，通过基于视觉的系统辨识建立的模型之间表现出一致的性能，验证了该技术在四旋翼飞行器建模与控制中的有效性。

Conclusion: 研究强调了基于视觉的技术在增强四旋翼飞行器建模和控制方面的潜力，有助于提高性能和操作能力，并为未来在性能增强、故障检测和决策过程等方面的研究提供了见解。

Abstract: This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.

</details>


### [423] [Integration of Visual SLAM into Consumer-Grade Automotive Localization](https://arxiv.org/abs/2511.06919)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 本文提出将视觉SLAM与车辆横向动力学模型融合，以实现消费级车辆在实际驾驶条件下的在线陀螺仪校准，显著提高定位精度。


<details>
  <summary>Details</summary>
Motivation: 消费级车辆的自我运动估计目前依赖于本体感受传感器（轮式里程计和IMU），其性能受系统误差和校准限制。尽管视觉惯性SLAM在机器人领域已是标准，但其在汽车自我运动估计中的整合仍未充分探索。

Method: 我们提出了一个框架，将视觉SLAM与车辆横向动力学模型融合，以在实际驾驶条件下实现陀螺仪的在线校准。

Result: 实验结果表明，基于视觉的整合显著提高了陀螺仪校准精度，从而增强了整体定位性能。在专有和公共数据集上都展示了改进的性能，并在公共基准上比现有最先进方法取得了更高的定位精度。

Conclusion: 将视觉SLAM集成到消费级车辆定位系统中，是提高汽车定位精度的有前景的途径。

Abstract: Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.

</details>


### [424] [Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics](https://arxiv.org/abs/2511.06998)
*Jin Huang,Yingqiang Wang,Ying Chen*

Main category: cs.RO

TL;DR: 本文提出Raspi^2USBL，一个基于树莓派的开源低成本水下定位系统，通过创新的硬件和软件设计，实现了研究级的水下定位精度，旨在降低水下机器人研究的门槛并促进合作。


<details>
  <summary>Details</summary>
Motivation: 由于全球导航卫星系统（GNSS）信号无法穿透海面，水下机器人的精确水下定位仍然是一个根本性挑战。

Method: Raspi^2USBL是一个基于树莓派的开源被动式倒置超短基线（piUSBL）定位系统。它由一个无源声学接收器和一个有源信标组成。接收器采用模块化硬件架构，集成了水听器阵列、多通道前置放大器、恒温晶体振荡器（OCXO）、树莓派5和MCC系列数据采集（DAQ）板。信标则包含阻抗匹配网络、功率放大器和发射换能器。开放源代码的C++软件框架提供高精度时钟同步和单向传播时间（OWTT）消息触发，并执行实时信号处理，包括匹配滤波、阵列波束形成和自适应增益控制，以估计信号的飞行时间（TOF）和到达方向（DOA）。

Result: Raspi^2USBL系统在消声水池、淡水湖和开阔海域进行了实验验证。结果表明，其斜距精度优于0.1%，方位角精度在0.1°以内，并在长达1.3公里的操作距离内表现稳定。

Conclusion: 研究结果证实，低成本、可复现的硬件能够提供研究级的水下定位精度。通过开源硬件和软件，Raspi^2USBL提供了一个统一的参考平台，降低了水下机器人实验室的准入门槛，促进了可复现性，并推动了水下声学导航和集群机器人领域的合作创新。

Abstract: Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.

</details>


### [425] [HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects](https://arxiv.org/abs/2511.07081)
*Guanghu Xie,Mingxu Li,Songwei Wu,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: 本文提出HDCNet，一个结合Transformer、CNN和Mamba的新型深度补全网络，旨在解决透明和反射物体深度感知难题，并在深度补全和机器人抓取任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度传感器难以对透明和反射物体提供可靠的深度测量，严重限制了机器人在感知和抓取任务中的表现。

Method: HDCNet采用双分支Transformer-CNN编码器提取模态特定特征。在编码器浅层引入轻量级多模态融合模块整合低级特征。在网络瓶颈处，开发了Transformer-Mamba混合融合模块，以深度融合高级语义和全局上下文信息。

Result: HDCNet在多个公共数据集上实现了深度补全任务的最先进（SOTA）性能。机器人抓取实验表明，HDCNet显著提高了透明和反射物体的抓取成功率，最高提升了60%。

Conclusion: HDCNet通过创新的网络架构有效解决了透明和反射物体的深度感知挑战，显著提升了深度补全的准确性和鲁棒性，并大幅改善了机器人对这类物体的抓取成功率。

Abstract: Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.

</details>


### [426] [Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2511.07155)
*Thomas Steinecker,Alexander Bienemann,Denis Trescher,Thorsten Luettel,Mirko Maehlisch*

Main category: cs.RO

TL;DR: 本文提出了一种框架，通过虚拟与真实车辆之间的时空对齐策略，将运动规划与车辆控制解耦，从而实现了基于强化学习的运动规划从仿真到真实车辆的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域（特别是车辆）面临挑战，主要原因在于车辆动力学复杂性以及仿真与现实之间的巨大差异，导致难以准确建模真实世界动力学，阻碍了仿真训练的强化学习智能体直接迁移到现实世界。

Method: 该框架通过虚拟车辆与真实系统之间的时空对齐策略，将运动规划与车辆控制解耦。首先，在仿真中使用运动学自行车模型训练一个输出连续控制动作的强化学习智能体。然后，将其行为蒸馏到一个轨迹预测智能体中，生成有限视野的自车轨迹，实现虚拟与真实车辆的同步。在部署时，采用Stanley控制器管理横向动力学，并通过自适应更新机制维持纵向对齐，以补偿虚拟与真实轨迹之间的偏差。

Result: 该方法在真实车辆上得到了验证，结果表明所提出的对齐策略能够实现基于强化学习的运动规划从仿真到现实的鲁棒零样本迁移，并成功地将高层轨迹生成与低层车辆控制解耦。

Conclusion: 所提出的对齐策略通过有效解耦运动规划与车辆控制，实现了基于强化学习的运动规划从仿真到现实的鲁棒零样本迁移。

Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

</details>


### [427] [Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets](https://arxiv.org/abs/2511.07175)
*Marvin Rüdt,Constantin Enke,Kai Furmans*

Main category: cs.RO

TL;DR: 本文提出了一种自动化的连续空间路线图生成方法，专为内部物流中的移动机器人车队设计，该方法结合了运输需求和距离约束，在多个用例中显著优于现有基线，实现了高效且鲁棒的路径规划。


<details>
  <summary>Details</summary>
Motivation: 内部物流中移动机器人车队的有效路径规划至关重要，因为延迟和死锁会严重降低系统吞吐量。现有路线图方法存在局限性：基于网格的方法牺牲了几何精度，而连续空间方法又忽视了实际约束。本研究旨在弥合这一差距，提供一种同时兼顾几何精度和实际约束的路线图生成方案。

Method: 该方法在连续空间中运行，整合了站点间的运输需求，并强制执行节点和边的最小距离约束。具体步骤包括：自由空间离散化、运输需求驱动的K-最短路径优化以及路径平滑，从而生成专门适用于内部物流应用的路线图。

Result: 在多个内部物流用例中进行评估，结果表明所提出的方法始终优于已建立的基线（4连接网格、8连接网格和随机采样），实现了更低的结构复杂性、更高的冗余度以及接近最优的路径长度。

Conclusion: 该方法能够实现移动机器人车队高效且鲁棒的路径规划，从而有效解决内部物流中的路由挑战。

Abstract: Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.

</details>


### [428] [Robotic versus Human Teleoperation for Remote Ultrasound](https://arxiv.org/abs/2511.07275)
*David Black,Septimiu Salcudean*

Main category: cs.RO

TL;DR: 本文比较了远程超声检查中人工远程操作与机器人远程操作的性能差异，发现人工操作在完成时间、定位精度上无显著差异，但在施力一致性、实用性和可及性方面更优。


<details>
  <summary>Details</summary>
Motivation: 诊断性医学超声检查需要专业知识，但在农村地区专家稀缺，导致就医困难。远程超声技术（包括机器人和近期的人工远程操作）旨在解决此问题，但两者尚未进行比较，其相对优势不明。人工远程操作可能因成本和复杂性较低而更适合小型社区，前提是其性能可与机器人相媲美。

Method: 研究评估了人工与机器人远程操作之间的差异，检查了设置时间、灵活性等实用方面，并通过实验比较了完成时间、位置跟踪和施力一致性等性能指标。

Result: 研究发现，人工远程操作在完成时间或位置精度方面与机器人操作无统计学上的显著差异（平均差异分别为1.8%和0.5%），且能提供更一致的施力，同时在实用性和可及性方面具有显著优势。

Conclusion: 人工远程超声操作在性能上与机器人操作相当，但在施力一致性、实用性和可及性方面表现更优，这表明其在解决农村地区医疗可及性问题上具有巨大潜力。

Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.

</details>


### [429] [PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving](https://arxiv.org/abs/2511.07292)
*Simon Gerstenecker,Andreas Geiger,Katrin Renz*

Main category: cs.RO

TL;DR: 本文提出PlanT 2.0，一个轻量级、以对象为中心的自动驾驶规划Transformer，用于系统性地分析模型失效、偏差和捷径学习。通过扰动输入和观察预测，揭示了现有模型在场景理解、专家行为和轨迹过拟合方面的不足，并呼吁转向以数据为中心的发展。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶研究过度关注基准性能和方法创新，而忽视对模型失效、偏差和捷径学习的深入分析，导致增量改进而非根本性理解。虽然模型失效显而易见，但其深层原因难以捉摸，这促使作者进行系统性研究。

Method: 本文引入PlanT 2.0，一个轻量级、以对象为中心的规划Transformer，专为CARLA环境下的自动驾驶研究设计。其对象级表示允许通过扰动输入（如改变对象位置、增删对象）进行受控分析。通过对PlanT进行多项升级，以应对CARLA Leaderboard 2.0中的挑战性场景。

Result: PlanT 2.0在Longest6 v2、Bench2Drive和CARLA验证路线上取得了最先进的性能。分析揭示了模型存在的深入问题，包括因障碍物多样性不足导致的场景理解缺失、僵化的专家行为导致的易被利用的捷径学习，以及对固定专家轨迹集的过拟合。

Conclusion: 基于研究发现，本文主张转向以数据为中心的发展模式，重点关注构建更丰富、更鲁棒、偏差更少的数据集。代码和模型已开源。

Abstract: Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.

</details>


### [430] [Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications](https://arxiv.org/abs/2511.07375)
*Shaohang Han,Joris Verhagen,Jana Tumova*

Main category: cs.RO

TL;DR: 本文提出了一种基于信号时序逻辑（STL）的运动规划方法，通过将STL合成转化为轨迹优化问题，并引入最大/最小算子的精确重构以实现无近似误差的可微性，从而得到一种精确、平滑且可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 信号时序逻辑（STL）是一种描述时空需求（spatial-temporal requirements）的有效形式化工具。研究动机在于如何利用STL的鲁棒性语义，将STL下的运动规划问题转化为可微分的轨迹优化问题，以实现精确且实用的规划方法。

Method: 研究方法是将STL合成问题建模为轨迹优化问题，并利用STL的鲁棒性语义。为了获得一个无近似误差的可微分问题，文中引入了对最大（max）和最小（min）算子的精确重构（exact reformulation）。

Result: 所提出的方法是精确的、平滑的且可靠的（sound）。通过数值模拟验证了该方法的实际性能。

Conclusion: 研究成功开发了一种精确、平滑且可靠的运动规划方法，该方法通过将STL合成转化为轨迹优化问题，并利用最大/最小算子的精确重构实现了无近似误差的可微性，并在实践中表现良好。

Abstract: We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.

</details>


### [431] [Residual Rotation Correction using Tactile Equivariance](https://arxiv.org/abs/2511.07381)
*Yizhe Zhu,Zhang Ye,Boce Hu,Haibo Zhao,Yu Qi,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: EquiTac是一个利用手持物体旋转的SO(2)对称性来提高视觉触觉策略学习的样本效率和泛化能力的框架，尤其适用于接触丰富的操作任务。


<details>
  <summary>Details</summary>
Motivation: 视觉触觉策略有助于接触丰富的操作，但触觉数据采集成本高昂，因此提高样本效率是开发此类策略的关键需求。

Method: EquiTac框架首先从视觉触觉传感器的原始RGB输入重建表面法线，使法向量场的旋转对应于手持物体的旋转。然后，一个SO(2)等变网络预测一个残余旋转动作，在测试时增强基础视觉运动策略，实现实时旋转校正，无需额外的重新定向演示。

Result: 在真实机器人上，EquiTac仅用少量训练样本就能实现对未见手持方向的鲁棒零样本泛化，而基线方法即使有更多训练数据也无法达到。这是首个明确编码触觉等变性进行策略学习的方法。

Conclusion: EquiTac通过显式编码触觉等变性，提供了一个轻量级、对称感知的模块，显著提高了接触丰富任务的可靠性，并提升了样本效率和泛化能力。

Abstract: Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.

</details>


### [432] [Unified Humanoid Fall-Safety Policy from a Few Demonstrations](https://arxiv.org/abs/2511.07407)
*Zhengjie Xu,Ye Li,Kwan-yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 本文提出了一种统一的策略，通过融合人类演示、强化学习和自适应扩散记忆，使人形机器人能够自主安全地处理整个跌倒和恢复过程，包括跌倒预防、冲击缓解和快速站立。


<details>
  <summary>Details</summary>
Motivation: 人形机器人固有的跌倒风险是其移动性的主要安全挑战。现有方法仅处理跌倒的孤立方面（如避免跌倒、控制下降或跌倒后站立），缺乏应对真实跌倒的集成策略。因此，研究旨在超越简单的保持平衡，使整个跌倒和恢复过程变得安全和自主。

Method: 研究通过融合稀疏的人类演示、强化学习以及基于自适应扩散的安全反应记忆，学习了全身行为。这些行为将跌倒预防、冲击缓解和快速恢复统一在一个策略中。

Result: 在模拟和Unitree G1机器人上的实验表明，该方法实现了鲁棒的仿真到现实迁移、更低的冲击力，并在各种干扰下始终保持快速恢复。这证明了其在实际环境中提高机器人安全性和韧性的潜力。

Conclusion: 该研究通过提供一个统一的跌倒处理策略，显著提升了人形机器人在真实环境中的安全性和韧性，能够预防跌倒、减轻冲击并快速恢复。

Abstract: Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.

</details>


### [433] [Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective](https://arxiv.org/abs/2511.07410)
*Hao Wang,Sathwik Karnik,Bea Lim,Somil Bansal*

Main category: cs.RO

TL;DR: 本文从控制理论角度探讨如何将视觉语言模型（VLMs）有效应用于机器人闭环符号规划，并研究了控制范围和预热启动对VLM符号规划器性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视觉语言模型（VLMs）已被广泛用于具身符号规划，但它们在闭环符号规划中的有效应用仍未充分探索。由于其黑箱特性，LLMs和VLMs可能产生不可预测或代价高昂的错误，这对高层机器人规划提出了挑战。

Method: 本研究从控制理论角度，探讨如何将VLMs用作机器人应用的闭环符号规划器。具体研究了控制范围（control horizon）和预热启动（warm-starting）如何影响VLM符号规划器的性能。设计并进行了受控实验以获取广泛适用的见解。

Result: 通过受控实验获得了对利用VLMs作为闭环符号规划器具有广泛适用性的见解。

Conclusion: 讨论了可以帮助提高VLM符号规划器性能的建议。

Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.

</details>


### [434] [Robot Learning from a Physical World Model](https://arxiv.org/abs/2511.07416)
*Jiageng Mao,Sicheng He,Hao-Ning Wu,Yang You,Shuyang Sun,Zhicheng Wang,Yanan Bao,Huizhong Chen,Leonidas Guibas,Vitor Guizilini,Howard Zhou,Yue Wang*

Main category: cs.RO

TL;DR: PhysWorld框架通过结合视频生成和物理世界建模，使机器人能够从生成视频中学习，从而将视觉指导转化为物理可执行的机器人动作，实现零样本通用操作。


<details>
  <summary>Details</summary>
Motivation: 虽然最近的视频生成模型可以从语言命令和图像合成逼真的视觉演示，但直接将生成的视频中的像素运动重定向到机器人会忽略物理定律，导致操作不准确。

Method: PhysWorld接收一张图像和一个任务命令，生成任务条件视频，并从这些视频中重建底层物理世界。然后，通过基于物理世界模型的以物体为中心的残差强化学习，将生成的视频运动转化为物理上准确的动作。

Result: 在多样化的真实世界任务中，PhysWorld显著提高了操作精度，优于现有方法。它消除了对真实机器人数据收集的需求，并实现了零样本可泛化的机器人操作。

Conclusion: PhysWorld成功地将视频生成模型提供的隐式视觉指导转化为物理上可执行的机器人轨迹，解决了传统方法忽略物理的问题，提高了操作准确性，并实现了零样本泛化能力。

Abstract: We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.

</details>


### [435] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: 本文提出Lightning Grasp算法，通过引入Contact Field数据结构，实现了比现有技术快几个数量级的实时多功能灵巧手抓取生成，并支持对不规则物体进行无监督抓取。


<details>
  <summary>Details</summary>
Motivation: 灵巧手实时多样化抓取合成是机器人和计算机图形学中尚未解决的核心挑战。现有方法存在诸多局限，如需要精心调整的能量函数和对初始化敏感。

Method: 本文提出一种名为Lightning Grasp的高性能程序化抓取合成算法。其核心思想是通过一个简单高效的数据结构——Contact Field，将复杂的几何计算与搜索过程解耦，从而简化问题复杂性，实现前所未有的搜索速度。

Result: Lightning Grasp算法实现了比现有最先进方法快几个数量级的速度提升，并能够对不规则的工具状物体进行无监督抓取生成。该方法避免了先前方法的许多限制，如对能量函数和敏感初始化的需求。

Conclusion: Lightning Grasp算法通过Contact Field的关键洞察，在灵巧手抓取合成方面取得了突破性进展。作者开源了该系统，以推动机器人操作领域的进一步创新。

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [436] [Learning-Based Multi-Stage Strategy for a Fixed-Wing Aircraft to Evade a Missile Detected at a Short Distance](https://arxiv.org/abs/2511.05828)
*Zhiguan Niu,Xiaochao Zhou,Hao Xiong*

Main category: eess.SY

TL;DR: 本研究提出了一种受自然界追逐-规避启发的多阶段强化学习规避策略，以提高飞机对抗现代导弹的生存能力，并在高保真模拟中取得了显著的规避成功率。


<details>
  <summary>Details</summary>
Motivation: 现代空战中，导弹对飞机构成重大威胁，且越来越难以探测和干扰。传统的规避策略受计算和气动限制，而现有基于学习的方法对有人驾驶飞机对抗现代导弹而言说服力不足。因此，需要开发新的策略来增强飞机的生存能力。

Method: 该研究受瞪羚与猎豹追逐-规避游戏的启发，提出了一种多阶段强化学习规避策略。该策略学习了三种子策略：大方位角转向规避、小方位角保持远离以及短距离执行敏捷激进机动规避。在每个阶段，根据距离和方位角激活其中一种策略。通过建模F-16飞机和导弹的高保真模拟环境，在各种条件下与基线策略进行比较评估。

Result: 实验结果表明，所提出的方法表现出卓越的性能，使F-16飞机在导弹速度范围（800-1400 m/s）、最大过载（40-50 g）、探测距离（5000-15000 m）和随机方位角等多种条件下，成功规避导弹的概率达到80.89%。当导弹在8000米外被探测到时，成功率提高到85.06%。

Conclusion: 所提出的多阶段强化学习规避策略能够有效提高飞机对抗现代导弹的生存能力，在各种复杂条件下均展现出优异的规避性能。

Abstract: Missiles pose a major threat to aircraft in modern air combat. Advances in technology make them increasingly difficult to detect until they are close to the target and highly resistant to jamming. The evasion maneuver is the last line of defense for an aircraft. However, conventional rule-based evasion strategies are limited by computational demands and aerodynamic constraints, and existing learning-based approaches remain unconvincing for manned aircraft against modern missiles. To enhance aircraft survivability, this study investigates missile evasion inspired by the pursuit-evasion game between a gazelle and a cheetah and proposes a multi-stage reinforcement learning-based evasion strategy. The strategy learns a large azimuth policy to turn to evade, a small azimuth policy to keep moving away, and a short distance policy to perform agile aggressive maneuvers to avoid. One of the three policies is activated at each stage based on distance and azimuth. To evaluate performance, a high-fidelity simulation environment modeling an F-16 aircraft and missile under various conditions is used to compare the proposed approach with baseline strategies. Experimental results show that the proposed method achieves superior performance, enabling the F-16 aircraft to successfully avoid missiles with a probability of 80.89 percent for velocities ranging from 800 m/s to 1400 m/s, maximum overloads from 40 g to 50 g, detection distances from 5000 m to 15000 m, and random azimuths. When the missile is detected beyond 8000 m, the success ratio increases to 85.06 percent.

</details>


### [437] [Disentangled Control of Multi-Agent Systems](https://arxiv.org/abs/2511.05900)
*Ruoyu Lin,Gennaro Notomista,Magnus Egerstedt*

Main category: eess.SY

TL;DR: 本文提出一个通用的多智能体控制合成框架，具有收敛性保证，适用于各种问题，并解决了智能体间纠缠动力学的去中心化难题，支持多目标和实时应用。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中存在诸多挑战，尤其是不同智能体之间纠缠动力学的去中心化问题。现有方法难以在复杂图拓扑和显式时间依赖目标函数下提供收敛性保证。此外，无近似的时间变化密度函数去中心化覆盖控制是一个长期未解决的开放问题。

Method: 本文开发了一个通用的多智能体控制合成框架。该框架系统性地解决了智能体间纠缠动力学的去中心化问题，并天然支持多目标机器人和实时实现。

Result: 该框架在三个实验中展示了其通用性和有效性：时变领导者-跟随者编队控制、无任何近似的时变密度函数去中心化覆盖控制（解决了长期存在的开放问题），以及密集环境中的安全编队导航。该框架提供了收敛性保证。

Conclusion: 所提出的通用多智能体控制合成框架在广泛的问题中具有收敛性保证，有效解决了纠缠动力学去中心化等挑战，并成功应用于多项实验，包括解决了一个长期未决的开放问题，证明了其通用性和有效性。

Abstract: This paper develops a general framework for multi-agent control synthesis, which applies to a wide range of problems with convergence guarantees, regardless of the complexity of the underlying graph topology and the explicit time dependence of the objective function. The proposed framework systematically addresses a particularly challenging problem in multi-agent systems, i.e., decentralization of entangled dynamics among different agents, and it naturally supports multi-objective robotics and real-time implementations. To demonstrate its generality and effectiveness, the framework is implemented across three experiments, namely time-varying leader-follower formation control, decentralized coverage control for time-varying density functions without any approximations, which is a long-standing open problem, and safe formation navigation in dense environments.

</details>


### [438] [Probe-and-Release Coordination of Platoons at Highway Bottlenecks with Unknown Parameters](https://arxiv.org/abs/2511.06026)
*Yi Gao,Xi Xiong,Karl H. Johansson,Li Jin*

Main category: eess.SY

TL;DR: 本文提出了一种“探测与释放”算法，用于在混合自动驾驶瓶颈处协调车队，同时考虑时变需求、随机车队规模和容量故障，以实现交通稳定并保证估计误差和交通队列有界。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决在混合自动驾驶瓶颈处，面对时变交通需求、随机互联自动驾驶车辆（CAV）车队规模和容量故障等实际因素时，如何有效协调CAV车队，以平滑CAV车队与非CAV交通的互动并稳定交通流。

Method: 基于流体排队模型，开发了一种“探测与释放”算法，该算法能够同时估计环境参数并协调CAV车队以稳定交通。通过构建一个联合惩罚估计误差和交通队列的Lyapunov函数以及嵌入式马尔可夫过程的漂移论证来证明其性能。通过微观仿真环境验证了所提出的算法，并与一种代表性的深度强化学习方法在控制性能和计算效率方面进行了比较。

Result: 该算法能够确保有界的估计误差和有界的交通队列。在标准微观仿真环境中验证了其有效性，并在控制性能和计算效率方面优于或与代表性的深度强化学习方法相当。

Conclusion: 所提出的“探测与释放”算法能够有效协调CAV车队，稳定交通流，并能应对时变需求、随机车队规模和容量故障等实际挑战，同时保证了估计误差和交通队列的有界性，展现了良好的性能和计算效率。

Abstract: This paper considers coordination of platoons of connected and autonomous vehicles (CAVs) at mixed-autonomy bottlenecks in the face of three practically important factors, viz. time-varying traffic demand, random CAV platoon sizes, and capacity breakdowns. Platoon coordination is essential to smoothen the interaction between CAV platoons and non-CAV traffic. Based on a fluid queuing model, we develop a "probe-and-release" algorithm that simultaneously estimates environmental parameters and coordinates CAV platoons for traffic stabilization. We show that this algorithm ensures bounded estimation errors and bounded traffic queues. The proof builds on a Lyapunov function that jointly penalizes estimation errors and traffic queues and a drift argument for an embedded Markov process. We validate the proposed algorithm in a standard micro-simulation environment and compare against a representative deep reinforcement learning method in terms of control performance and computational efficiency.

</details>


### [439] [Parameter Recovery from Tangential Interpolations for Systems with an LFT Structure](https://arxiv.org/abs/2511.06011)
*Tong Zhou,Yubing Li*

Main category: eess.SY

TL;DR: 本文研究如何从线性时不变系统传递函数矩阵的值和导数中恢复系统参数，其中系统矩阵通过线性分数变换依赖于这些参数。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决如何从复杂平面上特定点和方向的传递函数矩阵值及其导数中，唯一且鲁棒地确定线性时不变系统的未知参数，特别是当这些参数通过线性分数变换嵌入系统矩阵时。

Method: 本文推导了一个向量不等式作为唯一确定系统参数的充要条件，在特定情况下简化为常数矩阵的满列秩要求。此外，提出了一种参数恢复方法，表示为一个带有秩约束的向量线性方程，并阐明了其鲁棒性。

Result: 主要结果包括：一个表示为向量不等式的系统参数唯一确定充要条件；一个用于恢复系统参数的向量线性方程方法；以及对所提出恢复方法鲁棒性的澄清。数值例子展示了该方法的特性以及引入导数信息在参数恢复中的有效性。

Conclusion: 该研究提供了一套理论条件和实用方法，用于从传递函数的值和导数中恢复线性时不变系统的参数，强调了导数信息引入的有效性和方法的鲁棒性。

Abstract: This paper investigates how to recover parameters of a linear time invariant system from values and derivatives of its transfer function matrix, along several particular directions at a prescribed set of points in the complex plane, in which system matrices depend on these parameters through a linear fractional transformation. A necessary and sufficient condition is derived for a unique determination of these system parameters, which is expressed by a vector inequality. Under some particular situations, this condition reduces to a full column rank requirement on a constant matrix. Moreover, a method is given to recover system parameters from these values and derivatives, which is expressed by a vector linear equation with some rank constraints, for which various methods exist for finding its solutions. Robustness of the suggested recovery method is also clarified. A numerical example is given to illustrate characteristics of the suggested method, as well as effectiveness of derivative information introduction in parameter recovery, in which natural frequency and damping ratio are to be recovered for a transfer function.

</details>


### [440] [Model-free Adaptive Output Feedback Vibration Suppression in a Cantilever Beam](https://arxiv.org/abs/2511.06084)
*Juan Augusto Paredes Salazar,Ankit Goel*

Main category: eess.SY

TL;DR: 本文提出了一种基于回顾成本优化的无模型自适应控制方法，用于抑制悬臂梁在未知扰动下的振动，并比较了位移和加速度反馈的性能，同时开发了滤波器以改善加速度反馈。


<details>
  <summary>Details</summary>
Motivation: 抑制由未知扰动引起的悬臂梁振动。

Method: 采用无模型自适应控制方法，将受谐波激励的悬臂梁建模为集总参数系统。基于回顾成本优化，开发了采样数据自适应控制器。反馈考虑位移和加速度测量，并设计了滤波器以从加速度数据中提取关键位移信息，以应对溢出效应。

Result: 比较了使用位移和加速度测量进行振动抑制的性能。通过滤波器增强了加速度数据的抑制性能。

Conclusion: 开发了一种有效的无模型自适应控制策略来抑制悬臂梁振动，并分析了不同反馈测量（位移和加速度）的有效性，提出了一种滤波器来优化加速度反馈的性能。

Abstract: This paper presents a model-free adaptive control approach to suppress vibrations in a cantilevered beam excited by an unknown disturbance. The cantilevered beam under harmonic excitation is modeled using a lumped parameter approach. Based on retrospective cost optimization, a sampled-data adaptive controller is developed to suppress vibrations caused by external disturbances. Both displacement and acceleration measurements are considered for feedback. Since acceleration measurements are more sensitive to spillover, which excites higher frequency modes, a filter is developed to extract key displacement information from the acceleration data and enhance suppression performance. The vibration suppression performance is compared using both displacement and acceleration measurements.

</details>


### [441] [Koopman Operator for Stability Analysis: Theory with a Linear--Radial Product Reproducing Kernel](https://arxiv.org/abs/2511.06063)
*Wentao Tang,Xiuzhen Ye*

Main category: eess.SY

TL;DR: 本文提出了一种利用线性核与Wendland径向核的乘积核构建再生核希尔伯特空间（RKHS）的方法，以学习Koopman算子。该方法确保了算子在RKHS上的不变性，并证明了其谱与系统稳定性（稳定时在单位圆内，分岔时移出）之间的关系，从而为非线性系统的稳定性分析和基于Koopman的控制提供了一种可证明的稳定性证书。


<details>
  <summary>Details</summary>
Motivation: Koopman算子能将非线性动力系统线性化，若能在再生核希尔伯特空间（RKHS）上良好定义，则可高效地从数据中学习。然而，为了进行稳定性分析和控制相关问题，所需的RKHS应同时兼顾平衡点的局部稳定性特征和状态空间上动力学的全局规律性。

Method: 通过使用线性核与Wendland径向核形成的乘积核来构建再生核希尔伯特空间（RKHS）。在此RKHS中，研究Koopman算子的作用，并分析其谱与系统平衡点稳定性的关系。该方法在特定平滑条件下证明了RKHS在Koopman算子作用下的不变性。

Result: 使用线性核与Wendland径向核形成的乘积核，所产生的RKHS在特定平滑条件下在Koopman算子作用下是不变的。当平衡点渐近稳定时，Koopman算子的谱被证明限制在单位圆内；而当发生分岔时，谱会移出单位圆。因此，具有可证明概率误差界限的学习到的Koopman算子提供了一个稳定性证书。研究还进行了数值验证，并讨论了这种谱-稳定性关系对基于Koopman的控制的潜在用途。

Conclusion: 通过利用特定乘积核构建的RKHS，可以有效地学习Koopman算子，并建立其谱与非线性系统稳定性之间的明确关系。这种方法为从数据中获得非线性系统的稳定性证书提供了一种可靠途径，并为基于Koopman的控制策略提供了重要的理论基础和工具。

Abstract: Koopman operator, as a fully linear representation of nonlinear dynamical systems, if well-defined on a reproducing kernel Hilbert space (RKHS), can be efficiently learned from data. For stability analysis and control-related problems, it is desired that the defining RKHS of the Koopman operator should account for both the stability of an equilibrium point (as a local property) and the regularity of the dynamics on the state space (as a global property). To this end, we show that by using the product kernel formed by the linear kernel and a Wendland radial kernel, the resulting RKHS is invariant under the action of Koopman operator (under certain smoothness conditions). Furthermore, when the equilibrium is asymptotically stable, the spectrum of Koopman operator is provably confined inside the unit circle, and escapes therefrom upon bifurcation. Thus, the learned Koopman operator with provable probabilistic error bound provides a stability certificate. In addition to numerical verification, we further discuss how such a fundamental spectrum--stability relation would be useful for Koopman-based control.

</details>


### [442] [A Multi-Criterion Approach to Smart EV Charging with CO2 Emissions and Cost Minimization](https://arxiv.org/abs/2511.06131)
*Luca Ambrosino,Khai Manh Nguyen,Minh Binh Vu,Riadh Zorgati,Laurent El Ghaoui,Giuseppe C. Calafiore*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的三步框架，用于智能电动汽车充电，旨在共同最小化充电成本和二氧化碳排放。该框架结合了受机组组合问题启发的发电模型和线性规划的充电优化模型，并在越南的真实数据上进行了验证，结果显示在减少成本和排放方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，如何实现既经济又环保的智能充电策略成为重要挑战。特别是在像越南这样碳密集型能源系统国家，减少充电相关的碳排放尤为关键。

Method: 该研究采用了三步框架：1) 设计一个线性模型（受机组组合问题启发），利用越南真实数据确定24小时内的最优发电组合，以估算时变二氧化碳排放。2) 将估算的二氧化碳排放转化为排放成本信号。3) 将此环境成本纳入一个线性规划（LP）智能充电优化模型中。

Result: 数值模拟结果表明，所提出的策略显著优于基线“先到先服务”（FIFS）方法，并在二氧化碳排放和充电成本方面均实现了显著降低，甚至优于另一种优化方法。

Conclusion: 该多目标优化框架具有支持更可持续和更具成本效益的电动汽车充电策略的巨大潜力。

Abstract: In this work, we propose a novel three-step framework for smart electric vehicle (EV) charging that jointly minimizes charging costs and CO2 emissions. Drawing inspiration from the classical Unit Commitment Problem (UCP), we first design a linear model to determine the optimal power generation mix over a 24-hour horizon, using real-world data from Vietnam, a country with a highly carbon intensive energy system. This allows us to estimate time-varying CO2 emissions and translate them into an emission cost signal. We then incorporate this environmental cost into a smart charging optimization model, formulated as a linear program (LP). Numerical simulations confirm that the proposed strategy significantly outperforms a baseline First-In-First-Served (FIFS) approach, achieving notable reductions in both CO2 emissions and charging costs also compared to another optimization approach. The results demonstrate the potential of this multiobjective optimization framework to support more sustainable and cost-efficient EV charging strategies.

</details>


### [443] [A Passive Software-Defined Radio-based mmWave Sensing System for Blind Integrated Communication and Sensing](https://arxiv.org/abs/2511.06199)
*Shiqi Liu,Hang Song,Bo Wei,Nopphon Keerativoranan,Jun-ichi Takada*

Main category: eess.SY

TL;DR: 本文开发了一种全被动毫米波感知系统，利用环境通信信号进行盲ISAC，通过差分接收器结构克服了未知发射信号和同步问题，并在多种场景下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信 (ISAC) 是未来6G的关键技术，尤其是在毫米波频段。然而，主动发射无线电波进行实验面临监管限制，且收发模块需要同步并共享发射信息。因此，需要一种无需主动发射、无需同步且对发射信号无先验知识的感知方案。

Method: 本文开发了一个仅包含被动接收模块的毫米波感知系统，该系统不依赖于发射端且无需同步。为解决未知发射信号和失真问题，引入了一种具有两个方向相反接收器的差分结构。该系统利用环境毫米波通信信号进行感知，避免了对现有系统的干扰。

Result: 该系统首先通过已知运动模式的金属板进行了验证，测得的多普勒频谱与仿真结果吻合良好。进一步在复杂场景中（如挥手、单人和多人运动检测）进行了评估，感知结果成功反映了相应的运动，证明了该系统在盲ISAC各种应用中的有效性。

Conclusion: 所开发的全被动毫米波感知系统能够利用环境通信信号进行盲ISAC，且无需主动发射和同步。其独特的差分接收结构有效应对了未知信号和失真，在信号检测和动态人类活动识别等领域具有广泛应用潜力。

Abstract: Integrated Sensing and Communication (ISAC) is considered as a key component of future 6G technologies, especially in the millimeter-wave (mmWave) bands. Recently, the performances of ISAC were experimentally evaluated and demonstrated in various scenarios by developing ISAC systems. These systems generally consist of coherent transmitting (Tx) and receiving (Rx) modules. However, actively transmitting radio waves for experiments is not easy due to regulatory restrictions of radio. Meanwhile, the Tx/Rx should be synchronized and Rx need the information of Tx. In this paper, a fully passive mmWave sensing system is developed with software-defined radio for blind ISAC. It only consists of a passive Rx module which does not depend on the Tx. Since the proposed system is not synchronized with Tx and has no knowledge of the transmitted signals, a differential structure with two oppositely-oriented receivers is introduced to realize the sensing function. This structure can mitigate the influences of unknown source signals and other distortions. With the proposed sensing system, the ambient mmWave communication signals are leveraged for sensing without interrupting the existing systems. It can be deployed for field applications such as signal detection and dynamic human activity recognition since it does not emit signals. The efficacy of the developed system is first verified with a metallic plate with known motion pattern. The measured Doppler spectrogram shows good agreement with the simulation results, demonstrating the correctness of the sensing results. Further, the system is evaluated in complex scenarios, including handwaving, single- and multi-person motion detection. The sensing results successfully reflect the corresponding motions, demonstrating that the proposed sensing system can be utilized for blind ISAC in various applications.

</details>


### [444] [Learning-Based Robust Bayesian Persuasion with Conformal Prediction Guarantees](https://arxiv.org/abs/2511.06223)
*Heeseung Bang,Andreas A. Malikopoulos*

Main category: eess.SY

TL;DR: 本文提出一个结合神经网络和保形预测的学习框架，以在接收者信念形成不确定时实现鲁棒的贝叶斯说服，无需明确识别信念机制，并提供理论保障。


<details>
  <summary>Details</summary>
Motivation: 经典的贝叶斯说服假设发送者完全理解接收者如何形成信念和做出决策，但当接收者拥有私人信息或表现出非贝叶斯行为时，这一假设很少成立。

Method: 该研究开发了一个学习框架，将神经网络与保形预测相结合。神经网络学习从接收者观察和发送者信号到行动预测的端到端映射，避免了显式识别信念机制。保形预测构建具有可证明边际覆盖率的有限样本有效预测集，从而实现原则性的、无分布的鲁棒优化。

Result: 研究建立了数据生成策略的精确覆盖保证，并推导了策略转移下覆盖退化的界限。此外，提供了神经网络逼近和估计误差界限，样本复杂度为 $O(d \log(|\mathcal{U}||\mathcal{Y}||\mathcal{S}|)/\varepsilon^2)$，以及发送者预期效用的有限样本下界。在智能电网能源管理上的数值实验验证了该框架的鲁棒性。

Conclusion: 该框架在接收者信念形成不确定时，通过结合神经网络和保形预测，实现了鲁棒的说服，无需明确建模接收者信念机制，并提供了强大的理论保证和实际应用效果。

Abstract: Classical Bayesian persuasion assumes that senders fully understand how receivers form beliefs and make decisions--an assumption that rarely holds when receivers possess private information or exhibit non-Bayesian behavior. In this paper, we develop a learning-based framework that integrates neural networks with conformal prediction to achieve robust persuasion under uncertainty about receiver belief formation. The proposed neural architecture learns end-to-end mappings from receiver observations and sender signals to action predictions, eliminating the need to identify belief mechanisms explicitly. Conformal prediction constructs finite-sample valid prediction sets with provable marginal coverage, enabling principled, distribution-free robust optimization. We establish exact coverage guarantees for the data-generating policy and derive bounds on coverage degradation under policy shifts. Furthermore, we provide neural network approximation and estimation error bounds, with sample complexity $O(d \log(|\mathcal{U}||\mathcal{Y}||\mathcal{S}|)/\varepsilon^2)$, where $d$ denotes the effective network dimension, and finite-sample lower bounds on the sender's expected utility. Numerical experiments on smart-grid energy management illustrate the framework's robustness.

</details>


### [445] [Coherency Analysis in Nonlinear Heterogeneous Power Networks: A Blended Dynamics Approach](https://arxiv.org/abs/2511.06306)
*Yixuan Liu,Yingzhu Liu,Pengcheng You*

Main category: eess.SY

TL;DR: 本文将混合动力学方法扩展到电力系统，提出了一种新的相干性分析方法，揭示了高网络连通性或小扰动时间变化率是促进电力系统相干性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管有大量经验观察，但对复杂电力网络中相干性的理解仍不完整，尤其是在动态高度异构、非线性且受可再生能源波动等持续扰动影响日益增加的情况下。

Method: 本文将源于多智能体系统共识分析的混合动力学方法扩展到电力网络，以开发新颖的相干性分析。通过推导节点动力学轨迹与混合动力学之间差异的新界限，识别了促进相干性的关键因素。

Result: 研究表明，非线性潮流耦合的相干机器的频率响应可以近似地由混合动力学（节点动力学的加权平均）表示，即使在时变扰动下也是如此。高网络连通性或小的扰动时间变化率这两个关键因素有助于相干性，使节点频率能够从任意初始状态快速接近混合动力学轨迹，并确保即使系统未达到平衡，频率也能长期紧密跟踪该轨迹。

Conclusion: 这些见解有助于理解电力系统相干性，并得到了仿真结果的支持。

Abstract: Power system coherency refers to the phenomenon that machines in a power network exhibit similar frequency responses after disturbances, and is foundational for model reduction and control design. Despite abundant empirical observations, the understanding of coherence in complex power networks remains incomplete where the dynamics could be highly heterogeneous, nonlinear, and increasingly affected by persistent disturbances such as renewable energy fluctuations. To bridge this gap, this paper extends the blended dynamics approach, originally rooted in consensus analysis of multi-agent systems, to develop a novel coherency analysis in power networks. We show that the frequency responses of coherent machines coupled by nonlinear power flow can be approximately represented by the blended dynamics, which is a weighted average of nonlinear heterogeneous nodal dynamics, even under time-varying disturbances. Specifically, by developing novel bounds on the difference between the trajectories of nodal dynamics and the blended dynamics, we identify two key factors -- either high network connectivity or small time-variation rate of disturbances -- that contribute to coherence. They enable the nodal frequencies to rapidly approach the blended-dynamics trajectory from arbitrary initial state. Furthermore, they ensure the frequencies closely follow this trajectory in the long term, even when the system does not settle to an equilibrium. These insights contribute to the understanding of power system coherency and are further supported by simulation results.

</details>


### [446] [Partial-Power Flow Controller, Voltage Regulator, and Energy Router for Hybrid AC-DC Grids](https://arxiv.org/abs/2511.06335)
*Ehsan Asadi,Davood Keshavarzi,Alexander Koehler,Nima Tashakor,Stefan Goetz*

Main category: eess.SY

TL;DR: 本文提出了一种部分功率能量路由器架构，能够连接多个交流和直流线路，实现对电压、有功和无功功率流的精确控制，从而解决扩展型直流电网的稳定性问题以及交流与直流电网的接口需求。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源、负荷和储能设备在低压和中压电网中电子转换功率份额的增长，直流电网能够消除多余的转换级硬件和损耗。然而，扩展型直流电网缺乏交流阻抗的稳定性，电压更脆弱，功率流可能需要主动控制。此外，直流基础设施需要与现有交流电网接口。

Method: 所提出的系统采用模块化低压大电流串联模块，通过双有源桥供电。这些模块只需处理一小部分电压即可控制大功率流。该架构能够接口多个交流和直流线路，并可选择集成电池储能。

Result: 与传统技术相比，该拓扑结构将组件尺寸、成本、能量损耗和可靠性降低了三倍以上。通过相对于线路电压的动态电压注入，模块有效平衡馈线电流、调节无功功率并改善交流电网的功率因数。实时硬件在环和原型测量验证了该能量路由器在不同运行条件下的性能。

Conclusion: 实验结果证实，所提出的串联模块在交流和直流电网中均能有效控制扩展型电网，包括功率共享、电压和电能质量，是一种有效的解决方案。

Abstract: The share of electronically converted power from renewable sources, loads, and storage is continuously growing in the low- and medium-voltage grids. These sources and loads typically rectify the grid AC to DC, e.g., for a DC link, so that a DC grid could eliminate hardware and losses of these conversion stages. However, extended DC grids lack the stabilizing nature of AC impedances so that the voltage is more fragile and power flows may need active control, particularly if redundancy as known from AC, such as rings and meshing, is desired. Furthermore, a DC infrastructure will not replace but will need to interface with the existing AC grid. This paper presents a partial-power energy router architecture that can interface multiple AC and DC lines to enable precise control of voltages and both active as well as reactive power flows. The proposed system uses modular low-voltage high-current series modules supplied through dual active bridges. These modules only need to process a small share of the voltage to control large power flows. The topology reduces component size, cost, energy losses, and reliability more than three times compared to conventional technology. The optional integration of battery energy storage can furthermore eliminate the need for the sum of the power flows of all inputs to be zero at all times. Through dynamic voltage injection relative to the line voltage, the modules effectively balance feeder currents, regulate reactive power, and improve the power factor in AC grids. Real-time hardware-in-the-loop and prototype measurements validate the proposed energy router's performance under diverse operating conditions. Experimental results confirm the series module's functionality in both AC and DC grids as an effective solution for controlling extended grids, including power sharing, voltage, and power quality.

</details>


### [447] [Optical Network Digital Twin - Commercialization Barriers, Value Proposition, Early Use Cases, and Challenges](https://arxiv.org/abs/2511.06368)
*Hideki Nishizawa,Toru Mano,Kazuya Anazawa,Tatsuya Matsumura,Takeo Sasai,Masatoshi Namiki,Dmitrii Briantcev,Renato Ambrosone,Esther Le Rouzic,Stefan Melin,Oscar Gonzalez-de-Dios,Juan Pedro Fernandez-Palacios,Xiaocheng Zhang,Keigo Akahoshi,Gert Grammel,Andrea D'Amico,Giacomo Borraccini,Marco Ruffini,Daniel Kilper,Vittorio Curri*

Main category: eess.SY

TL;DR: 随着AI时代机器间通信的增长，光网络需求发生变化。本文提出一种光网络数字孪生（ONDN）架构，旨在实现超越传统管理的灵活端到端光路操作，并讨论了其商业化挑战和演进路径。


<details>
  <summary>Details</summary>
Motivation: AI的普及导致机器间通信迅速增长，对光网络提出了新的要求。尽管无线通信中的数字孪生已成熟，但光网络在商业化方面仍面临重大障碍。

Method: 本文提出了一种光网络数字孪生（Optical Network Digital Twin）架构。

Result: 所提出的架构能够实现超越传统管理的灵活端到端光路操作。论文还阐述了该架构的价值主张、商业化演进步骤以及实际部署面临的关键研究挑战。

Conclusion: 为应对AI时代光网络不断演进的需求，本文提出了一种光网络数字孪生架构，以实现更灵活的光路操作，并指出了其商业化前景和需克服的挑战。

Abstract: With the widespread adoption of AI, machine-to-machine communications are rapidly increasing, reshaping the requirements for optical networks. Recent advances in Gaussian noise modeling for digital coherent transmission have raised expectations for digital-twin-based operation. However, unlike digital twins in wireless communication, which are already well established, significant barriers remain for commercialization in optical networks. This paper discusses the evolving requirements of optical networks in the AI era and proposes an Optical Network Digital Twin architecture that enables flexible end-to-end light path operation beyond conventional management. The value propositions of the proposed architecture, its evolutionary steps toward commercialization, and key research challenges for practical deployment are presented.

</details>


### [448] [Dynamic Electric Vehicle Charging Pricing for Load Balancing in Power Distribution Networks based on Collaborative DDPG Agents](https://arxiv.org/abs/2511.06398)
*Leloko J. Lepolesa,Kayode E. Adetunji,Khmaies Ouahada,Zhenqing Liu,Ling Cheng*

Main category: eess.SY

TL;DR: 本文提出了一种基于深度强化学习的电动汽车动态充电定价策略，旨在实现配电网层面的削峰填谷和负荷平衡，以提高电网利用率。


<details>
  <summary>Details</summary>
Motivation: 电动汽车（EV）的普及是应对化石燃料环境问题的全球趋势。然而，大规模电动汽车充电可能导致电网不稳定，特别是对于未为此负荷设计的现有电网。

Method: 首先，基于历史环境变量（如温度、湿度、风速）和不同时段不同区域的车辆分布，预测配电网的负荷需求。然后，利用深度强化学习方法（比较了DDPG、SAC和PPO算法）来设定最优的动态电动汽车充电价格，以实现削峰填谷和相邻配电网之间的负荷平衡。

Result: 仿真结果表明，该策略显著改善了配电网层面的电网利用率，从而在更大范围内实现了电网的最佳使用。

Conclusion: 本研究成功开发并验证了一种动态电动汽车充电定价策略，该策略通过深度强化学习实现了配电网的削峰填谷、负荷平衡，并特别关注了相邻配电网间的负荷均衡，有效提高了电网的整体利用效率和稳定性。

Abstract: The transition from the Internal Combustion Engine Vehicles (ICEVs) to the Electric Vehicles (EVs) is globally recommended to combat the unfavourable environmental conditions caused by reliance on fossil fuels. However, it has been established that the charging of EVs can destabilize the grid when they penetrate the market in large numbers, especially in grids that were not initially built to handle the load from the charging of EVs. In this work, we present a dynamic EV charging pricing strategy that fulfills the following three objectives: distribution network-level load peak-shaving, valley-filling, and load balancing across distribution networks. Based on historical environmental variables such as temperature, humidity, wind speed, EV charging prices and distribution of vehicles in different areas in different times of the day, we first forecast the distribution network load demand, and then use deep reinforcement learning approach to set the optimal dynamic EV charging price. While most research seeks to achieve load peak-shaving and valley-filling to stabilize the grid, our work goes further into exploring the load-balancing between the distribution networks in the close vicinity to each other. We compare the performance of Deep Deterministic Policy Gradient (DDPG), Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO) algorithms for this purpose. The best algorithm is used for dymamic EV pricing. Simulation results show an improved utilization of the grid at the distribution network level, leading to the optimal usage of the grid on a larger scale.

</details>


### [449] [Sensor Importance towards Observability Degree via Shapley Values](https://arxiv.org/abs/2511.06409)
*Vishal Cholapadi Ravindra*

Main category: eess.SY

TL;DR: 本文提出利用Shapley值量化每个传感器对可观测度（衡量状态可观测程度的指标）的预期边际贡献，以改进卡尔曼滤波器设计中的传感器选择和布局。


<details>
  <summary>Details</summary>
Motivation: 在卡尔曼滤波器设计中，传感器选择通常仅满足基本的可观测性要求。然而，当存在多种传感器且有成本和物理限制时，难以量化每个传感器对可观测度（衡量状态可观测程度的指标）的具体贡献，而这对于实现期望的估计精度至关重要。

Method: 本文利用合作博弈论中的Shapley值（常用于机器学习中评估特征重要性）来量化给定传感器集合中每个传感器对可观测度的预期边际贡献。

Result: 研究表明，Shapley值能够有效量化每个传感器对可观测度的公平贡献。

Conclusion: 通过量化每个传感器对可观测度的公平贡献，滤波器设计者可以更好地进行传感器选择、布局和状态估计器设计。

Abstract: Sensor selection is an often under-appreciated aspect of state estimator or Kalman filter design. The basic minimum requirement for the choice of a sensor set while designing Kalman filters is that all states are observable. In addition, the sensors should be chosen with a view towards estimating the states with a desired accuracy. Often observability is treated as true/false check during filter design. Beyond observability -- the observability degree -- which measures \emph{how observable} the states are, has been used as the metric of choice to for sensor selection or placement applications. The higher the degree of observability, the better the possibility of designing Kalman filters that achieve the desired state estimation accuracy and consistency requirements. When a wide variety of sensors are available, sometimes with cost and physical constraints involved, sensor selection plays a crucial role in filter design. In such situations it is important to know the expected contribution of each sensor towards observability degree. Shapley values, developed in cooperative game theory for fair allocation of the payout of a multi-player game to individual players, are widely used in machine learning to assess feature importance. This paper shows that Shapley values can indeed be leveraged to quantify the expected marginal contribution of each sensor in any given sensor set towards the observability degree. This quantification of the fair contribution of each sensor towards the observability degree can be leveraged by filter designers for sensor selection, placement and filter (state estimator) design.

</details>


### [450] [Optimal Rank-1 Directional State Transition Tensors](https://arxiv.org/abs/2511.06508)
*Grace E. Calkins,Jay W. McMahon,Jackson Kulik*

Main category: eess.SY

TL;DR: 本文提出了一种状态转移张量（STT）的最优秩-1近似方法，通过求解张量z-特征对问题，在Frobenius范数意义上最大化信息保留，从而提高非线性不确定性量化的精度。


<details>
  <summary>Details</summary>
Motivation: 之前的方向性状态转移张量方法使用状态转移矩阵的主导右奇异子空间来构建降维表示，但这种方法可能不是最优的。研究动机是开发一种更高效、更准确的替代方案，以最大化秩-1近似中保留的信息，从而更好地进行非线性不确定性量化。

Method: 该研究开发了一种状态转移张量的最优秩-1近似。通过构建最优方向性状态转移张量，使其在Frobenius范数意义上最大化秩-1近似中保留的信息。具体方法是求解状态转移张量“平方”的张量z-特征对问题。

Result: 该方法提高了状态转移张量的近似精度，并改进了航空捕获等非线性飞行场景中的高斯矩传播效果。

Conclusion: 所提出的最优秩-1方向性状态转移张量方法，通过创新的求解张量z-特征对问题，显著提升了非线性不确定性量化中状态转移张量的近似精度和矩传播性能，为复杂非线性场景提供了更有效的工具。

Abstract: An optimal rank-1 approximation of state transition tensors was developed as an efficient alternative to state transition tensors for nonlinear uncertainty quantification. While previous directional state transition tensors used the dominant right singular subspace of the state transition matrix to construct a reduced-dimension representation of the state transition tensors, optimal directional state transition tensors are constructed to maximize the information retained in a rank-1 approximation of the state transition tensors in the Frobenius-norm sense. The optimal rank-1 directional state transition tensor is found by solving a tensor z-eigenpair problem of the "square" of the state transition tensor. This construct leads to increased approximation accuracy of the state transition tensors and improved Gaussian moment propagation for nonlinear flight scenarios like aerocapture.

</details>


### [451] [Investigation of lightning effects on solar power plants connected to transmission networks](https://arxiv.org/abs/2511.06523)
*Selma Grebovic,Abdulah Aksamovic,Bozidar Filipovic-Grcic,Samim Konjicija*

Main category: eess.SY

TL;DR: 本文研究了雷击对并网太阳能电站的影响，特别关注过电压效应，并通过仿真分析了避雷器在缓解雷击过电压方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 太阳能电站日益集成到输电网中，但其易受雷击等扰动的影响。将太阳能与易受雷击的输电线路连接时面临挑战，因此需要研究雷击事件对太阳能电站的影响。

Method: 研究通过在输电线路上不同距离处模拟雷击，考虑有无避雷器的情况。模拟中改变了峰值电流、前沿时间和尾部时间等关键雷击参数。此外，还对有无避雷器情况下的过电压进行了傅里叶变换分析和希尔伯特边际谱分析。

Result: 研究结果揭示了避雷器在减轻雷击过电压方面的有效性。

Conclusion: 研究强调了采取适当保护措施对于提高并网太阳能电站可靠性和安全性的重要性。

Abstract: The increasing integration of solar power plants into transmission grids has raised concerns about their vulnerability to disturbances, particularly lightning strokes. Solar energy, while offering significant environmental and economic benefits, faces challenges when connected to transmission lines that are prone to lightning discharges. This paper investigates the impact of lightning events on solar power plants, focusing on overvoltage effects. Lightning stroke simulations were conducted at various distances from the solar power plant along the transmission line, considering scenarios with and without surge arrester. Key lightning parameters such as peak current, front time, and tail time were varied to simulate different lightning strokes. The study also includes a Fourier transform analysis of the resulting overvoltages with and without a surge arrester, along with the Hilbert marginal spectrum of these overvoltages. The results provide insights into the effectiveness of surge arresters in mitigating lightning overvoltages and highlight the importance of proper protective measures for enhancing the reliability and safety of solar power plants connected to transmission networks.

</details>


### [452] [Input-Output Data-Driven Stabilization of Continuous-Time Linear MIMO Systems](https://arxiv.org/abs/2511.06524)
*Haihui Gao,Alessandro Bosso,Lei Wang,David Saussié,Bowen Yi*

Main category: eess.SY

TL;DR: 本文提出一种基于输入输出数据的数据驱动方法，利用Kreisselmeier自适应滤波器和线性矩阵不等式（LMI），实现连续时间多输入多输出（MIMO）线性时不变系统的动态输出反馈镇定，无需统一可观测性指标。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动输出反馈控制方法可能对连续时间MIMO系统有局限性，特别是对于不要求统一可观测性指标的系统。研究旨在开发一种更普适的数据驱动镇定方法。

Method: 核心思想是将Kreisselmeier自适应滤波器解释为系统可镇定非最小实现的观测器。通过该滤波器对输入输出数据进行后处理，然后推导出一个线性矩阵不等式（LMI），该LMI可以得到动态输出反馈稳定器的反馈增益。

Result: 提出了一种适用于广泛连续时间MIMO系统的方法，该方法不要求系统具有统一的可观测性指标，通过数据驱动方式获得了动态输出反馈稳定器的反馈增益。

Conclusion: 本文成功地提供了一种数据驱动的连续时间MIMO系统镇定方法，通过巧妙地利用Kreisselmeier自适应滤波器和LMI，克服了对系统可观测性指标的限制，拓宽了数据驱动控制的应用范围。

Abstract: In this paper, we address the problem of data-driven stabilization of continuous-time multi-input multi-output (MIMO) linear time-invariant systems using the input-output data collected from an experiment. Building on recent results for data-driven output-feedback control based on non-minimal realizations, we propose an approach that can be applied to a broad class of continuous-time MIMO systems without requiring a uniform observability index. The key idea is to show that Kreisselmeier's adaptive filter can be interpreted as an observer of a stabilizable non-minimal realization of the plant. Then, by postprocessing the input-output data with such a filter, we derive a linear matrix inequality that yields the feedback gain of a dynamic output-feedback stabilizer.

</details>


### [453] [Verification of low-frequency signal injection method for earth-fault detection](https://arxiv.org/abs/2511.06520)
*Nina Stipetic,Bozidar Filipovic-Grcic,Igor Ziger,Silvio Jancin,Bruno Jurisic,Dalibor Filipovic-Grcic,Alain Xémard*

Main category: eess.SY

TL;DR: 本文探讨了在MV不接地电网中使用信号注入法进行接地故障定位的可能性，并通过仿真和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 不接地电网在发生接地故障时仍能保持运行，但快速确定故障线路对于防止故障升级至关重要。信号注入法在低压不接地电网中常用，将其应用于中压电网需要解决信号注入方式的问题。

Method: 研究采用一组三相感应式电压互感器（IVTs）向不接地中压相线注入信号。首先通过电磁暂态（EMT）仿真验证了该方法的有效性，然后搭建实验平台进行了实际测量。

Result: 仿真结果表明，信号注入和中压电网接地故障检测具有良好前景。实验测试进一步证实了在中压级别应用信号注入法的有效性，并与EMT仿真结果相互印证。

Conclusion: 该研究成功描述了实验设置并展示了在中压电网中通过信号注入法进行接地故障检测的测量结果，证明了该方法在中压不接地电网中的可行性与有效性。

Abstract: Unearthed neutral is commonly used in networks which require continuous power supply. This is common in MV circuits of industrial and power plants. Unearthed networks can remain in operation during an earth-fault, but fast determination of the faulty line is key for prevention of further fault escalation. Signal injection is one of the fault location methods often used in LV unearthed networks. The possibility of applying this method in MV networks depends on how to inject the signal into unearthed phases. In such networks, it is possible to use a group of three inductive voltage transformers (IVTs) for signal injection. After the simulations have shown promising results of signal injection and earth-fault detection in MV network, an experimental test was performed. This paper describes the experimental setup and shows the measurement results of signal injection method at MV level supported by EMT simulations.

</details>


### [454] [Voltage-Regulated Sparse Optimization for Proactive Diagnosis of Voltage Collapses](https://arxiv.org/abs/2511.06528)
*Qinghua Ma,Seyyedali Hosseinalipour,Ming Shi,Jan Drgona,Shimiao Li*

Main category: eess.SY

TL;DR: 本文提出了一种电压调节稀疏优化方法，用于主动诊断和管理电网电压崩溃风险，通过在少量战略节点进行补偿，有效维持系统可行性和电压稳定。


<details>
  <summary>Details</summary>
Motivation: 研究旨在主动诊断和管理极端事件和突发情况导致的电压崩溃风险，即母线电压超出安全运行范围的风险。具体回答两个韧性相关问题：(Q1) 在极端事件后系统能否保持可行且电压在安全范围内？(Q2) 如果发生电压崩溃，主要的系统脆弱性来源是什么？

Method: 提出了一种电压调节稀疏优化方法。该方法寻找最少量的母线位置及其量化补偿（纠正措施），以同时强制执行交流网络平衡和电压边界。

Result: 在不同规模的输电系统（30母线到2383母线）上的结果表明，所提出的方法通过在少数几个战略性识别的节点进行补偿，有效地缓解了电压崩溃。该方法对大型系统具有高效的可扩展性，处理2000多个母线案例平均耗时不到4分钟。

Conclusion: 该工作可以作为更全面和可操作决策（例如无功功率规划以解决电压问题）的支柱，为规划和决策过程提供关键关注点。

Abstract: This paper aims to proactively diagnose and manage the voltage collapse risks, i.e., the risk of bus voltages violating the safe operational bounds, which can be caused by extreme events and contingencies. We jointly answer two resilience-related research questions: (Q1) Survivability: Upon having an extreme event/contingency, will the system remain feasible with voltage staying within a (preferred) safe range? (Q2) Dominant Vulnerability: If voltage collapses, what are the dominant sources of system vulnerabilities responsible for the failure? This highlights some key locations worth paying attention to in the planning or decision-making process. To address these questions, we propose a voltage-regulated sparse optimization that finds a minimal set of bus locations along with quantified compensations (corrective actions) that can simultaneously enforce AC network balance and voltage bounds. Results on transmission systems of varying sizes (30-bus to 2383-bus) demonstrate that the proposed method effectively mitigates voltage collapses by compensating at only a few strategically identified nodes, while scaling efficiently to large systems, taking on average less than 4 min for 2000+ bus cases. This work can further serve as a backbone for more comprehensive and actionable decision-making, such as reactive power planning to fix voltage issues.

</details>


### [455] [Dissipativity-Based Synthesis of Distributed Control and Communication Topology Co-Design for AC Microgrids](https://arxiv.org/abs/2511.06576)
*Mohammad Javad Najafirad,Shirantha Welikala,Lei Wu,Panos J. Antsaklis*

Main category: eess.SY

TL;DR: 本文提出了一种基于耗散性理论的协同设计框架，用于交流微电网中分布式控制器和通信拓扑的联合优化，以实现电压频率调节和功率比例共享。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将控制器合成和拓扑设计分开处理。研究人员需要一个统一的方法来同时优化这两方面，以在交流微电网中实现电压和频率调节以及分布式电源之间的比例功率共享。

Method: 将闭环交流微电网建模为网络化系统，其中分布式电源、配电线路和负载被视为具有耗散特性的互连子系统。每个分布式电源采用分层控制架构，结合本地控制器进行电压调节和分布式控制器通过归一化功率共识实现无下垂功率共享。利用耗散性理论，建立了子系统无源性的充要条件，并将协同设计问题转化为凸线性矩阵不等式（LMI）优化问题。

Result: 该框架能够系统地合成稀疏通信拓扑，同时处理交流微电网固有的耦合dq-坐标系动态和双重潮流目标。仿真结果表明，所提出的方法在实现精确电压调节、频率同步和比例功率共享方面是有效的。

Conclusion: 所提出的基于耗散性的协同设计框架能够有效且高效地实现交流微电网的电压频率调节和比例功率共享，并通过LMI优化保证了系统稳定性并合成了稀疏通信拓扑。

Abstract: This paper presents a novel dissipativity-based framework for co-designing distributed controllers and communication topologies in AC microgrids (MGs). Unlike existing methods that treat control synthesis and topology design separately, we propose a unified approach that simultaneously optimizes both aspects to achieve voltage and frequency regulation and proportional power sharing among distributed generators (DGs). We formulate the closed-loop AC MG as a networked system where DGs, distribution lines, and loads are interconnected subsystems characterized by their dissipative properties. Each DG employs a hierarchical architecture combining local controllers for voltage regulation and distributed controllers for droop-free power sharing through normalized power consensus. By leveraging dissipativity theory, we establish necessary and sufficient conditions for subsystem passivity and cast the co-design problem as a convex linear matrix inequality (LMI) optimization, enabling efficient computation and guaranteed stability. Our framework systematically synthesizes sparse communication topologies while handling the coupled dq-frame dynamics and dual power flow objectives inherent to AC MGs. Simulation results on a representative AC MG demonstrate the effectiveness of the proposed approach in achieving accurate voltage regulation, frequency synchronization, and proportional power sharing.

</details>


### [456] [On the Potential of Digital Twins for Distribution System State Estimation with Randomly Missing Data in Heterogeneous Measurements](https://arxiv.org/abs/2511.06583)
*Ying Zhang,Yihao Wang,Yuanshuo Zhang,Eric Larson,Di Shi,Fanping Sui*

Main category: eess.SY

TL;DR: 本文提出了一种基于数字孪生(DT)和注意力机制的分布式状态估计(DSSE)模型，通过整合物理实体、虚拟建模和数据融合，实现对电网的鲁棒监控，尤其是在存在异构测量数据缺失的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统的基于统计优化DSSE算法依赖于详细的电网参数和对所有不确定性的数学假设，并且容易因通信故障、拥堵和网络攻击导致的数据随机缺失而失效。

Method: 该研究提出了一种交互式注意力机制的DSSE模型。它整合了物理实体、虚拟建模和数据融合。为应对异构测量中的各种数据缺失，首先提出了物理信息驱动的数据增强和迁移方法。其次，提出了一种先进的基于注意力的时空特征学习方法。最后，采用了一种新颖的交叉交互特征融合技术来实现鲁棒的电压估计。

Result: 在一个真实的非平衡84节点配电系统中使用原始数据进行的案例研究表明，所提出的DT模型在估计电压状态方面具有高精度和鲁棒性，即使在存在随机位置和高达40%总测量数据缺失的情况下也能有效工作。

Conclusion: 所提出的基于数字孪生的模型能够准确且鲁棒地估计配电系统的电压状态，有效应对了传统方法在数据缺失情况下的挑战。

Abstract: Traditional statistical optimization-based state estimation (DSSE) algorithms rely on detailed grid parameters and mathematical assumptions of all possible uncertainties. Furthermore, random data missing due to communication failures, congestion, and cyberattacks, makes these methods easily infeasible. Inspired by recent advances in digital twins (DTs), this paper proposes an interactive attention-based DSSE model for robust grid monitoring by integrating three core components: physical entities, virtual modeling, and data fusion. To enable robustness against various data missing in heterogeneous measurements, we first propose physics-informed data augmentation and transfer. Moreover, a state-of-the-art attention-based spatiotemporal feature learning is proposed, followed by a novel cross-interaction feature fusion for robust voltage estimation. A case study in a real-world unbalanced 84-bus distribution system with raw data validates the accuracy and robustness of the proposed DT model in estimating voltage states, with random locational, arbitrary ratios (up to 40% of total measurements) of data missing.

</details>


### [457] [GNN-Enabled Robust Hybrid Beamforming with Score-Based CSI Generation and Denoising](https://arxiv.org/abs/2511.06663)
*Yuhang Li,Yang Lu,Bo Ai,Zhiguo Ding,Dusit Niyato,Arumugam Nallanathan*

Main category: eess.SY

TL;DR: 该研究提出结合图神经网络（GNNs）和基于分数的生成模型，以在不完善信道状态信息（CSI）条件下实现鲁棒的混合波束成形（HBF）。


<details>
  <summary>Details</summary>
Motivation: 准确的信道状态信息（CSI）对混合波束成形（HBF）至关重要，但在实际无线通信系统中，获取高分辨率CSI仍然具有挑战性。

Method: 首先，开发了混合消息图注意力网络（HMGAT），通过节点级和边级消息传递更新节点和边特征。其次，设计了基于BERT的噪声条件分数网络（NCSN），学习高分辨率CSI的分布，用于CSI生成和数据增强。最后，提出了去噪分数网络（DSN）框架及其实现DeBERT，用于在任意信道误差水平下去噪不完善的CSI。

Result: 在DeepMIMO城市数据集上的实验表明，所提出的模型在各种HBF任务中，无论CSI完美与否，都展现出卓越的泛化能力、可扩展性和鲁棒性。

Conclusion: 该研究通过结合GNN和基于分数的生成模型，有效解决了不完善CSI下的HBF问题，通过CSI生成、数据增强和去噪实现了鲁棒的HBF性能。

Abstract: Accurate Channel State Information (CSI) is critical for Hybrid Beamforming (HBF) tasks. However, obtaining high-resolution CSI remains challenging in practical wireless communication systems. To address this issue, we propose to utilize Graph Neural Networks (GNNs) and score-based generative models to enable robust HBF under imperfect CSI conditions. Firstly, we develop the Hybrid Message Graph Attention Network (HMGAT) which updates both node and edge features through node-level and edge-level message passing. Secondly, we design a Bidirectional Encoder Representations from Transformers (BERT)-based Noise Conditional Score Network (NCSN) to learn the distribution of high-resolution CSI, facilitating CSI generation and data augmentation to further improve HMGAT's performance. Finally, we present a Denoising Score Network (DSN) framework and its instantiation, termed DeBERT, which can denoise imperfect CSI under arbitrary channel error levels, thereby facilitating robust HBF. Experiments on DeepMIMO urban datasets demonstrate the proposed models' superior generalization, scalability, and robustness across various HBF tasks with perfect and imperfect CSI.

</details>


### [458] [F2GAN: A Feature-Feedback Generative Framework for Reliable AI-Based Fault Diagnosis in Inverter-Dominated Microgrids](https://arxiv.org/abs/2511.06677)
*Swetha Rani Kasimalla,Kuchan Park,Junho Hong,Young-Jin Kim*

Main category: eess.SY

TL;DR: 本文提出一个统一框架和F2GAN模型，用于生成逆变器主导微电网的合成故障数据，以解决真实故障数据稀缺和不平衡问题，从而提高AI故障诊断的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 逆变器主导微电网中，用于AI故障诊断的高保真故障数据（特别是针对罕见逆变器故障和极端外部线路故障）稀缺且不平衡，这限制了可靠模型的训练和验证。

Method: 1. 建立一个统一框架，对详细的逆变器主导微电网进行建模并系统地生成多种内部和外部故障场景。2. 开发增强型生成模型F2GAN（Feature Feedback GAN），通过集成基于均值方差、相关性和特征匹配损失的多级反馈机制，合成高维表格故障数据，以提高真实性和统计对齐性。3. 通过定量和定性分析评估生成数据集。4. 进行“在合成数据上训练，在真实数据上测试”（TSTR）实验。5. 在集成实时模拟器和图形界面的硬件在环（HIL）故障诊断平台上进行验证。

Result: 1. F2GAN能够合成具有更高真实性和统计对齐性的高维表格故障数据。2. TSTR实验表明，仅使用F2GAN样本训练的机器学习分类器具有很强的泛化能力。3. 在HIL故障诊断平台上进行实时测试，实现了100%的诊断准确率。4. 结果证实F2GAN有效弥合了仿真与现实世界微电网故障数据集之间的鸿沟。

Conclusion: F2GAN模型通过生成高质量的合成故障数据，有效解决了逆变器主导微电网中AI故障诊断的数据稀缺和不平衡问题，显著提升了诊断模型的可靠性和准确性，并在真实世界应用中表现出优异性能。

Abstract: Enhancing the reliability of AI based fault diagnosis in inverter dominated microgrids requires diverse and statistically balanced datasets. However, the scarcity and imbalance of high fidelity fault data, especially for rare inverter malfunctions and extreme external line faults, limit dependable model training and validation. This paper introduces a unified framework that models a detailed inverter dominated microgrid and systematically generates multiple internal and external fault scenarios to mitigate data scarcity and class imbalance. An enhanced generative model called F2GAN (Feature Feedback GAN) is developed to synthesize high dimensional tabular fault data with improved realism and statistical alignment. Unlike conventional GANs, F2GAN integrates multi level feedback based on mean variance, correlation, and feature matching losses, enabling the generator to refine output distributions toward real fault feature spaces. The generated datasets are evaluated through quantitative and qualitative analyses. Train on Synthetic, Test on Real (TSTR) experiments demonstrate strong generalization of machine learning classifiers trained exclusively on F2GAN samples. The framework is validated on a hardware-in-the-loop (HIL) fault diagnosis platform integrated with a real time simulator and graphical interface, achieving 100 % diagnostic accuracy under real-time testing. Results confirm that F2GAN effectively bridges the gap between simulated and real world microgrid fault datasets

</details>


### [459] [Pareto-Improvement-Driven Opinion Dynamics Explaining the Emergence of Pluralistic Ignorance](https://arxiv.org/abs/2511.06713)
*Yuheng Luo,Chuanzhe Zhang,Qingsong Liu,Hai Zhu,Wenjun Mei*

Main category: eess.SY

TL;DR: 本文提出了一种基于多目标博弈的意见动态模型，其中个体面临社会压力和认知失调两种独立成本。该模型能解释多元无知现象，并分析真理出现和传播的条件，以及网络结构和初始表达对真理共识的影响。


<details>
  <summary>Details</summary>
Motivation: 传统意见动态模型将多种动机聚合为单一目标，隐式地假设这些动机是可互换的。本文挑战了这一假设，认为个体动机是独立的，需要一个多目标框架来更准确地捕捉意见更新过程。

Method: 提出了一种基于多目标博弈框架的意见动态模型。每个个体经历两种不同的成本：与他人意见不一致产生的社会压力，以及偏离感知真理产生的认知失调。意见更新被建模为这两种成本之间的帕累托改进。通过分析性地刻画模型，推导出真理出现和流行的条件，并提出确保真理共识的初始播种策略。同时，通过数值模拟研究了网络密度和聚类如何影响真理的表达。

Result: 该模型对多元无知现象（即个体即使知晓真相也可能同意虚假观点）提供了简洁的解释。研究得出了真理出现和流行的条件，并提出了一种能确保真理共识的初始播种策略。数值和理论结果表明，如果没有人最初表达真理，任何网络结构都不能保证真理共识；适度稀疏但混合良好的网络最能缓解多元无知。

Conclusion: 该研究为多元无知的出现提供了一个简洁的解释，并提供了清晰且非平凡的社会学见解。结果表明，初始真理表达和特定的网络结构（如适度稀疏但混合良好的网络）对真理共识的形成至关重要。

Abstract: Opinion dynamics has recently been modeled from a game-theoretic perspective, where opinion updates are captured by individuals' cost functions representing their motivations. Conventional formulations aggregate multiple motivations into a single objective, implicitly assuming that these motivations are interchangeable. This paper challenges that assumption and proposes an opinion dynamics model grounded in a multi-objective game framework. In the proposed model, each individual experiences two distinct costs: social pressure from disagreement with others and cognitive dissonance from deviation from the perceived truth. Opinion updates are modeled as Pareto improvements between these two costs. This fwork provides a parsimonious explanation for the emergence of pluralistic ignorance, where individuals may agree on something untrue even though they all know the underlying truth. We analytically characterize the model, derive conditions for the emrameergence and prevalence of the truth, and propose an initial-seeding strategy that ensures consensus on truth. Numerical simulations are conducted on how network density and clustering affect the expression of truth. Both theoretical and numerical results lead to clear and non-trivial sociological insights. For example, no network structure guarantees truthful consensus if no one initially express the truth; moderately sparse but well-mixed networks best mitigate pluralistic ignorance.

</details>


### [460] [The Wisdom of the Crowd: High-Fidelity Classification of Cyber-Attacks and Faults in Power Systems Using Ensemble and Machine Learning](https://arxiv.org/abs/2511.06714)
*Emad Abukhousa,Syed Sohail Feroz Syed Afroz,Fahad Alsaeed,Abdulaziz Qwbaiban,Saman Zonouz,A. P. Sakis Meliopoulos*

Main category: eess.SY

TL;DR: 本文提出了一个高保真评估框架，用于在数字变电站中对网络攻击和物理故障进行基于机器学习的分类。研究发现，离线准确性不足以反映实际部署能力，实时流式环境下的性能（如MLP表现良好，集成模型常弃权）至关重要。


<details>
  <summary>Details</summary>
Motivation: 在数字变电站中，特别是在逆变器基资源（IBR）丰富的网络中，需要可靠地使用机器学习分类网络攻击和物理故障。传统的离线准确性指标不足以评估实际部署能力。

Method: 本研究提出了一个高保真评估框架，利用4.8 kHz的电磁暂态仿真和数字变电站仿真。训练了包括集成算法和多层感知器（MLP）在内的12种机器学习模型，使用带标签的时域测量数据。模型在为亚周期响应设计的实时流式环境中进行评估，并结合周期长度平滑滤波器和置信度阈值来稳定决策。

Result: 多个模型在离线评估中达到了近乎完美的准确率（高达99.9%）。但在实时流式环境中，只有MLP保持了鲁棒的覆盖率（98-99%），而集成模型虽然保持了完美的异常检测精度，但经常弃权（覆盖率10-49%）。

Conclusion: 离线准确性是衡量机器学习模型实际部署能力不可靠的指标。为了确保在逆变器基资源（IBR）丰富的网络中实现可靠的分类，需要进行真实的测试和推理管道评估。

Abstract: This paper presents a high-fidelity evaluation framework for machine learning (ML)-based classification of cyber-attacks and physical faults using electromagnetic transient simulations with digital substation emulation at 4.8 kHz. Twelve ML models, including ensemble algorithms and a multi-layer perceptron (MLP), were trained on labeled time-domain measurements and evaluated in a real-time streaming environment designed for sub-cycle responsiveness. The architecture incorporates a cycle-length smoothing filter and confidence threshold to stabilize decisions. Results show that while several models achieved near-perfect offline accuracies (up to 99.9%), only the MLP sustained robust coverage (98-99%) under streaming, whereas ensembles preserved perfect anomaly precision but abstained frequently (10-49% coverage). These findings demonstrate that offline accuracy alone is an unreliable indicator of field readiness and underscore the need for realistic testing and inference pipelines to ensure dependable classification in inverter-based resources (IBR)-rich networks.

</details>


### [461] [Learning stabilising policies for constrained nonlinear systems](https://arxiv.org/abs/2511.06832)
*Daniele Ravasio,Danilo Saccani,Marcello Farina,Giancarlo Ferrari-Trecate*

Main category: eess.SY

TL;DR: 提出一种针对受约束非线性系统（RNNs）的两层控制方案，通过基础控制器保证稳定性与约束满足，并通过结合内部模型控制和稳定算子（稳定神经网络）的额外控制器提升性能，即使训练不完全也能保证闭环稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对受扰动的受约束非线性系统（由RNN表示），需要一种控制方案来确保全局或区域闭环稳定性、满足输入输出约束，并能进一步提升系统性能。

Method: 采用两层控制方案。基础控制器确保跟踪误差的全局或区域l_p稳定性，并在鲁棒正不变集内满足输入输出约束。额外控制器结合内部模型控制与一个稳定算子（实现为稳定神经网络），通过无约束优化训练以改善系统性能，且即使优化提前停止，也不会损害闭环平衡跟踪或约束满足。此外，还表征了该架构可实现的闭环稳定行为类别。

Result: 在pH中和基准测试上的仿真结果证明了所提方法的有效性。

Conclusion: 所提出的两层控制方案为受约束的非线性RNN系统提供了一种有效方法，能够确保鲁棒的稳定性、约束满足，并通过可训练的稳定神经网络在不牺牲稳定性的前提下提升系统性能，即使训练不完全也能提供保证。

Abstract: This work proposes a two-layered control scheme for constrained nonlinear systems represented by a class of recurrent neural networks and affected by additive disturbances. In particular, a base controller ensures global or regional closed-loop l_p-stability of the error in tracking a desired equilibrium and the satisfaction of input and output constraints within a robustly positive invariant set. An additional control contribution, derived by combining the internal model control principle with a stable operator, is introduced to improve system performance. This operator, implemented as a stable neural network, can be trained via unconstrained optimisation on a chosen performance metric, without compromising closed-loop equilibrium tracking or constraint satisfaction, even if the optimisation is stopped prematurely. In addition, we characterise the class of closed-loop stable behaviours that can be achieved with the proposed architecture. Simulation results on a pH-neutralisation benchmark demonstrate the effectiveness of the proposed approach.

</details>


### [462] [Correct-by-Design Control Synthesis of Stochastic Multi-agent Systems: a Robust Tensor-based Solution](https://arxiv.org/abs/2511.06873)
*Ruohan Wang,Siyuan Liu,Zhiyong Sun,Sofie Haesaert*

Main category: eess.SY

TL;DR: 本文提出了一种基于抽象的框架，结合鲁棒动态规划和规范分解，为连续空间离散时间随机系统提供可扩展的控制策略，并附带可证明的时序逻辑满足度下界。


<details>
  <summary>Details</summary>
Motivation: 由于维度灾难，即使采用MDP抽象，具有连续空间的离散时间随机系统也难以验证和控制。

Method: 该研究提出了一种基于抽象的框架，利用鲁棒动态规划映射，通过近似随机模拟关系量化时序逻辑满足度的可证明下界。通过利用解耦动力学，揭示了价值函数中的规范多项式分解（CPD）张量结构，从而使动态规划具有可扩展性。

Result: 所提出的方法提供了具有可证明时序逻辑满足度下界的控制策略，并通过CPD张量结构实现了动态规划的可扩展性。它为时序逻辑规范提供了设计正确的概率保证，并在连续状态线性随机系统上得到了验证。

Conclusion: 该方法通过结合抽象和张量分解，为连续空间离散时间随机系统提供了可扩展且具有可证明概率保证的控制策略，以满足时序逻辑规范。

Abstract: Discrete-time stochastic systems with continuous spaces are hard to verify and control, even with MDP abstractions due to the curse of dimensionality. We propose an abstraction-based framework with robust dynamic programming mappings that deliver control strategies with provable lower bounds on temporal-logic satisfaction, quantified via approximate stochastic simulation relations. Exploiting decoupled dynamics, we reveal a Canonical Polyadic Decomposition tensor structure in value functions that makes dynamic programming scalable. The proposed method provides correct-by-design probabilistic guarantees for temporal logic specifications. We validate our results on continuous-state linear stochastic systems.

</details>


### [463] [Analysis of Traffic Congestion in North Campus, Delhi University Using Continuous Time Models](https://arxiv.org/abs/2511.06921)
*Siddhartha Mahajan,Harsh Raj,Sonam Tanwar*

Main category: eess.SY

TL;DR: 该项目利用UXSim的连续时间模拟，研究德里大学北校区的交通拥堵问题，评估传统交通管理措施，并通过信号优化和路口改造显著改善了模拟交通流。


<details>
  <summary>Details</summary>
Motivation: 研究德里大学北校区内的交通拥堵问题，识别经常性拥堵点，并评估传统交通管理措施的有效性。

Method: 使用UXSim实现的连续时间模拟来建模车辆运动和交互。研究重点是几个关键交叉口，并评估了信号配时优化和适度交叉口重新配置的效果。

Result: 识别了经常性拥堵点。实施信号配时优化和适度交叉口重新配置后，模拟交通流得到了可衡量的改善。

Conclusion: 研究结果为当地交通管理提供了实用见解，并展示了连续时间模拟方法在指导短期干预和长期规划方面的价值。

Abstract: This project investigates traffic congestion within North Campus, Delhi University (DU), using continuous time simulations implemented in UXSim to model vehicle movement and interaction. The study focuses on several key intersections, identifies recurring congestion points, and evaluates the effectiveness of conventional traffic management measures. Implementing signal timing optimization and modest intersection reconfiguration resulted in measurable improvements in simulated traffic flow. The results provide practical insights for local traffic management and illustrate the value of continuous time simulation methods for informing short-term interventions and longer-term planning.

</details>


### [464] [On the Redundant Distributed Observability of Mixed Traffic Transportation Systems](https://arxiv.org/abs/2511.06950)
*M. Doostmohammadian,U. A. Khan,N. Meskin*

Main category: eess.SY

TL;DR: 本文研究了混合交通系统中网联自动驾驶车辆（CAVs）对人类驾驶车辆（HDVs）的分布式状态估计问题，并分析了分布式可观测性及其在传感器故障下的冗余性。


<details>
  <summary>Details</summary>
Motivation: 在混合交通系统中，需要通过CAVs分布式估计HDVs的状态。面临的挑战包括如何确保分布式可观测性，以及如何在存在传感器故障或不可靠数据的情况下保持估计系统的鲁棒性。

Method: 1. 建立了分布式可观测状态空间模型。2. 推导了满足分布式可观测性的网络拓扑条件（强连通性结合适当的观测器增益）。3. 设计了一个通过局部信息共享的分布式观测器。4. 针对传感器故障和不可靠数据，提出了基于q节点/链路连通网络设计的冗余分布式可观测性条件。

Result: 1. 网络的强连通性以及适当的观测器增益足以实现分布式可观测性。2. 通过额外的网络信息共享，可以实现冗余分布式可观测性，即在不丧失可观测性的情况下，能够隔离或移除一定数量的故障传感器和不可靠链路。3. 仿真结果验证了所提出方法的有效性。

Conclusion: 所提出的方法能够有效地在混合交通系统中实现CAVs对HDVs的分布式状态估计，确保了系统的可观测性，并通过网络冗余设计提高了系统对传感器故障和链路不可靠性的鲁棒性。

Abstract: In this paper, the problem of distributed state estimation of human-driven vehicles (HDVs) by connected autonomous vehicles (CAVs) is investigated in mixed traffic transportation systems. Toward this, a distributed observable state-space model is derived, which paves the way for estimation and observability analysis of HDVs in mixed traffic scenarios. In this direction, first, we obtain the condition on the network topology to satisfy the distributed observability, i.e., the condition such that each HDV state is observable to every CAV via information-exchange over the network. It is shown that strong connectivity of the network, along with the proper design of the observer gain, is sufficient for this. A distributed observer is then designed by locally sharing estimates/observations of each CAV with its neighborhood. Second, in case there exist faulty sensors or unreliable observation data, we derive the condition for redundant distributed observability as a $q$-node/link-connected network design. This redundancy is achieved by extra information-sharing over the network and implies that a certain number of faulty sensors and unreliable links can be isolated/removed without losing the observability. Simulation results are provided to illustrate the effectiveness of the proposed approach.

</details>


### [465] [Capacity Estimation of Lithium-ion Batteries Using Invariance Property in Open Circuit Voltage Relationship](https://arxiv.org/abs/2511.06989)
*Yang Wang,Marta Zagorowska,Riccardo M. G. Ferrari*

Main category: eess.SY

TL;DR: 本文提出一种新型锂离子电池容量估计方法，仅需一个开路电压（OCV）测试循环进行建模，并能从部分充放电数据中估计容量，且可应用于动态放电数据，利用OCV-荷电状态（SOC）关系的不变性实现高精度估计。


<details>
  <summary>Details</summary>
Motivation: 传统锂离子电池容量估计方法需要大量昂贵的电池测试数据进行训练，且通常需要完整的充放电循环才能估计容量，这些是当前研究面临的局限性。

Method: 该方法仅利用一个OCV测试循环进行建模，允许从部分充放电数据中估计容量。通过与OCV识别算法结合，可从动态放电数据中估计容量，无需专门的数据采集测试。核心思想是利用OCV与SOC关系在老化周期中表现出的不变性，通过解决一个OCV对齐问题，仅使用电池的OCV和放电容量数据来估计容量。

Result: 仿真结果表明该方法有效，在344个老化周期中的12个样本上，容量估计的平均绝对相对误差为0.85%。

Conclusion: 该方法克服了传统容量估计方法的局限性，实现了仅用一个OCV循环建模和从部分数据中准确估计电池容量，且能应用于动态放电数据，显著提高了估计效率和便利性。

Abstract: Lithium-ion (Li-ion) batteries are ubiquitous in electric vehicles (EVs) as efficient energy storage devices. The reliable operation of Li-ion batteries depends critically on the accurate estimation of battery capacity. However, conventional estimation methods require extensive training datasets from costly battery tests for modeling, and a full cycle of charge and discharge is often needed to estimate the capacity. To overcome these limitations, we propose a novel capacity estimation method that leverages only one cycle of the open-circuit voltage (OCV) test in modeling and allows for estimating the capacity from partial charge or discharge data. Moreover, by applying it with OCV identification algorithms, we can estimate the capacity from dynamic discharge data without requiring dedicated data collection tests. We observed an invariance property in the OCV versus state of charge relationship across aging cycles. Leveraging this invariance, the proposed method estimates the capacity by solving an OCV alignment problem using only the OCV and the discharge capacity data from the battery. Simulation results demonstrate the method's efficacy, achieving a mean absolute relative error of 0.85\% in capacity estimation across 12 samples from 344 aging cycles.

</details>


### [466] [Koopman-Based Dynamic Environment Prediction for Safe UAV Navigation](https://arxiv.org/abs/2511.06990)
*Vitor Bueno,Ali Azarbahram,Marcello Farina,Lorenzo Fagiano*

Main category: eess.SY

TL;DR: 本文提出了一种基于Koopman算子的模型预测控制（MPC）框架，利用实时LiDAR数据实现无人机在动态环境中的安全导航。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，无人机需要高效、准确地预测移动障碍物的位置，以实现鲁棒、无碰撞的轨迹规划，确保实时执行。

Method: 该方法利用Koopman算子线性近似周围障碍物的动力学，从而实现对移动障碍物位置的高效准确预测。然后将此嵌入到MPC框架中，进行无碰撞轨迹规划，并使用实时LiDAR数据。

Result: 通过仿真和ROS2-Gazebo实现验证，该方法在传感器噪声、执行器延迟和环境不确定性下表现出可靠的性能。

Conclusion: 该Koopman-MPC框架能够实现鲁棒、无碰撞的轨迹规划，适用于无人机在动态环境中的实时安全导航。

Abstract: This paper presents a Koopman-based model predictive control (MPC) framework for safe UAV navigation in dynamic environments using real-time LiDAR data. By leveraging the Koopman operator to linearly approximate the dynamics of surrounding objets, we enable efficient and accurate prediction of the position of moving obstacles. Embedding this into an MPC formulation ensures robust, collision-free trajectory planning suitable for real-time execution. The method is validated through simulation and ROS2-Gazebo implementation, demonstrating reliable performance under sensor noise, actuation delays, and environmental uncertainty.

</details>


### [467] [Beyond Phasors: Solving Non-Sinusoidal Electrical Circuits using Geometry](https://arxiv.org/abs/2511.06997)
*Javier Castillo-Martínez,Raul Baños,Francisco G. Montoya*

Main category: eess.SY

TL;DR: 本文提出使用几何代数（GA）为多谐波交流电路提供一个统一且直接的分析方法，克服了传统相量分析的局限性。


<details>
  <summary>Details</summary>
Motivation: 经典的相量分析仅限于单频正弦条件，在存在谐波时面临挑战。传统的傅里叶级数分解和叠加方法是零散的，无法在频域提供统一的解决方案。

Method: 引入了几何代数（GA）方法。将所有非正弦电压和电流波形表示为$2N$维欧几里得空间中的简单向量。这些向量之间的关系通过一个统一的几何变换（称为“rotoflex”）来表征。阻抗概念被提升为一个单一的多向量，整体捕获电路响应，并统一所有谐波的幅值比例（flextance）和相位旋转（rotance）。

Result: 该方法建立了GA作为相量分析的结构统一且高效的替代方案，为电路分析提供了更严谨的基础。通过案例研究验证了其与传统方法完美数值一致性以及卓越的性能。

Conclusion: 几何代数（GA）为多谐波交流电路分析提供了一个结构统一、高效且更严谨的替代方案，克服了传统相量分析的局限性。

Abstract: Classical phasor analysis is fundamentally limited to sinusoidal single-frequency conditions, which poses challenges when working in the presence of harmonics. Furthermore, the conventional solution, which consists of decomposing signals using Fourier series and applying superposition, is a fragmented process that does not provide a unified solution in the frequency domain. This paper overcomes this limitation by introducing a complete and direct approach for multi-harmonic AC circuits using Geometric Algebra (GA). In this way, all non-sinusoidal voltage and current waveforms are represented as simple vectors in a $2N$-dimensional Euclidean space. The relationship between these vectors is characterized by a single and unified geometric transformation termed the \textit{rotoflex}. This operator elevates the concept of impedance from a set of complex numbers per frequency to a single multivector that holistically captures the circuit response, while unifying the magnitude scale (flextance) and phase rotation (rotance) across all harmonics. Thus, this work establishes GA as a structurally unified and efficient alternative to phasor analysis, providing a more rigorous foundation for electrical circuit analysis. The methodology is validated through case studies that demonstrate perfect numerical consistency with traditional methods and superior performance.

</details>


### [468] [Real-Time Co-Simulation for DC Microgrid Energy Management with Communication Delays](https://arxiv.org/abs/2511.07052)
*S. Gokul Krishnan,Mohd Asim Aftab,Shehab Ahmed,Charalambos Konstantinou*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的实时信息物理系统（CPS）测试平台，用于在考虑实际通信延迟的情况下评估直流微电网中能源管理系统（EMS）的性能。


<details>
  <summary>Details</summary>
Motivation: 可再生能源（RESs）的日益整合增加了对弹性高效微电网的需求。直流微电网因其损耗低、易于与直流RESs接口以及可靠性提高而受到关注。EMS对于管理RESs的固有可变性至关重要，但现有EMS算法的验证通常假设理想通信条件或使用简化网络模型，忽略了实际通信延迟对EMS性能的影响。

Method: 该研究开发了一个实时CPS测试平台。该平台将OPAL-RT中建模的直流微电网与在Raspberry Pi（RPi）上实现的EMS控制器集成。通信网络通过EXataCPS进行仿真，实现实际电力系统流量交换和复制真实的延迟条件。

Result: 本文提出了一个能够捕捉电力系统动态、EMS控制和通信网络行为之间相互作用的综合测试平台。该平台可以用于在实际通信延迟下评估直流微电网中EMS的性能。

Conclusion: 该测试平台提供了一个全面的设置，用于在考虑真实通信延迟的情况下，对直流微电网中的EMS性能进行深入评估，填补了现有研究的空白。

Abstract: The growing integration of renewable energy sources (RESs) in modern power systems has intensified the need for resilient and efficient microgrid solutions. DC microgrids have gained prominence due to their reduced conversion losses, simplified interfacing with DC-based RESs, and improved reliability. To manage the inherent variability of RESs and ensure stable operation, energy management systems (EMS) have become essential. While various EMS algorithms have been proposed and validated using real-time simulation platforms, most assume ideal communication conditions or rely on simplified network models, overlooking the impact of realistic communication delays on EMS performance. This paper presents a novel real-time cyber-physical system (CPS) testbed for evaluating EMS performance in DC microgrids under realistic communication delays. The proposed testbed integrates a DC microgrid modeled in OPAL-RT with an EMS controller implemented on a Raspberry Pi (RPi). The communication network is emulated using EXataCPS, enabling the exchange of actual power system traffic and the replication of realistic latency conditions. This comprehensive setup captures the interplay between power system dynamics, EMS control, and communication network behavior.

</details>


### [469] [Structural sign herdability of linear time-invariant systems:theory and design for arbitrary network structures](https://arxiv.org/abs/2511.07054)
*Pradeep M,Twinkle Tripathy*

Main category: eess.SY

TL;DR: 本文研究了线性时不变（LTI）系统结构符号（SS）可控群集（herdability）的图论条件，提出了一种基于分层一致符号图（LUG^H(G_s)）的图形测试方法。


<details>
  <summary>Details</summary>
Motivation: 结构可控群集在电力网络、生物网络、意见动力学和多机器人牧羊等领域有广泛应用。研究旨在为LTI系统找到其结构符号可控群集的图论条件。

Method: 引入了有符号有向图G的分层图表示G_s，并构建了分层一致符号图LUG(G_s)来捕获具有相同符号和统一路径长度的路径。定义了LUG(G_s)的一个特殊子图LUG^H(G_s)，并证明了LTI系统是SS可控群集的当且仅当存在一个覆盖所有节点的LUG^H(G_s)。此外，还将这些结果推广到具有多个领导者和驱动节点的有向图。

Result: 提出了第一个用于任意有向图拓扑的SS可控群集的图形测试方法。分析揭示了即使在相关有向图中存在（符号和层）膨胀的情况下，系统也可以是SS可控群集的（这在有向树中被认为是无法实现的）。结果还推广到具有多个领导者和驱动节点的有向图。

Conclusion: 本文为LTI系统的SS可控群集提供了一种新颖的图论测试方法，该方法基于LUG^H(G_s)并适用于任意有向图拓扑，甚至在存在膨胀的情况下。这些发现扩展了对可控群集行为的理解，并推广到多领导者/驱动者场景。

Abstract: The objective of this paper is to investigate graph-theoretic conditions for structural herdability of an LTI system. In particular, we are interested in the structural sign (SS) herdability of a system wherein the underlying digraph representing it is signed. Structural herdability finds applications in various domains like power networks, biological networks, opinion dynamics, multi-robot shepherding, etc. We begin the analysis by introducing a layered graph representation Gs of the signed digraph G; such a representation allows us to capture the signed distances between the nodes with ease. We construct a subgraph of G_s that characterizes paths of identical signs between layers and uniform path lengths, referred to as a layer-wise unisigned graph LUG(G_s). A special subgraph of an LUG(G_s), denoted as an LUG^H(G_s), is key to achieving SS herdability. This is because we prove that an LTI system is SS herdable if and only if there exists an LUG^H(G_s) which covers all the nodes of the given digraph. To the best of our knowledge, such a graphical test is one of the first methods which allows us to check SS herdability for arbitrary digraph topologies. Interestingly, the analysis also reveals that a system can be SS herdable even in the presence of (signed and layer) dilation in the associated digraph (note that such a behaviour has been shown to be impossible in directed trees). Additionally, we also extend these results to digraphs with multiple leader and driver nodes. In order to illustrate all the results, we present numerous examples throughout the paper.

</details>


### [470] [Characterisation and Quantification of Data Centre Flexibility for Power System Support](https://arxiv.org/abs/2511.07159)
*Mehmet Turker Takci,James Day,Meysam Qadrdan*

Main category: eess.SY

TL;DR: 该研究提出了一个数据中心全设施优化模型，以量化数据中心作为主动电网参与者的灵活性，并通过IT工作负载调度、UPS储能和冷却系统协同优化，评估其对电力系统的向上和向下灵活性。


<details>
  <summary>Details</summary>
Motivation: 数据中心的快速增长给高变可再生能源电力系统带来了日益严峻的挑战。传统上，数据中心被视为被动负荷，但它们有潜力成为电网的积极参与者，提供灵活性。然而，量化和利用这种灵活性尚未得到充分探索。

Method: 本文提出了一个集成的、全设施优化模型来研究数据中心的最低成本运行计划，并表征数据中心可向电力系统提供的总灵活性。该模型考虑了IT工作负载转移、UPS储能和冷却系统。主要贡献包括：(i) 一个整合IT调度、UPS运行和冷却动态的运行优化模型，以建立成本最优的基线操作；(ii) 一个持续时间感知的灵活性评估方法，用于计算从基线出发的最大可行持续时间，同时遵守所有运行、热和恢复约束。

Result: 结果揭示了灵活性提供中清晰的时间结构和显著的不对称性：向上灵活性（电力负荷减少）主要通过推迟IT工作负载实现，从而带来二次冷却功率减少。向下灵活性（电力负荷增加）则依赖于增加冷却系统功耗（由TES缓冲支持）和为UPS充电。该框架将抽象的灵活性潜力转化为量化的灵活性大小和持续时间。

Conclusion: 该框架将抽象的灵活性潜力转化为量化的灵活性大小和持续时间，系统运营商可以利用这些信息来探索诸如备用、频率响应和价格响应需求等服务。

Abstract: The rapid growth of data centres poses an evolving challenge for power systems with high variable renewable energy. Traditionally operated as passive electrical loads, data centres, have the potential to become active participants that provide flexibility to the grid. However, quantifying and utilising this flexibility have not yet been fully explored. This paper presents an integrated, whole facility optimisation model to investigate the least cost operating schedule of data centres and characterise the aggregate flexibility available from data centres to the power system. The model accounts for IT workload shifting, UPS energy storage, and cooling system. Motivated by the need to alleviate the increasing strain on power systems while leveraging their untapped flexibility potential, this study makes two primary contributions: (i) an operational optimisation model that integrates IT scheduling, UPS operation, and cooling dynamics to establish a cost optimal baseline operation, and (ii) a duration-aware flexibility assessment that, for any given start time and power deviation, computes the maximum feasible duration from this baseline while respecting all operational, thermal, and recovery constraints. This method characterises the aggregate flexibility envelope. Results reveal a clear temporal structure and a notable asymmetry in flexibility provision: upward flexibility (electricity load reduction) is driven by deferring IT workload, which allows for a secondary reduction in cooling power. Downward flexibility (electricity load increase) relies on increasing power consumption of the cooling system, supported by the TES buffer, and charging the UPS. This framework translates abstract flexibility potential into quantified flexibility magnitude and duration that system operators could investigate for use in services such as reserve, frequency response, and price responsive demand.

</details>


### [471] [Beyond Gaussian Assumptions: A General Fractional HJB Control Framework for Lévy-Driven Heavy-Tailed Channels in 6G](https://arxiv.org/abs/2511.07167)
*Mengqi Li,Lixin Li,Wensheng Lin,Zhu Han,Tamer Başar*

Main category: eess.SY

TL;DR: 本文提出了一种基于对称α-稳定Lévy过程的无线信道模型，并开发了一个基于分数阶Hamilton-Jacobi-Bellman (HJB)方程的广义最优控制框架，以应对6G系统中非高斯、重尾衰落信道下的性能挑战，并通过仿真验证了其在优化发射功率方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 新兴的6G无线系统在高速列车穿越密集城市走廊和无人机（UAV）在山区地形等挑战性环境中面临严重的性能下降。这些场景表现出非高斯、非平稳、重尾衰落和信号突然波动的信道特性，亟需新的模型和控制策略来解决。

Method: 该研究提出了一种基于对称α-稳定Lévy过程的无线信道模型，实现了长期和短期衰落的连续时间状态空间表征。在此模型基础上，通过引入Riesz分数阶算子以捕捉非局部空间效应和记忆依赖动态，开发了一个基于分数阶Hamilton-Jacobi-Bellman (HJB)方程的广义最优控制框架。理论上，严格证明了分数阶HJB方程粘性解的存在性和唯一性。

Result: 在多小区、多用户下行链路设置中进行的数值仿真表明，基于分数阶HJB的策略在重尾同信道和多用户干扰下，能够有效地优化传输功率。

Conclusion: 所提出的基于分数阶HJB的控制策略，结合新的信道模型，能够有效应对6G系统在复杂非高斯、重尾信道环境下的性能挑战，为优化传输功率提供了一种理论有效且实践可行的解决方案。

Abstract: Emerging 6G wireless systems suffer severe performance degradation in challenging environments like high-speed trains traversing dense urban corridors and Unmanned Aerial Vehicles (UAVs) links over mountainous terrain. These scenarios exhibit non-Gaussian, non-stationary channels with heavy-tailed fading and abrupt signal fluctuations. To address these challenges, this paper proposes a novel wireless channel model based on symmetric $α$-stable Lévy processes, thereby enabling continuous-time state-space characterization of both long-term and short-term fading. Building on this model, a generalized optimal control framework is developed via a fractional Hamilton-Jacobi-Bellman (HJB) equation that incorporates the Riesz fractional operator to capture non-local spatial effects and memory-dependent dynamics. The existence and uniqueness of viscosity solutions to the fractional HJB equation are rigorously established, thus ensuring the theoretical validity of the proposed control formulation. Numerical simulations conducted in a multi-cell, multi-user downlink setting demonstrate the effectiveness of the fractional HJB-based strategy in optimizing transmission power under heavy-tailed co-channel and multi-user interference.

</details>


### [472] [Fair and Efficient allocation of Mobility-on-Demand resources through a Karma Economy](https://arxiv.org/abs/2511.07225)
*Matteo Cederle,Saverio Bolognani,Gian Antonio Susto*

Main category: eess.SY

TL;DR: 本文提出了一种基于Karma的非货币机制，通过建模用户内生紧迫性来提高出行按需系统的效率和公平性，以应对现有系统加剧的不平等问题。


<details>
  <summary>Details</summary>
Motivation: 出行按需系统（如网约车）在改变城市交通的同时，也因动态定价策略等原因加剧了社会经济不平等。此外，现有的智能出行公平性框架往往忽略了用户紧迫性的时间和情境变异性。

Method: 引入了一种非货币的、基于Karma的机制，该机制能够建模内生紧迫性，并允许用户的时间敏感度根据系统条件和外部因素演变。在此基础上，开发了一个理论框架，该框架在保持经典Karma经济效率和公平性保障的同时，适应了这种现实的用户行为建模。最后，在模拟的出行按需场景中应用了该框架。

Result: 该框架在模拟的出行按需场景中表现出高水平的系统效率，并同时保证了用户资源的公平分配。

Conclusion: 通过引入基于Karma的机制并建模用户内生紧迫性，所提出的框架能够有效提升出行按需系统的效率，并保障资源分配的公平性，从而应对现有系统中的不平等问题。

Abstract: Mobility-on-demand systems like ride-hailing have transformed urban transportation, but they have also exacerbated socio-economic inequalities in access to these services, also due to surge pricing strategies. Although several fairness-aware frameworks have been proposed in smart mobility, they often overlook the temporal and situational variability of user urgency that shapes real-world transportation demands. This paper introduces a non-monetary, Karma-based mechanism that models endogenous urgency, allowing user time-sensitivity to evolve in response to system conditions as well as external factors. We develop a theoretical framework maintaining the efficiency and fairness guarantees of classical Karma economies, while accommodating this realistic user behavior modeling. Applied to a simulated mobility-on-demand scenario we show that our framework is able to achieve high levels of system efficiency, guaranteeing at the same time equitable resource allocation for the users.

</details>


### [473] [Beyond Prime Farmland: Solar Siting Tradeoffs for Cost-Effective Decarbonization](https://arxiv.org/abs/2511.07323)
*Papa Yaw Owusu-Obeng,Mai Shi,Max Vanatta,Michael T. Craig*

Main category: eess.SY

TL;DR: 本研究量化了美国东部太阳能光伏（PV）在不同土地类型（未开发地、棕地、屋顶）上的选址决策对成本和技术潜力的权衡，发现未开发地成本最低，而棕地和屋顶PV成本更高且潜力有限，为政策制定提供了依据。


<details>
  <summary>Details</summary>
Motivation: 太阳能光伏的增长可行性和成本效益与选址决策密切相关，但不同土地类别（特别是棕地和屋顶与未开发地）之间成本和技术潜力的权衡尚未量化。同时，对减少未开发土地（如优质农田）使用的阻力日益增加，政策对此也越来越关注。

Method: 研究构建了一个包含美国东部互联电网区域（约2,400个县）内各县按土地类型划分的太阳能光伏供应曲线数据库。这些供应曲线量化了未开发地、棕地和屋顶土地类型的技术潜力与平准化成本。利用这些供应曲线和与脱碳电力系统相符的2035年太阳能部署目标（435 GW），通过优先考虑不同土地类型的场景，量化了成本和容量的权衡。

Result: 研究发现，未开发地（尤其是优质农业用地）提供了满足容量目标的最低平准化成本，为39至57美元/兆瓦时。受污染土地（棕地）的技术潜力有限，相对于未开发地，成本溢价为14-33%。屋顶光伏虽然提供了足够的技术潜力来满足容量目标，但成本始终较高，最低LCOE约为70美元/兆瓦时，远高于成本最高的未开发地。结果揭示了美国东部各地选址权衡的异质性。

Conclusion: 研究结果详细阐述了美国东部地区选址决策的异质性权衡，有助于制定有针对性的政策，以在实现部署目标的同时平衡成本和土地利用冲突。

Abstract: The feasibility and cost-effectiveness of continued growth in solar photovoltaics are closely tied to siting decisions. But trade-offs between costs and technical potential between land categories, especially brownfields and rooftop sites, have not been quantified, despite increasing resistance to and policy interest in reducing use of greenfield sites (e.g., prime agricultural lands). We examine the effect of siting decisions across land types for utility-scale and rooftop PV on the feasibility and cost of meeting solar deployment targets across the Eastern U.S. We build a database of solar PV supply curves by land type for each county in the Eastern Interconnect (EI) region (~2,400 counties). Our supply curves quantify technical potential versus levelized cost across greenfield, brownfield, and rooftop land types. With these supply curves and a 2035 solar deployment target (435 GW) aligned with a decarbonized power system, we quantify cost and capacity trade-offs using scenarios that prioritize solar PV deployment on different land types. We find greenfield, particularly prime agriculture, sites offer the lowest levelized costs for meeting capacity targets, of 39 to 57 $/MWh. Contaminated lands, often prioritized in policy to reduce land use conflict, have limited technical potential and impose a cost premium of 14-33% relative to greenfield sites. Rooftop PV provides enough technical potential for meeting capacity targets but comes at consistently higher costs, with minimum LCOEs of roughly 70 $/MWh or well above the highest-cost greenfield sites. Our results detail heterogeneous siting trade-offs across the Eastern United States, enabling targeted policy design to meet deployment targets while balancing costs and land use conflicts.

</details>


### [474] [Robust Linear Design for Flight Control Systems with Operational Constraints](https://arxiv.org/abs/2511.07335)
*Marcel Menner,Eugene Lavretsky*

Main category: eess.SY

TL;DR: 本文提出了一种系统方法，用于设计鲁棒的线性比例-积分（PI）伺服控制器，以有效管理飞行控制系统中的控制输入和输出约束，同时保持鲁棒性、可分析性和抗饱和保护。


<details>
  <summary>Details</summary>
Motivation: 在飞行控制系统中，需要有效管理控制输入和输出约束，同时确保鲁棒性并避免传统硬饱和启发式方法导致的性能下降，尤其是在安全关键的飞机控制场景中。

Method: 该方法利用Nagumo定理和比较引理来证明约束满足性，并采用类似于控制障碍函数的最小范数最优控制器。这产生了一个连续的分段线性状态反馈策略，通过线性系统理论保持闭环系统的可分析性。此外，还推导了多输入多输出（MIMO）鲁棒性裕度，并提供了系统的抗饱和保护方法。

Result: 所提出的方法在存在操作约束的情况下，实现了对外部指令的鲁棒跟踪。在主动约束下，MIMO裕度分析表明，该方法保持了与无约束情况相当的增益和相位裕度，优于依赖硬饱和启发式的控制器。通过飞行控制权衡研究和非线性六自由度刚体飞机模型的仿真结果，验证了其在约束满足、鲁棒性和有效抗饱和保护方面的有效性。

Conclusion: 本文提出的系统方法能够有效设计鲁棒的PI伺服控制器，用于处理飞行控制系统中的约束和抗饱和问题，同时保持优异的鲁棒性裕度，并适用于实际安全关键的飞机控制场景。

Abstract: This paper presents a systematic approach for designing robust linear proportional-integral (PI) servo-controllers that effectively manage control input and output constraints in flight control systems. The control design leverages the Nagumo Theorem and the Comparison Lemma to prove constraint satisfaction, while employing min-norm optimal controllers in a manner akin to Control Barrier Functions. This results in a continuous piecewise-linear state feedback policy that maintains the analyzability of the closed-loop system through the principles of linear systems theory. Additionally, we derive multi-input multi-output (MIMO) robustness margins, demonstrating that our approach enables robust tracking of external commands even in the presence of operational constraints. Moreover, the proposed control design offers a systematic approach for anti-windup protection. Through flight control trade studies, we illustrate the applicability of the proposed framework to real-world safety-critical aircraft control scenarios. Notably, MIMO margin analysis with active constraints reveals that our method preserves gain and phase margins comparable to those of the unconstrained case, in contrast to controllers that rely on hard saturation heuristics, which suffer significant performance degradation under active constraints. Simulation results using a nonlinear six-degree-of-freedom rigid body aircraft model further validate the effectiveness of our method in achieving constraint satisfaction, robustness, and effective anti-windup protection.

</details>


### [475] [When the Correct Model Fails: The Optimality of Stackelberg Equilibria with Follower Intention Updates](https://arxiv.org/abs/2511.07363)
*Cayetana Salinas Rodriguez,Jonathan Rogers,Sarah H. Q. Li*

Main category: eess.SY

TL;DR: 本文研究了领导者在动态Stackelberg博弈中，对追随者最佳响应函数未知情况下的策略优化问题。通过信念更新机制，领导者在有限决策周期内重新优化控制，并分析了在开放环和反馈信息结构下的最优性保证，发现有时假设不正确的最佳响应反而能获得更低的成本。


<details>
  <summary>Details</summary>
Motivation: 经典的Stackelberg均衡假设领导者已知追随者的最佳响应映射，但在实际中这往往不成立。领导者需要同时推断追随者的最佳响应函数并实现自身目标。

Method: 研究了一个领导者根据对追随者最佳响应的初始信念选择控制策略的场景。在有限决策周期内，领导者的信念会更新，从而促使其重新优化控制。本文表征了在开放环（OL）和反馈（FB）信息结构下，这种信念更新导致的Stackelberg均衡解的最优性保证。通过线性二次（LQ）Stackelberg博弈的数值示例来支持这些主张。

Result: 研究表明，假设不正确的追随者最佳响应映射，有时在整个博弈周期内可能比知道真实的最佳响应获得更低的成本。

Conclusion: 该研究表征了在信念更新下Stackelberg均衡解的最优性保证，并揭示了一个反直觉的发现：在某些情况下，领导者即使对追随者的最佳响应存在错误假设，也可能比完全知晓真实响应获得更好的性能。

Abstract: We study a two-player dynamic Stackelberg game between a leader and a follower. Classical formulations of the Stackelberg equilibrium (SE) assume that the follower's best response (BR) mapping is known to the leader. However, this is not always true in practice. In those cases the leader needs to simultaneously infer this BR function while fulfilling an internal objective. We study a setting in which the leader selects a control strategy that optimizes an objective given an initial belief about the follower's best response. This belief is updated during the finite decision horizon, prompting the leader to reoptimize its control. We characterize the optimality guarantees of the SE solutions under this belief update for both open loop (OL) and feedback (FB) information structures. In particular, we show that it is possible that assuming an incorrect follower BR map obtains a lower cost over the game horizon than knowing the true BR. We support these claims with numerical examples in a linear quadratic (LQ) Stackelberg game.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [476] [Training-Free Adaptive Quantization for Variable Rate Image Coding for Machines](https://arxiv.org/abs/2511.05836)
*Yui Tatsumi,Ziyue Zeng,Hiroshi Watanabe*

Main category: eess.IV

TL;DR: 本文提出了一种无需训练的自适应量化步长控制方案，为机器图像编码（ICM）实现了灵活的比特率调整，并通过保留语义重要区域和粗略量化不重要区域来提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的ICM框架通常使用固定比特率的图像压缩模型，或需要大量训练才能实现可变比特率，这限制了其实用性并增加了计算和部署复杂性。此外，ICM的可变比特率控制尚未得到充分探索。

Method: 提出了一种无需训练的自适应量化步长控制方案，通过利用通道级熵依赖性和由超先验网络预测的空间尺度参数，在保留语义重要区域的同时粗略量化不重要区域。比特率可以通过单个参数连续控制。

Result: 实验结果表明，该方法有效，相对于非自适应可变比特率方法，实现了高达11.07%的BD-rate节省。

Conclusion: 所提出的无需训练的自适应量化方案能够为ICM提供灵活且连续的比特率调整，有效解决了现有方法的局限性，并提高了编码效率。

Abstract: Image Coding for Machines (ICM) has become increasingly important with the rapid integration of computer vision into real-world applications. However, most ICM frameworks utilize learned image compression (LIC) models that operate at a fixed rate and require separate training for each target bitrate, which may limit their practical applications. Existing variable rate LIC approaches mitigate this limitation but typically depend on training, increasing computational cost and deployment complexity. Moreover, variable rate control has not been thoroughly explored for ICM. To address these challenges, we propose a training-free, adaptive quantization step size control scheme that enables flexible bitrate adjustment. By leveraging both channel-wise entropy dependencies and spatial scale parameters predicted by the hyperprior network, the proposed method preserves semantically important regions while coarsely quantizing less critical areas. The bitrate can be continuously controlled through a single parameter. Experimental results demonstrate the effectiveness of our proposed method, achieving up to 11.07% BD-rate savings over the non-adaptive variable rate method.

</details>


### [477] [EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion](https://arxiv.org/abs/2511.05873)
*Tong Chen,Xinyu Ma,Long Bai,Wenyang Wang,Sun Yue,Luping Zhou*

Main category: eess.IV

TL;DR: EndoIR是一种一体化、与退化类型无关的扩散模型，用于恢复内窥镜图像中多种共存的退化（如低光、烟雾、出血），并在保持效率的同时实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像常遭受多种并存的退化，如低光、烟雾和出血，这些会遮蔽关键的临床细节。现有恢复方法通常是针对特定任务的，且常需预知退化类型，这限制了它们在真实临床应用中的鲁棒性。

Method: 本文提出了EndoIR，一个一体化、与退化类型无关的基于扩散的框架，使用单一模型恢复多种退化。它引入了双域提示器（Dual-Domain Prompter）来提取空间-频率联合特征，并结合自适应嵌入（adaptive embedding）来编码共享和任务特定线索作为去噪条件。为缓解特征混淆，设计了双流扩散架构（Dual-Stream Diffusion architecture），分别处理清晰和退化输入，并通过校正融合块（Rectified Fusion Block）以结构化、退化感知的方式整合它们。此外，噪声感知路由块（Noise-Aware Routing Block）通过动态选择与噪声相关的特征来提高效率。

Result: 在SegSTRONG-C和CEC数据集上的实验表明，EndoIR在多种退化场景下均达到了最先进的性能，同时比强基线模型使用更少的参数。下游分割实验也证实了其临床实用性。

Conclusion: EndoIR是一个有效、鲁棒且高效的解决方案，能够处理内窥镜图像中多样化的退化，具有显著的临床应用潜力。

Abstract: Endoscopic images often suffer from diverse and co-occurring degradations such as low lighting, smoke, and bleeding, which obscure critical clinical details. Existing restoration methods are typically task-specific and often require prior knowledge of the degradation type, limiting their robustness in real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic diffusion-based framework that restores multiple degradation types using a single model. EndoIR introduces a Dual-Domain Prompter that extracts joint spatial-frequency features, coupled with an adaptive embedding that encodes both shared and task-specific cues as conditioning for denoising. To mitigate feature confusion in conventional concatenation-based conditioning, we design a Dual-Stream Diffusion architecture that processes clean and degraded inputs separately, with a Rectified Fusion Block integrating them in a structured, degradation-aware manner. Furthermore, Noise-Aware Routing Block improves efficiency by dynamically selecting only noise-relevant features during denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR achieves state-of-the-art performance across multiple degradation scenarios while using fewer parameters than strong baselines, and downstream segmentation experiments confirm its clinical utility.

</details>


### [478] [Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation](https://arxiv.org/abs/2511.06163)
*Jyun-Ping Kao,Shinyeong Rho,Shahar Lazarev,Hyun-Hae Cho,Fangxu Xing,Taehoon Shin,C. -C. Jay Kuo,Jonghye Woo*

Main category: eess.IV

TL;DR: 本文提出一种参数高效的3D LoRA迁移学习方法，将预训练的CT基础模型成功应用于MRI辅助的ADHD分类，以极少参数实现最先进性能，并首次实现了神经影像领域的跨模态适应。


<details>
  <summary>Details</summary>
Motivation: 儿童ADHD的早期诊断对教育和心理健康结果至关重要，但由于表现异质性和症状重叠，使用神经影像数据进行诊断仍然具有挑战性。

Method: 提出一种新颖的参数高效迁移学习方法，通过将3D卷积核分解为2D低秩更新，在3D卷积网络中引入低秩适应（LoRA）。该方法将一个预训练在CT图像上的大规模3D卷积基础模型，适配到基于MRI的ADHD分类任务上，大幅减少了可训练参数。

Result: 在公共弥散MRI数据库上进行五折交叉验证评估，3D LoRA微调策略取得了最先进的结果，其中一个模型变体达到71.9%的准确率，另一个获得0.716的AUC。这两个变体都只使用了164万个可训练参数（比完全微调的基础模型少113倍以上）。

Conclusion: 研究结果代表了神经影像领域首次成功的跨模态（CT到MRI）基础模型适应，为ADHD分类建立了新基准，并大幅提高了效率。

Abstract: Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in children plays a crucial role in improving outcomes in education and mental health. Diagnosing ADHD using neuroimaging data, however, remains challenging due to heterogeneous presentations and overlapping symptoms with other conditions. To address this, we propose a novel parameter-efficient transfer learning approach that adapts a large-scale 3D convolutional foundation model, pre-trained on CT images, to an MRI-based ADHD classification task. Our method introduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutional kernels into 2D low-rank updates, dramatically reducing trainable parameters while achieving superior performance. In a five-fold cross-validated evaluation on a public diffusion MRI database, our 3D LoRA fine-tuning strategy achieved state-of-the-art results, with one model variant reaching 71.9% accuracy and another attaining an AUC of 0.716. Both variants use only 1.64 million trainable parameters (over 113x fewer than a fully fine-tuned foundation model). Our results represent one of the first successful cross-modal (CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing a new benchmark for ADHD classification while greatly improving efficiency.

</details>


### [479] [SPASHT: An image-enhancement method for sparse-view MPI SPECT](https://arxiv.org/abs/2511.06203)
*Zezhang Yang,Zitong Yu,Nuri Choi,Janice Tania,Wenxuan Xue,Barry A. Siegel,Abhinav K. Jha*

Main category: eess.IV

TL;DR: 本研究提出SPASHT算法，旨在改善稀疏投影视图的心肌灌注SPECT图像质量，从而提高灌注缺损检测性能，并已通过临床数据和人类观察者研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 心肌灌注SPECT扫描时间长，导致患者不适和运动伪影。减少投影视图可缩短扫描时间，但会增加采样伪影和噪声，降低图像质量。

Method: 提出SPASHT算法，旨在通过训练提升稀疏视图SPECT图像的缺陷检测性能。通过回顾性临床研究，在合成植入灌注缺损的患者数据上进行客观评估，使用不同数量的稀疏投影视图（1/6、1/3、1/2）并以AUC量化检测性能。此外，还进行了人类观察者研究以进一步评估性能。

Result: 与稀疏视图协议相比，SPASHT获得的图像在所有稀疏投影视图设置下，其缺陷检测的AUC均显著提高。人类观察者研究也显示，使用SPASHT重建的图像在缺陷检测方面表现出更高的性能。

Conclusion: SPASHT有效提升了稀疏视图心肌灌注SPECT图像的质量和缺陷检测性能，为进一步的临床验证提供了依据。

Abstract: Single-photon emission computed tomography for myocardial perfusion imaging (MPI SPECT) is a widely used diagnostic tool for coronary artery disease. However, the procedure requires considerable scanning time, leading to patient discomfort and the potential for motion-induced artifacts. Reducing the number of projection views while keeping the time per view unchanged provides a mechanism to shorten the scanning time. However, this approach leads to increased sampling artifacts, higher noise, and hence limited image quality. To address these issues, we propose sparseview SPECT image enhancement (SPASHT), inherently training the algorithm to improve performance on defect-detection tasks. We objectively evaluated SPASHT on the clinical task of detecting perfusion defects in a retrospective clinical study using data from patients who underwent MPI SPECT, where the defects were clinically realistic and synthetically inserted. The study was conducted for different numbers of fewer projection views, including 1/6, 1/3, and 1/2 of the typical projection views for MPI SPECT. Performance on the detection task was quantified using area under the receiver operating characteristic curve (AUC). Images obtained with SPASHT yielded significantly improved AUC compared to those obtained with the sparse-view protocol for all the considered numbers of fewer projection views. To further assess performance, a human observer study on the task of detecting perfusion defects was conducted. Results from the human observer study showed improved detection performance with images reconstructed using SPASHT compared to those from the sparse-view protocol. The results provide evidence of the efficacy of SPASHT in improving the quality of sparse-view MPI SPECT images and motivate further clinical validation.

</details>


### [480] [A Visual Perception-Based Tunable Framework and Evaluation Benchmark for H.265/HEVC ROI Encryption](https://arxiv.org/abs/2511.06394)
*Xiang Zhang,Geng Wu,Wenbin Huang,Daoyong Fu,Fei Peng,Zhangjie Fu*

Main category: eess.IV

TL;DR: 本文提出了一个基于视觉感知的可调谐H.265/HEVC ROI选择性加密框架和评估基准，以解决现有ROI加密方法灵活性不足和缺乏统一评估系统的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的ROI（感兴趣区域）视频加密方法存在灵活性不足和缺乏统一评估系统的问题，无法有效平衡安全性与编码效率。

Method: 该方案包含三项关键贡献：1) 基于视觉感知网络的ROI区域识别模块；2) 平衡安全性与实时性能的三级可调谐加密策略；3) 统一的ROI加密评估基准，为后续研究提供标准化定量平台。

Result: 实验结果表明，所提出的评估基准能全面衡量ROI选择性加密的性能。与现有算法相比，本文提出的增强型和高级别加密在多项性能指标上表现更优。

Conclusion: 所提出的框架有效满足了H.265/HEVC中的隐私保护需求，为敏感视频内容的安全高效处理提供了可靠解决方案。

Abstract: ROI selective encryption, as an efficient privacy protection technique, encrypts only the key regions in the video, thereby ensuring security while minimizing the impact on coding efficiency. However, existing ROI-based video encryption methods suffer from insufficient flexibility and lack of a unified evaluation system. To address these issues, we propose a visual perception-based tunable framework and evaluation benchmark for H.265/HEVC ROI encryption. Our scheme introduces three key contributions: 1) A ROI region recognition module based on visual perception network is proposed to accurately identify the ROI region in videos. 2) A three-level tunable encryption strategy is implemented while balancing security and real-time performance. 3) A unified ROI encryption evaluation benchmark is developed to provide a standardized quantitative platform for subsequent research. This triple strategy provides new solution and significant unified performance evaluation methods for ROI selective encryption field. Experimental results indicate that the proposed benchmark can comprehensively measure the performance of the ROI selective encryption. Compared to existing ROI encryption algorithms, our proposed enhanced and advanced level encryption exhibit superior performance in multiple performance metrics. In general, the proposed framework effectively meets the privacy protection requirements in H.265/HEVC and provides a reliable solution for secure and efficient processing of sensitive video content.

</details>


### [481] [Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression](https://arxiv.org/abs/2511.06424)
*Amit Vaisman,Guy Ohayon,Hila Manor,Michael Elad,Tomer Michaeli*

Main category: eess.IV

TL;DR: 本文提出了一种高效的零样本扩散模型图像压缩方法Turbo-DDCM，它比现有方法运行速度快得多，同时保持了与最先进技术相当的性能，并提供了灵活的压缩选项。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本扩散模型压缩方法速度慢且计算成本高昂。

Method: 该方法基于Denoising Diffusion Codebook Models (DDCMs) 压缩方案。通过Turbo-DDCM框架，在每个去噪步骤中高效地结合大量噪声向量，显著减少了所需的去噪操作。同时，它还改进了编码协议，并引入了两个灵活的变体：一个优先区域感知变体和一个基于目标PSNR而非BPP的失真控制变体。

Result: Turbo-DDCM比现有方法运行速度显著加快，同时保持了与最先进技术相当的性能。综合实验表明，它是一种引人注目、实用且灵活的图像压缩方案。

Conclusion: Turbo-DDCM是一种高效、实用且灵活的图像压缩方案，解决了现有扩散模型压缩方法速度慢的问题，并提供了多种实用功能。

Abstract: While zero-shot diffusion-based compression methods have seen significant progress in recent years, they remain notoriously slow and computationally demanding. This paper presents an efficient zero-shot diffusion-based compression method that runs substantially faster than existing methods, while maintaining performance that is on par with the state-of-the-art techniques. Our method builds upon the recently proposed Denoising Diffusion Codebook Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by sequentially choosing the diffusion noise vectors from reproducible random codebooks, guiding the denoiser's output to reconstruct the target image. We modify this framework with Turbo-DDCM, which efficiently combines a large number of noise vectors at each denoising step, thereby significantly reducing the number of required denoising operations. This modification is also coupled with an improved encoding protocol. Furthermore, we introduce two flexible variants of Turbo-DDCM, a priority-aware variant that prioritizes user-specified regions and a distortion-controlled variant that compresses an image based on a target PSNR rather than a target BPP. Comprehensive experiments position Turbo-DDCM as a compelling, practical, and flexible image compression scheme.

</details>


### [482] [Compressive Sensing Photoacoustic Imaging Receiver with Matrix-Vector-Multiplication SAR ADC](https://arxiv.org/abs/2511.06580)
*Huan-Cheng Liao,Shunyao Zhang,Yumin Su,Arvind Govinday,Yiwei Zou,Wei Wang,Vivek Boominathan,Ashok Veeraraghavan,Lei S. Li,Kaiyuan Yang*

Main category: eess.IV

TL;DR: 本文提出一种用于可穿戴光声成像的接收器（RX），通过在硬件中嵌入压缩感知，将输出数据速率降低4-8倍，同时保持低损耗的全阵列信息和高保真图像重建，为小型化可穿戴PA系统提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 可穿戴光声成像设备在健康监测和即时诊断方面前景广阔，但高密度换能器阵列产生的大量数据是实现紧凑、节能可穿戴系统的主要挑战。

Method: 该研究提出一个光声成像接收器，直接将压缩感知嵌入硬件中。它集成了16个AFE和四个矩阵-向量-乘法（MVM）SAR ADC，实现能量和面积高效的模拟域压缩。MVM SAR ADC使用用户定义的可编程三元权重进行被动和精确的MVM。信号重建方法包括：1) 基于快速迭代收缩阈值算法的优化方法，和2) 采用隐式神经表示的学习方法。芯片采用65纳米CMOS工艺制造。

Result: 该架构实现了4-8倍的输出数据速率降低，同时保留了低损耗的全阵列信息。ADC在20.41 MS/s下达到57.5 dB的SNDR，AFE输入参考噪声为3.5 nV/sqrt(Hz)。MVM线性度测量显示R^2 > 0.999。通过体模成像实验验证，在高达8倍压缩下实现了高保真图像重建。该RX每通道功耗为5.83 mW，并支持通用三元加权测量矩阵。

Conclusion: 该接收器为下一代小型化、可穿戴光声成像系统提供了一个引人注目的解决方案，有效解决了高数据量带来的瓶颈问题。

Abstract: Wearable photoacoustic imaging devices hold great promise for continuous health monitoring and point-of-care diagnostics. However, the large data volume generated by high-density transducer arrays presents a major challenge for realizing compact and power-efficient wearable systems. This paper presents a photoacoustic imaging receiver (RX) that embeds compressive sensing directly into the hardware to address this bottleneck. The RX integrates 16 AFEs and four matrix-vector-multiplication (MVM) SAR ADCs that perform energy- and area-efficient analog-domain compression. The architecture achieves a 4-8x reduction in output data rate while preserving low-loss full-array information. The MVM SAR ADC executes passive and accurate MVM using user-defined programmable ternary weights. Two signal reconstruction methods are implemented: (1) an optimization approach using the fast iterative shrinkage-thresholding algorithm, and (2) a learning-based approach employing implicit neural representation. Fabricated in 65 nm CMOS, the chip achieves an ADC's SNDR of 57.5 dB at 20.41 MS/s, with an AFE input-referred noise of 3.5 nV/sqrt(Hz). MVM linearity measurements show R^2 > 0.999 across a wide range of weights and input amplitudes. The system is validated through phantom imaging experiments, demonstrating high-fidelity image reconstruction under up to 8x compression. The RX consumes 5.83 mW/channel and supports a general ternary-weighted measurement matrix, offering a compelling solution for next-generation miniaturized, wearable PA imaging systems.

</details>


### [483] [Hierarchical Spatial-Frequency Aggregation for Spectral Deconvolution Imaging](https://arxiv.org/abs/2511.06751)
*Tao Lv,Daoming Zhou,Chenglong Huang,Chongde Zi,Linsen Chen,Xun Cao*

Main category: eess.IV

TL;DR: 本文提出了一种名为HSFAUT的基于Transformer的深度展开方法，用于解决光谱去卷积成像（SDI）中的逆问题，通过分层空间-光谱聚合和频率域投影，显著提升了成像质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的计算光谱成像（CSI）存在体积庞大和保真度有限的问题。虽然基于PSF工程的光谱去卷积成像（SDI）方法旨在提高保真度并实现紧凑设计，但其复合卷积-积分操作导致法方程系数矩阵依赖于场景，这阻碍了先验知识的有效利用并对准确重建构成了挑战。

Method: 为了解决SDI中固有的数据依赖性算子，本文引入了分层空间-光谱聚合展开框架（HSFAUF）。该框架通过分解子问题并将其投影到频域，将非线性过程转化为线性映射，从而实现高效求解。此外，为了在迭代细化过程中整合空间-光谱先验，提出了一种空间-频率聚合Transformer（SFAT），它明确地聚合了空间和频率域的信息。通过将SFAT集成到HSFAUF中，开发了Transformer的深度展开方法HSFAUT。

Result: 系统的模拟和真实实验表明，HSFAUT在更低的内存和计算成本下超越了现有最先进的方法，并在不同的SDI系统上表现出最佳性能。

Conclusion: HSFAUT通过引入分层空间-光谱聚合展开框架和空间-频率聚合Transformer，有效解决了SDI中数据依赖性算子和先验利用的挑战，实现了高保真度、高效率的光谱成像重建，并优于现有方法。

Abstract: Computational spectral imaging (CSI) achieves real-time hyperspectral imaging through co-designed optics and algorithms, but typical CSI methods suffer from a bulky footprint and limited fidelity. Therefore, Spectral Deconvolution imaging (SDI) methods based on PSF engineering have been proposed to achieve high-fidelity compact CSI design recently. However, the composite convolution-integration operations of SDI render the normal-equation coefficient matrix scene-dependent, which hampers the efficient exploitation of imaging priors and poses challenges for accurate reconstruction. To tackle the inherent data-dependent operators in SDI, we introduce a Hierarchical Spatial-Spectral Aggregation Unfolding Framework (HSFAUF). By decomposing subproblems and projecting them into the frequency domain, HSFAUF transforms nonlinear processes into linear mappings, thereby enabling efficient solutions. Furthermore, to integrate spatial-spectral priors during iterative refinement, we propose a Spatial-Frequency Aggregation Transformer (SFAT), which explicitly aggregates information across spatial and frequency domains. By integrating SFAT into HSFAUF, we develop a Transformer-based deep unfolding method, \textbf{H}ierarchical \textbf{S}patial-\textbf{F}requency \textbf{A}ggregation \textbf{U}nfolding \textbf{T}ransformer (HSFAUT), to solve the inverse problem of SDI. Systematic simulated and real experiments show that HSFAUT surpasses SOTA methods with cheaper memory and computational costs, while exhibiting optimal performance on different SDI systems.

</details>


### [484] [RRTS Dataset: A Benchmark Colonoscopy Dataset from Resource-Limited Settings for Computer-Aided Diagnosis Research](https://arxiv.org/abs/2511.06769)
*Ridoy Chandra Shil,Ragib Abid,Tasnia Binte Mamun,Samiul Based Shuvo,Masfique Ahmed Bhuiyan,Jahid Ferdous*

Main category: eess.IV

TL;DR: 本文引入了一个名为BUET息肉数据集（BPD）的新结肠镜图像数据集，旨在解决现有数据集缺乏真实世界复杂性和临床伪影的问题，并提供了分类和分割的基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的结肠镜息肉检测数据集（如CVC-ClinicDB和Kvasir-SEG）样本量小、图像经过筛选或缺乏真实世界的伪影，未能充分反映临床实践的复杂性，尤其是在资源受限的环境中。

Method: 研究人员收集了一个包含结肠镜图像的新数据集（BPD），这些图像是在常规临床条件下使用奥林巴斯170和宾得i-Scan系列内窥镜采集的。数据集包含专家标注的二值掩码，涵盖运动模糊、镜面反射、粪便伪影、血液和弱光等多种挑战。同时，使用VGG16、ResNet50和InceptionV3对分类任务进行基准测试，并使用VGG16、ResNet34和InceptionV4骨干网络的UNet变体对分割任务进行基准测试。

Result: 该数据集包含来自164名患者的1,288张带有息肉的图像及其对应的真实掩码，以及来自31名患者的1,657张无息肉图像。基准测试结果显示，二值分类的最高准确率为90.8%（VGG16），分割的最高Dice系数为0.64（InceptionV4-UNet）。与经过筛选的数据集相比，性能有所下降，这反映了真实世界图像中存在伪影和质量变化的难度。

Conclusion: BPD数据集更真实地反映了临床实践中的复杂性和挑战，为结直肠癌预防中的息肉检测提供了一个更具代表性的基准，尽管现有模型在该数据集上的性能低于在经过筛选数据集上的表现。

Abstract: Background and Objective: Colorectal cancer prevention relies on early detection of polyps during colonoscopy. Existing public datasets, such as CVC-ClinicDB and Kvasir-SEG, provide valuable benchmarks but are limited by small sample sizes, curated image selection, or lack of real-world artifacts. There remains a need for datasets that capture the complexity of clinical practice, particularly in resource-constrained settings. Methods: We introduce a dataset, BUET Polyp Dataset (BPD), of colonoscopy images collected using Olympus 170 and Pentax i-Scan series endoscopes under routine clinical conditions. The dataset contains images with corresponding expert-annotated binary masks, reflecting diverse challenges such as motion blur, specular highlights, stool artifacts, blood, and low-light frames. Annotations were manually reviewed by clinical experts to ensure quality. To demonstrate baseline performance, we provide benchmark results for classification using VGG16, ResNet50, and InceptionV3, and for segmentation using UNet variants with VGG16, ResNet34, and InceptionV4 backbones. Results: The dataset comprises 1,288 images with polyps from 164 patients with corresponding ground-truth masks and 1,657 polyp-free images from 31 patients. Benchmarking experiments achieved up to 90.8% accuracy for binary classification (VGG16) and a maximum Dice score of 0.64 with InceptionV4-UNet for segmentation. Performance was lower compared to curated datasets, reflecting the real-world difficulty of images with artifacts and variable quality.

</details>


### [485] [Anatomy-Aware Lymphoma Lesion Detection in Whole-Body PET/CT](https://arxiv.org/abs/2511.07047)
*Simone Bendazzoli,Antonios Tzortzakakis,Andreas Abrahamsson,Björn Engelbrekt Wahlin,Örjan Smedby,Maria Holstensson,Rodrigo Moreno*

Main category: eess.IV

TL;DR: 本研究探讨了在深度学习癌症病灶检测模型中加入解剖学先验信息（如器官分割掩码）的效果，发现其显著改善了基于CNN的模型的性能，但对Swin Transformer影响甚微，且Swin Transformer并未显示出明显优势。


<details>
  <summary>Details</summary>
Motivation: 早期癌症检测对改善患者预后至关重要，而18F FDG PET/CT成像在结合代谢和解剖信息方面发挥关键作用。然而，由于需要识别大小不一的多个病灶，准确的病灶检测仍然充满挑战。

Method: 研究通过将TotalSegmentator工具生成的器官分割掩码作为辅助输入，为最先进的病灶检测模型nnDetection（基于CNN）和Swin Transformer（视觉Transformer）提供解剖学背景。Swin Transformer采用两阶段训练，结合自监督预训练和有监督微调。方法在AutoPET和Karolinska淋巴瘤数据集上进行了测试。

Result: 结果表明，在nnDetection框架内，加入解剖学先验信息显著提高了检测性能，而对视觉Transformer（Swin Transformer）的性能几乎没有影响。此外，研究观察到Swin Transformer相对于nnDetection中使用的传统卷积神经网络（CNN）编码器并未提供明显的优势。

Conclusion: 这些发现强调了解剖学背景在癌症病灶检测中的关键作用，尤其是在基于CNN的模型中。

Abstract: Early cancer detection is crucial for improving patient outcomes, and 18F FDG PET/CT imaging plays a vital role by combining metabolic and anatomical information. Accurate lesion detection remains challenging due to the need to identify multiple lesions of varying sizes. In this study, we investigate the effect of adding anatomy prior information to deep learning-based lesion detection models. In particular, we add organ segmentation masks from the TotalSegmentator tool as auxiliary inputs to provide anatomical context to nnDetection, which is the state-of-the-art for lesion detection, and Swin Transformer. The latter is trained in two stages that combine self-supervised pre-training and supervised fine-tuning. The method is tested in the AutoPET and Karolinska lymphoma datasets. The results indicate that the inclusion of anatomical priors substantially improves the detection performance within the nnDetection framework, while it has almost no impact on the performance of the vision transformer. Moreover, we observe that Swin Transformer does not offer clear advantages over conventional convolutional neural network (CNN) encoders used in nnDetection. These findings highlight the critical role of the anatomical context in cancer lesion detection, especially in CNN-based models.

</details>


### [486] [TauFlow: Dynamic Causal Constraint for Complexity-Adaptive Lightweight Segmentation](https://arxiv.org/abs/2511.07057)
*Zidong Chen,Fadratul Hafinaz Hassan*

Main category: eess.IV

TL;DR: 本文提出了TauFlow，一种受类脑机制启发的轻量级医学图像分割模型，通过动态特征响应策略，有效解决了病灶边界对比度处理和极轻量化模型精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署轻量级医学图像分割模型面临两大挑战：1) 有效处理病灶边界与背景区域之间的显著对比；2) 在追求极轻量化设计（如参数量小于0.5M）时，精度会急剧下降。

Method: 本文提出了TauFlow模型，其核心是受类脑机制启发的动态特征响应策略。具体通过两项关键创新实现：1) 卷积长时常数单元（ConvLTC），动态调节特征更新速率，以“慢速”处理低频背景和“快速”响应高频边界；2) STDP自组织模块，显著缓解编码器和解码器之间的特征冲突。

Result: ConvLTC能够动态调节特征更新速率，有效处理低频背景和高频边界。STDP自组织模块将特征冲突率从大约35%-40%显著降低到8%-10%。

Conclusion: TauFlow是一个新颖的轻量级分割模型，通过其类脑的动态特征响应策略，成功解决了医学图像分割中病灶边界处理的效率问题以及极轻量级模型精度下降的挑战。

Abstract: Deploying lightweight medical image segmentation models on edge devices presents two major challenges: 1) efficiently handling the stark contrast between lesion boundaries and background regions, and 2) the sharp drop in accuracy that occurs when pursuing extremely lightweight designs (e.g., <0.5M parameters). To address these problems, this paper proposes TauFlow, a novel lightweight segmentation model. The core of TauFlow is a dynamic feature response strategy inspired by brain-like mechanisms. This is achieved through two key innovations: the Convolutional Long-Time Constant Cell (ConvLTC), which dynamically regulates the feature update rate to "slowly" process low-frequency backgrounds and "quickly" respond to high-frequency boundaries; and the STDP Self-Organizing Module, which significantly mitigates feature conflicts between the encoder and decoder, reducing the conflict rate from approximately 35%-40% to 8%-10%.

</details>


### [487] [Validation of Fully-Automated Deep Learning-Based Fibroglandular Tissue Segmentation for Efficient and Reliable Quantitation of Background Parenchymal Enhancement in Breast MRI](https://arxiv.org/abs/2511.07088)
*Yu-Tzu Kuo,Anum S. Kazerouni,Vivian Y. Park,Wesley Surento,Suleeporn Sujichantararat,Daniel S. Hippe,Habib Rahbar,Savannah C. Partridge*

Main category: eess.IV

TL;DR: 本研究评估了一种基于深度学习的全自动方法用于乳腺纤维腺体组织（FGT）分割，以量化背景实质增强（BPE），并发现其在效率和与放射科医生评估的一致性方面优于半自动方法，有助于乳腺癌风险评估。


<details>
  <summary>Details</summary>
Motivation: 乳腺动态对比增强磁共振成像（DCE-MRI）上的背景实质增强（BPE）被认为是乳腺癌的潜在风险标志物。临床上BPE通常由放射科医生定性评估，但定性评估存在主观性。定量BPE测量有望提供更精确的风险评估，因此需要开发并验证更高效、客观的定量方法。

Method: 本研究评估了一个现有的开源、全自动的基于深度学习（DL）的FGT分割方法，用于量化BPE，并将其与半自动模糊C均值方法进行比较。研究使用了100名女性的乳腺MRI检查数据，评估了两种方法的分割一致性、定量BPE指标的一致性以及与定性BPE的相关性。放射科医生对两种方法的FGT分割质量进行了评分。

Result: 结果显示，基于DL的方法和半自动方法在定量BPE测量方面表现出良好的一致性。然而，基于DL的测量与定性BPE评估的相关性更强，且放射科医生对基于DL的分割质量评分更高。

Conclusion: 本研究结果表明，基于深度学习的FGT分割提高了客观BPE量化的效率，并可能改进标准化乳腺癌风险评估的准确性。

Abstract: Background parenchymal enhancement (BPE) on breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) shows potential as a breast cancer risk marker. Clinically, BPE is qualitatively assessed by radiologists, but quantitative BPE measures offer potential for more precise risk evaluation. This study evaluated an existing open-source, fully-automated deep learning-based (DL-based) method for segmenting fibroglandular tissue (FGT) to quantify BPE and compared it to a semi-automated fuzzy c-means method. Using breast MRI examinations from 100 women, we evaluated segmentation agreement, concordance across quantitative BPE metrics, and associations with qualitative BPE. The quality of FGT segmentations from both methods was scored by a radiologist. While the DL-based and semi-automated methods showed good agreement for quantitative BPE measurements, DL-based measures more strongly correlated with qualitative BPE assessments and DL-based segmentations were scored as higher quality by the radiologist. Our findings suggest that DL-based FGT segmentation enhances efficiency for objective BPE quantification and may improve standardized breast cancer risk assessment.

</details>


### [488] [Task-Adaptive Low-Dose CT Reconstruction](https://arxiv.org/abs/2511.07094)
*Necati Sefercioglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: eess.IV

TL;DR: 深度学习低剂量CT重建方法在标准图像质量指标上表现良好，但在诊断所需的关键解剖细节保留方面存在不足。本文提出一种任务自适应重建框架，通过将预训练的任务网络作为正则化项整合到重建损失函数中，显著提升了诊断质量，优于联合训练和传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的低剂量CT重建方法虽然在PSNR和SSIM等标准图像质量指标上表现出色，但却常常无法保留诊断任务所需的关键解剖细节，这严重限制了其临床应用。

Method: 本文提出了一种新颖的任务自适应重建框架。该框架通过将一个冻结的预训练任务网络作为正则化项引入重建损失函数中。与现有同时优化重建和任务网络的联合训练方法不同，本方法利用预训练的任务模型来指导重建训练，同时保持诊断质量。

Result: 在肝脏和肝肿瘤分割任务上验证了该框架。任务自适应模型获得了高达0.707的Dice分数，接近全剂量扫描的性能（0.874），并显著优于联合训练方法（0.331）和传统重建方法（0.626）。此外，该框架可通过简单的损失函数修改集成到任何现有的深度学习重建模型中。

Conclusion: 所提出的任务自适应框架有效地弥补了低剂量CT重建中高图像质量指标与诊断实用性之间的差距。通过利用预训练的任务网络指导重建，该方法显著提高了诊断质量（例如分割性能），为临床实践提供了实用且可广泛采用的任务自适应优化解决方案。

Abstract: Deep learning-based low-dose computed tomography reconstruction methods already achieve high performance on standard image quality metrics like peak signal-to-noise ratio and structural similarity index measure. Yet, they frequently fail to preserve the critical anatomical details needed for diagnostic tasks. This fundamental limitation hinders their clinical applicability despite their high metric scores. We propose a novel task-adaptive reconstruction framework that addresses this gap by incorporating a frozen pre-trained task network as a regularization term in the reconstruction loss function. Unlike existing joint-training approaches that simultaneously optimize both reconstruction and task networks, and risk diverging from satisfactory reconstructions, our method leverages a pre-trained task model to guide reconstruction training while still maintaining diagnostic quality. We validate our framework on a liver and liver tumor segmentation task. Our task-adaptive models achieve Dice scores up to 0.707, approaching the performance of full-dose scans (0.874), and substantially outperforming joint-training approaches (0.331) and traditional reconstruction methods (0.626). Critically, our framework can be integrated into any existing deep learning-based reconstruction model through simple loss function modification, enabling widespread adoption for task-adaptive optimization in clinical practice. Our codes are available at: https://github.com/itu-biai/task_adaptive_ct

</details>


### [489] [CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video](https://arxiv.org/abs/2511.07290)
*Xinyi Wang,Angeliki Katsenou,Junxiao Shen,David Bull*

Main category: eess.IV

TL;DR: 本文提出CAMP-VQA，一个新颖的无参考视频质量评估（NR-VQA）框架，利用大型视觉语言模型和质量感知提示机制，整合视频元数据和帧间变化来生成细粒度质量描述。它在用户生成内容（UGC）视频质量评估方面超越了现有方法，无需昂贵的手动标注。


<details>
  <summary>Details</summary>
Motivation: 用户生成内容（UGC）在平台上的普及使得无参考感知视频质量评估（NR-VQA）对视频传输优化至关重要。然而，非专业采集和随后的转码给NR-VQA带来了巨大挑战。现有模型在压缩内容的NR-VQA方面表现有限，部分原因是缺乏对伪影类型的细粒度感知标注。

Method: 本文提出CAMP-VQA框架，利用大型视觉语言模型（如BLIP-2）的语义理解能力。该方法引入了一种质量感知提示机制，将视频元数据（如分辨率、帧率、比特率）与从帧间变化中提取的关键片段相结合，以指导BLIP-2生成细粒度的质量描述。设计了一个统一架构来从语义对齐、时间特性和空间特性三个维度建模感知质量，并提取、融合多模态特征，最终回归到视频质量分数。

Result: 在各种UGC数据集上的广泛实验表明，CAMP-VQA模型始终优于现有NR-VQA方法，在无需昂贵的手动细粒度标注的情况下提高了准确性。与最先进的方法相比，该方法在平均排名和线性相关性方面表现最佳（SRCC: 0.928, PLCC: 0.938）。

Conclusion: CAMP-VQA通过利用大型视觉语言模型的语义理解能力和质量感知提示机制，有效解决了UGC视频无参考质量评估的挑战。它在性能上显著优于现有方法，且无需耗时的人工标注，为优化视频传输提供了更高效和准确的解决方案。

Abstract: The prevalence of user-generated content (UGC) on platforms such as YouTube and TikTok has rendered no-reference (NR) perceptual video quality assessment (VQA) vital for optimizing video delivery. Nonetheless, the characteristics of non-professional acquisition and the subsequent transcoding of UGC video on sharing platforms present significant challenges for NR-VQA. Although NR-VQA models attempt to infer mean opinion scores (MOS), their modeling of subjective scores for compressed content remains limited due to the absence of fine-grained perceptual annotations of artifact types. To address these challenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the semantic understanding capabilities of large vision-language models. Our approach introduces a quality-aware prompting mechanism that integrates video metadata (e.g., resolution, frame rate, bitrate) with key fragments extracted from inter-frame variations to guide the BLIP-2 pretraining approach in generating fine-grained quality captions. A unified architecture has been designed to model perceptual quality across three dimensions: semantic alignment, temporal characteristics, and spatial characteristics. These multimodal features are extracted and fused, then regressed to video quality scores. Extensive experiments on a wide variety of UGC datasets demonstrate that our model consistently outperforms existing NR-VQA methods, achieving improved accuracy without the need for costly manual fine-grained annotations. Our method achieves the best performance in terms of average rank and linear correlation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods. The source code and trained models, along with a user-friendly demo, are available at: https://github.com/xinyiW915/CAMP-VQA.

</details>
