<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 33]
- [cs.CV](#cs.CV) [Total: 101]
- [cs.CL](#cs.CL) [Total: 41]
- [cs.RO](#cs.RO) [Total: 29]
- [eess.SY](#eess.SY) [Total: 10]
- [eess.IV](#eess.IV) [Total: 10]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: 本文提出一个基于LLM的智能预测性维护系统，通过将轴承振动数据转化为自然语言，结合多智能体处理维护手册和网络搜索，提供精确的维护建议，实现从状态监测到可操作维护计划的转化。


<details>
  <summary>Details</summary>
Motivation: 工业机械维护需要及时干预以防止灾难性故障并优化运营效率。现有方法通常仅限于异常检测，缺乏可操作的维护建议，因此需要一个能提供更深层次、更具体维护指导的智能系统。

Method: 该系统基于LLM，扩展了之前的LAMP框架。它将轴承振动频率数据（BPFO、BPFI、BSF、FTF）序列化为自然语言，供LLM处理以实现少样本异常检测。系统能分类故障类型并评估严重程度。一个多智能体组件通过向量嵌入和语义搜索处理维护手册，并进行网络搜索以获取全面的程序知识和最新维护实践。最终，Gemini模型生成结构化的维护建议，包括即时行动、检查清单、纠正措施、零件需求和时间表。

Result: 在轴承振动数据集上的实验验证表明，该系统能有效检测异常并提供与上下文相关的维护指导。它成功弥合了状态监测和可操作维护计划之间的差距，为工业从业者提供了智能决策支持。

Conclusion: 这项工作推动了LLM在工业维护中的应用，提供了一个可扩展的预测性维护框架，适用于各种机械部件和工业领域，为工业实践者提供了智能决策支持。

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [2] [GeoFlow: Agentic Workflow Automation for Geospatial Tasks](https://arxiv.org/abs/2508.04719)
*Amulya Bhattaram,Justin Chung,Stanley Chung,Ranit Gupta,Janani Ramamoorthy,Kartikeya Gullapalli,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: GeoFlow是一种自动生成地理空间任务代理工作流的方法，通过明确的工具调用目标，提高了代理成功率并显著降低了LLM的token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于推理分解，但对API选择不够明确，导致代理在地理空间任务中效率低下或成功率不高。

Method: GeoFlow为每个代理提供详细的工具调用目标，以在运行时指导地理空间API的调用。

Result: GeoFlow与现有最先进方法相比，将代理成功率提高了6.8%，并将主要LLM家族的token使用量减少了多达四倍。

Conclusion: GeoFlow通过更明确的工具调用指导，显著提升了地理空间任务自动化代理的性能和效率。

Abstract: We present GeoFlow, a method that automatically generates agentic workflows
for geospatial tasks. Unlike prior work that focuses on reasoning decomposition
and leaves API selection implicit, our method provides each agent with detailed
tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow
increases agentic success by 6.8% and reduces token usage by up to fourfold
across major LLM families compared to state-of-the-art approaches.

</details>


### [3] [Who is a Better Player: LLM against LLM](https://arxiv.org/abs/2508.04720)
*Yingjie Zhou,Jiezhang Cao,Farong Wen,Li Xu,Yanwei Jiang,Jun Jia,Ronghui Li,Xiaohong Liu,Yu Zhou,Xiongkuo Min,Jie Guo,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: 该研究提出了一个对抗性基准测试框架，通过棋盘游戏竞赛来评估大型语言模型（LLMs）的综合性能，以弥补传统问答基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 主流的基于问答的基准测试方法存在数据依赖性限制，无法全面评估LLMs在战略推理和智能方面的综合性能。棋盘游戏作为对抗性战略推理的典型领域，可作为评估AI系统的有效基准。

Method: 提出了一个对抗性基准测试框架，并开发了专用评估平台“齐镇”（Qi Town），支持5种棋盘游戏和20个LLM驱动的玩家。采用Elo等级分系统和新颖的性能循环图（PLG）来量化评估LLMs的技术能力，同时通过游戏过程中的积极情绪得分（PSS）评估心理素质。评估以循环赛锦标赛形式进行。

Result: 实验结果表明，尽管存在技术差异，大多数LLMs对输赢保持乐观，在对抗性高压环境下表现出比人类更强的适应性。然而，PLGs中循环胜负的复杂关系暴露了LLMs在游戏过程中技能发挥的不稳定性。

Conclusion: LLMs在对抗性棋盘游戏中表现出良好的心理适应性，但其技能发挥的稳定性仍需进一步解释和探索。

Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and
intelligence, have long served as both a popular competitive activity and a
benchmark for evaluating artificial intelligence (AI) systems. Building on this
foundation, we propose an adversarial benchmarking framework to assess the
comprehensive performance of Large Language Models (LLMs) through board games
competition, compensating the limitation of data dependency of the mainstream
Question-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a
specialized evaluation platform that supports 5 widely played games and
involves 20 LLM-driven players. The platform employs both the Elo rating system
and a novel Performance Loop Graph (PLG) to quantitatively evaluate the
technical capabilities of LLMs, while also capturing Positive Sentiment Score
(PSS) throughout gameplay to assess mental fitness. The evaluation is
structured as a round-robin tournament, enabling systematic comparison across
players. Experimental results indicate that, despite technical differences,
most LLMs remain optimistic about winning and losing, demonstrating greater
adaptability to high-stress adversarial environments than humans. On the other
hand, the complex relationship between cyclic wins and losses in PLGs exposes
the instability of LLMs' skill play during games, warranting further
explanation and exploration.

</details>


### [4] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: 本研究比较了AWebGIS的三种实现方法：基于云端LLM的在线方法、基于传统ML分类器的离线方法以及基于客户端微调SLM的离线方法。结果显示，客户端SLM方法表现最佳，且能有效解决隐私、可扩展性和服务器负载问题。


<details>
  <summary>Details</summary>
Motivation: AWebGIS旨在通过自然语言实现地理空间操作，提供直观、智能和免手动交互。然而，当前多数解决方案依赖云端LLM，存在持续网络连接需求、用户隐私泄露以及集中式服务器处理导致的可扩展性问题。

Method: 本研究比较了三种实现AWebGIS的方法：1) 完全自动化的在线方法，使用云端LLM（如Cohere）；2) 半自动化的离线方法，使用支持向量机和随机森林等经典机器学习分类器；3) 完全自主的离线（客户端）方法，基于微调的小型语言模型（具体为T5-small），在用户网络浏览器中执行。

Result: 第三种方法（利用SLM）在所有方法中取得了最高精度，精确匹配准确率为0.93，Levenshtein相似度为0.99，ROUGE-1和ROUGE-L得分均为0.98。这种客户端计算策略通过将处理负载转移到用户设备，减少了后端服务器的负担，消除了对服务器推理的需求。

Conclusion: 研究结果突显了浏览器可执行模型在AWebGIS解决方案中的可行性。客户端SLM方法不仅性能优越，还能有效解决云端LLM在隐私、可扩展性和服务器负载方面的固有问题，为AWebGIS提供了更高效和去中心化的实现途径。

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [5] [Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning](https://arxiv.org/abs/2508.04848)
*Chang Tian,Matthew B. Blaschko,Mingzhe Xing,Xiuxing Li,Yinliang Yue,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 研究发现，强化学习（RL）微调虽能提升大型语言模型（LLM）在理想条件下的推理能力，但在非理想场景（如摘要推理、噪声抑制、上下文过滤）下表现显著下降，揭示了当前LLM推理能力的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理基准多在理想环境下评估，忽略了实际应用中常见的非理想场景。受人类在不完美输入下仍能可靠推理的启发，本研究旨在探索RL增强的LLM在这些更具挑战性的非理想情境下的性能。

Method: 定义了摘要推理、细粒度噪声抑制和上下文过滤三种代表性非理想场景。使用策略梯度算法对三个LLM和一个LVLM进行RL微调，并在八个公开数据集上测试其在理想和非理想场景下的推理表现。此外，提出了一种场景特定的补救方法。

Result: RL微调虽然提升了LLM在理想设置下的基线推理能力，但在所有三种非理想场景中性能均显著下降，暴露出其高级推理能力的严重局限性。尽管提出了补救方法，但结果显示这些推理缺陷仍未得到有效解决。

Conclusion: 本研究强调大型模型的推理能力常被高估，并指出在非理想场景下评估模型的重要性。当前RL方法在处理这些挑战性推理缺陷方面仍存在显著不足。

Abstract: Reinforcement learning (RL) has become a key technique for enhancing the
reasoning abilities of large language models (LLMs), with policy-gradient
algorithms dominating the post-training stage because of their efficiency and
effectiveness. However, most existing benchmarks evaluate large-language-model
reasoning under idealized settings, overlooking performance in realistic,
non-ideal scenarios. We identify three representative non-ideal scenarios with
practical relevance: summary inference, fine-grained noise suppression, and
contextual filtering. We introduce a new research direction guided by
brain-science findings that human reasoning remains reliable under imperfect
inputs. We formally define and evaluate these challenging scenarios. We
fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)
using RL with a representative policy-gradient algorithm and then test their
performance on eight public datasets. Our results reveal that while RL
fine-tuning improves baseline reasoning under idealized settings, performance
declines significantly across all three non-ideal scenarios, exposing critical
limitations in advanced reasoning capabilities. Although we propose a
scenario-specific remediation method, our results suggest current methods leave
these reasoning deficits largely unresolved. This work highlights that the
reasoning abilities of large models are often overstated and underscores the
importance of evaluating models under non-ideal scenarios. The code and data
will be released at XXXX.

</details>


### [6] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: HealthFlow是一种自进化的AI智能体，通过元级别进化机制克服了传统AI在医疗研究中策略固定的问题，能学习成为更好的战略规划者，并在新基准EHRFlowBench上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体在医疗研究中依赖静态、预定义策略，使其只能成为工具使用者而非战略规划者，这在医疗等复杂领域是一个关键限制。

Method: 引入HealthFlow，一个自进化的AI智能体，通过新颖的元级别进化机制，将过程中的成功与失败提炼成持久的战略知识库，从而自主改进其高层问题解决策略。同时，引入EHRFlowBench作为新的基准，包含来自同行评审临床研究的复杂真实健康数据分析任务，以进行可复现的评估。

Result: 全面的实验表明，HealthFlow的自进化方法显著优于现有最先进的智能体框架。

Conclusion: 这项工作标志着从构建更好的工具使用者转向设计更智能、自进化的任务管理者，为实现更自主、有效的AI用于科学发现铺平了道路。

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [7] [The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding](https://arxiv.org/abs/2508.05006)
*Youzhi Zhang,Yufei Li,Gaofeng Meng,Hongbin Liu,Jiebo Luo*

Main category: cs.AI

TL;DR: 该研究提出了一种新颖的博弈论框架“Docking Game”和“Loop Self-Play (LoopPlay)”算法，用于解决分子对接中配体和蛋白质模块性能不均衡的问题，并通过相互适应和自我优化显著提升了结合模式预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前分子对接的多任务学习模型在配体对接方面的性能通常不如蛋白质口袋对接，主要原因在于配体和蛋白质结构复杂性的显著差异。

Method: 研究将蛋白质-配体相互作用建模为一个双人博弈，称为“Docking Game”，其中配体对接模块作为配体玩家，蛋白质口袋对接模块作为蛋白质玩家。为解决此博弈，开发了“Loop Self-Play (LoopPlay)”算法，该算法通过一个两级循环交替训练两个玩家：外层循环允许玩家交换预测姿态以促进相互适应；内层循环则让每个玩家通过整合自己的预测姿态进行动态自我优化。理论上证明了LoopPlay的收敛性。

Result: 在公共基准数据集上进行的广泛实验表明，LoopPlay在预测准确结合模式方面比现有最先进方法提高了约10%。此外，该算法的收敛性得到了理论证明。

Conclusion: LoopPlay算法的提出及其在准确结合模式预测上的显著改进，突显了其在药物发现中提升分子对接准确性的巨大潜力。

Abstract: Molecular docking is a crucial aspect of drug discovery, as it predicts the
binding interactions between small-molecule ligands and protein pockets.
However, current multi-task learning models for docking often show inferior
performance in ligand docking compared to protein pocket docking. This
disparity arises largely due to the distinct structural complexities of ligands
and proteins. To address this issue, we propose a novel game-theoretic
framework that models the protein-ligand interaction as a two-player game
called the Docking Game, with the ligand docking module acting as the ligand
player and the protein pocket docking module as the protein player. To solve
this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which
alternately trains these players through a two-level loop. In the outer loop,
the players exchange predicted poses, allowing each to incorporate the other's
structural predictions, which fosters mutual adaptation over multiple
iterations. In the inner loop, each player dynamically refines its predictions
by incorporating its own predicted ligand or pocket poses back into its model.
We theoretically show the convergence of LoopPlay, ensuring stable
optimization. Extensive experiments conducted on public benchmark datasets
demonstrate that LoopPlay achieves approximately a 10\% improvement in
predicting accurate binding modes compared to previous state-of-the-art
methods. This highlights its potential to enhance the accuracy of molecular
docking in drug discovery.

</details>


### [8] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: 研究探索了大型语言模型（LLMs）在整合大规模、异构、噪声城市空间数据方面的应用，发现LLMs在提供相关特征和采用“审查-修正”方法后，能有效进行空间数据集成。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的空间数据集成方法难以覆盖所有边缘情况，需要大量手动验证和修复；机器学习方法则需要收集和标注大量特定任务样本。因此，需要一种更灵活、高效的方法来处理大规模、异构、噪声的城市空间数据集成。

Method: 本研究首先分析了LLMs对人类经验介导的环境空间关系（如道路和人行道之间）的推理能力。随后，通过提供相关特征来减少对空间推理的依赖，并评估LLMs的性能。最后，采用并调整了一种“审查-修正”（review-and-refine）方法来纠正错误响应并保留准确响应。

Result: LLMs展现了空间推理能力，但在连接宏观环境与相关计算几何任务时表现不佳，常产生逻辑不连贯的响应。然而，当提供相关特征时（从而减少对空间推理的依赖），LLMs能够产生高性能的结果。“审查-修正”方法在纠正错误初始响应的同时保留准确响应方面非常有效。

Conclusion: LLMs是传统基于规则启发式方法的一种有前景且灵活的替代方案，能够提升自适应空间数据集成能力。未来研究方向包括后训练、多模态集成方法以及对多样数据格式的支持。

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [9] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: 该研究提出一种基于人类双系统认知理论的Web导航AI代理框架CogniWeb，有效整合离线模仿学习和在线探索，实现了高效且具竞争力的Web导航性能。


<details>
  <summary>Details</summary>
Motivation: Web导航是评估AGI的关键挑战，因为它涉及高熵、动态环境中的复杂决策和组合爆炸式的动作空间。现有自主Web代理方法要么侧重于离线模仿学习，要么侧重于在线探索，但很少有效整合两者。

Method: 受人类双过程认知理论启发，研究将Web导航分解为快速的系统1（直觉反应行为）和慢速的系统2（深思熟虑的规划能力）。该框架统一了现有方法，弥合了离线学习和在线能力获取之间的鸿沟。他们将此框架实现为模块化代理架构CogniWeb，可根据任务复杂性自适应地在快速直觉处理和深思熟虑推理之间切换。

Result: 在WebArena上的评估表明，CogniWeb实现了具有竞争力的性能（43.96%的成功率），同时显著提高了效率（代币使用量减少75%）。

Conclusion: 该研究提出的双过程框架为Web代理提供了一个统一的视角，有效整合了离线学习和在线探索，实现了高效且性能卓越的Web导航能力。

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [10] [MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.05083)
*Dexuan Xu,Jieyi Wang,Zhongyan Chai,Yongzhi Cao,Hanpin Wang,Huamin Zhang,Yu Huang*

Main category: cs.AI

TL;DR: 提出了MedMKEB，首个全面的多模态医学知识编辑基准，用于评估医疗多模态大语言模型的知识编辑能力，并揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）显著提升了医学AI，但医学知识不断演进，需要高效更新模型知识而不需从头训练。现有文本知识编辑研究广泛，但缺乏针对图像和文本模态的系统性多模态医学知识编辑基准。

Method: 构建了MedMKEB基准，基于高质量医学视觉问答数据集，并设计了反事实修正、语义泛化、知识迁移和对抗鲁棒性等编辑任务。通过人类专家验证确保基准的准确性和可靠性。在最先进的通用和医学MLLMs上进行了单次编辑和序列编辑实验。

Result: 广泛的实验表明，现有基于知识的编辑方法在医学领域存在局限性，凸显了开发专门编辑策略的必要性。

Conclusion: MedMKEB将作为标准基准，促进可信赖和高效的医学知识编辑算法的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly improved medical AI, enabling it to unify the understanding of
visual and textual information. However, as medical knowledge continues to
evolve, it is critical to allow these models to efficiently update outdated or
incorrect information without retraining from scratch. Although textual
knowledge editing has been widely studied, there is still a lack of systematic
benchmarks for multimodal medical knowledge editing involving image and text
modalities. To fill this gap, we present MedMKEB, the first comprehensive
benchmark designed to evaluate the reliability, generality, locality,
portability, and robustness of knowledge editing in medical multimodal large
language models. MedMKEB is built on a high-quality medical visual
question-answering dataset and enriched with carefully constructed editing
tasks, including counterfactual correction, semantic generalization, knowledge
transfer, and adversarial robustness. We incorporate human expert validation to
ensure the accuracy and reliability of the benchmark. Extensive single editing
and sequential editing experiments on state-of-the-art general and medical
MLLMs demonstrate the limitations of existing knowledge-based editing
approaches in medicine, highlighting the need to develop specialized editing
strategies. MedMKEB will serve as a standard benchmark to promote the
development of trustworthy and efficient medical knowledge editing algorithms.

</details>


### [11] [EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search](https://arxiv.org/abs/2508.05113)
*Xinyue Wu,Fan Hu,Shaik Jani Babu,Yi Zhao,Xinfei Guo*

Main category: cs.AI

TL;DR: EasySize是一个基于微调Qwen3-8B的轻量级模拟电路门尺寸设计框架，它能跨工艺节点、设计规范和电路拓扑通用，并显著减少对人工经验和计算资源的依赖，表现优于现有强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计耗时且依赖经验，现有AI方法在通用性、速度、稳定性方面存在挑战，且通常需要大型模型并缺乏跨技术节点的便携性。

Method: 提出了EasySize框架，基于微调的Qwen3-8B模型。它利用性能指标的“可实现难易度”（EOA）动态构建任务特定的损失函数，并通过结合全局差分进化（DE）和局部粒子群优化（PSO）的反馈增强流程进行高效启发式搜索。

Result: 尽管仅在350nm节点数据上进行微调，EasySize在180nm、45nm和22nm技术节点上的5个运算放大器（Op-Amp）网表中表现出色，无需额外针对性训练。与基于强化学习的AutoCkt相比，EasySize在86.67%的任务上表现更优，并减少了96.67%以上的仿真资源。

Conclusion: EasySize能显著降低模拟电路门尺寸设计对人工经验和计算资源的依赖，从而加速和简化模拟电路设计过程。

Abstract: Analog circuit design is a time-consuming, experience-driven task in chip
development. Despite advances in AI, developing universal, fast, and stable
gate sizing methods for analog circuits remains a significant challenge. Recent
approaches combine Large Language Models (LLMs) with heuristic search
techniques to enhance generalizability, but they often depend on large model
sizes and lack portability across different technology nodes. To overcome these
limitations, we propose EasySize, the first lightweight gate sizing framework
based on a finetuned Qwen3-8B model, designed for universal applicability
across process nodes, design specifications, and circuit topologies. EasySize
exploits the varying Ease of Attainability (EOA) of performance metrics to
dynamically construct task-specific loss functions, enabling efficient
heuristic search through global Differential Evolution (DE) and local Particle
Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned
solely on 350nm node data, EasySize achieves strong performance on 5
operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology
nodes without additional targeted training, and outperforms AutoCkt, a
widely-used Reinforcement Learning based sizing framework, on 86.67\% of tasks
with more than 96.67\% of simulation resources reduction. We argue that
EasySize can significantly reduce the reliance on human expertise and
computational resources in gate sizing, thereby accelerating and simplifying
the analog circuit design process. EasySize will be open-sourced at a later
date.

</details>


### [12] [Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures](https://arxiv.org/abs/2508.05116)
*Peer-Benedikt Degen,Igor Asanov*

Main category: cs.AI

TL;DR: 本研究评估了苏格拉底式AI导师在学生研究问题发展中的作用，发现其能促进批判性思维和元认知参与，并提出了由专业AI代理组成的“编排式多代理系统”概念，以实现混合学习生态系统中的人机共创和教学对齐。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在迅速成为高等教育的基础设施，重塑知识的生成、中介和验证方式。本研究旨在探索如何利用AI促进而非削弱学生的批判性思维能力，并挑战关于生成式AI导致技能退化的现有叙述。

Method: 本研究进行了一项对照实验，涉及德国的65名职前教师学生。实验比较了学生与基于建构主义理论设计的苏格拉底式AI导师（旨在通过结构化对话辅助研究问题发展）的互动，与学生与非指令性AI聊天机器人的互动。

Result: 使用苏格拉底式AI导师的学生报告称，他们获得了显著更高的批判性、独立性和反思性思维支持，表明对话式AI能够激发元认知参与。研究还提出了“编排式多代理系统”（MAS）的概念，即由教育者策划、具有差异化角色和协调交互的模块化、教学对齐的AI代理集合。

Conclusion: 本研究为多代理系统在教育中的应用提供了概念验证，并提出了一个适应性的“提供与使用”模型。研究强调了这种系统在高等教育机构和学生层面的系统性影响（包括资金、教师角色、课程、能力和评估实践），并通过成本效益分析突出了其可扩展性。最终，本研究为嵌入人机共创和教学对齐的混合学习生态系统提供了实证证据和概念路线图。

Abstract: Generative AI is no longer a peripheral tool in higher education. It is
rapidly evolving into a general-purpose infrastructure that reshapes how
knowledge is generated, mediated, and validated. This paper presents findings
from a controlled experiment evaluating a Socratic AI Tutor, a large language
model designed to scaffold student research question development through
structured dialogue grounded in constructivist theory. Conducted with 65
pre-service teacher students in Germany, the study compares interaction with
the Socratic Tutor to engagement with an uninstructed AI chatbot. Students
using the Socratic Tutor reported significantly greater support for critical,
independent, and reflective thinking, suggesting that dialogic AI can stimulate
metacognitive engagement and challenging recent narratives of de-skilling due
to generative AI usage. These findings serve as a proof of concept for a
broader pedagogical shift: the use of multi-agent systems (MAS) composed of
specialised AI agents. To conceptualise this, we introduce the notion of
orchestrated MAS, modular, pedagogically aligned agent constellations, curated
by educators, that support diverse learning trajectories through differentiated
roles and coordinated interaction. To anchor this shift, we propose an adapted
offer-and-use model, in which students appropriate instructional offers from
these agents. Beyond technical feasibility, we examine system-level
implications for higher education institutions and students, including funding
necessities, changes to faculty roles, curriculars, competencies and assessment
practices. We conclude with a comparative cost-effectiveness analysis
highlighting the scalability of such systems. In sum, this study contributes
both empirical evidence and a conceptual roadmap for hybrid learning ecosystems
that embed human-AI co-agency and pedagogical alignment.

</details>


### [13] [Graph-based Event Log Repair](https://arxiv.org/abs/2508.05145)
*Sebastiano Dissegna,Chiara Di Francescomarino,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 该研究提出了一种基于异构图神经网络（HGNN）的模型，用于在流程挖掘中重建事件日志中缺失的事件属性，能够有效恢复所有类型的缺失数据。


<details>
  <summary>Details</summary>
Motivation: 流程挖掘中事件日志的质量至关重要，但实际日志常因数据采集问题（如人工活动记录不全、属性收集不完整）而存在缺失信息。现有轨迹重建方法要么依赖于流程模型，要么使用机器学习/深度学习模型从相似案例中学习，但它们可能无法处理所有缺失属性或需要特定模型。

Method: 开发了一个异构图神经网络（HGNN）模型。该模型将流程挖掘中的执行轨迹表示为图结构，以更自然、更具表达力的方式编码复杂的多模态序列，从而推断并返回事件中缺失的完整属性集。

Result: 该方法在两种合成日志和四种真实事件日志上，针对不同类型的缺失值进行了评估。结果表明，与利用自编码器的现有最先进方法相比，所提出的HGNN方法在重建所有不同事件属性方面表现出非常好的性能，优于主要关注修复事件属性子集的现有无模型方法。

Conclusion: 异构图神经网络提供了一种强大且有效的解决方案，可以全面重建流程挖掘事件日志中缺失的事件属性，克服了现有无模型方法在处理所有缺失属性方面的局限性，从而显著提高了事件日志的质量。

Abstract: The quality of event logs in Process Mining is crucial when applying any form
of analysis to them. In real-world event logs, the acquisition of data can be
non-trivial (e.g., due to the execution of manual activities and related manual
recording or to issues in collecting, for each event, all its attributes), and
often may end up with events recorded with some missing information. Standard
approaches to the problem of trace (or log) reconstruction either require the
availability of a process model that is used to fill missing values by
leveraging different reasoning techniques or employ a Machine Learning/Deep
Learning model to restore the missing values by learning from similar cases. In
recent years, a new type of Deep Learning model that is capable of handling
input data encoded as graphs has emerged, namely Graph Neural Networks. Graph
Neural Network models, and even more so Heterogeneous Graph Neural Networks,
offer the advantage of working with a more natural representation of complex
multi-modal sequences like the execution traces in Process Mining, allowing for
more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural
Network model that, given a trace containing some incomplete events, will
return the full set of attributes missing from those events. We evaluate our
work against a state-of-the-art approach leveraging autoencoders on two
synthetic logs and four real event logs, on different types of missing values.
Different from state-of-the-art model-free approaches, which mainly focus on
repairing a subset of event attributes, the proposed approach shows very good
performance in reconstructing all different event attributes.

</details>


### [14] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: QA-Dragon是一个用于知识密集型视觉问答（VQA）的查询感知动态RAG系统，通过引入领域和搜索路由器，实现文本和图像的混合检索，从而支持多模态、多轮和多跳推理，显著提升了复杂VQA任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）方法在多模态大语言模型（MLLMs）中，通常仅从文本或图像中单独检索，这限制了它们处理需要多跳推理或最新事实知识的复杂查询的能力。

Method: 本文提出了QA-Dragon系统，一个查询感知动态RAG系统。它包含一个领域路由器，用于识别查询的主题领域以进行领域特定推理；以及一个搜索路由器，用于动态选择最优检索策略。通过混合设置中协调文本和图像搜索代理，该系统支持多模态、多轮和多跳推理。

Result: QA-Dragon在KDD Cup 2025的Meta CRAG-MM挑战赛中进行了评估，显著增强了基础模型在挑战性场景下的推理性能。在答案准确性和知识重叠得分方面均取得了显著提升，相比基线模型，在单源任务上超越5.06%，多源任务上超越6.35%，多轮任务上超越5.03%。

Conclusion: QA-Dragon通过其动态、多模态的检索机制，有效解决了现有RAG方法在处理复杂VQA任务时的局限性，显著提升了模型的推理能力和性能。

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [15] [An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication](https://arxiv.org/abs/2508.05267)
*Vítor N. Lourenço,Mohnish Dubey,Yunfei Bai,Audrey Depeige,Vivek Jain*

Main category: cs.AI

TL;DR: 该论文提出了一个结合RDF图数据库和大型语言模型（LLMs）的框架，旨在解决大型维护组织中专家识别和复杂实体间沟通效率低下的问题，实现精准且可解释的受众定位和信息传递。


<details>
  <summary>Details</summary>
Motivation: 大型维护组织在识别领域专家和管理复杂实体关系（如设备、制造商、工程师、设施等）间的沟通时面临重大挑战，包括信息过载和响应时间过长，而传统沟通方法无法有效解决这些问题。

Method: 提出了一种新颖的框架，该框架结合了RDF图数据库和大型语言模型（LLMs），通过规划-编排架构来处理自然语言查询，以实现精准的受众定位，并提供透明的推理过程。

Result: 该解决方案使得沟通负责人能够通过结合设备、制造商、维护工程师和设施等概念的直观查询来获取信息，并提供可解释的结果，从而在提高组织内部沟通效率的同时，也维护了系统信任。

Conclusion: 通过结合RDF图数据库和LLMs，该框架能有效应对大型维护组织中的沟通挑战，实现对专家识别和信息传递的精准化、高效化和可解释化。

Abstract: In large-scale maintenance organizations, identifying subject matter experts
and managing communications across complex entities relationships poses
significant challenges -- including information overload and longer response
times -- that traditional communication approaches fail to address effectively.
We propose a novel framework that combines RDF graph databases with LLMs to
process natural language queries for precise audience targeting, while
providing transparent reasoning through a planning-orchestration architecture.
Our solution enables communication owners to formulate intuitive queries
combining concepts such as equipment, manufacturers, maintenance engineers, and
facilities, delivering explainable results that maintain trust in the system
while improving communication efficiency across the organization.

</details>


### [16] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 该论文提出了一种混合架构，将基于决策树的符号推理与大型语言模型（LLM）的生成能力整合到一个协调的多智能体框架中，以实现强大的神经符号推理。


<details>
  <summary>Details</summary>
Motivation: 先前的研究在符号模块和神经模块之间存在松散耦合的问题，本研究旨在设计一个更紧密集成、可解释且统一的推理系统，能够处理结构化和非结构化输入。

Method: 该方法构建了一个混合架构，将决策树和随机森林作为可调用预言机嵌入到统一的推理系统中。LLM智能体负责溯因推理、泛化和交互式规划。一个中央协调器维护信念状态一致性并协调智能体与外部工具之间的通信。

Result: 该系统在推理基准测试中表现出色：在ProofWriter上，通过逻辑验证将蕴含一致性提高了+7.2%；在GSM8k上，通过符号增强在多步数学问题中实现了+5.3%的准确率提升；在ARC上，通过集成符号预言机将抽象准确率提高了+6.0%。此外，在临床决策支持和科学发现应用中也展现了其能力。

Conclusion: 该架构为通用神经符号推理提供了一个鲁棒、可解释和可扩展的解决方案。

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [17] [The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition](https://arxiv.org/abs/2508.05338)
*Brinnae Bent*

Main category: cs.AI

TL;DR: AI中“agent”一词含义模糊，尤其在大语言模型兴起后，导致沟通、评估和政策制定困难。本文提出一个重新定义“agent”的框架，明确其最低要求，并从多维度描述系统，以提高研究清晰度和可复现性。


<details>
  <summary>Details</summary>
Motivation: AI领域“agent”一词的多种解释造成混淆，近期大语言模型的发展加剧了这种模糊性，给研究交流、系统评估、可复现性以及政策制定带来了显著挑战。

Method: 通过历史分析和当代使用模式，本文提出了一个框架，该框架定义了系统被视为“agent”的明确最低要求，并从环境交互、学习与适应、自主性、目标复杂性和时间连贯性等多个维度对系统进行描述。

Result: 提出的框架提供了一种精确的词汇来描述系统，同时保留了该术语在历史上的多面性。它为改进研究清晰度和可复现性提供了实用工具，并支持更有效的政策制定。

Conclusion: “agent”一词需要重新定义，通过采纳本文提出的多维度框架和标准化术语，可以提高AI研究的清晰度、可复现性，并促进更有效的政策制定。

Abstract: The term 'agent' in artificial intelligence has long carried multiple
interpretations across different subfields. Recent developments in AI
capabilities, particularly in large language model systems, have amplified this
ambiguity, creating significant challenges in research communication, system
evaluation and reproducibility, and policy development. This paper argues that
the term 'agent' requires redefinition. Drawing from historical analysis and
contemporary usage patterns, we propose a framework that defines clear minimum
requirements for a system to be considered an agent while characterizing
systems along a multidimensional spectrum of environmental interaction,
learning and adaptation, autonomy, goal complexity, and temporal coherence.
This approach provides precise vocabulary for system description while
preserving the term's historically multifaceted nature. After examining
potential counterarguments and implementation challenges, we provide specific
recommendations for moving forward as a field, including suggestions for
terminology standardization and framework adoption. The proposed approach
offers practical tools for improving research clarity and reproducibility while
supporting more effective policy development.

</details>


### [18] [NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making](https://arxiv.org/abs/2508.05344)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.AI

TL;DR: 该研究引入了NomicLaw多智能体模拟平台，让大型语言模型（LLMs）在法律情境中进行协作立法，展示了LLMs的社会推理和说服能力，包括形成联盟和背叛信任。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在法律解释、论证等复杂推理任务上取得了进展，但其在开放式、多智能体、涉及法律和道德困境的审议环境中的行为理解仍然有限。

Method: 引入NomicLaw，一个结构化的多智能体模拟平台，LLMs在此对复杂的法律情景提出规则、进行辩护并对同伴提案进行投票。通过投票模式量化信任和互惠，并通过分析战略性语言定性评估代理人如何影响结果。实验涉及同质和异质的LLM群体。

Result: 实验表明，LLM代理人自发形成联盟、背叛信任并调整其修辞以影响集体决策。结果突出了十个开源LLM潜在的社会推理和说服能力。

Conclusion: 研究结果为未来能够自主协商、协调和起草法律的人工智能系统的设计提供了见解。

Abstract: Recent advancements in large language models (LLMs) have extended their
capabilities from basic text processing to complex reasoning tasks, including
legal interpretation, argumentation, and strategic interaction. However,
empirical understanding of LLM behavior in open-ended, multi-agent settings
especially those involving deliberation over legal and ethical dilemmas remains
limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs
engage in collaborative law-making, responding to complex legal vignettes by
proposing rules, justifying them, and voting on peer proposals. We
quantitatively measure trust and reciprocity via voting patterns and
qualitatively assess how agents use strategic language to justify proposals and
influence outcomes. Experiments involving homogeneous and heterogeneous LLM
groups demonstrate how agents spontaneously form alliances, betray trust, and
adapt their rhetoric to shape collective decisions. Our results highlight the
latent social reasoning and persuasive capabilities of ten open-source LLMs and
provide insights into the design of future AI systems capable of autonomous
negotiation, coordination and drafting legislation in legal settings.

</details>


### [19] [Minimal Model Reasoning in Description Logics: Don't Try This at Home!](https://arxiv.org/abs/2508.05350)
*Federica Di Stefano,Quentin Manière,Magdalena Ortiz,Mantas Šimkus*

Main category: cs.AI

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Reasoning with minimal models has always been at the core of many knowledge
representation techniques, but we still have only a limited understanding of
this problem in Description Logics (DLs). Minimization of some selected
predicates, letting the remaining predicates vary or be fixed, as proposed in
circumscription, has been explored and exhibits high complexity. The case of
`pure' minimal models, where the extension of all predicates must be minimal,
has remained largely uncharted. We address this problem in popular DLs and
obtain surprisingly negative results: concept satisfiability in minimal models
is undecidable already for $\mathcal{EL}$. This undecidability also extends to
a very restricted fragment of tuple-generating dependencies. To regain
decidability, we impose acyclicity conditions on the TBox that bring the
worst-case complexity below double exponential time and allow us to establish a
connection with the recently studied pointwise circumscription; we also derive
results in data complexity. We conclude with a brief excursion to the DL-Lite
family, where a positive result was known for DL-Lite$_{\text{core}}$, but our
investigation establishes ExpSpace-hardness already for its extension
DL-Lite$_{\text{horn}}$.

</details>


### [20] [StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models](https://arxiv.org/abs/2508.05383)
*Xiangxiang Zhang,Jingxuan Wei,Donghong Zhong,Qi Chen,Caijun Jia,Cheng Tan,Jinming Gu,Xiaobo Qin,Zhiping Liu,Liang Hu,Tong Sun,Yuchen Wu,Zewei Sun,Chenwei Lou,Hua Zheng,Tianyang Zhan,Changbao Wang,Shuangzhi Wu,Zefa Lin,Chang Guo,Sihang Yuan,Riwei Chen,Shixiong Zhao,Yingping Zhang,Gaowei Wu,Bihui Yu,Jiahui Wu,Zhehui Zhao,Qianqian Liu,Ruofeng Tang,Xingyue Huang,Bing Zhao,Mengyang Zhang,Youqiang Zhou*

Main category: cs.AI

TL;DR: StructVRM引入结构化可验证奖励模型，通过细粒度反馈提升视觉-语言模型在复杂多问题推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在处理复杂、多问题推理任务时表现不佳，因为传统的二元奖励机制过于粗糙，无法提供有效的细粒度指导，特别是在需要部分正确性判断的场景。

Method: 引入StructVRM方法，其核心是一个基于模型的验证器，该验证器经过训练，能够提供子问题级别的细粒度反馈。它评估语义和数学等价性，而非依赖严格的字符串匹配，从而在以前难以处理的问题格式中实现细致的部分得分。

Result: StructVRM训练出的模型Seed-StructVRM在十二个公共多模态基准中的六个以及新构建的高难度STEM-Bench上实现了最先进的性能。

Conclusion: 通过结构化、可验证的奖励进行训练，是提高多模态模型在复杂真实世界推理领域能力的有效方法。

Abstract: Existing Vision-Language Models often struggle with complex, multi-question
reasoning tasks where partial correctness is crucial for effective learning.
Traditional reward mechanisms, which provide a single binary score for an
entire response, are too coarse to guide models through intricate problems with
multiple sub-parts. To address this, we introduce StructVRM, a method that
aligns multimodal reasoning with Structured and Verifiable Reward Models. At
its core is a model-based verifier trained to provide fine-grained,
sub-question-level feedback, assessing semantic and mathematical equivalence
rather than relying on rigid string matching. This allows for nuanced, partial
credit scoring in previously intractable problem formats. Extensive experiments
demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM,
achieves state-of-the-art performance on six out of twelve public multimodal
benchmarks and our newly curated, high-difficulty STEM-Bench. The success of
StructVRM validates that training with structured, verifiable rewards is a
highly effective approach for advancing the capabilities of multimodal models
in complex, real-world reasoning domains.

</details>


### [21] [An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal](https://arxiv.org/abs/2508.05388)
*Silvia García-Méndez,Francisco de Arriba-Pérez,Fátima Leal,Bruno Veloso,Benedita Malheiro,Juan Carlos Burguillo-Rial*

Main category: cs.AI

TL;DR: 本文提出了一种用于智能交通系统（ITS）的实时数据驱动预测性维护解决方案，该方案包含样本预处理、增量机器学习分类和结果解释模块，首次实现了在线故障预测并提供自然语言和视觉解释，在铁路数据集上取得了高精度和F-measure，并能有效解释决策过程。


<details>
  <summary>Details</summary>
Motivation: 在铁路预测性维护中，高精度的故障预测至关重要，因为这直接影响服务可用性、成本降低和安全性。现有的解决方案缺乏实时在线处理能力和有效的决策解释。

Method: 该方法实现了一个在线处理流程，包括：1) 专用的样本预处理模块，实时构建统计和频率相关特征；2) 采用机器学习模型进行增量分类；3) 包含自然语言和视觉解释的解释模块。这是首次在线执行故障预测并提供可解释性。

Result: 在葡萄牙波尔图地铁运营商的MetroPT数据集上进行实验，F-measure超过98%，准确率超过99%。该流程即使在类别不平衡和噪声存在的情况下也能保持高性能，并且其解释能有效反映决策过程。高F-measure确保了高故障检测率和低误报率，高准确率则提升了可靠性、降低了成本并提高了安全性。

Conclusion: 该方法在方法论上是可靠的，并具有实际应用价值，可支持铁路运营中的主动维护决策。通过识别故障的早期迹象，该流程能帮助决策者理解潜在问题并迅速采取行动，从而最大限度地提高服务可用性并增强安全性。

Abstract: This work contributes to a real-time data-driven predictive maintenance
solution for Intelligent Transportation Systems. The proposed method implements
a processing pipeline comprised of sample pre-processing, incremental
classification with Machine Learning models, and outcome explanation. This
novel online processing pipeline has two main highlights: (i) a dedicated
sample pre-processing module, which builds statistical and frequency-related
features on the fly, and (ii) an explainability module. This work is the first
to perform online fault prediction with natural language and visual
explainability. The experiments were performed with the MetroPT data set from
the metro operator of Porto, Portugal. The results are above 98 % for F-measure
and 99 % for accuracy. In the context of railway predictive maintenance,
achieving these high values is crucial due to the practical and operational
implications of accurate failure prediction. In the specific case of a high
F-measure, this ensures that the system maintains an optimal balance between
detecting the highest possible number of real faults and minimizing false
alarms, which is crucial for maximizing service availability. Furthermore, the
accuracy obtained enables reliability, directly impacting cost reduction and
increased safety. The analysis demonstrates that the pipeline maintains high
performance even in the presence of class imbalance and noise, and its
explanations effectively reflect the decision-making process. These findings
validate the methodological soundness of the approach and confirm its practical
applicability for supporting proactive maintenance decisions in real-world
railway operations. Therefore, by identifying the early signs of failure, this
pipeline enables decision-makers to understand the underlying problems and act
accordingly swiftly.

</details>


### [22] [DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning](https://arxiv.org/abs/2508.05405)
*Xinrun Xu,Pi Bu,Ye Wang,Börje F. Karlsson,Ziming Wang,Tengtao Song,Qi Zhu,Jun Song,Zhiming Ding,Bo Zheng*

Main category: cs.AI

TL;DR: DeepPHY是一个新的基准框架，用于系统评估视觉语言模型（VLMs）对基本物理原理的理解和推理能力，发现当前SOTA VLM难以将物理知识转化为精确的预测控制。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs具有强大的感知和视觉推理能力，但在复杂动态环境中，它们在细节关注和精确行动规划方面表现不佳。现实世界任务需要复杂的交互、高级空间推理、长期规划和策略优化，通常需要理解物理规则，而真实场景评估成本过高。

Method: 引入了DeepPHY，一个新颖的基准框架，通过一系列具有挑战性的模拟环境来系统评估VLMs的物理理解和推理能力。DeepPHY集成了不同难度级别的物理推理环境，并包含了细粒度的评估指标。

Result: 评估发现，即使是当前最先进的VLMs也难以将描述性的物理知识转化为精确的、预测性的控制。

Conclusion: VLMs在将物理知识应用于精确预测控制方面存在显著不足，需要进一步研究来弥补这一差距。

Abstract: Although Vision Language Models (VLMs) exhibit strong perceptual abilities
and impressive visual reasoning, they struggle with attention to detail and
precise action planning in complex, dynamic environments, leading to subpar
performance. Real-world tasks typically require complex interactions, advanced
spatial reasoning, long-term planning, and continuous strategy refinement,
usually necessitating understanding the physics rules of the target scenario.
However, evaluating these capabilities in real-world scenarios is often
prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel
benchmark framework designed to systematically evaluate VLMs' understanding and
reasoning about fundamental physical principles through a series of challenging
simulated environments. DeepPHY integrates multiple physical reasoning
environments of varying difficulty levels and incorporates fine-grained
evaluation metrics. Our evaluation finds that even state-of-the-art VLMs
struggle to translate descriptive physical knowledge into precise, predictive
control.

</details>


### [23] [Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation](https://arxiv.org/abs/2508.05427)
*Kartar Kumar Lohana Tharwani,Rajesh Kumar,Sumita,Numan Ahmed,Yong Tang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）正在革新有机合成，通过结合图神经网络、量子计算和实时光谱等技术，它们已成为实用的实验室伙伴，加速发现周期并支持绿色化学。文章讨论了其局限性并展望了社区驱动的未来发展。


<details>
  <summary>Details</summary>
Motivation: LLMs在有机合成中展现出巨大潜力，能够规划合成路线、预测反应结果甚至指导机器人实验，这促使研究者探索如何将LLMs从推测性工具转变为实用的实验室伙伴，以重塑化学研究范式。

Method: 文章综述了LLMs与多种先进技术结合的应用，包括将其与图神经网络（GNNs）、量子计算（quantum calculations）和实时光谱（real-time spectroscopy）相结合，以提高其在化学发现中的效能。

Result: LLMs与相关技术的结合显著缩短了发现周期，支持了更环保和数据驱动的化学研究。它们能够提出合成路线、预测反应结果，甚至无需人工监督地指导机器人执行实验。

Conclusion: LLMs在化学领域的进步预示着快速、可靠和包容的分子创新未来，由人工智能和自动化驱动。然而，仍需解决数据偏见、推理不透明和安全风险等局限性，并需要通过开放基准、联邦学习和可解释接口等社区倡议来促进普及和确保人类控制。

Abstract: Large language models (LLMs) are beginning to reshape how chemists plan and
run reactions in organic synthesis. Trained on millions of reported
transformations, these text-based models can propose synthetic routes, forecast
reaction outcomes and even instruct robots that execute experiments without
human supervision. Here we survey the milestones that turned LLMs from
speculative tools into practical lab partners. We show how coupling LLMs with
graph neural networks, quantum calculations and real-time spectroscopy shrinks
discovery cycles and supports greener, data-driven chemistry. We discuss
limitations, including biased datasets, opaque reasoning and the need for
safety gates that prevent unintentional hazards. Finally, we outline community
initiatives open benchmarks, federated learning and explainable interfaces that
aim to democratize access while keeping humans firmly in control. These
advances chart a path towards rapid, reliable and inclusive molecular
innovation powered by artificial intelligence and automation.

</details>


### [24] [Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI](https://arxiv.org/abs/2508.05432)
*Krzysztof Janowicz,Zilong Liu,Gengchen Mai,Zhangyu Wang,Ivan Majic,Alexandra Fortacz,Grant McKenzie,Song Gao*

Main category: cs.AI

TL;DR: 本文探讨了AI对齐的地理差异性，强调了文化、政治和法律因素如何影响AI的“适当性”，并呼吁开发具有时空感知能力的对齐方法，而非“一刀切”的方案。


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐研究虽关注偏见，但对地理变异性探索不足。不同地区对“适当”、“真实”和“合法”的定义差异巨大，而AI正以空前规模自动化地传播知识和观点，却缺乏对地理背景管理的透明度。随着具身AI的临近，迫切需要解决AI输出与统计现实不符（如性别比例）以及地理敏感内容（如克什米尔问题）的处理问题。

Method: 本文综述了关键的地理研究问题，提出了未来的研究方向，并概述了评估对齐敏感性的方法。

Result: 指出AI对齐存在显著的地理差异性，当前的“一刀切”方法无法适应不同地区的文化规范、政治现实和法律。强调了AI在呈现地理现实时缺乏透明度的问题，并认为随着具身AI的发展，对时空感知的对齐需求日益紧迫。

Conclusion: 为确保AI系统符合社会规范和目标，必须开发具有时空感知能力的对齐方法，以取代当前普遍存在的“一刀切”方案，从而有效处理AI输出在全球范围内的地理敏感性和文化差异性问题。

Abstract: AI (super) alignment describes the challenge of ensuring (future) AI systems
behave in accordance with societal norms and goals. While a quickly evolving
literature is addressing biases and inequalities, the geographic variability of
alignment remains underexplored. Simply put, what is considered appropriate,
truthful, or legal can differ widely across regions due to cultural norms,
political realities, and legislation. Alignment measures applied to AI/ML
workflows can sometimes produce outcomes that diverge from statistical
realities, such as text-to-image models depicting balanced gender ratios in
company leadership despite existing imbalances. Crucially, some model outputs
are globally acceptable, while others, e.g., questions about Kashmir, depend on
knowing the user's location and their context. This geographic sensitivity is
not new. For instance, Google Maps renders Kashmir's borders differently based
on user location. What is new is the unprecedented scale and automation with
which AI now mediates knowledge, expresses opinions, and represents geographic
reality to millions of users worldwide, often with little transparency about
how context is managed. As we approach Agentic AI, the need for
spatio-temporally aware alignment, rather than one-size-fits-all approaches, is
increasingly urgent. This paper reviews key geographic research problems,
suggests topics for future work, and outlines methods for assessing alignment
sensitivity.

</details>


### [25] [Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?](https://arxiv.org/abs/2508.05464)
*Matteo Prandi,Vincenzo Suriani,Federico Pierucci,Marcello Galisai,Daniele Nardi,Piercosma Bisconti*

Main category: cs.AI

TL;DR: 研究发现，现有AI基准测试与欧盟AI法案等新法规关注的系统性风险之间存在巨大评估差距，尤其是在失控场景等关键能力方面几乎没有覆盖。


<details>
  <summary>Details</summary>
Motivation: 随着通用AI（GPAI）模型的快速发展和欧盟AI法案等新法规的出台，急需健壮的评估框架。然而，现有AI评估工具（基准测试）并非为衡量新监管关注的系统性风险而设计，因此需要量化这种“基准-法规”差距。

Method: 研究引入了名为Bench-2-CoP的新型系统框架，该框架利用经过验证的“LLM作为评判者”分析方法，将来自广泛使用的基准测试中的194,955个问题，映射到欧盟AI法案的模型能力和倾向分类法上，以评估其覆盖范围。

Result: 研究发现评估生态系统存在严重错位：绝大多数评估集中在狭窄的行为倾向（如“幻觉倾向”占53.7%，“歧视性偏见”占28.9%），而关键的功能能力却被危险地忽视。特别是与失控场景相关的能力，如规避人类监督、自我复制和自主AI开发，在整个基准语料库中覆盖率为零。这导致“失控”（0.4%覆盖率）和“网络攻击”（0.8%覆盖率）等系统性风险几乎完全没有评估。

Conclusion: 本研究首次对这一评估差距进行了全面、定量的分析，为政策制定者完善《行为准则》以及开发者构建下一代评估工具提供了关键见解，最终旨在促进更安全、更合规的AI发展。

Abstract: The rapid advancement of General Purpose AI (GPAI) models necessitates robust
evaluation frameworks, especially with emerging regulations like the EU AI Act
and its associated Code of Practice (CoP). Current AI evaluation practices
depend heavily on established benchmarks, but these tools were not designed to
measure the systemic risks that are the focus of the new regulatory landscape.
This research addresses the urgent need to quantify this "benchmark-regulation
gap." We introduce Bench-2-CoP, a novel, systematic framework that uses
validated LLM-as-judge analysis to map the coverage of 194,955 questions from
widely-used benchmarks against the EU AI Act's taxonomy of model capabilities
and propensities. Our findings reveal a profound misalignment: the evaluation
ecosystem is overwhelmingly focused on a narrow set of behavioral propensities,
such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory
bias" (28.9%), while critical functional capabilities are dangerously
neglected. Crucially, capabilities central to loss-of-control scenarios,
including evading human oversight, self-replication, and autonomous AI
development, receive zero coverage in the entire benchmark corpus. This
translates to a near-total evaluation gap for systemic risks like "Loss of
Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study
provides the first comprehensive, quantitative analysis of this gap, offering
critical insights for policymakers to refine the CoP and for developers to
build the next generation of evaluation tools, ultimately fostering safer and
more compliant AI.

</details>


### [26] [Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?](https://arxiv.org/abs/2508.05474)
*Burak Can Kaplan,Hugo Cesar De Castro Carneiro,Stefan Wermter*

Main category: cs.AI

TL;DR: 对话情感识别(ERC)数据稀缺且存在偏差。本文利用小型LLM生成多样化的ERC数据集，以补充现有基准，实验证明这些生成数据能显著提升ERC分类模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别(ERC)数据稀缺，现有数据集存在来源高度偏颇和软标签固有的主观性问题。尽管大型语言模型(LLM)在情感任务中表现出色，但训练成本高昂，且在ERC任务（特别是数据生成）中的应用受限。

Method: 采用小型、资源高效、通用型LLM来合成具有多样化属性的ERC数据集，以补充三个最广泛使用的ERC基准。共生成六个新数据集，每个基准对应两个。评估这些数据集在补充现有数据集进行ERC分类以及分析ERC中标签不平衡效应方面的效用。

Result: 在生成数据集上训练的ERC分类器模型表现出强大的鲁棒性，并在现有ERC基准上持续实现统计学上显著的性能提升。

Conclusion: 生成的ERC数据集能够有效补充现有数据，提高ERC分类模型的性能和鲁棒性，并有助于分析标签不平衡对模型表现的影响。

Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion
shifts within interactions, representing a significant step toward advancing
machine intelligence. However, ERC data remains scarce, and existing datasets
face numerous challenges due to their highly biased sources and the inherent
subjectivity of soft labels. Even though Large Language Models (LLMs) have
demonstrated their quality in many affective tasks, they are typically
expensive to train, and their application to ERC tasks--particularly in data
generation--remains limited. To address these challenges, we employ a small,
resource-efficient, and general-purpose LLM to synthesize ERC datasets with
diverse properties, supplementing the three most widely used ERC benchmarks. We
generate six novel datasets, with two tailored to enhance each benchmark. We
evaluate the utility of these datasets to (1) supplement existing datasets for
ERC classification, and (2) analyze the effects of label imbalance in ERC. Our
experimental results indicate that ERC classifier models trained on the
generated datasets exhibit strong robustness and consistently achieve
statistically significant performance improvements on existing ERC benchmarks.

</details>


### [27] [InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities](https://arxiv.org/abs/2508.05496)
*Shuo Cai,Su Lu,Qi Zhou,Kejing Yang,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAlign是一个可扩展且数据高效的后训练框架，通过结合SFT和DPO以及自动数据选择，显著提升大型语言模型的推理能力，同时大幅减少数据和计算需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽展现出强大的推理能力，但其后训练（post-training）过程资源密集，尤其是在数据和计算成本方面。现有数据选择方法常依赖启发式或任务特定策略，缺乏可扩展性，限制了样本效率的提升。

Method: 本文提出了InfiAlign框架，它将监督微调（SFT）与直接偏好优化（DPO）相结合，以对齐LLM以增强推理能力。其核心是一个鲁棒的数据选择流程，该流程利用多维度质量指标从开源推理数据集中自动筛选高质量的对齐数据。

Result: 将InfiAlign应用于Qwen2.5-Math-7B-Base模型时，其SFT模型仅使用约12%的训练数据，便达到了与DeepSeek-R1-Distill-Qwen-7B相当的性能，并在多样推理任务上展现出强大的泛化能力。通过应用DPO，模型性能进一步提升，尤其在数学推理任务上表现显著，在AIME 24/25基准测试中平均提升3.89%。

Conclusion: 研究结果强调了将原则性数据选择与全阶段后训练相结合的有效性，为以可扩展和数据高效的方式对齐大型推理模型提供了一个实用的解决方案。

Abstract: Large language models (LLMs) have exhibited impressive reasoning abilities on
a wide range of complex tasks. However, enhancing these capabilities through
post-training remains resource intensive, particularly in terms of data and
computational cost. Although recent efforts have sought to improve sample
efficiency through selective data curation, existing methods often rely on
heuristic or task-specific strategies that hinder scalability. In this work, we
introduce InfiAlign, a scalable and sample-efficient post-training framework
that integrates supervised fine-tuning (SFT) with Direct Preference
Optimization (DPO) to align LLMs for enhanced reasoning. At the core of
InfiAlign is a robust data selection pipeline that automatically curates
high-quality alignment data from open-source reasoning datasets using
multidimensional quality metrics. This pipeline enables significant performance
gains while drastically reducing data requirements and remains extensible to
new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model
achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only
approximately 12% of the training data, and demonstrates strong generalization
across diverse reasoning tasks. Additional improvements are obtained through
the application of DPO, with particularly notable gains in mathematical
reasoning tasks. The model achieves an average improvement of 3.89% on AIME
24/25 benchmarks. Our results highlight the effectiveness of combining
principled data selection with full-stage post-training, offering a practical
solution for aligning large reasoning models in a scalable and data-efficient
manner. The model checkpoints are available at
https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.

</details>


### [28] [GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning](https://arxiv.org/abs/2508.05498)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: GRAIL是一个针对知识图谱的图检索增强交互式学习框架，旨在解决大型语言模型（LLM）与检索增强生成（RAG）结合时，在处理结构化数据和图检索精度方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要处理非结构化数据，对知识图谱等结构化知识处理能力有限。当前的图检索方法难以同时捕捉整体图结构并控制精度（存在信息缺失或冗余连接），从而影响推理性能。

Method: GRAIL框架通过LLM引导的随机探索和路径过滤来建立数据合成管道，为每个任务生成细粒度的推理轨迹。基于合成数据，采用两阶段训练过程学习一个策略，动态决定每个推理步骤的最优行动。图检索的精度-简洁性平衡目标被解耦为细粒度的过程监督奖励，以提高数据效率和训练稳定性。部署时采用交互式检索范式，使模型能自主探索图路径并动态平衡检索广度和精度。

Result: 在三个知识图谱问答数据集上，GRAIL平均准确率提高了21.01%，F1分数提高了22.43%。

Conclusion: GRAIL通过其独特的数据合成和两阶段训练策略，有效解决了知识图谱检索中的精度和广度平衡问题，显著提升了LLM在结构化数据上的推理性能。

Abstract: Large Language Models (LLMs) integrated with Retrieval-Augmented Generation
(RAG) techniques have exhibited remarkable performance across a wide range of
domains. However, existing RAG approaches primarily operate on unstructured
data and demonstrate limited capability in handling structured knowledge such
as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally
struggle to capture holistic graph structures while simultaneously facing
precision control challenges that manifest as either critical information gaps
or excessive redundant connections, collectively undermining reasoning
performance. To address this challenge, we propose GRAIL: Graph-Retrieval
Augmented Interactive Learning, a framework designed to interact with
large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL
integrates LLM-guided random exploration with path filtering to establish a
data synthesis pipeline, where a fine-grained reasoning trajectory is
automatically generated for each task. Based on the synthesized data, we then
employ a two-stage training process to learn a policy that dynamically decides
the optimal actions at each reasoning step. The overall objective of
precision-conciseness balance in graph retrieval is decoupled into fine-grained
process-supervised rewards to enhance data efficiency and training stability.
In practical deployment, GRAIL adopts an interactive retrieval paradigm,
enabling the model to autonomously explore graph paths while dynamically
balancing retrieval breadth and precision. Extensive experiments have shown
that GRAIL achieves an average accuracy improvement of 21.01% and F1
improvement of 22.43% on three knowledge graph question-answering datasets. Our
source code and datasets is available at https://github.com/Changgeww/GRAIL.

</details>


### [29] [Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation](https://arxiv.org/abs/2508.05508)
*Roshita Bhonsle,Rishav Dutta,Sneha Vavilapalli,Harsh Seth,Abubakarr Jaye,Yapei Chang,Mukund Rungta,Emmanuel Aboah Boateng,Sadid Hasan,Ehi Nosakhare,Soundar Srinivasan*

Main category: cs.AI

TL;DR: 本文提出了一个通用、模块化的智能体任务完成评估框架，通过分解任务并逐步验证智能体的推理过程，模仿人类评估方式，在与人类评估的一致性方面优于现有的LLM-as-a-Judge方法。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法（如LLM-as-a-Judge）仅关注最终输出，忽略了智能体决策过程中的逐步推理；而现有的Agent-as-a-Judge系统通常是针对特定领域设计的，缺乏通用性。因此，需要一个通用且鲁棒的评估框架。

Method: 提出了一个通用、模块化的评估框架，该框架模仿人类评估方式，将任务分解为子任务，并利用智能体的输出和推理信息验证每一步。每个模块负责评估过程的特定方面，其输出被聚合以生成最终任务完成的判断。该框架在GAIA和BigCodeBench两个基准上对Magentic-One Actor Agent进行了验证。

Result: 与基于GPT-4o的LLM-as-a-Judge基线相比，本文提出的Judge Agent在预测任务成功方面与人类评估的一致性更高，在GAIA和BigCodeBench上分别实现了4.76%和10.52%的更高对齐准确率。

Conclusion: 所提出的通用评估框架具有潜力，能够更准确地预测任务成功，并与人类评估更紧密地对齐，展示了其在评估基础模型作为智能体方面的有效性。

Abstract: The increasing adoption of foundation models as agents across diverse domains
necessitates a robust evaluation framework. Current methods, such as
LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step
reasoning that drives agentic decision-making. Meanwhile, existing
Agent-as-a-Judge systems, where one agent evaluates another's task completion,
are typically designed for narrow, domain-specific settings. To address this
gap, we propose a generalizable, modular framework for evaluating agent task
completion independent of the task domain. The framework emulates human-like
evaluation by decomposing tasks into sub-tasks and validating each step using
available information, such as the agent's output and reasoning. Each module
contributes to a specific aspect of the evaluation process, and their outputs
are aggregated to produce a final verdict on task completion. We validate our
framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA
and BigCodeBench. Our Judge Agent predicts task success with closer agreement
to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,
respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This
demonstrates the potential of our proposed general-purpose evaluation
framework.

</details>


### [30] [Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program](https://arxiv.org/abs/2508.05513)
*Meryem Yilmaz Soylu,Adrian Gallard,Jeonghyun Lee,Gayane Grigoryan,Rushil Desai,Stephen Harmon*

Main category: cs.AI

TL;DR: 论文介绍LORI，一个基于AI的推荐信（LOR）领导力评估工具，利用自然语言处理（NLP）和大型语言模型（如RoBERTa和LLAMA）来识别申请人的领导力特质，如团队合作、沟通和创新。RoBERTa模型在测试数据上取得了91.6%的加权F1分数，旨在简化招生流程并提供更全面的能力评估。


<details>
  <summary>Details</summary>
Motivation: 推荐信审阅耗时且劳动密集；需要支持招生委员会为学生提供专业成长反馈；STEM领域领导力日益重要，需要准确评估申请人领导力。

Method: 开发了一个名为LORI的AI驱动检测工具。该工具利用自然语言处理（NLP）技术，并结合RoBERTa和LLAMA等大型语言模型来识别推荐信中的领导力属性。

Result: 成功识别了团队合作、沟通和创新等领导力特质。最新RoBERTa模型在测试数据上表现出高一致性，实现了91.6%的加权F1分数，92.4%的精确率和91.6%的召回率。

Conclusion: 将LORI整合到研究生招生流程中对于准确评估申请人的领导力至关重要。此方法不仅能简化招生流程，还能自动化并确保对候选人能力的更全面评估。

Abstract: Letters of recommendation (LORs) provide valuable insights into candidates'
capabilities and experiences beyond standardized test scores. However,
reviewing these text-heavy materials is time-consuming and labor-intensive. To
address this challenge and support the admission committee in providing
feedback for students' professional growth, our study introduces LORI: LOR
Insights, a novel AI-based detection tool for assessing leadership skills in
LORs submitted by online master's program applicants. By employing natural
language processing and leveraging large language models using RoBERTa and
LLAMA, we seek to identify leadership attributes such as teamwork,
communication, and innovation. Our latest RoBERTa model achieves a weighted F1
score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong
level of consistency in our test data. With the growing importance of
leadership skills in the STEM sector, integrating LORI into the graduate
admissions process is crucial for accurately assessing applicants' leadership
capabilities. This approach not only streamlines the admissions process but
also automates and ensures a more comprehensive evaluation of candidates'
capabilities.

</details>


### [31] [MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media](https://arxiv.org/abs/2508.05557)
*Rui Lu,Jinhe Bi,Yunpu Ma,Feng Xiao,Yuntao Du,Yijun Tian*

Main category: cs.AI

TL;DR: MV-Debate是一个多视图代理辩论框架，用于统一的多模态有害内容检测，通过多代理协作和动态反射门控显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中多模态有害意图（如讽刺、仇恨言论、虚假信息）的识别极具挑战性，原因包括跨模态矛盾、快速文化变迁和微妙的语用线索。

Method: 提出MV-Debate框架，包含四个互补的辩论代理（表面分析师、深度推理者、模态对比者、社会情境主义者），从不同视角分析内容。通过迭代辩论和动态反射门控，代理们在“反射增益”标准下优化响应，以确保准确性和效率。

Result: 在三个基准数据集上的实验表明，MV-Debate显著优于强大的单一模型和现有的多代理辩论基线。

Conclusion: 这项工作强调了多代理辩论在推进安全关键在线环境中可靠的社会意图检测方面的潜力。

Abstract: Social media has evolved into a complex multimodal environment where text,
images, and other signals interact to shape nuanced meanings, often concealing
harmful intent. Identifying such intent, whether sarcasm, hate speech, or
misinformation, remains challenging due to cross-modal contradictions, rapid
cultural shifts, and subtle pragmatic cues. To address these challenges, we
propose MV-Debate, a multi-view agent debate framework with dynamic reflection
gating for unified multimodal harmful content detection. MV-Debate assembles
four complementary debate agents, a surface analyst, a deep reasoner, a
modality contrast, and a social contextualist, to analyze content from diverse
interpretive perspectives. Through iterative debate and reflection, the agents
refine responses under a reflection-gain criterion, ensuring both accuracy and
efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate
significantly outperforms strong single-model and existing multi-agent debate
baselines. This work highlights the promise of multi-agent debate in advancing
reliable social intent detection in safety-critical online contexts.

</details>


### [32] [The Missing Reward: Active Inference in the Era of Experience](https://arxiv.org/abs/2508.05619)
*Bo Wen*

Main category: cs.AI

TL;DR: 本文提出主动推理（AIF）是实现自主AI代理的关键，它通过内在的自由能最小化驱动学习，从而解决当前AI范式中人类奖励工程的瓶颈和“扎根代理差距”，并建议将大型语言模型（LLM）与AIF结合。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统面临高质量训练数据枯竭和对大量人工奖励工程的依赖，导致可扩展性挑战，阻碍了自主智能的发展。尽管“经验时代”的愿景有前景，但它仍然依赖于大量人类奖励函数设计，将瓶颈从数据策展转移到奖励策展，暴露出AI系统无法自主制定、适应和追求目标的“扎根代理差距”。

Method: 提出用主动推理（AIF）来弥合“扎根代理差距”，通过用内在的自由能最小化来取代外部奖励信号，使代理能够通过统一的贝叶斯目标自然地平衡探索与利用。建议将大型语言模型作为生成世界模型，与AIF的原则性决策框架相结合。

Result: 通过将大型语言模型与AIF整合，可以创建能够高效地从经验中学习，同时保持与人类价值观一致的代理。这种综合方法为实现能够自主发展，同时遵守计算和物理约束的AI系统提供了一条引人注目的路径。

Conclusion: 主动推理（AIF）为开发能够自主学习而无需持续人类奖励工程的AI代理提供了关键基础。通过将AIF与大型语言模型结合，可以解决当前AI的可扩展性问题和“扎根代理差距”，实现真正自主、高效且与人类价值观对齐的智能系统。

Abstract: This paper argues that Active Inference (AIF) provides a crucial foundation
for developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust
high-quality training data and rely on increasingly large human workforces for
reward design, the current paradigm faces significant scalability challenges
that could impede progress toward genuinely autonomous intelligence. The
proposal for an ``Era of Experience,'' where agents learn from self-generated
data, is a promising step forward. However, this vision still depends on
extensive human engineering of reward functions, effectively shifting the
bottleneck from data curation to reward curation. This highlights what we
identify as the \textbf{grounded-agency gap}: the inability of contemporary AI
systems to autonomously formulate, adapt, and pursue objectives in response to
changing circumstances. We propose that AIF can bridge this gap by replacing
external reward signals with an intrinsic drive to minimize free energy,
allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF's principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with
human values. This synthesis offers a compelling path toward AI systems that
can develop autonomously while adhering to both computational and physical
constraints.

</details>


### [33] [Simulating Human-Like Learning Dynamics with LLM-Empowered Agents](https://arxiv.org/abs/2508.05622)
*Yu Yuan,Lili Zhao,Wei Chen,Guangting Zheng,Kai Zhang,Mengdi Zhang,Qi Liu*

Main category: cs.AI

TL;DR: 该研究引入了LearnerAgent，一个基于大语言模型（LLMs）的多智能体框架，用于模拟真实的教学环境并探索类人学习动态，揭示了LLM的默认学习行为是一种“勤奋但脆弱的表面学习者”。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉学习动态、跟踪进展和提供可解释性方面存在不足，主要依赖受控实验或基于规则的模型来探索认知过程。因此，需要一个能够模拟真实教学环境并深入理解学习行为的新方法。

Method: 引入LearnerAgent，一个基于LLMs的多智能体框架。构建了具有心理学基础的学习者档案（如深度学习者、表面学习者、懒惰学习者），以及一个无个性特征的通用学习者以考察LLM的默认行为。通过每周知识获取、每月策略选择、定期测试和同伴互动，跟踪学习者一年的动态学习进展。

Result: 1) 纵向分析显示，只有深度学习者实现了持续的认知成长，设计的“陷阱问题”有效诊断了表面学习者的浅层知识。2) 不同学习者的行为和认知模式与他们的心理学档案高度一致。3) 学习者的自我概念评分真实演变，通用学习者尽管有认知局限性，却发展出惊人的高自我效能感。4) 基础LLM的默认学习者档案是“勤奋但脆弱的表面学习者”，模仿优秀学生的行为但缺乏真正的、可泛化的理解。

Conclusion: LearnerAgent框架与真实场景高度吻合，为LLM的行为提供了更深入的见解。研究发现LLM的默认学习模式是一种“勤奋但脆弱的表面学习者”，这对于理解LLM的认知能力和局限性具有重要意义。

Abstract: Capturing human learning behavior based on deep learning methods has become a
major research focus in both psychology and intelligent systems. Recent
approaches rely on controlled experiments or rule-based models to explore
cognitive processes. However, they struggle to capture learning dynamics, track
progress over time, or provide explainability. To address these challenges, we
introduce LearnerAgent, a novel multi-agent framework based on Large Language
Models (LLMs) to simulate a realistic teaching environment. To explore
human-like learning dynamics, we construct learners with psychologically
grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free
General Learner to inspect the base LLM's default behavior. Through weekly
knowledge acquisition, monthly strategic choices, periodic tests, and peer
interaction, we can track the dynamic learning progress of individual learners
over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis
reveals that only Deep Learner achieves sustained cognitive growth. Our
specially designed "trap questions" effectively diagnose Surface Learner's
shallow knowledge. 2) The behavioral and cognitive patterns of distinct
learners align closely with their psychological profiles. 3) Learners'
self-concept scores evolve realistically, with the General Learner developing
surprisingly high self-efficacy despite its cognitive limitations. 4)
Critically, the default profile of base LLM is a "diligent but brittle Surface
Learner"-an agent that mimics the behaviors of a good student but lacks true,
generalizable understanding. Extensive simulation experiments demonstrate that
LearnerAgent aligns well with real scenarios, yielding more insightful findings
about LLMs' behavior.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

TL;DR: RetinexDual是一种基于Retinex理论的超高清图像恢复（UHD IR）框架，通过结合空间（SAMBA）和频率（FIA）域处理，有效解决了传统方法的局限性，并在多项UHD IR任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统超高清图像恢复方法存在显著缺陷：极端下采样导致不可逆的信息丢失，而纯频率域方法因丢失退化局部性而对空间局部图像伪影无效。

Method: 提出RetinexDual框架，基于Retinex理论，包含两个互补的子网络：1. Scale-Attentive maMBA (SAMBA)：负责校正反射分量，采用粗到细机制克服mamba的因果建模，减少伪影并恢复细节。2. Frequency Illumination Adaptor (FIA)：在频域操作，利用全局上下文，精确校正颜色和光照畸变。

Result: RetinexDual在去雨、去模糊、去雾和低光图像增强四项UHD IR任务上，定性和定量地优于现有方法。消融研究证明了RetinexDual中各分支独特设计的重要性及其组件的有效性。

Conclusion: RetinexDual通过其创新的双分支架构，结合了空间和频率域的优势，有效克服了现有UHD IR方法的局限性，实现了卓越的恢复性能。

Abstract: Advancements in image sensing have elevated the importance of
Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as
extreme downsampling or transformation from the spatial to the frequency
domain, encounter significant drawbacks: downsampling induces irreversible
information loss in UHD images, while our frequency analysis reveals that pure
frequency-domain approaches are ineffective for spatially confined image
artifacts, primarily due to the loss of degradation locality. To overcome these
limitations, we present RetinexDual, a novel Retinex theory-based framework
designed for generalized UHD IR tasks. RetinexDual leverages two complementary
sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination
Adaptor (FIA). SAMBA, responsible for correcting the reflectance component,
utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,
which effectively reduces artifacts and restores intricate details. On the
other hand, FIA ensures precise correction of color and illumination
distortions by operating in the frequency domain and leveraging the global
context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely
deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows
that it outperforms recent methods qualitatively and quantitatively. Ablation
studies demonstrate the importance of employing distinct designs for each
branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [35] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 该论文介绍了ENTRep，一个用于耳鼻喉科（ENT）内窥镜图像分析的综合性基准数据集和挑战赛，旨在解决现有自动化分析和案例检索的不足。


<details>
  <summary>Details</summary>
Motivation: 耳鼻喉科内窥镜图像的自动化分析是关键但发展不足，受限于设备和操作员差异、细微局部发现以及左右侧、声带状态等精细区分。此外，现有公共基准很少支持临床医生所需的视觉和文本描述的相似病例检索能力。

Method: 引入了ENTRep数据集，其中包含专家标注的图像（按解剖区域和正常/异常状态分类），并附有双语（越南语和英语）叙述性描述。定义了三个基准任务，标准化了提交协议，并使用服务器端评分在公共和私有测试集上评估性能。

Result: 论文报告了表现最佳团队的结果，并提供了深入的讨论和见解。 (具体数值结果未在摘要中给出)

Conclusion: ENTRep作为一个新的基准，通过整合精细解剖分类、图像到图像和文本到图像检索，并在双语临床监督下，为推动耳鼻喉科内窥镜分析的自动化和临床应用提供了重要的平台和资源。

Abstract: Automated analysis of endoscopic imagery is a critical yet underdeveloped
component of ENT (ear, nose, and throat) care, hindered by variability in
devices and operators, subtle and localized findings, and fine-grained
distinctions such as laterality and vocal-fold state. In addition to
classification, clinicians require reliable retrieval of similar cases, both
visually and through concise textual descriptions. These capabilities are
rarely supported by existing public benchmarks. To this end, we introduce
ENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,
which integrates fine-grained anatomical classification with image-to-image and
text-to-image retrieval under bilingual (Vietnamese and English) clinical
supervision. Specifically, the dataset comprises expert-annotated images,
labeled for anatomical region and normal or abnormal status, and accompanied by
dual-language narrative descriptions. In addition, we define three benchmark
tasks, standardize the submission protocol, and evaluate performance on public
and private test splits using server-side scoring. Moreover, we report results
from the top-performing teams and provide an insight discussion.

</details>


### [36] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

TL;DR: CoMAD是一个轻量级、无参数的框架，通过非对称掩码和联合共识门控，将多个自监督Vision Transformer教师模型的知识蒸馏到一个紧凑的学生网络中，实现了紧凑型SSL蒸馏的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习范式（如对比学习和掩码图像建模）通常独立预训练，忽略了互补洞察，并产生大型模型，不适用于资源受限的部署。

Method: CoMAD框架从MAE、MoCo v3和iBOT三个预训练ViT-Base教师模型中蒸馏知识。它采用非对称掩码：学生仅见25%的补丁，而每个教师接收更轻、独特的掩码。教师嵌入通过线性适配器和层归一化对齐到学生空间，并通过结合余弦相似度和教师间一致性的联合共识门控进行融合。学生通过对可见令牌和重建特征图的双层KL散度进行训练。

Result: CoMAD的ViT-Tiny模型在ImageNet-1K上实现了75.4%的Top-1准确率（比之前SOTA提升0.4%）。在密集预测任务中，ADE20K上达到47.3% mIoU，MS-COCO上达到44.5% box平均精度和40.5% mask平均精度，在紧凑型SSL蒸馏领域建立了新的SOTA。

Conclusion: CoMAD成功地将多个自监督学习教师模型的知识统一到一个紧凑的学生网络中，显著提升了紧凑模型的性能，并在ImageNet-1K、ADE20K和MS-COCO等数据集上取得了最先进的成果，证明了其在资源受限部署中的实用性。

Abstract: Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [37] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

TL;DR: RADAR是一种新型的实时异常检测方法，它利用基于注意力机制的扩散模型直接生成异常图，无需耗时的图像重建过程，解决了现有扩散模型在异常检测中的计算效率低和重建不准确等问题，并在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重建的扩散模型在异常检测中存在三个主要挑战：1) 重建过程计算成本高，不适用于实时应用；2) 对于复杂或细微的异常模式，重建图像可能偏离原始输入；3) 中间噪声水平的选择具有挑战性，依赖于先验知识，不适用于无监督设置。

Method: 本文提出了RADAR（Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time），该方法不通过重建输入图像来检测异常，而是直接利用扩散模型生成异常图。它采用基于注意力机制的扩散模型，旨在提高检测准确性和计算效率。

Result: RADAR在真实世界的3D打印材料和MVTec-AD数据集上进行了评估。结果显示，RADAR在准确率、精确率、召回率和F1分数等所有关键指标上均超越了最先进的基于扩散和统计机器学习模型。具体而言，RADAR在MVTec-AD数据集上将F1分数提高了7%，在3D打印材料数据集上提高了13%。

Conclusion: RADAR成功克服了传统基于重建的异常检测方法的局限性，通过直接生成异常图，显著提高了异常检测的准确性和计算效率，并在多个真实世界数据集上取得了优于现有最先进模型的表现。

Abstract: Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [38] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

TL;DR: 该研究利用事件相机和深度学习（CNN_LSTM模型）实现经济高效的眼球中心定位，旨在预测人类注意力并提升VR/AR用户体验，模型准确率达81%。


<details>
  <summary>Details</summary>
Motivation: 由于人眼运动速度快（可达300度/秒），精确的眼动追踪通常需要昂贵的高速相机。为了在消费电子（尤其是VR/AR）中广泛应用眼动分析，需要开发一种可解释且经济高效的算法来预测人类注意力，以提高设备舒适度和用户体验。

Method: 该研究使用事件相机作为输入，探索了多种深度学习方法。其中，CNN_LSTM模型被证明是最有效的。未来工作计划采用逐层相关性传播（LRP）来提高模型的可解释性和预测性能。

Result: CNN_LSTM模型在定位眼球中心位置(x, y)方面表现最佳，达到了约81%的准确率。

Conclusion: 该研究成功开发了一种基于深度学习的经济高效算法，用于通过事件相机追踪眼球中心，并有效预测人类注意力。未来将通过LRP进一步提升模型的可解释性和性能，以更好地应用于消费电子领域，改善用户体验。

Abstract: This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>


### [39] [LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction](https://arxiv.org/abs/2508.04847)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.CV

TL;DR: 本文提出LuKAN模型，一个基于Kolmogorov-Arnold网络（KANs）并使用Lucas多项式激活函数的3D人体运动预测模型，旨在平衡预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动预测方法难以在预测精度和计算效率之间取得平衡。

Method: 模型首先使用离散小波变换编码输入运动序列的时间信息，然后通过空间投影层捕获关节间依赖。核心是时间依赖学习器，它采用由Lucas多项式参数化的KAN层进行函数逼近，以提高计算效率和处理振荡行为的能力。最后，通过逆离散小波变换重建时间域的运动序列。

Result: 在三个基准数据集上的广泛实验表明，LuKAN模型与现有强基线相比具有竞争力，且其紧凑的架构结合Lucas多项式的线性递推确保了计算效率。

Conclusion: LuKAN模型通过结合离散小波变换、空间投影层和基于Lucas多项式激活的KAN核心，有效解决了3D人体运动预测中精度与效率的平衡问题，实现了高性能和高效率的预测。

Abstract: The goal of 3D human motion prediction is to forecast future 3D poses of the
human body based on historical motion data. Existing methods often face
limitations in achieving a balance between prediction accuracy and
computational efficiency. In this paper, we present LuKAN, an effective model
based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.
Our model first applies the discrete wavelet transform to encode temporal
information in the input motion sequence. Then, a spatial projection layer is
used to capture inter-joint dependencies, ensuring structural consistency of
the human body. At the core of LuKAN is the Temporal Dependency Learner, which
employs a KAN layer parameterized by Lucas polynomials for efficient function
approximation. These polynomials provide computational efficiency and an
enhanced capability to handle oscillatory behaviors. Finally, the inverse
discrete wavelet transform reconstructs motion sequences in the time domain,
generating temporally coherent predictions. Extensive experiments on three
benchmark datasets demonstrate the competitive performance of our model
compared to strong baselines, as evidenced by both quantitative and qualitative
evaluations. Moreover, its compact architecture coupled with the linear
recurrence of Lucas polynomials, ensures computational efficiency.

</details>


### [40] [VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence](https://arxiv.org/abs/2508.04852)
*Chenhui Qiang,Zhaoyang Wei,Xumeng Han Zipeng Wang,Siyao Li,Xiangyuan Lan,Jianbin Jiao,Zhenjun Han*

Main category: cs.CV

TL;DR: VER-Bench是一个新的基准，用于评估多模态大模型（MLLMs）在识别微小视觉线索（平均占图像面积0.25%）并将其与世界知识结合进行复杂推理的能力，揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准主要关注基本感知或突出元素推理，但未能评估基于细微、不显眼局部细节的深层视觉理解和复杂推理，而这些细节对于鲁棒分析至关重要。

Method: 引入VER-Bench框架，包含374个精心设计的问题，涵盖地理空间、时间、情境、意图、系统状态和符号推理六种类型。每个问题都附有结构化证据，包括视觉线索及其推导出的相关推理。该基准侧重于识别细粒度视觉线索（平均占图像面积0.25%）并将其与世界知识整合进行复杂推理。

Result: VER-Bench揭示了当前模型在提取细微视觉证据和构建基于证据的论证方面的局限性。

Conclusion: 为了实现真正的视觉理解和类人分析，需要增强模型在细粒度视觉证据提取、整合和推理方面的能力。

Abstract: With the rapid development of MLLMs, evaluating their visual capabilities has
become increasingly crucial. Current benchmarks primarily fall into two main
types: basic perception benchmarks, which focus on local details but lack deep
reasoning (e.g., "what is in the image?"), and mainstream reasoning benchmarks,
which concentrate on prominent image elements but may fail to assess subtle
clues requiring intricate analysis. However, profound visual understanding and
complex reasoning depend more on interpreting subtle, inconspicuous local
details than on perceiving salient, macro-level objects. These details, though
occupying minimal image area, often contain richer, more critical information
for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel
framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,
often occupying on average just 0.25% of the image area; 2) integrate these
clues with world knowledge for complex reasoning. Comprising 374 carefully
designed questions across Geospatial, Temporal, Situational, Intent, System
State, and Symbolic reasoning, each question in VER-Bench is accompanied by
structured evidence: visual clues and question-related reasoning derived from
them. VER-Bench reveals current models' limitations in extracting subtle visual
evidence and constructing evidence-based arguments, highlighting the need to
enhance models's capabilities in fine-grained visual evidence extraction,
integration, and reasoning for genuine visual understanding and human-like
analysis. Dataset and additional materials are available
https://github.com/verbta/ACMMM-25-Materials.

</details>


### [41] [AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content](https://arxiv.org/abs/2508.05016)
*Shushi Wang,Chunyi Li,Zicheng Zhang,Han Zhou,Wei Dong,Jun Chen,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 针对AI增强用户生成内容（AI-UGC）缺乏专用质量评估模型的问题，本文构建了一个新的基准数据集AU-IQA，并评估了现有质量评估方法在该数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: AI增强技术广泛应用于UGC，但缺乏专门的AI-UGC质量评估模型限制了用户体验和增强方法的发展。现有感知质量评估方法在UGC和AIGC上表现良好，但对结合两者特征的AI-UGC的有效性尚未充分探索。

Method: 构建了AU-IQA基准数据集，包含4800张由超分辨率、低光增强和去噪三种代表性增强类型生成的AI-UGC图像。在此数据集上，评估了一系列现有质量评估模型，包括传统IQA方法和大型多模态模型。

Result: 在AU-IQA数据集上评估了多种现有质量评估模型，并对当前方法在评估AI-UGC感知质量方面的表现进行了全面分析。

Conclusion: 通过对现有方法在AI-UGC质量评估上的表现进行分析，揭示了当前方法在此领域存在的不足，强调了对AI-UGC专用评估模型的需求。

Abstract: AI-based image enhancement techniques have been widely adopted in various
visual applications, significantly improving the perceptual quality of
user-generated content (UGC). However, the lack of specialized quality
assessment models has become a significant limiting factor in this field,
limiting user experience and hindering the advancement of enhancement methods.
While perceptual quality assessment methods have shown strong performance on
UGC and AIGC individually, their effectiveness on AI-enhanced UGC (AI-UGC)
which blends features from both, remains largely unexplored. To address this
gap, we construct AU-IQA, a benchmark dataset comprising 4,800 AI-UGC images
produced by three representative enhancement types which include
super-resolution, low-light enhancement, and denoising. On this dataset, we
further evaluate a range of existing quality assessment models, including
traditional IQA methods and large multimodal models. Finally, we provide a
comprehensive analysis of how well current approaches perform in assessing the
perceptual quality of AI-UGC. The access link to the AU-IQA is
https://github.com/WNNGGU/AU-IQA-Dataset.

</details>


### [42] [Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications](https://arxiv.org/abs/2508.04868)
*Noreen Anwar,Guillaume-Alexandre Bilodeau,Wassim Bouachir*

Main category: cs.CV

TL;DR: DAMM提出多模态查询和双流注意力机制，以解决基于Transformer的目标检测器在遮挡、精细定位和计算效率方面的挑战，并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的目标检测器在处理遮挡、精细定位以及因固定查询和密集注意力导致的计算效率低下问题时表现不佳。

Method: DAMM（Dual-stream Attention with Multi-Modal queries）框架引入了查询自适应和结构化交叉注意力。查询自适应利用三种查询类型：来自视觉-语言模型的基于外观的查询、使用多边形嵌入的位置查询以及用于场景覆盖的随机学习查询。此外，一个双流交叉注意力模块分别细化语义和空间特征，以提高复杂场景中的定位精度。

Result: DAMM在四个具有挑战性的基准测试中，实现了平均精度（AP）和召回率的最先进性能，验证了多模态查询自适应和双流注意力的有效性。

Conclusion: 多模态查询自适应和双流注意力机制能有效提升基于Transformer的目标检测器在精度和效率方面的表现，尤其是在处理遮挡和精细定位问题上。

Abstract: Transformer-based object detectors often struggle with occlusions,
fine-grained localization, and computational inefficiency caused by fixed
queries and dense attention. We propose DAMM, Dual-stream Attention with
Multi-Modal queries, a novel framework introducing both query adaptation and
structured cross-attention for improved accuracy and efficiency. DAMM
capitalizes on three types of queries: appearance-based queries from
vision-language models, positional queries using polygonal embeddings, and
random learned queries for general scene coverage. Furthermore, a dual-stream
cross-attention module separately refines semantic and spatial features,
boosting localization precision in cluttered scenes. We evaluated DAMM on four
challenging benchmarks, and it achieved state-of-the-art performance in average
precision (AP) and recall, demonstrating the effectiveness of multi-modal query
adaptation and dual-stream attention. Source code is at:
\href{https://github.com/DET-LIP/DAMM}{GitHub}.

</details>


### [43] [Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks](https://arxiv.org/abs/2508.05068)
*Ruiyu Li,Changyuan Qiu,Hangrui Cao,Qihan Ren,Yuqing Qiu*

Main category: cs.CV

TL;DR: 该项目旨在探索通过分类和对抗学习进行图像自动着色，以解决传统回归方法忽略颜色预测多模态性的问题。


<details>
  <summary>Details</summary>
Motivation: 图像着色在计算机视觉领域有重要应用，如颜色恢复和动画自动着色。然而，它是一个病态问题，自由度高。场景语义和表面纹理可以提供重要颜色线索，且有大量训练数据可用。现有方法（如回归）忽略了颜色预测的多模态性质。

Method: 通过分类和对抗学习进行图像自动着色。将在现有工作基础上构建模型，根据特定场景进行修改，并进行比较。

Result: 抽象中未提供具体结果，该部分描述的是项目计划。

Conclusion: 抽象中未提供具体结论，该部分描述的是项目目标和方法。

Abstract: Image colorization, the task of adding colors to grayscale images, has been
the focus of significant research efforts in computer vision in recent years
for its various application areas such as color restoration and automatic
animation colorization [15, 1]. The colorization problem is challenging as it
is highly ill-posed with two out of three image dimensions lost, resulting in
large degrees of freedom. However, semantics of the scene as well as the
surface texture could provide important cues for colors: the sky is typically
blue, the clouds are typically white and the grass is typically green, and
there are huge amounts of training data available for learning such priors
since any colored image could serve as a training data point [20].
  Colorization is initially formulated as a regression task[5], which ignores
the multi-modal nature of color prediction. In this project, we explore
automatic image colorization via classification and adversarial learning. We
will build our models on prior works, apply modifications for our specific
scenario and make comparisons.

</details>


### [44] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文研究了多模态仇恨视频检测中粗粒度标注导致的标签噪声问题，通过时间戳精细化处理和实验，揭示了其对模型性能和决策边界的负面影响，并强调了开发时间感知模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 在线多媒体内容中仇恨言论的迅速扩散带来了严重的社会和监管挑战。现有的大多数多模态仇恨视频检测方法依赖于粗粒度的视频级标注，忽略了仇恨内容的具体时间范围，导致视频中包含大量非仇恨片段，从而引入了显著的标签噪声。

Method: 研究者采用细粒度方法，利用标注的时间戳对HateMM和MultiHateClip英文数据集中的仇恨视频进行裁剪，以分离出明确的仇恨片段。随后，对这些裁剪后的片段进行了探索性分析，以检查仇恨和非仇恨内容的分布和特征。最后，通过对照实验证明了时间戳噪声的影响。

Result: 分析揭示了粗粒度视频级标注所引入的语义重叠和混淆程度。对照实验表明，时间戳噪声从根本上改变了模型的决策边界，削弱了分类置信度，突出了仇恨言论表达固有的上下文依赖性和时间连续性。

Conclusion: 研究结果提供了对多模态仇恨视频时间动态的新见解，并强调了开发时间感知模型和基准的必要性，以提高模型的鲁棒性和可解释性。

Abstract: The rapid proliferation of online multimedia content has intensified the
spread of hate speech, presenting critical societal and regulatory challenges.
While recent work has advanced multimodal hateful video detection, most
approaches rely on coarse, video-level annotations that overlook the temporal
granularity of hateful content. This introduces substantial label noise, as
videos annotated as hateful often contain long non-hateful segments. In this
paper, we investigate the impact of such label ambiguity through a fine-grained
approach. Specifically, we trim hateful videos from the HateMM and
MultiHateClip English datasets using annotated timestamps to isolate explicitly
hateful segments. We then conduct an exploratory analysis of these trimmed
segments to examine the distribution and characteristics of both hateful and
non-hateful content. This analysis highlights the degree of semantic overlap
and the confusion introduced by coarse, video-level annotations. Finally,
controlled experiments demonstrated that time-stamp noise fundamentally alters
model decision boundaries and weakens classification confidence, highlighting
the inherent context dependency and temporal continuity of hate speech
expression. Our findings provide new insights into the temporal dynamics of
multimodal hateful videos and highlight the need for temporally aware models
and benchmarks for improved robustness and interpretability. Code and data are
available at
https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.

</details>


### [45] [F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2508.05465)
*Lumin Chen,Zhiying Wu,Tianye Lei,Xuexue Bai,Ming Feng,Yuxi Wang,Gaofeng Meng,Zhen Lei,Hongbin Liu*

Main category: cs.CV

TL;DR: 该研究提出了一个新的垂体解剖结构分割数据集（PAS）和名为F2PASeg的模型，旨在实时准确地分割垂体手术中的关键解剖结构，以提高手术安全性。


<details>
  <summary>Details</summary>
Motivation: 垂体肿瘤常导致邻近重要结构变形或包绕，术前解剖结构分割能为外科医生提供风险区域预警，从而提高手术安全性。然而，用于垂体手术的像素级标注视频流数据集极其稀缺。

Method: 引入了包含7,845张时间连贯图像的PAS数据集，并采用数据增强技术模拟手术器械以缓解类别不平衡。针对术中遮挡、相机运动和出血导致特征表示不一致的问题，提出了F2PASeg模型，该模型通过集成特征融合模块，结合高分辨率图像特征和深层语义嵌入来细化解剖结构分割，增强对术中变化的鲁棒性。

Result: 实验结果表明，F2PASeg能够实时、稳定地分割关键解剖结构。

Conclusion: F2PASeg为术中垂体手术规划提供了一个可靠的解决方案。

Abstract: Pituitary tumors often cause deformation or encapsulation of adjacent vital
structures. Anatomical structure segmentation can provide surgeons with early
warnings of regions that pose surgical risks, thereby enhancing the safety of
pituitary surgery. However, pixel-level annotated video stream datasets for
pituitary surgeries are extremely rare. To address this challenge, we introduce
a new dataset for Pituitary Anatomy Segmentation (PAS). PAS comprises 7,845
time-coherent images extracted from 120 videos. To mitigate class imbalance, we
apply data augmentation techniques that simulate the presence of surgical
instruments in the training data. One major challenge in pituitary anatomy
segmentation is the inconsistency in feature representation due to occlusions,
camera motion, and surgical bleeding. By incorporating a Feature Fusion module,
F2PASeg is proposed to refine anatomical structure segmentation by leveraging
both high-resolution image features and deep semantic embeddings, enhancing
robustness against intraoperative variations. Experimental results demonstrate
that F2PASeg consistently segments critical anatomical structures in real time,
providing a reliable solution for intraoperative pituitary surgery planning.
Code: https://github.com/paulili08/F2PASeg.

</details>


### [46] [Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations](https://arxiv.org/abs/2508.04924)
*Zahidul Islam,Sujoy Paul,Mrigank Rochan*

Main category: cs.CV

TL;DR: 本文提出Highlight-TTA，一个视频精彩片段检测的测试时自适应框架，通过在测试阶段动态调整模型以适应每个视频的独特特征，从而提高泛化能力和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频精彩片段检测方法在泛化到所有测试视频时表现不佳，因为它们使用固定的通用模型，无法适应新视频多样化的内容、风格以及音视频质量，导致性能下降。

Method: 提出Highlight-TTA框架，在测试时动态调整模型。该框架与一个辅助任务（跨模态幻觉）共同优化，并采用元辅助训练方案。在测试时，利用辅助任务在测试视频上自适应训练好的模型，以进一步提升精彩片段检测性能。

Result: 在三个最先进的精彩片段检测模型和三个基准数据集上进行的广泛实验表明，引入Highlight-TTA显著提升了这些模型的性能，取得了卓越的结果。

Conclusion: Highlight-TTA通过测试时自适应有效地解决了视频精彩片段检测中模型泛化能力不足的问题，显著提升了检测性能。

Abstract: Existing video highlight detection methods, although advanced, struggle to
generalize well to all test videos. These methods typically employ a generic
highlight detection model for each test video, which is suboptimal as it fails
to account for the unique characteristics and variations of individual test
videos. Such fixed models do not adapt to the diverse content, styles, or audio
and visual qualities present in new, unseen test videos, leading to reduced
highlight detection performance. In this paper, we propose Highlight-TTA, a
test-time adaptation framework for video highlight detection that addresses
this limitation by dynamically adapting the model during testing to better
align with the specific characteristics of each test video, thereby improving
generalization and highlight detection performance. Highlight-TTA is jointly
optimized with an auxiliary task, cross-modality hallucinations, alongside the
primary highlight detection task. We utilize a meta-auxiliary training scheme
to enable effective adaptation through the auxiliary task while enhancing the
primary task. During testing, we adapt the trained model using the auxiliary
task on the test video to further enhance its highlight detection performance.
Extensive experiments with three state-of-the-art highlight detection models
and three benchmark datasets show that the introduction of Highlight-TTA to
these models improves their performance, yielding superior results.

</details>


### [47] [Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification](https://arxiv.org/abs/2508.05489)
*Samuel Räber,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 该研究构建了针对图像有损压缩防御的强白盒和自适应攻击，发现图像重建的“高真实感”显著增加了攻击难度。能生成高真实感图像的压缩模型对攻击更具抵抗力，而低真实感模型则易被攻破。这种鲁棒性并非源于梯度遮蔽，而是由于真实感重建保持了与自然图像的分布一致性，提供了内在的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究认为有损压缩可以防御对抗性扰动，但缺乏全面的攻击评估。本研究旨在填补这一空白，对各种压缩模型进行全面的攻击评估。

Method: 研究构建了强大的白盒和自适应攻击方法，并对多种压缩模型进行了严格的攻击评估，以测试其对抗性鲁棒性。

Result: 攻击评估结果表明，重建图像的“高真实感”显著增加了攻击难度。能够生成逼真、高保真重建图像的压缩模型对所提出的攻击具有更强的抵抗力，而低真实感压缩模型则容易被攻破。分析发现，这种鲁棒性并非由梯度遮蔽引起，而是因为保持了与自然图像的分布一致性的真实感重建提供了内在的鲁棒性。

Conclusion: 本研究揭示了图像重建的真实感对未来对抗性攻击构成了重大障碍，并指出开发更有效的技术来克服这种真实感是进行全面安全评估的关键挑战。

Abstract: Previous work has suggested that preprocessing images through lossy
compression can defend against adversarial perturbations, but comprehensive
attack evaluations have been lacking. In this paper, we construct strong
white-box and adaptive attacks against various compression models and identify
a critical challenge for attackers: high realism in reconstructed images
significantly increases attack difficulty. Through rigorous evaluation across
multiple attack scenarios, we demonstrate that compression models capable of
producing realistic, high-fidelity reconstructions are substantially more
resistant to our attacks. In contrast, low-realism compression models can be
broken. Our analysis reveals that this is not due to gradient masking. Rather,
realistic reconstructions maintaining distributional alignment with natural
images seem to offer inherent robustness. This work highlights a significant
obstacle for future adversarial attacks and suggests that developing more
effective techniques to overcome realism represents an essential challenge for
comprehensive security evaluation.

</details>


### [48] [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](https://arxiv.org/abs/2508.04928)
*Suchisrit Gangopadhyay,Jung-Hee Kim,Xien Chen,Patrick Rim,Hyoungseob Park,Alex Wong*

Main category: cs.CV

TL;DR: 该论文提出一种方法，通过引入“校准令牌”来调整潜在嵌入，将为透视图像训练的单目深度估计器（FMDEs）扩展到鱼眼图像，无需重新训练或微调。


<details>
  <summary>Details</summary>
Motivation: 为透视图像训练的FMDEs在面对鱼眼图像时，由于相机标定（内参、畸变）参数变化引起的协变量偏移，导致深度估计错误。重新训练成本高昂，且图像空间中的传统重标定或投影会引入伪影和损失。

Method: 核心方法是使鱼眼图像的潜在嵌入分布与透视图像的分布对齐。为此，引入了一组“校准令牌”作为轻量级自适应机制，用于调制潜在嵌入以实现对齐。该方法利用FMDEs已有的富有表达力的潜在空间，避免了传统图像空间重标定或投影的负面影响。它是自监督的，不直接需要鱼眼图像，而是利用公开的透视图像数据集，通过将透视图像重标定为鱼眼图像，并在训练期间强制其估计之间的一致性。

Result: 在室内和室外场景中，该方法使用单一的校准令牌集，在多个FMDEs上持续优于现有最先进的方法。

Conclusion: 该研究提供了一种有效、自监督且无需重新训练FMDEs的轻量级方法，使其能够适应鱼眼相机，并通过在潜在空间中进行调整，避免了图像空间操作的负面影响。

Abstract: We propose a method to extend foundational monocular depth estimators
(FMDEs), trained on perspective images, to fisheye images. Despite being
trained on tens of millions of images, FMDEs are susceptible to the covariate
shift introduced by changes in camera calibration (intrinsic, distortion)
parameters, leading to erroneous depth estimates. Our method aligns the
distribution of latent embeddings encoding fisheye images to those of
perspective images, enabling the reuse of FMDEs for fisheye cameras without
retraining or finetuning. To this end, we introduce a set of Calibration Tokens
as a light-weight adaptation mechanism that modulates the latent embeddings for
alignment. By exploiting the already expressive latent space of FMDEs, we posit
that modulating their embeddings avoids the negative impact of artifacts and
loss introduced in conventional recalibration or map projection to a canonical
reference frame in the image space. Our method is self-supervised and does not
require fisheye images but leverages publicly available large-scale perspective
image datasets. This is done by recalibrating perspective images to fisheye
images, and enforcing consistency between their estimates during training. We
evaluate our approach with several FMDEs, on both indoors and outdoors, where
we consistently improve over state-of-the-art methods using a single set of
tokens for both. Code available at:
https://github.com/JungHeeKim29/calibration-token.

</details>


### [49] [Toward Errorless Training ImageNet-1k](https://arxiv.org/abs/2508.04941)
*Bo Deng,Levi Heath*

Main category: cs.CV

TL;DR: 本文描述了一个使用新方法在ImageNet 2012数据集上训练的前馈人工神经网络，达到了98.3%的准确率和99.69%的Top-1准确率，并推测未达100%准确率是由于数据集的双重标注问题。


<details>
  <summary>Details</summary>
Motivation: 旨在展示一种新的训练方法在图像分类任务上的有效性，并通过前馈人工神经网络在ImageNet 2012数据集上实现高准确率。

Method: 使用了一种新的方法 ([5]) 训练了一个前馈人工神经网络。该模型在ImageNet 2012竞赛数据集上进行训练，并使用了322,430,160个参数。

Result: 模型在ImageNet 2012数据集上达到了98.3%的准确率和99.69%的Top-1准确率。在数据集的10个批次分区上，平均有285.9个标签被完美分类。表现最佳的模型拥有322,430,160个参数。

Conclusion: 作者推测模型未能达到100%准确率的原因是数据集存在“双重标注问题”，即数据集中有重复图像但被赋予了不同标签。

Abstract: In this paper, we describe a feedforward artificial neural network trained on
the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy
rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are
perfectly classified over the 10 batch partitions of the dataset. The best
performing model uses 322,430,160 parameters, with 4 decimal places precision.
We conjecture that the reason our model does not achieve a 100% accuracy rate
is due to a double-labeling problem, by which there are duplicate images in the
dataset with different labels.

</details>


### [50] [Accelerating Conditional Prompt Learning via Masked Image Modeling for Vision-Language Models](https://arxiv.org/abs/2508.04942)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: ProMIM是一个即插即用的框架，通过整合掩码图像建模（MIM）到现有视觉语言模型（VLM）的条件提示学习中，有效提升了模型对未知类别的泛化能力，同时计算成本可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本学习中表现出色，但适应新任务通常需要大量资源。现有提示学习技术（如CoOp、CoCoOp）虽然高效，却容易对已知类别过拟合，限制了对未见类别的泛化能力。

Method: ProMIM将掩码图像建模（MIM）集成到现有VLM管道中，采用一种简单有效的掩码策略生成鲁棒的、实例条件化的提示。它仅对可见图像块进行掩码，并利用这些表示来指导提示生成，从而增强特征鲁棒性并缓解过拟合，且不改变现有方法的底层架构。

Result: 在零样本和少样本分类任务中，ProMIM与现有方法结合后，持续提升了泛化性能。它引入的额外计算成本可以忽略不计。

Conclusion: ProMIM为现实世界的视觉语言应用提供了一个实用、轻量级的解决方案，显著提高了现有提示学习方法的泛化能力，尤其是在处理未见类别时。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning but often
require resource-intensive training to adapt to new tasks. Prompt learning
techniques, such as CoOp and CoCoOp, offer efficient adaptation but tend to
overfit to known classes, limiting generalization to unseen categories. We
introduce ProMIM, a plug-and-play framework that enhances conditional prompt
learning by integrating masked image modeling (MIM) into existing VLM
pipelines. ProMIM leverages a simple yet effective masking strategy to generate
robust, instance-conditioned prompts, seamlessly augmenting methods like CoOp
and CoCoOp without altering their core architectures. By masking only visible
image patches and using these representations to guide prompt generation,
ProMIM improves feature robustness and mitigates overfitting, all while
introducing negligible additional computational cost. Extensive experiments
across zero-shot and few-shot classification tasks demonstrate that ProMIM
consistently boosts generalization performance when plugged into existing
approaches, providing a practical, lightweight solution for real-world
vision-language applications.

</details>


### [51] [TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring](https://arxiv.org/abs/2508.04943)
*Zhu Xu,Ting Lei,Zhimin Li,Guan Wang,Qingchao Chen,Yuxin Peng,Yang liu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TRKT（Temporal-enhanced Relation-aware Knowledge Transferring）的方法，用于弱监督动态场景图生成（WS-DSGG），通过利用关系感知知识和时间信息来增强目标检测，解决了外部检测器在动态、关系感知场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的WS-DSGG方法依赖于预训练的外部目标检测器生成伪标签，但这些检测器通常在静态、以对象为中心的图像上训练，在动态、关系感知的视频场景中表现不佳，导致定位不准确和置信度低，从而影响DSGG的性能。

Method: TRKT方法包含两个核心组件：1) 关系感知知识挖掘：利用对象和关系类别解码器生成类别特定的注意力图，突出对象区域和交互区域。接着，提出跨帧注意力增强策略，利用光流信息增强注意力图，使其具有运动感知能力并对运动模糊具有鲁棒性。2) 双流融合模块：将生成的类别特定注意力图与外部检测结果融合，以优化目标定位并提高目标提议的置信度。

Result: TRKT在Action Genome数据集上取得了最先进的性能。

Conclusion: TRKT方法通过引入关系感知知识和时间增强机制，有效解决了弱监督动态场景图生成中外部目标检测器所面临的挑战，显著提升了目标检测的准确性和场景图生成的性能。

Abstract: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each
video frame by detecting objects and predicting their relationships. Weakly
Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized
scene graph from a single frame per video for training. Existing WS-DSGG
methods depend on an off-the-shelf external object detector to generate pseudo
labels for subsequent DSGG training. However, detectors trained on static,
object-centric images struggle in dynamic, relation-aware scenarios required
for DSGG, leading to inaccurate localization and low-confidence proposals. To
address the challenges posed by external object detectors in WS-DSGG, we
propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT)
method, which leverages knowledge to enhance detection in relation-aware
dynamic scenarios. TRKT is built on two key components:(1)Relation-aware
knowledge mining: we first employ object and relation class decoders that
generate category-specific attention maps to highlight both object regions and
interactive areas. Then we propose an Inter-frame Attention Augmentation
strategy that exploits optical flow for neighboring frames to enhance the
attention maps, making them motion-aware and robust to motion blur. This step
yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we
introduce a Dual-stream Fusion Module that integrates category-specific
attention maps into external detections to refine object localization and boost
confidence scores for object proposals. Extensive experiments demonstrate that
TRKT achieves state-of-the-art performance on Action Genome dataset. Our code
is avaliable at https://github.com/XZPKU/TRKT.git.

</details>


### [52] [AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics](https://arxiv.org/abs/2508.04955)
*Stella Su,Marc Harary,Scott J. Rodig,William Lotter*

Main category: cs.CV

TL;DR: AdvDINO是一个领域对抗自监督学习框架，它将梯度反转层集成到DINOv2架构中，以学习对领域偏移具有鲁棒性的特征，特别是在生物医学图像中减轻批次效应，并发现具有生物学意义的表征。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在视觉表征学习中表现出色，但其对领域偏移（数据源之间的系统差异）的鲁棒性尚不确定。这在生物医学成像中尤其关键，因为批次效应会掩盖真实的生物信号，阻碍模型泛化和可解释性。

Method: 提出了AdvDINO框架，通过将梯度反转层整合到DINOv2架构中，促进领域不变特征学习。该方法应用于非小细胞肺癌患者的六通道多重免疫荧光（mIF）全玻片图像。

Result: AdvDINO减轻了玻片特异性偏差，学习到比非对抗基线更鲁棒和具有生物学意义的表征。在超过546万个mIF图像块上，该模型发现了具有独特蛋白质组学特征和预后意义的表型簇，并改善了基于注意力的多实例学习中的生存预测。

Conclusion: AdvDINO有效解决了自监督学习在领域偏移下的鲁棒性问题，尤其在生物医学成像中表现出卓越的性能。该框架不仅适用于mIF数据，还可广泛应用于放射学、遥感和自动驾驶等其他图像领域，以解决领域偏移和标注数据有限的问题。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach for
learning visual representations without manual annotations. However, the
robustness of standard SSL methods to domain shift -- systematic differences
across data sources -- remains uncertain, posing an especially critical
challenge in biomedical imaging where batch effects can obscure true biological
signals. We present AdvDINO, a domain-adversarial self-supervised learning
framework that integrates a gradient reversal layer into the DINOv2
architecture to promote domain-invariant feature learning. Applied to a
real-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide
images from non-small cell lung cancer patients, AdvDINO mitigates
slide-specific biases to learn more robust and biologically meaningful
representations than non-adversarial baselines. Across $>5.46$ million mIF
image tiles, the model uncovers phenotype clusters with distinct proteomic
profiles and prognostic significance, and improves survival prediction in
attention-based multiple instance learning. While demonstrated on mIF data,
AdvDINO is broadly applicable to other imaging domains -- including radiology,
remote sensing, and autonomous driving -- where domain shift and limited
annotated data hinder model generalization and interpretability.

</details>


### [53] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: 本文提出了HOW-Seg，首个用于开放世界点云语义分割（OW-Seg）的人机协作框架，通过稀疏人工标注和迭代反馈，实现了对基类和新类的高质量分割。


<details>
  <summary>Details</summary>
Motivation: 现有的开放世界点云语义分割方法依赖于资源密集型的离线增量学习或密集的标注支持数据，这限制了它们的实用性。为了解决这些局限性，本文提出了新的方法。

Method: HOW-Seg框架直接在查询数据上构建类原型，避免了支持数据与查询数据之间类内分布偏移导致的偏差。它利用稀疏的人工标注作为指导，实现基于原型的基类和新类分割。为了解决初始原型粒度不足的问题，引入了分层原型消歧机制来细化模糊原型。此外，通过密集条件随机场（CRF）优化精炼原型的标签分配，并利用迭代的人机反馈动态改进预测。

Result: 在稀疏标注（如每新类一次点击）下，HOW-Seg在5-shot设置中匹配或超越了最先进的广义少样本分割（GFS-Seg）方法。在使用高级骨干网络（如Stratified Transformer）和更密集标注（如每个子场景10次点击）时，HOW-Seg在S3DIS上达到了85.27%的mIoU，在ScanNetv2上达到了66.37%的mIoU，显著优于其他替代方案。

Conclusion: HOW-Seg证明了人机协作在开放世界点云语义分割中的有效性，通过稀疏的人工指导和迭代优化，能够为基类和新类提供高质量的分割结果，解决了现有方法的实用性限制。

Abstract: Open-world point cloud semantic segmentation (OW-Seg) aims to predict point
labels of both base and novel classes in real-world scenarios. However,
existing methods rely on resource-intensive offline incremental learning or
densely annotated support data, limiting their practicality. To address these
limitations, we propose HOW-Seg, the first human-in-the-loop framework for
OW-Seg. Specifically, we construct class prototypes, the fundamental
segmentation units, directly on the query data, avoiding the prototype bias
caused by intra-class distribution shifts between the support and query data.
By leveraging sparse human annotations as guidance, HOW-Seg enables
prototype-based segmentation for both base and novel classes. Considering the
lack of granularity of initial prototypes, we introduce a hierarchical
prototype disambiguation mechanism to refine ambiguous prototypes, which
correspond to annotations of different classes. To further enrich contextual
awareness, we employ a dense conditional random field (CRF) upon the refined
prototypes to optimize their label assignments. Through iterative human
feedback, HOW-Seg dynamically improves its predictions, achieving high-quality
segmentation for both base and novel classes. Experiments demonstrate that with
sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or
surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)
method under the 5-shot setting. When using advanced backbones (e.g.,
Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),
HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,
significantly outperforming alternatives.

</details>


### [54] [UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS](https://arxiv.org/abs/2508.04968)
*Zhihao Guo,Peng Wang,Zidong Chen,Xiangyu Kong,Yan Lyu,Guanyu Gao,Liangxiu Han*

Main category: cs.CV

TL;DR: 该论文提出一种基于学习不确定性的自适应高斯权重方法，以解决3D高斯泼溅（3DGS）在稀疏视图下容易过拟合的问题，从而提升新颖视图合成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 大多数3DGS方法在渲染时对高斯点进行等权重处理，这导致其容易过拟合，尤其是在稀疏视图场景下，影响了渲染质量。

Method: 引入学习到的不确定性来表征高斯的自适应权重。该不确定性有两个作用：一是指导高斯不透明度的可微分更新，同时保持3DGS管线的完整性；二是通过软可微分dropout正则化，将不确定性转化为连续的丢弃概率，从而控制最终的高斯投影和混合渲染过程。

Result: 在广泛使用的数据集上，该方法在稀疏视图3D合成方面优于现有竞争对手，在大多数数据集中以更少的高斯点实现了更高质量的重建。例如，与DropGaussian相比，在MipNeRF 360数据集上PSNR提高了3.27%。

Conclusion: 通过引入基于学习不确定性的自适应高斯权重，该方法有效解决了3DGS在稀疏视图下的过拟合问题，显著提升了新颖视图合成的质量和效率。

Abstract: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view
synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian
projection and blending. However, Gaussians are treated equally weighted for
rendering in most 3DGS methods, making them prone to overfitting, which is
particularly the case in sparse-view scenarios. To address this, we investigate
how adaptive weighting of Gaussians affects rendering quality, which is
characterised by learned uncertainties proposed. This learned uncertainty
serves two key purposes: first, it guides the differentiable update of Gaussian
opacity while preserving the 3DGS pipeline integrity; second, the uncertainty
undergoes soft differentiable dropout regularisation, which strategically
transforms the original uncertainty into continuous drop probabilities that
govern the final Gaussian projection and blending process for rendering.
Extensive experimental results over widely adopted datasets demonstrate that
our method outperforms rivals in sparse-view 3D synthesis, achieving higher
quality reconstruction with fewer Gaussians in most datasets compared to
existing sparse-view approaches, e.g., compared to DropGaussian, our method
achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

</details>


### [55] [CSRAP: Enhanced Canvas Attention Scheduling for Real-Time Mission Critical Perception](https://arxiv.org/abs/2508.04976)
*Md Iftekharul Islam Sakib,Yigong Hu,Tarek Abdelzaher*

Main category: cs.CV

TL;DR: 本文扩展了基于Canvas的注意力调度方法，通过引入可变尺寸和可变帧率的Canvas帧，有效提升了边缘平台在有限资源下进行高分辨率目标检测的性能（mAP和召回率）和质量/成本权衡。


<details>
  <summary>Details</summary>
Motivation: 在计算资源有限的边缘平台上，实时高分辨率目标检测面临着严苛的延迟约束和资源需求挑战。

Method: 本文在现有基于Canvas的注意力调度基础上进行扩展，允许使用可变尺寸的Canvas帧，并采用可选择的Canvas帧率（可以与原始数据帧率不同）。通过在NVIDIA Jetson Orin Nano上运行YOLOv11，并使用Waymo开放数据集的视频帧进行评估。

Result: 研究结果表明，增加的自由度（可变尺寸和帧率）改善了可实现的质量/成本权衡，使得平均精度（mAP）和召回率相对于现有技术持续更高。

Conclusion: 通过引入Canvas帧的可变尺寸和可变帧率，该方法显著提升了边缘平台实时高分辨率目标检测的性能，并在质量/成本之间取得了更好的平衡。

Abstract: Real-time perception on edge platforms faces a core challenge: executing
high-resolution object detection under stringent latency constraints on limited
computing resources. Canvas-based attention scheduling was proposed in earlier
work as a mechanism to reduce the resource demands of perception subsystems. It
consolidates areas of interest in an input data frame onto a smaller area,
called a canvas frame, that can be processed at the requisite frame rate. This
paper extends prior canvas-based attention scheduling literature by (i)
allowing for variable-size canvas frames and (ii) employing selectable canvas
frame rates that may depart from the original data frame rate. We evaluate our
solution by running YOLOv11, as the perception module, on an NVIDIA Jetson Orin
Nano to inspect video frames from the Waymo Open Dataset. Our results show that
the additional degrees of freedom improve the attainable quality/cost
trade-offs, thereby allowing for a consistently higher mean average precision
(mAP) and recall with respect to the state of the art.

</details>


### [56] [Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression](https://arxiv.org/abs/2508.04979)
*Zheng Chen,Mingde Zhou,Jinpei Guo,Jiale Yuan,Yifei Ji,Yulun Zhang*

Main category: cs.CV

TL;DR: SODEC是一种新型的单步扩散图像压缩模型，解决了现有扩散模型解码延迟高和保真度差的问题，显著提升了编解码速度和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的图像压缩方法存在两个主要缺点：1) 由于多步采样导致解码延迟过高；2) 过度依赖生成先验导致保真度较差。

Method: SODEC提出以下方法：1) 利用预训练的VAE模型生成信息丰富的潜在表示，从而使多步细化变得不必要；2) 将迭代去噪过程替换为单步解码；3) 引入保真度引导模块，以确保输出忠实于原始图像；4) 设计了速率退火训练策略，以实现在极低比特率下的有效训练。

Result: SODEC在实验中显著优于现有方法，实现了卓越的速率-失真-感知性能。与之前的基于扩散的压缩模型相比，SODEC的解码速度提高了20倍以上。

Conclusion: SODEC通过单步解码、保真度引导和速率退火训练策略，有效解决了扩散图像压缩中的延迟和保真度问题，在性能上取得了显著提升。

Abstract: Diffusion-based image compression has demonstrated impressive perceptual
performance. However, it suffers from two critical drawbacks: (1) excessive
decoding latency due to multi-step sampling, and (2) poor fidelity resulting
from over-reliance on generative priors. To address these issues, we propose
SODEC, a novel single-step diffusion image compression model. We argue that in
image compression, a sufficiently informative latent renders multi-step
refinement unnecessary. Based on this insight, we leverage a pre-trained
VAE-based model to produce latents with rich information, and replace the
iterative denoising process with a single-step decoding. Meanwhile, to improve
fidelity, we introduce the fidelity guidance module, encouraging output that is
faithful to the original image. Furthermore, we design the rate annealing
training strategy to enable effective training under extremely low bitrates.
Extensive experiments show that SODEC significantly outperforms existing
methods, achieving superior rate-distortion-perception performance. Moreover,
compared to previous diffusion-based compression models, SODEC improves
decoding speed by more than 20$\times$. Code is released at:
https://github.com/zhengchen1999/SODEC.

</details>


### [57] [Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion](https://arxiv.org/abs/2508.04984)
*Shenglun Chen,Xinzhu Ma,Hong Zhang,Haojie Li,Zhihui Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的深度补全框架，利用深度基础模型提取环境线索，并结合双空间传播和可学习校正模块，在无需大规模训练的情况下，显著提高了模型在域外（OOD）场景下的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的深度补全模型依赖于有限的精心准备数据，导致在域外（OOD）场景中性能显著下降。而最近的深度基础模型通过大规模训练在单目深度估计中展现出卓越的鲁棒性，因此利用它们来增强深度补全模型的鲁棒性是一个有前景的解决方案。

Method: 该框架利用深度基础模型从RGB图像中提取环境线索（包括结构和语义上下文）来指导稀疏深度信息的传播。设计了一种无学习参数的双空间传播方法，在3D和2D空间中有效传播稀疏深度，以保持几何结构和局部一致性。引入一个可学习的校正模块，逐步调整深度预测以接近真实深度。

Result: 该框架在NYUv2和KITTI数据集上进行训练，并在16个其他数据集上进行广泛评估。结果表明，该框架在OOD场景中表现出色，优于现有的最先进深度补全方法。

Conclusion: 本文提出的深度补全框架通过有效利用深度基础模型，并结合创新的双空间传播和可学习校正机制，在无需大规模训练的前提下，显著提升了模型在各种域外场景中的鲁棒性和性能，超越了现有技术水平。

Abstract: Depth completion is a pivotal challenge in computer vision, aiming at
reconstructing the dense depth map from a sparse one, typically with a paired
RGB image. Existing learning based models rely on carefully prepared but
limited data, leading to significant performance degradation in
out-of-distribution (OOD) scenarios. Recent foundation models have demonstrated
exceptional robustness in monocular depth estimation through large-scale
training, and using such models to enhance the robustness of depth completion
models is a promising solution. In this work, we propose a novel depth
completion framework that leverages depth foundation models to attain
remarkable robustness without large-scale training. Specifically, we leverage a
depth foundation model to extract environmental cues, including structural and
semantic context, from RGB images to guide the propagation of sparse depth
information into missing regions. We further design a dual-space propagation
approach, without any learnable parameters, to effectively propagates sparse
depth in both 3D and 2D spaces to maintain geometric structure and local
consistency. To refine the intricate structure, we introduce a learnable
correction module to progressively adjust the depth prediction towards the real
depth. We train our model on the NYUv2 and KITTI datasets as in-distribution
datasets and extensively evaluate the framework on 16 other datasets. Our
framework performs remarkably well in the OOD scenarios and outperforms
existing state-of-the-art depth completion methods. Our models are released in
https://github.com/shenglunch/PSD.

</details>


### [58] [Unified modality separation: A vision-language framework for unsupervised domain adaptation](https://arxiv.org/abs/2508.04987)
*Xinyao Li,Jingjing Li,Zhekai Du,Lei Zhu,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种统一的模态分离框架，用于解决预训练视觉-语言模型（VLMs）在无监督域适应（UDA）中存在的模态间隙问题，通过分离并协同处理模态特有和模态不变知识来提升性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉-语言模型（VLMs）在无监督域适应（UDA）中展现出潜力，但其固有的“模态间隙”导致直接UDA仅能传递模态不变知识，从而限制了目标域的性能。

Method: 提出一个统一的模态分离框架：在训练时，将VLM特征分解为模态特有和模态不变组件并分别处理；在测试时，自动确定模态自适应的集成权重以最大化各组件的协同作用。此外，设计了一个模态差异度量指标来分类样本（模态不变、模态特有、不确定），利用模态不变样本进行跨模态对齐，并对不确定样本进行标注以增强模型能力。方法基于提示调优技术。

Result: 相比现有方法，该方法在性能上取得了高达9%的提升，同时计算效率提高了9倍。在各种骨干网络、基线、数据集和适应设置下的广泛实验和分析都证明了其设计的有效性。

Conclusion: 所提出的统一模态分离框架有效解决了VLM在UDA中的模态间隙问题，通过精细化处理模态特有和模态不变知识，显著提升了模型性能和计算效率。

Abstract: Unsupervised domain adaptation (UDA) enables models trained on a labeled
source domain to handle new unlabeled domains. Recently, pre-trained
vision-language models (VLMs) have demonstrated promising zero-shot performance
by leveraging semantic information to facilitate target tasks. By aligning
vision and text embeddings, VLMs have shown notable success in bridging domain
gaps. However, inherent differences naturally exist between modalities, which
is known as modality gap. Our findings reveal that direct UDA with the presence
of modality gap only transfers modality-invariant knowledge, leading to
suboptimal target performance. To address this limitation, we propose a unified
modality separation framework that accommodates both modality-specific and
modality-invariant components. During training, different modality components
are disentangled from VLM features then handled separately in a unified manner.
At test time, modality-adaptive ensemble weights are automatically determined
to maximize the synergy of different components. To evaluate instance-level
modality characteristics, we design a modality discrepancy metric to categorize
samples into modality-invariant, modality-specific, and uncertain ones. The
modality-invariant samples are exploited to facilitate cross-modal alignment,
while uncertain ones are annotated to enhance model capabilities. Building upon
prompt tuning techniques, our methods achieve up to 9% performance gain with 9
times of computational efficiencies. Extensive experiments and analysis across
various backbones, baselines, datasets and adaptation settings demonstrate the
efficacy of our design.

</details>


### [59] [Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks](https://arxiv.org/abs/2508.04988)
*Yue Li,Weifan Wang,Tai Sing Lee*

Main category: cs.CV

TL;DR: 本研究使用基于ViT的自编码器和LoRA实现的快速权重，探索了熟悉度训练如何在深度神经网络的早期层中引入全局上下文敏感性，并发现这种训练能重塑神经流形、对齐表征并拓宽注意力范围。


<details>
  <summary>Details</summary>
Motivation: 神经生理学研究表明，早期视觉皮层能快速学习全局图像上下文，表现为对熟悉而非新颖上下文的稀疏化和平均活动减少，这主要归因于局部循环交互。本研究旨在从功能角度，探讨熟悉度训练如何在深度神经网络的早期层中诱导对全局上下文的敏感性，并提出一种计算模型。

Method: 研究采用基于Vision Transformer (ViT) 的自编码器来模拟视觉皮层。通过熟悉度训练，探索其对网络早期层的影响。假设快速学习通过编码瞬时或短期记忆痕迹的“快速权重”实现，并使用低秩适应（LoRA）在每个Transformer层中实现这些快速权重。

Result: (1) 所提出的ViT自编码器的自注意力回路执行的流形变换与熟悉效应的神经电路模型相似。(2) 熟悉度训练使早期层中的潜在表征与包含全局上下文信息的顶层表征对齐。(3) 熟悉度训练拓宽了记忆图像上下文中的自注意力范围。(4) 基于LoRA的快速权重显著增强了这些效应。

Conclusion: 这些发现表明，熟悉度训练将全局敏感性引入到分层网络的早期层，并且混合了快速和慢速权重的架构可能为研究大脑中快速全局上下文学习提供一个可行的计算模型。

Abstract: Recent neurophysiological studies have revealed that the early visual cortex
can rapidly learn global image context, as evidenced by a sparsification of
population responses and a reduction in mean activity when exposed to familiar
versus novel image contexts. This phenomenon has been attributed primarily to
local recurrent interactions, rather than changes in feedforward or feedback
pathways, supported by both empirical findings and circuit-level modeling.
Recurrent neural circuits capable of simulating these effects have been shown
to reshape the geometry of neural manifolds, enhancing robustness and
invariance to irrelevant variations. In this study, we employ a Vision
Transformer (ViT)-based autoencoder to investigate, from a functional
perspective, how familiarity training can induce sensitivity to global context
in the early layers of a deep neural network. We hypothesize that rapid
learning operates via fast weights, which encode transient or short-term memory
traces, and we explore the use of Low-Rank Adaptation (LoRA) to implement such
fast weights within each Transformer layer. Our results show that (1) The
proposed ViT-based autoencoder's self-attention circuit performs a manifold
transform similar to a neural circuit model of the familiarity effect. (2)
Familiarity training aligns latent representations in early layers with those
in the top layer that contains global context information. (3) Familiarity
training broadens the self-attention scope within the remembered image context.
(4) These effects are significantly amplified by LoRA-based fast weights.
Together, these findings suggest that familiarity training introduces global
sensitivity to earlier layers in a hierarchical network, and that a hybrid
fast-and-slow weight architecture may provide a viable computational model for
studying rapid global context learning in the brain.

</details>


### [60] [Attribute Guidance With Inherent Pseudo-label For Occluded Person Re-identification](https://arxiv.org/abs/2508.04998)
*Rui Zhi,Zhen Yang,Haiyang Zhang*

Main category: cs.CV

TL;DR: 该论文提出AG-ReID框架，利用预训练模型提取细粒度语义属性，通过伪标签生成和双重引导机制，显著提升了遮挡和细微差异场景下行人重识别（Re-ID）的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉-语言模型在行人重识别任务中表现良好，但在遮挡场景下，由于其侧重整体图像语义而忽略细粒度属性信息，导致性能不佳，难以处理部分遮挡或细微外观差异的行人。

Method: AG-ReID框架分为两阶段：1. 生成细粒度属性伪标签，捕获微妙的视觉特征。2. 引入双重引导机制，结合整体和细粒度属性信息来增强图像特征提取。

Result: AG-ReID在多个常用行人重识别数据集上取得了最先进（SOTA）的结果，在处理遮挡和细微属性差异方面表现出显著改进，同时在标准Re-ID场景中保持了竞争力。

Conclusion: AG-ReID通过有效利用预训练模型内在能力提取细粒度属性，成功解决了遮挡场景下行人重识别的挑战，显著提升了模型的识别能力。

Abstract: Person re-identification (Re-ID) aims to match person images across different
camera views, with occluded Re-ID addressing scenarios where pedestrians are
partially visible. While pre-trained vision-language models have shown
effectiveness in Re-ID tasks, they face significant challenges in occluded
scenarios by focusing on holistic image semantics while neglecting fine-grained
attribute information. This limitation becomes particularly evident when
dealing with partially occluded pedestrians or when distinguishing between
individuals with subtle appearance differences. To address this limitation, we
propose Attribute-Guide ReID (AG-ReID), a novel framework that leverages
pre-trained models' inherent capabilities to extract fine-grained semantic
attributes without additional data or annotations. Our framework operates
through a two-stage process: first generating attribute pseudo-labels that
capture subtle visual characteristics, then introducing a dual-guidance
mechanism that combines holistic and fine-grained attribute information to
enhance image feature extraction. Extensive experiments demonstrate that
AG-ReID achieves state-of-the-art results on multiple widely-used Re-ID
datasets, showing significant improvements in handling occlusions and subtle
attribute differences while maintaining competitive performance on standard
Re-ID scenarios.

</details>


### [61] [CRAM: Large-scale Video Continual Learning with Bootstrapped Compression](https://arxiv.org/abs/2508.05001)
*Shivani Mall,Joao F. Henriques*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CRAM（Continually Refreshed Amodal Memory）的视频持续学习方法，通过存储视频编码（嵌入）而非原始视频来大幅减少内存占用，并引入了一种刷新机制以应对在线训练视频压缩器中的灾难性遗忘，从而在大型视频持续学习基准上取得了优于现有技术且内存占用显著降低的性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）旨在使神经网络能从连续输入流中学习，而非依赖于对整个数据集的随机访问，这能减少存储需求并提高部署系统的自给自足能力。然而，视频持续学习面临巨大挑战，主要在于视频数据的高内存需求，尤其是在处理长视频和连续流时，这与常见的排练缓冲区大小限制相悖。

Method: 该研究采用基于排练的方法，并提出使用压缩视觉，即存储视频编码（嵌入）而非原始视频输入，从一个滚动缓冲区中进行独立同分布（IID）采样来训练视频分类器。为了解决在线训练的视频压缩器面临的灾难性遗忘问题，他们提出了一种刷新视频编码的方案，该方案涉及使用旧版本网络进行解压缩，然后使用新版本网络进行重新压缩。该方法被命名为Continually Refreshed Amodal Memory (CRAM)。

Result: 研究人员将现有的视频持续学习基准扩展到大规模设置，包括EpicKitchens-100和Kinetics-700，成功地在2GB以下的内存中存储了数千个相对较长的视频。实验结果表明，该视频持续学习方法在显著减少内存占用的同时，性能优于现有技术。

Conclusion: CRAM方法通过创新的视频编码存储和刷新机制，有效解决了大规模视频持续学习中的内存瓶颈和灾难性遗忘问题，使得神经网络能够高效地从连续视频流中学习，并在实际应用中展现出卓越的性能和内存效率。

Abstract: Continual learning (CL) promises to allow neural networks to learn from
continuous streams of inputs, instead of IID (independent and identically
distributed) sampling, which requires random access to a full dataset. This
would allow for much smaller storage requirements and self-sufficiency of
deployed systems that cope with natural distribution shifts, similarly to
biological learning. We focus on video CL employing a rehearsal-based approach,
which reinforces past samples from a memory buffer. We posit that part of the
reason why practical video CL is challenging is the high memory requirements of
video, further exacerbated by long-videos and continual streams, which are at
odds with the common rehearsal-buffer size constraints. To address this, we
propose to use compressed vision, i.e. store video codes (embeddings) instead
of raw inputs, and train a video classifier by IID sampling from this rolling
buffer. Training a video compressor online (so not depending on any pre-trained
networks) means that it is also subject to catastrophic forgetting. We propose
a scheme to deal with this forgetting by refreshing video codes, which requires
careful decompression with a previous version of the network and recompression
with a new one. We name our method Continually Refreshed Amodal Memory (CRAM).
We expand current video CL benchmarks to large-scale settings, namely
EpicKitchens-100 and Kinetics-700, storing thousands of relatively long videos
in under 2 GB, and demonstrate empirically that our video CL method outperforms
prior art with a significantly reduced memory footprint.

</details>


### [62] [Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2508.05008)
*Xusheng Liang,Lihua Zhou,Nianxin Li,Miao Xu,Ziyang Song,Dong Yi,Jinlin Wu,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 针对医学图像分割中的域泛化问题，本文提出了多模态因果驱动表示学习（MCDRL）框架，通过结合CLIP和因果推断来消除域特异性变异的影响，从而提高分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在计算机视觉任务中表现出卓越的零样本能力，但在医学影像中应用受限，因为医学数据的高度变异性和复杂性，特别是设备差异、程序伪影和成像模式等混杂因素导致的显著域偏移，导致模型在未见域上泛化能力差。

Method: MCDRL框架分两步实现：1. 利用CLIP的跨模态能力和文本提示识别病灶区域，并构建一个混杂因素字典来表示域特异性变异。2. 训练一个因果干预网络，利用该字典识别并消除这些域特异性变异的影响，同时保留对分割任务至关重要的解剖结构信息。

Result: 大量实验表明，MCDRL始终优于竞争方法，产生了卓越的分割精度，并表现出鲁棒的泛化能力。

Conclusion: MCDRL通过将因果推断与VLM相结合，有效解决了医学图像分割中的域泛化问题，成功减轻了域特异性变异的影响，提升了模型性能和泛化性。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot capabilities in various computer vision tasks. However, their
application to medical imaging remains challenging due to the high variability
and complexity of medical data. Specifically, medical images often exhibit
significant domain shifts caused by various confounders, including equipment
differences, procedure artifacts, and imaging modes, which can lead to poor
generalization when models are applied to unseen domains. To address this
limitation, we propose Multimodal Causal-Driven Representation Learning
(MCDRL), a novel framework that integrates causal inference with the VLM to
tackle domain generalization in medical image segmentation. MCDRL is
implemented in two steps: first, it leverages CLIP's cross-modal capabilities
to identify candidate lesion regions and construct a confounder dictionary
through text prompts, specifically designed to represent domain-specific
variations; second, it trains a causal intervention network that utilizes this
dictionary to identify and eliminate the influence of these domain-specific
variations while preserving the anatomical structural information critical for
segmentation tasks. Extensive experiments demonstrate that MCDRL consistently
outperforms competing methods, yielding superior segmentation accuracy and
exhibiting robust generalizability.

</details>


### [63] [Skin-SOAP: A Weakly Supervised Framework for Generating Structured SOAP Notes](https://arxiv.org/abs/2508.05019)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: 该研究提出了一个名为skin-SOAP的弱监督多模态框架，能从有限的图像和文本输入生成临床SOAP笔记，旨在减轻医生负担并提高诊断效率。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球最常见的癌症，治疗成本高昂。早期诊断和及时治疗至关重要。然而，医生手动生成详细的SOAP笔记耗时费力，导致职业倦怠。因此，需要一种自动化、高效的临床文档生成方法。

Method: 本文提出了skin-SOAP框架，这是一个弱监督的多模态系统。它利用有限的输入（包括病变图像和稀疏临床文本）来生成结构化的SOAP笔记。该方法减少了对大量手动标注数据的依赖。同时，引入了两个新的评估指标：MedConceptEval（评估与医学概念的语义对齐）和Clinical Coherence Score (CCS)（评估与输入特征的一致性）来衡量临床相关性。

Result: skin-SOAP在关键临床相关性指标上取得了与GPT-4o、Claude和DeepSeek Janus Pro等大型模型相当的性能。

Conclusion: skin-SOAP框架能够实现可扩展、临床接地的文档生成，有效减轻临床医生的负担，并降低对大量标注数据的需求，有助于改善皮肤癌的诊断流程。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. Early diagnosis, accurate
and timely treatment are critical to improving patient survival rates. In
clinical settings, physicians document patient visits using detailed SOAP
(Subjective, Objective, Assessment, and Plan) notes. However, manually
generating these notes is labor-intensive and contributes to clinician burnout.
In this work, we propose skin-SOAP, a weakly supervised multimodal framework to
generate clinically structured SOAP notes from limited inputs, including lesion
images and sparse clinical text. Our approach reduces reliance on manual
annotations, enabling scalable, clinically grounded documentation while
alleviating clinician burden and reducing the need for large annotated data.
Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek
Janus Pro across key clinical relevance metrics. To evaluate this clinical
relevance, we introduce two novel metrics MedConceptEval and Clinical Coherence
Score (CCS) which assess semantic alignment with expert medical concepts and
input features, respectively.

</details>


### [64] [A Novel Image Similarity Metric for Scene Composition Structure](https://arxiv.org/abs/2508.05037)
*Md Redwanul Haque,Manzur Murshed,Manoranjan Paul,Tsz-Kwan Lee*

Main category: cs.CV

TL;DR: SCSSIM是一种新颖的、分析性的、免训练的度量标准，通过利用图像的立方体分层分区导出的统计度量来量化场景构成结构（SCS）的保留情况，用于评估生成式AI模型的图像质量。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型需要超越人类感知的图像质量评估新方法，特别是在评估图像底层场景构成结构（SCS）的完整性方面。传统的像素级、感知级和基于神经网络的度量方法在评估结构保真度方面存在不足，前者对视觉噪声敏感，后者偏重美学，而神经网络方法则存在训练开销和泛化问题。

Method: 提出了SCSSIM（SCS相似性指数度量），这是一种新颖、分析性且免训练的度量标准。它通过利用图像的立方体分层分区（Cuboidal hierarchical partitioning）导出的统计度量来量化SCS的保留情况，从而稳健地捕捉非基于对象的结构关系。

Result: 实验表明，SCSSIM对非构成性失真具有高度不变性，能准确反映未改变的SCS；同时，对于构成性失真，它显示出强烈的单调下降，能精确指示SCS何时被改变。与现有度量标准相比，SCSSIM在结构评估方面表现出卓越的性能。

Conclusion: SCSSIM是开发和评估生成模型以确保场景构成完整性的宝贵工具。

Abstract: The rapid advancement of generative AI models necessitates novel methods for
evaluating image quality that extend beyond human perception. A critical
concern for these models is the preservation of an image's underlying Scene
Composition Structure (SCS), which defines the geometric relationships among
objects and the background, their relative positions, sizes, orientations, etc.
Maintaining SCS integrity is paramount for ensuring faithful and structurally
accurate GenAI outputs. Traditional image similarity metrics often fall short
in assessing SCS. Pixel-level approaches are overly sensitive to minor visual
noise, while perception-based metrics prioritize human aesthetic appeal,
neither adequately capturing structural fidelity. Furthermore, recent
neural-network-based metrics introduce training overheads and potential
generalization issues. We introduce the SCS Similarity Index Measure (SCSSIM),
a novel, analytical, and training-free metric that quantifies SCS preservation
by exploiting statistical measures derived from the Cuboidal hierarchical
partitioning of images, robustly capturing non-object-based structural
relationships. Our experiments demonstrate SCSSIM's high invariance to
non-compositional distortions, accurately reflecting unchanged SCS. Conversely,
it shows a strong monotonic decrease for compositional distortions, precisely
indicating when SCS has been altered. Compared to existing metrics, SCSSIM
exhibits superior properties for structural evaluation, making it an invaluable
tool for developing and evaluating generative models, ensuring the integrity of
scene composition.

</details>


### [65] [HAMoBE: Hierarchical and Adaptive Mixture of Biometric Experts for Video-based Person ReID](https://arxiv.org/abs/2508.05038)
*Yiyang Su,Yunping Shi,Feng Liu,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的层次自适应生物特征专家混合（HAMoBE）框架，用于视频行人重识别（ReID），通过利用预训练大模型的多层特征，并自适应地整合外观、体态和步态等关键生物特征，从而显著提升了ReID性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频行人重识别方法常常忽视了从查询-图库对中的视频中识别并选择最具判别力的特征，这对于在动态环境中进行有效匹配至关重要。

Method: 本文提出了HAMoBE框架，该框架利用预训练大型模型（如CLIP）的多层特征，并模拟人类感知机制，独立建模外观、静态体态和动态步态等关键生物特征，并进行自适应整合。HAMoBE包含两个级别：第一级从冻结大模型提供的多层表示中提取低级特征；第二级由专注于长期、短期和时间特征的专业专家组成。此外，引入了一个新的双输入决策门控网络，根据输入场景的相关性动态调整每个专家的贡献。

Result: 在MEVID等基准测试上的广泛评估表明，该方法显著提升了性能（例如，Rank-1准确率提高了13.0%）。

Conclusion: HAMoBE框架通过有效识别和自适应整合多层和多模态生物特征，解决了视频行人重识别中特征选择的挑战，实现了显著的性能提升。

Abstract: Recently, research interest in person re-identification (ReID) has
increasingly focused on video-based scenarios, which are essential for robust
surveillance and security in varied and dynamic environments. However, existing
video-based ReID methods often overlook the necessity of identifying and
selecting the most discriminative features from both videos in a query-gallery
pair for effective matching. To address this issue, we propose a novel
Hierarchical and Adaptive Mixture of Biometric Experts (HAMoBE) framework,
which leverages multi-layer features from a pre-trained large model (e.g.,
CLIP) and is designed to mimic human perceptual mechanisms by independently
modeling key biometric features--appearance, static body shape, and dynamic
gait--and adaptively integrating them. Specifically, HAMoBE includes two
levels: the first level extracts low-level features from multi-layer
representations provided by the frozen large model, while the second level
consists of specialized experts focusing on long-term, short-term, and temporal
features. To ensure robust matching, we introduce a new dual-input decision
gating network that dynamically adjusts the contributions of each expert based
on their relevance to the input scenarios. Extensive evaluations on benchmarks
like MEVID demonstrate that our approach yields significant performance
improvements (e.g., +13.0% Rank-1 accuracy).

</details>


### [66] [Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?](https://arxiv.org/abs/2508.05053)
*Parth Thakkar,Ankush Agarwal,Prasad Kasu,Pulkit Bansal,Chaitanya Devaguptapu*

Main category: cs.CV

TL;DR: 本文引入了NiM基准测试和Spot-IT方法，旨在评估和提升多模态大语言模型（MLLMs）在复杂文档中识别和推理细粒度信息的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在文档理解方面表现出色，但它们在复杂文档中定位和推理细粒度细节的能力（如在菜单中查找营养信息或在报纸中识别免责声明）仍未得到充分研究。这类似于在图像中寻找“针”（NiM），现有模型在此方面存在不足。

Method: 1. 引入了NiM（Finding Needles in Images）基准测试，这是一个精心策划的基准，涵盖报纸、菜单和讲义图像等多种真实世界文档，专门用于评估MLLMs在这些复杂任务中的能力。2. 提出了Spot-IT，一种简单而有效的方法，通过智能补丁选择和高斯注意力机制来增强MLLMs的能力，其灵感来源于人类在搜索文档时如何放大和聚焦。

Result: 广泛的实验揭示了当前MLLMs在处理细粒度文档理解任务方面的能力和局限性。同时，Spot-IT方法被证明是有效的，它在基线方法上取得了显著改进，特别是在需要从复杂布局中精确提取细节的场景中。

Conclusion: Spot-IT方法能有效提升MLLMs在复杂文档中进行细粒度信息理解和提取的能力，解决了当前MLLMs在此领域面临的挑战。

Abstract: While Multi-modal Large Language Models (MLLMs) have shown impressive
capabilities in document understanding tasks, their ability to locate and
reason about fine-grained details within complex documents remains
understudied. Consider searching a restaurant menu for a specific nutritional
detail or identifying a disclaimer in a lengthy newspaper article tasks that
demand careful attention to small but significant details within a broader
narrative, akin to Finding Needles in Images (NiM). To address this gap, we
introduce NiM, a carefully curated benchmark spanning diverse real-world
documents including newspapers, menus, and lecture images, specifically
designed to evaluate MLLMs' capability in these intricate tasks. Building on
this, we further propose Spot-IT, a simple yet effective approach that enhances
MLLMs capability through intelligent patch selection and Gaussian attention,
motivated from how humans zoom and focus when searching documents. Our
extensive experiments reveal both the capabilities and limitations of current
MLLMs in handling fine-grained document understanding tasks, while
demonstrating the effectiveness of our approach. Spot-IT achieves significant
improvements over baseline methods, particularly in scenarios requiring precise
detail extraction from complex layouts.

</details>


### [67] [DualMat: PBR Material Estimation via Coherent Dual-Path Diffusion](https://arxiv.org/abs/2508.05060)
*Yifeng Huang,Zhang Chen,Yi Xu,Minh Hoai,Zhong Li*

Main category: cs.CV

TL;DR: DualMat是一个新颖的双路径扩散框架，用于在复杂光照条件下从单张图像估计PBR材质，通过优化路径和材质路径结合特征蒸馏和整流流实现高效高质量预测。


<details>
  <summary>Details</summary>
Motivation: 从单张图像在复杂光照条件下准确估计物理渲染（PBR）材质是一项具有挑战性的任务。

Method: 该方法采用双路径扩散框架，包括：1) 一个利用预训练视觉知识的RGB潜在空间中的反照率优化路径；2) 一个用于精确金属度和粗糙度估计的紧凑潜在空间中的材质专用路径。通过引入特征蒸馏确保两条路径预测的一致性。采用整流流（rectified flow）以减少推理步骤并提高效率。通过基于块的估计和跨视图注意力，扩展到高分辨率和多视图输入。

Result: DualMat在Objaverse和真实世界数据上均达到最先进的性能，显著优于现有方法。反照率估计提升高达28%，金属-粗糙度预测误差减少39%。

Conclusion: DualMat提供了一种有效且高效的PBR材质估计方法，能够从单张图像在复杂光照下生成高质量结果，并能无缝集成到图像到3D的流程中。

Abstract: We present DualMat, a novel dual-path diffusion framework for estimating
Physically Based Rendering (PBR) materials from single images under complex
lighting conditions. Our approach operates in two distinct latent spaces: an
albedo-optimized path leveraging pretrained visual knowledge through RGB latent
space, and a material-specialized path operating in a compact latent space
designed for precise metallic and roughness estimation. To ensure coherent
predictions between the albedo-optimized and material-specialized paths, we
introduce feature distillation during training. We employ rectified flow to
enhance efficiency by reducing inference steps while maintaining quality. Our
framework extends to high-resolution and multi-view inputs through patch-based
estimation and cross-view attention, enabling seamless integration into
image-to-3D pipelines. DualMat achieves state-of-the-art performance on both
Objaverse and real-world data, significantly outperforming existing methods
with up to 28% improvement in albedo estimation and 39% reduction in
metallic-roughness prediction errors.

</details>


### [68] [Decoupling Continual Semantic Segmentation](https://arxiv.org/abs/2508.05065)
*Yifu Guo,Yuquan Lu,Wentao Zhang,Zishan Xu,Dexia Chen,Siyu Zhang,Yizhe Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: DecoupleCSS是一个两阶段的持续语义分割框架，通过解耦类别感知检测和类别无关分割，有效解决了灾难性遗忘问题，实现了更好的知识保留和适应性，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续语义分割方法通常采用单阶段编码器-解码器架构，导致分割掩码和类别标签紧密耦合，引发新旧类别学习间的干扰，进而导致灾难性遗忘和次优的保留-可塑性平衡。

Method: DecoupleCSS是一个两阶段框架。第一阶段使用LoRA调整的预训练文本和图像编码器来编码类别特定信息并生成位置感知提示，实现类别感知检测。第二阶段利用Segment Anything Model (SAM) 生成精确的分割掩码，确保分割知识在新旧类别之间共享，实现类别无关分割。

Result: 该方法改善了持续语义分割中知识保留和适应性之间的平衡，并在各种具有挑战性的任务中取得了最先进的性能。

Conclusion: DecoupleCSS通过其创新的两阶段解耦方法，成功解决了持续语义分割中的关键挑战，有效平衡了知识保留和新知识学习。

Abstract: Continual Semantic Segmentation (CSS) requires learning new classes without
forgetting previously acquired knowledge, addressing the fundamental challenge
of catastrophic forgetting in dense prediction tasks. However, existing CSS
methods typically employ single-stage encoder-decoder architectures where
segmentation masks and class labels are tightly coupled, leading to
interference between old and new class learning and suboptimal
retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage
framework for CSS. By decoupling class-aware detection from class-agnostic
segmentation, DecoupleCSS enables more effective continual learning, preserving
past knowledge while learning new classes. The first stage leverages
pre-trained text and image encoders, adapted using LoRA, to encode
class-specific information and generate location-aware prompts. In the second
stage, the Segment Anything Model (SAM) is employed to produce precise
segmentation masks, ensuring that segmentation knowledge is shared across both
new and previous classes. This approach improves the balance between retention
and adaptability in CSS, achieving state-of-the-art performance across a
variety of challenging tasks. Our code is publicly available at:
https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.

</details>


### [69] [FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer](https://arxiv.org/abs/2508.05069)
*Jian Zhu,Shanyuan Liu,Liuzhuozheng Li,Yue Gong,He Wang,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin,Yang Xu*

Main category: cs.CV

TL;DR: FLUX-Makeup是一种高保真、身份一致且鲁棒的妆容迁移框架，它无需额外的面部控制模块，通过直接利用源参考图像对和创新的特征注入器实现卓越性能，并辅以高质量的数据生成管线。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GAN和扩散模型的方法在妆容迁移中常依赖辅助的面部控制组件来保持身份一致性，但这些组件往往引入额外误差，导致迁移效果不理想。

Method: 本文提出了FLUX-Makeup框架，基于FLUX-Kontext，将源图像作为原生条件输入。引入了RefLoRAInjector，一个轻量级妆容特征注入器，用于高效全面地提取妆容相关信息。同时，设计了一个鲁棒且可扩展的数据生成管线，以提供更高质量的配对妆容数据集进行训练。

Result: FLUX-Makeup在广泛的实验中展现出最先进的性能，并在各种场景下表现出强大的鲁棒性。其生成的数据集质量显著超越了所有现有数据集。

Conclusion: FLUX-Makeup成功克服了传统妆容迁移方法对辅助组件的依赖，通过创新的架构设计和高质量数据生成，实现了高保真、身份一致且鲁棒的妆容迁移效果，达到了SOTA水平。

Abstract: Makeup transfer aims to apply the makeup style from a reference face to a
target face and has been increasingly adopted in practical applications.
Existing GAN-based approaches typically rely on carefully designed loss
functions to balance transfer quality and facial identity consistency, while
diffusion-based methods often depend on additional face-control modules or
algorithms to preserve identity. However, these auxiliary components tend to
introduce extra errors, leading to suboptimal transfer results. To overcome
these limitations, we propose FLUX-Makeup, a high-fidelity,
identity-consistent, and robust makeup transfer framework that eliminates the
need for any auxiliary face-control components. Instead, our method directly
leverages source-reference image pairs to achieve superior transfer
performance. Specifically, we build our framework upon FLUX-Kontext, using the
source image as its native conditional input. Furthermore, we introduce
RefLoRAInjector, a lightweight makeup feature injector that decouples the
reference pathway from the backbone, enabling efficient and comprehensive
extraction of makeup-related information. In parallel, we design a robust and
scalable data generation pipeline to provide more accurate supervision during
training. The paired makeup datasets produced by this pipeline significantly
surpass the quality of all existing datasets. Extensive experiments demonstrate
that FLUX-Makeup achieves state-of-the-art performance, exhibiting strong
robustness across diverse scenarios.

</details>


### [70] [AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models](https://arxiv.org/abs/2508.05084)
*Yuxiang Xiao,Yang Hu,Bin Li,Tianyang Zhang,Zexi Li,Huazhu Fu,Jens Rittscher,Kaixiang Yang*

Main category: cs.CV

TL;DR: AdaFusion是一种新的提示引导推理框架，通过动态整合来自多个病理基础模型（PFMs）的互补知识，解决了PFM预训练上下文导致的偏差问题，从而提高了下游任务的泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 病理基础模型（PFMs）在组织病理学图像上通过自监督预训练展现出强大的表征能力，但其多样且不透明的预训练上下文（受数据和结构/训练因素影响）引入了潜在偏差，阻碍了其在下游应用的泛化能力和透明度。

Method: 本文提出了AdaFusion，一个提示引导的推理框架。该方法通过压缩和对齐来自不同PFM的瓦片级特征，并利用轻量级注意力机制，根据组织表型上下文自适应地融合这些特征。

Result: AdaFusion在三个真实世界基准测试（包括治疗反应预测、肿瘤分级和空间基因表达推断）中进行了评估。结果显示，AdaFusion在分类和回归任务中均持续超越单个PFM，同时为每个模型的生物语义专业化提供了可解释的见解。

Conclusion: AdaFusion能够弥合异构PFM之间的差距，实现性能的提升和模型特定归纳偏差的可解释性，突出了其整合多模型知识的有效性。

Abstract: Pathology foundation models (PFMs) have demonstrated strong representational
capabilities through self-supervised pre-training on large-scale, unannotated
histopathology image datasets. However, their diverse yet opaque pretraining
contexts, shaped by both data-related and structural/training factors,
introduce latent biases that hinder generalisability and transparency in
downstream applications. In this paper, we propose AdaFusion, a novel
prompt-guided inference framework that, to our knowledge, is among the very
first to dynamically integrate complementary knowledge from multiple PFMs. Our
method compresses and aligns tile-level features from diverse models and
employs a lightweight attention mechanism to adaptively fuse them based on
tissue phenotype context. We evaluate AdaFusion on three real-world benchmarks
spanning treatment response prediction, tumour grading, and spatial gene
expression inference. Our approach consistently surpasses individual PFMs
across both classification and regression tasks, while offering interpretable
insights into each model's biosemantic specialisation. These results highlight
AdaFusion's ability to bridge heterogeneous PFMs, achieving both enhanced
performance and interpretability of model-specific inductive biases.

</details>


### [71] [PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation](https://arxiv.org/abs/2508.05091)
*Jingxuan He,Busheng Su,Finn Wong*

Main category: cs.CV

TL;DR: PoseGen是一个新颖的框架，能够从单张参考图像和姿态序列生成任意长度的、具有身份一致性和精确运动控制的视频。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型在生成长视频时面临巨大挑战，常常出现身份漂移，并且仅限于生成短片段，难以实现对主体身份和运动的精确控制。

Method: PoseGen采用了一种上下文LoRA微调策略，在token级别注入主体外观以保持身份，同时在通道级别根据姿态信息进行条件控制以实现精细运动控制。为克服时长限制，它创新性地引入了交错片段生成方法，通过共享KV缓存机制和专门的过渡过程，无缝拼接视频片段，确保背景一致性和时间平滑性。

Result: PoseGen在仅33小时的视频数据集上训练，但其在身份保真度、姿态准确性方面显著优于现有最先进方法，并能生成无限时长、连贯且无伪影的视频。

Conclusion: PoseGen成功解决了现有扩散模型在长视频生成中身份漂移和时长限制的问题，实现了对视频主体身份和运动的精确控制，能够生成高质量、任意长度的视频。

Abstract: Generating long, temporally coherent videos with precise control over subject
identity and motion is a formidable challenge for current diffusion models,
which often suffer from identity drift and are limited to short clips. We
introduce PoseGen, a novel framework that generates arbitrarily long videos of
a specific subject from a single reference image and a driving pose sequence.
Our core innovation is an in-context LoRA finetuning strategy that injects
subject appearance at the token level for identity preservation, while
simultaneously conditioning on pose information at the channel level for
fine-grained motion control. To overcome duration limits, PoseGen pioneers an
interleaved segment generation method that seamlessly stitches video clips
together, using a shared KV cache mechanism and a specialized transition
process to ensure background consistency and temporal smoothness. Trained on a
remarkably small 33-hour video dataset, extensive experiments show that PoseGen
significantly outperforms state-of-the-art methods in identity fidelity, pose
accuracy, and its unique ability to produce coherent, artifact-free videos of
unlimited duration.

</details>


### [72] [Latent Expression Generation for Referring Image Segmentation and Grounding](https://arxiv.org/abs/2508.05123)
*Seonghoon Yu,Joonbeom Hong,Joonseok Lee,Jeany Son*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉定位框架，通过从单个文本输入生成多个潜在表达，并融入互补的视觉细节，解决了视觉信息丰富而文本描述稀疏导致的目标识别问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法依赖单一文本输入，无法充分捕捉视觉域中丰富的细节，导致在识别相似物体时出现错误。视觉细节与稀疏文本线索之间的不匹配是主要动机。

Method: 引入了“主题分发器”（subject distributor）和“视觉概念注入器”（visual concept injector）模块，将共享主题和独特属性概念嵌入到潜在表示中，以捕获独特和目标特定的视觉线索。同时，提出了一种“正边距对比学习”策略，将所有潜在表达与原始文本对齐，同时保留细微变化。

Result: 实验结果表明，该方法在多个基准测试中超越了最先进的指代图像分割（RIS）和指代表达理解（REC）方法，并在广义指代表达分割（GRES）基准测试中也取得了出色表现。

Conclusion: 通过利用多个潜在表达和融合互补视觉细节，该框架显著提升了视觉定位任务的性能，有效解决了文本描述稀疏性带来的挑战。

Abstract: Visual grounding tasks, such as referring image segmentation (RIS) and
referring expression comprehension (REC), aim to localize a target object based
on a given textual description. The target object in an image can be described
in multiple ways, reflecting diverse attributes such as color, position, and
more. However, most existing methods rely on a single textual input, which
captures only a fraction of the rich information available in the visual
domain. This mismatch between rich visual details and sparse textual cues can
lead to the misidentification of similar objects. To address this, we propose a
novel visual grounding framework that leverages multiple latent expressions
generated from a single textual input by incorporating complementary visual
details absent from the original description. Specifically, we introduce
subject distributor and visual concept injector modules to embed both
shared-subject and distinct-attributes concepts into the latent
representations, thereby capturing unique and target-specific visual cues. We
also propose a positive-margin contrastive learning strategy to align all
latent expressions with the original text while preserving subtle variations.
Experimental results show that our method not only outperforms state-of-the-art
RIS and REC approaches on multiple benchmarks but also achieves outstanding
performance on the generalized referring expression segmentation (GRES)
benchmark.

</details>


### [73] [Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier Calibration for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.05094)
*Liang Bai,Hong Song,Jinfu Li,Yucong Lin,Jingfan Fan,Tianyu Fu,Danni Ai,Deqiang Xiao,Jian Yang*

Main category: cs.CV

TL;DR: 针对小样本类增量学习（FSCIL）中基类判别性和新类泛化性难以平衡的问题，本文提出SMP方法，在参数高效微调范式下，通过在基任务阶段的适配器合并（MIAM）和增量任务阶段的分类器校准（MPCC）中引入边际惩罚，有效提升了性能并实现了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据隐私限制和高获取成本导致增量任务中训练数据不足，进而导致类增量学习性能显著下降。现有前向兼容学习方法在FSCIL中难以平衡基类判别性和新类泛化性。此外，增量任务中原始数据访问受限导致类间决策边界模糊。

Method: 提出SMP（Sculpting Margin Penalty）方法，在参数高效微调范式中策略性地整合边际惩罚。具体地，在基任务学习阶段引入“边际感知任务内适配器合并（MIAM）”机制，训练两组低秩适配器：一组带边际惩罚以增强基类判别性，另一组无边际约束以促进对未来新类的泛化，然后自适应合并。在增量任务阶段，提出“基于边际惩罚的分类器校准（MPCC）”策略，通过对所有已见类的嵌入应用边际惩罚来微调分类器，以优化决策边界。

Result: 在CIFAR100、ImageNet-R和CUB200数据集上的广泛实验表明，SMP在FSCIL中达到了最先进的性能，并更好地平衡了基类和新类。

Conclusion: SMP通过在不同阶段策略性地整合边际惩罚，有效解决了FSCIL中的挑战，提升了基类判别性和新类泛化性，实现了SOTA性能。

Abstract: Real-world applications often face data privacy constraints and high
acquisition costs, making the assumption of sufficient training data in
incremental tasks unrealistic and leading to significant performance
degradation in class-incremental learning. Forward-compatible learning, which
prospectively prepares for future tasks during base task training, has emerged
as a promising solution for Few-Shot Class-Incremental Learning (FSCIL).
However, existing methods still struggle to balance base-class discriminability
and new-class generalization. Moreover, limited access to original data during
incremental tasks often results in ambiguous inter-class decision boundaries.
To address these challenges, we propose SMP (Sculpting Margin Penalty), a novel
FSCIL method that strategically integrates margin penalties at different stages
within the parameter-efficient fine-tuning paradigm. Specifically, we introduce
the Margin-aware Intra-task Adapter Merging (MIAM) mechanism for base task
learning. MIAM trains two sets of low-rank adapters with distinct
classification losses: one with a margin penalty to enhance base-class
discriminability, and the other without margin constraints to promote
generalization to future new classes. These adapters are then adaptively merged
to improve forward compatibility. For incremental tasks, we propose a Margin
Penalty-based Classifier Calibration (MPCC) strategy to refine decision
boundaries by fine-tuning classifiers on all seen classes' embeddings with a
margin penalty. Extensive experiments on CIFAR100, ImageNet-R, and CUB200
demonstrate that SMP achieves state-of-the-art performance in FSCIL while
maintaining a better balance between base and new classes.

</details>


### [74] [FedGIN: Federated Learning with Dynamic Global Intensity Non-linear Augmentation for Organ Segmentation using Multi-modal Images](https://arxiv.org/abs/2508.05137)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: 该论文提出了FedGIN，一个联邦学习（FL）框架，通过集成全局强度非线性（GIN）增强模块，在不共享原始患者数据的情况下实现多模态器官分割，有效解决了数据稀缺、领域偏移和隐私限制等挑战，并取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在AI辅助诊断、手术规划和治疗监测中至关重要。然而，由于图像模态间的固有差异、数据稀缺、模态间领域偏移以及隐私限制（阻止数据共享），开发一个能够有效泛化到多种模态的统一模型面临巨大挑战。一个统一模型将简化临床工作流程并减少对特定模态训练的需求。

Method: 提出FedGIN，一个联邦学习（FL）框架，用于在不共享原始患者数据的情况下进行多模态器官分割。该方法整合了一个轻量级的全局强度非线性（GIN）增强模块，用于在本地训练期间协调模态特定的强度分布。FedGIN在两种数据集场景下进行了评估：一个受限（初始仅MRI，后添加CT）数据集和一个完整数据集（MRI和CT数据充分利用）。

Result: 在受限数据场景下，FedGIN在MRI测试病例上的3D Dice得分比没有GIN的FL提高了12%到18%，并持续优于本地基线。在完整数据集场景下，FedGIN表现出接近中心化的性能，比仅MRI基线提高了30%的Dice得分，比仅CT基线提高了10%。

Conclusion: FedGIN框架在隐私限制下，有效解决了多模态医学图像分割中的挑战，通过其GIN模块实现了强大的跨模态泛化能力，并在两种数据场景下均取得了显著优于基线的性能，特别是数据受限情况下表现突出。

Abstract: Medical image segmentation plays a crucial role in AI-assisted diagnostics,
surgical planning, and treatment monitoring. Accurate and robust segmentation
models are essential for enabling reliable, data-driven clinical decision
making across diverse imaging modalities. Given the inherent variability in
image characteristics across modalities, developing a unified model capable of
generalizing effectively to multiple modalities would be highly beneficial.
This model could streamline clinical workflows and reduce the need for
modality-specific training. However, real-world deployment faces major
challenges, including data scarcity, domain shift between modalities (e.g., CT
vs. MRI), and privacy restrictions that prevent data sharing. To address these
issues, we propose FedGIN, a Federated Learning (FL) framework that enables
multimodal organ segmentation without sharing raw patient data. Our method
integrates a lightweight Global Intensity Non-linear (GIN) augmentation module
that harmonizes modality-specific intensity distributions during local
training. We evaluated FedGIN using two types of datasets: an imputed dataset
and a complete dataset. In the limited dataset scenario, the model was
initially trained using only MRI data, and CT data was added to assess its
performance improvements. In the complete dataset scenario, both MRI and CT
data were fully utilized for training on all clients. In the limited-data
scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test
cases compared to FL without GIN and consistently outperformed local baselines.
In the complete dataset scenario, FedGIN demonstrated near-centralized
performance, with a 30% Dice score improvement over the MRI-only baseline and a
10% improvement over the CT-only baseline, highlighting its strong
cross-modality generalization under privacy constraints.

</details>


### [75] [AHDMIL: Asymmetric Hierarchical Distillation Multi-Instance Learning for Fast and Accurate Whole-Slide Image Classification](https://arxiv.org/abs/2508.05114)
*Jiuyang Dong,Jiahan Li,Junjun Jiang,Kui Jiang,Yongbing Zhang*

Main category: cs.CV

TL;DR: AHDMIL是一种非对称分层蒸馏多实例学习框架，通过两步训练过程（自蒸馏和非对称蒸馏）消除病理图像中不相关的补丁，从而实现快速准确的分类推理，并引入了基于Chebyshev多项式的Kolmogorov-Arnold分类器。


<details>
  <summary>Details</summary>
Motivation: 多实例学习（MIL）在病理图像分类中表现出色，但由于需要处理千兆像素全玻片图像（WSI）中的数千个补丁，导致推理成本高昂。

Method: AHDMIL框架包含两个核心组件：动态多实例网络（DMIN）处理高分辨率WSI，以及双分支轻量级实例预筛选网络（DB-LIPN）分析对应的低分辨率图像。训练分两步：1. 自蒸馏（SD）：DMIN训练WSI分类并生成实例级注意力分数以识别不相关补丁。2. 非对称蒸馏（AD）：DB-LIPN学习预测低分辨率补丁的相关性，其预测的相关补丁用于DMIN的微调和高效推理。此外，设计了首个基于Chebyshev多项式的Kolmogorov-Arnold（CKA）分类器，通过可学习激活层提升分类性能。

Result: AHDMIL在四个公开数据集上持续优于现有最先进方法，在分类性能和推理速度上均有提升。例如，在Camelyon16数据集上，准确率相对提升5.3%，推理速度加快1.2倍。在所有数据集上，AUC、准确率、F1分数和Brier分数均有显著提升，平均推理速度提升1.2至2.1倍。

Conclusion: AHDMIL通过新颖的非对称分层蒸馏框架，有效解决了病理图像分类中MIL推理成本高的问题，通过智能地消除不相关补丁，实现了卓越的分类性能和推理速度。

Abstract: Although multi-instance learning (MIL) has succeeded in pathological image
classification, it faces the challenge of high inference costs due to the need
to process thousands of patches from each gigapixel whole slide image (WSI). To
address this, we propose AHDMIL, an Asymmetric Hierarchical Distillation
Multi-Instance Learning framework that enables fast and accurate classification
by eliminating irrelevant patches through a two-step training process. AHDMIL
comprises two key components: the Dynamic Multi-Instance Network (DMIN), which
operates on high-resolution WSIs, and the Dual-Branch Lightweight Instance
Pre-screening Network (DB-LIPN), which analyzes corresponding low-resolution
counterparts. In the first step, self-distillation (SD), DMIN is trained for
WSI classification while generating per-instance attention scores to identify
irrelevant patches. These scores guide the second step, asymmetric distillation
(AD), where DB-LIPN learns to predict the relevance of each low-resolution
patch. The relevant patches predicted by DB-LIPN have spatial correspondence
with patches in high-resolution WSIs, which are used for fine-tuning and
efficient inference of DMIN. In addition, we design the first
Chebyshev-polynomial-based Kolmogorov-Arnold (CKA) classifier in computational
pathology, which improves classification performance through learnable
activation layers. Extensive experiments on four public datasets demonstrate
that AHDMIL consistently outperforms previous state-of-the-art methods in both
classification performance and inference speed. For example, on the Camelyon16
dataset, it achieves a relative improvement of 5.3% in accuracy and accelerates
inference by 1.2.times. Across all datasets, area under the curve (AUC),
accuracy, f1 score, and brier score show consistent gains, with average
inference speedups ranging from 1.2 to 2.1 times. The code is available.

</details>


### [76] [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221)
*Xiao Wang,Liye Jin,Xufeng Lou,Shiao Wang,Lan Chen,Bo Jiang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReasoningTrack的基于推理的视觉语言跟踪框架，利用Qwen2.5-VL模型，并通过SFT和GRPO优化推理与语言生成。同时，还构建了一个大规模长期视觉语言跟踪基准数据集TNLLT。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言跟踪方法存在局限性，如语言与视觉特征的固定融合或简单注意力机制，以及文本生成方法未能提供模型推理洞察且未充分利用大模型优势，导致性能受限。研究旨在解决目标指定的不灵活性和不准确性问题。

Method: 本文提出基于预训练视觉语言模型Qwen2.5-VL的ReasoningTrack框架。采用SFT（监督微调）和强化学习GRPO优化推理和语言生成。将更新的语言描述嵌入后与视觉特征一同输入统一的跟踪骨干网络，并通过跟踪头预测目标位置。此外，构建了一个包含200个视频序列的大规模长期视觉语言跟踪基准数据集TNLLT，并重新训练和评估了20个基线视觉跟踪器。

Result: 在多个视觉语言跟踪基准数据集上的大量实验充分验证了所提出的基于推理的自然语言生成策略的有效性。新提出的TNLLT数据集为视觉语言视觉跟踪任务奠定了坚实基础。

Conclusion: 所提出的基于推理的视觉语言跟踪框架ReasoningTrack及其创新的语言生成策略，结合新发布的TNLLT数据集，有效提升了视觉语言跟踪的性能，并为该领域未来的研究提供了新方向和坚实基础。

Abstract: Vision-language tracking has received increasing attention in recent years,
as textual information can effectively address the inflexibility and inaccuracy
associated with specifying the target object to be tracked. Existing works
either directly fuse the fixed language with vision features or simply modify
using attention, however, their performance is still limited. Recently, some
researchers have explored using text generation to adapt to the variations in
the target during tracking, however, these works fail to provide insights into
the model's reasoning process and do not fully leverage the advantages of large
models, which further limits their overall performance. To address the
aforementioned issues, this paper proposes a novel reasoning-based
vision-language tracking framework, named ReasoningTrack, based on a
pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning)
and reinforcement learning GRPO are used for the optimization of reasoning and
language generation. We embed the updated language descriptions and feed them
into a unified tracking backbone network together with vision features. Then,
we adopt a tracking head to predict the specific location of the target object.
In addition, we propose a large-scale long-term vision-language tracking
benchmark dataset, termed TNLLT, which contains 200 video sequences. 20
baseline visual trackers are re-trained and evaluated on this dataset, which
builds a solid foundation for the vision-language visual tracking task.
Extensive experiments on multiple vision-language tracking benchmark datasets
fully validated the effectiveness of our proposed reasoning-based natural
language generation strategy. The source code of this paper will be released on
https://github.com/Event-AHU/Open_VLTrack

</details>


### [77] [Deep Learning-based Animal Behavior Analysis: Insights from Mouse Chronic Pain Models](https://arxiv.org/abs/2508.05138)
*Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,Hong-Yuan Mark Liao,James C. Liao,Chien-Chang Chen*

Main category: cs.CV

TL;DR: 该研究提出了一种无需人工标注，自动发现小鼠慢性疼痛行为特征的框架，其分类准确性显著优于人类专家和现有方法，并揭示了药物疗效差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估小鼠慢性疼痛行为的方法过度依赖人工标注，且人类对哪些行为最能代表慢性疼痛缺乏清晰理解，导致难以准确捕捉慢性疼痛的隐匿性和持续性行为变化。

Method: 本研究提出一个框架，利用通用动作空间投影器自动从小鼠视频中提取动作特征，避免了人工标注的潜在偏差，并保留了原始视频中丰富的行为信息。同时，收集了一个包含神经性疼痛和炎症性疼痛在多个时间点的疾病进展的小鼠疼痛行为数据集。

Result: 在15类疼痛分类任务中，该方法实现了48.41%的准确率，显著优于人类专家（21.33%）和B-SOiD（30.52%）。在简化为神经性疼痛、炎症性疼痛和无痛三类分类时，准确率达到73.1%，远高于人类专家（48%）和B-SOiD（58.43%）。此外，该方法在零样本加巴喷丁药物测试中揭示了不同类型疼痛的药物疗效差异，与过往文献一致。

Conclusion: 该研究证明了所提方法在临床应用方面的潜力，能为疼痛研究和相关药物开发提供新见解。

Abstract: Assessing chronic pain behavior in mice is critical for preclinical studies.
However, existing methods mostly rely on manual labeling of behavioral
features, and humans lack a clear understanding of which behaviors best
represent chronic pain. For this reason, existing methods struggle to
accurately capture the insidious and persistent behavioral changes in chronic
pain. This study proposes a framework to automatically discover features
related to chronic pain without relying on human-defined action labels. Our
method uses universal action space projector to automatically extract mouse
action features, and avoids the potential bias of human labeling by retaining
the rich behavioral information in the original video. In this paper, we also
collected a mouse pain behavior dataset that captures the disease progression
of both neuropathic and inflammatory pain across multiple time points. Our
method achieves 48.41\% accuracy in a 15-class pain classification task,
significantly outperforming human experts (21.33\%) and the widely used method
B-SOiD (30.52\%). Furthermore, when the classification is simplified to only
three categories, i.e., neuropathic pain, inflammatory pain, and no pain, then
our method achieves an accuracy of 73.1\%, which is notably higher than that of
human experts (48\%) and B-SOiD (58.43\%). Finally, our method revealed
differences in drug efficacy for different types of pain on zero-shot
Gabapentin drug testing, and the results were consistent with past drug
efficacy literature. This study demonstrates the potential clinical application
of our method, which can provide new insights into pain research and related
drug development.

</details>


### [78] [Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2508.05237)
*Zane Xu,Jason Sun*

Main category: cs.CV

TL;DR: 该报告综合分析了八篇关于视觉语言模型（如CLIP）零样本对抗鲁棒性的重要论文，探讨了鲁棒性与泛化能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 核心挑战在于提升对抗鲁棒性与保持模型零样本泛化能力之间存在固有的权衡。

Method: 分析了两种主要防御范式：对抗微调（AFT，修改模型参数）和免训练/测试时防御（保留参数）。具体方法包括：对齐保持方法（TeCoA）、嵌入空间重构（LAAT, TIMA）、输入启发式方法（AOM, TTC）以及潜在空间净化（CLIPure）。

Result: 追溯了防御方法从对齐保持到嵌入空间重构，再到输入启发式和潜在空间净化的演变过程。

Conclusion: 识别了关键挑战和未来方向，包括混合防御策略和对抗预训练。

Abstract: This report synthesizes eight seminal papers on the zero-shot adversarial
robustness of vision-language models (VLMs) like CLIP. A central challenge in
this domain is the inherent trade-off between enhancing adversarial robustness
and preserving the model's zero-shot generalization capabilities. We analyze
two primary defense paradigms: Adversarial Fine-Tuning (AFT), which modifies
model parameters, and Training-Free/Test-Time Defenses, which preserve them. We
trace the evolution from alignment-preserving methods (TeCoA) to embedding
space re-engineering (LAAT, TIMA), and from input heuristics (AOM, TTC) to
latent-space purification (CLIPure). Finally, we identify key challenges and
future directions including hybrid defense strategies and adversarial
pre-training.

</details>


### [79] [Rotation Equivariant Arbitrary-scale Image Super-Resolution](https://arxiv.org/abs/2508.05160)
*Qi Xie,Jiahong Fu,Zongben Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 该研究提出了一种旋转等变任意尺度图像超分辨率（ASISR）方法，通过重新设计编码器和隐式神经表示（INR）模块，解决低分辨率图像中几何模式扭曲导致的伪影问题，首次实现端到端旋转等变性。


<details>
  <summary>Details</summary>
Motivation: 当前的ASISR方法在处理低分辨率图像中常见的几何模式（如重复纹理、边缘、形状）时，由于其严重扭曲和变形，导致高分辨率恢复中出现意想不到的伪影。将旋转等变性嵌入ASISR网络可以帮助忠实地保持几何模式的原始方向和结构完整性，从而减少伪影。

Method: 研究人员重新设计了INR和编码器模块的基本架构，使其具有内在的旋转等变能力，从而实现了ASISR网络从输入到输出的端到端旋转等变性。此外，还提供了理论分析来评估其内在等变误差，证明了其嵌入等变结构的固有性质。

Result: 所提出的方法在模拟和真实数据集上都表现出卓越的性能。研究还验证了该框架可以以即插即用的方式轻松集成到当前的ASISR方法中，进一步增强其性能。

Conclusion: 本研究成功构建了一种旋转等变的ASISR方法，有效解决了低分辨率图像中几何模式扭曲导致的伪影问题，首次实现了ASISR的端到端旋转等变性，并在实验中证明了其优越性和可集成性。

Abstract: The arbitrary-scale image super-resolution (ASISR), a recent popular topic in
computer vision, aims to achieve arbitrary-scale high-resolution recoveries
from a low-resolution input image. This task is realized by representing the
image as a continuous implicit function through two fundamental modules, a
deep-network-based encoder and an implicit neural representation (INR) module.
Despite achieving notable progress, a crucial challenge of such a highly
ill-posed setting is that many common geometric patterns, such as repetitive
textures, edges, or shapes, are seriously warped and deformed in the
low-resolution images, naturally leading to unexpected artifacts appearing in
their high-resolution recoveries. Embedding rotation equivariance into the
ASISR network is thus necessary, as it has been widely demonstrated that this
enhancement enables the recovery to faithfully maintain the original
orientations and structural integrity of geometric patterns underlying the
input image. Motivated by this, we make efforts to construct a rotation
equivariant ASISR method in this study. Specifically, we elaborately redesign
the basic architectures of INR and encoder modules, incorporating intrinsic
rotation equivariance capabilities beyond those of conventional ASISR networks.
Through such amelioration, the ASISR network can, for the first time, be
implemented with end-to-end rotational equivariance maintained from input to
output. We also provide a solid theoretical analysis to evaluate its intrinsic
equivariance error, demonstrating its inherent nature of embedding such an
equivariance structure. The superiority of the proposed method is substantiated
by experiments conducted on both simulated and real datasets. We also validate
that the proposed framework can be readily integrated into current ASISR
methods in a plug \& play manner to further enhance their performance.

</details>


### [80] [RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding](https://arxiv.org/abs/2508.05244)
*Tianchen Fang,Guiru Liu*

Main category: cs.CV

TL;DR: RegionMed-CLIP是一个区域感知的多模态对比学习框架，通过整合局部病理信号和全局语义表示，解决了医学图像理解中数据稀缺和对全局特征过度依赖的问题。该框架引入了ROI处理器和渐进式训练策略，并构建了大规模区域级医学图像-文本数据集MedRegion-500k，在多项任务上显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 医学图像理解面临两大挑战：高质量标注医学数据稀缺，以及过度依赖全局图像特征，导致错过细微但临床重要的病理区域。

Method: 本文提出了RegionMed-CLIP，一个区域感知的多模态对比学习框架。其核心是一个创新的感兴趣区域（ROI）处理器，能够自适应地将细粒度区域特征与全局上下文整合。同时，采用渐进式训练策略以增强分层多模态对齐。为支持大规模区域级表示学习，构建了包含大量区域标注和多级临床描述的医学图像-文本语料库MedRegion-500k。

Result: 在图像-文本检索、零样本分类和视觉问答任务上进行的广泛实验表明，RegionMed-CLIP持续大幅超越现有最先进的视觉语言模型。

Conclusion: 研究结果强调了区域感知对比预训练的关键重要性，并将RegionMed-CLIP定位为推动多模态医学图像理解的强大基础。

Abstract: Medical image understanding plays a crucial role in enabling automated
diagnosis and data-driven clinical decision support. However, its progress is
impeded by two primary challenges: the limited availability of high-quality
annotated medical data and an overreliance on global image features, which
often miss subtle but clinically significant pathological regions. To address
these issues, we introduce RegionMed-CLIP, a region-aware multimodal
contrastive learning framework that explicitly incorporates localized
pathological signals along with holistic semantic representations. The core of
our method is an innovative region-of-interest (ROI) processor that adaptively
integrates fine-grained regional features with the global context, supported by
a progressive training strategy that enhances hierarchical multimodal
alignment. To enable large-scale region-level representation learning, we
construct MedRegion-500k, a comprehensive medical image-text corpus that
features extensive regional annotations and multilevel clinical descriptions.
Extensive experiments on image-text retrieval, zero-shot classification, and
visual question answering tasks demonstrate that RegionMed-CLIP consistently
exceeds state-of-the-art vision language models by a wide margin. Our results
highlight the critical importance of region-aware contrastive pre-training and
position RegionMed-CLIP as a robust foundation for advancing multimodal medical
image understanding.

</details>


### [81] [X-MoGen: Unified Motion Generation across Humans and Animals](https://arxiv.org/abs/2508.05162)
*Xuan Wang,Kai Ruan,Liyang Qian,Zhizhi Guo,Chang Su,Gaoang Wang*

Main category: cs.CV

TL;DR: X-MoGen是首个统一的跨物种文本驱动运动生成框架，能同时处理人类和动物运动，通过两阶段模型和形态一致性模块解决形态差异，并在新数据集UniMo4D上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将人类和动物运动分开建模，而统一的跨物种方法具有表示统一和泛化能力强的优势。然而，不同物种间的形态差异是一个主要挑战，常导致运动不真实。

Method: 提出X-MoGen，一个两阶段架构：1) 条件图变分自编码器学习T-pose先验，自编码器将运动编码到共享潜在空间，并用形态损失正则化。2) 执行掩码运动建模，根据文本描述生成运动嵌入。训练期间使用形态一致性模块确保骨骼合理性。构建了UniMo4D数据集，包含115个物种和11.9万个运动序列，采用共享骨骼拓扑结构以支持统一建模。

Result: 在UniMo4D数据集上的大量实验表明，X-MoGen在已知和未知物种上均优于现有最先进的方法。

Conclusion: X-MoGen是首个成功实现跨物种文本驱动运动统一生成的框架，通过其独特的两阶段架构、形态一致性模块和大规模统一数据集UniMo4D，有效克服了物种间形态差异的挑战，展现出卓越的性能和泛化能力。

Abstract: Text-driven motion generation has attracted increasing attention due to its
broad applications in virtual reality, animation, and robotics. While existing
methods typically model human and animal motion separately, a joint
cross-species approach offers key advantages, such as a unified representation
and improved generalization. However, morphological differences across species
remain a key challenge, often compromising motion plausibility. To address
this, we propose \textbf{X-MoGen}, the first unified framework for
cross-species text-driven motion generation covering both humans and animals.
X-MoGen adopts a two-stage architecture. First, a conditional graph variational
autoencoder learns canonical T-pose priors, while an autoencoder encodes motion
into a shared latent space regularized by morphological loss. In the second
stage, we perform masked motion modeling to generate motion embeddings
conditioned on textual descriptions. During training, a morphological
consistency module is employed to promote skeletal plausibility across species.
To support unified modeling, we construct \textbf{UniMo4D}, a large-scale
dataset of 115 species and 119k motion sequences, which integrates human and
animal motions under a shared skeletal topology for joint training. Extensive
experiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art
methods on both seen and unseen species.

</details>


### [82] [A Study of Gender Classification Techniques Based on Iris Images: A Deep Survey and Analysis](https://arxiv.org/abs/2508.05246)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 本文综述了基于生物特征的性别分类方法，重点关注虹膜特征，并分析了现有技术、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 性别分类在监控、企业画像和人机交互等领域有广泛应用。虽然面部特征是主流，但虹膜作为一种稳定、可见且非侵入性的生物特征，在性别分类中具有巨大潜力，且已有高质量的虹膜图像处理方法。

Method: 本研究通过文献综述的方式，讨论了多种性别判定方法，简要回顾了现有工作，并分析了性别分类不同步骤的各种方法。

Result: 本文为研究人员提供了现有性别分类方法的知识和分析，协助该领域的研究者了解现有方法、识别领域空白和挑战。

Conclusion: 该研究旨在帮助研究人员了解性别分类的现有方法，突出该领域的差距和挑战，并为未来的改进提供建议和方向。

Abstract: Gender classification is attractive in a range of applications, including
surveillance and monitoring, corporate profiling, and human-computer
interaction. Individuals' identities may be gleaned from information about
their gender, which is a kind of soft biometric.Over the years, several methods
for determining a person's gender have been devised. Some of the most
well-known ones are based on physical characteristics like face, fingerprint,
palmprint, DNA, ears, gait, and iris. On the other hand, facial features
account for the vast majority of gender classification methods. Also, the iris
is a significant biometric trait because the iris, according to research,
remains basically constant during an individual's life. Besides that, the iris
is externally visible and is non-invasive to the user, which is important for
practical applications. Furthermore, there are already high-quality methods for
segmenting and encoding iris images, and the current methods facilitate
selecting and extracting attribute vectors from iris textures. This study
discusses several approaches to determining gender. The previous works of
literature are briefly reviewed. Additionally, there are a variety of
methodologies for different steps of gender classification. This study provides
researchers with knowledge and analysis of the existing gender classification
approaches. Also, it will assist researchers who are interested in this
specific area, as well as highlight the gaps and challenges in the field, and
finally provide suggestions and future paths for improvement.

</details>


### [83] [PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems](https://arxiv.org/abs/2508.05167)
*Qi Guo,Xiaojun Jia,Shanmin Pang,Simeng Qin,Lin Wang,Ju Jia,Yang Liu,Qing Guo*

Main category: cs.CV

TL;DR: PhysPatch是一种针对多模态大语言模型（MLLMs）自动驾驶系统设计的、可物理实现且可迁移的对抗性补丁攻击框架，通过联合优化补丁的位置、形状和内容，显著提高了攻击有效性和实际适用性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在自动驾驶（AD）系统中日益重要，但易受对抗性攻击，尤其是对抗性补丁攻击，这在现实世界中构成严重威胁。现有补丁攻击方法主要针对目标检测模型，由于MLLMs复杂的架构和推理能力，它们在MLLM系统上的效果不佳。

Method: 本文提出了PhysPatch框架，它联合优化补丁的位置、形状和内容，以增强攻击效果和实际适用性。具体方法包括：引入基于语义的掩码初始化策略以实现真实放置；采用基于SVD的局部对齐损失和补丁引导的裁剪-调整大小，以提高可迁移性；以及使用基于势场的掩码细化方法。

Result: 在开源、商业和具备推理能力的MLLMs上的大量实验表明，PhysPatch在引导MLLM自动驾驶系统产生目标对齐的感知和规划输出方面，显著优于现有方法。此外，PhysPatch始终将对抗性补丁放置在自动驾驶场景中物理可行的区域，确保了强大的实际适用性和可部署性。

Conclusion: PhysPatch是一个有效、可物理实现且可迁移的对抗性补丁框架，专门为基于MLLM的自动驾驶系统设计，能够成功攻击这些系统并具有实际部署潜力。

Abstract: Multimodal Large Language Models (MLLMs) are becoming integral to autonomous
driving (AD) systems due to their strong vision-language reasoning
capabilities. However, MLLMs are vulnerable to adversarial attacks,
particularly adversarial patch attacks, which can pose serious threats in
real-world scenarios. Existing patch-based attack methods are primarily
designed for object detection models and perform poorly when transferred to
MLLM-based systems due to the latter's complex architectures and reasoning
abilities. To address these limitations, we propose PhysPatch, a physically
realizable and transferable adversarial patch framework tailored for MLLM-based
AD systems. PhysPatch jointly optimizes patch location, shape, and content to
enhance attack effectiveness and real-world applicability. It introduces a
semantic-based mask initialization strategy for realistic placement, an
SVD-based local alignment loss with patch-guided crop-resize to improve
transferability, and a potential field-based mask refinement method. Extensive
experiments across open-source, commercial, and reasoning-capable MLLMs
demonstrate that PhysPatch significantly outperforms prior methods in steering
MLLM-based AD systems toward target-aligned perception and planning outputs.
Moreover, PhysPatch consistently places adversarial patches in physically
feasible regions of AD scenes, ensuring strong real-world applicability and
deployability.

</details>


### [84] [CF3: Compact and Fast 3D Feature Fields](https://arxiv.org/abs/2508.05254)
*Hyunjoon Lee,Joonkyu Min,Jaesik Park*

Main category: cs.CV

TL;DR: 本文提出CF3，一种自上而下的方法，用于构建紧凑且快速的3D高斯特征场，通过多视角特征融合、高斯编码器训练和自适应稀疏化，显著减少高斯数量并保持几何细节。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯Splatting（3DGS）方法在整合2D基础模型特征时，大多采用自下而上的优化过程，将原始2D特征视为真值，导致计算成本增加。

Method: 1. 提出自上而下的CF3流程。2. 对多视角2D特征与预训练高斯进行快速加权融合。3. 直接在提升的特征上训练每个高斯的自编码器，而非在2D域训练。4. 引入自适应稀疏化方法，优化高斯属性，并剪枝、合并冗余高斯。

Result: 与Feature-3DGS相比，本文方法仅使用5%的高斯数量即可构建具有竞争力的3D特征场，同时保持了几何细节。

Conclusion: CF3通过自上而下的特征融合和自适应稀疏化，有效地解决了传统方法计算成本高的问题，构建了高效且紧凑的3D高斯特征表示。

Abstract: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D
foundation models. However, most approaches rely on a bottom-up optimization
process that treats raw 2D features as ground truth, incurring increased
computational costs. We propose a top-down pipeline for constructing compact
and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast
weighted fusion of multi-view 2D features with pre-trained Gaussians. This
approach enables training a per-Gaussian autoencoder directly on the lifted
features, instead of training autoencoders in the 2D domain. As a result, the
autoencoder better aligns with the feature distribution. More importantly, we
introduce an adaptive sparsification method that optimizes the Gaussian
attributes of the feature field while pruning and merging the redundant
Gaussians, constructing an efficient representation with preserved geometric
details. Our approach achieves a competitive 3D feature field using as little
as 5% of the Gaussians compared to Feature-3DGS.

</details>


### [85] [Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering](https://arxiv.org/abs/2508.05172)
*Zewei Wu,Longhao Wang,Cui Wang,César Teixeira,Wei Ke,Zhang Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种名为多轨迹段跟踪（MTT）的增强型跟踪器，通过灵活的轨迹段生成和多轨迹段关联框架，有效应对未知类别目标跟踪中低置信度检测、弱约束和长期遮挡等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉多目标跟踪方法在处理现实世界中未见过的目标类别时面临挑战，因为它们经常遇到低置信度检测、弱运动和外观约束以及长期遮挡问题。

Method: MTT集成灵活的轨迹段生成到多轨迹段关联框架中。它首先根据短期时空相关性自适应地将检测结果聚类成鲁棒的轨迹段，然后利用位置和随时间变化的外观等多种线索估计最佳轨迹段划分，以减轻长期关联中的误差传播。

Result: 在通用多目标跟踪基准上的大量实验证明了所提出框架的竞争力。

Conclusion: 所提出的MTT框架通过其创新的轨迹段生成和关联方法，有效解决了未知类别目标跟踪中的关键挑战，并展示了优异的性能。

Abstract: Tracking specific targets, such as pedestrians and vehicles, has been the
focus of recent vision-based multitarget tracking studies. However, in some
real-world scenarios, unseen categories often challenge existing methods due to
low-confidence detections, weak motion and appearance constraints, and
long-term occlusions. To address these issues, this article proposes a
tracklet-enhanced tracker called Multi-Tracklet Tracking (MTT) that integrates
flexible tracklet generation into a multi-tracklet association framework. This
framework first adaptively clusters the detection results according to their
short-term spatio-temporal correlation into robust tracklets and then estimates
the best tracklet partitions using multiple clues, such as location and
appearance over time to mitigate error propagation in long-term association.
Finally, extensive experiments on the benchmark for generic multiple object
tracking demonstrate the competitiveness of the proposed framework.

</details>


### [86] [Robust Tracking with Particle Filtering for Fluorescent Cardiac Imaging](https://arxiv.org/abs/2508.05262)
*Suresh Guttikonda,Maximilian Neidhart,Johanna Sprenger,Johannes Petersen,Christian Detter,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出了一种基于循环一致性检查的粒子滤波跟踪器，用于在心脏手术中实时、鲁棒地跟踪心脏特征点，以估计局部心肌灌注，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在冠状动脉旁路移植术后的术中荧光心脏成像中，心脏运动和血管结构丰富导致的图像特征显著波动限制了传统跟踪方法，使得难以准确估计局部定量指标如心肌灌注。

Method: 提出了一种基于循环一致性检查的粒子滤波跟踪器，用于鲁棒地跟踪采样以跟随目标地标的粒子。

Result: 该方法能够同时跟踪117个目标，帧率为25.4 fps，跟踪误差为(5.00 ± 0.22 px)，优于其他深度学习跟踪器(22.3 ± 1.1 px)和传统跟踪器(58.1 ± 27.1 px)。

Conclusion: 所提出的跟踪器实现了实时、准确的心脏特征点跟踪，使得在介入手术中能够进行实时估计，有助于冠状动脉旁路移植术后的质量控制。

Abstract: Intraoperative fluorescent cardiac imaging enables quality control following
coronary bypass grafting surgery. We can estimate local quantitative
indicators, such as cardiac perfusion, by tracking local feature points.
However, heart motion and significant fluctuations in image characteristics
caused by vessel structural enrichment limit traditional tracking methods. We
propose a particle filtering tracker based on cyclicconsistency checks to
robustly track particles sampled to follow target landmarks. Our method tracks
117 targets simultaneously at 25.4 fps, allowing real-time estimates during
interventions. It achieves a tracking error of (5.00 +/- 0.22 px) and
outperforms other deep learning trackers (22.3 +/- 1.1 px) and conventional
trackers (58.1 +/- 27.1 px).

</details>


### [87] [SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation](https://arxiv.org/abs/2508.05182)
*Zhiqing Xiao,Haobo Wang,Xu Lu,Wentao Ye,Gang Chen,Junbo Zhao*

Main category: cs.CV

TL;DR: 本文提出SPA++框架，通过图谱对齐和细粒度邻居感知传播，解决了域适应中跨域可迁移性和域内结构辨别力之间的权衡问题，并在复杂场景下表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有域适应（DA）方法侧重于捕获域间可迁移性，但却忽视了丰富的域内结构，这导致辨别力下降。为了解决这种权衡，研究者提出了新的方法。

Method: 本文提出了广义图谱对齐框架SPA++，其核心包括：1) 将DA问题转换为图原语，通过新颖的谱正则化器在特征空间中对齐域图；2) 开发细粒度邻居感知传播机制，增强目标域的辨别力；3) 结合数据增强和一致性正则化，使其适应多种DA设置和复杂分布场景。此外，还提供了图基DA泛化界限及谱对齐和平滑一致性作用的理论分析。

Result: 在基准数据集上的大量实验表明，SPA++始终优于现有先进方法，在各种挑战性适应场景中展现出卓越的鲁棒性和适应性。

Conclusion: SPA++是一个有效且鲁棒的域适应框架，通过创新的图谱对齐和细粒度传播机制，成功平衡了域间迁移和域内结构，并在复杂适应场景中取得了领先性能，并提供了理论支持。

Abstract: Domain Adaptation (DA) aims to transfer knowledge from a labeled source
domain to an unlabeled or sparsely labeled target domain under domain shifts.
Most prior works focus on capturing the inter-domain transferability but
largely overlook rich intra-domain structures, which empirically results in
even worse discriminability. To tackle this tradeoff, we propose a generalized
graph SPectral Alignment framework, SPA++. Its core is briefly condensed as
follows: (1)-by casting the DA problem to graph primitives, it composes a
coarse graph alignment mechanism with a novel spectral regularizer toward
aligning the domain graphs in eigenspaces; (2)-we further develop a
fine-grained neighbor-aware propagation mechanism for enhanced discriminability
in the target domain; (3)-by incorporating data augmentation and consistency
regularization, SPA++ can adapt to complex scenarios including most DA settings
and even challenging distribution scenarios. Furthermore, we also provide
theoretical analysis to support our method, including the generalization bound
of graph-based DA and the role of spectral alignment and smoothing consistency.
Extensive experiments on benchmark datasets demonstrate that SPA++ consistently
outperforms existing cutting-edge methods, achieving superior robustness and
adaptability across various challenging adaptation scenarios.

</details>


### [88] [SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion](https://arxiv.org/abs/2508.05264)
*Xiaoyang Zhang,Zhen Hua,Yakun Ju,Wei Zhou,Jun Liu,Alex C. Kot*

Main category: cs.CV

TL;DR: SGDFuse提出了一种由SAM引导的条件扩散模型，用于红外与可见光图像融合，通过语义掩码显式指导融合过程，解决了现有方法目标保留不足、伪影和细节丢失的问题，实现了高保真和语义感知的融合。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光图像融合方法缺乏对场景的深度语义理解，导致无法有效保留关键目标；融合过程易引入伪影和细节损失，严重损害图像质量和下游任务性能。

Method: 本文提出了SGDFuse，一个由Segment Anything Model (SAM)引导的条件扩散模型。其核心是通过SAM生成的高质量语义掩码作为显式先验，指导融合过程的优化。该框架分为两阶段：首先进行多模态特征的初步融合；然后将SAM语义掩码与初步融合图像共同作为条件，驱动扩散模型的从粗到精的去噪生成，从而确保融合过程的语义方向性和高保真度。

Result: SGDFuse在主观和客观评估中均取得了最先进的性能，并展现出对下游任务的良好适应性。

Conclusion: SGDFuse为图像融合中的核心挑战提供了一个强大的解决方案，显著提升了融合图像的质量和对下游任务的适用性。

Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal
radiation information from infrared images with the rich texture details from
visible images to enhance perceptual capabilities for downstream visual tasks.
However, existing methods often fail to preserve key targets due to a lack of
deep semantic understanding of the scene, while the fusion process itself can
also introduce artifacts and detail loss, severely compromising both image
quality and task performance. To address these issues, this paper proposes
SGDFuse, a conditional diffusion model guided by the Segment Anything Model
(SAM), to achieve high-fidelity and semantically-aware image fusion. The core
of our method is to utilize high-quality semantic masks generated by SAM as
explicit priors to guide the optimization of the fusion process via a
conditional diffusion model. Specifically, the framework operates in a
two-stage process: it first performs a preliminary fusion of multi-modal
features, and then utilizes the semantic masks from SAM jointly with the
preliminary fused image as a condition to drive the diffusion model's
coarse-to-fine denoising generation. This ensures the fusion process not only
has explicit semantic directionality but also guarantees the high fidelity of
the final result. Extensive experiments demonstrate that SGDFuse achieves
state-of-the-art performance in both subjective and objective evaluations, as
well as in its adaptability to downstream tasks, providing a powerful solution
to the core challenges in image fusion. The code of SGDFuse is available at
https://github.com/boshizhang123/SGDFuse.

</details>


### [89] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 该研究提出一种双源数据策略，通过收集原生网络alt-text和MLLM生成的字幕，构建了MELLA数据集，以提升多模态大语言模型（MLLMs）在低资源语言中的语言能力和文化接地性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在低资源语言中的表现显著下降。现有方法多限于文本模态或依赖机器翻译，导致模型缺乏多模态信息性和文化接地性，无法有效服务低资源语言用户。

Method: 研究明确了MLLM在低资源语言设置中的两个目标：语言能力和文化接地性（特别强调文化意识）。为此，提出了一个双源策略来指导数据收集：利用原生网络alt-text获取文化信息，利用MLLM生成的字幕获取语言信息。具体实现是引入了MELLA，一个多模态、多语言数据集，并用其对多种MLLM骨干模型进行微调。

Result: 在MELLA数据集上进行微调后，八种语言在不同的MLLM骨干模型上均表现出普遍的性能提升，模型能够生成“厚描述”（thick descriptions）。研究验证了性能提升来源于文化知识和语言能力的双重增强。

Conclusion: 所提出的双源数据策略和MELLA数据集能有效提升MLLMs在低资源语言环境中的性能，通过同时增强模型的语言能力和文化意识，使其能生成更具信息量和文化相关性的描述。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in
high-resource languages. However, their effectiveness diminishes significantly
in the contexts of low-resource languages. Current multilingual enhancement
methods are often limited to text modality or rely solely on machine
translation. While such approaches help models acquire basic linguistic
capabilities and produce "thin descriptions", they neglect the importance of
multimodal informativeness and cultural groundedness, both of which are crucial
for serving low-resource language users effectively. To bridge this gap, in
this study, we identify two significant objectives for a truly effective MLLM
in low-resource language settings, namely 1) linguistic capability and 2)
cultural groundedness, placing special emphasis on cultural awareness. To
achieve these dual objectives, we propose a dual-source strategy that guides
the collection of data tailored to each goal, sourcing native web alt-text for
culture and MLLM-generated captions for linguistics. As a concrete
implementation, we introduce MELLA, a multimodal, multilingual dataset.
Experiment results show that after fine-tuning on MELLA, there is a general
performance improvement for the eight languages on various MLLM backbones, with
models producing "thick descriptions". We verify that the performance gains are
from both cultural knowledge enhancement and linguistic capability enhancement.
Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [90] [SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images](https://arxiv.org/abs/2508.05202)
*Dongchen Si,Di Wang,Erzhong Gao,Xiaolei Qin,Liu Zhao,Jing Zhang,Minqiang Xu,Jianbo Zhan,Jianshe Wang,Lin Liu,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: 该研究构建了SPIE数据集，将地物光谱先验编码为文本属性，并提出了SPEX模型，一个多模态大语言模型，用于光谱遥感图像中的指令驱动地物覆盖提取，性能优于现有SOTA方法并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 光谱信息在遥感观测中至关重要，但现有视觉-语言模型未能充分利用光谱信息，导致在多光谱场景下性能不佳，尤其是在像素级解释方面。

Method: 1. 构建了SPIE数据集：将地物光谱先验（基于经典光谱指数计算）编码为大型语言模型可识别的文本属性。2. 提出了SPEX模型：一个用于指令驱动地物覆盖提取的多模态大语言模型。3. 引入了多尺度特征聚合、令牌上下文凝缩和多光谱视觉预训练等组件和训练策略，以实现精确和灵活的像素级解释。

Result: SPEX在五个公开多光谱数据集上的实验表明，其在提取植被、建筑物、水体等典型地物类别方面持续优于现有最先进方法。此外，SPEX能够为其预测生成文本解释，增强了可解释性和用户友好性。

Conclusion: SPEX是首个专门用于光谱遥感图像中地物覆盖提取的多模态视觉-语言模型，它通过有效利用光谱信息，实现了精确、灵活、可解释和用户友好的像素级地物解释。

Abstract: Spectral information has long been recognized as a critical cue in remote
sensing observations. Although numerous vision-language models have been
developed for pixel-level interpretation, spectral information remains
underutilized, resulting in suboptimal performance, particularly in
multispectral scenarios. To address this limitation, we construct a
vision-language instruction-following dataset named SPIE, which encodes
spectral priors of land-cover objects into textual attributes recognizable by
large language models (LLMs), based on classical spectral index computations.
Leveraging this dataset, we propose SPEX, a multimodal LLM designed for
instruction-driven land cover extraction. To this end, we introduce several
carefully designed components and training strategies, including multiscale
feature aggregation, token context condensation, and multispectral visual
pre-training, to achieve precise and flexible pixel-level interpretation. To
the best of our knowledge, SPEX is the first multimodal vision-language model
dedicated to land cover extraction in spectral remote sensing imagery.
Extensive experiments on five public multispectral datasets demonstrate that
SPEX consistently outperforms existing state-of-the-art methods in extracting
typical land cover categories such as vegetation, buildings, and water bodies.
Moreover, SPEX is capable of generating textual explanations for its
predictions, thereby enhancing interpretability and user-friendliness. Code
will be released at: https://github.com/MiliLab/SPEX.

</details>


### [91] [VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test](https://arxiv.org/abs/2508.05299)
*Meiqi Wu,Yaxuan Kang,Xuchen Li,Shiyu Hu,Xiaotang Chen,Yunfeng Kang,Weiqiang Wang,Kaiqi Huang*

Main category: cs.CV

TL;DR: 本文提出一种基于大语言模型（LLM）的视觉-语义（VS-LLM）方法，用于自动化分析“一个人从树上摘苹果”主题的绘画投射测试（DPT）草图，以评估参与者的抑郁状态，旨在解决传统人工评估耗时且依赖经验的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的绘画投射测试（DPT）中，“一个人从树上摘苹果”（PPAT）主题的草图解读费时费力，且高度依赖心理学家的经验。此外，PPAT测试有时间限制且禁止口头提醒，导致绘画准确性低和细节描绘不足，这些都限制了大规模自动化评估的实施。

Method: 本文提出了一种有效的识别方法来支持大规模自动化DPT。具体措施包括：1) 提供一个实验环境，用于PPAT草图的自动化抑郁评估分析；2) 提出一种基于大语言模型（LLM）的视觉-语义（VS-LLM）抑郁评估方法，该方法更侧重于草图的整体评估，如颜色使用和空间利用；3) 提供了数据集和代码。

Result: 实验结果表明，与心理学家的人工评估方法相比，本文提出的方法在抑郁评估方面提升了17.6%。

Conclusion: 这项工作有望促进基于PPAT草图元素识别的心理状态评估研究，为心理学家进行大规模自动化DPT提供支持。研究的数据集和代码已公开。

Abstract: The Drawing Projection Test (DPT) is an essential tool in art therapy,
allowing psychologists to assess participants' mental states through their
sketches. Specifically, through sketches with the theme of "a person picking an
apple from a tree (PPAT)", it can be revealed whether the participants are in
mental states such as depression. Compared with scales, the DPT can enrich
psychologists' understanding of an individual's mental state. However, the
interpretation of the PPAT is laborious and depends on the experience of the
psychologists. To address this issue, we propose an effective identification
method to support psychologists in conducting a large-scale automatic DPT.
Unlike traditional sketch recognition, DPT more focus on the overall evaluation
of the sketches, such as color usage and space utilization. Moreover, PPAT
imposes a time limit and prohibits verbal reminders, resulting in low drawing
accuracy and a lack of detailed depiction. To address these challenges, we
propose the following efforts: (1) Providing an experimental environment for
automated analysis of PPAT sketches for depression assessment; (2) Offering a
Visual-Semantic depression assessment based on LLM (VS-LLM) method; (3)
Experimental results demonstrate that our method improves by 17.6% compared to
the psychologist assessment method. We anticipate that this work will
contribute to the research in mental state assessment based on PPAT sketches'
elements recognition. Our datasets and codes are available at
https://github.com/wmeiqi/VS-LLM.

</details>


### [92] [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
*Luozheng Qin,Jia Gong,Yuqing Sun,Tianjiao Li,Mengping Yang,Xiaomeng Yang,Chao Qu,Zhiyu Tan,Hao Li*

Main category: cs.CV

TL;DR: Uni-CoT是一个统一的思维链框架，通过一个能够理解和生成图像的单一模型，实现了连贯且有根据的多模态推理，并在视觉推理任务上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链（CoT）方法在视觉-语言推理任务中面临挑战，因为它们难以有效建模视觉状态转换，或由于碎片化架构导致视觉轨迹不连贯。这限制了CoT在多模态推理中的应用。

Method: Uni-CoT提出了一个统一的思维链框架，利用一个同时具备图像理解和生成能力的模型来处理视觉内容和演变的视觉状态。为降低计算成本，它引入了两级推理范式：用于高层任务规划的宏观级CoT和用于子任务执行的微观级CoT。此外，采用结构化训练范式，结合交错的图像-文本监督（宏观级CoT）和多任务目标（微观级CoT）。

Result: Uni-CoT在推理驱动的图像生成基准（WISE）和编辑基准（RISE和KRIS）上均取得了最先进的（SOTA）性能，并展现出强大的泛化能力。其设计使得所有实验仅需8个A100 GPU即可高效完成。

Conclusion: Uni-CoT通过其统一模型、两级推理范式和结构化训练，成功克服了多模态思维链的挑战，为多模态推理提供了一个有前景的、可扩展且连贯的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large
Language Models (LLMs) by decomposing complex tasks into simpler, sequential
subtasks. However, extending CoT to vision-language reasoning tasks remains
challenging, as it often requires interpreting transitions of visual states to
support reasoning. Existing methods often struggle with this due to limited
capacity of modeling visual state transitions or incoherent visual trajectories
caused by fragmented architectures.
  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought
framework that enables coherent and grounded multimodal reasoning within a
single unified model. The key idea is to leverage a model capable of both image
understanding and generation to reason over visual content and model evolving
visual states. However, empowering a unified model to achieve that is
non-trivial, given the high computational cost and the burden of training. To
address this, Uni-CoT introduces a novel two-level reasoning paradigm: A
Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask
execution. This design significantly reduces the computational overhead.
Furthermore, we introduce a structured training paradigm that combines
interleaved image-text supervision for macro-level CoT with multi-task
objectives for micro-level CoT. Together, these innovations allow Uni-CoT to
perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our
design, all experiments can be efficiently completed using only 8 A100 GPUs
with 80GB VRAM each. Experimental results on reasoning-driven image generation
benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT
demonstrates SOTA performance and strong generalization, establishing Uni-CoT
as a promising solution for multi-modal reasoning. Project Page and Code:
https://sais-fuxi.github.io/projects/uni-cot/

</details>


### [93] [EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery](https://arxiv.org/abs/2508.05205)
*Bingyu Yang,Qingyao Tian,Yimeng Geng,Huai Liao,Xinyan Huang,Jiebo Luo,Hongbin Liu*

Main category: cs.CV

TL;DR: 本文提出了EndoMatcher，一个可泛化的内窥镜图像匹配器，通过大规模、多域数据预训练来解决内窥镜图像匹配中视觉条件困难和标注数据稀缺的问题，并在零样本泛化任务中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像中的可泛化密集特征匹配对于机器人辅助任务（如3D重建、导航和手术场景理解）至关重要。然而，由于视觉条件困难（如纹理弱、视角变化大）和标注数据稀缺，这仍然是一个挑战。

Method: 本文提出EndoMatcher，采用双分支Vision Transformer提取多尺度特征，并通过双重交互块增强，以实现鲁棒的对应学习。为解决数据稀缺性，构建了首个多域内窥镜匹配数据集Endo-Mix6（包含约1.2M真实和合成图像对，跨越六个领域，标签通过SfM和模拟变换生成）。为应对数据集多样性和规模带来的训练挑战，采用了渐进式多目标训练策略，以促进平衡学习和提高跨域表示质量。

Result: EndoMatcher能够以零样本方式泛化到未见器官和成像条件。在Hamlyn和Bladder数据集上，EndoMatcher的内点匹配数量分别比现有SOTA方法增加了140.69%和201.43%。在Gastro-Matching数据集上，匹配方向预测准确率（MDPA）提高了9.40%，在挑战性的内窥镜条件下实现了密集且准确的匹配。

Conclusion: EndoMatcher通过创新的模型架构、大规模多域数据集和渐进式多目标训练策略，有效解决了内窥镜图像匹配中的核心挑战，实现了卓越的零样本泛化能力和高精度匹配，显著超越了现有技术水平。

Abstract: Generalizable dense feature matching in endoscopic images is crucial for
robot-assisted tasks, including 3D reconstruction, navigation, and surgical
scene understanding. Yet, it remains a challenge due to difficult visual
conditions (e.g., weak textures, large viewpoint variations) and a scarcity of
annotated data. To address these challenges, we propose EndoMatcher, a
generalizable endoscopic image matcher via large-scale, multi-domain data
pre-training. To address difficult visual conditions, EndoMatcher employs a
two-branch Vision Transformer to extract multi-scale features, enhanced by dual
interaction blocks for robust correspondence learning. To overcome data
scarcity and improve domain diversity, we construct Endo-Mix6, the first
multi-domain dataset for endoscopic matching. Endo-Mix6 consists of
approximately 1.2M real and synthetic image pairs across six domains, with
correspondence labels generated using Structure-from-Motion and simulated
transformations. The diversity and scale of Endo-Mix6 introduce new challenges
in training stability due to significant variations in dataset sizes,
distribution shifts, and error imbalance. To address them, a progressive
multi-objective training strategy is employed to promote balanced learning and
improve representation quality across domains. This enables EndoMatcher to
generalize across unseen organs and imaging conditions in a zero-shot fashion.
Extensive zero-shot matching experiments demonstrate that EndoMatcher increases
the number of inlier matches by 140.69% and 201.43% on the Hamlyn and Bladder
datasets over state-of-the-art methods, respectively, and improves the Matching
Direction Prediction Accuracy (MDPA) by 9.40% on the Gastro-Matching dataset,
achieving dense and accurate matching under challenging endoscopic conditions.
The code is publicly available at https://github.com/Beryl2000/EndoMatcher.

</details>


### [94] [mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering](https://arxiv.org/abs/2508.05318)
*Xu Yuan,Liangbo Ning,Wenqi Fan,Qing Li*

Main category: cs.CV

TL;DR: 针对知识密集型视觉问答（VQA）任务，本文提出了一种名为mKG-RAG的新型多模态知识增强生成框架，通过构建结构化的多模态知识图谱并采用双阶段检索策略，解决了传统RAG方法依赖非结构化文档导致内容不准确的问题，显著提升了VQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检索增强生成（RAG）的多模态大语言模型（MLLMs）在知识密集型VQA任务中，过度依赖非结构化文档，并忽视知识元素间的结构化关系，导致生成内容中常出现不相关或误导性信息，从而降低了答案的准确性和可靠性。

Method: 本文提出了多模态知识增强生成框架（mKG-RAG）。具体方法包括：1) 利用MLLM驱动的关键词提取和视觉-文本匹配技术，从多模态文档中提炼出语义一致、模态对齐的实体和关系，构建高质量的多模态知识图谱作为结构化知识表示。2) 引入双阶段检索策略，并配备问题感知多模态检索器，以提高检索效率并优化精度。

Result: 全面的实验证明，本文提出的方法显著优于现有方法，为知识密集型VQA任务设定了新的技术水平（SOTA）。

Conclusion: 通过将结构化的多模态知识图谱整合到RAG-based VQA框架中，本文提出的mKG-RAG方法有效解决了传统RAG在处理非结构化知识时面临的挑战，极大地提升了知识密集型VQA任务的性能和可靠性。

Abstract: Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand
internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating
external knowledge databases into the generation process, which is widely used
for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive
advancements, vanilla RAG-based VQA methods that rely on unstructured documents
and overlook the structural relationships among knowledge elements frequently
introduce irrelevant or misleading content, reducing answer accuracy and
reliability. To overcome these challenges, a promising solution is to integrate
multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the
generation by introducing structured multimodal knowledge. Therefore, in this
paper, we propose a novel multimodal knowledge-augmented generation framework
(mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks.
Specifically, our approach leverages MLLM-powered keyword extraction and
vision-text matching to distill semantically consistent and modality-aligned
entities/relationships from multimodal documents, constructing high-quality
multimodal KGs as structured knowledge representations. In addition, a
dual-stage retrieval strategy equipped with a question-aware multimodal
retriever is introduced to improve retrieval efficiency while refining
precision. Comprehensive experiments demonstrate that our approach
significantly outperforms existing methods, setting a new state-of-the-art for
knowledge-based VQA.

</details>


### [95] [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
*Yong Du,Yuchen Yan,Fei Tang,Zhengxi Lu,Chang Zong,Weiming Lu,Shengpei Jiang,Yongliang Shen*

Main category: cs.CV

TL;DR: 该研究提出GUI-RC和GUI-RCPO两种方法，通过利用模型对GUI元素多个预测的空间一致性来提高GUI接地任务的定位精度，无需额外训练或标签。


<details>
  <summary>Details</summary>
Motivation: 现有GUI接地方法依赖昂贵的像素级标注进行大量监督训练或强化学习，受限于数据成本和可用性。作者观察到模型生成多个预测时，空间重叠模式能揭示隐式置信度信号。

Method: 1. **GUI-RC (Region Consistency)**: 一种测试时缩放方法，通过从多个采样预测构建空间投票网格，识别模型一致性最高的共识区域，无需训练。2. **GUI-RCPO (Region Consistency Policy Optimization)**: 将GUI-RC的一致性模式转化为奖励，用于测试时强化学习，使模型能在推理时利用无标签数据迭代优化输出。

Result: GUI-RC在ScreenSpot基准上使准确率提高2-3%。在ScreenSpot-v2上，GUI-RC将Qwen2.5-VL-3B-Instruct的准确率从80.11%提升到83.57%，而GUI-RCPO通过自监督优化进一步提升到85.14%。

Conclusion: 该方法揭示了测试时缩放和测试时强化学习在GUI接地方面的巨大潜力，为开发更鲁棒、数据高效的GUI代理提供了有前景的路径。

Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural
language instructions to precise screen coordinates, is fundamental to
autonomous GUI agents. While existing methods achieve strong performance
through extensive supervised training or reinforcement learning with labeled
rewards, they remain constrained by the cost and availability of pixel-level
annotations. We observe that when models generate multiple predictions for the
same GUI element, the spatial overlap patterns reveal implicit confidence
signals that can guide more accurate localization. Leveraging this insight, we
propose GUI-RC (Region Consistency), a test-time scaling method that constructs
spatial voting grids from multiple sampled predictions to identify consensus
regions where models show highest agreement. Without any training, GUI-RC
improves accuracy by 2-3% across various architectures on ScreenSpot
benchmarks. We further introduce GUI-RCPO (Region Consistency Policy
Optimization), which transforms these consistency patterns into rewards for
test-time reinforcement learning. By computing how well each prediction aligns
with the collective consensus, GUI-RCPO enables models to iteratively refine
their outputs on unlabeled data during inference. Extensive experiments
demonstrate the generality of our approach: GUI-RC boosts
Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO
further improves it to 85.14% through self-supervised optimization. Our
approach reveals the untapped potential of test-time scaling and test-time
reinforcement learning for GUI grounding, offering a promising path toward more
robust and data-efficient GUI agents.

</details>


### [96] [VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)
*Sihan Yang,Runsen Xu,Chenhang Cui,Tai Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.CV

TL;DR: VFlowOpt是一种视觉令牌剪枝框架，通过引入新的重要性图计算、渐进式剪枝和循环机制，并利用视觉信息流指导优化剪枝策略，显著降低大型多模态模型（LMMs）的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在视觉-语言任务中表现出色，但其大量视觉令牌导致计算成本高昂。现有视觉令牌剪枝方法过于简化且探索不足，常导致性能显著下降。

Method: 本文提出了VFlowOpt框架，包含：1) 基于注意力派生的上下文相关性和补丁级信息熵计算视觉令牌重要性图；2) 带有循环机制的渐进式剪枝模块，聚合被剪枝令牌以避免信息损失；3) 视觉信息流引导方法，通过最小化剪枝前后LMM中令牌表示的差异来优化剪枝策略的超参数，将LMM中最后一个令牌视为文本-视觉交互最具代表性的信号。

Result: VFlowOpt能够剪枝90%的视觉令牌，同时保持可比的性能，KV-Cache内存减少89%，推理速度提升3.8倍。

Conclusion: VFlowOpt通过其创新的重要性图推导、渐进式剪枝及循环机制，并结合视觉信息流引导的优化策略，有效解决了LMM中视觉令牌冗余导致的计算效率问题，实现了高性能和高效率的平衡。

Abstract: Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging
numerous visual tokens for fine-grained visual information, but this token
redundancy results in significant computational costs. Previous research aimed
at reducing visual tokens during inference typically leverages importance maps
derived from attention scores among vision-only tokens or vision-language
tokens to prune tokens across one or multiple pruning stages. Despite this
progress, pruning frameworks and strategies remain simplistic and
insufficiently explored, often resulting in substantial performance
degradation. In this paper, we propose VFlowOpt, a token pruning framework that
introduces an importance map derivation process and a progressive pruning
module with a recycling mechanism. The hyperparameters of its pruning strategy
are further optimized by a visual information flow-guided method. Specifically,
we compute an importance map for image tokens based on their attention-derived
context relevance and patch-level information entropy. We then decide which
tokens to retain or prune and aggregate the pruned ones as recycled tokens to
avoid potential information loss. Finally, we apply a visual information
flow-guided method that regards the last token in the LMM as the most
representative signal of text-visual interactions. This method minimizes the
discrepancy between token representations in LMMs with and without pruning,
thereby enabling superior pruning strategies tailored to different LMMs.
Experiments demonstrate that VFlowOpt can prune 90% of visual tokens while
maintaining comparable performance, leading to an 89% reduction in KV-Cache
memory and 3.8 times faster inference.

</details>


### [97] [PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine Decoding for Chest X-ray Report Generation](https://arxiv.org/abs/2508.05353)
*Kang Liu,Zhuoqi Ma,Zikang Fang,Yunan Li,Kun Xie,Qiguang Miao*

Main category: cs.CV

TL;DR: 该论文提出了PriorRG框架，通过两阶段训练流程（先验引导对比预训练和先验感知粗到细解码），将患者特异性先验知识（如临床背景、既往影像）整合到胸部X光报告生成中，以提高报告的临床准确性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有胸部X光报告生成方法大多忽略了放射科医生诊断时依赖的患者特异性先验知识（如临床背景、最新既往影像），导致无法捕捉诊断意图或疾病进展，从而限制了报告的质量和临床实用性。

Method: 提出PriorRG框架，采用两阶段训练管道：1. 先验引导对比预训练：利用临床背景指导时空特征提取，使模型与放射学报告的内在时空语义对齐。2. 先验感知粗到细解码：在报告生成过程中，逐步将患者特异性先验知识与视觉编码器的隐藏状态融合，以对齐诊断焦点并追踪疾病进展。

Result: PriorRG在MIMIC-CXR和MIMIC-ABN数据集上均优于现有最先进方法。在MIMIC-CXR上，BLEU-4提高3.6%，F1分数提高3.8%；在MIMIC-ABN上，BLEU-1提高5.9%。

Conclusion: PriorRG框架通过有效利用患者特异性先验知识，生成了更具临床准确性和流畅性的胸部X光报告，有望减轻放射科医生的工作量。

Abstract: Chest X-ray report generation aims to reduce radiologists' workload by
automatically producing high-quality preliminary reports. A critical yet
underexplored aspect of this task is the effective use of patient-specific
prior knowledge -- including clinical context (e.g., symptoms, medical history)
and the most recent prior image -- which radiologists routinely rely on for
diagnostic reasoning. Most existing methods generate reports from single
images, neglecting this essential prior information and thus failing to capture
diagnostic intent or disease progression. To bridge this gap, we propose
PriorRG, a novel chest X-ray report generation framework that emulates
real-world clinical workflows via a two-stage training pipeline. In Stage 1, we
introduce a prior-guided contrastive pre-training scheme that leverages
clinical context to guide spatiotemporal feature extraction, allowing the model
to align more closely with the intrinsic spatiotemporal semantics in radiology
reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for
report generation that progressively integrates patient-specific prior
knowledge with the vision encoder's hidden states. This decoding allows the
model to align with diagnostic focus and track disease progression, thereby
enhancing the clinical accuracy and fluency of the generated reports. Extensive
experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG
outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score
improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and
checkpoints will be released upon acceptance.

</details>


### [98] [Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2508.05213)
*Jianming Liu,Wenlong Qiu,Haitao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种无需源数据的跨域小样本分割（CD-FSS）方法，通过结合文本和视觉信息，在目标域上实现任务自适应，显著优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有CD-FSS方法依赖源域数据进行训练，但数据隐私、数据传输和训练成本的增加，使得开发无需源数据的CD-FSS方法变得至关重要。

Method: 该方法首先在预训练主干网络的特征金字塔中添加任务特定注意力适配器（TSAA），以适应目标任务的多级特征。TSAA的参数通过视觉-视觉嵌入对齐（VVEA）模块和文本-视觉嵌入对齐（TVEA）模块进行训练。VVEA模块利用全局-局部视觉特征对齐不同视图的图像特征，TVEA模块利用预对齐多模态特征（如CLIP）的文本先验来指导跨模态自适应。最后，通过密集比较操作和跳跃连接融合这些模块的输出，生成精炼的预测掩码。

Result: 在1-shot和5-shot设置下，该方法在四个跨域数据集上分别实现了平均2.18%和4.11%的分割精度提升，显著优于现有最先进的CD-FSS方法。

Conclusion: 所提出的无需源数据的CD-FSS方法通过有效利用文本和视觉信息，克服了跨域挑战，并在数据隐私和成本限制下取得了卓越的分割性能。

Abstract: Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with
few labeled samples. However, its performance significantly degrades when
domain discrepancies exist between training and deployment. Cross-Domain
Few-Shot Segmentation(CD-FSS) is proposed to mitigate such performance
degradation. Current CD-FSS methods primarily sought to develop segmentation
models on a source domain capable of cross-domain generalization. However,
driven by escalating concerns over data privacy and the imperative to minimize
data transfer and training expenses, the development of source-free CD-FSS
approaches has become essential. In this work, we propose a source-free CD-FSS
method that leverages both textual and visual information to facilitate target
domain task adaptation without requiring source domain data. Specifically, we
first append Task-Specific Attention Adapters (TSAA) to the feature pyramid of
a pretrained backbone, which adapt multi-level features extracted from the
shared pre-trained backbone to the target task. Then, the parameters of the
TSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and
a Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes
global-local visual features to align image features across different views,
while the TVEA module leverages textual priors from pre-aligned multi-modal
features (e.g., from CLIP) to guide cross-modal adaptation. By combining the
outputs of these modules through dense comparison operations and subsequent
fusion via skip connections, our method produces refined prediction masks.
Under both 1-shot and 5-shot settings, the proposed approach achieves average
segmentation accuracy improvements of 2.18\% and 4.11\%, respectively, across
four cross-domain datasets, significantly outperforming state-of-the-art CD-FSS
methods. Code are available at https://github.com/ljm198134/TVGTANet.

</details>


### [99] [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation](https://arxiv.org/abs/2508.05399)
*Wonjun Kang,Byeongkeun Ahn,Minjae Lee,Kevin Galim,Seunghyuk Oh,Hyung Il Koo,Nam Ik Cho*

Main category: cs.CV

TL;DR: 本文提出UNCAGE，一种无需训练的方法，通过利用注意力图优先解掩码代表独立对象的token，以提高掩码生成Transformer在文本到图像生成中的组合性保真度。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）生成，尤其是组合性T2I生成，仍具挑战性。尽管扩散模型在此方面已广泛研究，但掩码生成Transformer（MGTs）也存在类似的局性（属性绑定和文本-图像对齐），且在这方面探索不足。

Method: 提出Unmasking with Contrastive Attention Guidance (UNCAGE)，一种新颖的、无需训练的方法。该方法通过利用注意力图，优先解掩码清晰代表独立对象的token，从而提高组合性保真度。

Result: UNCAGE在多个基准和指标的定量和定性评估中持续提升性能，且推理开销可忽略不计。

Conclusion: UNCAGE有效解决了掩码生成Transformer在文本到图像生成中的组合性保真度问题，且无需额外训练，取得了显著改进。

Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion
Models and Autoregressive Models. Recently, Masked Generative Transformers have
gained attention as an alternative to Autoregressive Models to overcome the
inherent limitations of causal attention and autoregressive decoding through
bidirectional attention and parallel decoding, enabling efficient and
high-quality image generation. However, compositional T2I generation remains
challenging, as even state-of-the-art Diffusion Models often fail to accurately
bind attributes and achieve proper text-image alignment. While Diffusion Models
have been extensively studied for this issue, Masked Generative Transformers
exhibit similar limitations but have not been explored in this context. To
address this, we propose Unmasking with Contrastive Attention Guidance
(UNCAGE), a novel training-free method that improves compositional fidelity by
leveraging attention maps to prioritize the unmasking of tokens that clearly
represent individual objects. UNCAGE consistently improves performance in both
quantitative and qualitative evaluations across multiple benchmarks and
metrics, with negligible inference overhead. Our code is available at
https://github.com/furiosa-ai/uncage.

</details>


### [100] [Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2](https://arxiv.org/abs/2508.05227)
*Semanur Küçük,Cosimo Della Santina,Angeliki Laskari*

Main category: cs.CV

TL;DR: 该研究首次证明，通过少量图像微调的SAM v2.1模型能准确分割多相流中高度不规则的气泡结构。


<details>
  <summary>Details</summary>
Motivation: 多相流中气泡分割是一个关键但未解决的挑战，传统方法和现有学习方法假设气泡接近球形，无法处理变形、聚结或破裂的非球形气泡，特别是在空气润滑系统等场景中。

Method: 将气泡分割任务视为迁移学习问题，并对Segment Anything Model (SAM) v2.1进行微调。

Result: 微调后的SAM v2.1模型能够使用低至100张带标注的图像，准确分割高度非凸、不规则的气泡结构。

Conclusion: SAM v2.1的微调为解决多相流中复杂气泡的精确分割提供了一种有效的新方法。

Abstract: Segmenting gas bubbles in multiphase flows is a critical yet unsolved
challenge in numerous industrial settings, from metallurgical processing to
maritime drag reduction. Traditional approaches-and most recent learning-based
methods-assume near-spherical shapes, limiting their effectiveness in regimes
where bubbles undergo deformation, coalescence, or breakup. This complexity is
particularly evident in air lubrication systems, where coalesced bubbles form
amorphous and topologically diverse patches. In this work, we revisit the
problem through the lens of modern vision foundation models. We cast the task
as a transfer learning problem and demonstrate, for the first time, that a
fine-tuned Segment Anything Model SAM v2.1 can accurately segment highly
non-convex, irregular bubble structures using as few as 100 annotated images.

</details>


### [101] [Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions](https://arxiv.org/abs/2508.05430)
*Hubert Baniecki,Maximilian Muschalik,Fabian Fumagalli,Barbara Hammer,Eyke Hüllermeier,Przemyslaw Biecek*

Main category: cs.CV

TL;DR: FIxLIP是一种基于博弈论的统一方法，用于解释语言-图像预训练（LIP）模型中的相似度，通过二阶交互解释解决了现有显著图仅捕获一阶归因的局限性，并提高了计算效率和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有的LIP模型解释方法（如显著图）只能捕获一阶归因，无法揭示模型中固有的复杂跨模态交互，限制了对模型相似度输出的深入理解。

Method: 引入了基于博弈论的FIxLIP方法，利用加权Banzhaf交互指数来分解视觉-语言编码器中的相似度，相比Shapley框架更灵活且计算高效。同时，提出了将指向游戏和插入/删除曲线下面积等解释评估指标自然地扩展到二阶交互解释的方法。

Result: 在MS COCO和ImageNet-1k基准测试上的实验验证了FIxLIP等二阶方法优于一阶归因方法，能提供高质量的解释。此外，FIxLIP还可用于比较不同模型（如CLIP vs. SigLIP-2，ViT-B/32 vs. ViT-L/16）。

Conclusion: FIxLIP提供了一种更准确、高效的LIP模型解释方法，通过捕捉二阶跨模态交互，提升了对模型行为的理解，并为比较不同视觉-语言模型提供了实用工具。

Abstract: Language-image pre-training (LIP) enables the development of vision-language
models capable of zero-shot classification, localization, multimodal retrieval,
and semantic understanding. Various explanation methods have been proposed to
visualize the importance of input image-text pairs on the model's similarity
outputs. However, popular saliency maps are limited by capturing only
first-order attributions, overlooking the complex cross-modal interactions
intrinsic to such encoders. We introduce faithful interaction explanations of
LIP models (FIxLIP) as a unified approach to decomposing the similarity in
vision-language encoders. FIxLIP is rooted in game theory, where we analyze how
using the weighted Banzhaf interaction index offers greater flexibility and
improves computational efficiency over the Shapley interaction quantification
framework. From a practical perspective, we propose how to naturally extend
explanation evaluation metrics, like the pointing game and area between the
insertion/deletion curves, to second-order interaction explanations.
Experiments on MS COCO and ImageNet-1k benchmarks validate that second-order
methods like FIxLIP outperform first-order attribution methods. Beyond
delivering high-quality explanations, we demonstrate the utility of FIxLIP in
comparing different models like CLIP vs. SigLIP-2 and ViT-B/32 vs. ViT-L/16.

</details>


### [102] [ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models](https://arxiv.org/abs/2508.05236)
*Yatong Lan,Jingfeng Chen,Yiru Wang,Lei He*

Main category: cs.CV

TL;DR: Arbiviewgen是一个新颖的基于扩散模型的框架，用于生成任意视角的车辆可控相机图像，通过特征感知自适应视图拼接（FAVS）和跨视图一致性自监督学习（CVC-SSL）解决了外推视图缺乏真实数据的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的任意视角图像生成任务面临挑战，因为外推视角缺乏真实数据，这阻碍了高保真生成模型的训练。

Method: 提出Arbiviewgen框架，基于扩散模型。包含两个关键组件：1. 特征感知自适应视图拼接（FAVS）：通过相机姿态进行粗略几何对应，再通过改进的特征匹配进行细粒度对齐，并用聚类分析识别高置信度匹配区域。2. 跨视图一致性自监督学习（CVC-SSL）：模型从合成的拼接图像中重建原始相机视图，通过扩散模型强制实现跨视图一致性，无需外推数据监督。训练仅需要多相机图像及其关联姿态。

Result: Arbiviewgen是首个能够实现多种车辆配置下可控任意视角相机图像生成的方法，且无需额外的传感器或深度图。

Conclusion: Arbiviewgen通过创新的自监督学习方法，有效解决了任意视角图像生成中真实数据缺失的问题，为自动驾驶领域提供了高保真、可控的图像生成能力，且仅依赖于标准的多相机数据进行训练。

Abstract: Arbitrary viewpoint image generation holds significant potential for
autonomous driving, yet remains a challenging task due to the lack of
ground-truth data for extrapolated views, which hampers the training of
high-fidelity generative models. In this work, we propose Arbiviewgen, a novel
diffusion-based framework for the generation of controllable camera images from
arbitrary points of view. To address the absence of ground-truth data in unseen
views, we introduce two key components: Feature-Aware Adaptive View Stitching
(FAVS) and Cross-View Consistency Self-Supervised Learning (CVC-SSL). FAVS
employs a hierarchical matching strategy that first establishes coarse
geometric correspondences using camera poses, then performs fine-grained
alignment through improved feature matching algorithms, and identifies
high-confidence matching regions via clustering analysis. Building upon this,
CVC-SSL adopts a self-supervised training paradigm where the model reconstructs
the original camera views from the synthesized stitched images using a
diffusion model, enforcing cross-view consistency without requiring supervision
from extrapolated data. Our framework requires only multi-camera images and
their associated poses for training, eliminating the need for additional
sensors or depth maps. To our knowledge, Arbiviewgen is the first method
capable of controllable arbitrary view camera image generation in multiple
vehicle configurations.

</details>


### [103] [B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding](https://arxiv.org/abs/2508.05269)
*Changho Choi,Youngwoo Shin,Gyojin Han,Dong-Jae Lee,Junmo Kim*

Main category: cs.CV

TL;DR: 该论文提出了B4DL，一个用于4D激光雷达理解的新基准，以及一个可扩展的数据生成管道和首个直接处理原始4D激光雷达并与语言理解结合的多模态大语言模型（MLLM）。


<details>
  <summary>Details</summary>
Motivation: 尽管4D激光雷达在理解动态户外环境方面具有巨大潜力，但由于缺乏高质量的模态特定标注和能处理其高维组成的多模态大语言模型架构，它在MLLM领域仍未得到充分探索。

Method: 引入了B4DL基准数据集，设计了一个可扩展的数据生成管道，并提出了一个MLLM模型，该模型通过与语言理解相结合，首次直接处理原始4D激光雷达数据。

Result: 结合其数据集和基准，所提出的模型为动态户外环境中的时空推理提供了一个统一的解决方案，并提供了渲染的4D激光雷达视频、生成的数据集和在各种场景下的推理输出。

Conclusion: 该研究提供了一个统一的解决方案，通过结合新的基准、数据生成方法和定制的MLLM模型，显著推动了4D激光雷达在动态户外环境时空推理方面的理解和应用。

Abstract: Understanding dynamic outdoor environments requires capturing complex object
interactions and their evolution over time. LiDAR-based 4D point clouds provide
precise spatial geometry and rich temporal cues, making them ideal for
representing real-world scenes. However, despite their potential, 4D LiDAR
remains underexplored in the context of Multimodal Large Language Models
(MLLMs) due to the absence of high-quality, modality-specific annotations and
the lack of MLLM architectures capable of processing its high-dimensional
composition. To address these challenges, we introduce B4DL, a new benchmark
specifically designed for training and evaluating MLLMs on 4D LiDAR
understanding. In addition, we propose a scalable data generation pipeline and
an MLLM model that, for the first time, directly processes raw 4D LiDAR by
bridging it with language understanding. Combined with our dataset and
benchmark, our model offers a unified solution for spatio-temporal reasoning in
dynamic outdoor environments. We provide rendered 4D LiDAR videos, generated
dataset, and inference outputs on diverse scenarios at:
https://mmb4dl.github.io/mmb4dl/

</details>


### [104] [Wavelet-Guided Dual-Frequency Encoding for Remote Sensing Change Detection](https://arxiv.org/abs/2508.05271)
*Xiaoyang Zhang,Guodong Fan,Guang-Yong Chen,Zhen Hua,Jinjiang Li,Min Gan,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种名为WGDF的小波引导双频编码方法，用于遥感图像变化检测，通过结合高频（细节）和低频（结构）信息来提高对细微变化的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像变化检测方法主要依赖空间域建模，导致特征表示多样性有限，难以检测细微变化区域。作者观察到频域特征建模，特别是在小波域中，可以放大频率分量中的细粒度差异，从而增强对空间域中难以捕获的边缘变化的感知。

Method: 提出WGDF方法。首先使用离散小波变换（DWT）将输入图像分解为高频（建模局部细节）和低频（建模全局结构）分量。在高频分支中，设计了双频特征增强（DFFE）模块以强化边缘细节表示，并引入频域交互差异（FDID）模块以增强细粒度变化的建模。在低频分支中，利用Transformer捕获全局语义关系，并采用渐进式上下文差异模块（PCDM）逐步细化变化区域。最后，协同融合高频和低频特征，以统一局部敏感性和全局判别性。

Result: 在多个遥感数据集上的大量实验表明，WGDF显著缓解了边缘模糊问题，并与现有最先进的方法相比，实现了卓越的检测精度和鲁棒性。

Conclusion: WGDF方法通过有效融合高频和低频信息，显著提高了遥感图像变化检测的性能，尤其是在处理边缘模糊和细微变化方面表现出色，实现了局部敏感性和全局判别性的协同统一。

Abstract: Change detection in remote sensing imagery plays a vital role in various
engineering applications, such as natural disaster monitoring, urban expansion
tracking, and infrastructure management. Despite the remarkable progress of
deep learning in recent years, most existing methods still rely on
spatial-domain modeling, where the limited diversity of feature representations
hinders the detection of subtle change regions. We observe that
frequency-domain feature modeling particularly in the wavelet domain an amplify
fine-grained differences in frequency components, enhancing the perception of
edge changes that are challenging to capture in the spatial domain. Thus, we
propose a method called Wavelet-Guided Dual-Frequency Encoding (WGDF).
Specifically, we first apply Discrete Wavelet Transform (DWT) to decompose the
input images into high-frequency and low-frequency components, which are used
to model local details and global structures, respectively. In the
high-frequency branch, we design a Dual-Frequency Feature Enhancement (DFFE)
module to strengthen edge detail representation and introduce a
Frequency-Domain Interactive Difference (FDID) module to enhance the modeling
of fine-grained changes. In the low-frequency branch, we exploit Transformers
to capture global semantic relationships and employ a Progressive Contextual
Difference Module (PCDM) to progressively refine change regions, enabling
precise structural semantic characterization. Finally, the high- and
low-frequency features are synergistically fused to unify local sensitivity
with global discriminability. Extensive experiments on multiple remote sensing
datasets demonstrate that WGDF significantly alleviates edge ambiguity and
achieves superior detection accuracy and robustness compared to
state-of-the-art methods. The code will be available at
https://github.com/boshizhang123/WGDF.

</details>


### [105] [CoCAViT: Compact Vision Transformer with Robust Global Coordination](https://arxiv.org/abs/2508.05307)
*Xuyang Wang,Lingjuan Miao,Zhiqiang Zhou*

Main category: cs.CV

TL;DR: 该论文提出CoCAViT，一种新型视觉骨干网络，旨在解决高效小模型在域外（OOD）数据上泛化性能差的问题，通过引入动态、域感知的全局注意力机制，实现了鲁棒的实时视觉表示。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型视觉骨干网络通过预训练展现出强大的通用特征学习能力，同时许多高效架构在域内基准测试上表现出色。然而，作者观察到小型模型在域外数据上的性能下降不成比例地大，这表明现有高效模型的泛化性能存在缺陷。

Method: 作者首先识别并解决了导致小型模型鲁棒性不足的关键架构瓶颈和不当设计选择。其次，引入了一种名为“协调器-补丁交叉注意力”（CoCA）的机制，该机制具有动态、域感知的全局tokens，以增强局部-全局特征建模，并以最小的计算开销自适应地捕获跨域的鲁棒模式。将这些改进集成后，提出了CoCAViT模型。

Result: CoCAViT-28M在224*224分辨率下，ImageNet-1K上达到84.0%的top-1准确率，并在多个OOD基准测试上相比竞品模型有显著提升。同时，在COCO目标检测上达到52.2 mAP，在ADE20K语义分割上达到51.3 mIOU，并保持低延迟。

Conclusion: CoCAViT是一种为鲁棒实时视觉表示设计的新型视觉骨干网络。通过解决现有高效模型在OOD数据上的泛化不足问题，并引入CoCA机制增强局部-全局特征建模，CoCAViT在保持计算效率的同时，显著提升了模型在各种视觉任务上的性能和跨域泛化能力。

Abstract: In recent years, large-scale visual backbones have demonstrated remarkable
capabilities in learning general-purpose features from images via extensive
pre-training. Concurrently, many efficient architectures have emerged that have
performance comparable to that of larger models on in-domain benchmarks.
However, we observe that for smaller models, the performance drop on
out-of-distribution (OOD) data is disproportionately larger, indicating a
deficiency in the generalization performance of existing efficient models. To
address this, we identify key architectural bottlenecks and inappropriate
design choices that contribute to this issue, retaining robustness for smaller
models. To restore the global field of pure window attention, we further
introduce a Coordinator-patch Cross Attention (CoCA) mechanism, featuring
dynamic, domain-aware global tokens that enhance local-global feature modeling
and adaptively capture robust patterns across domains with minimal
computational overhead. Integrating these advancements, we present CoCAViT, a
novel visual backbone designed for robust real-time visual representation.
Extensive experiments empirically validate our design. At a resolution of
224*224, CoCAViT-28M achieves 84.0% top-1 accuracy on ImageNet-1K, with
significant gains on multiple OOD benchmarks, compared to competing models. It
also attains 52.2 mAP on COCO object detection and 51.3 mIOU on ADE20K semantic
segmentation, while maintaining low latency.

</details>


### [106] [Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting](https://arxiv.org/abs/2508.05323)
*Frank Ruis,Gertjan Burghouts,Hugo Kuijf*

Main category: cs.CV

TL;DR: 本文提出了一种受Textual Inversion启发的开放词汇目标检测方法，通过学习新的或改进现有词元，仅需少量样本即可让大型视觉语言模型（VLM）识别新颖或细粒度对象，同时保持VLM原有能力和冻结其权重。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练视觉语言模型（VLMs）在目标检测方面表现出色并具有强大的零样本能力，但为了在特定目标上获得最佳性能，仍需要某种形式的微调。然而，这种微调通常会导致模型丧失其原始的自然语言查询和零样本能力。

Method: 受Textual Inversion（TI）在文本到图像扩散模型中个性化成功的启发，作者提出了一种类似的公式用于开放词汇目标检测。该方法通过学习新的或改进现有词元来扩展VLM的词汇，仅需最少三个样本即可准确检测新颖或细粒度对象。学习到的词元与原始VLM权重完全兼容，同时保持VLM权重冻结，保留了模型的原始基准性能及其零样本域迁移等现有能力。存储和梯度计算仅限于词元嵌入维度，计算量远低于全模型微调。

Result: 该方法学习的词元与原始VLM权重完全兼容，保持了模型的原始基准性能和零样本域迁移等现有能力（例如，在仅用真实照片训练后检测对象的草图）。存储和梯度计算需求显著低于全模型微调。通过广泛的定量和定性实验，该方法匹配或超越了那些受遗忘问题影响的基线方法。

Conclusion: 所提出的方法成功地将Textual Inversion的理念应用于开放词汇目标检测，使得VLM能够高效地学习识别新颖或细粒度对象，同时完整保留其强大的零样本能力和原始性能，且计算成本较低，有效解决了传统微调带来的能力退化问题。

Abstract: Recent progress in large pre-trained vision language models (VLMs) has
reached state-of-the-art performance on several object detection benchmarks and
boasts strong zero-shot capabilities, but for optimal performance on specific
targets some form of finetuning is still necessary. While the initial VLM
weights allow for great few-shot transfer learning, this usually involves the
loss of the original natural language querying and zero-shot capabilities.
Inspired by the success of Textual Inversion (TI) in personalizing
text-to-image diffusion models, we propose a similar formulation for
open-vocabulary object detection. TI allows extending the VLM vocabulary by
learning new or improving existing tokens to accurately detect novel or
fine-grained objects from as little as three examples. The learned tokens are
completely compatible with the original VLM weights while keeping them frozen,
retaining the original model's benchmark performance, and leveraging its
existing capabilities such as zero-shot domain transfer (e.g., detecting a
sketch of an object after training only on real photos). The storage and
gradient calculations are limited to the token embedding dimension, requiring
significantly less compute than full-model fine-tuning. We evaluated whether
the method matches or outperforms the baseline methods that suffer from
forgetting in a wide variety of quantitative and qualitative experiments.

</details>


### [107] [3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering](https://arxiv.org/abs/2508.05343)
*Junyu Zhou,Yuyang Huang,Wenrui Dai,Junni Zou,Ziyang Zheng,Nuowen Kan,Chenglin Li,Hongkai Xiong*

Main category: cs.CV

TL;DR: 本文提出3DGabSplat，利用新颖的3D Gabor基元代替3D高斯基元，以解决3DGS在表示高频细节、训练效率和内存消耗方面的局限性，实现了卓越的新视图合成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）虽然实现了实时渲染和高保真新视图合成，但其高斯函数本质上是低通的，难以表示3D场景中的高频细节。此外，它会导致冗余基元，降低训练和渲染效率，并带来过多的内存开销。

Method: 本文提出了3D Gabor溅射（3DGabSplat），它利用一种新颖的、基于3D Gabor的基元，该基元具有多个方向性3D频率响应，用于辐射场表示。该基元形成一个滤波器组，包含不同频率的多个3D Gabor核，以增强捕获精细3D细节的灵活性和效率。此外，开发了一个高效的基于CUDA的栅格化器来投影3D Gabor基元表征的多个方向性3D频率分量到2D图像平面，并引入了一种频率自适应机制用于基元的自适应联合优化。3DGabSplat被设计为可插拔的内核，可无缝集成到现有3DGS范式中。

Result: 实验结果表明，3DGabSplat优于3DGS及其使用替代基元的变体，并在真实世界和合成场景中实现了最先进的渲染质量。显著地，相对于3DGS，PSNR增益高达1.35 dB，同时减少了基元数量和内存消耗。

Conclusion: 3DGabSplat通过引入多方向3D频率响应的Gabor基元，成功克服了3DGS在高频细节表示、效率和内存方面的局限性。它提供了一种可扩展的即插即用内核，显著提升了新视图合成的效率和质量，达到了最先进的性能。

Abstract: Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time
rendering while maintaining high-fidelity novel view synthesis. However, 3DGS
resorts to the Gaussian function that is low-pass by nature and is restricted
in representing high-frequency details in 3D scenes. Moreover, it causes
redundant primitives with degraded training and rendering efficiency and
excessive memory overhead. To overcome these limitations, we propose 3D Gabor
Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with
multiple directional 3D frequency responses for radiance field representation
supervised by multi-view images. The proposed 3D Gabor-based primitive forms a
filter bank incorporating multiple 3D Gabor kernels at different frequencies to
enhance flexibility and efficiency in capturing fine 3D details. Furthermore,
to achieve novel view rendering, an efficient CUDA-based rasterizer is
developed to project the multiple directional 3D frequency components
characterized by 3D Gabor-based primitives onto the 2D image plane, and a
frequency-adaptive mechanism is presented for adaptive joint optimization of
primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless
integration into existing 3DGS paradigms to enhance both efficiency and quality
of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat
outperforms 3DGS and its variants using alternative primitives, and achieves
state-of-the-art rendering quality across both real-world and synthetic scenes.
Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously
reduced number of primitives and memory consumption.

</details>


### [108] [Cross-View Localization via Redundant Sliced Observations and A-Contrario Validation](https://arxiv.org/abs/2508.05369)
*Yongjun Zhang,Mingtao Xiong,Yi Wan,Gui-Song Xia*

Main category: cs.CV

TL;DR: Slice-Loc是一种两阶段跨视角定位（CVL）方法，通过将查询图像分割成子图像并对每个子图像进行姿态估计，引入冗余观测值，并通过a-contrario可靠性验证来提高定位精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的CVL方法仅输出单一的相机姿态观测值，缺乏测量学原理所需的冗余观测，导致难以通过观测数据相互验证来评估定位的可靠性，特别是在GNSS受限环境中智能车辆的离线自定位。

Method: Slice-Loc是一个两阶段方法。首先，它将查询图像分割成多个子图像（slices），并为每个子图像估计3自由度（3-DoF）姿态，从而创建冗余且独立的观测值。其次，提出了一种几何刚度公式来滤除错误的3-DoF姿态，并将内点合并以生成最终的相机姿态。此外，通过估计误报数量（NFA）并根据切片图像位置的分布，提出了一个量化定位意义的模型。

Result: 通过消除粗大误差，Slice-Loc显著提高了定位精度并有效检测故障。在滤除错误定位后，超过10米误差的比例降至3%以下。在DReSS数据集的跨城市测试中，Slice-Loc将平均定位误差从4.47米降至1.86米，平均方向误差从3.42°降至1.24°，性能优于现有最先进的方法。

Conclusion: Slice-Loc通过引入冗余观测和a-contrario可靠性验证，有效解决了传统CVL方法缺乏可靠性评估的问题，显著提高了定位精度并能有效检测定位失败，使其在GNSS受限环境下具有更强的实用性。

Abstract: Cross-view localization (CVL) matches ground-level images with aerial
references to determine the geo-position of a camera, enabling smart vehicles
to self-localize offline in GNSS-denied environments. However, most CVL methods
output only a single observation, the camera pose, and lack the redundant
observations required by surveying principles, making it challenging to assess
localization reliability through the mutual validation of observational data.
To tackle this, we introduce Slice-Loc, a two-stage method featuring an
a-contrario reliability validation for CVL. Instead of using the query image as
a single input, Slice-Loc divides it into sub-images and estimates the 3-DoF
pose for each slice, creating redundant and independent observations. Then, a
geometric rigidity formula is proposed to filter out the erroneous 3-DoF poses,
and the inliers are merged to generate the final camera pose. Furthermore, we
propose a model that quantifies the meaningfulness of localization by
estimating the number of false alarms (NFA), according to the distribution of
the locations of the sliced images. By eliminating gross errors, Slice-Loc
boosts localization accuracy and effectively detects failures. After filtering
out mislocalizations, Slice-Loc reduces the proportion of errors exceeding 10 m
to under 3\%. In cross-city tests on the DReSS dataset, Slice-Loc cuts the mean
localization error from 4.47 m to 1.86 m and the mean orientation error from
$\mathbf{3.42^{\circ}}$ to $\mathbf{1.24^{\circ}}$, outperforming
state-of-the-art methods. Code and dataset will be available at:
https://github.com/bnothing/Slice-Loc.

</details>


### [109] [CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation](https://arxiv.org/abs/2508.05375)
*Hamza Kalisch,Fabian Hörst,Jens Kleesiek,Ken Herrmann,Constantin Seibold*

Main category: cs.CV

TL;DR: CT-GRAPH是一种分层图注意力网络，通过显式建模解剖区域关系，结合细粒度器官特征和全局患者背景，显著提高了CT报告的自动生成质量，F1分数较现有技术提升7.9%。


<details>
  <summary>Details</summary>
Motivation: 医学影像在诊断中至关重要，但放射科医生工作量繁重，需要自动化生成放射报告来辅助。现有方法主要依赖全局图像特征，未能捕捉对准确报告至关重要的精细器官关系。

Method: 提出CT-GRAPH，一个分层图注意力网络，通过将解剖区域构建成图来显式建模放射学知识，将细粒度器官特征与粗粒度解剖系统及全局患者上下文联系起来。该方法利用预训练的3D医学特征编码器和解剖掩模获取全局和器官级别特征，在图中进一步精炼后集成到大型语言模型中以生成详细的医学报告。

Result: 在大型胸部CT数据集CT-RATE上评估了该方法在报告生成任务上的表现。对用于CT报告生成的预训练特征编码器进行了深入分析，结果显示该方法在F1分数上比现有最先进方法绝对提高了7.9%。

Conclusion: CT-GRAPH通过显式建模解剖区域之间的关系，显著提升了放射报告自动生成的准确性和详细程度，为辅助放射科医生工作提供了有效方案。

Abstract: As medical imaging is central to diagnostic processes, automating the
generation of radiology reports has become increasingly relevant to assist
radiologists with their heavy workloads. Most current methods rely solely on
global image features, failing to capture fine-grained organ relationships
crucial for accurate reporting. To this end, we propose CT-GRAPH, a
hierarchical graph attention network that explicitly models radiological
knowledge by structuring anatomical regions into a graph, linking fine-grained
organ features to coarser anatomical systems and a global patient context. Our
method leverages pretrained 3D medical feature encoders to obtain global and
organ-level features by utilizing anatomical masks. These features are further
refined within the graph and then integrated into a large language model to
generate detailed medical reports. We evaluate our approach for the task of
report generation on the large-scale chest CT dataset CT-RATE. We provide an
in-depth analysis of pretrained feature encoders for CT report generation and
show that our method achieves a substantial improvement of absolute 7.9\% in F1
score over current state-of-the-art methods. The code is publicly available at
https://github.com/hakal104/CT-GRAPH.

</details>


### [110] [Deformable Attention Graph Representation Learning for Histopathology Whole Slide Image Analysis](https://arxiv.org/abs/2508.05382)
*Mingxi Fu,Xitong Ling,Yuxuan Chen,Jiawen Li,fanglei fu,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于可变形注意力机制的图神经网络（GNN）框架，用于病理图像分析，通过引入学习到的空间偏移量，有效捕获全玻片图像（WSIs）和感兴趣区域（ROIs）中复杂的空间结构和组织依赖关系。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中全玻片图像和感兴趣区域的精确分类是一个基本挑战。现有主流的多实例学习（MIL）方法难以捕获组织结构间的空间依赖性。图神经网络（GNNs）虽能建模实例间关系，但多数依赖静态图拓扑，且忽略了组织斑块的物理空间位置。此外，传统注意力机制缺乏特异性，限制了其聚焦结构相关区域的能力。

Method: 提出了一种带有可变形注意力的新型GNN框架。该方法基于斑块特征构建动态加权有向图，每个节点通过注意力加权边从邻居聚合上下文信息。特别地，模型融入了由每个斑块真实坐标指导的可学习空间偏移量，使其能够自适应地关注幻灯片中形态学相关的区域，从而显著增强上下文感知能力同时保留空间特异性。

Result: 该框架在四个基准数据集（TCGA-COAD、BRACS、胃肠化生分级和肠道ROI分类）上取得了最先进的性能，证明了可变形注意力在捕获WSIs和ROIs中复杂空间结构方面的强大能力。

Conclusion: 所提出的带有可变形注意力机制的GNN框架能够有效处理病理图像中的空间依赖性问题，通过学习空间偏移量来增强上下文感知和空间特异性，并在多个基准测试中展现出卓越的性能，证明了其在计算病理学领域的巨大潜力。

Abstract: Accurate classification of Whole Slide Images (WSIs) and Regions of Interest
(ROIs) is a fundamental challenge in computational pathology. While mainstream
approaches often adopt Multiple Instance Learning (MIL), they struggle to
capture the spatial dependencies among tissue structures. Graph Neural Networks
(GNNs) have emerged as a solution to model inter-instance relationships, yet
most rely on static graph topologies and overlook the physical spatial
positions of tissue patches. Moreover, conventional attention mechanisms lack
specificity, limiting their ability to focus on structurally relevant regions.
In this work, we propose a novel GNN framework with deformable attention for
pathology image analysis. We construct a dynamic weighted directed graph based
on patch features, where each node aggregates contextual information from its
neighbors via attention-weighted edges. Specifically, we incorporate learnable
spatial offsets informed by the real coordinates of each patch, enabling the
model to adaptively attend to morphologically relevant regions across the
slide. This design significantly enhances the contextual field while preserving
spatial specificity. Our framework achieves state-of-the-art performance on
four benchmark datasets (TCGA-COAD, BRACS, gastric intestinal metaplasia
grading, and intestinal ROI classification), demonstrating the power of
deformable attention in capturing complex spatial structures in WSIs and ROIs.

</details>


### [111] [From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization](https://arxiv.org/abs/2508.05409)
*Farah Wahida,M. A. P. Chamikara,Yashothara Shanmugarasa,Mohan Baruwal Chhetri,Thilina Ranbaduge,Ibrahim Khalil*

Main category: cs.CV

TL;DR: TrueBiometric是一种新颖且通用的方法，利用多模态大视觉语言模型（LVLM）的多数投票机制，准确检测并纠正生物识别系统（如人脸识别）中的后门攻击中毒图像，同时不影响正常图像的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络驱动的生物识别系统依赖于敏感数据，易受后门攻击。攻击者通过在训练图像中植入微小触发器（如贴纸、化妆品），可在认证时欺骗系统，实现未经授权的访问。现有防御机制在精确识别和缓解中毒图像方面面临挑战，且可能损害数据实用性，从而影响系统可靠性。

Method: 提出TrueBiometric方法。首先，利用多个先进的大视觉语言模型（LVLM）进行多数投票，以准确检测中毒图像。其次，一旦识别出中毒样本，则使用有针对性且经过校准的校正噪声对其进行修正。

Result: TrueBiometric能够以100%的准确率检测并纠正中毒图像，且不影响对干净图像的准确性。与现有最先进的方法相比，TrueBiometric为缓解人脸识别系统中的后门攻击提供了一种更实用、更准确、更有效的解决方案。

Conclusion: TrueBiometric是一种有效且实用的防御后门攻击的方案，它通过结合多模态大视觉语言模型的检测能力和精细化的噪声校正技术，显著提高了生物识别系统的安全性与可靠性。

Abstract: Biometric systems, such as face recognition systems powered by deep neural
networks (DNNs), rely on large and highly sensitive datasets. Backdoor attacks
can subvert these systems by manipulating the training process. By inserting a
small trigger, such as a sticker, make-up, or patterned mask, into a few
training images, an adversary can later present the same trigger during
authentication to be falsely recognized as another individual, thereby gaining
unauthorized access. Existing defense mechanisms against backdoor attacks still
face challenges in precisely identifying and mitigating poisoned images without
compromising data utility, which undermines the overall reliability of the
system. We propose a novel and generalizable approach, TrueBiometric:
Trustworthy Biometrics, which accurately detects poisoned images using a
majority voting mechanism leveraging multiple state-of-the-art large vision
language models. Once identified, poisoned samples are corrected using targeted
and calibrated corrective noise. Our extensive empirical results demonstrate
that TrueBiometric detects and corrects poisoned images with 100\% accuracy
without compromising accuracy on clean images. Compared to existing
state-of-the-art approaches, TrueBiometric offers a more practical, accurate,
and effective solution for mitigating backdoor attacks in face recognition
systems.

</details>


### [112] [Physical Adversarial Camouflage through Gradient Calibration and Regularization](https://arxiv.org/abs/2508.05414)
*Jiawei Liang,Siyuan Liang,Jianjie Huang,Chenxi Si,Ming Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该研究提出一种基于梯度优化的新型物理对抗伪装框架，通过梯度校准和梯度去相关方法解决了现有技术在变动物理环境中攻击效果不稳定的问题，显著提升了对抗攻击成功率，并强调了系统鲁棒性设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 深度目标检测器在自动驾驶等安全关键领域应用广泛，但物理对抗伪装通过改变物体纹理来欺骗检测器，构成重大安全风险。现有技术在可变物理环境中面临挑战：1) 距离导致采样点密度不一致，影响梯度优化；2) 多角度更新纹理梯度产生冲突，降低优化稳定性和攻击效果。

Method: 提出一种基于梯度优化的对抗伪装框架。1) 引入梯度校准策略，通过将梯度从稀疏采样点传播到未采样纹理点，确保跨距离的梯度更新一致性。2) 开发梯度去相关方法，根据损失值优先化和正交化梯度，消除冗余或冲突的更新，从而增强多角度优化的稳定性和有效性。

Result: 在多种检测模型、角度和距离上的大量实验结果表明，该方法显著优于现有技术，攻击成功率（ASR）在不同距离上平均提高13.46%，在不同角度上平均提高11.03%。此外，真实世界场景的实证评估强调了更鲁棒系统设计的必要性。

Conclusion: 所提出的梯度优化框架有效解决了物理对抗伪装在复杂环境中的稳定性与有效性问题，显著提升了攻击成功率。研究结果也进一步凸显了在实际应用中设计更具鲁棒性的目标检测系统的紧迫性。

Abstract: The advancement of deep object detectors has greatly affected safety-critical
fields like autonomous driving. However, physical adversarial camouflage poses
a significant security risk by altering object textures to deceive detectors.
Existing techniques struggle with variable physical environments, facing two
main challenges: 1) inconsistent sampling point densities across distances
hinder the gradient optimization from ensuring local continuity, and 2)
updating texture gradients from multiple angles causes conflicts, reducing
optimization stability and attack effectiveness. To address these issues, we
propose a novel adversarial camouflage framework based on gradient
optimization. First, we introduce a gradient calibration strategy, which
ensures consistent gradient updates across distances by propagating gradients
from sparsely to unsampled texture points. Additionally, we develop a gradient
decorrelation method, which prioritizes and orthogonalizes gradients based on
loss values, enhancing stability and effectiveness in multi-angle optimization
by eliminating redundant or conflicting updates. Extensive experimental results
on various detection models, angles and distances show that our method
significantly exceeds the state of the art, with an average increase in attack
success rate (ASR) of 13.46% across distances and 11.03% across angles.
Furthermore, empirical evaluation in real-world scenarios highlights the need
for more robust system design.

</details>


### [113] [Smoothing Slot Attention Iterations and Recurrences](https://arxiv.org/abs/2508.05417)
*Rongzhen Zhao,Wenyan Yang,Juho Kannala,Joni Pajarinen*

Main category: cs.CV

TL;DR: 该论文提出了SmoothSA，通过预热冷启动查询和区分视频帧的迭代方式，改进了Slot Attention在图像和视频对象中心学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有Slot Attention（SA）存在两个问题：1) 冷启动查询缺乏样本特定信息，导致图像或视频首帧的聚合不够精确；2) 视频非首帧的查询已是样本特定，但与首帧使用同质转换，效率低下。

Method: 本文提出了SmoothSA来解决上述问题：1) 对于图像或视频的首帧，通过一个自蒸馏的小模块，利用输入特征的丰富信息“预热”冷启动查询；2) 对于所有视频帧的迭代，区分首帧（使用完整迭代）和非首帧（使用单次迭代）的转换方式。

Result: 在对象发现、识别和下游基准测试上的综合实验验证了该方法的有效性。进一步的分析直观地阐明了该方法如何平滑SA的迭代和循环。

Conclusion: SmoothSA首次解决了Slot Attention在图像和视频对象中心学习中的冷启动和同质转换问题，显著提升了性能。

Abstract: Slot Attention (SA) and its variants lie at the heart of mainstream
Object-Centric Learning (OCL). Objects in an image can be aggregated into
respective slot vectors, by \textit{iteratively} refining cold-start query
vectors, typically three times, via SA on image features. For video, such
aggregation is \textit{recurrently} shared across frames, with queries
cold-started on the first frame while transitioned from the previous frame's
slots on non-first frames. However, the cold-start queries lack sample-specific
cues thus hinder precise aggregation on the image or video's first frame; Also,
non-first frames' queries are already sample-specific thus require transforms
different from the first frame's aggregation. We address these issues for the
first time with our \textit{SmoothSA}: (1) To smooth SA iterations on the image
or video's first frame, we \textit{preheat} the cold-start queries with rich
information of input features, via a tiny module self-distilled inside OCL; (2)
To smooth SA recurrences across all video frames, we \textit{differentiate} the
homogeneous transforms on the first and non-first frames, by using full and
single iterations respectively. Comprehensive experiments on object discovery,
recognition and downstream benchmarks validate our method's effectiveness.
Further analyses intuitively illuminate how our method smooths SA iterations
and recurrences. Our code is available in the supplement.

</details>


### [114] [How and Why: Taming Flow Matching for Unsupervised Anomaly Detection and Localization](https://arxiv.org/abs/2508.05461)
*Liangwei Li,Lin Liu,Juanxiu Liu,Jing Zhang,Ruqian Hao,Xiaohui Du*

Main category: cs.CV

TL;DR: 本文提出了一种基于流匹配（Flow Matching, FM）的无监督异常检测和定位新范式，通过引入时间反向流匹配（rFM）和最差传输（WT）位移插值来解决传统流方法表达能力不足的问题，并在MVTec数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于流的方法在模型表达能力上存在局限性，无法有效处理无监督异常检测任务。

Method: 本文提出时间反向流匹配（rFM）概念，将其形式化为沿预定义概率路径的向量场回归，以将未知数据分布转换为标准高斯分布。研究发现，线性插值概率路径的FM本质上不可逆，且在高维空间中使用反向高斯概率路径会导致平凡向量场。基于此，提出最差传输（WT）位移插值来重建非概率演化路径（WT-Flow），通过构建“退化势阱”来约束正常样本，同时允许异常样本逃逸，从而实现异常样本的理论分离机制。FM提供了一个计算上可行的框架。

Result: 本文首次成功将流匹配应用于无监督异常检测任务，并在MVTec数据集上以单尺度实现了最先进的性能。

Conclusion: WT-Flow为异常样本提供了一种理论上可靠的分离机制。流匹配为处理复杂数据提供了一个计算上可行的框架，并在无监督异常检测任务中取得了显著的SOTA成果。

Abstract: We propose a new paradigm for unsupervised anomaly detection and localization
using Flow Matching (FM), which fundamentally addresses the model expressivity
limitations of conventional flow-based methods. To this end, we formalize the
concept of time-reversed Flow Matching (rFM) as a vector field regression along
a predefined probability path to transform unknown data distributions into
standard Gaussian. We bring two core observations that reshape our
understanding of FM. First, we rigorously prove that FM with linear
interpolation probability paths is inherently non-invertible. Second, our
analysis reveals that employing reversed Gaussian probability paths in
high-dimensional spaces can lead to trivial vector fields. This issue arises
due to the manifold-related constraints. Building on the second observation, we
propose Worst Transport (WT) displacement interpolation to reconstruct a
non-probabilistic evolution path. The proposed WT-Flow enhances dynamical
control over sample trajectories, constructing ''degenerate potential wells''
for anomaly-free samples while allowing anomalous samples to escape. This novel
unsupervised paradigm offers a theoretically grounded separation mechanism for
anomalous samples. Notably, FM provides a computationally tractable framework
that scales to complex data. We present the first successful application of FM
for the unsupervised anomaly detection task, achieving state-of-the-art
performance at a single scale on the MVTec dataset. The reproducible code for
training will be released upon camera-ready submission.

</details>


### [115] [SMOL-MapSeg: Show Me One Label](https://arxiv.org/abs/2508.05501)
*Yunshuang Yuan,Frank Thiemann,Thorsten Dahms,Monika Sester*

Main category: cs.CV

TL;DR: 该研究提出了一种名为“按需声明式（OND）知识提示”的新机制，用于改进基础模型SAM在历史地图上的语义分割表现。通过显式提示指导模型识别不一致的地图样式，SMOL-MapSeg模型在历史地图分割中取得了优于基线模型的性能，并支持少样本学习。


<details>
  <summary>Details</summary>
Motivation: 历史地图对研究地球表面变化具有重要价值，但现有深度学习模型（特别是预训练的基础模型）在处理历史地图时表现不佳。这是因为基础模型通常在现代或特定领域的图像上训练，而历史地图缺乏一致性，相似概念可能以截然不同的形状和样式出现，导致模型难以泛化。

Method: 研究提出“按需声明式（OND）知识提示”机制，通过引入显式提示来指导模型识别特定模式与概念的对应关系，允许用户在推理时指定目标概念和模式。具体实现上，该机制替换了基础模型SAM的提示编码器，并在历史地图上对由此产生的模型SMOL-MapSeg进行微调。

Result: 实验结果表明，SMOL-MapSeg能够准确分割由OND知识定义的类别。此外，它还能通过少量样本微调来适应未见过的新类别。在平均分割性能方面，SMOL-MapSeg优于基于UNet的基线模型。

Conclusion: OND知识提示机制有效解决了基础模型在处理样式不一致的历史地图时所面临的挑战。SMOL-MapSeg模型为历史地图的语义分割提供了一个准确且适应性强的新方法，展现了其在文化遗产和地理信息研究中的潜力。

Abstract: Historical maps are valuable for studying changes to the Earth's surface.
With the rise of deep learning, models like UNet have been used to extract
information from these maps through semantic segmentation. Recently,
pre-trained foundation models have shown strong performance across domains such
as autonomous driving, medical imaging, and industrial inspection. However,
they struggle with historical maps. These models are trained on modern or
domain-specific images, where patterns can be tied to predefined concepts
through common sense or expert knowledge. Historical maps lack such consistency
-- similar concepts can appear in vastly different shapes and styles. To
address this, we propose On-Need Declarative (OND) knowledge-based prompting,
which introduces explicit prompts to guide the model on what patterns
correspond to which concepts. This allows users to specify the target concept
and pattern during inference (on-need inference). We implement this by
replacing the prompt encoder of the foundation model SAM with our OND prompting
mechanism and fine-tune it on historical maps. The resulting model is called
SMOL-MapSeg (Show Me One Label). Experiments show that SMOL-MapSeg can
accurately segment classes defined by OND knowledge. It can also adapt to
unseen classes through few-shot fine-tuning. Additionally, it outperforms a
UNet-based baseline in average segmentation performance.

</details>


### [116] [AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated Industrial Anomaly Detection](https://arxiv.org/abs/2508.05503)
*Dongwei Ji,Bingzhang Hu,Yi Zhou*

Main category: cs.CV

TL;DR: AutoIAD是一个多智能体协作框架，旨在实现工业视觉异常检测的端到端自动化开发，显著提升性能并减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统的工业异常检测（IAD）需要大量人工干预，且难以适应各种应用场景，因此需要一种自动化、端到端的解决方案。

Method: AutoIAD采用管理器驱动的中心智能体，协调数据准备、数据加载、模型设计和训练等专业子智能体。它还整合了领域特定知识库，以智能地处理从原始工业图像数据到训练好的异常检测模型的整个流程。该框架在MVTec AD数据集上进行了综合基准测试，并评估了不同的LLM后端。

Result: AutoIAD在任务完成率和模型性能（AUROC）方面显著优于现有通用智能体协作框架和传统AutoML框架。它通过迭代优化有效缓解了幻觉问题。消融研究证实，管理器中心智能体和领域知识库模块在生成鲁棒、高质量的IAD解决方案中发挥着关键作用。

Conclusion: AutoIAD提供了一个有效、鲁棒且高质量的工业异常检测自动化开发解决方案，能够显著提高效率并降低人工成本。

Abstract: Industrial anomaly detection (IAD) is critical for manufacturing quality
control, but conventionally requires significant manual effort for various
application scenarios. This paper introduces AutoIAD, a multi-agent
collaboration framework, specifically designed for end-to-end automated
development of industrial visual anomaly detection. AutoIAD leverages a
Manager-Driven central agent to orchestrate specialized sub-agents (including
Data Preparation, Data Loader, Model Designer, Trainer) and integrates a
domain-specific knowledge base, which intelligently handles the entire pipeline
using raw industrial image data to develop a trained anomaly detection model.
We construct a comprehensive benchmark using MVTec AD datasets to evaluate
AutoIAD across various LLM backends. Extensive experiments demonstrate that
AutoIAD significantly outperforms existing general-purpose agentic
collaboration frameworks and traditional AutoML frameworks in task completion
rate and model performance (AUROC), while effectively mitigating issues like
hallucination through iterative refinement. Ablation studies further confirm
the crucial roles of the Manager central agent and the domain knowledge base
module in producing robust and high-quality IAD solutions.

</details>


### [117] [Symmetry Understanding of 3D Shapes via Chirality Disentanglement](https://arxiv.org/abs/2508.05505)
*Weikang Wang,Tobias Weißberg,Nafie El Amrani,Florian Bernard*

Main category: cs.CV

TL;DR: 该研究提出了一个无监督的左右手征（chirality）特征提取管道，用于为三维形状（点云和网格）顶点提供手征感知信息，以解决现有形状描述符无法区分左右对称部分的问题。


<details>
  <summary>Details</summary>
Motivation: 手征信息在计算机视觉的各种数据模式中普遍存在，但在形状分析（如点云和网格）中的探索不足。尽管许多形状顶点描述符对刚体变换具有鲁棒性，但它们通常无法区分左右对称部分。鉴于手征信息在不同形状分析问题中的普遍性以及当前形状描述符缺乏手征感知特征，开发手征特征提取器变得必要且紧迫。

Method: 基于现有的Diff3F框架，提出了一种无监督的手征特征提取管道，利用2D基础模型提取手征感知信息来装饰形状顶点。

Result: 通过在不同数据集上进行定量和定性实验评估了提取的手征特征。在左右解耦、形状匹配和部件分割等下游任务中的结果表明，这些特征具有有效性和实用性。

Conclusion: 所开发的手征特征提取器能够有效地为三维形状提供手征感知信息，并在多个下游任务中展示了其有效性和实际应用价值，解决了现有形状描述符无法区分左右对称部分的问题。

Abstract: Chirality information (i.e. information that allows distinguishing left from
right) is ubiquitous for various data modes in computer vision, including
images, videos, point clouds, and meshes. While chirality has been extensively
studied in the image domain, its exploration in shape analysis (such as point
clouds and meshes) remains underdeveloped. Although many shape vertex
descriptors have shown appealing properties (e.g. robustness to rigid-body
transformations), they are often not able to disambiguate between left and
right symmetric parts. Considering the ubiquity of chirality information in
different shape analysis problems and the lack of chirality-aware features
within current shape descriptors, developing a chirality feature extractor
becomes necessary and urgent. Based on the recent Diff3F framework, we propose
an unsupervised chirality feature extraction pipeline to decorate shape
vertices with chirality-aware information, extracted from 2D foundation models.
We evaluated the extracted chirality features through quantitative and
qualitative experiments across diverse datasets. Results from downstream tasks
including left-right disentanglement, shape matching, and part segmentation
demonstrate their effectiveness and practical utility. Project page:
https://wei-kang-wang.github.io/chirality/

</details>


### [118] [MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips](https://arxiv.org/abs/2508.05506)
*Shibo Wang,Haonan He,Maria Parelli,Christoph Gebhardt,Zicong Fan,Jie Song*

Main category: cs.CV

TL;DR: MagicHOI是一种从单目交互视频中重建手和物体的方法，即使在有限视角下也能通过利用新视角合成扩散模型作为未观察物体区域的先验，实现逼真重建。


<details>
  <summary>Details</summary>
Motivation: 大多数基于RGB的手物体重建方法依赖物体模板或假设物体完全可见，这在现实世界中由于固定视角和静态抓握导致部分物体不可见时，会产生不合理的重建结果。

Method: MagicHOI将新视角合成扩散模型集成到手物体重建框架中，利用其作为先验来规范未观察到的物体区域。此外，通过结合可见接触约束来对齐手和物体。

Result: MagicHOI显著优于现有最先进的手物体重建方法。新视角合成扩散先验能有效规范未观察到的物体区域，从而增强3D手物体重建效果。

Conclusion: MagicHOI成功解决了在有限视角下进行手物体重建的挑战，通过利用大规模新视角合成扩散模型提供的丰富物体监督作为先验，有效补全了未观察到的物体区域。

Abstract: Most RGB-based hand-object reconstruction methods rely on object templates,
while template-free methods typically assume full object visibility. This
assumption often breaks in real-world settings, where fixed camera viewpoints
and static grips leave parts of the object unobserved, resulting in implausible
reconstructions. To overcome this, we present MagicHOI, a method for
reconstructing hands and objects from short monocular interaction videos, even
under limited viewpoint variation. Our key insight is that, despite the
scarcity of paired 3D hand-object data, large-scale novel view synthesis
diffusion models offer rich object supervision. This supervision serves as a
prior to regularize unseen object regions during hand interactions. Leveraging
this insight, we integrate a novel view synthesis model into our hand-object
reconstruction framework. We further align hand to object by incorporating
visible contact constraints. Our results demonstrate that MagicHOI
significantly outperforms existing state-of-the-art hand-object reconstruction
methods. We also show that novel view synthesis diffusion priors effectively
regularize unseen object regions, enhancing 3D hand-object reconstruction.

</details>


### [119] [Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events](https://arxiv.org/abs/2508.05507)
*Lin Zhu,Ruonan Liu,Xiao Wang,Lizhi Wang,Hua Huang*

Main category: cs.CV

TL;DR: 本文提出了一种自监督预训练框架，通过三阶段方法从稀疏、噪声大的事件数据中提取丰富的边缘和纹理信息，显著提升了事件相机在各种下游任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据虽然具有高时间分辨率和宽动态范围，但在挑战性场景中仍难以有效提取特征，因为事件数据本质上稀疏且噪声大，主要反映亮度变化，这限制了其在视觉任务中的应用。

Method: 该框架包含三个阶段：1. 差异引导掩码建模：受事件物理采样过程启发，重建时间强度差分图以增强原始事件数据信息。2. 主干固定特征转换：在不更新主干网络的情况下对比事件和图像特征，以保留掩码建模学习到的表示并稳定对比学习。3. 聚焦对比学习：更新整个模型，通过关注高价值区域来提高语义判别能力。

Result: 实验结果表明，该框架鲁棒性强，在物体识别、语义分割和光流估计等多种下游任务上持续优于现有最先进的方法。

Conclusion: 所提出的自监督预训练框架能够充分揭示事件数据中潜在的边缘和纹理信息，有效解决了事件数据稀疏和噪声大的问题，从而显著提升了事件相机在各种视觉任务上的性能。

Abstract: Event camera, a novel neuromorphic vision sensor, records data with high
temporal resolution and wide dynamic range, offering new possibilities for
accurate visual representation in challenging scenarios. However, event data is
inherently sparse and noisy, mainly reflecting brightness changes, which
complicates effective feature extraction. To address this, we propose a
self-supervised pre-training framework to fully reveal latent information in
event data, including edge information and texture cues. Our framework consists
of three stages: Difference-guided Masked Modeling, inspired by the event
physical sampling process, reconstructs temporal intensity difference maps to
extract enhanced information from raw event data. Backbone-fixed Feature
Transition contrasts event and image features without updating the backbone to
preserve representations learned from masked modeling and stabilizing their
effect on contrastive learning. Focus-aimed Contrastive Learning updates the
entire model to improve semantic discrimination by focusing on high-value
regions. Extensive experiments show our framework is robust and consistently
outperforms state-of-the-art methods on various downstream tasks, including
object recognition, semantic segmentation, and optical flow estimation. The
code and dataset are available at https://github.com/BIT-Vision/EventPretrain.

</details>


### [120] [Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking](https://arxiv.org/abs/2508.05514)
*Zewei Wu,César Teixeira,Wei Ke,Zhang Xiong*

Main category: cs.CV

TL;DR: 该论文提出了一种增强的行人跟踪框架，通过融合更丰富的特征表示（检测器回归/分类特征、头部关键点）和更鲁棒的运动模型（迭代卡尔曼滤波与3D先验），以应对复杂场景下的严重遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 在智能监控、行为分析等实际应用中，行人跟踪面临严重的遮挡挑战。传统方法依赖于Re-ID模型提取的全身边界框特征和线性匀速运动假设，在多行人交互或重叠导致特征丢失时，难以维持稳定的轨迹。

Method: 该方法从以下两方面进行改进：1. 特征表示：整合目标检测器回归和分类分支的检测特征（嵌入空间和位置信息），并引入头部关键点检测模型（头部受遮挡影响较小）。2. 运动建模：提出迭代卡尔曼滤波方法，与现代检测器假设对齐，并集成3D先验以更好地补全复杂场景中的运动轨迹。

Result: 通过结合外观和运动建模方面的这些进步，所提出的方法为在遮挡普遍存在的拥挤环境中进行多目标跟踪提供了更鲁棒的解决方案。

Conclusion: 所提出的方法通过增强外观特征表示和改进运动模型，有效解决了多目标跟踪在严重遮挡下的鲁棒性问题，为复杂场景下的行人跟踪提供了更可靠的方案。

Abstract: Visual pedestrian tracking represents a promising research field, with
extensive applications in intelligent surveillance, behavior analysis, and
human-computer interaction. However, real-world applications face significant
occlusion challenges. When multiple pedestrians interact or overlap, the loss
of target features severely compromises the tracker's ability to maintain
stable trajectories. Traditional tracking methods, which typically rely on
full-body bounding box features extracted from {Re-ID} models and linear
constant-velocity motion assumptions, often struggle in severe occlusion
scenarios. To address these limitations, this work proposes an enhanced
tracking framework that leverages richer feature representations and a more
robust motion model. Specifically, the proposed method incorporates detection
features from both the regression and classification branches of an object
detector, embedding spatial and positional information directly into the
feature representations. To further mitigate occlusion challenges, a head
keypoint detection model is introduced, as the head is less prone to occlusion
compared to the full body. In terms of motion modeling, we propose an iterative
Kalman filtering approach designed to align with modern detector assumptions,
integrating 3D priors to better complete motion trajectories in complex scenes.
By combining these advancements in appearance and motion modeling, the proposed
method offers a more robust solution for multi-object tracking in crowded
environments where occlusions are prevalent.

</details>


### [121] [FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment](https://arxiv.org/abs/2508.05516)
*Ekaterina Shumitskaya,Dmitriy Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的IQA模型认证防御方法，通过在特征空间而非输入空间应用随机平滑，在保持图像保真度的同时提供鲁棒性保证，并显著提高计算效率和主观质量相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的认证防御方法通常直接在输入图像中注入高斯噪声，这常常会降低视觉质量。因此，需要一种既能提供鲁巴斯性保证又能保持图像保真度的方法。

Method: 该方法基于随机平滑，将噪声应用于特征空间而非输入空间。通过分析骨干网络雅可比矩阵的最大奇异值，形式化地连接特征空间噪声水平与相应的输入空间扰动。该方法支持全参考（FR）和无参考（NR）IQA模型，无需修改模型架构，且计算效率高，每张图像仅需一次骨干网络前向传播。

Result: 与现有方法相比，该方法在不进行认证时推理时间减少99.5%，在进行认证时减少20.6%。在两个基准数据集上对六种广泛使用的FR和NR IQA模型进行了验证，并与五种最先进的认证防御方法进行了比较。结果表明，与主观质量评分的相关性一致提高了30.9%。

Conclusion: 所提出的特征空间随机平滑认证防御方法能有效保护IQA模型，同时保持图像质量和高计算效率，显著提高了IQA模型与主观质量评分的相关性。

Abstract: We propose a novel certified defense method for Image Quality Assessment
(IQA) models based on randomized smoothing with noise applied in the feature
space rather than the input space. Unlike prior approaches that inject Gaussian
noise directly into input images, often degrading visual quality, our method
preserves image fidelity while providing robustness guarantees. To formally
connect noise levels in the feature space with corresponding input-space
perturbations, we analyze the maximum singular value of the backbone network's
Jacobian. Our approach supports both full-reference (FR) and no-reference (NR)
IQA models without requiring any architectural modifications, suitable for
various scenarios. It is also computationally efficient, requiring a single
backbone forward pass per image. Compared to previous methods, it reduces
inference time by 99.5% without certification and by 20.6% when certification
is applied. We validate our method with extensive experiments on two benchmark
datasets, involving six widely-used FR and NR IQA models and comparisons
against five state-of-the-art certified defenses. Our results demonstrate
consistent improvements in correlation with subjective quality scores by up to
30.9%.

</details>


### [122] [Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods](https://arxiv.org/abs/2508.05519)
*Matthew Purri,Amit Patel,Erik Deurrell*

Main category: cs.CV

TL;DR: 本文提出并验证了Octozi平台，一个结合大语言模型和领域特定启发式规则的AI辅助系统，显著提升了临床试验数据清洗的效率和准确性，同时减少了假阳性查询。


<details>
  <summary>Details</summary>
Motivation: 临床试验数据清洗是药物开发的关键瓶颈，手动审查流程难以应对指数级增长的数据量和复杂性。

Method: 开发了Octozi平台，该平台结合了大型语言模型和领域特定启发式规则。通过与10名经验丰富的临床审阅员进行的对照实验研究，评估了AI辅助系统的性能。

Result: AI辅助使数据清洗吞吐量提高了6.03倍，同时将清洗错误从54.67%降低到8.48%（6.44倍的改进）。系统将假阳性查询减少了15.48倍，且这些改进在不同经验水平的审阅员中均保持一致。

Conclusion: AI辅助方法可以解决临床试验操作中的根本性低效率问题，有望加速药物开发时间表并降低成本，同时保持监管合规性。这项工作为将AI整合到安全关键的临床工作流程中建立了框架，并展示了人机协作在制药临床试验中的变革潜力。

Abstract: Clinical trial data cleaning represents a critical bottleneck in drug
development, with manual review processes struggling to manage exponentially
increasing data volumes and complexity. This paper presents Octozi, an
artificial intelligence-assisted platform that combines large language models
with domain-specific heuristics to transform clinical data review. In a
controlled experimental study with experienced clinical reviewers (n=10), we
demonstrate that AI assistance increased data cleaning throughput by 6.03-fold
while simultaneously decreasing cleaning errors from 54.67% to 8.48% (a
6.44-fold improvement). Crucially, the system reduced false positive queries by
15.48-fold, minimizing unnecessary site burden. These improvements were
consistent across reviewers regardless of experience level, suggesting broad
applicability. Our findings indicate that AI-assisted approaches can address
fundamental inefficiencies in clinical trial operations, potentially
accelerating drug development timelines and reducing costs while maintaining
regulatory compliance. This work establishes a framework for integrating AI
into safety-critical clinical workflows and demonstrates the transformative
potential of human-AI collaboration in pharmaceutical clinical trials.

</details>


### [123] [Optimal Brain Connection: Towards Efficient Structural Pruning](https://arxiv.org/abs/2508.05521)
*Shaowu Chen,Wei Ma,Binhua Huang,Qingyuan Wang,Guoxin Wang,Weize Sun,Lei Huang,Deepu John*

Main category: cs.CV

TL;DR: 本文提出一种名为“最优大脑连接”（Optimal Brain Connection）的结构化剪枝框架，通过引入考虑参数间相互连接的雅可比准则和利用自编码器在微调时保留剪枝连接贡献的等效剪枝机制，有效提升了神经网络剪枝的性能。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法在评估参数时，往往忽略参数之间的相互连接，导致剪枝效果不理想。

Method: 1. 提出了“雅可比准则”（Jacobian Criterion），这是一种一阶度量，用于评估结构参数的显著性，它能显式捕获组件内部交互和层间依赖关系。2. 提出了“等效剪枝机制”（Equivalent Pruning mechanism），该机制利用自编码器在微调过程中保留所有原始连接（包括已剪枝连接）的贡献。

Result: 实验结果表明，雅可比准则在保持模型性能方面优于几种流行的度量标准，而等效剪枝机制则有效缓解了微调后的性能下降。

Conclusion: 所提出的Optimal Brain Connection框架，通过创新性地考虑参数互联和在微调阶段保留剪枝连接的贡献，显著提高了结构化剪枝的性能和模型效果。

Abstract: Structural pruning has been widely studied for its effectiveness in
compressing neural networks. However, existing methods often neglect the
interconnections among parameters. To address this limitation, this paper
proposes a structural pruning framework termed Optimal Brain Connection. First,
we introduce the Jacobian Criterion, a first-order metric for evaluating the
saliency of structural parameters. Unlike existing first-order methods that
assess parameters in isolation, our criterion explicitly captures both
intra-component interactions and inter-layer dependencies. Second, we propose
the Equivalent Pruning mechanism, which utilizes autoencoders to retain the
contributions of all original connection--including pruned ones--during
fine-tuning. Experimental results demonstrate that the Jacobian Criterion
outperforms several popular metrics in preserving model performance, while the
Equivalent Pruning mechanism effectively mitigates performance degradation
after fine-tuning. Code: https://github.com/ShaowuChen/Optimal_Brain_Connection

</details>


### [124] [When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework](https://arxiv.org/abs/2508.05526)
*Haoyu Liu,Chaoyu Gong,Mengke He,Jiate Li,Kai Han,Siqiang Luo*

Main category: cs.CV

TL;DR: SSTGNN是一个轻量级的空-谱-时图神经网络框架，用于检测AI生成和篡改的视频，通过联合推理空间、时间、频谱信息，实现了卓越的性能和泛化能力，同时参数量远少于现有模型。


<details>
  <summary>Details</summary>
Motivation: 生成式视频模型的普及使得检测AI生成和篡改视频成为紧迫挑战。现有检测方法由于依赖孤立的空间、时间或频谱信息，难以泛化到多样化的篡改类型，且通常需要大型模型才能表现良好。

Method: 本文提出了SSTGNN（Spatial-Spectral-Temporal Graph Neural Network），一个轻量级的图神经网络框架。它将视频表示为结构化图，能够联合推理空间不一致性、时间伪影和频谱失真。SSTGNN在图基架构中融入了可学习的频谱滤波器和时间微分建模，以更有效地捕捉微小的篡改痕迹。

Result: 在多样化的基准数据集上进行了广泛实验，SSTGNN在域内和跨域设置中均实现了卓越的性能，并对未知篡改表现出强大的鲁棒性。值得注意的是，SSTGNN的参数量比最先进的模型减少了高达42.4倍。

Conclusion: SSTGNN以其卓越的性能、强大的鲁棒性和极低的参数量，成为一个高度轻量级且可扩展的真实世界AI视频检测解决方案。

Abstract: The proliferation of generative video models has made detecting AI-generated
and manipulated videos an urgent challenge. Existing detection approaches often
fail to generalize across diverse manipulation types due to their reliance on
isolated spatial, temporal, or spectral information, and typically require
large models to perform well. This paper introduces SSTGNN, a lightweight
Spatial-Spectral-Temporal Graph Neural Network framework that represents videos
as structured graphs, enabling joint reasoning over spatial inconsistencies,
temporal artifacts, and spectral distortions. SSTGNN incorporates learnable
spectral filters and temporal differential modeling into a graph-based
architecture, capturing subtle manipulation traces more effectively. Extensive
experiments on diverse benchmark datasets demonstrate that SSTGNN not only
achieves superior performance in both in-domain and cross-domain settings, but
also offers strong robustness against unseen manipulations. Remarkably, SSTGNN
accomplishes these results with up to 42.4$\times$ fewer parameters than
state-of-the-art models, making it highly lightweight and scalable for
real-world deployment.

</details>


### [125] [AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety](https://arxiv.org/abs/2508.05527)
*Adi Levi,Or Levi,Sardhendu Mishra,Jonathan Morra*

Main category: cs.CV

TL;DR: 本文基准测试了多模态大语言模型（MLLMs）在视频品牌安全分类中的能力，引入了一个新的多模态多语言数据集，并比较了MLLMs与人类审核员的准确性和成本效率，同时讨论了MLLMs的局限性。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容呈指数级增长，人工审核已无法满足需求，带来了运营和心理健康挑战。尽管MLLMs在视频理解方面表现出色，但其在需要细致理解视觉和文本线索的多模态内容审核（特别是品牌安全分类）中的应用仍未被充分探索。

Method: 本文通过引入一个由专业审核员精心标注的、涵盖多种风险类别的多模态多语言数据集，基准测试了Gemini、GPT和Llama等MLLMs在品牌安全分类任务中的能力。通过详细的比较分析，评估了MLLMs的准确性和成本效率，并与专业人工审核员进行了对比。

Result: 研究表明，Gemini、GPT和Llama等MLLMs在多模态品牌安全方面表现出有效性。评估了它们相对于专业人工审核员的准确性和成本效率。此外，还深入讨论了MLLMs的局限性和失败案例。

Conclusion: MLLMs在视频品牌安全分类中具有潜力，但仍存在局限性。本文发布了所创建的数据集，以促进未来在有效和负责任的品牌安全及内容审核方面的研究。

Abstract: As the volume of video content online grows exponentially, the demand for
moderation of unsafe videos has surpassed human capabilities, posing both
operational and mental health challenges. While recent studies demonstrated the
merits of Multimodal Large Language Models (MLLMs) in various video
understanding tasks, their application to multimodal content moderation, a
domain that requires nuanced understanding of both visual and textual cues,
remains relatively underexplored. In this work, we benchmark the capabilities
of MLLMs in brand safety classification, a critical subset of content
moderation for safe-guarding advertising integrity. To this end, we introduce a
novel, multimodal and multilingual dataset, meticulously labeled by
professional reviewers in a multitude of risk categories. Through a detailed
comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,
GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost
efficiency compared to professional human reviewers. Furthermore, we present an
in-depth discussion shedding light on limitations of MLLMs and failure cases.
We are releasing our dataset alongside this paper to facilitate future research
on effective and responsible brand safety and content moderation.

</details>


### [126] [Looking into the Unknown: Exploring Action Discovery for Segmentation of Known and Unknown Actions](https://arxiv.org/abs/2508.05529)
*Federico Spurio,Emad Bahrami,Olga Zatsarynna,Yazan Abu Farha,Gianpiero Francesca,Juergen Gall*

Main category: cs.CV

TL;DR: 本文提出了一种名为“动作发现”的新型时间动作分割设置，旨在解决部分标注数据集中未知或模糊动作的识别问题。为此，提出了一种两步法（GGSM和UASA），并在多个数据集上证明其显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在时间动作分割中，现有方法难以处理标注不完整或模糊的动作。特别是在神经科学等领域，数据集中通常只标注了部分“已知动作”，而存在大量未被标注的“未知动作”。因此，需要一种方法来发现、识别和定义这些未标注的动作。

Method: 本文提出一个两步法：1. **粒度引导分割模块 (GGSM)**：通过模仿已知动作的粒度，识别已知和未知动作的时间间隔。2. **未知动作段分配 (UASA)**：基于学习到的嵌入相似性，在未知动作中识别出具有语义意义的类别。

Result: 在Breakfast、50Salads和Desktop Assembly这三个挑战性数据集上的系统性实验表明，所提出的方法在“动作发现”设置下，显著优于现有基线。

Conclusion: 本文引入了“动作发现”这一新设置，并提出了一种有效且创新的两步法来处理部分标注数据集中未知动作的识别和语义分类问题，为应对不完整标注数据集带来了显著改进。

Abstract: We introduce Action Discovery, a novel setup within Temporal Action
Segmentation that addresses the challenge of defining and annotating ambiguous
actions and incomplete annotations in partially labeled datasets. In this
setup, only a subset of actions - referred to as known actions - is annotated
in the training data, while other unknown actions remain unlabeled. This
scenario is particularly relevant in domains like neuroscience, where
well-defined behaviors (e.g., walking, eating) coexist with subtle or
infrequent actions that are often overlooked, as well as in applications where
datasets are inherently partially annotated due to ambiguous or missing labels.
To address this problem, we propose a two-step approach that leverages the
known annotations to guide both the temporal and semantic granularity of
unknown action segments. First, we introduce the Granularity-Guided
Segmentation Module (GGSM), which identifies temporal intervals for both known
and unknown actions by mimicking the granularity of annotated actions. Second,
we propose the Unknown Action Segment Assignment (UASA), which identifies
semantically meaningful classes within the unknown actions, based on learned
embedding similarities. We systematically explore the proposed setting of
Action Discovery on three challenging datasets - Breakfast, 50Salads, and
Desktop Assembly - demonstrating that our method considerably improves upon
existing baselines.

</details>


### [127] [Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis](https://arxiv.org/abs/2508.05580)
*Kunyu Feng,Yue Ma,Xinhua Zhang,Boshi Liu,Yikuang Yuluo,Yinhan Zhang,Runtao Liu,Hongyu Liu,Zhiyuan Qin,Shanhui Mo,Qifeng Chen,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出Follow-Your-Instruction框架，一个由多模态大语言模型（MLLM）驱动的自动化数据合成方案，用于生成高质量的2D、3D和4D数据，以应对AIGC对大规模数据的需求。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC需求的增长，对高质量、多样化和可扩展数据的需求日益迫切。然而，收集大规模真实世界数据成本高昂且耗时，现有方法（如渲染）仍依赖手动场景构建，限制了可扩展性和准确性，阻碍了下游应用的发展。

Method: 本文提出Follow-Your-Instruction框架，包含四个模块：1) MLLM-Collector：通过多模态输入收集资产及其描述；2) MLLM-Generator：构建3D布局；3) MLLM-Optimizer：利用视觉-语言模型（VLM）通过多视角场景进行语义细化；4) MLLM-Planner：生成时间上连贯的未来帧。该框架能自动合成2D、3D和4D数据。

Result: 通过在2D、3D和4D生成任务上的综合实验，结果表明，本文合成的数据显著提升了现有基线模型的性能。

Conclusion: Follow-Your-Instruction框架展示了其作为生成智能的可扩展且有效的数据引擎的潜力，解决了大规模数据收集的挑战。

Abstract: With the growing demands of AI-generated content (AIGC), the need for
high-quality, diverse, and scalable data has become increasingly crucial.
However, collecting large-scale real-world data remains costly and
time-consuming, hindering the development of downstream applications. While
some works attempt to collect task-specific data via a rendering process, most
approaches still rely on manual scene construction, limiting their scalability
and accuracy. To address these challenges, we propose Follow-Your-Instruction,
a Multimodal Large Language Model (MLLM)-driven framework for automatically
synthesizing high-quality 2D, 3D, and 4D data. Our
\textbf{Follow-Your-Instruction} first collects assets and their associated
descriptions through multimodal inputs using the MLLM-Collector. Then it
constructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic
refinement through multi-view scenes with the MLLM-Generator and
MLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate
temporally coherent future frames. We evaluate the quality of the generated
data through comprehensive experiments on the 2D, 3D, and 4D generative tasks.
The results show that our synthetic data significantly boosts the performance
of existing baseline models, demonstrating Follow-Your-Instruction's potential
as a scalable and effective data engine for generative intelligence.

</details>


### [128] [DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition](https://arxiv.org/abs/2508.05585)
*Haijing Liu,Tao Pu,Hefeng Wu,Keze Wang,Liang Lin*

Main category: cs.CV

TL;DR: 提出DART框架，通过自适应的类内细化和类间知识迁移，提升开放词汇多标签识别（OV-MLR）在可见和不可见类别上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLP模型在弱监督下难以进行精细定位，且未能有效利用结构化关系知识，导致在OV-MLR，特别是不可见类别上的性能受限。

Method: 提出DART（Dual Adaptive Refinement Transfer）框架，增强冻结的VLP骨干。它包含两个协同模块：1) 自适应细化模块（ARM）结合弱监督补丁选择（WPS）损失，实现基于图像级标签的判别性类内定位；2) 自适应迁移模块（ATM）利用从大型语言模型（LLM）中挖掘的类关系图（CRG）和图注意力网络，进行类间关系信息的自适应迁移。

Result: DART在挑战性基准测试上取得了新的最先进性能。

Conclusion: DART是首个将LLM衍生的关系知识与弱监督下的自适应类内细化相结合，用于OV-MLR的框架，有效解决了现有方法的局限性。

Abstract: Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple
seen and unseen object categories within an image, requiring both precise
intra-class localization to pinpoint objects and effective inter-class
reasoning to model complex category dependencies. While Vision-Language
Pre-training (VLP) models offer a strong open-vocabulary foundation, they often
struggle with fine-grained localization under weak supervision and typically
fail to explicitly leverage structured relational knowledge beyond basic
semantics, limiting performance especially for unseen classes. To overcome
these limitations, we propose the Dual Adaptive Refinement Transfer (DART)
framework. DART enhances a frozen VLP backbone via two synergistic adaptive
modules. For intra-class refinement, an Adaptive Refinement Module (ARM)
refines patch features adaptively, coupled with a novel Weakly Supervised Patch
Selecting (WPS) loss that enables discriminative localization using only
image-level labels. Concurrently, for inter-class transfer, an Adaptive
Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed
using structured knowledge mined from a Large Language Model (LLM), and employs
graph attention network to adaptively transfer relational information between
class representations. DART is the first framework, to our knowledge, to
explicitly integrate external LLM-derived relational knowledge for adaptive
inter-class transfer while simultaneously performing adaptive intra-class
refinement under weak supervision for OV-MLR. Extensive experiments on
challenging benchmarks demonstrate that our DART achieves new state-of-the-art
performance, validating its effectiveness.

</details>


### [129] [WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction](https://arxiv.org/abs/2508.05599)
*Shaobin Zhuang,Yiwei Guo,Canmiao Fu,Zhipeng Huang,Zeyue Tian,Ying Zhang,Chen Li,Yali Wang*

Main category: cs.CV

TL;DR: WeTok是一种新型视觉分词器，通过分组无查找量化（GQ）和生成式解码（GD）两大创新，显著提升了压缩率与重建保真度之间的平衡，在主流基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器在压缩率和重建保真度之间存在不尽人意的权衡，无法同时实现高压缩和高质量重建。

Method: WeTok采用了两项核心创新：1) 分组无查找量化（GQ）：将潜在特征分组，对每组进行无查找量化，有效克服了现有分词器的内存和计算限制，并实现了可扩展码本，突破了重建性能。2) 生成式解码（GD）：引入了一个带有额外噪声变量先验的生成式解码器，能够概率性地建模以离散令牌为条件的视觉数据分布，从而更好地重建视觉细节，尤其是在高压缩率下。

Result: WeTok在主流基准测试中表现出卓越性能。在ImageNet 50k验证集上，WeTok实现了创纪录的零样本rFID（0.12），优于FLUX-VAE（0.18）和SD-VAE 3.5（0.19）。此外，其最高压缩模型在768的压缩比下实现了3.49的零样本rFID，优于Cosmos（384压缩比下为4.57），后者压缩率仅为WeTok的一半。

Conclusion: WeTok是一种强大且简洁的视觉分词器，通过其创新的量化和解码机制，成功解决了现有分词器在压缩与重建权衡方面的不足，实现了卓越的性能，并为视觉生成任务设定了新的基准。

Abstract: Visual tokenizer is a critical component for vision generation. However, the
existing tokenizers often face unsatisfactory trade-off between compression
ratios and reconstruction fidelity. To fill this gap, we introduce a powerful
and concise WeTok tokenizer, which surpasses the previous leading tokenizers
via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We
partition the latent features into groups, and perform lookup-free quantization
for each group. As a result, GQ can efficiently overcome memory and computation
limitations of prior tokenizers, while achieving a reconstruction breakthrough
with more scalable codebooks. (2) Generative Decoding (GD). Different from
prior tokenizers, we introduce a generative decoder with a prior of extra noise
variable. In this case, GD can probabilistically model the distribution of
visual data conditioned on discrete tokens, allowing WeTok to reconstruct
visual details, especially at high compression ratios. Extensive experiments on
mainstream benchmarks show superior performance of our WeTok. On the ImageNet
50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs.
FLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression
model achieves a zero-shot rFID of 3.49 with a compression ratio of 768,
outperforming Cosmos (384) 4.57 which has only 50% compression rate of ours.
Code and models are available: https://github.com/zhuangshaobin/WeTok.

</details>


### [130] [LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model](https://arxiv.org/abs/2508.05602)
*Tao Sun,Oliver Liu,JinJin Li,Lan Ma*

Main category: cs.CV

TL;DR: 本文提出了LLaVA-RE，一个基于多模态大语言模型（MLLM）的二元图像-文本相关性评估器，并构建了一个新的多任务二元相关性数据集，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态生成式AI中，图像-文本相关性评估对于衡量响应质量或排序候选响应至关重要。特别是，二元相关性评估（“相关” vs. “不相关”）是一个基本但具有挑战性的问题，因为文本格式多样且相关性定义因场景而异。作者发现MLLM能灵活处理复杂文本格式并利用额外任务信息，是构建此类评估器的理想选择。

Method: 本文提出了LLaVA-RE，首次尝试使用MLLM进行二元图像-文本相关性评估。它遵循LLaVA架构，并采用了详细的任务指令和多模态上下文样本。此外，作者还提出了一个涵盖各种任务的新型二元相关性数据集。

Result: 实验结果验证了所提出框架的有效性。

Conclusion: LLaVA-RE是使用MLLM进行二元图像-文本相关性评估的首次尝试，结合了LLaVA架构、详细指令和多模态上下文样本，并通过新数据集验证了其有效性，为多模态AI的评估提供了新的解决方案。

Abstract: Multimodal generative AI usually involves generating image or text responses
given inputs in another modality. The evaluation of image-text relevancy is
essential for measuring response quality or ranking candidate responses. In
particular, binary relevancy evaluation, i.e., ``Relevant'' vs. ``Not
Relevant'', is a fundamental problem. However, this is a challenging task
considering that texts have diverse formats and the definition of relevancy
varies in different scenarios. We find that Multimodal Large Language Models
(MLLMs) are an ideal choice to build such evaluators, as they can flexibly
handle complex text formats and take in additional task information. In this
paper, we present LLaVA-RE, a first attempt for binary image-text relevancy
evaluation with MLLM. It follows the LLaVA architecture and adopts detailed
task instructions and multimodal in-context samples. In addition, we propose a
novel binary relevancy data set that covers various tasks. Experimental results
validate the effectiveness of our framework.

</details>


### [131] [Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity](https://arxiv.org/abs/2508.05609)
*Yuhan Zhang,Long Zhuo,Ziyang Chu,Tong Wu,Zhibing Li,Liang Pan,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出Hi3DEval，一个针对3D生成内容的分层评估框架，结合物体级和部件级评估，并扩展材质真实性评估。同时构建了Hi3DBench数据集和3D感知的自动化评分系统，表现优于现有图像基方法且与人类偏好高度一致。


<details>
  <summary>Details</summary>
Motivation: 尽管3D内容生成技术进步迅速，但其质量评估仍面临挑战。现有方法主要依赖图像度量且仅限于物体级别，无法捕捉空间连贯性、材质真实性和高保真局部细节。

Method: 1) 引入Hi3DEval分层评估框架，结合物体级和部件级评估，并扩展材质评估至真实感属性（如反照率、饱和度、金属度）。2) 构建Hi3DBench大规模数据集，包含多样化的3D资产和高质量标注。3) 提出基于混合3D表示的3D感知自动化评分系统，利用视频表示进行物体级和材质主题评估，并使用预训练3D特征进行部件级感知。

Result: 实验证明，该方法在建模3D特性方面优于现有图像基度量，并与人类偏好实现更高的对齐，为手动评估提供了一个可扩展的替代方案。

Conclusion: Hi3DEval框架、Hi3DBench数据集及其3D感知自动化评分系统，为3D生成内容的质量评估提供了一个全面、准确且可扩展的解决方案，显著提升了评估与人类感知的匹配度。

Abstract: Despite rapid advances in 3D content generation, quality assessment for the
generated 3D assets remains challenging. Existing methods mainly rely on
image-based metrics and operate solely at the object level, limiting their
ability to capture spatial coherence, material authenticity, and high-fidelity
local details. 1) To address these challenges, we introduce Hi3DEval, a
hierarchical evaluation framework tailored for 3D generative content. It
combines both object-level and part-level evaluation, enabling holistic
assessments across multiple dimensions as well as fine-grained quality
analysis. Additionally, we extend texture evaluation beyond aesthetic
appearance by explicitly assessing material realism, focusing on attributes
such as albedo, saturation, and metallicness. 2) To support this framework, we
construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and
high-quality annotations, accompanied by a reliable multi-agent annotation
pipeline. We further propose a 3D-aware automated scoring system based on
hybrid 3D representations. Specifically, we leverage video-based
representations for object-level and material-subject evaluations to enhance
modeling of spatio-temporal consistency and employ pretrained 3D features for
part-level perception. Extensive experiments demonstrate that our approach
outperforms existing image-based metrics in modeling 3D characteristics and
achieves superior alignment with human preference, providing a scalable
alternative to manual evaluations. The project page is available at
https://zyh482.github.io/Hi3DEval/.

</details>


### [132] [MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2508.05630)
*Henghui Ding,Kaining Ying,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang,Philip H. S. Torr,Song Bai*

Main category: cs.CV

TL;DR: 本文介绍了MOSEv2，一个用于复杂真实世界场景视频目标分割（VOS）的新型挑战性数据集，旨在推动现有方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管现有VOS方法在DAVIS和YouTube-VOS等基准数据集上表现出色（例如，超过90%的J&F），但这些数据集主要包含显著、主导和孤立的目标，限制了它们在真实世界场景中的泛化能力。MOSEv1虽然尝试引入复杂性，但仍有局限。因此，需要一个更具挑战性的数据集来推进VOS在真实世界条件下的研究。

Method: 本文提出了MOSEv2数据集，包含5,024个视频、超过701,976个高质量掩码和10,074个对象（涵盖200个类别）。MOSEv2引入了显著更高的场景复杂性，包括更频繁的对象消失和再现、严重遮挡和拥挤、更小的对象，以及恶劣天气、低光照、多镜头序列、伪装对象、非物理目标（如阴影、反射）和需要外部知识的场景等新挑战。研究人员在5种不同设置下对20种代表性VOS方法和9种视频目标跟踪（VOT）方法进行了基准测试。

Result: 基准测试结果显示，所有测试方法在MOSEv2上的性能均出现显著下降。例如，SAM2在MOSEv1上的性能从76.4%下降到MOSEv2上的50.9%。VOT方法也观察到类似的性能下降。这些结果表明，MOSEv2对不同任务都带来了挑战。

Conclusion: 尽管现有VOS方法在现有数据集上取得了高精度，但在真实世界的复杂性面前仍然面临困难。MOSEv2的发布旨在推动VOS方法在更具挑战性的真实世界场景中的发展，并揭示了当前技术的局限性。

Abstract: Video object segmentation (VOS) aims to segment specified target objects
throughout a video. Although state-of-the-art methods have achieved impressive
performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and
YouTube-VOS, these datasets primarily contain salient, dominant, and isolated
objects, limiting their generalization to real-world scenarios. To advance VOS
toward more realistic environments, coMplex video Object SEgmentation (MOSEv1)
was introduced to facilitate VOS research in complex scenes. Building on the
strengths and limitations of MOSEv1, we present MOSEv2, a significantly more
challenging dataset designed to further advance VOS methods under real-world
conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks
for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2
introduces significantly greater scene complexity, including more frequent
object disappearance and reappearance, severe occlusions and crowding, smaller
objects, as well as a range of new challenges such as adverse weather (e.g.,
rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot
sequences, camouflaged objects, non-physical targets (e.g., shadows,
reflections), scenarios requiring external knowledge, etc. We benchmark 20
representative VOS methods under 5 different settings and observe consistent
performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9%
on MOSEv2. We further evaluate 9 video object tracking methods and find similar
declines, demonstrating that MOSEv2 presents challenges across tasks. These
results highlight that despite high accuracy on existing datasets, current VOS
methods still struggle under real-world complexities. MOSEv2 is publicly
available at https://MOSE.video.

</details>


### [133] [GAP: Gaussianize Any Point Clouds with Text Guidance](https://arxiv.org/abs/2508.05631)
*Weiqi Zhang,Junsheng Zhou,Haotian Geng,Wenyuan Zhang,Yu-Shen Liu*

Main category: cs.CV

TL;DR: 本文提出GAP，一种新颖的方法，利用文本指导将无色原始点云转换为高质量的3D高斯，解决了无色点云到高斯转换的挑战。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) 在快速高质量渲染方面表现出色，而点云是一种常用且易于获取的3D表示形式。将点云转换为高斯变得日益重要。现有研究已探索彩色点云到高斯的转换，但直接从无色3D点云生成高斯仍是一个未解决的挑战。

Method: GAP方法设计了一个多视角优化框架，利用深度感知图像扩散模型合成不同视角的连贯外观。为确保几何精度，引入了表面锚定机制，在优化过程中将高斯有效约束在3D形状表面。此外，GAP还结合了基于扩散的修复策略，专门用于补全难以观察的区域，并使用文本指导。

Result: GAP在不同复杂程度的点云到高斯生成任务上进行了评估，包括合成点云、具有挑战性的真实世界扫描以及大规模场景，结果表明其能生成高保真度的3D高斯。

Conclusion: GAP成功地解决了从无色原始点云生成高保真3D高斯的问题，通过其独特的多视角优化框架、表面锚定机制、基于扩散的修复策略和文本指导实现了这一目标。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving
fast and high-quality rendering. As point clouds serve as a widely-used and
easily accessible form of 3D representation, bridging the gap between point
clouds and Gaussians becomes increasingly important. Recent studies have
explored how to convert the colored points into Gaussians, but directly
generating Gaussians from colorless 3D point clouds remains an unsolved
challenge. In this paper, we propose GAP, a novel approach that gaussianizes
raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key
idea is to design a multi-view optimization framework that leverages a
depth-aware image diffusion model to synthesize consistent appearances across
different viewpoints. To ensure geometric accuracy, we introduce a
surface-anchoring mechanism that effectively constrains Gaussians to lie on the
surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a
diffuse-based inpainting strategy that specifically targets at completing
hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation
task across varying complexity levels, from synthetic point clouds to
challenging real-world scans, and even large-scale scenes. Project Page:
https://weiqi-zhang.github.io/GAP.

</details>


### [134] [FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing](https://arxiv.org/abs/2508.05636)
*Mohammed Talha Alam,Fahad Shamshad,Fakhri Karray,Karthik Nandakumar*

Main category: cs.CV

TL;DR: FaceAnonyMixer是一个可撤销人脸生成框架，它利用预训练生成模型的潜在空间，合成保护隐私的人脸图像，同时满足生物识别模板保护的可撤销性、不可链接性和不可逆性要求。


<details>
  <summary>Details</summary>
Motivation: 人脸识别技术的发展加剧了隐私担忧，需要既能保护身份又能保持识别效用的方法。现有的人脸匿名化方法通常只侧重于模糊身份，未能满足生物识别模板保护（包括可撤销性、不可链接性和不可逆性）的要求。

Method: FaceAnonyMixer通过以下方式实现：1) 利用预训练生成模型的潜在空间合成隐私保护的人脸图像。2) 将真实人脸图像的潜在编码与从可撤销密钥导出的合成编码不可逆地混合。3) 通过精心设计的多目标损失进一步优化混合后的潜在编码，以满足所有可撤销生物识别要求。生成的图像可直接用于现有的人脸识别系统。

Result: FaceAnonyMixer在基准数据集上表现出卓越的识别准确性，并提供了显著更强的隐私保护。与最近的可撤销生物识别方法相比，在商业API上实现了超过11%的性能提升。

Conclusion: FaceAnonyMixer成功地生成了高质量的可撤销人脸，可以在不修改现有FR系统的情况下直接匹配，有效平衡了识别效用和隐私保护，并满足了可撤销生物识别的关键要求。

Abstract: Advancements in face recognition (FR) technologies have amplified privacy
concerns, necessitating methods that protect identity while maintaining
recognition utility. Existing face anonymization methods typically focus on
obscuring identity but fail to meet the requirements of biometric template
protection, including revocability, unlinkability, and irreversibility. We
propose FaceAnonyMixer, a cancelable face generation framework that leverages
the latent space of a pre-trained generative model to synthesize
privacy-preserving face images. The core idea of FaceAnonyMixer is to
irreversibly mix the latent code of a real face image with a synthetic code
derived from a revocable key. The mixed latent code is further refined through
a carefully designed multi-objective loss to satisfy all cancelable biometric
requirements. FaceAnonyMixer is capable of generating high-quality cancelable
faces that can be directly matched using existing FR systems without requiring
any modifications. Extensive experiments on benchmark datasets demonstrate that
FaceAnonyMixer delivers superior recognition accuracy while providing
significantly stronger privacy protection, achieving over an 11% gain on
commercial API compared to recent cancelable biometric methods. Code is
available at: https://github.com/talha-alam/faceanonymixer.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [135] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 本文提出一种对话后处理方法，通过结合冻结的音频基础模型和大型语言模型（LLM），无需特定任务微调，为转录的对话添加说话人特征（如年龄、性别、情感）元数据标签。


<details>
  <summary>Details</summary>
Motivation: 现有对话转录流程中，LLM常用于改善语法和可读性。本文旨在探索一个补充的后处理步骤：通过添加说话人特征元数据来丰富转录对话，从而提供更全面的信息。

Method: 该方法将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型结合，通过轻量级连接器桥接音频和语言表示，以推断说话人属性。此过程无需对任一模型进行任务特定微调。此外，研究还展示了冻结的LLAMA模型可以直接比较x-vectors。

Result: 该方法在说话人分析任务上取得了具有竞争力的性能，同时保持了模块化和速度。实验表明，在某些场景下，冻结的LLAMA模型直接比较x-vectors可达到8.8%的等错误率（EER）。

Conclusion: 通过结合冻结的音频和语言模型，无需微调，可以有效地为对话转录添加说话人元数据，实现模块化且高效的说话人特征推断。此外，LLM具备直接处理和比较音频嵌入的能力。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [136] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 提出一种新的BPE变体Parity-aware BPE，旨在解决标准分词器对低资源语言不公平的问题，实现跨语言分词公平性，同时对性能影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法基于频率，偏向训练数据中的优势语言，导致低资源语言的分词结果过长、不合理或包含大量未知词元，从而加剧了不同语言用户之间的计算和经济不平等。

Method: 引入Parity-aware BPE，作为BPE算法的变体。在每个合并步骤中，该算法优先最大化当前压缩最差语言的压缩增益，以少量全局压缩损失换取跨语言公平性。

Result: 经验证明，Parity-aware BPE能使不同语言间的词元计数更公平，对全局压缩率影响可忽略，且对下游任务中的语言模型性能无实质性影响。

Conclusion: Parity-aware BPE有效提升了多语言分词的公平性，尤其对低资源语言有利，且不显著牺牲整体性能。

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [137] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: 通过引入联合ASR和重音检测模型，可以显著提升基于半监督语音表示的自动语音识别（ASR）系统性能，同时在重音检测任务上达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过互补的重音检测模块来提升使用半监督语音表示的ASR系统性能，并强调预训练语音模型保留或重新学习韵律线索（如重音）的重要性。

Method: 引入了一个联合ASR和重音检测模型，该模型包含一个互补的重音检测组件，并进行联合训练。

Result: 重音检测组件在F1分数上取得了显著改进，将与SOTA的差距缩小了41%。此外，在有限资源微调下，联合训练使LibriSpeech上的ASR词错误率（WER）降低了28.3%。

Conclusion: 研究结果表明，扩展预训练语音模型以保留或重新学习重要的韵律线索（如重音）对于提升ASR性能至关重要。

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [138] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）缺乏行为一致性，即使是大型模型也表现出显著的响应变异性，且旨在稳定行为的干预措施可能适得其反，表明当前LLM不具备真正的行为一致性基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要一致的行为模式以实现安全部署，但其类人格特质尚未被充分理解，这促使研究评估其行为稳定性。

Method: 本研究提出了PERSIST（合成文本中的人格稳定性）评估框架，测试了25+个开源模型（1B-671B参数），产生了500,000+个响应。研究使用了传统（BFI-44, SD3）和新型LLM适配的人格量表，并系统性地改变了问题顺序、释义、角色设定和推理模式。

Result: 主要发现包括：(1) 即使是400B+模型也表现出显著的响应变异性（标准差 > 0.4）；(2) 仅轻微的提示词重新排序就能使人格测量结果改变高达20%；(3) 预期能稳定行为的干预措施（如思维链推理、详细角色指令、包含对话历史）反而可能增加变异性；(4) LLM适配的量表与以人为中心的版本显示出同等的不稳定性，证实了是架构而非转换限制导致的问题。

Conclusion: 持续存在于不同规模和缓解策略下的不稳定性表明，当前的LLMs缺乏真正行为一致性的基础。对于需要可预测行为的安全关键应用，这些发现表明基于人格的对齐策略可能从根本上不足。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [139] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: RCR-Router是一个多智能体LLM的上下文路由框架，它通过动态选择记忆子集和严格的token预算来提高协作效率，同时保持或提升回答质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型多智能体系统在协调方案上依赖静态或全上下文路由策略，导致token消耗过高、记忆冗余暴露以及交互轮次间的适应性受限。

Method: 本文提出了RCR-Router框架，它是一个模块化且角色感知的上下文路由框架。RCR-Router根据智能体的角色和任务阶段动态选择语义相关的记忆子集，并严格遵守token预算。它采用轻量级评分策略指导记忆选择，并将智能体输出迭代整合到共享记忆存储中以进行渐进式上下文细化。此外，为更好地评估模型行为，作者还提出了一个“回答质量分数”指标，用于衡量LLM生成的解释，超越了标准的问答准确性。

Result: 在HotPotQA、MuSiQue和2WikiMultihop三个多跳问答基准测试上的实验表明，RCR-Router在提高或保持回答质量的同时，将token使用量减少了高达30%。

Conclusion: 研究结果强调了结构化记忆路由和输出感知评估对于推动可扩展多智能体LLM系统发展的重要性。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [140] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: 本文提出了一个评估大型语言模型（LLMs）对语言特征（如委婉语）反应的基准，发现LLMs系统性地惩罚某些语言模式，尤其导致对委婉语的评价降低，从而揭示了其潜在的人口统计学偏见。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估LLMs如何响应可能无意中揭示人口统计学属性（如性别、社会阶层或地域背景）的微妙语言标记（即语言试金石），并揭示自动化评估系统中存在的潜在人口统计学偏见。

Method: 研究构建了一个包含100对已验证问答的采访模拟基准，生成受控的语言变体以隔离特定现象并保持语义等效性。通过这种方法，精确测量自动化评估系统中的人口统计学偏见，并验证了其在多个语言维度上的有效性。

Result: 研究发现，LLMs系统性地惩罚某些语言模式，特别是委婉语，即使内容质量相同。平均而言，委婉的回答会收到25.6%的更低评分。该基准被证明能有效识别特定模型的偏见。

Conclusion: 这项工作为检测和测量人工智能系统中的语言歧视建立了一个基础框架，对自动化决策环境中的公平性具有广泛的应用价值。

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [141] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 该论文提出了一种视觉-语言聚类框架，通过构建动词语义簇来解决现有视觉活动识别系统评估中动词语义和图像解释的模糊性问题，并证明其评估结果与人类判断更一致。


<details>
  <summary>Details</summary>
Motivation: 评估视觉活动识别系统面临挑战，因为动词语义和图像解释存在固有的模糊性（例如同义词、不同视角导致不同但有效的动词选择）。标准的精确匹配评估方法依赖单一“黄金答案”，无法捕捉这些模糊性，导致模型性能评估不完整。

Method: 提出了一种视觉-语言聚类框架，用于构建动词语义簇。该方法将每个图像映射到多个语义簇，每个簇代表图像的不同视角。通过这种聚类方法，对多个活动识别模型进行了评估，并与标准评估方法进行了比较。此外，还进行了人类对齐分析。

Result: 对imSitu数据集的分析表明，每张图像平均映射到2.8个语义簇，每个簇代表图像的一个独特视角。聚类评估方法与标准评估方法进行了比较。人类对齐分析表明，基于聚类的评估方法与人类判断更一致，提供了更细致的模型性能评估。

Conclusion: 所提出的视觉-语言聚类框架能够构建动词语义簇，提供了一种更鲁棒的视觉活动识别系统评估方法。这种基于聚类的评估方法能更好地捕捉动词语义和图像解释的模糊性，并与人类判断更一致，从而提供更细致和全面的模型性能评估。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [142] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 该研究提出了一个多阶段大型语言模型框架，用于从非结构化文本中提取与自杀相关的社会健康决定因素（SDoH），提高了提取准确性和模型可解释性，并证明了小型模型也能达到高性能。


<details>
  <summary>Details</summary>
Motivation: 理解导致自杀事件的社会健康决定因素（SDoH）对早期干预和预防至关重要。然而，现有数据驱动方法面临挑战，包括因素分布的长尾效应、关键应激源分析困难以及模型可解释性有限。

Method: 研究提出了一个多阶段大型语言模型框架，用于增强从非结构化文本中提取SDoH因素。该方法与BioBERT、GPT-3.5-turbo和DeepSeek-R1等先进语言模型和推理模型进行了比较。同时，通过自动化比较和一项初步用户研究，评估了模型解释如何帮助人们更快速、准确地标注SDoH因素。

Result: 所提出的框架在SDoH因素提取的总体任务以及检索相关上下文的细粒度任务中均表现出性能提升。此外，研究发现，对小型、特定任务模型进行微调，可以在降低推理成本的同时，达到可比甚至更好的性能。多阶段设计不仅增强了提取能力，还提供了中间解释，提高了模型的可解释性。

Conclusion: 该方法提高了从非结构化文本中提取与自杀相关的SDoH的准确性和透明度。这些进展有望支持对高危个体的早期识别，并为制定更有效的预防策略提供信息。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [143] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出一种对话方面情感四元组抽取（DiaASQ）方法，通过结构熵最小化算法将对话分割成语义独立的子对话，并采用两步框架进行抽取，实现了SOTA性能和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有DiaASQ方法在整个对话中学习词关系，假设情感元素均匀分布，但对话常包含语义独立的子对话，导致这种全局学习引入额外噪声，影响抽取准确性。

Method: 该方法首先利用结构熵最小化算法将对话划分为语义独立的子对话，旨在保留相关话语并区分不相关话语。接着，采用两步框架进行四元组抽取：第一步在话语级别抽取单个情感元素，第二步在子对话级别匹配四元组。

Result: 该方法在DiaASQ任务上实现了最先进（SOTA）的性能，并且计算成本显著降低。

Conclusion: 通过有效解决对话结构复杂性（即存在多个语义独立的子对话）的问题，该方法显著提升了对话方面情感四元组抽取的准确性和效率。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [144] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 本文评估了微调四种解码器专用大型语言模型（LLM）进行AMR解析的性能，结果表明其能达到与复杂SOTA解析器相当的水平，其中LLaMA 3.2表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 探索将解码器专用LLM进行微调作为AMR解析的一种有前景且直接的新方向。

Method: 使用LDC2020T02 Gold AMR3.0测试集，对Phi 3.5、Gemma 2、LLaMA 3.2和DeepSeek R1 LLaMA Distilled这四种解码器专用LLM架构进行了微调，并评估其性能，主要指标为SMATCH F1。

Result: 直接微调解码器专用LLM可以达到与复杂SOTA AMR解析器相当的性能。LLaMA 3.2在LDC2020T02测试集上取得了0.804的SMATCH F1，与APT + Silver (IBM)持平，并接近Graphene Smatch (MBSE)的0.854。研究还发现LLaMA 3.2在语义性能上领先，而Phi 3.5在结构有效性上表现出色。

Conclusion: 直接微调解码器专用LLM是AMR解析的一种有效且有竞争力的方法，尤其LLaMA 3.2展现出与当前最先进AMR解析器相当的强大性能。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [145] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 本文挑战了多任务学习中PEFT的LoRA多组件范式，发现简单的单适配器LoRA或高相似度多头架构表现更优。提出Align-LoRA，通过显式对齐任务表示，显著提升了多任务LLM适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要适应多领域多样任务（多任务学习，MTL）。当前PEFT（特别是LoRA变体）倾向于使用多适配器或多头等复杂结构来捕获任务特定知识，但这种方法的有效性有待验证。

Method: 1. 实验证明，具有高头间相似度的简化多头架构优于复杂的多适配器和多头系统。2. 进一步展示了增加秩的标准单适配器LoRA也能达到极具竞争力的性能。3. 基于“有效MTL泛化依赖于学习鲁棒共享表示”的假设，提出了Align-LoRA，引入显式损失函数以在共享适配器空间内对齐任务表示。

Result: 1. 简化多头架构（高头间相似度）显著优于复杂的多适配器和多头系统。2. 具有足够高秩的单一适配器LoRA也表现出高度竞争力。3. Align-LoRA显著超越所有基线模型，验证了其在LLM多任务适应中的有效性。

Conclusion: 有效的MTL泛化关键在于学习鲁棒的共享表示，而非隔离任务特定特征。Align-LoRA通过显式对齐任务表示，提供了一种更简单、更有效的大模型多任务适应范式。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [146] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: 本文提出了一种名为“MultiCheck”的统一框架，用于细粒度多模态事实核查，通过结合文本和视觉信号进行推理，并在Factify 2数据集上取得了显著优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 日益增长的多模态虚假信息（由文本和图像共同支持）对主要依赖文本证据的事实核查系统提出了巨大挑战。

Method: MultiCheck框架结合了专用的文本和图像编码器，一个使用元素级交互捕获跨模态关系的融合模块，一个用于预测声明真实性的分类头，以及一个对比学习目标，以促进共享潜在空间中声明-证据对的语义对齐。

Result: 在Factify 2数据集上，该方法取得了0.84的加权F1分数，显著优于基线。

Conclusion: 研究结果强调了显式多模态推理的有效性，并展示了该方法在复杂现实场景中实现可扩展和可解释事实核查的潜力。

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [147] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 针对RAG在长上下文中的性能下降问题（源于熵增长和注意力稀释），本文提出了BEE-RAG框架，通过平衡上下文熵来提高RAG的适应性，并实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在固有的知识限制，RAG（检索增强生成）是弥补这一缺陷的关键方法。然而，RAG通常处理大量检索信息，导致上下文长度过长，进而引发无约束的熵增长和注意力稀释，严重影响RAG性能。

Method: 本文提出了平衡熵工程RAG（BEE-RAG）框架。该框架基于熵不变性原理，通过平衡上下文熵来重新构建注意力动态，从而使注意力敏感性与上下文长度解耦，确保稳定的熵水平。此外，引入了用于多重要性估计的零样本推理策略和参数高效的自适应微调机制，以获取不同设置下的最佳平衡因子。

Result: 在多个RAG任务上的大量实验证明了BEE-RAG框架的有效性。

Conclusion: BEE-RAG通过工程化地平衡上下文熵，成功解决了RAG在长上下文长度下的性能问题，显著提高了RAG系统的适应性和表现。

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [148] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）存在“注意力盆地”现象，即对输入序列首尾信息关注度高，中间信息关注度低。基于此，提出AttnRank框架，通过校准模型注意力偏好并重新排序输入内容，将关键信息置于高注意力位置，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: LLMs的性能对输入信息的位置敏感，但其背后的机制尚不清楚。研究旨在揭示这种位置偏差的机制，并基于此开发一种方法来提升模型性能。

Method: 通过大量实验揭示了“注意力盆地”现象，即模型倾向于关注序列首尾信息。基于此洞察，提出两阶段框架AttnRank：(i) 使用小型校准集估计模型的内在位置注意力偏好；(ii) 重新排序检索到的文档或少样本示例，将最显著的内容与高注意力位置对齐。AttnRank是模型无关、无需训练且即插即用的方法。

Result: 实验证明，AttnRank在多跳问答和少样本上下文学习任务上，显著提升了10种不同架构和规模的LLMs的性能，且无需修改模型参数或训练过程。核心发现是，将更高注意力分配给关键信息是提升模型性能的关键。

Conclusion: LLMs存在显著的位置注意力偏见（注意力盆地现象）。通过识别并利用这种偏见，AttnRank能够有效且高效地将关键信息置于LLMs更关注的位置，从而在不改变模型本身的情况下，显著提升其在多种任务上的表现。

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [149] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 该研究引入了PrinciplismQA基准，用于系统评估大型语言模型在医疗伦理方面的推理能力，发现模型在实际应用中存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常忽视对大型语言模型（LLMs）伦理推理的严格评估，特别是在医疗保健领域，需要一个能系统评估LLM是否符合核心医疗伦理的工具。

Method: 开发了PrinciplismQA，一个包含3,648个问题的综合基准，以“原则主义”为基础。问题包括源自权威教科书的多项选择题和源自权威医疗伦理案例研究文献的开放式问题，所有问题均经过医学专家验证。

Result: 实验显示，模型在伦理知识和实际应用之间存在显著差距，尤其是在将伦理原则动态应用于现实场景时。大多数LLM在处理“受益”原则的困境时表现挣扎，常过度强调其他原则。领先的闭源模型表现最佳，而医学领域微调可以提升模型的整体伦理能力，但仍需更好地与医学伦理知识对齐。

Conclusion: PrinciplismQA提供了一个可扩展的框架，用于诊断LLM特定的伦理弱点，为开发更平衡、更负责任的医疗AI铺平了道路。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [150] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: 本文介绍了ATLANTIS团队在SemEval-2025任务3中检测问答系统幻觉文本的工作，通过少样本提示、token级分类或微调LLM，在西班牙语上取得了顶级排名。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言生成方面取得了显著进展，但仍容易产生幻觉，即生成不正确或误导性的内容，这需要被检测和解决。

Method: 探索了使用和不使用外部上下文的方法，具体包括：使用LLM进行少样本提示、进行token级分类，或在合成数据上对LLM进行微调。

Result: 在西班牙语中取得了顶级排名，并在英语和德语中获得了有竞争力的成绩。

Conclusion: 强调了整合相关上下文对于减轻幻觉的重要性，并展示了微调模型和提示工程的潜力。

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [151] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 本文提出MulCoT-RD模型，通过“教师-助手-学生”蒸馏范式，使轻量级模型在资源受限环境下同时进行多模态情感推理生成和情感分类。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖参数量大的（多模态）大型语言模型进行情感分类，忽略了在资源受限环境中自主多模态情感推理生成的能力。

Method: 提出MulCoT-RD模型，用于资源受限的联合多模态情感推理与分类（JMSRC）任务。该模型采用“教师-助手-学生”蒸馏范式：首先利用高性能多模态大语言模型（MLLM）生成初始推理数据集；然后训练一个中等大小的助手模型，采用多任务学习机制；最后联合训练一个轻量级的学生模型，以高效执行多模态情感推理生成和分类。

Result: 在四个数据集上的广泛实验表明，MulCoT-RD模型仅用3B参数就在JMSRC任务上取得了强大性能，并展现出鲁棒的泛化能力和增强的可解释性。

Conclusion: MulCoT-RD模型成功解决了资源受限环境下的多模态情感分析问题，实现了情感推理生成和分类的联合任务，并表现出高效、高性能和良好的可解释性。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [152] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 该研究提出一种受人脑启发的LLM剪枝方法，通过识别并保留LLM中的功能网络及其关键神经元，克服了传统剪枝忽视神经元交互的问题，实现了高效模型压缩。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法通常忽视人工神经元之间的交互与协作，导致LLM宏观功能架构被破坏，进而影响剪枝性能。研究旨在解决这一问题，提升LLM在实际应用中的效率。

Method: 受人脑功能神经网络的启发，将LLM视为“数字大脑”，将其分解为多个功能网络，类似于识别神经影像数据中的功能脑网络。然后，通过保留这些功能网络中的关键神经元来实现LLM的剪枝。

Result: 实验结果表明，所提出的方法能够成功识别并定位LLM中的功能网络和关键神经元，从而实现高效的模型剪枝。

Conclusion: 该研究通过引入功能网络的概念，为LLM的结构化剪枝提供了一种新颖有效的方法，解决了传统剪枝方法中忽视神经元交互的局限性，提升了剪枝性能。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [153] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
*Sijie Wang,Quanjiang Guo,Kai Zhao,Yawei Zhang,Xin Li,Xiang Li,Siqi Li,Rui She,Shangshu Yu,Wee Peng Tay*

Main category: cs.CL

TL;DR: CodeBoost是一种无需人工标注指令，仅利用代码片段对代码大语言模型进行后训练的框架，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的代码大语言模型后训练依赖于稀缺且难以规模化收集的人工标注指令，而代码片段资源丰富，这种不平衡限制了基于指令的后训练。

Method: CodeBoost框架包含五个关键组件：1) 最大团簇精选，从代码中选择有代表性和多样性的训练语料；2) 双向预测，使模型能从正向和反向预测目标中学习；3) 错误感知预测，利用正确和不正确输出的学习信号；4) 异构增强，多样化训练分布以丰富代码语义；5) 异构奖励，通过多种奖励类型（格式正确性、执行反馈）指导模型学习。

Result: 在多个代码大语言模型和基准测试上的大量实验验证，CodeBoost始终能提升模型性能。

Conclusion: CodeBoost证明了其作为一种可扩展且有效的训练流程，能够纯粹利用代码片段来增强代码大语言模型的能力。

Abstract: Code large language models (LLMs) have become indispensable tools for
building efficient and automated coding pipelines. Existing models are
typically post-trained using reinforcement learning (RL) from general-purpose
LLMs using "human instruction-final answer" pairs, where the instructions are
usually from manual annotations. However, collecting high-quality coding
instructions is both labor-intensive and difficult to scale. On the other hand,
code snippets are abundantly available from various sources. This imbalance
presents a major bottleneck in instruction-based post-training. We propose
CodeBoost, a post-training framework that enhances code LLMs purely from code
snippets, without relying on human-annotated instructions. CodeBoost introduces
the following key components: (1) maximum-clique curation, which selects a
representative and diverse training corpus from code; (2) bi-directional
prediction, which enables the model to learn from both forward and backward
prediction objectives; (3) error-aware prediction, which incorporates learning
signals from both correct and incorrect outputs; (4) heterogeneous
augmentation, which diversifies the training distribution to enrich code
semantics; and (5) heterogeneous rewarding, which guides model learning through
multiple reward types including format correctness and execution feedback from
both successes and failures. Extensive experiments across several code LLMs and
benchmarks verify that CodeBoost consistently improves performance,
demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [154] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
*Dongxu Zhang,Ning Yang,Jihua Zhu,Jinnan Yang,Miao Xin,Baoliang Tian*

Main category: cs.CL

TL;DR: 本文挑战了CoT推理中“早期错误更致命”的假设，揭示了“后期脆弱性”现象，即后期错误对最终答案影响更大。为解决此问题，提出了ASCoT方法，通过自适应验证和多视角纠正显著提高了LLM推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链（CoT）提示显著提升了大型语言模型（LLM）的推理能力，但推理链的可靠性仍是关键挑战。传统的“级联失败”假设认为早期错误最具破坏性，但本文通过实验质疑了这一假设，发现后期错误对最终答案的影响更大（“后期脆弱性”），这促使研究更具针对性的纠错策略。

Method: 本文提出了自适应自我纠正思维链（ASCoT）方法。该方法采用模块化流程，包括：1. 自适应验证管理器（AVM），它利用位置影响评分函数I(k)根据推理链中的位置分配不同权重，以识别并优先处理高风险的后期步骤，解决后期脆弱性问题。2. 多视角自我纠正引擎（MSCE），它对AVM识别出的关键错误部分进行鲁棒的双路径纠正。

Result: 在GSM8K和MATH等基准测试上的广泛实验表明，ASCoT方法取得了卓越的准确性，优于包括标准CoT在内的强大基线方法。

Conclusion: 本研究强调了诊断LLM推理中特定故障模式的重要性，并主张从统一的验证策略转向自适应的、漏洞感知的纠正机制。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning
capabilities of Large Language Models (LLMs), yet the reliability of these
reasoning chains remains a critical challenge. A widely held "cascading
failure" hypothesis suggests that errors are most detrimental when they occur
early in the reasoning process. This paper challenges that assumption through
systematic error-injection experiments, revealing a counter-intuitive
phenomenon we term "Late-Stage Fragility": errors introduced in the later
stages of a CoT chain are significantly more likely to corrupt the final answer
than identical errors made at the beginning. To address this specific
vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought
(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive
Verification Manager (AVM) operates first, followed by the Multi-Perspective
Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score
function I(k) that assigns different weights based on the position within the
reasoning chains, addressing the Late-Stage Fragility issue by identifying and
prioritizing high-risk, late-stage steps. Once these critical steps are
identified, the MSCE applies robust, dual-path correction specifically to the
failure parts. Extensive experiments on benchmarks such as GSM8K and MATH
demonstrate that ASCoT achieves outstanding accuracy, outperforming strong
baselines, including standard CoT. Our work underscores the importance of
diagnosing specific failure modes in LLM reasoning and advocates for a shift
from uniform verification strategies to adaptive, vulnerability-aware
correction mechanisms.

</details>


### [155] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
*Sukannya Purkayastha,Nils Dycke,Anne Lauscher,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出使用大语言模型（LLMs）通过自精炼策略生成高质量合成数据，以训练对话代理来辅助元评审员进行决策，从而提高元评审效率。


<details>
  <summary>Details</summary>
Motivation: 元评审不仅是总结评论报告，更是一个复杂的决策过程，需要权衡论点并置于更广阔的背景中。现有研究表明对话代理能有效辅助决策者，但在元评审领域实现此类代理面临数据稀缺的挑战。

Method: 1. 采用大语言模型（LLMs）结合自精炼策略生成合成对话数据，以解决数据稀缺问题并提高数据与专家领域的关联性。2. 利用这些合成数据训练专门用于元评审的对话代理。3. 将训练出的代理与现成的基于LLM的助手进行比较。4. 将所开发的代理应用于真实的元评审场景。

Result: 1. 所提出的方法能生成更高质量的合成数据，并可作为训练元评审助手的宝贵资源。2. 训练出的对话代理在元评审任务中表现优于现成的基于LLM的助手。3. 在真实世界的元评审场景中，这些代理有效提升了元评审的效率。

Conclusion: 通过LLMs的自精炼策略生成高质量合成数据，可以有效训练出专门的对话代理，这些代理能够显著辅助元评审员，提高元评审过程的效率。

Abstract: Meta-reviewing is a pivotal stage in the peer-review process, serving as the
final step in determining whether a paper is recommended for acceptance. Prior
research on meta-reviewing has treated this as a summarization problem over
review reports. However, complementary to this perspective, meta-reviewing is a
decision-making process that requires weighing reviewer arguments and placing
them within a broader context. Prior research has demonstrated that
decision-makers can be effectively assisted in such scenarios via dialogue
agents. In line with this framing, we explore the practical challenges for
realizing dialog agents that can effectively assist meta-reviewers. Concretely,
we first address the issue of data scarcity for training dialogue agents by
generating synthetic data using Large Language Models (LLMs) based on a
self-refinement strategy to improve the relevance of these dialogues to expert
domains. Our experiments demonstrate that this method produces higher-quality
synthetic data and can serve as a valuable resource towards training
meta-reviewing assistants. Subsequently, we utilize this data to train dialogue
agents tailored for meta-reviewing and find that these agents outperform
\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our
agents in real-world meta-reviewing scenarios and confirm their effectiveness
in enhancing the efficiency of meta-reviewing.\footnote{Code and Data:
https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [156] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
*Nikita Dragunov,Temurbek Rahmatullaev,Elizaveta Goncharova,Andrey Kuznetsov,Anton Razzhigaev*

Main category: cs.CL

TL;DR: SONAR-LLM是一种新型解码器Transformer，它在连续SONAR嵌入空间中思考，但通过冻结的SONAR解码器进行基于token的交叉熵监督训练，从而在不使用扩散采样器的情况下实现与LCM相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的LCM模型虽然通过预测句子级嵌入来生成文本，但其训练方式（均方误差或扩散目标）引入了扩散采样器，这可能增加复杂性并缺乏基于似然的训练信号。研究动机是保留LCM的语义抽象能力，同时消除扩散采样器并恢复基于似然的训练信号。

Method: 该研究提出了SONAR-LLM，一个仅解码器Transformer模型。它在连续的SONAR嵌入空间中处理信息（“思考”），但通过一个冻结的SONAR解码器将监督信号（token级别的交叉熵）反向传播。这种混合目标既保留了LCM的语义抽象，又实现了基于似然的训练信号，避免了扩散采样器。

Result: SONAR-LLM在39M到1.3B参数规模的模型上均实现了有竞争力的生成质量。研究报告了模型扩展趋势、消融实验和基准测试结果，并发布了完整的训练代码和所有预训练检查点，以促进复现和未来研究。

Conclusion: SONAR-LLM成功地结合了语义抽象和基于似然的训练，在不需要扩散采样器的情况下实现了高质量的文本生成。这种方法为未来的研究提供了更简单、更可复现的路径。

Abstract: The recently proposed Large Concept Model (LCM) generates text by predicting
a sequence of sentence-level embeddings and training with either mean-squared
error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer
that "thinks" in the same continuous SONAR embedding space, yet is supervised
through token-level cross-entropy propagated via the frozen SONAR decoder. This
hybrid objective retains the semantic abstraction of LCM while eliminating its
diffusion sampler and restoring a likelihood-based training signal. Across
model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive
generation quality. We report scaling trends, ablations, benchmark results, and
release the complete training code and all pretrained checkpoints to foster
reproducibility and future research.

</details>


### [157] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: 本文提出了一种名为CGRS（Certainty-Guided Reflection Suppression）的新方法，通过在大型推理语言模型（LRLM）对其响应高度自信时动态抑制反思触发词，从而减少过度思考问题，显著降低token使用量并保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: LRLM中复杂的链式思考和反思行为（如使用“Wait”、“Alternatively”等触发词）虽然能提升性能，但常导致“过度思考”问题，即生成冗余的推理步骤，从而不必要地增加token使用、提高推理成本并降低实用性。

Method: CGRS方法通过在模型对其当前响应表现出高置信度时，动态抑制其生成反思触发词，从而防止冗余的反思循环，同时不损害输出质量。该方法与模型无关，无需重新训练或修改架构，可无缝集成到现有自回归生成流程中。

Result: 在AIME24、AMC23、MATH500和GPQA-D四个推理基准上的广泛实验表明，CGRS平均减少了18.5%至41.9%的token使用量，同时保持了准确性。与最先进的基线相比，它在长度缩减和性能之间实现了最佳平衡。这些结果在不同模型架构（如DeepSeek-R1-Distill系列、QwQ-32B和Qwen3家族）和规模（4B到32B参数）上均保持一致。

Conclusion: CGRS是一种实用且高效的推理方法，能够有效缓解LRLM中的过度思考问题，显著节省资源并提高效率，同时不牺牲推理准确性，具有重要的实际应用价值。

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [158] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本研究调查了在Microsoft Hololens 2上为手语（SL）虚拟形象添加调整功能的影响。结果显示，尽管用户偏好可调整设置，但用户体验和可理解性并未显著提高，反而因动画和实现问题导致压力增加。研究强调个性化不足以弥补默认可理解性的缺失。


<details>
  <summary>Details</summary>
Motivation: 旨在探究为现有手语虚拟形象添加调整功能，对德国手语（DGS）用户在微软HoloLens 2设备上的交互、可理解性、用户体验和系统可接受性的影响。

Method: 通过详细分析德语手语专家用户与可调整和不可调整虚拟形象在特定使用案例中的交互，识别影响可理解性、用户体验（UX）和系统可接受性的关键因素。

Result: 用户偏好可调整设置，但用户体验和可理解性未见显著改善，仍处于较低水平。主要原因包括缺少手语元素（口型、面部表情）和实现问题（手形不清晰、缺乏反馈、菜单定位）。享乐质量高于实用质量。可调整虚拟形象导致更高的压力水平。用户对HoloLens调整手势的直观性存疑。可调整概念的接受度普遍积极，但强烈依赖于可用性和动画质量。

Conclusion: 仅靠个性化不足以解决问题，手语虚拟形象必须默认具有可理解性。关键建议包括增强口型和面部动画、改进交互界面以及采用参与式设计。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


### [159] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
*Samy Ateia,Udo Kruschwitz*

Main category: cs.CL

TL;DR: 本文探讨了在生物医学等专业领域，代理式检索增强生成（RAG）和“深度研究”系统在处理专家信息需求时面临的挑战，并研究了大型语言模型（LLM）的自我反馈机制如何影响其性能。


<details>
  <summary>Details</summary>
Motivation: 将自主搜索系统应用于生物医学等专业领域搜索时，可能导致用户参与度降低，且与专家信息需求不符，因为专业搜索任务通常需要高水平的用户专业知识和透明度。因此，需要研究LLM在这些场景下的表现，特别是其自我纠正能力。

Method: 研究利用BioASQ CLEF 2025挑战赛的专家提问，测试了Gemini-Flash 2.0、o3-mini、o4-mini和DeepSeek-R1等推理和非推理LLM。核心方法是采用自我反馈机制，LLM生成、评估并改进其输出，用于查询扩展和多种答案类型（是/否、事实、列表、理想答案）。研究旨在探究迭代自我纠正是否能提高性能，以及推理模型是否能生成更有用的反馈。

Result: 初步结果表明，自我反馈策略在不同模型和任务上的表现各异。

Conclusion: 这项工作为LLM的自我纠正提供了见解，并为未来比较LLM生成的反馈与人类专家直接输入在这些搜索系统中的有效性提供了基础。

Abstract: Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim
to enable autonomous search processes where Large Language Models (LLMs)
iteratively refine outputs. However, applying these systems to domain-specific
professional search, such as biomedical research, presents challenges, as
automated systems may reduce user involvement and misalign with expert
information needs. Professional search tasks often demand high levels of user
expertise and transparency. The BioASQ CLEF 2025 challenge, using
expert-formulated questions, can serve as a platform to study these issues. We
explored the performance of current reasoning and nonreasoning LLMs like
Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our
methodology was a self-feedback mechanism where LLMs generated, evaluated, and
then refined their outputs for query expansion and for multiple answer types
(yes/no, factoid, list, ideal). We investigated whether this iterative
self-correction improves performance and if reasoning models are more capable
of generating useful feedback. Preliminary results indicate varied performance
for the self-feedback strategy across models and tasks. This work offers
insights into LLM self-correction and informs future work on comparing the
effectiveness of LLM-generated feedback with direct human expert input in these
search systems.

</details>


### [160] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
*Eleftherios Avramidis,Vera Czehmann,Fabian Deckert,Lorenz Hufe,Aljoscha Lipski,Yuni Amaloa Quintero Villalobos,Tae Kwon Rhee,Mengqian Shi,Lennart Stölting,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文介绍了一个包含12种手语的视频平行语料库，总时长超过1300小时，并配有相应国家主流口语的字幕。


<details>
  <summary>Details</summary>
Motivation: 手语资源稀缺，尤其是拉丁美洲手语缺乏一致的平行语料库，且现有语料库（如德国手语）规模较小，限制了研究和应用。

Method: 通过从新闻节目、政府机构和教育频道等在线来源收集和处理多手语视频，包括数据收集、通知内容创作者并征求使用许可、爬取和裁剪等多个阶段。

Result: 创建了一个包含超过1300小时、4381个视频文件、130万个字幕（含1400万个词元）的平行语料库，其中首次包含了8种拉丁美洲手语的一致平行语料，并将德国手语语料库的规模扩大了十倍。

Conclusion: 该工作成功构建了一个大规模、多样的手语平行语料库，显著丰富了手语研究资源，特别是填补了拉丁美洲手语的空白，并详细介绍了其构建方法和统计数据。

Abstract: We present a collection of parallel corpora of 12 sign languages in video
format, together with subtitles in the dominant spoken languages of the
corresponding countries. The entire collection includes more than 1,300 hours
in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.
Most notably, it includes the first consistent parallel corpora for 8 Latin
American sign languages, whereas the size of the German Sign Language corpora
is ten times the size of the previously available corpora. The collection was
created by collecting and processing videos of multiple sign languages from
various online sources, mainly broadcast material of news shows, governmental
bodies and educational channels. The preparation involved several stages,
including data collection, informing the content creators and seeking usage
approvals, scraping, and cropping. The paper provides statistics on the
collection and an overview of the methods used to collect the data.

</details>


### [161] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 该论文引入了MyCulture基准测试，旨在通过一种新颖的开放式多项选择题格式，全面评估大型语言模型（LLMs）在马来西亚文化方面的表现，以解决现有模型存在的文化和语言偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs的训练数据主要由高资源语言（如英语和中文）主导，导致模型存在文化偏见，难以准确表示和评估多样化的文化语境，尤其是在低资源语言环境下。

Method: 引入MyCulture基准测试，评估LLMs在马来西亚文化（涵盖艺术、服饰、习俗、娱乐、食物、宗教六大支柱）方面的表现，使用马来语提问。采用创新的开放式多项选择题格式，不提供预设选项，以减少猜测和格式偏差。提供了该开放式结构在提高公平性和区分度方面的理论依据。此外，通过比较模型在结构化和自由形式输出上的性能来分析结构性偏见，并通过多语言提示变体评估语言偏见。

Result: 对一系列区域和国际LLMs的评估显示，模型在文化理解方面存在显著差异。

Conclusion: 迫切需要在LLMs的开发和评估中，采用以文化为基础且语言包容的基准测试。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [162] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
*Ming Zhang,Yujiong Shen,Jingyi Deng,Yuhui Wang,Yue Zhang,Junzhe Wang,Shichun Liu,Shihan Dou,Huayu Sha,Qiyuan Peng,Changhao Jiang,Jingqi Tong,Yilong Wu,Zhihao Zhang,Mingqi Wu,Zhiheng Xi,Mingxu Chai,Tao Liang,Zhihui Fei,Zhen Wang,Mingyang Wan,Guojun Ma,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMEval-3是一个动态评估大型语言模型（LLMs）的框架，旨在解决现有静态基准测试中数据污染和排行榜过拟合的问题，提供更真实、可靠的模型能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估方法依赖静态基准测试，容易受到数据污染和排行榜过拟合的影响，这模糊了模型的真实能力。

Method: LLMEval-3基于一个包含22万个研究生水平问题的专有题库，每次评估动态采样未见过的测试集。其自动化流程包括抗污染数据管理、新颖的防作弊架构，以及一个与人类专家判断一致性达90%的LLM作为评判者机制，并辅以相对排名系统进行公平比较。

Result: 一项对近50个领先模型进行的20个月纵向研究表明，模型在知识记忆方面存在性能上限，并揭示了静态基准测试无法检测到的数据污染漏洞。该框架在排名稳定性和一致性方面表现出卓越的鲁棒性。

Conclusion: LLMEval-3提供了一种强大且可信的方法来评估LLMs的真实能力，超越了排行榜分数，促进了更值得信赖的评估标准的发展。

Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is
vulnerable to data contamination and leaderboard overfitting, critical issues
that obscure true model capabilities. To address this, we introduce LLMEval-3,
a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary
bank of 220k graduate-level questions, from which it dynamically samples unseen
test sets for each evaluation run. Its automated pipeline ensures integrity via
contamination-resistant data curation, a novel anti-cheating architecture, and
a calibrated LLM-as-a-judge process achieving 90% agreement with human experts,
complemented by a relative ranking system for fair comparison. An 20-month
longitudinal study of nearly 50 leading models reveals a performance ceiling on
knowledge memorization and exposes data contamination vulnerabilities
undetectable by static benchmarks. The framework demonstrates exceptional
robustness in ranking stability and consistency, providing strong empirical
validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and
credible methodology for assessing the true capabilities of LLMs beyond
leaderboard scores, promoting the development of more trustworthy evaluation
standards.

</details>


### [163] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
*Chenzhuo Zhao,Xinda Wang,Yue Huang,Junting Lu,Ziqian Liu*

Main category: cs.CL

TL;DR: TASE是一个评估大型语言模型（LLMs）在多语言环境下对细粒度、标记级别信息理解和结构推理能力的综合基准测试，结果显示当前LLMs在此方面仍远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在高级语义任务上表现出色，但它们在细粒度的标记级别理解和结构推理方面常常遇到困难，而这些能力对于需要精确性和控制的应用至关重要。

Method: 引入了TASE基准测试，包含10个任务，分为标记感知和结构理解两大类，涵盖中文、英文和韩文。数据集包含35,927个实例，并提供可扩展的合成数据生成管道用于训练。评估了超过30种主流LLMs，并使用GRPO训练方法训练了一个定制的Qwen2.5-14B模型。

Result: 评估结果显示，人类在标记级别推理方面的表现显著优于当前的LLMs，揭示了LLMs在这一领域持续存在的弱点。

Conclusion: TASE基准测试揭示了LLMs在低级别语言理解和跨语言泛化方面的局限性，并为未来改进这些能力提供了新的诊断视角。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
on high-level semantic tasks, they often struggle with fine-grained,
token-level understanding and structural reasoning--capabilities that are
essential for applications requiring precision and control. We introduce TASE,
a comprehensive benchmark designed to evaluate LLMs' ability to perceive and
reason about token-level information across languages. TASE covers 10 tasks
under two core categories: token awareness and structural understanding,
spanning Chinese, English, and Korean, with a 35,927-instance evaluation set
and a scalable synthetic data generation pipeline for training. Tasks include
character counting, token alignment, syntactic structure parsing, and length
constraint satisfaction. We evaluate over 30 leading commercial and open-source
LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a
custom Qwen2.5-14B model using the GRPO training method. Results show that
human performance significantly outpaces current LLMs, revealing persistent
weaknesses in token-level reasoning. TASE sheds light on these limitations and
provides a new diagnostic lens for future improvements in low-level language
understanding and cross-lingual generalization. Our code and dataset are
publicly available at https://github.com/cyzcz/Tase .

</details>


### [164] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
*Li-Chun Lu,Miri Liu,Pin-Chun Lu,Yufei Tian,Shao-Hua Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 该研究系统评估了多种创造力衡量指标，发现它们一致性有限且存在各自局限，强调需要更鲁棒的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前的创造力评估指标可能存在局限性和不一致性，无法准确捕捉创造力的多维度特征并与人类判断对齐，因此需要系统地检验和比较这些指标。

Method: 研究系统地检查、分析并比较了四种代表性创造力衡量指标：创造力指数、困惑度、句法模板和LLM-as-a-Judge。评估涵盖了创意写作、非常规问题解决和研究构思等不同创意领域。

Result: 分析显示这些指标的一致性有限，各自捕捉了创造力的不同维度。具体而言，创造力指数侧重词汇多样性，困惑度对模型置信度敏感，句法模板无法捕捉概念创造力，而LLM-as-a-Judge则表现出不稳定性和偏见。

Conclusion: 现有创造力评估框架存在不足，研究结果强调迫切需要开发更鲁棒、更具通用性且能更好与人类创造力判断对齐的评估框架。

Abstract: We systematically examine, analyze, and compare representative creativity
measures--creativity index, perplexity, syntactic templates, and
LLM-as-a-Judge--across diverse creative domains, including creative writing,
unconventional problem-solving, and research ideation. Our analyses reveal that
these metrics exhibit limited consistency, capturing different dimensions of
creativity. We highlight key limitations, including the creativity index's
focus on lexical diversity, perplexity's sensitivity to model confidence, and
syntactic templates' inability to capture conceptual creativity. Additionally,
LLM-as-a-Judge shows instability and bias. Our findings underscore the need for
more robust, generalizable evaluation frameworks that better align with human
judgments of creativity.

</details>


### [165] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在知识密集型任务中存在幻觉和复杂推理能力不足的问题。本文提出逻辑增强生成（LAG）范式，通过系统性问题分解和依赖感知推理，显著提升LLMs的推理鲁棒性并减少幻觉，提供了一种比现有检索增强生成（RAG）系统更具原则性的替代方案。


<details>
  <summary>Details</summary>
Motivation: LLMs在知识密集型任务中易产生幻觉，且现有检索增强生成（RAG）系统因依赖直接语义检索和缺乏结构化逻辑组织，在复杂推理场景中表现不佳。

Method: LAG范式受笛卡尔原理启发，首先将复杂问题分解为按逻辑依赖排序的原子子问题。然后，顺序解决这些子问题，利用先前答案指导后续子问题的上下文检索，确保逐步的逻辑链条。为防止错误传播，LAG引入逻辑终止机制，在遇到无法回答的子问题时停止推理。最后，综合所有子问题的解决方案生成验证过的响应。

Result: 在四个基准数据集上的实验表明，LAG显著增强了推理鲁棒性，减少了幻觉，并使LLM的问题解决方式与人类认知更加一致。

Conclusion: LAG提供了一种比现有RAG系统更具原则性的替代方案，能够有效提升LLMs在知识密集型任务中的推理能力和准确性，减少幻觉。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [166] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 本研究通过“20个问题”游戏评估大型语言模型（LLMs）的隐性地理偏见，发现模型在推断全球北方和西方实体时表现优于全球南方和东方，且这种偏见与预训练数据频率关联不大，语言影响也微乎其微。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs已针对显性偏见进行调整，但其预训练数据中仍存在微妙的隐性偏见。传统的直接提问方式可能触发模型的安全机制，无法有效揭示这些偏见。因此，研究模型主动提问时的行为，能更自然地揭示其内在偏见。

Method: 研究采用“20个问题”的多轮演绎任务作为测试平台，构建了新的数据集Geo20Q+，包含来自不同地区的著名人物和文化物品。在两种游戏配置（经典20问和无限轮次）和七种语言（英语、印地语、普通话、日语、法语、西班牙语和土耳其语）下，系统评估了主流LLMs的地理推断性能。

Result: 研究发现显著的地理性能差异：LLMs在推断全球北方和西方实体时比全球南方和东方实体更成功。维基百科页面浏览量和预训练语料库频率与性能有轻微关联，但不足以完全解释这些差异。值得注意的是，游戏所用语言对性能差距的影响微乎其微。

Conclusion: 这些发现表明，创新的、自由形式的评估框架对于揭示LLMs中标准提示设置下难以发现的微妙偏见具有重要价值。通过分析模型如何启动和追求多轮推理目标，揭示了其推理过程中嵌入的地理和文化差异。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [167] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
*Santosh T. Y. S. S,Youssef Tarek Elkhayat,Oana Ichim,Pranav Shetty,Dongsheng Wang,Zhiqiang Ma,Armineh Nourbakhsh,Xiaomo Liu*

Main category: cs.CL

TL;DR: 针对法律文本生成中大型语言模型（LLM）输出不忠实的问题，本文提出了CoCoLex，一种信心引导的基于复制的解码策略，通过动态融合模型预测和上下文复制来增强忠实性，并在法律基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: LLM虽能处理复杂长文本，对法律领域有益，但其输出常不忠实、无根据或产生幻觉，阻碍了应用。现有RAG方法无法保证上下文有效整合，而上下文感知解码策略也未明确强制忠实性。

Method: 引入CoCoLex（Confidence-guided Copy-based Decoding for Legal Text Generation），这是一种解码策略。它根据模型的置信度，动态地将模型生成的词汇分布与从上下文中复制而来的分布进行插值，以鼓励直接复制，从而确保对源文本的更高忠实度。

Result: 在五个法律基准测试上的实验结果表明，CoCoLex优于现有的上下文感知解码方法，尤其在长文本生成任务中表现更佳。

Conclusion: CoCoLex通过信心引导的复制机制，有效解决了法律文本生成中LLM输出不忠实的问题，显著提升了生成内容对源上下文的忠实性，并超越了现有方法。

Abstract: Due to their ability to process long and complex contexts, LLMs can offer key
benefits to the Legal domain, but their adoption has been hindered by their
tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While
Retrieval-Augmented Generation offers a promising solution by grounding
generations in external knowledge, it offers no guarantee that the provided
context will be effectively integrated. To address this, context-aware decoding
strategies have been proposed to amplify the influence of relevant context, but
they usually do not explicitly enforce faithfulness to the context. In this
work, we introduce Confidence-guided Copy-based Decoding for Legal Text
Generation (CoCoLex)-a decoding strategy that dynamically interpolates the
model produced vocabulary distribution with a distribution derived based on
copying from the context. CoCoLex encourages direct copying based on the
model's confidence, ensuring greater fidelity to the source. Experimental
results on five legal benchmarks demonstrate that CoCoLex outperforms existing
context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [168] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 该研究提出了一种基于频率的不确定性量化方法，结合共形预测，以提高黑盒大型语言模型在多项选择题问答中的可靠性，并证明其优于基于logit的方法且能提供覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多项选择题问答中表现出色，但其固有的不可靠性（如幻觉和过度自信）限制了它们在高风险领域的应用，因此需要一种可靠的不确定性量化方法。

Method: 提出了一种在黑盒设置下基于频率的不确定性量化方法。该方法利用共形预测来确保可证明的覆盖保证，涉及对模型输出分布进行多次独立采样，并以最频繁的样本作为参考来计算预测熵（PE）。

Result: 在六个大型语言模型和四个数据集（MedMCQA, MedQA, MMLU, MMLU-Pro）上的实验表明，基于频率的预测熵在区分正确和错误预测方面优于基于logit的预测熵（通过AUROC衡量）。此外，该方法能有效控制用户指定风险水平下的经验未覆盖率，验证了采样频率在黑盒场景中可以作为基于logit概率的有效替代。

Conclusion: 这项工作提供了一个无分布、模型无关的框架，用于在多项选择题问答中进行可靠的不确定性量化，并具有覆盖保证，从而增强了大型语言模型在实际应用中的可信度。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [169] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
*Franziska Weeber,Tanise Ceron,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究发现，多语言大型语言模型（MLLMs）中的政治观点在西方语言之间是相互迁移的，即使只用英语数据进行对齐，观点也会在所有语言中统一变化，这揭示了实现MLLMs社会语言和文化政治对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 公众舆论调查显示跨文化背景下政治观点存在差异，但目前尚不清楚这些差异是否会体现在多语言大型语言模型（MLLMs）的跨语言观点中。研究旨在探究MLLMs中的观点是跨语言迁移，还是每种语言都有独立的观点。

Method: 研究分析了不同规模的MLLMs在五种西方语言中的表现。通过提示模型对来自投票建议应用的政治声明表达（不）同意来评估其观点。为了理解语言间的交互，研究在模型使用直接偏好优化（DPO）和仅限英语的对齐数据进行左右政治观点对齐之前和之后都进行了评估。

Result: 研究发现，未经对齐的模型在反映政治观点时，跨语言差异极少且不显著。而政治对齐（即使仅使用英语数据）几乎在所有五种语言中都统一地改变了模型的政治观点。

Conclusion: 研究得出结论，在西方语言背景下，多语言大型语言模型中的政治观点在语言之间是相互迁移的。这表明在实现MLLMs明确的社会语言、文化和政治对齐方面存在挑战。

Abstract: Public opinion surveys show cross-cultural differences in political opinions
between socio-cultural contexts. However, there is no clear evidence whether
these differences translate to cross-lingual differences in multilingual large
language models (MLLMs). We analyze whether opinions transfer between languages
or whether there are separate opinions for each language in MLLMs of various
sizes across five Western languages. We evaluate MLLMs' opinions by prompting
them to report their (dis)agreement with political statements from voting
advice applications. To better understand the interaction between languages in
the models, we evaluate them both before and after aligning them with more left
or right views using direct preference optimization and English alignment data
only. Our findings reveal that unaligned models show only very few significant
cross-lingual differences in the political opinions they reflect. The political
alignment shifts opinions almost uniformly across all five languages. We
conclude that in Western language contexts, political opinions transfer between
languages, demonstrating the challenges in achieving explicit socio-linguistic,
cultural, and political alignment of MLLMs.

</details>


### [170] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
*Shaoxiong Zhan,Yanlin Lai,Ziyu Lu,Dahua Lin,Ziqing Yang,Fei Tang*

Main category: cs.CL

TL;DR: MathSmith是一个新颖的框架，通过从头开始合成高难度数学问题来增强大型语言模型的数学推理能力，并在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面取得进展，但受限于高质量、高难度训练数据的稀缺性。现有合成方法依赖于转换人工模板，限制了多样性和可扩展性。

Method: MathSmith通过从PlanetMath随机采样概念-解释对来从头构建新的数学问题，确保数据独立性。它设计了九种预定义策略作为软约束来增加推理难度，并采用强化学习共同优化结构有效性、推理复杂性和答案一致性。推理轨迹长度被用作认知复杂度的衡量标准。此外，它还包含一个弱点聚焦的变体生成模块以实现针对性改进。

Result: 在五个数学基准测试（包括简单、中等和困难类别）中，MathSmith在短CoT和长CoT设置下均持续优于现有基线。其还展示了强大的可扩展性、泛化性和可迁移性。

Conclusion: MathSmith证明了高难度合成数据在提升大型语言模型数学推理能力方面的巨大潜力，具有强大的可扩展性、泛化性和可迁移性。

Abstract: Large language models have achieved substantial progress in mathematical
reasoning, yet their advancement is limited by the scarcity of high-quality,
high-difficulty training data. Existing synthesis methods largely rely on
transforming human-written templates, limiting both diversity and scalability.
We propose MathSmith, a novel framework for synthesizing challenging
mathematical problems to enhance LLM reasoning. Rather than modifying existing
problems, MathSmith constructs new ones from scratch by randomly sampling
concept-explanation pairs from PlanetMath, ensuring data independence and
avoiding contamination. To increase difficulty, we design nine predefined
strategies as soft constraints during rationales. We further adopts
reinforcement learning to jointly optimize structural validity, reasoning
complexity, and answer consistency. The length of the reasoning trace generated
under autoregressive prompting is used to reflect cognitive complexity,
encouraging the creation of more demanding problems aligned with
long-chain-of-thought reasoning. Experiments across five benchmarks,
categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,
OlympiadBench), show that MathSmith consistently outperforms existing baselines
under both short and long CoT settings. Additionally, a weakness-focused
variant generation module enables targeted improvement on specific concepts.
Overall, MathSmith exhibits strong scalability, generalization, and
transferability, highlighting the promise of high-difficulty synthetic data in
advancing LLM reasoning capabilities.

</details>


### [171] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 本文提出Cooper框架，通过联合优化策略模型和奖励模型，解决了LLM强化学习中基于规则奖励的鲁棒性不足和基于模型奖励的奖励作弊问题，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习中的奖励范式（基于规则和基于模型）存在局限性：基于规则的奖励缺乏鲁棒性，而基于模型的奖励容易受到奖励作弊（reward hacking）的影响。

Method: 本文提出Cooper框架，联合优化策略模型和奖励模型。Cooper利用基于规则奖励的高精度来识别正确响应，并动态构建和选择正负样本对以持续训练奖励模型，从而增强鲁棒性并减轻奖励作弊风险。此外，还引入了混合标注策略高效生成奖励模型训练数据，并提出了基于参考的奖励建模范式，训练出VerifyRM奖励模型。

Result: VerifyRM在VerifyBench上实现了更高的精度。实验表明，Cooper不仅缓解了奖励作弊问题，还提升了端到端RL性能，例如在Qwen2.5-1.5B-Instruct上平均准确率提升了0.54%。

Conclusion: 动态更新奖励模型是应对奖励作弊的有效方法，为更好地将奖励模型整合到强化学习中提供了参考。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [172] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: OmniEAR是一个评估大型语言模型具身推理能力的综合框架，揭示了在物理交互、工具使用和多智能体协调等具身任务中，模型在处理隐式约束和复杂情境时的显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在抽象推理方面表现出色，但其在具身智能体推理（涉及物理交互、工具使用和多智能体协调）方面的能力尚待充分探索。现有基准测试通常提供预定义工具集或明确协作指令，无法评估模型动态获取能力和自主决定协作策略的能力。

Method: 本文提出了OmniEAR框架，通过文本表示环境来建模连续物理属性和复杂空间关系。该框架包含1500个跨家庭和工业领域的场景，要求智能体动态获取能力并根据任务需求自主确定协调策略。通过系统评估来衡量模型在面对约束条件时的推理能力。

Result: 模型在处理显式指令时成功率达85-96%，但在需要从约束中推理时性能严重下降：工具推理成功率为56-85%，隐式协作成功率为63-85%，复合任务失败率超过50%。令人惊讶的是，提供完整的环境信息反而会降低协调性能，表明模型无法过滤任务相关约束。微调显著改善了单智能体任务（0.6%升至76.3%），但对多智能体任务提升微乎其微（1.5%升至5.5%），暴露了模型架构的根本局限性。

Conclusion: 具身推理对当前模型提出了根本性的不同挑战。OmniEAR作为一个严格的基准，揭示了大型语言模型在具身推理方面的不足，并为评估和推进具身AI系统提供了重要工具。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [173] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
*Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas Oğuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau Yih*

Main category: cs.CL

TL;DR: 本文提出一种新的奖励函数，用于在线强化学习以提升推理大型语言模型（R-LLMs）的事实推理能力，显著降低幻觉率并保持回答的详细度和相关性。


<details>
  <summary>Details</summary>
Motivation: 推理大型语言模型（R-LLMs）在复杂推理任务上表现出色，但在长篇事实性基准测试中，其幻觉率远高于非推理模型。将在线强化学习（RL）应用于长篇事实性设置面临缺乏可靠验证方法的挑战。现有离线RL方法（如FActScore）直接用于在线RL时会导致奖励作弊，例如生成不够详细或不相关的回答。

Method: 本文提出了一种新颖的奖励函数，该函数同时考虑了事实准确性、回答详细程度和答案相关性。在此奖励函数的基础上，应用在线强化学习来学习高质量的事实推理能力。

Result: 在六个长篇事实性基准测试中，所提出的事实推理模型平均将幻觉率降低了23.1个百分点，回答详细程度增加了23%，并且整体回答的有用性没有下降。

Conclusion: 通过设计新的多维度奖励函数并结合在线强化学习，可以有效提升推理大型语言模型的事实推理能力，显著减少幻觉，同时保持并提升回答的详细度和相关性，克服了以往奖励函数导致的作弊问题。

Abstract: Reasoning Large Language Models (R-LLMs) have significantly advanced complex
reasoning tasks but often struggle with factuality, generating substantially
more hallucinations than their non-reasoning counterparts on long-form
factuality benchmarks. However, extending online Reinforcement Learning (RL), a
key component in recent R-LLM advancements, to the long-form factuality setting
poses several unique challenges due to the lack of reliable verification
methods. Previous work has utilized automatic factuality evaluation frameworks
such as FActScore to curate preference data in the offline RL setting, yet we
find that directly leveraging such methods as the reward in online RL leads to
reward hacking in multiple ways, such as producing less detailed or relevant
responses. We propose a novel reward function that simultaneously considers the
factual precision, response detail level, and answer relevance, and applies
online RL to learn high quality factual reasoning. Evaluated on six long-form
factuality benchmarks, our factual reasoning model achieves an average
reduction of 23.1 percentage points in hallucination rate, a 23% increase in
answer detail level, and no degradation in the overall response helpfulness.

</details>


### [174] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 该研究使用线性探针（linear probes）来分析大型语言模型（LLMs）的劝说能力，发现探针能有效且高效地揭示劝说动态，甚至优于基于提示的方法，为研究LLMs的复杂行为提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已展现出劝说人类的能力，但我们对这种动态如何发生的理解仍然有限。现有工作已使用线性探针分析LLMs的其他能力，这促使研究者思考是否能将探针应用于劝说动态的分析。

Method: 研究者将线性探针应用于自然、多轮对话中的劝说动态研究。他们结合认知科学的见解，训练探针以识别劝说成功、被劝说者个性以及劝说策略等不同劝说方面。同时，将探针方法与基于提示（prompting-based）的方法进行了比较。

Result: 研究表明，尽管线性探针结构简单，但它们能在样本和数据集层面捕捉到劝说的各个方面。例如，探针可以识别对话中被劝说者被说服的时间点，或整个数据集中劝说成功的普遍发生点。此外，探针不仅比昂贵的基于提示的方法更快，而且在某些情况下（如揭示劝说策略时）表现得同样好甚至更好。

Conclusion: 该研究表明，线性探针是研究劝说等复杂行为（如欺骗和操纵）的有效途径，尤其是在多轮对话设置和大规模数据集分析中，探针比基于提示的方法更具计算效率。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [175] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: H-NET++是一种分层动态分块的字节级语言模型，它通过端到端训练学习语言学分段，在波斯语等形态丰富的语言上实现了最先进的性能，同时保持了计算效率。


<details>
  <summary>Details</summary>
Motivation: 字节级语言模型消除了脆弱的分词器，但在形态丰富的语言（MRLs）中面临计算挑战，因为单词可能跨越许多字节，导致效率低下。

Method: 提出了H-NET++，一个分层动态分块模型，通过端到端训练学习语言学分段。关键创新包括：1) 用于跨块注意力的轻量级Transformer上下文混合器（1.9M参数）；2) 用于文档级一致性的两级潜在超先验；3) 对正字法伪影（如波斯语ZWNJ）的专门处理；4) 基于课程的分阶段序列长度训练。

Result: 在14亿词元的波斯语语料库上，H-NET++实现了最先进的结果：相对于基于BPE的GPT-2-fa，BPB（每字节比特）减少0.159（压缩率提高12%）；在ParsGLUE上性能提升5.4pp；对ZWNJ损坏的鲁棒性提高53%；在黄金形态边界上F1分数达到73.8%。其学习到的分块与波斯语形态学对齐，无需显式监督。

Conclusion: 分层动态分块为形态丰富的语言提供了一种有效的、无需分词器的解决方案，同时保持了计算效率，并且能够学习到与语言学对齐的语素边界。

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [176] [On the causality between affective impact and coordinated human-robot reactions](https://arxiv.org/abs/2508.04834)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 研究表明，机器人主动分享对事件的反应会显著改变人类对其情感影响的感知，并且对于不同类型的机器人和目标，存在最佳反应时间。


<details>
  <summary>Details</summary>
Motivation: 为了改善机器人在社交环境中的功能，研究旨在探究机器人主动与人类分享对事件的反应是否会改变人类对机器人情感影响的感知。

Method: 设计了两种测试设置：第一种旨在突出并隔离情感反应元素，通过测试组和对照组（n=84）与机器人互动进行；第二种旨在调查特定时间延迟对机器人对物理接触反应的影响，共有110名参与者，每10名参与者增加一次反应延迟。

Result: 结果显示，当机器人对与人类共享的事件做出反应时，其感知情感影响有统计学上的显著变化（p<.05），而非随机反应。对于共享的物理互动，机器人接近人类的反应时间最合适。此外，对于小型非人形机器人，约200毫秒的延迟可能产生最大影响；而当目标是让人类感觉对机器人影响最大时，约100毫秒的反应时间最有效。

Conclusion: 机器人主动分享反应能显著增强其情感影响力。对于小型非人形机器人，约200毫秒的反应延迟能产生最大影响；若目标是让观察者感到对机器人影响最大，则约100毫秒的反应时间最有效。

Abstract: In an effort to improve how robots function in social contexts, this paper
investigates if a robot that actively shares a reaction to an event with a
human alters how the human perceives the robot's affective impact. To verify
this, we created two different test setups. One to highlight and isolate the
reaction element of affective robot expressions, and one to investigate the
effects of applying specific timing delays to a robot reacting to a physical
encounter with a human. The first test was conducted with two different groups
(n=84) of human observers, a test group and a control group both interacting
with the robot. The second test was performed with 110 participants using
increasingly longer reaction delays for the robot with every ten participants.
The results show a statistically significant change (p$<$.05) in perceived
affective impact for the robots when they react to an event shared with a human
observer rather than reacting at random. The result also shows for shared
physical interaction, the near-human reaction times from the robot are most
appropriate for the scenario. The paper concludes that a delay time around
200ms may render the biggest impact on human observers for small-sized
non-humanoid robots. It further concludes that a slightly shorter reaction time
around 100ms is most effective when the goal is to make the human observers
feel they made the biggest impact on the robot.

</details>


### [177] [INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM](https://arxiv.org/abs/2508.04931)
*Jin Wang,Weijie Wang,Boyuan Deng,Heng Zhang,Rui Dai,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: INTENTION是一个新颖的机器人操控框架，它结合了视觉语言模型（VLM）的场景推理和交互驱动的记忆，使机器人能够在多样化场景中实现自主操控和学习交互直觉。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人控制方法依赖精确模型和预定义序列，在复杂真实世界中因模型不准确和泛化能力差而失效。而人类能直观地与环境互动，展现出卓越的适应性和高效决策能力，这启发了本研究。

Method: 提出INTENTION框架，整合基于VLM的场景推理和交互驱动记忆。引入“记忆图谱”（Memory Graph）记录过往任务交互场景，以模拟人类理解和决策。设计“直觉感知器”（Intuitive Perceptor）从视觉场景中提取物理关系和可供性。

Result: 通过记忆图谱和直觉感知器，机器人能够在新的场景中推断出合适的交互行为，而无需依赖重复指令，从而实现自主操控和泛化能力。

Conclusion: INTENTION框架通过融合VLM场景推理和交互记忆，赋予机器人学习到的交互直觉和自主操控能力，使其能适应多样化的真实世界场景。

Abstract: Traditional control and planning for robotic manipulation heavily rely on
precise physical models and predefined action sequences. While effective in
structured environments, such approaches often fail in real-world scenarios due
to modeling inaccuracies and struggle to generalize to novel tasks. In
contrast, humans intuitively interact with their surroundings, demonstrating
remarkable adaptability, making efficient decisions through implicit physical
understanding. In this work, we propose INTENTION, a novel framework enabling
robots with learned interactive intuition and autonomous manipulation in
diverse scenarios, by integrating Vision-Language Models (VLMs) based scene
reasoning with interaction-driven memory. We introduce Memory Graph to record
scenes from previous task interactions which embodies human-like understanding
and decision-making about different tasks in real world. Meanwhile, we design
an Intuitive Perceptor that extracts physical relations and affordances from
visual scenes. Together, these components empower robots to infer appropriate
interaction behaviors in new scenes without relying on repetitive instructions.
Videos: https://robo-intention.github.io

</details>


### [178] [Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation](https://arxiv.org/abs/2508.04981)
*Tianyuan Zheng,Jingang Yi,Kaiyan Yu*

Main category: cs.RO

TL;DR: 本文提出了一种名为HCMR的算法，用于解决多机器人双重覆盖问题，该算法能生成最优且无碰撞的路径，显著提升路径长度和任务时间效率。


<details>
  <summary>Details</summary>
Motivation: 在已知环境中，需要为多机器人规划高效、无碰撞的路径，以同时覆盖线性特征（服务）和区域（探索）。服务功能成本较高但覆盖范围小，探索功能成本较低但覆盖范围大，因此需要优化规划以平衡这两种角色。

Method: 本文提出了分层循环合并调节（HCMR）算法。该方法从莫尔斯理论角度分析流形附着过程以降低规划复杂性，确保解决方案属于莫尔斯有界集合。HCMR通过循环合并搜索来调节遍历行为，并利用边序列反向传播将这些调节转换为图边遍历序列。结合平衡分区，选择最优序列为每个机器人生成路径。

Result: HCMR算法在固定扫描方向下被证明是最优的。多机器人仿真结果表明，与现有最先进的规划方法相比，HCMR算法规划的路径长度至少缩短了10.0%，平均任务时间至少减少了16.9%，并确保了无冲突操作。

Conclusion: HCMR算法为多机器人双重覆盖问题提供了一种最优且高效的解决方案，在路径长度、任务时间和冲突避免方面显著优于其他现有方法。

Abstract: The double coverage problem focuses on determining efficient, collision-free
routes for multiple robots to simultaneously cover linear features (e.g.,
surface cracks or road routes) and survey areas (e.g., parking lots or local
regions) in known environments. In these problems, each robot carries two
functional roles: service (linear feature footprint coverage) and exploration
(complete area coverage). Service has a smaller operational footprint but
incurs higher costs (e.g., time) compared to exploration. We present optimal
planning algorithms for the double coverage problems using hierarchical cyclic
merging regulation (HCMR). To reduce the complexity for optimal planning
solutions, we analyze the manifold attachment process during graph traversal
from a Morse theory perspective. We show that solutions satisfying minimum path
length and collision-free constraints must belong to a Morse-bounded
collection. To identify this collection, we introduce the HCMR algorithm. In
HCMR, cyclic merging search regulates traversal behavior, while edge sequence
back propagation converts these regulations into graph edge traversal
sequences. Incorporating balanced partitioning, the optimal sequence is
selected to generate routes for each robot. We prove the optimality of the HCMR
algorithm under a fixed sweep direction. The multi-robot simulation results
demonstrate that the HCMR algorithm significantly improves planned path length
by at least 10.0%, reduces task time by at least 16.9% in average, and ensures
conflict-free operation compared to other state-of-the-art planning methods.

</details>


### [179] [Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots](https://arxiv.org/abs/2508.04994)
*Wenjie Hu,Ye Zhou,Hann Woei Ho*

Main category: cs.RO

TL;DR: 针对DDPG在迷宫导航中稀疏奖励、探索效率低和长期规划困难的问题，本文提出了一种分层DDPG（HDDPG）算法，通过高低层策略、离线策略修正、自适应噪声和重塑奖励函数等改进，显著提高了导航成功率和平均奖励。


<details>
  <summary>Details</summary>
Motivation: DDPG算法在迷宫导航任务中存在稀疏奖励、探索效率低下和长期规划困难等问题，导致成功率和平均奖励较低，甚至无法有效导航。

Method: 提出高效分层DDPG（HDDPG）算法，包含高层和低层策略。高层策略使用改进DDPG生成长期子目标，低层策略也使用改进DDPG生成原始动作以实现子目标。方法增强了离线策略修正以细化子目标，利用自适应参数空间噪声改进探索，采用重塑的内在-外在奖励函数提升学习效率。此外，还通过梯度裁剪和Xavier初始化优化了鲁棒性。

Result: 通过ROS和Gazebo进行的数值仿真实验表明，HDDPG在自主迷宫导航任务中显著克服了标准DDPG及其变体的局限性，相比基线算法，成功率至少提高了56.59%，平均奖励至少提升了519.03%。

Conclusion: HDDPG算法有效解决了标准DDPG在迷宫导航中的挑战，通过分层策略和多项优化，大幅提升了导航性能，实现了更高的成功率和奖励。

Abstract: Maze navigation is a fundamental challenge in robotics, requiring agents to
traverse complex environments efficiently. While the Deep Deterministic Policy
Gradient (DDPG) algorithm excels in control tasks, its performance in maze
navigation suffers from sparse rewards, inefficient exploration, and
long-horizon planning difficulties, often leading to low success rates and
average rewards, sometimes even failing to achieve effective navigation. To
address these limitations, this paper proposes an efficient Hierarchical DDPG
(HDDPG) algorithm, which includes high-level and low-level policies. The
high-level policy employs an advanced DDPG framework to generate intermediate
subgoals from a long-term perspective and on a higher temporal scale. The
low-level policy, also powered by the improved DDPG algorithm, generates
primitive actions by observing current states and following the subgoal
assigned by the high-level policy. The proposed method enhances stability with
off-policy correction, refining subgoal assignments by relabeling historical
experiences. Additionally, adaptive parameter space noise is utilized to
improve exploration, and a reshaped intrinsic-extrinsic reward function is
employed to boost learning efficiency. Further optimizations, including
gradient clipping and Xavier initialization, are employed to improve
robustness. The proposed algorithm is rigorously evaluated through numerical
simulation experiments executed using the Robot Operating System (ROS) and
Gazebo. Regarding the three distinct final targets in autonomous maze
navigation tasks, HDDPG significantly overcomes the limitations of standard
DDPG and its variants, improving the success rate by at least 56.59% and
boosting the average reward by a minimum of 519.03 compared to baseline
algorithms.

</details>


### [180] [MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding](https://arxiv.org/abs/2508.05021)
*Weifan Zhang,Tingguang Li,Yuzhen Liu*

Main category: cs.RO

TL;DR: 该论文提出一个基于视觉语言模型（VLM）的机器人导航框架，通过主动视角调整和历史记忆回溯机制，实现在未知环境中仅依靠自然语言描述进行导航，无需预训练或微调。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中仅依靠自然语言描述进行视觉导航是智能机器人的关键能力。现有方法被动依赖偶然的视觉输入，导致在复杂环境中视觉-语言匹配（grounding）能力不足，且泛化性受限。

Method: 该方法构建于现成的视觉语言模型（VLMs）之上，并引入两种受人类启发机制：1. 基于视角的活动接地（perspective-based active grounding），动态调整机器人视角以优化视觉感知；2. 历史记忆回溯（historical memory backtracking），使系统能够保留并重新评估不确定的观测结果。该框架以零样本方式运行，无需标记数据或模型微调。

Result: 在Habitat-Matterport 3D (HM3D) 数据集上的实验结果表明，该方法在语言驱动的对象导航方面优于最先进的方法。此外，在四足机器人上的实际部署也展示了其鲁棒和有效的导航性能。

Conclusion: 该框架通过主动优化感知和利用记忆来解决歧义，显著提高了在复杂、未知环境中的视觉-语言接地能力，并对多样化和开放式语言描述表现出强大的泛化能力，为智能机器人的自然语言导航提供了一种有效且实用的解决方案。

Abstract: Visual navigation in unknown environments based solely on natural language
descriptions is a key capability for intelligent robots. In this work, we
propose a navigation framework built upon off-the-shelf Visual Language Models
(VLMs), enhanced with two human-inspired mechanisms: perspective-based active
grounding, which dynamically adjusts the robot's viewpoint for improved visual
inspection, and historical memory backtracking, which enables the system to
retain and re-evaluate uncertain observations over time. Unlike existing
approaches that passively rely on incidental visual inputs, our method actively
optimizes perception and leverages memory to resolve ambiguity, significantly
improving vision-language grounding in complex, unseen environments. Our
framework operates in a zero-shot manner, achieving strong generalization to
diverse and open-ended language descriptions without requiring labeled data or
model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show
that our method outperforms state-of-the-art approaches in language-driven
object navigation. We further demonstrate its practicality through real-world
deployment on a quadruped robot, achieving robust and effective navigation
performance.

</details>


### [181] [Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning](https://arxiv.org/abs/2508.05027)
*Philip Huang,Yorai Shaoul,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文对多臂机器人轨迹的现有快捷优化方法进行了定量比较研究，分析了它们的优缺点，并提出了两种组合策略以实现最佳性能-运行时权衡。


<details>
  <summary>Details</summary>
Motivation: 多臂机器人运动规划因高维度和潜在的臂间碰撞而具有挑战性，传统方法生成的运动在平滑度和执行时间上往往不理想。快捷优化是提高运动质量的常用方法，但在多臂场景中，优化一条臂的运动不能引入与其他臂的碰撞。现有研究虽提及快捷优化，但其具体方法和对性能的影响描述模糊。

Method: 通过在不同模拟场景中对现有多种多臂轨迹快捷优化方法进行定量比较研究，仔细分析每种方法的优缺点，并提出了两种简单的组合策略以平衡性能和运行时间。

Result: 对现有快捷优化方法进行了全面的定量比较，分析了它们的优缺点，并提出了两种实现最佳性能-运行时权衡的简单组合策略。

Conclusion: 该研究为多臂机器人轨迹优化提供了关于不同快捷优化方法的深入见解，并提出了实用的组合策略以提高规划质量和效率。

Abstract: Generating high-quality motion plans for multiple robot arms is challenging
due to the high dimensionality of the system and the potential for inter-arm
collisions. Traditional motion planning methods often produce motions that are
suboptimal in terms of smoothness and execution time for multi-arm systems.
Post-processing via shortcutting is a common approach to improve motion quality
for efficient and smooth execution. However, in multi-arm scenarios, optimizing
one arm's motion must not introduce collisions with other arms. Although
existing multi-arm planning works often use some form of shortcutting
techniques, their exact methodology and impact on performance are often vaguely
described. In this work, we present a comprehensive study quantitatively
comparing existing shortcutting methods for multi-arm trajectories across
diverse simulated scenarios. We carefully analyze the pros and cons of each
shortcutting method and propose two simple strategies for combining these
methods to achieve the best performance-runtime tradeoff. Video, code, and
dataset are available at https://philip-huang.github.io/mr-shortcut/.

</details>


### [182] [A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System](https://arxiv.org/abs/2508.05040)
*Boyang Zhang,Jiahui Zuo,Zeyu Duan,Fumin Zhang*

Main category: cs.RO

TL;DR: 本文提出一种基于视觉的碰撞检测模块，用于软抓手在抓取圆形物体时，即使发生外部碰撞也能保持稳定抓取。


<details>
  <summary>Details</summary>
Motivation: 机器人执行器受到的外部碰撞会危及圆形物体的抓取稳定性。

Method: 开发了一个基于视觉的传感模块，配备广角掌中相机，同时监测手指和被抓取物体的运动。此外，还开发了一种富含碰撞的抓取策略，以确保动态抓取过程的稳定性和安全性。

Result: 碰撞检测机制的响应时间测试证实系统能即时响应碰撞。躲避测试表明夹持器能精确检测外部碰撞的方向和规模。

Conclusion: 该系统能有效检测并响应外部碰撞，从而在动态抓取过程中保持软抓手对圆形物体的稳定和安全抓取。

Abstract: External collisions to robot actuators typically pose risks to grasping
circular objects. This work presents a vision-based sensing module capable of
detecting collisions to maintain stable grasping with a soft gripper system.
The system employs an eye-in-palm camera with a broad field of view to
simultaneously monitor the motion of fingers and the grasped object.
Furthermore, we have developed a collision-rich grasping strategy to ensure the
stability and security of the entire dynamic grasping process. A physical soft
gripper was manufactured and affixed to a collaborative robotic arm to evaluate
the performance of the collision detection mechanism. An experiment regarding
testing the response time of the mechanism confirmed the system has the
capability to react to the collision instantaneously. A dodging test was
conducted to demonstrate the gripper can detect the direction and scale of
external collisions precisely.

</details>


### [183] [Examining the legibility of humanoid robot arm movements in a pointing task](https://arxiv.org/abs/2508.05104)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Ana Farić,Kristína Malinovská,Michal Vavrecka,Igor Farkaš*

Main category: cs.RO

TL;DR: 本研究探讨了类人机器人手臂动作在指向任务中的可读性，发现人类能通过截断的动作和身体线索（如凝视和指向）预测机器人意图。


<details>
  <summary>Details</summary>
Motivation: 人机交互需要机器人动作具有可读性，以便人类能够理解、预测并感到安全。本研究旨在理解人类如何从截断的机器人动作和身体线索中预测机器人意图。

Method: 实验使用NICO类人机器人，参与者观察其手臂指向触摸屏上目标。机器人线索包括凝视、指向，以及凝视与指向一致或不一致的情况。手臂轨迹在完成60%或80%时停止，参与者预测最终目标。实验旨在检验多模态优越性和眼部优先假设。

Result: 实验结果支持了多模态优越性假设和眼部优先假设。

Conclusion: 人类能够利用多模态线索（如凝视和指向）有效预测机器人意图，且眼部线索在预测中具有重要作用，这对于提升人机交互中的机器人可读性至关重要。

Abstract: Human--robot interaction requires robots whose actions are legible, allowing
humans to interpret, predict, and feel safe around them. This study
investigates the legibility of humanoid robot arm movements in a pointing task,
aiming to understand how humans predict robot intentions from truncated
movements and bodily cues. We designed an experiment using the NICO humanoid
robot, where participants observed its arm movements towards targets on a
touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing
with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or
80\% of their full length, and participants predicted the final target. We
tested the multimodal superiority and ocular primacy hypotheses, both of which
were supported by the experiment.

</details>


### [184] [From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation](https://arxiv.org/abs/2508.05143)
*Siméon Capy,Thomas M. Kwok,Kevin Joseph,Yuichiro Kawasumi,Koichi Nagashima,Tomoya Sasaki,Yue Hu,Eiichi Yoshida*

Main category: cs.RO

TL;DR: 本研究发现，在机器人远程操作（RTo）中，距离对用户感知没有显著影响，表明远程机器人可作为传统本地控制的有效替代方案，尤其适用于老年护理。


<details>
  <summary>Details</summary>
Motivation: 机器人远程操作是本地控制的一种可行替代方案，尤其是在仍需要人工干预的情况下。本研究旨在探讨距离对RTo中用户感知的影响，并探索远程操作机器人在老年人护理中的潜力。

Method: 研究设计了一个评估方案，考察非专业用户对长距离RTo的感知，比较交互前后以及与本地操作机器人的感知差异。采用了多份问卷和基于ROS与Unity的专用软件架构。

Result: 结果显示，本地和远程机器人操作条件之间没有统计学上的显著差异。

Conclusion: 研究得出结论，远程操作机器人可能是传统本地控制的一种可行替代方案。

Abstract: Robot teleoperation (RTo) has emerged as a viable alternative to local
control, particularly when human intervention is still necessary. This research
aims to study the distance effect on user perception in RTo, exploring the
potential of teleoperated robots for older adult care. We propose an evaluation
of non-expert users' perception of long-distance RTo, examining how their
perception changes before and after interaction, as well as comparing it to
that of locally operated robots. We have designed a specific protocol
consisting of multiple questionnaires, along with a dedicated software
architecture using the Robotics Operating System (ROS) and Unity. The results
revealed no statistically significant differences between the local and remote
robot conditions, suggesting that robots may be a viable alternative to
traditional local control.

</details>


### [185] [Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories](https://arxiv.org/abs/2508.05148)
*Francisco Munguia-Galeano,Zhengxue Zhou,Satheeshkumar Veeramani,Hatem Fakhruldeen,Louis Longley,Rob Clowes,Andrew I. Cooper*

Main category: cs.RO

TL;DR: Chemist Eye是一个分布式安全监控系统，利用多传感器和视觉-语言模型（VLM）提升自动驾驶实验室（SDLs）中的态势感知，监测PPE合规性、火灾和人员事故，并与机器人联动以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶实验室（SDLs）中集成机器人和自动化技术引入了额外的安全复杂性，包括对人员防护设备（PPE）的需求、火灾风险（尤其是移动机器人使用的易燃锂电池），以及传统实验室中已有的安全挑战。因此，需要一个系统来增强SDLs中的态势感知和安全保障。

Method: Chemist Eye系统包含多个配备RGB、深度和红外摄像头的监测站。它使用视觉-语言模型（VLM）进行决策，以识别潜在事故、医疗紧急情况、PPE合规性问题和火灾隐患。系统可与移动机器人实时通信，根据VLM的建议引导机器人避开火灾、出口或未佩戴PPE的人员，并发出声音警告。此外，它还与第三方消息平台集成，向实验室人员提供即时通知。

Result: 在配备三台移动机器人的实际SDL数据上测试了Chemist Eye。结果显示，发现潜在安全隐患的准确率达到97%，决策性能达到95%。

Conclusion: Chemist Eye系统有效地提高了自动驾驶实验室的安全监控和态势感知能力，能够准确识别安全隐患并做出相应决策，从而增强了实验室人员和设备的安全性。

Abstract: The integration of robotics and automation into self-driving laboratories
(SDLs) can introduce additional safety complexities, in addition to those that
already apply to conventional research laboratories. Personal protective
equipment (PPE) is an essential requirement for ensuring the safety and
well-being of workers in laboratories, self-driving or otherwise. Fires are
another important risk factor in chemical laboratories. In SDLs, fires that
occur close to mobile robots, which use flammable lithium batteries, could have
increased severity. Here, we present Chemist Eye, a distributed safety
monitoring system designed to enhance situational awareness in SDLs. The system
integrates multiple stations equipped with RGB, depth, and infrared cameras,
designed to monitor incidents in SDLs. Chemist Eye is also designed to spot
workers who have suffered a potential accident or medical emergency, PPE
compliance and fire hazards. To do this, Chemist Eye uses decision-making
driven by a vision-language model (VLM). Chemist Eye is designed for seamless
integration, enabling real-time communication with robots. Based on the VLM
recommendations, the system attempts to drive mobile robots away from potential
fire locations, exits, or individuals not wearing PPE, and issues audible
warnings where necessary. It also integrates with third-party messaging
platforms to provide instant notifications to lab personnel. We tested Chemist
Eye with real-world data from an SDL equipped with three mobile robots and
found that the spotting of possible safety hazards and decision-making
performances reached 97 % and 95 %, respectively.

</details>


### [186] [FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction](https://arxiv.org/abs/2508.05153)
*Mohammed Daba,Jing Qiu*

Main category: cs.RO

TL;DR: 该论文提出了FCBV-Net，一种用于机器人服装平滑的3D点云方法，通过将几何特征与双臂动作价值学习解耦，显著提高了类别级泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人服装操作（如双臂平滑）的类别级泛化面临高维度、复杂动力学和类别内变化的挑战。现有方法要么对特定实例过拟合，要么尽管具有类别级感知泛化能力，却无法预测协同双臂动作的价值。

Method: 本文提出了特征条件双臂价值网络（FCBV-Net），它在3D点云上操作，以增强服装平滑的类别级策略泛化。FCBV-Net将双臂动作价值预测基于预训练、冻结的密集几何特征，以确保对类别内服装变化的鲁棒性。可训练的下游组件随后利用这些静态特征学习特定任务的策略。

Result: 在GarmentLab模拟实验中，FCBV-Net在CLOTH3D数据集上展示了卓越的类别级泛化能力。与基于2D图像的基线相比，FCBV-Net在未见过服装上的效率下降仅为11.5%（Steps80），而后者为96.2%。它实现了89%的最终覆盖率，优于使用相同点几何特征但固定原语的基于3D对应关系的基线（83%覆盖率）。

Conclusion: 研究结果表明，将几何理解与双臂动作价值学习解耦，能够实现更好的类别级泛化能力。

Abstract: Category-level generalization for robotic garment manipulation, such as
bimanual smoothing, remains a significant hurdle due to high dimensionality,
complex dynamics, and intra-category variations. Current approaches often
struggle, either overfitting with concurrently learned visual features for a
specific instance or, despite category-level perceptual generalization, failing
to predict the value of synergistic bimanual actions. We propose the
Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point
clouds to specifically enhance category-level policy generalization for garment
smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,
frozen dense geometric features, ensuring robustness to intra-category garment
variations. Trainable downstream components then learn a task-specific policy
using these static features. In simulated GarmentLab experiments with the
CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.
It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments
compared to 96.2% for a 2D image-based baseline, and achieved 89% final
coverage, outperforming an 83% coverage from a 3D correspondence-based baseline
that uses identical per-point geometric features but a fixed primitive. These
results highlight that the decoupling of geometric understanding from bimanual
action value learning enables better category-level generalization.

</details>


### [187] [A General Control Method for Human-Robot Integration](https://arxiv.org/abs/2412.14762)
*Maddalena Feder,Giorgio Grioli,Manuel G. Catalano,Antonio Bicchi*

Main category: cs.RO

TL;DR: 本文提出了一种新型通用控制方法，旨在帮助运动受限者控制多自由度辅助设备，通过将用户补偿运动转化为机器人指令，实现目标任务并减少用户努力和不适。


<details>
  <summary>Details</summary>
Motivation: 为运动受限者提供有效且舒适的控制策略，以操作假肢、多余肢体乃至远程机器人化身等复杂多自由度机器人辅助设备，解决低维用户运动到高维机器人控制的映射挑战。

Method: 开发了一个通用框架，将用户执行的补偿运动转化为必要的机器人指令，以达到目标并消除或减少补偿。该框架将人与机器人整合为一个独特系统。通过模拟场景测试和涉及机器人部件虚拟孪生体及物理人形化身的真实世界试验进行了验证和应用。

Result: 该框架能够控制通用多自由度辅助系统，将用户补偿运动转化为机器人命令以达到目标，同时取消或减少补偿。它可扩展应用于任何自由度的假肢，乃至全机器人化身，将其视为身体的人工延伸。

Conclusion: 所提出的通用控制策略通过将用户补偿运动转化为机器人指令，成功实现了多自由度辅助设备对运动受限者的有效控制，显著减少了用户的努力和不适，并将机器人视为身体的自然延伸，已通过仿真和实际测试验证。

Abstract: This paper introduces a new generalized control method designed for
multi-degrees-of-freedom devices to help people with limited motion
capabilities in their daily activities. The challenge lies in finding the most
adapted strategy for the control interface to effectively map user's motions in
a low-dimensional space to complex robotic assistive devices, such as
prostheses, supernumerary limbs, up to remote robotic avatars. The goal is a
system which integrates the human and the robotic parts into a unique system,
moving so as to reach the targets decided by the human while autonomously
reducing the user's effort and discomfort. We present a framework to control
general multi DoFs assistive systems, which translates user-performed
compensatory motions into the necessary robot commands for reaching targets
while canceling or reducing compensation. The framework extends to prostheses
of any number of DoF up to full robotic avatars, regarded here as a sort of
whole-body prosthesis of the person who sees the robot as an artificial
extension of their own body without a physical link but with a sensory-motor
integration. We have validated and applied this control strategy through tests
encompassing simulated scenarios and real-world trials involving a virtual twin
of the robotic parts (prosthesis and robot) and a physical humanoid avatar.

</details>


### [188] [Learning to See and Act: Task-Aware View Planning for Robotic Manipulation](https://arxiv.org/abs/2508.05186)
*Yongjie Bai,Zhouxia Wang,Yang Liu,Weixing Chen,Ziliang Chen,Mingtong Dai,Yongsen Zheng,Lingbo Liu,Guanbin Li,Liang Lin*

Main category: cs.RO

TL;DR: 该论文提出TAVP框架，通过整合主动视角规划和任务特定表征学习，克服了现有VLA模型在多任务机器人操作中因静态视角和共享视觉编码器导致的3D感知受限和任务干扰问题，显著提升了动作预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务机器人操作视觉-语言-动作（VLA）模型依赖于静态视角和共享视觉编码器，这限制了3D感知能力并导致任务间干扰，从而影响了模型的鲁棒性和泛化能力。

Method: 本文提出了任务感知视角规划（TAVP）框架。它通过高效的探索策略（由新型伪环境加速）主动获取信息丰富的视角。此外，引入了专家混合（MoE）视觉编码器，以解耦不同任务的特征，从而提高表征的保真度和任务泛化能力。

Result: TAVP通过任务感知的方式生成更完整、更具区分性的视觉表征，显著增强了在各种操作挑战中的动作预测能力。在RLBench任务上的大量实验表明，TAVP模型优于现有最先进的固定视角方法。

Conclusion: TAVP通过学习任务感知的视觉方式，有效地解决了静态视角和共享编码器在机器人VLA模型中的局限性，从而生成更完整和有区分度的视觉表征，显著提高了机器人操作的动作预测性能、鲁棒性和泛化能力。

Abstract: Recent vision-language-action (VLA) models for multi-task robotic
manipulation commonly rely on static viewpoints and shared visual encoders,
which limit 3D perception and cause task interference, hindering robustness and
generalization. In this work, we propose Task-Aware View Planning (TAVP), a
framework designed to overcome these challenges by integrating active view
planning with task-specific representation learning. TAVP employs an efficient
exploration policy, accelerated by a novel pseudo-environment, to actively
acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)
visual encoder to disentangle features across different tasks, boosting both
representation fidelity and task generalization. By learning to see the world
in a task-aware way, TAVP generates more complete and discriminative visual
representations, demonstrating significantly enhanced action prediction across
a wide array of manipulation challenges. Extensive experiments on RLBench tasks
show that our proposed TAVP model achieves superior performance over
state-of-the-art fixed-view approaches. Visual results and code are provided
at: https://hcplab-sysu.github.io/TAVP.

</details>


### [189] [A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry](https://arxiv.org/abs/2508.05368)
*Tong Hua,Jiale Han,Wei Ouyang*

Main category: cs.RO

TL;DR: 本文提出了一种多视角仅位姿估计方法，通过推导新的视觉测量模型，解决了传统IEKF在GNSS-视觉-惯性里程计（GVIO）中联合优化位姿和路标时计算量大的问题，提高了效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统的不变扩展卡尔曼滤波器（IEKF）在视觉辅助传感器融合中，联合优化相机位姿和路标时通常计算负担很高。

Method: 提出了一种多视角仅位姿估计方法，并应用于GNSS-视觉-惯性里程计（GVIO）。主要贡献是推导了一个视觉测量模型，该模型直接将路标表示与多个相机位姿和观测值关联起来。这种仅位姿测量被证明在路标和位姿之间是紧耦合的，并保持一个独立于估计位姿的完美零空间。最后，将该方法应用于基于滤波器的GVIO，并采用了一种新颖的特征管理策略。

Result: 所提出的仅位姿测量模型被证明是紧耦合的，并具有独立于估计位姿的完美零空间。仿真测试和真实世界实验均表明，该方法在效率和精度方面优于现有方法。

Conclusion: 本文提出的多视角仅位姿估计方法，通过创新的视觉测量模型，显著提高了GNSS-视觉-惯性里程计（GVIO）的计算效率和定位精度，解决了传统IEKF的计算负担问题。

Abstract: Invariant Extended Kalman Filter (IEKF) has been a significant technique in
vision-aided sensor fusion. However, it usually suffers from high computational
burden when jointly optimizing camera poses and the landmarks. To improve its
efficiency and applicability for multi-sensor fusion, we present a multi-view
pose-only estimation approach with its application to GNSS-Visual-Inertial
Odometry (GVIO) in this paper. Our main contribution is deriving a visual
measurement model which directly associates landmark representation with
multiple camera poses and observations. Such a pose-only measurement is proven
to be tightly-coupled between landmarks and poses, and maintain a perfect null
space that is independent of estimated poses. Finally, we apply the proposed
approach to a filter based GVIO with a novel feature management strategy. Both
simulation tests and real-world experiments are conducted to demonstrate the
superiority of the proposed method in terms of efficiency and accuracy.

</details>


### [190] [Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting](https://arxiv.org/abs/2508.05208)
*Victor Ngo,Rachel,Ramchurn,Roma Patel,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: 该论文评估了儿童在真实艺术装置中与自主机器人手臂表演者NED的互动体验，揭示了儿童与机器人交互在参与度、表达性及期望方面面临的挑战，并强调了在表演艺术中优化人机交互的重要性。


<details>
  <summary>Details</summary>
Motivation: 旨在了解儿童在真实艺术装置（Thingamabobas）中与自主机器人手臂表演者NED的“野外”互动体验，并识别儿童与机器人交互中存在的关键挑战。

Method: 通过对18名儿童在英国Thingamabobas装置中与机器人手臂NED的“野外”互动进行观察分析。研究详细描述了NED的设计，包括服装、行为和人机交互方式。

Result: 观察分析揭示了儿童与机器人交互的三个主要挑战：1) 启动和维持参与度，2) 机器人缺乏表现力和互惠性，3) 未满足的期望。研究发现儿童天生好奇且擅长与机器人艺术表演者互动，但人机交互系统仍需优化。

Conclusion: 在表演艺术背景下，为了为年轻观众提供引人入胜且有意义的体验，迫切需要通过仔细考虑观众的能力、感知和期望来优化人机交互（HRI）系统。

Abstract: This paper presents an evaluation of 18 children's in-the-wild experiences
with the autonomous robot arm performer NED (Never-Ending Dancer) within the
Thingamabobas installation, showcased across the UK. We detail NED's design,
including costume, behaviour, and human interactions, all integral to the
installation. Our observational analysis revealed three key challenges in
child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of
robot expressivity and reciprocity, and 3) Unmet expectations. Our findings
show that children are naturally curious, and adept at interacting with a
robotic art performer. However, our observations emphasise the critical need to
optimise human-robot interaction (HRI) systems through careful consideration of
audience's capabilities, perceptions, and expectations, within the performative
arts context, to enable engaging and meaningful experiences, especially for
young audiences.

</details>


### [191] [Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling](https://arxiv.org/abs/2508.05634)
*Jianpeng Yao,Xiaopan Zhang,Yu Xia,Zejin Wang,Amit K. Roy-Chowdhury,Jiachen Li*

Main category: cs.RO

TL;DR: 该研究提出一种方法，通过考虑行人预测不确定性来训练移动机器人，使其在人群中导航时对分布变化具有鲁棒性，显著提高了安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 使用强化学习训练的移动机器人在人群中导航时，在面对分布外（OOD）场景时性能会下降。

Method: 该方法通过自适应共形推断生成预测不确定性估计，并将其添加到机器人的观察中。这些不确定性估计通过约束强化学习来指导机器人的行为，从而调节其动作并适应分布变化。

Result: 在同分布设置下，成功率达到96.93%，比现有最优基线高出8.80%，碰撞次数减少3.72倍，侵入真实人类未来轨迹的次数减少2.43倍。在三种分布外场景中，该方法在面对速度变化、策略改变以及从个体到群体动态的转换等分布变化时，表现出更强的鲁棒性。在真实机器人上的部署实验也表明，机器人在与稀疏和密集人群互动时能做出安全且鲁棒的决策。

Conclusion: 通过适当考虑行人的不确定性，机器人可以学习到对分布变化具有鲁棒性的安全导航策略，显著提高了在复杂人群环境中的导航性能和安全性。

Abstract: Mobile robots navigating in crowds trained using reinforcement learning are
known to suffer performance degradation when faced with out-of-distribution
scenarios. We propose that by properly accounting for the uncertainties of
pedestrians, a robot can learn safe navigation policies that are robust to
distribution shifts. Our method augments agent observations with prediction
uncertainty estimates generated by adaptive conformal inference, and it uses
these estimates to guide the agent's behavior through constrained reinforcement
learning. The system helps regulate the agent's actions and enables it to adapt
to distribution shifts. In the in-distribution setting, our approach achieves a
96.93% success rate, which is over 8.80% higher than the previous
state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times
fewer intrusions into ground-truth human future trajectories. In three
out-of-distribution scenarios, our method shows much stronger robustness when
facing distribution shifts in velocity variations, policy changes, and
transitions from individual to group dynamics. We deploy our method on a real
robot, and experiments show that the robot makes safe and robust decisions when
interacting with both sparse and dense crowds. Our code and videos are
available on https://gen-safe-nav.github.io/.

</details>


### [192] [Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction](https://arxiv.org/abs/2508.05294)
*Sahar Salimpour,Lei Fu,Farhad Keramat,Leonardo Militano,Giovanni Toffetti,Harry Edelman,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: 该综述探讨了基础模型（如LLMs、VLMs、VLAs）如何推动机器人自主性和人机交互，尤其关注其在智能体架构中的应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型为机器人自主性和人机交互带来了新方法，同时视觉语言动作模型也在提升机器人灵巧性。本研究旨在梳理这些模型在机器人智能体应用和架构中的进展。

Method: 本文采用综述形式，涵盖同行评审研究、社区项目、ROS包和工业框架，以捕捉快速发展的领域趋势。研究方法包括提出模型集成方法的分类法，并对智能体在不同解决方案中的作用进行比较分析。

Result: 智能体架构使机器人能够理解自然语言指令、调用API、规划任务序列以及辅助操作和诊断。本文提供了一个模型集成方法的分类法，并对现有文献中智能体所扮演的角色进行了比较分析。

Conclusion: 基础模型正在加速机器人向智能体应用和架构发展，本综述系统地分类并分析了当前智能体在机器人领域中的集成方法和作用，揭示了新兴趋势。

Abstract: Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (BLMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those words advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.

</details>


### [193] [GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming](https://arxiv.org/abs/2508.05298)
*Jian Gong,Youwei Huang,Bo Yuan,Ming Zhu,Juncheng Zhan,Jinke Wang,Hang Shu,Mingyue Xiong,Yanjun Ye,Yufan Zu,Yang Zhou,Yihan Ding,Xuannian Chen,Xingyu Lu,Runjie Ban,Bingchao Huang,Fusen Liu*

Main category: cs.RO

TL;DR: GhostShell是一种利用LLM实现具身系统流式和并发行为编程的新方法，通过增量函数调用实时驱动机器人，显著提升了响应速度和行为正确性。


<details>
  <summary>Details</summary>
Motivation: 传统的具身系统依赖预设动作序列或行为树，缺乏灵活性。研究旨在利用LLM实现具身系统的实时、并发行为控制，以适应动态环境。

Method: GhostShell通过流式XML函数令牌解析器、动态函数接口映射器和多通道调度器，将LLM流式输出的令牌增量转换为函数调用。多通道调度器协调通道内同步和通道间异步函数调用，从而协调机器人多组件的串行-并行动作。

Result: 在机器人COCO上进行了34项真实世界交互任务的评估，使用Claude-4 Sonnet实现了0.85的SOTA行为正确性指标，响应时间比LLM原生函数调用API快66倍。在长周期多模态任务中也表现出强大的鲁棒性和泛化能力。

Conclusion: GhostShell为具身系统提供了一种新颖、高效的流式和并发行为编程范式，显著优于现有方法，提升了机器人实时控制的性能和适应性。

Abstract: We present GhostShell, a novel approach that leverages Large Language Models
(LLMs) to enable streaming and concurrent behavioral programming for embodied
systems. In contrast to conventional methods that rely on pre-scheduled action
sequences or behavior trees, GhostShell drives embodied systems to act
on-the-fly by issuing function calls incrementally as tokens are streamed from
the LLM. GhostShell features a streaming XML function token parser, a dynamic
function interface mapper, and a multi-channel scheduler that orchestrates
intra-channel synchronous and inter-channel asynchronous function calls,
thereby coordinating serial-parallel embodied actions across multiple robotic
components as directed by the LLM. We evaluate GhostShell on our robot
prototype COCO through comprehensive grounded experiments across 34 real-world
interaction tasks and multiple LLMs. The results demonstrate that our approach
achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4
Sonnet and up to 66X faster response times compared to LLM native function
calling APIs. GhostShell also proves effective in long-horizon multimodal
tasks, demonstrating strong robustness and generalization.

</details>


### [194] [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](https://arxiv.org/abs/2508.05342)
*Shunlei Li,Longsen Gao,Jin Wang,Chang Che,Xi Xiao,Jiuwen Cao,Yingbai Hu,Hamid Reza Karimi*

Main category: cs.RO

TL;DR: 本文提出了GF-VLA框架，使双臂机器人能直接从人类视频演示中学习并执行灵巧任务。该框架利用信息论场景图和语言模型生成分层行为树，并引入交叉手选择策略，实现了高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 从人类视频中教授机器人灵巧技能仍然面临挑战，因为传统的低级轨迹模仿难以泛化到不同物体类型、空间布局和机械臂配置。

Method: GF-VLA框架首先提取基于香农信息论的线索来识别最相关的双手和物体，然后将这些线索编码成包含手-物体和物体-物体交互的时间有序场景图。这些图与语言条件Transformer融合，生成分层行为树和可解释的笛卡尔运动指令。此外，为提高双臂执行效率，引入了无需显式几何推理的交叉手选择策略。

Result: 信息论场景表示实现了超过95%的图准确率和93%的子任务分割，支持LLM规划器生成可靠且易读的任务策略。双臂机器人执行这些策略时，在堆叠、字母构建和几何重构场景中，抓取成功率为94%，放置精度为89%，整体任务成功率为90%。

Conclusion: GF-VLA框架在多样化的空间和语义变化下，展示了强大的泛化能力和鲁棒性，有效解决了从人类视频中学习机器人灵巧技能的挑战。

Abstract: Teaching robots dexterous skills from human videos remains challenging due to
the reliance on low-level trajectory imitation, which fails to generalize
across object types, spatial layouts, and manipulator configurations. We
propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables
dual-arm robotic systems to perform task-level reasoning and execution directly
from RGB and Depth human demonstrations. GF-VLA first extracts
Shannon-information-based cues to identify hands and objects with the highest
task relevance, then encodes these cues into temporally ordered scene graphs
that capture both hand-object and object-object interactions. These graphs are
fused with a language-conditioned transformer that generates hierarchical
behavior trees and interpretable Cartesian motion commands. To improve
execution efficiency in bimanual settings, we further introduce a cross-hand
selection policy that infers optimal gripper assignment without explicit
geometric reasoning. We evaluate GF-VLA on four structured dual-arm block
assembly tasks involving symbolic shape construction and spatial
generalization. Experimental results show that the information-theoretic scene
representation achieves over 95 percent graph accuracy and 93 percent subtask
segmentation, supporting the LLM planner in generating reliable and
human-readable task policies. When executed by the dual-arm robot, these
policies yield 94 percent grasp success, 89 percent placement accuracy, and 90
percent overall task success across stacking, letter-building, and geometric
reconfiguration scenarios, demonstrating strong generalization and robustness
across diverse spatial and semantic variations.

</details>


### [195] [Affecta-Context: The Context-Guided Behavior Adaptation Framework](https://arxiv.org/abs/2508.05359)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 本文提出了一个名为Affecta-context的通用框架，旨在通过利用物理上下文信息，使社交机器人在人机交互中能自主学习和调整行为优先级。


<details>
  <summary>Details</summary>
Motivation: 社交机器人需要根据其所处的物理环境和用户的偏好来调整行为，以实现更自然和有效的交互。

Method: Affecta-context框架包含两部分：一是通过测量物理属性对遇到的物理上下文进行聚类表示；二是通过人机交互学习行为优先级。框架学习优化机器人行为的物理属性，使其符合当前环境和用户偏好。通过在两种不同物理上下文和6名人类参与者的72次交互中训练机器人，使其自主学习离散行为的优先级。

Result: 该研究成功地训练了Affecta-context框架，验证了机器人泛化输入并将其行为匹配到以前未访问的物理上下文的能力。

Conclusion: Affecta-context框架能有效帮助社交机器人根据物理上下文和用户偏好自主学习和适应行为，并展现出良好的泛化能力。

Abstract: This paper presents Affecta-context, a general framework to facilitate
behavior adaptation for social robots. The framework uses information about the
physical context to guide its behaviors in human-robot interactions. It
consists of two parts: one that represents encountered contexts and one that
learns to prioritize between behaviors through human-robot interactions. As
physical contexts are encountered the framework clusters them by their measured
physical properties. In each context, the framework learns to prioritize
between behaviors to optimize the physical attributes of the robot's behavior
in line with its current environment and the preferences of the users it
interacts with. This paper illlustrates the abilities of the Affecta-context
framework by enabling a robot to autonomously learn the prioritization of
discrete behaviors. This was achieved by training across 72 interactions in two
different physical contexts with 6 different human test participants. The paper
demonstrates the trained Affecta-context framework by verifying the robot's
ability to generalize over the input and to match its behaviors to a previously
unvisited physical context.

</details>


### [196] [Robots can defuse high-intensity conflict situations](https://arxiv.org/abs/2508.05373)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: 本研究探讨了机器人在高强度人机冲突中通过情感表达来化解冲突的有效性，发现所有测试的情感表达方式均有效，但运动方式有所不同，并强调了机器人情境感知和恰当反应的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解机器人在与人类发生高强度冲突时如何化解矛盾，特别是通过情感表达来减轻人们对表现不佳机器人的敌意。

Method: 使用定制的情感机器人，在模拟冲突情境中与105名参与者进行测试。研究了五种不同的情感表达方式作为化解冲突的主要驱动因素，通过让机器人承认冲突并表达悔意来化解情境。

Result: 所有测试的情感表达方式都能成功化解冲突并传达对对抗的承认。评估结果非常相似，但运动方式与其他方式存在显著差异（p<.05）。参与者对机器人受冲突影响的程度在所有表达方式下都有相似的情感解读。

Conclusion: 化解高强度人机交互可能不需要特别关注机器人的表达能力，而更需要关注其对情境的社会意识以及相应的反应能力。

Abstract: This paper investigates the specific scenario of high-intensity
confrontations between humans and robots, to understand how robots can defuse
the conflict. It focuses on the effectiveness of using five different affective
expression modalities as main drivers for defusing the conflict. The aim is to
discover any strengths or weaknesses in using each modality to mitigate the
hostility that people feel towards a poorly performing robot. The defusing of
the situation is accomplished by making the robot better at acknowledging the
conflict and by letting it express remorse. To facilitate the tests, we used a
custom affective robot in a simulated conflict situation with 105 test
participants. The results show that all tested expression modalities can
successfully be used to defuse the situation and convey an acknowledgment of
the confrontation. The ratings were remarkably similar, but the movement
modality was different (ANON p$<$.05) than the other modalities. The test
participants also had similar affective interpretations on how impacted the
robot was of the confrontation across all expression modalities. This indicates
that defusing a high-intensity interaction may not demand special attention to
the expression abilities of the robot, but rather require attention to the
abilities of being socially aware of the situation and reacting in accordance
with it.

</details>


### [197] [Real-Time Iteration Scheme for Diffusion Policy](https://arxiv.org/abs/2508.05396)
*Yufei Duan,Hang Yin,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出一种受实时迭代（RTI）方案启发的扩散策略加速方法，通过利用前一时间步的解作为初始猜测来显著减少推理时间，无需额外训练或策略重设计。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现出色，但其冗长的推理时间和需要顺序执行动作的特性限制了其在延迟敏感或短周期任务中的应用。现有加速方法通常需要额外的、资源密集型的训练。

Method: 引入一种受最优控制中实时迭代（RTI）方案启发的新方法，该方法通过利用前一时间步的解决方案作为后续迭代的初始猜测来加速优化。同时，提出一种基于缩放的方法来有效处理机器人操作中的离散动作（如抓取）。

Result: 该方案显著降低了运行时计算成本，无需蒸馏或策略重设计，可无缝集成到许多预训练的扩散模型中。实验结果显示，与使用全步去噪的扩散策略相比，推理时间大幅缩短，同时保持了可比的整体性能。论文还提供了收缩性的理论条件。

Conclusion: 所提出的受RTI启发的方案有效地加速了机器人操作中的扩散策略推理，使其适用于延迟敏感任务，且无需额外的训练或模型重新设计，同时保持了高性能。

Abstract: Diffusion Policies have demonstrated impressive performance in robotic
manipulation tasks. However, their long inference time, resulting from an
extensive iterative denoising process, and the need to execute an action chunk
before the next prediction to maintain consistent actions limit their
applicability to latency-critical tasks or simple tasks with a short cycle
time. While recent methods explored distillation or alternative policy
structures to accelerate inference, these often demand additional training,
which can be resource-intensive for large robotic models. In this paper, we
introduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a
method from optimal control that accelerates optimization by leveraging
solutions from previous time steps as initial guesses for subsequent
iterations. We explore the application of this scheme in diffusion inference
and propose a scaling-based method to effectively handle discrete actions, such
as grasping, in robotic manipulation. The proposed scheme significantly reduces
runtime computational costs without the need for distillation or policy
redesign. This enables a seamless integration into many pre-trained
diffusion-based models, in particular, to resource-demanding large models. We
also provide theoretical conditions for the contractivity which could be useful
for estimating the initial denoising step. Quantitative results from extensive
simulation experiments show a substantial reduction in inference time, with
comparable overall performance compared with Diffusion Policy using full-step
denoising. Our project page with additional resources is available at:
https://rti-dp.github.io/.

</details>


### [198] [DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model](https://arxiv.org/abs/2508.05402)
*Rui Yu,Xianghang Zhang,Runkai Zhao,Huaicheng Yan,Meng Wang*

Main category: cs.RO

TL;DR: DistillDrive是一个基于知识蒸馏的端到端自动驾驶模型，通过多样化实例模仿和引入规划模型作为教师，提升了多模态运动特征学习和决策鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型过度关注自我车辆状态，缺乏面向规划的理解，导致决策过程的鲁棒性受限。

Method: 引入DistillDrive模型，利用基于结构化场景表示的规划模型作为教师，将其多样化规划实例作为多目标学习目标。同时，结合强化学习优化状态到决策的映射，并利用生成模型构建面向规划的实例，促进潜在空间内的复杂交互。

Result: 在nuScenes和NAVSIM数据集上，与基线模型相比，碰撞率降低了50%，闭环性能提高了3点。

Conclusion: DistillDrive通过知识蒸馏和面向规划的实例学习，显著提升了端到端自动驾驶模型的鲁棒性和性能。

Abstract: End-to-end autonomous driving has been recently seen rapid development,
exerting a profound influence on both industry and academia. However, the
existing work places excessive focus on ego-vehicle status as their sole
learning objectives and lacks of planning-oriented understanding, which limits
the robustness of the overall decision-making prcocess. In this work, we
introduce DistillDrive, an end-to-end knowledge distillation-based autonomous
driving model that leverages diversified instance imitation to enhance
multi-mode motion feature learning. Specifically, we employ a planning model
based on structured scene representations as the teacher model, leveraging its
diversified planning instances as multi-objective learning targets for the
end-to-end model. Moreover, we incorporate reinforcement learning to enhance
the optimization of state-to-decision mappings, while utilizing generative
modeling to construct planning-oriented instances, fostering intricate
interactions within the latent space. We validate our model on the nuScenes and
NAVSIM datasets, achieving a 50\% reduction in collision rate and a 3-point
improvement in closed-loop performance compared to the baseline model. Code and
model are publicly available at https://github.com/YuruiAI/DistillDrive

</details>


### [199] [Computational Design and Fabrication of Modular Robots with Untethered Control](https://arxiv.org/abs/2508.05410)
*Manas Bhargava,Takefumi Hiraki,Malina Strugaru,Michal Piovarci,Chiara Daraio,Daisuke Iwai,Bernd Bickel*

Main category: cs.RO

TL;DR: 该研究提出了一种利用分布式驱动的机器人设计与控制框架，通过结合3D打印骨骼和液晶弹性体（LCE）肌肉实现模块化、无束缚的类生物机器人，并结合计算工具优化其形态和运动，以实现复杂的变形和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人难以模仿自然生物广泛的适应性和运动范围，尤其是软体机器人通常功能单一、缺乏按需变形能力或依赖笨重的控制系统，这促使研究者寻求一种更接近生物学原理的分布式驱动方法。

Method: 研究提出了一种新型构建模块，将3D打印骨骼与液晶弹性体（LCE）肌肉结合作为轻量级执行器，实现肌肉骨骼机器人的模块化组装。LCE杆通过红外辐射收缩，实现对骨骼分布式网络的局部、无束缚控制。此外，开发了两种计算工具：一个用于优化机器人骨骼图以实现多种目标变形，另一个用于共同优化骨骼设计和控制步态以实现目标运动。

Result: 通过构建多个机器人，验证了该系统能够实现复杂的形状变形、多样的控制方案以及对环境的适应性。该系统整合了模块化材料构建、无束缚分布式控制和计算设计方面的进展。

Conclusion: 该研究引入了新一代机器人，通过整合材料、控制和计算设计方面的进步，使机器人更接近活体生物的能力，有望解决现有机器人适应性和运动范围的挑战。

Abstract: Natural organisms use distributed actuation via their musculoskeletal systems
to adapt their gait for traversing diverse terrains or to morph their bodies to
perform varied tasks. A longstanding challenge in the field of robotics is to
mimic this extensive adaptability and range of motion. This has led humans to
develop various soft robotic systems that emulate natural organisms. However,
such systems are generally optimized for a single functionality, lack the
ability to change form or function on demand, or are often tethered to bulky
control systems. To address these challenges, we present our framework for
designing and controlling robots that mimic nature's blueprint by utilizing
distributed actuation. We propose a novel building block that combines
3D-printed bones with liquid crystal elastomer (LCE) muscles as lightweight
actuators and enables the modular assembly of musculoskeletal robots. We
developed LCE rods that contract in response to infrared radiation, thereby
achieving local and untethered control over the distributed network of bones,
which in turn results in global deformation of the robot. Furthermore, to
capitalize on the extensive design space, we develop two computational tools:
one to optimize the robot's skeletal graph, enabling multiple target
deformations, and another to co-optimize the skeletal designs and control gaits
to achieve target locomotion. We validate our system by building several robots
that show complex shape morphing, varying control schemes, and adaptability to
their environment. Our system integrates advances in modular material building,
untethered and distributed control, and computational design to introduce a new
generation of robots that brings us closer to the capabilities of living
organisms.

</details>


### [200] [Do Robots Really Need Anthropomorphic Hands?](https://arxiv.org/abs/2508.05415)
*Alexander Fabisch,Wadhah Zai El Amri,Chandandeep Singh,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 该综述探讨了机器人是否需要拟人手，发现手腕灵活性和手指开合能力对操控很重要，而增加手指数量或自由度常无必要。三指设计是简洁性和灵巧性的良好折衷，非拟人手甚至可能超越人类灵巧性。


<details>
  <summary>Details</summary>
Motivation: 人类操纵技能高度复杂，涉及多自由度协调和高维传感器输入。研究旨在探究人类手及其生物力学特性、传感器和控制机制是否是机器人学的理想目标，即机器人是否真的需要拟人手，并帮助从业者在手部复杂性和潜在操纵技能之间做出权衡。

Method: 该研究通过综述方式进行，概述了人手、比较了市售机器人手和假肢手，并系统回顾了各种手部机制及其所能实现的技能。在此基础上，提出并回答了关于机器人所需技能的最低机制和传感器要求、与人类灵巧性差距以及超越人类灵巧性的可能性等问题。

Result: 研究发现，手腕的灵活性和手指的内外展（开合）对操纵能力至关重要。相反，增加手指、执行器或自由度的数量通常没有必要。三指设计是简洁性和灵巧性的良好折衷。具有两对相对手指的非拟人手设计或具有六指的人手可以进一步提高灵巧性，这表明人手可能并非最佳设计。

Conclusion: 尽管复杂的多指手常被视为机器人操纵器的终极目标，但它们并非所有任务都必需。手腕灵活性和手指开合能力更为重要，而三指设计已能提供良好的灵巧性。非拟人手设计甚至可能超越人类的灵巧性，挑战了人手作为机器人设计最优模型的观念。

Abstract: Human manipulation skills represent a pinnacle of their voluntary motor
functions, requiring the coordination of many degrees of freedom and processing
of high-dimensional sensor input to achieve such a high level of dexterity.
Thus, we set out to answer whether the human hand, with its associated
biomechanical properties, sensors, and control mechanisms, is an ideal that we
should strive for in robotics-do we really need anthropomorphic robotic hands?
  This survey can help practitioners to make the trade-off between hand
complexity and potential manipulation skills. We provide an overview of the
human hand, a comparison of commercially available robotic and prosthetic
hands, and a systematic review of hand mechanisms and skills that they are
capable of. This leads to follow-up questions. What is the minimum requirement
for mechanisms and sensors to implement most skills that a robot needs? What is
missing to reach human-level dexterity? Can we improve upon human dexterity?
  Although complex five-fingered hands are often used as the ultimate goal for
robotic manipulators, they are not necessary for all tasks. We found that wrist
flexibility and finger abduction/adduction are important for manipulation
capabilities. On the contrary, increasing the number of fingers, actuators, or
degrees of freedom is often not necessary. Three fingers are a good compromise
between simplicity and dexterity. Non-anthropomorphic hand designs with two
opposing pairs of fingers or human hands with six fingers can further increase
dexterity, suggesting that the human hand may not be the optimum.

</details>


### [201] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: MICoBot是一个基于混合主动对话范式的人机协作系统，允许人机双方通过自然语言提出、接受或拒绝任务步骤的提议，旨在适应多样化的人类伙伴并优化协作策略，显著提升了任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 长周期人机协作系统需要适应广泛的人类伙伴，其物理行为、协助意愿和对机器人能力的理解可能随时间变化。这要求一个紧密耦合的通信循环，使双方都能灵活地提出、接受或拒绝请求，以有效协调完成任务。

Method: MICoBot系统应用混合主动对话范式，处理人机双方使用自然语言主动提出、接受或拒绝任务步骤分配的场景。它在三个层面做出决策：1) 元规划器根据人类对话制定高层协作策略；2) 规划器基于机器人能力（通过预训练的示能模型测量）和人类可用性估算，优化分配剩余任务步骤；3) 行动执行器决定低级动作或对人类的言语。

Result: 在模拟和真实世界（与18名独特人类参与者进行27小时的物理机器人实验）的广泛评估表明，MICoBot能够有效地与多样化的人类用户协作，与纯LLM基线和其他代理分配模型相比，显著提高了任务成功率和用户体验。

Conclusion: MICoBot系统通过其混合主动对话和三层决策机制，有效地解决了人机协作中适应多样化人类伙伴的挑战，显著提升了任务的成功率和用户体验，为长期人机协作提供了有效方案。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


### [202] [CleanUpBench: Embodied Sweeping and Grasping Benchmark](https://arxiv.org/abs/2508.05543)
*Wenbo Li,Guanting Chen,Tao Zhao,Jiyao Wang,Tianxin Hu,Yuwen Liao,Weixiang Guo,Shenghai Yuan*

Main category: cs.RO

TL;DR: 本文介绍了CleanUpBench，一个用于评估移动清洁机器人在现实室内场景中执行多目标清洁任务的可复现、可扩展基准。


<details>
  <summary>Details</summary>
Motivation: 现有的具身AI基准多针对复杂人形代理或大规模模拟，与实际部署相去甚远。尽管具有扫地和抓取双模式的移动清洁机器人正在迅速兴起，但目前缺乏系统评估这些代理在结构化、多目标清洁任务中表现的基准，这揭示了学术研究与实际应用之间的关键差距。

Method: CleanUpBench基于NVIDIA Isaac Sim构建，模拟了一个配备扫地机构和六自由度机械臂的移动服务机器人，能够与异构物体交互。该基准包含手动设计的环境和一个程序生成布局以评估泛化能力，以及一个涵盖任务完成度、空间效率、运动质量和控制性能的综合评估套件。为支持比较研究，提供了基于启发式策略和基于地图规划的基线代理。

Result: CleanUpBench提供了一个可扩展的测试平台，用于在日常环境中评估具身智能，它弥合了低级技能评估与全场景测试之间的鸿沟。

Conclusion: CleanUpBench作为一个可复现、可扩展的基准，为评估现实室内清洁场景中的具身智能代理提供了一个关键工具，有效填补了学术研究与实际应用之间的空白。

Abstract: Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,
but most target complex humanoid agents or large-scale simulations that are far
from real-world deployment. In contrast, mobile cleaning robots with dual mode
capabilities, such as sweeping and grasping, are rapidly emerging as realistic
and commercially viable platforms. However, no benchmark currently exists that
systematically evaluates these agents in structured, multi-target cleaning
tasks, revealing a critical gap between academic research and real-world
applications. We introduce CleanUpBench, a reproducible and extensible
benchmark for evaluating embodied agents in realistic indoor cleaning
scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service
robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic
arm, enabling interaction with heterogeneous objects. The benchmark includes
manually designed environments and one procedurally generated layout to assess
generalization, along with a comprehensive evaluation suite covering task
completion, spatial efficiency, motion quality, and control performance. To
support comparative studies, we provide baseline agents based on heuristic
strategies and map-based planning. CleanUpBench bridges the gap between
low-level skill evaluation and full-scene testing, offering a scalable testbed
for grounded, embodied intelligence in everyday settings.

</details>


### [203] [Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator](https://arxiv.org/abs/2508.05584)
*Van Cuong Pham,Minh Hai Tran,Phuc Anh Nguyen,Ngoc Son Vu,Nga Nguyen Thi*

Main category: cs.RO

TL;DR: 本研究提出一种鲁棒自适应模糊滑模控制（AFSMC）方法，以提升圆柱形机械臂的轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 圆柱形机械臂广泛应用于CNC和3D打印等领域，需要更高的轨迹跟踪精度和鲁棒性。

Method: 该方法将模糊逻辑（用于近似系统不确定动力学）与滑模控制（SMC）（用于确保强大性能）相结合，形成自适应模糊滑模控制（AFSMC）策略。

Result: MATLAB/Simulink仿真结果表明，AFSMC在轨迹跟踪精度、稳定性和扰动抑制方面显著优于传统控制方法。

Conclusion: 研究结果强调了AFSMC在控制机械臂方面的有效性，有助于提高工业机器人应用的精度。

Abstract: This research proposes a robust adaptive fuzzy sliding mode control (AFSMC)
approach to enhance the trajectory tracking performance of cylindrical robotic
manipulators, extensively utilized in applications such as CNC and 3D printing.
The proposed approach integrates fuzzy logic with sliding mode control (SMC) to
bolster adaptability and robustness, with fuzzy logic approximating the
uncertain dynamics of the system, while SMC ensures strong performance.
Simulation results in MATLAB/Simulink demonstrate that AFSMC significantly
improves trajectory tracking accuracy, stability, and disturbance rejection
compared to traditional methods. This research underscores the effectiveness of
AFSMC in controlling robotic manipulators, contributing to enhanced precision
in industrial robotic applications.

</details>


### [204] [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/abs/2508.05635)
*Yue Liao,Pengfei Zhou,Siyuan Huang,Donglin Yang,Shengcong Chen,Yuxin Jiang,Yue Hu,Jingbin Cai,Si Liu,Jianlan Luo,Liliang Chen,Shuicheng Yan,Maoqing Yao,Guanghui Ren*

Main category: cs.RO

TL;DR: Genie Envisioner (GE) 是一个统一的机器人操作世界基础平台，它将策略学习、评估和模拟集成到单一的视频生成框架中，实现了指令驱动的通用具身智能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作领域缺乏一个统一的平台，能够整合策略学习、评估和模拟，并实现对不同机器人体的通用化策略推理。

Method: GE平台包含三个核心组件：GE-Base是一个指令条件视频扩散模型，用于捕捉机器人交互的动态；GE-Act通过轻量级流匹配解码器将潜在表示映射到可执行动作轨迹；GE-Sim是一个动作条件神经模拟器，用于生成高保真推演。此外，还引入了EWMBench作为标准化基准套件进行评估。

Result: Genie Envisioner作为一个可扩展且实用的基础平台，为指令驱动的通用具身智能提供了支持。所有代码、模型和基准都将公开发布。

Conclusion: Genie Envisioner成功构建了一个统一、可扩展且实用的机器人操作基础平台，通过视频生成框架集成了策略学习、评估和模拟，为实现指令驱动的通用具身智能奠定了基础。

Abstract: We introduce Genie Envisioner (GE), a unified world foundation platform for
robotic manipulation that integrates policy learning, evaluation, and
simulation within a single video-generative framework. At its core, GE-Base is
a large-scale, instruction-conditioned video diffusion model that captures the
spatial, temporal, and semantic dynamics of real-world robotic interactions in
a structured latent space. Built upon this foundation, GE-Act maps latent
representations to executable action trajectories through a lightweight,
flow-matching decoder, enabling precise and generalizable policy inference
across diverse embodiments with minimal supervision. To support scalable
evaluation and training, GE-Sim serves as an action-conditioned neural
simulator, producing high-fidelity rollouts for closed-loop policy development.
The platform is further equipped with EWMBench, a standardized benchmark suite
measuring visual fidelity, physical consistency, and instruction-action
alignment. Together, these components establish Genie Envisioner as a scalable
and practical foundation for instruction-driven, general-purpose embodied
intelligence. All code, models, and benchmarks will be released publicly.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [205] [Linear Program-Based Stability Conditions for Nonlinear Autonomous Systems](https://arxiv.org/abs/2508.04871)
*Sadredin Hokmi,Mohammad Khajenejad*

Main category: eess.SY

TL;DR: 本文提出一种利用线性规划（LP）条件评估连续和离散非线性自治系统渐近稳定性的新方法，取代了传统的半定规划（SDP），显著降低了高维系统的计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有基于半定规划（SDP）的渐近稳定性评估方法在高维系统上计算负担（时间、内存）过重，因此需要开发更高效的替代方案。

Method: 该方法通过间接Lyapunov方法和雅可比矩阵对系统动力学进行线性化，并用计算效率更高的线性规划（LP）条件取代传统的半定规划（SDP）。稳定性判据通过矩阵变换和利用系统结构特性来开发，以提高可扩展性。

Result: 所提出的方法显著降低了计算负担，包括时间和内存使用，尤其是在高维系统上。多个示例证明了该方法与现有基于SDP的判据相比，在高维系统上的计算效率更高。

Conclusion: 该研究提供了一种计算效率更高的新方法，用于评估连续和离散非线性自治系统的渐近稳定性，特别适用于高维系统，有效解决了传统SDP方法的计算瓶颈。

Abstract: This paper introduces a novel approach to evaluating the asymptotic stability
of equilibrium points in both continuous-time (CT) and discrete-time (DT)
nonlinear autonomous systems. By utilizing indirect Lyapunov methods and
linearizing system dynamics through Jacobian matrices, the methodology replaces
traditional semi-definite programming (SDP) techniques with computationally
efficient linear programming (LP) conditions. This substitution substantially
lowers the computational burden, including time and memory usage, particularly
for high-dimensional systems. The stability criteria are developed using matrix
transformations and leveraging the structural characteristics of the system,
improving scalability. Several examples demonstrated the computational
efficiency of the proposed approach compared to the existing SDP-based
criteria, particularly for high-dimensional systems.

</details>


### [206] [Sequence Aware SAC Control for Engine Fuel Consumption Optimization in Electrified Powertrain](https://arxiv.org/abs/2508.04874)
*Wafeeq Jaleel,Md Ragib Rownak,Athar Hanif,Sidra Ghayour Bhatti,Qadeer Ahmed*

Main category: eess.SY

TL;DR: 该研究提出了一种基于SAC算法的强化学习框架，通过整合GRU和Decision Transformer来优化串联式混合动力重型卡车的发动机控制，以实现燃油效率和电池电量保持，并在各种工况下展现出优异的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着混合动力电动汽车在重型卡车中的应用日益广泛，自适应和高效的能量管理对于降低燃油消耗、同时维持电池电量以支持长时间运行至关重要。

Method: 将发动机控制任务重新定义为序列决策问题，并基于Soft Actor-Critic (SAC) 算法构建强化学习框架。通过在Actor和Critic网络中分别融入门控循环单元（GRU）和决策Transformer（DT），以捕获时间依赖性并改进长期规划。模型在多样化的初始电池状态、驾驶循环持续时间、功率需求和输入序列长度下进行训练，以评估其鲁棒性和泛化能力。

Result: 实验结果显示，在HFET循环上，采用DT-actor和GRU-critic的SAC智能体在燃油经济性方面与动态规划（DP）的差距在1.8%以内；而采用GRU-actor和GRU-critic的SAC智能体以及FFN-actor-critic智能体的差距分别为3.16%和3.43%。在未曾见过的驾驶循环（US06和HHDDT巡航段）中，泛化的序列感知智能体始终优于基于前馈网络（FFN）的智能体。

Conclusion: 所提出的序列感知智能体（特别是结合了DT和GRU的SAC代理）在混合动力重型卡车的能量管理中表现出卓越的适应性和鲁棒性，在真实世界场景中具有显著优势，能够有效降低燃油消耗并维持电池电量。

Abstract: As hybrid electric vehicles (HEVs) gain traction in heavy-duty trucks,
adaptive and efficient energy management is critical for reducing fuel
consumption while maintaining battery charge for long operation times. We
present a new reinforcement learning (RL) framework based on the Soft
Actor-Critic (SAC) algorithm to optimize engine control in series HEVs. We
reformulate the control task as a sequential decision-making problem and
enhance SAC by incorporating Gated Recurrent Units (GRUs) and Decision
Transformers (DTs) into both actor and critic networks to capture temporal
dependencies and improve planning over time. To evaluate robustness and
generalization, we train the models under diverse initial battery states, drive
cycle durations, power demands, and input sequence lengths. Experiments show
that the SAC agent with a DT-based actor and GRU-based critic was within 1.8%
of Dynamic Programming (DP) in fuel savings on the Highway Fuel Economy Test
(HFET) cycle, while the SAC agent with GRUs in both actor and critic networks,
and FFN actor-critic agent were within 3.16% and 3.43%, respectively. On unseen
drive cycles (US06 and Heavy Heavy-Duty Diesel Truck (HHDDT) cruise segment),
generalized sequence-aware agents consistently outperformed feedforward network
(FFN)-based agents, highlighting their adaptability and robustness in
real-world settings.

</details>


### [207] [Uncovering the Influence Flow Model of Transistor Amplifiers, Its Reconstruction and Application](https://arxiv.org/abs/2508.04977)
*Mohammed Tuhin Rana,Mishfad Shaikh Veedu,Murti V. Salapaka*

Main category: eess.SY

TL;DR: 本文提出了一种将多级晶体管放大器建模为线性动态影响模型（LDIM）的方法，并结合数据驱动的网络重构技术，仅通过电压时间序列数据即可识别电路参数、诊断故障，并在仿真和硬件中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多级晶体管放大器可以被有效地建模为动态系统网络，其中各个放大器级通过动态耦合相互作用。对这类复杂系统进行高效的特性分析、故障识别和关键参数确定是设计和调试放大器电路的关键挑战。

Method: 研究人员使用电路分析技术将多级晶体管放大器建模为线性动态影响模型（LDIM），其中级间相互作用被建模为线性动态方程。在此基础上，他们结合图形建模技术和维纳滤波，从电路中特定点的电压时间序列测量数据中重构网络结构。

Result: 该研究成功地证明了网络重构方法在多级放大器中的有效性，通过Cadence中的大量仿真和物理硬件上的实验结果均得到了验证。此外，还展示了利用这些技术进行故障诊断的方法。

Conclusion: 从测量数据直接推断网络结构的能力为设计人员和用户提供了设计、分析和调试放大器电路的有效工具，特别是对于故障诊断具有实用价值。

Abstract: Multistage transistor amplifiers can be effectively modeled as network of
dynamic systems where individual amplifier stages interact through couplings
that are dynamic in nature. Using circuit analysis techniques, we show that a
large class of transistor amplifiers can be modeled as Linear Dynamic Influence
Model (LDIM), where the interactions between different amplifier stages are
modeled as linear dynamic equations. LDIM modeling of transistor circuits leads
to application of data-driven network reconstruction techniques to characterize
stage interactions and identify faults and critical circuit parameters
efficiently. Employing graphical modeling techniques and Wiener filtering, we
demonstrate that the network structure can be reconstructed solely from voltage
time-series measurements sampled at specified points in the circuit. The
efficacy of these network reconstruction methods in multistage amplifiers is
demonstrated through extensive simulations involving multiple amplifier
circuits in Cadence, as well as experimental results on physical hardware. The
ability to infer network structure directly from measurement data offers
designers and users efficient tools to design, analyze, and debug amplifier
circuits. To demonstrate the utility of network reconstruction in multistage
amplifier circuits, a fault diagnosis method leveraging these techniques is
presented.

</details>


### [208] [Probabilistic Alternating Simulations for Policy Synthesis in Uncertain Stochastic Dynamical Systems](https://arxiv.org/abs/2508.05062)
*Thom Badings,Alessandro Abate*

Main category: eess.SY

TL;DR: 本文提出了一种扩展的概率模拟关系，用于同时存在随机和非确定性干扰的系统中的形式化策略合成。该关系结合了对随机不确定性的概率推理和对非确定性干扰的鲁棒性（对抗性）推理，并在Dubins飞行器上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统的随机动态系统形式化策略合成方法依赖于有限状态抽象（如MDP）和概率模拟关系来确保正确性。然而，当系统动态除了随机性外，还受非确定性（即集合值）干扰影响时，现有的概率模拟关系不足以保证正确性。

Method: 作者将概率模拟关系扩展到同时具有随机和非确定性干扰的系统。该关系受交替模拟概念启发，并推广了现有的一些用于验证和策略合成的关系。直观上，它允许对随机不确定性进行概率推理，同时对非确定性干扰进行鲁棒（对抗性）推理。

Result: 本文成功构建了一种新的模拟关系，能够处理同时存在随机和非确定性干扰的系统。作者通过实验证明了所提出的关系在4D状态Dubins飞行器上的策略合成中的适用性。

Conclusion: 该研究提供了一种处理同时具有随机和非确定性干扰的动态系统形式化策略合成的有效方法，通过扩展概率模拟关系，实现了对混合不确定性的严谨推理，从而拓宽了形式化策略合成的应用范围。

Abstract: A classical approach to formal policy synthesis in stochastic dynamical
systems is to construct a finite-state abstraction, often represented as a
Markov decision process (MDP). The correctness of these approaches hinges on a
behavioural relation between the dynamical system and its abstraction, such as
a probabilistic simulation relation. However, probabilistic simulation
relations do not suffice when the system dynamics are, next to being
stochastic, also subject to nondeterministic (i.e., set-valued) disturbances.
In this work, we extend probabilistic simulation relations to systems with both
stochastic and nondeterministic disturbances. Our relation, which is inspired
by a notion of alternating simulation, generalises existing relations used for
verification and policy synthesis used in several works. Intuitively, our
relation allows reasoning probabilistically over stochastic uncertainty, while
reasoning robustly (i.e., adversarially) over nondeterministic disturbances. We
experimentally demonstrate the applicability of our relations for policy
synthesis in a 4D-state Dubins vehicle.

</details>


### [209] [Preparing for the worst: Long-term and short-term weather extremes in resource adequacy assessment](https://arxiv.org/abs/2508.05163)
*Aleksander Grochowicz,Hannah C. Bloomfield,Marta Victoria*

Main category: eess.SY

TL;DR: 该研究通过影子价格识别并分析了欧洲净零电力系统中，极端天气导致的系统压力事件（系统定义事件），揭示了韧性备用容量的必要性及其财务困境，并提出了区分短期和长期韧性挑战的方法。


<details>
  <summary>Details</summary>
Motivation: 在净零电力系统中整合可再生能源时，供应安全是一个普遍且重要的考量。极端天气影响供需，导致电力系统压力，这种压力在欧洲大陆范围内传播，超出了气象根源。

Method: 研究采用基于影子价格的方法来识别高压力时期，称之为“系统定义事件”，并对其对电力系统的影响进行分析。通过对不同类型系统定义事件进行分类，识别了电力系统运行和规划面临的挑战。此外，研究使用不同的指标和压力测试来区分短期和长期韧性挑战。

Result: 研究发现，电力系统需要足够的韧性备用容量，但由于天气多变性，其财务可行性不稳定。同时，研究成功区分了短期和长期韧性挑战，并提出了相应的评估方法。

Conclusion: 该研究的方法和在PyPSA-Eur开放模型中的实现可以应用于其他系统，有助于研究人员和政策制定者构建更具韧性和充足的能源系统。

Abstract: Security of supply is a common and important concern when integrating
renewables in net-zero power systems. Extreme weather affects both demand and
supply leading to power system stress; in Europe this stress spreads
continentally beyond the meteorological root cause. We use an approach based on
shadow prices to identify periods of elevated stress called system-defining
events and analyse their impact on the power system. By classifying different
types of system-defining events, we identify challenges to power system
operation and planning. Crucially, we find the need for sufficient resilience
back-up (power) capacities whose financial viability is precarious due to
weather variability. Furthermore, we disentangle short- and long-term
resilience challenges with distinct metrics and stress tests to incorporate
both into future energy modelling assessments. Our methodology and
implementation in the open model PyPSA-Eur can be re-applied to other systems
and help researchers and policymakers in building more resilient and adequate
energy systems.

</details>


### [210] [Overview of Controllability Definitions in Supervisory Control Theory](https://arxiv.org/abs/2508.05177)
*Jeroen J. A. Keiren,Michel A. Reniers*

Main category: eess.SY

TL;DR: 本文比较了监督控制理论中“可控性”的各种定义，研究了它们在确定性与非确定性自动机环境下的关系，并指出了哪些概念是等价的或能推出传统语言可控性。


<details>
  <summary>Details</summary>
Motivation: 监督控制理论文献中对同一概念（尤其是“监督器相对于受控系统的可控性”）存在多种定义，导致理解这些定义之间关系困难。

Method: 列举文献中已有的可控性定义，并研究它们在确定性与非确定性自动机设置下的相互关系。

Result: 在监督器和受控系统均为非确定性的一般情境下，Flordal和Malik的可控性定义与Kushi和Takai的不可控事件可接纳性定义等价，且它们是唯一能推出传统（语言）可控性的概念。在“受控系统相对于受控系统的可控性”情境下，除前两者外，Zhou等人的状态可控性也意味着语言可控性。

Conclusion: 论文明确了监督控制理论中不同可控性定义之间的关系，尤其是在非确定性环境下，并指出了哪些定义是等价的以及哪些能推出传统的语言可控性。

Abstract: In the field of supervisory control theory, the literature often proposes
different definitions for the same concept, making it difficult to understand
how these definitions are related. This is definitely so for the fundamental
notion of controllability of a supervisor w.r.t. a plant. This paper lists
definitions of controllability found in the literature and studies their
relationships in settings of both deterministic and nondeterministic automata.
In the general context, where both the supervisor and the plant are allowed to
be nondeterministic, the notions of controllability as described by Flordal and
Malik, and uncontrollable event admissibility by Kushi and Takai are
equivalent. These are also the only notions that imply the traditional notion
of (language) controllability. From a practical perspective, one is often more
interested in controllability of a supervised plant w.r.t. a plant. In this
context, in addition to the previous two controllability notions, state
controllability by Zhou et al. implies language controllability.

</details>


### [211] [Passive nonlinear FIR filters for data-driven control](https://arxiv.org/abs/2508.05279)
*Zixing Wang,Fulvio Forni*

Main category: eess.SY

TL;DR: 本文提出了一种新型的无源非线性有限脉冲响应（FIR）算子，通过在提升空间中应用FIR滤波器构建，并结合约束优化、最小二乘拟合和频域采样确保无源性，特别适用于物理系统控制。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够有效进行控制合成，并能保证无源性以适用于物理系统（如机电系统）的新型操作器。

Method: 通过在提升空间中应用有限脉冲响应滤波器来构建新型无源非线性FIR算子。控制合成采用约束优化，闭环性能通过基于虚拟参考反馈调谐理论的最小二乘拟合实现。无源性通过基于频域采样的有效线性约束来确立。

Result: 提出了一类新的无源非线性有限脉冲响应算子，由于其无源性，特别适用于物理系统（如机电系统）的控制。

Conclusion: 所提出的无源非线性FIR算子由于其固有的无源性，非常适合用于物理系统的控制，并且其设计方法高效可行。

Abstract: We propose a new class of passive nonlinear finite impulse response
operators. This class is constructed by the action of finite impulse response
filters in a lifted space. This allows for efficient control synthesis through
constrained optimization. Closed-loop performance is taken into account through
least-squares fitting, based on the theory of virtual reference feedback
tuning. Passivity is established through efficient linear constraints, based on
sampling in the frequency domain. Because of passivity, this class of operators
is particularly suited for the control of physical systems, such as
electromechanical systems.

</details>


### [212] [A 20-Year Retrospective on Power and Thermal Modeling and Management](https://arxiv.org/abs/2508.05495)
*David Atienza,Kai Zhu,Darong Huang,Luis Costero*

Main category: eess.SY

TL;DR: 该综述回顾了二十多年来现代处理器中功耗与热建模及管理的研究进展。


<details>
  <summary>Details</summary>
Motivation: 随着处理器性能的提升，日益增长的功耗密度和复杂的散热行为对能效和系统可靠性构成了威胁。

Method: 文章比较了功耗估计的分析、基于回归和神经网络技术，回顾了热建模的有限元、有限差分和数据驱动方法，并分类了平衡性能、功耗和可靠性的动态运行时管理策略。

Result: 该综述全面覆盖了功耗和热建模以及管理领域的研究，包括各种估计、建模和管理方法。

Conclusion: 文章总结了当前面临的挑战并指出了未来有前景的研究方向。

Abstract: As processor performance advances, increasing power densities and complex
thermal behaviors threaten both energy efficiency and system reliability. This
survey covers more than two decades of research on power and thermal modeling
and management in modern processors. We start by comparing analytical,
regression-based, and neural network-based techniques for power estimation,
then review thermal modeling methods, including finite element, finite
difference, and data-driven approaches. Next, we categorize dynamic runtime
management strategies that balance performance, power consumption, and
reliability. Finally, we conclude with a discussion of emerging challenges and
promising research directions.

</details>


### [213] [Research on integrated intelligent energy management system based on big data analysis and machine learning](https://arxiv.org/abs/2508.05583)
*Jinzhou Xu,Yadan Zhang,Paola Tapia*

Main category: eess.SY

TL;DR: 本文探讨了将大数据和机器学习应用于综合智慧能源项目文档管理，以提高项目管理效率和控制水平。


<details>
  <summary>Details</summary>
Motivation: 将大数据应用于综合智慧能源项目的文档管理，对于提高项目管理和控制效率具有重要意义。

Method: 文章首先讨论了在大数据分析应用于文档管理中的益处和挑战，然后开发了一个大数据分析的实施框架，并提出了一种通过机器学习优化文档管理效率的方法。具体使用了三种不同的机器学习方法，并通过拟合惩罚线性回归模型进行效率优化。

Result: 当有足够的训练数据时，惩罚线性回归模型的准确率可以达到95%以上。通过大数据分析和机器学习，可以追踪整个项目文档过程并优化业务流程。

Conclusion: 利用大数据分析和机器学习可以有效分析和优化综合智慧能源项目文档管理效率，从而加强项目建设控制并提高项目建设效率。

Abstract: The application of big data is one of the significant features of integrated
smart energy. Applying it to the file management of integrated smart energy
projects is of great significance for improving the efficiency of project
management and control. This article first discussed the benefits and
challenges of implementing big data analysis in document management and control
of integrated smart energy projects. In addition, an implementation framework
for big data analysis in integrated smart energy project document management
was developed, and a method for optimizing the efficiency of integrated smart
energy project document management through machine learning was proposed. Using
various types of data and information generated during the project document
management process, the efficiency of the entire process project document
control through three different machine learning methods was optimized. The
result of fitting a penalty linear regression model shows that when there is
enough data as a training set, the accuracy of the model achieved can reach
over 95\%. By using big data analysis and machine learning to analyze the
efficiency of comprehensive smart energy project document management, it is
possible to track the entire process of comprehensive smart energy project
documents and optimize business processes, thereby strengthening project
construction control and improving project construction efficiency.

</details>


### [214] [Error Bounds for Radial Network Topology Learning from Quantized Measurements](https://arxiv.org/abs/2508.05620)
*Samuel Talkington,Aditya Rangarajan,Pedro A. de Alcântara,Line Roald,Daniel K. Molzahn,Daniel R. Fuhrmann*

Main category: eess.SY

TL;DR: 本文概率性地界定径向网络拓扑学习的误差，该学习同时估计连接性和线路参数，并考虑传感器量化误差。结果显示误差与量化区间宽度成正比，并随节点数量亚线性增长。


<details>
  <summary>Details</summary>
Motivation: 传统的电力系统估计算法通常采用加性噪声模型，但实际中传感器精度（量化）引入非线性误差。本文旨在建立一个更符合实际的测量模型，将传感器通信网络操作嵌入学习问题中，超越传统的加性噪声模型。

Method: 构建了一个径向网络拓扑学习的概率模型，该模型将传感器精度（量化）引入的数据误差纳入考虑，从而产生了一个非线性测量模型。该方法旨在概率性地界定学习解决方案的误差。

Result: 研究表明，学习到的径向网络拓扑的误差与量化区间宽度成正比。在每个节点的样本数量与节点数量的对数成比例的条件下，误差随节点数量亚线性增长。

Conclusion: 本文成功地量化了在传感器量化误差存在下径向网络拓扑学习的误差界限，明确了误差与量化区间宽度、节点数量和样本数量之间的具体关系。

Abstract: We probabilistically bound the error of a solution to a radial network
topology learning problem where both connectivity and line parameters are
estimated. In our model, data errors are introduced by the precision of the
sensors, i.e., quantization. This produces a nonlinear measurement model that
embeds the operation of the sensor communication network into the learning
problem, expanding beyond the additive noise models typically seen in power
system estimation algorithms. We show that the error of a learned radial
network topology is proportional to the quantization bin width and grows
sublinearly in the number of nodes, provided that the number of samples per
node is logarithmic in the number of nodes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [215] [Neural Field-Based 3D Surface Reconstruction of Microstructures from Multi-Detector Signals in Scanning Electron Microscopy](https://arxiv.org/abs/2508.04728)
*Shuo Chen,Yijin Li,Xi Zheng,Guofeng Zhang*

Main category: eess.IV

TL;DR: NFH-SEM是一种基于神经场的新型混合扫描电镜（SEM）三维重建方法，能够从多视角、多探测器二维图像中高精度重建复杂微结构，并实现自校准和阴影解耦。


<details>
  <summary>Details</summary>
Motivation: 传统的二维SEM图像无法直接显示微观样品的三维形貌。现有SEM三维重建方法面临离散三维表示的局限性、需要参考样品校准以及阴影引起的梯度误差等挑战，难以重建复杂微结构。

Method: 本文提出NFH-SEM，一种基于神经场的混合SEM三维重建方法。它以多视角、多探测器二维SEM图像为输入，将几何和光度信息融合到连续的神经场表示中。该方法通过端到端自校准消除了手动校准程序，并在训练过程中自动将阴影从SEM图像中分离。

Result: NFH-SEM在真实和模拟数据集上进行了验证，实验结果表明其能够高保真地重建各种具有挑战性的样品，包括双光子光刻微结构、桃子花粉和碳化硅颗粒表面，展示了精确的细节和广泛的适用性。

Conclusion: NFH-SEM能够准确重建复杂的微结构，具有精确的细节和广泛的适用性，解决了现有SEM三维重建方法的局限性。

Abstract: The scanning electron microscope (SEM) is a widely used imaging device in
scientific research and industrial applications. Conventional two-dimensional
(2D) SEM images do not directly reveal the three-dimensional (3D) topography of
micro samples, motivating the development of SEM 3D surface reconstruction
methods. However, reconstruction of complex microstructures remains challenging
for existing methods due to the limitations of discrete 3D representations, the
need for calibration with reference samples, and shadow-induced gradient
errors. Here, we introduce NFH-SEM, a neural field-based hybrid SEM 3D
reconstruction method that takes multi-view, multi-detector 2D SEM images as
input and fuses geometric and photometric information into a continuous neural
field representation. NFH-SEM eliminates the manual calibration procedures
through end-to-end self-calibration and automatically disentangles shadows from
SEM images during training, enabling accurate reconstruction of intricate
microstructures. We validate the effectiveness of NFH-SEM on real and simulated
datasets. Our experiments show high-fidelity reconstructions of diverse,
challenging samples, including two-photon lithography microstructures, peach
pollen, and silicon carbide particle surfaces, demonstrating precise detail and
broad applicability.

</details>


### [216] [Super-Resolution of Sentinel-2 Images Using a Geometry-Guided Back-Projection Network with Self-Attention](https://arxiv.org/abs/2508.04729)
*Ivan Pereira-Sánchez,Daniel Torres,Francesc Alcover,Bartomeu Garau,Julia Navarro,Catalina Sbert,Joan Duran*

Main category: eess.IV

TL;DR: 本文提出了一种几何引导的超分辨率模型，用于融合Sentinel-2卫星的10米和20米多光谱波段，以结合高空间细节和丰富的光谱信息，并在多个地貌类型上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2任务提供不同分辨率的波段（10米波段提供精细结构细节，20米波段捕获更丰富的光谱信息）。研究动机在于融合这些不同分辨率的波段，以结合它们各自的优势，实现更高质量的图像。

Method: 该方法引入了一个几何引导的超分辨率模型。具体包括：1) 基于聚类的学习过程，从10米波段生成几何丰富的引导图像；2) 将引导图像集成到展开式反向投影架构中；3) 利用多头注意力机制建模非局部块间交互，以利用图像自相似性；4) 构建了包含城市、乡村和沿海景观的评估数据集。

Result: 实验结果表明，所提出的方法在超分辨率和融合技术方面均优于传统的和基于深度学习的方法。

Conclusion: 该研究成功开发了一种有效的几何引导超分辨率模型，能够有效融合Sentinel-2的10米和20米波段，提升图像质量，并在不同地貌类型的测试中表现出卓越性能。

Abstract: The Sentinel-2 mission provides multispectral imagery with 13 bands at
resolutions of 10m, 20m, and 60m. In particular, the 10m bands offer fine
structural detail, while the 20m bands capture richer spectral information. In
this paper, we propose a geometry-guided super-resolution model for fusing the
10m and 20m bands. Our approach introduces a cluster-based learning procedure
to generate a geometry-rich guiding image from the 10m bands. This image is
integrated into an unfolded back-projection architecture that leverages image
self-similarities through a multi-head attention mechanism, which models
nonlocal patch-based interactions across spatial and spectral dimensions. We
also generate a dataset for evaluation, comprising three testing sets that
include urban, rural, and coastal landscapes. Experimental results demonstrate
that our method outperforms both classical and deep learning-based
super-resolution and fusion techniques.

</details>


### [217] [Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization](https://arxiv.org/abs/2508.04790)
*MD Shaikh Rahman,Feiroz Humayara,Syed Maudud E Rabbi,Muhammad Mahbubur Rashid*

Main category: eess.IV

TL;DR: 该研究开发了一个综合评估框架，系统比较了不同CNN架构和先进训练策略在乳腺X线图像内容检索中的表现，实现了对五类BIRADS精确匹配的高精度，并建立了新的性能基准。


<details>
  <summary>Details</summary>
Motivation: 内容基乳腺X线图像检索需要精确匹配五种BIRADS类别，比常见的二元分类任务复杂得多。当前的医学图像检索研究存在方法学局限性，包括样本量不足、数据分割不当和统计验证不足，这些都阻碍了临床转化。

Method: 开发了一个综合评估框架，系统比较了CNN架构（DenseNet121、ResNet50、VGG16）与先进的训练策略，包括精细微调、度量学习和超集成优化。采用严格的分层数据分割（50%/20%/30% 训练/验证/测试），使用602个测试查询，并通过1,000个样本的自助法置信区间进行系统验证。采用差异学习率进行高级微调，并结合互补架构进行超集成优化。

Result: 高级微调显著提高了性能：DenseNet121（精度@10为34.79%，提高19.64%）和ResNet50（34.54%，提高19.58%）。超集成优化达到了36.33%的精度@10（95% CI: [34.78%, 37.88%]），比基线提高了24.93%，每次查询提供3.6个相关病例。统计分析显示优化策略之间存在显著性能差异（p<0.001），效应量大（Cohen's d>0.8），同时保持了实际搜索效率（2.8毫秒）。性能显著超出了五类医学检索任务的实际预期（文献建议20-25%的精度@10）。

Conclusion: 该框架建立了新的性能基准，并为诊断支持和质量保证应用中的临床部署提供了基于证据的架构选择指南。

Abstract: Content-based mammographic image retrieval systems require exact BIRADS
categorical matching across five distinct classes, presenting significantly
greater complexity than binary classification tasks commonly addressed in
literature. Current medical image retrieval studies suffer from methodological
limitations including inadequate sample sizes, improper data splitting, and
insufficient statistical validation that hinder clinical translation. We
developed a comprehensive evaluation framework systematically comparing CNN
architectures (DenseNet121, ResNet50, VGG16) with advanced training strategies
including sophisticated fine-tuning, metric learning, and super-ensemble
optimization. Our evaluation employed rigorous stratified data splitting
(50%/20%/30% train/validation/test), 602 test queries, and systematic
validation using bootstrap confidence intervals with 1,000 samples. Advanced
fine-tuning with differential learning rates achieved substantial improvements:
DenseNet121 (34.79% precision@10, 19.64% improvement) and ResNet50 (34.54%,
19.58% improvement). Super-ensemble optimization combining complementary
architectures achieved 36.33% precision@10 (95% CI: [34.78%, 37.88%]),
representing 24.93% improvement over baseline and providing 3.6 relevant cases
per query. Statistical analysis revealed significant performance differences
between optimization strategies (p<0.001) with large effect sizes (Cohen's
d>0.8), while maintaining practical search efficiency (2.8milliseconds).
Performance significantly exceeds realistic expectations for 5-class medical
retrieval tasks, where literature suggests 20-25% precision@10 represents
achievable performance for exact BIRADS matching. Our framework establishes new
performance benchmarks while providing evidence-based architecture selection
guidelines for clinical deployment in diagnostic support and quality assurance
applications.

</details>


### [218] [Deep Distillation Gradient Preconditioning for Inverse Problems](https://arxiv.org/abs/2508.04832)
*Romario Gualdrón-Hurtado,Roman Jacome,Leon Suarez,Laura Galvis,Henry Arguello*

Main category: eess.IV

TL;DR: 本文提出一种基于知识蒸馏的非线性预处理算子，通过梯度匹配使教师算法（使用良条件感知矩阵）指导学生算法（使用病态感知矩阵），以改善成像逆问题的收敛性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 成像逆问题中，即使有高性能的信号先验，病态感知矩阵也会阻碍收敛并降低重建质量。传统的线性预处理方法受限于感知矩阵结构，而基于学习的线性预处理仅针对数据保真度优化，可能导致空域解。

Method: 采用知识蒸馏来设计非线性预处理算子。一个使用良条件（合成）感知矩阵的教师算法，通过预处理神经网络的梯度匹配，指导使用病态感知矩阵的学生算法。该方法在即插即用FISTA中得到验证。

Result: 在单像素成像、磁共振成像和超分辨率成像任务中，该非线性预处理算子显示出一致的性能提升和更好的经验收敛性。

Conclusion: 所提出的基于知识蒸馏的非线性预处理方法能有效改善病态感知矩阵在成像逆问题中引起的收敛性差和重建质量低的问题，提升了算法性能。

Abstract: Imaging inverse problems are commonly addressed by minimizing measurement
consistency and signal prior terms. While huge attention has been paid to
developing high-performance priors, even the most advanced signal prior may
lose its effectiveness when paired with an ill-conditioned sensing matrix that
hinders convergence and degrades reconstruction quality. In optimization
theory, preconditioners allow improving the algorithm's convergence by
transforming the gradient update. Traditional linear preconditioning techniques
enhance convergence, but their performance remains limited due to their
dependence on the structure of the sensing matrix. Learning-based linear
preconditioners have been proposed, but they are optimized only for
data-fidelity optimization, which may lead to solutions in the null-space of
the sensing matrix. This paper employs knowledge distillation to design a
nonlinear preconditioning operator. In our method, a teacher algorithm using a
better-conditioned (synthetic) sensing matrix guides the student algorithm with
an ill-conditioned sensing matrix through gradient matching via a
preconditioning neural network. We validate our nonlinear preconditioner for
plug-and-play FISTA in single-pixel, magnetic resonance, and super-resolution
imaging tasks, showing consistent performance improvements and better empirical
convergence.

</details>


### [219] [CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction](https://arxiv.org/abs/2508.04929)
*Suyi Chen,Haibin Ling*

Main category: eess.IV

TL;DR: cryoGS是一种基于高斯混合模型（GMM）的冷冻电镜（cryo-EM）三维重建方法，它将高斯泼溅（Gaussian splatting）与冷冻电镜成像物理相结合，首次实现了从原始图像直接进行稳定高效的同源重建，无需外部初始化。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜中的高斯混合模型（GMM）方法在分子密度表示方面具有优势，但现有方法依赖于外部共识图或原子模型进行初始化，限制了它们在独立管道中的应用。

Method: 本文引入了cryoGS，一种基于GMM的方法，它将高斯泼溅技术与冷冻电镜图像形成物理原理相结合。具体来说，开发了一种正交投影感知的高斯泼溅，并进行了适应性调整，如归一化项和FFT对齐的坐标系，以适应冷冻电镜成像。这些创新使得从原始冷冻电镜粒子图像直接进行稳定高效的同源重建成为可能，且支持随机初始化。

Result: 在真实数据集上的实验结果验证了cryoGS相对于代表性基线方法的有效性和鲁棒性。

Conclusion: cryoGS成功解决了现有GMM-based冷冻电镜重建方法对外部初始化的依赖问题，实现了从原始冷冻电镜粒子图像直接、稳定、高效地进行同源三维重建。

Abstract: As a critical modality for structural biology, cryogenic electron microscopy
(cryo-EM) facilitates the determination of macromolecular structures at
near-atomic resolution. The core computational task in single-particle cryo-EM
is to reconstruct the 3D electrostatic potential of a molecule from a large
collection of noisy 2D projections acquired at unknown orientations. Gaussian
mixture models (GMMs) provide a continuous, compact, and physically
interpretable representation for molecular density and have recently gained
interest in cryo-EM reconstruction. However, existing methods rely on external
consensus maps or atomic models for initialization, limiting their use in
self-contained pipelines. Addressing this issue, we introduce cryoGS, a
GMM-based method that integrates Gaussian splatting with the physics of cryo-EM
image formation. In particular, we develop an orthogonal projection-aware
Gaussian splatting, with adaptations such as a normalization term and
FFT-aligned coordinate system tailored for cryo-EM imaging. All these
innovations enable stable and efficient homogeneous reconstruction directly
from raw cryo-EM particle images using random initialization. Experimental
results on real datasets validate the effectiveness and robustness of cryoGS
over representative baselines. The code will be released upon publication.

</details>


### [220] [MedMambaLite: Hardware-Aware Mamba for Medical Image Classification](https://arxiv.org/abs/2508.05049)
*Romina Aalishah,Mozhgan Navardi,Tinoosh Mohsenin*

Main category: eess.IV

TL;DR: MedMambaLite是一个硬件感知、基于Mamba的医学图像分类模型，通过架构优化和知识蒸馏，显著减小模型尺寸、提高推理速度和能效，同时保持高精度，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: AI驱动的医疗设备需要实时、设备上的推理能力，如生物医学图像分类。然而，在边缘设备上部署深度学习模型面临模型大小和计算能力的限制，难以达到理想性能。

Method: 本文提出了MedMambaLite，一个通过知识蒸馏优化的硬件感知Mamba模型。首先，从强大的MedMamba模型开始，集成Mamba结构进行高效特征提取。其次，通过修改和减少架构冗余，使模型在训练和推理时更轻更快。最后，通过减少嵌入维度，将优化后的MedMamba知识蒸馏到一个更小的学生模型中。

Result: MedMambaLite在10个MedMNIST数据集上实现了94.5%的总体准确率。与MedMamba相比，参数量减少了22.8倍。部署在NVIDIA Jetson Orin Nano上，每次推理能耗达到35.6 GOPS/J，比MedMamba的每次推理能耗提高了63%。

Conclusion: MedMambaLite通过结合Mamba结构、架构优化和知识蒸馏，成功解决了边缘设备上部署深度学习模型进行医学图像分类的挑战，在保持高准确率的同时，显著降低了模型复杂度和能耗，为实时边缘AI医疗应用提供了高效解决方案。

Abstract: AI-powered medical devices have driven the need for real-time, on-device
inference such as biomedical image classification. Deployment of deep learning
models at the edge is now used for applications such as anomaly detection and
classification in medical images. However, achieving this level of performance
on edge devices remains challenging due to limitations in model size and
computational capacity. To address this, we present MedMambaLite, a
hardware-aware Mamba-based model optimized through knowledge distillation for
medical image classification. We start with a powerful MedMamba model,
integrating a Mamba structure for efficient feature extraction in medical
imaging. We make the model lighter and faster in training and inference by
modifying and reducing the redundancies in the architecture. We then distill
its knowledge into a smaller student model by reducing the embedding
dimensions. The optimized model achieves 94.5% overall accuracy on 10 MedMNIST
datasets. It also reduces parameters 22.8x compared to MedMamba. Deployment on
an NVIDIA Jetson Orin Nano achieves 35.6 GOPS/J energy per inference. This
outperforms MedMamba by 63% improvement in energy per inference.

</details>


### [221] [Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations](https://arxiv.org/abs/2508.05168)
*Caner Özer,Patryk Rygiel,Bram de Wilde,İlkay Öksüz,Jelmer M. Wolterink*

Main category: eess.IV

TL;DR: 本文提出利用隐式神经表示（INRs）进行医学图像质量评估，以克服传统方法中信息丢失和高内存需求的问题，并在保持性能的同时减少模型参数。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的伪影严重影响诊断准确性和后续分析。现有基于图像的方法常依赖预处理，导致信息丢失和高内存占用，限制了分类模型的可扩展性。

Method: 提出使用隐式神经表示（INRs）进行图像质量评估。INRs提供紧凑且连续的医学图像表示，自然处理分辨率和尺寸变化，并减少内存开销。在此基础上，开发了深度权重空间网络、图神经网络和关系注意力转换器来执行图像质量评估。

Result: 在带有合成伪影模式的ACDC数据集上进行了评估，结果表明该方法在评估图像质量方面表现出有效性，并且在参数更少的情况下实现了与传统方法相似的性能。

Conclusion: 基于隐式神经表示的医学图像质量评估方法是有效的，它解决了传统方法的内存和信息丢失问题，并在更少参数的情况下保持了性能，为医学图像分析提供了更高效的工具。

Abstract: Artifacts pose a significant challenge in medical imaging, impacting
diagnostic accuracy and downstream analysis. While image-based approaches for
detecting artifacts can be effective, they often rely on preprocessing methods
that can lead to information loss and high-memory-demand medical images,
thereby limiting the scalability of classification models. In this work, we
propose the use of implicit neural representations (INRs) for image quality
assessment. INRs provide a compact and continuous representation of medical
images, naturally handling variations in resolution and image size while
reducing memory overhead. We develop deep weight space networks, graph neural
networks, and relational attention transformers that operate on INRs to achieve
image quality assessment. Our method is evaluated on the ACDC dataset with
synthetically generated artifact patterns, demonstrating its effectiveness in
assessing image quality while achieving similar performance with fewer
parameters.

</details>


### [222] [Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer](https://arxiv.org/abs/2508.05240)
*Junyi Wang,Xi Zhu,Yikun Guo,Zixi Wang,Haichuan Gao,Le Zhang,Fan Zhang*

Main category: eess.IV

TL;DR: 该研究开发了一种用于配准术前MR图像和术后US图像的流程，利用3D CycleGAN进行非配对风格迁移生成合成T1图像，并结合粗到细的仿射和局部形变配准，以提高图像一致性。


<details>
  <summary>Details</summary>
Motivation: 配准术前磁共振（MR）图像和术后超声（US）图像面临挑战，可能存在模态差异和组织形变。研究旨在提高这两种不同模态图像之间的配准性能和一致性。

Method: 1. 使用3D CycleGAN进行非配对风格迁移，生成合成T1图像，以增强配准性能。2. 采用粗到细的配准策略，首先进行仿射变换，然后进行局部形变变换。

Result: 结果表明，该方法在大多数情况下提高了MR和US图像对之间的一致性。

Conclusion: 所开发的配准流程能够有效提升术前MR图像和术后US图像的配准性能，并改善它们之间的一致性。

Abstract: We developed a pipeline for registering pre-surgery Magnetic Resonance (MR)
images and post-resection Ultrasound (US) images. Our approach leverages
unpaired style transfer using 3D CycleGAN to generate synthetic T1 images,
thereby enhancing registration performance. Additionally, our registration
process employs both affine and local deformable transformations for a
coarse-to-fine registration. The results demonstrate that our approach improves
the consistency between MR and US image pairs in most cases.

</details>


### [223] [Artificial Intelligence-Based Classification of Spitz Tumors](https://arxiv.org/abs/2508.05391)
*Ruben T. Lucassen,Marjanna Romers,Chiel F. Ebbelaar,Aia N. Najem,Donal P. Hayes,Antien L. Mooyaart,Sara Roshani,Liliane C. D. Wynaendts,Nikolas Stathonikos,Gerben E. Breimer,Anne M. L. Jansen,Mitko Veta,Willeke A. M. Blokx*

Main category: eess.IV

TL;DR: 论文开发了AI模型，用于区分Spitz肿瘤与黑色素瘤，并预测Spitz肿瘤的遗传变异和诊断类别。AI模型表现良好，优于病理学家，并能优化诊断流程。


<details>
  <summary>Details</summary>
Motivation: Spitz肿瘤与传统黑色素瘤在非典型组织学特征上存在重叠，导致诊断极具挑战性。

Method: 开发并验证了基于组织学和/或临床特征的AI模型，数据集包含393例Spitz肿瘤和379例传统黑色素瘤。使用AUROC和准确性评估模型性能，并与四位经验丰富的病理学家进行比较。还进行了模拟实验，评估AI辅助诊断建议对病理科工作流程的影响。

Result: 最佳AI模型在区分Spitz肿瘤和黑色素瘤方面达到了0.95的AUROC和0.86的准确性。预测遗传变异的准确性为0.55（随机猜测为0.25）。预测诊断类别的准确性为0.51（随机猜测为0.33）。在所有三项任务中，AI模型表现优于病理学家（尽管大多数个体比较差异不显著）。模拟实验表明，AI推荐可降低材料成本、周转时间和检查量。

Conclusion: AI模型在区分Spitz肿瘤和传统黑色素瘤方面表现出强大的预测能力。在预测Spitz肿瘤的遗传变异和诊断类别等更具挑战性的任务上，AI模型表现优于随机猜测。

Abstract: Spitz tumors are diagnostically challenging due to overlap in atypical
histological features with conventional melanomas. We investigated to what
extent AI models, using histological and/or clinical features, can: (1)
distinguish Spitz tumors from conventional melanomas; (2) predict the
underlying genetic aberration of Spitz tumors; and (3) predict the diagnostic
category of Spitz tumors. The AI models were developed and validated using a
dataset of 393 Spitz tumors and 379 conventional melanomas. Predictive
performance was measured using the AUROC and the accuracy. The performance of
the AI models was compared with that of four experienced pathologists in a
reader study. Moreover, a simulation experiment was conducted to investigate
the impact of implementing AI-based recommendations for ancillary diagnostic
testing on the workflow of the pathology department. The best AI model based on
UNI features reached an AUROC of 0.95 and an accuracy of 0.86 in
differentiating Spitz tumors from conventional melanomas. The genetic
aberration was predicted with an accuracy of 0.55 compared to 0.25 for randomly
guessing. The diagnostic category was predicted with an accuracy of 0.51, where
random chance-level accuracy equaled 0.33. On all three tasks, the AI models
performed better than the four pathologists, although differences were not
statistically significant for most individual comparisons. Based on the
simulation experiment, implementing AI-based recommendations for ancillary
diagnostic testing could reduce material costs, turnaround times, and
examinations. In conclusion, the AI models achieved a strong predictive
performance in distinguishing between Spitz tumors and conventional melanomas.
On the more challenging tasks of predicting the genetic aberration and the
diagnostic category of Spitz tumors, the AI models performed better than random
chance.

</details>


### [224] [MM2CT: MR-to-CT translation for multi-modal image fusion with mamba](https://arxiv.org/abs/2508.05476)
*Chaohui Gong,Zhiying Wu,Zisheng Huang,Gaofeng Meng,Zhen Lei,Hongbin Liu*

Main category: eess.IV

TL;DR: 该论文提出了一种名为MM2CT的多模态MR到CT图像转换方法，利用T1和T2加权MRI数据，并基于Mamba架构克服了传统CNN和Transformer的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MR到CT转换方法主要基于单模态MR数据，研究多模态融合较少。MR到CT转换具有消除CT辐射暴露和减少患者运动伪影的显著优势。

Method: 引入了MM2CT多模态MR到CT转换方法，该方法利用多模态T1和T2加权MRI数据。核心是一个创新的基于Mamba的框架，用于多模态医学图像合成，它能有效处理长距离依赖并整合多模态MR特征。此外，还集成了动态局部卷积模块和动态增强模块以改进合成效果。

Result: 在公开的骨盆数据集上的实验表明，MM2CT在结构相似性指数（SSIM）和峰值信噪比（PSNR）方面达到了最先进的性能。

Conclusion: MM2CT方法通过结合多模态MR数据和Mamba架构的优势，显著提升了MR到CT图像转换的质量，是该领域的一个重要进展。

Abstract: Magnetic resonance (MR)-to-computed tomography (CT) translation offers
significant advantages, including the elimination of radiation exposure
associated with CT scans and the mitigation of imaging artifacts caused by
patient motion. The existing approaches are based on single-modality MR-to-CT
translation, with limited research exploring multimodal fusion. To address this
limitation, we introduce Multi-modal MR to CT (MM2CT) translation method by
leveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based
framework for multi-modal medical image synthesis. Mamba effectively overcomes
the limited local receptive field in CNNs and the high computational complexity
issues in Transformers. MM2CT leverages this advantage to maintain long-range
dependencies modeling capabilities while achieving multi-modal MR feature
integration. Additionally, we incorporate a dynamic local convolution module
and a dynamic enhancement module to improve MRI-to-CT synthesis. The
experiments on a public pelvis dataset demonstrate that MM2CT achieves
state-of-the-art performance in terms of Structural Similarity Index Measure
(SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at
https://github.com/Gots-ch/MM2CT.

</details>
