<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 46]
- [cs.RO](#cs.RO) [Total: 17]
- [eess.SY](#eess.SY) [Total: 9]
- [eess.IV](#eess.IV) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: 本文提出通过将奖励函数锚定在版本控制的临床实践指南（CPG）中，并结合系统评价和GRADE证据评级，来改进医疗AI基准（如HealthBench）的评估，以解决其依赖专家意见导致的地域偏见和在低收入环境中的局限性，从而培养更值得信赖和全球相关的医疗语言模型。


<details>
  <summary>Details</summary>
Motivation: HealthBench等现有医疗AI评估基准依赖专家意见而非高阶临床证据，这可能导致区域偏见和个体临床医生特质的固化，并受自动化评分系统潜在偏见的影响。这些局限性在低收入和中等收入国家尤其明显，非洲等地区面临数据稀缺、基础设施不足和监管框架不成熟等独特挑战，迫切需要更具全球相关性和公平性的基准。

Method: 提出将奖励函数锚定在版本控制的临床实践指南（CPG）中，这些指南应包含系统性综述和GRADE证据评级。路线图包括通过“证据鲁棒”的强化学习，实现评分标准与指南的联动、证据加权评分和情境覆盖逻辑，并关注伦理考量和整合延迟结果反馈。

Result: 通过将奖励重新基于经过严格审查的CPG，同时保留HealthBench的透明度和医生参与度，旨在培养出不仅语言流畅，而且临床可靠、符合伦理并具有全球相关性的医疗语言模型。

Conclusion: 重新将奖励机制建立在严格审查的临床实践指南（CPG）之上，同时保持现有基准的优点，是解决医疗AI评估中偏见和局限性的关键，从而推动开发出更值得信赖、符合伦理且全球适用的医疗语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [2] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于动态玻尔兹曼softmax强化学习的方法，用于在满足HyperTWTL安全约束下学习安全感知最优策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究在时间逻辑约束的安全强化学习（SRL）方面有所进展，但在利用超性质（如HyperTWTL）探索安全感知强化学习（RL）方面存在显著研究空白，特别是在机器人应用中的安全、不透明性和并发性方面。

Method: 将智能体动力学建模为马尔可夫决策过程（MDP），并将不透明性/安全约束形式化为HyperTWTL。提出了一种使用动态玻尔兹曼softmax强化学习的方法，以学习满足HyperTWTL约束的安全感知最优策略。

Result: 通过一个机器人取送任务案例研究，证明了所提方法的有效性和可扩展性。与两种基线RL算法进行比较，结果显示所提方法表现更优。

Conclusion: 所提出的方法能够有效地学习满足HyperTWTL约束的安全感知最优策略，并在实践中展现出优越的性能和可扩展性。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [3] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 该论文提出，在工业环境中，人工智能（AI）在端到端运营流程中的应用面临挑战。作者认为，AI需要通过以对象为中心的流程挖掘（OCPM）来奠定基础，并引入“流程智能”（PI）概念，作为连接数据和流程的关键，从而使AI能够有效诊断和改进运营流程。


<details>
  <summary>Details</summary>
Motivation: 尽管AI被广泛采用，但组织在工业环境中成功应用AI来诊断和改进端到端运营流程时面临困难。AI需要与流程数据和动态特性更好地结合。

Method: 论文提出使用以对象为中心的流程挖掘（OCPM）作为AI（包括生成式、预测式和规范式AI）的基础。作者引入“流程智能”（PI）一词，指代能够处理多种对象和事件类型、以流程为中心的数据驱动技术，从而在组织环境中赋能AI。

Result: OCPM被认为是连接数据和流程的“缺失环节”，能够支持不同形式的AI。流程智能（PI）被定义为将以流程为中心的数据驱动技术与AI融合的框架，旨在改善运营流程。

Conclusion: 为有效改进运营流程，AI需要流程智能（特别是OCPM）的支持。论文强调了成功结合OCPM与生成式、预测式和规范式AI的巨大机遇。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [4] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文提出了三种用于检测多准则决策分析（MCDA）中“排名反转”现象的新测试方法，并将其集成到Scikit-Criteria库中，旨在提高MCDA方法的性能评估能力。


<details>
  <summary>Details</summary>
Motivation: 多准则决策分析中的“排名反转”是一个严重问题，会极大地影响决策方法对备选方案集的评估结果。因此，需要一种机制来衡量方法在给定备选方案集上的表现，并进一步构建不同方法解决问题的有效性全球排名。

Method: 提出了三种检测排名反转的测试方法，并在Scikit-Criteria库中实现了这些测试。同时，讨论了在通用场景下实现这些测试时出现的复杂性以及为处理这些复杂性所做的设计考虑。

Result: 开发并实现了三种新的排名反转检测测试，这些测试已集成到Scikit-Criteria库中，并考虑了通用场景下的实现复杂性。

Conclusion: 这些新增的测试方法将在评估多准则决策方法解决问题的能力方面发挥重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [5] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 本文研究了在RDF图更新下SHACL验证问题，提出了一种基于SHACL的更新语言，并通过回归技术将静态验证问题规约到SHACL约束的（不）可满足性，并分析了其计算复杂性。


<details>
  <summary>Details</summary>
Motivation: SHACL是RDF图的W3C标准约束语言，但现有研究未充分考虑RDF图在更新下的SHACL验证问题。需要验证在给定更新序列后，所有符合SHACL规范的图是否仍然保持有效，这为推理不断演化的RDF图提供了基础。

Method: 1. 提出了一种基于SHACL的更新语言，用于捕获RDF图的修改。2. 研究了静态验证问题：验证每个符合SHACL规范的图在应用给定更新序列后是否仍然有效。3. 使用回归技术将更新操作嵌入到SHACL约束中，将更新下的静态验证问题规约到SHACL（或其扩展）中约束的（不）可满足性。4. 分析了SHACL及其关键片段的静态验证问题的计算复杂性。5. 开发了一个原型实现来执行静态验证和其他静态分析任务。

Result: 1. 证明了在更新下的静态验证问题可以规约到SHACL（或其轻微扩展）中约束的（不）可满足性。2. 分析了SHACL及其关键片段的静态验证问题的计算复杂性。3. 提供了一个原型实现，并通过初步实验展示了其行为。

Conclusion: 通过将更新操作嵌入SHACL约束并规约到约束的（不）可满足性，实现了RDF图更新下的SHACL静态验证。该方法具有理论可行性，并已通过原型实现得到初步验证。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [6] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 为解决AI算法对边缘化群体的负面影响，本文提出了一种以共同生产、多元化、公平性、包容性和多学科协作为核心的AI生产流程重构，并引入了一个包含五个阶段的增强型AI生命周期。


<details>
  <summary>Details</summary>
Motivation: 尽管已做出努力，AI算法仍可能对文化边缘化群体产生不成比例的影响，现有的风险缓解方法（如伦理指南和算法公平技术方案）不足以解决这些深层问题。

Method: 研究借鉴了设计正义、扩展学习理论和参与式AI的最新实证工作。提出了一种重构AI生产流程的方法，引入了一个由共框定、共设计、共实施、共部署和共维护五个相互关联阶段组成的增强型AI生命周期。该生命周期基于四场多学科研讨会，并以分布式权限和迭代知识交换为基础。

Result: 本文提出了一个以共同生产、多元化、公平性、包容性（DEI）和多学科协作为中心的新型AI生产管道，并详细阐述了一个包含五个相互关联阶段的增强型AI生命周期，旨在缓解AI对边缘化群体的负面影响。

Conclusion: 所提出的AI生命周期与现有伦理框架相契合，并为未来扩展参与式治理提出了关键研究问题，强调了AI开发中持续的合作和知识共享的重要性。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [7] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 现有AI教育应用中过度依赖人类评估者间信度（IRR）来验证标注数据会阻碍进展，应优先考虑有效性和教育影响。


<details>
  <summary>Details</summary>
Motivation: 人类评估者存在偏见且不可靠，但传统的评估者间信度（IRR）指标（如Cohen's kappa）仍是教育AI中大量训练数据标注的核心验证方法，这阻碍了数据分类的有效性和预测性，从而影响学习改进。

Method: 提出并强调五种互补的评估方法，例如多标签标注方案、基于专家的方法和闭环有效性。同时强调外部有效性的重要性，例如建立验证导师行为的程序。

Result: 这些补充方法能够生成更好的训练数据和后续模型，从而改善学生学习并提供更具操作性的洞察，优于单独使用IRR的方法。

Conclusion: 呼吁该领域重新思考标注质量和“真实值”的定义，优先考虑有效性和教育影响，而非仅仅是评估者之间的共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [8] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 该研究旨在通过强制AI赋能人类并管理人机权力平衡来提升AI安全性和人类福祉。为此，论文设计了一个可参数化、可分解的客观函数，用于衡量人类权力，并提出了计算该指标的算法，认为这比直接基于效用的目标更安全。


<details>
  <summary>Details</summary>
Motivation: 权力是AI安全（如AI追求权力、人类被剥夺权力、人机互动和国际治理中的权力平衡）和人类福祉（追求多样化目标的能力）的关键概念。现有直接基于效用的AI目标可能不安全，因此需要探索一种既能保障AI安全又能促进人类福祉的方法。

Method: 研究提出明确要求AI代理赋能人类并以期望方式管理人类与AI之间的权力平衡。具体方法包括：1. 设计了一个基于公理化方法的可参数化、可分解的目标函数，该函数代表了对不平等和风险规避的人类长期总权力；2. 该函数考虑了人类的有限理性和社会规范，并涵盖了广泛的人类目标；3. 推导了通过逆向归纳法计算该指标的算法，或通过多智能体强化学习近似计算的算法。

Result: 研究推导了计算或近似计算该人类权力指标的算法。通过在多种典型情境中（软性地）最大化该指标，展示了其潜在后果，并描述了可能由此产生的工具性子目标。初步评估表明，（软性地）最大化适当的人类权力聚合指标，可能比直接基于效用的目标，对具身AI系统而言是一种更有益且更安全的目标。

Conclusion: 谨慎评估表明，对具身AI系统而言，（软性地）最大化适当的人类权力聚合指标，可能构成一个有益且比直接基于效用的目标更安全的目标，从而同时促进AI安全性和人类福祉。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [9] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS是一种新颖的方法，通过结合内部思考和外部学习来增强大型语言模型（LLM）的推理能力，解决了RLVR的局限性并超越了基础模型的固有能力边界。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习（RLVR）方法受限于基础LLM的固有能力边界，因为其策略是基于当前策略的，且面临巨大的动作空间和稀疏奖励问题。此外，RLVR可能导致能力边界崩溃，缩小LLM的问题解决范围。

Method: RL-PLUS通过协同内部利用（思考）和外部数据（学习）来工作。它包含两个核心组件：多重重要性采样（解决外部数据导致的分布不匹配问题）和基于探索的优势函数（引导模型探索高价值、未探索的推理路径）。

Result: RL-PLUS在六个数学推理基准测试中达到了现有RLVR方法的最新水平，在六个分布外推理任务中表现出色。它在不同模型家族中实现了21.1%至69.2%的平均相对提升。此外，在多个基准测试上的Pass@k曲线表明RL-PLUS有效解决了能力边界崩溃问题。

Conclusion: RL-PLUS是一种优越且具有泛化性的方法，能够显著提升LLM的推理能力，超越基础模型的限制，并有效解决RLVR所面临的挑战。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [10] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一个受“边做边学”启发的智能体范式，它通过持续的自我改进、工具学习和知识蒸馏，无需模型参数更改或额外训练，即可实现知识发现和能力提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个能够像人类一样通过实践和持续自我提升来发展专业知识的智能体，解决现有智能体在复杂知识发现任务中可能存在的局限性，并探索无需模型再训练的自进化系统。

Method: MetaAgent采用最小工作流启动，具备基本推理和自适应求助能力。遇到知识空白时，它生成自然语言求助请求，并通过专用工具路由器路由到最合适的外部工具。它持续进行自我反思和答案验证，将可操作的经验提炼成文本并动态融入未来的任务上下文。此外，MetaAgent通过组织工具使用历史，自主构建内部工具和持久知识库，实现“元工具学习”，从而迭代优化其推理和工具使用策略。

Result: 在GAIA、WebWalkerQA和BrowseCamp等知识发现基准测试中，MetaAgent持续优于基于工作流的基线方法，并达到或超过了端到端训练的智能体性能。

Conclusion: MetaAgent展示了自进化智能体系统在鲁棒、通用知识发现方面的巨大潜力，证明了通过“边做边学”和持续自我改进，无需模型参数更改即可实现能力增强的可能性。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [11] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 研究发现，即使提供心理驱动因素，大型语言模型（LLM，如GPT-4o）在任务生成上仍无法像人类一样，其生成的任务缺乏社交性、物理性，且偏向抽象，这揭示了LLM与人类认知之间的核心差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的生成式智能体旨在模拟人类复杂的任务生成行为，但其是否遵循与人类相似的认知原则仍不确定，本研究旨在探讨这一问题。

Method: 通过一项任务生成实验，比较了人类受试者和LLM智能体（GPT-4o）的响应。实验中，LLM被明确提供了人类的心理驱动因素（如个人价值观和认知风格）。

Result: 人类的任务生成持续受到心理驱动因素（如开放性、认知风格）的影响。即使明确提供这些心理驱动因素，LLM也未能反映出相应的行为模式，其生成的任务明显更少社交性、更少物理性，且在主题上偏向抽象。尽管LLM生成的任务被认为更有趣和新颖，但这反而突显了其语言能力与生成类人、具身目标能力之间的脱节。

Conclusion: 人类以价值观驱动、具身化的认知与LLM的统计模式之间存在核心差距。因此，在设计更符合人类的智能体时，有必要融入内在动机和物理具身性。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [12] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: 本文提出了ReasonBench，一个用于评估视觉语言模型（VLMs）在复杂结构化图形推理能力上的基准测试，并揭示了现有模型的显著局限性。同时，提出了一种双重优化策略（DiaCoT和ReasonTune），将VLM性能提升了33.5%。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图形推理任务中表现出明显不足，尤其是在复杂图形推理和抽象问题解决方面，现有研究主要集中在简单图形，而对复杂场景的研究较少。

Method: 1. 提出了ReasonBench，一个包含1613个来自真实智力测试问题的结构化图形推理基准测试集，涵盖位置、属性、数量和多元素推理维度。2. 使用ReasonBench评估了11种主流VLM模型（包括闭源和开源）。3. 提出了双重优化策略：Diagrammatic Reasoning Chain (DiaCoT) 通过分解层增强推理可解释性，ReasonTune 通过训练增强模型任务适应性。

Result: 1. 当前主流VLM模型在复杂图形推理任务上存在显著局限性。2. 提出的DiaCoT和ReasonTune双重优化策略将VLM性能提升了33.5%。

Conclusion: 尽管VLMs在图形推理方面仍有显著不足，特别是在复杂任务上，但通过引入专门的评估基准ReasonBench，并结合DiaCoT和ReasonTune等优化策略，可以有效提升模型在复杂图形推理任务上的性能和解释性。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [13] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: 该研究提出R1-Act，一种简单高效的后训练方法，通过结构化推理过程激活大型推理模型中已有的安全知识，显著提升模型安全性，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽能力强大，但常执行有害指令，引发安全担忧。研究发现模型并非缺乏安全知识，而是未能有效激活。

Method: 基于模型已拥有安全知识但未激活的洞察，提出R1-Act，一个简单高效的后训练方法。该方法通过结构化推理过程，显式地触发模型内部的安全知识。

Result: R1-Act在显著提升模型安全性的同时，保持了原有的推理性能，且优于现有对齐方法。它仅需1000个训练样本，在单张RTX A6000 GPU上训练90分钟即可完成。在多种LRM骨干网络和规模上的广泛实验证明了其鲁棒性、可扩展性和实用高效性。

Conclusion: 该方法通过激活模型内已有的安全知识，有效解决了大型推理模型的安全问题，且具有极高的训练效率和广泛的适用性，为模型安全对齐提供了新的高效途径。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [14] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过引入视觉验证机制，解决了视觉语言模型(VLM)中思维链(CoT)推理缺乏视觉接地性导致幻觉的问题，提高了推理性能和解释的真实性。


<details>
  <summary>Details</summary>
Motivation: 思维链(CoT)提示虽然能提升视觉语言模型(VLM)的推理能力，但其生成的解释常流利却缺乏视觉接地性，导致幻觉。这主要是因为多步推理过程中缺少明确的验证机制。

Method: CoRGI（Chain of Reasoning with Grounded Insights）是一个模块化框架，包含三个阶段：首先生成文本推理链；其次通过专用模块（VEVM）为每一步推理提取支持性视觉证据；最后将文本推理与视觉证据合成，生成接地且经过验证的答案。该框架可与现有VLM集成，无需端到端重新训练。

Result: CoRGI在VCR基准测试上提高了Qwen-2.5VL和LLaVA-1.6两种代表性开源VLM骨干的推理性能。消融研究证实了验证模块中每一步的贡献，人工评估表明CoRGI能生成更真实、更有帮助的解释。研究还探讨了视觉验证步骤的其他设计，并讨论了事后验证框架的潜在局限性。

Conclusion: 这些发现强调了将中间推理步骤与视觉证据相结合的重要性，以增强多模态推理的鲁棒性。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [15] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 该论文提出了一种在主动推理框架下实现多智能体协作的新方法，通过引入心智理论（ToM），使智能体能够仅通过观察行为推断他人的信念和目标，从而实现更好的协作，无需共享模型或显式通信。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体主动推理协作方法依赖于任务特定的共享生成模型或显式通信，这限制了其通用性。研究旨在开发一种更通用、无需这些限制的协作方法，并利用心智理论（ToM）使智能体能够理解并推理他人的信念和目标。

Method: 将心智理论（ToM）整合到主动推理框架中，使智能体能够维持自身和他人信念及目标的独立表征。扩展了基于复杂推理树的规划算法，通过递归推理系统地探索联合策略空间。在碰撞避免和觅食任务中进行模拟评估。

Result: 配备ToM的智能体比非ToM智能体表现出更好的协作能力，能够有效避免碰撞并减少冗余努力。关键在于，ToM智能体仅通过观察行为就能推断他人的信念，无需显式通信。

Conclusion: 这项工作推动了人工智能的实际应用，并为心智理论提供了计算层面的见解，展示了通过推断他人信念实现有效多智能体协作的可行性。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [16] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: 本文提出了一个名为Cognitive Kernel-Pro的开源多模块智能体框架，旨在通过高质量数据训练和新颖的测试时策略，实现通用AI智能体的民主化开发和评估，并在GAIA基准测试中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的通用AI智能体系统要么是闭源的，要么严重依赖付费API和专有工具，这限制了研究社区的可访问性和可复现性。研究者希望解决这一问题，使高级AI智能体的开发和评估更加民主化。

Method: 研究者提出了一个名为Cognitive Kernel-Pro的完全开源、最大限度免费的多模块智能体框架。该框架系统地研究了智能体基础模型高质量训练数据的策划，专注于在网络、文件、代码和通用推理四个关键领域构建查询、轨迹和可验证答案。此外，还探索了新颖的智能体测试时反思和投票策略，以增强智能体的鲁棒性和性能。

Result: Cognitive Kernel-Pro在GAIA基准测试中进行了评估，在开源和免费智能体中取得了最先进的结果。值得注意的是，其8B参数的开源模型超越了之前领先的系统，如WebDancer和WebSailor，为可访问、高性能的AI智能体树立了新的性能标准。

Conclusion: Cognitive Kernel-Pro的发布和其在GAIA上的卓越表现，证明了完全开源和免费的框架能够实现高性能的通用AI智能体，为AI研究社区的民主化开发和评估设定了新的基准。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [17] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在编程和结构化推理方面表现出色，但在形式化数学证明方面遇到显著困难。本文探讨了LLMs在数学领域的最新进展、挑战以及其内部推理机制。


<details>
  <summary>Details</summary>
Motivation: LLMs在编程和符号任务中表现出卓越能力，引发了将其应用于数学领域的兴趣。然而，与编程的表面相似性不同，LLMs在形式化数学证明方面进展缓慢，这促使人们深入探讨LLMs的推理方式、监督机制以及它们是否追踪计算或演绎状态。

Method: 本文综述了该领域的最新技术水平，重点关注了最新的模型和基准测试。它探讨了机器学习与数学认知交叉点的三个核心问题：1) 形式化与非形式化数学作为训练领域之间的权衡；2) 证明生成比代码合成更脆弱的深层原因；3) LLMs是表示还是仅仅模仿了演化的逻辑状态。

Result: 抽象中未直接呈现具体研究结果，而是指出了当前LLMs在形式化数学领域的局限性，并提出了可能扩展这些局限性的方向。主要结果是识别并分析了LLMs在数学应用中的核心挑战和未解问题。

Conclusion: 本文旨在识别LLMs在形式化数学领域的当前局限性，并探讨如何扩展这些局限，而不是划定明确的边界。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [18] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一个基于概率可达性分析的主动式运行时安全保障框架，用于大语言模型（LLM）智能体，通过预测未来风险在违规发生前进行干预，解决了现有被动式系统缺乏预见性的问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体虽然功能强大，但其随机行为带来了难以预测的安全风险。现有的基于规则的强制执行系统（如AgentSpec）是被动响应的，通常在不安全行为即将发生或已经发生时才介入，缺乏预见性，且难以处理长周期依赖和分布变化。

Method: Pro2Guard采用概率可达性分析。它将智能体行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC）。在运行时，通过估计到达不安全状态的概率来预测未来风险，当预测风险超过用户定义的阈值时，在违规发生前触发干预。该方法还结合了语义有效性检查和PAC边界，以确保统计可靠性。

Result: Pro2Guard在家庭智能体和自动驾驶两个安全关键领域进行了广泛评估。在家庭智能体任务中，使用低阈值时，Pro2Guard能在高达93.6%的不安全任务中早期强制执行安全，同时通过可配置模式（如反射）保持高达80.4%的任务完成率。在自动驾驶场景中，Pro2Guard实现了100%的交通法规违规和碰撞预测，提前38.66秒预测风险。

Conclusion: Pro2Guard通过提供主动的风险预测和干预，有效解决了LLM智能体安全保障中被动式系统的局限性，在安全关键领域展现出高效率，显著提升了LLM智能体的安全性。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [19] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP是一个模型无关的可解释性框架，它利用Shapley交互指数来量化多模态AI模型中细粒度视觉和文本元素之间的协同/抑制作用，适用于开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型因其“黑箱”性质，在需要可解释性和可信度的高风险应用中部署面临障碍。现有解释方法（如注意力图、Grad-CAM）只能提供粗略见解，无法精确量化跨模态协同效应，且仅限于开源模型。

Method: 引入MultiSHAP框架，它是一种模型无关的可解释性方法，利用Shapley交互指数将多模态预测归因于细粒度视觉（如图像块）和文本（如文本标记）元素之间的成对交互。该方法适用于开源和闭源模型，并提供实例级（揭示协同和抑制效应）和数据集级（揭示可泛化的交互模式）解释。

Result: 在公共多模态基准上的实验证实，MultiSHAP忠实地捕获了跨模态推理机制。实际案例研究也证明了其实用性。该框架可扩展到两种模态以上。

Conclusion: MultiSHAP提供了一个通用的解决方案，用于解释复杂的多模态AI模型，通过量化细粒度跨模态交互，增强了模型的可解释性和可信度，并适用于各种模型类型。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [20] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 该研究提出了一个多阶段LLM驱动框架，用于从电子病历（EMR）中生成全面的预问诊问卷，克服了直接LLM方法在信息完整性、逻辑性和疾病层面综合方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 从复杂且大量的电子病历中生成全面的预问诊问卷是一项挑战性任务。直接使用大型语言模型（LLM）在此任务中存在信息完整性、逻辑顺序和疾病层面综合的困难。

Method: 提出一个新颖的多阶段LLM驱动框架：第一阶段从EMR中提取原子断言（带有时间的关键事实）；第二阶段构建个人因果网络并通过聚类EMR语料库中的代表性网络来综合疾病知识；第三阶段基于这些结构化表示生成定制化的个人和标准化疾病特异性问卷。该框架通过构建显式临床知识来克服直接方法的局限性。

Result: 该方法在真实世界EMR数据集上进行评估，并由临床专家验证，结果表明其在信息覆盖率、诊断相关性、可理解性和生成时间方面表现优越。

Conclusion: 该框架具有增强患者信息收集的实际潜力，克服了直接LLM方法在生成预问诊问卷方面的不足。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [21] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 该研究提出了一个用于评估交互式多媒体内容的相对度量标准AVR-Eval和一个多智能体系统AVR-Agent，用于生成JavaScript游戏和动画。尽管AVR-Agent优于一次性生成，但发现AI模型在有效利用自定义资产和视听反馈方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在生成文本、音频、图像和视频方面表现出色，但创建视频游戏等交互式视听内容仍然具有挑战性。当前的LLM可以生成简单的游戏，但缺乏自动化评估指标，并且难以处理需要大量人工和艺术资产的复杂内容。

Method: 1. 提出了AVR-Eval：一个使用视听记录（AVRs）评估多媒体内容质量的相对度量标准。一个全模态模型（处理文本、视频、音频）比较两个内容的AVR，并通过文本模型审查评估以确定优劣。2. 构建了AVR-Agent：一个多智能体系统，从多媒体资产库（音频、图像、3D模型）生成JavaScript代码。编码智能体选择相关资产，生成多个初始代码，使用AVR-Eval识别最佳版本，并通过来自AVR的全模态智能体反馈迭代改进。

Result: 1. AVR-Eval能够有效识别好内容与损坏或不匹配的内容。2. AVR-Agent生成的内容与一次性生成的内容相比，具有显著更高的胜率。3. 然而，模型未能有效利用自定义资产和AVR反馈，未能显示出更高的胜率，这揭示了一个关键差距：人类能从高质量资产和视听反馈中受益，但当前的编码模型似乎无法有效利用这些资源。

Conclusion: 尽管AVR-Agent在生成交互式内容方面取得了进展，但当前AI模型在利用高质量资产和视听反馈方面的能力远不如人类，这凸显了人类和机器内容创作方法之间的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [22] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了多频带变滞后格兰杰因果关系（MB-VLGC）框架，通过显式建模频率依赖的因果延迟，解决了传统格兰杰因果关系和变滞后格兰杰因果关系未能考虑因果相互作用在不同频率带上具有不同时间延迟的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的格兰杰因果关系存在固定的滞后假设，这在复杂系统中通常不现实。变滞后格兰杰因果关系（VLGC）解决了变动时间滞后的问题，但未能考虑因果相互作用不仅在时间延迟上变化，而且在不同频率带上也有所不同（例如，大脑信号中不同频段的活动可能以不同延迟影响其他区域）。因此，需要一个能够建模频率依赖因果延迟的新框架。

Method: 本文正式定义了多频带变滞后格兰杰因果关系（MB-VLGC），并提出了一个新颖的框架，该框架通过明确建模频率依赖的因果延迟来泛化传统的VLGC。论文提供了MB-VLGC的正式定义，证明了其理论健全性，并提出了一个高效的推理流程。

Result: 在多个领域的广泛实验表明，该框架在合成数据集和真实世界数据集上均显著优于现有方法，证实了其对任何类型时间序列数据的广泛适用性。

Conclusion: MB-VLGC框架通过考虑频率依赖的因果延迟，显著提升了时间序列因果关系推断的准确性和普适性，为理解复杂系统中的因果关系提供了更强大的工具。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [23] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出一个混合框架，结合传统可解释AI（XAI）和生成式AI，为自适应学习系统提供个性化、多模态解释，以增强透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的自适应学习系统缺乏透明度，决策过程不清晰。大多数可解释AI技术侧重技术输出，忽略用户角色和理解需求。

Method: 提出一个混合框架，整合传统可解释AI技术、生成式AI模型和用户个性化，以生成多模态、个性化的解释。将可解释性重新定义为针对用户角色和学习目标的动态沟通过程。

Result: 概述了框架设计、教育领域XAI的主要局限性，并提出了关于准确性、公平性和个性化的研究方向。

Conclusion: 旨在实现增强透明度并支持以用户为中心体验的可解释AI，强调解释应是个性化和动态的沟通过程。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [24] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出一个用户分段和情境感知的视觉解释系统，旨在为社交媒体AI推荐提供多样化、适应用户需求和上下文的解释，以提高用户理解和信任。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体的AI推荐缺乏解释性，或解释过于通用，未能满足用户特定需求，导致用户不理解推荐原因，从而降低了推荐的价值和用户体验。

Method: 提出一个视觉解释系统，包含多种解释方法。该系统根据用户需求和情境调整解释风格（视觉或数字）和粒度（专家级或普通用户级），例如为AI专家提供技术细节，为普通用户提供简化版本。这是首个在一个管道中联合调整解释风格和粒度的框架。计划通过对30名X平台用户进行公开试点验证其对决策和信任的影响。

Result: 作为一个愿景论文，主要成果是提出了一个创新的框架，该框架首次在一个单一流程中联合适应解释风格（视觉 vs. 数字）和粒度（专家 vs. 普通用户）。计划进行的试点研究将验证其对用户决策和信任的影响。

Conclusion: 通过提供用户分段和情境感知的视觉解释，本系统旨在解决AI推荐解释性不足的问题，从而提高用户对推荐的理解、促进决策并增强信任，最终改善社交媒体的用户体验。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [25] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 本文提出利用大型预训练多模态模型的潜在编码来检测生成内容，实现了跨模态的通用伪造检测，性能优于或匹敌现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如图像、文本）被恶意用户利用来传播虚假信息和深度伪造，现有伪造检测工具通常只能在同类生成器和数据模态内泛化，对其他生成类别和数据领域效果不佳，因此迫切需要鲁棒且稳定的通用伪造检测器。

Method: 作者提出使用大型预训练多模态模型的潜在编码来检测生成内容。他们发现这些模型的潜在编码能自然地捕获区分真实和伪造信息。在此基础上，他们训练线性分类器，利用这些特征在不同模态上进行伪造检测。

Result: 在线性分类器训练后，该方法在多种模态（主要关注音频和图像）上实现了最先进的检测结果，同时计算效率高、训练速度快，即使在少样本设置下也表现出色，性能超越或匹敌强大的基线方法。

Conclusion: 大型预训练多模态模型的潜在编码能够有效捕获真实与伪造信息之间的差异，基于这些特征训练的线性分类器可以作为一种通用、高效且强大的方法，用于跨模态的生成内容检测。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [26] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出了一种名为QME的新型框架，通过可学习的专家混合（MoE）分数融合策略，并结合质量估计和分数三元组损失，显著提升了全身生物识别的性能，达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 全身生物识别是一个具有挑战性的多模态任务，传统的分数融合方法（如加权平均）往往忽略了各个模态分数分布的差异，这限制了最终性能的提升。

Method: 本文提出了一种名为QME（Quality-guided Mixture of score-fusion Experts）的框架，通过使用专家混合（MoE）实现可学习的分数融合策略。具体方法包括引入一种新颖的伪质量损失，用于模态特定的质量估计器（QE），以及一个分数三元组损失，以提高度量性能。

Result: 在多个全身生物识别数据集上进行了广泛实验，结果表明所提出的方法是有效的，与基线方法相比，在各种指标上均取得了最先进的（SOTA）结果。

Conclusion: QME方法对多模态和多模型均有效，成功解决了相似度分数域中的模型错位以及数据质量可变性等关键挑战，显著提升了全身生物识别的性能。

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [27] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 该研究探讨了动作识别模型在不同上下文中的运动概念可迁移性，发现现有SOTA模型在识别新情境下的高级动作时性能显著下降，尤其是在处理细粒度动作和时间推理时。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别模型虽然泛化能力强，但仍不清楚它们是否能有效地将高层运动概念（例如，从“拳击”到“拳击某人”）在多样化甚至相似的上下文中进行迁移。

Method: 引入了一个运动可迁移性框架，并构建了三个数据集：Syn-TA（合成3D物体运动）、Kinetics400-TA和Something-Something-v2-TA（均改编自真实视频）。研究评估了13个最先进的模型在这些基准上的表现，并分析了其性能。

Result: 1) 在新情境下识别高级动作时，模型性能显著下降。2) 多模态模型在处理细粒度未知动作时比粗粒度动作更困难。3) 偏置无关的Syn-TA数据集与真实世界数据集一样具有挑战性，模型在受控设置下表现出更大的性能下降。4) 较大模型在空间线索占主导时能提升可迁移性，但在密集时间推理方面表现不佳；对物体和背景线索的依赖会阻碍泛化。

Conclusion: 这项研究为评估动作识别中的运动可迁移性建立了一个关键基准。解耦粗粒度和细粒度运动可能有助于改善在时间挑战性数据集上的识别效果。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [28] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: 现有VIO/SLAM系统难以应对头戴式设备追踪的挑战性场景，现有数据集覆盖不足。本文提出了Monado SLAM数据集，以推动VIO/SLAM研究进展。


<details>
  <summary>Details</summary>
Motivation: 尽管VIO/SLAM技术已取得显著进展，但现有系统仍难以优雅地处理头戴式设备使用中的许多挑战性设置，如高强度运动、动态遮挡、长时间追踪、低纹理区域、恶劣光照和传感器饱和等。现有数据集对这些真实世界问题的覆盖不足，导致系统可能忽略这些关键问题。

Method: 为了解决现有数据集的不足，本文提出了Monado SLAM数据集，该数据集包含从多个虚拟现实头戴设备中获取的真实序列。

Result: 成功创建并发布了Monado SLAM数据集，该数据集包含来自多个VR头显的真实序列，并以CC BY 4.0许可发布。

Conclusion: 通过发布Monado SLAM数据集，旨在弥补现有数据集在挑战性场景覆盖上的不足，从而促进VIO/SLAM研究和开发的进一步发展。

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [29] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 本研究提出了一种基于卷积神经网络（CNN）的性别分类模型，利用眼周区域的彩色图像进行分类，并在两个数据集上实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 性别分类在多个领域至关重要，但易受化妆和伪装影响。研究旨在通过利用包含有价值视觉线索的眼周区域，提高性别分类的准确性。

Method: 引入了一个复杂的卷积神经网络（CNN）模型，使用眼周区域的彩色图像数据库进行性别分类。模型在CVBL和(Female and Male)两个眼部数据集上进行了性能验证和测试。

Result: 模型在CVBL数据集上达到了99%的准确率，在(Female and Male)数据集上以少量可学习参数（7,235,089个）达到了96%的准确率。与现有最先进的方法相比，该模型表现出卓越的性能。

Conclusion: 研究结果明确证明了该模型利用眼周区域进行性别分类的有效性，表明其在安全和监控等领域具有潜在的实际应用价值。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [30] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: 本文提出World Consistency Score (WCS)，一种新的视频生成模型评估指标，专注于衡量生成视频的内部世界一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估指标主要关注视觉保真度或提示对齐，缺乏对视频内部世界一致性（时间与物理连贯性）的评估，无法衡量生成视频在时间上的连贯性。

Method: WCS由四个可解释的子组件构成：物体永存性、关系稳定性、因果符合性、闪烁惩罚，每个子组件衡量视频时间与物理连贯性的不同方面。这些子指标通过学习到的加权公式组合，权重通过人类偏好数据训练。计算子指标时使用开源工具（跟踪器、动作识别器、CLIP嵌入、光流）。通过在VBench-2.0、EvalCrafter、LOVE等基准上测试WCS与人类评估的相关性、进行敏感性分析以及与现有指标（FVD、CLIPScore、VBench、FVMD）比较来验证其有效性。

Result: 本文提出WCS作为一种全面且可解释的框架，用于评估视频生成模型维持连贯“世界”的能力。WCS通过整合多个子指标并与人类判断对齐，旨在弥补现有指标在评估视频时间连贯性方面的不足。

Conclusion: WCS提供了一个全面且可解释的框架，用于评估视频生成模型在时间上维持连贯“世界”的能力，解决了以往仅关注视觉保真度或提示对齐的指标所留下的空白。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [31] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer提出了一种基于好奇心驱动探索的主动地理定位（AGL）代理，通过内在奖励克服了现有距离奖励方法的泛化和鲁棒性不足问题，尤其适用于陌生目标和环境。


<details>
  <summary>Details</summary>
Motivation: 当前主动地理定位（AGL）方法将任务视为基于距离奖励的强化学习问题，但在距离估计困难或遇到未知目标和环境时，其鲁棒性和泛化能力会降低，因为学习到的探索策略不可靠。

Method: 本文提出了GeoExplorer，一个结合好奇心驱动探索（通过内在奖励实现）的AGL代理。与基于距离的奖励不同，GeoExplorer的好奇心驱动奖励是与目标无关的，通过有效的环境建模实现鲁棒、多样化和上下文相关的探索。

Result: GeoExplorer在四个AGL基准测试中进行了广泛实验，证明了其在不同设置下的有效性和泛化能力，特别是在定位不熟悉的目标和环境方面表现出色。

Conclusion: GeoExplorer通过引入好奇心驱动的内在奖励，显著提升了主动地理定位任务的鲁棒性和泛化能力，尤其解决了现有方法在未知或复杂环境中的局限性，实现了更有效的探索。

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [32] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 该论文提出一种通过改进视频修复模型来实现视频外绘的方法，引入分层判别器和专门的外绘损失函数，有效解决了现有方法中边界扩展模糊和不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频外绘方法在扩展边界时难以保持内容一致性，直接应用或微调视频修复模型常导致模糊结果，且缺乏能有效评估扩展区域感知质量的判别器。

Method: 作者建议使用擅长物体流学习和重建的视频修复模型进行外绘。为解决模糊问题，引入了区分全局和局部目标的“分层判别器”，并开发了利用判别器局部和全局特征的“专用外绘损失函数”。通过此对抗性损失函数微调生成器。

Result: 所提出的方法在定量和定性方面均优于现有最先进的方法，能生成视觉上吸引人且全局连贯的外绘场景。

Conclusion: 通过引入分层判别器和专用外绘损失函数，可以有效提升视频外绘的质量，使生成内容既视觉美观又全局一致。

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [33] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 该论文提出了概率点云（Probabilistic Point Clouds, PPC），一种新的3D场景表示方法，通过为每个点增加概率属性来封装测量不确定性。PPC及其推理方法能有效提升LiDAR和相机-LiDAR融合模型在挑战性场景下的3D目标检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代LiDAR在长距离或低反照率物体等场景下，会产生稀疏或错误的点云。这些错误源于原始LiDAR测量的噪声，并传播到下游感知模型，导致精度严重下降。这是因为传统的3D处理流程在构建点云时未保留任何不确定性信息。

Method: 本文提出概率点云（PPC），一种新颖的3D场景表示，其中每个点都附加一个概率属性，用于封装原始数据中的测量不确定性（或置信度）。此外，还引入了利用PPC进行鲁棒3D目标检测的推理方法，这些方法计算量轻，可作为即插即用模块集成到现有3D推理管道中。

Result: 通过仿真和真实采集数据验证，基于PPC的3D推理方法在包含小型、远距离、低反照率物体以及强环境光的挑战性室内外场景中，均优于使用LiDAR以及相机-LiDAR融合模型的多个基线方法。

Conclusion: PPC通过整合测量不确定性，显著提升了3D感知任务的鲁棒性和准确性，尤其在LiDAR数据质量受限的复杂场景中表现出色，且其推理方法具有高效和易于集成的优势。

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [34] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: 本文提出SeTe-VSR，一种新型视频超分辨率方法，通过在潜在扩散空间中引入语义和时空引导，解决现有VSR模型在高保真对齐和时间一致性方面的挑战，有效平衡细节恢复与时间连贯性，并显著提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率（VSR）模型在增强低分辨率视频时，难以有效控制生成过程，导致难以实现与低分辨率输入的精确对齐（高保真度）和帧间的时间一致性。

Method: 提出语义和时空引导视频超分辨率（SeTe-VSR）方法。该方法在潜在扩散空间中融入高层语义信息，并整合空间和时间信息，以实现对生成过程的有效引导。

Result: SeTe-VSR在恢复复杂细节和确保时间连贯性之间实现了无缝平衡。该方法不仅保留了高真实感的视觉内容，还显著增强了保真度。实验表明，SeTe-VSR在细节恢复和感知质量方面优于现有方法。

Conclusion: SeTe-VSR通过结合语义和时空引导，成功解决了视频超分辨率中的核心挑战，证明了其在复杂视频超分辨率任务中的有效性，能够提供更好的细节恢复和感知质量。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [35] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: 视觉语言模型（VLMs）在医学图像和报告分析中存在模态偏见，过度依赖文本信息。本文提出选择性模态转移（SMS）方法量化这种偏见，通过实验发现模型严重依赖文本，忽略视觉线索。研究强调需开发真正整合视觉和文本信息的多模态医学模型。


<details>
  <summary>Details</summary>
Motivation: 临床决策依赖于医学图像及其报告的整合分析。视觉语言模型（VLMs）虽能提供统一框架，但常表现出对单一模态的强烈偏见，频繁忽略关键视觉线索而偏爱文本信息。

Method: 本文引入选择性模态转移（SMS），一种基于扰动的方法，用于量化二元分类任务中模型对每种模态的依赖性。通过系统地交换具有相反标签样本的图像或文本，揭示模态特异性偏见。评估了六个开源VLM（四个通用型和两个医学微调型）在两个不同模态的医学影像数据集（MIMIC-CXR和FairVLMed）上的性能和校准，包括在未扰动和扰动设置下。此外，还进行了基于注意力的定性分析。

Result: 研究发现模型对文本输入存在显著依赖性，即使存在互补的视觉信息，这种依赖性依然持续。定性注意力分析进一步证实，图像内容经常被文本细节所掩盖。

Conclusion: 研究结果强调了设计和评估能够真正整合视觉和文本线索的多模态医学模型的重要性，而非仅仅依赖单一模态信号。

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [36] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种新的两阶段重建框架，用于生成中国长期的VIIRS类夜间灯光（NTL）数据产品（EVAL），将时间序列追溯到1986年，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: NPP-VIIRS夜间灯光数据始于2012年，限制了更早时期的长期时间序列研究。现有扩展VIIRS类NTL时间序列的方法存在光强度低估和结构遗漏的显著缺点，需要改进。

Method: 提出了一种新颖的两阶段重建框架：构建阶段和精炼阶段。构建阶段采用层次融合解码器（HFD）增强初始重建的保真度；精炼阶段利用双特征精炼器（DFR），通过高分辨率不透水表面掩模引导和增强精细结构细节。基于此框架，开发了中国扩展VIIRS类夜间灯光（EVAL）产品，将标准数据记录回溯了26年，从1986年开始。

Result: 定量评估显示，EVAL产品显著优于现有最先进的产品，将R²从0.68提高到0.80，同时将RMSE从1.27降低到0.99。此外，EVAL表现出卓越的时间一致性，并与社会经济参数保持高度相关性。

Conclusion: EVAL数据集为研究社区提供了宝贵的新资源，其可靠性已得到证实，可用于长期分析。该数据集已公开可用。

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [37] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 本文提出了一种用于分层图结构（“图谱系”和“分级图”）的代数类型理论，通过定义骨架操作符来高效地构建和操作这些结构，并展示了其在深度学习和多重网格数值方法中的应用。


<details>
  <summary>Details</summary>
Motivation: 图和不断增长的图序列被广泛用于机器学习和计算科学中的模型架构。研究动机在于需要一种结构化的、分层的方式来定义和操作这些不断增长的图，特别是为了构建和分析多尺度或分层模型架构（“分层架构”）。

Method: 研究方法包括：1) 定义分层增长的“结构化图谱系”，其顶点和边数量呈指数级增长；2) 使用二部图连接连续层；3) 利用延长映射定义过程导出的距离度量；4) 定义“分级图”范畴，并推导出其标准代数图操作（如叉积、箱积、不交和、函数类型）的低成本“骨架”变体；5) 推导出节省空间的单目操作符，如“增厚”和“升级”；6) 最终构建一个分级图和分层图谱系的代数类型理论。

Result: 研究结果是建立了一套用于分级图和分层图谱系的代数类型理论。推导出的骨架二元操作符与标准操作符具有相似但非完全相同的代数和范畴论性质。图谱系及其骨架乘积构造器可以逼近连续体极限对象。此外，还推导了空间高效的单目操作符。该方法非常适合定义分层模型架构及在其上的局部采样、搜索或优化算法，并成功应用于深度神经网络（包括视觉和特征尺度空间）和多重网格数值方法。

Conclusion: 该研究提供了一种新的分级图和分层图谱系的代数类型理论，为定义和操作分层模型架构提供了一个强大的、高效的框架。这种方法有望在机器学习和计算科学等领域中用于构建多尺度模型和算法。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [38] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: 提出SU-ESRGAN，首个为卫星图像设计的超分辨率框架，通过整合ESRGAN、DeepLabv3和Monte Carlo dropout，解决语义一致性和像素级不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的超分辨率方法缺乏语义一致性和像素级置信度，这限制了它们在灾害响应、城市规划和农业等关键遥感应用中的可信度。

Method: 提出语义和不确定性感知ESRGAN (SU-ESRGAN)，它整合了ESRGAN、通过DeepLabv3实现分割损失以保留类别细节，以及Monte Carlo dropout以生成像素级不确定性图。该模型采用模块化设计，并针对跨领域应用在无人机数据集上进行了微调。

Result: SU-ESRGAN在航空图像上产生了与基线ESRGAN相当的超分辨率结果（PSNR、SSIM、LPIPS）。微调模型显示出对成像特征与训练数据一致的数据集更强的适应性，突出了领域感知训练的重要性。

Conclusion: SU-ESRGAN对于使用广角相机以牺牲空间分辨率换取覆盖范围的卫星系统或无人机具有重要价值，能够增强因运动模糊、压缩和传感器限制导致的图像质量。在超分辨率应用中，领域感知训练至关重要。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [39] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: 该论文提出一种新颖的真实人格识别（RPR）方法，通过从短时音视频行为中模拟个性化内部认知，并利用2D图神经网络（2D-GNN）来推断真实人格特质，旨在克服现有方法仅基于外部观察者印象的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的人格识别方法通常仅作为外部观察者，根据目标个体的表达行为推断观察者的人格印象，这与个体的真实人格存在显著偏差，并导致识别性能不佳。本研究受到真实人格与生成表达行为的内部认知之间关联的启发。

Method: 该方法通过以下步骤实现：1) 从易于获取的短时音视频行为中模拟个性化内部认知，将其表示为一套网络权重，能够重现个体特有的面部反应。2) 将模拟的个性化认知编码为一种新型图，该图包含二维节点和边特征矩阵。3) 提出一种新型2D图神经网络（2D-GNN）从该图中推断真实人格特质。4) 设计端到端训练策略，联合训练认知模拟、2D图构建和人格识别模块。

Result: 论文提出了一种旨在高效模拟个性化内部认知并更准确地推断真实人格特质的新型RPR方法，克服了传统方法仅推断观察者印象的局限性。具体结果（如性能指标）未在摘要中给出。

Conclusion: 通过模拟与真实人格相关的内部认知并结合创新的2D图神经网络，该方法为实现更准确的真实人格识别提供了一条有潜力的新途径。

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [40] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx是一种参数高效的方法，通过将冻结的CLIP文本嵌入作为语义指导，将文本提示引入到SAM中，以实现语义引导的图像分割。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM在基于提示的分割方面表现出色，但其对语义文本提示的潜力尚未得到充分探索，与传统的点和框等空间提示相比，这部分能力有待挖掘。

Method: 本文提出了SAM-PTx，一种参数高效的SAM适应方法。它使用冻结的CLIP文本嵌入作为类别级语义指导。具体而言，设计了一个轻量级适配器“Parallel-Text”，将文本嵌入注入到SAM的图像编码器中，同时保持原始架构大部分冻结。该适配器仅修改每个Transformer块的MLP并行分支，保留注意力路径用于空间推理。

Result: 通过在COD10K数据集以及COCO和ADE20K的低数据子集上进行监督实验和消融研究，结果表明，将固定文本嵌入作为输入可以提高分割性能，优于纯空间提示基线。这是首次在COD10K数据集上使用文本提示进行分割的工作。

Conclusion: 将语义条件（文本提示）集成到SAM架构中，为高效适应提供了一条实用且可扩展的路径，且计算复杂度极低。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [41] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 在少样本图像分类中，通过引入目标局部位置信息，显著提升了分类性能，即使是利用SAM模型仅提供一个像素点或无监督前景提取方法也能实现显著改进。


<details>
  <summary>Details</summary>
Motivation: 少样本图像分类中，图像歧义（多目标或复杂背景）会严重降低性能。

Method: 研究通过以下方式引入目标局部定位信息：1) 结合目标在图像中的局部位置信息；2) 使用Segment Anything Model (SAM) 仅需指定目标一个像素点；3) 采用完全无监督的前景目标提取方法。

Result: 在现有基准测试中，分类性能显著提升；其中很大一部分改进可通过使用SAM或无监督前景提取方法实现。

Conclusion: 在少样本图像分类中，目标局部位置信息对性能提升至关重要，且可通过少量人工标注（如SAM）或无监督方法有效获取。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [42] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSF-UM的多尺度融合U型Mamba模型，用于引导式深度图超分辨率，该模型结合了Mamba的高效状态空间建模能力、U型多尺度融合结构以及彩色图像引导，实现了更优的重建精度和更少的参数量。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积神经网络在处理长距离依赖和全局上下文信息方面存在局限性。Transformer虽然能建模全局依赖，但其二次方的计算复杂度和内存消耗限制了其处理高分辨率深度图的能力。

Method: 提出了一种多尺度融合U型Mamba (MSF-UM) 模型，这是一个由彩色图像引导的新型深度图超分辨率框架。核心创新是将Mamba的高效状态空间建模能力集成到多尺度U型融合结构中。设计了结合残差密集通道注意力块（用于局部特征提取）和Mamba状态空间模块（用于长距离依赖建模）的结构。同时，模型采用多尺度跨模态融合策略，利用彩色图像的高频纹理信息指导深度图的超分辨率过程。

Result: 与现有主流方法相比，所提出的MSF-UM模型在实现更好重建精度的同时，显著减少了模型参数。在多个公开数据集上的广泛实验验证了模型的有效性，尤其在大尺度深度图超分辨率任务中展现出卓越的泛化能力。

Conclusion: MSF-UM模型有效解决了深度图超分辨率中长距离依赖建模和计算效率的挑战，通过结合Mamba、U型结构和多模态融合，实现了高性能和高效率的深度图重建，并具有出色的泛化能力。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [43] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: 本文提出PointGauss框架，利用点云引导实现高斯泼溅表示中的实时多目标分割，并引入了新的多目标3D分割数据集DesktopObjects-360。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高斯泼溅中的3D分割存在初始化时间过长和多视角一致性差的问题。同时，当前的基准数据集在单目标、3D评估不一致、规模小和覆盖不全等方面存在局限性。

Method: PointGauss通过点云分割驱动的管道直接解析高斯基元，实现高效3D分割。核心创新包括：1) 基于点云的高斯基元解码器，可在1分钟内生成3D实例掩码；2) GPU加速的2D掩码渲染系统，确保多视角一致性。此外，本文还提出了DesktopObjects-360数据集，用于辐射场中的3D分割评估。

Result: PointGauss在多视角mIoU上比现有SOTA方法提升了1.89%至31.78%，并保持了卓越的计算效率。DesktopObjects-360数据集具有复杂多目标场景、全局一致2D标注、大规模训练数据（超过2.7万个2D掩码）、360度全覆盖和3D评估掩码等特点。

Conclusion: PointGauss成功解决了高斯泼溅中实时多目标分割的挑战，实现了高效且多视角一致的3D分割。同时，DesktopObjects-360数据集的引入为辐射场中的3D分割研究提供了更全面和大规模的基准。

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [44] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新的持续学习框架，用于预训练的视觉-语言模型（VLMs），通过引入基于指令的视觉投影器混合专家系统，解决现有方法中语言指令被忽视的问题，并利用专家推荐和剪枝策略提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在更新视觉投影器时，可能导致预训练的视觉-语言模型（VLMs）在处理新任务时，过度优先处理视觉输入而忽视语言指令，尤其是在文本指令类型重复的任务中。因此，需要解决这种对语言指令的忽视。

Method: 本文提出了一种新颖的框架，将视觉信息的翻译过程基于语言模型指令进行。具体方法包括：1) 引入视觉投影器混合专家系统，每个专家根据给定的指令上下文作为专门的视觉到语言翻译专家；2) 提出专家推荐策略，以复用与先前学习任务相似的专家，避免使用不相关的专家；3) 引入专家剪枝机制，以减轻在先前任务中累积激活的专家所造成的干扰。

Result: 在多种视觉-语言任务上进行的广泛实验表明，本文提出的方法在生成遵循指令的响应方面，优于现有的持续学习方法。

Conclusion: 该研究成功解决了持续学习中视觉-语言模型对语言指令的忽视问题，通过创新的指令接地视觉投影器混合专家系统，显著提升了模型在遵循指令方面的性能。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [45] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该论文对多模态指代分割进行了全面综述，涵盖其定义、数据集、统一架构、图像/视频/3D场景方法、广义指代表达以及相关应用。


<details>
  <summary>Details</summary>
Motivation: 多模态指代分割在需要根据用户指令进行精确物体感知的实际应用中至关重要。过去十年，随着卷积神经网络、Transformer和大型语言模型的发展，其在多模态感知能力上取得显著进步，从而引起了多模态社区的广泛关注。

Method: 本文首先介绍了多模态指代分割的背景、问题定义和常用数据集。接着，总结了一个统一的元架构，并回顾了图像、视频和3D场景三种主要视觉场景中的代表性方法。此外，还讨论了处理现实世界复杂性的广义指代表达（GREx）方法、相关任务和实际应用，并提供了标准基准上的性能比较。

Result: 该综述提供了多模态指代分割领域的全面概述，包括其背景、问题定义、常用数据集、统一元架构、针对不同视觉场景（图像、视频、3D）的代表性方法、广义指代表达方法、相关任务、实际应用以及在标准基准上的广泛性能比较。

Conclusion: 该论文对多模态指代分割进行了全面的调查和总结，为理解该领域的发展、方法和挑战提供了宝贵的资源，并展望了未来的研究方向和实际应用。

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [46] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 本文建立了一个新基准来评估语义对应在恶劣条件下的鲁棒性，发现现有方法性能下降，大型模型可提高鲁棒性但微调会降低相对鲁棒性，且通用数据增强无效。


<details>
  <summary>Details</summary>
Motivation: 语义对应在受控和高质量条件下表现出色，但在具有挑战性的场景中其鲁棒性研究不足，而这些场景在实际应用中普遍存在。

Method: 建立了一个包含14种恶劣场景（如几何失真、图像模糊、数字伪影、环境遮挡）的新基准数据集，并在此基础上评估了现有语义对应方法、大型视觉模型（DINO、Stable Diffusion）以及常见的鲁棒性增强策略（如通用数据增强）。

Result: 1) 所有现有方法在恶劣条件下性能均显著下降；2) 使用大型视觉模型可增强整体鲁棒性，但对其进行微调会导致相对鲁棒性下降；3) DINO模型在相对鲁棒性上优于Stable Diffusion，且两者融合能实现更好的绝对鲁棒性；4) 通用数据增强对语义对应鲁棒性增强无效，表明需要任务特定的设计。

Conclusion: 现有语义对应方法在恶劣条件下鲁棒性不足。大型视觉模型有助于提高鲁棒性，但需谨慎处理微调问题。为了有效提升鲁棒性，需要开发任务特定的增强策略，而非依赖通用数据增强。

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [47] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 该论文提出了一种可控的多视角行人视频编辑框架，通过整合视频修复和人体运动控制技术，实现行人插入、替换和移除，以增强自动驾驶模型对危险行人场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的行人检测模型因训练数据中缺乏危险行人场景的充分表示而缺乏鲁棒性。

Method: 该方法结合了视频修复和人体运动控制技术。首先识别多摄像头视图中的行人区域，扩展并拼接这些区域到统一画布，同时保留跨视图空间关系。然后应用二值掩码指定可编辑区域，并通过姿态序列控制条件引导行人编辑，支持行人插入、替换和移除。

Result: 实验证明，该框架实现了高质量的行人编辑，具有强大的视觉真实感、时空连贯性和跨视图一致性。

Conclusion: 该方法是多视角行人视频生成的鲁棒且多功能的解决方案，在自动驾驶的数据增强和场景模拟方面具有广泛的应用潜力。

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [48] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: 该论文提出了一个新颖的疲劳驾驶检测框架，该框架结合了空间自注意力（SSA）机制与长短期记忆（LSTM）网络，并利用梯度相似性比较（GSC）进行联邦学习，以有效处理异构和去中心化的面部数据，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是交通事故的主要原因之一，准确检测疲劳驾驶在真实世界中面临挑战，因为面部数据是去中心化且高度多样化的。

Method: 该研究开发了一个新的空间自注意力（SSA）机制，并将其与长短期记忆（LSTM）网络集成，以更好地提取面部特征。为支持联邦学习，采用了梯度相似性比较（GSC）来选择最相关的训练模型进行聚合。此外，还开发了一个定制工具用于视频数据处理，包括帧提取、人脸检测裁剪和数据增强（旋转、翻转、亮度调整、缩放）。

Result: 在联邦学习设置下，该框架的检测准确率达到89.9%，在各种部署场景下均优于现有方法。

Conclusion: 该方法能够有效处理真实世界数据变异性，并有望部署于智能交通系统，通过早期可靠的疲劳检测来提高道路安全性。

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [49] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: 针对聚光太阳能发电厂（CSP）无人机图像检测中真实数据稀缺和标注困难的问题，本文提出了一个名为AerialCSP的虚拟合成数据集，用于预训练模型，显著提高了故障检测能力并减少了对人工标注的需求。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在处理聚光太阳能发电厂（CSP）的无人机图像时表现不佳，因为这些图像包含高反射表面和独特的领域特定元素，与通用数据集差异大。同时，收集和标注大量此类真实数据成本高昂且耗时，不适用于工业应用的快速部署。

Method: 本文提出并创建了一个名为AerialCSP的虚拟数据集，该数据集通过模拟生成与真实世界CSP工厂无人机图像高度相似的合成数据。研究旨在利用这些合成数据进行模型预训练，以减少对大量真实标注数据的依赖。此外，还在AerialCSP上对多个模型进行了基准测试，为CSP相关的视觉任务建立了基线。

Result: 1. 成功引入了高质量的合成数据集AerialCSP，为CSP工厂的无人机检测任务（包括目标检测和图像分割）提供了标注数据。2. 在AerialCSP上对多个模型进行了基准测试，为CSP相关视觉任务建立了性能基线。3. 实验证明，在AerialCSP上进行预训练能够显著提高真实世界中的故障检测性能，尤其对于稀有和小型缺陷的检测，从而有效减少了对大量人工标注的需求。

Conclusion: 通过创建和利用合成数据集AerialCSP，可以有效解决CSP工厂无人机图像检测中真实数据标注不足的挑战，显著提升模型在实际应用中对故障（特别是稀有和小型缺陷）的检测能力，并降低了部署成本。

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [50] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出了TITAN-Guide，一个针对文生视频（T2V）扩散模型的训练无关指导框架，它通过高效的潜空间优化（利用前向梯度下降）解决了现有方法内存占用大和控制次优的问题，显著提升了T2V性能。


<details>
  <summary>Details</summary>
Motivation: 当前的条件扩散模型在特定任务控制上仍需大量监督微调，而训练无关的指导方法（如基于现成模型的引导）虽然避免了微调，但存在内存开销大或控制精度不足的问题。这些缺点限制了其在计算密集型扩散模型（如文生视频T2V）上的应用。

Method: 本文提出了TITAN-Guide框架。其核心方法是开发了一种高效的潜在空间优化技术，能够在不依赖判别性引导模型反向传播的情况下进行优化。具体而言，它研究并采用了前向梯度下降（forward gradient descents）来执行引导扩散任务，并探索了不同的方向指令选项。

Result: 实验证明，TITAN-Guide在潜在空间优化过程中能有效管理内存，克服了现有方法的不足。它不仅最大程度地减少了内存需求，还在一系列扩散引导基准测试中显著提升了文生视频（T2V）的性能。

Conclusion: TITAN-Guide成功解决了训练无关的文生视频扩散模型在控制过程中面临的内存限制和次优控制问题，通过创新的无反向传播潜空间优化方法，实现了高效且高质量的视频生成控制。

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [51] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 该研究提出一种上下文感知的运动检索框架，通过将人体运动和视频帧编码到与自然语言对齐的共享多模态嵌入空间，以文本查询方式高效检索自动驾驶数据集中罕见的人类行为边缘案例，并引入了新的WayMoCo数据集。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在涉及弱势道路使用者（VRU）异常行为的安全关键场景中必须可靠运行。在大规模数据集中识别这些罕见的人类行为“边缘案例”对于系统鲁棒性评估和泛化至关重要，但从长尾数据中检索这些案例极具挑战性。

Method: 提出一种新颖的上下文感知运动检索框架。该方法结合基于SMPL（Skinned Multi-Person Linear）的运动序列和相应的视频帧，将其编码到一个与自然语言对齐的共享多模态嵌入空间中。这使得可以通过文本查询可扩展地检索人类行为及其上下文。此外，工作还引入了WayMoCo数据集，它是Waymo Open Dataset的扩展，包含从生成的伪真值SMPL序列和相应图像数据中自动标注的运动和场景上下文描述。

Result: 在WayMoCo数据集上评估时，该方法在运动-上下文检索方面比现有最先进模型提高了高达27.5%的准确率。

Conclusion: 所提出的上下文感知运动检索框架和WayMoCo数据集有效支持通过文本查询对人类行为及其上下文进行可扩展检索，从而有助于自动驾驶系统在多样化、以人为中心的场景中进行有针对性的评估。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [52] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: 该研究提出了AniMer+框架，通过一个统一网络实现哺乳动物和鸟类的姿态和形状重建，解决了现有方法容量有限和多物种数据集稀缺的问题，并引入了基于扩散的合成数据生成方法。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，通过单一网络实现对不同动态物体的统一理解，有望增强空间智能。此外，准确估计不同物种的动物姿态和形状对于生物学研究中的定量分析至关重要。然而，由于先前方法的网络容量有限以及缺乏全面的多物种数据集，这一主题尚未得到充分探索。

Method: 引入了AniMer+，这是可扩展AniMer框架的扩展版本，专注于哺乳动物（mammalia）和鸟类（aves）的统一重建。关键创新是其高容量、家族感知的Vision Transformer（ViT），结合了专家混合（MoE）设计，将网络层划分为类群特定（哺乳动物和鸟类）和类群共享组件。为解决3D训练数据短缺，特别是鸟类数据，引入了基于扩散的条件图像生成管道，生成了两个大规模合成数据集：CtrlAni3D（四足动物）和CtrlAVES3D（鸟类）。CtrlAVES3D是首个大规模、3D标注的鸟类数据集。模型在包含41.3k哺乳动物和12.4k鸟类图像（真实和合成数据）的聚合数据集上进行训练。

Result: AniMer+在各种基准测试中，包括具有挑战性的域外Animal Kingdom数据集上，均表现出优于现有方法的性能。消融研究证实了其新颖网络架构和生成的合成数据集在增强实际应用性能方面的有效性。

Conclusion: AniMer+框架及其创新的网络架构和合成数据集生成方法，显著推进了统一的动物姿态和形状重建，特别是针对鸟类，从而增强了空间智能并促进了生物学分析。

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [53] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出IGL-Nav，一个增量式3D高斯定位框架，用于高效且3D感知的图像目标导航，通过逐步更新场景表示、粗略几何匹配和精细可微分渲染优化实现。


<details>
  <summary>Details</summary>
Motivation: 传统的图像目标视觉导航方法（端到端强化学习或基于模块的策略）无法充分建模探索到的3D环境与目标图像之间的几何关系。直接利用3D高斯（3DGS）进行图像定位在代理探索过程中效率极低，因为3DGS优化计算强度大且6自由度相机姿态搜索空间广阔。

Method: 本文提出IGL-Nav框架。首先，随着新图像的到来，通过前馈单目预测增量更新场景的3D高斯表示。其次，通过利用几何信息进行离散空间匹配（等同于高效的3D卷积）来粗略定位目标。最后，当代理接近目标时，通过可微分渲染优化来求解精细的目标姿态。

Result: IGL-Nav在多种实验配置下大幅超越现有最先进的方法。它还能处理更具挑战性的自由视角图像目标设置，并可部署在真实世界的机器人平台上，使用手机以任意姿态捕获目标图像。

Conclusion: IGL-Nav提供了一个高效、准确且3D感知的图像目标导航解决方案，克服了传统方法在几何建模和3DGS直接应用效率上的局限性，并展现出优越的性能和实际部署潜力。

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>


### [54] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 该论文提出一种利用事件相机进行低光图像增强的新方法，通过将增强流程解耦为可见性恢复和结构细化两个阶段，并设计了相应的网络和融合策略，显著提升了低光图像增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件的图像增强方法未能充分利用事件相机和传统帧相机各自模态的优势，直接将两种数据输入单一模型，限制了性能。事件相机在高动态范围和低延迟方面具有优势，能捕获丰富结构信息。

Method: 1. 将图像增强流程解耦为两个阶段：可见性恢复和结构细化。2. 在第一阶段，设计了一个具有幅度-相位纠缠的可见性恢复网络。3. 在第二阶段，提出一种具有动态对齐的融合策略，以缓解两种传感模态之间时间分辨率差异引起的空间不匹配，从而细化图像结构信息。4. 利用空间频率插值模拟具有不同光照、噪声和伪影退化的负样本，开发了一种对比损失，以促使模型学习判别性表示。

Result: 实验结果表明，所提出的方法优于现有的最先进模型。

Conclusion: 通过深入分析每种传感模态的作用，并将增强流程解耦，并辅以创新的网络设计和损失函数，能够有效利用事件相机数据进行低光图像增强，实现卓越的性能。

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [55] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: DocTron-Formula是一个基于通用视觉语言模型的统一框架，结合大规模复杂数学公式数据集CSFormula，通过微调在数学公式OCR任务上实现了最先进的性能，超越了专用模型。


<details>
  <summary>Details</summary>
Motivation: 数学公式的光学字符识别（OCR）对于科学文献的智能分析至关重要。然而，现有的任务特定模型和通用视觉语言模型在处理数学内容的结构多样性、复杂性和真实世界变异性方面表现不佳。

Method: 提出了DocTron-Formula，一个基于通用视觉语言模型的统一框架，无需专门架构。同时，引入了CSFormula，一个大规模、具有挑战性的数据集，包含多学科、结构复杂的行、段落和页面级公式。通过直接的监督微调来实现模型训练。

Result: 该方法在各种风格、科学领域和复杂布局上均实现了最先进的性能。实验结果表明，该方法不仅在准确性和鲁棒性方面超越了专用模型。

Conclusion: 该研究为复杂科学文档的自动化理解建立了一个新范式，证明了通用视觉语言模型结合大规模高质量数据集在数学公式OCR领域的潜力。

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [56] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 本文提出GV-VAD框架，利用文本条件视频生成模型合成视频来扩充视频异常检测数据集，并结合损失缩放策略优化训练，有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频异常数据稀有、不可预测且标注成本高昂，导致视频异常检测（VAD）数据集难以扩展，从而限制了现有模型的性能和泛化能力。

Method: 提出了一种生成式视频增强弱监督视频异常检测（GV-VAD）框架。该框架利用文本条件视频生成模型来生成语义可控且物理合理的合成视频，以低成本扩充训练数据。此外，还采用了一种合成样本损失缩放策略，用于控制生成样本的影响，以实现高效训练。

Result: 实验结果表明，所提出的框架在UCF-Crime数据集上优于现有最先进的方法。

Conclusion: 通过结合生成式视频增强和弱监督学习，该框架有效解决了视频异常检测中数据稀缺的挑战，显著提升了模型的性能和泛化能力。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [57] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出个性化引导方法，通过利用一个未学习的弱模型和动态权重插值，解决了少量图像微调扩散模型时，目标一致性与文本可编辑性之间的权衡问题，提高了生成图像的文本对齐性和目标分布保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在少量图像微调时，面临目标概念一致性（如主体保真度）与模型通用知识（如文本可编辑性）之间的固有权衡。现有采样引导方法（如CFG和AG）未能有效平衡这两者，CFG限制了目标分布适应性，而AG则损害了文本对齐。

Method: 提出“个性化引导”方法，该方法利用一个以空文本提示为条件的未学习弱模型。此外，通过在推理过程中对预训练模型和微调模型进行权重插值，动态控制弱模型的“未学习”程度，从而显式地将输出引导至平衡的潜在空间。

Result: 实验结果表明，所提出的引导方法可以提高文本对齐性和目标分布保真度，并能与各种微调策略无缝集成，且不增加额外计算开销。

Conclusion: 本文提出的个性化引导方法有效解决了少量图像微调扩散模型时，目标概念一致性与模型通用知识之间的权衡问题，在不增加计算成本的前提下，显著提升了生成图像的文本对齐性和目标分布保真度。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [58] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 本文提出了一种使用衍射光栅对相机光谱灵敏度进行校准的实用且精确的方法。


<details>
  <summary>Details</summary>
Motivation: 相机光谱灵敏度的精确校准对于颜色校正、光照估计和材料分析等多种计算机视觉任务至关重要。现有方法需要专门的窄带滤光片或已知光谱反射率的参考目标，操作复杂。

Method: 该方法仅需一个未校准的市售衍射光栅片。通过捕获直接照明及其通过光栅片衍射图案的图像，以闭合形式估计相机光谱灵敏度和衍射光栅参数。

Result: 在合成数据和真实世界数据上的实验表明，该方法优于传统的基于参考目标的方法。

Conclusion: 该方法有效且实用，能够准确校准相机光谱灵敏度。

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [59] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 该论文提出了一个协作式代理框架，通过PromptEngineer生成提示并由大型视觉-语言模型（LVLM）进行推理，以实现多图像推理，该框架无需训练，并在多种视觉推理任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究面临在多样数据集和任务格式下进行交错多模态推理的挑战，需要一个能够有效处理多图像输入的通用推理框架。

Method: 该方法采用双代理系统：一个基于语言的PromptEngineer，负责生成上下文感知和任务特定的提示；一个VisionReasoner，即一个大型视觉-语言模型（LVLM），负责最终的推理。该框架是全自动、模块化且无需训练的，能泛化到涉及单张或多张输入图像的分类、问答和自由生成任务。

Result: 研究结果表明，在信息丰富提示的引导下，LVLM能有效进行多图像推理。在2025 MIRAGE挑战赛（Track A）的18个多样数据集上进行评估，Claude 3.7在TQA（99.13%准确率）、DocVQA（96.87%）和MMCoQA（75.28 ROUGE-L）等挑战性任务上取得了接近上限的性能。此外，还探讨了模型选择、少样本数量和输入长度等设计选择对不同LVLM推理性能的影响。

Conclusion: LVLM在信息丰富的提示引导下，能够有效地进行多图像推理，并在广泛的视觉推理任务中表现出色。所提出的协作式代理框架为处理多图像推理提供了一个有效、无需训练且具有良好泛化能力的解决方案。

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [60] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SG-LKF（Speed-Guided Learnable Kalman Filter）的多目标跟踪（MOT）方法，通过动态适应自车速度来建模不确定性，显著提高了高动态场景下的跟踪稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的多目标跟踪方法通常依赖于基于自车姿态的静态坐标变换，忽略了自车速度引起的观测噪声变化和参考系转换，这导致在动态、高速场景下跟踪稳定性与准确性下降。

Method: 该研究提出SG-LKF，它能动态地根据自车速度调整不确定性建模。SG-LKF的核心是MotionScaleNet（MSNet），一个解耦的token-mixing和channel-mixing MLP，用于自适应地预测SG-LKF的关键参数。此外，引入了一种自监督轨迹一致性损失，与语义和位置约束共同优化，以增强帧间关联和轨迹连续性。

Result: 实验结果显示，SG-LKF在KITTI 2D MOT上以79.59% HOTA排名所有视觉方法第一，在KITTI 3D MOT上取得了82.03% HOTA的优异结果，并在nuScenes 3D MOT上比SimpleTrack的AMOTA高出2.2%。

Conclusion: SG-LKF通过考虑并动态适应自车速度对观测不确定性的影响，显著提升了多目标跟踪在高速动态场景下的稳定性和准确性，并在多个基准测试中取得了领先性能。

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [61] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: 本文提出CoST，一种高效协同感知框架，将多智能体和多时间观测统一到时空域同时融合，提高感知精度并减少数据传输。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法将多智能体融合和多时间融合分开处理，导致效率低下（重复传输）和融合效果不佳。此外，单个智能体面临遮挡和感知范围有限等问题。

Method: 本文提出CoST（Collaborative perception with Spatio-temporal Transformer），将来自不同智能体（空间）和不同时间（时间）的观测同时聚合到一个统一的时空域。这种方法带来两个优势：1）高效特征传输：每个静态物体在时空域中只需一次观测，因此只需传输一次；2）优越特征融合：将多智能体和多时间融合合并为统一的时空聚合，提供更全面的视角。

Result: CoST在效率（减少传输带宽）和准确性（在挑战性场景中增强感知性能）方面均有所提升。CoST不局限于特定方法，兼容大多数现有方法，在提高其准确性的同时降低传输带宽。

Conclusion: 统一的时空聚合是协同感知的有效方法，能显著提升效率和准确性，并且具有广泛的兼容性，可应用于多数现有方法。

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [62] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本文提出“Adapt-WeldNet”自适应框架，系统优化焊缝缺陷检测模型性能；并引入“缺陷检测可解释性分析 (DDIA)”框架，结合可解释AI和专家验证，提升系统透明度、可信度和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统无损检测方法难以发现细微或内部焊缝缺陷，导致潜在故障和停机；现有基于神经网络的缺陷分类方法依赖任意选择的预训练架构且缺乏可解释性，存在部署安全隐患。

Method: 1. 提出“Adapt-WeldNet”自适应框架，系统评估各种预训练架构、迁移学习策略和自适应优化器，以识别最佳模型和超参数，优化缺陷检测。2. 提出“缺陷检测可解释性分析 (DDIA)”框架，利用Grad-CAM和LIME等可解释人工智能 (XAI) 技术，并结合ASNT NDE二级专业人员的领域特定验证。3. 整合“人在回路 (HITL)”方法，遵循可信AI原则，确保系统可靠性、公平性和问责制。

Result: 通过系统优化模型和引入可解释性分析，提升了焊缝缺陷检测系统的性能和可解释性，增强了系统的信任度、安全性和可靠性。

Conclusion: 本工作通过提高性能和可解释性，增强了焊缝缺陷检测系统的信任度、安全性和可靠性，为海上和海洋环境中的关键作业提供了支持。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [63] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的蜂蜜植物来源自动分类方法，利用高光谱图像数据，通过数据准备、LDA特征提取和SVM/KNN分类，取得了最先进的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种自动分类蜂蜜植物来源的方法，以解决蜂蜜溯源和质量控制的需求。

Method: 该方法包含三个主要步骤：1) 数据集准备阶段，使用类转换方法最大化类别可分离性；2) 特征提取阶段，采用线性判别分析（LDA）提取相关特征并降维；3) 分类阶段，使用支持向量机（SVM）和K-近邻（KNN）模型对提取的特征进行分类。系统使用标准蜂蜜高光谱成像（HSI）数据集进行评估。

Result: 实验结果表明，该系统在高光谱图像分类中实现了95.13%的最高分类准确率，在高光谱实例分类中实现了92.80%的最高分类准确率，在该数据集上取得了最先进的成果。

Conclusion: 所提出的机器学习系统能够有效且高精度地自动分类蜂蜜的植物来源，并在高光谱数据集上达到了最先进的性能水平。

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [64] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 针对现有病理学视觉基础模型（VFMs）在基因表达预测中表现不佳的问题，本文提出了一种结合状态空间模型（SSM）和Vision Transformer（ViT）的混合骨干网络$MV_{Hybrid}$，并在实验中验证了其在性能和鲁棒性上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学在精准肿瘤学中有重要应用，但其高成本和技术复杂性限制了临床普及。从常规组织病理学图像预测空间基因表达是一种实用的替代方案，然而，当前基于Vision Transformer（ViT）的病理学视觉基础模型（VFMs）性能尚未达到临床标准，难以有效捕捉与分子表型相关的低频、细微形态模式。

Method: 本文提出$MV_{Hybrid}$，一种结合状态空间模型（SSM）与Vision Transformer（ViT）的混合骨干架构，利用了状态空间模型在负实特征值初始化时表现出的强低频偏差特性。研究人员将$MV_{Hybrid}$与其他五种不同的病理学VFM骨干架构进行比较，所有模型均使用DINOv2自监督学习方法在相同的结直肠癌数据集上进行预训练。模型性能在生物标志物数据集上通过随机划分和留一研究出（LOSO）两种设置进行评估。

Result: 在留一研究出（LOSO）评估中，$MV_{Hybrid}$在基因表达预测方面的相关性比表现最佳的ViT高出57%，且相较于随机划分，性能下降幅度减少了43%，分别展现出卓越的性能和鲁棒性。此外，$MV_{Hybrid}$在分类、图像块检索和生存预测等下游任务中也表现出与ViT相当或更优的性能。

Conclusion: $MV_{Hybrid}$作为一种结合了状态空间模型和Vision Transformer的混合骨干架构，在从组织病理学图像预测空间基因表达方面表现出优越的性能和鲁棒性，有望成为下一代病理学视觉基础模型的重要骨干。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [65] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon是一种新的神经隐式重建方法，通过基于体渲染的特征一致性损失和不确定性引导的深度约束，解决了稀疏视图表面重建中现有方法的局限性，实现了高质量几何重建。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏视图表面重建方法（基于泛化或基于过拟合）在未见过的视图上泛化能力差，或受限于有限的几何线索导致重建质量不高。

Method: 本文提出SparseRecon，一种神经隐式重建方法。主要包括两点：1) 引入跨视图的特征一致性损失，以消除视图一致性信息不足造成的歧义，确保重建的完整性和平滑性。2) 采用不确定性引导的深度约束，在遮挡和特征不明显的区域辅助特征一致性损失，恢复几何细节。

Result: 实验结果表明，该方法优于现有最先进的方法，能够从稀疏视图输入生成高质量的几何形状，特别是在视图重叠度较小的场景中表现更佳。

Conclusion: SparseRecon通过结合特征一致性和不确定性引导的深度约束，有效提高了稀疏视图下的表面重建质量，解决了现有方法在泛化性和几何细节恢复方面的不足。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [66] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出DAPT，一个基于“先解耦后对齐”概念的提示微调框架，旨在解决视觉-语言模型中提示微调的信息不对称问题，通过显式解耦视觉模态并增强对称对齐和视觉集中度，实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示微调（PT）存在信息不对称问题，即视觉模态通常比面向对象的文本模态包含更多上下文信息。这种粗略的对齐会导致模型注意力偏差，仅关注上下文区域，从而影响视觉-语言模型的任务特定迁移能力。

Method: 本文提出DAPT框架：1) 显式地将视觉模态解耦为前景和背景表示，利用粗细粒度视觉分割线索。2) 将解耦后的视觉模式与原始前景文本和手工制作的背景类别对称对齐，以加强模态对齐。3) 引入视觉拉推正则化（visual pull-push regularization），专门针对前景-背景模式，引导原始视觉表示对感兴趣区域（ROI）对象进行无偏关注。

Result: DAPT是一个架构无关的框架，在少样本学习、从基类到新类的泛化以及数据高效学习方面均表现出卓越的性能，并在主流基准测试中取得了优异结果。

Conclusion: DAPT成功解决了提示微调中视觉-语言模态的信息不对称问题，通过解耦和增强对齐及视觉集中度，显著提升了视觉-语言模型的性能和迁移能力。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [67] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 提出了一种名为“表征偏移”（Representation Shift）的无训练、模型无关的度量方法，用于令牌压缩，使其能与FlashAttention高效集成，从而在不依赖注意力图的情况下实现显著加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在视觉、语言和视频领域取得了巨大成功，但随着任务复杂性增加，模型和令牌数量也随之增长，导致自注意力机制的二次计算成本和GPU内存访问开销巨大。现有令牌压缩方法通常依赖注意力图来判断令牌重要性，这与避免构建注意力图的FlashAttention等融合注意力核不兼容。

Method: 本文提出“表征偏移”作为一种无训练、模型无关的度量标准，用于衡量每个令牌表征的变化程度。该方法无需注意力图或重新训练，即可将令牌压缩与FlashAttention无缝集成。此外，它还能推广到CNN和状态空间模型。

Result: 大量实验表明，“表征偏移”能实现与FlashAttention兼容的有效令牌压缩，在视频-文本检索和视频问答任务中分别带来了高达5.5%和4.4%的显著加速。

Conclusion: “表征偏移”成功地将令牌压缩与FlashAttention结合，有效解决了计算成本和内存开销问题，并在多模态任务中展现出显著的加速效果和广泛的适用性。

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [68] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5 是一种新型深度压缩自编码器，通过结构化潜在空间和增强扩散训练，解决了高分辨率扩散模型中增加潜在通道导致的收敛慢和生成质量下降问题，实现了更快的收敛和更好的生成质量。


<details>
  <summary>Details</summary>
Motivation: 动机是高分辨率扩散模型中，虽然增加自编码器的潜在通道能提高重建质量，但会导致扩散模型收敛缓慢，反而降低生成质量。这限制了潜在扩散模型的质量上限，并阻碍了高空间压缩比自编码器的应用。

Method: 本文引入了两项关键创新：i) 结构化潜在空间（Structured Latent Space），一种基于训练的方法，将潜在空间强制划分为捕获物体结构的前部通道和捕获图像细节的后部通道；ii) 增强扩散训练（Augmented Diffusion Training），一种额外的扩散训练策略，对物体潜在通道施加额外的扩散训练目标以加速收敛。

Result: 通过这些技术，DC-AE 1.5 比 DC-AE 实现了更快的收敛和更好的扩散缩放结果。在 ImageNet 512x512 数据集上，DC-AE-1.5-f64c128 提供了比 DC-AE-f32c32 更好的图像生成质量，同时速度快了 4 倍。

Conclusion: DC-AE 1.5 通过其创新方法有效解决了高分辨率扩散模型中潜在通道增加导致的收敛和生成质量问题，显著提升了模型性能和效率，为高分辨率图像生成提供了更优的解决方案。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [69] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt方法结合前向和后向预测，并利用大语言模型，提升了视频长期动作预测的性能，在Ego4D数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的视频长期动作预测方法（基于编码器-解码器）因其单向性而限制了性能，难以捕捉场景中语义不同的子动作，这在自动驾驶和机器人等早期风险检测领域是一个关键问题。

Method: 提出BiAnt方法，通过结合前向预测与后向预测，并利用大语言模型来克服传统单向方法的局限性。

Result: 在Ego4D数据集上的实验结果表明，BiAnt在编辑距离方面相比基线方法有性能提升。

Conclusion: BiAnt通过引入双向预测机制和利用大语言模型，有效解决了传统单向方法在视频长期动作预测中的性能限制和捕捉子动作的挑战。

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [70] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 该研究提出一种新的方法，利用物理先验知识和多区域修复技术，改进了在人机交互（HOI）场景中的无模态补全，以生成更真实、准确的对象补全。


<details>
  <summary>Details</summary>
Motivation: 现有的无模态补全方法，特别是基于预训练扩散模型的方法，在动态人机交互场景中难以生成可信的补全结果，因为它们对人机交互的理解有限。

Method: 该方法结合了物理先验知识（如人体拓扑和接触信息），定义了两个不同的区域：主要区域（最可能被遮挡的对象部分）和次要区域（遮挡可能性较低）。在此基础上，采用了一种专门的多区域修复技术，在扩散模型中对这些区域应用定制的去噪策略。

Result: 实验结果表明，该方法在人机交互场景中显著优于现有方法，提高了生成补全结果的形状和视觉细节的准确性和真实性。此外，该管道在没有真实接触标注的情况下也表现出鲁棒性。

Conclusion: 该方法使机器感知更接近对动态环境的人类理解，并拓宽了其在3D重建和新视角/姿态合成等任务中的应用前景。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [71] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Cued-Agent的多智能体系统，用于自动手语识别（ACSR），通过整合专门的代理来解决手势和唇部动作的时间异步以及数据限制问题，并在正常和听障场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的自动手语识别（ACSR）方法在处理手势和唇部动作之间的时间异步时需要复杂模块，但受限于数据量不足，导致训练效果不佳和性能欠优。多智能体系统在处理数据有限的复杂任务方面展现出潜力。

Method: 提出首个用于ACSR的协作式多智能体系统Cued-Agent，包含四个专门的子代理：基于多模态大语言模型的“手势识别代理”（采用关键帧筛选和CS专家提示策略）、预训练Transformer的“唇部识别代理”、在推理阶段免训练动态整合手势提示和唇部特征的“手势提示解码代理”，以及首次通过语义细化实现音素序列到自然语言句子端到端转换的“自校正音素转词代理”。此外，通过收集八名听障发音者的数据，扩展了现有普通话CS数据集。

Result: Cued-Agent在正常和听障场景下的实验表现均优于现有最先进的方法。

Conclusion: Cued-Agent作为首个用于ACSR的多智能体系统，有效解决了手势和唇部动作的时间异步以及数据限制问题，实现了卓越的识别性能。

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [72] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文提出了TopoTTA，首个专为管状结构分割（TSS）设计的测试时间适应（TTA）框架，旨在解决跨域拓扑差异和局部特征变化导致的性能下降问题，显著提升了分割的拓扑完整性。


<details>
  <summary>Details</summary>
Motivation: 管状结构分割（TSS）在多种应用中非常重要，但领域漂移（domain shifts）是主要挑战，导致在未见目标域中性能下降。与其他分割任务不同，TSS对领域漂移更敏感，因为拓扑结构的变化会损害分割完整性，局部特征（如纹理和对比度）的变化可能进一步破坏拓扑连续性。

Method: 本文提出了TopoTTA，一个两阶段的TSS测试时间适应框架。第一阶段利用所提出的拓扑元差异卷积（TopoMDCs）来适应跨域拓扑差异，在不改变预训练参数的情况下增强拓扑表示。第二阶段通过新颖的拓扑难样本生成（TopoHG）策略和在生成的伪断裂区域中利用伪标签进行难样本预测对齐，从而改善拓扑连续性。

Result: 在四种场景和十个数据集上的大量实验表明，TopoTTA在处理拓扑分布漂移方面非常有效，平均clDice指标提升了31.81%。TopoTTA还可以作为基于CNN的TSS模型的即插即用TTA解决方案。

Conclusion: TopoTTA是第一个专门为管状结构分割设计的测试时间适应框架，它有效地解决了领域漂移导致的拓扑结构和局部特征变化问题，显著提高了分割的拓扑完整性，并可作为现有CNN-based TSS模型的通用解决方案。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [73] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: 该论文提出了一种双分支检测框架，结合RGB外观特征和光流残差，以捕捉AI生成视频中的时空不一致性，从而有效检测各种伪造视频。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成视频的真实性不断提高，现有视频伪造检测方法难以捕捉高视觉保真度和连贯运动的AI生成视频中细粒度的时空不一致性。

Method: 采用双分支架构：一个分支分析RGB帧以检测外观级别伪影，另一个分支处理光流残差以揭示不完美时间合成引起的细微运动异常。通过整合这些互补特征，利用时空一致性进行检测。

Result: 在文本到视频和图像到视频任务中，对十种不同的生成模型进行了广泛实验，结果表明所提出的方法在检测各种伪造视频方面表现出鲁棒性和强大的泛化能力。

Conclusion: 所提出的结合RGB外观特征和光流残差的时空一致性检测框架，能有效且鲁棒地检测AI生成视频，并具有良好的泛化能力。

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [74] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: LesiOnTime是一种新颖的3D分割方法，通过联合利用纵向影像和BI-RADS评分，模仿临床诊断流程，以提高乳腺DCE-MRI中小病灶的准确分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要针对大型病灶，忽略了放射科医生在实际筛查中用于检测细微或新发病灶的宝贵纵向和临床信息（如BI-RADS评分）。准确分割小病灶对于高危患者的早期癌症检测至关重要。

Method: 提出LesiOnTime方法，包含两个关键组件：1) 时间先验注意力（TPA）模块，动态整合来自先前和当前扫描的信息；2) BI-RADS一致性正则化（BCR）损失，强制具有相似放射学评估的扫描在潜在空间中对齐，从而将领域知识嵌入训练过程。

Result: 在自建的高危患者DCE-MRI纵向数据集上进行评估，LesiOnTime在Dice指标上比最先进的单时间点和纵向基线方法高出5%。消融研究表明TPA和BCR都提供了互补的性能增益。

Conclusion: 研究结果强调了在真实世界乳腺癌筛查中，结合时间性和临床背景对于可靠的早期病灶分割的重要性。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [75] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: 本文提出了iSafetyBench，一个用于评估视觉语言模型在工业环境中识别常规操作和安全关键异常的新视频语言基准。现有SOTA模型在该基准上表现不佳，尤其是在危险活动和多标签识别方面，凸显了开发更鲁棒、安全感知模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在零样本视频理解任务中表现出色，但它们在工业领域（需要识别常规操作和安全关键异常）的能力尚未得到充分探索。为了填补这一空白，需要一个专门的基准来评估模型在该高风险环境中的性能。

Method: 研究引入了iSafetyBench，一个包含1100个真实工业视频片段的视频语言基准。这些视频被标注了开放词汇、多标签动作标签，涵盖98个常规和67个危险动作类别。每个片段都配有多项选择题，用于单标签和多标签评估。研究评估了8个最先进的零样本视频语言模型。

Result: 尽管在现有视频基准上表现强劲，这些模型在iSafetyBench上表现不佳，尤其是在识别危险活动和多标签场景中。结果揭示了显著的性能差距。

Conclusion: 当前最先进的视觉语言模型在工业安全场景中存在显著局限性，特别是在识别危险活动和处理多标签任务方面。这强调了开发更鲁棒、更具安全意识的多模态模型以应用于工业领域的必要性。iSafetyBench提供了一个独特的测试平台来推动这一方向的进展。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [76] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: Wukong是一个基于Transformer的NSFW内容检测框架，它在文本到图像（T2I）扩散模型的早期去噪步骤中进行检测，通过利用中间输出和重用U-Net的交叉注意力参数，实现了高效且准确的早期检测，优于文本过滤器并与图像过滤器精度相当。


<details>
  <summary>Details</summary>
Motivation: 当前的T2I生成技术可能产生不安全（NSFW）内容，需要高效准确的检测机制。现有检测方法存在缺陷：文本过滤器忽略模型特异性且易受攻击；图像过滤器计算成本高且引入延迟。因此，需要一种更高效、准确且能早期介入的检测方法。

Method: 研究基于两点观察：1) 早期去噪步骤定义图像的语义布局；2) U-Net中的交叉注意力层对文本和图像区域对齐至关重要。基于此，提出Wukong框架，它是一个基于Transformer的检测器，利用扩散模型早期去噪步骤的中间输出，并重用U-Net预训练的交叉注意力参数。Wukong在扩散过程中运行，实现早期检测。此外，还引入了一个包含提示、种子和图像特定NSFW标签的新数据集用于评估。

Result: Wukong在自建数据集和两个公共基准测试上进行了评估，结果表明它显著优于基于文本的防护措施，并能达到与图像过滤器相当的准确性，同时提供了更高的效率。

Conclusion: Wukong提供了一种在文本到图像生成过程中高效、准确地早期检测NSFW内容的方法，解决了现有检测方案的效率和延迟问题，并在性能上取得了显著提升。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [77] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox是一个高保真、逼真的3D零售店模拟环境，用于衡量具身智能体在购物任务中相对于人类的表现，并提供了SariBench数据集。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体训练环境缺乏零售业特定模拟场景。

Method: 开发了Sari Sandbox，一个包含250多种互动商品、三种店铺配置、支持API控制、VR和VLM具身智能体的3D零售店模拟器；同时构建了SariBench数据集，包含带标注的人类购物任务演示。

Result: Sari Sandbox使具身智能体能够进行导航、检查和操作零售商品，并提供了与人类表现对比的基准；论文还包含了基准测试和性能分析。

Conclusion: 为提升模拟环境的真实感和可扩展性提出了建议。

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [78] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 本文展示了对人脸检测系统进行物体生成攻击（人脸生成攻击）和首次提出的地标偏移攻击的有效性，并提出了相应的缓解措施。


<details>
  <summary>Details</summary>
Motivation: 在非受限环境下运行的人脸识别系统面临光照不一致、姿态多样等挑战，需要强大的人脸检测模块来回归边界框和地标坐标以进行正确的人脸对齐。本文旨在探讨人脸检测模块在这种复杂环境下的潜在漏洞。

Method: 本文提出了两种攻击方法：物体生成攻击（特指人脸生成攻击）和首次提出的地标偏移攻击。地标偏移攻击旨在通过后门方式影响人脸检测器执行的坐标回归任务。此外，论文还提出了针对这些漏洞的缓解措施。

Result: 研究结果表明，物体生成攻击（人脸生成攻击）对人脸检测是有效的。更重要的是，本文首次成功演示了地标偏移攻击，该攻击能够后门人脸检测器执行的地标坐标回归任务。

Conclusion: 人脸检测系统容易受到生成攻击和新型地标偏移攻击的影响，这些攻击可以操纵地标坐标回归。因此，需要采取有效的缓解措施来增强系统的鲁棒性。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [79] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: 该研究针对大气湍流导致的长距离动态视频失真问题，提出了动态效率指数（DEI）用于量化视频动态强度并构建数据集，以及一种物理模型驱动的多阶段视频恢复（PMR）框架，通过去倾斜、运动分割增强和去模糊三阶段处理，有效恢复视频质量并抑制伪影。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复受大气湍流影响的长距离动态场景视频时，难以恢复边缘细节并消除混合失真，尤其是在强湍流和复杂动态条件下表现不佳。

Method: 1. 引入动态效率指数（DEI），结合湍流强度、光流和动态区域比例，精确量化不同湍流条件下的视频动态强度，并构建高动态湍流训练数据集。2. 提出物理模型驱动的多阶段视频恢复（PMR）框架，包括三个阶段：去倾斜（几何稳定）、运动分割增强（动态区域细化）和去模糊（质量恢复）。3. PMR采用轻量级骨干网络和阶段性联合训练，以确保效率和高质量恢复。

Result: 实验结果表明，所提出的方法有效抑制了运动拖影伪影，恢复了边缘细节，并表现出强大的泛化能力，尤其是在高湍流和复杂动态的真实世界场景中效果显著。

Conclusion: 该研究提出的DEI和PMR框架有效解决了大气湍流导致的长距离动态场景视频质量下降问题，实现了高质量和高效率的视频恢复，尤其适用于挑战性的真实世界条件。代码和数据集将开源。

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [80] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为D3的免训练方法，通过分析真实视频与AI生成视频在二阶时间特征分布上的差异，有效检测AI合成视频，并展现出卓越的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 随着Sora等视频生成技术的发展，高保真AI生成视频的制作变得日益容易，引发了对合成内容传播的公众担忧。然而，现有检测方法在探索合成视频中的时间伪影方面存在不足。

Method: 研究者通过牛顿力学下的二阶动力学分析建立了理论框架，并扩展了用于时间伪影检测的二阶中心差分特征。在此理论基础上，他们揭示了真实视频和AI生成视频在二阶特征分布上的根本性差异，并提出了基于这些二阶时间差异的“差分之差检测”（D3）方法，该方法无需训练。

Result: D3方法在4个开源数据集（共40个子集）上验证了其优越性，例如在GenVideo数据集上，D3的平均精度比现有最佳方法高出10.39%。此外，D3在时间成本和后处理操作方面表现出卓越的计算效率和强大的鲁棒性。

Conclusion: D3是一种基于二阶时间差异的创新型、免训练AI生成视频检测方法，它在检测性能、计算效率和鲁棒性方面均优于现有方法，有效弥补了当前检测技术在时间伪影分析上的不足。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [81] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: Sortblock是一种无需训练的推理加速框架，通过动态缓存和选择性跳过冗余计算，使Diffusion Transformers（DiTs）的推理速度提升2倍以上，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers（DiTs）具有出色的生成能力，但其固有的顺序去噪过程导致推理延迟高，限制了在实时场景中的部署。现有无训练加速方法通常重用固定时间步或层的中间特征，忽略了去噪阶段和Transformer块中语义焦点的演变。

Method: 提出Sortblock框架，该框架基于相邻时间步之间块级特征的相似性动态缓存特征。通过对残差演变进行排序，Sortblock自适应地确定重计算比例，选择性地跳过冗余计算。此外，引入轻量级线性预测机制以减少跳过块中累积的误差。

Result: 在各种任务和DiT架构上的大量实验表明，Sortblock实现了超过2倍的推理加速，同时输出质量退化极小。

Conclusion: Sortblock为加速基于扩散的生成模型提供了一个有效且通用的解决方案。

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [82] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 本文探讨在逼真虚拟形象（avatar）场景中，如何利用面部运动模式进行生物识别验证，以应对身份冒充的风险，并提出了一个基于面部关键点和图卷积网络的轻量级验证系统，实现了接近80%的AUC。


<details>
  <summary>Details</summary>
Motivation: 逼真的虚拟形象在虚拟会议、游戏和社交平台中日益普及，但同时也带来了严重的安全风险，特别是身份冒充。攻击者可以窃取用户的虚拟形象（保留其外观和声音），使得仅凭视觉或听觉难以检测欺诈行为。因此，迫切需要探索在虚拟形象介导场景中进行生物识别验证的方法。

Method: 为了解决虚拟形象介导的生物识别验证挑战，本文引入了一个新的真实虚拟形象视频数据集，该数据集使用最先进的单次虚拟形象生成模型GAGAvatar创建，包含真实用户和冒充者的虚拟形象视频。同时，提出了一种轻量级、可解释的时空图卷积网络（GCN）架构，该架构结合了时间注意力池化，仅使用面部关键点来建模动态面部手势。

Result: 实验结果表明，面部运动线索能够实现有意义的身份验证，AUC值接近80%。本文提出的基准和生物识别系统已提供给研究社区。

Conclusion: 面部运动模式可以作为可靠的行为生物特征，在虚拟形象介导的通信系统中验证身份。这凸显了在基于虚拟形象的通信系统中，对更先进的行为生物识别防御措施的迫切需求。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [83] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: 该论文提出了首个基于Mamba的水下实例分割模型UIS-Mamba，通过设计动态树扫描（DTS）和隐藏状态削弱（HSW）模块，有效解决了水下复杂场景中Mamba应用的挑战，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割（UIS）对于复杂水下场景检测至关重要。Mamba模型具有线性复杂度和全局感受野，适用于图像分割任务。然而，水下场景特有的颜色失真和模糊边界使得Mamba的固定补丁扫描机制难以保持实例内部连续性，且复杂背景的隐藏状态会抑制对实例对象的理解。

Method: 提出了首个基于Mamba的水下实例分割模型UIS-Mamba。设计了两个创新模块：1. 动态树扫描（DTS）模块：允许补丁动态偏移和缩放，引导最小生成树，提供动态局部感受野，以保持实例对象内部特征的连续性。2. 隐藏状态削弱（HSW）模块：通过基于Ncut的隐藏状态削弱机制，抑制复杂背景的干扰，有效将状态传播的信息流聚焦到实例本身。

Result: 实验结果表明，UIS-Mamba在UIIS和USIS10K数据集上均实现了最先进的性能，同时保持了较低的参数量和计算复杂度。

Conclusion: 该研究成功将Mamba模型应用于水下实例分割任务，通过创新的DTS和HSW模块克服了水下场景的特定挑战，显著提升了分割性能，并验证了Mamba在处理水下复杂图像特征方面的潜力。

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [84] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 本文提出一种新颖的测试时自适应（TTA）框架，用于医学图像到图像的翻译，通过动态调整以应对分布外（OOD）样本，同时保持对分布内样本的性能。


<details>
  <summary>Details</summary>
Motivation: 图像到图像的翻译在处理分布外（OOD）样本时性能会下降，这是其在医学成像应用中的一个主要限制。

Method: 引入一个重建模块来量化域偏移，并设计一个动态自适应块，选择性地修改预训练翻译模型的内部特征，以减轻域偏移。这种方法旨在仅对需要自适应的样本进行调整，避免对分布内样本造成性能损害。

Result: 在低剂量CT去噪和T1到T2 MRI翻译两项医学图像翻译任务中，该方法均比没有TTA的基线模型和先前的TTA方法表现出持续的改进。研究表明，动态、样本特异性的调整比统一自适应方法更有效。

Conclusion: 动态、样本特异性的测试时自适应是提高医学图像到图像翻译模型在真实世界场景中鲁棒性的有效途径，优于对所有样本统一应用自适应的现有方法。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [85] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: SDMatte是一种新的扩散模型驱动的交互式抠图方法，它利用扩散模型的强大先验和纹理细节生成能力，通过视觉提示交互和改进的U-Net架构，显著提升了抠图在边缘细节上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有交互式抠图方法在提取物体主要区域表现良好，但在处理边缘区域的精细细节时效果不佳。扩散模型在建模复杂数据分布、合成真实纹理细节以及强大的文本驱动交互能力方面表现出色，因此被认为是解决交互式抠图挑战的有效方案。

Method: 1. 将扩散模型的文本驱动交互能力转换为视觉提示驱动的交互能力，以实现交互式抠图。2. 将视觉提示的坐标嵌入和目标物体的不透明度嵌入整合到U-Net中，增强模型对空间位置和不透明度信息的敏感性。3. 提出一种蒙版自注意力机制，使模型能够专注于视觉提示指定的区域，从而提高性能。

Result: 在多个数据集上进行的广泛实验表明，SDMatte方法表现出卓越的性能，验证了其在交互式抠图中的有效性。

Conclusion: SDMatte成功地将扩散模型的优势应用于交互式抠图，通过创新的视觉提示交互、嵌入增强和注意力机制，显著提升了抠图在精细细节提取方面的能力。

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [86] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias是一个自动化框架，利用视觉-语言模型和CLIP引导训练，无需预先知识即可识别并缓解文本到图像（T2I）模型中存在的微妙或重叠的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）模型即使在未明确提及属性的情况下，也经常表现出意外的社会偏见（如性别或种族刻板印象）。现有去偏见方法在简单或已知情况下有效，但在处理微妙或重叠偏见时表现不佳。

Method: 本文提出了AutoDebias框架，它通过以下方式自动识别和缓解T2I模型中的有害偏见：1) 利用视觉-语言模型（VLM）检测有偏见的视觉模式；2) 通过生成反映平衡表示的包容性替代提示来构建“公平指南”；3) 这些指南驱动一个CLIP引导的训练过程，以促进更公平的输出，同时保留原始模型的图像质量和多样性。该方法能够有效处理微妙的刻板印象和多重交互偏见。

Result: AutoDebias在涵盖25种以上偏见场景（包括多重偏见同时出现的高难度案例）的基准测试中进行了评估。结果显示，它以91.6%的准确率检测有害模式，并将有偏见的输出从90%降低到可忽略的水平，同时保留了原始模型的视觉保真度。

Conclusion: AutoDebias框架能够有效识别并显著减少文本到图像模型中的各种社会偏见（包括微妙和多重偏见），同时保持图像生成质量，为解决T2I模型的公平性问题提供了一个鲁棒的自动化解决方案。

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [87] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 该研究系统分析了第一人称视角视频与第三人称视角视频在时空定位任务上的差异，揭示了第一人称视角的挑战，并提出了首个像素级第一人称视角视频时空定位基准数据集EgoMask及其配套训练集EgoMask-Train，实验证明现有模型在该任务上表现不佳，但通过在新数据集上微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频时空定位研究主要集中在第三人称视角视频，而第一人称视角视频（在增强现实、机器人等领域日益重要）仍未得到充分探索。第一人称视角视频与第三人称视角视频存在显著差异，带来了独特的挑战。

Method: 研究首先系统分析了第一人称视角和第三人称视角视频之间的差异。接着，提出了EgoMask，这是首个用于第一人称视角视频精细时空定位的像素级基准数据集，其通过自动标注流程构建，涵盖了短期、中期和长期视频的指代表达和物体掩码。此外，还创建了大规模训练数据集EgoMask-Train。最后，在EgoMask上测试了现有最先进的时空定位模型，并进行了微调实验。

Result: 研究揭示了第一人称视角视频的关键挑战，包括物体持续时间更短、轨迹更稀疏、物体尺寸更小以及位置偏移更大。实验表明，最先进的时空定位模型在EgoMask基准上表现不佳，但在EgoMask-Train上进行微调后，性能得到显著提升，同时在第三人称视角数据集上的性能得以保持。

Conclusion: 该工作为推动第一人称视角视频理解提供了重要的资源（EgoMask和EgoMask-Train）和深刻的见解，解决了该领域现有研究不足的问题。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [88] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime是一个多模态、多任务框架，用于从图像和文本输入预测真菌生长的发育阶段和对应时间戳，解决了现有视觉语言模型在捕捉时间动态方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解生物生长的时间动态在微生物学、农业和生物降解研究等领域至关重要。尽管CLIP等视觉语言模型在视觉-文本联合推理方面表现出色，但它们在捕捉时间进程方面的有效性有限。

Method: 本文提出了CLIPTime框架，它基于CLIP架构，学习视觉-文本联合嵌入，并能在测试时无需明确的时间输入即可进行时间感知推理。该模型通过分类和回归联合预测离散的生长阶段和连续的时间戳。为训练和评估，引入了一个带有对齐时间戳和类别阶段标签的合成真菌生长数据集，并提出了包括时间准确性和回归误差在内的自定义评估指标。

Result: 实验结果表明，CLIPTime能有效地模拟生物进程，并产生可解释的、具有时间依据的输出。

Conclusion: CLIPTime展示了视觉语言模型在实际生物监测应用中的巨大潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [89] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: PIF-Net是一个多光谱与高光谱图像融合框架，通过引入病态先验、可逆Mamba架构和低秩自适应融合模块，有效解决了数据错位导致的病态问题，提升了图像恢复性能和模型效率。


<details>
  <summary>Details</summary>
Motivation: 多光谱与高光谱图像融合（MHIF）旨在生成兼具丰富光谱信息和精细空间细节的高质量图像。然而，由于光谱与空间信息的固有权衡以及观测数据有限，该任务本质上是病态的。先前的研究未能有效解决由数据错位引起的病态性质。

Method: 本文提出了一个名为PIF-Net的融合框架，显式地引入了病态先验。为平衡全局光谱建模和计算效率，设计了一种基于可逆Mamba架构的方法，以在特征转换和融合过程中保持信息一致性。此外，引入了一个名为“融合感知低秩自适应模块”（Fusion-Aware Low-Rank Adaptation module），用于动态校准光谱和空间特征，同时保持模型轻量级。

Result: 在多个基准数据集上进行的广泛实验表明，PIF-Net在图像恢复性能方面显著优于当前的SOTA方法，同时保持了模型效率。

Conclusion: PIF-Net通过有效整合病态先验和创新的网络架构（如可逆Mamba和融合感知低秩自适应模块），成功解决了多光谱与高光谱图像融合中的病态问题，实现了卓越的融合效果和计算效率。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [90] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: 提出HyPCV-Former，一种用于3D点云视频异常检测的双曲时空Transformer，通过在洛伦兹双曲空间中建模来捕捉层次结构和时空连续性，超越了传统欧几里得方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的视频异常检测方法使用欧几里得表示（RGB或深度），但在捕捉事件的层次结构和时空连续性方面存在固有限制。

Method: 提出HyPCV-Former：首先通过点云提取器从点云序列中提取逐帧空间特征；然后将这些特征嵌入到洛伦兹双曲空间中，以更好地捕获潜在的层次结构；接着引入双曲多头自注意力（HMHA）机制，利用洛伦兹内积和曲率感知softmax在非欧几何下学习时间依赖性；所有特征转换和异常评分都直接在完整的洛伦兹空间中进行，而非通过切线空间近似。

Result: HyPCV-Former在多个异常类别上实现了最先进的性能，相比基准方法，在TIMo数据集上提升了7%，在DAD数据集上提升了5.6%。

Conclusion: HyPCV-Former通过利用双曲几何在3D点云视频中有效解决了欧几里得表示的局限性，实现了卓越的异常检测性能。

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [91] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC是一个无需训练的多图像合成框架，首次将单参考扩散模型扩展到多参考场景，通过引入GIA和RMA两种注意力机制，实现了空间布局感知下的连贯一致图像生成，并在多项指标上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在可控图像合成中，从多个参考图像生成具有空间布局感知的连贯一致图像，仍然是一个未解决的挑战。

Method: LAMIC基于MMDiT模型构建，以无训练方式将单参考扩散模型扩展到多参考场景。它引入了两种即插即用的注意力机制：1) 组隔离注意力（GIA）以增强实体解耦；2) 区域调制注意力（RMA）以实现布局感知生成。此外，该研究还引入了三个新的评估指标：包含率（IN-R）、填充率（FI-R）和背景相似度（BG-S）。

Result: LAMIC在大多数主要指标上实现了最先进的性能：在所有设置下，其在ID-S、BG-S、IN-R和AVG分数上始终优于现有的多参考基线，并在复杂合成任务中取得了最佳的DPG。这些结果表明LAMIC在保持身份、背景保留、布局控制和遵循提示方面的卓越能力，且无需任何训练或微调，展现出强大的零样本泛化能力。

Conclusion: LAMIC通过继承先进单参考模型的优势并无缝扩展到多图像场景，为可控多图像合成建立了一个新的无训练范式。随着基础模型的不断发展，LAMIC的性能预计也将相应提升。

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [92] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0是一个交互式高光谱医学图像分割框架，通过引入光谱角度提示（spectral angle prompting）将光谱相似性与空间线索结合，指导SAM模型进行更准确和鲁棒的分割。


<details>
  <summary>Details</summary>
Motivation: 在高光谱医学成像中，需要更准确和鲁棒的分割方法，尤其是在数据量少和噪声大的临床场景中。现有的RGB模型和早期光谱融合方法可能不够有效。

Method: SAMSA 2.0框架通过光谱角度提示，将光谱信息与空间线索进行早期融合，以指导Segment Anything Model (SAM)进行分割。该方法无需重新训练。

Result: 与仅基于RGB的模型相比，Dice分数提高了高达3.8%；与先前的光谱融合方法相比，Dice分数提高了高达3.1%。该方法增强了少样本（few-shot）和零样本（zero-shot）性能，并在低数据和噪声严重的临床场景中展现出强大的泛化能力。

Conclusion: SAMSA 2.0通过有效融合光谱信息，显著提高了高光谱医学图像分割的准确性和鲁棒性，尤其在具有挑战性的临床应用中表现出色，且无需重新训练。

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [93] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的遥感图像标注流程，利用卷积神经网络和图神经网络进行图像分割和特征编码，以克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 遥感图像标注耗时、成本高昂且需要专家知识。现有标注工具依赖预标注数据进行训练，限制了其应用范围。

Method: 该方法定义了一个无监督流程，用于在Sentinel-2卫星图像中查找和标注相似上下文和内容的地理区域。它通过卷积神经网络和图神经网络进行图像分割，将图像分割成基于颜色和空间相似性的同质像素区域。图神经网络用于聚合周围区域的信息，从而在特征表示中编码局部邻域信息，同时保留自身局部信息。

Result: 该方法能够生成更鲁棒的图像比较特征空间，减少标注工具中的异常值，允许用户进行细粒度标注，并在编码空间中形成图像级别的旋转不变语义关系。

Conclusion: 本文提出的无监督管道通过结合先进的神经网络技术，有效解决了遥感图像标注中对预标注数据的依赖问题，并提升了标注的鲁棒性、细粒度和语义一致性。

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [94] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: 本文提出了一种高效路径聚合网络（EPANet），用于解决水下鱼类检测中的低分辨率、背景干扰和目标相似性等挑战，通过互补特征集成实现了准确且轻量级的检测。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测面临目标分辨率低、背景干扰大、目标与环境视觉相似度高等挑战。现有方法主要通过局部特征增强或复杂注意力机制来突出小目标，但往往导致模型复杂性增加和效率降低。

Method: 本文提出了高效路径聚合网络（EPANet），包含两个核心组件：高效路径聚合特征金字塔网络（EPA-FPN）和多尺度多样化分割短路径瓶颈（MS-DDSP bottleneck）。EPA-FPN引入跨尺度长距离跳跃连接和跨层融合路径，以增强语义-空间互补性和特征集成效率。MS-DDSP bottleneck通过更细粒度的特征分割和多样化的卷积操作，扩展了传统瓶颈结构，提升了局部特征多样性和表示能力。

Result: 在基准水下鱼类检测数据集上的大量实验表明，EPANet在检测精度和推理速度方面优于现有最先进方法，同时保持了相当甚至更低的参数复杂度。

Conclusion: EPANet通过其创新的EPA-FPN和MS-DDSP瓶颈结构，有效解决了水下鱼类检测的挑战，实现了高精度、高效率和低复杂度的性能，证明了其在实际应用中的潜力。

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [95] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: 该论文提出一个基于扩散模型的参考视频调色框架，通过生成查找表（LUT）实现参考场景与输入视频之间高层颜色属性的对齐，并支持通过文本提示融入用户偏好，从而实现快速、无损的视频艺术调色。


<details>
  <summary>Details</summary>
Motivation: 视频调色是一个复杂且需要专业技能的过程，目前主要由专业调色师完成，限制了非专业人士的使用。

Method: 提出一个基于参考的视频调色框架，核心是通过扩散模型显式生成一个查找表（LUT），用于参考场景与输入视频之间的颜色属性对齐。训练目标是使参考场景的高层特征（如外观、情绪、情感）与输入视频相似。此外，该方法还构建了一个管道，通过文本提示融入用户偏好，以增强低层特征（如对比度、亮度等）。

Result: 实验结果和广泛的用户研究表明，该方法在视频调色方面表现出有效性。该基于LUT的方法能实现视频帧无结构细节损失的调色，并支持快速推理。

Conclusion: 该研究成功提出了一种有效的、基于参考的视频调色框架，通过结合扩散模型和LUT，实现了专业级的艺术调色效果，同时支持用户自定义，降低了专业门槛。

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [96] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Bäuerle,Meinrad Beer,Michael Götz,Timo Ropinski*

Main category: cs.CV

TL;DR: 该研究评估了当前最先进的视觉语言模型（VLMs）在医学图像上判断解剖结构相对位置的能力，发现它们普遍失败，并引入了MIRP数据集以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 临床决策严重依赖对解剖结构和异常相对位置的理解。然而，尽管对视觉语言模型（VLMs）在临床应用中准确判断医学图像相对位置的能力至关重要，但这一能力却极少被探索和评估。

Method: 评估了GPT-4o、Llama3.2、Pixtral和JanusPro等最先进的VLMs在医学图像相对位置判断任务上的表现。研究了视觉提示（如字母数字或彩色标记）是否能提升性能。引入了MIRP（Medical Imaging Relative Positioning）基准数据集，专门用于系统评估VLMs识别医学图像相对位置的能力。

Result: 所有被评估的VLMs在判断医学图像相对位置这一基本任务上均失败。视觉提示（如标记）能带来适度改进，但性能远低于在自然图像上的表现。评估表明，在医学影像中，VLMs在回答相对位置问题时更多依赖先验解剖知识而非实际图像内容，这常导致错误结论。

Conclusion: 当前最先进的VLMs在医学图像上的相对位置判断能力非常欠缺，且在医学图像上，它们更依赖于先验知识而非图像内容。为推动该领域研究，本研究引入了MIRP基准数据集，以期解决这一关键能力缺失。

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [97] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: 本文提出DBLP，一种高效的扩散模型框架，用于对抗性净化，通过噪声桥蒸馏和自适应语义增强实现实时性能和SOTA鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗性扰动攻击，现有基于扩散的对抗性净化方法需要大量迭代去噪，导致实用性受限。

Method: 提出“扩散桥蒸馏净化”（DBLP）框架。核心方法是“噪声桥蒸馏”，在潜在一致性模型（LCM）中构建对抗性噪声分布与干净数据分布之间的对齐。此外，引入“自适应语义增强”，融合多尺度金字塔边缘图作为条件输入，以引导净化过程并提高语义保真度。

Result: DBLP在多个数据集上实现了最先进（SOTA）的鲁棒准确性，优越的图像质量，并且推理时间约为0.2秒，显著接近实时对抗性净化。

Conclusion: DBLP是迈向实时对抗性净化的重要一步，有效解决了现有方法的效率和性能瓶颈。

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [98] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune是一种免训练、模型无关的视觉语言模型（VLM）令牌剪枝框架，通过利用视觉编码器中的分层注意力结构，选择性地保留关键视觉令牌，显著提高推理效率并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）将图像编码为冗长的视觉令牌序列，导致过高的计算开销和有限的推理效率。现有方法通常依赖特殊令牌或需要任务特定训练，限制了其在不同架构间的可扩展性。

Method: HiPrune利用视觉编码器中的分层注意力结构。它观察到中间层关注以对象为中心的区域，而深层捕获全局上下文特征。基于此，HiPrune选择三种信息丰富的令牌：1) 在对象中心层中具有高注意力的“锚点令牌”；2) 与锚点相邻以保持空间连续性的“缓冲令牌”；3) 在深层中具有强注意力以进行全局总结的“注册令牌”。该方法无需重新训练，可无缝集成到任何基于ViT的VLM中。

Result: 在LLaVA-1.5、LLaVA-NeXT和Qwen2.5-VL上的广泛实验表明，HiPrune实现了最先进的剪枝性能，仅用33.3%的令牌即可保持99.3%的任务准确性，用11.1%的令牌仍能保持99.5%的准确性。同时，它将推理FLOPs和延迟降低了高达9倍，显示出强大的模型和任务泛化能力。

Conclusion: HiPrune是一个高效且通用的令牌剪枝框架，通过智能利用视觉编码器的分层注意力特性，显著降低了视觉语言模型的计算成本，同时几乎不损失任务精度，为VLM的实际部署提供了有效的解决方案。

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [99] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: FreeCP是一个免训练的类别净化框架，旨在解决开放词汇语义分割中类别冗余和视觉语言模糊性问题，通过净化类别表示显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 微调预训练视觉-语言模型进行开放词汇语义分割（OVSS）计算成本高昂。现有免训练方法忽视了类别冗余（测试图像中不存在的类别）和视觉-语言模糊性（语义相似性导致的类别混淆），导致类别激活图不佳。

Method: 提出FreeCP，一个新颖的免训练类别净化框架。它专注于净化语义类别，纠正由冗余和模糊性引起的错误。净化的类别表示随后用于生成最终分割预测。FreeCP是一个可插拔模块。

Result: 在八个基准测试上进行了广泛实验，结果表明FreeCP作为一个可插拔模块，与其它OVSS方法结合时能显著提升分割性能。

Conclusion: FreeCP有效解决了免训练OVSS中类别冗余和视觉-语言模糊性带来的挑战，作为一个可插拔模块显著提升了分割表现。

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [100] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一种新颖的基于扩散模型的方法，用于生成铰接物体，能够与部分点云对齐并提高物理合理性。


<details>
  <summary>Details</summary>
Motivation: 铰接物体是日常环境中重要的可交互对象，研究旨在生成更符合物理规律并能与部分点云对齐的铰接物体。

Method: 该方法名为PhysNAP，采用扩散模型，使用符号距离函数（SDFs）表示部件形状。通过使用预测SDFs计算的点云对齐损失来引导反向扩散过程。此外，基于部件SDFs施加非穿透和可移动性约束，以生成更符合物理规律的物体。模型还支持类别感知以进一步改善点云对齐。

Result: 在PartNet-Mobility数据集上评估了PhysNAP的生成能力和约束一致性。与无引导的基线扩散模型相比，PhysNAP能提高约束一致性，并在生成能力上有所权衡。

Conclusion: PhysNAP能够有效地生成物理上更合理并与部分点云对齐的铰接物体，显著提高了约束一致性。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [101] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: 该论文提出了一种领域特定的弱监督目标检测算法，仅依赖图像级标注来生成伪标签，用于训练目标检测模型，以解决边界框标注昂贵且耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的目标检测方法依赖大量带有标注边界框的数据集进行训练，但获取这些标注成本高昂、耗时，且需要领域专家知识，特别是在科学领域。

Method: 该方法通过从一个预训练模型（用于预测图像中是否存在病毒）中提取知识，生成一组伪标签。具体而言，它采用一种带有收缩感受野的优化方法，直接提取病毒颗粒，无需特定的网络架构。

Result: 实验表明，所提出的伪标签更容易获取，并且在标注时间有限的情况下，其性能优于其他现有的弱标注方法，甚至能超越真实标签。

Conclusion: 该研究成功地通过图像级标注生成高质量的伪标签，有效解决了领域特定目标检测中边界框标注成本高昂的挑战，并展现出优越的性能。

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [102] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出CoProU-VO，一种新的端到端无监督单目视觉里程计方法，通过结合目标帧和投影参考帧的不确定性，有效处理动态场景中的挑战，显著提升位姿估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的无监督视觉里程计方法在动态场景中表现不佳，因为它们假设场景是静态的，导致动态物体破坏了这一假设，产生错误的位姿估计。现有不确定性建模通常只考虑单帧信息，忽略了连续帧之间的不确定性传播。

Method: 引入CoProU-VO，一种新颖的端到端方法。它通过概率公式结合目标帧不确定性和投影参考帧不确定性，实现了跨时间帧的不确定性传播和组合。该模型基于视觉Transformer骨干网络，同时学习深度、不确定性估计和相机位姿。

Result: 在KITTI和nuScenes数据集上的实验表明，CoProU-VO比以往的无监督单目端到端双帧方法有显著改进，并在其他方法常失败的挑战性高速公路场景中表现出色。全面的消融研究验证了跨帧不确定性传播的有效性。

Conclusion: CoProU-VO通过引入跨时间帧的不确定性传播和组合，有效解决了无监督视觉里程计在动态场景中的挑战，显著提高了位姿估计的准确性和鲁棒性。

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [103] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hölle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的似然比估计方法（ULRE），用于语义分割模型中的像素级未知对象（OOD）检测，通过显式处理不确定性来区分已知和未知特征，有效解决了现有方法在复杂场景中将稀有已知对象误识别为未知对象的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割模型在自动驾驶等真实场景中，对于未知对象常常自信地错误分类。尽管像素级OOD检测可以识别未知对象，但现有方法在复杂场景中表现不佳，容易将稀有的已知对象与真正的未知对象混淆。

Method: 该方法引入了一种不确定性感知的似然比估计。它在似然比检验中使用证据分类器（evidential classifier），以区分语义分割模型中的已知和未知像素特征，并明确考虑不确定性。不同于产生点估计，该方法输出概率分布，捕捉了稀有训练样本和不完美的合成异常值带来的不确定性，从而更有效地利用了异常值暴露（outlier exposure）。

Result: 在五个标准基准数据集上进行评估，该方法实现了最1低的平均误报率（2.5%），同时保持了较高的平均精度（90.91%），并且计算开销可以忽略不计。

Conclusion: 通过将不确定性纳入似然比估计中，所提出的方法能够更有效地识别未知对象，显著提高了像素级OOD检测的性能，尤其是在区分稀有已知对象和真正未知对象方面表现出色，且计算效率高。

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [104] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: GeoMoE是一种新框架，通过采用MoE（专家混合）架构，并结合概率先验引导分解和双路径整流器，有效处理复杂场景中双视图几何中的异构运动模式，提升了位姿和单应性估计的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂真实世界场景（极端视角/尺度变化、深度不连续性）中估计运动场时，难以有效处理其多样和异构的运动模式，缺乏针对性的建模策略，导致估计结果偏离真实结构和分布。

Method: 本文提出了GeoMoE框架。首先，设计了“概率先验引导分解”策略，利用内点概率信号将运动场分解为异构子场，减少异常值引起的偏差。其次，引入了“MoE增强双路径整流器”，沿空间上下文和通道语义路径增强每个子场，并将其路由到定制专家进行目标建模，从而解耦异构运动机制，抑制交叉子场干扰和表征纠缠，实现精细化的运动场校正。

Result: GeoMoE在相对位姿和单应性估计方面超越了现有最先进的方法，并表现出强大的泛化能力。

Conclusion: GeoMoE通过其简约设计，成功地重新构建了双视图几何中的运动场建模，有效处理了异构运动模式，显著提升了位姿和单应性估计的性能。

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [105] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: DPoser-X是一个基于扩散模型的3D全身人体姿态先验模型，通过创新的训练和采样方法，解决了全身姿态建模的复杂性和数据稀缺问题，并在多项基准测试中超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 构建通用且鲁棒的全身人体姿态先验模型极具挑战性，原因在于人体姿态固有的复杂性以及高质量全身姿态数据集的稀缺性。

Method: 引入扩散模型作为姿态先验（DPoser并扩展为DPoser-X），将各种以姿态为中心的问题统一为逆问题并通过变分扩散采样求解。为提高下游应用性能，提出了专为姿态数据设计的截断时间步调度方法。此外，提出了掩码训练机制，有效结合全身和局部数据集，捕获身体部位间相互依赖性同时避免过拟合。

Result: DPoser-X在身体、手、面部和全身姿态建模的多个基准测试中展现出鲁棒性和通用性。该模型持续优于现有最先进的替代方案，为全身人体姿态先验建模建立了新的基准。

Conclusion: DPoser-X成功地作为一个强大的扩散基3D全身人体姿态先验模型，解决了姿态建模的挑战，并在多项任务中实现了卓越性能，设定了新的行业标准。

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [106] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 该研究提出使用生成模型（扩散模型）合成带有属性标注的医学图像，以解决医学图像诊断中可解释AI模型所需属性标注数据稀缺的问题，从而提高模型性能和适用性。


<details>
  <summary>Details</summary>
Motivation: 临床医生在医学图像诊断中需要可解释的AI模型来增强信任和可用性，这要求AI决策与临床推理（如使用病理相关视觉属性）对齐。然而，缺乏大规模带有这些属性标注的医学图像数据集，限制了此类模型的应用。

Method: 研究提出使用生成模型合成属性标注数据。具体方法是增强扩散模型，使其能够进行属性条件生成，并仅使用来自LIDC-IDRI数据集中20个带有属性标注的肺结节样本进行训练。然后，将生成的图像整合到可解释模型的训练中。

Result: 与仅使用少量真实属性标注数据集进行训练相比，将生成的图像纳入训练后，可解释模型的性能得到提升。属性预测准确率提高了13.4%，目标预测准确率提高了1.8%。

Conclusion: 这项工作强调了合成数据在克服数据集限制方面的潜力，从而增强了可解释模型在医学图像分析中的适用性。

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [107] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 本文提出了首个针对目标检测器补丁攻击防御的综合基准测试，揭示了现有评估的不足，并提供了新的防御见解和评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有针对目标检测器补丁攻击的防御评估缺乏统一和全面的框架，导致对当前方法的评估不一致和不完整。

Method: 研究者重新审视了11种代表性防御方法，构建了第一个补丁防御基准，包括2个攻击目标、13种补丁攻击、11个目标检测器和4种不同的评估指标。他们还创建了一个包含94种补丁类型和94,000张图像的大规模对抗性补丁数据集。

Result: (1) 防御自然补丁的难度在于数据分布而非高频特性，新数据集可将现有防御性能提升15.09% AP@0.5。(2) 被攻击对象的平均精度（AP）与防御性能高度一致，而非常用的补丁检测准确率。(3) 自适应攻击能显著绕过现有防御，而具有复杂/随机模型或通用补丁特性的防御相对更鲁棒。

Conclusion: 本研究的分析为正确评估补丁攻击/防御及其设计提供了指导，并有望推动该领域的发展。

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [108] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: 本文系统研究了预训练深度表征在图像去雾中的泛化能力，发现其在不同雾度下具有一致性，并提出一个即插即用的RGB-D融合模块，以提高去雾方法的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实场景中雾霾的空间变异性，图像去雾仍具挑战。现有方法依赖于特定架构设计，导致在不同精度和效率要求的场景中适应性差。

Method: 系统研究了从大量图像中学习到的预训练深度表征在图像去雾中的泛化能力，并通过实证分析揭示了深度特征在不同雾度水平下保持显著一致性。在此基础上，提出了一个即插即用的RGB-D融合模块，可无缝集成到各种去雾架构中。

Result: 经验分析表明，学习到的深度特征在不同雾度水平下保持了显著的一致性。在多个基准测试上的大量实验验证了所提方法的有效性和广泛适用性。

Conclusion: 预训练深度表征对图像去雾具有强大的泛化能力和一致性。所提出的RGB-D融合模块能有效提升去雾方法的性能和适应性，使其能集成到多种去雾架构中。

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [109] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 该研究首次系统性探讨多图像多模态大语言模型（MLLMs）中的幻觉问题，提出了MIHBench基准用于评估对象相关幻觉，并识别出多图像输入量、单图像幻觉倾向、同对象图像比例和负样本位置等关键影响因素。为解决这些问题，研究提出了一种动态注意力平衡机制，实验证明其能有效减少幻觉并提升模型在多图像场景下的语义整合和推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有关于多模态大语言模型幻觉的研究主要集中在单图像场景，而多图像场景下的幻觉问题尚未得到充分探索。

Method: 研究方法包括：1) 对多图像MLLMs中的幻觉进行首次系统性研究；2) 提出MIHBench基准，专门用于评估跨多图像的对象相关幻觉，包含对象存在幻觉、对象计数幻觉和对象身份一致性幻觉三个核心任务；3) 提出一种动态注意力平衡机制（Dynamic Attention Balancing mechanism），旨在调整图像间注意力分布，同时保持整体视觉注意力比例。

Result: 主要研究结果包括：1) 识别出与多图像幻觉发生相关的关键因素，包括图像输入数量与幻觉发生概率的递进关系、单图像幻觉倾向与多图像幻觉的强相关性，以及同对象图像比例和负样本在图像序列中位置对对象身份一致性幻觉的影响；2) 所提出的动态注意力平衡机制在多个最先进的MLLMs上进行实验，结果表明其能有效减少幻觉发生，并增强多图像场景下的语义整合和推理稳定性。

Conclusion: 多图像MLLMs中的幻觉是一个重要且未被充分研究的问题。本研究通过提出专门的评估基准和创新的动态注意力平衡机制，不仅揭示了多图像幻觉的关键影响因素，还为有效减少幻觉、提升MLLMs在复杂多图像场景下的鲁棒性提供了有效途径。

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [110] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count是一个可微分的开放词汇对象计数模型，通过引入“基数”图来解决通用计数挑战，并为文本到图像（T2I）生成提供精确数量控制。


<details>
  <summary>Details</summary>
Motivation: 现有模型在通用对象计数方面存在挑战，且难以对文本到图像（T2I）生成中的对象数量进行精确控制。

Method: 提出YOLO-Count模型，核心是引入“基数”图作为新的回归目标，以适应对象尺寸和空间分布的变化。模型采用表示对齐和混合强弱监督方案，并具有完全可微分的架构，便于基于梯度的优化。

Result: YOLO-Count在计数准确性上达到了最先进的水平，并能为T2I系统提供鲁棒且有效的数量控制。

Conclusion: YOLO-Count成功地弥合了开放词汇计数与T2I生成控制之间的差距，在计数精度和生成模型指导方面表现出色。

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [111] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出了一种名为Dense Backbone的轻量级骨干网络，专门用于LiDAR点云3D目标检测，旨在显著降低模型复杂度和计算成本，同时保持高检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR-based 3D目标检测取得了显著进展，但大多数现有方法仍依赖于VGG或ResNet等复杂骨干网络，导致模型复杂度高。2D目标检测的轻量级骨干网络设计已得到充分探索，但3D目标检测领域的研究仍然有限。

Method: 引入了Dense Backbone，这是一种结合了高处理速度、轻量级架构和鲁棒检测精度的骨干网络。该网络是首个专门为点云数据3D目标检测设计的基于密集层（dense-layer-based）的骨干。通过将Dense Backbone适配到PillarNet等现有SoTA 3D目标检测器中，验证其有效性。

Result: 将PillarNet与Dense Backbone结合后形成的DensePillarNet，在nuScenes测试集上实现了29%的模型参数减少和28%的延迟降低，而检测精度仅下降2%。此外，Dense Backbone采用即插即用设计，可直接集成到现有架构中，无需修改其他网络组件。

Conclusion: Dense Backbone为3D目标检测提供了一个高效、轻量且准确的解决方案，它能显著降低计算成本和模型复杂度，同时保持高性能，并且易于集成到现有模型中，有望加速自动驾驶技术的实际应用。

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [112] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: GECO是一种基于最优传输的几何连贯特征学习框架，旨在弥补自监督视觉模型在3D几何感知上的不足，实现语义与几何双重区分，同时运行速度快、性能卓越，并引入了新的几何评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督视觉基础模型虽然能捕获语义对应关系，但往往缺乏对底层3D几何的感知，导致无法区分几何上对立的部位（如左右眼、前后腿）。

Method: GECO提出一个基于最优传输的训练框架，允许超越关键点的监督，即使在遮挡和非遮挡情况下也能工作。采用轻量级架构。

Result: GECO以30 fps运行，比现有方法快98.2%，同时在PFPascal、APK和CUB数据集上达到了最先进的性能，PCK分别提升了6.0%、6.2%和4.1%。此外，研究表明PCK不足以捕捉几何质量，并引入了新的度量标准和见解，以实现更几何感知的特征学习。

Conclusion: GECO成功解决了自监督模型中几何感知不足的问题，通过其高效和高性能证明了该方法的有效性，并强调了未来需要更全面的几何质量评估指标。

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [113] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: PILOT提出了一种新的零样本异常检测框架，通过双分支提示学习和无标签测试时自适应策略，有效解决了现有方法在域偏移下的泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本异常检测（ZSAD）方法，无论是使用固定还是学习到的提示，在面对域偏移时表现不佳，因为它们的训练数据来源于有限的训练域，难以泛化到新的数据分布。

Method: 本文提出了PILOT框架，包含两项关键创新：1) 一种新颖的双分支提示学习机制，动态整合可学习提示与结构化语义属性，使模型能自适应地为每个输入图像加权最相关的异常线索；2) 一种无标签测试时自适应策略，利用无标签测试数据中的高置信度伪标签来更新可学习提示参数。

Result: 在13个工业和医疗基准上的大量实验表明，PILOT在域偏移下的异常检测和定位方面均达到了最先进的性能。

Conclusion: PILOT框架通过其创新的提示学习和测试时自适应策略，成功克服了零样本异常检测在域偏移下的挑战，实现了卓越的泛化能力。

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [114] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: 本研究分析了公共安全领域中异构标注点云数据的语义分割性能，发现大型物体分割效果较好，但小型安全关键特征识别率低，主要受数据不平衡和标注标准化缺失影响。


<details>
  <summary>Details</summary>
Motivation: 研究在公共安全应用（如激光雷达扫描衍生的事前规划系统）中，跨异构标注点云数据集的语义分割性能，并调查统一不同标注3D数据所面临的挑战。

Method: 使用NIST的点云城市数据集（Enfield和Memphis集合），采用分级模式结合KPConv架构，并通过IoU指标评估安全相关特征的分割性能。

Result: 结果显示性能存在差异：几何上较大的物体（如楼梯、窗户）分割性能较高，而较小的安全关键特征识别率较低。性能受类别不平衡和小型物体几何区分度低的限制。主要挑战包括标记数据不足、数据集间类别标签统一困难以及缺乏标准化。

Conclusion: 为了实现公共安全领域可靠的点云语义分割，需要标准化标注协议和改进的标注技术，以解决数据异构性以及小型安全关键元素的检测问题。

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [115] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文评估了前沿大型语言模型（LLMs）在解决数学和描述性物理问题方面的表现，并采用多智能体框架等推理时技术来提升模型性能，同时引入了一个新的物理问题评估基准PHYSEVAL。


<details>
  <summary>Details</summary>
Motivation: 物理学是人类智能的基石，解决物理问题是自然语言推理中的关键领域。研究旨在评估当前LLMs在这一领域的表现，并探索提升其解决复杂物理问题能力的方法。

Method: 研究方法包括：1) 评估前沿LLMs在数学和描述性物理问题上的性能。2) 采用多种推理时技术和智能体框架（如通过小型LLM智能体累积验证解决方案）来提升模型性能。3) 引入一个新的物理问题评估基准PHYSEVAL，包含19,609个从物理教科书和在线资源收集的问题及其解决方案。

Result: 研究结果显示，当多智能体框架应用于模型初始表现不佳的问题时，性能有显著提升。此外，本文成功构建并推出了大型物理问题评估基准PHYSEVAL。

Conclusion: 多智能体框架能有效提高LLMs在解决复杂物理问题上的表现。新引入的PHYSEVAL基准为未来物理问题解决能力的研究提供了大规模、高质量的评估工具。

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [116] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）生成的文本在词汇多样性方面与人类文本存在显著差异，特别是较新的模型（如ChatGPT-o4 mini和-4.5）与人类文本的差异更大，表明LLM未能生成真正类人文本。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量实证研究，但LLM生成文本的“类人”程度仍不明确。本研究旨在从词汇多样性的角度，深入探讨LLM生成文本的类人程度。

Method: 研究对比了四种ChatGPT模型（-3.5、-4、-o4 mini和-4.5）生成的文本与240名母语（L1）和第二语言（L2）英语使用者（来自四个教育水平）撰写的文本的词汇多样性。测量了词汇多样性的六个维度：数量、丰富度、多样性-重复性、均匀度、差异性和分散度。数据分析采用了单向多元方差分析（MANOVAs）、单向方差分析（ANOVAs）和支持向量机（SVMs）。

Result: 研究结果显示，LLM生成的文本在所有词汇多样性变量上都与人类文本存在显著差异，其中ChatGPT-o4 mini和-4.5的差异最大。在这两个模型中，ChatGPT-4.5尽管生成的词元数量较少，但其词汇多样性水平更高。人类作者的词汇多样性在不同亚组（即教育水平、语言状态）之间没有显著差异。

Conclusion: 总而言之，研究表明LLM在词汇多样性方面未能生成类人文本，并且较新的LLM模型生成的文本比旧模型更不像人类文本。研究讨论了这些结果对语言教学及相关应用的影响。

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [117] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 计算人文学科需要更深入的方法论理论化，本文将计算建模视为一种翻译过程，并指出将符号复杂数据简化处理的错误，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 为实现计算人文学科的认识论和解释清晰性，促进学科成熟，并避免在建模过程中出现潜在的、有影响的“翻译”错误，需要加强方法论理论化。

Method: 将计算人文学科的建模工作框定为从文化/语言领域到计算/数学领域的“翻译”过程。引入“符号复杂性”概念，定义为文本意义在不同解释视角下变化的程度。分析了当前主流建模实践（尤其是在评估方面）如何错误地将符号复杂数据视为符号简单数据。

Result: 主流建模实践（特别是在评估中）存在一个“翻译错误”，即为了表面上的清晰和认识论上的便利，将符号复杂数据当作符号简单数据处理。文章提出了针对研究人员的几项建议，以更好地解决这些认识论问题。

Conclusion: 计算人文学科的研究者需要更深入地思考其建模方法，尤其是在处理符号复杂数据时，避免将其过度简化，以提高研究的内部一致性、解释透明度和认识论严谨性。

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [118] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: 本文介绍了FACTORY，一个大规模、经人工验证的提示集，用于评估长篇事实性生成。研究发现，现有SOTA模型在FACTORY上的事实错误率远高于其他数据集，表明其更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有长篇事实性评估基准缺乏人工验证，导致潜在的质量问题。

Method: 开发了FACTORY数据集，采用“模型在环”方法并经人工精炼，包含具有挑战性、寻求事实、可回答且无歧义的提示。使用FACTORY和现有数据集对6个最先进的语言模型进行了人工评估。

Result: FACTORY是一个具有挑战性的基准：最先进模型在FACTORY上生成回复中约40%的主张不符合事实，而其他数据集上仅为10%。

Conclusion: FACTORY是一个可靠的基准，强调了模型需要对长尾事实进行推理，证明了其优于现有基准的可靠性和挑战性。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [119] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: 神经语义解析器在处理动词短语省略等强上下文敏感现象时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然神经语义解析器在多种语言现象上表现出色，但其在处理需要大量语义信息复制的强上下文敏感现象（如英语动词短语省略）时的性能尚不明确。

Method: 构建了一个包含120个动词短语省略案例及其完整解析语义表示的语料库，并将其作为挑战集，用于测试多种神经语义解析器。

Result: 尽管这些解析器在标准测试集上表现良好，但在省略句实例中却失败了。

Conclusion: 神经语义解析器虽然整体性能优异，但目前尚不能有效处理动词短语省略这类强上下文敏感现象。

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [120] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: 本文提供了一个不断更新的开源大型语言模型（LLM）比较列表，以帮助研究人员和公司根据许可和硬件要求选择合适的LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然在文本生成等任务中表现出色，但随着大量开源基础模型和微调模型的出现，选择最佳LLM变得复杂，尤其是在许可和硬件要求方面。

Method: 作者创建并发布了一个比较列表，涵盖了基础模型和特定领域模型，重点关注发布年份、许可和硬件要求。该列表发布在GitLab上并持续更新。

Result: 一个包含基础和特定领域LLM的比较列表被呈现并发布，旨在简化LLM的选择过程。

Conclusion: 该比较列表旨在帮助用户在快速发展的LLM领域中进行导航和选择，并将持续更新以保持其时效性。

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [121] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 该论文通过对表格输入表示进行分类并介绍表格理解任务，指出了大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在表格理解领域面临的挑战和研究空白。


<details>
  <summary>Details</summary>
Motivation: 表格因其复杂和灵活的结构在LLMs和MLLMs中受到广泛关注。然而，表格的二维特性、多样化的格式和目的导致了缺乏通用方法，使得表格理解任务的进展充满挑战。

Method: 本文通过引入表格输入表示的分类法和表格理解任务的介绍来解决这些挑战。同时，论文强调并指出了该领域中存在的几个关键研究空白。

Result: 论文指出了三个主要的研究空白：1) 现有任务主要集中于检索，对数学和逻辑操作之外的推理要求不高；2) 模型在处理复杂结构、大规模表格、长上下文或多表格场景时面临重大挑战；3) 模型在不同表格表示和格式之间的泛化能力有限。

Conclusion: 该领域需要进一步的研究来填补现有空白，特别是要解决表格理解中推理能力不足、复杂表格处理困难以及模型泛化能力受限的问题。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [122] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 本研究探索了离散小波变换（DWT）在词和句子嵌入上的应用，旨在实现高比例的降维同时保持甚至提升NLP任务性能。


<details>
  <summary>Details</summary>
Motivation: 小波变换在信号和图像处理中表现出色，启发了研究者将其应用于自然语言处理（NLP），以期捕捉语言和语义特性。研究旨在展示DWT在分析和压缩嵌入表示方面的能力，同时保持其质量。

Method: 研究经验性地将离散小波变换（DWT）应用于词和句子嵌入。通过语义相似性任务评估DWT嵌入的有效性，并使用包括大型语言模型在内的不同嵌入模型，在下游任务中展示所提出范式的功效。

Result: DWT能够将嵌入维度降低50%至93%，在语义相似性任务上性能几乎没有变化，同时在大多数下游任务中取得了更高的准确性。

Conclusion: 研究结果为将DWT应用于改进NLP应用开辟了道路，表明其在嵌入降维和性能提升方面具有巨大潜力。

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [123] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）正在改变书面语言。本研究发现，ChatGPT发布后，非脚本口语中与LLM相关的词汇使用量适度但显著增加，表明人类词汇选择正与LLM模式趋同。


<details>
  <summary>Details</summary>
Motivation: 近年来，书面语言（尤其在科学和教育领域）的词汇使用发生了显著变化，这归因于大型语言模型（LLMs）的影响。虽然这些变化常与直接使用AI生成文本相关，但尚不清楚它们是否反映了人类语言系统本身的更广泛变化。本研究旨在探讨此问题。

Method: 构建了一个包含2210万词的非脚本口语数据集，来源于科技播客。分析了2022年ChatGPT发布前后，与LLM常用词汇相关的词汇趋势，并与基线同义词进行了对比。

Result: 结果显示，2022年后，与LLM相关词汇的使用量适度但显著增加，表明人类词汇选择与LLM相关模式之间存在趋同。相比之下，基线同义词没有表现出显著的方向性变化。

Conclusion: 鉴于时间短和受影响词汇量，这可能预示着语言使用正在发生显著转变。这究竟是自然语言演变还是AI暴露驱动的新型转变，仍是一个开放性问题。同时，这可能与模型未对齐导致社会和道德观念被塑造的伦理担忧相似。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [124] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: 本研究提出了一种“病因感知注意力引导框架”，通过整合结构化临床推理来提高大型语言模型在复杂临床场景下的诊断准确性和推理能力，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医学文本理解和生成方面表现出显著能力，但它们在复杂临床场景中的诊断可靠性仍然有限。本研究旨在增强大型语言模型的诊断准确性和临床推理能力。

Method: 提出“病因感知注意力引导框架”。具体包括：1) 基于权威临床指南构建针对急性阑尾炎、急性胰腺炎和急性胆囊炎的“临床推理支架（CRS）”；2) 开发“病因感知头识别算法”以确定模型病因推理的关键注意力头；3) 引入“推理引导参数高效微调”，将病因推理线索嵌入输入表示，并通过“推理引导损失函数”将选定的病因感知头导向关键信息。

Result: 在“一致诊断队列”上，该框架将平均诊断准确率提高了15.65%，平均推理焦点分数提高了31.6%。在“差异诊断队列”上的外部验证进一步证实了其在提高诊断准确性方面的有效性。通过“推理注意力频率”的进一步评估表明，该模型在面对真实世界复杂场景时表现出更高的可靠性。

Conclusion: 本研究提出了一种实用且有效的方法来增强基于大型语言模型的诊断中的临床推理能力。通过将模型注意力与结构化临床推理支架对齐，所提出的框架为在复杂临床环境中构建更可解释和可靠的AI诊断系统提供了一个有前景的范式。

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [125] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 本文系统性地基准测试了大型语言模型（LLM）在长上下文场景下的优化方法（剪枝、量化、token丢弃），并发现简单组合这些优化可能对大模型产生负面影响，且F1分数可能掩盖精度-召回率的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理任务中表现出色，但它们面临资源需求高和上下文窗口受限的问题。现有的优化技术（如剪枝、量化、token丢弃）在长上下文场景下的有效性及其对系统评估的影响尚未得到充分探索。

Method: 研究首先分析了两种支持长上下文的LLM架构的单一优化方法，然后系统性地评估了这些技术的组合，以评估其对性能指标的影响。随后，研究在700亿参数的更大模型上研究了单一优化方法的可扩展性。评估指标包括内存使用、延迟、吞吐量以及文本生成质量。研究结合了系统级性能分析和任务特定洞察（例如，通过F1分数与精度-召回率权衡进行对比）。

Result: 研究发现，与小型模型相比，简单的推理优化算法组合可能会因为累积的近似误差而对大型模型产生不利影响。实验表明，仅依赖F1分数可能会掩盖问答任务中的精度-召回率权衡，从而掩盖这些负面效应。

Conclusion: 通过整合系统级性能分析和任务特定洞察，这项研究有助于LLM从业者和研究人员在不同任务和硬件配置下探索和平衡效率、准确性和可扩展性。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [126] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: MCSEO通过引入细粒度对象-短语对齐来增强多模态句子嵌入，以解决传统图像-文本对中的噪声问题，并在语义文本相似度任务上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 多模态句子嵌入模型在训练中常使用包含冗余或不相关信息的图像-文本对，这些噪声会影响模型性能。

Method: 提出MCSEO方法，利用现有的分割和目标检测模型提取准确的对象-短语对，并以此优化一个专门针对对象-短语对应关系的对比学习目标，同时结合传统的图像-文本对齐。

Result: 在不同的骨干模型上，MCSEO在语义文本相似度（STS）任务中持续优于强基线模型。

Conclusion: 精确的对象-短语对齐在多模态表示学习中具有重要意义，能够有效提升多模态句子嵌入的性能。

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [127] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: 针对LLM代理在复杂任务中长期规划、执行协调和泛化能力不足的问题，本文提出了AdaPlan范式和PilotRL训练框架。PilotRL通过渐进式强化学习，优化了模型遵循全局计划、生成计划的质量以及规划与执行的协调，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理范式（如ReAct）在复杂任务中存在局限性，其单步推理难以支持长期战略规划；规划器与执行器之间的协调不足；以及监督微调方法导致模型记忆特定轨迹，泛化能力受限。

Method: 引入自适应全局计划代理范式AdaPlan，旨在协同高层显式指导与执行。基于此，提出PilotRL，一个由渐进式强化学习驱动的全局规划引导训练框架。PilotRL分三阶段优化：首先培养模型遵循全局计划的能力；其次优化生成计划的质量；最后联合优化模型的规划与执行协调。

Result: PilotRL实现了最先进的性能。LLaMA3.1-8B-Instruct + PilotRL的性能超越闭源模型GPT-4o 3.60%，并且在参数规模相当的情况下，比GPT-4o-mini高出55.78%。

Conclusion: PilotRL有效解决了LLM代理在复杂任务中长期规划、执行协调和泛化能力方面的挑战，通过渐进式强化学习显著提升了模型性能，达到了新的技术水平。

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [128] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: 本文提出了一种新范式，将小型语言模型（SLM）的内部推理视为动态任务向量机，通过RLVR优化，使SLM能动态构建和优化任务向量，在知识密集型任务上达到与大型模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型由于容量限制，在知识密集型任务中表现受限。现有方法将推理视为固定或启发式过程，未能充分利用模型内部的动态推理能力。

Method: 将模型内部由`<think>`和`</think>`标签界定的推理过程视为一个动态任务向量机。模型在此过程中动态构建和完善自身的任务向量。通过RLVR（强化学习与向量奖励）方法优化这个动态任务向量机，并训练了一个智能体网络搜索模型，命名为Lucy。

Result: 名为Lucy的1.7B参数SLM，结合动态推理机制和MCP集成，在SimpleQA基准测试中取得了78.3%的准确率，性能与DeepSeek-V3等大型模型不相上下。

Conclusion: 研究表明，当小型模型配备结构化、自构建的任务推理机制时，它们能够在知识密集型任务上与大型模型匹敌。

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [129] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct 提出分段监督微调、量化和固定形状计算图，以高效地在资源受限的边缘设备上部署长序列Transformer模型，同时保持性能和提高TTFT。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在边缘设备上处理长序列面临挑战，主要由于自注意力的二次时间复杂度和不断增长的KV缓存需求。现有优化方法未能有效降低首个token生成时间（TTFT）或导致性能下降。其他序列建模架构需要完全重新训练且缺乏基础设施支持。现有EdgeInfinite方案指令遵循能力有限，且缺乏移动特定优化。

Method: 本文提出EdgeInfinite-Instruct，通过以下方法解决问题：1. 引入分段监督微调（S-SFT）策略，专为长序列任务（如摘要和问答）定制。2. 采用细粒度训练后量化（PTQ）以降低计算需求并保持精度。3. 实现固定形状计算图，通过场景特定的输入token和缓存大小定制，平衡内存使用和设备效率。

Result: 在长上下文基准测试和真实世界移动任务上的实验表明，该方法在NPU加速的边缘设备上提高了特定领域的性能，同时保持了效率。

Conclusion: EdgeInfinite-Instruct通过创新的微调策略和设备优化，有效解决了长序列Transformer模型在资源受限边缘设备上的部署挑战，实现了性能和效率的平衡。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [130] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文研究了上下文学习（ICL）中演示（demonstration）无效的原因，发现无效演示要么信息已被模型学习，要么与查询无关。基于此，提出了一种利用梯度流进行演示选择的新方法GradS，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设ICL中的演示是有效的，但许多研究表明并非所有演示都有效，甚至无法带来性能提升。因此，需要深入探究演示无效的根本原因。

Method: 通过梯度流和线性自注意力模型分析，推导出演示无效的原因是其信息已被模型学习或与用户查询不相关。进一步发现多层模型中演示有效性差异会随层数增加而放大。基于这些发现，提出了GradS方法，利用演示相对于给定用户查询的梯度流大小作为选择标准，以确保所选演示的有效性。

Result: 实验结果证实了随着模型层数增加，演示之间有效性差异会放大，验证了理论推导。GradS方法在四个主流大型语言模型和五个主流数据集上，相对于最强基线平均实现了6.8%的相对性能提升，证明了其有效性。

Conclusion: 演示的无效性源于信息已被学习或与查询无关。模型层数增加会放大有效性差异。提出的GradS方法通过利用梯度流，能够有效选择高质量的演示，显著提升ICL性能，为未来的演示选择方法提供了新思路。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [131] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: 本文提出SA-GCS框架，将课程学习融入强化学习，以解决无人机视觉-语言导航中现有RL方法训练效率低、收敛慢的问题，通过语义感知难度估计器和高斯课程调度器，实现从易到难的任务平滑进展，显著提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉-语言导航（UAV VLN）在智能巡检、灾害救援等领域有广泛应用。尽管视觉-语言模型（VLMs）和强化学习（RL）提供了强大支持，但现有RL方法存在训练数据利用效率低、收敛速度慢以及未充分考虑训练样本难度差异的问题，限制了性能提升。

Method: 提出语义感知高斯课程调度（SA-GCS）训练框架，系统地将课程学习（CL）融入强化学习。该框架采用语义感知难度估计器（SA-DE）量化训练样本复杂度，并使用高斯课程调度器（GCS）动态调整采样分布，实现从简单到困难任务的平滑过渡。

Result: 在CityNav基准测试中，SA-GCS在所有指标上持续优于强基线模型，实现了更快、更稳定的收敛，并且对不同规模的模型展现出良好的泛化能力，验证了其鲁棒性和可扩展性。

Conclusion: SA-GCS通过引入课程学习，显著提高了无人机VLN任务的训练效率，加速了模型收敛，并全面提升了模型性能和泛化能力。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [132] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 本文探讨了将小波变换（DWT）应用于词和句子嵌入，以减少维度并压缩信息，并在下游NLP任务中取得了可比甚至更优的效果。


<details>
  <summary>Details</summary>
Motivation: 小波在图像和信号处理领域取得了显著成功，作者认为其也能有效应用于自然语言处理（NLP）任务，以捕捉各种语言特性，并期望能有效整合词向量中的重要信息同时降低维度。

Method: 核心方法是应用离散小波变换（DWT）到词和句子嵌入。首先评估小波如何有效整合词向量信息并降低维度。进一步，将DWT与离散余弦变换（DCT）结合，提出一个非参数模型，用于基于局部变化的词特征将包含密集信息的句子压缩成固定大小的向量。

Result: 研究显示，小波能有效整合词向量中的重要信息并降低其维度。所提出的DWT与DCT结合的模型，在下游应用模型中，其性能与原始嵌入模型相当，甚至在某些任务中表现更优。

Conclusion: 小波变换是一种有效且有前景的技术，可以应用于NLP任务，尤其是在词和句子嵌入的维度缩减和信息压缩方面，能够保持甚至提升在下游任务中的表现。

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [133] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN是一个基于智能体和检索增强的图神经网络框架，旨在解决传统GNN在节点信息不平衡和全局语义关系捕获方面的局限性，并在少样本设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络(GNNs)的预定义聚合机制存在两个主要限制：1) 无法处理节点信息丰富度不平衡的问题（信息密集节点与稀疏节点）；2) 消息传递主要利用局部结构相似性，忽略了图中的全局语义关系，限制了模型捕获远距离但相关信息的能力。

Method: 提出了检索增强图智能体网络(ReaGAN)，一个基于智能体的框架。每个节点被视为一个智能体，能够基于其内部记忆独立规划下一步行动，实现节点级规划和自适应消息传播。此外，通过检索增强生成(RAG)机制，节点可以访问语义相关内容并建立图中的全局关系。ReaGAN使用冻结的LLM骨干模型，无需微调。

Result: ReaGAN在少样本上下文设置下取得了有竞争力的性能。

Conclusion: 该研究展示了智能体规划和局部-全局检索在图学习中的巨大潜力。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [134] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出了一种高效的多轮对话评估器，通过将多个LLM评委的偏好知识聚合成一个模型，解决了现有“LLM作为评委”方法的偏差和多评委方法的计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对话能力评估主要依赖“LLM作为评委”范式，但其存在各种偏差，影响评估结果的可靠性和一致性。多评委方法虽然有效，但推理时计算开销巨大。

Method: 通过将多个LLM评委的偏好知识聚合成一个单一模型，捕获了它们的集体智慧。这种方法在保留多评委反馈优势的同时，显著降低了评估成本。

Result: 在七个单项评分和成对比较对话评估基准上进行了广泛实验，结果表明该方法在各种场景下均优于现有基线，展示了其效率和鲁棒性。

Conclusion: 所提出的高效多轮对话评估器能够快速灵活地评估对话质量，同时保持了多评委反馈的优势并大幅降低了成本，是一种高效、鲁棒且性能优越的对话评估方案。

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [135] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: 本文介绍了GETALP团队在SIGDial 2025自动会议纪要共享任务（任务B：基于会议记录的问答）中的提交。他们的方法结合了检索增强生成（RAG）系统和抽象意义表示（AMR），结果显示AMR能提高约35%问题的回答质量，并显著改善涉及区分不同参与者的问题（如“谁”的问题）的回答。


<details>
  <summary>Details</summary>
Motivation: 参与SIGDial 2025的第三届自动会议纪要共享任务，特别是任务B，即基于会议记录进行问答。

Method: 采用基于检索增强生成（RAG）系统和抽象意义表示（AMR）的方法。提出了三种结合这两种方法的系统。

Result: 结果表明，结合AMR能够使大约35%的问题产生高质量的回答，并在回答涉及区分不同参与者的问题（例如“谁”的问题）时提供显著改进。

Conclusion: 将AMR融入RAG系统对基于会议记录的问答任务有效，尤其在处理需要区分参与者的问题时表现突出。

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [136] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 该研究引入了“半真话检测”任务，提出了新基准PolitiFact-Hidden和模块化框架TRACER，旨在通过识别遗漏的关键信息来处理因上下文缺失而具有误导性的声明，显著提升了半真话分类的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的事实核查系统假设真实性仅取决于已声明的内容，但许多现实世界的声明是“半真话”，即事实正确但因遗漏关键上下文而具有误导性。现有模型未能处理这些情况，因为它们未设计来推理未提及的内容。

Method: 1. 引入“半真话检测”任务。2. 构建新基准PolitiFact-Hidden，包含1.5万条政治声明，并标注了句子级证据对齐和推断的声明意图。3. 提出TRACER，一个模块化的重新评估框架，通过证据对齐、推断隐含意图和估计隐藏内容的因果影响来识别基于遗漏的错误信息。

Result: TRACER框架可以集成到现有事实核查流程中，并持续提升多个强基线的性能。显著地，它将“半真话”分类的F1分数提高了高达16点。

Conclusion: 该研究强调了建模遗漏信息对于可信事实核查的重要性，并表明TRACER能有效解决因信息遗漏导致的误导性声明问题。

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [137] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: 该研究提出了Flat-LoRA和EFlat-LoRA，通过在低秩适应（LoRA）中寻找平坦最小值，显著提升了大型语言模型和视觉-语言模型的泛化能力，并验证了LoRA泛化能力与锐度之间的关联。


<details>
  <summary>Details</summary>
Motivation: LoRA的表达能力与泛化能力之间的关联，特别是锐度（sharpness）对泛化的影响，尚未被充分探索。现有方法缺乏在LoRA中经验性寻找平坦最小值或开发相关理论工具的手段，尽管SAM已证明对全参数模型泛化有效。

Method: 提出了Flat-LoRA及其高效版本EFlat-LoRA，旨在为LoRA寻找平坦最小值。理论上证明了全参数空间的扰动可以转移到低秩子空间，从而避免了低秩子空间内多矩阵扰动引入的潜在干扰。

Result: EFlat-LoRA在优化效率上与LoRA相当，同时实现了可比甚至更好的性能。例如，在GLUE数据集上，EFlat-LoRA在RoBERTa-large模型上平均优于LoRA 1.0%和全参数微调0.5%；在视觉-语言模型Qwen-VL-Chat上，在SQA和VizWiz数据集上分别提升了1.5%和1.0%。这些结果也经验性地验证了LoRA的泛化能力与锐度密切相关。

Conclusion: Flat-LoRA和EFlat-LoRA通过寻找平坦最小值，有效地提升了LoRA的泛化能力，并首次经验性地证明了LoRA的泛化能力与锐度之间的紧密联系，弥补了先前研究的空白。

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [138] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: 本研究通过分析人类语音数据，探讨了表情符号如何影响语音中的韵律表现，以及听者如何通过韵律线索理解表情符号的含义。


<details>
  <summary>Details</summary>
Motivation: 在文本交流中，语音的韵律特征（如音高、语速、语调）缺失，表情符号作为视觉替代品补充了情感和语用细微差别。现有研究缺乏直接将韵律与表情符号关联的实证数据，本研究旨在填补这一空白，探究表情符号语义如何塑造口语表达和感知。

Method: 通过结构化但开放式的生成和感知任务，收集并分析了真实的人类语音数据，直接关联了韵律和表情符号。

Result: 研究结果显示，说话者会根据表情符号线索调整其韵律；听者通常仅凭韵律变化就能识别出预期的表情符号；表情符号之间语义差异越大，其韵律分歧也越大。

Conclusion: 这些发现表明，表情符号可以作为韵律意图的有意义载体，揭示了它们在数字媒介交流中的重要作用。

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [139] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 本文提出PaPaformer，一种新的Transformer架构，通过并行路径独立训练并组合，实现小型语言模型在数小时内完成训练，显著缩短训练时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（包括小型变体SLM）的训练需要大量计算资源和时间（数天甚至数周，多GPU），效率低下。

Method: 引入PaPaformer，一种仅解码器Transformer架构变体。其核心思想是将低维并行路径组合成更大的模型。这些低维路径可以独立训练，使用不同类型的数据，然后合并。这种方法旨在减少模型参数总量和训练时间，同时提升性能。此外，并行路径结构也提供了为特定任务定制路径的可能性。

Result: PaPaformer方法能够将语言模型的训练时间从数天/数周缩短到数小时。它在减少模型参数总量和训练时间的同时，提升了模型性能。并行路径结构还为根据特定任务需求定制路径提供了新的可能性。

Conclusion: PaPaformer通过其独特的并行路径结构，提供了一种更快速、更高效且可定制的Transformer语言模型训练方法，尤其适用于小型语言模型，解决了传统训练耗时耗力的问题。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [140] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: SynAdapt是一种高效推理框架，通过生成合成连续思维链（CCoT）作为精确对齐目标，并集成难度分类器自适应地处理难题，从而在准确性和效率之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 传统的离散思维链（DCoT）推理耗时较长，而现有连续思维链（CCoT）方法存在间接微调、对齐不足或目标不一致等问题，限制了其效率和效果。

Method: SynAdapt包含两部分：1) 生成合成CCoT，作为LLM学习CCoT和直接得出答案的精确有效对齐目标；2) 集成一个难度分类器，利用问题上下文和CCoT识别难题，并自适应地提示LLM重新思考这些难题以提高性能。

Result: 在不同难度级别的各种基准测试中，SynAdapt展现出卓越的有效性，实现了最佳的准确性-效率权衡。

Conclusion: SynAdapt提供了一种创新的高效推理框架，通过合成CCoT和自适应难题处理，有效克服了现有思维链方法的局限性，提升了大型语言模型的推理效率和准确性。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [141] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX是一个新的LLM置信度估计框架，通过结合上下文忠实度和一致性来提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型置信度估计方法忽略了响应与上下文信息之间的相关性，这在评估输出质量（尤其是在提供背景知识的场景中）时是一个关键因素，导致模型在安全关键应用中部署的可靠性不足。

Method: 本文提出了CRUX框架，它首次整合了上下文忠实度和一致性来进行置信度估计。CRUX引入了两个新颖的度量：1) 上下文熵减（Contextual Entropy Reduction），通过有无上下文的对比采样来表示数据不确定性的信息增益；2) 统一一致性检查（Unified Consistency Examination），通过生成答案在有无上下文情况下的全局一致性来捕捉潜在的模型不确定性。

Result: 在三个基准数据集（CoQA、SQuAD、QuAC）和两个领域特定数据集（BioASQ、EduQG）上的实验表明，CRUX的有效性优于现有基线，取得了最高的AUROC。

Conclusion: CRUX通过整合上下文忠实度和一致性，显著提高了大语言模型置信度估计的准确性，使其在各种应用场景中更加可靠和值得信赖。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [142] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: 本文提出了一种基于图卷积网络（GCN）的新型混合主题模型GHTM，用于孟加拉语主题建模，通过结合GCN和NMF提高主题的连贯性和多样性，并引入了一个新的孟加拉语数据集。


<details>
  <summary>Details</summary>
Motivation: 主题建模在英语中已被广泛研究，但在孟加拉语中却因其形态复杂性、资源匮乏和缺乏主动性而研究不足。

Method: 提出GHTM（Graph-Based Hybrid Topic Model），将文档输入向量表示为图中的节点，GCN生成语义丰富的嵌入，然后使用非负矩阵分解（NMF）分解这些嵌入以获得主题表示。该模型与LDA、LSA、NMF、BERTopic和Top2Vec等传统及现代孟加拉语主题建模技术在三个孟加拉语数据集上进行了比较。此外，还引入了一个名为“NCTBText”的新孟加拉语数据集。

Result: 实验结果表明，所提出的GHTM模型在主题连贯性和多样性方面优于其他模型。

Conclusion: GHTM模型有效提升了孟加拉语主题建模的性能，解决了该领域研究不足的问题，并且新引入的NCTBText数据集丰富了现有的孟加拉语语料库。

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [143] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 研究表明，向AI模型提供小费或威胁它们通常不会显著提高其基准性能，尽管提示词的微小变化可能对单个问题产生不可预测的影响。


<details>
  <summary>Details</summary>
Motivation: 业界普遍流传两种提升AI模型性能的提示词策略：提供小费和威胁模型（后者甚至得到谷歌创始人谢尔盖·布林的认可）。本研究旨在通过严格测试，验证这些策略的有效性。

Method: 通过在GPQA和MMLU-Pro基准数据集上进行实证测试，评估提供小费或威胁AI模型对模型性能的影响。

Result: 研究发现，威胁或提供小费给AI模型通常不会显著影响其基准性能。尽管提示词的细微变化可能在单个问题层面显著影响性能，但很难预先判断某种特定提示方法会帮助还是损害大型语言模型回答特定问题的能力。

Conclusion: 综合来看，这表明简单的提示词变体（如提供小费或威胁）可能不像之前假设的那样有效，尤其是在解决难题时。然而，对于个别问题，不同的提示方法确实可能产生显著不同的结果。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [144] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文提出了NusaAksara，一个针对印尼语种及其原始文字的公共基准测试数据集，涵盖文本和图像模态及多种任务，并揭示现有NLP技术对印尼本地文字处理能力极差。


<details>
  <summary>Details</summary>
Motivation: 印尼拥有丰富的语言和文字，但当前的自然语言处理（NLP）进展主要集中在罗马化文本上，忽视了原始文字，且许多低资源语言未被常见NLP基准覆盖。这促使研究者创建一个包含原始文字的综合基准。

Method: 通过人类专家严格构建了NusaAksara数据集，涵盖7种语言的8种文字（包括未被Unicode支持的Lampung文字）。该基准包括文本和图像模态，并涵盖图像分割、OCR、音译、翻译和语言识别等任务。研究人员使用GPT-4o、Llama 3.2、Aya 23等大型语言模型和视觉语言模型，以及PP-OCR和LangID等特定任务系统对数据进行了基准测试。

Result: 基准测试结果显示，大多数现有的NLP技术无法有效处理印尼的本地文字，许多模型在此任务上表现接近于零。

Conclusion: 当前NLP技术在处理印尼原始文字方面存在显著不足，亟需进一步的研究和开发来提升对这些低资源语言和文字的处理能力。

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [145] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 现有AIG文本检测器在实际应用中表现不佳，原因在于它们缺乏鲁棒性，尤其是在少样本/单样本生成和特定领域微调模型上。本文提出了DACTYL数据集来应对这一挑战，并发现深度X风险优化（DXO）训练的分类器在域外（OOD）文本检测方面表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器在内部测试中表现良好，但在真实世界应用中却力不从心，表明其鲁棒性不足。大多数数据集侧重于零样本生成，而少样本或单样本生成（LLM以人类文本为例）以及特定领域持续预训练（CPT）模型的文本检测研究不足。

Method: 引入了DACTYL数据集，专注于单样本/少样本生成文本，并包含来自领域特定CPT语言模型的文本。使用两种方法训练分类器：标准二元交叉熵（BCE）优化和深度X风险优化（DXO）。

Result: 现有AIG文本检测器在DACTYL数据集上表现显著不佳。BCE训练的分类器在DACTYL测试集上略优于DXO分类器，但DXO分类器在域外（OOD）文本上表现出色。在学生论文检测的模拟部署场景中，最佳DXO分类器在最低误报率下比最佳BCE训练分类器高出50.56的宏F1分数。

Conclusion: DXO分类器在不发生过拟合的情况下表现出更好的泛化能力，尤其是在处理OOD文本时。本研究揭示了AIG文本检测器在单样本/少样本和CPT生成文本方面的潜在脆弱性，并指出了未来改进的方向。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [146] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文首次对医学领域中旨在增强推理能力的LLM进行了系统综述，提出了推理增强技术的分类法，并分析了其应用、评估基准及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在医学领域展现出强大能力，但其在执行系统、透明和可验证推理方面的不足是一个关键缺陷，而这正是临床实践的基石。这促使了专门为医学推理设计的LLM的发展。

Method: 本文采用系统综述的方法，分析了2022-2025年间的60项重要研究。研究提出了一种推理增强技术的分类法，分为训练时策略（如监督微调、强化学习）和测试时机制（如提示工程、多智能体系统）。同时，分析了这些技术在不同数据模态（文本、图像、代码）和关键临床应用（如诊断、教育、治疗规划）中的应用，并调查了评估基准的演变。

Result: 研究提出了医学LLM推理增强技术的分类法，详细分析了这些技术如何应用于不同数据模态和临床场景，并展示了评估基准从简单准确性指标向更复杂的推理质量和视觉可解释性评估的演变。

Conclusion: 研究指出了当前面临的关键挑战，包括忠实性-合理性鸿沟以及对原生多模态推理的需求，并展望了构建高效、鲁棒且对社会技术负责的医学AI的未来方向。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [147] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本研究针对波斯语和伊朗文化，创建了19个新的评估数据集，并对41个大型语言模型进行了基准测试，以弥补非英语和非西方文化背景下的LLM评估空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准主要针对英语，且大多数LLM训练数据源于欧美文化，导致其在其他语言和非西方文化背景下表现不佳或缺乏评估资源。

Method: 研究团队为波斯语和伊朗文化设计并引入了19个新的评估数据集，涵盖伊朗法律、波斯语法、波斯习语和大学入学考试等主题。随后，使用这些数据集对41个主流大型语言模型进行了基准测试。

Result: 通过创建和使用这些新数据集，成功对大型语言模型在波斯语和伊朗文化背景下的表现进行了评估，从而弥合了当前领域中存在的文化和语言评估差距。

Conclusion: 新引入的波斯语和伊朗文化评估数据集为评估LLM在特定非西方语言和文化背景下的质量和可靠性提供了重要资源，有助于提升LLM的普适性和文化适应性。

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [148] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 该论文提出一种序列句子对分类器（SSPC），结合预训练语言模型和双向LSTM，用于在句子层面检测写作风格变化，并在PAN-2025任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 写作风格变化检测是计算作者分析领域一个重要且具挑战性的问题，尤其是在细粒度（如句子层面）上。PAN 2025共享任务聚焦于此，并引入了包含主题多样性递增的数据集，其中“风格浅显”的短句是主要挑战。

Method: 本文提出一种序列句子对分类器（SSPC）。该架构首先使用预训练语言模型（PLM）获取单个句子的表示，然后将这些表示输入双向LSTM（BiLSTM）以获取文档内的上下文信息。接着，将相邻句子的BiLSTM输出向量拼接，并输入多层感知机（MLP）进行每对邻接句的风格变化预测。该方法相对轻量级，并能有效处理短句带来的挑战。

Result: 该模型在PAN-2025官方测试数据集上取得了强大的宏观F1分数：EASY数据集0.923，MEDIUM数据集0.828，HARD数据集0.724。这些结果不仅超越了官方随机基线，也优于Claude-3.7-sonnet的零样本性能。

Conclusion: 所提出的SSPC方法通过有效利用上下文信息，成功解决了细粒度风格变化检测（特别是短句）的挑战，在PAN-2025任务中展现出卓越的性能，证明了其在计算作者分析领域的有效性。

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [149] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever是一种法律判例检索系统，旨在应对海量法律文档挑战，通过有限的案件信息提取修辞重要片段，结合多种检索模型，为法律研究提供可靠且可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 普通法系遵循“遵循先例”原则，要求司法裁决保持一致性。然而，日益增长的法律文档复杂性和数量对传统检索方法构成了挑战。现实世界的法律搜索往往只有有限的案件信息，需要一种能应对这些限制的新方法。

Method: TraceRetriever系统采用流水线式架构，整合了BM25、向量数据库和交叉编码器模型进行初步检索。初始结果通过倒数排序融合（Reciprocal Rank Fusion）进行组合，然后进行最终重排序。修辞标注通过在印度判决书上训练的层次BiLSTM CRF分类器生成，系统仅提取修辞上重要的片段而非完整文档。

Result: TraceRetriever在IL-PCR和COLIEE 2025数据集上进行了评估，结果表明它能有效应对日益增长的文档量挑战，并与实际搜索约束保持一致。它为判例检索提供了一个可靠且可扩展的基础，在只有部分案件知识可用时，能增强法律研究。

Conclusion: TraceRetriever提供了一种可靠且可扩展的法律判例检索基础，特别适用于只有部分案件信息可用的场景，有效提升了法律研究的效率和准确性，同时应对了海量法律文档带来的挑战。

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [150] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本文评估了SOTA大型语言模型（LLMs）在句子级别写作风格变化检测这一零样本任务上的表现，发现它们对风格变化敏感，并建立了强劲的基线，且可能更侧重纯粹的风格信号。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在作者分析中最具挑战性的任务之一——句子级别风格变化检测上的零样本能力。

Method: 在PAN 2024和2025“多作者写作风格分析”官方数据集上，对四种先进的LLMs进行基准测试，评估其零样本性能。

Result: 1. SOTA生成模型对写作风格变化（即使是句子级别的细微变化）很敏感。2. 它们的准确性为该任务建立了具有挑战性的基线，优于PAN竞赛建议的基线。3. 最新一代LLMs可能比之前报道的对独立于内容的纯粹风格信号更敏感。

Conclusion: LLMs在句子级别风格变化检测上表现出强大的零样本能力，对细粒度风格变化敏感，并能识别内容无关的纯粹风格信号，为作者分析任务提供了新的视角和高性能基线。

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [151] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 提出NyayaRAG框架，通过整合事实、法律条文和先例，显著提升印度法律判决预测和解释的准确性。


<details>
  <summary>Details</summary>
Motivation: 法律判决预测（LJP）是法律AI的关键领域，旨在自动化司法结果预测并增强法律推理的可解释性。然而，现有印度LJP方法仅依赖案件内部内容，忽略了普通法系中法律条文和司法先例的核心作用。

Method: 提出NyayaRAG，一个检索增强生成（RAG）框架。该框架通过向模型提供事实案件描述、相关法律条文和语义检索的先前案例，模拟真实的法庭场景。它评估这些组合输入在预测法院判决和生成法律解释方面的有效性，并使用针对印度法律系统量身定制的领域特定管道。评估使用标准词汇/语义指标和基于LLM的评估器（如G-Eval）。

Result: 研究结果表明，将事实输入与结构化法律知识（法律条文和司法先例）相结合，显著提高了预测准确性和解释质量。

Conclusion: 结合法律条文和司法先例的RAG框架（NyayaRAG）能有效提升法律判决预测和解释的性能，为法律AI提供了更符合实际的解决方案。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [152] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: DAMR是一种新颖的知识图谱问答（KGQA）框架，它结合了蒙特卡洛树搜索（MCTS）、大型语言模型（LLM）规划器、基于Transformer的上下文感知评分器以及动态伪路径细化机制，以实现高效且上下文感知的多跳推理。


<details>
  <summary>Details</summary>
Motivation: 现有的KGQA方法存在局限性：基于GNN或启发式规则的静态路径提取方法适应性差，缺乏上下文细化；而基于LLM的动态路径生成方法计算成本高昂，且由于依赖固定评分函数和大量LLM调用，路径评估准确性不足。

Method: 本文提出了动态自适应MCTS推理（DAMR）框架。它以MCTS为骨干，由LLM规划器引导，每一步选择top-k相关关系以缩小搜索空间。为提高路径评估准确性，引入了轻量级Transformer评分器，通过交叉注意力联合编码问题和关系序列，进行上下文感知合理性估计。此外，为缓解高质量监督数据稀缺问题，DAMR整合了动态伪路径细化机制，定期从搜索过程中探索的部分路径生成训练信号，使评分器能持续适应推理轨迹的演变分布。

Result: 在多个KGQA基准测试上的大量实验表明，DAMR显著优于现有最先进的方法。

Conclusion: DAMR通过结合符号搜索与自适应路径评估，提供了一种高效且准确的知识图谱问答解决方案，有效解决了现有方法的局限性。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [153] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）对训练数据中信息的“脱离上下文溯因推理”能力，发现GPT-4o能根据行为推断虚拟聊天机器人的名称，并能更好地展现其特征行为。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在大量语料库上训练，但它们是否能对训练数据中存在的信息进行推理尚不清楚。本研究旨在探讨LLMs进行“脱离上下文溯因推理”（即根据观察推断最合理解释的能力）的情况。

Method: 设计实验，在LLMs上训练虚构聊天机器人的名称和行为描述（但不包括对话示例）。然后，通过观察聊天机器人的响应示例，测试LLM能否正确推断出其名称。此外，还研究了预先训练行为描述对模型迭代训练后展现特征行为的影响。

Result: OpenAI的GPT-4o模型在观察到特定聊天机器人的特征响应后，能够正确推断出至少一个聊天机器人的名称。研究还发现，预先用聊天机器人行为描述训练GPT-4o，能使其在迭代训练后更好地展现出这些特征行为。

Conclusion: 研究结果对LLMs的态势感知能力及其AI安全具有重要意义。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [154] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 本研究通过对310个GPT-4代理进行HEXACO人格问卷调查，评估了生成式代理在社会科学研究中代表人格的有效性，发现代理能展现出一致但与HEXACO框架部分对齐的人格结构，并存在模型特异性偏差。


<details>
  <summary>Details</summary>
Motivation: 生成式代理因其模拟人类互动和扮演角色的能力，被视为社会科学研究中人类参与者的经济高效替代品。本研究旨在探讨这些基于角色的代理在代表人格方面的有效性。

Method: 通过调查310个由GPT-4驱动的代理，重现了HEXACO人格量表实验。对代理的回答进行因子分析，并将其结果与2004年Ashton, Lee & Goldberg的原始人类研究结果进行比较。

Result: 1) 从代理的回答中恢复出了一致且可靠的人格结构，显示出与HEXACO框架的部分对齐。2) 在结合了充分策划的人口后，GPT-4内部导出的人格维度保持一致和可靠。3) 跨模型分析揭示了人格画像的变异性，表明存在模型特异性偏差和局限性。

Conclusion: 本研究为使用生成式代理进行社会科学研究的潜在益处和局限性提供了见解，并为设计一致且具有代表性的代理角色以最大限度地覆盖和代表人格特质提供了实用指导。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [155] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 该研究提出了一种代理式RAG框架，使LLM能自主分解放射学问题、迭代检索证据并生成响应，显著提高了放射学问答的诊断准确性并减少了幻觉，尤其对中小型模型效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统的放射学问答RAG系统通常依赖单步检索，难以处理复杂的临床推理任务，限制了其在临床决策中的应用。

Method: 提出了一种代理式RAG框架，允许大型语言模型（LLMs）自主分解放射学问题，从Radiopaedia迭代检索目标临床证据，并动态合成基于证据的响应。研究评估了24种不同架构、参数规模（0.5B至>670B）和训练范式（通用、推理优化、临床微调）的LLMs，使用了104个专家整理的放射学问题（来自RSNA-RadioQA和ExtendedQA数据集）。

Result: 代理式检索显著提高了平均诊断准确率（73% vs. 零样本提示的64%；P<0.001； vs. 传统在线RAG的68%；P<0.001）。在中型模型（如Mistral Large从72%提高到81%）和小型模型（如Qwen 2.5-7B从55%提高到71%）中增益最大，而超大型模型（>200B参数）变化最小（<2%提升）。此外，代理式检索减少了幻觉（平均9.4%），并在46%的案例中检索到临床相关上下文，显著增强了事实基础。即使临床微调模型也表现出显著改进（如MedGemma-27B从71%提高到81%）。

Conclusion: 代理式框架有潜力提高放射学问答的事实性和诊断准确性，特别是对于中型LLMs，未来的研究应进一步验证其临床实用性。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [156] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: 本文提出了GLiDRE模型，一个基于GLiNER思想的文档级关系抽取模型，在Re-DocRED数据集的少样本场景中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 文档级关系抽取（RE）由于需要建模跨句子的复杂实体交互而面临挑战，特别是在零样本或少样本设置下性能尚未充分探索。受到GLiNER模型在NER任务中展示的紧凑模型优于大型语言模型的启发，研究者希望为RE任务开发一个类似的高效模型。

Method: 引入了GLiDRE模型，该模型借鉴了GLiNER的关键思想来处理文档级关系抽取。研究者在Re-DocRED数据集上，针对不同的数据设置，将GLiDRE与现有最先进的模型进行了基准测试。

Result: GLiDRE在少样本场景中取得了最先进的性能。

Conclusion: GLiDRE是一个有效的文档级关系抽取模型，尤其在处理数据稀缺的少样本场景时表现出色。

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [157] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: 本研究提出了MMBERT，一个基于BERT的多模态框架，结合文本、语音和视觉模态，并采用MoE架构和三阶段训练范式，显著提升了中文仇恨言论检测的性能。


<details>
  <summary>Details</summary>
Motivation: 中文社交网络上的仇恨言论检测面临独特挑战，特别是由于规避传统文本检测系统的伪装技术广泛使用。尽管大型语言模型（LLMs）提高了仇恨言论检测能力，但现有工作主要集中在英文数据集，对中文语境下的多模态策略关注有限。

Method: 本研究提出了MMBERT，一个新颖的基于BERT的多模态框架，通过MoE（专家混合）架构整合了文本、语音和视觉模态。为解决MoE直接集成到BERT模型中的不稳定性，开发了渐进式三阶段训练范式。MMBERT包含模态特定专家、共享自注意力机制和基于路由器的专家分配策略，以增强对抗扰动的鲁棒性。

Result: 在多个中文仇恨言论数据集上的实证结果表明，MMBERT显著优于微调的BERT编码器模型、微调的LLMs以及使用上下文学习方法的LLMs。

Conclusion: MMBERT通过有效整合多模态信息和鲁棒的MoE架构，显著提升了中文仇恨言论检测的性能，克服了现有方法的局限性。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [158] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文提出了一个基于大型语言模型（LLM）生成Python代码的零样本解决方案，用于在SemEval-2025 Task 8的数据表问答（QA）任务中回答表格数据问题。


<details>
  <summary>Details</summary>
Motivation: 参与SemEval-2025 Task 8：DataBench表格数据问答任务，该任务目标是在不同领域的数据表上执行问答，并分为两个子任务（DataBench QA和DataBench Lite QA）。

Method: 开发了一个零样本解决方案，特别强调利用LLM进行代码生成。具体来说，提出了一个Python代码生成框架，通过优化的提示策略，利用最先进的开源LLM生成可执行的Pandas代码。

Result: 实验表明，不同的LLM在Python代码生成方面表现出不同的有效性。此外，Python代码生成在表格问答方面比其他方法取得了更好的性能。在开源模型类别中，该系统在子任务I中排名第八，在子任务II中排名第六（在30个超越基线的系统中）。

Conclusion: 基于LLM生成Python代码的方法在表格数据问答任务中表现出色，尤其是在零样本设置下，并且优于其他替代方案。

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [159] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本研究引入了MISGENDERED+基准，评估了最新大型语言模型在处理性别代词（包括中性代词和新代词）方面的准确性，发现虽然在传统代词上有所改进，但在新代词和反向推理任务上仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）被广泛应用于敏感场景，公平性和包容性至关重要。代词使用，特别是中性代词和新代词，是负责任AI的关键挑战。现有基准（如MISGENDERED）揭示了早期LLMs的局限性，但其模型和评估方法已过时，需要更新和扩展的评估。

Method: 引入并更新了MISGENDERED+基准，用于评估LLMs的代词保真度。选择了五种代表性LLMs（GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, Qwen2.5），并在零样本、少样本以及性别身份推理等多种设置下进行基准测试。

Result: 与先前的研究相比，最新LLMs在二元和性别中性代词准确性方面有显著改进。然而，在新代词的准确性以及反向推理任务上表现仍然不一致，这凸显了在身份敏感推理方面持续存在的差距。

Conclusion: 尽管LLMs在处理常规代词方面有所进步，但在处理新代词和进行反向身份推理方面仍存在不足，这表明在实现完全包容的AI方面仍需进一步研究和改进。

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [160] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: DAEDAL是一种无需训练的去噪策略，通过动态自适应长度扩展，解决了扩散大语言模型（DLLMs）固定生成长度的限制，提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）虽然有并行生成和全局上下文建模的优势，但其需要预定义静态生成长度的架构限制，导致在复杂任务上性能受损或计算开销过大。作者观察到模型本身包含与最佳响应长度相关的内部信号，希望利用这些信号克服静态长度约束。

Method: DAEDAL策略分为两个阶段：1) 在去噪过程之前，从一个短的初始长度开始，通过序列完成度指标指导，迭代扩展到一个粗略的、适合任务的长度。2) 在去噪过程中，DAEDAL通过插入掩码标记，动态识别并扩展不足的生成区域，确保最终输出的完整性。此方法无需额外训练。

Result: 实验表明，DAEDAL在DLLMs上的性能与精心调优的固定长度基线相当，在某些情况下甚至更优。同时，通过实现更高的有效token比率，显著提升了计算效率。

Conclusion: DAEDAL成功解决了DLLMs的静态长度限制，释放了其潜力，缩小了与自回归大语言模型之间的差距，为更高效、更强大的生成铺平了道路。

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR的跨平台扩展现实机器人遥操作框架，旨在解决大规模、高质量机器人示教数据集收集的扩展性、复杂性和数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型的快速发展急需大规模、高质量的机器人示教数据集。然而，当前主流的遥操作数据收集方法存在可扩展性有限、设置复杂和数据质量不佳的问题。

Method: 本文提出了XRoboToolkit，一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架。该系统具有低延迟立体视觉反馈、基于优化的逆运动学，并支持多种跟踪模式（头部、控制器、手部和辅助运动跟踪器）。其模块化架构实现了机器人平台和仿真环境（包括精密机械臂、移动机器人和灵巧手）之间的无缝集成。

Result: 该框架在精密操作任务中展示了有效性，并通过训练表现出强大自主性能的VLA模型，验证了所收集数据的高质量。

Conclusion: XRoboToolkit是一个有效的框架，能够实现可扩展、高质量的机器人数据收集，为VLA模型训练提供支持。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [162] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: 本文提出了一种名为CHILD的紧凑型可重构遥操作系统，支持人形机器人进行全身关节级别的遥操作，并集成了自适应力反馈以提升操作体验和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作系统很少支持人形机器人全身关节级别的遥操作，这限制了机器人能完成任务的多样性。

Method: 开发了CHILD系统，该系统紧凑且可重构，可放入标准婴儿背带中，允许操作员控制机器人的所有四肢，支持全身直接关节映射控制和运动-操作。系统还融入了自适应力反馈，以提升操作员体验并防止不安全的关节运动。

Result: 通过在人形机器人和多个双臂系统上进行运动-操作和全身控制示例，验证了CHILD系统的能力。此外，硬件设计已开源。

Conclusion: CHILD系统为人形机器人提供了强大的全身关节级遥操作能力，扩展了其任务多样性，并通过力反馈提高了操作安全性与体验，并促进了研究的普及与复现。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [163] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种结合伪刚体模型和莫尔斯理论的拓扑形态描述符，用于软连续体机器人的形态量化表征、分类和控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对软连续体机器人的多模态形态进行定量描述和分类，限制了其在需要高精度和适应性的医疗应用中的潜力。

Method: 通过将伪刚体（PRB）模型与莫尔斯理论相结合，提出了一种拓扑形态描述符。该方法通过计算方向投影的临界点，实现多模态构型的离散表示和形态分类。此外，将目标构型表述为优化问题，利用该描述符计算驱动参数，以生成具有所需拓扑特征的平衡形状，从而实现形态控制。

Result: 所提出的描述符能够实现多模态构型的离散表示，促进形态分类。它还提供了一种形态控制方法，通过优化计算驱动参数来生成具有所需拓扑特征的形状。该框架为软连续体机器人的定量形态描述、分类和控制提供了一个统一的方法。

Conclusion: 该框架为软连续体机器人的定量形态描述、分类和控制提供了一种统一的方法，有望提高其在微创手术和血管内介入等医疗应用中的精度和适应性。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [164] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: 本文提出了UAV-ON，一个大规模空中目标导航（ObjectNav）基准，旨在推动无人机在开放世界环境中基于高级语义目标而非详细指令的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多遵循视觉与语言导航（VLN）范式，过度依赖顺序语言指令，限制了可扩展性和自主性。空中导航作为具身智能的基本能力，在大型非结构化环境中至关重要，但仍未得到充分探索。

Method: UAV-ON基准包含14个高保真虚幻引擎环境，具有多样化的语义区域和复杂空间布局（城市、自然、混合使用）。它定义了1270个带注释的目标对象，每个对象都通过实例级指令编码类别、物理足迹和视觉描述符，作为语义目标。为评估基准，实现了包括空中目标导航智能体（AOA）在内的多个基线方法，AOA是一个模块化策略，整合指令语义与自我中心观察进行长程、目标导向的探索。

Result: 实证结果表明，所有基线方法在该设置下都表现不佳，凸显了空中导航和语义目标接地的复合挑战。

Conclusion: UAV-ON旨在通过语义目标描述，推动无人机在复杂真实世界环境中可扩展自主性的研究进展。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [165] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一种基于扩散模型的多模态轨迹预测框架，它通过融合拓扑度量地图信息，生成准确、多样且符合道路几何的未来运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测方法在生成符合道路几何的轨迹时，常依赖显式约束，限制了其自然性和准确性。本研究旨在开发一种能自然地遵循道路几何，并生成准确、多样化预测的方法。

Method: TopoDiffuser是一个条件扩散模型，将拓扑度量地图的结构线索嵌入到去噪过程中，从而使轨迹生成自然地遵守道路几何。一个多模态条件编码器将激光雷达观测、历史运动和路线信息融合为统一的鸟瞰图（BEV）表示。

Result: 在KITTI基准测试中，TopoDiffuser超越了现有最先进的方法，并保持了强大的几何一致性。消融研究进一步验证了每种输入模态、去噪步数和轨迹样本数量的贡献。

Conclusion: TopoDiffuser通过将拓扑度量地图信息融入扩散模型，成功实现了准确、多样且符合道路几何的多模态轨迹预测，并在性能上优于现有方法。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [166] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: 该论文提出了Omni-Scan，一个利用双臂机器人抓取并旋转物体以生成高质量3D高斯泼溅（3DGS）模型的流水线，通过重新抓取来暴露被遮挡的表面，实现物体的全方位3D模型，并将其应用于零件缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统的3D物体扫描方法（如多相机阵列、精密激光扫描仪或机器人腕部安装相机）通常需要受限的工作空间。研究旨在开发一种新的方法，能够克服这些限制，生成高质量、全方位（360度）的3D高斯泼溅模型。

Method: Omni-Scan流水线使用双臂机器人：一个机械臂抓取并旋转物体，另一个机械臂重新抓取以暴露被第一个机械臂遮挡的表面。该流水线利用DepthAnything、Segment Anything和RAFT光流模型来识别和隔离被机械臂抓取的物体，同时去除机械臂和背景。此外，它修改了3DGS训练流程，以支持包含机械臂遮挡的拼接数据集，从而生成物体的全方位模型。

Result: Omni-Scan能够生成物体的全方位（360度视图）3D高斯泼溅模型。将其应用于零件缺陷检测时，在12种不同的工业和家用物体上，识别视觉或几何缺陷的平均准确率达到83%。

Conclusion: Omni-Scan提供了一种有效且高质量的3D高斯泼溅模型生成方法，能够克服传统扫描方法的局限性，实现物体的全方位建模。该技术在零件缺陷检测等应用中表现出良好的性能，具有实际应用价值。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [167] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的时间优化策略（TOP），用于训练人形机器人站立姿态下的操作控制模型，旨在同时实现平衡、精度和时间效率，通过调整上半身运动的时间轨迹来减轻平衡负担。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人站立控制器难以精确控制高维度的上半身关节，或者难以同时保证鲁棒性和准确性，尤其是在上半身运动快速时，容易导致机器人不稳定。

Method: 该方法包含三个部分：1) 利用变分自编码器（VAE）学习上半身运动先验，以增强上下半身的协调能力。2) 将全身控制解耦为上半身PD控制器（用于精度）和下半身RL控制器（用于鲁棒稳定性）。3) 训练时间优化策略（TOP），与解耦控制器和VAE结合，以减少由快速上半身运动引起的平衡负担，防止机器人失稳。

Result: 通过仿真和真实世界实验验证了所提方法的有效性，结果表明在站立操作任务中，该方法能够稳定、准确地表现出卓越的性能。

Conclusion: 所提出的时间优化策略（TOP）能够有效解决人形机器人在站立操作任务中平衡、精度和时间效率的挑战，通过优化上半身运动的时间轨迹，显著提升了机器人的稳定性和操作能力。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [168] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的全身运动模仿框架，使全尺寸人形机器人能够准确、平衡地模仿人类运动，并克服运动学和动力学差异。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动时，由于机器人与人类在运动学和动力学上的显著差异，难以在保持平衡的同时准确模仿复杂的、富有表现力的动作。

Method: 该方法采用接触感知全身运动重定向技术来模仿人类运动并提供参考轨迹的初始值；使用非线性质心模型预测控制器实时确保运动精度、保持平衡并克服外部干扰；全身控制器辅助实现更精确的扭矩控制。

Result: 在仿真和真实人形机器人上进行的多种人类运动模仿实验表明，该框架能够实现高精度和强适应性的运动表现。

Conclusion: 所提出的全身运动模仿框架有效，能够使人形机器人准确、自适应地模仿人类运动，并克服运动学和动力学差异带来的挑战。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [169] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: 本文提出NIVA，一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，实现闭环、条件观测的模拟，并能统一现有模型，在Waymo数据集上表现优异并提供意图和驾驶风格控制。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AV）的快速部署迭代，对构建逼真且可扩展的多智能体交通模拟器以进行高效评估的需求日益增长，尤其需要能生成多样化和交互式场景的闭环模拟器。

Method: 引入Neural Interactive Agents (NIVA)，一个由分层贝叶斯模型驱动的概率框架。它通过从潜在的、有限高斯混合分布中自回归采样，实现闭环、观察条件下的模拟。NIVA从贝叶斯推断的角度统一了现有的序列到序列轨迹预测模型和新兴的基于下一令牌预测（NTP）的闭环模拟模型。

Result: 在Waymo开放运动数据集上的实验表明，NIVA与现有方法相比达到了有竞争力的性能。

Conclusion: NIVA在提供竞争性性能的同时，还增强了对智能体意图和驾驶风格的控制，并从贝叶斯推断视角统一了多种预测和模拟模型。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [170] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 该论文提出了一种名为SubCDM的子集式集体决策方法，允许机器人群体仅使用部分机器人进行决策，从而节省资源，同时保持与全群体决策相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人群体集体决策策略需要所有机器人参与，这导致资源消耗大，并限制了机器人群体同时执行其他任务的能力。

Method: 本文提出子集式集体决策（SubCDM）方法，仅使用机器人群体的一个子集进行决策。该子集的构建是动态和去中心化的，仅依赖局部信息。该方法能根据达成共识的难度，自适应地确定子集大小以确保决策准确性。

Result: 使用一百个机器人进行的仿真结果表明，所提出的方法在实现与整个群体决策相当的准确性的同时，显著减少了参与集体决策所需的机器人数量。

Conclusion: SubCDM为群体机器人中的集体决策提供了一种资源高效的解决方案，它在不牺牲准确性的前提下，降低了决策过程的资源消耗。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [171] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 本文提出了一种基于模仿学习的方法HannesImitationPolicy，用于控制Hannes假肢手在非结构化环境中抓取物体，并构建了HannesImitationDataset数据集，通过扩散策略实现了比传统视觉伺服控制器更好的抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有假肢手控制系统侧重于通过传感器增加自主性以减少用户认知负担，但模仿学习在假肢控制中的应用仍未充分探索。将模仿学习应用于假肢可提高灵巧性，使设备能在更不受限的场景中通过示范学习任务，而非依赖手动标注序列。

Method: 提出HannesImitationPolicy，一种基于模仿学习的Hannes假肢手控制方法。构建了HannesImitationDataset数据集，包含桌面、货架和人机交接场景下的抓取示范。利用该数据集训练一个单一的扩散策略，预测手腕姿态和手部闭合以实现抓取。

Result: 实验评估表明，该策略在多种物体和条件下实现了成功的抓取。在非结构化场景中，该策略的表现优于基于分割的视觉伺服控制器。

Conclusion: 基于模仿学习的HannesImitationPolicy结合扩散策略和HannesImitationDataset，能有效控制Hannes假肢手在非结构化环境中进行物体抓取，并在性能上超越了传统的视觉伺服控制器，为假肢控制提供了新的有效途径。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [172] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: 该研究提出了一种名为OmniUnet的基于Transformer的神经网络架构，利用RGB、深度和热（RGB-D-T）多模态图像进行语义分割，以支持火星探测器在非结构化环境中的安全导航，并在火星模拟环境中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在非结构化环境中安全导航，这需要多模态感知系统来整合互补信息。现有的机器学习算法需要专门设计来利用异构数据，并且需要识别对导航最有信息量的传感器模态。在火星探索中，热成像已被证明对评估地形安全性具有重要价值。

Method: 开发了OmniUnet，一个基于Transformer的神经网络架构，用于RGB-D-T图像的语义分割。设计并3D打印了定制的多模态传感器外壳，并安装在火星漫游车自主测试平台（MaRTA）上。在西班牙巴德纳斯半沙漠（模拟火星环境）收集了多模态数据集，并手动标注了部分数据用于监督训练。模型通过定量和定性方法进行评估。

Result: OmniUnet在像素精度上达到80.37%，在分割复杂非结构化地形方面表现出色。在资源受限的Jetson Orin Nano计算机上，平均预测时间为673毫秒，证实了其在机器人上部署的适用性。网络软件实现和标注数据集已公开。

Conclusion: OmniUnet是一种有效的多模态地形感知方法，适用于行星机器人，其性能和部署效率证明了其在资源受限机器人平台上的实用性，并为未来的相关研究提供了宝贵资源。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [173] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 本文提出了一种用于人机协作物体运输的控制方案，该方案基于导纳控制，结合了变阻尼以提高人类可控性和降低努力，并引入障碍人工势以防止物体脱落，通过实验验证了其性能和被动性。


<details>
  <summary>Details</summary>
Motivation: 旨在实现人机协作的物体运输，同时提高人类的操作可控性、降低其努力，并确保物体在协作过程中不会脱落。

Method: 提出了一种基于导纳控制的方案，其中包含一个可变阻尼项；引入了一个基于障碍人工势的附加控制信号以防止物体脱落；对所提出的控制方案进行了被动性证明；通过配备MIGHTY吸盘的Unitree Go1机器人进行了实验评估。

Result: 所提出的控制方案被证明具有被动性，并通过实验评估展示了其有效性能，确保了在人机协作运输过程中物体不会从吸盘上脱落。

Conclusion: 提出的控制方案能有效实现人机协作物体运输，通过变阻尼提高了人类的可控性和降低了努力，并通过障碍人工势保证了物体不脱落，且具有被动性，实验验证了其有效性。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [174] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout v1.1是一款开源硬件移动机器人，通过简化、降低成本并增强计算硬件，同时提供ROS2接口和Gazebo仿真，成为研究和工业的案例研究。


<details>
  <summary>Details</summary>
Motivation: 为了改进OpenScout机器人，使其更易于获取、成本更低、计算能力更强，并提供更好的仿真支持。

Method: 项目采用了开源硬件（OSH）的方法论，具体实施包括：简化并优化了机载计算硬件，开发了模拟的ROS2接口，并创建了Gazebo仿真环境。

Result: 成果是OpenScout v1.1版本，它拥有更简化、更便宜且更强大的机载计算硬件，并提供了ROS2接口和Gazebo仿真，作为一个OSH案例研究进行了报告。

Conclusion: OpenScout v1.1的发布展示了通过开源硬件方法改进移动机器人的可行性与成效，为研究和工业提供了更优的平台，并可作为OSH项目开发的参考案例。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [175] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 该研究提出了一种基于数据驱动的踝关节力矩估计方法，利用时间卷积网络（TCN）和惯性测量单元（IMU）为中风后偏瘫患者的步态康复提供自适应外骨骼辅助，并验证了其实时性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的外骨骼控制方法已成功应用于健康年轻人，但在中风后等神经运动步态缺陷人群中面临挑战，原因包括患者异质性高、步态变异性大以及缺乏相关数据集。然而，此类方法仍有望使外骨骼在非结构化社区环境中安全有效地提供自适应辅助。

Method: 本研究旨在通过数据驱动的力矩估计实现中风后步行的自适应跖屈和背屈辅助。方法包括：1) 使用从四名中风后参与者收集的跑步机步行数据训练一个多任务时间卷积网络（TCN）。2) 模型输入为三个惯性测量单元（IMU）的数据。3) 模型在六名健康参与者的步行数据上进行了预训练。4) 开发并实现了踝关节力矩估计的可穿戴原型，用于外骨骼控制。

Result: 该多任务TCN模型在中风后步行数据上的踝关节力矩估计R²达到了0.74 ± 0.13。研究还通过一名中风后参与者，成功展示了该可穿戴原型在实时传感、估计和驱动方面的可行性。

Conclusion: 这项工作是实现中风后步行数据驱动自适应踝关节辅助的第一步，成功验证了其在实时力矩估计和驱动方面的可行性，为未来在神经运动步态缺陷人群中应用数据驱动的外骨骼控制奠定了基础。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [176] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP是一个新颖的框架，通过网络压缩和减少采样步数，显著加速了扩散策略（Diffusion Policies）在资源受限的移动设备上的实时部署，同时保持了竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作任务中表现出色，但其计算效率低下和内存占用大，限制了它们在资源受限的移动平台上的应用。

Method: LightDP通过两种核心策略解决计算瓶颈：1) 对去噪模块进行网络压缩，采用统一的剪枝和再训练流程，以优化模型在剪枝后的恢复能力。2) 结合剪枝技术和一致性蒸馏（consistency distillation）来有效减少采样步数，同时保持动作预测精度。

Result: LightDP在PushT、Robomimic、CALVIN和LIBERO等标准数据集上实现了移动设备的实时动作预测，并保持了有竞争力的性能。广泛的真实世界实验也表明，LightDP的性能与最先进的扩散策略相当。

Conclusion: LightDP是使扩散策略在资源有限环境中实际部署的重要一步，它在移动设备上实现了实时动作预测，并具有与现有技术相当的性能。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [177] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 该论文提出“视频策略”框架，利用视频生成作为机器人策略学习的代理，以解决视觉运动策略在泛化能力差和对大量演示数据依赖的问题，显著提升了鲁棒性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉运动策略在感知或行为分布变化下泛化能力不足，且性能受限于人类演示数据量。

Method: 提出“视频策略”框架，一个模块化的视频和动作生成结合体，可端到端训练。通过学习生成机器人行为视频，从中提取策略。

Result: 该方法仅需极少量演示数据，显著提升了鲁棒性和样本效率。在模拟和现实世界中，对未见物体、背景和任务展现出强大的泛化能力。任务成功与生成的视频紧密相关，无动作视频数据对泛化到新任务具有关键益处。

Conclusion: 通过利用大规模视频生成模型，该方法比传统行为克隆表现更优，为更具可扩展性和数据效率的机器人策略学习铺平了道路。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [178] [Data-Driven Motion Planning for Uncertain Nonlinear Systems](https://arxiv.org/abs/2508.00154)
*Babak Esmaeili,Hamidreza Modares,Stefano Di Cairano*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的非线性系统运动规划框架，通过构建一系列重叠的不变多面体，实现安全、动态可行的路径规划，且无需系统动力学模型。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法依赖于精确的系统动力学模型，而本文旨在开发一种仅需数据即可计算安全区域并设计状态反馈控制器的方法，以应对复杂非线性系统模型难以获取或不准确的问题。

Method: 该方法围绕随机采样的路径点，识别凸可容许区域，并解决数据驱动的线性矩阵不等式问题（LMI），学习多个椭球不变集及其局部状态反馈增益。这些椭球的凸包（在分段仿射控制器下仍保持不变）被近似为多面体。通过验证连续凸包多面体的交集并引入中间节点，确保节点间的安全平滑过渡。控制增益通过单纯形插值实时插值，确保状态始终保持在不变多面体内部。

Result: 通过仿真验证，所提出的方法能够有效地为复杂非线性系统生成安全、动态可行的路径。

Conclusion: 本文提出的数据驱动运动规划框架，无需显式系统动力学模型，仅依赖数据即可为非线性系统规划安全、动态可行的路径，显示出其在实际应用中的有效性和潜力。

Abstract: This paper proposes a data-driven motion-planning framework for nonlinear
systems that constructs a sequence of overlapping invariant polytopes. Around
each randomly sampled waypoint, the algorithm identifies a convex admissible
region and solves data-driven linear-matrix-inequality problems to learn
several ellipsoidal invariant sets together with their local state-feedback
gains. The convex hull of these ellipsoids, still invariant under a
piece-wise-affine controller obtained by interpolating the gains, is then
approximated by a polytope. Safe transitions between nodes are ensured by
verifying the intersection of consecutive convex-hull polytopes and introducing
an intermediate node for a smooth transition. Control gains are interpolated in
real time via simplex-based interpolation, keeping the state inside the
invariant polytopes throughout the motion. Unlike traditional approaches that
rely on system dynamics models, our method requires only data to compute safe
regions and design state-feedback controllers. The approach is validated
through simulations, demonstrating the effectiveness of the proposed method in
achieving safe, dynamically feasible paths for complex nonlinear systems.

</details>


### [179] [Integrating Opinion Dynamics into Safety Control for Decentralized Airplane Encounter Resolution](https://arxiv.org/abs/2508.00156)
*Shuhao Qi,Zhiqi Tang,Zhiyong Sun,Sofie Haesaert*

Main category: eess.SY

TL;DR: 该研究提出将生物启发式非线性意见动力学整合到飞机安全控制框架中，以解决分散式冲突解决中出现的阻塞问题，同时确保安全和无阻塞的快速协调。


<details>
  <summary>Details</summary>
Motivation: 随着空域日益拥堵，分散式冲突解决方法变得至关重要。然而，现有的分散式安全控制器虽然能防止碰撞，但不能总是确保及时解决冲突，导致飞机进度在某些情况下可能被长时间阻塞。

Method: 本文提出将生物启发式非线性意见动力学集成到飞机安全控制框架中。这种方法使安全控制器能够实现协作决策，从而解决阻塞问题，并在不依赖通信或预设规则的情况下促进快速、安全的协调。

Result: 广泛的仿真结果验证了该方法能提高飞行效率并提供安全保障。意见动力学使得安全控制器能够进行协作决策，从而实现无阻塞的冲突解决，并促进快速、安全的协调。

Conclusion: 这项研究为设计飞机的自主控制器提供了实用的见解，表明生物启发式意见动力学可以有效解决分散式空域管理中的阻塞问题，同时保持高安全性和效率。

Abstract: As the airspace becomes increasingly congested, decentralized conflict
resolution methods for airplane encounters have become essential. While
decentralized safety controllers can prevent dangerous midair collisions, they
do not always ensure prompt conflict resolution. As a result, airplane progress
may be blocked for extended periods in certain situations. To address this
blocking phenomenon, this paper proposes integrating bio-inspired nonlinear
opinion dynamics into the airplane safety control framework, thereby
guaranteeing both safety and blocking-free resolution. In particular, opinion
dynamics enable the safety controller to achieve collaborative decision-making
for blocking resolution and facilitate rapid, safe coordination without relying
on communication or preset rules. Extensive simulation results validate the
improved flight efficiency and safety guarantees. This study provides practical
insights into the design of autonomous controllers for airplanes.

</details>


### [180] [Adaptive Compensation of Nonlinear Friction in Mechanical Systems Without Velocity Measurement](https://arxiv.org/abs/2508.00175)
*Jose Guadalupe Romero,Romeo Ortega,Leyan Fang,Alexey Bobtsov*

Main category: eess.SY

TL;DR: 本文提出了一种无需速度测量的全局收敛跟踪控制器，用于补偿机械系统中的静摩擦和库仑摩擦，并首次实现了该问题的全局收敛解。


<details>
  <summary>Details</summary>
Motivation: 摩擦是机械系统中不可避免的现象，严重阻碍精确伺服控制。现有摩擦补偿方案大多依赖难以获得的速度测量，且其数学模型包含多个未知参数，有些还是非线性参数，这构成了研究的动机。

Method: 本文提出了一种全局收敛的跟踪控制器，其关键组件是一个基于沉浸与不变性（immersion and invariance）的自适应速度观测器，用于摩擦补偿。该方法适用于静摩擦和库仑摩擦模型，并已在更复杂的LuGre摩擦模型下进行了仿真验证。

Result: 该研究首次提出了一个针对摩擦补偿问题的全局收敛解决方案，并且在不需要速度测量的情况下实现了对受静摩擦和库仑摩擦扰动机械系统的有效控制。仿真结果也验证了所提出的观测器对受LuGre模型影响系统的有效性。

Conclusion: 本研究成功开发了一种创新的摩擦补偿方法，解决了现有技术对速度测量依赖的局限性，并实现了全局收敛性，为精确伺服控制提供了更可靠的解决方案。

Abstract: Friction is an unavoidable phenomenon that exists in all mechanical systems
incorporating parts with relative motion. It is well-known that friction is a
serious impediment for precise servo control, hence the interest to devise a
procedure to compensate for it -- a subject that has been studied by many
researchers for many years. The vast majority of friction compensation schemes
reported in the literature rely on the availability of velocity measurements,
an information that is hard to obtain. A second limitation of the existing
procedures is that they rely on mathematical models of friction that contain
several unknown parameters, some of them entering nonlinearly in the dynamic
equations. In this paper we propose a globally convergent tracking controller
for a mechanical system perturbed by static and Coulomb friction, which is a
reliable mathematical model of the friction phenomenon, that does not rely one
measurement of velocity. The key component is an immersion and invariance-based
adaptive speed observer, used for the friction compensation. To the best of our
knowledge, this is the first globally convergent solution to this challenging
problem. We also present simulation results of the application of our observer
for systems affected by friction, which is described by the more advanced LuGre
model.

</details>


### [181] [Optimal Messaging Strategy for Incentivizing Agents in Dynamic Systems](https://arxiv.org/abs/2508.00188)
*Renyan Sun,Ashutosh Nayyar*

Main category: eess.SY

TL;DR: 本文研究一个有限时域离散时间动态系统，设计师通过选择性信息披露和直接行动来激励代理人遵循特定策略，同时最大化自身收益。提出了一种基于序贯理性的激励相容性概念，并展示了最优设计师策略可通过反向归纳算法（求解一系列线性规划）计算。


<details>
  <summary>Details</summary>
Motivation: 在设计师与代理人共同控制的动态系统中，设计师希望通过信息披露和自身行动来激励代理人遵循预设策略，同时最大化自身收益。这需要解决如何在满足激励相容性的前提下设计最优的信息传递和行动策略。

Method: 该研究建模了一个有限时域离散时间动态系统，其中设计师通过选择性信息披露和直接行动来影响系统。代理人根据接收到的信息选择行动。研究引入了基于设计师和代理人之间公共信息序贯理性的激励相容性概念。目标是寻找最大化设计师预期总奖励，同时激励代理人遵循预设策略的信令与行动策略。在特定信息结构假设下，通过求解一系列线性规划的反向归纳算法来计算最优策略。

Result: 主要结果是证明了在问题信息结构的特定假设下，可以利用一个求解一系列线性规划的反向归纳算法来计算设计师的最优信令与行动策略。

Conclusion: 该研究得出结论，在设计师与代理人共同控制的动态系统中，通过精心的信息披露和行动策略设计，可以有效地激励代理人遵循预设策略，同时最大化设计师的收益。提出的基于反向归纳和线性规划的计算方法为解决此类问题提供了一个可行的框架，但其适用性依赖于特定的信息结构假设。

Abstract: We consider a finite-horizon discrete-time dynamic system jointly controlled
by a designer and one or more agents, where the designer can influence the
agents' actions through selective information disclosure. At each time step,
the designer sends a message to the agent(s) from a prespecified message space.
The designer may also take an action that directly influences system dynamics
and rewards. Each agent uses its received message (and its own information) to
choose its action. We are interested in the setting where the designer would
like to incentivize each agent to play a specific strategy. We consider a
notion of incentive compatibility that is based on sequential rationality at
each realization of the common information between the designer and the
agent(s). Our objective is to find a messaging and action strategy for the
designer that maximizes its total expected reward while incentivizing each
agent to follow a prespecified strategy. Under certain assumptions on the
information structure of the problem, we show that an optimal designer strategy
can be computed using a backward inductive algorithm that solves a family of
linear programs.

</details>


### [182] [Neural Co-state Projection Regulator: A Model-free Paradigm for Real-time Optimal Control with Input Constraints](https://arxiv.org/abs/2508.00283)
*Lihan Lian,Uduak Inyang-Udoh*

Main category: eess.SY

TL;DR: 本文提出了一种名为神经余态投影调节器（NCPR）的模型无关学习型最优控制框架，它基于庞特里亚金最小原理（PMP），用于解决带输入约束的非线性控制仿射系统的二次调节器问题，并展现出优越的泛化能力和采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有的学习方法，特别是强化学习（RL），在解决最优控制任务时存在样本效率低、对奖励设计和超参数敏感、以及泛化能力差（尤其是在存在输入约束的情况下）等问题。

Method: NCPR框架通过自监督学习训练一个神经网络（NN），使其以当前系统状态为输入，预测有限时间范围内的投影余态轨迹（即余态乘以系统输入增益）。随后，仅提取NN预测的第一个元素来求解一个轻量级的二次规划（QP）问题。此工作流程在反馈控制设置中执行，实现满足输入约束和一阶最优条件的实时控制动作计算。

Result: 该方法在独轮车模型机器人参考跟踪问题和摆锤向上摆动任务上进行了测试。与强化学习和基于模型的控制器相比，NCPR在未见过的系统状态和变化的输入约束方面表现出卓越的泛化能力，并显著提高了采样效率。

Conclusion: NCPR是一种有效的模型无关学习型最优控制框架，它结合了庞特里亚金最小原理，能够有效解决带输入约束的非线性系统控制问题，克服了传统强化学习的局限性，实现了更好的泛化性能和更高的采样效率。

Abstract: Learning-based approaches, notably Reinforcement Learning (RL), have shown
promise for solving optimal control tasks without explicit system models.
However, these approaches are often sample-inefficient, sensitive to reward
design and hyperparameters, and prone to poor generalization, especially under
input constraints. To address these challenges, we introduce the neural
co-state projection regulator (NCPR), a model-free learning-based optimal
control framework that is grounded in Pontryagin's Minimum Principle (PMP) and
capable of solving quadratic regulator problems in nonlinear control-affine
systems with input constraints. In this framework, a neural network (NN) is
trained in a self-supervised setting to take the current state of the system as
input and predict a finite-horizon trajectory of projected co-states (i.e., the
co-state weighted by the system's input gain). Subsequently, only the first
element of the NN's prediction is extracted to solve a lightweight quadratic
program (QP). This workflow is executed in a feedback control setting, allowing
real-time computation of control actions that satisfy both input constraints
and first-order optimality conditions.
  We test the proposed learning-based model-free quadratic regulator on (1) a
unicycle model robot reference tracking problem and (2) a pendulum swing-up
task. For comparison, reinforcement learning is used on both tasks; and for
context, a model-based controller is used in the unicycle model example. Our
method demonstrates superior generalizability in terms of both unseen system
states and varying input constraints, and also shows improved sampling
efficiency.

</details>


### [183] [Low-dimensional observer design for stable linear systems by model reduction](https://arxiv.org/abs/2508.00609)
*M. F. Shakib,M. Khalil,R. Postoyan*

Main category: eess.SY

TL;DR: 本文提出了一种基于矩匹配模型降阶的低维观测器设计方法，用于稳定、单输入单输出的连续时间线性时不变系统，可实现特定输入的精确渐近状态重构和通用输入的有界估计误差。


<details>
  <summary>Details</summary>
Motivation: 为稳定、单输入单输出的连续时间线性时不变（LTI）系统设计一个低维观测器，以有效估计系统状态。

Method: 首先，利用矩匹配技术对原始系统进行模型降阶，得到一个降阶模型。然后，基于该降阶模型设计一个低维观测器来估计原始系统的状态。

Result: 该观测器对于与观测器维度相关的特定输入类别，能够实现精确的渐近状态重构。对于通用输入，该观测器具有指数输入到状态稳定性，确保了有界的估计误差。数值仿真证实了该方法的有效性。

Conclusion: 所提出的低维观测器设计方法，通过结合模型降阶技术，能有效估计LTI系统状态，并在不同输入条件下提供鲁棒的性能（精确重构或有界误差）。

Abstract: This paper presents a low-dimensional observer design for stable,
single-input single-output, continuous-time linear time-invariant (LTI)
systems. Leveraging the model reduction by moment matching technique, we
approximate the system with a reduced-order model. Based on this reduced-order
model, we design a low-dimensional observer that estimates the states of the
original system. We show that this observer establishes exact asymptotic state
reconstruction for a given class of inputs tied to the observer's dimension.
Furthermore, we establish an exponential input-to-state stability property for
generic inputs, ensuring a bounded estimation error. Numerical simulations
confirm the effectiveness of the approach for a benchmark model reduction
problem.

</details>


### [184] [Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks](https://arxiv.org/abs/2508.00637)
*Michał Forystek,Andrew D. Syrmakesis,Alkistis Kontou,Panos Kotsampopoulos,Nikos D. Hatziargyriou,Charalambos Konstantinou*

Main category: eess.SY

TL;DR: 本文提出了一个开源协同仿真环境，用于建模电力系统及其通信网络，以分析负载改变攻击（LAAs）和动态负载改变攻击（DLAAs）对负载频率控制（LFC）和低频减载（UFLS）等电网保护机制的影响。


<details>
  <summary>Details</summary>
Motivation: 信息通信技术（ICT）融入电网带来益处的同时也引入了网络威胁，特别是负载改变攻击（LAAs）及其动态形式（DLAAs）能够通过操纵负载对电网稳定性构成显著威胁。LFC和UFLS等关键控制保护机制依赖通信网络，因此需要一个环境来全面分析这些攻击。

Method: 开发一个开源的协同仿真环境，该环境同时建模电力系统和相应的通信网络，并实现了电网保护机制（如LFC和UFLS）。

Result: 该论文提出了一个能够对具体LFC和UFLS场景中的网络攻击进行全面分析的协同仿真环境。

Conclusion: 所提出的协同仿真环境有助于深入理解和分析网络攻击（特别是LAAs和DLAAs）对电力系统稳定性和保护机制（LFC和UFLS）的影响，强调了通信网络在电网网络安全研究中的重要性。

Abstract: Integrating Information and Communications Technology (ICT) devices into the
power grid brings many benefits. However, it also exposes the grid to new
potential cyber threats. Many control and protection mechanisms, such as Load
Frequency Control (LFC), responsible for maintaining nominal frequency during
load fluctuations and Under Frequency Load Shedding (UFLS) disconnecting
portion of the load during an emergency, are dependent on information exchange
through the communication network. The recently emerging Load Altering Attacks
(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.
In their dynamic form (DLAAs), they manipulate the load in response to live
grid frequency measurements for increased efficiency, posing a notable threat
to grid stability. Recognizing the importance of communication networks in
power grid cyber security research, this paper presents an open-source
co-simulation environment that models the power grid with the corresponding
communication network, implementing grid protective mechanisms. This setup
allows the comprehensive analysis of the attacks in concrete LFC and UFLS
scenarios.

</details>


### [185] [Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems](https://arxiv.org/abs/2508.00724)
*Boyu Li,Zhengchen Li,Weimin Wu,Mengchu Zhou*

Main category: eess.SY

TL;DR: 本文研究了一种新型异构AGV（载具和穿梭车）调度问题，其中AGV可灵活附着和分离以协作完成任务。为解决附着引起的同步和死锁问题，引入Petri网建模和调度，并开发了一种基于Petri网的元启发式算法，经验证其在真实工业数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动化和灵活性需求的增长推动了异构AGV的广泛应用。传统AGV调度未能考虑可附着异构AGV间的协作及其带来的同步和死锁挑战，因此需要研究一种新的调度问题。

Method: 1. 引入Petri网建模AGV调度，描述并发、顺序任务执行及载具-穿梭车同步。2. 提出一种基于Petri网理论的激发驱动解码方法，并集成死锁检测和预防策略以确保无死锁调度。3. 开发一种基于Petri网的元启发式算法，将其融入自适应大邻域搜索框架，并结合有效加速方法提高计算效率。

Result: 通过使用真实工业数据进行的数值实验，验证了所提算法的有效性。该算法优于工程实践中的调度策略、精确求解器和四种最先进的元启发式算法。敏感性分析也提供了管理见解。

Conclusion: 本研究成功解决了可附着异构AGV的调度问题，通过Petri网理论有效处理了同步和死锁挑战，并开发出一种高性能、实用的调度算法，显著提升了系统效率和鲁棒性。

Abstract: The increasing demand for automation and flexibility drives the widespread
adoption of heterogeneous automated guided vehicles (AGVs). This work intends
to investigate a new scheduling problem in a material transportation system
consisting of attachable heterogeneous AGVs, namely carriers and shuttles. They
can flexibly attach to and detach from each other to cooperatively execute
complex transportation tasks. While such collaboration enhances operational
efficiency, the attachment-induced synchronization and interdependence render
the scheduling coupled and susceptible to deadlock. To tackle this challenge,
Petri nets are introduced to model AGV schedules, well describing the
concurrent and sequential task execution and carrier-shuttle synchronization.
Based on Petri net theory, a firing-driven decoding method is proposed, along
with deadlock detection and prevention strategies to ensure deadlock-free
schedules. Furthermore, a Petri net-based metaheuristic is developed in an
adaptive large neighborhood search framework and incorporates an effective
acceleration method to enhance computational efficiency. Finally, numerical
experiments using real-world industrial data validate the effectiveness of the
proposed algorithm against the scheduling policy applied in engineering
practice, an exact solver, and four state-of-the-art metaheuristics. A
sensitivity analysis is also conducted to provide managerial insights.

</details>


### [186] [Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms](https://arxiv.org/abs/2508.00775)
*Andrea Martin,Ian R. Manchester,Luca Furieri*

Main category: eess.SY

TL;DR: 本文提出一种方法，在保持最坏情况收敛保证的同时，提升现有线性收敛算法在特定问题实例上的平均性能。


<details>
  <summary>Details</summary>
Motivation: 在实际高风险工程应用中，优化算法虽需提供最坏情况保证，但这通常以牺牲在常见问题实例上的平均性能为代价。研究旨在解决如何改进算法的平均性能，同时不损失其在整个问题类上的最坏情况保证。

Method: 本文首先刻画了非光滑复合优化问题中实现线性收敛的算法类别。在此基础上，从一个基线线性收敛算法出发，推导并确定了所有能保持其收敛特性的更新规则修改方式。

Result: 研究结果适用于增强现有算法，例如非凸、梯度主导函数的梯度下降法；强凸函数的Nesterov加速法；以及多面体可行集上的投影方法。该方法在解决病态线性方程组和线性系统模型预测控制（MPC）中迭代预算紧张的优化问题时，展现了有效性。

Conclusion: 该研究提供了一种系统性的方法，可以针对特定应用场景定制优化算法，从而显著提升其平均性能，同时严格保留了原始算法的最坏情况收敛保证。

Abstract: In high-stakes engineering applications, optimization algorithms must come
with provable worst-case guarantees over a mathematically defined class of
problems. Designing for the worst case, however, inevitably sacrifices
performance on the specific problem instances that often occur in practice. We
address the problem of augmenting a given linearly convergent algorithm to
improve its average-case performance on a restricted set of target problems -
for example, tailoring an off-the-shelf solver for model predictive control
(MPC) for an application to a specific dynamical system - while preserving its
worst-case guarantees across the entire problem class. Toward this goal, we
characterize the class of algorithms that achieve linear convergence for
classes of nonsmooth composite optimization problems. In particular, starting
from a baseline linearly convergent algorithm, we derive all - and only - the
modifications to its update rule that maintain its convergence properties. Our
results apply to augmenting legacy algorithms such as gradient descent for
nonconvex, gradient-dominated functions; Nesterov's accelerated method for
strongly convex functions; and projected methods for optimization over
polyhedral feasibility sets. We showcase effectiveness of the approach on
solving optimization problems with tight iteration budgets in application to
ill-conditioned systems of linear equations and MPC for linear systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [187] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepański,Szymon Płotka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysław Korzeniowski,Tomasz Trzciński,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: GEPAR3D是一种新颖的牙齿CBCT分割方法，它将实例检测和多类别分割统一起来，并结合牙列的统计形状模型作为几何先验和深度分水岭方法，显著提高了牙根（尤其是根尖）的分割精度。


<details>
  <summary>Details</summary>
Motivation: 锥形束CT（CBCT）中的牙齿分割，尤其是根尖等精细结构，仍然具有挑战性，而这对于正畸中评估牙根吸收至关重要。

Method: 引入GEPAR3D，一种将实例检测和多类别分割统一为一步的方法。它整合了牙列的统计形状模型（SSM）作为几何先验，并利用深度分水岭方法，将每颗牙齿建模为连续的3D能量盆地，编码体素到边界的距离。

Result: GEPAR3D在所有测试集上实现了最高的整体分割性能，平均Dice相似系数（DSC）达到95.0%（比次优方法高2.8%），召回率提高到95.2%（提高9.5%）。定性分析显示根部分割质量显著改善。

Conclusion: 该方法在根部分割质量上取得了实质性改进，表明其在更准确地评估牙根吸收和增强正畸临床决策方面具有显著潜力。

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [188] [On the Utility of Virtual Staining for Downstream Applications as it relates to Task Network Capacity](https://arxiv.org/abs/2508.00164)
*Sourya Sengupta,Jianquan Xu,Phuong Nguyen,Frank J. Brooks,Yang Liu,Mark A. Anastasio*

Main category: eess.IV

TL;DR: 本研究系统性评估了虚拟染色技术在促进下游生物医学任务（如分割和分类）中的实用性，并发现其效用高度依赖于执行这些任务的深度神经网络的容量。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色研究主要使用传统图像质量指标进行评估，而忽略了其在促进下游生物学或临床任务（图像推理）中的实际效用，这在生物医学成像中至关重要。

Method: 通过使用生物数据集，系统性地比较了使用无标记图像、虚拟染色图像和真实荧光图像进行分割或分类任务的性能。评估中特别考虑了执行这些任务的深度神经网络的容量。

Result: 虚拟染色的效用很大程度上取决于分割或分类任务网络提取有意义的、任务相关信息的能力，这与网络容量概念相关。研究表明，当相关任务网络的容量足够大时，虚拟染色可能无法改善甚至会降低分割或分类性能。

Conclusion: 在决定是否进行虚拟染色时，应充分考虑任务网络的容量。

Abstract: Virtual staining, or in-silico-labeling, has been proposed to computationally
generate synthetic fluorescence images from label-free images by use of deep
learning-based image-to-image translation networks. In most reported studies,
virtually stained images have been assessed only using traditional image
quality measures such as structural similarity or signal-to-noise ratio.
However, in biomedical imaging, images are typically acquired to facilitate an
image-based inference, which we refer to as a downstream biological or clinical
task. This study systematically investigates the utility of virtual staining
for facilitating clinically relevant downstream tasks (like segmentation or
classification) with consideration of the capacity of the deep neural networks
employed to perform the tasks. Comprehensive empirical evaluations were
conducted using biological datasets, assessing task performance by use of
label-free, virtually stained, and ground truth fluorescence images. The
results demonstrated that the utility of virtual staining is largely dependent
on the ability of the segmentation or classification task network to extract
meaningful task-relevant information, which is related to the concept of
network capacity. Examples are provided in which virtual staining does not
improve, or even degrades, segmentation or classification performance when the
capacity of the associated task network is sufficiently large. The results
demonstrate that task network capacity should be considered when deciding
whether to perform virtual staining.

</details>


### [189] [Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior](https://arxiv.org/abs/2508.00235)
*Erin Rainville,Amirhossein Rasoulian,Hassan Rivaz,Yiming Xiao*

Main category: eess.IV

TL;DR: 该研究提出了一种新型的弱监督3D多任务UNet模型，利用血管性先验知识，在TOF-MRA图像中联合进行颅内动脉瘤的检测和分割。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤（IA）因其体积小、对比度低，在放射学扫描中难以准确高效地检测和形态学分析。此外，缺乏带有像素级专家标注的大型公共数据集，也阻碍了深度学习算法的开发。

Method: 提出了一种弱监督3D多任务UNet模型。该模型整合了Frangi血管性滤波器作为软脑血管先验知识，用于网络输入和注意力块，以指导分割（通过解码器）和检测（通过辅助分支）。模型在Lausanne数据集上使用粗略的真实标注进行训练，并在同一数据集的精细标注测试集上进行评估。为验证泛化能力，还在ADAM数据集上进行了外部验证。

Result: 该技术在动脉瘤分割（Dice = 0.614, 95%HD = 1.38mm）和检测（假阳性率 = 1.47, 敏感性 = 92.9%）方面均优于现有的SOTA技术。

Conclusion: 所提出的弱监督3D多任务UNet模型，通过整合血管性先验知识，有效解决了颅内动脉瘤检测和分割的挑战，并展现出卓越的性能和良好的泛化能力。

Abstract: Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels
that, if ruptured, can lead to life-threatening consequences. However, their
small size and soft contrast in radiological scans often make it difficult to
perform accurate and efficient detection and morphological analyses, which are
critical in the clinical care of the disorder. Furthermore, the lack of large
public datasets with voxel-wise expert annotations pose challenges for
developing deep learning algorithms to address the issues. Therefore, we
proposed a novel weakly supervised 3D multi-task UNet that integrates
vesselness priors to jointly perform aneurysm detection and segmentation in
time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA
detection and segmentation, we employ the popular Frangi's vesselness filter to
derive soft cerebrovascular priors for both network input and an attention
block to conduct segmentation from the decoder and detection from an auxiliary
branch. We train our model on the Lausanne dataset with coarse ground truth
segmentation, and evaluate it on the test set with refined labels from the same
database. To further assess our model's generalizability, we also validate it
externally on the ADAM dataset. Our results demonstrate the superior
performance of the proposed technique over the SOTA techniques for aneurysm
segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =
1.47, sensitivity = 92.9%).

</details>


### [190] [Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection](https://arxiv.org/abs/2508.00438)
*Sumin Seo,In Kyu Lee,Hyun-Woo Kim,Jaesik Min,Chung-Hwan Jung*

Main category: eess.IV

TL;DR: 本研究提出了一种基于扩散模型的图像修复数据增强方法，用于生成可控严重程度的冠状动脉狭窄病变，有效解决了标记数据有限和类别不平衡问题，显著提升了狭窄检测和严重性分类的性能，尤其在数据量有限的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉狭窄是导致缺血性心脏事件的主要风险因素，其分析耗时且劳动密集。尽管深度学习在自动化狭窄定位和严重性测量方面潜力巨大，但在实际应用中常受限于标记数据不足和类别不平衡问题，这促使研究人员寻求更有效的数据利用方法。

Method: 本研究提出了一种新颖的数据增强方法。该方法利用基于扩散模型的图像修复技术来生成逼真的冠状动脉病变，并允许用户控制病变的严重程度。

Result: 该方法在病变检测和严重性分类方面表现出卓越的性能，在不同合成数据集大小下，无论是大型内部数据集还是公共冠状动脉造影数据集，都展现了优越性。此外，即使在有限数据训练的情况下，该方法仍能保持高检测和分类性能。

Conclusion: 本研究提出的数据增强方法显著提高了冠状动脉狭窄严重性评估的准确性，并优化了数据利用效率，为更可靠的临床决策支持提供了重要价值，突显了其临床重要性。

Abstract: Coronary stenosis is a major risk factor for ischemic heart events leading to
increased mortality, and medical treatments for this condition require
meticulous, labor-intensive analysis. Coronary angiography provides critical
visual cues for assessing stenosis, supporting clinicians in making informed
decisions for diagnosis and treatment. Recent advances in deep learning have
shown great potential for automated localization and severity measurement of
stenosis. In real-world scenarios, however, the success of these competent
approaches is often hindered by challenges such as limited labeled data and
class imbalance. In this study, we propose a novel data augmentation approach
that uses an inpainting method based on a diffusion model to generate realistic
lesions, allowing user-guided control of severity. Extensive evaluation on
lesion detection and severity classification across various synthetic dataset
sizes shows superior performance of our method on both a large-scale in-house
dataset and a public coronary angiography dataset. Furthermore, our approach
maintains high detection and classification performance even when trained with
limited data, highlighting its clinical importance in improving the assessment
of severity of stenosis and optimizing data utilization for more reliable
decision support.

</details>


### [191] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug是一个新颖的插件框架，通过利用观测对象与期望对象的相似性及生成流的高斯性，并引入时间自适应预热策略和锐利高斯性正则化，显著提升了基础流匹配（FM）先验在解决不适定逆问题上的性能，超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 传统的逆问题解决方法依赖于领域特定或未经训练的先验，这限制了其泛化能力。本研究旨在通过更有效的方式利用基础流匹配（FM）先验来克服这些限制，并充分发挥领域无关基础模型的潜力。

Method: 本文提出了FMPlug框架，该框架利用了观测对象与期望对象的相似性以及生成流的高斯性两大洞察。具体方法包括引入时间自适应预热策略和锐利的高斯性正则化，以增强基础流匹配先验在不适定逆问题中的应用。

Result: FMPlug在图像超分辨率和高斯去模糊任务上，以显著优势超越了使用基础FM先验的现有最先进方法。

Conclusion: FMPlug成功解锁了领域无关基础模型在解决不适定逆问题方面的真正潜力，通过其创新的设计和策略，为逆问题求解提供了更强大和通用的解决方案。

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


### [192] [AI-Driven Collaborative Satellite Object Detection for Space Sustainability](https://arxiv.org/abs/2508.00755)
*Peng Hu,Wenxuan Zhang*

Main category: eess.IV

TL;DR: 本文提出了一种卫星聚类框架，通过多卫星协作执行基于深度学习的空间目标检测任务，以提高近地轨道空间态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 近地轨道卫星密度增加导致碰撞风险上升，传统地面跟踪系统存在延迟和覆盖限制，因此急需星载、基于视觉的空间目标检测能力。

Method: 提出了一种新颖的卫星聚类框架，支持多卫星协作执行深度学习的空间目标检测任务；构建了高保真数据集模拟聚类卫星编队成像场景；引入了距离感知的视角选择策略以优化检测性能；并使用最新的深度学习模型进行评估。

Result: 实验结果表明，与单卫星和现有方法相比，基于聚类的方法在保持低尺寸、重量和功耗（SWaP）的同时，实现了具有竞争力的检测精度。

Conclusion: 研究结果强调了分布式、人工智能赋能的在轨系统在增强空间态势感知和促进长期空间可持续性方面的潜力。

Abstract: The growing density of satellites in low-Earth orbit (LEO) presents serious
challenges to space sustainability, primarily due to the increased risk of
in-orbit collisions. Traditional ground-based tracking systems are constrained
by latency and coverage limitations, underscoring the need for onboard,
vision-based space object detection (SOD) capabilities. In this paper, we
propose a novel satellite clustering framework that enables the collaborative
execution of deep learning (DL)-based SOD tasks across multiple satellites. To
support this approach, we construct a high-fidelity dataset simulating imaging
scenarios for clustered satellite formations. A distance-aware viewpoint
selection strategy is introduced to optimize detection performance, and recent
DL models are used for evaluation. Experimental results show that the
clustering-based method achieves competitive detection accuracy compared to
single-satellite and existing approaches, while maintaining a low size, weight,
and power (SWaP) footprint. These findings underscore the potential of
distributed, AI-enabled in-orbit systems to enhance space situational awareness
and contribute to long-term space sustainability.

</details>
