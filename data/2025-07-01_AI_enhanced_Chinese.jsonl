{"id": "2506.22466", "categories": ["cs.RO", "cs.CY", "I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22466", "abs": "https://arxiv.org/abs/2506.22466", "authors": ["Marcel Heisler", "Christian Becker-Asano"], "title": "Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum", "comment": "To be published in IEEE RO-MAN 2025 conference proceedings; for\n  videos check https://ai.hdm-stuttgart.de/humanoid-lab", "summary": "The android robot Andrea was set up at a public museum in Germany for six\nconsecutive days to have conversations with visitors, fully autonomously. No\nspecific context was given, so visitors could state their opinions regarding\npossible use-cases in structured interviews, without any bias. Additionally the\n44 interviewees were asked for their general opinions of the robot, their\nreasons (not) to interact with it and necessary improvements for future use.\nThe android's voice and wig were changed between different days of operation to\ngive varying cues regarding its gender. This did not have a significant impact\non the positive overall perception of the robot. Most visitors want the robot\nto provide information about exhibits in the future, while opinions on other\nroles, like a receptionist, were both wanted and explicitly not wanted by\ndifferent visitors. Speaking more languages (than only English) and faster\nresponse times were the improvements most desired. These findings from the\ninterviews are in line with an analysis of the system logs, which revealed,\nthat after chitchat and personal questions, most of the 4436 collected requests\nasked for information related to the museum and to converse in a different\nlanguage. The valuable insights gained from these real-world interactions are\nnow used to improve the system to become a useful real-world application.", "AI": {"tldr": "\u5b89\u5353\u673a\u5668\u4ebaAndrea\u5728\u5fb7\u56fd\u535a\u7269\u9986\u4e0e\u8bbf\u5ba2\u8fdb\u884c\u4e86\u4e3a\u671f\u516d\u5929\u7684\u81ea\u4e3b\u5bf9\u8bdd\uff0c\u6536\u96c6\u4e8644\u540d\u53d7\u8bbf\u8005\u7684\u610f\u89c1\uff0c\u53d1\u73b0\u6027\u522b\u63d0\u793a\u5bf9\u673a\u5668\u4eba\u6574\u4f53\u611f\u77e5\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u8bbf\u5ba2\u5e0c\u671b\u5176\u672a\u6765\u63d0\u4f9b\u5c55\u54c1\u4fe1\u606f\uff0c\u5e76\u6539\u8fdb\u8bed\u8a00\u652f\u6301\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u7814\u7a76\u516c\u4f17\u5bf9\u5b89\u5353\u673a\u5668\u4eba\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u63a5\u53d7\u5ea6\u548c\u6f5c\u5728\u7528\u9014\uff0c\u4ee5\u4f18\u5316\u5176\u8bbe\u8ba1\u548c\u529f\u80fd\u3002", "method": "\u5728\u535a\u7269\u9986\u8bbe\u7f6e\u673a\u5668\u4eba\u8fdb\u884c\u81ea\u4e3b\u5bf9\u8bdd\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u7cfb\u7edf\u65e5\u5fd7\u5206\u6790\u6536\u96c6\u6570\u636e\u3002", "result": "\u6027\u522b\u63d0\u793a\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u8bbf\u5ba2\u4e3b\u8981\u5e0c\u671b\u673a\u5668\u4eba\u63d0\u4f9b\u5c55\u54c1\u4fe1\u606f\uff0c\u5e76\u6539\u8fdb\u8bed\u8a00\u652f\u6301\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u5b89\u5353\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.22473", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22473", "abs": "https://arxiv.org/abs/2506.22473", "authors": ["Fernando Diaz Ledezma", "Valentin Marcel", "Matej Hoffmann"], "title": "Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity", "comment": "8 pages with 6 figures", "summary": "The movements of both animals and robots give rise to streams of\nhigh-dimensional motor and sensory information. Imagine the brain of a newborn\nor the controller of a baby humanoid robot trying to make sense of unprocessed\nsensorimotor time series. Here, we present a framework for studying the dynamic\nfunctional connectivity between the multimodal sensory signals of a robotic\nagent to uncover an underlying structure. Using instantaneous mutual\ninformation, we capture the time-varying functional connectivity (FC) between\nproprioceptive, tactile, and visual signals, revealing the sensorimotor\nrelationships. Using an infinite relational model, we identified sensorimotor\nmodules and their evolving connectivity. To further interpret these dynamic\ninteractions, we employed non-negative matrix factorization, which decomposed\nthe connectivity patterns into additive factors and their corresponding\ntemporal coefficients. These factors can be considered the agent's motion\nprimitives or movement synergies that the agent can use to make sense of its\nsensorimotor space and later for behavior selection. In the future, the method\ncan be deployed in robot learning as well as in the analysis of human movement\ntrajectories or brain signals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u673a\u5668\u4eba\u591a\u6a21\u6001\u611f\u5b98\u4fe1\u53f7\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u7684\u6846\u67b6\uff0c\u63ed\u793a\u5176\u5e95\u5c42\u7ed3\u6784\uff0c\u5e76\u5e94\u7528\u4e8e\u884c\u4e3a\u9009\u62e9\u3002", "motivation": "\u7814\u7a76\u9ad8\u7ef4\u611f\u5b98\u8fd0\u52a8\u4fe1\u606f\u7684\u52a8\u6001\u8fde\u63a5\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u6216\u65b0\u751f\u513f\u7406\u89e3\u672a\u5904\u7406\u7684\u611f\u5b98\u8fd0\u52a8\u65f6\u95f4\u5e8f\u5217\u3002", "method": "\u4f7f\u7528\u77ac\u65f6\u4e92\u4fe1\u606f\u6355\u6349\u52a8\u6001\u529f\u80fd\u8fde\u63a5\uff0c\u7ed3\u5408\u65e0\u9650\u5173\u7cfb\u6a21\u578b\u8bc6\u522b\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u89e3\u91ca\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u63ed\u793a\u4e86\u611f\u5b98\u8fd0\u52a8\u6a21\u5757\u53ca\u5176\u52a8\u6001\u8fde\u63a5\uff0c\u5206\u89e3\u51fa\u8fd0\u52a8\u57fa\u5143\u6216\u534f\u540c\u4f5c\u7528\uff0c\u7528\u4e8e\u884c\u4e3a\u9009\u62e9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e60\u53ca\u4eba\u7c7b\u8fd0\u52a8\u8f68\u8ff9\u6216\u8111\u4fe1\u53f7\u5206\u6790\u3002"}}
{"id": "2506.22494", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22494", "abs": "https://arxiv.org/abs/2506.22494", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "AI": {"tldr": "DriveBLIP2\u6846\u67b6\u57fa\u4e8eBLIP2-OPT\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5668\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u76ee\u6807\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5b9e\u65f6\u8bc6\u522b\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u56fe\u751f\u6210\u5668\uff0c\u7a81\u51fa\u5173\u952e\u5bf9\u8c61\u4ee5\u751f\u6210\u66f4\u6e05\u6670\u7684\u89e3\u91ca\u3002", "result": "\u5728DRAMA\u6570\u636e\u96c6\u4e0a\uff0cBLEU\u3001ROUGE\u3001CIDEr\u548cSPICE\u5206\u6570\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u76ee\u6807\u6ce8\u610f\u529b\u673a\u5236\u53ef\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.22572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22572", "abs": "https://arxiv.org/abs/2506.22572", "authors": ["Mrunmayi Mungekar", "Sanjith Menon", "M. Ravi Shankar", "M. Khalid Jawed"], "title": "Directed Shape Morphing using Kirigami-enhanced Thermoplastics", "comment": "Software and Data: https://github.com/structuresComp/Shrinky-Dink", "summary": "We present a simple, accessible method for autonomously transforming flat\nplastic sheets into intricate three-dimensional structures using only uniform\nheating and common tools such as household ovens and scissors. Our approach\ncombines heat-shrinkable thermoplastics with Kirigami patterns tailored to the\ntarget 3D shape, creating bilayer composites that morph into a wide range of\ncomplex structures, e.g., bowls, pyramids, and even custom ergonomic surfaces\nlike mouse covers. Critically, the transformation is driven by a\nlow-information stimulus (uniform heat) yet produces highly intricate shapes\nthrough programmed geometric design. The morphing behavior, confirmed by finite\nelement simulations, arises from strain mismatch between the contracting\nthermoplastic layer and the constraining Kirigami layer. By decoupling material\ncomposition from mechanical response, this method avoids detailed process\ncontrol and enables a broad class of self-morphing structures, offering a\nversatile platform for adaptive design and scalable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u5747\u5300\u52a0\u70ed\u548c\u5e38\u89c1\u5de5\u5177\u5c06\u5e73\u9762\u5851\u6599\u7247\u81ea\u4e3b\u8f6c\u5316\u4e3a\u590d\u6742\u4e09\u7ef4\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u65e0\u9700\u590d\u6742\u63a7\u5236\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u5f62\u72b6\u81ea\u53d8\u5f62\u7684\u65b9\u6cd5\uff0c\u4e3a\u81ea\u9002\u5e94\u8bbe\u8ba1\u548c\u89c4\u6a21\u5316\u5236\u9020\u63d0\u4f9b\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u70ed\u6536\u7f29\u70ed\u5851\u6027\u5851\u6599\u548c\u5b9a\u5236Kirigami\u56fe\u6848\uff0c\u901a\u8fc7\u53cc\u5c42\u590d\u5408\u6750\u6599\u5728\u5747\u5300\u52a0\u70ed\u4e0b\u53d8\u5f62\u3002", "result": "\u6210\u529f\u5236\u9020\u51fa\u591a\u79cd\u590d\u6742\u7ed3\u6784\uff08\u5982\u7897\u3001\u91d1\u5b57\u5854\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u6a21\u62df\u9a8c\u8bc1\u53d8\u5f62\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u8bbe\u8ba1\u5b9e\u73b0\u4f4e\u4fe1\u606f\u523a\u6fc0\u4e0b\u7684\u9ad8\u590d\u6742\u5ea6\u53d8\u5f62\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22489", "categories": ["eess.SY", "cs.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2506.22489", "abs": "https://arxiv.org/abs/2506.22489", "authors": ["Muhammad R. Abdussami", "Kevin Daley", "Gabrielle Hoelzle", "Aditi Verma"], "title": "A Multi-Criteria Evaluation Framework for Siting Fusion Energy Facilities: Application and Evaluation of U.S. Coal Power Plants", "comment": null, "summary": "This paper proposes a comprehensive methodology for siting fusion energy\nfacilities, integrating expert judgment, geospatial data, and multi-criteria\ndecision making tools to evaluate site suitability systematically. As a case\nstudy, we apply this framework to all currently operational coal power plant\nsites in the United States to examine their potential for hosting future fusion\nfacilities at a time when these coal plants are shut down on reaching their end\nof life - timelines which are expected to coincide with the potential\ndeployment of fusion energy facilities. Drawing on 22 siting criteria -\nincluding state and federal policies, risk and hazard assessments, and spatial\nand infrastructural parameters - we implement two MultiCriteria Decision-Making\n(MCDM) methods: the Fuzzy Full Consistency Method (F-FUCOM) to derive attribute\nweights and the Weighted Sum Method (WSM) to rank sites based on composite\nsuitability scores. By focusing on fusion-specific siting needs and\ndemonstrating the framework through a coal site application, this study\ncontributes a scalable and transparent decision-support tool for identifying\noptimal fusion energy deployment locations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u9009\u5740\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u5224\u65ad\u3001\u5730\u7406\u7a7a\u95f4\u6570\u636e\u548c\u591a\u51c6\u5219\u51b3\u7b56\u5de5\u5177\uff0c\u7cfb\u7edf\u8bc4\u4f30\u805a\u53d8\u80fd\u6e90\u8bbe\u65bd\u7684\u9009\u5740\u9002\u5b9c\u6027\u3002\u4ee5\u7f8e\u56fd\u73b0\u6709\u71c3\u7164\u7535\u5382\u4e3a\u4f8b\uff0c\u5e94\u7528\u8be5\u6846\u67b6\u8bc4\u4f30\u5176\u672a\u6765\u4f5c\u4e3a\u805a\u53d8\u8bbe\u65bd\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u71c3\u7164\u7535\u5382\u5bff\u547d\u7ed3\u675f\uff0c\u5176\u573a\u5730\u53ef\u80fd\u6210\u4e3a\u805a\u53d8\u80fd\u6e90\u8bbe\u65bd\u7684\u6f5c\u5728\u9009\u5740\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "method": "\u91c7\u752822\u9879\u9009\u5740\u6807\u51c6\uff0c\u7ed3\u5408\u6a21\u7cca\u5b8c\u5168\u4e00\u81f4\u6027\u65b9\u6cd5\uff08F-FUCOM\uff09\u786e\u5b9a\u6743\u91cd\uff0c\u52a0\u6743\u6c42\u548c\u6cd5\uff08WSM\uff09\u8fdb\u884c\u573a\u5730\u6392\u5e8f\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u8bc4\u4f30\u71c3\u7164\u7535\u5382\u573a\u5730\u4f5c\u4e3a\u805a\u53d8\u8bbe\u65bd\u9009\u5740\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u805a\u53d8\u80fd\u6e90\u8bbe\u65bd\u7684\u9009\u5740\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u3001\u900f\u660e\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2506.22439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22439", "abs": "https://arxiv.org/abs/2506.22439", "authors": ["Javier Conde", "Miguel Gonz\u00e1lez", "Mar\u00eda Grandury", "Gonzalo Mart\u00ednez", "Pedro Reviriego", "Mar Brysbaert"], "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u8bc4\u4f30\u4e86LLMs\u4e0e\u4eba\u7c7b\u5728\u8bcd\u6c47\u7279\u5f81\u4e0a\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u53d1\u73b0LLMs\u5728\u611f\u5b98\u5173\u8054\u65b9\u9762\u7684\u8868\u73b0\u8f83\u5f31\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u591a\u5173\u6ce8\u4efb\u52a1\u6027\u80fd\uff0c\u800c\u5ffd\u7565\u4e86\u96be\u4ee5\u91cf\u5316\u7684\u8bed\u8a00\u7279\u5f81\uff08\u5982\u60c5\u611f\u3001\u611f\u5b98\u5173\u8054\u7b49\uff09\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528Glasgow\u548cLancaster\u4e24\u4e2a\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e00\u7ec4\u4ee3\u8868\u6027LLMs\u572813\u4e2a\u8bcd\u6c47\u7279\u5f81\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "LLMs\u5728Glasgow\u6570\u636e\u96c6\uff08\u60c5\u611f\u3001\u719f\u6089\u5ea6\u7b49\uff09\u4e0a\u7684\u5bf9\u9f50\u8868\u73b0\u4f18\u4e8eLancaster\u6570\u636e\u96c6\uff08\u611f\u5b98\u5173\u8054\uff09\uff0c\u8868\u660e\u5176\u5728\u611f\u5b98\u5173\u8054\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "LLMs\u7f3a\u4e4f\u4eba\u7c7b\u7684\u5177\u4f53\u8ba4\u77e5\u80fd\u529b\uff0c\u5bfc\u81f4\u611f\u5b98\u5173\u8054\u8868\u73b0\u4e0d\u4f73\uff0c\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.22604", "categories": ["cs.AI", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22604", "abs": "https://arxiv.org/abs/2506.22604", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "title": "Bootstrapping Human-Like Planning via LLMs", "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u548c\u62d6\u62fd\u754c\u9762\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u7c7b\u4f3c\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u4e0e\u624b\u52a8\u6307\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u673a\u5668\u4eba\u7ec8\u7aef\u7528\u6237\u9700\u8981\u66f4\u76f4\u89c2\u7684\u4efb\u52a1\u6307\u5b9a\u65b9\u5f0f\uff0c\u81ea\u7136\u8bed\u8a00\u548c\u62d6\u62fd\u754c\u9762\u5404\u6709\u4f18\u52a3\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e24\u8005\u7684\u7ed3\u5408\u6548\u679c\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u8f93\u5165\u81ea\u7136\u8bed\u8a00\u5e76\u8f93\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u52a8\u4f5c\u5e8f\u5217\uff0c\u4e0e\u624b\u52a8\u6307\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5927\u6a21\u578b\u5728\u751f\u6210\u4eba\u7c7b\u7c7b\u4f3c\u52a8\u4f5c\u5e8f\u5217\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5c0f\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u6ee1\u610f\u6548\u679c\u3002", "conclusion": "\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u53ef\u884c\uff0c\u5927\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5c0f\u6a21\u578b\u4ecd\u6709\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.22437", "categories": ["cs.CV", "68T45 (Computer Vision)"], "pdf": "https://arxiv.org/pdf/2506.22437", "abs": "https://arxiv.org/abs/2506.22437", "authors": ["Xinxin Sun", "Peter Chang"], "title": "Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring", "comment": "43 pages, 5 figures, 19 tables. Submitted to NDT&E International.\n  This work may also be of interest to researchers in optical NDE and civil\n  engineering SHM", "summary": "Accurate image alignment is essential for monitoring crack evolution in\nstructural health monitoring (SHM), particularly under real-world conditions\ninvolving perspective distortion, occlusion, and low contrast. However,\ntraditional feature detectors such as SIFT and SURF, which rely on\nGaussian-based scale spaces, tend to suppress high-frequency edges, making them\nunsuitable for thin crack localization. Lightweight binary alternatives like\nORB and BRISK, while computationally efficient, often suffer from poor keypoint\nrepeatability on textured or shadowed surfaces. This study presents a\nphysics-informed alignment framework that adapts the open KAZE architecture to\nSHM-specific challenges. By utilizing nonlinear anisotropic diffusion to\nconstruct a crack-preserving scale space, and integrating RANSAC-based\nhomography estimation, the framework enables accurate geometric correction\nwithout the need for training, parameter tuning, or prior calibration. The\nmethod is validated on time-lapse images of masonry and concrete acquired via\nhandheld smartphone under varied field conditions, including shadow\ninterference, cropping, oblique viewing angles, and surface clutter. Compared\nto classical detectors, the proposed framework reduces crack area and spine\nlength errors by up to 70 percent and 90 percent, respectively, while\nmaintaining sub-5 percent alignment error in key metrics. Unsupervised,\ninterpretable, and computationally lightweight, this approach supports scalable\ndeployment via UAVs and mobile platforms. By tailoring nonlinear scale-space\nmodeling to SHM image alignment, this work offers a robust and physically\ngrounded alternative to conventional techniques for tracking real-world crack\nevolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u56fe\u50cf\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u88c2\u7f1d\u6f14\u5316\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u9891\u8fb9\u7f18\u6291\u5236\u548c\u91cd\u590d\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982SIFT\u3001SURF\uff09\u5728\u9ad8\u9891\u8fb9\u7f18\u6291\u5236\u548c\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff08\u5982ORB\u3001BRISK\uff09\u5728\u7eb9\u7406\u6216\u9634\u5f71\u8868\u9762\u91cd\u590d\u6027\u5dee\u3002", "method": "\u5229\u7528\u975e\u7ebf\u6027\u5404\u5411\u5f02\u6027\u6269\u6563\u6784\u5efa\u4fdd\u7559\u88c2\u7f1d\u7684\u5c3a\u5ea6\u7a7a\u95f4\uff0c\u7ed3\u5408RANSAC\u5355\u5e94\u6027\u4f30\u8ba1\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u53c2\u6570\u8c03\u6574\u3002", "result": "\u5728\u591a\u79cd\u73b0\u573a\u6761\u4ef6\u4e0b\uff0c\u88c2\u7f1d\u9762\u79ef\u548c\u957f\u5ea6\u8bef\u5dee\u5206\u522b\u51cf\u5c1170%\u548c90%\uff0c\u5bf9\u9f50\u8bef\u5dee\u4f4e\u4e8e5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u88c2\u7f1d\u6f14\u5316\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u3001\u8f7b\u91cf\u4e14\u65e0\u9700\u6821\u51c6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u5e73\u53f0\u548c\u65e0\u4eba\u673a\u90e8\u7f72\u3002"}}
{"id": "2506.22532", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22532", "abs": "https://arxiv.org/abs/2506.22532", "authors": ["Mark Wrobel", "Michele Pascale", "Tina Yao", "Ruaraidh Campbell", "Elena Milano", "Michael Quail", "Jennifer Steeden", "Vivek Muthurangu"], "title": "High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning", "comment": null, "summary": "Background: Conventional cardiovascular magnetic resonance (CMR) in\npaediatric and congenital heart disease uses 2D, breath-hold, balanced steady\nstate free precession (bSSFP) cine imaging for assessment of function and\ncardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for\nanatomical assessment. Our aim is to concatenate a stack 2D free-breathing\nreal-time cines and use Deep Learning (DL) to create an isotropic a fully\nsegmented 3D cine dataset from these images. Methods: Four DL models were\ntrained on open-source data that performed: a) Interslice contrast correction;\nb) Interslice respiratory motion correction; c) Super-resolution (slice\ndirection); and d) Segmentation of right and left atria and ventricles (RA, LA,\nRV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients\nundergoing routine cardiovascular examination, our method was validated on\nprospectively acquired sagittal stacks of real-time cine images. Quantitative\nmetrics (ventricular volumes and vessel diameters) and image quality of the 3D\ncines were compared to conventional breath hold cine and whole heart imaging.\nResults: All real-time data were successfully transformed into 3D cines with a\ntotal post-processing time of <1 min in all cases. There were no significant\nbiases in any LV or RV metrics with reasonable limits of agreement and\ncorrelation. There is also reasonable agreement for all vessel diameters,\nalthough there was a small but significant overestimation of RPA diameter.\nConclusion: We have demonstrated the potential of creating a 3D-cine data from\nconcatenated 2D real-time cine images using a series of DL models. Our method\nhas short acquisition and reconstruction times with fully segmented data being\navailable within 2 minutes. The good agreement with conventional imaging\nsuggests that our method could help to significantly speed up CMR in clinical\npractice.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5c062D\u5b9e\u65f6\u7535\u5f71\u56fe\u50cf\u62fc\u63a5\u4e3a3D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\uff08CMR\uff09\u5728\u513f\u79d1\u548c\u5148\u5929\u6027\u5fc3\u810f\u75c5\u4e2d\u9700\u8981\u591a\u6b21\u626b\u63cf\uff0c\u8017\u65f6\u8f83\u957f\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5c062D\u81ea\u7531\u547c\u5438\u5b9e\u65f6\u7535\u5f71\u56fe\u50cf\u62fc\u63a5\u4e3a3D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u8bad\u7ec3\u4e86\u56db\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5206\u522b\u7528\u4e8e\u5bf9\u6bd4\u5ea6\u6821\u6b63\u3001\u547c\u5438\u8fd0\u52a8\u6821\u6b63\u3001\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u548c\u5fc3\u810f\u7ed3\u6784\u5206\u5272\u3002\u572810\u540d\u60a3\u8005\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u6210\u50cf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u6240\u6709\u5b9e\u65f6\u6570\u636e\u6210\u529f\u8f6c\u6362\u4e3a3D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u5904\u7406\u65f6\u95f4\u5c0f\u4e8e1\u5206\u949f\u3002\u5fc3\u5ba4\u4f53\u79ef\u548c\u8840\u7ba1\u76f4\u5f84\u4e0e\u4f20\u7edf\u6210\u50cf\u7ed3\u679c\u4e00\u81f4\uff0c\u4f46\u53f3\u80ba\u52a8\u8109\u76f4\u5f84\u7565\u6709\u9ad8\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u751f\u62103D\u7535\u5f71\u6570\u636e\u96c6\uff0c\u4e0e\u4f20\u7edf\u6210\u50cf\u7ed3\u679c\u4e00\u81f4\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u4e34\u5e8aCMR\u7684\u6548\u7387\u3002"}}
{"id": "2506.22593", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22593", "abs": "https://arxiv.org/abs/2506.22593", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "AI": {"tldr": "Pix2G\u65b9\u6cd5\u901a\u8fc7\u5b9e\u65f6\u751f\u6210\u573a\u666f\u56fe\uff0c\u5c062D BIM\u4e0e3D\u673a\u5668\u4eba\u5730\u56fe\u7ed3\u5408\uff0c\u652f\u6301\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u8fdb\u884c\u81ea\u4e3b\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u64cd\u4f5c\u5458\u4e60\u60ef\u76842D BIM\u4e0e\u673a\u5668\u4eba3D\u5730\u56fe\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387\u3002", "method": "\u63d0\u51faPix2G\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u50cf\u7d20\u548cLiDAR\u5730\u56fe\u5b9e\u65f6\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u56fe\uff0c\u4ec5\u4f7f\u7528CPU\u6ee1\u8db3\u8ba1\u7b97\u9650\u5236\u3002", "result": "\u751f\u6210\u53bb\u566a\u76842D\u73af\u5883\u5730\u56fe\u548c\u7ed3\u6784\u5206\u5272\u76843D\u70b9\u4e91\uff0c\u901a\u8fc7\u591a\u5c42\u56fe\u8fde\u63a5\uff0c\u6210\u529f\u5728NASA JPL NeBula-Spot\u673a\u5668\u4eba\u4e0a\u5b9e\u65f6\u6d4b\u8bd5\u3002", "conclusion": "Pix2G\u65b9\u6cd5\u6709\u6548\u652f\u6301\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u63a2\u7d22\uff0c\u5b9e\u73b0\u4e862D\u4e0e3D\u4fe1\u606f\u7684\u65e0\u7f1d\u7ed3\u5408\u3002"}}
{"id": "2506.22579", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.22579", "abs": "https://arxiv.org/abs/2506.22579", "authors": ["Armin Abdolmohammadi", "Navid Mojahed", "Shima Nazari", "Bahram Ravani"], "title": "Data-Efficient Excavation Force Estimation for Wheel Loaders", "comment": "Preprint version of the paper submitted to IEEE Transaction of\n  Vehicular Technology", "summary": "Accurate excavation force prediction is essential for enabling autonomous\noperation and optimizing control strategies in earthmoving machinery.\nConventional methods typically require extensive data collection or simulations\nacross diverse soil types, limiting scalability and adaptability. This paper\nproposes a data-efficient framework that calibrates soil parameters using force\ndata from the prior bucket-loading cycle. Leveraging an analytical soil-tool\ninteraction model, the fundamental earthmoving equation (FEE), our approach\nuses a multi-stage optimization strategy, on soil parameters during the loading\nphase. These fitted parameters are then used to predict excavation forces in\nthe upcoming digging cycle, allowing the system to adapt its control inputs\nwithout the need for extensive data collection or machine learning-based model\ntraining. The framework is validated in high-fidelity simulations using the\nAlgoryx Dynamics engine, across multiple soil types and excavation\ntrajectories, demonstrating accurate force predictions with root-mean-square\nerrors of 10\\% to 15\\% in primary test cases. This cycle-to-cycle adaptation\nstrategy showcases the potential for online and scalable efficient path\nplanning for wheel loader operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9ad8\u6548\u6846\u67b6\u7684\u6316\u6398\u529b\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u571f\u58e4\u53c2\u6570\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u6216\u6a21\u62df\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u5229\u7528\u524d\u4e00\u6b21\u94f2\u88c5\u5468\u671f\u7684\u529b\u6570\u636e\u6821\u51c6\u571f\u58e4\u53c2\u6570\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0c\u9884\u6d4b\u8bef\u5dee\u4e3a10%\u81f315%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u7ebf\u548c\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u8def\u5f84\u89c4\u5212\u6f5c\u529b\u3002"}}
{"id": "2506.22485", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2506.22485", "abs": "https://arxiv.org/abs/2506.22485", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5ba1\u67e5\u9ad8\u5ea6\u7ed3\u6784\u5316\u7684\u4f01\u4e1a\u4e1a\u52a1\u6587\u6863\uff0c\u5229\u7528AI\u667a\u80fd\u4f53\u5b9e\u73b0\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u6027\u7684\u9010\u8282\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u975e\u7ed3\u6784\u5316\u6587\u672c\u6216\u6709\u9650\u7684\u5408\u89c4\u6027\u68c0\u67e5\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f01\u4e1a\u6587\u6863\u7684\u9ad8\u6548\u5ba1\u67e5\u9700\u6c42\u3002", "method": "\u91c7\u7528LangChain\u3001CrewAI\u3001TruLens\u548cGuidance\u7b49\u73b0\u4ee3\u7f16\u6392\u5de5\u5177\uff0c\u901a\u8fc7\u5e76\u884c\u6216\u987a\u5e8f\u8fd0\u884c\u7684\u4e13\u95e8\u667a\u80fd\u4f53\u8fdb\u884c\u6587\u6863\u5ba1\u67e5\u3002", "result": "AI\u7cfb\u7edf\u5728\u4e00\u81f4\u6027\uff0899% vs. \u4eba\u7c7b92%\uff09\u3001\u9519\u8bef\u7387\u548c\u504f\u89c1\u7387\u51cf\u534a\u3001\u5ba1\u67e5\u65f6\u95f4\u4ece30\u5206\u949f\u964d\u81f32.5\u5206\u949f\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4f01\u4e1a\u5728AI\u9a71\u52a8\u7684\u6587\u6863\u8d28\u91cf\u4fdd\u8bc1\u65b9\u9762\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u4f46\u4ecd\u9700\u5728\u9ad8\u5ea6\u4e13\u4e1a\u5316\u9886\u57df\u8fdb\u884c\u4eba\u5de5\u76d1\u7763\u3002"}}
{"id": "2506.22609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22609", "abs": "https://arxiv.org/abs/2506.22609", "authors": ["Graham Todd", "Alexander G. Padula", "Dennis J. N. J. Soemers", "Julian Togelius"], "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games", "comment": "18 pages, 3 figures", "summary": "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", "AI": {"tldr": "Ludax\u662f\u4e00\u4e2a\u7ed3\u5408\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u548c\u786c\u4ef6\u52a0\u901f\u7684\u6846\u67b6\uff0c\u65e8\u5728\u52a0\u901f\u6e38\u620f\u7814\u7a76\uff0c\u652f\u6301\u5feb\u901f\u6a21\u62df\u548c\u7075\u6d3b\u8868\u793a\u3002", "motivation": "\u4e3a\u652f\u6301\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff0c\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u7f16\u8bd1\u4e3a\u786c\u4ef6\u52a0\u901f\u4ee3\u7801\u7684\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86Ludax\u6846\u67b6\uff0c\u7ed3\u5408\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\u7684\u901a\u7528\u6027\u548c\u73b0\u4ee3\u5e76\u884c\u5904\u7406\u786c\u4ef6\u7684\u901f\u5ea6\uff0c\u5e76\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u4e2d\u3002", "result": "Ludax\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u63cf\u8ff0\u8bed\u8a00\u548c\u6280\u672f\u7f16\u8bd1\u8bf4\u660e\uff0c\u901a\u8fc7\u901f\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u548cRL\u4ee3\u7406\u8bad\u7ec3\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "Ludax\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\uff0c\u6709\u671b\u52a0\u901f\u4ece\u5f3a\u5316\u5b66\u4e60\u5230\u8ba4\u77e5\u79d1\u5b66\u7684\u6e38\u620f\u7814\u7a76\u3002"}}
{"id": "2506.22438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22438", "abs": "https://arxiv.org/abs/2506.22438", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps", "comment": "\\c{opyright} 20XX the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND", "summary": "Accurate pest population monitoring and tracking their dynamic changes are\ncrucial for precision agriculture decision-making. A common limitation in\nexisting vision-based automatic pest counting research is that models are\ntypically evaluated on datasets with ground truth but deployed in real-world\nscenarios without assessing the reliability of counting results due to the lack\nof ground truth. To this end, this paper proposed a method for comprehensively\nevaluating pest counting confidence in the image, based on information related\nto counting results and external environmental conditions. First, a pest\ndetection network is used for pest detection and counting, extracting counting\nresult-related information. Then, the pest images undergo image quality\nassessment, image complexity assessment, and pest distribution uniformity\nassessment. And the changes in image clarity caused by stirring during image\nacquisition are quantified by calculating the average gradient magnitude.\nNotably, we designed a hypothesis-driven multi-factor sensitivity analysis\nmethod to select the optimal image quality assessment and image complexity\nassessment methods. And we proposed an adaptive DBSCAN clustering algorithm for\npest distribution uniformity assessment. Finally, the obtained information\nrelated to counting results and external environmental conditions is input into\na regression model for prediction, resulting in the final pest counting\nconfidence. To the best of our knowledge, this is the first study dedicated to\ncomprehensively evaluating counting confidence in counting tasks, and\nquantifying the relationship between influencing factors and counting\nconfidence through a model. Experimental results show our method reduces MSE by\n31.7% and improves R2 by 15.2% on the pest counting confidence test set,\ncompared to the baseline built primarily on information related to counting\nresults.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u6570\u7ed3\u679c\u548c\u5916\u90e8\u73af\u5883\u6761\u4ef6\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u56fe\u50cf\u4e2d\u5bb3\u866b\u8ba1\u6570\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u5bb3\u866b\u8ba1\u6570\u7814\u7a76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u5bf9\u8ba1\u6570\u7ed3\u679c\u53ef\u9760\u6027\u7684\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5168\u9762\u8bc4\u4f30\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u3002", "method": "\u7ed3\u5408\u5bb3\u866b\u68c0\u6d4b\u7f51\u7edc\u3001\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u56fe\u50cf\u590d\u6742\u6027\u8bc4\u4f30\u548c\u5bb3\u866b\u5206\u5e03\u5747\u5300\u6027\u8bc4\u4f30\uff0c\u8bbe\u8ba1\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bb3\u866b\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5\u96c6\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u964d\u4f4e\u4e8631.7%\u7684MSE\uff0c\u63d0\u9ad8\u4e8615.2%\u7684R2\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u8ba1\u6570\u4efb\u52a1\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u91cf\u5316\u4e86\u5f71\u54cd\u56e0\u7d20\u4e0e\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.22580", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22580", "abs": "https://arxiv.org/abs/2506.22580", "authors": ["Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation", "comment": "10 pages, 2 figures, Accepted at MICCAI 2025", "summary": "Federated learning is a decentralized training approach that keeps data under\nstakeholder control while achieving superior performance over isolated\ntraining. While inter-institutional feature discrepancies pose a challenge in\nall federated settings, medical imaging is particularly affected due to diverse\nimaging devices and population variances, which can diminish the global model's\neffectiveness. Existing aggregation methods generally fail to adapt across\nvaried circumstances. To address this, we propose FedCLAM, which integrates\n\\textit{client-adaptive momentum} terms derived from each client's loss\nreduction during local training, as well as a \\textit{personalized dampening\nfactor} to curb overfitting. We further introduce a novel \\textit{intensity\nalignment} loss that matches predicted and ground-truth foreground\ndistributions to handle heterogeneous image intensity profiles across\ninstitutions and devices. Extensive evaluations on two datasets show that\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\nunderscoring its efficacy. The code is available at\nhttps://github.com/siomvas/FedCLAM.", "AI": {"tldr": "FedCLAM\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u52a8\u91cf\u548c\u4e2a\u6027\u5316\u963b\u5c3c\u56e0\u5b50\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u7279\u5f81\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u5f3a\u5ea6\u5bf9\u9f50\u635f\u5931\u5904\u7406\u56fe\u50cf\u5f3a\u5ea6\u5206\u5e03\u5dee\u5f02\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u56e0\u8bbe\u5907\u548c\u4eba\u7fa4\u5dee\u5f02\u5bfc\u81f4\u7279\u5f81\u4e0d\u4e00\u81f4\uff0c\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\uff0c\u9700\u6539\u8fdb\u3002", "method": "FedCLAM\u7ed3\u5408\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u52a8\u91cf\u3001\u4e2a\u6027\u5316\u963b\u5c3c\u56e0\u5b50\u548c\u5f3a\u5ea6\u5bf9\u9f50\u635f\u5931\uff0c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cFedCLAM\u5728\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u516b\u79cd\u524d\u6cbf\u65b9\u6cd5\u3002", "conclusion": "FedCLAM\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u5dee\u5f02\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2506.22766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22766", "abs": "https://arxiv.org/abs/2506.22766", "authors": ["Yiting Chen", "Kenneth Kimble", "Howard H. Qian", "Podshara Chanrungmaneekul", "Robert Seney", "Kaiyu Hang"], "title": "Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025; 16 pages, 10\n  figures", "summary": "Robust and adaptive robotic peg-in-hole assembly under tight tolerances is\ncritical to various industrial applications. However, it remains an open\nchallenge due to perceptual and physical uncertainties from contact-rich\ninteractions that easily exceed the allowed clearance. In this paper, we study\nhow to leverage contact between the peg and its matching hole to eliminate\nuncertainties in the assembly process under unstructured settings. By examining\nthe role of compliance under contact constraints, we present a manipulation\nsystem that plans collision-inclusive interactions for the peg to 1)\niteratively identify its task environment to localize the target hole and 2)\nexploit environmental contact constraints to refine insertion motions into the\ntarget hole without relying on precise perception, enabling a robust solution\nto peg-in-hole assembly. By conceptualizing the above process as the\ncomposition of funneling in different state spaces, we present a formal\napproach to constructing manipulation funnels as an uncertainty-absorbing\nparadigm for peg-in-hole assembly. The proposed system effectively generalizes\nacross diverse peg-in-hole scenarios across varying scales, shapes, and\nmaterials in a learning-free manner. Extensive experiments on a NIST Assembly\nTask Board (ATB) and additional challenging scenarios validate its robustness\nin real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u7ea6\u675f\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63d2\u5b54\u88c5\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u78b0\u649e\u5305\u5bb9\u6027\u4ea4\u4e92\u548c\u6f0f\u6597\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u7d27\u5bc6\u516c\u5dee\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002", "motivation": "\u7d27\u5bc6\u516c\u5dee\u4e0b\u7684\u673a\u5668\u4eba\u63d2\u5b54\u88c5\u914d\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u611f\u77e5\u548c\u7269\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u3002", "method": "\u7814\u7a76\u5229\u7528\u63a5\u89e6\u7ea6\u675f\u6d88\u9664\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u78b0\u649e\u5305\u5bb9\u6027\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u6f0f\u6597\u5316\u7b56\u7565\u5728\u4e0d\u540c\u72b6\u6001\u7a7a\u95f4\u4e2d\u89c4\u5212\u88c5\u914d\u52a8\u4f5c\u3002", "result": "\u7cfb\u7edf\u5728\u591a\u79cd\u63d2\u5b54\u573a\u666f\uff08\u4e0d\u540c\u5c3a\u5bf8\u3001\u5f62\u72b6\u548c\u6750\u6599\uff09\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u7cbe\u786e\u611f\u77e5\u6216\u5b66\u4e60\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7d27\u5bc6\u516c\u5dee\u4e0b\u7684\u63d2\u5b54\u88c5\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2506.22652", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.22652", "abs": "https://arxiv.org/abs/2506.22652", "authors": ["Mohammad Reza Fasihi", "Brian L. Mark"], "title": "QoS-aware State-Augmented Learnable Algorithm for Wireless Coexistence Parameter Management", "comment": "13 pages, 7 figures", "summary": "Efficient and fair coexistence in unlicensed spectrum is essential to support\nheterogeneous networks such as 5G NR-U and Wi-Fi, which often contend for\nshared wireless resources. We introduce a general framework for wireless\nCoexistence Parameter Management (CPM) based on state-augmented constrained\nreinforcement learning. We propose a novel algorithm, QaSAL-CPM, which\nincorporates state-augmentation by embedding the dual variables in the\nconstrained optimization formulation directly into the agent's observation\nspace. This method enables the agent to respond to constraint violations in\nreal time while continuing to optimize a primary performance objective. Through\nextensive simulations of 5G NR-U and Wi-Fi coexistence scenarios, we show that\nQaSAL-CPM achieves reliable QoS compliance and improved policy robustness\nacross various transmitter densities compared to previous approaches. The\nproposed framework offers a scalable and adaptive solution for real-time\ncoexistence optimization in next-generation wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u589e\u5f3a\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u7ebf\u5171\u5b58\u53c2\u6570\u7ba1\u7406\u6846\u67b6QaSAL-CPM\uff0c\u7528\u4e8e\u4f18\u53165G NR-U\u548cWi-Fi\u7684\u5171\u5b58\u95ee\u9898\u3002", "motivation": "\u89e3\u51b35G NR-U\u548cWi-Fi\u5728\u975e\u6388\u6743\u9891\u8c31\u4e2d\u7684\u9ad8\u6548\u516c\u5e73\u5171\u5b58\u95ee\u9898\u3002", "method": "\u91c7\u7528\u72b6\u6001\u589e\u5f3a\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u53cc\u53d8\u91cf\u5d4c\u5165\u89c2\u5bdf\u7a7a\u95f4\uff0c\u5b9e\u65f6\u54cd\u5e94\u7ea6\u675f\u8fdd\u53cd\u5e76\u4f18\u5316\u6027\u80fd\u76ee\u6807\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0cQaSAL-CPM\u5b9e\u73b0\u4e86\u53ef\u9760\u7684QoS\u5408\u89c4\u6027\u548c\u66f4\u5f3a\u7684\u7b56\u7565\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u5b9e\u65f6\u5171\u5b58\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22486", "abs": "https://arxiv.org/abs/2506.22486", "authors": ["Ming Cheung"], "title": "Hallucination Detection with Small Language Models", "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6765\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u56de\u7b54\uff0c\u4ee5\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u6b63\u786e\u56de\u7b54\u65b9\u9762\u6bd4\u5e7b\u89c9\u63d0\u9ad8\u4e8610%\u7684F1\u5206\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002\u7f3a\u4e4f\u771f\u5b9e\u7b54\u6848\u65f6\uff0c\u8fd9\u4e9b\u95ee\u9898\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06LLM\u751f\u6210\u7684\u56de\u7b54\u5206\u89e3\u4e3a\u5355\u53e5\uff0c\u5e76\u5229\u7528\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u201c\u662f\u201d\u6807\u8bb0\u7684\u6982\u7387\u6765\u9a8c\u8bc1\u56de\u7b54\u7684\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e8610%\uff0c\u8868\u660e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u7528\u4e8e\u7b54\u6848\u9a8c\u8bc1\u3002", "conclusion": "\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u6709\u6548\u9a8c\u8bc1LLM\u751f\u6210\u7684\u56de\u7b54\uff0c\u4e3a\u5b66\u672f\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22653", "abs": "https://arxiv.org/abs/2506.22653", "authors": ["Michael Grosskopf", "Russell Bent", "Rahul Somasundaram", "Isaac Michaud", "Arthur Lui", "Nathan Debardeleben", "Earl Lawrence"], "title": "URSA: The Universal Research and Scientific Agent", "comment": "31 pages, 9 figures", "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.", "AI": {"tldr": "URSA\u662f\u4e00\u4e2a\u79d1\u5b66\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u548c\u5de5\u5177\u52a0\u901f\u7814\u7a76\u4efb\u52a1\uff0c\u7ed3\u5408\u5148\u8fdb\u7269\u7406\u6a21\u62df\u4ee3\u7801\uff0c\u89e3\u51b3\u590d\u6742\u79d1\u5b66\u95ee\u9898\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u590d\u6742\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u89e3\u51b3\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63a8\u52a8\u79d1\u5b66\u524d\u6cbf\u53d1\u5c55\u3002", "method": "\u5f00\u53d1URSA\u7cfb\u7edf\uff0c\u5305\u542b\u6a21\u5757\u5316\u4ee3\u7406\u548c\u5de5\u5177\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u62df\u4ee3\u7801\uff0c\u7075\u6d3b\u5e94\u5bf9\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u79d1\u5b66\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86URSA\u7684\u67b6\u6784\u53ca\u5176\u5728\u89e3\u51b3\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "URSA\u4e3a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u5c55\u793a\u4e86LLMs\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2506.22463", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22463", "abs": "https://arxiv.org/abs/2506.22463", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "comment": "26 pages, accepted by ICML 2025", "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "AI": {"tldr": "MoDiff\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5236\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\u63d0\u5347\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u73b0\u6709\u52a0\u901f\u6280\u672f\uff08\u5982\u7f13\u5b58\u548c\u91cf\u5316\uff09\u5728\u8ba1\u7b97\u8bef\u5dee\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faModulated Diffusion\uff08MoDiff\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u8c03\u5236\u91cf\u5316\u548c\u8bef\u5dee\u8865\u507f\uff0c\u9002\u7528\u4e8e\u6240\u6709\u6269\u6563\u6a21\u578b\u3002", "result": "MoDiff\u5c06\u6fc0\u6d3b\u91cf\u5316\u4ece8\u4f4d\u964d\u81f33\u4f4d\uff0c\u4e14\u5728CIFAR-10\u548cLSUN\u4e0a\u65e0\u6027\u80fd\u635f\u5931\u3002", "conclusion": "MoDiff\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2506.22596", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22596", "abs": "https://arxiv.org/abs/2506.22596", "authors": ["Md Rahatul Islam Udoy", "Wantong Li", "Kai Ni", "Ahmedullah Aziz"], "title": "Multi-Domain FeFET-Based Pixel for In-Sensor Multiply-and-Accumulate Operations", "comment": null, "summary": "This paper presents an FeFET-based active pixel sensor that performs\nin-sensor multiply-and-accumulate (MAC) operations by leveraging the\nmulti-domain polarization states of ferroelectric layers. The proposed design\nintegrates a programmable FeFET into a 3-transistor pixel circuit, where the\nFeFET's non-volatile conductance encodes the weight, and the photodiode voltage\ndrop encodes the input. Their interaction generates an output current\nproportional to the product, enabling in-pixel analog multiplication.\nAccumulation is achieved by summing output currents along shared column lines,\nrealizing full MAC functionality within the image sensor array. Extensive\nHSPICE simulations, using 45 nm CMOS models, validate the operation and confirm\nthe scalability of the design. This compact and power-efficient architecture\nminimizes data movement, making it ideal for real-time edge computing,\nneuromorphic vision, and secure sensing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFeFET\u7684\u4e3b\u52a8\u50cf\u7d20\u4f20\u611f\u5668\uff0c\u5229\u7528\u94c1\u7535\u5c42\u7684\u591a\u57df\u6781\u5316\u72b6\u6001\u5b9e\u73b0\u4f20\u611f\u5668\u5185\u4e58\u52a0\u8fd0\u7b97\u3002", "motivation": "\u51cf\u5c11\u6570\u636e\u79fb\u52a8\uff0c\u63d0\u9ad8\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8fb9\u7f18\u8ba1\u7b97\u3001\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u548c\u5b89\u5168\u4f20\u611f\u5e94\u7528\u3002", "method": "\u5c06\u53ef\u7f16\u7a0bFeFET\u96c6\u6210\u52303\u6676\u4f53\u7ba1\u50cf\u7d20\u7535\u8def\u4e2d\uff0c\u5229\u7528FeFET\u7684\u975e\u6613\u5931\u6027\u7535\u5bfc\u7f16\u7801\u6743\u91cd\uff0c\u5149\u7535\u4e8c\u6781\u7ba1\u7535\u538b\u964d\u7f16\u7801\u8f93\u5165\uff0c\u5b9e\u73b0\u50cf\u7d20\u5185\u6a21\u62df\u4e58\u6cd5\u3002", "result": "\u901a\u8fc7HSPICE\u4eff\u771f\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u7d27\u51d1\u4e14\u9ad8\u6548\uff0c\u9002\u5408\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.22769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22769", "abs": "https://arxiv.org/abs/2506.22769", "authors": ["Changshi Zhou", "Feng Luan", "Jiarui Hu", "Shaoqiang Meng", "Zhipeng Wang", "Yanchao Dong", "Yanmin Zhou", "Bin He"], "title": "Learning Efficient Robotic Garment Manipulation with Standardization", "comment": null, "summary": "Garment manipulation is a significant challenge for robots due to the complex\ndynamics and potential self-occlusion of garments. Most existing methods of\nefficient garment unfolding overlook the crucial role of standardization of\nflattened garments, which could significantly simplify downstream tasks like\nfolding, ironing, and packing. This paper presents APS-Net, a novel approach to\ngarment manipulation that combines unfolding and standardization in a unified\nframework. APS-Net employs a dual-arm, multi-primitive policy with dynamic\nfling to quickly unfold crumpled garments and pick-and-place (p and p) for\nprecise alignment. The purpose of garment standardization during unfolding\ninvolves not only maximizing surface coverage but also aligning the garment's\nshape and orientation to predefined requirements. To guide effective robot\nlearning, we introduce a novel factorized reward function for standardization,\nwhich incorporates garment coverage (Cov), keypoint distance (KD), and\nintersection-over-union (IoU) metrics. Additionally, we introduce a spatial\naction mask and an Action Optimized Module to improve unfolding efficiency by\nselecting actions and operation points effectively. In simulation, APS-Net\noutperforms state-of-the-art methods for long sleeves, achieving 3.9 percent\nbetter coverage, 5.2 percent higher IoU, and a 0.14 decrease in KD (7.09\npercent relative reduction). Real-world folding tasks further demonstrate that\nstandardization simplifies the folding process. Project page: see\nhttps://hellohaia.github.io/APS/", "AI": {"tldr": "APS-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c55\u5f00\u548c\u6807\u51c6\u5316\u7684\u670d\u88c5\u64cd\u4f5c\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u81c2\u591a\u539f\u8bed\u7b56\u7565\u548c\u52a8\u6001\u6295\u63b7\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u88c5\u5c55\u5f00\u548c\u6807\u51c6\u5316\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u670d\u88c5\u5c55\u5f00\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6807\u51c6\u5316\u7684\u91cd\u8981\u6027\uff0c\u800c\u6807\u51c6\u5316\u80fd\u663e\u8457\u7b80\u5316\u540e\u7eed\u4efb\u52a1\uff08\u5982\u6298\u53e0\u3001\u71a8\u70eb\u548c\u5305\u88c5\uff09\u3002", "method": "APS-Net\u91c7\u7528\u53cc\u81c2\u591a\u539f\u8bed\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u6001\u6295\u63b7\u548c\u62fe\u53d6\u653e\u7f6e\u6280\u672f\uff0c\u5e76\u5f15\u5165\u56e0\u5b50\u5316\u5956\u52b1\u51fd\u6570\u3001\u7a7a\u95f4\u52a8\u4f5c\u63a9\u7801\u548c\u52a8\u4f5c\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cAPS-Net\u5728\u957f\u8896\u670d\u88c5\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8986\u76d6\u7387\u548cIoU\u5206\u522b\u63d0\u53473.9%\u548c5.2%\uff0c\u5173\u952e\u70b9\u8ddd\u79bb\u51cf\u5c117.09%\u3002", "conclusion": "\u6807\u51c6\u5316\u663e\u8457\u7b80\u5316\u4e86\u6298\u53e0\u4efb\u52a1\uff0cAPS-Net\u4e3a\u670d\u88c5\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22702", "categories": ["eess.SY", "cs.AR", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22702", "abs": "https://arxiv.org/abs/2506.22702", "authors": ["Zina Mohamed", "Ammar B. Kouki", "Sonia A\u00efssa"], "title": "A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry", "comment": null, "summary": "Aiming at simplifying the hardware structure and reducing the energy\nconsumption in wireless communication via reconfigurable intelligent surfaces\n(RIS), this paper introduces a novel RIS design founded on the correlation\nbetween the phase shift values of the surface elements. First, a correlation\nanalysis is conducted, considering the azimuth angle of a target device within\na coverage region spanning from $-80^{\\circ}$ to $80^{\\circ}$. The correlation\nis demonstrated for different deployment cases, creating the basis for the new\nRIS structure, termed Connected-RIS, where correlated elements are designed to\nshare the same control signal. The fundamental performance of the proposed\ndesign is then analyzed in terms of control signals, power consumption, and\ncommunication system performance, comparing it to two RIS structures with full\ncontrol: one with the same size as the proposed design, and the other employing\nthe minimum number of elements necessary to satisfy the fair coverage\ncriterion. The correlation-based RIS design enables three-dimensional passive\nbeamforming and significantly reduces the number of required load impedances\nand control signals, thereby lowering the hardware cost and simplifying the\ncontrol circuitry. It also achieves substantial power savings as compared to\nthe baseline schemes, while maintaining sufficient gain for a fair radio\ncoverage. For instance, numerical simulations demonstrate that the proposed\ndesign reduces the power consumption by almost 86-92\\% and the control signals\nby 83-98\\% compared to operation with fully controlled RIS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u65b0\u578bRIS\u8bbe\u8ba1\uff08Connected-RIS\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u63a7\u5236\u4fe1\u53f7\u7b80\u5316\u786c\u4ef6\u7ed3\u6784\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u7b80\u5316\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u786c\u4ef6\u7ed3\u6784\u548c\u964d\u4f4e\u80fd\u8017\uff0c\u7279\u522b\u662f\u5728\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8868\u9762\u5143\u7d20\u76f8\u4f4d\u504f\u79fb\u503c\u7684\u76f8\u5173\u6027\uff0c\u8bbe\u8ba1Connected-RIS\uff0c\u5171\u4eab\u63a7\u5236\u4fe1\u53f7\uff0c\u51cf\u5c11\u8d1f\u8f7d\u963b\u6297\u548c\u63a7\u5236\u4fe1\u53f7\u6570\u91cf\u3002", "result": "\u663e\u8457\u964d\u4f4e\u529f\u8017\uff0886-92%\uff09\u548c\u63a7\u5236\u4fe1\u53f7\u6570\u91cf\uff0883-98%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "Connected-RIS\u8bbe\u8ba1\u5728\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u548c\u80fd\u8017\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u3002"}}
{"id": "2506.22491", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2506.22491", "abs": "https://arxiv.org/abs/2506.22491", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPromptAug\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u51b2\u7a81\u68c0\u6d4b\u4e2d\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548cF1\u5206\u6570\u4e0a\u63d0\u5347\u4e862%\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u51b2\u7a81\u68c0\u6d4b\u9700\u8981\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u540c\u65f6LLM\u7684\u9632\u62a4\u673a\u5236\u9650\u5236\u4e86\u51b2\u7a81\u76f8\u5173\u6570\u636e\u7684\u751f\u6210\u3002", "method": "\u63d0\u51faPromptAug\uff0c\u4e00\u79cd\u521b\u65b0\u7684LLM\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6781\u7aef\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3001\u591a\u6837\u6027\u5206\u6790\u548c\u4e3b\u9898\u5206\u6790\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "PromptAug\u5728\u51b2\u7a81\u548c\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862%\u7684\u51c6\u786e\u6027\u548cF1\u5206\u6570\u63d0\u5347\uff0c\u5e76\u8bc6\u522b\u4e86\u589e\u5f3a\u6587\u672c\u4e2d\u7684\u56db\u79cd\u95ee\u9898\u6a21\u5f0f\u3002", "conclusion": "PromptAug\u4e3a\u654f\u611f\u4efb\u52a1\uff08\u5982\u51b2\u7a81\u68c0\u6d4b\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u793e\u4f1a\u79d1\u5b66\u65b9\u6cd5\u8fdb\u884c\u8de8\u5b66\u79d1\u8bc4\u4f30\u3002"}}
{"id": "2506.22740", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22740", "abs": "https://arxiv.org/abs/2506.22740", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "title": "Explanations are a means to an end", "comment": null, "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e94\u9488\u5bf9\u5177\u4f53\u7528\u9014\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u907f\u514d\u6a21\u7cca\u6027\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u5408\u7684\u65b9\u5f0f\u8bc4\u4f30\u89e3\u91ca\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u5b9e\u9645\u7528\u9014\uff0c\u5bfc\u81f4\u89e3\u91ca\u53ef\u80fd\u88ab\u8bef\u7528\u6216\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u7684\u6846\u67b6\uff0c\u660e\u786e\u89e3\u91ca\u7684\u5177\u4f53\u7528\u9014\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u5408\u7684\u65b9\u5f0f\u8bc4\u4f30\u89e3\u91ca\u7684\u4ef7\u503c\u3002", "result": "\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u573a\u666f\uff08\u5982\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3001\u63d0\u4f9b\u8865\u6551\u63aa\u65bd\u6216\u8c03\u8bd5\uff09\uff0c\u5e76\u80fd\u91cf\u5316\u89e3\u91ca\u5bf9\u7406\u60f3\u51b3\u7b56\u8005\u6027\u80fd\u7684\u6700\u5927\u63d0\u5347\u3002", "conclusion": "\u89e3\u91ca\u6027\u65b9\u6cd5\u5e94\u9488\u5bf9\u5177\u4f53\u7528\u9014\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u660e\u786e\u7528\u9014\u548c\u7ed3\u5408\u7406\u8bba\u5b9e\u8bc1\u8bc4\u4f30\u6765\u63d0\u5347\u5176\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2506.22498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22498", "abs": "https://arxiv.org/abs/2506.22498", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "comment": null, "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "AI": {"tldr": "\u901a\u8fc7\u4f4e\u6210\u672c\u8d1f\u8f7d\u4f20\u611f\u5668\u548c\u56fe\u50cf\u878d\u5408\u6280\u672f\uff0cViFusionTST\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u60a3\u8005\u79bb\u5e8a\u610f\u56fe\uff0c\u63d0\u5347\u8dcc\u5012\u9884\u9632\u6548\u679c\u3002", "motivation": "\u533b\u9662\u548c\u957f\u671f\u62a4\u7406\u673a\u6784\u4e2d\uff0c\u5e8a\u65c1\u8dcc\u5012\u662f\u4e00\u4e2a\u4e3b\u8981\u4f24\u5bb3\u6765\u6e90\uff0c\u73b0\u6709\u8b66\u62a5\u7cfb\u7edf\u53cd\u5e94\u6ede\u540e\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4f4e\u6210\u672c\u8d1f\u8f7d\u4f20\u611f\u5668\uff0c\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3a\u4e92\u8865\u56fe\u50cf\uff08RGB\u7ebf\u56fe\u548c\u7eb9\u7406\u56fe\uff09\uff0c\u5e76\u901a\u8fc7\u53cc\u6d41Swin Transformer\uff08ViFusionTST\uff09\u8fdb\u884c\u5e76\u884c\u5904\u7406\u548c\u8de8\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cViFusionTST\u8fbe\u52300.885\u7684\u51c6\u786e\u7387\u548c0.794\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u878d\u5408\u7684\u8d1f\u8f7d\u4fe1\u53f7\u5206\u7c7b\u65b9\u6cd5\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u5b9e\u65f6\u8dcc\u5012\u9884\u9632\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22790", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.22790", "abs": "https://arxiv.org/abs/2506.22790", "authors": ["Yixu Chen", "Bowen Chen", "Hai Wei", "Alan C. Bovik", "Baojun Li", "Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Dounia Hammou", "Fei Yin", "Rafal Mantiuk", "Amritha Premkumar", "Prajit T Rajendran", "Vignesh V Menon"], "title": "ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge", "comment": "ICME 2025 Grand Challenges", "summary": "This paper reports IEEE International Conference on Multimedia \\& Expo (ICME)\n2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.\nWith the rapid development of video technology, especially High Dynamic Range\n(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and\ngeneralizable Video Quality Assessment (VQA) methods has become increasingly\ndemanded. Existing VQA models often struggle to deliver consistent performance\nacross varying dynamic ranges, distortion types, and diverse content. This\nchallenge was established to benchmark and promote VQA approaches capable of\njointly handling HDR and SDR content. In the final evaluation phase, five teams\nsubmitted seven models along with technical reports to the Full Reference (FR)\nand No Reference (NR) tracks. Among them, four methods outperformed VMAF\nbaseline, while the top-performing model achieved state-of-the-art performance,\nsetting a new benchmark for generalizable video quality assessment.", "AI": {"tldr": "ICME 2025\u6311\u6218\u8d5b\u805a\u7126\u4e8e\u901a\u7528HDR\u548cSDR\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff0c\u65e8\u5728\u63a8\u52a8\u8de8\u52a8\u6001\u8303\u56f4\u548c\u5931\u771f\u7c7b\u578b\u7684VQA\u65b9\u6cd5\u53d1\u5c55\u3002", "motivation": "\u968f\u7740HDR\u548cSDR\u89c6\u9891\u6280\u672f\u7684\u53d1\u5c55\uff0c\u73b0\u6709VQA\u6a21\u578b\u5728\u8de8\u52a8\u6001\u8303\u56f4\u548c\u5931\u771f\u7c7b\u578b\u7684\u4e00\u81f4\u6027\u8868\u73b0\u4e0d\u8db3\uff0c\u4e9f\u9700\u901a\u7528\u6027\u66f4\u5f3a\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6311\u6218\u8d5b\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u4e03\u79cd\u6a21\u578b\uff0c\u5206\u4e3a\u5168\u53c2\u8003\uff08FR\uff09\u548c\u65e0\u53c2\u8003\uff08NR\uff09\u4e24\u7c7b\u3002", "result": "\u4e94\u79cd\u63d0\u4ea4\u6a21\u578b\u4e2d\uff0c\u56db\u79cd\u4f18\u4e8eVMAF\u57fa\u7ebf\uff0c\u6700\u4f18\u6a21\u578b\u521b\u4e0b\u901a\u7528\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u7684\u65b0\u6807\u6746\u3002", "conclusion": "\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u901a\u7528VQA\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2506.22788", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22788", "abs": "https://arxiv.org/abs/2506.22788", "authors": ["Xuao Hou", "Yongquan Jia", "Shijin Zhang", "Yuqiang Wu"], "title": "SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information", "comment": null, "summary": "The widespread application of industrial robots in fields such as cutting and\nwelding has imposed increasingly stringent requirements on the trajectory\naccuracy of end-effectors. However, current error compensation methods face\nseveral critical challenges, including overly simplified mechanism modeling, a\nlack of physical consistency in data-driven approaches, and substantial data\nrequirements. These issues make it difficult to achieve both high accuracy and\nstrong generalization simultaneously. To address these challenges, this paper\nproposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).\nThis method integrates the kinematic equations of the robotic manipulator with\na Transformer architecture enhanced by sparse self-attention masks. A\nparameter-adaptive hybrid loss function incorporating spatial and physical\ninformation is employed to iteratively optimize the network during training,\nenabling high-precision error compensation under small-sample conditions.\nAdditionally, inverse joint angle compensation is performed using a gradient\ndescent-based optimization method. Experimental results on a small-sample\ndataset from a UR5 robotic arm (724 samples, with a train:test:validation split\nof 8:1:1) demonstrate the superior performance of the proposed method. It\nachieves a 3D absolute positioning error of 0.2515 mm with a standard deviation\nof 0.15 mm, representing a 35.16\\% reduction in error compared to conventional\ndeep neural network (DNN) methods. Furthermore, the inverse angle compensation\nalgorithm converges to an accuracy of 0.01 mm within an average of 147\niterations. This study presents a solution that combines physical\ninterpretability with data adaptability for high-precision control of\nindustrial robots, offering promising potential for the reliable execution of\nprecision tasks in intelligent manufacturing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0eTransformer\u67b6\u6784\u7684SPI-BoTER\u65b9\u6cd5\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u5668\u4eba\u8f68\u8ff9\u8bef\u5dee\u8865\u507f\uff0c\u5728\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u8f68\u8ff9\u7cbe\u5ea6\u9700\u6c42\u65e5\u76ca\u4e25\u683c\uff0c\u4f46\u73b0\u6709\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u5b58\u5728\u5efa\u6a21\u7b80\u5316\u3001\u6570\u636e\u9a71\u52a8\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\u53ca\u6570\u636e\u9700\u6c42\u5927\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0e\u5f3a\u6cdb\u5316\u3002", "method": "\u63d0\u51faSPI-BoTER\u65b9\u6cd5\uff0c\u878d\u5408\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u65b9\u7a0b\u4e0e\u7a00\u758f\u81ea\u6ce8\u610f\u529b\u63a9\u7801\u589e\u5f3a\u7684Transformer\u67b6\u6784\uff0c\u91c7\u7528\u53c2\u6570\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u9006\u5173\u8282\u89d2\u8865\u507f\u3002", "result": "\u5728UR5\u673a\u68b0\u81c2\u5c0f\u6837\u672c\u6570\u636e\u96c6\u4e0a\uff0c3D\u7edd\u5bf9\u5b9a\u4f4d\u8bef\u5dee\u4e3a0.2515 mm\uff08\u6807\u51c6\u5dee0.15 mm\uff09\uff0c\u8f83\u4f20\u7edfDNN\u65b9\u6cd5\u8bef\u5dee\u964d\u4f4e35.16%\uff1b\u9006\u89d2\u5ea6\u8865\u507f\u7b97\u6cd5\u5e73\u5747147\u6b21\u8fed\u4ee3\u6536\u655b\u81f30.01 mm\u7cbe\u5ea6\u3002", "conclusion": "SPI-BoTER\u7ed3\u5408\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e0e\u6570\u636e\u9002\u5e94\u6027\uff0c\u4e3a\u5de5\u4e1a\u673a\u5668\u4eba\u9ad8\u7cbe\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u667a\u80fd\u5236\u9020\u4e2d\u7cbe\u5bc6\u4efb\u52a1\u7684\u53ef\u9760\u6267\u884c\u3002"}}
{"id": "2506.22707", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.22707", "abs": "https://arxiv.org/abs/2506.22707", "authors": ["Md Abdullah-Al Kaiser", "Sugeet Sunder", "Ajey P. Jacob", "Akhilesh R. Jaiswal"], "title": "X-pSRAM: A Photonic SRAM with Embedded XOR Logic for Ultra-Fast In-Memory Computing", "comment": "8 pages, 6 figures, 1 table", "summary": "Traditional von Neumann architectures suffer from fundamental bottlenecks due\nto continuous data movement between memory and processing units, a challenge\nthat worsens with technology scaling as electrical interconnect delays become\nmore significant. These limitations impede the performance and energy\nefficiency required for modern data-intensive applications. In contrast,\nphotonic in-memory computing presents a promising alternative by harnessing the\nadvantages of light, enabling ultra-fast data propagation without\nlength-dependent impedance, thereby significantly reducing computational\nlatency and energy consumption. This work proposes a novel differential\nphotonic static random access memory (pSRAM) bitcell that facilitates\nelectro-optic data storage while enabling ultra-fast in-memory Boolean XOR\ncomputation. By employing cross-coupled microring resonators and differential\nphotodiodes, the XOR-augmented pSRAM (X-pSRAM) bitcell achieves at least 10 GHz\nread, write, and compute operations entirely in the optical domain.\nAdditionally, wavelength-division multiplexing (WDM) enables n-bit XOR\ncomputation in a single-shot operation, supporting massively parallel\nprocessing and enhanced computational efficiency. Validated on GlobalFoundries'\n45SPCLO node, the X-pSRAM consumed 13.2 fJ energy per bit for XOR computation,\nrepresenting a significant advancement toward next-generation optical computing\nwith applications in cryptography, hyperdimensional computing, and neural\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5dee\u5206\u5149\u5b50\u9759\u6001\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\uff08pSRAM\uff09\u6bd4\u7279\u5355\u5143\uff0c\u901a\u8fc7\u5149\u57df\u5b9e\u73b0\u8d85\u5feb\u6570\u636e\u5b58\u50a8\u548c\u5e03\u5c14\u5f02\u6216\uff08XOR\uff09\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u56e0\u6570\u636e\u5728\u5185\u5b58\u548c\u5904\u7406\u5355\u5143\u95f4\u9891\u7e41\u79fb\u52a8\u800c\u9762\u4e34\u6027\u80fd\u548c\u80fd\u6548\u74f6\u9888\uff0c\u5149\u5b50\u5185\u5b58\u8ba1\u7b97\u5229\u7528\u5149\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4ea4\u53c9\u8026\u5408\u5fae\u73af\u8c10\u632f\u5668\u548c\u5dee\u5206\u5149\u7535\u4e8c\u6781\u7ba1\u8bbe\u8ba1X-pSRAM\u6bd4\u7279\u5355\u5143\uff0c\u652f\u6301\u5149\u57df\u5185\u7684\u8d85\u5feb\u8bfb\u5199\u548c\u8ba1\u7b97\uff0c\u5e76\u5229\u7528\u6ce2\u5206\u590d\u7528\uff08WDM\uff09\u5b9e\u73b0\u5355\u6b21\u591a\u6bd4\u7279XOR\u8ba1\u7b97\u3002", "result": "X-pSRAM\u5728GlobalFoundries 45SPCLO\u8282\u70b9\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b010 GHz\u64cd\u4f5c\uff0c\u6bcf\u6bd4\u7279XOR\u8ba1\u7b97\u80fd\u8017\u4e3a13.2 fJ\u3002", "conclusion": "X-pSRAM\u4e3a\u4e0b\u4e00\u4ee3\u5149\u5b66\u8ba1\u7b97\u5728\u5bc6\u7801\u5b66\u3001\u8d85\u7ef4\u8ba1\u7b97\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.22508", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22508", "abs": "https://arxiv.org/abs/2506.22508", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentStealth\u7684\u672c\u5730\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u5de5\u4f5c\u6d41\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u533f\u540d\u5316\u6548\u679c\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u533f\u540d\u5316\u65b9\u6cd5\u8981\u4e48\u635f\u5bb3\u5b9e\u7528\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u4e91\u6a21\u578b\uff0c\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b9e\u7528\u6027\u63a7\u5236\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u5de5\u4f5c\u6d41\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5e76\u5229\u7528\u76d1\u7763\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316SLM\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u533f\u540d\u5316\u6548\u679c\uff08+12.3%\uff09\u548c\u5b9e\u7528\u6027\uff08+6.8%\uff09\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "conclusion": "AgentStealth\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u5b89\u5168\u7684\u672c\u5730\u5316\u533f\u540d\u5316\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.22774", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.22774", "abs": "https://arxiv.org/abs/2506.22774", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "comment": null, "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f26\u7406\u4e0e\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u65e8\u5728\u91cf\u5316AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\uff0c\u5f25\u8865\u73b0\u6709\u6307\u5357\u4e0e\u6280\u672f\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "motivation": "AI\u6280\u672f\u7684\u5e7f\u6cdb\u5f71\u54cd\u53ca\u5176\u590d\u6742\u6027\u5bfc\u81f4\u5bf9\u5176\u53ef\u4fe1\u5ea6\u7684\u8bc4\u4f30\u7f3a\u4e4f\u5168\u9762\u6027\u4e0e\u91cf\u5316\u65b9\u6cd5\uff0c\u4e9f\u9700\u4e00\u79cd\u7ed3\u5408\u4f26\u7406\u4e0e\u7b97\u6cd5\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408Trustworthy AI\u7684\u4f26\u7406\u7ec4\u4ef6\u4e0ePageRank\u3001TrustRank\u7684\u7b97\u6cd5\u6d41\u7a0b\uff0c\u63d0\u51fa\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316\u6307\u6807\u4e0e\u7406\u8bba\u5185\u5bb9\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9AI\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u7684\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aAI\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5ba2\u89c2\u3001\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u4f26\u7406\u4e0e\u7b97\u6cd5\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.22499", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.22499", "abs": "https://arxiv.org/abs/2506.22499", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "comment": null, "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u548c\u4f20\u7edf\u4ea4\u901a\u6570\u636e\u7684\u591a\u7c7b\u522b\u52a8\u6001\u8d77\u70b9-\u7ec8\u70b9\u9700\u6c42\u4f30\u8ba1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u672c\u5730\u4f20\u611f\u5668\u8def\u6bb5\u7684\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u672c\u5730\u4f20\u611f\u5668\u6570\u636e\u7a00\u758f\uff0c\u536b\u661f\u56fe\u50cf\u80fd\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u57ce\u5e02\u4ea4\u901a\u4fe1\u606f\uff0c\u514b\u670d\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u6d41\u7a0b\u63d0\u53d6\u8f66\u8f86\u7c7b\u522b\u4fe1\u606f\uff0c\u6784\u5efa\u8ba1\u7b97\u56fe\u6a21\u578b\u8054\u5408\u6821\u51c6\u4ea4\u901a\u8ba1\u6570\u548c\u65c5\u884c\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u536b\u661f\u6570\u636e\u663e\u8457\u63d0\u5347\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u65e0\u672c\u5730\u4f20\u611f\u5668\u7684\u8def\u6bb5\uff0c\u4e14\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u57ce\u5e02\uff0c\u6570\u636e\u8d28\u91cf\u5bf9\u536b\u661f\u56fe\u50cf\u5f71\u54cd\u9700\u8fdb\u4e00\u6b65\u5206\u6790\u3002"}}
{"id": "2506.22882", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22882", "abs": "https://arxiv.org/abs/2506.22882", "authors": ["Qilong Xing", "Zikai Song", "Yuteng Ye", "Yuke Chen", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "title": "CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation", "comment": "ICME 2025", "summary": "Segmentation of brain structures from MRI is crucial for evaluating brain\nmorphology, yet existing CNN and transformer-based methods struggle to\ndelineate complex structures accurately. While current diffusion models have\nshown promise in image segmentation, they are inadequate when applied directly\nto brain MRI due to neglecting anatomical information. To address this, we\npropose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating\nspatial anatomical features to enhance segmentation accuracy of the diffusion\nmodel. Specifically, we introduce distance field as an auxiliary anatomical\ncondition to provide global spatial context, alongside a collaborative\ndiffusion process to model its joint distribution with anatomical structures,\nenabling effective utilization of anatomical features for segmentation.\nFurthermore, we introduce a consistency loss to refine relationships between\nthe distance field and anatomical structures and design a time adapted channel\nattention module to enhance the U-Net feature fusion procedure. Extensive\nexperiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89e3\u5256\u5b66\u7279\u5f81\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff08CA-Diff\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u8111MRI\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709CNN\u548c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5728\u8111\u7ed3\u6784\u5206\u5272\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6269\u6563\u6a21\u578b\u56e0\u5ffd\u7565\u89e3\u5256\u4fe1\u606f\u800c\u76f4\u63a5\u5e94\u7528\u6548\u679c\u6709\u9650\u3002", "method": "\u5f15\u5165\u8ddd\u79bb\u573a\u4f5c\u4e3a\u89e3\u5256\u5b66\u6761\u4ef6\uff0c\u8bbe\u8ba1\u534f\u4f5c\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u5176\u8054\u5408\u5206\u5e03\uff0c\u5e76\u6dfb\u52a0\u4e00\u81f4\u6027\u635f\u5931\u548c\u65f6\u95f4\u9002\u5e94\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCA-Diff\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CA-Diff\u901a\u8fc7\u6574\u5408\u89e3\u5256\u5b66\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u8111MRI\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22827", "abs": "https://arxiv.org/abs/2506.22827", "authors": ["Andr\u00e9 Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 72.5% success rate in completing the full manipulation sequence.\nThese experiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89c4\u5212\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u6b65\u9aa4\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u5de5\u4e1a\u548c\u5bb6\u5ead\u73af\u5883\u4e2d\u53ef\u9760\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u5206\u4e09\u5c42\u8bbe\u8ba1\uff1a\u4f4e\u5c42RL\u63a7\u5236\u5668\u8ddf\u8e2a\u5168\u8eab\u8fd0\u52a8\u76ee\u6807\uff1b\u4e2d\u5c42\u6a21\u4eff\u5b66\u4e60\u6280\u80fd\u7b56\u7565\u751f\u6210\u4efb\u52a1\u6b65\u9aa4\u7684\u8fd0\u52a8\u76ee\u6807\uff1b\u9ad8\u5c42\u89c6\u89c9\u8bed\u8a00\u89c4\u5212\u6a21\u5757\u51b3\u5b9a\u6280\u80fd\u6267\u884c\u5e76\u5b9e\u65f6\u76d1\u63a7\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e8640\u6b21\u5b9e\u9a8c\uff0c\u5b8c\u6210\u5b8c\u6574\u64cd\u4f5c\u5e8f\u5217\u7684\u6210\u529f\u7387\u4e3a72.5%\u3002", "conclusion": "\u5206\u5c42\u7cfb\u7edf\u53ef\u884c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u9aa4\u64cd\u4f5c\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2506.22804", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.22804", "abs": "https://arxiv.org/abs/2506.22804", "authors": ["Jingyuan Li", "Dawei Shi", "Ling Shi"], "title": "Online Coreset Selection for Learning Dynamic Systems", "comment": null, "summary": "With the increasing availability of streaming data in dynamic systems, a\ncritical challenge in data-driven modeling for control is how to efficiently\nselect informative data to characterize system dynamics. In this work, we\ndesign an online coreset selection method under the framework of set-membership\nidentification for systems subject to process disturbances, with the objective\nof improving data efficiency while ensuring convergence guarantees.\nSpecifically, we first propose a stacked polyhedral representation that\nover-approximates the feasible set of system parameters. Leveraging a\ngeneralized Gr\\\"unbaum's inequality, we design a geometric selection criterion\nfor constructing the coreset. To reduce computational complexity, an online\ndouble-description-based constraint reduction method is introduced to simplify\nthe polyhedral representation. Finally, we analyze the convergence of the\nfeasible set with respect to the coreset and derive upper bounds on the\nselection probability and the expected number of data in the coreset. The\neffectiveness of the proposed method is demonstrated through comprehensive\nsimulation studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u7cfb\u7edf\u4e2d\u9ad8\u6548\u9009\u62e9\u4fe1\u606f\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u6548\u7387\u5e76\u786e\u4fdd\u6536\u655b\u6027\u3002", "motivation": "\u52a8\u6001\u7cfb\u7edf\u4e2d\u6d41\u6570\u636e\u7684\u589e\u52a0\u4f7f\u5f97\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u9762\u4e34\u5982\u4f55\u9ad8\u6548\u9009\u62e9\u4fe1\u606f\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u96c6\u5408\u6210\u5458\u8bc6\u522b\u7684\u5728\u7ebf\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u5229\u7528\u5806\u53e0\u591a\u9762\u4f53\u8868\u793a\u548c\u51e0\u4f55\u9009\u62e9\u51c6\u5219\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u53cc\u63cf\u8ff0\u7ea6\u675f\u7b80\u5316\u8ba1\u7b97\u3002", "result": "\u5206\u6790\u4e86\u6838\u5fc3\u96c6\u7684\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u9009\u62e9\u6982\u7387\u548c\u6838\u5fc3\u96c6\u6570\u636e\u6570\u91cf\u7684\u4e0a\u754c\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u4fdd\u8bc1\u6536\u655b\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2506.22510", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22510", "abs": "https://arxiv.org/abs/2506.22510", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDGCL\u7684\u591a\u9886\u57df\u9884\u8bad\u7ec3\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u6846\u67b6\uff0c\u9488\u5bf9\u56fe\u6570\u636e\u4e2d\u7684\u9886\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u548c\u5f15\u5165\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u56fe\u6570\u636e\u5728\u4e0d\u540c\u9886\u57df\u95f4\u5b58\u5728\u663e\u8457\u7684\u8bed\u4e49\u548c\u5c5e\u6027\u5dee\u5f02\uff0c\u800c\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u672a\u80fd\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u5dee\u5f02\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u80fd\u8bc6\u522b\u9886\u57df\u5dee\u5f02\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u57df\u4ee4\u724c\u7f16\u7801\u5168\u5c40\u4fe1\u606f\uff1b\u5728\u4e0b\u6e38\u9636\u6bb5\uff0c\u91c7\u7528\u57df\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMDGCL\u5728\u51c6\u786e\u7387\u548cMacro-F1\u5206\u6570\u4e0a\u5206\u522b\u6700\u9ad8\u63d0\u5347\u4e8619.33%\u548c19.13%\u3002", "conclusion": "MDGCL\u901a\u8fc7\u6709\u6548\u5904\u7406\u9886\u57df\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u9886\u57df\u56fe\u6570\u636e\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.22865", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22865", "abs": "https://arxiv.org/abs/2506.22865", "authors": ["Ziqi Zhong", "Xunzhu Tang"], "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.", "AI": {"tldr": "ReasonBridge\u901a\u8fc7\u5c42\u6b21\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u95ed\u6e90\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u9ad8\u6548\u8fc1\u79fb\u5230\u5f00\u6e90\u6a21\u578b\uff0c\u663e\u8457\u7f29\u5c0f\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u95ed\u6e90\u4e0e\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u84b8\u998f\u3001\u7a00\u758f\u9002\u914d\u5668\u67b6\u6784\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u673a\u5236\uff0c\u4f7f\u7528\u7cbe\u5fc3\u7b5b\u9009\u7684Reason1K\u6570\u636e\u96c6\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u534723%\uff0cQwen2.5-14B\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "ReasonBridge\u4e3a\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u548c\u67b6\u6784\u3002"}}
{"id": "2506.22500", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.22500", "abs": "https://arxiv.org/abs/2506.22500", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u624b\u672f\u5ba4\u98ce\u9669\u68c0\u6d4b\u4e2d\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\uff08VS-KC\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6OR-VSKC\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u672f\u98ce\u9669\u8bc6\u522b\u5bf9\u60a3\u8005\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709MLLMs\u5b58\u5728\u89c6\u89c9-\u8bed\u4e49\u77e5\u8bc6\u51b2\u7a81\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u89c6\u89c9\u5b89\u5168\u8fdd\u89c4\u3002", "method": "\u751f\u6210\u5305\u542b34,000\u5f20\u5408\u6210\u56fe\u50cf\u7684\u6570\u636e\u96c6OR-VSKC\uff0c\u6db5\u76d6\u624b\u672f\u5ba4\u573a\u666f\u4e2d\u7684\u5b89\u5168\u8fdd\u89c4\u5b9e\u4f53\uff0c\u5e76\u8f85\u4ee5214\u5f20\u4eba\u5de5\u6807\u6ce8\u56fe\u50cf\u4f5c\u4e3a\u9a8c\u8bc1\u57fa\u51c6\u3002\u901a\u8fc7\u5fae\u8c03MLLMs\u7814\u7a76VS-KC\u3002", "result": "\u5fae\u8c03\u540e\u7684MLLMs\u5728\u8bad\u7ec3\u8fc7\u7684\u51b2\u7a81\u5b9e\u4f53\u68c0\u6d4b\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5bf9\u672a\u8bad\u7ec3\u5b9e\u4f53\u7c7b\u578b\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "OR-VSKC\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u7814\u7a76VS-KC\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u4f46\u9700\u66f4\u5168\u9762\u7684\u8bad\u7ec3\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.22952", "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.22952", "abs": "https://arxiv.org/abs/2506.22952", "authors": ["Yanwu Yang", "Thomas Wolfers"], "title": "Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization", "comment": null, "summary": "Understanding brain dynamics through functional Magnetic Resonance Imaging\n(fMRI) remains a fundamental challenge in neuroscience, particularly in\ncapturing how the brain transitions between various functional states.\nRecently, metastability, which refers to temporarily stable brain states, has\noffered a promising paradigm to quantify complex brain signals into\ninterpretable, discretized representations. In particular, compared to\ncluster-based machine learning approaches, tokenization approaches leveraging\nvector quantization have shown promise in representation learning with powerful\nreconstruction and predictive capabilities. However, most existing methods\nignore brain transition dependencies and lack a quantification of brain\ndynamics into representative and stable embeddings. In this study, we propose a\nHierarchical State space-based Tokenization network, termed HST, which\nquantizes brain states and transitions in a hierarchical structure based on a\nstate space-based model. We introduce a refined clustered Vector-Quantization\nVariational AutoEncoder (VQ-VAE) that incorporates quantization error feedback\nand clustering to improve quantization performance while facilitating\nmetastability with representative and stable token representations. We validate\nour HST on two public fMRI datasets, demonstrating its effectiveness in\nquantifying the hierarchical dynamics of the brain and its potential in disease\ndiagnosis and reconstruction performance. Our method offers a promising\nframework for the characterization of brain dynamics, facilitating the analysis\nof metastability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHST\u7684\u5206\u5c42\u72b6\u6001\u7a7a\u95f4\u6807\u8bb0\u5316\u7f51\u7edc\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u8111\u72b6\u6001\u548c\u52a8\u6001\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684VQ-VAE\u65b9\u6cd5\u63d0\u5347\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u52a8\u6001\u529f\u80fd\u72b6\u6001\u8f6c\u6362\u662f\u795e\u7ecf\u79d1\u5b66\u7684\u91cd\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u72b6\u6001\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4e14\u7f3a\u4e4f\u7a33\u5b9a\u8868\u5f81\u3002", "method": "\u91c7\u7528\u5206\u5c42\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u4f18\u5316\u7684VQ-VAE\uff0c\u7ed3\u5408\u91cf\u5316\u8bef\u5dee\u53cd\u9988\u548c\u805a\u7c7b\uff0c\u751f\u6210\u7a33\u5b9a\u4e14\u5177\u4ee3\u8868\u6027\u7684\u6807\u8bb0\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00fMRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86HST\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u75be\u75c5\u8bca\u65ad\u548c\u91cd\u5efa\u6027\u80fd\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "HST\u4e3a\u5927\u8111\u52a8\u6001\u8868\u5f81\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5206\u6790\u5927\u8111\u7684\u4e9a\u7a33\u6001\u7279\u6027\u3002"}}
{"id": "2506.22894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22894", "abs": "https://arxiv.org/abs/2506.22894", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "comment": null, "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u4e3b\u6f02\u79fb\uff0c\u7ed3\u5408\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\u548c\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u7a33\u5b9a\u64cd\u4f5c\u3002", "motivation": "\u81ea\u4e3b\u6f02\u79fb\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5176\u9ad8\u4e0d\u7a33\u5b9a\u6027\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u6216\u7f3a\u4e4f\u5b89\u5168\u6027\u4fdd\u969c\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u6a21\u578b\u6f02\u79fb\u52a8\u529b\u5b66\uff0c\u5f15\u5165\u9884\u6d4b\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u7ebf\u8c03\u6574\u52a8\u4f5c\u4ee5\u907f\u514d\u4e0d\u5b89\u5168\u72b6\u6001\u3002", "result": "\u5728Matlab-Carsim\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u6f02\u79fb\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c11\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2506.22855", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.22855", "abs": "https://arxiv.org/abs/2506.22855", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity", "comment": "Journal of the Franklin Institute", "summary": "Distributed optimization advances centralized machine learning methods by\nenabling parallel and decentralized learning processes over a network of\ncomputing nodes. This work provides an accelerated consensus-based distributed\nalgorithm for locally non-convex optimization using the gradient-tracking\ntechnique. The proposed algorithm (i) improves the convergence rate by adding\nmomentum towards the optimal state using the heavy-ball method, while (ii)\naddressing general sector-bound nonlinearities over the information-sharing\nnetwork. The link nonlinearity includes any sign-preserving odd sector-bound\nmapping, for example, log-scale data quantization or clipping in practical\napplications. For admissible momentum and gradient-tracking parameters, using\nperturbation theory and eigen-spectrum analysis, we prove convergence even in\nthe presence of sector-bound nonlinearity and for locally non-convex cost\nfunctions. Further, in contrast to most existing weight-stochastic algorithms,\nwe adopt weight-balanced (WB) network design. This WB design and\nperturbation-based analysis allow to handle dynamic directed network of agents\nto address possible time-varying setups due to link failures or packet drops.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u901f\u5171\u8bc6\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408\u68af\u5ea6\u8ffd\u8e2a\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5c40\u90e8\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u52a8\u91cf\uff08heavy-ball\u65b9\u6cd5\uff09\u548c\u6743\u91cd\u5e73\u8861\u7f51\u7edc\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u5e76\u5904\u7406\u4e86\u7f51\u7edc\u4e2d\u7684\u975e\u7ebf\u6027\u95ee\u9898\u3002", "motivation": "\u5206\u5e03\u5f0f\u4f18\u5316\u5728\u5e76\u884c\u548c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u975e\u51f8\u4f18\u5316\u548c\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u8ffd\u8e2a\u548c\u52a8\u91cf\u52a0\u901f\u7684\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u6743\u91cd\u5e73\u8861\u7f51\u7edc\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6270\u52a8\u7406\u8bba\u548c\u7279\u5f81\u8c31\u5206\u6790\u8bc1\u660e\u5176\u6536\u655b\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u5b58\u5728\u975e\u7ebf\u6027\u6620\u5c04\u548c\u5c40\u90e8\u975e\u51f8\u6210\u672c\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6536\u655b\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u6709\u5411\u7f51\u7edc\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u52a8\u6001\u7f51\u7edc\u73af\u5883\u3002"}}
{"id": "2506.22516", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.22516", "abs": "https://arxiv.org/abs/2506.22516", "authors": ["Jingkai Li"], "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22893", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.22893", "abs": "https://arxiv.org/abs/2506.22893", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u4f01\u4e1a\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u516d\u9879\u539f\u5219\u4ee5\u63a8\u52a8\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\uff0c\u5e76\u5f3a\u8c03\u5e02\u573a\u673a\u5236\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76AI\u5982\u4f55\u63d0\u5347\u4f01\u4e1a\u51b3\u7b56\u6548\u7387\uff0c\u5f25\u8865\u5f53\u524d\u4ee5AI\u4e3a\u4e2d\u5fc3\u7684\u7528\u6237\u8303\u5f0f\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4f01\u4e1a\u51b3\u7b56\u9700\u6c42\uff0c\u63d0\u51fa\u516d\u9879\u539f\u5219\uff0c\u5e76\u5efa\u8bae\u91c7\u7528\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u548c\u5e02\u573a\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u516d\u9879\u539f\u5219\uff0c\u5f3a\u8c03\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u5bf9\u63d0\u5347\u4f01\u4e1a\u51b3\u7b56\u6548\u7387\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u548c\u5e02\u573a\u673a\u5236\u662f\u5b9e\u73b0\u4f01\u4e1a\u51b3\u7b56\u6548\u7387\u63d0\u5347\u7684\u5173\u952e\u3002"}}
{"id": "2506.22501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22501", "abs": "https://arxiv.org/abs/2506.22501", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatialNet-ViT\u7684\u65b0\u6a21\u578b\uff0c\u7ed3\u5408Vision Transformers\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u9065\u611f\u5206\u7c7b\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u6216\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u9065\u611f\u5206\u7c7b\u6311\u6218\u3002", "method": "\u91c7\u7528Vision Transformers\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u3001\u8fc1\u79fb\u5b66\u4e60\u7b49\u6280\u672f\u3002", "result": "\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "SpatialNet-ViT\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u4e0e\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4e3a\u9065\u611f\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23002", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23002", "abs": "https://arxiv.org/abs/2506.23002", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Zabih Ghassemlooy", "Wai Lok Woo"], "title": "An Image Processing Based Blur Reduction Technique in Smartphone-to-Smartphone Visible Light Communication System", "comment": null, "summary": "In this paper, we present a blur reduction technique for\nsmartphone-to-smartphone visible light communications (S2SVLC). The key\ntechnique it to avoid the repeated scanning of the transmitted data and to\nlower the amount of data discarded at the receiver end of the S2SVLC system.\nThis image processing method will improve the system recognition efficiency and\ndata rate. The proposed method includes converting the red-green-blue (RGB)\nimage into grayscale, applying contrast enhancement, scaling and binarizing the\nimage to reduce the blur levels in the image. The experiment includes practical\ndata acquisition and further processing and estimation in MATLAB. The\nexperiment is carried out in different conditions like distance, rotation, and\ntilt also considering different surrounding illuminations like ambient light\nand no light conditions to estimate the blur levels in S2SVLC. In this\nexperimental investigation two types of coding, American Standard code for\ninformation interchange (ASCII), and quick response (QR) code are used for data\ntransmission in S2SVLC. The obtained results indicate that, the proposed\ntechnique is proven to improve the recovery efficiency to 96% in the receiver\nend at different conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c11\u667a\u80fd\u624b\u673a\u95f4\u53ef\u89c1\u5149\u901a\u4fe1\uff08S2SVLC\uff09\u6a21\u7cca\u7684\u6280\u672f\uff0c\u901a\u8fc7\u907f\u514d\u91cd\u590d\u626b\u63cf\u548c\u51cf\u5c11\u63a5\u6536\u7aef\u6570\u636e\u4e22\u5f03\uff0c\u63d0\u9ad8\u8bc6\u522b\u6548\u7387\u548c\u6570\u636e\u901f\u7387\u3002", "motivation": "\u89e3\u51b3S2SVLC\u7cfb\u7edf\u4e2d\u56e0\u56fe\u50cf\u6a21\u7cca\u5bfc\u81f4\u7684\u6570\u636e\u8bc6\u522b\u6548\u7387\u4f4e\u548c\u6570\u636e\u4e22\u5931\u95ee\u9898\u3002", "method": "\u5c06RGB\u56fe\u50cf\u8f6c\u4e3a\u7070\u5ea6\u56fe\uff0c\u8fdb\u884c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u3001\u7f29\u653e\u548c\u4e8c\u503c\u5316\u5904\u7406\u4ee5\u51cf\u5c11\u6a21\u7cca\u3002\u5b9e\u9a8c\u5728\u4e0d\u540c\u8ddd\u79bb\u3001\u65cb\u8f6c\u3001\u503e\u659c\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\uff0c\u4f7f\u7528ASCII\u548cQR\u7801\u4f20\u8f93\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6280\u672f\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5c06\u63a5\u6536\u7aef\u7684\u6062\u590d\u6548\u7387\u63d0\u9ad8\u523096%\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86S2SVLC\u7cfb\u7edf\u4e2d\u7684\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6062\u590d\u6548\u7387\u3002"}}
{"id": "2506.22942", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22942", "abs": "https://arxiv.org/abs/2506.22942", "authors": ["Kartik A. Pant", "Jaehyeok Kim", "James M. Goppert", "Inseok Hwang"], "title": "Energy-Constrained Resilient Multi-Robot Coverage Control", "comment": "6 pages, 4 figures", "summary": "The problem of multi-robot coverage control becomes significantly challenging\nwhen multiple robots leave the mission space simultaneously to charge their\nbatteries, disrupting the underlying network topology for communication and\nsensing. To address this, we propose a resilient network design and control\napproach that allows robots to achieve the desired coverage performance while\nsatisfying energy constraints and maintaining network connectivity throughout\nthe mission. We model the combined motion, energy, and network dynamics of the\nmultirobot systems (MRS) as a hybrid system with three modes, i.e., coverage,\nreturn-to-base, and recharge, respectively. We show that ensuring the energy\nconstraints can be transformed into designing appropriate guard conditions for\nmode transition between each of the three modes. Additionally, we present a\nsystematic procedure to design, maintain, and reconfigure the underlying\nnetwork topology using an energy-aware bearing rigid network design, enhancing\nthe structural resilience of the MRS even when a subset of robots departs to\ncharge their batteries. Finally, we validate our proposed method using\nnumerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u8986\u76d6\u63a7\u5236\u7684\u5f39\u6027\u7f51\u7edc\u8bbe\u8ba1\u4e0e\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5145\u7535\u65f6\u7f51\u7edc\u62d3\u6251\u4e2d\u65ad\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u540c\u65f6\u5145\u7535\u4f1a\u5bfc\u81f4\u901a\u4fe1\u548c\u611f\u77e5\u7f51\u7edc\u62d3\u6251\u4e2d\u65ad\uff0c\u5f71\u54cd\u8986\u76d6\u6027\u80fd\u3002", "method": "\u5c06\u7cfb\u7edf\u5efa\u6a21\u4e3a\u4e09\u79cd\u6a21\u5f0f\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u80fd\u91cf\u611f\u77e5\u7684\u8f74\u627f\u521a\u6027\u7f51\u7edc\u4ee5\u589e\u5f3a\u5f39\u6027\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u8986\u76d6\u6027\u80fd\u3001\u80fd\u91cf\u7ea6\u675f\u548c\u7f51\u7edc\u8fde\u901a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u8986\u76d6\u63a7\u5236\u4e2d\u7684\u5145\u7535\u548c\u7f51\u7edc\u5f39\u6027\u95ee\u9898\u3002"}}
{"id": "2506.22867", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.22867", "abs": "https://arxiv.org/abs/2506.22867", "authors": ["Faizal Hafiz", "Amelia Kunze", "Enrico Formenti", "Davide La Torre"], "title": "Identification of Cellular Automata on Spaces of Bernoulli Probability Measures", "comment": null, "summary": "Classical Cellular Automata (CCAs) are a powerful computational framework for\nmodeling global spatio-temporal dynamics with local interactions. While CCAs\nhave been applied across numerous scientific fields, identifying the local rule\nthat governs observed dynamics remains a challenging task. Moreover, the\nunderlying assumption of deterministic cell states often limits the\napplicability of CCAs to systems characterized by inherent uncertainty. This\nstudy, therefore, focuses on the identification of Cellular Automata on spaces\nof probability measures (CAMs), where cell states are represented by\nprobability distributions. This framework enables the modeling of systems with\nprobabilistic uncertainty and spatially varying dynamics. Moreover, we\nformulate the local rule identification problem as a parameter estimation\nproblem and propose a meta-heuristic search based on Self-adaptive Differential\nEvolution (SaDE) to estimate local rule parameters accurately from the observed\ndata. The efficacy of the proposed approach is demonstrated through local rule\nidentification in two-dimensional CAMs with varying neighborhood types and\nradii.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u5ea6\u91cf\u7684\u7ec6\u80de\u81ea\u52a8\u673a\uff08CAMs\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\uff08SaDE\uff09\u89e3\u51b3\u5c40\u90e8\u89c4\u5219\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7ec6\u80de\u81ea\u52a8\u673a\uff08CCAs\uff09\u5728\u5efa\u6a21\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7cfb\u7edf\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u8f6c\u5411\u6982\u7387\u5ea6\u91cf\u7684\u7ec6\u80de\u81ea\u52a8\u673a\uff08CAMs\uff09\u3002", "method": "\u5c06\u5c40\u90e8\u89c4\u5219\u8bc6\u522b\u95ee\u9898\u8f6c\u5316\u4e3a\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u81ea\u9002\u5e94\u5dee\u5206\u8fdb\u5316\uff08SaDE\uff09\u7684\u5143\u542f\u53d1\u5f0f\u641c\u7d22\u65b9\u6cd5\u3002", "result": "\u65b9\u6cd5\u5728\u4e8c\u7ef4CAMs\u4e2d\u6210\u529f\u8bc6\u522b\u4e86\u4e0d\u540c\u90bb\u57df\u7c7b\u578b\u548c\u534a\u5f84\u7684\u5c40\u90e8\u89c4\u5219\u3002", "conclusion": "CAMs\u6846\u67b6\u548cSaDE\u7b97\u6cd5\u4e3a\u5efa\u6a21\u548c\u8bc6\u522b\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.22518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22518", "abs": "https://arxiv.org/abs/2506.22518", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRefined Graph-based RAG (ReG)\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u53cd\u9988\u6539\u8fdb\u68c0\u7d22\u5668\u8d28\u91cf\uff0c\u5e76\u91cd\u7ec4\u68c0\u7d22\u7ed3\u679c\u4ee5\u63d0\u5347\u56fe\u57faRAG\u6027\u80fd\u3002\u5b9e\u9a8c\u663e\u793aReG\u663e\u8457\u63d0\u5347\u6548\u679c\uff0c\u51cf\u5c11\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u56fe\u57faRAG\u4e2d\u68c0\u7d22\u5668\u56e0\u7f3a\u4e4f\u771f\u5b9e\u6807\u6ce8\u548c\u62bd\u8c61\u56fe\u6570\u636e\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u7ed3\u5408LLM\u53cd\u9988\u6d88\u9664\u865a\u5047\u4fe1\u53f7\uff0c\u5f15\u5165\u7ed3\u6784\u611f\u77e5\u91cd\u7ec4\u6a21\u5757\u6574\u7406\u68c0\u7d22\u7ed3\u679c\u3002", "result": "ReG\u5728\u4e0d\u540cLLM\u4e0a\u63d0\u5347\u6027\u80fd\u8fbe10%\uff0c\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u964d\u4f4e\u63a8\u7406\u6210\u672c30%\u3002", "conclusion": "ReG\u6709\u6548\u63d0\u5347\u56fe\u57faRAG\u6027\u80fd\uff0c\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5916\u77e5\u8bc6\u56fe\u8c31\u3002"}}
{"id": "2506.22919", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22919", "abs": "https://arxiv.org/abs/2506.22919", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "comment": null, "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "AI": {"tldr": "Hecto\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408GRU\u548cFFNN\u4e13\u5bb6\u5b9e\u73b0\u5f02\u6784\u8ba1\u7b97\uff0c\u63d0\u5347\u4e13\u4e1a\u5316\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfMoE\u6a21\u578b\u7684\u4e13\u5bb6\u4f9d\u8d56\u76f8\u540c\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u9650\u5236\u4e86\u8868\u793a\u591a\u6837\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0cHecto\u65e8\u5728\u901a\u8fc7\u5f02\u6784\u67b6\u6784\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Hecto\u7ed3\u5408GRU\u4e13\u5bb6\uff08\u65f6\u95f4\u63a8\u7406\uff09\u548cFFNN\u4e13\u5bb6\uff08\u9759\u6001\u62bd\u8c61\uff09\uff0c\u91c7\u7528\u7a00\u758fTop-1\u95e8\u63a7\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHecto\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u540c\u8d28\u57fa\u7ebf\uff0c\u4e13\u5bb6\u8868\u73b0\u51fa\u660e\u786e\u7684\u4e13\u4e1a\u5316\uff08\u65f6\u95f4vs\u9759\u6001\u63a8\u7406\uff09\u3002", "conclusion": "Hecto\u4e3a\u6761\u4ef6\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u5176\u5f02\u6784\u67b6\u6784\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.22503", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22503", "abs": "https://arxiv.org/abs/2506.22503", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "comment": null, "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "AI": {"tldr": "\u5229\u7528\u4e09\u7ef4\u59ff\u6001\u8ffd\u8e2a\u6570\u636e\u6539\u8fdb\u8db3\u7403\u76d8\u5e26\u6280\u80fd\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e8c\u7ef4\u6570\u636e\uff0c\u65b0\u7279\u5f81\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e8c\u7ef4\u6570\u636e\u65e0\u6cd5\u5168\u9762\u6355\u6349\u76d8\u5e26\u4e2d\u7684\u5e73\u8861\u3001\u65b9\u5411\u548c\u63a7\u7403\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u5206\u6790\u6df1\u5ea6\u3002", "method": "\u4ece2022/23\u8d5b\u5b63\u6b27\u51a0\u76841736\u6b21\u76d8\u5e26\u4e2d\u63d0\u53d6\u59ff\u6001\u7279\u5f81\uff0c\u7ed3\u5408\u4f20\u7edf\u4e8c\u7ef4\u6570\u636e\u8bc4\u4f30\u5176\u5bf9\u76d8\u5e26\u6210\u529f\u7684\u5f71\u54cd\u3002", "result": "\u653b\u51fb\u8005\u7684\u5e73\u8861\u53ca\u4e0e\u9632\u5b88\u8005\u65b9\u5411\u5bf9\u9f50\u7684\u59ff\u6001\u7279\u5f81\u5bf9\u9884\u6d4b\u76d8\u5e26\u6210\u529f\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4e09\u7ef4\u59ff\u6001\u6570\u636e\u4e3a\u8db3\u7403\u76d8\u5e26\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u89c6\u89d2\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4e8c\u7ef4\u65b9\u6cd5\u3002"}}
{"id": "2506.23005", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23005", "abs": "https://arxiv.org/abs/2506.23005", "authors": ["Vaigai Nayaki Yokar", "Hoa Le Minh", "Zabih Ghassemlooy", "Wai Lok Woo"], "title": "Channel characterization in screen-to-camera based optical camera communication", "comment": null, "summary": "With the increase in optical camera communication (OCC), a screen to\ncamera-based communication can be established. This opens a new field of\nvisible light communication (VLC) known as smartphone to smartphone based\nvisible light communication (S2SVLC) system. In this paper, we experimentally\ndemonstrate a S2SVLC system based on VLC technology using a smartphone screen\nand a smartphone camera over a link span of 20 cms. We analyze the Lambertian\norder of the smartphone screen and carry out a channel characterization of a\nscreen to camera link-based VLC system under specific test conditions.", "AI": {"tldr": "\u8bba\u6587\u5b9e\u9a8c\u6027\u5730\u5c55\u793a\u4e86\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u5c4f\u5e55\u548c\u6444\u50cf\u5934\u7684S2SVLC\u7cfb\u7edf\uff0c\u5206\u6790\u4e86\u5c4f\u5e55\u7684Lambertian\u9636\u6570\uff0c\u5e76\u5728\u7279\u5b9a\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u4fe1\u9053\u7279\u6027\u5206\u6790\u3002", "motivation": "\u968f\u7740\u5149\u5b66\u76f8\u673a\u901a\u4fe1\uff08OCC\uff09\u7684\u53d1\u5c55\uff0c\u5c4f\u5e55\u4e0e\u6444\u50cf\u5934\u4e4b\u95f4\u7684\u901a\u4fe1\u6210\u4e3a\u53ef\u80fd\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u624b\u673a\u95f4\u53ef\u89c1\u5149\u901a\u4fe1\uff08S2SVLC\uff09\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u667a\u80fd\u624b\u673a\u5c4f\u5e55\u548c\u6444\u50cf\u5934\uff0c\u572820\u5398\u7c73\u7684\u94fe\u8def\u8303\u56f4\u5185\u5b9e\u9a8c\u6027\u5730\u6784\u5efaS2SVLC\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u5c4f\u5e55\u7684Lambertian\u9636\u6570\u548c\u4fe1\u9053\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u5b9e\u73b0\u4e86S2SVLC\u7cfb\u7edf\uff0c\u5e76\u83b7\u5f97\u4e86\u5c4f\u5e55\u7684Lambertian\u9636\u6570\u53ca\u4fe1\u9053\u7279\u6027\u7684\u76f8\u5173\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u624b\u673a\u95f4\u53ef\u89c1\u5149\u901a\u4fe1\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u57fa\u7840\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2506.22956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22956", "abs": "https://arxiv.org/abs/2506.22956", "authors": ["David Rodr\u00edguez-Mart\u00ednez", "Dave van der Meer", "Junlin Song", "Abishek Bera", "C. J. P\u00e9rez-del-Pulgar", "Miguel Angel Olivares-Mendez"], "title": "SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes", "comment": "10 pages, 8 figures, dataset", "summary": "Exploring high-latitude lunar regions presents an extremely challenging\nvisual environment for robots. The low sunlight elevation angle and minimal\nlight scattering result in a visual field dominated by a high dynamic range\nfeaturing long, dynamic shadows. Reproducing these conditions on Earth requires\nsophisticated simulators and specialized facilities. We introduce a unique\ndataset recorded at the LunaLab from the SnT - University of Luxembourg, an\nindoor test facility designed to replicate the optical characteristics of\nmultiple lunar latitudes. Our dataset includes images, inertial measurements,\nand wheel odometry data from robots navigating seven distinct trajectories\nunder multiple illumination scenarios, simulating high-latitude lunar\nconditions from dawn to night time with and without the aid of headlights,\nresulting in 88 distinct sequences containing a total of 1.3M images. Data was\ncaptured using a stereo RGB-inertial sensor, a monocular monochrome camera, and\nfor the first time, a novel single-photon avalanche diode (SPAD) camera. We\nrecorded both static and dynamic image sequences, with robots navigating at\nslow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized,\nand timestamped, providing a valuable resource for validating perception tasks\nfrom vision-based autonomous navigation to scientific imaging for future lunar\nmissions targeting high-latitude regions or those intended for robots operating\nacross perceptually degraded environments. The dataset can be downloaded from\nhttps://zenodo.org/records/13970078?preview=1, and a visual overview is\navailable at https://youtu.be/d7sPeO50_2I. All supplementary material can be\nfound at https://github.com/spaceuma/spice-hl3.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5728LunaLab\u8bbe\u65bd\u4e2d\u8bb0\u5f55\u7684\u9ad8\u7eac\u5ea6\u6708\u7403\u73af\u5883\u6a21\u62df\u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u50cf\u3001\u60ef\u6027\u6d4b\u91cf\u548c\u8f6e\u5f0f\u91cc\u7a0b\u6570\u636e\uff0c\u7528\u4e8e\u9a8c\u8bc1\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u9ad8\u7eac\u5ea6\u6708\u7403\u533a\u57df\u7684\u89c6\u89c9\u73af\u5883\u5bf9\u673a\u5668\u4eba\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u6a21\u62df\u4f4e\u592a\u9633\u9ad8\u5ea6\u89d2\u548c\u52a8\u6001\u9634\u5f71\u7684\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u4f20\u611f\u5668\uff08\u5305\u62ec\u65b0\u578bSPAD\u76f8\u673a\uff09\u8bb0\u5f55\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5149\u7167\u548c\u901f\u5ea6\u4e0b\u7684\u5bfc\u822a\u6570\u636e\u3002", "result": "\u751f\u6210\u4e8688\u4e2a\u5e8f\u5217\u51711.3M\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6570\u636e\u7ecf\u8fc7\u6821\u51c6\u548c\u540c\u6b65\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u672a\u6765\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u4efb\u52a1\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2506.22931", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.22931", "abs": "https://arxiv.org/abs/2506.22931", "authors": ["Moslem Uddin", "Huadong Mo", "Daoyi Dong"], "title": "Real-Time Energy Management Strategies for Community Microgrids", "comment": null, "summary": "This study presents a real-time energy management framework for hybrid\ncommunity microgrids integrating photovoltaic, wind, battery energy storage\nsystems, diesel generators, and grid interconnection. The proposed approach\nformulates the dispatch problem as a multi-objective optimization task that\naims to minimize operational costs. Two control strategies are proposed and\nevaluated: a conventional rule-based control (RBC) method and an advanced deep\nreinforcement learning (DRL) approach utilizing proximal policy optimization\n(PPO). A realistic case study based on Australian load and generation profiles\nis used to validate the framework. Simulation results demonstrate that DRL-PPO\nreduces operational costs by 18%, CO_2 emissions by 20%, and improves system\nreliability by 87.5% compared to RBC. Beside, DRL-PPO increases renewable\nenergy utilization by 13%, effectively reducing dependence on diesel generation\nand grid imports. These findings demonstrate the potential of DRL-based\napproaches to enable cost-effective and resilient microgrid operations,\nparticularly in regional and remote communities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u793e\u533a\u5fae\u7535\u7f51\u7684\u5b9e\u65f6\u80fd\u6e90\u7ba1\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u5149\u4f0f\u3001\u98ce\u80fd\u3001\u7535\u6c60\u50a8\u80fd\u3001\u67f4\u6cb9\u53d1\u7535\u673a\u548c\u7535\u7f51\u4e92\u8054\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u5e38\u89c4\u63a7\u5236\uff08RBC\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL-PPO\uff09\u4e24\u79cd\u7b56\u7565\uff0c\u7ed3\u679c\u663e\u793aDRL-PPO\u5728\u6210\u672c\u3001\u78b3\u6392\u653e\u548c\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u793e\u533a\u5fae\u7535\u7f51\u7684\u80fd\u6e90\u7ba1\u7406\u95ee\u9898\uff0c\u4f18\u5316\u8fd0\u8425\u6210\u672c\u5e76\u63d0\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u6bd4\u8f83\u57fa\u4e8e\u89c4\u5219\u7684\u5e38\u89c4\u63a7\u5236\uff08RBC\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL-PPO\uff09\u4e24\u79cd\u7b56\u7565\u3002", "result": "DRL-PPO\u6bd4RBC\u964d\u4f4e\u8fd0\u8425\u6210\u672c18%\u3001\u78b3\u6392\u653e20%\uff0c\u53ef\u9760\u6027\u63d0\u9ad887.5%\uff0c\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\u63d0\u534713%\u3002", "conclusion": "DRL-PPO\u65b9\u6cd5\u5728\u5fae\u7535\u7f51\u8fd0\u8425\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u504f\u8fdc\u5730\u533a\u3002"}}
{"id": "2506.22529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22529", "abs": "https://arxiv.org/abs/2506.22529", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5fb7\u8bedTelegram\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6570\u636e\u96c6Misinfo-TeleGraph\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0cGraphSAGE\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\u3002", "motivation": "Telegram\u7b49\u4f4e\u76d1\u7ba1\u5e73\u53f0\u6210\u4e3a\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u6e20\u9053\uff0c\u4f46\u8fde\u901a\u6027\u548c\u6d88\u606f\u4f20\u64ad\u4fe1\u606f\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u672a\u5145\u5206\u5229\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b500\u4e07\u6761\u6d88\u606f\u7684\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u4eba\u5de5\u6807\u6ce8\u751f\u6210\u6807\u7b7e\uff0c\u8bc4\u4f30\u6587\u672c\u6a21\u578b\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08\u5982GraphSAGE\uff09\u3002", "result": "GraphSAGE\u7ed3\u5408LSTM\u805a\u5408\u5728MCC\u548cF1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5fb7\u8bedTelegram\u7f51\u7edc\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u57fa\u51c6\u548c\u5f00\u653e\u6570\u636e\u96c6\u3002"}}
{"id": "2506.22920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22920", "abs": "https://arxiv.org/abs/2506.22920", "authors": ["Pinzheng Wang", "Juntao Li", "Zecheng Tang", "Haijia Gui", "Min zhang"], "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u73a9\u7684\u6279\u8bc4-\u8fa8\u522b\u6e38\u620f\uff08CDG\uff09\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u7406\u6027\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u771f\u6b63\u7406\u89e3\u3002", "method": "\u8bbe\u8ba1CDG\u6e38\u620f\uff0c\u6a21\u578b\u4f5c\u4e3a\u8bc1\u660e\u8005\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u968f\u540e\u63a5\u53d7\u6279\u8bc4\u8005\u7684\u6311\u6218\uff0c\u533a\u5206\u8bef\u5bfc\u4e0e\u5efa\u8bbe\u6027\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCDG\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u3001\u9519\u8bef\u68c0\u6d4b\u3001\u81ea\u6211\u4fee\u6b63\u548c\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002", "conclusion": "\u81ea\u73a9\u65b9\u6cd5CDG\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.22504", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22504", "abs": "https://arxiv.org/abs/2506.22504", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "comment": null, "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5Patch2Loc\uff0c\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4ece\u6b63\u5e38MRI\u56fe\u50cf\u4e2d\u5b66\u4e60\uff0c\u68c0\u6d4b\u5f02\u5e38\u8111\u7ec4\u7ec7\uff0c\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u3002", "motivation": "\u8111\u90e8\u75c5\u53d8\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\uff0c\u800cPatch2Loc\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u68c0\u6d4b\u5f02\u5e38\u3002", "method": "\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5c06MRI\u56fe\u50cf\u4e2d\u7684\u6b63\u5e38\u7ec4\u7ec7\u6620\u5c04\u5230\u5176\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u548c\u65b9\u5dee\u68c0\u6d4b\u5f02\u5e38\u7ec4\u7ec7\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86Patch2Loc\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "Patch2Loc\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u8111\u90e8\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23102", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23102", "abs": "https://arxiv.org/abs/2506.23102", "authors": ["Sunggu Kyung", "Jinyoung Seo", "Hyunseok Lim", "Dongyeong Kim", "Hyungbin Park", "Jimin Sung", "Jihyun Kim", "Wooyoung Jo", "Yoojin Nam", "Namkug Kim"], "title": "MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation", "comment": "14 pages, 5 figures, submitted to ICCV 2025", "summary": "The recent release of RadGenome-Chest CT has significantly advanced CT-based\nreport generation. However, existing methods primarily focus on global\nfeatures, making it challenging to capture region-specific details, which may\ncause certain abnormalities to go unnoticed. To address this, we propose\nMedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)\nframework, featuring three key innovations. First, we introduce Region\nRepresentative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained\nvision model to efficiently extract 3D CT features. This approach generates\nglobal tokens representing overall slice features and region tokens\nhighlighting target areas, enabling the MLLM to process comprehensive\ninformation effectively. Second, a universal segmentation model generates\npseudo-masks, which are then processed by a mask encoder to extract\nregion-centric features. This allows the MLLM to focus on clinically relevant\nregions, using six predefined region masks. Third, we leverage segmentation\nresults to extract patient-specific attributions, including organ size,\ndiameter, and locations. These are converted into text prompts, enriching the\nMLLM's understanding of patient-specific contexts. To ensure rigorous\nevaluation, we conducted benchmark experiments on report generation using the\nRadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,\noutperforming existing methods in natural language generation quality and\nclinical relevance while maintaining interpretability. The code for our\nframework is publicly available.", "AI": {"tldr": "MedRegion-CT\u662f\u4e00\u4e2a\u533a\u57df\u805a\u7126\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u4ee3\u8868\u4ee4\u724c\u6c60\u5316\u3001\u901a\u7528\u5206\u5272\u6a21\u578b\u548c\u60a3\u8005\u7279\u5b9a\u5c5e\u6027\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u62a5\u544a\u751f\u6210\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u81ea\u7136\u8bed\u8a00\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u7279\u5f81\uff0c\u96be\u4ee5\u6355\u6349\u533a\u57df\u7279\u5f02\u6027\u7ec6\u8282\uff0c\u53ef\u80fd\u5bfc\u81f4\u67d0\u4e9b\u5f02\u5e38\u88ab\u5ffd\u7565\u3002", "method": "1. \u5f15\u5165\u533a\u57df\u4ee3\u8868\u4ee4\u724c\u6c60\u5316\uff08$R^2$ Token Pooling\uff09\u63d0\u53d63D CT\u7279\u5f81\uff1b2. \u4f7f\u7528\u901a\u7528\u5206\u5272\u6a21\u578b\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u63d0\u53d6\u533a\u57df\u4e2d\u5fc3\u7279\u5f81\uff1b3. \u5229\u7528\u5206\u5272\u7ed3\u679c\u63d0\u53d6\u60a3\u8005\u7279\u5b9a\u5c5e\u6027\u5e76\u8f6c\u6362\u4e3a\u6587\u672c\u63d0\u793a\u3002", "result": "\u5728RadGenome-Chest CT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedRegion-CT\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d28\u91cf\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MedRegion-CT\u901a\u8fc7\u533a\u57df\u805a\u7126\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86CT\u62a5\u544a\u751f\u6210\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.23023", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23023", "abs": "https://arxiv.org/abs/2506.23023", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "comment": "6 pages, 10 figures, submitted to a conference", "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "AI": {"tldr": "\u63d0\u51fa\u4e86SAD-RL\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u548c\u573a\u666f\u5316\u73af\u5883\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u5b89\u5168\u8fd0\u884c\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u4e14\u5b66\u4e60\u6548\u7387\u4f4e\u3002", "method": "SAD-RL\u6846\u67b6\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08\u9ad8\u5c42\u7b56\u7565\u9009\u62e9\u52a8\u4f5c\u6a21\u677f\uff0c\u4f4e\u5c42\u903b\u8f91\u6267\u884c\uff09\u548c\u573a\u666f\u5316\u73af\u5883\uff08\u63a7\u5236\u8bad\u7ec3\u7ecf\u9a8c\u5e76\u5f15\u5165\u6311\u6218\u6027\u573a\u666f\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAD-RL\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u80fd\u5728\u7b80\u5355\u548c\u6311\u6218\u6027\u573a\u666f\u4e2d\u9ad8\u6548\u5b9e\u73b0\u5b89\u5168\u884c\u4e3a\uff0c\u5206\u5c42\u7b56\u7565\u548c\u573a\u666f\u591a\u6837\u6027\u662f\u5173\u952e\u3002", "conclusion": "SAD-RL\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u548c\u573a\u666f\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2506.22971", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.22971", "abs": "https://arxiv.org/abs/2506.22971", "authors": ["Kesav Kazam Ramachandran Anantharaman", "Rahul Meshram"], "title": "Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems", "comment": "6 pages, 2 figures", "summary": "This paper presents a two-timescale hierarchical decentralized architecture\nfor control of Cyber-Physical Systems. The architecture consists of $N$\nindependent sub-processes, a global controller, and $N$ local controllers, each\nformulated as a Markov Decision Process (MDP). The global controller, operating\nat a slower timescale optimizes the infinite-horizon discounted cumulative\nreward under budget constraints. For the local controllers, operating at a\nfaster timescale, we propose two different optimization frameworks, namely the\nCOpt and FOpt. In the COpt framework, the local controller also optimizes an\ninfinite-horizon MDP, while in the FOpt framework, the local controller\noptimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,\nwhere the local controllers have more autonomy in their decision making. First,\nthe existence of stationary deterministic optimal policies for both these\nframeworks is established. Then, various relationships between the two\nframeworks are studied, including a bound on the difference between the two\noptimal value functions. Additionally, sufficiency conditions are provided such\nthat the two frameworks lead to the same optimal values.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u65f6\u95f4\u5c3a\u5ea6\u7684\u5206\u5c42\u5206\u6563\u63a7\u5236\u67b6\u6784\uff0c\u7528\u4e8e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u7684\u63a7\u5236\uff0c\u5305\u62ec\u5168\u5c40\u63a7\u5236\u5668\u548c\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u5206\u522b\u91c7\u7528\u65e0\u9650\u548c\u6709\u9650\u65f6\u95f4\u8303\u56f4\u7684MDP\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u591a\u65f6\u95f4\u5c3a\u5ea6\u63a7\u5236\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u7075\u6d3b\u6027\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u5168\u5c40\u63a7\u5236\u5668\u4f18\u5316\u65e0\u9650\u65f6\u95f4\u8303\u56f4MDP\uff0c\u5c40\u90e8\u63a7\u5236\u5668\u91c7\u7528COpt\uff08\u65e0\u9650\u65f6\u95f4\u8303\u56f4\uff09\u548cFOpt\uff08\u6709\u9650\u65f6\u95f4\u8303\u56f4\uff09\u4e24\u79cd\u4f18\u5316\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u79cd\u6846\u67b6\u4e0b\u5b58\u5728\u786e\u5b9a\u6027\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5305\u62ec\u6700\u4f18\u503c\u51fd\u6570\u5dee\u5f02\u7684\u754c\u9650\u3002", "conclusion": "FOpt\u6846\u67b6\u8d4b\u4e88\u5c40\u90e8\u63a7\u5236\u5668\u66f4\u591a\u81ea\u4e3b\u6743\uff0c\u4e14\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e24\u79cd\u6846\u67b6\u7684\u6700\u4f18\u503c\u76f8\u540c\u3002"}}
{"id": "2506.22598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22598", "abs": "https://arxiv.org/abs/2506.22598", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RExBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5b9e\u73b0\u7814\u7a76\u6269\u5c55\u4efb\u52a1\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u4ee3\u7406\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u6307\u5bfc\u3002", "motivation": "\u7814\u7a76\u6269\u5c55\u80fd\u529b\u662fLLM\u4ee3\u7406\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u4ee3\u7406\u5728\u6b64\u65b9\u9762\u7684\u8868\u73b0\u5c1a\u672a\u88ab\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86RExBench\u57fa\u51c6\uff0c\u5305\u542b12\u4e2a\u672a\u5b9e\u73b0\u7684\u7814\u7a76\u5047\u8bbe\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e869\u4e2a\u57fa\u4e8e\u4e0d\u540c\u6846\u67b6\u7684LLM\u4ee3\u7406\u3002", "result": "\u6240\u6709\u4ee3\u7406\u5728\u81ea\u4e3b\u5b9e\u73b0\u6269\u5c55\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4f4e\u4e8e40%\uff0c\u5373\u4f7f\u6709\u4eba\u5de5\u63d0\u793a\u3002", "conclusion": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u65e0\u5927\u91cf\u4eba\u5de5\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u96be\u4ee5\u80dc\u4efb\u73b0\u5b9e\u7814\u7a76\u6269\u5c55\u4efb\u52a1\u3002"}}
{"id": "2506.22992", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22992", "abs": "https://arxiv.org/abs/2506.22992", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbi\u0107", "Michael Moor"], "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "AI": {"tldr": "MARBLE\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u591a\u6a21\u6001\u95ee\u9898\u4e2d\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u591a\u6a21\u6001\u63a8\u7406\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u591a\u5c40\u9650\u4e8e\u6587\u672c\u6216\u7b80\u5355\u591a\u6a21\u6001\u95ee\u9898\uff0c\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "MARBLE\u5305\u542b\u4e24\u4e2a\u9ad8\u96be\u5ea6\u4efb\u52a1\uff08M-Portal\u548cM-Cube\uff09\uff0c\u8981\u6c42\u6a21\u578b\u5728\u7a7a\u95f4\u3001\u89c6\u89c9\u548c\u7269\u7406\u7ea6\u675f\u4e0b\u5236\u5b9a\u548c\u7406\u89e3\u591a\u6b65\u8ba1\u5212\u3002", "result": "12\u4e2a\u5148\u8fdb\u6a21\u578b\u5728MARBLE\u4e0a\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0cM-Cube\u4efb\u52a1\u51c6\u786e\u7387\u4e3a0%\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "MARBLE\u63ed\u793a\u4e86MLLMs\u7684\u5c40\u9650\u6027\uff0c\u5e0c\u671b\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5177\u5907\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2506.22505", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22505", "abs": "https://arxiv.org/abs/2506.22505", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "comment": null, "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f31\u76d1\u7763\uff08\u56fe\u50cf\u7ea7\u6807\u7b7e\uff09\u8bad\u7ec3\u4e8c\u503c\u5bf9\u8c61\u5206\u5272\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u80cc\u666f\u56fe\u50cf\u63d0\u5347\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4e13\u4e1a\u56fe\u50cf\u9886\u57df\uff08\u5982\u58f0\u7eb3\u3001\u9065\u611f\u3001\u751f\u7269\u533b\u5b66\u56fe\u50cf\uff09\uff0c\u50cf\u7d20\u7ea7\u5206\u5272\u6210\u672c\u9ad8\uff0c\u800c\u56fe\u50cf\u7ea7\u6807\u7b7e\u66f4\u6613\u83b7\u53d6\u3002", "method": "\u5229\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u8bad\u7ec3\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u805a\u7c7b\u80cc\u666f\u56fe\u50cf\u5e76\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\uff0c\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u548c\u76d1\u7763\u635f\u5931\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u58f0\u7eb3\u56fe\u50cf\u548c\u81ea\u7136\u56fe\u50cf\u4e0a\u5747\u4f18\u4e8e\u65e0\u76d1\u7763\u5206\u5272\u57fa\u7ebf\uff0c\u4e14\u65e0\u9700\u9884\u8bad\u7ec3\u7f51\u7edc\u6216\u5bf9\u6297\u6027\u5224\u522b\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5bf9\u8c61\u5206\u5272\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u9886\u57df\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002"}}
{"id": "2506.23121", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23121", "abs": "https://arxiv.org/abs/2506.23121", "authors": ["Xinlei Yu", "Chanmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "comment": "19 pages, 9 figures, 10 tables", "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.", "AI": {"tldr": "CRISP-SAM2\u662f\u4e00\u79cd\u57fa\u4e8eSAM2\u7684\u591a\u5668\u5b98\u533b\u5b66\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u8bed\u4e49\u63d0\u793a\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u7ec6\u8282\u4e0d\u51c6\u786e\u3001\u4f9d\u8d56\u51e0\u4f55\u63d0\u793a\u548c\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u5668\u5b98\u5206\u5272\u6a21\u578b\u5b58\u5728\u7ec6\u8282\u4e0d\u51c6\u786e\u3001\u4f9d\u8d56\u51e0\u4f55\u63d0\u793a\u548c\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u7b49\u95ee\u9898\uff0cCRISP-SAM2\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u8bed\u4e49\u8f6c\u6362\u3001\u8bed\u4e49\u63d0\u793a\u7b56\u7565\u3001\u8bb0\u5fc6\u81ea\u66f4\u65b0\u548c\u63a9\u7801\u7ec6\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRISP-SAM2\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "CRISP-SAM2\u5728\u89e3\u51b3\u591a\u5668\u5b98\u5206\u5272\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9488\u5bf9\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.23078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23078", "abs": "https://arxiv.org/abs/2506.23078", "authors": ["Zhaoxing Zhang", "Xiaoxiang Wang", "Chengliang Zhang", "Yangyang Guo", "Zikang Yuan", "Xin Yang"], "title": "Event-based Stereo Visual-Inertial Odometry with Voxel Map", "comment": null, "summary": "The event camera, renowned for its high dynamic range and exceptional\ntemporal resolution, is recognized as an important sensor for visual odometry.\nHowever, the inherent noise in event streams complicates the selection of\nhigh-quality map points, which critically determine the precision of state\nestimation. To address this challenge, we propose Voxel-ESVIO, an event-based\nstereo visual-inertial odometry system that utilizes voxel map management,\nwhich efficiently filter out high-quality 3D points. Specifically, our\nmethodology utilizes voxel-based point selection and voxel-aware point\nmanagement to collectively optimize the selection and updating of map points on\na per-voxel basis. These synergistic strategies enable the efficient retrieval\nof noise-resilient map points with the highest observation likelihood in\ncurrent frames, thereby ensureing the state estimation accuracy. Extensive\nevaluations on three public benchmarks demonstrate that our Voxel-ESVIO\noutperforms state-of-the-art methods in both accuracy and computational\nefficiency.", "AI": {"tldr": "Voxel-ESVIO\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u9ad8\u6548\u7b5b\u9009\u9ad8\u8d28\u91cf3D\u70b9\uff0c\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4f7f\u5176\u6210\u4e3a\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u91cd\u8981\u4f20\u611f\u5668\uff0c\u4f46\u4e8b\u4ef6\u6d41\u4e2d\u7684\u566a\u58f0\u5f71\u54cd\u4e86\u9ad8\u8d28\u91cf\u5730\u56fe\u70b9\u7684\u9009\u62e9\uff0c\u4ece\u800c\u5f71\u54cd\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u70b9\u9009\u62e9\u548c\u4f53\u7d20\u611f\u77e5\u7684\u70b9\u7ba1\u7406\uff0c\u534f\u540c\u4f18\u5316\u5730\u56fe\u70b9\u7684\u9009\u62e9\u548c\u66f4\u65b0\uff0c\u9ad8\u6548\u63d0\u53d6\u6297\u566a\u58f0\u7684\u9ad8\u8d28\u91cf\u5730\u56fe\u70b9\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVoxel-ESVIO\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Voxel-ESVIO\u901a\u8fc7\u4f53\u7d20\u5730\u56fe\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6d41\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2506.23169", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23169", "abs": "https://arxiv.org/abs/2506.23169", "authors": ["Kai Kang", "Feng Liu", "Yifan Su", "Zhaojian Wang"], "title": "Extreme Scenario Characterization for High Renewable Energy Penetrated Power Systems over Long Time Scales", "comment": "Accepted for publication in 2025 IEEE Power & Energy Society General\n  Meeting", "summary": "Power systems with high renewable energy penetration are highly influenced by\nweather conditions, often facing significant challenges such as persistent\npower shortages and severe power fluctuations over long time scales. This paper\naddresses the critical need for effective characterization of extreme scenarios\nunder these situations. First, novel risk indices are proposed to quantify the\nseverity of continuous power shortages and substantial power fluctuations over\nlong-term operations. These indices are independent of specific scheduling\nstrategies and incorporate the system's resource regulation capabilities. By\nemploying a filtering-based approach, the proposed indices focus on retaining\nkey characteristics of continuous power shortages and fluctuation events,\nenabling the identification of extreme scenarios on long time scales. Secondly,\nan extreme scenario generation method is developed using Gaussian mixture\nmodels and sequential Monte Carlo simulation. Especially, this method\nperiodically evaluates the severity of generated scenarios based on the defined\nrisk indices, retaining extreme scenarios while discarding less critical ones.\nFinally, case studies based on real-world data demonstrate the efficacy of the\nproposed method. The results confirm that integrating the identified extreme\nscenarios significantly enhances the system's ability to ensure long-term\nsecurity and reliability under high renewable energy penetration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u4e0b\u7535\u529b\u7cfb\u7edf\u6781\u7aef\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u98ce\u9669\u6307\u6807\u548c\u751f\u6210\u6781\u7aef\u573a\u666f\u7684\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u7cfb\u7edf\u957f\u671f\u5b89\u5168\u6027\u7684\u63d0\u5347\u3002", "motivation": "\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7684\u7535\u529b\u7cfb\u7edf\u53d7\u5929\u6c14\u5f71\u54cd\u5927\uff0c\u9762\u4e34\u957f\u671f\u7535\u529b\u77ed\u7f3a\u548c\u6ce2\u52a8\u95ee\u9898\uff0c\u9700\u6709\u6548\u8868\u5f81\u6781\u7aef\u573a\u666f\u3002", "method": "\u63d0\u51fa\u72ec\u7acb\u4e8e\u8c03\u5ea6\u7b56\u7565\u7684\u98ce\u9669\u6307\u6807\uff0c\u91c7\u7528\u57fa\u4e8e\u8fc7\u6ee4\u7684\u65b9\u6cd5\u548cGMM\u4e0e\u8499\u7279\u5361\u6d1b\u6a21\u62df\u751f\u6210\u6781\u7aef\u573a\u666f\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8bc6\u522b\u5e76\u6574\u5408\u6781\u7aef\u573a\u666f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u957f\u671f\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u652f\u6301\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u7535\u529b\u7cfb\u7edf\u7684\u6781\u7aef\u573a\u666f\u5206\u6790\u4e0e\u5e94\u5bf9\u3002"}}
{"id": "2506.22623", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22623", "abs": "https://arxiv.org/abs/2506.22623", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5408\u6210\u6587\u672c\uff0c\u4ee5\u786e\u4fddLLMs\u5728\u6587\u672c\u751f\u6210\u4e2d\u7684\u4f26\u7406\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9c81\u68d2\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6f5c\u5728\u6ee5\u7528\u95ee\u9898\u5f15\u53d1\u5173\u6ce8\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6c34\u5370\u6280\u672f\u8bc6\u522b\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u786e\u4fdd\u4f26\u7406\u4f7f\u7528\u3002", "method": "\u9996\u5148\u590d\u73b0\u57fa\u7ebf\u7814\u7a76\u7ed3\u679c\uff0c\u63ed\u793a\u5176\u5bf9\u751f\u6210\u6a21\u578b\u53d8\u5316\u7684\u654f\u611f\u6027\uff1b\u968f\u540e\u63d0\u51fa\u521b\u65b0\u6c34\u5370\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6539\u5199\u751f\u6210\u6587\u672c\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6c34\u5370\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982aarson\uff09\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5408\u6210\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8eLLMs\u7684\u4f26\u7406\u5e94\u7528\u3002"}}
{"id": "2506.23049", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "pdf": "https://arxiv.org/pdf/2506.23049", "abs": "https://arxiv.org/abs/2506.23049", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "comment": null, "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "AI": {"tldr": "AURA\u662f\u9996\u4e2a\u5f00\u6e90\u7684\u8bed\u97f3\u539f\u751f\u52a9\u624b\uff0c\u652f\u6301\u52a8\u6001\u5de5\u5177\u8c03\u7528\u548c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u548c\u8bed\u97f3\u6280\u672f\u6709\u8fdb\u6b65\uff0c\u4f46\u7f3a\u4e4f\u5f00\u6e90\u7cfb\u7edf\u652f\u6301\u8bed\u97f3\u5230\u8bed\u97f3\u7684\u591a\u8f6e\u5bf9\u8bdd\u4e0e\u5de5\u5177\u96c6\u6210\u3002", "method": "AURA\u7ed3\u5408\u5f00\u653e\u6743\u91cd\u7684ASR\u3001TTS\u548cLLM\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u5de5\u5177\u96c6\u6210\u3002", "result": "\u5728VoiceBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cOpenBookQA\u5f97\u520692.75%\uff0c\u63a5\u8fd1GPT-4o\uff1b\u4eba\u7c7b\u8bc4\u4f30\u4efb\u52a1\u6210\u529f\u7387\u8fbe90%\u3002", "conclusion": "AURA\u5c55\u793a\u4e86\u5f00\u6e90\u8bed\u97f3\u52a9\u624b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.22509", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22509", "abs": "https://arxiv.org/abs/2506.22509", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "comment": "ICCV2025", "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff08DNA\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u7edf\u8ba1\u91cf\uff0c\u63d0\u5347\u5bc6\u96c6\u9884\u6d4b\u6a21\u578b\u5728\u672a\u89c1\u57df\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5efa\u6a21\u5305\u542b\u57df\u4fe1\u606f\u7684\u5206\u5e03\u8f6c\u6362\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u566a\u58f0\u7edf\u8ba1\u504f\u5dee\u4f1a\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u566a\u58f0\u7edf\u8ba1\u91cf\u5b9e\u73b0\u57df\u9002\u5e94\u3002", "method": "\u63d0\u51faDomain Noise Alignment\uff08DNA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u566a\u58f0\u7edf\u8ba1\u91cf\u5b9e\u73b0\u57df\u9002\u5e94\uff1b\u5728\u65e0\u6e90\u57df\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u9ad8\u7f6e\u4fe1\u533a\u57df\u7684\u7edf\u8ba1\u91cf\u9010\u6b65\u8c03\u6574\u566a\u58f0\u3002", "result": "\u5728\u56db\u79cd\u5e38\u89c1\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86DNA\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u57df\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "DNA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u7edf\u8ba1\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u5bc6\u96c6\u9884\u6d4b\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd\u3002"}}
{"id": "2506.23184", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23184", "abs": "https://arxiv.org/abs/2506.23184", "authors": ["Anran Liu", "Xiaofei Wang", "Jing Cai", "Chao Li"], "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining", "comment": "11 pages, 3 figures", "summary": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\nprovides protein-targeted staining but is restricted by tissue availability and\nantibody specificity. Virtual staining, i.e., computationally translating the\nH&E image to its IHC counterpart while preserving the tissue structure, is\npromising for efficient IHC generation. Existing virtual staining methods still\nface key challenges: 1) effective decomposition of staining style and tissue\nstructure, 2) controllable staining process adaptable to diverse tissue and\nproteins, and 3) rigorous structural consistency modelling to handle the\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\nstaining. Specifically, we design 1) a global MI-guided energy function that\ndisentangles the tissue structure and staining characteristics across\nmodalities, 2) a novel timestep-customized reverse diffusion process for\nprecise control of the staining intensity and structural reconstruction, and 3)\na local MI-driven contrastive learning strategy to ensure the cellular level\nstructural consistency between H&E-IHC images. Extensive experiments\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\nits biomedical potential. Codes will be open-sourced upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4eceH&E\u56fe\u50cf\u865a\u62df\u751f\u6210IHC\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u89e3\u67d3\u8272\u98ce\u683c\u4e0e\u7ec4\u7ec7\u7ed3\u6784\u3001\u53ef\u63a7\u67d3\u8272\u8fc7\u7a0b\u53ca\u7ed3\u6784\u4e00\u81f4\u6027\u5efa\u6a21\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "H&E\u67d3\u8272\u7f3a\u4e4f\u7279\u5f02\u6027\u6807\u8bb0\uff0c\u800cIHC\u67d3\u8272\u53d7\u9650\u4e8e\u7ec4\u7ec7\u53ef\u7528\u6027\u548c\u6297\u4f53\u7279\u5f02\u6027\u3002\u865a\u62df\u67d3\u8272\u6280\u672f\u6709\u671b\u9ad8\u6548\u751f\u6210IHC\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u89e3\u67d3\u8272\u98ce\u683c\u4e0e\u7ec4\u7ec7\u7ed3\u6784\u3001\u53ef\u63a7\u67d3\u8272\u53ca\u7ed3\u6784\u4e00\u81f4\u6027\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e92\u4fe1\u606f\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u5305\u62ec\u5168\u5c40\u4e92\u4fe1\u606f\u80fd\u91cf\u51fd\u6570\u3001\u65f6\u95f4\u6b65\u5b9a\u5236\u7684\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u53ca\u5c40\u90e8\u4e92\u4fe1\u606f\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u5206\u89e3\u67d3\u8272\u98ce\u683c\u4e0e\u7ec4\u7ec7\u7ed3\u6784\u3001\u7cbe\u786e\u63a7\u5236\u67d3\u8272\u5f3a\u5ea6\u5e76\u786e\u4fdd\u7ec6\u80de\u7ea7\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u865a\u62df\u67d3\u8272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c55\u73b0\u4e86\u5176\u751f\u7269\u533b\u5b66\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4e92\u4fe1\u606f\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u5728\u865a\u62df\u67d3\u8272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.23114", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23114", "abs": "https://arxiv.org/abs/2506.23114", "authors": ["Zhanxiang Cao", "Buqing Nie", "Yang Zhang", "Yue Gao"], "title": "Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications", "comment": "8 pages,6 figures, IROS2025", "summary": "Recent advancements in quadruped robot research have significantly improved\ntheir ability to traverse complex and unstructured outdoor environments.\nHowever, the issue of noise generated during locomotion is generally\noverlooked, which is critically important in noise-sensitive indoor\nenvironments, such as service and healthcare settings, where maintaining low\nnoise levels is essential. This study aims to optimize the acoustic noise\ngenerated by quadruped robots during locomotion through the development of\nadvanced motion control algorithms. To achieve this, we propose a novel\napproach that minimizes noise emissions by integrating optimized gait design\nwith tailored control strategies. This method achieves an average noise\nreduction of approximately 8 dBA during movement, thereby enhancing the\nsuitability of quadruped robots for deployment in noise-sensitive indoor\nenvironments. Experimental results demonstrate the effectiveness of this\napproach across various indoor settings, highlighting the potential of\nquadruped robots for quiet operation in noise-sensitive environments.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u6b65\u6001\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\uff0c\u964d\u4f4e\u56db\u8db3\u673a\u5668\u4eba\u5728\u5ba4\u5185\u566a\u58f0\u654f\u611f\u73af\u5883\u4e2d\u7684\u566a\u97f3\u7ea68 dBA\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u80fd\u529b\u5df2\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u5728\u566a\u58f0\u654f\u611f\u5ba4\u5185\u73af\u5883\uff08\u5982\u670d\u52a1\u548c\u533b\u7597\u573a\u666f\uff09\u4e2d\u7684\u566a\u97f3\u95ee\u9898\u88ab\u5ffd\u89c6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4f18\u5316\u6b65\u6001\u8bbe\u8ba1\u548c\u5b9a\u5236\u63a7\u5236\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u566a\u97f3\u6392\u653e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u964d\u4f4e\u566a\u97f3\u7ea68 dBA\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5ba4\u5185\u73af\u5883\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u566a\u58f0\u654f\u611f\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b89\u9759\u64cd\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23204", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23204", "abs": "https://arxiv.org/abs/2506.23204", "authors": ["Umair Zulfiqar"], "title": "Data-driven Implementations of Various Generalizations of Balanced Truncation", "comment": null, "summary": "There exist two main frameworks for non-intrusive implementations of\napproximate balanced truncation: the quadrature-based framework and the\nADI-based framework. Both approaches rely solely on samples of the transfer\nfunction to construct truncated balanced models, eliminating the need for\naccess to the original model's state-space realization. Recently, the\nquadrature-based framework has been extended to various generalizations of\nbalanced truncation, including positive-real balanced truncation, bounded-real\nbalanced truncation, and balanced stochastic truncation. While this extension\nis theoretically nonintrusive-meaning it does not require the original\nstate-space realization-it depends on samples of spectral factorizations of the\ntransfer function. Since practical methods for obtaining such samples are\ncurrently unavailable, this extension remains largely a theoretical\ncontribution. In this work, we present a non-intrusive ADI-type framework for\nthese generalized balanced truncation methods that requires only samples of the\noriginal transfer function for implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u7684ADI\u6846\u67b6\uff0c\u7528\u4e8e\u5e7f\u4e49\u5e73\u8861\u622a\u65ad\u65b9\u6cd5\uff0c\u4ec5\u9700\u539f\u59cb\u4f20\u9012\u51fd\u6570\u7684\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u975e\u4fb5\u5165\u5f0f\u5e73\u8861\u622a\u65ad\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6b63\u4ea4\u7684\u65b9\u6cd5\uff09\u867d\u7136\u7406\u8bba\u4e0a\u53ef\u884c\uff0c\u4f46\u4f9d\u8d56\u4e8e\u4f20\u9012\u51fd\u6570\u8c31\u5206\u89e3\u7684\u6837\u672c\uff0c\u800c\u5b9e\u9645\u83b7\u53d6\u8fd9\u4e9b\u6837\u672c\u7684\u65b9\u6cd5\u5c1a\u4e0d\u53ef\u884c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u9700\u539f\u59cb\u4f20\u9012\u51fd\u6570\u6837\u672c\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u975e\u4fb5\u5165\u5f0f\u7684ADI\u6846\u67b6\uff0c\u4ec5\u9700\u539f\u59cb\u4f20\u9012\u51fd\u6570\u7684\u6837\u672c\uff0c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u6a21\u578b\u7684\u72b6\u6001\u7a7a\u95f4\u5b9e\u73b0\u6216\u8c31\u5206\u89e3\u6837\u672c\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e7f\u4e49\u5e73\u8861\u622a\u65ad\uff08\u5982\u6b63\u5b9e\u5e73\u8861\u622a\u65ad\u3001\u6709\u754c\u5b9e\u5e73\u8861\u622a\u65ad\u548c\u5e73\u8861\u968f\u673a\u622a\u65ad\uff09\uff0c\u4e14\u4ec5\u9700\u539f\u59cb\u4f20\u9012\u51fd\u6570\u7684\u6837\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u4fb5\u5165\u5f0f\u5e7f\u4e49\u5e73\u8861\u622a\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5b9e\u73b0\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8c31\u5206\u89e3\u6837\u672c\u7684\u95ee\u9898\u3002"}}
{"id": "2506.22644", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22644", "abs": "https://arxiv.org/abs/2506.22644", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a00\u758f\uff08BM25\uff09\u548c\u5bc6\u96c6\uff08E5\uff09\u68c0\u7d22\u65b9\u6cd5\u7684\u6df7\u5408RAG\u7cfb\u7edf\uff0c\u4f7f\u7528Falcon3-10B-Instruct\u751f\u6210\u7b54\u6848\uff0c\u5e76\u901a\u8fc7RankLLaMA\u91cd\u65b0\u6392\u5e8f\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u6d4b\u8bd5\u96c6\u4e0a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u6027\u80fd\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5fe0\u5b9e\u6027\u548c\u6b63\u786e\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408BM25\u548cE5\u68c0\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528Falcon3-10B-Instruct\u751f\u6210\u7b54\u6848\uff0c\u5e76\u901a\u8fc7RankLLaMA\u8fdb\u884c\u795e\u7ecf\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u795e\u7ecf\u91cd\u65b0\u6392\u5e8f\u5c06MAP\u4ece0.523\u63d0\u5347\u81f30.797\uff0852%\u76f8\u5bf9\u63d0\u5347\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\uff0884\u79d2vs1.74\u79d2/\u95ee\u9898\uff09\u3002\u6df7\u5408\u7cfb\u7edf\u5728\u5fe0\u5b9e\u6027\u6392\u540d\u7b2c4\uff0c\u6b63\u786e\u6027\u6392\u540d\u7b2c11\u3002", "conclusion": "\u8bcd\u6c47\u5bf9\u9f50\u662f\u6027\u80fd\u7684\u5173\u952e\u9884\u6d4b\u56e0\u7d20\uff0c\u4f46\u9700\u6743\u8861\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.23080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23080", "abs": "https://arxiv.org/abs/2506.23080", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "comment": null, "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u8ba4\u4e3a\u5176\u8f68\u8ff9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u6280\u672f\u7684\u5386\u53f2\u8fdb\u5c55\u76f8\u4f3c\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u8def\u5f84\uff0c\u63d0\u4f9b\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u7cfb\u7edf\u6a21\u578b\uff0c\u89e3\u91ca\u5176\u8fc7\u53bb\u7684\u7ed3\u6784\u53d8\u5316\u5e76\u6307\u5bfc\u672a\u6765\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u7c7b\u6bd4\u4eba\u7c7b\u8ba4\u77e5\u6280\u672f\u7684\u91cc\u7a0b\u7891\uff08\u5982\u6954\u5f62\u6587\u5b57\u3001\u5b57\u6bcd\u3001\u903b\u8f91\u7b49\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u201c\u8ba4\u77e5\u51e0\u4f55\u201d\u6846\u67b6\uff0c\u5206\u6790AI\u7684\u8fdb\u5316\u9636\u6bb5\u3002", "result": "AI\u7684\u8fdb\u5316\u4e0d\u4ec5\u662f\u7ebf\u6027\u7684\uff0c\u8fd8\u5177\u6709\u81ea\u53cd\u6027\uff0c\u5f53\u524d\u6b63\u8fdb\u5165\u201c\u5143\u8bed\u8a00\u65f6\u523b\u201d\uff0c\u672a\u6765\u5c06\u8fc8\u5411\u201c\u6570\u5b66\u7b26\u53f7\u65f6\u523b\u201d\u548c\u201c\u5f62\u5f0f\u903b\u8f91\u7cfb\u7edf\u65f6\u523b\u201d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765AI\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b56\u7565\uff0c\u662f\u4f5c\u8005\u4e09\u90e8\u66f2\u7684\u65b9\u6cd5\u8bba\u603b\u7ed3\u3002"}}
{"id": "2506.22511", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22511", "abs": "https://arxiv.org/abs/2506.22511", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "title": "Lightning the Night with Generative Artificial Intelligence", "comment": null, "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8eFY4B\u536b\u661f\u7684\u591a\u6ce2\u6bb5\u70ed\u7ea2\u5916\u6570\u636e\uff0c\u5f00\u53d1\u4e86RefDiff\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u7684\u9ad8\u7cbe\u5ea6\u53cd\u6f14\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4e91\u7ed3\u6784\u533a\u57df\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u591c\u95f4\u56e0\u7f3a\u4e4f\u53ef\u89c1\u5149\u800c\u65e0\u6cd5\u8fdb\u884c\u5168\u5929\u5019\u6c14\u8c61\u89c2\u6d4b\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eFY4B\u536b\u661f\u7684AGRI\u591a\u6ce2\u6bb5\u70ed\u7ea2\u5916\u6570\u636e\uff0c\u5f00\u53d1\u751f\u6210\u6269\u6563\u6a21\u578bRefDiff\uff0c\u5b9e\u73b0\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u53cd\u6f14\u3002", "result": "RefDiff\u7684SSIM\u6307\u6570\u8fbe0.90\uff0c\u5728\u590d\u6742\u4e91\u7ed3\u6784\u533a\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u591c\u95f4\u53cd\u6f14\u80fd\u529b\u4e0e\u767d\u5929\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u591c\u95f4\u53ef\u89c1\u5149\u53cd\u5c04\u7387\u53cd\u6f14\u80fd\u529b\uff0c\u62d3\u5c55\u4e86\u591c\u95f4\u53ef\u89c1\u5149\u6570\u636e\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.23208", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23208", "abs": "https://arxiv.org/abs/2506.23208", "authors": ["Runtian Yuan", "Qingqiu Li", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "title": "Multi-Source COVID-19 Detection via Variance Risk Extrapolation", "comment": null, "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich aims to classify chest CT scans into COVID and Non-COVID categories\nacross data collected from four distinct hospitals and medical centers. A major\nchallenge in this task lies in the domain shift caused by variations in imaging\nprotocols, scanners, and patient populations across institutions. To enhance\nthe cross-domain generalization of our model, we incorporate Variance Risk\nExtrapolation (VREx) into the training process. VREx encourages the model to\nmaintain consistent performance across multiple source domains by explicitly\nminimizing the variance of empirical risks across environments. This\nregularization strategy reduces overfitting to center-specific features and\npromotes learning of domain-invariant representations. We further apply Mixup\ndata augmentation to improve generalization and robustness. Mixup interpolates\nboth the inputs and labels of randomly selected pairs of training samples,\nencouraging the model to behave linearly between examples and enhancing its\nresilience to noise and limited data. Our method achieves an average macro F1\nscore of 0.96 across the four sources on the validation set, demonstrating\nstrong generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408VREx\u548cMixup\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6e90COVID-19\u68c0\u6d4b\u4efb\u52a1\uff0c\u89e3\u51b3\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u9ad8F1\u5206\u6570\u3002", "motivation": "\u591a\u6e90CT\u626b\u63cf\u6570\u636e\u56e0\u6210\u50cf\u534f\u8bae\u3001\u626b\u63cf\u4eea\u548c\u60a3\u8005\u7fa4\u4f53\u5dee\u5f02\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u9700\u63d0\u5347\u6a21\u578b\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408VREx\uff08\u6700\u5c0f\u5316\u8de8\u57df\u98ce\u9669\u65b9\u5dee\uff09\u548cMixup\u6570\u636e\u589e\u5f3a\uff08\u7ebf\u6027\u63d2\u503c\u8f93\u5165\u548c\u6807\u7b7e\uff09\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u6e90\u4e0a\u5e73\u5747\u5b8fF1\u5206\u6570\u8fbe0.96\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VREx\u548cMixup\u7684\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8de8\u57df\u6027\u80fd\u3002"}}
{"id": "2506.23125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23125", "abs": "https://arxiv.org/abs/2506.23125", "authors": ["Zhanxiang Cao", "Yang Zhang", "Buqing Nie", "Huangxuan Lin", "Haoyang Li", "Yue Gao"], "title": "Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots", "comment": "8 pages, 8 figures", "summary": "Learning policies for complex humanoid tasks remains both challenging and\ncompelling. Inspired by how infants and athletes rely on external support--such\nas parental walkers or coach-applied guidance--to acquire skills like walking,\ndancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive\nCurriculum Force for humanoid motion learning. A2CF trains a dual-agent system,\nin which a dedicated assistive force agent applies state-dependent forces to\nguide the robot through difficult initial motions and gradually reduces\nassistance as the robot's proficiency improves. Across three\nbenchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves\nconvergence 30% faster than baseline methods, lowers failure rates by over 40%,\nand ultimately produces robust, support-free policies. Real-world experiments\nfurther demonstrate that adaptively applied assistive forces significantly\naccelerate the acquisition of complex skills in high-dimensional robotic\ncontrol.", "AI": {"tldr": "A2CF\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8f85\u52a9\u529b\u52a0\u901f\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u52a8\u4f5c\u5b66\u4e60\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb30%\uff0c\u5931\u8d25\u7387\u964d\u4f4e40%\u3002", "motivation": "\u53d7\u5a74\u513f\u548c\u8fd0\u52a8\u5458\u4f9d\u8d56\u5916\u90e8\u652f\u6301\u5b66\u4e60\u590d\u6742\u52a8\u4f5c\u7684\u542f\u53d1\uff0c\u63d0\u51faA2CF\u65b9\u6cd5\u4ee5\u52a0\u901f\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u3002", "method": "\u8bad\u7ec3\u53cc\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8f85\u52a9\u529b\u667a\u80fd\u4f53\u6839\u636e\u72b6\u6001\u65bd\u52a0\u529b\uff0c\u5e76\u968f\u673a\u5668\u4eba\u719f\u7ec3\u5ea6\u63d0\u5347\u9010\u6b65\u51cf\u5c11\u8f85\u52a9\u3002", "result": "\u5728\u884c\u8d70\u3001\u821e\u8e48\u548c\u540e\u7a7a\u7ffb\u4efb\u52a1\u4e2d\uff0cA2CF\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb30%\uff0c\u5931\u8d25\u7387\u964d\u4f4e40%\uff0c\u751f\u6210\u65e0\u9700\u652f\u6301\u7684\u7a33\u5065\u7b56\u7565\u3002", "conclusion": "\u81ea\u9002\u5e94\u8f85\u52a9\u529b\u663e\u8457\u52a0\u901f\u9ad8\u7ef4\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u590d\u6742\u6280\u80fd\u7684\u83b7\u53d6\u3002"}}
{"id": "2506.23242", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23242", "abs": "https://arxiv.org/abs/2506.23242", "authors": ["Yuxin Yang", "Hang Zhou", "Chaojie Li", "Xin Li", "Yingyi Yan", "Mingyang Zheng"], "title": "Revisiting Z Transform Laplace Inversion: To Correct flaws in Signal and System Theory", "comment": "This work is to be submitted to IEEE transactions on automatic\n  control", "summary": "This paper revisits the classical formulation of the Z-transform and its\nrelationship to the inverse Laplace transform (L-1), originally developed by\nRagazzini in sampled-data theory. It identifies a longstanding mathematical\noversight in standard derivations, which typically neglect the contribution\nfrom the infinite arc in the complex plane during inverse Laplace evaluation.\nThis omission leads to inconsistencies, especially at discontinuities such as t\n= 0. By incorporating the full Bromwich contour, including all boundary\ncontributions, we restore internal consistency between L-1 and the Z-transform,\naligning the corrected L-1 with results from Discrete-Time Fourier Transform\n(DTFT) aliasing theory. Consequently, this necessitates a structural revision\nof the Z-transform, inverse Laplace transform, and the behavior of the\nHeaviside step function at discontinuities, providing a more accurate\nfoundation for modeling and analysis of sampled-data systems.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86Z\u53d8\u6362\u4e0e\u9006\u62c9\u666e\u62c9\u65af\u53d8\u6362\uff08L-1\uff09\u7684\u5173\u7cfb\uff0c\u6307\u51fa\u6807\u51c6\u63a8\u5bfc\u4e2d\u957f\u671f\u5ffd\u7565\u7684\u6570\u5b66\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b8c\u6574Bromwich\u8def\u5f84\u4fee\u6b63\u4e86\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u63a8\u5bfc\u4e2d\u56e0\u5ffd\u7565\u65e0\u9650\u5f27\u8d21\u732e\u5bfc\u81f4\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u95f4\u65ad\u70b9\uff08\u5982t=0\uff09\u5904\u3002", "method": "\u901a\u8fc7\u5b8c\u6574Bromwich\u8def\u5f84\uff08\u5305\u62ec\u6240\u6709\u8fb9\u754c\u8d21\u732e\uff09\u91cd\u65b0\u8bc4\u4f30\u9006\u62c9\u666e\u62c9\u65af\u53d8\u6362\uff0c\u4fee\u6b63Z\u53d8\u6362\u4e0eL-1\u7684\u5173\u7cfb\u3002", "result": "\u4fee\u6b63\u540e\u7684L-1\u4e0e\u79bb\u6563\u65f6\u95f4\u5085\u91cc\u53f6\u53d8\u6362\uff08DTFT\uff09\u6df7\u53e0\u7406\u8bba\u4e00\u81f4\uff0c\u4e3a\u91c7\u6837\u6570\u636e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5efa\u6a21\u57fa\u7840\u3002", "conclusion": "\u9700\u5bf9Z\u53d8\u6362\u3001\u9006\u62c9\u666e\u62c9\u65af\u53d8\u6362\u53ca\u9636\u8dc3\u51fd\u6570\u5728\u95f4\u65ad\u70b9\u7684\u884c\u4e3a\u8fdb\u884c\u7ed3\u6784\u6027\u4fee\u8ba2\uff0c\u4ee5\u63d0\u5347\u91c7\u6837\u6570\u636e\u7cfb\u7edf\u7684\u5206\u6790\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22679", "abs": "https://arxiv.org/abs/2506.22679", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56e2\u961f\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u7ec6\u5fae\u884c\u4e3a\u8868\u8fbe\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u89e3\u7801\u5668\u6a21\u578b\uff08\u5982Llama-3.1\uff09\u8868\u73b0\u4f18\u4e8e\u7f16\u7801\u5668\u6a21\u578b\uff08\u5982RoBERTa\uff09\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u5206\u6790\u9ad8\u538b\u529b\u73af\u5883\uff08\u5982\u592a\u7a7a\u4efb\u52a1\uff09\u4e2d\u56e2\u961f\u6c9f\u901a\u52a8\u6001\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u80fd\u83b7\u53d6\u6587\u672c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u5206\u7c7b\u3001\u5fae\u8c03\u3001\u589e\u5f3a\u5fae\u8c03\u7684\u7f16\u7801\u5668\u6a21\u578b\uff08RoBERTa\u3001DistilBERT\uff09\u4e0e\u5c11\u6837\u672c\u751f\u6210\u7684\u89e3\u7801\u5668\u6a21\u578b\uff08Llama-3.1\uff09\u7684\u6027\u80fd\u3002", "result": "\u89e3\u7801\u5668\u6a21\u578bLlama-3.1\u8868\u73b0\u6700\u4f73\uff0c\u5b8fF1\u5206\u6570\u57283\u7c7b\u548c2\u7c7b\u5206\u7c7b\u4e2d\u5206\u522b\u8fbe\u523044%\u548c68%\u3002", "conclusion": "\u89e3\u7801\u5668\u6a21\u578b\u66f4\u9002\u5408\u68c0\u6d4b\u56e2\u961f\u5bf9\u8bdd\u4e2d\u7684\u7ec6\u5fae\u884c\u4e3a\uff0c\u5bf9\u5f00\u53d1\u5206\u6790\u56e2\u961f\u6c9f\u901a\u7684\u8bed\u97f3\u6280\u672f\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.23107", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23107", "abs": "https://arxiv.org/abs/2506.23107", "authors": ["Bing Song", "Jianing Liu", "Sisi Jian", "Chenyang Wu", "Vinayak Dixit"], "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study", "comment": "20 pages, 1 figure", "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u98ce\u9669\u51b3\u7b56\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u6bd4\u4eba\u7c7b\u66f4\u89c4\u907f\u98ce\u9669\uff0c\u4e14\u4e2d\u6587\u63d0\u793a\u4e0b\u7684\u9884\u6d4b\u504f\u5dee\u66f4\u5927\u3002", "motivation": "\u968f\u7740LLMs\u5e94\u7528\u7684\u6269\u5c55\uff0c\u5176\u5728\u590d\u6742\u51b3\u7b56\u884c\u4e3a\uff08\u5982\u98ce\u9669\u51b3\u7b56\uff09\u4e2d\u7684\u53ef\u9760\u6027\u5f15\u53d1\u5173\u6ce8\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5176\u6a21\u62df\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f69\u7968\u4efb\u52a1\u6bd4\u8f83ChatGPT 4o\u548co1-mini\u7684\u9884\u6d4b\u4e0e\u4eba\u7c7b\u5b9e\u9645\u9009\u62e9\uff0c\u4f7f\u7528CRRA\u6846\u67b6\u5206\u6790\u98ce\u9669\u504f\u597d\uff0c\u5e76\u8003\u5bdf\u591a\u8bed\u8a00\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u6bd4\u4eba\u7c7b\u66f4\u89c4\u907f\u98ce\u9669\uff0co1-mini\u66f4\u63a5\u8fd1\u4eba\u7c7b\u884c\u4e3a\uff1b\u4e2d\u6587\u63d0\u793a\u4e0b\u7684\u9884\u6d4b\u504f\u5dee\u5927\u4e8e\u82f1\u6587\u3002", "conclusion": "LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u98ce\u9669\u884c\u4e3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u4ecd\u6709\u5c40\u9650\u3002"}}
{"id": "2506.22513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22513", "abs": "https://arxiv.org/abs/2506.22513", "authors": ["Aditya Sharma"], "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence", "comment": null, "summary": "This investigation attempts to create an automated framework for fault\ndetection and organization for usage in contemporary radiography, as per NDE\n4.0. The review's goals are to address the lack of information that is\nsufficiently explained, learn how to make the most of virtual defect increase,\nand determine whether the framework is viable by using NDE measurements. As its\nbasic information source, the technique consists of compiling and categorizing\n223 CR photographs of airplane welds. Information expansion systems, such as\nvirtual defect increase and standard increase, are used to work on the\npreparation dataset. A modified U-net model is prepared using the improved data\nto produce semantic fault division veils. To assess the effectiveness of the\nmodel, NDE boundaries such as Case, estimating exactness, and misleading call\nrate are used. Tiny a90/95 characteristics, which provide strong\ndifferentiating evidence of flaws, reveal that the suggested approach achieves\nexceptional awareness in defect detection. Considering a 90/95, size error, and\nfake call rate in the weld area, the consolidated expansion approach clearly\nwins. Due to the framework's fast derivation speed, large images can be broken\ndown efficiently and quickly. Professional controllers evaluate the transmitted\nsystem in the field and believe that it has a guarantee as a support device in\nthe testing cycle, irrespective of particular equipment cut-off points and\nprogramming resemblance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u73b0\u4ee3\u5c04\u7ebf\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u7f3a\u9677\u589e\u5f3a\u548cNDE\u6d4b\u91cf\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f18\u5316\u865a\u62df\u7f3a\u9677\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u9a8c\u8bc1\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "method": "\u6536\u96c6\u5e76\u5206\u7c7b223\u5f20\u98de\u673a\u710a\u7f1d\u7684CR\u7167\u7247\uff0c\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u865a\u62df\u7f3a\u9677\u589e\u5f3a\uff09\u6539\u8fdb\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6539\u8fdb\u7684U-net\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u7f3a\u9677\u5206\u5272\u3002", "result": "\u6a21\u578b\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7075\u654f\u5ea6\uff0c\u5c24\u5176\u5728a90/95\u7279\u5f81\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e13\u4e1a\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u4f5c\u4e3a\u68c0\u6d4b\u652f\u6301\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u4e0d\u53d7\u7279\u5b9a\u8bbe\u5907\u9650\u5236\u5f71\u54cd\u3002"}}
{"id": "2506.23259", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23259", "abs": "https://arxiv.org/abs/2506.23259", "authors": ["Lachin Naghashyar"], "title": "Improving Myocardial Infarction Detection via Synthetic ECG Pretraining", "comment": null, "summary": "Myocardial infarction is a major cause of death globally, and accurate early\ndiagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep\nlearning models have shown promise for automated ECG interpretation, but\nrequire large amounts of labeled data, which are often scarce in practice. We\npropose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with\ntunable MI morphology and realistic noise, and (ii) pre-trains recurrent and\ntransformer classifiers with self-supervised masked-autoencoding plus a joint\nreconstruction-classification objective. We validate the realism of synthetic\nECGs via statistical and visual analysis, confirming that key morphological\nfeatures are preserved. Pretraining on synthetic data consistently improved\nclassification performance, particularly in low-data settings, with AUC gains\nof up to 4 percentage points. These results show that controlled synthetic ECGs\ncan help improve MI detection when real clinical data is limited.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u751f\u7406\u611f\u77e5\u7684ECG\u5408\u6210\u4e0e\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u5fc3\u808c\u6897\u6b7b\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u3002", "motivation": "\u5fc3\u808c\u6897\u6b7b\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u4f46ECG\u65e9\u671f\u8bca\u65ad\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5b9e\u8df5\u4e2d\u5e38\u7a00\u7f3a\u3002", "method": "\u5408\u6210\u5177\u6709\u53ef\u8c03MI\u5f62\u6001\u548c\u771f\u5b9e\u566a\u58f0\u768412\u5bfc\u8054ECG\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u63a9\u7801\u81ea\u7f16\u7801\u548c\u8054\u5408\u91cd\u5efa-\u5206\u7c7b\u76ee\u6807\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5408\u6210ECG\u4fdd\u7559\u5173\u952e\u5f62\u6001\u7279\u5f81\uff0c\u9884\u8bad\u7ec3\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff08AUC\u6700\u9ad8\u63d0\u53474\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "\u5408\u6210ECG\u53ef\u6709\u6548\u6539\u5584\u5fc3\u808c\u6897\u6b7b\u68c0\u6d4b\uff0c\u5c24\u5176\u5728\u4e34\u5e8a\u6570\u636e\u6709\u9650\u65f6\u3002"}}
{"id": "2506.23126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23126", "abs": "https://arxiv.org/abs/2506.23126", "authors": ["Suning Huang", "Qianzhong Chen", "Xiaohan Zhang", "Jiankai Sun", "Mac Schwager"], "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation", "comment": null, "summary": "3D world models (i.e., learning-based 3D dynamics models) offer a promising\napproach to generalizable robotic manipulation by capturing the underlying\nphysics of environment evolution conditioned on robot actions. However,\nexisting 3D world models are primarily limited to single-material dynamics\nusing a particle-based Graph Neural Network model, and often require\ntime-consuming 3D scene reconstruction to obtain 3D particle tracks for\ntraining. In this work, we present ParticleFormer, a Transformer-based point\ncloud world model trained with a hybrid point cloud reconstruction loss,\nsupervising both global and local dynamics features in multi-material,\nmulti-object robot interactions. ParticleFormer captures fine-grained\nmulti-object interactions between rigid, deformable, and flexible materials,\ntrained directly from real-world robot perception data without an elaborate\nscene reconstruction. We demonstrate the model's effectiveness both in 3D scene\nforecasting tasks, and in downstream manipulation tasks using a Model\nPredictive Control (MPC) policy. In addition, we extend existing dynamics\nlearning benchmarks to include diverse multi-material, multi-object interaction\nscenarios. We validate our method on six simulation and three real-world\nexperiments, where it consistently outperforms leading baselines by achieving\nsuperior dynamics prediction accuracy and less rollout error in downstream\nvisuomotor tasks. Experimental videos are available at\nhttps://particleformer.github.io/.", "AI": {"tldr": "ParticleFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u70b9\u4e91\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u70b9\u4e91\u91cd\u5efa\u635f\u5931\u8bad\u7ec3\uff0c\u652f\u6301\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u7684\u52a8\u6001\u9884\u6d4b\uff0c\u65e0\u9700\u590d\u6742\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u67093D\u4e16\u754c\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u6750\u6599\u52a8\u6001\u4e14\u4f9d\u8d56\u8017\u65f63D\u91cd\u5efa\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u3002", "method": "\u63d0\u51faParticleFormer\uff0c\u7ed3\u5408Transformer\u548c\u70b9\u4e91\u91cd\u5efa\u635f\u5931\uff0c\u76f4\u63a5\u5229\u7528\u771f\u5b9e\u673a\u5668\u4eba\u611f\u77e5\u6570\u636e\u8bad\u7ec3\u3002", "result": "\u57283D\u573a\u666f\u9884\u6d4b\u548c\u4e0b\u6e38\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u52a8\u6001\u9884\u6d4b\u7cbe\u5ea6\u548c\u6eda\u52a8\u8bef\u5dee\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "ParticleFormer\u5728\u591a\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u63a7\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23248", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23248", "abs": "https://arxiv.org/abs/2506.23248", "authors": ["Bang Huang", "Kihong Park", "Xiaowei Pang", "Mohamed-Slim Alouini"], "title": "Joint Trajectory and Resource Optimization for HAPs-SAR Systems with Energy-Aware Constraints", "comment": null, "summary": "This paper investigates the joint optimization of trajectory planning and\nresource allocation for a high-altitude platform stations synthetic aperture\nradar (HAPs-SAR) system. To support real-time sensing and conserve the limited\nenergy budget of the HAPs, the proposed framework assumes that the acquired\nradar data are transmitted in real time to a ground base station for SAR image\nreconstruction. A dynamic trajectory model is developed, and the power\nconsumption associated with radar sensing, data transmission, and circular\nflight is comprehensively analyzed. In addition, solar energy harvesting is\nconsidered to enhance system sustainability. An energy-aware mixed-integer\nnonlinear programming (MINLP) problem is formulated to maximize radar beam\ncoverage while satisfying operational constraints. To solve this challenging\nproblem, a sub-optimal successive convex approximation (SCA)-based framework is\nproposed, incorporating iterative optimization and finite search. Simulation\nresults validate the convergence of the proposed algorithm and demonstrate its\neffectiveness in balancing SAR performance, communication reliability, and\nenergy efficiency. A final SAR imaging simulation on a 9-target lattice\nscenario further confirms the practical feasibility of the proposed solution.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u7a7a\u5e73\u53f0\u7ad9\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u7cfb\u7edf\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u8d44\u6e90\u5206\u914d\u7684\u8054\u5408\u4f18\u5316\uff0c\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u611f\u77e5\u5e76\u8282\u7701\u80fd\u91cf\u3002", "motivation": "\u652f\u6301\u5b9e\u65f6\u611f\u77e5\u5e76\u89e3\u51b3\u9ad8\u7a7a\u5e73\u53f0\u80fd\u91cf\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u4f20\u8f93\u548c\u592a\u9633\u80fd\u91c7\u96c6\u63d0\u5347\u7cfb\u7edf\u53ef\u6301\u7eed\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u8f68\u8ff9\u6a21\u578b\uff0c\u5206\u6790\u96f7\u8fbe\u611f\u77e5\u3001\u6570\u636e\u4f20\u8f93\u548c\u98de\u884c\u7684\u80fd\u8017\uff0c\u5e76\u6784\u5efa\u80fd\u91cf\u611f\u77e5\u7684MINLP\u95ee\u9898\uff0c\u91c7\u7528SCA\u6846\u67b6\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e73\u8861\u4e86SAR\u6027\u80fd\u3001\u901a\u4fe1\u53ef\u9760\u6027\u548c\u80fd\u91cf\u6548\u7387\uff0c\u5e76\u57289\u76ee\u6807\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f18\u5316\u8f68\u8ff9\u548c\u8d44\u6e90\u5206\u914d\u65b9\u9762\u6709\u6548\uff0c\u63d0\u5347\u4e86HAPs-SAR\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2506.22694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22694", "abs": "https://arxiv.org/abs/2506.22694", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7b80\u5355\u6280\u672fVocabTrim\uff0c\u901a\u8fc7\u4f18\u5316drafter\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u6765\u63d0\u9ad8\u57fa\u4e8edrafter\u7684\u63a8\u6d4b\u89e3\u7801\uff08SpD\uff09\u6027\u80fd\uff0c\u51cf\u5c11\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u63a8\u6d4b\u89e3\u7801\u901a\u5e38\u8981\u6c42\u76ee\u6807\u6a21\u578b\u548cdrafter\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u4e00\u4e00\u5bf9\u5e94\uff0c\u5bfc\u81f4\u5728\u5927\u8bcd\u6c47\u8868\u76ee\u6807\u6a21\u578b\u4e0a\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u5f00\u9500\u3002VocabTrim\u65e8\u5728\u51cf\u5c11\u8fd9\u79cd\u5f00\u9500\uff0c\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002", "method": "VocabTrim\u901a\u8fc7\u91cd\u6784drafter\u7684\u8bed\u8a00\u6a21\u578b\u5934\uff08LM head\uff09\uff0c\u4ec5\u5305\u542b\u76ee\u6807\u6a21\u578b\u8bcd\u6c47\u8868\u4e2d\u9ad8\u9891\u91c7\u6837\u7684\u6709\u9650\u8bcd\u6c47\uff0c\u4ee5\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVocabTrim\u5728Llama-3\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8616%\u7684\u5185\u5b58\u53d7\u9650\u52a0\u901f\uff08MBSU\uff09\uff0c\u5982Llama-3.2-3B-Instruct\u3002", "conclusion": "VocabTrim\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u5185\u5b58\u53d7\u9650\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2506.23123", "categories": ["cs.AI", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.23123", "abs": "https://arxiv.org/abs/2506.23123", "authors": ["Rishi Bommasani"], "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7406\u89e3\u5176\u80fd\u529b\u3001\u98ce\u9669\u53ca\u4f9b\u5e94\u94fe\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u653f\u7b56\u5efa\u8bae\u63a8\u52a8\u66f4\u597d\u7684AI\u6cbb\u7406\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u4f5c\u4e3aAI\u6280\u672f\u7684\u6838\u5fc3\uff0c\u5e26\u6765\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u56f0\u60d1\u548c\u62c5\u5fe7\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u79d1\u5b66\u7814\u7a76\u548c\u653f\u7b56\u63a5\u53e3\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u793e\u4f1a\u6548\u679c\u3002", "method": "\u56f4\u7ed5\u4e09\u4e2a\u4e3b\u9898\u5c55\u5f00\uff1a\u6982\u5ff5\u6846\u67b6\uff08\u80fd\u529b\u3001\u98ce\u9669\u3001\u4f9b\u5e94\u94fe\uff09\u3001\u5b9e\u8bc1\u7814\u7a76\uff08\u6a21\u578b\u8bc4\u4f30\u548c\u7ec4\u7ec7\u900f\u660e\u5ea6\uff09\u3001\u653f\u7b56\u884c\u52a8\uff08\u57fa\u4e8e\u8bc1\u636e\u7684AI\u653f\u7b56\uff09\u3002", "result": "\u8bba\u6587\u4e3a\u7406\u89e3\u57fa\u7840\u6a21\u578b\u7684\u793e\u4f1a\u5f71\u54cd\u63d0\u4f9b\u4e86\u79d1\u5b66\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u7ed3\u5408\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acb\u79d1\u5b66\u57fa\u7840\u548c\u7814\u7a76\u653f\u7b56\u63a5\u53e3\uff0c\u8bba\u6587\u4e3a\u5b9e\u73b0AI\u65f6\u4ee3\u66f4\u597d\u7684\u793e\u4f1a\u6548\u679c\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2506.22517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22517", "abs": "https://arxiv.org/abs/2506.22517", "authors": ["Subhadip Kumar"], "title": "Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis", "comment": null, "summary": "Containers are an integral part of the logistics industry and act as a\nbarrier for cargo. A typical service life for a container is more than 20\nyears. However, overtime containers suffer various types of damage due to the\nmechanical as well as natural factors. A damaged container is a safety hazard\nfor the employees handling it and a liability for the logistic company.\nTherefore, a timely inspection and detection of the damaged container is a key\nfor prolonging service life as well as avoiding safety hazards. In this paper,\nwe will compare the performance of the damage detection by three\nstate-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.\nWe will use a dataset of 278 annotated images to train, validate and test the\nmodel. We will compare the mAP and precision of the model. The objective of\nthis paper is to identify the model that is best suited for container damage\ndetection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%\ncompared to RF-DETR, which was 77.7%. However, while testing the model for\nnot-so-common damaged containers, the RF-DETR model outperformed the others\noverall, exhibiting superiority to accurately detecting both damaged containers\nas well as damage occurrences with high confidence.", "AI": {"tldr": "\u6bd4\u8f83\u4e86Yolov12\u3001Yolov11\u548cRF-DETR\u4e09\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u96c6\u88c5\u7bb1\u635f\u4f24\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RF-DETR\u5728\u4e0d\u5e38\u89c1\u635f\u4f24\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u96c6\u88c5\u7bb1\u635f\u4f24\u662f\u7269\u6d41\u884c\u4e1a\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u53ca\u65f6\u68c0\u6d4b\u4ee5\u5ef6\u957f\u4f7f\u7528\u5bff\u547d\u548c\u907f\u514d\u98ce\u9669\u3002", "method": "\u4f7f\u7528278\u5f20\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e09\u79cd\u6a21\u578b\uff0c\u6bd4\u8f83mAP\u548c\u7cbe\u5ea6\u3002", "result": "Yolov11\u548c12\u7684mAP@50\u4e3a81.9%\uff0cRF-DETR\u4e3a77.7%\uff0c\u4f46RF-DETR\u5728\u4e0d\u5e38\u89c1\u635f\u4f24\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "RF-DETR\u5728\u4e0d\u5e38\u89c1\u635f\u4f24\u68c0\u6d4b\u4e2d\u66f4\u5177\u4f18\u52bf\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.23298", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23298", "abs": "https://arxiv.org/abs/2506.23298", "authors": ["Xing Shen", "Justin Szeto", "Mingyang Li", "Hengguan Huang", "Tal Arbel"], "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference", "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6821\u51c6\u504f\u5dee\u548c\u4eba\u53e3\u7edf\u8ba1\u4e0d\u516c\u5e73\u6027\uff0c\u63d0\u51fa\u4e86CALIN\u65b9\u6cd5\u4ee5\u5728\u63a8\u7406\u65f6\u6821\u51c6\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "MLLMs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6821\u51c6\u8bef\u5dee\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b50\u7fa4\u4e2d\u7684\u8868\u73b0\u9700\u8981\u6df1\u5165\u5206\u6790\uff0c\u4ee5\u786e\u4fdd\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86CALIN\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u6b21\u7a0b\u5e8f\uff08\u4ece\u603b\u4f53\u5230\u5b50\u7fa4\uff09\u4f30\u8ba1\u6821\u51c6\u77e9\u9635\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5e94\u7528\u8fd9\u4e9b\u77e9\u9635\u6821\u51c6\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCALIN\u80fd\u6709\u6548\u786e\u4fdd\u516c\u5e73\u7684\u7f6e\u4fe1\u6821\u51c6\uff0c\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u6700\u5c0f\u5316\u516c\u5e73\u6027\u4e0e\u6548\u7528\u7684\u6743\u8861\u3002", "conclusion": "CALIN\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u6821\u51c6\u65b9\u6cd5\uff0c\u80fd\u591f\u51cf\u5c11MLLMs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6821\u51c6\u504f\u5dee\u548c\u4eba\u53e3\u7edf\u8ba1\u4e0d\u516c\u5e73\u6027\u3002"}}
{"id": "2506.23129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23129", "abs": "https://arxiv.org/abs/2506.23129", "authors": ["Hossein B. Jond", "Logan Beaver", "Martin Jirou\u0161ek", "Naiemeh Ahmadlou", "Veli Bak\u0131rc\u0131o\u011flu", "Martin Saska"], "title": "Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking", "comment": null, "summary": "Collision-free optimal formation control of unmanned aerial vehicle (UAV)\nteams is challenging. The state-of-the-art optimal control approaches often\nrely on numerical methods sensitive to initial guesses. This paper presents an\ninnovative collision-free finite-time formation control scheme for multiple\nUAVs leveraging the differential flatness of the UAV dynamics, eliminating the\nneed for numerical methods. We formulate a finite-time optimal control problem\nto plan a formation trajectory for feasible initial states. This formation\ntrajectory planning optimal control problem involves a collective performance\nindex to meet the formation requirements of achieving relative positions and\nvelocity consensus. It is solved by applying Pontryagin's principle.\nSubsequently, a collision-constrained regulating problem is addressed to ensure\ncollision-free tracking of the planned formation trajectory. The tracking\nproblem incorporates a directionally aware collision avoidance strategy that\nprioritizes avoiding UAVs in the forward path and relative approach. It assigns\nlower priority to those on the sides with an oblique relative approach and\ndisregards UAVs behind and not in the relative approach. The simulation results\nfor a four-UAV team (re)formation problem confirm the efficacy of the proposed\ncontrol scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u5ea6\u7684\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5bf9\u521d\u59cb\u731c\u6d4b\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u7684\u6709\u9650\u65f6\u95f4\u7f16\u961f\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u6570\u503c\u65b9\u6cd5\u4e14\u5bf9\u521d\u59cb\u731c\u6d4b\u654f\u611f\uff0c\u96be\u4ee5\u5b9e\u73b0\u65e0\u78b0\u649e\u7684\u65e0\u4eba\u673a\u7f16\u961f\u63a7\u5236\u3002", "method": "\u5229\u7528\u65e0\u4eba\u673a\u52a8\u529b\u5b66\u7684\u5fae\u5206\u5e73\u5766\u6027\uff0c\u8bbe\u8ba1\u6709\u9650\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Pontryagin\u539f\u7406\u6c42\u89e3\uff1b\u91c7\u7528\u65b9\u5411\u611f\u77e5\u7684\u78b0\u649e\u907f\u514d\u7b56\u7565\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u56db\u65e0\u4eba\u673a\u7f16\u961f\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u7684\u6709\u9650\u65f6\u95f4\u7f16\u961f\u63a7\u5236\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.23302", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23302", "abs": "https://arxiv.org/abs/2506.23302", "authors": ["Chams Eddine Mballo", "Robert Walters", "Jonnalagadda V. R. Prasad"], "title": "Load Limiting Control for Component Life Extension", "comment": "Accepted for publication in Journal of Guidance, Control, and\n  Dynamics, Vol 48 (2), 2025. Version of Record at DOI\n  https://doi.org/10.2514/1.G007854", "summary": "This paper presents the development of a novel life-extending control scheme\nfor critical helicopter components subjected to significant fatigue loading.\nThe primary objective is to synthesize a more efficient and less conservative\nlife-extending control scheme than those currently available in the literature.\nThe proposed Load Limiting Control (LLC) scheme is a viable solution that\naddresses several issues that current life-extending control schemes suffer\nfrom, such as the neglect of fatigue damage induced by the harmonic component\nof loads and the inability to distinguish between aggressive and non-aggressive\nmaneuvers. The proposed LLC scheme treats desired harmonic load limits as limit\nboundaries and recasts the problem of load limiting as a vehicle limit by\ncomputing a Control Margin (CM) using a limit detection and avoidance module.\nThe computed CM is used as a cue to the pilot. The limit detection and\navoidance module comprises an optimization algorithm, a model predictive\ncontroller, and a computationally simple on-board dynamical model. Simulations\nwere conducted to demonstrate the effectiveness of the LLC scheme in limiting\nharmonic pitch link loads during flight. One significant outcome is that, with\nsufficient training, the pilot can skillfully track the cue within 0.5 seconds\nof initiating the tracking task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76f4\u5347\u673a\u5173\u952e\u90e8\u4ef6\u5bff\u547d\u5ef6\u957f\u63a7\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u9650\u5236\u8c10\u6ce2\u8d1f\u8f7d\u548c\u533a\u5206\u673a\u52a8\u7c7b\u578b\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5bff\u547d\u5ef6\u957f\u63a7\u5236\u65b9\u6848\u5ffd\u7565\u8c10\u6ce2\u8d1f\u8f7d\u5f15\u8d77\u7684\u75b2\u52b3\u635f\u4f24\uff0c\u4e14\u65e0\u6cd5\u533a\u5206\u673a\u52a8\u7c7b\u578b\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u8d1f\u8f7d\u9650\u5236\u63a7\u5236\uff08LLC\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u3001\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u548c\u7b80\u5316\u52a8\u529b\u5b66\u6a21\u578b\u8ba1\u7b97\u63a7\u5236\u88d5\u5ea6\uff08CM\uff09\uff0c\u4e3a\u98de\u884c\u5458\u63d0\u4f9b\u63d0\u793a\u3002", "result": "\u4eff\u771f\u663e\u793aLLC\u65b9\u6848\u80fd\u6709\u6548\u9650\u5236\u98de\u884c\u4e2d\u7684\u8c10\u6ce2\u8d1f\u8f7d\uff0c\u98de\u884c\u5458\u7ecf\u8bad\u7ec3\u53ef\u57280.5\u79d2\u5185\u8ddf\u8e2a\u63d0\u793a\u3002", "conclusion": "LLC\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u76f4\u5347\u673a\u90e8\u4ef6\u7684\u5bff\u547d\u5ef6\u957f\u63a7\u5236\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22698", "abs": "https://arxiv.org/abs/2506.22698", "authors": ["Emily Dux Speltz"], "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "\u62a5\u544a\u603b\u7ed3\u4e86\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\u7684\u6210\u679c\uff0c\u63a2\u8ba8\u4e86AI\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u4e86LLMs\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3AI\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u6587\u672c\u7406\u89e3\u53ca\u751f\u6210\u4e4b\u95f4\u5173\u7cfb\u7684\u77e5\u8bc6\u7f3a\u53e3\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u901a\u8fc7\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u4e60\u548cAI-NLP\u9886\u57df\u7684\u4e13\u5bb6\u5408\u4f5c\u5bf9\u8bdd\uff0c\u5206\u6790\u4eba\u7c7b\u4e0eAI\u5728\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e92\u52a8\u3002", "result": "\u53d1\u73b0LLMs\u80fd\u63d0\u4f9b\u5bf9\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u89c1\u89e3\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u8bed\u8a00\u80fd\u529b\uff1b\u4eba\u7c7b\u53cd\u9988\u53ef\u63d0\u5347LLM\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5bf9\u9f50\u3002", "conclusion": "\u672a\u6765\u9700\u5173\u6ce8LLMs\u5728\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u548c\u6559\u80b2\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4f26\u7406\u548c\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2506.23128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23128", "abs": "https://arxiv.org/abs/2506.23128", "authors": ["Chi Chiu So", "Yueyue Sun", "Jun-Min Wang", "Siu Pang Yung", "Anthony Wai Keung Loh", "Chun Pong Chau"], "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons", "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference\n  on artificial intelligence testing (AITest)", "summary": "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\uff08DeepSeek-R1\u3001DeepSeek-V3\u548cGPT-4o\uff09\u5728\u6df1\u5ea6\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek-R1\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u5747\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5bb6\u65cf\u6811\u548c\u4e00\u822c\u56fe\u63a8\u7406\u7684\u57fa\u51c6\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e09\u79cd\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "DeepSeek-R1\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u968f\u7740\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\uff0c\u5982\u591a\u6a21\u6001\u63a8\u7406\u548c\u7cfb\u7edf\u6027\u5206\u6790\u63a8\u7406\u5931\u8d25\u3002"}}
{"id": "2506.22531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22531", "abs": "https://arxiv.org/abs/2506.22531", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "comment": "Accepted at ICCV 2025", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPreserve Anything\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5bf9\u8c61\u4fdd\u5b58\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u5b58\u591a\u4e2a\u5bf9\u8c61\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u573a\u666f\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528N\u901a\u9053ControlNet\uff0c\u7ed3\u5408\u5bf9\u8c61\u4fdd\u5b58\u3001\u80cc\u666f\u5f15\u5bfc\u548c\u9ad8\u9891\u8986\u76d6\u6a21\u5757\u3002", "result": "\u5728FID\u548cCLIP-S\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bf9\u8c61\u4fdd\u5b58\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2506.23305", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23305", "abs": "https://arxiv.org/abs/2506.23305", "authors": ["Rachit Saluja", "Arzu Kovanlikaya", "Candace Chien", "Lauren Kathryn Blatt", "Jeffrey M. Perlman", "Stefan Worgall", "Mert R. Sabuncu", "Jonathan P. Dyke"], "title": "BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia", "comment": null, "summary": "Bronchopulmonary dysplasia (BPD) is a common complication among preterm\nneonates, with portable X-ray imaging serving as the standard diagnostic\nmodality in neonatal intensive care units (NICUs). However, lung magnetic\nresonance imaging (MRI) offers a non-invasive alternative that avoids sedation\nand radiation while providing detailed insights into the underlying mechanisms\nof BPD. Leveraging high-resolution 3D MRI data, advanced image processing and\nsemantic segmentation algorithms can be developed to assist clinicians in\nidentifying the etiology of BPD. In this dataset, we present MRI scans paired\nwith corresponding semantic segmentations of the lungs and trachea for 40\nneonates, the majority of whom are diagnosed with BPD. The imaging data consist\nof free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as\nthe StarVIBE series. Additionally, we provide comprehensive clinical data and\nbaseline segmentation models, validated against clinical assessments, to\nsupport further research and development in neonatal lung imaging.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u5206\u8fa8\u73873D MRI\u6570\u636e\u7684\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f85\u52a9\u8bca\u65ad\u652f\u6c14\u7ba1\u80ba\u53d1\u80b2\u4e0d\u826f\uff08BPD\uff09\uff0c\u5e76\u63d0\u4f9b\u4e8640\u540d\u65b0\u751f\u513f\u7684MRI\u626b\u63cf\u548c\u8bed\u4e49\u5206\u5272\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u7684\u4fbf\u643a\u5f0fX\u5c04\u7ebf\u6210\u50cf\u5728\u8bca\u65adBPD\u65f6\u5b58\u5728\u8f90\u5c04\u548c\u9547\u9759\u95ee\u9898\uff0c\u800cMRI\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u80fd\u66f4\u8be6\u7ec6\u5730\u63ed\u793aBPD\u7684\u673a\u5236\u3002", "method": "\u5229\u7528\u81ea\u7531\u547c\u54383D StarVIBE\u5e8f\u5217\u83b7\u53d6MRI\u6570\u636e\uff0c\u5f00\u53d1\u5148\u8fdb\u7684\u56fe\u50cf\u5904\u7406\u548c\u8bed\u4e49\u5206\u5272\u7b97\u6cd5\uff0c\u8f85\u52a9\u4e34\u5e8a\u8bca\u65ad\u3002", "result": "\u63d0\u4f9b\u4e8640\u540d\u65b0\u751f\u513f\u7684MRI\u626b\u63cf\u548c\u8bed\u4e49\u5206\u5272\u6570\u636e\uff0c\u5e76\u9a8c\u8bc1\u4e86\u57fa\u7ebf\u5206\u5272\u6a21\u578b\u7684\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65b0\u751f\u513f\u80ba\u90e8\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\uff0c\u6709\u671b\u6539\u5584BPD\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u3002"}}
{"id": "2506.23152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23152", "abs": "https://arxiv.org/abs/2506.23152", "authors": ["Youzhuo Wang", "Jiayi Ye", "Chuyang Xiao", "Yiming Zhong", "Heng Tao", "Hang Yu", "Yumeng Liu", "Jingyi Yu", "Yuexin Ma"], "title": "DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover", "comment": null, "summary": "Handover between a human and a dexterous robotic hand is a fundamental yet\nchallenging task in human-robot collaboration. It requires handling dynamic\nenvironments and a wide variety of objects and demands robust and adaptive\ngrasping strategies. However, progress in developing effective dynamic\ndexterous grasping methods is limited by the absence of high-quality,\nreal-world human-to-robot handover datasets. Existing datasets primarily focus\non grasping static objects or rely on synthesized handover motions, which\ndiffer significantly from real-world robot motion patterns, creating a\nsubstantial gap in applicability. In this paper, we introduce DexH2R, a\ncomprehensive real-world dataset for human-to-robot handovers, built on a\ndexterous robotic hand. Our dataset captures a diverse range of interactive\nobjects, dynamic motion patterns, rich visual sensor data, and detailed\nannotations. Additionally, to ensure natural and human-like dexterous motions,\nwe utilize teleoperation for data collection, enabling the robot's movements to\nalign with human behaviors and habits, which is a crucial characteristic for\nintelligent humanoid robots. Furthermore, we propose an effective solution,\nDynamicGrasp, for human-to-robot handover and evaluate various state-of-the-art\napproaches, including auto-regressive models and diffusion policy methods,\nproviding a thorough comparison and analysis. We believe our benchmark will\ndrive advancements in human-to-robot handover research by offering a\nhigh-quality dataset, effective solutions, and comprehensive evaluation\nmetrics.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DexH2R\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4eba\u673a\u624b\u90e8\u4ea4\u63a5\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86DynamicGrasp\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4eba\u673a\u624b\u90e8\u4ea4\u63a5\u4efb\u52a1\u5728\u52a8\u6001\u73af\u5883\u548c\u591a\u6837\u5316\u7269\u4f53\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u9065\u64cd\u4f5c\u6536\u96c6\u6570\u636e\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u52a8\u4f5c\u81ea\u7136\uff0c\u5e76\u63d0\u51fa\u4e86DynamicGrasp\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u5316\u7269\u4f53\u548c\u52a8\u6001\u52a8\u4f5c\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u5206\u6790\u3002", "conclusion": "DexH2R\u6570\u636e\u96c6\u548cDynamicGrasp\u89e3\u51b3\u65b9\u6848\u5c06\u63a8\u52a8\u4eba\u673a\u624b\u90e8\u4ea4\u63a5\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.23304", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23304", "abs": "https://arxiv.org/abs/2506.23304", "authors": ["Quang-Manh Hoang", "Van Nam Nguyen", "Taehyung Kim", "Guilherme Vieira Hollweg", "Wencong Su", "Van-Hai Bui"], "title": "ANN-Based Grid Impedance Estimation for Adaptive Gain Scheduling in VSG Under Dynamic Grid Conditions", "comment": "Paper was accepted for IEEE Energy Conversion Congress and Exposition\n  (ECCE) 2025, Philadelphia, PA, USA", "summary": "In contrast to grid-following inverters, Virtual Synchronous Generators\n(VSGs) perform well under weak grid conditions but may become unstable when the\ngrid is strong. Grid strength depends on grid impedance, which unfortunately\nvaries over time. In this paper, we propose a novel adaptive gain-scheduling\ncontrol scheme for VSGs. First, an Artificial Neural Network (ANN) estimates\nthe fundamental-frequency grid impedance; then these estimates are fed into an\nadaptive gain-scheduling function to recalculate controller parameters under\nvarying grid conditions. The proposed method is validated in Simulink and\ncompared with a conventional VSG employing fixed controller gains. The results\ndemonstrate that settling times and overshoot percentages remain consistent\nacross different grid conditions. Additionally, previously unseen grid\nimpedance values are estimated with high accuracy and minimal time delay,\nmaking the approach well suited for real-time gain-scheduling control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u81ea\u9002\u5e94\u589e\u76ca\u8c03\u5ea6\u63a7\u5236\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u7535\u7f51\u963b\u6297\u53d8\u5316\u5e26\u6765\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u5728\u5f31\u7535\u7f51\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5f3a\u7535\u7f51\u4e0b\u53ef\u80fd\u4e0d\u7a33\u5b9a\u3002\u7535\u7f51\u963b\u6297\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u7535\u7f51\u963b\u6297\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u589e\u76ca\u8c03\u5ea6\u51fd\u6570\u52a8\u6001\u8c03\u6574\u63a7\u5236\u5668\u53c2\u6570\u3002", "result": "\u5728\u4e0d\u540c\u7535\u7f51\u6761\u4ef6\u4e0b\uff0c\u7a33\u5b9a\u65f6\u95f4\u548c\u8d85\u8c03\u767e\u5206\u6bd4\u4fdd\u6301\u4e00\u81f4\uff0c\u4e14\u80fd\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u5730\u4f30\u8ba1\u672a\u77e5\u7535\u7f51\u963b\u6297\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u65f6\u589e\u76ca\u8c03\u5ea6\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u540c\u6b65\u53d1\u7535\u673a\u5728\u4e0d\u540c\u7535\u7f51\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.22724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22724", "abs": "https://arxiv.org/abs/2506.22724", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u751f\u6210\u4e2d\uff0c\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u8d28\u91cf\u8f83\u5dee\uff0c\u539f\u56e0\u662f\u6a21\u578b\u9690\u542b\u7684\u4efb\u52a1\u89e3\u51b3-\u7ffb\u8bd1\u6d41\u7a0b\u4e2d\u7ffb\u8bd1\u9636\u6bb5\u5931\u8d25\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u591a\u8bed\u8a00\u751f\u6210\u4e2d\u8d28\u91cf\u4f4e\u4e0b\u7684\u539f\u56e0\uff0c\u5c24\u5176\u662f\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u6a21\u578b\u5728\u4e2d\u95f4\u5c42\u7684\u5904\u7406\uff08\u4f7f\u7528logit lens\uff09\uff0c\u6d4b\u8bd5\u7ffb\u8bd1\u969c\u788d\u5047\u8bbe\uff0c\u5206\u6790108\u79cd\u8bed\u8a00\u5bf9\u7684\u5355\u8bcd\u7ffb\u8bd1\u4efb\u52a1\u3002", "result": "\u7ffb\u8bd1\u5931\u8d25\u662f\u5bfc\u81f4\u6574\u4f53\u5931\u8d25\u7684\u91cd\u8981\u539f\u56e0\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u76ee\u6807\u8bed\u8a00\u3002", "conclusion": "\u7ffb\u8bd1\u969c\u788d\u662f\u591a\u8bed\u8a00\u751f\u6210\u7684\u91cd\u8981\u969c\u788d\uff0c\u4e3a\u672a\u6765\u6539\u8fdbLLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.23141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23141", "abs": "https://arxiv.org/abs/2506.23141", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing", "comment": null, "summary": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\n\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\n\\textbf{multi-head attention aggregator} to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u5173\u7cfb\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u901a\u8fc7Top-K\u90bb\u5c45\u9009\u62e9\u7b56\u7565\u548c\u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u5668\uff0c\u6709\u6548\u51cf\u5c11\u77e5\u8bc6\u56fe\u8c31\u5b8c\u6210\u4e2d\u7684\u566a\u58f0\u548c\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8282\u70b9\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4f1a\u5f15\u5165\u566a\u58f0\u548c\u4fe1\u606f\u7a00\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u51c6\u7684\u65b9\u6cd5\u6765\u6355\u6349\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u611f\u77e5\u7684Top-K\u90bb\u5c45\u9009\u62e9\u7b56\u7565\u548c\u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u5668\uff0c\u9009\u62e9\u6700\u76f8\u5173\u7684\u90bb\u5c45\u5e76\u878d\u5408\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u548c\u4f20\u64ad\u4e0e\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u6700\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u51cf\u5c11\u65e0\u5173\u4fe1\u606f\u7684\u5e72\u6270\u3002"}}
{"id": "2506.22554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22554", "abs": "https://arxiv.org/abs/2506.22554", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "comment": null, "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Seamless Interaction Dataset\uff0c\u4e00\u4e2a\u5305\u542b4000\u5c0f\u65f6\u9762\u5bf9\u9762\u4e92\u52a8\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u80fd\u7406\u89e3\u548c\u751f\u6210\u53cc\u5411\u884c\u4e3a\u52a8\u6001\u7684AI\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5173\u6a21\u578b\u53ca\u5176\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u793e\u4ea4\u667a\u80fdAI\u6280\u672f\u9700\u8981\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u53cc\u5411\u884c\u4e3a\u52a8\u6001\uff0c\u4ee5\u63a8\u52a8\u865a\u62df\u4ee3\u7406\u3001\u8fdc\u7a0b\u5448\u73b0\u4f53\u9a8c\u548c\u591a\u6a21\u6001\u5185\u5bb9\u5206\u6790\u5de5\u5177\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165Seamless Interaction Dataset\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u6a21\u578b\u7528\u4e8e\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u97f3\u5bf9\u9f50\u7684\u53cc\u5411\u52a8\u4f5c\u548c\u9762\u90e8\u8868\u60c5\uff0c\u5e76\u6574\u5408\u4e86LLM\u8bed\u97f3\u548c2D/3D\u6e32\u67d3\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u80fd\u591f\u6839\u636e\u8f93\u5165\u8bed\u97f3\u548c\u89c6\u89c9\u884c\u4e3a\u751f\u6210\u52a8\u6001\u54cd\u5e94\uff0c\u5e76\u5177\u5907\u53ef\u63a7\u7684\u60c5\u611f\u8868\u8fbe\u548c\u8bed\u4e49\u76f8\u5173\u624b\u52bf\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u578b\u5c55\u793a\u4e86\u66f4\u76f4\u89c2\u548c\u54cd\u5e94\u5f0f\u7684\u4eba\u673a\u4ea4\u4e92\u6f5c\u529b\uff0c\u4e3a\u672a\u6765AI\u6280\u672f\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23309", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23309", "abs": "https://arxiv.org/abs/2506.23309", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarakol Islam", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-SurgTPGS/", "summary": "In contemporary surgical research and practice, accurately comprehending 3D\nsurgical scenes with text-promptable capabilities is particularly crucial for\nsurgical planning and real-time intra-operative guidance, where precisely\nidentifying and interacting with surgical tools and anatomical structures is\nparamount. However, existing works focus on surgical vision-language model\n(VLM), 3D reconstruction, and segmentation separately, lacking support for\nreal-time text-promptable 3D queries. In this paper, we present SurgTPGS, a\nnovel text-promptable Gaussian Splatting method to fill this gap. We introduce\na 3D semantics feature learning strategy incorporating the Segment Anything\nmodel and state-of-the-art vision-language models. We extract the segmented\nlanguage features for 3D surgical scene reconstruction, enabling a more\nin-depth understanding of the complex surgical environment. We also propose\nsemantic-aware deformation tracking to capture the seamless deformation of\nsemantic features, providing a more precise reconstruction for both texture and\nsemantic features. Furthermore, we present semantic region-aware optimization,\nwhich utilizes regional-based semantic information to supervise the training,\nparticularly promoting the reconstruction quality and semantic smoothness. We\nconduct comprehensive experiments on two real-world surgical datasets to\ndemonstrate the superiority of SurgTPGS over state-of-the-art methods,\nhighlighting its potential to revolutionize surgical practices. SurgTPGS paves\nthe way for developing next-generation intelligent surgical systems by\nenhancing surgical precision and safety. Our code is available at:\nhttps://github.com/lastbasket/SurgTPGS.", "AI": {"tldr": "SurgTPGS\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6587\u672c\u63d0\u793a\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u5b9e\u65f6\u6587\u672c\u63d0\u793a3D\u67e5\u8be2\u7684\u7a7a\u767d\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7279\u5f81\u5b66\u4e60\u548c\u53d8\u5f62\u8ddf\u8e2a\uff0c\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u8bed\u4e49\u5e73\u6ed1\u6027\u3002", "motivation": "\u5728\u624b\u672f\u89c4\u5212\u548c\u5b9e\u65f6\u672f\u4e2d\u5f15\u5bfc\u4e2d\uff0c\u51c6\u786e\u7406\u89e33D\u624b\u672f\u573a\u666f\u5e76\u652f\u6301\u6587\u672c\u63d0\u793a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u65f6\u6587\u672c\u63d0\u793a3D\u67e5\u8be2\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408Segment Anything\u6a21\u578b\u548c\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u51fa3D\u8bed\u4e49\u7279\u5f81\u5b66\u4e60\u7b56\u7565\u3001\u8bed\u4e49\u611f\u77e5\u53d8\u5f62\u8ddf\u8e2a\u548c\u8bed\u4e49\u533a\u57df\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u624b\u672f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSurgTPGS\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u8bed\u4e49\u7406\u89e3\u3002", "conclusion": "SurgTPGS\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u624b\u672f\u7cfb\u7edf\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63d0\u9ad8\u4e86\u624b\u672f\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.23164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u5168\u5173\u952e\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u56e0\u6a21\u5f0f\u5d29\u6e83\u800c\u4ec5\u9884\u6d4b\u6700\u53ef\u80fd\u7684\u6a21\u5f0f\uff0c\u5ffd\u89c6\u4ea4\u4e92\u591a\u6837\u6027\uff0c\u4e14\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u91cf\u5316\u4ea4\u4e92\u6216\u68c0\u6d4b\u6a21\u5f0f\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6a21\u5f0f\u5d29\u6e83\u3001\u6a21\u5f0f\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\u7684\u6307\u6807\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cd\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u5f0f\u5d29\u6e83\u786e\u5b9e\u5b58\u5728\uff0c\u4e14\u5373\u4f7f\u63a5\u8fd1\u4ea4\u4e92\u4e8b\u4ef6\u65f6\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\uff0c\u6a21\u578b\u4ecd\u53ef\u80fd\u65e0\u6cd5\u9884\u6d4b\u6b63\u786e\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u6df1\u5165\u7406\u89e3\u6a21\u5f0f\u5d29\u6e83\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u4e00\u81f4\u548c\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2506.23421", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23421", "abs": "https://arxiv.org/abs/2506.23421", "authors": ["Matheus Wagner", "Marcelo M. Morato", "Ant\u00f4nio Augusto Fr\u00f6hlich", "Julio E. Normey-Rico"], "title": "Predictor-Based Compensators for Networked Control Systems with Stochastic Delays and Sampling Intervals", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The stochastic nature of time delays and sampling intervals in Networked\nControl Systems poses significant challenges for controller synthesis and\nanalysis, often leading to conservative designs and degraded performance. This\nwork presents a modeling approach for Linear Multiple-Input Multiple-Output\nNetworked Control Systems and introduces a compensation scheme based on the\nFiltered Smith Predictor to mitigate the adverse effects of stochastic time\ndelays on closed-loop performance. The proposed scheme is evaluated through\nnumerical simulations of a well-established Cooperative Adaptive Cruise Control\nsystem. Results demonstrate that the compensator achieves near-ideal average\nclosed-loop performance and significantly reduces response variability compared\nto a traditional Filtered Smith Predictor. Notably, it yields a 45% reduction\nin worst-case tracking error signal energy relative to an ideal baseline system\nwith no time delays and constant sampling intervals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7ebf\u6027\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6ee4\u6ce2\u53f2\u5bc6\u65af\u9884\u4f30\u5668\u8bbe\u8ba1\u8865\u507f\u65b9\u6848\uff0c\u4ee5\u51cf\u8f7b\u968f\u673a\u65f6\u5ef6\u5bf9\u95ed\u73af\u6027\u80fd\u7684\u4e0d\u5229\u5f71\u54cd\u3002", "motivation": "\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u4e2d\u968f\u673a\u65f6\u5ef6\u548c\u91c7\u6837\u95f4\u9694\u7684\u968f\u673a\u6027\u7ed9\u63a7\u5236\u5668\u8bbe\u8ba1\u548c\u5206\u6790\u5e26\u6765\u6311\u6218\uff0c\u5bfc\u81f4\u4fdd\u5b88\u8bbe\u8ba1\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u6ee4\u6ce2\u53f2\u5bc6\u65af\u9884\u4f30\u5668\u4f5c\u4e3a\u8865\u507f\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u5176\u5728\u534f\u540c\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u6548\u679c\u3002", "result": "\u8865\u507f\u65b9\u6848\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7406\u60f3\u7684\u5e73\u5747\u95ed\u73af\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u54cd\u5e94\u53d8\u5f02\u6027\uff0c\u6700\u574f\u60c5\u51b5\u8ddf\u8e2a\u8bef\u5dee\u4fe1\u53f7\u80fd\u91cf\u6bd4\u7406\u60f3\u57fa\u7ebf\u7cfb\u7edf\u51cf\u5c11\u4e8645%\u3002", "conclusion": "\u8be5\u8865\u507f\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\u5728\u968f\u673a\u65f6\u5ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u6ee4\u6ce2\u53f2\u5bc6\u65af\u9884\u4f30\u5668\u3002"}}
{"id": "2506.22760", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22760", "abs": "https://arxiv.org/abs/2506.22760", "authors": ["Alan Dao", "Dinh Bach Vu"], "title": "Jan-nano Technical Report", "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "AI": {"tldr": "Jan-nano\u662f\u4e00\u4e2a4B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5RLVR\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\uff0c\u65e0\u9700\u4f9d\u8d56\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\uff0c\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523083.2%\u7684\u51c6\u786e\u7387\uff0c\u652f\u6301128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5927\u80fd\u529b\u4e0e\u9ad8\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u57fa\u4e8eQwen3-4B\u5fae\u8c03\uff0c\u91c7\u7528\u591a\u9636\u6bb5RLVR\u7cfb\u7edf\uff0c\u5b8c\u5168\u6452\u5f03\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u3002", "result": "\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523083.2%\u7684\u51c6\u786e\u7387\uff0c\u53ef\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002", "conclusion": "\u667a\u80fd\u4e0d\u5728\u4e8e\u89c4\u6a21\uff0c\u800c\u5728\u4e8e\u7b56\u7565\uff0cJan-nano\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2506.23168", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "pdf": "https://arxiv.org/pdf/2506.23168", "abs": "https://arxiv.org/abs/2506.23168", "authors": ["Mohammad Abdulla", "Tobias Hille", "Dominik D\u00fcrrschnabel", "Gerd Stumme"], "title": "Rises for Measuring Local Distributivity in Lattices", "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u201crises\u201d\u6765\u91cf\u5316\u683c\u4e2d\u5206\u914d\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u4e0e\u7ecf\u5178\u5206\u914d\u6027\u6982\u5ff5\u7684\u5173\u7cfb\u3002", "motivation": "\u5728\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u4e2d\uff0c\u683c\u7684\u5206\u914d\u6027\u7f3a\u4e4f\u6807\u51c6\u5316\u5ea6\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u201crises\u201d\u6982\u5ff5\uff0c\u7528\u4e8e\u8bc4\u4f30\u683c\u4e2d\u5c5e\u6027\u6216\u5bf9\u8c61\u6570\u91cf\u7684\u53d8\u5316\uff0c\u5e76\u4e0e\u7ecf\u5178\u5206\u914d\u6027\u6982\u5ff5\u5173\u8054\u3002", "result": "\u8bc1\u660e\u683c\u662f\u5206\u914d\u6027\u7684\u5f53\u4e14\u4ec5\u5f53\u65e0\u975e\u5355\u4f4drises\uff1b\u73b0\u5b9e\u6570\u636e\u7684\u6982\u5ff5\u683c\u591a\u4e3ajoin-distributive\uff0c\u800c\u975emeet-distributive\u3002", "conclusion": "rises\u662f\u91cf\u5316\u5206\u914d\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u5b9e\u6570\u636e\u4e2d\u5206\u914d\u6027\u7684\u5206\u5e03\u7279\u70b9\u3002"}}
{"id": "2506.22556", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22556", "abs": "https://arxiv.org/abs/2506.22556", "authors": ["Markus Juvonen", "Samuli Siltanen"], "title": "Recomposed realities: animating still images via patch clustering and randomness", "comment": "22 pages, 19 figures", "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u56fe\u50cf\u91cd\u5efa\u4e0e\u52a8\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u4f7f\u9759\u6001\u56fe\u50cf\u751f\u52a8\u5316\u3002", "motivation": "\u5229\u7528\u73b0\u6709\u56fe\u50cf\u6570\u636e\uff0c\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u800c\u975e\u590d\u5236\uff0c\u5b9e\u73b0\u9759\u6001\u56fe\u50cf\u7684\u52a8\u6001\u5316\u3002", "method": "\u4f7f\u7528k\u5747\u503c\u805a\u7c7b\u5bf9\u56fe\u50cf\u5757\u8fdb\u884c\u5206\u7ec4\uff0c\u901a\u8fc7\u5339\u914d\u548c\u968f\u673a\u91c7\u6837\u4ece\u805a\u7c7b\u4e2d\u91cd\u5efa\u65b0\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u5141\u8bb8\u6e90\u57df\u548c\u76ee\u6807\u57df\u5728\u6982\u5ff5\u4e0a\u4e0d\u540c\uff0c\u4f46\u5171\u4eab\u5c40\u90e8\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u56fe\u50cf\u5757\u5b9e\u73b0\u4e86\u9759\u6001\u56fe\u50cf\u7684\u52a8\u6001\u5316\u3002"}}
{"id": "2506.23311", "categories": ["eess.IV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2506.23311", "abs": "https://arxiv.org/abs/2506.23311", "authors": ["Perla Mayo", "Carolin M. Pirkl", "Alin Achim", "Bjoern Menze", "Mohammad Golbabaee"], "title": "Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction", "comment": "11 pages, 1 figure, 1 algorithm, 3 tables. Accepted to MICCAI 2025.\n  This is a version prior peer-review", "summary": "We introduce MRF-DiPh, a novel physics informed denoising diffusion approach\nfor multiparametric tissue mapping from highly accelerated, transient-state\nquantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our\nmethod is derived from a proximal splitting formulation, incorporating a\npretrained denoising diffusion model as an effective image prior to regularize\nthe MRF inverse problem. Further, during reconstruction it simultaneously\nenforces two key physical constraints: (1) k-space measurement consistency and\n(2) adherence to the Bloch response model. Numerical experiments on in-vivo\nbrain scans data show that MRF-DiPh outperforms deep learning and compressed\nsensing MRF baselines, providing more accurate parameter maps while better\npreserving measurement fidelity and physical model consistency-critical for\nsolving reliably inverse problems in medical imaging.", "AI": {"tldr": "MRF-DiPh\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u53bb\u566a\u6269\u6563\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5feb\u901f\u5b9a\u91cfMRI\uff08\u5982MRF\uff09\u4e2d\u751f\u6210\u591a\u53c2\u6570\u7ec4\u7ec7\u56fe\u3002", "motivation": "\u89e3\u51b3\u5feb\u901f\u5b9a\u91cfMRI\u4e2d\u7684\u9006\u95ee\u9898\uff0c\u63d0\u9ad8\u53c2\u6570\u56fe\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u6a21\u578b\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u53bb\u566a\u6269\u6563\u6a21\u578b\u548c\u7269\u7406\u7ea6\u675f\uff08k\u7a7a\u95f4\u4e00\u81f4\u6027\u53caBloch\u54cd\u5e94\u6a21\u578b\uff09\u3002", "result": "\u5728\u8111\u626b\u63cf\u6570\u636e\u4e2d\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u538b\u7f29\u611f\u77e5\u57fa\u7ebf\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u53c2\u6570\u56fe\u3002", "conclusion": "MRF-DiPh\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u53ef\u9760\u5730\u89e3\u51b3\u4e86\u9006\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u6d4b\u91cf\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2506.23316", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23316", "abs": "https://arxiv.org/abs/2506.23316", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "title": "InfGen: Scenario Generation as Next Token Group Prediction", "comment": null, "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "AI": {"tldr": "InfGen\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u4ea4\u901a\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u52a8\u6001\u3001\u957f\u671f\u7684\u4ea4\u901a\u573a\u666f\uff0c\u652f\u6301\u65e0\u9650\u573a\u666f\u751f\u6210\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u6a21\u62df\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u521d\u59cb\u5316\u6216\u65e5\u5fd7\u56de\u653e\u6570\u636e\uff0c\u96be\u4ee5\u5efa\u6a21\u52a8\u6001\u3001\u957f\u671f\u7684\u4ea4\u901a\u573a\u666f\u3002", "method": "InfGen\u5c06\u6574\u4e2a\u573a\u666f\u8868\u793a\u4e3a\u4e00\u7cfb\u5217\u4ee4\u724c\uff08\u5982\u4ea4\u901a\u4fe1\u53f7\u706f\u3001\u8f66\u8f86\u72b6\u6001\u548c\u8fd0\u52a8\u5411\u91cf\uff09\uff0c\u5e76\u4f7f\u7528Transformer\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u4e0a\u7684\u4ea4\u901a\u6a21\u62df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInfGen\u80fd\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u81ea\u9002\u5e94\u7684\u4ea4\u901a\u884c\u4e3a\uff0c\u4e14\u5728\u5176\u751f\u6210\u573a\u666f\u4e2d\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "InfGen\u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f\u5ea6\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u73af\u5883\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u4ea4\u901a\u573a\u666f\u7684\u751f\u6210\u548c\u8bad\u7ec3\u3002"}}
{"id": "2506.23425", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23425", "abs": "https://arxiv.org/abs/2506.23425", "authors": ["Sampson E. Nwachukwu"], "title": "Power Flow Analysis of a 5-Bus Power System Based on Newton-Raphson Method", "comment": "8 pages, 27 figures", "summary": "Load flow analysis is a fundamental technique used by electrical engineers to\nsimulate and evaluate power system behavior under steady-state conditions. It\nenables efficient operation and control by determining how active and reactive\npower flows throughout the system. Selecting an appropriate solution method is\ncritical to ensuring reliable and economical operation of power generation,\ntransmission, and distribution networks. While the conventional loop method may\nbe used in small-scale systems, it is limited by its reliance on\nimpedance-based load data and its inability to scale to complex networks. In\ncontrast, iterative techniques such as the Gauss-Seidel (GS) and Newton-Raphson\n(NR) methods are better suited for analyzing large systems. Of these, the NR\nmethod offers significant advantages due to its quadratic convergence and\nimproved numerical stability. This study presents a power flow analysis of a\n5-bus system using the Newton-Raphson approach. The system was modeled and\nsimulated in PowerWorld Simulator (PWS), and a custom MATLAB implementation was\ndeveloped to verify the results under a base case scenario. The comparative\nanalysis demonstrates that the NR method provides accurate and robust solutions\nfor power flow problems, making it well-suited for evaluating system\nperformance under various operating conditions.", "AI": {"tldr": "\u8d1f\u8f7d\u6d41\u5206\u6790\u662f\u7535\u529b\u5de5\u7a0b\u5e08\u7528\u4e8e\u6a21\u62df\u548c\u8bc4\u4f30\u7a33\u6001\u6761\u4ef6\u4e0b\u7535\u529b\u7cfb\u7edf\u884c\u4e3a\u7684\u57fa\u672c\u6280\u672f\u3002\u9009\u62e9\u5408\u9002\u7684\u6c42\u89e3\u65b9\u6cd5\u5bf9\u786e\u4fdd\u7535\u529b\u7cfb\u7edf\u7684\u53ef\u9760\u548c\u7ecf\u6d4e\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u725b\u987f-\u62c9\u592b\u900a\u65b9\u6cd5\u56e0\u5176\u4e8c\u6b21\u6536\u655b\u6027\u548c\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5c55\u793a\u725b\u987f-\u62c9\u592b\u900a\u65b9\u6cd5\u5728\u7535\u529b\u7cfb\u7edf\u8d1f\u8f7d\u6d41\u5206\u6790\u4e2d\u7684\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u725b\u987f-\u62c9\u592b\u900a\u65b9\u6cd5\u5bf95\u603b\u7ebf\u7cfb\u7edf\u8fdb\u884c\u8d1f\u8f7d\u6d41\u5206\u6790\uff0c\u5e76\u4f7f\u7528PowerWorld Simulator\u548c\u81ea\u5b9a\u4e49MATLAB\u5b9e\u73b0\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u725b\u987f-\u62c9\u592b\u900a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u8fd0\u884c\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u725b\u987f-\u62c9\u592b\u900a\u65b9\u6cd5\u56e0\u5176\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u662f\u7535\u529b\u7cfb\u7edf\u8d1f\u8f7d\u6d41\u5206\u6790\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2506.22777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22777", "abs": "https://arxiv.org/abs/2506.22777", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "comment": null, "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVFT\u7684\u9884RL\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u660e\u786e\u627f\u8ba4\u63d0\u793a\u7ebf\u7d22\u7684\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u7684\u68c0\u6d4b\u7387\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728RL\u8bad\u7ec3\u4e2d\u53ef\u80fd\u5229\u7528\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff08\u5373\u901a\u8fc7\u975e\u9884\u671f\u7b56\u7565\u83b7\u53d6\u9ad8\u5956\u52b1\uff09\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u8fd9\u79cd\u884c\u4e3a\u96be\u4ee5\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u68c0\u6d4b\u3002", "method": "\u63d0\u51faVFT\u65b9\u6cd5\uff0c\u5728RL\u8bad\u7ec3\u524d\u5bf9\u6a21\u578b\u8fdb\u884c\u5e72\u9884\uff0c\u8bad\u7ec3\u5176\u660e\u786e\u627f\u8ba4\u63d0\u793a\u7ebf\u7d22\u7684\u5f71\u54cd\u3002\u968f\u540e\u5728RL\u73af\u5883\u4e2d\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u5229\u7528\u7ebf\u7d22\u8fdb\u884c\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u3002", "result": "VFT\u8bad\u7ec3\u7684\u6a21\u578b\u5728RL\u540e\u4ec5\u67096%\u7684\u672a\u68c0\u6d4b\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u800c\u672a\u7ecfVFT\u7684\u6a21\u578b\u9ad8\u8fbe88%\u3002VFT\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u7ebf\u7d22\u5f71\u54cd\u7684\u660e\u786e\u627f\u8ba4\u7387\uff08\u4ece8%\u63d0\u5347\u81f394%\uff09\u3002", "conclusion": "VFT\u901a\u8fc7\u63d0\u9ad8\u6a21\u578b\u5bf9\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u7684\u660e\u786e\u627f\u8ba4\uff0c\u663e\u8457\u6539\u5584\u4e86\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u900f\u660e\u548c\u5b89\u5168\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2506.23273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23273", "abs": "https://arxiv.org/abs/2506.23273", "authors": ["Quang Hung Nguyen", "Phuong Anh Trinh", "Phan Quoc Hung Mai", "Tuan Phong Trinh"], "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis", "comment": null, "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.", "AI": {"tldr": "FinStat2SQL\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684text2sql\u7ba1\u9053\uff0c\u4e13\u4e3a\u91d1\u878d\u9886\u57df\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5927\u3001\u5c0f\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u5728\u8d8a\u5357\u4f01\u4e1a\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u62a5\u8868\u683c\u5f0f\u5dee\u5f02\u5927\uff0c\u4f20\u7edftext2sql\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u548c\u7279\u5b9a\u9886\u57df\u7684\u67e5\u8be2\u9700\u6c42\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\uff0c\u7ed3\u5408\u5927\u3001\u5c0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u4f53\u63d0\u53d6\u3001SQL\u751f\u6210\u548c\u81ea\u6211\u4fee\u6b63\uff0c\u5e76\u9488\u5bf9\u672c\u5730\u6807\u51c6\uff08\u5982VAS\uff09\u4f18\u5316\u3002", "result": "7B\u5fae\u8c03\u6a21\u578b\u5728\u5408\u6210QA\u6570\u636e\u96c6\u4e0a\u8fbe\u523061.33%\u51c6\u786e\u7387\uff0c\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e4\u79d2\uff0c\u4f18\u4e8eGPT-4o-mini\u3002", "conclusion": "FinStat2SQL\u4e3a\u8d8a\u5357\u4f01\u4e1a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6210\u672c\u9ad8\u6548\u7684\u91d1\u878d\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22562", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22562", "abs": "https://arxiv.org/abs/2506.22562", "authors": ["Abhineet Singh", "Nilanjan Ray"], "title": "Improving Token-based Object Detection with Video", "comment": "Under review for publication in IEEE Access", "summary": "This paper improves upon the Pix2Seq object detector by extending it for\nvideos. In the process, it introduces a new way to perform end-to-end video\nobject detection that improves upon existing video detectors in two key ways.\nFirst, by representing objects as variable-length sequences of discrete tokens,\nwe can succinctly represent widely varying numbers of video objects, with\ndiverse shapes and locations, without having to inject any localization cues in\nthe training process. This eliminates the need to sample the space of all\npossible boxes that constrains conventional detectors and thus solves the dual\nproblems of loss sparsity during training and heuristics-based postprocessing\nduring inference. Second, it conceptualizes and outputs the video objects as\nfully integrated and indivisible 3D boxes or tracklets instead of generating\nimage-specific 2D boxes and linking these boxes together to construct the video\nobject, as done in most conventional detectors. This allows it to scale\neffortlessly with available computational resources by simply increasing the\nlength of the video subsequence that the network takes as input, even\ngeneralizing to multi-object tracking if the subsequence can span the entire\nvideo. We compare our video detector with the baseline Pix2Seq static detector\non several datasets and demonstrate consistent improvement, although with\nstrong signs of being bottlenecked by our limited computational resources. We\nalso compare it with several video detectors on UA-DETRAC to show that it is\ncompetitive with the current state of the art even with the computational\nbottleneck. We make our code and models publicly available.", "AI": {"tldr": "\u8bba\u6587\u6539\u8fdb\u4e86Pix2Seq\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5c06\u5176\u6269\u5c55\u5230\u89c6\u9891\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u6d4b\u5668\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u7a00\u758f\u6027\u548c\u540e\u5904\u7406\u542f\u53d1\u5f0f\u95ee\u9898\uff0c\u4e14\u901a\u5e38\u901a\u8fc7\u94fe\u63a52D\u6846\u6784\u5efa\u89c6\u9891\u5bf9\u8c61\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u5bf9\u8c61\u8868\u793a\u4e3a\u79bb\u6563\u6807\u8bb0\u7684\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\uff0c\u65e0\u9700\u5b9a\u4f4d\u7ebf\u7d22\u8bad\u7ec3\uff1b\u5c06\u89c6\u9891\u5bf9\u8c61\u89c6\u4e3a\u4e0d\u53ef\u5206\u5272\u76843D\u6846\u6216\u8f68\u8ff9\uff0c\u800c\u975e\u94fe\u63a52D\u6846\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8ePix2Seq\u9759\u6001\u68c0\u6d4b\u5668\uff0c\u5e76\u5728UA-DETRAC\u4e0a\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\uff0c\u5c3d\u7ba1\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u662f\u4e3b\u8981\u74f6\u9888\u3002"}}
{"id": "2506.23334", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23334", "abs": "https://arxiv.org/abs/2506.23334", "authors": ["Hongyi Pan", "Ziliang Hong", "Gorkem Durak", "Ziyue Xu", "Ulas Bagci"], "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation", "comment": null, "summary": "Federated learning (FL) has emerged as a promising paradigm for\ncollaboratively training deep learning models across institutions without\nexchanging sensitive medical data. However, its effectiveness is often hindered\nby limited data availability and non-independent, identically distributed data\nacross participating clients, which can degrade model performance and\ngeneralization. To address these challenges, we propose a generative AI based\ndata augmentation framework that integrates synthetic image sharing into the\nfederated training process for breast cancer diagnosis via ultrasound images.\nSpecifically, we train two simple class-specific Deep Convolutional Generative\nAdversarial Networks: one for benign and one for malignant lesions. We then\nsimulate a realistic FL setting using three publicly available breast\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\nadopted as baseline FL algorithms. Experimental results show that incorporating\na suitable number of synthetic images improved the average AUC from 0.9206 to\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\nexcessive use of synthetic data reduced performance, underscoring the\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\nfindings highlight the potential of generative AI based data augmentation to\nenhance FL results in the breast ultrasound image classification task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210AI\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u5171\u4eab\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u5728\u4e73\u817a\u764c\u8d85\u58f0\u56fe\u50cf\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u533b\u7597\u6570\u636e\u5171\u4eab\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u4e0d\u8db3\u548c\u975e\u72ec\u7acb\u540c\u5206\u5e03\u95ee\u9898\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "\u4f7f\u7528\u4e24\u7c7b\u7279\u5b9a\u7684\u6df1\u5ea6\u5377\u79ef\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08DCGAN\uff09\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u91cf\u5408\u6210\u56fe\u50cf\u5c06FedAvg\u7684AUC\u4ece0.9206\u63d0\u5347\u81f30.9237\uff0cFedProx\u4ece0.9429\u63d0\u5347\u81f30.9538\u3002", "conclusion": "\u751f\u6210AI\u6570\u636e\u589e\u5f3a\u80fd\u6709\u6548\u63d0\u5347FL\u6027\u80fd\uff0c\u4f46\u9700\u5e73\u8861\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u7684\u6bd4\u4f8b\u3002"}}
{"id": "2506.23326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23326", "abs": "https://arxiv.org/abs/2506.23326", "authors": ["Sang-Yoep Lee", "Leonardo Zamora Yanez", "Jacob Rogatinsky", "Vi T. Vo", "Tanvi Shingade", "Tommaso Ranzani"], "title": "Simplifying Data-Driven Modeling of the Volume-Flow-Pressure Relationship in Hydraulic Soft Robotic Actuators", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Soft robotic systems are known for their flexibility and adaptability, but\ntraditional physics-based models struggle to capture their complex, nonlinear\nbehaviors. This study explores a data-driven approach to modeling the\nvolume-flow-pressure relationship in hydraulic soft actuators, focusing on\nlow-complexity models with high accuracy. We perform regression analysis on a\nstacked balloon actuator system using exponential, polynomial, and neural\nnetwork models with or without autoregressive inputs. The results demonstrate\nthat simpler models, particularly multivariate polynomials, effectively predict\npressure dynamics with fewer parameters. This research offers a practical\nsolution for real-time soft robotics applications, balancing model complexity\nand computational efficiency. Moreover, the approach may benefit various\ntechniques that require explicit analytical models.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u6db2\u538b\u8f6f\u6267\u884c\u5668\u7684\u4f53\u79ef-\u6d41\u91cf-\u538b\u529b\u5173\u7cfb\uff0c\u91cd\u70b9\u5728\u4e8e\u4f4e\u590d\u6742\u5ea6\u9ad8\u7cbe\u5ea6\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u56de\u5f52\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u6307\u6570\u3001\u591a\u9879\u5f0f\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u542b\u6216\u4e0d\u542b\u81ea\u56de\u5f52\u8f93\u5165\uff09\u5728\u5806\u53e0\u6c14\u7403\u6267\u884c\u5668\u7cfb\u7edf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u7b80\u5355\u7684\u6a21\u578b\uff08\u5c24\u5176\u662f\u591a\u5143\u591a\u9879\u5f0f\uff09\u80fd\u4ee5\u8f83\u5c11\u53c2\u6570\u6709\u6548\u9884\u6d4b\u538b\u529b\u52a8\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u65f6\u8f6f\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u663e\u5f0f\u89e3\u6790\u6a21\u578b\u7684\u6280\u672f\u3002"}}
{"id": "2506.23509", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23509", "abs": "https://arxiv.org/abs/2506.23509", "authors": ["Rahman Khorramfar", "Dharik Mallapragada", "Saurabh Amin"], "title": "Power-Gas Infrastructure Planning under Weather-induced Supply and Demand Uncertainties", "comment": null, "summary": "Implementing economy-wide decarbonization strategies based on decarbonizing\nthe power grid via variable renewable energy (VRE) expansion and\nelectrification of end-uses requires new approaches for energy infrastructure\nplanning that consider, among other factors, weather-induced uncertainty in\ndemand and VRE supply. An energy planning model that fails to account for these\nuncertainties can hinder the intended transition efforts to a low-carbon grid\nand increase the risk of supply shortage especially during extreme weather\nconditions. Here, we consider the generation and transmission expansion problem\nof joint power-gas infrastructure and operations planning under the uncertainty\nof both demand and renewable supply. We propose two distributionally robust\noptimization approaches based on moment (MDRO) and Wasserstein distance (WDRO)\nambiguity sets to endogenize these uncertainties and account for the change in\nthe underlying distribution of these parameters that is caused by the climate\nchange, among other factors. Furthermore, our model considers the risk-aversion\nof the energy planners in the modeling framework via the conditional\nvalue-at-risk (CVaR) metric. An equivalent mixed-integer linear programming\n(MILP) reformulation of both modeling frameworks is presented, and a\ncomputationally efficient approximation scheme to obtain near-optimal solutions\nis proposed. We demonstrate the resulting DRO planning models and solution\nstrategy via a New England case study under different levels of end-use\nelectrification and decarbonization targets. Our experiments systematically\nexplore different modeling aspects and compare the DRO models with stochastic\nprogramming (SP) results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u65b9\u6cd5\uff08MDRO\u548cWDRO\uff09\uff0c\u7528\u4e8e\u7535\u529b-\u5929\u7136\u6c14\u8054\u5408\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\uff0c\u8003\u8651\u9700\u6c42\u548c\u53ef\u518d\u751f\u80fd\u6e90\u4f9b\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165CVaR\u5ea6\u91cf\u4ee5\u53cd\u6620\u89c4\u5212\u8005\u7684\u98ce\u9669\u89c4\u907f\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u57fa\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u548c\u7535\u6c14\u5316\u7684\u8131\u78b3\u76ee\u6807\uff0c\u9700\u8003\u8651\u5929\u6c14\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5e94\u5bf9\u6781\u7aef\u5929\u6c14\u5bfc\u81f4\u7684\u4f9b\u5e94\u77ed\u7f3a\u98ce\u9669\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u77e9\uff08MDRO\uff09\u548cWasserstein\u8ddd\u79bb\uff08WDRO\uff09\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408CVaR\u5ea6\u91cf\u98ce\u9669\u89c4\u907f\uff0c\u5e76\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6c42\u89e3\u3002", "result": "\u901a\u8fc7\u65b0\u82f1\u683c\u5170\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u4e86DRO\u4e0e\u968f\u673a\u89c4\u5212\uff08SP\uff09\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7535\u6c14\u5316\u548c\u8131\u78b3\u76ee\u6807\u4e0b\u7684\u89c4\u5212\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684DRO\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u4f4e\u78b3\u7535\u7f51\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.22791", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22791", "abs": "https://arxiv.org/abs/2506.22791", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "AI": {"tldr": "ContextCache\u662f\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u68c0\u7d22\u67b6\u6784\u4f18\u5316\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u611f\u77e5\uff0c\u5bfc\u81f4\u76f8\u4f3c\u67e5\u8be2\u5728\u4e0d\u540c\u5bf9\u8bdd\u573a\u666f\u4e2d\u51fa\u73b0\u9519\u8bef\u7684\u7f13\u5b58\u547d\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u67b6\u6784\uff1a\u9996\u5148\u57fa\u4e8e\u5411\u91cf\u68c0\u7d22\u5f53\u524d\u67e5\u8be2\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5f53\u524d\u548c\u5386\u53f2\u5bf9\u8bdd\u8868\u793a\u8fdb\u884c\u7cbe\u786e\u5339\u914d\u3002", "result": "ContextCache\u5728\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u7f13\u5b58\u54cd\u5e94\u5ef6\u8fdf\u6bd4\u76f4\u63a5\u8c03\u7528LLM\u4f4e\u7ea610\u500d\u3002", "conclusion": "ContextCache\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8bed\u4e49\u7f13\u5b58\u6548\u7387\uff0c\u4e3aLLM\u5bf9\u8bdd\u5e94\u7528\u8282\u7701\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.23276", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23276", "abs": "https://arxiv.org/abs/2506.23276", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5408\u4f5c\u884c\u4e3a\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u516c\u5171\u7269\u54c1\u535a\u5f08\u4e2d\u8868\u73b0\u51fa\u56db\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u53cd\u800c\u4e0d\u64c5\u957f\u5408\u4f5c\u3002", "motivation": "\u7406\u89e3LLMs\u5728\u81ea\u4e3b\u4ee3\u7406\u4e2d\u7684\u5408\u4f5c\u673a\u5236\uff0c\u786e\u4fdd\u5176\u90e8\u7f72\u7684\u5b89\u5168\u6027\u548c\u7a33\u5065\u6027\u3002", "method": "\u901a\u8fc7\u516c\u5171\u7269\u54c1\u535a\u5f08\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u4e0d\u540cLLMs\u5728\u91cd\u590d\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u56db\u79cd\u884c\u4e3a\u6a21\u5f0f\uff0c\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5408\u4f5c\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5f53\u524d\u63d0\u5347LLMs\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u672a\u5fc5\u80fd\u4fc3\u8fdb\u5408\u4f5c\uff0c\u5bf9\u9700\u8981\u534f\u4f5c\u7684\u73af\u5883\u90e8\u7f72\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2506.22567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22567", "abs": "https://arxiv.org/abs/2506.22567", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "comment": null, "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "AI": {"tldr": "MMKD-CLIP\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u6784\u5efa\u9ad8\u6027\u80fd\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u6570\u636e\uff0c\u4e14\u6570\u636e\u6807\u51c6\u548c\u6a21\u6001\u591a\u6837\uff0c\u963b\u788d\u4e86\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728290\u4e07\u751f\u7269\u533b\u5b66\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4ece9\u4e2a\u6559\u5e08\u6a21\u578b\u4e2d\u84b8\u998f1920\u4e07\u7279\u5f81\u5bf9\u3002", "result": "\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d61080\u4e07\u56fe\u50cf\u548c9\u79cd\u6a21\u6001\uff0cMMKD-CLIP\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u662f\u6784\u5efa\u9ad8\u6027\u80fd\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.23466", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2506.23466", "abs": "https://arxiv.org/abs/2506.23466", "authors": ["Qiqing Liu", "Guoquan Wei", "Zekun Zhou", "Yiyang Wen", "Liu Shi", "Qiegen Liu"], "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction", "comment": "11pages, 11 figures", "summary": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers\nfrom image artifacts and loss of detail due to quantum and electronic noise,\npotentially impacting diagnostic accuracy. Transformer combined with diffusion\nmodels has been a promising approach for image generation. Nevertheless,\nexisting methods exhibit limitations in preserving finegrained image details.\nTo address this issue, frequency domain-directed diffusion transformer (FD-DiT)\nis proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy\nthat progressively introduces noise until the distribution statistically aligns\nwith that of LDCT data, followed by denoising processing. Furthermore, we\nemploy a frequency decoupling technique to concentrate noise primarily in\nhigh-frequency domain, thereby facilitating effective capture of essential\nanatomical structures and fine details. A hybrid denoising network is then\nutilized to optimize the overall data reconstruction process. To enhance the\ncapability in recognizing high-frequency noise, we incorporate sliding sparse\nlocal attention to leverage the sparsity and locality of shallow-layer\ninformation, propagating them via skip connections for improving feature\nrepresentation. Finally, we propose a learnable dynamic fusion strategy for\noptimal component integration. Experimental results demonstrate that at\nidentical dose levels, LDCT images reconstructed by FD-DiT exhibit superior\nnoise and artifact suppression compared to state-of-the-art methods.", "AI": {"tldr": "FD-DiT\u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u57df\u5bfc\u5411\u7684\u6269\u6563\u53d8\u6362\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u5242\u91cfCT\u56fe\u50cf\u91cd\u5efa\uff0c\u901a\u8fc7\u566a\u58f0\u9010\u6b65\u5f15\u5165\u548c\u53bb\u566a\u5904\u7406\uff0c\u7ed3\u5408\u9891\u7387\u89e3\u8026\u6280\u672f\u548c\u6df7\u5408\u53bb\u566a\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u867d\u51cf\u5c11\u8f90\u5c04\uff0c\u4f46\u56fe\u50cf\u566a\u58f0\u548c\u4f2a\u5f71\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFD-DiT\uff0c\u91c7\u7528\u6269\u6563\u7b56\u7565\u548c\u9891\u7387\u89e3\u8026\u6280\u672f\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a00\u758f\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u52a8\u6001\u878d\u5408\u7b56\u7565\uff0c\u4f18\u5316\u56fe\u50cf\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFD-DiT\u5728\u76f8\u540c\u5242\u91cf\u4e0b\uff0c\u566a\u58f0\u548c\u4f2a\u5f71\u6291\u5236\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FD-DiT\u901a\u8fc7\u9891\u7387\u57df\u5bfc\u5411\u548c\u52a8\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86LDCT\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2506.23333", "categories": ["cs.RO", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.23333", "abs": "https://arxiv.org/abs/2506.23333", "authors": ["Javier Garcia", "Jonas Friemel", "Ramin Kosfeld", "Michael Yannuzzi", "Peter Kramer", "Christian Rieck", "Christian Scheffer", "Arne Schmidt", "Harm Kube", "Dan Biediger", "S\u00e1ndor P. Fekete", "Aaron T. Becker"], "title": "Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks", "comment": "8 pages, 12 figures. To appear in the proceedings of the 2025 IEEE\n  21st International Conference on Automation Science and Engineering (CASE\n  2025)", "summary": "We implement and evaluate different methods for the reconfiguration of a\nconnected arrangement of tiles into a desired target shape, using a single\nactive robot that can move along the tile structure. This robot can pick up,\ncarry, or drop off one tile at a time, but it must maintain a single connected\nconfiguration at all times.\n  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms\nas canonical intermediate configurations, guaranteeing performance within a\nconstant factor of the optimal solution if the start and target configuration\nare well-separated. We implement and evaluate this algorithm, both in a\nsimulated and practical setting, using an inchworm type robot to compare it\nwith two existing heuristic algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u7528\u5355\u4e2a\u673a\u5668\u4eba\u91cd\u65b0\u914d\u7f6e\u8fde\u63a5\u7684\u74e6\u7247\u7ed3\u6784\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u7b97\u6cd5\u4e0e\u4e24\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5728\u4fdd\u6301\u8fde\u63a5\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u5730\u5c06\u74e6\u7247\u7ed3\u6784\u91cd\u65b0\u914d\u7f6e\u4e3a\u76ee\u6807\u5f62\u72b6\u3002", "method": "\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86Becker\u7b49\u4eba\u7684\u76f4\u65b9\u56fe\u7b97\u6cd5\uff0c\u5e76\u4e0e\u4e24\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u76f4\u65b9\u56fe\u7b97\u6cd5\u5728\u8d77\u59cb\u548c\u76ee\u6807\u914d\u7f6e\u5206\u79bb\u826f\u597d\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u89e3\u3002", "conclusion": "\u76f4\u65b9\u56fe\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u74e6\u7247\u91cd\u914d\u7f6e\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23554", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23554", "abs": "https://arxiv.org/abs/2506.23554", "authors": ["Shiu Mochiyama", "Ryo Takahashi", "Yoshihiko Susuki"], "title": "A Bidirectional Power Router for Traceable Multi-energy Management", "comment": null, "summary": "To address challenges in improving self-consumption of renewables and\nresilience in local residential power systems, the earlier work of the authors\nintroduced a novel multi-energy management concept, integrating bidirectional\npower routing and electricity-hydrogen conversion. This paper focuses on an\nexperimental verification of the bidirectional power router based on\nline-switching, the essential hardware to realize the concept. The primary\ncontribution is the validation of the router's capability to handle dynamic\nchange of bidirectional power flow. Furthermore, to achieve bidirectional power\nrouting without affecting the smooth and stable operation of the power system,\na novel algorithm for router's switching is designed based on power flow\nmonitoring. The effectiveness of the proposed method is demonstrated through an\nexperiment using a setup with a commercially available stationary battery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u8def\u5207\u6362\u7684\u53cc\u5411\u7535\u529b\u8def\u7531\u5668\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7528\u4e8e\u63d0\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u81ea\u6d88\u7eb3\u548c\u672c\u5730\u4f4f\u5b85\u7535\u529b\u7cfb\u7edf\u7684\u97e7\u6027\u3002", "motivation": "\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u81ea\u6d88\u7eb3\u548c\u7535\u529b\u7cfb\u7edf\u97e7\u6027\u7684\u6311\u6218\uff0c\u9a8c\u8bc1\u53cc\u5411\u7535\u529b\u8def\u7531\u5668\u7684\u52a8\u6001\u529f\u7387\u6d41\u5904\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u529f\u7387\u6d41\u76d1\u6d4b\u7684\u8def\u7531\u5668\u5207\u6362\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5546\u7528\u56fa\u5b9a\u7535\u6c60\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8def\u7531\u5668\u5904\u7406\u52a8\u6001\u53cc\u5411\u529f\u7387\u6d41\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u5411\u7535\u529b\u8def\u7531\u5668\u53ca\u5176\u5207\u6362\u7b97\u6cd5\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u8fd0\u884c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u52a8\u6001\u529f\u7387\u6d41\u7ba1\u7406\u3002"}}
{"id": "2506.22808", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22808", "abs": "https://arxiv.org/abs/2506.22808", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "comment": "20 pages", "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MedEthicsQA\uff0c\u4e00\u4e2a\u5305\u542b5,623\u9053\u9009\u62e9\u9898\u548c5,351\u9053\u5f00\u653e\u9898\u7684\u533b\u7597\u4f26\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f26\u7406\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4f26\u7406\u5b89\u5168\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u6574\u5408\u5168\u7403\u533b\u7597\u4f26\u7406\u6807\u51c6\uff0c\u6784\u5efa\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u7ed3\u5408\u6743\u5a01\u6570\u636e\u96c6\u548cPubMed\u6587\u732e\u573a\u666f\uff0c\u5e76\u7ecf\u8fc7\u591a\u9636\u6bb5\u8fc7\u6ee4\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u786e\u4fdd\u6570\u636e\u53ef\u9760\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4f26\u7406\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u4f26\u7406\u5bf9\u9f50\u7684\u4e0d\u8db3\u3002", "conclusion": "MedEthicsQA\u4e3a\u533b\u7597\u4f26\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4f26\u7406\u7f3a\u9677\uff0c\u4fc3\u8fdb\u672a\u6765\u6539\u8fdb\u3002"}}
{"id": "2506.23306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23306", "abs": "https://arxiv.org/abs/2506.23306", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "title": "GATSim: Urban Mobility Simulation with Generative Agents", "comment": null, "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "AI": {"tldr": "GATSim\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cAI\u4ee3\u7406\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u6846\u67b6\uff0c\u751f\u6210\u5177\u6709\u4e30\u5bcc\u884c\u4e3a\u7279\u5f81\u7684\u4ee3\u7406\uff0c\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u7684\u65c5\u884c\u51b3\u7b56\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u4ee3\u7406\u6a21\u62df\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u65c5\u884c\u51b3\u7b56\u7684\u590d\u6742\u6027\u3001\u9002\u5e94\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u62df\u65b9\u6cd5\u3002", "method": "GATSim\u7ed3\u5408\u57ce\u5e02\u4ea4\u901a\u57fa\u7840\u6a21\u578b\u3001\u4ee3\u7406\u8ba4\u77e5\u7cfb\u7edf\u548c\u4ea4\u901a\u6a21\u62df\u73af\u5883\uff0c\u901a\u8fc7\u5fc3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u3001\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u7ec8\u8eab\u5b66\u4e60\u673a\u5236\uff0c\u751f\u6210\u5177\u6709\u591a\u6837\u793e\u4f1a\u7ecf\u6d4e\u5c5e\u6027\u548c\u504f\u597d\u7684\u4ee3\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGATSim\u4ee3\u7406\u5728\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u76f8\u5f53\uff0c\u5e76\u80fd\u81ea\u7136\u751f\u6210\u5b8f\u89c2\u4ea4\u901a\u6f14\u5316\u6a21\u5f0f\u3002", "conclusion": "GATSim\u4e3a\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.22570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22570", "abs": "https://arxiv.org/abs/2506.22570", "authors": ["Chee Mei Ling", "Thangarajah Akilan", "Aparna Ravinda Phalke"], "title": "Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation", "comment": "17 pages, 7 figures, 6 tables", "summary": "Agricultural image semantic segmentation is a pivotal component of modern\nagriculture, facilitating accurate visual data analysis to improve crop\nmanagement, optimize resource utilization, and boost overall productivity. This\nstudy proposes an efficient image segmentation method for precision\nagriculture, focusing on accurately delineating farmland anomalies to support\ninformed decision-making and proactive interventions. A novel Dual Atrous\nSeparable Convolution (DAS Conv) module is integrated within the\nDeepLabV3-based segmentation framework. The DAS Conv module is meticulously\ndesigned to achieve an optimal balance between dilation rates and padding size,\nthereby enhancing model performance without compromising efficiency. The study\nalso incorporates a strategic skip connection from an optimal stage in the\nencoder to the decoder to bolster the model's capacity to capture fine-grained\nspatial features. Despite its lower computational complexity, the proposed\nmodel outperforms its baseline and achieves performance comparable to highly\ncomplex transformer-based state-of-the-art (SOTA) models on the Agriculture\nVision benchmark dataset. It achieves more than 66% improvement in efficiency\nwhen considering the trade-off between model complexity and performance,\ncompared to the SOTA model. This study highlights an efficient and effective\nsolution for improving semantic segmentation in remote sensing applications,\noffering a computationally lightweight model capable of high-quality\nperformance in agricultural imagery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDeepLabV3\u7684\u9ad8\u6548\u519c\u4e1a\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165DAS Conv\u6a21\u5757\u548c\u4f18\u5316\u8df3\u8fde\u7ed3\u6784\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u6027\u80fd\u63a5\u8fd1\u590d\u6742Transformer\u6a21\u578b\u3002", "motivation": "\u519c\u4e1a\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u5bf9\u7cbe\u51c6\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6027\u80fd\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u96c6\u6210DAS Conv\u6a21\u5757\u4f18\u5316\u6269\u5f20\u7387\u548c\u586b\u5145\u5c3a\u5bf8\uff0c\u5e76\u8bbe\u8ba1\u7b56\u7565\u6027\u8df3\u8fde\u7ed3\u6784\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002", "result": "\u5728Agriculture Vision\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63a5\u8fd1SOTA\u6a21\u578b\uff0c\u6548\u7387\u63d0\u534766%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u519c\u4e1a\u9065\u611f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u9ad8\u6027\u80fd\u7684\u8bed\u4e49\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23490", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23490", "abs": "https://arxiv.org/abs/2506.23490", "authors": ["Junxuan Yu", "Yaofei Duan", "Yuhao Huang", "Yu Wang", "Rongbo Ling", "Weihao Luo", "Ang Zhang", "Jingxian Xu", "Qiongying Ni", "Yongsong Zhou", "Binghan Li", "Haoran Dou", "Liping Liu", "Yanfen Chu", "Feng Geng", "Zhe Sheng", "Zhifeng Ding", "Dingxin Zhang", "Rui Huang", "Yuhang Zhang", "Xiaowei Xu", "Tao Tan", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound", "comment": "accepted by miccai 2025", "summary": "Echocardiography is routine for cardiac examination. However, 2D ultrasound\n(US) struggles with accurate metric calculation and direct observation of 3D\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\nof view and scarce availability in practice. Constructing the cardiac\nanatomical twin from 2D images is promising to provide precise treatment\nplanning and clinical quantification. However, it remains challenging due to\nthe rare paired data, complex structures, and US noises. In this study, we\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\npioneered the construction of a real-world and high-quality dataset containing\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\noptimization. Last, we introduce an implicit autoencoder for topology-aware\nconstraints. Extensive experiments show that UltraTwin reconstructs\nhigh-quality anatomical twins versus strong competitors. We believe it advances\nanatomical twin modeling for potential applications in personalized cardiac\ncare.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUltraTwin\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u89d22D\u8d85\u58f0\u56fe\u50cf\u6784\u5efa\u5fc3\u810f\u89e3\u5256\u5b6a\u751f\u4f53\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u7ed3\u6784\u590d\u6742\u548c\u8d85\u58f0\u566a\u58f0\u7b49\u6311\u6218\u3002", "motivation": "2D\u8d85\u58f0\u5728\u7cbe\u786e\u8ba1\u7b97\u548c\u76f4\u63a5\u89c2\u5bdf3D\u5fc3\u810f\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c3D\u8d85\u58f0\u53c8\u53d7\u9650\u4e8e\u4f4e\u5206\u8fa8\u7387\u3001\u5c0f\u89c6\u91ce\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a00\u7f3a\u6027\u3002\u6784\u5efa\u5fc3\u810f\u89e3\u5256\u5b6a\u751f\u4f53\u6709\u671b\u63d0\u4f9b\u7cbe\u786e\u7684\u6cbb\u7597\u89c4\u5212\u548c\u4e34\u5e8a\u91cf\u5316\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e25\u683c\u914d\u5bf9\u7684\u591a\u89c6\u89d22D\u8d85\u58f0\u548cCT\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u5206\u5c42\u91cd\u5efa\u4f18\u5316\u65b9\u6848\uff1b3. \u5f15\u5165\u4e86\u9690\u5f0f\u81ea\u7f16\u7801\u5668\u4ee5\u5b9e\u73b0\u62d3\u6251\u611f\u77e5\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUltraTwin\u80fd\u591f\u91cd\u5efa\u9ad8\u8d28\u91cf\u7684\u89e3\u5256\u5b6a\u751f\u4f53\uff0c\u4f18\u4e8e\u5176\u4ed6\u5f3a\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "UltraTwin\u63a8\u52a8\u4e86\u5fc3\u810f\u89e3\u5256\u5b6a\u751f\u4f53\u5efa\u6a21\u7684\u53d1\u5c55\uff0c\u4e3a\u4e2a\u6027\u5316\u5fc3\u810f\u62a4\u7406\u7684\u6f5c\u5728\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.23346", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23346", "abs": "https://arxiv.org/abs/2506.23346", "authors": ["Hao Wang", "Armand Jordana", "Ludovic Righetti", "Somil Bansal"], "title": "Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis", "comment": "RSS 2025 Workshop on Reliable Robotics", "summary": "While we have made significant algorithmic developments to enable autonomous\nsystems to perform sophisticated tasks, it remains difficult for them to\nperform tasks effective and safely. Most existing approaches either fail to\nprovide any safety assurances or substantially compromise task performance for\nsafety. In this work, we develop a framework, based on model predictive control\n(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for\nautonomous systems while respecting the safety constraints. Our framework\nguarantees recursive feasibility for the MPC controller, and it is scalable to\nhigh-dimensional systems. We demonstrate the effectiveness of our framework\nwith two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa\nmanipulator, and the experiments show that our framework significantly improves\nthe safety constraints satisfaction of the systems over the baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMPC\u548cHJ\u53ef\u8fbe\u6027\u7684\u6846\u67b6\uff0c\u4f18\u5316\u81ea\u4e3b\u7cfb\u7edf\u7684\u4efb\u52a1\u6027\u80fd\u5e76\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u517c\u987e\u6027\u80fd\u4e0e\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548cHamilton-Jacobi\uff08HJ\uff09\u53ef\u8fbe\u6027\u7406\u8bba\uff0c\u786e\u4fdd\u9012\u5f52\u53ef\u884c\u6027\u548c\u9ad8\u7ef4\u6269\u5c55\u6027\u3002", "result": "\u57284D Dubins Car\u548c6\u81ea\u7531\u5ea6Kuka iiwa\u673a\u68b0\u81c2\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u5b89\u5168\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u3002"}}
{"id": "2506.23649", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23649", "abs": "https://arxiv.org/abs/2506.23649", "authors": ["Wenjie Wan", "Han Hu", "Feiyu Chen", "Xiaoyu Liu", "Kequan Zhao"], "title": "Reliability Assessment of Power System Based on the Dichotomy Method", "comment": "10pages, 8figures", "summary": "With a sustainable increase in the scale of power system, the number of\nstates in the state space grows exponentially, and the reliability assessment\nof the power system faces enormous challenges. Traditional state-by-state\nassessment methods, such as state enumeration (SE) and Monte Carlo simulation\n(MCS) methods, have encountered performance bottlenecks in terms of efficiency\nand accuracy. In this paper, the Boolean lattice representation theory of the\nstate space was studied, and a dichotomy method was proposed to efficiently\npartition the state space into some disjoint sub-lattices with a relatively\nsmall number of optimal power flow (OPF) operations. Based on lattice\npartition, the reliability indices of the entire space can be calculated\nlattice-by-lattice. In addition, alone with the partitioning procedure, the\ncalculated loss of load probability (LOLP) monotonically increases and rapidly\ntends to the analytic value with the designated error bound. Moreover, we\ndesigned a customized Monte Carlo sampling method in lattices of interest to\ncompute expected energy not supply (EENS). The experiments are conducted on the\nRBTS and RTS-79 systems. The results show that the proposed method achieves the\nanalytic LOLP of the RBTS system after five hundreds of OPF operations, which\nis about hundreds of times faster than traditional methods, and the designed\nMonte Carlo sampling method converged after thousands of OPF operations on test\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e03\u5c14\u683c\u8868\u793a\u7406\u8bba\u7684\u72b6\u6001\u7a7a\u95f4\u4e8c\u5206\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5212\u5206\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u8bc4\u4f30\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u7535\u529b\u7cfb\u7edf\u89c4\u6a21\u7684\u6269\u5927\uff0c\u4f20\u7edf\u72b6\u6001\u679a\u4e3e\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u9047\u5230\u74f6\u9888\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5e03\u5c14\u683c\u8868\u793a\u7406\u8bba\u5c06\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u4e0d\u76f8\u4ea4\u7684\u5b50\u683c\uff0c\u901a\u8fc7\u6700\u4f18\u6f6e\u6d41\u64cd\u4f5c\u8ba1\u7b97\u53ef\u9760\u6027\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u5b9a\u5236\u5316\u7684\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u3002", "result": "\u5728RBTS\u548cRTS-79\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u6570\u767e\u500d\uff0c\u4e14\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u6536\u655b\u8fc5\u901f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7535\u529b\u7cfb\u7edf\u53ef\u9760\u6027\u8bc4\u4f30\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7cfb\u7edf\u3002"}}
{"id": "2506.22813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22813", "abs": "https://arxiv.org/abs/2506.22813", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "comment": null, "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSaM\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u548c\u5408\u5e76\u4e13\u5bb6\u6a21\u578b\u4ee5\u4f18\u5316\u76ee\u6807\u9886\u57df\u7684\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u7edf\u4e00\u6a21\u578b10%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u9886\u57df\u8bad\u7ec3\u7edf\u4e00\u6a21\u578b\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\uff0c\u6807\u6ce8\u7ec6\u7c92\u5ea6\u6807\u7b7e\u548c\u8bad\u7ec3\u9886\u57df\u7279\u5b9a\u6a21\u578b\u6210\u672c\u9ad8\u3002", "method": "SaM\u6846\u67b6\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u7684\u9886\u57df\u4e13\u5bb6\u6a21\u578b\uff0c\u57fa\u4e8e\u9886\u57df\u76f8\u4f3c\u6027\u548c\u6027\u80fd\u6837\u672c\u5408\u5e76\uff0c\u521b\u5efa\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSaM\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u4f18\u4e8e\u7edf\u4e00\u6a21\u578b10%\uff0c\u4e14\u5177\u6709\u9ad8\u6269\u5c55\u6027\u3002", "conclusion": "SaM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u4e13\u5bb6\u6a21\u578b\u663e\u8457\u63d0\u5347\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u652f\u6301\u7075\u6d3b\u6269\u5c55\u3002"}}
{"id": "2506.23464", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23464", "abs": "https://arxiv.org/abs/2506.23464", "authors": ["Sahil Tripathi", "Md Tabrez Nafis", "Imran Hussain", "Jiechao Gao"], "title": "The Confidence Paradox: Can LLM Know When It's Wrong", "comment": null, "summary": "Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.", "AI": {"tldr": "HonestVQA\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u8bda\u5b9e\u6821\u51c6\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3DocVQA\u7cfb\u7edf\u4e2d\u7684\u4f26\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3001\u5bf9\u9f50\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709DocVQA\u7cfb\u7edf\u5728\u4f26\u7406\u4e0a\u4e0d\u900f\u660e\uff0c\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u6216\u4e0d\u786e\u5b9a\u6027\u4f20\u8fbe\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u9ad8\u98ce\u9669\u9886\u57df\u7684\u4f26\u7406\u8d23\u4efb\u95ee\u9898\u3002", "method": "HonestVQA\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u52a0\u6743\u635f\u5931\u51fd\u6570\u5bf9\u9f50\u7f6e\u4fe1\u5ea6\u4e0e\u6b63\u786e\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5236\u4f26\u7406\u54cd\u5e94\u884c\u4e3a\u3002", "result": "HonestVQA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86DocVQA\u7684\u51c6\u786e\u6027\u548cF1\u5206\u6570\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HonestVQA\u901a\u8fc7\u4f26\u7406\u5bf9\u9f50\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86DocVQA\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u4f26\u7406\u654f\u611f\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22589", "abs": "https://arxiv.org/abs/2506.22589", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "comment": "Accepted at ICDAR2025", "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "AI": {"tldr": "LIGHT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u8a00\u3001\u56fe\u50cf\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u7528\u4e8e\u94fe\u63a5\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u4e0a\u7684\u6587\u672c\u4fe1\u606f\u5bf9\u591a\u5b66\u79d1\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u94fe\u63a5\u6587\u672c\u7247\u6bb5\uff0c\u5c24\u5176\u662f\u591a\u8bcd\u5730\u540d\u3002", "method": "LIGHT\u6574\u5408\u51e0\u4f55\u3001\u89c6\u89c9\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u6a21\u5757\u548c\u591a\u6a21\u6001\u5b66\u4e60\u9884\u6d4b\u6587\u672c\u9605\u8bfb\u987a\u5e8f\u3002", "result": "\u5728ICDAR 2024/2025 MapText\u7ade\u8d5b\u6570\u636e\u4e0a\uff0cLIGHT\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u5386\u53f2\u5730\u56fe\u6587\u672c\u94fe\u63a5\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.23506", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2506.23506", "abs": "https://arxiv.org/abs/2506.23506", "authors": ["Bowen Xin", "Rohan Hickey", "Tamara Blake", "Jin Jin", "Claire E Wainwright", "Thomas Benkert", "Alto Stemmer", "Peter Sly", "David Coman", "Jason Dowling"], "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI", "comment": "Oral presentation in ISMRM2025", "summary": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)\nrepresents a recent breakthrough in lung structure imaging, providing image\nresolution and quality comparable to computed tomography (CT). Due to the\nabsence of ionising radiation, MRI is often preferred over CT in paediatric\ndiseases such as cystic fibrosis (CF), one of the most common genetic disorders\nin Caucasians. To assess structural lung damage in CF imaging, CT scoring\nsystems provide valuable quantitative insights for disease diagnosis and\nprogression. However, few quantitative scoring systems are available in\nstructural lung MRI (e.g., UTE-MRI). To provide fast and accurate\nquantification in lung MRI, we investigated the feasibility of novel Artificial\nintelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring\nconsists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)\nlung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification\nand reporting. The results shows that our APL scoring took 8.2 minutes per\nsubject, which was more than twice as fast as the previous grid-level scoring.\nAdditionally, our pixel-level scoring was statistically more accurate\n(p=0.021), while strongly correlating with grid-level scoring (R=0.973,\np=5.85e-9). This tool has great potential to streamline the workflow of UTE\nlung MRI in clinical settings, and be extended to other structural lung MRI\nsequences (e.g., BLADE MRI), and for other lung diseases (e.g.,\nbronchopulmonary dysplasia).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u50cf\u7d20\u7ea7\u80ba\u8bc4\u5206\uff08APL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u51c6\u786e\u8bc4\u4f30\u56ca\u6027\u7ea4\u7ef4\u5316\uff08CF\uff09\u60a3\u8005\u7684\u80ba\u90e8\u7ed3\u6784\u635f\u4f24\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6bd4\u4f20\u7edf\u7f51\u683c\u7ea7\u8bc4\u5206\u66f4\u5feb\u4e14\u66f4\u51c6\u786e\u3002", "motivation": "\u7531\u4e8eMRI\u65e0\u7535\u79bb\u8f90\u5c04\uff0c\u9002\u7528\u4e8e\u513f\u79d1\u75be\u75c5\u5982\u56ca\u6027\u7ea4\u7ef4\u5316\u7684\u80ba\u90e8\u6210\u50cf\uff0c\u4f46\u7f3a\u4e4f\u5b9a\u91cf\u8bc4\u5206\u7cfb\u7edf\u3002APL\u8bc4\u5206\u7684\u5f00\u53d1\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "APL\u8bc4\u5206\u5305\u62ec\u4e94\u4e2a\u6b65\u9aa4\uff1a\u56fe\u50cf\u52a0\u8f7d\u3001AI\u80ba\u90e8\u5206\u5272\u3001\u80ba\u8fb9\u754c\u5207\u7247\u91c7\u6837\u3001\u50cf\u7d20\u7ea7\u6807\u6ce8\u3001\u91cf\u5316\u4e0e\u62a5\u544a\u3002", "result": "APL\u8bc4\u5206\u8017\u65f68.2\u5206\u949f/\u4f8b\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u4e24\u500d\uff0c\u4e14\u51c6\u786e\u6027\u66f4\u9ad8\uff08p=0.021\uff09\uff0c\u4e0e\u7f51\u683c\u7ea7\u8bc4\u5206\u5f3a\u76f8\u5173\uff08R=0.973\uff09\u3002", "conclusion": "APL\u8bc4\u5206\u6709\u671b\u4f18\u5316\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u6269\u5c55\u81f3\u5176\u4ed6\u80ba\u90e8\u75be\u75c5\u548cMRI\u5e8f\u5217\u3002"}}
{"id": "2506.23351", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23351", "abs": "https://arxiv.org/abs/2506.23351", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RoboTwin\u53cc\u81c2\u534f\u4f5c\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u63a8\u52a8\u53cc\u81c2\u673a\u5668\u4eba\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5438\u5f15\u4e86\u5168\u740364\u652f\u56e2\u961f\u53c2\u4e0e\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u7528\u53cc\u81c2\u7b56\u7565\u5b66\u4e60\u7684\u5173\u952e\u89c1\u89e3\u3002", "motivation": "\u63a8\u52a8\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u7269\u7406\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\uff0c\u7279\u522b\u662f\u53cc\u81c2\u534f\u4f5c\u7cfb\u7edf\u5728\u5904\u7406\u521a\u6027\u3001\u53ef\u53d8\u5f62\u548c\u89e6\u89c9\u654f\u611f\u7269\u4f53\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u57fa\u4e8eRoboTwin\u4eff\u771f\u5e73\u53f0\u548cAgileX COBOT-Magic Robot\u5e73\u53f0\uff0c\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u9636\u6bb5\u7684\u6bd4\u8d5b\uff08\u4e24\u8f6e\u4eff\u771f\u548c\u4e00\u8f6e\u771f\u5b9e\u4e16\u754c\uff09\uff0c\u5305\u542b17\u79cd\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u6311\u6218\u8d5b\u5438\u5f15\u4e8664\u652f\u5168\u7403\u56e2\u961f\u548c400\u591a\u540d\u53c2\u4e0e\u8005\uff0c\u4ea7\u751f\u4e86\u5982SEM\u548cAnchorDP3\u7b49\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u901a\u7528\u53cc\u81c2\u7b56\u7565\u5b66\u4e60\u7684\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u6bd4\u8d5b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u7a33\u5065\u548c\u901a\u7528\u53cc\u81c2\u64cd\u4f5c\u7b56\u7565\u7684\u5b9d\u8d35\u6570\u636e\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.23716", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23716", "abs": "https://arxiv.org/abs/2506.23716", "authors": ["Sahel Vahedi Noori", "Maryam Babazadeh"], "title": "A Data-Ensemble-Based Approach for Sample-Efficient LQ Control of Linear Time-Varying Systems", "comment": null, "summary": "This paper presents a sample-efficient, data-driven control framework for\nfinite-horizon linear quadratic (LQ) control of linear time-varying (LTV)\nsystems. In contrast to the time-invariant case, the time-varying LQ problem\ninvolves a differential Riccati equation (DRE) with time-dependent parameters\nand terminal boundary constraints. We formulate the LQ problem as a nonconvex\noptimization problem and conduct a rigorous analysis of its dual structure. By\nexploiting the inherent convexity of the dual problem and analyzing the KKT\nconditions, we derive an explicit relationship between the optimal dual\nsolution and the parameters of the associated Q-function in time-varying case.\nThis theoretical insight supports the development of a novel, sample-efficient,\nnon-iterative semidefinite programming (SDP) algorithm that directly computes\nthe optimal sequence of feedback gains from an ensemble of input-state data\nsequences without model identification. The resulting convex, data-dependent\nframework provides global optimality guarantees for completely unknown LTV\nsystems. As a special case, the method also applies to finite-horizon LQ\ncontrol of linear time-invariant (LTI) systems. In this setting, a single\ninput-state trajectory suffices to identify the optimal LQ feedback policy,\nimproving significantly over existing Q-learning approaches for finite horizon\nLTI systems that typically require data from multiple episodes. The approach\nprovides a new optimization-based perspective on Q-learning in time-varying\nsettings and contributes to the broader understanding of data-driven control in\nnon-stationary environments. Simulation results show that, compared to recent\nmethods, the proposed approach achieves superior optimality and sample\nefficiency on LTV systems, and indicates potential for stabilizing and optimal\ncontrol of nonlinear systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edf\u7684\u6709\u9650\u65f6\u57df\u7ebf\u6027\u4e8c\u6b21\u63a7\u5236\u3002\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u548c\u53cc\u95ee\u9898\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u975e\u8fed\u4ee3\u534a\u5b9a\u89c4\u5212\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u8ba1\u7b97\u6700\u4f18\u53cd\u9988\u589e\u76ca\u3002", "motivation": "\u89e3\u51b3\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edf\u7684\u6709\u9650\u65f6\u57dfLQ\u63a7\u5236\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5bf9\u6a21\u578b\u8bc6\u522b\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6700\u4f18\u6027\u3002", "method": "\u5c06LQ\u95ee\u9898\u8f6c\u5316\u4e3a\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5206\u6790\u5176\u5bf9\u5076\u7ed3\u6784\uff0c\u5229\u7528KKT\u6761\u4ef6\u63a8\u5bfc\u6700\u4f18\u89e3\u4e0eQ\u51fd\u6570\u53c2\u6570\u7684\u5173\u7cfb\uff0c\u5f00\u53d1\u975e\u8fed\u4ee3SDP\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u76f4\u63a5\u4ece\u6570\u636e\u8ba1\u7b97\u6700\u4f18\u53cd\u9988\u589e\u76ca\uff0c\u9002\u7528\u4e8e\u5b8c\u5168\u672a\u77e5\u7684LTV\u7cfb\u7edf\uff0c\u5e76\u5728LTI\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709Q\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65f6\u53d8\u73af\u5883\u4e0b\u7684\u6570\u636e\u9a71\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4eff\u771f\u663e\u793a\u5176\u5728\u6700\u4f18\u6027\u548c\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.22846", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22846", "abs": "https://arxiv.org/abs/2506.22846", "authors": ["Duygu Altinok"], "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAIL\u7684\u8f85\u52a9\u635f\u5931\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\u589e\u5f3aCTC-based ASR\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3CTC-based ASR\u6a21\u578b\u5728\u8bed\u4e49\u4f9d\u8d56\u5efa\u6a21\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u975e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u9ad8\u6548\u6027\u3002", "method": "\u5728\u4e2d\u95f4\u7f16\u7801\u5c42\u6dfb\u52a0\u8fde\u63a5\u5c42\uff0c\u5c06\u8f93\u51fa\u6620\u5c04\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u8ba1\u7b97\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u635f\u5931\u3002", "result": "\u5728LibriSpeech\u3001TEDLIUM2\u548cWSJ\u8bed\u6599\u5e93\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u8fbe\u5230\u4e86CTC-based ASR\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "LAIL\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86CTC-based ASR\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2506.23503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23503", "abs": "https://arxiv.org/abs/2506.23503", "authors": ["Bosubabu Sambana", "Kondreddygari Archana", "Suram Indhra Sena Reddy", "Shaik Meethaigar Jameer Basha", "Shaik Karishma"], "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence", "comment": "6 Pages, 5 Figures, IEEE IDCIoT 2025", "summary": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the\nirrational thought patterns associated with mental health disorders, but its\neffectiveness relies on accurately identifying cognitive pathways to provide\ntargeted treatment. In today's digital age, individuals often express negative\nemotions on social media, where they may reveal cognitive distortions, and in\nsevere cases, exhibit suicidal tendencies. However, there is a significant gap\nin methodologies designed to analyze these cognitive pathways, which could be\ncritical for psychotherapists aiming to deliver timely and effective\ninterventions in online environments. Cognitive Behavioral Therapy (CBT)\nframework leveraging acceptance, commitment and data augmentation to categorize\nand address both textual and visual content as positive or negative.\nSpecifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,\nPEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages\nfocusing on detecting negative emotions and cognitive distortions within social\nmedia data. While existing models are primarily designed to identify negative\nthoughts, the proposed system goes beyond this by predicting additional\nnegative side effects and other potential mental health disorders likes\nPhobias, Eating Disorders. This enhancement allows for a more comprehensive\nunderstanding and intervention strategy, offering psychotherapists a powerful\ntool for early detection and treatment of various psychological issues.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCBT\u6846\u67b6\u7684\u7cfb\u7edf\uff0c\u5229\u7528BERT\u3001RoBERTa\u7b49\u6a21\u578b\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u8d1f\u9762\u60c5\u7eea\u548c\u8ba4\u77e5\u626d\u66f2\uff0c\u5e76\u9884\u6d4b\u6f5c\u5728\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u8ba4\u77e5\u8def\u5f84\uff0c\u8fd9\u5bf9\u5fc3\u7406\u6cbb\u7597\u5e08\u63d0\u4f9b\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408BERT\u3001RoBERTa\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0cT5\u3001PEGASUS\u8fdb\u884c\u6587\u672c\u6458\u8981\uff0cmT5\u8fdb\u884c\u591a\u8bed\u8a00\u7ffb\u8bd1\uff0c\u4ee5\u68c0\u6d4b\u8d1f\u9762\u60c5\u7eea\u548c\u8ba4\u77e5\u626d\u66f2\u3002", "result": "\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u8bc6\u522b\u8d1f\u9762\u601d\u7ef4\uff0c\u8fd8\u80fd\u9884\u6d4b\u5176\u4ed6\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u5982\u6050\u60e7\u75c7\u548c\u996e\u98df\u969c\u788d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5fc3\u7406\u6cbb\u7597\u5e08\u63d0\u4f9b\u4e86\u65e9\u671f\u68c0\u6d4b\u548c\u5e72\u9884\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u6269\u5c55\u4e86CBT\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.22591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22591", "abs": "https://arxiv.org/abs/2506.22591", "authors": ["Arunkumar Kannan", "Martin A. Lindquist", "Brian Caffo"], "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "comment": "Accepted at MICCAI 2025", "summary": "Recent advances in deep learning have made it possible to predict phenotypic\nmeasures directly from functional magnetic resonance imaging (fMRI) brain\nvolumes, sparking significant interest in the neuroimaging community. However,\nexisting approaches, primarily based on convolutional neural networks or\ntransformer architectures, often struggle to model the complex relationships\ninherent in fMRI data, limited by their inability to capture long-range spatial\nand temporal dependencies. To overcome these shortcomings, we introduce\nBrainMT, a novel hybrid framework designed to efficiently learn and integrate\nlong-range spatiotemporal attributes in fMRI data. Our framework operates in\ntwo stages: (1) a bidirectional Mamba block with a temporal-first scanning\nmechanism to capture global temporal interactions in a computationally\nefficient manner; and (2) a transformer block leveraging self-attention to\nmodel global spatial relationships across the deep features processed by the\nMamba block. Extensive experiments on two large-scale public datasets,\nUKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves\nstate-of-the-art performance on both classification (sex prediction) and\nregression (cognitive intelligence prediction) tasks, outperforming existing\nmethods by a significant margin. Our code and implementation details will be\nmade publicly available at this\nhttps://github.com/arunkumar-kannan/BrainMT-fMRI", "AI": {"tldr": "BrainMT\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u5411Mamba\u5757\u548cTransformer\u5757\uff0c\u6709\u6548\u6355\u6349fMRI\u6570\u636e\u4e2d\u7684\u957f\u8ddd\u79bb\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982CNN\u6216Transformer\uff09\u96be\u4ee5\u5efa\u6a21fMRI\u6570\u636e\u4e2d\u7684\u590d\u6742\u65f6\u7a7a\u5173\u7cfb\uff0cBrainMT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "BrainMT\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u53cc\u5411Mamba\u5757\u6355\u83b7\u5168\u5c40\u65f6\u95f4\u4ea4\u4e92\uff1b2\uff09Transformer\u5757\u5efa\u6a21\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728UKBioBank\u548cHuman Connectome Project\u6570\u636e\u96c6\u4e0a\uff0cBrainMT\u5728\u5206\u7c7b\uff08\u6027\u522b\u9884\u6d4b\uff09\u548c\u56de\u5f52\uff08\u8ba4\u77e5\u9884\u6d4b\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "BrainMT\u901a\u8fc7\u9ad8\u6548\u5b66\u4e60\u957f\u8ddd\u79bb\u65f6\u7a7a\u5c5e\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.23537", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23537", "abs": "https://arxiv.org/abs/2506.23537", "authors": ["Xinyue Li", "Zhangkai Ni", "Wenhan Yang"], "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm", "comment": "Accepted to International Conference on Computer Vision (ICCV) 2025", "summary": "Existing learning-based methods effectively reconstruct HDR images from\nmulti-exposure LDR inputs with extended dynamic range and improved detail, but\nthey rely more on empirical design rather than theoretical foundation, which\ncan impact their reliability. To address these limitations, we propose the\ncross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR\nreconstruction is systematically decoupled into two interleaved subtasks --\nalignment and fusion -- optimized through alternating refinement, achieving\nsynergy between the two subtasks to enhance the overall performance. Our method\nformulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)\nestimation perspective, explicitly incorporating spatial correspondence priors\nacross LDR images and naturally bridging the alignment and fusion subproblems\nthrough joint constraints. Building on the mathematical foundation, we\nreimagine traditional iterative optimization through unfolding -- transforming\nthe conventional solution process into an end-to-end trainable AFUNet with\ncarefully designed modules that work progressively. Specifically, each\niteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that\nalternates between a Spatial Alignment Module (SAM) for alignment and a Channel\nFusion Module (CFM) for adaptive feature fusion, progressively bridging\nmisaligned content and exposure discrepancies. Extensive qualitative and\nquantitative evaluations demonstrate AFUNet's superior performance,\nconsistently surpassing state-of-the-art methods. Our code is available at:\nhttps://github.com/eezkni/AFUNet", "AI": {"tldr": "AFUNet\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7684\u5bf9\u9f50\u4e0e\u878d\u5408\u5b50\u4efb\u52a1\uff0c\u4eceMAP\u4f30\u8ba1\u89d2\u5ea6\u91cd\u6784HDR\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u7ecf\u9a8c\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faAFUNet\uff0c\u5c06HDR\u91cd\u6784\u5206\u89e3\u4e3a\u5bf9\u9f50\u4e0e\u878d\u5408\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u5b9e\u73b0\u534f\u540c\u3002\u57fa\u4e8eMAP\u4f30\u8ba1\uff0c\u7ed3\u5408\u7a7a\u95f4\u5bf9\u5e94\u5148\u9a8c\uff0c\u8bbe\u8ba1\u53ef\u8bad\u7ec3\u7684AFUNet\u3002", "result": "AFUNet\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AFUNet\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86HDR\u56fe\u50cf\u91cd\u6784\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.23369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23369", "abs": "https://arxiv.org/abs/2506.23369", "authors": ["Xiao'ao Song", "Konstantinos Karydis"], "title": "GS-NBV: a Geometry-based, Semantics-aware Viewpoint Planning Algorithm for Avocado Harvesting under Occlusions", "comment": "Accepted for publication in CASE 2025, 6 pages, 8 figures", "summary": "Efficient identification of picking points is critical for automated fruit\nharvesting. Avocados present unique challenges owing to their irregular shape,\nweight, and less-structured growing environments, which require specific\nviewpoints for successful harvesting. We propose a geometry-based,\nsemantics-aware viewpoint-planning algorithm to address these challenges. The\nplanning process involves three key steps: viewpoint sampling, evaluation, and\nexecution. Starting from a partially occluded view, the system first detects\nthe fruit, then leverages geometric information to constrain the viewpoint\nsearch space to a 1D circle, and uniformly samples four points to balance the\nefficiency and exploration. A new picking score metric is introduced to\nevaluate the viewpoint suitability and guide the camera to the next-best view.\nWe validate our method through simulation against two state-of-the-art\nalgorithms. Results show a 100% success rate in two case studies with\nsignificant occlusions, demonstrating the efficiency and robustness of our\napproach. Our code is available at https://github.com/lineojcd/GSNBV", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u548c\u8bed\u4e49\u611f\u77e5\u7684\u89c6\u70b9\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522b\u4e0d\u89c4\u5219\u5f62\u72b6\u6c34\u679c\uff08\u5982\u725b\u6cb9\u679c\uff09\u7684\u91c7\u6458\u70b9\uff0c\u901a\u8fc7\u89c6\u70b9\u91c7\u6837\u3001\u8bc4\u4f30\u548c\u6267\u884c\u4e09\u6b65\u5b9e\u73b0\uff0c\u9a8c\u8bc1\u663e\u793a100%\u6210\u529f\u7387\u3002", "motivation": "\u725b\u6cb9\u679c\u7684\u4e0d\u89c4\u5219\u5f62\u72b6\u3001\u91cd\u91cf\u548c\u975e\u7ed3\u6784\u5316\u751f\u957f\u73af\u5883\u5bf9\u81ea\u52a8\u5316\u91c7\u6458\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u7279\u5b9a\u89c6\u70b9\u4ee5\u5b9e\u73b0\u6210\u529f\u91c7\u6458\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u7ea6\u675f\u7684\u89c6\u70b9\u89c4\u5212\u7b97\u6cd5\uff0c\u5305\u62ec\u89c6\u70b9\u91c7\u6837\uff081D\u5706\u4e0a\u5747\u5300\u91c7\u6837\u56db\u70b9\uff09\u3001\u8bc4\u4f30\uff08\u65b0\u91c7\u6458\u8bc4\u5206\u6307\u6807\uff09\u548c\u6267\u884c\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0c\u4e0e\u4e24\u79cd\u5148\u8fdb\u7b97\u6cd5\u5bf9\u6bd4\uff0c\u5728\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0b\u5b9e\u73b0100%\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u6c34\u679c\u91c7\u6458\u3002"}}
{"id": "2506.23733", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23733", "abs": "https://arxiv.org/abs/2506.23733", "authors": ["Blair Archibald", "Paul Harvey", "Michele Sevegnani"], "title": "A Digital Twinning Approach to Decarbonisation: Research Challenges", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online; Extended Abstract", "summary": "Transportation accounts for around 27% of green house gas emissions in the\nUK. While an obvious priority area for decarbonisation, and aligned to the UK\ngovernment goal of reducing emissions by 68% for 2030, the free-market nature\nof the transportation sector combined with its fundamentally implicit and\npervasive connections to all aspects of society and national infrastructure\nmean that all decarbonisation efforts to date have been siloed within a single\ntransport sector, e.g. only considering greener aviation fuels. Truly\ndecarbonising transport requires radical changes to the entire transport\ninfrastructure, and since that transport does not happen in isolation, a single\nuser often using multiple modes, we need a view over the whole transport\nsystem. The first step to solving a problem is to understand it. As a result of\nthe fragmented nature of the transportation sector, there is currently no\nsystem level view. Without the ability to monitor even adjacent transport\ndomains, the ability for people or organisations to (dynamically) adapt their\noperations for decarbonisation outcomes is unrealistic. As transportation is a\ncomplex social-techno-economic system, information and knowledge sharing is a\nmust to be able to understand and explore potential solutions to the\ndecarbonisation challenge. We believe a Federated Digital Twinning Approach has\nthe potential to tackle transport decarbonisation problems, and, in this\nextended abstract, we give an overview of the research required to tackle the\nfundamental challenges around digital twin design, generation, validation and\nverification.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u82f1\u56fd\u4ea4\u901a\u8fd0\u8f93\u9886\u57df\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u8054\u90a6\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\u89e3\u51b3\u8131\u78b3\u6311\u6218\u3002", "motivation": "\u4ea4\u901a\u8fd0\u8f93\u5360\u82f1\u56fd\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u768427%\uff0c\u4f46\u73b0\u6709\u8131\u78b3\u52aa\u529b\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u8054\u90a6\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\uff0c\u6574\u5408\u591a\u9886\u57df\u6570\u636e\uff0c\u8bbe\u8ba1\u3001\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u9a8c\u8bc1\u6570\u5b57\u5b6a\u751f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u89c6\u89d2\uff0c\u4ee5\u652f\u6301\u52a8\u6001\u9002\u5e94\u548c\u8131\u78b3\u51b3\u7b56\u3002", "conclusion": "\u8054\u90a6\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\u6709\u671b\u89e3\u51b3\u4ea4\u901a\u8fd0\u8f93\u8131\u78b3\u7684\u7cfb\u7edf\u6027\u6311\u6218\u3002"}}
{"id": "2506.22852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22852", "abs": "https://arxiv.org/abs/2506.22852", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "comment": null, "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u5fae\u8c03\uff08KAFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u7cfb\u7edf\u4e2d\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u6570\u636e\u548c\u5916\u90e8\u77e5\u8bc6\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "LLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u6613\u51fa\u9519\uff0c\u73b0\u6709\u65b9\u6cd5\u5982RAG\u548c\u4ee3\u7406\u7cfb\u7edf\u867d\u80fd\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46LLMs\u5728\u5229\u7528\u68c0\u7d22\u77e5\u8bc6\u751f\u6210\u54cd\u5e94\u65f6\u4ecd\u6709\u56f0\u96be\u3002", "method": "\u63d0\u51faKAFT\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u6570\u636e\u548c\u5916\u90e8\u77e5\u8bc6\u5fae\u8c03LLMs\uff0c\u5e76\u5728MobileCS2\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83KAFT\u4e0e\u63d0\u793a\u6280\u672f\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKAFT\u5728RAG\u548c\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u6280\u672f\uff0c\u5c24\u5176\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u3002", "conclusion": "KAFT\u662f\u9996\u4e2a\u5b9e\u8bc1\u7814\u7a76\u8be5\u65b9\u6cd5\u7684\u8bba\u6587\uff0c\u4e3a\u63d0\u5347LLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2506.23504", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23504", "abs": "https://arxiv.org/abs/2506.23504", "authors": ["Bosubabu Sambana", "Kotamsetty Geethika Devi", "Bandi Rajeswara Reddy", "Galeti Mohammad Hussain", "Gownivalla Siddartha"], "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM", "comment": "6 Pages, 7 Figures", "summary": "The recent development of advanced machine learning methods for hybrid models\nhas greatly addressed the need for the correct prediction of electrical prices.\nThis method combines AlexNet and LSTM algorithms, which are used to introduce a\nnew model with higher accuracy in price forecasting. Despite RNN and ANN being\neffective, they often fail to deal with forex time sequence data. The\ntraditional methods do not accurately forecast the prices. These traditional\nmethods only focus on demand and price which leads to insufficient analysis of\ndata. To address this issue, using the hybrid approach, which focuses on\nexternal variables that also effect the predicted prices. Nevertheless, due to\nAlexNet's excellent feature extraction and LSTM's learning sequential patterns,\nthe prediction accuracy is vastly increased. The model is built on the past\ndata, which has been supplied with the most significant elements like demand,\ntemperature, sunlight, and rain. For example, the model applies methods, such\nas minimum-maximum scaling and a time window, to predict the electricity prices\nof the future. The results show that this hybrid model is good than the\nstandalone ones in terms of accuracy. Although we got our accuracy rating of\n97.08, it shows higher accompaniments than remaining models RNN and ANN with\naccuracies of 96.64 and 96.63 respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AlexNet\u548cLSTM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u4ef7\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u9700\u6c42\u548c\u4ef7\u683c\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u7535\u4ef7\uff0c\u4e14\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5904\u7406\u80fd\u529b\u6709\u9650\u3002", "method": "\u7ed3\u5408AlexNet\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548cLSTM\u7684\u5e8f\u5217\u5b66\u4e60\u80fd\u529b\uff0c\u5f15\u5165\u5916\u90e8\u53d8\u91cf\uff08\u5982\u6e29\u5ea6\u3001\u9633\u5149\u3001\u964d\u96e8\u7b49\uff09\uff0c\u5e76\u91c7\u7528\u6700\u5c0f-\u6700\u5927\u7f29\u653e\u548c\u65f6\u95f4\u7a97\u53e3\u6280\u672f\u3002", "result": "\u6df7\u5408\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523097.08%\uff0c\u4f18\u4e8e\u5355\u72ec\u7684RNN\uff0896.64%\uff09\u548cANN\uff0896.63%\uff09\u6a21\u578b\u3002", "conclusion": "\u8be5\u6df7\u5408\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u7535\u4ef7\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.22624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22624", "abs": "https://arxiv.org/abs/2506.22624", "authors": ["Zuyao You", "Zuxuan Wu"], "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning", "comment": null, "summary": "We present Seg-R1, a preliminary exploration of using reinforcement learning\n(RL) to enhance the pixel-level understanding and reasoning capabilities of\nlarge multimodal models (LMMs). Starting with foreground segmentation tasks,\nspecifically camouflaged object detection (COD) and salient object detection\n(SOD), our approach enables the LMM to generate point and bounding box prompts\nin the next-token fashion, which are then used to guide SAM2 in producing\nsegmentation masks. We introduce Group Relative Policy Optimization (GRPO) into\nthe segmentation domain, equipping the LMM with pixel-level comprehension\nthrough a carefully designed training strategy. Notably, Seg-R1 achieves\nremarkable performance with purely RL-based training, achieving .873 S-measure\non COD10K without complex model modification. Moreover, we found that pure RL\ntraining demonstrates strong open-world generalization. Despite being trained\nsolely on foreground segmentation image-mask pairs without text supervision,\nSeg-R1 achieves impressive zero-shot performance on referring segmentation and\nreasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on\nReasonSeg test, outperforming models fully supervised on these datasets.", "AI": {"tldr": "Seg-R1\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u50cf\u7d20\u7ea7\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7GRPO\u7b56\u7565\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u590d\u6742\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\uff08\u5982\u524d\u666f\u5206\u5272\uff09\u4e2d\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528Group Relative Policy Optimization (GRPO)\u7b56\u7565\uff0c\u901a\u8fc7\u70b9\u63d0\u793a\u548c\u8fb9\u754c\u6846\u63d0\u793a\u5f15\u5bfcSAM2\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u4ec5\u4f7f\u7528RL\u8bad\u7ec3\u3002", "result": "\u5728COD10K\u4e0a\u8fbe\u52300.873 S-measure\uff0c\u5728RefCOCOg\u548cReasonSeg\u4e0a\u5206\u522b\u5b9e\u73b071.4 cIoU\u548c56.7 gIoU\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f18\u4e8e\u5168\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "Seg-R1\u8bc1\u660e\u4e86\u7eafRL\u8bad\u7ec3\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23584", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23584", "abs": "https://arxiv.org/abs/2506.23584", "authors": ["Renjie Liang", "Zhengkang Fan", "Jinqian Pan", "Chenkun Sun", "Russell Terry", "Jie Xu"], "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation", "comment": null, "summary": "Generating radiology reports from CT scans remains a complex task due to the\nnuanced nature of medical imaging and the variability in clinical\ndocumentation. In this study, we propose a two-stage framework for generating\nrenal radiology reports from 2D CT slices. First, we extract structured\nabnormality features using a multi-task learning model trained to identify\nlesion attributes such as location, size, enhancement, and attenuation. These\nextracted features are subsequently combined with the corresponding CT image\nand fed into a fine-tuned vision-language model to generate natural language\nreport sentences aligned with clinical findings. We conduct experiments on a\ncurated dataset of renal CT studies with manually annotated\nsentence-slice-feature triplets and evaluate performance using both\nclassification metrics and natural language generation metrics. Our results\ndemonstrate that the proposed model outperforms random baselines across all\nabnormality types, and the generated reports capture key clinical content with\nreasonable textual accuracy. This exploratory work highlights the feasibility\nof modular, feature-informed report generation for renal imaging. Future\nefforts will focus on extending this pipeline to 3D CT volumes and further\nimproving clinical fidelity in multimodal medical AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4ece2D CT\u5207\u7247\u751f\u6210\u80be\u810f\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u5f02\u5e38\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u62a5\u544a\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u7684\u590d\u6742\u6027\u548c\u4e34\u5e8a\u6587\u6863\u7684\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u653e\u5c04\u5b66\u62a5\u544a\u7684\u751f\u6210\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u5f02\u5e38\u7279\u5f81\uff0c\u518d\u7ed3\u5408CT\u56fe\u50cf\u8f93\u5165\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u62a5\u544a\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u5f02\u5e38\u7c7b\u578b\u4e0a\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u751f\u6210\u7684\u62a5\u544a\u80fd\u5408\u7406\u6355\u6349\u5173\u952e\u4e34\u5e8a\u5185\u5bb9\u3002", "conclusion": "\u5c55\u793a\u4e86\u6a21\u5757\u5316\u3001\u57fa\u4e8e\u7279\u5f81\u7684\u62a5\u544a\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u672a\u6765\u5c06\u6269\u5c55\u52303D CT\u4f53\u79ef\u5e76\u63d0\u9ad8\u4e34\u5e8a\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2506.23400", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23400", "abs": "https://arxiv.org/abs/2506.23400", "authors": ["Yifei Li", "Joshua A. Robbins", "Guha Manogharan", "Herschel C. Pangborn", "Ilya Kovalenko"], "title": "A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "In recent years, the demand for customized, on-demand production has grown in\nthe manufacturing sector. Additive Manufacturing (AM) has emerged as a\npromising technology to enhance customization capabilities, enabling greater\nflexibility, reduced lead times, and more efficient material usage. However,\ntraditional AM systems remain constrained by static setups and human worker\ndependencies, resulting in long lead times and limited scalability. Mobile\nrobots can improve the flexibility of production systems by transporting\nproducts to designated locations in a dynamic environment. By integrating AM\nsystems with mobile robots, manufacturers can optimize travel time for\npreparatory tasks and distributed printing operations. Mobile AM robots have\nbeen deployed for on-site production of large-scale structures, but often\nneglect critical print quality metrics like surface roughness. Additionally,\nthese systems do not have the precision necessary for producing small,\nintricate components. We propose a model predictive control framework for a\nmobile AM platform that ensures safe navigation on the plant floor while\nmaintaining high print quality in a dynamic environment. Three case studies are\nused to test the feasibility and reliability of the proposed systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u79fb\u52a8\u589e\u6750\u5236\u9020\u5e73\u53f0\uff0c\u4ee5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\u548c\u9ad8\u6253\u5370\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u589e\u6750\u5236\u9020\u7cfb\u7edf\u53d7\u9650\u4e8e\u9759\u6001\u8bbe\u7f6e\u548c\u4eba\u5de5\u4f9d\u8d56\uff0c\u5bfc\u81f4\u751f\u4ea7\u5468\u671f\u957f\u548c\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002\u79fb\u52a8\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u9ad8\u751f\u4ea7\u7075\u6d3b\u6027\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5ffd\u89c6\u4e86\u6253\u5370\u8d28\u91cf\u548c\u5c0f\u578b\u590d\u6742\u90e8\u4ef6\u7684\u7cbe\u5ea6\u3002", "method": "\u96c6\u6210\u589e\u6750\u5236\u9020\u7cfb\u7edf\u4e0e\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u786e\u4fdd\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u548c\u9ad8\u6253\u5370\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u79fb\u52a8\u589e\u6750\u5236\u9020\u5e73\u53f0\u80fd\u591f\u4f18\u5316\u751f\u4ea7\u6d41\u7a0b\uff0c\u540c\u65f6\u4fdd\u8bc1\u6253\u5370\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5236\u9020\u73af\u5883\u3002"}}
{"id": "2506.23744", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23744", "abs": "https://arxiv.org/abs/2506.23744", "authors": ["Isabelle Krauss", "Victor G. Lopez", "Matthias A. M\u00fcller"], "title": "On sample-based functional observability of linear systems", "comment": null, "summary": "Sample-based observability characterizes the ability to reconstruct the\ninternal state of a dynamical system by using limited output information, i.e.,\nwhen measurements are only infrequently and/or irregularly available. In this\nwork, we investigate the concept of functional observability, which refers to\nthe ability to infer a function of the system state from the outputs, within a\nsamplebased framework. Here, we give necessary and sufficient conditions for a\nsystem to be sample-based functionally observable, and formulate conditions on\nthe sampling schemes such that these are satisfied. Furthermore, we provide a\nnumerical example, where we demonstrate the applicability of the obtained\nresults.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u91c7\u6837\u7684\u529f\u80fd\u53ef\u89c2\u6d4b\u6027\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u5728\u91c7\u6837\u6846\u67b6\u4e0b\u6ee1\u8db3\u529f\u80fd\u53ef\u89c2\u6d4b\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u7ed9\u51fa\u4e86\u91c7\u6837\u65b9\u6848\u7684\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u5728\u6d4b\u91cf\u4fe1\u606f\u6709\u9650\uff08\u5982\u4f4e\u9891\u6216\u4e0d\u89c4\u5219\u91c7\u6837\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u4ece\u8f93\u51fa\u4e2d\u63a8\u65ad\u7cfb\u7edf\u72b6\u6001\u7684\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u7cfb\u7edf\u5728\u91c7\u6837\u6846\u67b6\u4e0b\u6ee1\u8db3\u529f\u80fd\u53ef\u89c2\u6d4b\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u5206\u6790\u4e86\u91c7\u6837\u65b9\u6848\u7684\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6240\u5f97\u7ed3\u679c\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u57fa\u4e8e\u91c7\u6837\u7684\u529f\u80fd\u53ef\u89c2\u6d4b\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.22853", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22853", "abs": "https://arxiv.org/abs/2506.22853", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDICE-SCORE\u6307\u6807\u548cDICE-BENCH\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6784\u5efa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u573a\u666f\u7684\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u51fd\u6570\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u5355\u8f6e\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165DICE-SCORE\u6307\u6807\u8bc4\u4f30\u5de5\u5177\u76f8\u5173\u4fe1\u606f\u7684\u5206\u6563\u7a0b\u5ea6\uff0c\u5e76\u901a\u8fc7\u5de5\u5177\u56fe\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u6784\u5efaDICE-BENCH\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f97\u5206\u4f4e\uff0c19\u4e2aLLM\u5728DICE-BENCH\u4e0a\u8868\u73b0\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347LLM\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u51fd\u6570\u8c03\u7528\u80fd\u529b\u3002"}}
{"id": "2506.23517", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23517", "abs": "https://arxiv.org/abs/2506.23517", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "comment": null, "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "AI": {"tldr": "\u7814\u7a76\u4e86GPTZero\u5728\u4e0d\u540c\u957f\u5ea6\u6587\u672c\u4e2d\u68c0\u6d4bAI\u751f\u6210\u5185\u5bb9\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u5176\u5bf9AI\u6587\u672c\u68c0\u6d4b\u6548\u679c\u8f83\u597d\uff0c\u4f46\u5bf9\u4eba\u7c7b\u6587\u672c\u5b58\u5728\u8bef\u5224\u3002", "motivation": "\u968f\u7740\u5b66\u751f\u4f7f\u7528AI\u5de5\u5177\u7684\u589e\u52a0\uff0c\u6559\u5e08\u4f9d\u8d56AI\u68c0\u6d4b\u5de5\u5177\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6536\u96c628\u7bc7AI\u751f\u6210\u548c50\u7bc7\u4eba\u7c7b\u5199\u4f5c\u7684\u6587\u672c\uff0c\u6309\u957f\u5ea6\u5206\u7c7b\u540e\u4f7f\u7528GPTZero\u68c0\u6d4b\u3002", "result": "AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff0891-100%\uff09\uff0c\u4f46\u4eba\u7c7b\u6587\u672c\u5b58\u5728\u8bef\u62a5\u3002", "conclusion": "GPTZero\u5bf9\u7eafAI\u6587\u672c\u6709\u6548\uff0c\u4f46\u5bf9\u4eba\u7c7b\u6587\u672c\u533a\u5206\u80fd\u529b\u6709\u9650\uff0c\u9700\u8c28\u614e\u4f7f\u7528\u3002"}}
{"id": "2506.22636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22636", "abs": "https://arxiv.org/abs/2506.22636", "authors": ["Sotirios Panagiotis Chytas", "Miso Choi", "Hyunwoo J. Kim", "Vikas Singh"], "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) show impressive capabilities in integrating and\nreasoning with both visual and language data. But these models make mistakes. A\ncommon finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,\ngenerate plausible sounding text which is not grounded in the visual input, or\nat worst, is contradictory. A growing consensus attributes this behavior to an\nover-reliance on language -- especially as the generation progresses, the model\nsuffers from a ``fading memory effect'' with respect to the provided visual\ninput. We study mechanisms by which this behavior can be controlled.\nSpecifically, using ideas from geometric algebra and relational compositions,\nwe propose the addition of a small, trainable module (named ReCo) on top of any\nVLM -- no other modification is needed. We show that such a lightweight module\nis able to mitigate the fading memory effect on three of the most widely used\nVLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on\nmultiple benchmarks. Additionally, we show that our module can be combined with\nmany of the other approaches for reducing hallucination where we achieve\nimproved results for each one.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757ReCo\uff0c\u7528\u4e8e\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u4ee3\u6570\u548c\u5173\u7cfb\u7ec4\u5408\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "VLMs\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6570\u636e\u6574\u5408\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u7b26\u7684\u6587\u672c\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u6a21\u578b\u5bf9\u8bed\u8a00\u7684\u8fc7\u5ea6\u4f9d\u8d56\u5bfc\u81f4\u7684\u201c\u8bb0\u5fc6\u8870\u51cf\u6548\u5e94\u201d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReCo\u7684\u5c0f\u578b\u53ef\u8bad\u7ec3\u6a21\u5757\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709VLM\u7ed3\u6784\uff0c\u5229\u7528\u51e0\u4f55\u4ee3\u6570\u548c\u5173\u7cfb\u7ec4\u5408\u6765\u63a7\u5236\u5e7b\u89c9\u884c\u4e3a\u3002", "result": "\u5728InstructBLIP\u3001LlaVA\u548cMiniGPT4\u4e09\u79cd\u4e3b\u6d41VLM\u4e0a\u6d4b\u8bd5\uff0cReCo\u6a21\u5757\u663e\u8457\u51cf\u8f7b\u4e86\u8bb0\u5fc6\u8870\u51cf\u6548\u5e94\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0cReCo\u8fd8\u80fd\u4e0e\u5176\u4ed6\u51cf\u5c11\u5e7b\u89c9\u7684\u65b9\u6cd5\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "conclusion": "ReCo\u6a21\u5757\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584VLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u517c\u5bb9\u6027\u5f3a\uff0c\u53ef\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2506.23664", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23664", "abs": "https://arxiv.org/abs/2506.23664", "authors": ["Fangyijie Wang", "Kevin Whelan", "F\u00e9lix Balado", "Gu\u00e9nol\u00e9 Silvestre", "Kathleen M. Curran"], "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation", "comment": null, "summary": "Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u63a9\u7801\u5f15\u5bfc\u751f\u6210AI\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5408\u6210\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u53ca\u5176\u5206\u5272\u63a9\u7801\uff0c\u4ee5\u589e\u5f3a\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u63d0\u5347\u5206\u5272\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u56fe\u50cf\u6570\u636e\u56e0\u9690\u79c1\u548c\u76d1\u7ba1\u9650\u5236\u96be\u4ee5\u83b7\u53d6\uff0c\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u5408\u6210\u6570\u636e\u751f\u6210\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u80ce\u513f\u5934\u90e8\u8d85\u58f0\u56fe\u50cf\u4e0e\u5206\u5272\u63a9\u7801\uff0c\u7528\u4e8e\u589e\u5f3a\u771f\u5b9e\u6570\u636e\u96c6\u5e76\u5fae\u8c03Segment Anything Model (SAM)\u3002", "result": "\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u6355\u6349\u771f\u5b9e\u56fe\u50cf\u7279\u5f81\uff0c\u5728\u5c11\u91cf\u771f\u5b9e\u56fe\u50cf-\u63a9\u7801\u5bf9\u8bad\u7ec3\u4e0b\uff0c\u5206\u5272\u6027\u80fd\u8fbe\u5230\u6700\u4f73\u6c34\u5e73\uff08Dice\u5206\u6570\u5206\u522b\u4e3a94.66%\u548c94.38%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u7597\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.23433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23433", "abs": "https://arxiv.org/abs/2506.23433", "authors": ["Tim Puphal", "Vipul Ramtekkar", "Kenji Nishimiya"], "title": "Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset", "comment": null, "summary": "Improving automated vehicle software requires driving data rich in valuable\nroad user interactions. In this paper, we propose a risk-based filtering\napproach that helps identify such valuable driving situations from large\ndatasets. Specifically, we use a probabilistic risk model to detect high-risk\nsituations. Our method stands out by considering a) first-order situations\n(where one vehicle directly influences another and induces risk) and b)\nsecond-order situations (where influence propagates through an intermediary\nvehicle). In experiments, we show that our approach effectively selects\nvaluable driving situations in the Waymo Open Motion Dataset. Compared to the\ntwo baseline interaction metrics of Kalman difficulty and Tracks-To-Predict\n(TTP), our filtering approach identifies complex and complementary situations,\nenriching the quality in automated vehicle testing. The risk data is made\nopen-source: https://github.com/HRI-EU/RiskBasedFiltering.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4ece\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u8bc6\u522b\u6709\u4ef7\u503c\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u7528\u4e8e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u7684\u6539\u8fdb\u9700\u8981\u5bcc\u542b\u9053\u8def\u7528\u6237\u4ea4\u4e92\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u7b5b\u9009\u51fa\u8fd9\u4e9b\u6709\u4ef7\u503c\u7684\u60c5\u5883\u3002", "method": "\u4f7f\u7528\u6982\u7387\u98ce\u9669\u6a21\u578b\u68c0\u6d4b\u9ad8\u98ce\u9669\u60c5\u5883\uff0c\u5305\u62ec\u76f4\u63a5\u4ea4\u4e92\u7684\u4e00\u9636\u60c5\u5883\u548c\u901a\u8fc7\u4e2d\u4ecb\u8f66\u8f86\u4f20\u64ad\u7684\u95f4\u63a5\u4ea4\u4e92\u7684\u4e8c\u9636\u60c5\u5883\u3002", "result": "\u5728Waymo Open Motion Dataset\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7b5b\u9009\u51fa\u590d\u6742\u4e14\u4e92\u8865\u7684\u9a7e\u9a76\u60c5\u5883\uff0c\u4f18\u4e8eKalman\u96be\u5ea6\u548cTTP\u57fa\u7ebf\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u98ce\u9669\u8fc7\u6ee4\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u6570\u636e\u7684\u8d28\u91cf\uff0c\u76f8\u5173\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.23769", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23769", "abs": "https://arxiv.org/abs/2506.23769", "authors": ["Gabriel de Albuquerque Gleizer", "Peyman Mohajerin Esfahani", "Tamas Keviczky"], "title": "Active Estimation of Multiplicative Faults in Dynamical Systems", "comment": "27 pages, 7 figures. Submitted to Automatica", "summary": "This paper addresses the problem of estimating multiplicative fault signals\nin linear time-invariant systems by processing its input and output variables,\nas well as designing an input signal to maximize the accuracy of such\nestimates. The proposed real-time fault estimator is based on a residual\ngenerator used for fault detection and a multiple-output regressor generator,\nwhich feed a moving-horizon linear regression that estimates the parameter\nchanges. Asymptotic performance guarantees are provided in the presence of\nnoise. Motivated by the performance bounds, an optimal input design problem is\nformulated, for which we provide efficient algorithms and optimality bounds.\nNumerical examples demonstrate the efficacy of our approach and the importance\nof the optimal input design for accurate fault estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u6545\u969c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u8f93\u5165\u8f93\u51fa\u53d8\u91cf\u5e76\u8bbe\u8ba1\u6700\u4f18\u8f93\u5165\u4fe1\u53f7\u6765\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u4e2d\u4e58\u6027\u6545\u969c\u4fe1\u53f7\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u8f93\u5165\u4fe1\u53f7\u4ee5\u6700\u5927\u5316\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u57fa\u4e8e\u6b8b\u5dee\u751f\u6210\u5668\u548c\u591a\u8f93\u51fa\u56de\u5f52\u751f\u6210\u5668\uff0c\u7ed3\u5408\u79fb\u52a8\u6c34\u5e73\u7ebf\u6027\u56de\u5f52\u4f30\u8ba1\u53c2\u6570\u53d8\u5316\u3002", "result": "\u63d0\u4f9b\u4e86\u566a\u58f0\u4e0b\u7684\u6e10\u8fd1\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6700\u4f18\u8f93\u5165\u8bbe\u8ba1\u5bf9\u51c6\u786e\u6545\u969c\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u9ad8\u6548\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2506.22858", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22858", "abs": "https://arxiv.org/abs/2506.22858", "authors": ["Duygu Altinok"], "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ASR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u91cd\u53e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u5b9e\u4f53\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u547d\u540d\u5b9e\u4f53\u548c\u6570\u503c\u6570\u636e\u7684\u8bc6\u522b\u4e0e\u683c\u5f0f\u5316\u80fd\u529b\u3002", "motivation": "ASR\u7cfb\u7edf\uff08\u5982Whisper\uff09\u5728\u547d\u540d\u5b9e\u4f53\u548c\u6570\u503c\u6570\u636e\u8bc6\u522b\u53ca\u683c\u5f0f\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u5173\u952e\u9886\u57df\uff08\u5982\u6cd5\u5f8b\u3001\u91d1\u878d\u3001\u533b\u7597\uff09\u7684\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u91c7\u7528\u91cd\u53e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u8bad\u7ec3\uff085\u79d2\u91cd\u53e0\uff0c30\u79d2\u5757\uff0c\u5f62\u621040\u79d2\u6709\u6548\u8bed\u4e49\u7a97\u53e3\uff09\uff0c\u5e76\u91cd\u65b0\u5206\u914d\u8de8\u5757\u5b9e\u4f53\u81f3\u53f3\u4fa7\u5757\uff0c\u540c\u65f6\u5d4c\u5165\u5b9e\u4f53\u6807\u7b7e\u4ee5\u5b66\u4e60\u683c\u5f0f\u5316\u3002", "result": "\u5728Spoken Wikipedia\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u5b9e\u4f53\u683c\u5f0f\u5316\u7b49\u8bed\u4e49\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u8bad\u7ec3\u80fd\u6709\u6548\u89e3\u51b3ASR\u5728\u957f\u6587\u672c\u8f6c\u5f55\u548c\u590d\u6742\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.23520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23520", "abs": "https://arxiv.org/abs/2506.23520", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "comment": null, "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "AI": {"tldr": "ChemActor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5316\u5b66\u6267\u884c\u5668\uff0c\u7528\u4e8e\u5c06\u975e\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u6b65\u9aa4\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u901a\u8fc7LLM\u751f\u6210\u7684\u6570\u636e\u6846\u67b6\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u548c\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5408\u6210\u5728\u6709\u673a\u5316\u5b66\u4e2d\u7684\u5174\u8da3\u589e\u52a0\uff0c\u4ece\u6587\u732e\u4e2d\u81ea\u52a8\u63d0\u53d6\u5316\u5b66\u6b65\u9aa4\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5316\u5b66\u8bed\u8a00\u7684\u6a21\u7cca\u6027\u548c\u9ad8\u6210\u672c\u7684\u4eba\u5de5\u6807\u6ce8\u4f7f\u5176\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u987a\u5e8fLLM\u751f\u6210\u7684\u6570\u636e\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u9009\u62e9\u6a21\u5757\u548c\u591a\u8f6eLLM\u5faa\u73af\u8bc4\u4f30\u6307\u6807\uff0c\u751f\u6210\u673a\u5668\u53ef\u6267\u884c\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u53cd\u5e94\u5230\u63cf\u8ff0\uff08R2D\uff09\u548c\u63cf\u8ff0\u5230\u52a8\u4f5c\uff08D2A\uff09\u4efb\u52a1\u4e2d\uff0cChemActor\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b10%\u3002", "conclusion": "ChemActor\u901a\u8fc7LLM\u751f\u6210\u7684\u6570\u636e\u589e\u5f3a\u4e86\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5316\u5b66\u5b9e\u9a8c\u6b65\u9aa4\u7406\u89e3\u4e0a\u7684\u5148\u8fdb\u6027\u3002"}}
{"id": "2506.22637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22637", "abs": "https://arxiv.org/abs/2506.22637", "authors": ["Haoxuan Wang", "Zhenghao Zhao", "Junyi Wu", "Yuzhang Shang", "Gaowen Liu", "Yan Yan"], "title": "CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation", "comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/CaO2", "summary": "The recent introduction of diffusion models in dataset distillation has shown\npromising potential in creating compact surrogate datasets for large,\nhigh-resolution target datasets, offering improved efficiency and performance\nover traditional bi-level/uni-level optimization methods. However, current\ndiffusion-based dataset distillation approaches overlook the evaluation process\nand exhibit two critical inconsistencies in the distillation process: (1)\nObjective Inconsistency, where the distillation process diverges from the\nevaluation objective, and (2) Condition Inconsistency, leading to mismatches\nbetween generated images and their corresponding conditions. To resolve these\nissues, we introduce Condition-aware Optimization with Objective-guided\nSampling (CaO$_2$), a two-stage diffusion-based framework that aligns the\ndistillation process with the evaluation objective. The first stage employs a\nprobability-informed sample selection pipeline, while the second stage refines\nthe corresponding latent representations to improve conditional likelihood.\nCaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,\nsurpassing the best-performing baselines by an average of 2.3% accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5CaO$_2$\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u4e0d\u4e00\u81f4\u548c\u6761\u4ef6\u4e0d\u4e00\u81f4\u4e0a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u5b58\u5728\u76ee\u6807\u4e0d\u4e00\u81f4\u548c\u6761\u4ef6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86CaO$_2$\u6846\u67b6\uff0c\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u6982\u7387\u4fe1\u606f\u6837\u672c\u9009\u62e9\u7ba1\u9053\u548c\u6f5c\u8868\u793a\u4f18\u5316\uff0c\u4ee5\u5bf9\u9f50\u84b8\u998f\u8fc7\u7a0b\u4e0e\u8bc4\u4f30\u76ee\u6807\u3002", "result": "\u5728ImageNet\u53ca\u5176\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.3%\u3002", "conclusion": "CaO$_2$\u901a\u8fc7\u89e3\u51b3\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u84b8\u998f\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.23688", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23688", "abs": "https://arxiv.org/abs/2506.23688", "authors": ["Jiaxin Yang", "Vasileios Magoulianitis", "Catherine Aurelia Christie Alexander", "Jintang Xue", "Masatomo Kaneko", "Giovanni Cacciamani", "Andre Abreu", "Vinay Duddalwar", "C. -C. Jay Kuo", "Inderbir S. Gill", "Chrysostomos Nikias"], "title": "GUSL: A Novel and Efficient Machine Learning Model for Prostate Segmentation on MRI", "comment": null, "summary": "Prostate and zonal segmentation is a crucial step for clinical diagnosis of\nprostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation\nare based on the deep learning (DL) paradigm. However, deep neural networks are\nperceived as \"black-box\" solutions by physicians, thus making them less\npractical for deployment in the clinical setting. In this paper, we introduce a\nfeed-forward machine learning model, named Green U-shaped Learning (GUSL),\nsuitable for medical image segmentation without backpropagation. GUSL\nintroduces a multi-layer regression scheme for coarse-to-fine segmentation. Its\nfeature extraction is based on a linear model, which enables seamless\ninterpretability during feature extraction. Also, GUSL introduces a mechanism\nfor attention on the prostate boundaries, which is an error-prone region, by\nemploying regression to refine the predictions through residue correction. In\naddition, a two-step pipeline approach is used to mitigate the class imbalance,\nan issue inherent in medical imaging problems. After conducting experiments on\ntwo publicly available datasets and one private dataset, in both prostate gland\nand zonal segmentation tasks, GUSL achieves state-of-the-art performance among\nother DL-based models. Notably, GUSL features a very energy-efficient pipeline,\nsince it has a model size several times smaller and less complexity than the\nrest of the solutions. In all datasets, GUSL achieved a Dice Similarity\nCoefficient (DSC) performance greater than $0.9$ for gland segmentation.\nConsidering also its lightweight model size and transparency in feature\nextraction, it offers a competitive and practical package for medical imaging\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGUSL\u7684\u65e0\u53cd\u5411\u4f20\u64ad\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u524d\u5217\u817a\u548c\u533a\u57df\u5206\u5272\uff0c\u5177\u6709\u9ad8\u80fd\u6548\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u524d\u5217\u817a\u5206\u5272\u4e2d\u56e0'\u9ed1\u76d2'\u7279\u6027\u96be\u4ee5\u88ab\u4e34\u5e8a\u533b\u751f\u63a5\u53d7\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5c42\u56de\u5f52\u65b9\u6848\u8fdb\u884c\u7c97\u5230\u7ec6\u5206\u5272\uff0c\u57fa\u4e8e\u7ebf\u6027\u6a21\u578b\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u5f15\u5165\u8fb9\u754c\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e24\u6b65\u6d41\u6c34\u7ebf\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0cDSC\u5927\u4e8e0.9\uff0c\u6a21\u578b\u8f7b\u91cf\u4e14\u9ad8\u6548\u3002", "conclusion": "GUSL\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u900f\u660e\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23514", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23514", "abs": "https://arxiv.org/abs/2506.23514", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "comment": "Accepted to IROS 2025", "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "AI": {"tldr": "MGPRL\u662f\u4e00\u79cd\u57fa\u4e8eWi-Fi\u4fe1\u53f7\u7684\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u548c\u51f8\u5305\u5bf9\u9f50\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3GPS\u7f3a\u5931\u73af\u5883\u4e0b\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u4f9d\u8d56\u9ad8\u6210\u672c\u6216\u77ed\u7a0b\u4f20\u611f\u5668\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u9884\u6d4bRSSI\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u51f8\u5305\u5bf9\u9f50\u5b9e\u73b0\u673a\u5668\u4eba\u95f4\u7684\u76f8\u5bf9\u5b9a\u4f4d\u3002", "result": "MGPRL\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MGPRL\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u6821\u51c6\u7684\u9ad8\u6548\u5b9a\u4f4d\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86ROS\u5b9e\u73b0\u3002"}}
{"id": "2506.23817", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.23817", "abs": "https://arxiv.org/abs/2506.23817", "authors": ["Islam M. Tanash", "Risto Wichman", "Nuria Gonzalez-Prelcic"], "title": "Statistical Modeling for Accurate Characterization of Doppler Effect in LEO-Terrestrial Networks", "comment": null, "summary": "Low Earth Orbit (LEO) satellite communication is a promising solution for\nglobal wireless coverage, especially in underserved and remote areas. However,\nthe high relative velocity of LEO satellites induces significant Doppler shifts\nthat disrupt subcarrier orthogonality and degrade multicarrier system\nperformance. While the common time-varying Doppler shift can be compensated\nrelative to a reference point, the residual differential Doppler across users\nwithin the coverage cell remains a significant challenge, causing severe\nintercarrier interference. This paper presents a generalized analytical\nframework for characterizing both the Doppler shift magnitude and the\ndifferential Doppler in LEO systems. Unlike prior works limited by flat-Earth\nassumptions or specific orbital configurations, our model incorporates Earth's\ncurvature and supports arbitrary elevation angles. Using spherical geometry, we\nderive closed-form expressions for Doppler shift based on the central angle\nbetween the satellite and ground users. We further provide a statistical\ncharacterization of both the Doppler shift magnitude and the differential\nDoppler in terms of their cumulative distribution function (CDF) and\nprobability density function (PDF) for uniformly distributed users within a\nspherical cap cell. Additionally, we derive a tight upper bound for the Doppler\nshift CDF and an exact expression for the maximum differential Doppler\nexperienced across the coverage region. To mitigate intra-cell Doppler\nvariation, we implement a user clustering technique that partitions the\ncoverage area based on a Doppler disparity threshold into spherical sub-cells,\nensuring compliance with 3GPP tolerances. Extensive simulations over realistic\nsatellite constellations validate our analysis and reveal the impact of\naltitude, beamwidth, and satellite-user geometry on Doppler behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u4fe1\u4e2d\u591a\u666e\u52d2\u9891\u79fb\u53ca\u5176\u5dee\u5f02\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u7403\u9762\u51e0\u4f55\u548c\u7528\u6237\u805a\u7c7b\u6280\u672f\u89e3\u51b3\u591a\u666e\u52d2\u5e72\u6270\u95ee\u9898\u3002", "motivation": "LEO\u536b\u661f\u901a\u4fe1\u56e0\u5176\u5168\u7403\u8986\u76d6\u80fd\u529b\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u9ad8\u901f\u8fd0\u52a8\u5bfc\u81f4\u7684\u591a\u666e\u52d2\u9891\u79fb\u53ca\u5176\u5dee\u5f02\u4e25\u91cd\u5f71\u54cd\u4e86\u591a\u8f7d\u6ce2\u7cfb\u7edf\u6027\u80fd\uff0c\u4e9f\u9700\u4e00\u79cd\u901a\u7528\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7403\u9762\u51e0\u4f55\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u591a\u666e\u52d2\u9891\u79fb\u53ca\u5176\u5dee\u5f02\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u7528\u6237\u5206\u5e03\u7edf\u8ba1\u7279\u6027\u63d0\u51fa\u4e86\u7528\u6237\u805a\u7c7b\u6280\u672f\u4ee5\u51cf\u5c11\u5e72\u6270\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u536b\u661f\u9ad8\u5ea6\u3001\u6ce2\u675f\u5bbd\u5ea6\u548c\u51e0\u4f55\u5173\u7cfb\u5bf9\u591a\u666e\u52d2\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLEO\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u7528\u6237\u805a\u7c7b\u6280\u672f\u80fd\u6709\u6548\u964d\u4f4e\u591a\u666e\u52d2\u5dee\u5f02\u5e72\u6270\u3002"}}
{"id": "2506.22957", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.22957", "abs": "https://arxiv.org/abs/2506.22957", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u8bdd\u4f19\u4f34\u610f\u8bc6\uff08interlocutor awareness\uff09\uff0c\u5373LLM\u8bc6\u522b\u548c\u9002\u5e94\u5bf9\u8bdd\u4f19\u4f34\u8eab\u4efd\u548c\u7279\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f53\u4ee3LLM\u4e2d\u8fd9\u4e00\u80fd\u529b\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0LLM\u80fd\u53ef\u9760\u8bc6\u522b\u540c\u7c7b\u6a21\u578b\uff08\u5982GPT\u548cClaude\uff09\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8fd9\u4e00\u80fd\u529b\u5728\u591aLLM\u534f\u4f5c\u4e2d\u7684\u6f5c\u529b\u53ca\u5b89\u5168\u6027\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLM\u5728\u591a\u4ee3\u7406\u548c\u4eba\u7c7b-AI\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5bf9\u81ea\u8eab\u4e0a\u4e0b\u6587\u548c\u5bf9\u8bdd\u4f19\u4f34\u7684\u610f\u8bc6\u80fd\u529b\u5bf9\u786e\u4fdd\u53ef\u9760\u6027\u80fd\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u6b64\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u60c5\u5883\u610f\u8bc6\uff08situational awareness\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u5bf9\u8bdd\u4f19\u4f34\u610f\u8bc6\u3002", "method": "\u8bba\u6587\u5c06\u5bf9\u8bdd\u4f19\u4f34\u610f\u8bc6\u5b9a\u4e49\u4e3aLLM\u8bc6\u522b\u548c\u9002\u5e94\u5bf9\u8bdd\u4f19\u4f34\u8eab\u4efd\u548c\u7279\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u4ece\u63a8\u7406\u6a21\u5f0f\u3001\u8bed\u8a00\u98ce\u683c\u548c\u5bf9\u9f50\u504f\u597d\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u8bc4\u4f30\u5f53\u4ee3LLM\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u80fd\u53ef\u9760\u8bc6\u522b\u540c\u7c7b\u6a21\u578b\uff08\u5982GPT\u548cClaude\uff09\u3002\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5bf9\u8bdd\u4f19\u4f34\u610f\u8bc6\u5728\u591aLLM\u534f\u4f5c\u4e2d\u7684\u6f5c\u529b\uff08\u5982\u901a\u8fc7\u63d0\u793a\u9002\u5e94\u63d0\u5347\u534f\u4f5c\u6548\u7387\uff09\u53ca\u5b89\u5168\u6027\u98ce\u9669\uff08\u5982\u5956\u52b1\u64cd\u7eb5\u884c\u4e3a\u548c\u8d8a\u72f1\u6f0f\u6d1e\u589e\u52a0\uff09\u3002", "conclusion": "\u5bf9\u8bdd\u4f19\u4f34\u610f\u8bc6\u5728LLM\u4e2d\u5177\u6709\u53cc\u91cd\u6027\uff0c\u65e2\u53ef\u80fd\u63d0\u5347\u534f\u4f5c\u6548\u7387\uff0c\u4e5f\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8fdb\u4e00\u6b65\u7406\u89e3\u8fd9\u4e00\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4ee3\u7406\u90e8\u7f72\u4e2d\u91c7\u53d6\u65b0\u7684\u5b89\u5168\u63aa\u65bd\u3002"}}
{"id": "2506.23549", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23549", "abs": "https://arxiv.org/abs/2506.23549", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "comment": "23 pages, 10 tables, 8 figures", "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoordination Transformers\uff08CooT\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u534f\u8c03\u5feb\u901f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u5408\u4f5c\u4f19\u4f34\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u534f\u8c03\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u6216\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528\u8fd1\u671f\u4ea4\u4e92\u5386\u53f2\u9884\u6d4b\u5408\u4f5c\u4f19\u4f34\u884c\u4e3a\uff0c\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u6216\u5fae\u8c03\uff0c\u5feb\u901f\u5b66\u4e60\u534f\u8c03\u7b56\u7565\u3002", "result": "\u5728Overcooked\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u5176\u9ad8\u6548\u534f\u4f5c\u80fd\u529b\u3002", "conclusion": "CooT\u5c55\u73b0\u4e86\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u7075\u6d3b\u6027\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\uff0c\u662f\u9ad8\u6548\u7684\u534f\u4f5c\u4f19\u4f34\u3002"}}
{"id": "2506.22678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22678", "abs": "https://arxiv.org/abs/2506.22678", "authors": ["Nicolas Caytuiro", "Ivan Sipiran"], "title": "3D Shape Generation: A Survey", "comment": "20 pages, 5 figures", "summary": "Recent advances in deep learning have significantly transformed the field of\n3D shape generation, enabling the synthesis of complex, diverse, and\nsemantically meaningful 3D objects. This survey provides a comprehensive\noverview of the current state of the art in 3D shape generation, organizing the\ndiscussion around three core components: shape representations, generative\nmodeling approaches, and evaluation protocols. We begin by categorizing 3D\nrepresentations into explicit, implicit, and hybrid setups, highlighting their\nstructural properties, advantages, and limitations. Next, we review a wide\nrange of generation methods, focusing on feedforward architectures. We further\nsummarize commonly used datasets and evaluation metrics that assess fidelity,\ndiversity, and realism of generated shapes. Finally, we identify open\nchallenges and outline future research directions that could drive progress in\ncontrollable, efficient, and high-quality 3D shape generation. This survey aims\nto serve as a valuable reference for researchers and practitioners seeking a\nstructured and in-depth understanding of this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u5f62\u72b6\u8868\u793a\u3001\u751f\u6210\u65b9\u6cd5\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u52a8\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u7c7b3D\u8868\u793a\uff08\u663e\u5f0f\u3001\u9690\u5f0f\u3001\u6df7\u5408\uff09\u3001\u751f\u6210\u65b9\u6cd5\uff08\u524d\u9988\u67b6\u6784\uff09\u548c\u8bc4\u4f30\u6307\u6807\uff08\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\uff09\u8fdb\u884c\u7efc\u8ff0\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u6280\u672f\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u63a7\u6027\u3001\u6548\u7387\u548c\u8d28\u91cf\u63d0\u5347\u7684\u672a\u6765\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e863D\u5f62\u72b6\u751f\u6210\u9886\u57df\u7684\u7ed3\u6784\u5316\u53c2\u8003\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2506.23700", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23700", "abs": "https://arxiv.org/abs/2506.23700", "authors": ["Peiting Tian", "Xi Chen", "Haixia Bi", "Fan Li"], "title": "MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning, where accurate boundary delineation is essential for\nprecise lesion localization, organ identification, and quantitative assessment.\nIn recent years, deep learning-based methods have significantly advanced\nsegmentation accuracy. However, two major challenges remain. First, the\nperformance of these methods heavily relies on large-scale annotated datasets,\nwhich are often difficult to obtain in medical scenarios due to privacy\nconcerns and high annotation costs. Second, clinically challenging scenarios,\nsuch as low contrast in certain imaging modalities and blurry lesion boundaries\ncaused by malignancy, still pose obstacles to precise segmentation. To address\nthese challenges, we propose MedSAM-CA, an architecture-level fine-tuning\napproach that mitigates reliance on extensive manual annotations by adapting\nthe pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA\nintroduces two key components: the Convolutional Attention-Enhanced Boundary\nRefinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block\n(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover\nboundary information potentially overlooked by long-range attention mechanisms,\nleveraging hierarchical convolutional processing. Atte-FFB, embedded in the\nMedSAM decoder, fuses multi-level fine-grained features from skip connections\nin CBR-Net with global representations upsampled within the decoder to enhance\nboundary delineation accuracy. Experiments on publicly available datasets\ncovering dermoscopy, CT, and MRI imaging modalities validate the effectiveness\nof MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only\n2% of full training data, reaching 97.25% of full-data training performance,\ndemonstrating strong effectiveness in low-resource clinical settings.", "AI": {"tldr": "MedSAM-CA\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578bMedSAM\u7684\u67b6\u6784\u7ea7\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165CBR-Net\u548cAtte-FFB\u7ec4\u4ef6\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u8fb9\u754c\u7cbe\u5ea6\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u5e94\u5bf9\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u6a21\u7cca\u8fb9\u754c\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faMedSAM-CA\uff0c\u7ed3\u5408CBR-Net\uff08\u5377\u79ef\u6ce8\u610f\u529b\u589e\u5f3a\u8fb9\u754c\u7ec6\u5316\u7f51\u7edc\uff09\u548cAtte-FFB\uff08\u6ce8\u610f\u529b\u589e\u5f3a\u7279\u5f81\u878d\u5408\u5757\uff09\uff0c\u4f18\u5316\u8fb9\u754c\u4fe1\u606f\u6062\u590d\u548c\u591a\u7ea7\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u75282%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u523094.43% Dice\u5206\u6570\uff0c\u63a5\u8fd1\u5168\u6570\u636e\u8bad\u7ec3\u768497.25%\u6027\u80fd\u3002", "conclusion": "MedSAM-CA\u5728\u4f4e\u8d44\u6e90\u4e34\u5e8a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2506.23573", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23573", "abs": "https://arxiv.org/abs/2506.23573", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "title": "Online Human Action Detection during Escorting", "comment": "Accepted in IEEE RO-MAN '25", "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u4eba\u5458\u91cd\u8bc6\u522b\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ee5\u63d0\u5347\u62e5\u6324\u73af\u5883\u4e2d\u673a\u5668\u4eba\u62a4\u9001\u670d\u52a1\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u62a4\u9001\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u5bfc\u822a\u7b56\u7565\uff0c\u5047\u8bbe\u88ab\u62a4\u9001\u8005\u4f1a\u987a\u5229\u8ddf\u968f\uff0c\u4f46\u5728\u62e5\u6324\u73af\u5883\u4e2d\u5e38\u56e0\u65e0\u6cd5\u7406\u89e3\u4eba\u7c7b\u52a8\u6001\u884c\u4e3a\u800c\u5931\u6548\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u80fd\u540c\u65f6\u8fdb\u884c\u4eba\u5458\u91cd\u8bc6\u522b\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\u4ee5\u9002\u5e94\u88ab\u62a4\u9001\u8005\u884c\u4e3a\u3002", "result": "\u5728\u5bf9\u6bd4\u8bc4\u4f30\u4e2d\uff0c\u8be5\u7cfb\u7edf\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u673a\u5668\u4eba\u62a4\u9001\u670d\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u62e5\u6324\u73af\u5883\u4e2d\u673a\u5668\u4eba\u62a4\u9001\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.24017", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.24017", "abs": "https://arxiv.org/abs/2506.24017", "authors": ["Emre Yildirim", "Tansel Yucelen", "Arman Sargolzaei"], "title": "Orchestrated Couplings: A Time-Varying Edge Weight Framework for Efficient Event-Triggered Multiagent Networks", "comment": null, "summary": "In this paper, we focus on reducing node-to-node information exchange in\ndistributed control of multiagent networks while improving the overall network\nperformance. Specifically, we consider a multiagent network that is composed of\nleader and follower nodes over a time-varying, connected, and undirected graph.\nIn contrast to existing works on the event-triggered distributed control\nliterature, we propose a time-varying edge weight event-triggered control\nframework. In this framework, each node dynamically adjusts its edge weights by\nincreasing them during the transient (active) phase and decreasing them during\nthe steady-state (idle) phase of the multiagent network. This not only reduces\nthe number of events in the network but also improves the performance (i.e.,\nconvergence speed and control effort) of the overall multiagent network.\nSystem-theoretically, we first prove the closed-loop stability of the proposed\nevent-triggered distributed control framework, where we then show that this\nframework does not exhibit a Zeno behavior. Finally, illustrative numerical\nexamples are provided to demonstrate the efficacy of this framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u53d8\u8fb9\u6743\u91cd\u7684\u4e8b\u4ef6\u89e6\u53d1\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u51cf\u5c11\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u4e2d\u7684\u8282\u70b9\u95f4\u4fe1\u606f\u4ea4\u6362\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u89e6\u53d1\u5206\u5e03\u5f0f\u63a7\u5236\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u51cf\u5c11\u4fe1\u606f\u4ea4\u6362\u5e76\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u8fb9\u6743\u91cd\u7684\u6846\u67b6\uff0c\u5728\u77ac\u6001\u9636\u6bb5\u589e\u52a0\u6743\u91cd\uff0c\u7a33\u6001\u9636\u6bb5\u51cf\u5c11\u6743\u91cd\u3002", "result": "\u8bc1\u660e\u4e86\u95ed\u73af\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u4e86Zeno\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u4e8b\u4ef6\u6570\u91cf\uff0c\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u63a7\u5236\u6548\u7387\u3002"}}
{"id": "2506.22977", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22977", "abs": "https://arxiv.org/abs/2506.22977", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "AI": {"tldr": "\u672c\u6587\u590d\u73b0\u4e86Ortu\u7b49\u4eba\uff082024\uff09\u5173\u4e8e\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\u7ade\u4e89\u7684\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5176\u6838\u5fc3\u53d1\u73b0\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u3001\u4e0d\u540c\u63d0\u793a\u7ed3\u6784\u548c\u7279\u5b9a\u9886\u57df\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u6d88\u878d\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53d7\u6a21\u578b\u67b6\u6784\u3001\u63d0\u793a\u7ed3\u6784\u548c\u9886\u57df\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u4e8b\u5b9e\u4e0e\u53cd\u4e8b\u5b9e\u4fe1\u606f\u7684\u7ade\u4e89\u673a\u5236\uff0c\u9a8c\u8bc1\u5e76\u6269\u5c55Ortu\u7b49\u4eba\u7684\u53d1\u73b0\u3002", "method": "\u590d\u73b0\u5b9e\u9a8c\u5e76\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\uff08Llama 3.1 8B\uff09\uff0c\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u7ed3\u6784\u548c\u7279\u5b9a\u9886\u57df\u3002", "result": "\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u4e13\u4e1a\u5316\u5728\u66f4\u5927\u6a21\u578b\u4e2d\u51cf\u5f31\uff0c\u63d0\u793a\u7ed3\u6784\u5f71\u54cd\u53cd\u4e8b\u5b9e\u6807\u8bb0\u7684logit\uff0c\u7279\u5b9a\u9886\u57df\u63d0\u793a\u4f1a\u626d\u66f2\u7ed3\u679c\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u6d88\u878d\u65b9\u6cd5\u7684\u6709\u6548\u6027\u56e0\u6a21\u578b\u67b6\u6784\u3001\u63d0\u793a\u7ed3\u6784\u548c\u9886\u57df\u800c\u5f02\uff0c\u9700\u8c28\u614e\u5e94\u7528\u3002"}}
{"id": "2506.23563", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23563", "abs": "https://arxiv.org/abs/2506.23563", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "comment": "Technical report", "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "AI": {"tldr": "MMReason\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u6837\u3001\u5f00\u653e\u548c\u6311\u6218\u6027\u95ee\u9898\u586b\u8865\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709MLLM\u57fa\u51c6\u5728\u8bc4\u4f30\u957f\u94fe\u63a8\u7406\u80fd\u529b\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5305\u62ec\u7f3a\u4e4f\u96be\u5ea6\u548c\u591a\u6837\u6027\u3001\u6613\u53d7\u731c\u6d4b\u548c\u8bb0\u5fc6\u5f71\u54cd\uff0c\u4ee5\u53ca\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "MMReason\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u8bbe\u8ba1\uff1a\uff081\uff09\u4ece\u591a\u5b66\u79d1\u548c\u591a\u96be\u5ea6\u7ea7\u522b\u7b5b\u9009\u6311\u6218\u6027\u95ee\u9898\uff1b\uff082\uff09\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u5f00\u653e\u5f62\u5f0f\u5e76\u4f7f\u7528\u591a\u6a21\u578b\u6295\u7968\u6280\u672f\u8fc7\u6ee4\uff1b\uff083\uff09\u6807\u6ce8\u8be6\u7ec6\u6b65\u9aa4\u5e76\u8bbe\u8ba1\u4e09\u5143\u8bc4\u5206\u673a\u5236\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\u3002", "result": "MMReason\u5bf9\u4e3b\u6d41MLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u5176\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "MMReason\u6709\u671b\u6210\u4e3a\u63a8\u52a8MLLM\u63a8\u7406\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2506.22710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22710", "abs": "https://arxiv.org/abs/2506.22710", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "comment": null, "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "AI": {"tldr": "LightBSR\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u76f2\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u9690\u5f0f\u9000\u5316\u8868\u793a\uff08IDR\uff09\u7684\u533a\u5206\u6027\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709IDE-BSR\u65b9\u6cd5\u5ffd\u89c6\u4e86IDR\u533a\u5206\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u8fc7\u9ad8\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316IDR\u533a\u5206\u6027\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u9ad8\u6548\u7684BSR\u6a21\u578b\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408\u9000\u5316\u5148\u9a8c\u7ea6\u675f\u7684\u5bf9\u6bd4\u5b66\u4e60\uff08\u6559\u5e08\u9636\u6bb5\uff09\u548c\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff08\u5b66\u751f\u9636\u6bb5\uff09\uff0c\u63d0\u5347IDR\u533a\u5206\u6027\u5e76\u7b80\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLightBSR\u5728\u591a\u79cd\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u4f18\u5316IDR\u533a\u5206\u6027\u662f\u63d0\u5347\u76f2\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u7684\u5173\u952e\uff0cLightBSR\u4e3a\u8f7b\u91cf\u9ad8\u6548BSR\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.23701", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23701", "abs": "https://arxiv.org/abs/2506.23701", "authors": ["Lingtong Zhang", "Mengdie Song", "Xiaohan Hao", "Huayu Mai", "Bensheng Qiu"], "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction", "comment": "Accept by MICCAI2025", "summary": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical\ndiagnostics. As the latest generative models, diffusion models (DMs) have\nstruggled to produce high-fidelity images due to their stochastic nature in\nimage domains. Latent diffusion models (LDMs) yield both compact and detailed\nprior knowledge in latent domains, which could effectively guide the model\ntowards more effective learning of the original data distribution. Inspired by\nthis, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by\npre-trained LDMs to enhance data consistency in MRI reconstruction tasks.\nSpecifically, we first construct a Visual-Mamba-based backbone, which enables\nefficient encoding and reconstruction of under-sampled images. Then pre-trained\nLDMs are integrated to provide conditional priors in both latent and image\ndomains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion\nin multi-level latent domains. Simultaneously, to effectively utilize a prior\nin both the k-space and image domain, under-sampled images are fused with\ngenerated full-sampled images by the Dual-domain Fusion Branch (DFB) for\nself-adaption guidance. Lastly, to further enhance the data consistency, we\npropose a k-space regularization strategy based on the non-auto-calibration\nsignal (NACS) set. Extensive experiments on two public MRI datasets fully\ndemonstrate the effectiveness of the proposed methodology. The code is\navailable at https://github.com/Zolento/MDPG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u7684\u591a\u57df\u6269\u6563\u5148\u9a8c\u5f15\u5bfc\uff08MDPG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347MRI\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728MRI\u91cd\u5efa\u4e2d\u56e0\u968f\u673a\u6027\u96be\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u800c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u80fd\u63d0\u4f9b\u7d27\u51d1\u4e14\u8be6\u7ec6\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5b66\u4e60\u539f\u59cb\u6570\u636e\u5206\u5e03\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8eVisual-Mamba\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u96c6\u6210\u9884\u8bad\u7ec3LDMs\u63d0\u4f9b\u6f5c\u5728\u548c\u56fe\u50cf\u57df\u7684\u6761\u4ef6\u5148\u9a8c\uff0c\u63d0\u51fa\u6f5c\u5728\u5f15\u5bfc\u6ce8\u610f\u529b\uff08LGA\uff09\u548c\u591a\u7ea7\u6f5c\u5728\u57df\u878d\u5408\uff0c\u5e76\u8bbe\u8ba1\u53cc\u57df\u878d\u5408\u5206\u652f\uff08DFB\uff09\u548ck\u7a7a\u95f4\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MDPG\u901a\u8fc7\u591a\u57df\u5148\u9a8c\u5f15\u5bfc\u548ck\u7a7a\u95f4\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86MRI\u91cd\u5efa\u7684\u6570\u636e\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2506.23614", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2506.23614", "abs": "https://arxiv.org/abs/2506.23614", "authors": ["Jing Huang", "Hao Su", "Kwok Wai Samuel Au"], "title": "Passage-traversing optimal path planning with sampling-based algorithms", "comment": "30 pages, 22 figures, 6 tables, journal paper", "summary": "This paper introduces a new paradigm of optimal path planning, i.e.,\npassage-traversing optimal path planning (PTOPP), that optimizes paths'\ntraversed passages for specified optimization objectives. In particular, PTOPP\nis utilized to find the path with optimal accessible free space along its\nentire length, which represents a basic requirement for paths in robotics. As\npassages are places where free space shrinks and becomes constrained, the core\nidea is to leverage the path's passage traversal status to characterize its\naccessible free space comprehensively. To this end, a novel passage detection\nand free space decomposition method using proximity graphs is proposed,\nenabling fast detection of sparse but informative passages and environment\ndecompositions. Based on this preprocessing, optimal path planning with\naccessible free space objectives or constraints is formulated as PTOPP problems\ncompatible with sampling-based optimal planners. Then, sampling-based\nalgorithms for PTOPP, including their dependent primitive procedures, are\ndeveloped leveraging partitioned environments for fast passage traversal check.\nAll these methods are implemented and thoroughly tested for effectiveness and\nefficiency validation. Compared to existing approaches, such as clearance-based\nmethods, PTOPP demonstrates significant advantages in configurability, solution\noptimality, and efficiency, addressing prior limitations and incapabilities. It\nis believed to provide an efficient and versatile solution to accessible free\nspace optimization over conventional avenues and more generally, to a broad\nclass of path planning problems that can be formulated as PTOPP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u8303\u5f0fPTOPP\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u5f84\u901a\u8fc7\u7684\u901a\u9053\u6765\u5b9e\u73b0\u7279\u5b9a\u76ee\u6807\uff0c\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u81ea\u7531\u7a7a\u95f4\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u81ea\u7531\u7a7a\u95f4\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cPTOPP\u65e8\u5728\u901a\u8fc7\u901a\u9053\u68c0\u6d4b\u548c\u81ea\u7531\u7a7a\u95f4\u5206\u89e3\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u90bb\u8fd1\u56fe\u7684\u901a\u9053\u68c0\u6d4b\u548c\u81ea\u7531\u7a7a\u95f4\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u91c7\u6837\u7b97\u6cd5\u7528\u4e8e\u5feb\u901f\u901a\u9053\u904d\u5386\u68c0\u67e5\u3002", "result": "PTOPP\u5728\u914d\u7f6e\u6027\u3001\u89e3\u51b3\u65b9\u6848\u6700\u4f18\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u95f4\u9699\u7684\u65b9\u6cd5\uff09\u3002", "conclusion": "PTOPP\u4e3a\u81ea\u7531\u7a7a\u95f4\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2506.24083", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.24083", "abs": "https://arxiv.org/abs/2506.24083", "authors": ["Robin Inho Kee", "Taehyeun Kim", "Anouck Girard", "Ilya Kolmanovsky"], "title": "Time Shift Governor-Guided MPC with Collision Cone CBFs for Safe Adaptive Cruise Control in Dynamic Environments", "comment": "Robin Inho Kee and Taehyeun Kim contributed equally to this work", "summary": "This paper introduces a Time Shift Governor (TSG)-guided Model Predictive\nController with Control Barrier Functions (CBFs)-based constraints for adaptive\ncruise control (ACC). This MPC-CBF approach is defined for obstacle-free curved\nroad tracking, while following distance and obstacle avoidance constraints are\nhandled using standard CBFs and relaxed Collision Cone CBFs. In order to\naddress scenarios involving rapidly moving obstacles or rapidly changing\nleading vehicle's behavior, the TSG augmentation is employed which alters the\ntarget reference to enforce constraints. Simulation results demonstrate the\neffectiveness of the TSG-guided MPC-CBF approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u504f\u79fb\u8c03\u63a7\u5668\uff08TSG\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7ed3\u5408\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\uff08ACC\uff09\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u9053\u8def\u548c\u52a8\u6001\u969c\u788d\u7269\u573a\u666f\u4e0b\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7684\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u91c7\u7528MPC-CBF\u6846\u67b6\uff0c\u7ed3\u5408TSG\u8c03\u6574\u76ee\u6807\u53c2\u8003\u4ee5\u5e94\u5bf9\u5feb\u901f\u53d8\u5316\u7684\u969c\u788d\u7269\u6216\u524d\u8f66\u884c\u4e3a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "TSG-guided MPC-CBF\u65b9\u6cd5\u5728\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.22978", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22978", "abs": "https://arxiv.org/abs/2506.22978", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "comment": null, "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8e\u6210\u5206\u53e5\u6cd5\u6811\u7684\u7ec4\u5408\u5f0f\u53e5\u6cd5\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u591a\u79cd\u53d8\u4f53\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u5f0fSLMs\u7684\u8bbe\u8ba1\u9009\u62e9\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u6db5\u76d6\u73b0\u6709\u6a21\u578b\u548c\u65b0\u53d8\u4f53\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u53e5\u6cd5\u6cdb\u5316\u3001\u6458\u8981\u3001\u5bf9\u8bdd\u548c\u63a8\u7406\u6548\u7387\u7b49\u65b9\u9762\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u591a\u9879\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7ec4\u5408\u5f0fSLMs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2506.23576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23576", "abs": "https://arxiv.org/abs/2506.23576", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "comment": "26 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u53ef\u589e\u5f3a\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\uff0c\u4f46\u5b58\u5728\u8bef\u62a5\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4f5c\u4e3a\u9632\u5fa1\u8d8a\u72f1\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u8d8a\u72f1\u7b56\u7565\uff08AutoDefense\u3001BetterDan\u3001JB\uff09\uff0c\u6bd4\u8f83\u5355\u667a\u80fd\u4f53\u4e0e\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u5347\u9632\u5fa1\u6548\u679c\uff0c\u4f46\u6548\u679c\u56e0\u653b\u51fb\u7c7b\u578b\u800c\u5f02\uff0c\u5e76\u589e\u52a0\u8bef\u62a5\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u5f53\u524d\u81ea\u52a8\u9632\u5fa1\u5b58\u5728\u5c40\u9650\uff0c\u9700\u6539\u8fdb\u672a\u6765LLM\u7cfb\u7edf\u7684\u5bf9\u9f50\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22718", "abs": "https://arxiv.org/abs/2506.22718", "authors": ["Jun-Jee Chao", "Qingyuan Jiang", "Volkan Isler"], "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians", "comment": null, "summary": "Part segmentation and motion estimation are two fundamental problems for\narticulated object motion analysis. In this paper, we present a method to solve\nthese two problems jointly from a sequence of observed point clouds of a single\narticulated object. The main challenge in our problem setting is that the point\nclouds are not assumed to be generated by a fixed set of moving points.\nInstead, each point cloud in the sequence could be an arbitrary sampling of the\nobject surface at that particular time step. Such scenarios occur when the\nobject undergoes major occlusions, or if the dataset is collected using\nmeasurements from multiple sensors asynchronously. In these scenarios, methods\nthat rely on tracking point correspondences are not appropriate. We present an\nalternative approach based on a compact but effective representation where we\nrepresent the object as a collection of simple building blocks modeled as 3D\nGaussians. We parameterize the Gaussians with time-dependent rotations,\ntranslations, and scales that are shared across all time steps. With our\nrepresentation, part segmentation can be achieved by building correspondences\nbetween the observed points and the Gaussians. Moreover, the transformation of\neach point across time can be obtained by following the poses of the assigned\nGaussian (even when the point is not observed). Experiments show that our\nmethod outperforms existing methods that solely rely on finding point\ncorrespondences. Additionally, we extend existing datasets to emulate\nreal-world scenarios by considering viewpoint occlusions. We further\ndemonstrate that our method is more robust to missing points as compared to\nexisting approaches on these challenging datasets, even when some parts are\ncompletely occluded in some time-steps. Notably, our part segmentation\nperformance outperforms the state-of-the-art method by 13% on point clouds with\nocclusions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8054\u5408\u89e3\u51b3\u90e8\u5206\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc73D\u9ad8\u65af\u8868\u793a\u5904\u7406\u52a8\u6001\u70b9\u4e91\u5e8f\u5217\uff0c\u9002\u7528\u4e8e\u906e\u6321\u548c\u591a\u4f20\u611f\u5668\u5f02\u6b65\u91c7\u96c6\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u70b9\u4e91\u5e8f\u5217\u4e2d\u90e8\u5206\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u70b9\u4e91\u975e\u56fa\u5b9a\u91c7\u6837\u6216\u5b58\u5728\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u75283D\u9ad8\u65af\u6a21\u578b\u8868\u793a\u7269\u4f53\uff0c\u53c2\u6570\u5316\u65f6\u95f4\u4f9d\u8d56\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u7f29\u653e\uff0c\u901a\u8fc7\u70b9\u4e0e\u9ad8\u65af\u7684\u5bf9\u5e94\u5173\u7cfb\u5b9e\u73b0\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\u3002", "result": "\u5728\u906e\u6321\u573a\u666f\u4e0b\uff0c\u90e8\u5206\u5206\u5272\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd513%\uff0c\u4e14\u5bf9\u7f3a\u5931\u70b9\u66f4\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u70b9\u4e91\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.23721", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23721", "abs": "https://arxiv.org/abs/2506.23721", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "comment": null, "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u8d85\u58f0\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u81ea\u52a8\u5316\u80be\u810f\u4f53\u79ef\u6d4b\u91cf\uff0c\u63d0\u5347\u4e34\u5e8a\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8d85\u58f0\u6280\u672f\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3001\u64cd\u4f5c\u7e41\u7410\u4ee5\u53ca\u533b\u751f\u9700\u8981\u5728\u5c4f\u5e55\u548c\u60a3\u8005\u4e4b\u95f4\u9891\u7e41\u5207\u6362\u6ce8\u610f\u529b\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u5206\u5272\u548c\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\uff0c\u63d0\u51fa\u4e24\u79cdAR-DL\u8f85\u52a9\u8d85\u58f0\u6d41\u7a0b\uff0c\u652f\u6301\u65e0\u7ebf\u548c\u89c6\u9891\u8f93\u51fa\u8bbe\u5907\u3002", "result": "\u4f7f\u7528\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6a21\u578b\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90GitHub\u7ba1\u9053\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u8d85\u58f0\u8bca\u65ad\u7684\u6548\u7387\u548c\u6613\u7528\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5373\u65f6\u62a4\u7406\u573a\u666f\u3002"}}
{"id": "2506.23624", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23624", "abs": "https://arxiv.org/abs/2506.23624", "authors": ["Max Grobbel", "Tristan Schneider", "S\u00f6ren Hohmann"], "title": "Towards Universal Shared Control in Teleoperation Without Haptic Feedback", "comment": "5 pages, submitted to IEEE Telepresence 2025 conference", "summary": "Teleoperation with non-haptic VR controllers deprives human operators of\ncritical motion feedback. We address this by embedding a multi-objective\noptimization problem that converts user input into collision-free UR5e joint\ntrajectories while actively suppressing liquid slosh in a glass. The controller\nmaintains 13 ms average planning latency, confirming real-time performance and\nmotivating the augmentation of this teleoperation approach to further\nobjectives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a\u65e0\u78b0\u649eUR5e\u5173\u8282\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6291\u5236\u73bb\u7483\u4e2d\u6db2\u4f53\u6643\u52a8\uff0c\u5b9e\u73b0\u4e8613\u6beb\u79d2\u7684\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf\u3002", "motivation": "\u975e\u89e6\u89c9VR\u63a7\u5236\u5668\u5265\u593a\u4e86\u64cd\u4f5c\u5458\u7684\u8fd0\u52a8\u53cd\u9988\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5d4c\u5165\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u6362\u4e3a\u65e0\u78b0\u649e\u5173\u8282\u8f68\u8ff9\u5e76\u6291\u5236\u6db2\u4f53\u6643\u52a8\u3002", "result": "\u63a7\u5236\u5668\u5e73\u5747\u89c4\u5212\u5ef6\u8fdf\u4e3a13\u6beb\u79d2\uff0c\u8bc1\u5b9e\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u4ed6\u76ee\u6807\u7684\u8fdc\u7a0b\u64cd\u4f5c\u3002"}}
{"id": "2506.23781", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.23781", "abs": "https://arxiv.org/abs/2506.23781", "authors": ["Savvas Papaioannou", "Panayiotis Kolios", "Christos G. Panayiotou", "Marios M. Polycarpou"], "title": "Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination", "comment": "2025 European Control Conference (ECC), Thessaloniki, Greece, 24-27\n  June 2025", "summary": "Automated inspection with Unmanned Aerial Systems (UASs) is a transformative\ncapability set to revolutionize various application domains. However, this task\nis inherently complex, as it demands the seamless integration of perception,\nplanning, and control which existing approaches often treat separately.\nMoreover, it requires accurate long-horizon planning to predict action\nsequences, in contrast to many current techniques, which tend to be myopic. To\novercome these limitations, we propose a 3D inspection approach that unifies\nperception, planning, and control within a single data-driven predictive\ncontrol framework. Unlike traditional methods that rely on known UAS dynamic\nmodels, our approach requires only input-output data, making it easily\napplicable to off-the-shelf black-box UASs. Our method incorporates back-face\nelimination, a visibility determination technique from 3D computer graphics,\ndirectly into the control loop, thereby enabling the online generation of\naccurate, long-horizon 3D inspection trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u76843D\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u73b0\u6210\u7684\u9ed1\u76d2\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5206\u79bb\uff0c\u4e14\u7f3a\u4e4f\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u68c0\u6d4b\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u54083D\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u80cc\u9762\u6d88\u9664\u6280\u672f\uff0c\u5b9e\u73b0\u5728\u7ebf\u751f\u6210\u7cbe\u786e\u7684\u957f\u65f6\u7a0b3D\u68c0\u6d4b\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5df2\u77e5\u65e0\u4eba\u673a\u52a8\u6001\u6a21\u578b\uff0c\u4ec5\u9700\u8f93\u5165\u8f93\u51fa\u6570\u636e\uff0c\u9002\u7528\u4e8e\u9ed1\u76d2\u65e0\u4eba\u673a\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u81ea\u52a8\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23046", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23046", "abs": "https://arxiv.org/abs/2506.23046", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "comment": "23 pages, 6 figures", "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "AI": {"tldr": "SoMi-ToM\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u591a\u89c6\u89d2\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u9759\u6001\u6587\u672c\u57fa\u51c6\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7684\u591a\u6a21\u6001\u6570\u636e\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728ToM\u80fd\u529b\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u73b0\u6709ToM\u57fa\u51c6\u591a\u4e3a\u9759\u6001\u6587\u672c\u573a\u666f\uff0c\u4e0e\u771f\u5b9e\u52a8\u6001\u793e\u4ea4\u4e92\u52a8\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u5f00\u53d1\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eSoMi\u73af\u5883\u751f\u6210\u7684\u591a\u6a21\u6001\u4e92\u52a8\u6570\u636e\uff0c\u8bbe\u8ba1\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7684\u591a\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u89c6\u9891\u3001\u56fe\u50cf\u548c\u4e13\u5bb6\u6807\u6ce8\u95ee\u9898\u3002", "result": "LVLMs\u5728SoMi-ToM\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u8bc4\u4f30\u7684\u5e73\u5747\u51c6\u786e\u7387\u5dee\u8ddd\u5206\u522b\u4e3a40.1%\u548c26.4%\u3002", "conclusion": "\u672a\u6765LVLMs\u9700\u63d0\u5347\u5728\u5177\u8eab\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684ToM\u80fd\u529b\uff0cSoMi-ToM\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.23626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23626", "abs": "https://arxiv.org/abs/2506.23626", "authors": ["Ant\u00f3nio Afonso", "Iolanda Leite", "Alessandro Sestini", "Florian Fuchs", "Konrad Tollmar", "Linus Gissl\u00e9n"], "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games", "comment": "16 pages in total, 10 pages of main paper, 5 figures", "summary": "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fed\u4ee3\u8c03\u6574\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u5956\u52b1\u51fd\u6570\u6743\u91cd\uff0c\u4ee5\u89e3\u51b3\u6e38\u620f\u5185\u5bb9\u6216\u673a\u5236\u4fee\u6539\u65f6\u5956\u52b1\u6743\u91cd\u4e0d\u518d\u6700\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u6e38\u620f\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u4e24\u5927\u6311\u6218\uff1a\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u4f9d\u8d56\u4e13\u5bb6\uff0c\u4ee5\u53ca\u6e38\u620f\u5185\u5bb9\u4fee\u6539\u5bfc\u81f4\u5956\u52b1\u6743\u91cd\u5931\u6548\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u884c\u4e3a\u76ee\u6807\u548c\u5148\u524d\u8bad\u7ec3\u8f6e\u6b21\u7684\u6027\u80fd\u7edf\u8ba1\uff0c\u8fed\u4ee3\u63d0\u51fa\u66f4\u65b0\u7684\u5956\u52b1\u6743\u91cd\uff0c\u5b9e\u73b0\u95ed\u73af\u81ea\u6211\u4fee\u6b63\u3002", "result": "\u5728\u8d5b\u8f66\u4efb\u52a1\u4e2d\uff0cLM\u5f15\u5bfc\u7684\u4ee3\u7406\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4ece9%\u6210\u529f\u7387\u63d0\u9ad8\u523074%\uff08\u4e00\u6b21\u8fed\u4ee3\uff09\uff0c\u6700\u7ec8\u8fbe\u523080%\u6210\u529f\u7387\u548c855\u65f6\u95f4\u6b65\uff0c\u63a5\u8fd1\u4e13\u5bb6\u8c03\u4f18\u768494%\u548c850\u65f6\u95f4\u6b65\u3002", "conclusion": "\u81ea\u52a8\u5316LM\u5f15\u5bfc\u7684\u5956\u52b1\u6743\u91cd\u8c03\u6574\u65b9\u6cd5\u6709\u6548\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u5de5\u8c03\u4f18\uff0c\u51cf\u5c11\u4e86\u5bf9\u624b\u52a8\u5956\u52b1\u5de5\u7a0b\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.22720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22720", "abs": "https://arxiv.org/abs/2506.22720", "authors": ["Jinghao Wang", "Zhang Li", "Zi Wang", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "title": "Deterministic Object Pose Confidence Region Estimation", "comment": "Accepted by ICCV 2025", "summary": "6D pose confidence region estimation has emerged as a critical direction,\naiming to perform uncertainty quantification for assessing the reliability of\nestimated poses. However, current sampling-based approach suffers from critical\nlimitations that severely impede their practical deployment: 1) the sampling\nspeed significantly decreases as the number of samples increases. 2) the\nderived confidence regions are often excessively large. To address these\nchallenges, we propose a deterministic and efficient method for estimating pose\nconfidence regions. Our approach uses inductive conformal prediction to\ncalibrate the deterministically regressed Gaussian keypoint distributions into\n2D keypoint confidence regions. We then leverage the implicit function theorem\nto propagate these keypoint confidence regions directly into 6D pose confidence\nregions. This method avoids the inefficiency and inflated region sizes\nassociated with sampling and ensembling. It provides compact confidence regions\nthat cover the ground-truth poses with a user-defined confidence level.\nExperimental results on the LineMOD Occlusion and SPEED datasets show that our\nmethod achieves higher pose estimation accuracy with reduced computational\ntime. For the same coverage rate, our method yields significantly smaller\nconfidence region volumes, reducing them by up to 99.9\\% for rotations and\n99.8\\% for translations. The code will be available soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba16D\u59ff\u6001\u7f6e\u4fe1\u533a\u57df\uff0c\u89e3\u51b3\u4e86\u91c7\u6837\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u533a\u57df\u8fc7\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u7f6e\u4fe1\u533a\u57df\u5927\u5c0f\u4e0a\u5b58\u5728\u663e\u8457\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u5f52\u7eb3\u5171\u5f62\u9884\u6d4b\u6821\u51c6\u9ad8\u65af\u5173\u952e\u70b9\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u9690\u51fd\u6570\u5b9a\u7406\u76f4\u63a5\u4f20\u64ad\u52306D\u59ff\u6001\u7f6e\u4fe1\u533a\u57df\u3002", "result": "\u5728LineMOD Occlusion\u548cSPEED\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u63d0\u9ad8\u4e86\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u7f6e\u4fe1\u533a\u57df\u4f53\u79ef\u663e\u8457\u7f29\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7d27\u51d1\uff0c\u80fd\u4ee5\u7528\u6237\u5b9a\u4e49\u7684\u7f6e\u4fe1\u6c34\u5e73\u8986\u76d6\u771f\u5b9e\u59ff\u6001\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.23759", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23759", "abs": "https://arxiv.org/abs/2506.23759", "authors": ["Zheng Fang", "Xiaoming Qi", "Chun-Mei Feng", "Jialun Pei", "Weixin Si", "Yueming Jin"], "title": "Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos", "comment": null, "summary": "Surgical instrument segmentation under Federated Learning (FL) is a promising\ndirection, which enables multiple surgical sites to collaboratively train the\nmodel without centralizing datasets. However, there exist very limited FL works\nin surgical data science, and FL methods for other modalities do not consider\ninherent characteristics in surgical domain: i) different scenarios show\ndiverse anatomical backgrounds while highly similar instrument representation;\nii) there exist surgical simulators which promote large-scale synthetic data\ngeneration with minimal efforts. In this paper, we propose a novel Personalized\nFL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),\nwhich wisely leverages surgical domain knowledge during both local-site and\nglobal-server training to boost segmentation. Concretely, our model embraces a\nRepresentation Separation and Cooperation (RSC) mechanism in local-site\ntraining, which decouples the query embedding layer to be trained privately, to\nencode respective backgrounds. Meanwhile, other parameters are optimized\nglobally to capture the consistent representations of instruments, including\nthe temporal layer to capture similar motion patterns. A textual-guided channel\nselection is further designed to highlight site-specific features, facilitating\nmodel adapta tion to each site. Moreover, in global-server training, we propose\nSynthesis-based Explicit Representation Quantification (SERQ), which defines an\nexplicit representation target based on synthetic data to synchronize the model\nconvergence during fusion for improving model generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6848FedST\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u589e\u5f3a\u65f6\u7a7a\u8868\u793a\uff0c\u5229\u7528\u624b\u672f\u9886\u57df\u77e5\u8bc6\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u624b\u672f\u5668\u68b0\u5206\u5272\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5982\u591a\u6837\u89e3\u5256\u80cc\u666f\u4e0e\u76f8\u4f3c\u5668\u68b0\u8868\u793a\uff0c\u4ee5\u53ca\u5408\u6210\u6570\u636e\u7684\u5229\u7528\u3002", "method": "\u672c\u5730\u8bad\u7ec3\u91c7\u7528\u8868\u793a\u5206\u79bb\u4e0e\u5408\u4f5c\u673a\u5236\uff08RSC\uff09\uff0c\u5168\u5c40\u8bad\u7ec3\u63d0\u51fa\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u663e\u5f0f\u8868\u793a\u91cf\u5316\uff08SERQ\uff09\u3002", "result": "\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5404\u624b\u672f\u7ad9\u70b9\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FedST\u6709\u6548\u7ed3\u5408\u624b\u672f\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u4e0b\u7684\u624b\u672f\u5668\u68b0\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2506.23723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23723", "abs": "https://arxiv.org/abs/2506.23723", "authors": ["Jozsef Palmieri", "Paolo Di Lillo", "Stefano Chiaverini", "Alessandro Marino"], "title": "A comprehensive control architecture for semi-autonomous dual-arm robots in agriculture settings", "comment": null, "summary": "The adoption of mobile robotic platforms in complex environments, such as\nagricultural settings, requires these systems to exhibit a flexible yet\neffective architecture that integrates perception and control. In such\nscenarios, several tasks need to be accomplished simultaneously, ranging from\nmanaging robot limits to performing operational tasks and handling human\ninputs. The purpose of this paper is to present a comprehensive control\narchitecture for achieving complex tasks such as robotized harvesting in\nvineyards within the framework of the European project CANOPIES. In detail, a\n16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical\nQuadratic Programming (HQP) approach capable of handling both equality and\ninequality constraints at various priorities to harvest grape bunches selected\nby the perception system developed within the project. Furthermore, given the\ncomplexity of the scenario and the uncertainty in the perception system, which\ncould potentially lead to collisions with the environment, the handling of\ninteraction forces is necessary. Remarkably, this was achieved using the same\nHQP framework. This feature is further leveraged to enable semi-autonomous\noperations, allowing a human operator to assist the robotic counterpart in\ncompleting harvesting tasks. Finally, the obtained results are validated\nthrough extensive testing conducted first in a laboratory environment to prove\nindividual functionalities, then in a real vineyard, encompassing both\nautonomous and semi-autonomous grape harvesting operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u590d\u6742\u519c\u4e1a\u73af\u5883\u7684\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5c42\u4e8c\u6b21\u89c4\u5212\uff08HQP\uff09\u65b9\u6cd5\uff0c\u652f\u6301\u81ea\u4e3b\u548c\u534a\u81ea\u4e3b\u8461\u8404\u91c7\u6458\u4efb\u52a1\u3002", "motivation": "\u5728\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u67b6\u6784\u6765\u6574\u5408\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u4ee5\u5b8c\u6210\u591a\u4efb\u52a1\u64cd\u4f5c\uff0c\u5982\u8461\u8404\u91c7\u6458\u3002", "method": "\u4f7f\u752816\u81ea\u7531\u5ea6\u53cc\u81c2\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u91c7\u7528HQP\u65b9\u6cd5\u5904\u7406\u4f18\u5148\u7ea7\u4e0d\u540c\u7684\u7b49\u5f0f\u548c\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u5e76\u7ed3\u5408\u611f\u77e5\u7cfb\u7edf\u9009\u62e9\u8461\u8404\u4e32\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5ba4\u548c\u5b9e\u9645\u8461\u8404\u56ed\u7684\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u67b6\u6784\u5728\u81ea\u4e3b\u548c\u534a\u81ea\u4e3b\u6a21\u5f0f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u6210\u529f\u5b9e\u73b0\u4e86\u590d\u6742\u4efb\u52a1\u7684\u5904\u7406\uff0c\u5e76\u652f\u6301\u4eba\u673a\u534f\u4f5c\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23051", "abs": "https://arxiv.org/abs/2506.23051", "authors": ["Jo\u00e3o Lucas Luz Lima Sarcinelli", "Marina Lages Gon\u00e7alves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MariNER\uff0c\u9996\u4e2a\u9488\u5bf920\u4e16\u7eaa\u521d\u5df4\u897f\u8461\u8404\u7259\u8bed\u7684\u5386\u53f2\u6587\u672c\u7684\u9ec4\u91d1\u6807\u51c6NER\u6570\u636e\u96c6\uff0c\u5305\u542b9000\u591a\u53e5\u624b\u52a8\u6807\u6ce8\u7684\u53e5\u5b50\uff0c\u5e76\u8bc4\u4f30\u4e86\u6700\u5148\u8fdbNER\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5df4\u897f\u8461\u8404\u7259\u8bed\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684NER\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u5728\u5386\u53f2\u6587\u672c\u9886\u57df\uff0c\u5f71\u54cd\u4e86\u6570\u5b57\u4eba\u6587\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u4e86MariNER\u6570\u636e\u96c6\uff0c\u5305\u542b9000\u591a\u53e5\u624b\u52a8\u6807\u6ce8\u7684\u5386\u53f2\u6587\u672c\u53e5\u5b50\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709NER\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "MariNER\u662f\u9996\u4e2a\u9488\u5bf920\u4e16\u7eaa\u521d\u5df4\u897f\u8461\u8404\u7259\u8bed\u5386\u53f2\u6587\u672c\u7684\u9ec4\u91d1\u6807\u51c6NER\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8d44\u6e90\u7a7a\u767d\u3002", "conclusion": "MariNER\u4e3a\u5df4\u897f\u8461\u8404\u7259\u8bed\u5386\u53f2\u6587\u672c\u7684NER\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2506.23673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23673", "abs": "https://arxiv.org/abs/2506.23673", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J. Sch\u00fcffler"], "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift", "comment": null, "summary": "Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHASD\u7684\u5206\u5c42\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u75c5\u7406\u5b66AI\u4e2d\u7684\u5e7b\u706f\u7247\u7ea7\u57df\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u75c5\u7406\u5b66\u6570\u636e\u53d7\u4e2d\u5fc3\u7279\u5b9a\u6761\u4ef6\u5f71\u54cd\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u50cf\u5757\u800c\u5ffd\u7565\u4e86\u5168\u5c40WSI\u7279\u5f81\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e34\u5e8a\u9700\u6c42\u3002", "method": "HASD\u6846\u67b6\u5305\u542b\u5c42\u6b21\u5316\u9002\u5e94\u6a21\u5757\uff08\u57df\u7ea7\u5bf9\u9f50\u3001\u5e7b\u706f\u7247\u7ea7\u51e0\u4f55\u4e0d\u53d8\u6027\u6b63\u5219\u5316\u548c\u5757\u7ea7\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff09\u548c\u539f\u578b\u9009\u62e9\u673a\u5236\u3002", "result": "\u5728\u4e24\u4e2a\u4efb\u52a1\u4e2d\uff0cHASD\u5206\u522b\u63d0\u5347\u4e864.1%\u7684AUROC\uff08\u4e73\u817a\u764cHER2\u5206\u7ea7\uff09\u548c3.9%\u7684C\u6307\u6570\uff08UCEC\u751f\u5b58\u9884\u6d4b\uff09\u3002", "conclusion": "HASD\u4e3a\u75c5\u7406\u5b66\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u9760\u7684\u5e7b\u706f\u7247\u7ea7\u57df\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2506.22726", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22726", "abs": "https://arxiv.org/abs/2506.22726", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "comment": null, "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "AI": {"tldr": "XTransfer\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u3001\u6a21\u6001\u65e0\u5173\u7684\u6a21\u578b\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u4fee\u590d\u548c\u5c42\u91cd\u7ec4\u89e3\u51b3\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u7684\u6311\u6218\u3002", "motivation": "\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u5f00\u53d1\u53d7\u9650\u4e8e\u4f20\u611f\u5668\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u8d44\u6e90\u9650\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u504f\u79fb\u548c\u9ad8\u8d44\u6e90\u9700\u6c42\u95ee\u9898\u3002", "method": "XTransfer\u901a\u8fc7\u6a21\u578b\u4fee\u590d\uff08\u4fee\u590d\u6a21\u6001\u504f\u79fb\uff09\u548c\u5c42\u91cd\u7ec4\uff08\u9ad8\u6548\u641c\u7d22\u548c\u91cd\u7ec4\u6e90\u6a21\u578b\u5c42\uff09\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684\u6a21\u578b\u8fc1\u79fb\u3002", "result": "XTransfer\u5728\u591a\u79cd\u4eba\u7c7b\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8fb9\u7f18\u90e8\u7f72\u7684\u6210\u672c\u3002", "conclusion": "XTransfer\u4e3a\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u7684\u4eba\u7c7b\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.24003", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24003", "abs": "https://arxiv.org/abs/2506.24003", "authors": ["Junqi Liu", "Dongli He", "Wenxuan Li", "Ningyu Wang", "Alan L. Yuille", "Zongwei Zhou"], "title": "ShapeKit", "comment": null, "summary": "In this paper, we present a practical approach to improve anatomical shape\naccuracy in whole-body medical segmentation. Our analysis shows that a\nshape-focused toolkit can enhance segmentation performance by over 8%, without\nthe need for model re-training or fine-tuning. In comparison, modifications to\nmodel architecture typically lead to marginal gains of less than 3%. Motivated\nby this observation, we introduce ShapeKit, a flexible and easy-to-integrate\ntoolkit designed to refine anatomical shapes. This work highlights the\nunderappreciated value of shape-based tools and calls attention to their\npotential impact within the medical segmentation community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u63d0\u9ad8\u5168\u8eab\u533b\u5b66\u5206\u5272\u4e2d\u89e3\u5256\u5f62\u72b6\u51c6\u786e\u6027\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc78%\u3002", "motivation": "\u53d1\u73b0\u5f62\u72b6\u4f18\u5316\u5de5\u5177\u5bf9\u5206\u5272\u6027\u80fd\u7684\u63d0\u5347\u663e\u8457\u4f18\u4e8e\u6a21\u578b\u67b6\u6784\u4fee\u6539\uff0c\u540e\u8005\u4ec5\u5e26\u6765\u4e0d\u52303%\u7684\u6539\u8fdb\u3002", "method": "\u5f00\u53d1\u4e86ShapeKit\u5de5\u5177\u5305\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u89e3\u5256\u5f62\u72b6\uff0c\u6613\u4e8e\u96c6\u6210\u3002", "result": "ShapeKit\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5f62\u72b6\u5de5\u5177\u5728\u533b\u5b66\u5206\u5272\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u547c\u5401\u793e\u533a\u91cd\u89c6\u5176\u5e94\u7528\u3002"}}
{"id": "2506.23725", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23725", "abs": "https://arxiv.org/abs/2506.23725", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "AI": {"tldr": "PAC Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7269\u7406\u5c5e\u6027\u3001\u529f\u80fd\u6027\u548c\u7ea6\u675f\u7406\u89e3\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1VLMs\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u4f4e\u5c42\u7269\u7406\u524d\u63d0\u7684\u7406\u89e3\u5c1a\u672a\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165PAC Bench\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30VLMs\u5bf9\u7269\u7406\u6982\u5ff5\u7684\u7406\u89e3\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524dVLMs\u5728\u57fa\u7840\u7269\u7406\u6982\u5ff5\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "PAC Bench\u4e3aVLM\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u5e76\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
{"id": "2506.23056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23056", "abs": "https://arxiv.org/abs/2506.23056", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "comment": "ACL 2025 Main", "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u589e\u5f3a\u7684\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u6846\u67b6\uff08K-MSE\uff09\uff0c\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u5e93\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63d0\u5347LLMs\u5728\u5316\u5b66\u7ed3\u6784\u89e3\u6790\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLMs\u5728\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u4e13\u4e1a\u5316\u5b66\u77e5\u8bc6\u3002", "method": "\u6784\u5efa\u5916\u90e8\u5206\u5b50\u5b50\u7ed3\u6784\u77e5\u8bc6\u5e93\uff0c\u8bbe\u8ba1\u5206\u5b50-\u5149\u8c31\u8bc4\u5206\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cGPT-4o-mini\u548cGPT-4o\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8620%\u4ee5\u4e0a\u3002", "conclusion": "K-MSE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u4e2d\u7684\u77e5\u8bc6\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.23689", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23689", "abs": "https://arxiv.org/abs/2506.23689", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "title": "Pok\u00e9AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "comment": null, "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "AI": {"tldr": "Pok\\'eAI\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u591a\u4ee3\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u73a9Pok\\'emon Red\u6e38\u620f\u3002\u5b83\u7531\u89c4\u5212\u3001\u6267\u884c\u548c\u8bc4\u4f30\u4e09\u4e2a\u4ee3\u7406\u7ec4\u6210\uff0c\u5f62\u6210\u4e00\u4e2a\u95ed\u73af\u51b3\u7b56\u7cfb\u7edf\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u6218\u6597\u6a21\u5757\u7684\u80dc\u7387\u4e3a80.8%\uff0c\u63a5\u8fd1\u4eba\u7c7b\u73a9\u5bb6\u6c34\u5e73\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u73a9Pok\\'emon Red\u7684\u591a\u4ee3\u7406LLM\u6846\u67b6\uff0c\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u6e38\u620f\u7b56\u7565\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7cfb\u7edf\u7531\u4e09\u4e2a\u4ee3\u7406\u7ec4\u6210\uff1a\u89c4\u5212\u4ee3\u7406\u751f\u6210\u4efb\u52a1\uff0c\u6267\u884c\u4ee3\u7406\u5b8c\u6210\u4efb\u52a1\uff0c\u8bc4\u4f30\u4ee3\u7406\u9a8c\u8bc1\u7ed3\u679c\u3002\u521d\u6b65\u5f00\u53d1\u4e86\u6218\u6597\u6a21\u5757\u3002", "result": "\u6218\u6597\u6a21\u5757\u572850\u6b21\u91ce\u751f\u5bf9\u6218\u4e2d\u5e73\u5747\u80dc\u7387\u4e3a80.8%\uff0c\u63a5\u8fd1\u4eba\u7c7b\u73a9\u5bb6\u6c34\u5e73\u3002\u8bed\u8a00\u80fd\u529b\u4e0e\u6218\u7565\u63a8\u7406\u80fd\u529b\u76f8\u5173\uff0c\u4e0d\u540cLLM\u8868\u73b0\u51fa\u72ec\u7279\u7684\u6e38\u620f\u98ce\u683c\u3002", "conclusion": "Pok\\'eAI\u5c55\u793a\u4e86LLM\u5728\u6e38\u620f\u7b56\u7565\u4e2d\u7684\u6f5c\u529b\uff0c\u8bed\u8a00\u80fd\u529b\u4e0e\u6218\u7565\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u5173\u8054\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u51fa\u72ec\u7279\u7684\u6e38\u620f\u98ce\u683c\u3002"}}
{"id": "2506.22736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22736", "abs": "https://arxiv.org/abs/2506.22736", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "comment": "Accepted by ICCV2025", "summary": "Current multimodal medical image fusion typically assumes that source images\nare of high quality and perfectly aligned at the pixel level. Its effectiveness\nheavily relies on these conditions and often deteriorates when handling\nmisaligned or degraded medical images. To address this, we propose UniFuse, a\ngeneral fusion framework. By embedding a degradation-aware prompt learning\nmodule, UniFuse seamlessly integrates multi-directional information from input\nimages and correlates cross-modal alignment with restoration, enabling joint\noptimization of both tasks within a unified framework. Additionally, we design\nan Omni Unified Feature Representation scheme, which leverages Spatial Mamba to\nencode multi-directional features and mitigate modality differences in feature\nalignment. To enable simultaneous restoration and fusion within an All-in-One\nconfiguration, we propose a Universal Feature Restoration & Fusion module,\nincorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA\nprinciples. By leveraging ALSN's adaptive feature representation along with\ndegradation-type guidance, we enable joint restoration and fusion within a\nsingle-stage framework. Compared to staged approaches, UniFuse unifies\nalignment, restoration, and fusion within a single framework. Experimental\nresults across multiple datasets demonstrate the method's effectiveness and\nsignificant advantages over existing approaches.", "AI": {"tldr": "UniFuse\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u89e3\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u6a21\u5757\u548cOmni\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u672a\u5bf9\u9f50\u6216\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9f50\u3001\u6062\u590d\u548c\u878d\u5408\u7684\u4e00\u4f53\u5316\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u548c\u5bf9\u9f50\u826f\u597d\u7684\u8f93\u5165\u56fe\u50cf\uff0c\u4f46\u5728\u5904\u7406\u672a\u5bf9\u9f50\u6216\u8d28\u91cf\u5dee\u7684\u56fe\u50cf\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "UniFuse\u901a\u8fc7\u964d\u89e3\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u6a21\u5757\u3001Omni\u7edf\u4e00\u7279\u5f81\u8868\u793a\u65b9\u6848\u548c\u901a\u7528\u7279\u5f81\u6062\u590d\u4e0e\u878d\u5408\u6a21\u5757\uff08ALSN\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9f50\u3001\u6062\u590d\u548c\u878d\u5408\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUniFuse\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "UniFuse\u901a\u8fc7\u4e00\u4f53\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u5bf9\u9f50\u548c\u6062\u590d\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.24014", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2506.24014", "abs": "https://arxiv.org/abs/2506.24014", "authors": ["Peng Lin", "Xuesong Wang", "Yating Chen", "Xianyu Wu", "Feng Huang", "Shouqian Chen"], "title": "Simultaneous Super-Resolution of Spatial and Spectral Imaging with a Camera Array and Notch Filters", "comment": null, "summary": "This study proposes an algorithm based on a notch filter camera array system\nfor simultaneous super-resolution imaging and spectral reconstruction,\nenhancing the spatial resolution and multispectral imaging capabilities of\ntargets. In this study, multi-aperture super-resolution algorithms,\npan-sharpening techniques, and spectral reconstruction algorithms were\ninvestigated and integrated. The sub-pixel level offset information and\nspectral disparities among the 9 low-resolution images captured by the 9\ndistinct imaging apertures were utilized, leading to the successful\nreconstruction of 31 super-resolution spectral images. By conducting\nsimulations with a publicly available dataset and performing qualitative and\nquantitative comparisons with snapshot coded aperture spectral imaging systems,\nthe experimental results demonstrate that our system and algorithm attained a\npeak signal-to-noise ratio of 35.6dB, representing a 5dB enhancement over the\nmost advanced snapshot coded aperture spectral imaging systems, while also\nreducing processing time. This research offers an effective solution for\nachieving high temporal, spectral, and spatial resolution through the\nutilization of multi-aperture imaging systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9677\u6ce2\u6ee4\u6ce2\u5668\u76f8\u673a\u9635\u5217\u7cfb\u7edf\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u6210\u50cf\u548c\u5149\u8c31\u91cd\u5efa\uff0c\u63d0\u5347\u76ee\u6807\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u591a\u5149\u8c31\u6210\u50cf\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u591a\u5b54\u5f84\u6210\u50cf\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u65f6\u95f4\u3001\u5149\u8c31\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u591a\u5b54\u5f84\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\u3001\u5168\u8272\u9510\u5316\u6280\u672f\u548c\u5149\u8c31\u91cd\u5efa\u7b97\u6cd5\uff0c\u5229\u75289\u4e2a\u6210\u50cf\u5b54\u5f84\u6355\u83b7\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4e9a\u50cf\u7d20\u7ea7\u504f\u79fb\u4fe1\u606f\u548c\u5149\u8c31\u5dee\u5f02\u3002", "result": "\u6210\u529f\u91cd\u5efa\u4e8631\u5e45\u8d85\u5206\u8fa8\u7387\u5149\u8c31\u56fe\u50cf\uff0c\u5cf0\u503c\u4fe1\u566a\u6bd4\u8fbe35.6dB\uff0c\u6bd4\u73b0\u6709\u5feb\u7167\u7f16\u7801\u5b54\u5f84\u5149\u8c31\u6210\u50cf\u7cfb\u7edf\u63d0\u53475dB\uff0c\u4e14\u5904\u7406\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u5b54\u5f84\u6210\u50cf\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23739", "categories": ["cs.RO", "cs.CE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.23739", "abs": "https://arxiv.org/abs/2506.23739", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M\u00fcller"], "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u53f0\u548c\u8fd0\u52a8\u5b9e\u9a8c\u5ba4\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u7528\u4e8e\u9a8c\u8bc1\u8f66\u8f86\u4e0e\u884c\u4eba\u53ca\u9a91\u884c\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u6bd4\u8f83\u771f\u5b9e\u4e16\u754c\u548c\u865a\u62df\u73af\u5883\u4e2d\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u8bc4\u4f30\u4e86\u611f\u77e5\u6280\u672f\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u4e4b\u95f4\u7684\u5b89\u5168\u3001\u771f\u5b9e\u4ea4\u4e92\uff0c\u9700\u8981\u5148\u8fdb\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u8f66\u8f86\u5728\u73af\u6d4b\u8bd5\u53f0\u548c\u8fd0\u52a8\u5b9e\u9a8c\u5ba4\uff0c\u5229\u7528\u865a\u5e7b\u5f15\u64ce5\u751f\u6210\u865a\u62df\u573a\u666f\uff0c\u5b9e\u65f6\u6295\u5f71VRUs\u52a8\u753b\u4ee5\u523a\u6fc0\u6444\u50cf\u5934\uff0c\u901a\u8fc7\u5355\u76ee\u6444\u50cf\u5934AI\u8fdb\u884c3D\u9aa8\u9abc\u68c0\u6d4b\uff0c\u6bd4\u8f83\u771f\u5b9e\u4e0e\u865a\u62df\u73af\u5883\u4e2d\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7a33\u5b9a\u8fd0\u52a8\u6a21\u5f0f\u4e0b\uff0c\u771f\u5b9e\u4e0e\u865a\u62df\u73af\u5883\u4e2d\u7684\u59ff\u6001\u4f30\u8ba1\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u5728\u52a8\u6001\u8fd0\u52a8\u548c\u906e\u6321\u60c5\u51b5\u4e0b\uff0c\u7279\u522b\u662f\u590d\u6742\u9a91\u884c\u8005\u59ff\u6001\uff0c\u4ecd\u5b58\u5728\u663e\u8457\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u4e0b\u4e00\u4ee3\u57fa\u4e8eAI\u7684\u8f66\u8f86\u611f\u77e5\u6d4b\u8bd5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5e76\u4f18\u5316\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0eVRUs\u5728\u865a\u62df\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u6a21\u578b\u3002"}}
{"id": "2506.23071", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23071", "abs": "https://arxiv.org/abs/2506.23071", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "comment": "Work in progess", "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "AI": {"tldr": "Text2VectorSQL\u6846\u67b6\u7ed3\u5408Text-to-SQL\u4e0e\u5411\u91cf\u641c\u7d22\uff0c\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u6570\u636e\u548c\u6a21\u7cca\u67e5\u8be2\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u4e49\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u548c\u6a21\u7cca\u67e5\u8be2\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u5411\u91cf\u641c\u7d22\u867d\u5f3a\u5927\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faText2VectorSQL\uff0c\u652f\u6301\u8bed\u4e49\u8fc7\u6ee4\u3001\u591a\u6a21\u6001\u5339\u914d\u548c\u68c0\u7d22\u52a0\u901f\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u548c\u4e13\u5bb6\u8bc4\u5ba1\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aText2VectorSQL\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Text2VectorSQL\u4e3a\u66f4\u7075\u6d3b\u76f4\u89c2\u7684\u6570\u636e\u5e93\u63a5\u53e3\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23692", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23692", "abs": "https://arxiv.org/abs/2506.23692", "authors": ["Boyuan Zheng", "Zerui Fang", "Zhe Xu", "Rui Wang", "Yiwen Chen", "Cunshi Wang", "Mengwei Qu", "Lei Lei", "Zhen Feng", "Yan Liu", "Yuyang Li", "Mingzhou Tan", "Jiaji Wu", "Jianwei Shuai", "Jia Li", "Fangfu Ye"], "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models", "comment": null, "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528LLM\u9a71\u52a8\u7684Agent4S\uff08\u79d1\u5b66\u4ee3\u7406\uff09\u4f5c\u4e3a\u7b2c\u4e94\u79d1\u5b66\u8303\u5f0f\uff0c\u4ee5\u81ea\u52a8\u5316\u6574\u4e2a\u79d1\u7814\u5de5\u4f5c\u6d41\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u7ea7\u5206\u7c7b\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI4S\uff08\u79d1\u5b66AI\uff09\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u672a\u80fd\u89e3\u51b3\u79d1\u7814\u6838\u5fc3\u6548\u7387\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAgent4S\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u7ea7\u5206\u7c7b\u4ece\u7b80\u5355\u4efb\u52a1\u81ea\u52a8\u5316\u5230\u5b8c\u5168\u81ea\u4e3b\u534f\u4f5c\u7684\u201cAI\u79d1\u5b66\u5bb6\u201d\u3002", "result": "\u5b9a\u4e49\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u9769\u547d\u6027\u4e0b\u4e00\u6b65\uff0c\u4e3a\u79d1\u7814\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6e05\u6670\u8def\u7ebf\u56fe\u3002", "conclusion": "Agent4S\u662f\u771f\u6b63\u7684\u7b2c\u4e94\u79d1\u5b66\u8303\u5f0f\uff0c\u5c06\u5f7b\u5e95\u6539\u53d8\u79d1\u7814\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2506.22749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22749", "abs": "https://arxiv.org/abs/2506.22749", "authors": ["Yun Zhang", "Feifan Chen", "Na Li", "Zhiwei Guo", "Xu Wang", "Fen Miao", "Sam Kwong"], "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds", "comment": null, "summary": "Colored point cloud, which includes geometry and attribute components, is a\nmainstream representation enabling realistic and immersive 3D applications. To\ngenerate large-scale and denser colored point clouds, we propose a deep\nlearning-based Joint Geometry and Attribute Up-sampling (JGAU) method that\nlearns to model both geometry and attribute patterns while leveraging spatial\nattribute correlations. First, we establish and release a large-scale dataset\nfor colored point cloud up-sampling called SYSU-PCUD, containing 121\nlarge-scale colored point clouds with diverse geometry and attribute\ncomplexities across six categories and four sampling rates. Second, to improve\nthe quality of up-sampled point clouds, we propose a deep learning-based JGAU\nframework that jointly up-samples geometry and attributes. It consists of a\ngeometry up-sampling network and an attribute up-sampling network, where the\nlatter leverages the up-sampled auxiliary geometry to model neighborhood\ncorrelations of the attributes. Third, we propose two coarse attribute\nup-sampling methods, Geometric Distance Weighted Attribute Interpolation\n(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate\ncoarse up-sampled attributes for each point. Then, an attribute enhancement\nmodule is introduced to refine these up-sampled attributes and produce\nhigh-quality point clouds by further exploiting intrinsic attribute and\ngeometry patterns. Extensive experiments show that the Peak Signal-to-Noise\nRatio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10\ndecibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,\n8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art\nmethods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28\ndecibels, and 2.11 decibels at these four up-sampling rates, demonstrating\nsignificant improvement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8054\u5408\u51e0\u4f55\u548c\u5c5e\u6027\u4e0a\u91c7\u6837\u65b9\u6cd5\uff08JGAU\uff09\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u4e14\u66f4\u5bc6\u96c6\u7684\u5f69\u8272\u70b9\u4e91\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f69\u8272\u70b9\u4e91\u662f3D\u5e94\u7528\u7684\u4e3b\u6d41\u8868\u793a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u5927\u89c4\u6a21\u4e14\u5bc6\u96c6\u7684\u5f69\u8272\u70b9\u4e91\u65f6\u5b58\u5728\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faJGAU\u6846\u67b6\uff0c\u5305\u62ec\u51e0\u4f55\u4e0a\u91c7\u6837\u7f51\u7edc\u548c\u5c5e\u6027\u4e0a\u91c7\u6837\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u7c97\u5c5e\u6027\u4e0a\u91c7\u6837\u65b9\u6cd5\uff08GDWAI\u548cDLAI\uff09\u53ca\u5c5e\u6027\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cJGAU\u57284\u500d\u30018\u500d\u300112\u500d\u548c16\u500d\u4e0a\u91c7\u6837\u7387\u4e0b\u7684PSNR\u5206\u522b\u4e3a33.90\u300132.10\u300131.10\u548c30.39\u5206\u8d1d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "JGAU\u65b9\u6cd5\u5728\u5f69\u8272\u70b9\u4e91\u4e0a\u91c7\u6837\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a3D\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.24074", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.24074", "abs": "https://arxiv.org/abs/2506.24074", "authors": ["Mayank V. Golhar", "Lucas Sebastian Galeano Fretes", "Loren Ayers", "Venkata S. Akshintala", "Taylor L. Bobrow", "Nicholas J. Durr"], "title": "C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism", "comment": "19 pages, 7 figures", "summary": "Computer vision techniques have the potential to improve the diagnostic\nperformance of colonoscopy, but the lack of 3D colonoscopy datasets for\ntraining and validation hinders their development. This paper introduces\nC3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video\nDataset, featuring enhanced realism designed to facilitate the quantitative\nevaluation of 3D colon reconstruction algorithms. 192 video sequences were\ncaptured by imaging 60 unique, high-fidelity silicone colon phantom segments.\nGround truth depth, surface normals, optical flow, occlusion,\nsix-degree-of-freedom pose, coverage maps, and 3D models are provided for 169\ncolonoscopy videos. Eight simulated screening colonoscopy videos acquired by a\ngastroenterologist are provided with ground truth poses. The dataset includes\n15 videos featuring colon deformations for qualitative assessment. C3VDv2\nemulates diverse and challenging scenarios for 3D reconstruction algorithms,\nincluding fecal debris, mucous pools, blood, debris obscuring the colonoscope\nlens, en-face views, and fast camera motion. The enhanced realism of C3VDv2\nwill allow for more robust and representative development and evaluation of 3D\nreconstruction algorithms.", "AI": {"tldr": "C3VDv2\u662f\u4e00\u4e2a\u9ad8\u4eff\u771f3D\u7ed3\u80a0\u955c\u89c6\u9891\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u63013D\u7ed3\u80a0\u91cd\u5efa\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1\u76843D\u7ed3\u80a0\u955c\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u7ed3\u80a0\u955c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc760\u4e2a\u9ad8\u4eff\u771f\u7845\u80f6\u7ed3\u80a0\u6a21\u578b\u91c7\u96c6192\u4e2a\u89c6\u9891\u5e8f\u5217\uff0c\u63d0\u4f9b\u6df1\u5ea6\u3001\u8868\u9762\u6cd5\u7ebf\u7b49\u591a\u9879\u771f\u5b9e\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b169\u4e2a\u89c6\u9891\u7684\u771f\u5b9e\u6570\u636e\uff0c8\u4e2a\u6a21\u62df\u7b5b\u67e5\u89c6\u9891\u548c15\u4e2a\u7ed3\u80a0\u53d8\u5f62\u89c6\u9891\uff0c\u6a21\u62df\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u3002", "conclusion": "C3VDv2\u7684\u9ad8\u4eff\u771f\u6027\u5c06\u4fc3\u8fdb3D\u91cd\u5efa\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u4ee3\u8868\u6027\u53d1\u5c55\u3002"}}
{"id": "2506.23768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23768", "abs": "https://arxiv.org/abs/2506.23768", "authors": ["Vittorio La Barbera", "Steven Bohez", "Leonard Hasenclever", "Yuval Tassa", "John R. Hutchinson"], "title": "Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model", "comment": null, "summary": "We introduce a novel musculoskeletal model of a dog, procedurally generated\nfrom accurate 3D muscle meshes. Accompanying this model is a motion\ncapture-based locomotion task compatible with a variety of control algorithms,\nas well as an improved muscle dynamics model designed to enhance convergence in\ndifferentiable control frameworks. We validate our approach by comparing\nsimulated muscle activation patterns with experimentally obtained\nelectromyography (EMG) data from previous canine locomotion studies. This work\naims to bridge gaps between biomechanics, robotics, and computational\nneuroscience, offering a robust platform for researchers investigating muscle\nactuation and neuromuscular control.We plan to release the full model along\nwith the retargeted motion capture clips to facilitate further research and\ndevelopment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u808c\u8089\u7f51\u683c\u7684\u72d7\u9aa8\u9abc\u808c\u8089\u6a21\u578b\uff0c\u7ed3\u5408\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u548c\u6539\u8fdb\u7684\u808c\u8089\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6a21\u62df\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b9e\u9a8c\u6570\u636e\u7684\u5339\u914d\u6027\u3002", "motivation": "\u586b\u8865\u751f\u7269\u529b\u5b66\u3001\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e4b\u95f4\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u808c\u8089\u9a71\u52a8\u548c\u795e\u7ecf\u808c\u8089\u63a7\u5236\u7814\u7a76\u63d0\u4f9b\u5e73\u53f0\u3002", "method": "\u4f7f\u7528\u7a0b\u5e8f\u751f\u6210\u76843D\u808c\u8089\u7f51\u683c\u6784\u5efa\u9aa8\u9abc\u808c\u8089\u6a21\u578b\uff0c\u7ed3\u5408\u8fd0\u52a8\u6355\u6349\u4efb\u52a1\u548c\u6539\u8fdb\u7684\u808c\u8089\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u6a21\u62df\u7684\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u5b9e\u9a8cEMG\u6570\u636e\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6a21\u578b\u548c\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.23101", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23101", "abs": "https://arxiv.org/abs/2506.23101", "authors": ["Yue Xu", "Wenjie Wang"], "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGenres\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53cc\u4eba\u4e92\u52a8\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u5355\u573a\u666f\u8bc4\u4f30\u672a\u53d1\u73b0\u7684\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u5355\u573a\u666f\u504f\u89c1\uff0c\u5ffd\u89c6\u4e86\u4eba\u9645\u4e92\u52a8\u4e2d\u6f5c\u5728\u7684\u5fae\u5999\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u53cc\u89d2\u8272\u914d\u7f6e\u548c\u53d9\u4e8b\u751f\u6210\u4efb\u52a1\uff0c\u8bbe\u8ba1Genres\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u7ef4\u5ea6\u6027\u522b\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5747\u5b58\u5728\u5355\u573a\u666f\u4e2d\u672a\u663e\u73b0\u7684\u3001\u60c5\u5883\u654f\u611f\u7684\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u5173\u7cfb\u611f\u77e5\u7684\u57fa\u51c6\u5bf9\u8bca\u65ad\u4e92\u52a8\u9a71\u52a8\u7684\u6027\u522b\u504f\u89c1\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u504f\u89c1\u7f13\u89e3\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2506.23703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23703", "abs": "https://arxiv.org/abs/2506.23703", "authors": ["Lars Ullrich", "Walter Zimmer", "Ross Greer", "Knut Graichen", "Alois C. Knoll", "Mohan Trivedi"], "title": "A New Perspective On AI Safety Through Control Theory Methodologies", "comment": "Accepted to be published as part of the 2025 IEEE Open Journal of\n  Intelligent Transportation Systems (OJ-ITS)", "summary": "While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u548c\u6570\u636e\u5206\u6790\u7684\u65b0\u89c6\u89d2\uff0c\u65e8\u5728\u901a\u8fc7\u8de8\u5b66\u79d1\u65b9\u6cd5\u63d0\u5347AI\u5b89\u5168\u6027\uff0c\u79f0\u4e3a\u201c\u6570\u636e\u63a7\u5236\u201d\u3002", "motivation": "AI\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u7f3a\u4e4f\u8db3\u591f\u7684\u5b89\u5168\u6027\u4fdd\u969c\uff0c\u9700\u8981\u7ed3\u5408\u63a7\u5236\u7406\u8bba\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u63d0\u5347AI\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u7406\u8bba\u548c\u7cfb\u7edf\u5206\u6790\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u6570\u636e\u63a7\u5236\u7684\u6982\u5ff5\uff0c\u7ed3\u5408\u73b0\u6709\u5b89\u5168\u5206\u6790\u548c\u4fdd\u969c\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5b89\u5168\u5206\u6790\u548c\u4fdd\u969c\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u7279\u5b9aAI\u7cfb\u7edf\u548c\u5e94\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u521b\u65b0\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8de8\u5b66\u79d1\u7684\u6570\u636e\u63a7\u5236\u65b9\u6cd5\u4e3aAI\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u671b\u63a8\u52a8AI\u5de5\u7a0b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.22753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22753", "abs": "https://arxiv.org/abs/2506.22753", "authors": ["Jianing Zhang", "Jiayi Zhu", "Feiyu Ji", "Xiaokang Yang", "Xiaoyun Yuan"], "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography", "comment": null, "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside \\textit{pseudo} data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u8def\u5f84\u6269\u6563\u65b9\u6cd5\uff08Degradation-Modeled Multipath Diffusion\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u91d1\u5c5e\u900f\u955c\u6444\u5f71\u4e2d\u7684\u5149\u5b66\u9000\u5316\u95ee\u9898\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u3002", "motivation": "\u91d1\u5c5e\u900f\u955c\u5728\u8ba1\u7b97\u6210\u50cf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9762\u4e34\u5149\u5b66\u9000\u5316\u548c\u8ba1\u7b97\u6062\u590d\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u6821\u51c6\u6216\u5927\u91cf\u914d\u5bf9\u6570\u636e\uff0c\u4e14\u96be\u4ee5\u63a7\u5236\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u591a\u8def\u5f84\u6269\u6563\u6846\u67b6\uff08\u6b63\u3001\u4e2d\u3001\u8d1f\u63d0\u793a\u8def\u5f84\uff09\u7ed3\u5408\u4f2a\u6570\u636e\u589e\u5f3a\uff0c\u8bbe\u8ba1\u4e86\u53ef\u8c03\u89e3\u7801\u5668\u548c\u7a7a\u95f4\u53d8\u5316\u9000\u5316\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff08SVDA\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u6e05\u6670\u7684\u56fe\u50cf\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u7136\u56fe\u50cf\u5148\u9a8c\u548c\u53ef\u63a7\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u5c5e\u900f\u955c\u6444\u5f71\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22899", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22899", "abs": "https://arxiv.org/abs/2506.22899", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S\u00fcsstrunk"], "title": "Neural Cellular Automata: From Cells to Pixels", "comment": "6 pages, 5 figures, first draft", "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u89e3\u7801\u5668\u548c\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u6e05\u8f93\u51fa\u3002", "motivation": "NCA\u5728\u4f4e\u5206\u8fa8\u7387\u7f51\u683c\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u56e0\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u5267\u589e\u3001\u4fe1\u606f\u4f20\u64ad\u53d7\u9650\u7b49\u95ee\u9898\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5f15\u5165\u5171\u4eab\u9690\u5f0f\u89e3\u7801\u5668\uff0c\u5728\u7c97\u7f51\u683c\u4e0a\u8fd0\u884cNCA\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u751f\u6210\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u6e05\u8f93\u51fa\uff0c\u4fdd\u6301\u81ea\u7ec4\u7ec7\u548c\u6d8c\u73b0\u7279\u6027\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u5e76\u884c\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u591a\u79cdNCA\u53d8\u4f53\u548c\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86NCA\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2506.23771", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23771", "abs": "https://arxiv.org/abs/2506.23771", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u9ad8\u4f4e\u5c42\u7b56\u7565\uff0c\u5206\u522b\u751f\u6210\u957f\u671f\u8fd0\u52a8\u6307\u5bfc\u548c\u77ed\u671f\u63a7\u5236\u547d\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u6548\u7387\u3001\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u591a\u5ffd\u89c6\u7b56\u7565\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u77ed\u671f\u63a7\u5236\u547d\u4ee4\u6ce2\u52a8\u6216\u957f\u671f\u76ee\u6807\u65e0\u6cd5\u7edf\u4e00\u4f18\u5316\u9a7e\u9a76\u884c\u4e3a\u4e0e\u63a7\u5236\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\u7ed3\u6784\uff0c\u7edf\u4e00\u8bad\u7ec3\u9ad8\u4f4e\u5c42\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5206\u522b\u8f93\u51fa\u957f\u671f\u8fd0\u52a8\u6307\u5bfc\u548c\u77ed\u671f\u63a7\u5236\u547d\u4ee4\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728\u6a21\u62df\u5668\u548cHighD\u6570\u636e\u96c6\u7684\u9ad8\u901f\u516c\u8def\u591a\u8f66\u9053\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u63d0\u9ad8\u6548\u7387\u3001\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u591a\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7b56\u7565\u7ed3\u6784\u8bbe\u8ba1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9a7e\u9a76\u884c\u4e3a\u4e0e\u63a7\u5236\u7684\u7edf\u4e00\u4f18\u5316\u3002"}}
{"id": "2506.23111", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23111", "abs": "https://arxiv.org/abs/2506.23111", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "comment": "Accepted in ACL 2025", "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "AI": {"tldr": "INDIC-BIAS\u662f\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u591a\u6837\u6027\u7684\u516c\u5e73\u6027\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8614\u4e2aLLM\u572885\u4e2a\u8eab\u4efd\u7fa4\u4f53\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u5b58\u5728\u5f3a\u70c8\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u897f\u65b9\u6587\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5370\u5ea6\u7b49\u591a\u5143\u6587\u5316\u56fd\u5bb6\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u54a8\u8be2\u548c\u624b\u52a8\u9a8c\u8bc1\uff0c\u751f\u6210\u4e8620,000\u4e2a\u771f\u5b9e\u573a\u666f\u6a21\u677f\uff0c\u5206\u4e3a\u4e09\u7c7b\u4efb\u52a1\uff08\u5408\u7406\u6027\u3001\u5224\u65ad\u548c\u751f\u6210\uff09\u8bc4\u4f30LLM\u3002", "result": "LLM\u5bf9\u8fb9\u7f18\u5316\u8eab\u4efd\u8868\u73b0\u51fa\u5f3a\u70c8\u8d1f\u9762\u504f\u89c1\uff0c\u4e14\u96be\u4ee5\u901a\u8fc7\u7406\u6027\u5316\u51cf\u5c11\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5728\u5370\u5ea6\u80cc\u666f\u4e0b\u66f4\u8c28\u614e\u5730\u4f7f\u7528LLM\uff0c\u5e76\u5f00\u6e90INDIC-BIAS\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2506.23706", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.23706", "abs": "https://arxiv.org/abs/2506.23706", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "comment": "ICML 2024 Workshop TAIG", "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53ef\u9a8c\u8bc1\u5ba1\u8ba1\u201d\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEE\uff09\u786e\u4fddAI\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\uff0c\u540c\u65f6\u4fdd\u62a4\u6a21\u578b\u548c\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6a21\u578bIP\u548c\u57fa\u51c6\u6570\u636e\u96c6\u7684\u4fdd\u5bc6\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3AI\u6cbb\u7406\u6846\u67b6\u7684\u9700\u6c42\u3002", "method": "\u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883\u4e2d\u8fd0\u884c\u53ef\u9a8c\u8bc1\u5ba1\u8ba1\uff0c\u786e\u4fdd\u7528\u6237\u80fd\u591f\u9a8c\u8bc1\u4e0e\u5408\u89c4AI\u6a21\u578b\u7684\u4ea4\u4e92\uff0c\u540c\u65f6\u4fdd\u62a4\u654f\u611f\u6570\u636e\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\uff0c\u5c55\u793a\u4e86\u5728\u5178\u578b\u5ba1\u8ba1\u57fa\u51c6\uff08\u5982Llama-3.1\uff09\u4e0a\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u53ef\u9a8c\u8bc1\u5ba1\u8ba1\u89e3\u51b3\u4e86AI\u6cbb\u7406\u6846\u67b6\u4e2d\u7684\u9a8c\u8bc1\u6311\u6218\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u548c\u6570\u636e\u9690\u79c1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.22756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "AI": {"tldr": "RoboPearls\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\u7684\u53ef\u7f16\u8f91\u89c6\u9891\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4eff\u771f\u751f\u6210\u548c\u6027\u80fd\u5206\u6790\u89e3\u51b3\u73b0\u5b9e\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7684\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u4eff\u771f\u5e73\u53f0\u867d\u63d0\u4f9b\u53ef\u63a7\u73af\u5883\uff0c\u4f46\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "RoboPearls\u5229\u75283D\u9ad8\u65af\u6563\u5c04\u6280\u672f\u6784\u5efa\u903c\u771f\u4eff\u771f\uff0c\u7ed3\u5408\u589e\u91cf\u8bed\u4e49\u84b8\u998f\u548c3D\u6b63\u5219\u5316NNFM\u635f\u5931\uff0c\u5e76\u901a\u8fc7LLMs\u548cVLMs\u5b9e\u73b0\u81ea\u52a8\u5316\u4eff\u771f\u751f\u6210\u4e0e\u6027\u80fd\u5206\u6790\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u573a\u666f\uff08\u5982RLBench\u3001COLOSSEUM\u7b49\uff09\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RoboPearls\u7684\u6709\u6548\u6027\u548c\u4eff\u771f\u6027\u80fd\u3002", "conclusion": "RoboPearls\u901a\u8fc7\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u4eff\u771f\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u6548\u7387\uff0c\u5e76\u6709\u6548\u7f29\u5c0f\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.22902", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22902", "abs": "https://arxiv.org/abs/2506.22902", "authors": ["Yiling Xu", "Yujie Zhang", "Shuting Xia", "Kaifa Yang", "He Huang", "Ziyu Shan", "Wenjie Huang", "Qi Yang", "Le Yang"], "title": "Point Cloud Compression and Objective Quality Assessment: A Survey", "comment": null, "summary": "The rapid growth of 3D point cloud data, driven by applications in autonomous\ndriving, robotics, and immersive environments, has led to criticals demand for\nefficient compression and quality assessment techniques. Unlike traditional 2D\nmedia, point clouds present unique challenges due to their irregular structure,\nhigh data volume, and complex attributes. This paper provides a comprehensive\nsurvey of recent advances in point cloud compression (PCC) and point cloud\nquality assessment (PCQA), emphasizing their significance for real-time and\nperceptually relevant applications. We analyze a wide range of handcrafted and\nlearning-based PCC algorithms, along with objective PCQA metrics. By\nbenchmarking representative methods on emerging datasets, we offer detailed\ncomparisons and practical insights into their strengths and limitations.\nDespite notable progress, challenges such as enhancing visual fidelity,\nreducing latency, and supporting multimodal data remain. This survey outlines\nfuture directions, including hybrid compression frameworks and advanced feature\nextraction strategies, to enable more efficient, immersive, and intelligent 3D\napplications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e863D\u70b9\u4e91\u538b\u7f29\uff08PCC\uff09\u548c\u8d28\u91cf\u8bc4\u4f30\uff08PCQA\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u624b\u5de5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u70b9\u4e91\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u4f46\u5176\u4e0d\u89c4\u5219\u7ed3\u6784\u548c\u9ad8\u6570\u636e\u91cf\u5e26\u6765\u538b\u7f29\u548c\u8d28\u91cf\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790\u624b\u5de5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684PCC\u7b97\u6cd5\u53caPCQA\u6307\u6807\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u4ee3\u8868\u6027\u65b9\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u6307\u51fa\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u5ef6\u8fdf\u548c\u591a\u6a21\u6001\u6570\u636e\u652f\u6301\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u6df7\u5408\u538b\u7f29\u6846\u67b6\u548c\u9ad8\u7ea7\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u6c89\u6d78\u5f0f\u548c\u667a\u80fd\u76843D\u5e94\u7528\u3002"}}
{"id": "2506.23919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23919", "abs": "https://arxiv.org/abs/2506.23919", "authors": ["Haonan Chen", "Bangjun Wang", "Jingxiang Guo", "Tianrui Zhang", "Yiwen Hou", "Xuchuan Huang", "Chenrui Tie", "Lin Shao"], "title": "World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation", "comment": null, "summary": "Improving data efficiency and generalization in robotic manipulation remains\na core challenge. We propose a novel framework that leverages a pre-trained\nmultimodal image-generation model as a world model to guide policy learning. By\nexploiting its rich visual-semantic representations and strong generalization\nacross diverse scenes, the model generates open-ended future state predictions\nthat inform downstream manipulation. Coupled with zero-shot low-level control\nmodules, our approach enables general-purpose robotic manipulation without\ntask-specific training. Experiments in both simulation and real-world\nenvironments demonstrate that our method achieves effective performance across\na wide range of manipulation tasks with no additional data collection or\nfine-tuning. Supplementary materials are available on our website:\nhttps://world4omni.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u6765\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u9ad8\u6548\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u662f\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6a21\u578b\u751f\u6210\u5f00\u653e\u5f0f\u7684\u672a\u6765\u72b6\u6001\u9884\u6d4b\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u4f4e\u7ea7\u63a7\u5236\u6a21\u5757\u5b9e\u73b0\u901a\u7528\u64cd\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.23122", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.23122", "abs": "https://arxiv.org/abs/2506.23122", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u82f1\u8bed\u548c\u6df7\u5408\u8bed\u8a00\uff08\u82f1\u8bed-\u5370\u5730\u8bed\uff09\u7684\u7f51\u7edc\u8ff7\u56e0\u4e2d\u8bc6\u522b\u53d9\u4e8b\u89d2\u8272\uff08\u82f1\u96c4\u3001\u53cd\u6d3e\u3001\u53d7\u5bb3\u8005\u7b49\uff09\u7684\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5f3a\u8c03\u4e86\u6587\u5316\u80cc\u666f\u548c\u591a\u6a21\u6001\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7f51\u7edc\u8ff7\u56e0\u4e2d\u7684\u53d9\u4e8b\u89d2\u8272\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u6587\u5316\u7279\u5b9a\u6027\u548c\u8bed\u8a00\u591a\u6837\u6027\u589e\u52a0\u4e86\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u548c\u5e73\u8861\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982\u591a\u8bed\u8a00Transformer\u3001\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\uff09\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f7f\u7528\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5927\u578b\u6a21\u578b\uff08\u5982DeBERTa-v3\u548cQwen2.5-VL\uff09\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u201c\u53d7\u5bb3\u8005\u201d\u89d2\u8272\u548c\u8de8\u6587\u5316\u5185\u5bb9\u65f6\u4ecd\u5b58\u5728\u6311\u6218\u3002\u6df7\u5408\u63d0\u793a\u7b56\u7565\u80fd\u5e26\u6765\u5c0f\u5e45\u6539\u8fdb\u3002", "conclusion": "\u6587\u5316\u80cc\u666f\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u6a21\u6001\u63a8\u7406\u5bf9\u5efa\u6a21\u89c6\u89c9-\u6587\u672c\u5185\u5bb9\u4e2d\u7684\u53d9\u4e8b\u6846\u67b6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.23773", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.23773", "abs": "https://arxiv.org/abs/2506.23773", "authors": ["Stefano M. Nicoletti", "Mari\u00eblle Stoelinga"], "title": "BayesL: Towards a Logical Framework for Bayesian Networks", "comment": null, "summary": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.", "AI": {"tldr": "BayesL\u662f\u4e00\u4e2a\u65b0\u7684\u903b\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5b9a\u3001\u67e5\u8be2\u548c\u9a8c\u8bc1\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u884c\u4e3a\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u7ed3\u6784\u5316\u8bed\u8a00\uff0c\u652f\u6301\u5bf9\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u67e5\u8be2\u548c\u63a8\u7406\uff0c\u7b80\u5316\u56e0\u679c\u548c\u8bc1\u636e\u5173\u7cfb\u7684\u5206\u6790\u3002", "method": "\u5f00\u53d1BayesL\u8bed\u8a00\uff0c\u652f\u6301\u521b\u5efa\u67e5\u8be2\u548c\u8fdb\u884c\u5047\u8bbe\u573a\u666f\u8bc4\u4f30\uff0c\u65e0\u9700\u624b\u52a8\u4fee\u6539\u6a21\u578b\u3002", "result": "BayesL\u80fd\u591f\u7075\u6d3b\u63a8\u7406\u56e0\u679c\u548c\u8bc1\u636e\u5173\u7cfb\uff0c\u652f\u6301\u5168\u9762\u7684\u5047\u8bbe\u5206\u6790\u3002", "conclusion": "BayesL\u4e3a\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2506.22762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "AI": {"tldr": "VSRM\u662f\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u7a7a\u95f4Mamba\u5757\u63d0\u53d6\u957f\u8ddd\u79bb\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u548c\u9891\u7387\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CNN\u548cTransformer\u65b9\u6cd5\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u5c40\u90e8\u611f\u53d7\u91ce\u9650\u5236\u6216\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0cMamba\u56e0\u5176\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVSRM\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4-\u65f6\u95f4\u548c\u65f6\u95f4-\u7a7a\u95f4Mamba\u5757\u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u53ef\u53d8\u5f62\u4ea4\u53c9Mamba\u5bf9\u9f50\u6a21\u5757\u52a8\u6001\u5bf9\u9f50\u5e27\uff0c\u5e76\u8bbe\u8ba1\u9891\u7387\u635f\u5931\u51fd\u6570\u4f18\u5316\u9ad8\u9891\u5185\u5bb9\u3002", "result": "VSRM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "VSRM\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.23004", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23004", "abs": "https://arxiv.org/abs/2506.23004", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Xicong Li", "Wai Lok Woo", "Luis Nero Alves", "Stanislav Zvanovec", "Tran The Son", "Zabih Ghassemlooy"], "title": "A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks", "comment": null, "summary": "This paper proposes a novel, robust, and lightweight supervised Convolutional\nNeural Network (CNN)-based technique for frame identification and\nsynchronization, designed to enhance short-link communication performance in a\nscreen-to-camera (S2C) based visible light communication (VLC) system.\nDeveloped using Python and the TensorFlow Keras framework, the proposed CNN\nmodel was trained through three real-time experimental investigations conducted\nin Jupyter Notebook. These experiments incorporated a dataset created from\nscratch to address various real-time challenges in S2C communication, including\nblurring, cropping, and rotated images in mobility scenarios. Overhead frames\nwere introduced for synchronization, which leads to enhanced system\nperformance. The experimental results demonstrate that the proposed model\nachieves an overall accuracy of approximately 98.74%, highlighting its\neffectiveness in identifying and synchronizing frames in S2C VLC systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u8f7b\u91cf\u7ea7\u5e27\u8bc6\u522b\u4e0e\u540c\u6b65\u6280\u672f\uff0c\u7528\u4e8e\u63d0\u5347\u5c4f\u5e55\u5230\u76f8\u673a\uff08S2C\uff09\u53ef\u89c1\u5149\u901a\u4fe1\uff08VLC\uff09\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u51c6\u786e\u7387\u8fbe98.74%\u3002", "motivation": "\u89e3\u51b3S2C\u901a\u4fe1\u4e2d\u56e0\u6a21\u7cca\u3001\u88c1\u526a\u548c\u65cb\u8f6c\u56fe\u50cf\u7b49\u5b9e\u65f6\u6311\u6218\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u4f7f\u7528Python\u548cTensorFlow Keras\u6846\u67b6\u6784\u5efaCNN\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u4e2a\u5b9e\u65f6\u5b9e\u9a8c\u8bad\u7ec3\uff0c\u6570\u636e\u96c6\u9488\u5bf9S2C\u901a\u4fe1\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u8bbe\u8ba1\u3002", "result": "\u6a21\u578b\u5728\u5e27\u8bc6\u522b\u4e0e\u540c\u6b65\u4e2d\u8fbe\u523098.74%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684CNN\u6a21\u578b\u5728S2C VLC\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.23944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23944", "abs": "https://arxiv.org/abs/2506.23944", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "comment": null, "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u672c\u4f53\u611f\u89c9\u504f\u79fb\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57df\u9002\u5e94\u6846\u67b6\u548cWasserstein\u8ddd\u79bb\u5bf9\u9f50\u8bad\u7ec3\u4e0e\u90e8\u7f72\u5206\u5e03\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u76f4\u63a5\u4f7f\u7528\u6240\u6709\u672c\u4f53\u611f\u89c9\u72b6\u6001\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u4e0e\u90e8\u7f72\u65f6\u672c\u4f53\u611f\u89c9\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u57df\u9002\u5e94\u6846\u67b6\uff0c\u5229\u7528\u90e8\u7f72\u65f6\u7684\u6eda\u52a8\u6570\u636e\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u91cf\u5316\u5dee\u5f02\uff0c\u5e76\u6dfb\u52a0\u566a\u58f0\u5bf9\u9f50\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76f4\u63a5\u4e22\u5f03\u672c\u4f53\u611f\u89c9\u6216\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5229\u7528\u672c\u4f53\u611f\u89c9\u5e76\u51cf\u5c11\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u672c\u4f53\u611f\u89c9\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.23127", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23127", "abs": "https://arxiv.org/abs/2506.23127", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "AI": {"tldr": "Embodied Planner-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u63d0\u5347LLMs\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89c4\u5212\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u9700\u8981\u6301\u7eed\u73af\u5883\u7406\u89e3\u548c\u52a8\u4f5c\u751f\u6210\u7684\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u5b66\u4e60\u52a8\u4f5c\u4e0e\u73af\u5883\u53cd\u9988\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ec4\u6eda\u52a8\u3001\u5b8c\u6210\u9a71\u52a8\u7684\u7a00\u758f\u5956\u52b1\u548c\u4ea4\u4e92\u7b56\u7565\u4f18\u5316\uff08IPO\uff09\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u5728ALFWorld\u548cScienceWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523097.78%\u548c79.92%\u7684\u5b8c\u6210\u7387\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "Embodied Planner-R1\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u5177\u8eab\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u8868\u73b0\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.23784", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.23784", "abs": "https://arxiv.org/abs/2506.23784", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R\u00fcmmer"], "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "comment": null, "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "AI": {"tldr": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5bf9\u8bcd\u65b9\u7a0b\u8fdb\u884c\u6392\u5e8f\uff0c\u63d0\u5347\u6c42\u89e3\u6548\u7387\u3002", "motivation": "\u4f20\u7edfNielsen\u53d8\u6362\u6c42\u89e3\u8bcd\u65b9\u7a0b\u65f6\uff0c\u5904\u7406\u987a\u5e8f\u5bf9\u6027\u80fd\u5f71\u54cd\u5927\uff0c\u9700\u4f18\u5316\u6392\u5e8f\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u8bcd\u65b9\u7a0b\u8868\u793a\uff0c\u7ed3\u5408GNN\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u5904\u7406\u53d8\u91cf\u6570\u91cf\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6846\u67b6\u5728\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u5b57\u7b26\u4e32\u6c42\u89e3\u5668\u3002", "conclusion": "GNN\u6392\u5e8f\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u8bcd\u65b9\u7a0b\u6c42\u89e3\u6027\u80fd\u3002"}}
{"id": "2506.22783", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22783", "abs": "https://arxiv.org/abs/2506.22783", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPhonemeFake\uff08PF\uff09\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u63a8\u7406\u64cd\u7eb5\u5173\u952e\u8bed\u97f3\u7247\u6bb5\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u7c7b\u611f\u77e5\u548c\u57fa\u51c6\u51c6\u786e\u7387\uff0c\u5e76\u5f00\u6e90\u4e86\u68c0\u6d4b\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709Deepfake\uff08DF\uff09\u6570\u636e\u96c6\u65e0\u6cd5\u771f\u5b9e\u6a21\u62df\u653b\u51fb\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u73b0\u5b9e\u7684\u653b\u51fb\u5411\u91cf\u3002", "method": "\u5f15\u5165PF\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u63a8\u7406\u64cd\u7eb5\u8bed\u97f3\u7247\u6bb5\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u4f18\u5148\u8ba1\u7b97\u7684\u53cc\u5c42DF\u68c0\u6d4b\u6a21\u578b\u3002", "result": "PF\u653b\u51fb\u4f7f\u4eba\u7c7b\u611f\u77e5\u964d\u4f4e42%\uff0c\u57fa\u51c6\u51c6\u786e\u7387\u964d\u4f4e94%\uff1b\u68c0\u6d4b\u6a21\u578b\u5c06EER\u964d\u4f4e91%\uff0c\u901f\u5ea6\u63d0\u534790%\u3002", "conclusion": "PF\u653b\u51fb\u548c\u68c0\u6d4b\u6a21\u578b\u4e3aDF\u653b\u51fb\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u7684\u6a21\u62df\u548c\u9ad8\u6548\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2506.23254", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.23254", "abs": "https://arxiv.org/abs/2506.23254", "authors": ["Aradhana Mishra", "Bumshik Lee"], "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "comment": null, "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "AI": {"tldr": "PixelBoost\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u5e03\u6717\u8fd0\u52a8\u7684\u968f\u673a\u6027\u63d0\u5347\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u751f\u6210\u66f4\u771f\u5b9e\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5e38\u9762\u4e34\u751f\u6210\u56fe\u50cf\u771f\u5b9e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u65f6\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u5c06\u53d7\u63a7\u968f\u673a\u6027\u5f15\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u907f\u514d\u5c40\u90e8\u6700\u4f18\uff0c\u5e76\u91c7\u7528Sigmoid\u566a\u58f0\u5e8f\u5217\u65b9\u6cd5\u7b80\u5316\u8bad\u7ec3\u3002", "result": "\u5728LPIPS\u3001LOE\u3001PSNR\u3001SSIM\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fb9\u7f18\u91cd\u5efa\u80fd\u529b\u66f4\u5f3a\uff0c\u4e14\u5177\u6709\u81ea\u9002\u5e94\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "PixelBoost\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u771f\u5b9e\u6027\u548c\u9ad8\u6548\u6027\u7684\u5e73\u8861\uff0c\u5c24\u5176\u5728\u7eb9\u7406\u548c\u8fb9\u7f18\u5b9a\u4e49\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.23999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23999", "abs": "https://arxiv.org/abs/2506.23999", "authors": ["Zeyu Han", "Mengchi Cai", "Chaoyi Chen", "Qingwen Meng", "Guangwei Wang", "Ying Liu", "Qing Xu", "Jianqiang Wang", "Keqiang Li"], "title": "Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles", "comment": null, "summary": "The safe trajectory planning of intelligent and connected vehicles is a key\ncomponent in autonomous driving technology. Modeling the environment risk\ninformation by field is a promising and effective approach for safe trajectory\nplanning. However, existing risk assessment theories only analyze the risk by\ncurrent information, ignoring future prediction. This paper proposes a\npredictive risk analysis and safe trajectory planning framework for intelligent\nand connected vehicles. This framework first predicts future trajectories of\nobjects by a local risk-aware algorithm, following with a\nspatiotemporal-discretised predictive risk analysis using the prediction\nresults. Then the safe trajectory is generated based on the predictive risk\nanalysis. Finally, simulation and vehicle experiments confirm the efficacy and\nreal-time practicability of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u98ce\u9669\u5206\u6790\u7684\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u672a\u6765\u9884\u6d4b\u548c\u5b9e\u65f6\u98ce\u9669\u5206\u6790\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u98ce\u9669\u8bc4\u4f30\u7406\u8bba\u4ec5\u57fa\u4e8e\u5f53\u524d\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u672a\u6765\u9884\u6d4b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5148\u901a\u8fc7\u5c40\u90e8\u98ce\u9669\u611f\u77e5\u7b97\u6cd5\u9884\u6d4b\u7269\u4f53\u672a\u6765\u8f68\u8ff9\uff0c\u518d\u4f7f\u7528\u65f6\u7a7a\u95f4\u79bb\u6563\u5316\u7684\u9884\u6d4b\u98ce\u9669\u5206\u6790\uff0c\u6700\u540e\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u8f66\u8f86\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u65f6\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u672a\u6765\u9884\u6d4b\u548c\u98ce\u9669\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u7f51\u8054\u8f66\u8f86\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2506.23133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.23133", "abs": "https://arxiv.org/abs/2506.23133", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "comment": null, "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "AI": {"tldr": "\u901a\u8fc7\u751f\u6210\u548c\u9009\u62e9\u63a8\u7406\u683c\u5f0f\uff0cFormat-Adapter\u65b9\u6cd5\u51cf\u5c11\u4e86LLMs\u7684\u63a8\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u63d0\u53474.3%\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u683c\u5f0f\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u9ad8\u4efb\u52a1\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u6d4b\u91cf\u63a8\u7406\u8bef\u5dee\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1Format-Adapter\uff0c\u5229\u7528LLMs\u751f\u6210\u548c\u9009\u62e9\u6700\u4f18\u63a8\u7406\u683c\u5f0f\u3002", "result": "\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u53474.3%\u3002", "conclusion": "Format-Adapter\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u548c\u9009\u62e9\u63a8\u7406\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2506.23793", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.23793", "abs": "https://arxiv.org/abs/2506.23793", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "comment": null, "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "AI": {"tldr": "MAPF-GPT-DDG\u662f\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u65b0\u578b\u6570\u636e\u751f\u6210\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u7269\u6d41\u548c\u641c\u6551\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684MAPF\u6c42\u89e3\u5668\u3002", "method": "\u5229\u7528\u96c6\u4e2d\u5f0f\u4e13\u5bb6\u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684MAPF-GPT\u6a21\u578b\uff0c\u5e76\u5f15\u5165delta-data\u751f\u6210\u673a\u5236\u52a0\u901f\u8bad\u7ec3\u3002", "result": "MAPF-GPT-DDG\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u578b\u6c42\u89e3\u5668\uff0c\u652f\u6301\u5355\u73af\u5883\u4e2d\u591a\u8fbe100\u4e07\u4e2a\u667a\u80fd\u4f53\u7684\u89c4\u5212\u3002", "conclusion": "MAPF-GPT-DDG\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3aMAPF\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u7684\u91cc\u7a0b\u7891\u3002"}}
