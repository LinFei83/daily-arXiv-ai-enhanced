<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.CV](#cs.CV) [Total: 82]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 21]
- [eess.SY](#eess.SY) [Total: 16]
- [eess.IV](#eess.IV) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT\u662f\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u57faLLM\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u5e76\u884c\u6267\u884c\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\uff0c\u89e3\u51b3\u73b0\u6709\u94fe\u5f0f\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\u6548\u7387\u4f4e\u3001\u4ee3\u5e01\u6d88\u8017\u9ad8\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\uff08\u5982TrafficGPT\uff09\u5b58\u5728\u4efb\u52a1\u987a\u5e8f\u6267\u884c\u3001\u4ee3\u5e01\u4f7f\u7528\u91cf\u5927\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002

Method: \u63d0\u51faGraphTrafficGPT\uff0c\u4e00\u79cd\u56fe\u57fa\u67b6\u6784\uff0c\u5c06\u4efb\u52a1\u53ca\u5176\u4f9d\u8d56\u8868\u793a\u4e3a\u6709\u5411\u56fe\u4e2d\u7684\u8282\u70b9\u548c\u8fb9\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5e76\u884c\u6267\u884c\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u3002\u6838\u5fc3\u601d\u60f3\u662f\u4e00\u4e2a\u201c\u5927\u8111\u4ee3\u7406\u201d\uff0c\u8d1f\u8d23\u5206\u89e3\u7528\u6237\u67e5\u8be2\u3001\u6784\u5efa\u4f18\u5316\u7684\u4f9d\u8d56\u56fe\uff0c\u5e76\u534f\u8c03\u4e13\u95e8\u4ee3\u7406\uff08\u6570\u636e\u68c0\u7d22\u3001\u5206\u6790\u3001\u53ef\u89c6\u5316\u3001\u6a21\u62df\uff09\u7f51\u7edc\u3002\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4ee3\u5e01\u7ba1\u7406\uff0c\u5e76\u652f\u6301\u5e76\u53d1\u591a\u67e5\u8be2\u5904\u7406\u3002

Result: \u4e0eTrafficGPT\u76f8\u6bd4\uff0cGraphTrafficGPT\u5c06\u4ee3\u5e01\u6d88\u8017\u964d\u4f4e\u4e8650.2%\uff0c\u5e73\u5747\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e\u4e8619.0%\uff0c\u540c\u65f6\u5728\u652f\u6301\u5e76\u53d1\u591a\u67e5\u8be2\u6267\u884c\u65b9\u9762\u6548\u7387\u63d0\u5347\u9ad8\u8fbe23.0%\u3002

Conclusion: GraphTrafficGPT\u901a\u8fc7\u91c7\u7528\u56fe\u57fa\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u9a71\u52a8\u4ea4\u901a\u5e94\u7528\u4e2d\u987a\u5e8f\u6267\u884c\u3001\u9ad8\u4ee3\u5e01\u6d88\u8017\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u5e76\u53d1\u5904\u7406\u80fd\u529b\u3002

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [2] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u504f\u597d\u5206\u89e3\u4e3a\u5c5e\u6027\u7ef4\u5ea6\u5e76\u6839\u636e\u793e\u4ea4\u793e\u533a\u7684\u4ef7\u503c\u89c2\u8fdb\u884c\u5b9a\u5236\uff0c\u5b9e\u73b0\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u504f\u597d\u9884\u6d4b\uff0c\u5176\u9884\u6d4b\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8eGPT-4o\uff0c\u5e76\u63d0\u4f9b\u4e86\u793e\u533a\u7279\u6709\u7684\u6d1e\u5bdf\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u524d\u7684\u504f\u597d\u6a21\u578b\u5c06\u4eba\u7c7b\u5224\u65ad\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u65e0\u6cd5\u7406\u89e3\u504f\u597d\u80cc\u540e\u7684\u6df1\u5c42\u539f\u56e0\uff0c\u963b\u788d\u4e86AI\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u53d1\u5c55\u3002\u7814\u7a76\u65e8\u5728\u8d85\u8d8a\u7b80\u5355\u7684\u504f\u597d\u9884\u6d4b\uff0c\u63ed\u793a\u9a71\u52a8\u4eba\u7c7b\u5224\u65ad\u7684\u8bc4\u4f30\u6846\u67b6\u3002

Method: PrefPalette\u6846\u67b6\u57fa\u4e8e\u591a\u5c5e\u6027\u51b3\u7b56\u539f\u7406\uff0c\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff1a1) \u53ef\u6269\u5c55\u7684\u53cd\u4e8b\u5b9e\u5c5e\u6027\u5408\u6210\u6b65\u9aa4\uff0c\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4ee5\u9694\u79bb\u4e2a\u4f53\u5c5e\u6027\u6548\u5e94\uff08\u5982\u6b63\u5f0f\u6027\u3001\u5e7d\u9ed8\u3001\u6587\u5316\u4ef7\u503c\u89c2\uff09\uff1b2) \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u504f\u597d\u5efa\u6a21\uff0c\u5b66\u4e60\u4e0d\u540c\u793e\u4ea4\u793e\u533a\u5982\u4f55\u52a8\u6001\u5730\u6743\u8861\u8fd9\u4e9b\u5c5e\u6027\u3002

Result: \u5728Reddit\u768445\u4e2a\u793e\u4ea4\u793e\u533a\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cPrefPalette\u7684\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\u6bd4GPT-4o\u9ad8\u51fa46.6%\u3002\u6b64\u5916\uff0cPrefPalette\u63ed\u793a\u4e86\u76f4\u89c2\u7684\u3001\u793e\u533a\u7279\u5b9a\u7684\u504f\u597d\u7279\u5f81\uff1a\u5b66\u672f\u793e\u533a\u4f18\u5148\u8003\u8651\u5197\u957f\u548c\u523a\u6fc0\u6027\uff0c\u51b2\u7a81\u5bfc\u5411\u7684\u793e\u533a\u91cd\u89c6\u8bbd\u523a\u548c\u76f4\u63a5\u6027\uff0c\u800c\u652f\u6301\u6027\u793e\u533a\u5219\u5f3a\u8c03\u540c\u7406\u5fc3\u3002

Conclusion: PrefPalette\u901a\u8fc7\u5efa\u6a21\u4eba\u7c7b\u5224\u65ad\u7684\u5c5e\u6027\u4ecb\u5bfc\u7ed3\u6784\uff0c\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u504f\u597d\u5efa\u6a21\u80fd\u529b\uff0c\u8fd8\u5e26\u6765\u4e86\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u503c\u5f97\u4fe1\u8d56\u3001\u66f4\u6ce8\u91cd\u4ef7\u503c\u89c2\u7684\u4e2a\u6027\u5316\u5e94\u7528\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [3] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u53d7\u63a7\u548c\u900f\u660e\u65b9\u5f0f\u5f00\u53d1\u4e13\u5bb6\u7cfb\u7edf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u7531\u4eba\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u548c\u7ea0\u6b63\u7684Prolog\u7b26\u53f7\u77e5\u8bc6\u8868\u793a\uff0c\u89e3\u51b3\u4e86LLMs\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u578b\u7cfb\u7edf\uff08\u5982\u5f00\u653e\u57df\u95ee\u7b54\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u5e76\u80fd\u751f\u6210\u5927\u91cf\u4fe1\u606f\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u5e7b\u89c9\uff08\u5373\u81ea\u4fe1\u5730\u751f\u6210\u4e0d\u6b63\u786e\u6216\u65e0\u6cd5\u9a8c\u8bc1\u7684\u4e8b\u5b9e\uff09\u7b49\u7f3a\u70b9\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u9ad8\u51c6\u786e\u6027\u7684\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002

Method: \u8be5\u7814\u7a76\u901a\u8fc7\u9650\u5236\u9886\u57df\u5e76\u91c7\u7528\u7ed3\u6784\u5316\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f7f\u7528LLMs\uff08\u5982Claude Sonnet 3.7\u548cGPT-4.1\uff09\u751f\u6210Prolog\u683c\u5f0f\u7684\u7b26\u53f7\u77e5\u8bc6\u8868\u793a\u3002\u8fd9\u79cd\u8868\u793a\u53ef\u4ee5\u7531\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u9a8c\u8bc1\u548c\u7ea0\u6b63\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002

Result: \u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\uff0c\u7814\u7a76\u8868\u660e\u6240\u751f\u6210\u7684\u77e5\u8bc6\u5e93\u5728\u4e8b\u5b9e\u9075\u5faa\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u7684\u6df7\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86LLMs\u7684\u53ec\u56de\u80fd\u529b\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\u3002

Conclusion: \u8be5\u7814\u7a76\u4e3a\u5728\u654f\u611f\u9886\u57df\u5f00\u53d1\u53ef\u9760\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u9614\u77e5\u8bc6\u4e0e\u7b26\u53f7\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u4e13\u5bb6\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u548c\u53ef\u4fe1\u5ea6\u3002

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [4] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: \u672c\u6587\u63a2\u8ba8\u4e86AI\u5f53\u524d\u8fc7\u5ea6\u5173\u6ce8\u50cf\u7d20\u3001\u6587\u5b57\u7b49\u611f\u77e5\u6570\u636e\uff0c\u800c\u5ffd\u89c6\u4e86\u66f4\u672c\u8d28\u7684\u5b9e\u4f53\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u5173\u7cfb\u5b66\u4e60\u672a\u80fd\u666e\u53ca\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4f7f\u5176\u53d1\u6325\u5e94\u6709\u4f5c\u7528\u7684\u5efa\u8bae\u3002


<details>
  <summary>Details</summary>
Motivation: AI\u7cfb\u7edf\u76ee\u524d\u4e3b\u8981\u5efa\u6a21\u50cf\u7d20\u3001\u6587\u5b57\u548c\u97f3\u7d20\uff0c\u4f46\u8fd9\u4e0e\u4e16\u754c\u7531\u5b9e\u4f53\u3001\u5c5e\u6027\u53ca\u5173\u7cfb\u6784\u6210\u7684\u672c\u8d28\u4e0d\u7b26\u3002\u4f01\u4e1a\u6700\u6709\u4ef7\u503c\u7684\u6570\u636e\u591a\u4ee5\u7535\u5b50\u8868\u683c\u3001\u6570\u636e\u5e93\u7b49\u5173\u7cfb\u578b\u683c\u5f0f\u5b58\u5728\uff0c\u800c\u975e\u7b80\u5355\u7684\u6587\u672c\u548c\u56fe\u50cf\u3002\u5c3d\u7ba1\u5b58\u5728\u5173\u7cfb\u5b66\u4e60\u9886\u57df\uff0c\u4f46\u5176\u672a\u80fd\u50cf\u5176\u4ed6AI\u9886\u57df\u4e00\u6837\u666e\u53ca\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u63a2\u8ba8\u5176\u539f\u56e0\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002

Method: \u672c\u6587\u901a\u8fc7\u5206\u6790\u548c\u89e3\u91ca\u7684\u65b9\u5f0f\uff0c\u63a2\u8ba8\u4e86\u5173\u7cfb\u5b66\u4e60\uff08\u5305\u62ec\u7edf\u8ba1\u5173\u7cfbAI\u7b49\uff09\u672a\u80fd\u5e7f\u6cdb\u5e94\u7528\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u9700\u8981\u91c7\u53d6\u7684\u63aa\u65bd\uff0c\u4ee5\u63d0\u5347\u5176\u91cd\u8981\u6027\u3002

Result: \u5173\u7cfb\u5b66\u4e60\u76ee\u524d\u9664\u4e86\u5728\u5c11\u6570\u53d7\u9650\u5173\u7cfb\u573a\u666f\u5916\uff0c\u5e76\u672a\u50cf\u5176\u4ed6AI\u6280\u672f\u4e00\u6837\u201c\u63a5\u7ba1\u4e16\u754c\u201d\u3002

Conclusion: \u4e3a\u4e86\u4f7f\u5173\u7cfb\u5b66\u4e60\u8fbe\u5230\u5176\u5e94\u6709\u7684\u91cd\u8981\u5730\u4f4d\uff0c\u9700\u8981\u91c7\u53d6\u7279\u5b9a\u7684\u63aa\u65bd\u6765\u514b\u670d\u5176\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u548c\u9650\u5236\u3002

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [5] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG\u662f\u4e00\u79cd\u53cc\u56feRAG\u96c6\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u904d\u5386\u548c\u5411\u91cf\u8bed\u4e49\u641c\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u590d\u6742\u6cd5\u89c4\u6587\u672c\u4e2d\u8fdb\u884c\u591a\u8df3\u4fe1\u606f\u68c0\u7d22\u548c\u95ee\u7b54\u7684\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u81ea\u52a8\u65bd\u5de5\u5408\u89c4\u6027\u68c0\u67e5\u9700\u8981\u4ece\u5b89\u5168\u6cd5\u89c4\u4e2d\u68c0\u7d22\u4fe1\u606f\u548c\u8fdb\u884c\u95ee\u7b54\uff0c\u4f46\u6cd5\u89c4\u6587\u672c\u7684\u8bed\u8a00\u548c\u7ed3\u6784\u590d\u6742\u6027\u4ee5\u53ca\u591a\u8df3\u67e5\u8be2\u9700\u6c42\u5bf9\u4f20\u7edfRAG\u7cfb\u7edf\u6784\u6210\u4e86\u6311\u6218\u3002

Method: \u5f15\u5165\u4e86BifrostRAG\u7cfb\u7edf\uff0c\u5b83\u662f\u4e00\u4e2a\u53cc\u56feRAG\u96c6\u6210\u7cfb\u7edf\uff0c\u660e\u786e\u5efa\u6a21\u4e86\u8bed\u8a00\u5173\u7cfb\uff08\u901a\u8fc7\u5b9e\u4f53\u7f51\u7edc\u56fe\uff09\u548c\u6587\u6863\u7ed3\u6784\uff08\u901a\u8fc7\u6587\u6863\u5bfc\u822a\u56fe\uff09\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u7ed3\u5408\u56fe\u904d\u5386\u548c\u57fa\u4e8e\u5411\u91cf\u7684\u8bed\u4e49\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u540c\u65f6\u7406\u89e3\u6587\u672c\u7684\u610f\u4e49\u548c\u7ed3\u6784\u3002

Result: \u5728\u591a\u8df3\u95ee\u9898\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cBifrostRAG\u7684\u51c6\u786e\u7387\u4e3a92.8%\uff0c\u53ec\u56de\u7387\u4e3a85.5%\uff0cF1\u5206\u6570\u4e3a87.3%\u3002\u8fd9\u4e9b\u7ed3\u679c\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u5411\u91cf\u548c\u4ec5\u57fa\u4e8e\u56fe\u7684RAG\u57fa\u7ebf\u65b9\u6cd5\u3002

Conclusion: BifrostRAG\u88ab\u786e\u7acb\u4e3aLLM\u9a71\u52a8\u5408\u89c4\u6027\u68c0\u67e5\u7684\u5f3a\u5927\u77e5\u8bc6\u5f15\u64ce\u3002\u5176\u53cc\u56fe\u3001\u6df7\u5408\u68c0\u7d22\u673a\u5236\u4e3a\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5de5\u7a0b\u9886\u57df\u4e2d\u5904\u7406\u590d\u6742\u6280\u672f\u6587\u6863\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [6] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: \u672c\u7814\u7a76\u63a2\u7d22\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u81ea\u52a8\u9519\u8bef\u8bca\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u5b66\u751f\u5408\u5e76\u591a\u4e2a\u6b65\u9aa4\u65f6\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u9762\u4e34\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u5b66\u751f\u5728\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u5c06\u591a\u4e2a\u6b65\u9aa4\u5408\u5e76\u4e3a\u4e00\u4e2a\u6b65\u9aa4\u65f6\uff0c\u4f20\u7edf\u8fde\u63a5\u8fde\u7eed\u8f93\u5165\u7684\u8bca\u65ad\u65b9\u6cd5\u4f1a\u9762\u4e34\u5de8\u5927\u7684\u8def\u5f84\u7ec4\u5408\u7206\u70b8\uff0c\u5bfc\u81f4\u9519\u8bef\u8bca\u65ad\u56f0\u96be\u3002\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u8bca\u65ad\u53ef\u4ee5\u51cf\u5c11\u53ef\u80fd\u7684\u9519\u8bef\u7b54\u6848\u6570\u91cf\uff0c\u4ece\u800c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002

Method: \u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u670d\u52a1\uff0c\u5f53\u5b66\u751f\u5408\u5e76\u591a\u4e2a\u6b65\u9aa4\u65f6\uff0c\u80fd\u63d0\u4f9b\u9519\u8bef\u89c4\u5219\u8bca\u65ad\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6839\u636e\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u7b56\u7565\u81ea\u52a8\u5b8c\u6210\u4e2d\u95f4\u8f93\u5165\u5e76\u8bca\u65ad\u5176\u89e3\u51b3\u65b9\u6848\u6765\u5b9e\u73b0\u3002\u7814\u7a76\u5c06\u6b64\u670d\u52a1\u5e94\u7528\u4e8e\u4e00\u4e2a\u5305\u542b1939\u4e2a\u552f\u4e00\u5b66\u751f\u6b65\u9aa4\u7684\u73b0\u6709\u6570\u636e\u96c6\uff08\u5b66\u751f\u6c42\u89e3\u4e8c\u6b21\u65b9\u7a0b\uff09\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u65e0\u6cd5\u901a\u8fc7\u5c1d\u8bd5\u7528\u5355\u4e00\u89c4\u5219\u8fde\u63a5\u8fde\u7eed\u8f93\u5165\u7684\u73b0\u6709\u670d\u52a1\u8fdb\u884c\u8bca\u65ad\u3002

Result: \u8be5\u65b9\u6cd5\u80fd\u591f\u8bca\u65ad\u51fa29.4%\u7684\u8fd9\u4e9b\u6b65\u9aa4\u3002\u6b64\u5916\uff0c\u5728115\u4e2a\u6b65\u9aa4\u7684\u5b50\u96c6\u4e0a\uff0c\u751f\u6210\u7684\u8bca\u65ad\u7ed3\u679c\u4e0e\u6559\u5e08\u8bca\u65ad\u7ed3\u679c\u7684\u543b\u5408\u5ea6\u9ad8\u8fbe97%\u3002

Conclusion: \u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u81ea\u52a8\u9519\u8bef\u8bca\u65ad\u65b9\u6cd5\u5728\u5904\u7406\u5b66\u751f\u5408\u5e76\u6b65\u9aa4\u65f6\u7684\u8bca\u65ad\u95ee\u9898\u4e0a\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u8bca\u65ad\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u63a2\u7d22\u7684\u57fa\u7840\u3002

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [7] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u8ffd\u8e2a\u548c\u57fa\u4e8e\u7ea6\u675f\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bca\u65ad\u5b66\u751f\u5728\u5206\u6b65\u4efb\u52a1\u4e2d\u7684\u8f93\u5165\uff0c\u5373\u4f7f\u5b66\u751f\u5408\u5e76\u4e86\u591a\u4e2a\u6b65\u9aa4\u4e5f\u80fd\u63d0\u4f9b\u51c6\u786e\u8bca\u65ad\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u6a21\u578b\u8ffd\u8e2a\u65b9\u6cd5\u64c5\u957f\u8bc6\u522b\u8fde\u7eed\u7684\u95ee\u9898\u89e3\u51b3\u6b65\u9aa4\uff0c\u800c\u57fa\u4e8e\u7ea6\u675f\u7684\u5efa\u6a21\u5219\u652f\u6301\u8bca\u65ad\u5b66\u751f\u5408\u5e76\u591a\u4e2a\u6b65\u9aa4\u7684\u60c5\u51b5\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u8fd9\u4e24\u79cd\u8303\u5f0f\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u5404\u81ea\u7684\u5c40\u9650\u6027\uff0c\u5373\u4f7f\u5b66\u751f\u504f\u79bb\u7b56\u7565\u6216\u5408\u5e76\u6b65\u9aa4\u4e5f\u80fd\u63d0\u4f9b\u8bca\u65ad\u3002

Method: \u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7ea6\u675f\u5b9a\u4e49\u4e3a\u5b66\u751f\u8f93\u5165\u4e0e\u7b56\u7565\u6b65\u9aa4\u5171\u6709\u5c5e\u6027\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u878d\u5408\u4e86\u6a21\u578b\u8ffd\u8e2a\u548c\u57fa\u4e8e\u7ea6\u675f\u5efa\u6a21\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6b65\u7b56\u7565\u8bca\u65ad\u7cfb\u7edf\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u5305\u542b\u5b66\u751f\u89e3\u51b3\u4e8c\u6b21\u65b9\u7a0b\u6b65\u9aa4\u7684\u73b0\u6709\u6570\u636e\u96c6\uff08n=2136\uff09\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\u3002\u4e3a\u4e86\u4e0e\u4eba\u5de5\u8bca\u65ad\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e24\u4f4d\u6559\u5e08\u5bf9\u968f\u673a\u62bd\u53d6\u7684\u504f\u5dee\uff08n=70\uff09\u548c\u7b56\u7565\u5e94\u7528\uff08n=70\uff09\u6837\u672c\u8fdb\u884c\u4e86\u7f16\u7801\u3002

Result: \u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u8bca\u65ad\u4e0e\u6559\u5e08\u5bf9\u6240\u6709140\u4e2a\u5b66\u751f\u6b65\u9aa4\u7684\u7f16\u7801\u5b8c\u5168\u4e00\u81f4\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u8bca\u65ad\u5b66\u751f\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u5e94\u7528\u548c\u504f\u5dee\uff0c\u5373\u4f7f\u5b66\u751f\u5408\u5e76\u4e86\u591a\u4e2a\u6b65\u9aa4\uff0c\u5176\u8bca\u65ad\u7ed3\u679c\u4e5f\u4e0e\u4eba\u7c7b\u6559\u5e08\u7684\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [8] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u7ea7LLM\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a/\u624b\u8868\u4f20\u611f\u5668\uff0c\u6574\u5408\u591a\u7ef4\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\u751f\u6210\u548c\u603b\u7ed3\u6d3b\u52a8\u65e5\u5fd7\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u4e30\u5bcc\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6d3b\u52a8\u65e5\u5fd7\u5bf9\u4e8e\u7528\u6237\u884c\u4e3a\u5206\u6790\u548c\u5065\u5eb7\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u4e3a\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u5e26\u6765\u4e86\u65b0\u673a\u9047\u3002

Method: \u63d0\u51faDailyLLM\u7cfb\u7edf\uff0c\u9996\u6b21\u5168\u9762\u6574\u5408\u6765\u81ea\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u624b\u8868\u4f20\u611f\u5668\u7684\u4f4d\u7f6e\u3001\u8fd0\u52a8\u3001\u73af\u5883\u548c\u751f\u7406\u56db\u7ef4\u4e0a\u4e0b\u6587\u6d3b\u52a8\u4fe1\u606f\u3002\u5b83\u91c7\u7528\u8f7b\u91cf\u7ea7LLM\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u9ad8\u7ea7\u6d3b\u52a8\u7406\u89e3\u3002

Result: DailyLLM\u5728\u65e5\u5fd7\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002\u4f7f\u75281.5B\u53c2\u6570\u7684LLM\u6a21\u578b\uff0c\u5176BERTScore\u7cbe\u5ea6\u6bd470B\u53c2\u6570\u7684SOTA\u57fa\u7ebf\u63d0\u9ad8\u4e8617%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\u8fd110\u500d\u3002\u8be5\u7cfb\u7edf\u53ef\u9ad8\u6548\u90e8\u7f72\u5728\u4e2a\u4eba\u7535\u8111\u548c\u6811\u8393\u6d3e\u4e0a\u3002

Conclusion: DailyLLM\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7LLM\u5728\u666e\u9002\u8ba1\u7b97\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u6d3b\u52a8\u65e5\u5fd7\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [9] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: \u672c\u6587\u4ecb\u7ecd\u4e86OntView\uff0c\u4e00\u4e2a\u672c\u4f53\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u65e8\u5728\u901a\u8fc7\u76f4\u89c2\u7684\u7528\u6237\u754c\u9762\u548c\u63a8\u7406\u5668\u652f\u6301\uff0c\u6709\u6548\u5c55\u793a\u672c\u4f53\u6982\u5ff5\u3001\u5173\u7cfb\u53ca\u63a8\u65ad\u77e5\u8bc6\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u7b80\u5316\u89c6\u56fe\u4ee5\u907f\u514d\u4fe1\u606f\u8fc7\u8f7d\u3002


<details>
  <summary>Details</summary>
Motivation: \u672c\u4f53\u5728\u77e5\u8bc6\u7ba1\u7406\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7528\u4e8e\u5efa\u6a21\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u6709\u6548\u7684\u53ef\u89c6\u5316\u529f\u80fd\uff0c\u96be\u4ee5\u6709\u610f\u4e49\u5730\u56fe\u5f62\u5316\u8868\u793a\u672c\u4f53\u7ed3\u6784\uff0c\u5bfc\u81f4\u7528\u6237\u96be\u4ee5\u7406\u89e3\u5927\u578b\u672c\u4f53\u6846\u67b6\u4e2d\u7684\u4f9d\u8d56\u548c\u5c5e\u6027\u3002

Method: OntView\u901a\u8fc7\u7528\u6237\u53cb\u597d\u7684\u754c\u9762\u63d0\u4f9b\u672c\u4f53\u6982\u5ff5\u53ca\u5176\u5f62\u5f0f\u5b9a\u4e49\u7684\u76f4\u89c2\u89c6\u89c9\u8868\u793a\u3002\u5b83\u57fa\u4e8e\u63cf\u8ff0\u903b\u8f91\uff08DL\uff09\u63a8\u7406\u5668\uff0c\u9075\u5faa\u201c\u6240\u89c1\u5373\u6240\u6307\u201d\u8303\u5f0f\uff0c\u663e\u793a\u5b9e\u9645\u63a8\u65ad\u51fa\u7684\u77e5\u8bc6\u3002\u5173\u952e\u65b9\u6cd5\u5305\u62ec\uff1a\u53ef\u89c6\u5316\u901a\u7528\u6982\u5ff5\u5305\u542b\uff08GCI\uff09\uff0c\u901a\u8fc7\u8bc4\u4f30\u6982\u5ff5\u91cd\u8981\u6027\u521b\u5efa\u672c\u4f53\u6458\u8981\uff0c\u805a\u7126\u4e8e\u7ed9\u5b9a\u7c7b\u4e4b\u95f4\u7684TBox\u5143\u7d20\uff0c\u4ee5\u53ca\u52a8\u6001\u9690\u85cf/\u663e\u793a\u5206\u652f\u800c\u4e0d\u5931\u8bed\u4e49\u3002

Result: OntView\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u76f4\u89c2\u7684\u672c\u4f53\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u5c55\u793a\u672c\u4f53\u6982\u5ff5\u548c\u63a8\u65ad\u77e5\u8bc6\u3002\u5b83\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u53ef\u89c6\u5316GCI\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\uff08\u5982\u672c\u4f53\u6458\u8981\u3001\u805a\u7126\u89c6\u56fe\u3001\u52a8\u6001\u5206\u652f\u63a7\u5236\uff09\u6709\u6548\u907f\u514d\u4e86\u4fe1\u606f\u8fc7\u8f7d\u3002OntView\u5df2\u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u53d1\u5e03\u3002

Conclusion: OntView\u4e3a\u672c\u4f53\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6574\u5408\u63a8\u7406\u80fd\u529b\u548c\u591a\u79cd\u89c6\u56fe\u7b80\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7406\u89e3\u548c\u7ba1\u7406\u590d\u6742\u672c\u4f53\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728GCI\u53ef\u89c6\u5316\u548c\u4fe1\u606f\u8fc7\u8f7d\u5904\u7406\u65b9\u9762\u7684\u7a7a\u767d\u3002

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [10] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u4ee3\u7406\u589e\u5f3a\u7684\u6218\u7565\u63a8\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u6fc0\u6d3b\u548c\u7ec4\u5408\u5408\u6210\u6765\u878d\u5408\u51b2\u7a81\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684\u51b3\u7b56\u5f15\u64ce\u503e\u5411\u4e8e\u9009\u62e9\u201c\u6700\u4f73\u201d\u89c4\u5219\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5c06\u51b2\u7a81\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u878d\u5408\u6210\u8fde\u8d2f\u4e14\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u53d9\u8ff0\u3002

Method: \u91c7\u7528\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u63d0\u53d6\u3001\u8bed\u4e49\u6fc0\u6d3b\u548c\u7ec4\u5408\u5408\u6210\u3002\u6a21\u578b\u901a\u8fc7\u53d7\u91cf\u5b50\u8ba4\u77e5\u542f\u53d1\u7684\u8bed\u4e49\u76f8\u4e92\u4f9d\u8d56\u8fc7\u7a0b\u6fc0\u6d3b\u548c\u7ec4\u5408\u591a\u91cd\u542f\u53d1\u5f0f\u89c4\u5219\u3002\u5b83\u901a\u8fc7\u8bed\u4e49\u4ea4\u4e92\u5efa\u6a21\u548c\u4fee\u8f9e\u6846\u67b6\uff0c\u5c06\u51b2\u7a81\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u878d\u5408\u4e3a\u8fde\u8d2f\u7684\u53d9\u8ff0\u3002

Result: \u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2aMeta vs. FTC\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6307\u6807\u8fdb\u884c\u4e86\u521d\u6b65\u9a8c\u8bc1\u3002

Conclusion: \u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u878d\u5408\u51b2\u7a81\u542f\u53d1\u5f0f\u89c4\u5219\u7684\u4ee3\u7406\u589e\u5f3a\u6218\u7565\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u521d\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u7684\u6269\u5c55\u65b9\u5411\u3002

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [11] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u56fe\u4e2d\u7684\u65f6\u95f4\u94fe\u8def\u9884\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u77ed\u671f\u65f6\u95f4\u8fd1\u90bb\u548c\u957f\u671f\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u65f6\u95f4\u56fe\u795e\u7ecf\u7f51\u7edc\uff08T-GNNs\uff09\u5728\u5efa\u6a21\u65f6\u95f4\u53ca\u7ed3\u6784\u4f9d\u8d56\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5e38\u5e38\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6311\u6218\u3002

Method: EAGLE\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u65f6\u95f4\u611f\u77e5\u6a21\u5757\uff08\u805a\u5408\u6700\u65b0\u90bb\u5c45\u4fe1\u606f\u53cd\u6620\u5373\u65f6\u504f\u597d\uff09\u548c\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u6a21\u5757\uff08\u5229\u7528\u65f6\u95f4\u4e2a\u6027\u5316PageRank\u6355\u83b7\u5168\u5c40\u91cd\u8981\u8282\u70b9\u5f71\u54cd\uff09\u3002\u5b83\u91c7\u7528\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\u52a8\u6001\u8c03\u6574\u4e24\u8005\u7684\u8d21\u732e\uff0c\u5e76\u907f\u514d\u4e86\u590d\u6742\u7684\u591a\u8df3\u6d88\u606f\u4f20\u9012\u6216\u5185\u5b58\u5bc6\u96c6\u578b\u673a\u5236\u3002

Result: \u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u56fe\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEAGLE\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684T-GNNs\uff0c\u76f8\u6bd4\u6709\u6548\u7684\u57fa\u4e8eTransformer\u7684T-GNNs\uff0c\u901f\u5ea6\u63d0\u5347\u8d85\u8fc750\u500d\u3002

Conclusion: EAGLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u52a8\u6001\u56fe\u4e2d\u7684\u65f6\u95f4\u94fe\u8def\u9884\u6d4b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u8bbe\u8ba1\u514b\u670d\u4e86\u73b0\u6709T-GNNs\u7684\u6548\u7387\u74f6\u9888\u3002

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [12] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u4f7f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u80fd\u96f6\u6837\u672c\u9002\u5e94\u65b0\u969c\u788d\uff0c\u901a\u8fc7\u5171\u4eab\u56e0\u679c\u6062\u590d\u52a8\u4f5c\u5b8f\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\uff0c\u4e14\u667a\u80fd\u4f53\u9002\u5e94\u65b0\u73af\u5883\u5e38\u9700\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002

Method: \u5f15\u5165\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u5c06\u78b0\u649e\u5efa\u6a21\u4e3a\u56e0\u679c\u5e72\u9884\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3a\u6062\u590d\u52a8\u4f5c\u5e8f\u5217\uff08\u5b8f\uff09\u3002\u8fd9\u4e9b\u5b8f\u4ee3\u8868\u89c4\u907f\u969c\u788d\u7684\u56e0\u679c\u77e5\u8bc6\uff0c\u53ef\u5728\u7ebf\u4ece\u5176\u4ed6\u667a\u80fd\u4f53\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u901a\u8fc7\u67e5\u8be2\u67e5\u627e\u6a21\u578b\u5e76\u7ed3\u5408\u5c40\u90e8\u4e0a\u4e0b\u6587\uff08\u78b0\u649e\uff09\u5e94\u7528\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002

Result: \u7814\u7a76\u53d1\u73b0\uff1a1) \u76ee\u6807\u5f02\u6784\u7684\u667a\u80fd\u4f53\u5728\u9002\u5e94\u65b0\u73af\u5883\u65f6\uff0c\u5176\u8868\u73b0\u80fd\u5f25\u8865\u968f\u673a\u63a2\u7d22\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u4e4b\u95f4\u7ea6\u4e00\u534a\u7684\u5dee\u8ddd\uff1b2) \u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u7684\u6548\u679c\u53d7\u73af\u5883\u590d\u6742\u5ea6\u548c\u667a\u80fd\u4f53\u76ee\u6807\u5f02\u6784\u6027\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u56e0\u679c\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\u80fd\u6709\u6548\u5e2e\u52a9MARL\u667a\u80fd\u4f53\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u8fdb\u884c\u96f6\u6837\u672c\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u548c\u76ee\u6807\u5f02\u6784\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u6f5c\u529b\u3002

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [13] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u521b\u610f\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5bfc\u822a\u601d\u60f3\u7684\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\uff0c\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u65b0\u9896\u4e14\u76f8\u5173\u521b\u610f\u65b9\u9762\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u53ef\u63a7\u548c\u53ef\u6269\u5c55\u7684\u521b\u9020\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u65e2\u65b0\u9896\u53c8\u76f8\u5173\u7684\u521b\u610f\u65b9\u9762\u5b58\u5728\u6838\u5fc3\u6311\u6218\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u590d\u5236\u8bad\u7ec3\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u521b\u9020\u6027\u53d1\u6563\u80fd\u529b\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u9886\u57df\u7279\u5b9a\u542f\u53d1\u5f0f\u548c\u7ed3\u6784\u5316\u63d0\u793a\u7ba1\u9053\uff09\u8106\u5f31\u4e14\u96be\u4ee5\u6cdb\u5316\u3002

Method: \u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u521b\u610f\u751f\u6210\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bfc\u822a\u601d\u60f3\u7684\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u6765\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u6269\u5c55\u7684\u521b\u9020\u529b\u3002\u5b83\u4e0d\u9700\u8981\u624b\u5de5\u5236\u4f5c\u7684\u89c4\u5219\uff0c\u5e76\u4e14\u6613\u4e8e\u9002\u5e94\u4e0d\u540c\u7684\u9886\u57df\u3001\u8f93\u5165\u683c\u5f0f\u548c\u521b\u610f\u4efb\u52a1\u3002

Result: \u672c\u6587\u4ecb\u7ecd\u4e86\u8be5\u65b9\u6cd5\u7684\u65e9\u671f\u539f\u578b\uff0c\u6982\u8ff0\u4e86\u5176\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u521d\u6b65\u7ed3\u679c\uff0c\u7a81\u51fa\u5176\u4f5c\u4e3a\u4eba\u673a\u534f\u4f5c\u901a\u7528\u5171\u540c\u521b\u610f\u8005\u7684\u6f5c\u529b\u3002

Conclusion: \u8be5\u6f5c\u5728\u7a7a\u95f4\u521b\u610f\u751f\u6210\u6846\u67b6\u5177\u6709\u4f5c\u4e3a\u4eba\u673a\u534f\u4f5c\u901a\u7528\u5171\u540c\u521b\u610f\u8005\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u63a7\u548c\u53ef\u6269\u5c55\u7684\u521b\u9020\u529b\uff0c\u4e14\u65e0\u9700\u624b\u5de5\u89c4\u5219\uff0c\u6613\u4e8e\u6cdb\u5316\u3002

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [14] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADPC\u7684\u89c6\u89c9-\u8bed\u8a00\u56e0\u679c\u5e72\u9884\u6846\u67b6\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bca\u65ad\u8f85\u52a9\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5904\u7406\u4e34\u5e8a\u6587\u672c\u6570\u636e\u3001\u591a\u6a21\u6001\u5f71\u50cf\uff08MRI/fMRI\uff09\u4ee5\u53ca\u56e0\u679c\u5e72\u9884\u6765\u6d88\u9664\u6df7\u6742\u56e0\u7d20\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bf9\u6b63\u5e38\u8ba4\u77e5\uff08CN\uff09\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u548cAD\u60a3\u8005\u7684\u51c6\u786e\u5206\u7c7b\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u662f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u5148\u5146\u9636\u6bb5\uff0c\u65e9\u671f\u8bc6\u522b\u548c\u5e72\u9884\u5bf9\u5ef6\u7f13\u75be\u75c5\u8fdb\u5c55\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u591a\u6a21\u6001\u6570\u636e\u9009\u62e9\u504f\u5dee\u5bfc\u81f4\u7684\u6df7\u6742\u56e0\u7d20\u4ee5\u53ca\u53d8\u91cf\u95f4\u590d\u6742\u5173\u7cfb\uff0cAD\u7684\u8bca\u65ad\u5728\u795e\u7ecf\u75c5\u5b66\u4e2d\u4ecd\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u975e\u56e0\u679c\u6a21\u578b\u6613\u6355\u83b7\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9760\u3002

Method: \u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8de8\u6a21\u6001\u56e0\u679c\u5e72\u9884\u9884\u6d4b\u201d\uff08ADPC\uff09\u7684\u89c6\u89c9-\u8bed\u8a00\u56e0\u679c\u5e72\u9884\u6846\u67b6\u3002ADPC\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u4e34\u5e8a\u6570\u636e\u603b\u7ed3\u4e3a\u7ed3\u6784\u5316\u6587\u672c\uff0c\u5373\u4f7f\u9762\u5bf9\u4e0d\u5b8c\u6574\u6216\u5206\u5e03\u4e0d\u5747\u7684\u6570\u636e\u96c6\u4e5f\u80fd\u4fdd\u6301\u8f93\u51fa\u4e00\u81f4\u6027\u3002\u8be5\u6a21\u578b\u6574\u5408\u4e86\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u3001\u529f\u80fd\u6027\u78c1\u5171\u632f\u6210\u50cf\uff08fMRI\uff09\u56fe\u50cf\u4ee5\u53caLLM\u751f\u6210\u7684\u6587\u672c\u6570\u636e\uff0c\u7528\u4e8e\u5c06\u53c2\u4e0e\u8005\u5206\u7c7b\u4e3a\u6b63\u5e38\u8ba4\u77e5\uff08CN\uff09\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u3002\u901a\u8fc7\u56e0\u679c\u5e72\u9884\uff0c\u8be5\u6846\u67b6\u9690\u5f0f\u5730\u6d88\u9664\u4e86\u795e\u7ecf\u5f71\u50cf\u4f2a\u5f71\u548c\u5e74\u9f84\u76f8\u5173\u751f\u7269\u6807\u5fd7\u7269\u7b49\u6df7\u6742\u56e0\u7d20\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u533a\u5206CN/MCI/AD\u75c5\u4f8b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u5927\u591a\u6570\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002

Conclusion: \u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u56e0\u679c\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u76f8\u7ed3\u5408\u5728\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [15] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u65f6\u95f4\u548c\u7ea6\u675f\u7684Here-and-There\u903b\u8f91\u6269\u5c55\uff0c\u7528\u4e8e\u89e3\u51b3ASP\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\u7cfb\u7edf\u65f6\u7684\u6311\u6218\u3002


<details>
  <summary>Details</summary>
Motivation: \u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\uff08\u5982ASP\uff09\u5728\u63a8\u7406\u5177\u6709\u7cbe\u7ec6\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u6570\u503c\u5206\u8fa8\u7387\u7684\u52a8\u6001\u7cfb\u7edf\u65f6\u9762\u4e34\u663e\u8457\u6311\u6218\u3002

Method: \u901a\u8fc7\u534f\u540c\u7ed3\u5408\u7ebf\u6027\u65f6\u95f4Here-and-There\u903b\u8f91\uff08\u63d0\u4f9b\u975e\u5355\u8c03\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff09\u548c\u5e26\u7ea6\u675f\u7684Here-and-There\u903b\u8f91\uff08\u96c6\u6210\u548c\u64cd\u4f5c\u6570\u503c\u7ea6\u675f\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u57fa\u4e8e\u65f6\u95f4\u548c\u7ea6\u675f\u7684Here-and-There\u903b\u8f91\u53ca\u5176\u975e\u5355\u8c03\u5e73\u8861\u6269\u5c55\u3002

Result: \u5efa\u7acb\u4e86\u9996\u4e2a\u4e13\u4e3aASP\u8bbe\u8ba1\u7684\u975e\u5355\u8c03\u65f6\u95f4\u63a8\u7406\u4e0e\u7ea6\u675f\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4e3a\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u5904\u7406\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u903b\u8f91\u57fa\u7840\u3002

Conclusion: \u8be5\u5de5\u4f5c\u4e3a\u5728ASP\u8303\u5f0f\u5185\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u7684\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u903b\u8f91\u6846\u67b6\u3002

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [16] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u672c\u4f53\u5339\u914d\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\uff0c\u5e76\u8f85\u4ee5\u53cc\u76f8\u4f3c\u6027\u5339\u914d\u548c\u672c\u4f53\u7ec6\u5316\u6b65\u9aa4\uff0c\u4ee5\u52a8\u6001\u589e\u5f3a\u8bed\u4e49\u4e0a\u4e0b\u6587\u5e76\u63d0\u9ad8\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u672c\u4f53\u5339\u914d\uff08OM\uff09\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u624b\u5de5\u89c4\u5219\u6216\u7279\u5b9a\u6a21\u578b\uff0c\u9002\u5e94\u6027\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u52a8\u6001\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u672c\u4f53\u5339\u914d\u65b9\u6cd5\u3002

Method: KROMA\u6846\u67b6\u5229\u7528RAG\u7ba1\u9053\u4e2d\u7684LLMs\uff0c\u901a\u8fc7\u7ed3\u6784\u3001\u8bcd\u6c47\u548c\u5b9a\u4e49\u77e5\u8bc6\u52a8\u6001\u4e30\u5bccOM\u4efb\u52a1\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002\u4e3a\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\uff0c\u5b83\u96c6\u6210\u4e86\u57fa\u4e8e\u53cc\u76f8\u4f3c\u6027\u7684\u6982\u5ff5\u5339\u914d\u548c\u8f7b\u91cf\u7ea7\u672c\u4f53\u7ec6\u5316\u6b65\u9aa4\uff0c\u4ee5\u4fee\u526a\u5019\u9009\u6982\u5ff5\u5e76\u663e\u8457\u51cf\u5c11LLM\u8c03\u7528\u5e26\u6765\u7684\u901a\u4fe1\u5f00\u9500\u3002

Result: \u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u77e5\u8bc6\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u578bLLMs\u663e\u8457\u63d0\u5347\u4e86\u672c\u4f53\u5339\u914d\u6548\u679c\uff0c\u4f18\u4e8e\u7ecf\u5178\u7684OM\u7cfb\u7edf\u548c\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u5f00\u9500\u53ef\u6bd4\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6240\u63d0\u51fa\u7684\u4f18\u5316\u6280\u672f\uff08\u76ee\u6807\u77e5\u8bc6\u68c0\u7d22\u3001\u63d0\u793a\u4e30\u5bcc\u548c\u672c\u4f53\u7ec6\u5316\uff09\u5bf9\u4e8e\u5927\u89c4\u6a21\u672c\u4f53\u5339\u914d\u7684\u53ef\u884c\u6027\u548c\u76ca\u5904\u3002

Conclusion: \u5c06\u77e5\u8bc6\u68c0\u7d22\u4e0e\u4e0a\u4e0b\u6587\u589e\u5f3a\u578bLLMs\u76f8\u7ed3\u5408\uff0c\u5e76\u8f85\u4ee5\u4f18\u5316\u7684\u526a\u679d\u548c\u7ec6\u5316\u6280\u672f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u672c\u4f53\u5339\u914d\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u672c\u4f53\u5339\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u6709\u76ca\u7684\u89e3\u51b3\u65b9\u6848\u3002

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [17] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: \u8be5\u7814\u7a76\u63a8\u51fa\u4e86Glucose-ML\uff0c\u4e00\u4e2a\u5305\u542b10\u4e2a\u516c\u5f00\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u7684\u5927\u578b\u96c6\u5408\uff0c\u5e76\u8fdb\u884c\u4e86\u8840\u7cd6\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u6570\u636e\u96c6\u5dee\u5f02\u5bf9AI\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\u3002


<details>
  <summary>Details</summary>
Motivation: \u9ad8\u8d28\u91cf\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u963b\u788d\u4e86\u7cd6\u5c3f\u75c5\u9886\u57df\u7a33\u5065AI\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002

Method: \u7814\u7a76\u8005\u6536\u96c6\u4e8610\u4e2a\u8fc7\u53bb7\u5e74\u5185\u53d1\u5e03\u7684\u516c\u5f00\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\uff0c\u7ec4\u6210\u4e86Glucose-ML\u96c6\u5408\uff08\u5305\u542b30\u4e07\u5929CGM\u6570\u636e\uff0c3800\u4e07\u8840\u7cd6\u6837\u672c\uff0c\u6765\u81ea2500+\u4eba\uff09\u3002\u4ed6\u4eec\u5bf9\u8fd9\u4e9b\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u4ee5\u8840\u7cd6\u9884\u6d4b\u4e3a\u4f8b\uff0c\u5728\u6240\u6709\u6570\u636e\u96c6\u4e2d\u5bf9\u540c\u4e00\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002

Result: Glucose-ML\u96c6\u5408\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u516c\u5f00\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u4e4b\u4e00\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u540c\u4e00\u8840\u7cd6\u9884\u6d4b\u7b97\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684\u9884\u6d4b\u7ed3\u679c\u3002

Conclusion: \u6570\u636e\u96c6\u7684\u9009\u62e9\u5bf9AI\u7b97\u6cd5\u7684\u6027\u80fd\u6709\u5173\u952e\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u4e3a\u7cd6\u5c3f\u75c5\u53ca\u66f4\u5e7f\u6cdb\u5065\u5eb7\u9886\u57df\u5f00\u53d1\u7a33\u5065AI\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6570\u636e\u9009\u62e9\u6307\u5bfc\u548c\u57fa\u51c6\uff0c\u5e76\u5f00\u653e\u4e86\u6570\u636e\u96c6\u94fe\u63a5\u548c\u4ee3\u7801\u3002

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [18] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: \u672c\u7814\u7a76\u63d0\u51faG-AI-HMS\uff08\u751f\u6210\u5f0fAI\u8d4b\u80fd\u7684\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u4efb\u52a1\u4e2d\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\u7684\u4fdd\u771f\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\uff08HMS\uff09\u65b9\u6cd5\u8fd0\u52a8\u4fdd\u771f\u5ea6\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bc4\u4f30\u5de5\u4eba\u884c\u4e3a\u3001\u5b89\u5168\u548c\u751f\u4ea7\u529b\u65b9\u9762\u7684\u6210\u672c\u6548\u76ca\u3002

Method: \u672c\u7814\u7a76\u5f15\u5165G-AI-HMS\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6587\u672c\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\u3002\u5b83\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u5229\u7528\u4e0eMotionGPT\u8bad\u7ec3\u8bcd\u6c47\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u8fd0\u52a8\u611f\u77e5\u8bed\u8a00\uff1b2) \u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u5bf9AI\u589e\u5f3a\u7684\u8fd0\u52a8\u8fdb\u884c\u771f\u5b9e\u4eba\u4f53\u8fd0\u52a8\u9a8c\u8bc1\uff0c\u901a\u8fc7\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\u4ece\u5b9e\u65f6\u89c6\u9891\u4e2d\u63d0\u53d6\u5173\u8282\u5730\u6807\uff0c\u5e76\u4f7f\u7528\u8fd0\u52a8\u76f8\u4f3c\u6027\u5ea6\u91cf\u8fdb\u884c\u6bd4\u8f83\u3002

Result: \u5728\u6d89\u53ca\u516b\u9879\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAI\u589e\u5f3a\u7684\u8fd0\u52a8\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u663e\u793a\u51fa\u6bd4\u4eba\u7c7b\u521b\u5efa\u63cf\u8ff0\u66f4\u4f4e\u7684\u8bef\u5dee\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u7a7a\u95f4\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u516d\u9879\u4efb\u52a1\uff0c\u5728\u59ff\u6001\u5f52\u4e00\u5316\u540e\u7684\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u56db\u9879\u4efb\u52a1\uff0c\u5728\u6574\u4f53\u65f6\u95f4\u76f8\u4f3c\u6027\u65b9\u9762\u4f18\u4e8e\u4e03\u9879\u4efb\u52a1\u3002\u7edf\u8ba1\u5206\u6790\u8868\u660e\uff0cAI\u589e\u5f3a\u7684\u63d0\u793a\u663e\u8457\uff08p < 0.0001\uff09\u51cf\u5c11\u4e86\u5173\u8282\u8bef\u5dee\u548c\u65f6\u95f4\u9519\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u59ff\u6001\u51c6\u786e\u6027\u3002

Conclusion: G-AI-HMS\u901a\u8fc7\u5229\u7528\u751f\u6210\u5f0fAI\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u7406\u4efb\u52a1\u4e2d\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\u7684\u8d28\u91cf\uff0c\u751f\u6210\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u548c\u903c\u771f\u7684\u8fd0\u52a8\u8f93\u51fa\u3002

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [19] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: \u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u91ca\u65e0\u635f\u68c0\u6d4b\uff08NDE\uff09\u7b49\u9ad8\u7ebf\u56fe\u65b9\u9762\u7684\u80fd\u529b\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u6865\u6881\u72b6\u51b5\u5206\u6790\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86LLM\u8f85\u52a9\u7684\u6865\u6881\u68c0\u67e5\u5de5\u4f5c\u6d41\u7a0b\u6846\u67b6\u3002


<details>
  <summary>Details</summary>
Motivation: \u6865\u6881\u7ef4\u62a4\u4e2d\uff0c\u65e0\u635f\u68c0\u6d4b\uff08NDE\uff09\u6570\u636e\u89e3\u91ca\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u53ef\u80fd\u5ef6\u8fdf\u51b3\u7b56\u3002LLMs\u7684\u8fdb\u6b65\u4e3a\u81ea\u52a8\u5316\u548c\u6539\u8fdb\u6b64\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002

Method: \u672c\u8bd5\u70b9\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u7279\u5b9a\u63d0\u793a\uff0c\u63a2\u7d22\u4e86\u591a\u4e2aLLMs\u89e3\u91ca\u4e94\u79cd\u4e0d\u540cNDE\u7b49\u9ad8\u7ebf\u56fe\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u6807\u51c6\u5305\u62ec\u8be6\u7ec6\u63cf\u8ff0\u3001\u7f3a\u9677\u8bc6\u522b\u3001\u53ef\u64cd\u4f5c\u5efa\u8bae\u548c\u6574\u4f53\u51c6\u786e\u6027\u3002\u8868\u73b0\u6700\u4f73\u7684LLM\u8f93\u51fa\u518d\u7531\u5176\u4ed6LLMs\u8fdb\u884c\u603b\u7ed3\u3002

Result: \u7814\u7a76\u53d1\u73b0\u4e5d\u4e2a\u6a21\u578b\u4e2d\u6709\u56db\u4e2a\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u6709\u6548\u8986\u76d6\u6865\u6881\u72b6\u51b5\u7684\u5e7f\u6cdb\u4e3b\u9898\u3002\u5176\u4e2d\uff0cChatGPT-4\u548cClaude 3.5 Sonnet\u5728\u751f\u6210\u6709\u6548\u603b\u7ed3\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002\u7ed3\u679c\u8868\u660eLLMs\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002

Conclusion: LLMs\u5728\u6865\u6881\u7ef4\u62a4\u548c\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u56fe\u50cf\u63cf\u8ff0\u548c\u603b\u7ed3\u529f\u80fd\uff0c\u80fd\u591f\u63d0\u9ad8\u51b3\u7b56\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u5b89\u5168\u8bc4\u4f30\u3002

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [20] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316CUDA\u4f18\u5316\u6846\u67b6\uff0c\u5728\u591a\u79cdGPU\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6709\u6548\u89e3\u51b3\u4e86GPU\u8d44\u6e90\u9700\u6c42\u589e\u957f\u7684\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4GPU\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u8feb\u5207\u9700\u8981\u81ea\u52a8\u5316\u7684CUDA\u4f18\u5316\u7b56\u7565\u3002\u7136\u800c\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\u5728\u6539\u8fdbCUDA\u901f\u5ea6\u65b9\u9762\u7684\u6210\u529f\u7387\u8f83\u4f4e\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86CUDA-L1\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eCUDA\u4f18\u5316\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u52a0\u901f\u6bd4\u7684\u5956\u52b1\u4fe1\u53f7\u6765\u8bad\u7ec3\uff0c\u5c06\u521d\u59cb\u6027\u80fd\u4e0d\u4f73\u7684LLM\u8f6c\u5316\u4e3a\u6709\u6548\u7684CUDA\u4f18\u5316\u5668\uff0c\u65e0\u9700\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\u6216\u9886\u57df\u77e5\u8bc6\u3002

Result: CUDA-L1\u5728NVIDIA A100\u4e0a\u8bad\u7ec3\uff0c\u5728KernelBench\u7684250\u4e2aCUDA\u6838\u51fd\u6570\u4e0a\u5e73\u5747\u5b9e\u73b0\u4e8617.7\u500d\u7684\u52a0\u901f\uff0c\u5cf0\u503c\u52a0\u901f\u8fbe\u5230449\u500d\u3002\u5b83\u8fd8\u5c55\u793a\u4e86\u51fa\u8272\u7684\u8de8GPU\u67b6\u6784\u7684\u53ef\u79fb\u690d\u6027\uff0c\u5728H100\u3001RTX 3090\u3001L40\u3001H800\u548cH20\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8617.8\u500d\u300119.0\u500d\u300116.5\u500d\u300114.7\u500d\u548c13.9\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002\u6b64\u5916\uff0cCUDA-L1\u80fd\u53d1\u73b0\u591a\u79cd\u4f18\u5316\u6280\u672f\u5e76\u8fdb\u884c\u7b56\u7565\u6027\u7ec4\u5408\uff0c\u63ed\u793aCUDA\u4f18\u5316\u7684\u57fa\u672c\u539f\u7406\uff0c\u5e76\u8bc6\u522b\u975e\u663e\u800c\u6613\u89c1\u7684\u6027\u80fd\u74f6\u9888\u3002

Conclusion: \u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5c06\u6027\u80fd\u4e0d\u4f73\u7684LLM\u8f6c\u53d8\u4e3a\u6709\u6548\u7684CUDA\u4f18\u5316\u5668\uff0c\u4ec5\u901a\u8fc7\u57fa\u4e8e\u52a0\u901f\u6bd4\u7684\u5956\u52b1\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\uff0c\u65e0\u9700\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002\u8bad\u7ec3\u540e\u7684RL\u6a21\u578b\u80fd\u5c06\u5176\u4e60\u5f97\u7684\u63a8\u7406\u80fd\u529b\u6269\u5c55\u5230\u65b0\u7684\u6838\u51fd\u6570\uff0c\u4e3aCUDA\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u4f18\u5316\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\uff0c\u6709\u671b\u5927\u5e45\u63d0\u5347GPU\u6548\u7387\u5e76\u7f13\u89e3GPU\u8ba1\u7b97\u8d44\u6e90\u7684\u538b\u529b\u3002

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives](https://arxiv.org/abs/2507.13359)
*Yang Zhou,Junjie Li,CongYang Ou,Dawei Yan,Haokui Zhang,Xizhe Xue*

Main category: cs.CV

TL;DR: \u8be5\u8bba\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u65e0\u4eba\u673a\uff08UAV\uff09\u822a\u7a7a\u573a\u666f\u4e2d\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff08OVOD\uff09\uff0c\u6db5\u76d6\u4e86\u5176\u6838\u5fc3\u539f\u7406\u3001\u73b0\u6709\u65b9\u6cd5\u5206\u7c7b\u3001\u76f8\u5173\u6570\u636e\u96c6\u3001\u5173\u952e\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684\u65e0\u4eba\u673a\u822a\u7a7a\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u6781\u5927\u5730\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u8de8\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6280\u672f\uff08\u5982CLIP\uff09\u7684\u51fa\u73b0\uff0c\u4f7f\u5f97\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6210\u4e3a\u53ef\u80fd\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8bc6\u522b\u672a\u89c1\u8fc7\u7684\u7269\u4f53\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u5728\u822a\u7a7a\u573a\u666f\u7406\u89e3\u4e2d\u7684\u667a\u80fd\u6027\u548c\u81ea\u4e3b\u6027\u3002

Method: \u8be5\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u9996\u5148\u5c06OVOD\u7684\u6838\u5fc3\u539f\u7406\u4e0e\u65e0\u4eba\u673a\u89c6\u89c9\u7684\u72ec\u7279\u7279\u6027\u76f8\u7ed3\u5408\uff1b\u5176\u6b21\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u5206\u7c7b\u6cd5\uff0c\u5bf9\u73b0\u6709\u822a\u7a7a\u56fe\u50cf\u7684OVOD\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u6570\u636e\u96c6\u7684\u5168\u9762\u6982\u8ff0\uff1b\u6700\u540e\uff0c\u6279\u5224\u6027\u5730\u5256\u6790\u4e86\u8be5\u9886\u57df\u4ea4\u53c9\u70b9\u7684\u5173\u952e\u6311\u6218\u548c\u672a\u89e3\u51b3\u95ee\u9898\u3002

Result: \u8bba\u6587\u7cfb\u7edf\u5730\u68b3\u7406\u4e86\u65e0\u4eba\u673a\u822a\u7a7a\u573a\u666fOVOD\u7684\u6838\u5fc3\u539f\u7406\u3001\u73b0\u6709\u65b9\u6cd5\u5206\u7c7b\u3001\u76f8\u5173\u6570\u636e\u96c6\uff0c\u5e76\u660e\u786e\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u548c\u5f00\u653e\u6027\u95ee\u9898\u3002

Conclusion: \u57fa\u4e8e\u5206\u6790\uff0c\u8bba\u6587\u5c55\u671b\u4e86\u6709\u524d\u666f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u524d\u666f\uff0c\u65e8\u5728\u4e3a\u8be5\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u521d\u5b66\u8005\u548c\u8d44\u6df1\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002

Abstract: Due to its extensive applications, aerial image object detection has long
been a hot topic in computer vision. In recent years, advancements in Unmanned
Aerial Vehicles (UAV) technology have further propelled this field to new
heights, giving rise to a broader range of application requirements. However,
traditional UAV aerial object detection methods primarily focus on detecting
predefined categories, which significantly limits their applicability. The
advent of cross-modal text-image alignment (e.g., CLIP) has overcome this
limitation, enabling open-vocabulary object detection (OVOD), which can
identify previously unseen objects through natural language descriptions. This
breakthrough significantly enhances the intelligence and autonomy of UAVs in
aerial scene understanding. This paper presents a comprehensive survey of OVOD
in the context of UAV aerial scenes. We begin by aligning the core principles
of OVOD with the unique characteristics of UAV vision, setting the stage for a
specialized discussion. Building on this foundation, we construct a systematic
taxonomy that categorizes existing OVOD methods for aerial imagery and provides
a comprehensive overview of the relevant datasets. This structured review
enables us to critically dissect the key challenges and open problems at the
intersection of these fields. Finally, based on this analysis, we outline
promising future research directions and application prospects. This survey
aims to provide a clear road map and a valuable reference for both newcomers
and seasoned researchers, fostering innovation in this rapidly evolving domain.
We keep tracing related works at
https://github.com/zhouyang2002/OVOD-in-UVA-imagery

</details>


### [22] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDNIG\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u3002\u5b83\u57fa\u4e8eU-Net\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u6765\u81ea\u4eae\u901a\u9053\u5148\u9a8c\u7684\u7167\u5ea6\u56fe\u4f5c\u4e3a\u5f15\u5bfc\uff0c\u5e76\u5229\u7528\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\u548cSwish\u6fc0\u6d3b\u51fd\u6570\uff0c\u5728GAN\u6846\u67b6\u4e0b\u901a\u8fc7\u590d\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u9762\u53ef\u80fd\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u66dd\u5149\u4e0d\u8db3\u533a\u57df\u548c\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u65f6\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u4ee5\u5728\u4fdd\u6301\u8f83\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u589e\u5f3a\u56fe\u50cf\u7684\u8d28\u91cf\u3002

Method: EDNIG\u6846\u67b6\u91c7\u7528\u57fa\u4e8eU-Net\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u7ed3\u6784\u3002\u5b83\u5c06\u4ece\u4eae\u901a\u9053\u5148\u9a8c\uff08BCP\uff09\u5bfc\u51fa\u7684\u7167\u5ea6\u56fe\u4f5c\u4e3a\u5f15\u5bfc\u8f93\u5165\uff0c\u4ee5\u5e2e\u52a9\u7f51\u7edc\u5173\u6ce8\u66dd\u5149\u4e0d\u8db3\u533a\u57df\u3002\u4e3a\u63d0\u53d6\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u6a21\u578b\u96c6\u6210\u4e86\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\uff08SPP\uff09\u6a21\u5757\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u91c7\u7528Swish\u6fc0\u6d3b\u51fd\u6570\u4ee5\u786e\u4fdd\u5e73\u6ed1\u7684\u68af\u5ea6\u4f20\u64ad\u3002EDNIG\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6846\u67b6\u5185\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u7528\u7ed3\u5408\u4e86\u5bf9\u6297\u635f\u5931\u3001\u50cf\u7d20\u7ea7\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u611f\u77e5\u635f\u5931\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEDNIG\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5b83\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u663e\u793a\u51fa\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002

Conclusion: EDNIG\u6846\u67b6\u5728\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176\u7ed3\u5408\u4e86\u7167\u5ea6\u5f15\u5bfc\u3001\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548cGAN\u4f18\u5316\u7684\u8bbe\u8ba1\uff0c\u4f7f\u5176\u5728\u5b9e\u73b0\u9ad8\u8d28\u91cf\u589e\u5f3a\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u6a21\u578b\u590d\u6742\u6027\uff0c\u975e\u5e38\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002

Abstract: This paper introduces a novel deep learning framework for low-light image
enhancement, named the Encoder-Decoder Network with Illumination Guidance
(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination
map, derived from Bright Channel Prior (BCP), as a guidance input. This
illumination guidance helps the network focus on underexposed regions,
effectively steering the enhancement process. To further improve the model's
representational power, a Spatial Pyramid Pooling (SPP) module is incorporated
to extract multi-scale contextual features, enabling better handling of diverse
lighting conditions. Additionally, the Swish activation function is employed to
ensure smoother gradient propagation during training. EDNIG is optimized within
a Generative Adversarial Network (GAN) framework using a composite loss
function that combines adversarial loss, pixel-wise mean squared error (MSE),
and perceptual loss. Experimental results show that EDNIG achieves competitive
performance compared to state-of-the-art methods in quantitative metrics and
visual quality, while maintaining lower model complexity, demonstrating its
suitability for real-world applications. The source code for this work is
available at https://github.com/tranleanh/ednig.

</details>


### [23] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: \u672c\u6587\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u5728\u6b64\u7c7b\u5bf9\u4eba\u7c7b\u800c\u8a00\u7b80\u5355\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u7f3a\u4e4f\u6838\u5fc3\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\uff08\u5982VQA\u548c\u56fe\u8868\u7406\u89e3\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5728\u7b80\u5355\u7684\u611f\u77e5\u6d4b\u8bd5\u4e2d\u5b58\u5728\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u6d4b\u8bd5VLMs\u8fdb\u884c\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u7684\u80fd\u529b\uff0c\u5373\u9700\u8981\u6574\u5408\u56fe\u50cf\u4e2d\u591a\u4e2a\uff08\u53ef\u80fd\u76f8\u8ddd\u9065\u8fdc\uff09\u533a\u57df\u8bc1\u636e\u7684\u63a8\u7406\u3002

Method: \u7814\u7a76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u6765\u8bc4\u4f30VLMs\uff1a\u6bd4\u8f83\u611f\u77e5\uff08\u9700\u8981\u6bd4\u8f83\u4e24\u5f20\u56fe\u50cf\uff09\u3001\u773c\u8df3\u5f0f\u641c\u7d22\uff08\u9700\u8981\u6839\u636e\u8bc1\u636e\u79bb\u6563\u8df3\u8f6c\u5b9a\u4f4d\u76ee\u6807\uff09\u548c\u5e73\u6ed1\u89c6\u89c9\u641c\u7d22\uff08\u9700\u8981\u6cbf\u8fde\u7eed\u8f6e\u5ed3\u5e73\u6ed1\u641c\u7d22\uff09\u3002\u7814\u7a76\u4f7f\u7528\u65d7\u8230\u6a21\u578b\uff08\u5982Gemini 2.5 Pro, Claude Vision 3.7, GPT-o4-mini\uff09\u8fdb\u884c\u6d4b\u8bd5\u3002

Result: \u5373\u4f7f\u5728\u5148\u524d\u7684\u539f\u59cb\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u7684\u65d7\u8230\u6a21\u578b\uff0c\u5728\u8fd9\u4e9b\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u5931\u8d25\uff0c\u5728\u4e24\u4e2a\u5bf9\u4eba\u7c7b\u800c\u8a00\u5fae\u4e0d\u8db3\u9053\u7684\u4efb\u52a1\u53d8\u4f53\u4e0a\uff0c\u5176\u51c6\u786e\u7387\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u731c\u6d4b\u3002\u8fd9\u8868\u660e\u5f53\u524d\u6a21\u578b\u7f3a\u4e4f\u6838\u5fc3\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u539f\u59cb\u89c6\u89c9\u654f\u9510\u5ea6\u65b9\u9762\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u7f3a\u4e4f\u6838\u5fc3\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u6837\u6267\u884c\u67d0\u4e9b\u89c6\u89c9\u7b97\u6cd5\u3002

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [24] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u6784\u5316CoT\u63d0\u793a\u548c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u53d1\u73b0GRPO\u76f8\u6bd4SFT\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709VLM\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7b80\u5355\u7684CoT\u63d0\u793a\u751a\u81f3\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u7a7a\u95f4\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002

Method: \u7814\u7a76\u9996\u5148\u8bc4\u4f30\u4e86\u4e0d\u540cCoT\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u7ed3\u6784\u5316\u591a\u9636\u6bb5\u63d0\u793a\uff08SceneGraph CoT\uff09\u6709\u6548\u3002\u968f\u540e\uff0c\u4f7f\u7528\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5728SAT\u6570\u636e\u96c6\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728CVBench\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u7279\u522b\u5173\u6ce8\u4e86OOD\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002

Result: \u7b80\u5355\u7684CoT\u63d0\u793a\u4e0d\u4ec5\u65e0\u76ca\u53cd\u800c\u6709\u5bb3\u3002\u7ed3\u6784\u5316\u7684SceneGraph CoT\u663e\u8457\u63d0\u9ad8\u4e86\u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u6027\u3002\u4e0eSFT\u76f8\u6bd4\uff0cGRPO\u5728Pass@1\u8bc4\u4f30\u4e0a\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002SFT\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u5c42\u8bed\u8a00\u6a21\u5f0f\uff0c\u800cGRPO\u5728\u63aa\u8f9e\u53d8\u5316\u4e0b\u80fd\u66f4\u53ef\u9760\u5730\u6cdb\u5316\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002

Conclusion: \u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u548c\u7ed3\u6784\u5316\u63d0\u793a\uff08SceneGraph CoT\uff09\u80fd\u591f\u6709\u6548\u63d0\u5347\u73b0\u4ee3VLM\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u884c\u4e3a\u3002

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [25] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75282D\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u8bcd\u6c473D\u76ee\u6807\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u76843D\u6807\u7b7e\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\uff0c\u5728\u591a\u79cd\u8f93\u5165\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u4ee33D\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u72ed\u7a84\u7684\u7c7b\u522b\u5206\u7c7b\u548c\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5f00\u653e\u4e16\u754c\u573a\u666f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u4e8e\u7f51\u7edc\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u5bf9\u8bad\u7ec3\u76842D\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u3002

Method: \u8be5\u65b9\u6cd5\u5229\u75282D\u89c6\u89c9-\u8bed\u8a00\u68c0\u6d4b\u5668\u751f\u6210\u6587\u672c\u6761\u4ef6\u4e0b\u7684\u63d0\u8bae\uff0c\u7136\u540e\u4f7f\u7528SAM\u8fdb\u884c\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u76f8\u673a\u51e0\u4f55\u548cLiDAR\u6216\u5355\u76ee\u4f2a\u6df1\u5ea6\u5c06\u5176\u53cd\u6295\u5f71\u52303D\u7a7a\u95f4\u3002\u4e3a\u63a8\u65ad3D\u8fb9\u754c\u6846\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8eDBSCAN\u805a\u7c7b\u548c\u65cb\u8f6c\u5361\u5c3a\uff08Rotating Calipers\uff09\u7684\u51e0\u4f55\u81a8\u80c0\u7b56\u7565\uff0c\u65e0\u9700\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86Pseudo-nuScenes\u6570\u636e\u96c6\uff08nuScenes\u7684\u96fe\u589e\u5f3a\u3001\u7eafRGB\u53d8\u4f53\uff09\u6765\u6a21\u62df\u6076\u52a3\u7684\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u8fdb\u884c\u8bc4\u4f30\u3002

Result: \u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\uff08\u5305\u62ec\u57fa\u4e8eLiDAR\u548c\u7eafRGB-D\u8f93\u5165\uff09\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u514d\u8bad\u7ec3\u548c\u5f00\u653e\u8bcd\u6c47\u7684\u7279\u6027\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e862D\u57fa\u7840\u6a21\u578b\u5728\u53ef\u6269\u5c553D\u611f\u77e5\u65b9\u9762\u5c1a\u672a\u5f00\u53d1\u7684\u5de8\u5927\u6f5c\u529b\u3002

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [26] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u7f51\u7edc\u53ca\u5176\u8bad\u7ec3\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u7ea612\u79cd\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u7684Transformer\u67b6\u6784\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6570\u636e\u6295\u5f71\u5230\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u52a8\u673a\u662f\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u548c\u591a\u4efb\u52a1\u573a\u666f\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u6765\u81ea\u591a\u79cd\u6570\u636e\u6e90\u7684\u590d\u6742\u4fe1\u606f\u3002

Method: \u8be5\u65b9\u6cd5\u5305\u62ec\u6a21\u6001\u4e13\u7528\u5206\u8bcd\u5668\u3001\u5171\u4eab\u7684Transformer\u67b6\u6784\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u6765\u81ea\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\u6295\u5f71\u5230\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\u3002\u5b83\u901a\u8fc7\u4e3a\u4e0d\u540c\u6a21\u6001\u4e2d\u7684\u4e0d\u540c\u4efb\u52a1\u96c6\u6210\u6a21\u6001\u7279\u5b9a\u7684\u4efb\u52a1\u5934\u6765\u89e3\u51b3\u591a\u6a21\u6001\u548c\u591a\u4efb\u52a1\u573a\u666f\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u8fed\u4ee3\u6a21\u6001\u5207\u6362\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u521d\u59cb\u5316\u7f51\u7edc\uff0c\u4ee5\u53ca\u4e00\u79cd\u8bad\u7ec3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6743\u8861\u4e86\u6240\u6709\u6a21\u6001\u4e0a\u7684\u5b8c\u5168\u8054\u5408\u8bad\u7ec3\u4e0e\u4e00\u6b21\u8bad\u7ec3\u4e00\u5bf9\u6a21\u6001\u3002

Result: \u5728\u6765\u81ea12\u79cd\u6a21\u6001\u768425\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u9002\u5e94\u6027\u591a\u4efb\u52a1\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u7f51\u7edc\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u5728\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\u3002

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [27] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u548cTransformer\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u533b\u7597\u5eb7\u590d\u6548\u679c\uff0c\u89e3\u51b3\u6570\u636e\u566a\u58f0\u548c\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38\u8fd0\u52a8\u3002


<details>
  <summary>Details</summary>
Motivation: \u5149\u5b66\u8fd0\u52a8\u6355\u6349\u6570\u636e\u5e38\u56e0\u906e\u6321\u548c\u73af\u5883\u56e0\u7d20\u4ea7\u751f\u566a\u58f0\u548c\u7f3a\u5931\uff0c\u4e14\u9700\u8981\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38\u8fd0\u52a8\u4ee5\u4fdd\u969c\u60a3\u8005\u5b89\u5168\u3002\u73b0\u6709\u5eb7\u590d\u65b9\u6848\u53ef\u80fd\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u9700\u8981\u51cf\u5c11\u73b0\u573a\u76d1\u7763\u3002

Method: \u91c7\u7528\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u6280\u672f\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u5229\u7528\u65f6\u5e8f\u5e8f\u5217\u5efa\u6a21\u5bf9\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8fdb\u884c\u53bb\u566a\u548c\u8865\u5168\uff0c\u5e76\u5b9e\u73b0\u5f02\u5e38\u8fd0\u52a8\u7684\u5b9e\u65f6\u68c0\u6d4b\u3002

Result: \u5728\u9488\u5bf9\u4e2d\u98ce\u548c\u9aa8\u79d1\u5eb7\u590d\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u6570\u636e\u91cd\u5efa\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002

Conclusion: \u8be5\u6846\u67b6\u4e3a\u8fdc\u7a0b\u5eb7\u590d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u80fd\u6709\u6548\u51cf\u5c11\u73b0\u573a\u76d1\u7763\u7684\u9700\u6c42\u3002

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [28] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408Vision Transformers (ViT) \u548c Graph Neural Networks (GNN) \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e73\u817a\u764c\u68c0\u6d4b\uff0c\u5728CBIS-DDSM\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8684.2%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u3002


<details>
  <summary>Details</summary>
Motivation: \u4e73\u817a\u764c\u662f\u5168\u7403\u5973\u6027\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002

Method: \u8be5\u7814\u7a76\u6574\u5408\u4e86ViT\u4ee5\u6355\u83b7\u5168\u5c40\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u5229\u7528GNN\u5efa\u6a21\u7ed3\u6784\u5173\u7cfb\u3002\u6a21\u578b\u5728CBIS-DDSM\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u3002

Result: \u8be5\u6846\u67b6\u5b9e\u73b0\u4e8684.2%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u4e3a\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002

Conclusion: \u6240\u63d0\u51fa\u7684ViT\u548cGNN\u96c6\u6210\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4e73\u817a\u764c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [29] [Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection](https://arxiv.org/abs/2507.13373)
*Xiaojian Lin,Wenxin Zhang,Yuchu Jiang,Wangyu Wu,Yiran Guo,Kangxu Wang,Zongzheng Zhang,Guijin Wang,Lei Jin,Hao Zhao*

Main category: cs.CV

TL;DR: Butter\u662f\u4e00\u79cd\u65b0\u578b\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u5206\u5c42\u7279\u5f81\u8868\u793a\u6765\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\u4e0e\u7cbe\u5ea6\u6548\u7387\u5e73\u8861\u7684\u6311\u6218\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\uff08\u5982YOLO\u548cDETR\uff09\u5728\u4fdd\u6301\u8de8\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\u3001\u5e73\u8861\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u591a\u7ea7\u8bed\u4e49\u7406\u89e3\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u51c6\u786e\u8bc6\u522b\u76ee\u6807\u81f3\u5173\u91cd\u8981\u3002

Method: Butter\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a1) \u9891\u7387\u81ea\u9002\u5e94\u7279\u5f81\u4e00\u81f4\u6027\u589e\u5f3a\uff08FAFCE\uff09\u7ec4\u4ef6\uff0c\u5229\u7528\u81ea\u9002\u5e94\u9891\u7387\u6ee4\u6ce2\u7ec6\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u7ed3\u6784\u548c\u8fb9\u754c\u7cbe\u5ea6\uff1b2) \u6e10\u8fdb\u5f0f\u5206\u5c42\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08PHFFNet\uff09\u6a21\u5757\uff0c\u9010\u6b65\u6574\u5408\u591a\u7ea7\u7279\u5f81\u4ee5\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\u5e76\u5f3a\u5316\u5206\u5c42\u7279\u5f81\u5b66\u4e60\u3002

Result: \u5728BDD100K\u3001KITTI\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cButter\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u53ef\u90e8\u7f72\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002

Conclusion: Butter\u901a\u8fc7\u4e13\u6ce8\u4e8e\u5206\u5c42\u7279\u5f81\u7684\u7ec6\u5316\u548c\u6574\u5408\uff0c\u4e3a\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u3001\u53ef\u90e8\u7f72\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002

Abstract: Hierarchical feature representations play a pivotal role in computer vision,
particularly in object detection for autonomous driving. Multi-level semantic
understanding is crucial for accurately identifying pedestrians, vehicles, and
traffic signs in dynamic environments. However, existing architectures, such as
YOLO and DETR, struggle to maintain feature consistency across different scales
while balancing detection precision and computational efficiency. To address
these challenges, we propose Butter, a novel object detection framework
designed to enhance hierarchical feature representations for improving
detection robustness. Specifically, Butter introduces two key innovations:
Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which
refines multi-scale feature consistency by leveraging adaptive frequency
filtering to enhance structural and boundary precision, and Progressive
Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively
integrates multi-level features to mitigate semantic gaps and strengthen
hierarchical feature learning. Through extensive experiments on BDD100K, KITTI,
and Cityscapes, Butter demonstrates superior feature representation
capabilities, leading to notable improvements in detection accuracy while
reducing model complexity. By focusing on hierarchical feature refinement and
integration, Butter provides an advanced approach to object detection that
achieves a balance between accuracy, deployability, and computational
efficiency in real-time autonomous driving scenarios. Our model and
implementation are publicly available at https://github.com/Aveiro-Lin/Butter,
facilitating further research and validation within the autonomous driving
community.

</details>


### [30] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u8def\u7531\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f73\u6a21\u6001\u8fdb\u884c\u591a\u6a21\u6001\u89c6\u9891\u68c0\u7d22\uff0c\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u68c0\u7d22\u6548\u679c\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5bc6\u96c6\u6587\u672c\u5b57\u5e55\uff09\u867d\u7136\u80fd\u8fbe\u523075.9%\u7684Recall@5\uff0c\u4f46\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u5904\u7406\uff0c\u5e76\u4e14\u4f1a\u9057\u6f0f\u5173\u952e\u7684\u89c6\u89c9\u4fe1\u606f\uff08\u4f8b\u598234%\u89c6\u9891\u7247\u6bb5\u4e2d\u7684\u573a\u666f\u6587\u672c\u672a\u88abASR\u6355\u83b7\uff09\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5bfb\u6c42\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u80fd\u6355\u83b7\u591a\u6a21\u6001\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002

Method: ModaRoute\u5229\u7528GPT-4.1\u5206\u6790\u67e5\u8be2\u610f\u56fe\u5e76\u9884\u6d4b\u4fe1\u606f\u9700\u6c42\uff0c\u667a\u80fd\u5730\u5c06\u67e5\u8be2\u8def\u7531\u5230ASR\uff08\u8bed\u97f3\uff09\u3001OCR\uff08\u6587\u672c\uff09\u548c\u89c6\u89c9\u7d22\u5f15\u7b49\u4e0d\u540c\u6a21\u6001\u3002\u5b83\u5e73\u5747\u6bcf\u4e2a\u67e5\u8be2\u4ec5\u4f7f\u75281.78\u4e2a\u6a21\u6001\uff0c\u663e\u8457\u5c11\u4e8e\u7a77\u4e3e\u641c\u7d22\u76843.0\u4e2a\u6a21\u6001\u3002

Result: ModaRoute\u5728\u4fdd\u630160.9% Recall@5\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e\u4e8641%\u3002\u5728\u5bf9180\u4e07\u4e2a\u89c6\u9891\u7247\u6bb5\u7684\u8bc4\u4f30\u4e2d\uff0c\u8be5\u7cfb\u7edf\u88ab\u8bc1\u660e\u662f\u6269\u5c55\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002

Conclusion: \u667a\u80fd\u8def\u7531\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u964d\u4f4e\u57fa\u7840\u8bbe\u65bd\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e0e\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u76f8\u7b26\u7684\u6709\u6548\u6027\u3002

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [31] [A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects](https://arxiv.org/abs/2507.13378)
*Yuqi Cheng,Yunkang Cao,Haiming Yao,Wei Luo,Cheng Jiang,Hui Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: \u8fd9\u7bc7\u7efc\u8ff0\u6df1\u5165\u5206\u6790\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u9886\u57df\uff0c\u6db5\u76d6\u4e862D\u548c3D\u6a21\u6001\u4e0b\u7684\u95ed\u96c6\u4e0e\u5f00\u96c6\u68c0\u6d4b\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u7684\u6f14\u53d8\u3001\u6311\u6218\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5c24\u5176\u5f3a\u8c03\u4e86\u5f00\u96c6\u6280\u672f\u7684\u91cd\u8981\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684\u5de5\u4e1a\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u5236\u9020\u4e1a\u5bf9\u7cbe\u5ea6\u3001\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u6027\u7684\u9ad8\u8981\u6c42\u3002\u5c3d\u7ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u76ee\u524d\u4ecd\u7f3a\u4e4f\u5bf9\u8be5\u9886\u57df\u8fde\u8d2f\u548c\u5168\u9762\u7684\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u4ece\u95ed\u96c6\u5411\u5f00\u96c6\u68c0\u6d4b\u6846\u67b6\u8f6c\u53d8\u7684\u80cc\u666f\u4e0b\u3002

Method: \u672c\u6587\u91c7\u7528\u7efc\u8ff0\uff08survey\uff09\u7684\u5f62\u5f0f\uff0c\u5bf92D\u548c3D\u6a21\u6001\u4e0b\u7684\u95ed\u96c6\uff08closed-set\uff09\u548c\u5f00\u96c6\uff08open-set\uff09\u7f3a\u9677\u68c0\u6d4b\u7b56\u7565\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002\u5b83\u8ffd\u6eaf\u4e86\u8fd9\u4e9b\u6280\u672f\u8fd1\u5e74\u6765\u7684\u6f14\u53d8\uff0c\u5f3a\u8c03\u4e86\u5f00\u96c6\u6280\u672f\u7684\u65e5\u76ca\u7a81\u51fa\uff0c\u5e76\u63d0\u70bc\u4e86\u5b9e\u9645\u68c0\u6d4b\u73af\u5883\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u65b0\u5174\u8d8b\u52bf\u3002

Result: \u8be5\u7efc\u8ff0\u63d0\u4f9b\u4e86\u5bf92D\u548c3D\u6a21\u6001\u4e0b\u95ed\u96c6\u548c\u5f00\u96c6\u7f3a\u9677\u68c0\u6d4b\u7b56\u7565\u7684\u6df1\u5165\u5206\u6790\uff0c\u5c55\u793a\u4e86\u8be5\u9886\u57df\u7684\u6f14\u53d8\u8fc7\u7a0b\u4ee5\u53ca\u5f00\u96c6\u6280\u672f\u7684\u65e5\u76ca\u91cd\u8981\u6027\u3002\u5b83\u8fd8\u8bc6\u522b\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u9610\u660e\u4e86\u672a\u6765\u7684\u65b0\u5174\u8d8b\u52bf\u3002

Conclusion: \u672c\u6587\u4e3a\u5feb\u901f\u53d1\u5c55\u7684\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f53\u524d\u4e14\u5168\u9762\u7684\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u73b0\u72b6\u3001\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002

Abstract: Industrial defect detection is vital for upholding product quality across
contemporary manufacturing systems. As the expectations for precision,
automation, and scalability intensify, conventional inspection approaches are
increasingly found wanting in addressing real-world demands. Notable progress
in computer vision and deep learning has substantially bolstered defect
detection capabilities across both 2D and 3D modalities. A significant
development has been the pivot from closed-set to open-set defect detection
frameworks, which diminishes the necessity for extensive defect annotations and
facilitates the recognition of novel anomalies. Despite such strides, a
cohesive and contemporary understanding of industrial defect detection remains
elusive. Consequently, this survey delivers an in-depth analysis of both
closed-set and open-set defect detection strategies within 2D and 3D
modalities, charting their evolution in recent years and underscoring the
rising prominence of open-set techniques. We distill critical challenges
inherent in practical detection environments and illuminate emerging trends,
thereby providing a current and comprehensive vista of this swiftly progressing
field.

</details>


### [32] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u5149\u5b66\u536b\u661f\u56fe\u50cf\u4e0e\u591a\u6837\u5316\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u878d\u5408\uff0c\u80fd\u663e\u8457\u63d0\u5347\u536b\u661f\u673a\u5668\u5b66\u4e60\uff08SatML\uff09\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u548c\u5730\u7406\u5916\u6837\u672c\u573a\u666f\u4e0b\uff0c\u4e14\u7b80\u5355\u7684\u786c\u7f16\u7801\u878d\u5408\u7b56\u7565\u8868\u73b0\u4f18\u4e8e\u5b66\u4e60\u5230\u7684\u7b56\u7565\u3002


<details>
  <summary>Details</summary>
Motivation: \u5927\u591a\u6570\u536b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u5149\u5b66\u8f93\u5165\u6a21\u6001\uff08\u5982\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\uff09\u8bbe\u8ba1\uff0c\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5728\u76d1\u7763\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u7ed3\u5408\u5176\u4ed6\u8f93\u5165\u6a21\u6001\uff08\u5982DEM\u3001\u571f\u5730\u8986\u76d6\u56fe\u3001\u73af\u5883\u4f20\u611f\u5668\u6570\u636e\uff09\u4e0e\u5149\u5b66\u56fe\u50cf\u7684\u4ef7\u503c\u3002

Method: \u901a\u8fc7\u5c06\u989d\u5916\u7684\u5730\u7406\u6570\u636e\u5c42\u9644\u52a0\u5230\u73b0\u6709\u7684\u536b\u661f\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u4efb\u52a1\u6570\u636e\u96c6\uff08\u6db5\u76d6\u5206\u7c7b\u3001\u56de\u5f52\u548c\u5206\u5272\uff09\u4e0a\uff0c\u751f\u6210\u4e86\u589e\u5f3a\u7248\u6570\u636e\u96c6\u3002\u4f7f\u7528\u8fd9\u4e9b\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u878d\u5408\u7b56\u7565\uff08\u786c\u7f16\u7801\u4e0e\u5b66\u4e60\u578b\uff09\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002

Result: \u5c06\u989d\u5916\u7684\u5730\u7406\u8f93\u5165\u4e0e\u5149\u5b66\u56fe\u50cf\u878d\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u536b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u5730\u7406\u5916\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u8fd9\u79cd\u76ca\u5904\u6700\u5927\uff0c\u8868\u660e\u591a\u6a21\u6001\u8f93\u5165\u5bf9\u4e8eSatML\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u548c\u5916\u6837\u672c\u6027\u80fd\u7279\u522b\u6709\u4ef7\u503c\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u786c\u7f16\u7801\u878d\u5408\u7b56\u7565\u4f18\u4e8e\u5b66\u4e60\u578b\u7b56\u7565\u3002

Conclusion: \u591a\u6a21\u6001\u8f93\u5165\u5bf9\u4e8e\u63d0\u9ad8\u536b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u548c\u5916\u6837\u672c\u6027\u80fd\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u7b80\u5355\u7684\u3001\u786c\u7f16\u7801\u7684\u878d\u5408\u7b56\u7565\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u66f4\u6709\u6548\uff0c\u8fd9\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u8da3\u7684\u542f\u793a\u3002

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [33] [From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.13387)
*Chihiro Noguchi,Takaki Yamamoto*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6210\u672c\u8f83\u4f4e\u7684\u5927\u89c4\u6a21\u4e8c\u5143\u5360\u636e\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5b66\u4e60\u578b\u81ea\u52a8\u6807\u6ce8\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6602\u8d35\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u81ea\u52a8\u9a7e\u9a76\u4e2d\u7cbe\u786e\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6570\u636e\uff08\u5e26\u6807\u6ce8\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\uff09\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u3002\u800c\u5927\u89c4\u6a21\u4e8c\u5143\u5360\u636e\u6570\u636e\uff08\u4ec5\u533a\u5206\u5360\u636e/\u7a7a\u95f2\uff09\u6210\u672c\u8f83\u4f4e\u4e14\u6613\u4e8e\u83b7\u53d6\uff0c\u4f46\u5176\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u53d1\u6398\u3002

Method: \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4e8c\u5143\u5360\u636e\u7684\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e8c\u5143\u5360\u636e\u6a21\u5757\u548c\u8bed\u4e49\u5360\u636e\u6a21\u5757\u3002\u8be5\u7814\u7a76\u4ece\u9884\u8bad\u7ec3\u548c\u5b66\u4e60\u578b\u81ea\u52a8\u6807\u6ce8\u4e24\u4e2a\u89d2\u5ea6\uff0c\u63a2\u7d22\u4e86\u5927\u89c4\u6a21\u4e8c\u5143\u5360\u636e\u6570\u636e\u7684\u5229\u7528\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u9884\u8bad\u7ec3\u548c\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u5176\u5728\u589e\u5f3a3D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002

Conclusion: \u5927\u89c4\u6a21\u4e8c\u5143\u5360\u636e\u6570\u636e\u80fd\u591f\u88ab\u6709\u6548\u5229\u7528\uff0c\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\uff08\u9884\u8bad\u7ec3\u3001\u81ea\u52a8\u6807\u6ce8\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u7684\u6027\u80fd\u3002

Abstract: Accurate perception of the surrounding environment is essential for safe
autonomous driving. 3D occupancy prediction, which estimates detailed 3D
structures of roads, buildings, and other objects, is particularly important
for vision-centric autonomous driving systems that do not rely on LiDAR
sensors. However, in 3D semantic occupancy prediction -- where each voxel is
assigned a semantic label -- annotated LiDAR point clouds are required, making
data acquisition costly. In contrast, large-scale binary occupancy data, which
only indicate occupied or free space without semantic labels, can be collected
at a lower cost. Despite their availability, the potential of leveraging such
data remains unexplored. In this study, we investigate the utilization of
large-scale binary occupancy data from two perspectives: (1) pre-training and
(2) learning-based auto-labeling. We propose a novel binary occupancy-based
framework that decomposes the prediction process into binary and semantic
occupancy modules, enabling effective use of binary occupancy data. Our
experimental results demonstrate that the proposed framework outperforms
existing methods in both pre-training and auto-labeling tasks, highlighting its
effectiveness in enhancing 3D semantic occupancy prediction. The code is
available at https://github.com/ToyotaInfoTech/b2s-occupancy

</details>


### [34] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u8f93\u51fa\u5206\u5e03\u8ddd\u79bb\u7684\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u548c\u795e\u7ecf\u5143\u63a9\u853d\uff0c\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6574\u4f53\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u751f\u6210\u6a21\u578b\u4e2d\u6982\u5ff5\u7684\u9c81\u68d2\u64e6\u9664\uff0c\u4ee5\u5e94\u5bf9\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u4f9d\u8d56\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\u3002\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5f80\u5f80\u8fc7\u5ea6\u4fee\u6539\u6a21\u578b\uff0c\u635f\u5bb3\u5176\u6574\u4f53\u5b9e\u7528\u6027\u3002

Method: \u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u76ee\u6807\uff0c\u4ec5\u57fa\u4e8e\u6700\u7ec8\u751f\u6210\u8f93\u51fa\u7684\u5206\u5e03\u8ddd\u79bb\u3002\u4ed6\u4eec\u63a8\u5bfc\u51fa\u4e00\u4e2a\u53ef\u5fae\u5206\u4f18\u5316\u7684\u53ef\u5904\u7406\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u7aef\u5230\u7aef\u7684\u53cd\u5411\u4f20\u64ad\u3002\u4e3a\u589e\u5f3a\u64e6\u9664\u7684\u9c81\u68d2\u6027\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u795e\u7ecf\u5143\u63a9\u853d\u4f5c\u4e3a\u6a21\u578b\u5fae\u8c03\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7406\u8bba\u5206\u6790\u3002

Result: \u5728\u6700\u5148\u8fdb\u7684\u6d41\u5339\u914d\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9c81\u68d2\u5730\u64e6\u9664\u6982\u5ff5\uff0c\u4e14\u4e0d\u964d\u4f4e\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002

Conclusion: \u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u8d1f\u8d23\u4efb\u7684\u751f\u6210\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6982\u5ff5\u64e6\u9664\u4e2d\u7684\u8fc7\u5ea6\u4fee\u6539\u95ee\u9898\u3002

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [35] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: \u672c\u6587\u6df1\u5165\u5206\u6790\u4e86StyleGAN\u751f\u6210\u5668\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u63a2\u8ba8\u4e86\u5176\u67b6\u6784\u3001\u6743\u91cd\u4fee\u526a\u548c\u6f5c\u5728\u5411\u91cf\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u5f71\u54cd\uff0c\u5e76\u5f3a\u8c03\u4e86\u6f5c\u5728\u7684\u4f26\u7406\u98ce\u9669\u3002


<details>
  <summary>Details</summary>
Motivation: \u968f\u7740AI\u751f\u6210\u56fe\u50cf\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u4eba\u4eec\u5bf9StyleGAN\u7b49\u6a21\u578b\u751f\u6210\u7684\u9ad8\u5ea6\u903c\u771f\u5408\u6210\u4eba\u8138\u7684\u6f5c\u5728\u5371\u9669\u65e5\u76ca\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u8fd9\u7c7b\u6a21\u578b\u5982\u4f55\u8fd0\u4f5c\uff0c\u7279\u522b\u662fStyleGAN\u751f\u6210\u5668\u7684\u5185\u90e8\u673a\u5236\uff0c\u4ee5\u63ed\u793a\u5176\u884c\u4e3a\u548c\u6f5c\u5728\u7684\u6ee5\u7528\u98ce\u9669\u3002

Method: \u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a\u8be6\u7ec6\u63a2\u8ba8StyleGAN\u751f\u6210\u5668\u7684\u5173\u952e\u67b6\u6784\u5143\u7d20\u548c\u6280\u672f\uff08\u5982Equalized Learning Rate\uff09\uff1b\u4f7f\u7528PyTorch\u6846\u67b6\u8bad\u7ec3StyleGAN\u6a21\u578b\uff0c\u4ee5\u4fbf\u76f4\u63a5\u68c0\u67e5\u5176\u5b66\u4e60\u5230\u7684\u6743\u91cd\uff1b\u901a\u8fc7\u6743\u91cd\u4fee\u526a\u6280\u672f\u8bc4\u4f30\u5176\u8ba1\u7b97\u6548\u7387\uff1b\u5bc6\u5207\u68c0\u67e5\u6f5c\u5728\u5411\u91cf\u5bf9\u751f\u6210\u4eba\u8138\u5916\u89c2\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5168\u5c40\u548c\u9488\u5bf9\u6027\u6539\u53d8\u3002

Result: \u7814\u7a76\u53d1\u73b0\uff1aStyleGAN\u6a21\u578b\u7684\u5927\u91cf\u6743\u91cd\u53ef\u4ee5\u5728\u4e0d\u663e\u8457\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u88ab\u79fb\u9664\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff1b\u5bf9\u6f5c\u5728\u5411\u91cf\u7684\u5168\u5c40\u6539\u53d8\u4e3b\u8981\u5f71\u54cd\u56fe\u50cf\u7684\u8272\u8c03\uff1b\u5bf9\u6f5c\u5728\u5411\u91cf\u4e2a\u4f53\u7ef4\u5ea6\u7684\u9488\u5bf9\u6027\u6539\u53d8\u53ef\u4ee5\u7cbe\u786e\u64cd\u63a7\u7279\u5b9a\u7684\u9762\u90e8\u7279\u5f81\u3002

Conclusion: \u8be5\u7814\u7a76\u63ed\u793a\u4e86StyleGAN\u5fae\u8c03\u89c6\u89c9\u7279\u5f81\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u8fd9\u4e0d\u4ec5\u5177\u6709\u5b66\u672f\u4ef7\u503c\uff0c\u4e5f\u51f8\u663e\u4e86\u8be5\u6280\u672f\u88ab\u6076\u610f\u884c\u4e3a\u8005\u5229\u7528\u6765\u4f2a\u9020\u903c\u771f\u865a\u5047\u8eab\u4efd\u7684\u4e25\u91cd\u4f26\u7406\u98ce\u9669\uff0c\u5bf9\u6570\u5b57\u6b3a\u9a97\u548c\u7f51\u7edc\u72af\u7f6a\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [36] [InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2507.13397)
*Kaiyuan Zhai,Juan Chen,Chao Wang,Zeyi Xu*

Main category: cs.CV

TL;DR: \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInSyn\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u884c\u4eba\u8f68\u8ff9\uff0c\u5b83\u80fd\u663e\u5f0f\u6355\u6349\u591a\u6837\u5316\u7684\u4ea4\u4e92\u6a21\u5f0f\u548c\u65b9\u5411\u654f\u611f\u7684\u793e\u4ea4\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86SSOS\u8bad\u7ec3\u7b56\u7565\u4ee5\u7f13\u89e3\u521d\u59cb\u6b65\u53d1\u6563\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e\u5efa\u6a21\u884c\u4eba\u4ea4\u4e92\uff0c\u4f46\u5ffd\u7565\u4e86\u914d\u5bf9\u884c\u8d70\u6216\u51b2\u7a81\u884c\u4e3a\u7b49\u7279\u5b9a\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5728\u62e5\u6324\u573a\u666f\u4e2d\u9884\u6d4b\u51c6\u786e\u6027\u53d7\u9650\u3002\u540c\u65f6\uff0c\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u666e\u904d\u5b58\u5728\u521d\u59cb\u6b65\u53d1\u6563\u95ee\u9898\u3002

Method: \u63d0\u51fa\u4e86InSyn\uff08Interaction-Synchronization Network\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u663e\u5f0f\u6355\u83b7\u591a\u6837\u5316\u4ea4\u4e92\u6a21\u5f0f\uff08\u5982\u540c\u6b65\u884c\u8d70\u6216\u51b2\u7a81\uff09\u5e76\u6709\u6548\u5efa\u6a21\u65b9\u5411\u654f\u611f\u7684\u793e\u4ea4\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u540d\u4e3aSSOS\uff08Seq-Start of Seq\uff09\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u65e8\u5728\u7f13\u89e3\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e38\u89c1\u7684\u521d\u59cb\u6b65\u53d1\u6563\u95ee\u9898\u3002

Result: \u5728ETH\u548cUCY\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cInSyn\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e2d\u3002\u6b64\u5916\uff0cSSOS\u7b56\u7565\u88ab\u8bc1\u660e\u80fd\u6709\u6548\u63d0\u5347\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u5c06\u521d\u59cb\u6b65\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea66.58%\u3002

Conclusion: InSyn\u6a21\u578b\u901a\u8fc7\u6355\u6349\u591a\u6837\u5316\u4ea4\u4e92\u6a21\u5f0f\u548c\u65b9\u5411\u654f\u611f\u884c\u4e3a\uff0c\u7ed3\u5408SSOS\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u521d\u59cb\u6b65\u53d1\u6563\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u62e5\u6324\u73af\u5883\u3002

Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent
applications, yet it remains highly challenging due to the complexity of
interactions among pedestrians. Previous methods have primarily relied on
relative positions to model pedestrian interactions; however, they tend to
overlook specific interaction patterns such as paired walking or conflicting
behaviors, limiting the prediction accuracy in crowded scenarios. To address
this issue, we propose InSyn (Interaction-Synchronization Network), a novel
Transformer-based model that explicitly captures diverse interaction patterns
(e.g., walking in sync or conflicting) while effectively modeling
direction-sensitive social behaviors. Additionally, we introduce a training
strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue
of initial-step divergence in numerical time-series prediction. Experiments on
the ETH and UCY datasets demonstrate that our model outperforms recent
baselines significantly, especially in high-density scenarios. Furthermore, the
SSOS strategy proves effective in improving sequential prediction performance,
reducing the initial-step prediction error by approximately 6.58%.

</details>


### [37] [A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data](https://arxiv.org/abs/2507.13852)
*Luigi Russo,Francesco Mauro,Babak Memar,Alessandro Sebastianelli,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Quanvolutional\u9884\u5904\u7406\u4e0eAttention U-Net\u6a21\u578b\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5229\u7528Sentinel-1 SAR\u56fe\u50cf\u8fdb\u884c\u57ce\u5e02\u5efa\u7b51\u5206\u5272\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u57ce\u5e02\u89c4\u5212\u3001\u707e\u5bb3\u54cd\u5e94\u548c\u4eba\u53e3\u6d4b\u7ed8\u7b49\u9886\u57df\uff0c\u57ce\u5e02\u5efa\u7b51\u5206\u5272\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u536b\u661f\u56fe\u50cf\u5c3a\u5bf8\u5927\u3001\u5206\u8fa8\u7387\u9ad8\uff0c\u5728\u5bc6\u96c6\u7684\u57ce\u5e02\u533a\u57df\u51c6\u786e\u5206\u5272\u5efa\u7b51\u9762\u4e34\u6311\u6218\u3002

Method: \u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528Quanvolutional\u9884\u5904\u7406\u6765\u589e\u5f3aAttention U-Net\u6a21\u578b\u5728\u5efa\u7b51\u5206\u5272\u65b9\u9762\u7684\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u5229\u7528Sentinel-1\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u56fe\u50cf\uff0c\u805a\u7126\u4e8e\u7a81\u5c3c\u65af\u7684\u57ce\u5e02\u666f\u89c2\u3002Quanvolution\u88ab\u7528\u4e8e\u63d0\u53d6\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u7279\u5f81\u56fe\uff0c\u4ee5\u6355\u6349\u96f7\u8fbe\u56fe\u50cf\u4e2d\u91cd\u8981\u7684\u7ed3\u6784\u7ec6\u8282\u3002

Result: \u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d4b\u8bd5\u51c6\u786e\u6027\u65b9\u9762\u4e0e\u6807\u51c6Attention U-Net\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u7f51\u7edc\u53c2\u6570\u3002\u8fd9\u4e00\u7ed3\u679c\u4e0e\u5148\u524d\u7684\u5de5\u4f5c\u4e00\u81f4\uff0c\u8bc1\u5b9e\u4e86Quanvolution\u4e0d\u4ec5\u4fdd\u6301\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002

Conclusion: \u8fd9\u4e9b\u6709\u524d\u666f\u7684\u6210\u679c\u7a81\u51fa\u4e86\u91cf\u5b50\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u5efa\u7b51\u5206\u5272\u7684\u6f5c\u529b\u3002

Abstract: Building segmentation in urban areas is essential in fields such as urban
planning, disaster response, and population mapping. Yet accurately segmenting
buildings in dense urban regions presents challenges due to the large size and
high resolution of satellite images. This study investigates the use of a
Quanvolutional pre-processing to enhance the capability of the Attention U-Net
model in the building segmentation. Specifically, this paper focuses on the
urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)
imagery. In this work, Quanvolution was used to extract more informative
feature maps that capture essential structural details in radar imagery,
proving beneficial for accurate building segmentation. Preliminary results
indicate that proposed methodology achieves comparable test accuracy to the
standard Attention U-Net model while significantly reducing network parameters.
This result aligns with findings from previous works, confirming that
Quanvolution not only maintains model accuracy but also increases computational
efficiency. These promising outcomes highlight the potential of
quantum-assisted Deep Learning frameworks for large-scale building segmentation
in urban environments.

</details>


### [38] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faMADI\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u63a9\u7801\u589e\u5f3a\u6269\u6563\u8bad\u7ec3\u7b56\u7565\uff08MAgD\uff09\u548c\u63a8\u7406\u65f6\u6682\u505c\u4ee4\u724c\uff08Pause Tokens\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u8fdb\u884c\u66f4\u7cbe\u7ec6\u3001\u7ed3\u6784\u611f\u77e5\u7684\u751f\u6210\u548c\u7f16\u8f91\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u89c6\u89c9\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u53d7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u5efa\u6a21\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u65e8\u5728\u589e\u5f3a\u6269\u6563\u6a21\u578b\u7684\u7ed3\u6784\u5316\u3001\u53ef\u63a7\u751f\u6210\u80fd\u529b\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86MADI\uff08Masking-Augmented Diffusion with Inference-Time Scaling\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u6838\u5fc3\u521b\u65b0\uff1a1. **\u63a9\u7801\u589e\u5f3a\u9ad8\u65af\u6269\u6563\uff08MAgD\uff09**\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u6807\u51c6\u53bb\u566a\u5206\u6570\u5339\u914d\u548c\u901a\u8fc7\u63a9\u853d\u524d\u5411\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u8f93\u5165\u8fdb\u884c\u7684\u63a9\u7801\u91cd\u5efa\uff0c\u65e8\u5728\u5b66\u4e60\u5224\u522b\u6027\u548c\u7ec4\u5408\u6027\u89c6\u89c9\u8868\u793a\u30022. **\u57fa\u4e8e\u6682\u505c\u4ee4\u724c\uff08Pause Tokens\uff09\u7684\u63a8\u7406\u65f6\u5bb9\u91cf\u6269\u5c55\u673a\u5236**\uff1a\u5c06\u7279\u6b8a\u5360\u4f4d\u7b26\u63d2\u5165\u63d0\u793a\u4e2d\uff0c\u4ee5\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u5bb9\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u65f6\u4f7f\u7528\u8868\u8fbe\u4e30\u5bcc\u548c\u5bc6\u96c6\u7684\u63d0\u793a\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002

Result: MADI\u6846\u67b6\uff08\u7279\u522b\u662fMAgD\u548c\u6682\u505c\u4ee4\u724c\uff09\u663e\u8457\u589e\u5f3a\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\u3001\u7ec4\u5408\u6027\u548c\u53ef\u63a7\u6027\u3002MAgD\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u5c40\u90e8\u5316\u548c\u7ed3\u6784\u611f\u77e5\u7684\u7f16\u8f91\u3002\u8bad\u7ec3\u671f\u95f4\u91c7\u7528\u8868\u8fbe\u4e30\u5bcc\u548c\u5bc6\u96c6\u7684\u63d0\u793a\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002

Conclusion: MADI\u4e2d\u7684\u8d21\u732e\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u4e3a\u5c06\u6269\u6563\u6a21\u578b\u96c6\u6210\u5230\u66f4\u901a\u7528\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u751f\u6210\u6269\u6563\u67b6\u6784\u4e2d\u94fa\u5e73\u4e86\u9053\u8def\u3002

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [39] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u516c\u5171\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u9762\u90e8\u3001\u884c\u4e3a\u548c\u751f\u7406\u6307\u6807\uff0c\u5e76\u8bb0\u5f55\u4e86\u9a7e\u9a76\u5458\u72b6\u6001\u7684\u6e10\u53d8\u8fc7\u7a0b\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u6355\u6349\u5e7f\u6cdb\u7684\u751f\u7406\u3001\u884c\u4e3a\u548c\u9a7e\u9a76\u76f8\u5173\u4fe1\u53f7\uff0c\u7f3a\u4e4f\u8fde\u7eed\u7684\u75b2\u52b3\u6e10\u53d8\u6570\u636e\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u6784\u5efa\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u591a\u6a21\u6001\u7684\u6570\u636e\u96c6\u3002

Method: \u6570\u636e\u96c6\u901a\u8fc7\u6a21\u62df\u9a7e\u9a76\u73af\u5883\u6536\u96c6\uff0c\u5305\u62ec19\u540d\u53d7\u8bd5\u8005\uff0815\u75374\u5973\uff09\u5728\u6e05\u9192\u548c\u75b2\u52b3\u4e24\u79cd\u72b6\u6001\u4e0b\u7684\u6570\u636e\u3002\u91c7\u96c6\u4fe1\u53f7\u5305\u62ec\uff1a3D\u9762\u90e8\u89c6\u9891\u3001\u7ea2\u5916\u6444\u50cf\u5934\u753b\u9762\u3001\u540e\u65b9\u89c6\u9891\u3001\u5fc3\u7387\u3001\u76ae\u7535\u6d3b\u52a8\u3001\u8840\u6c27\u9971\u548c\u5ea6\u3001\u76ae\u80a4\u6e29\u5ea6\u3001\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u3001\u65b9\u5411\u76d8\u63e1\u529b\u4f20\u611f\u5668\u6570\u636e\u548c\u6a21\u62df\u5668\u9065\u6d4b\u6570\u636e\u3002\u75b2\u52b3\u7a0b\u5ea6\u6bcf\u56db\u5206\u949f\u901a\u8fc7KSS\u91cf\u8868\u81ea\u62a5\u3002\u6bcf\u4e2a\u6570\u636e\u91c7\u96c6\u4f1a\u8bdd\u6301\u7eed40\u5206\u949f\uff0c\u603b\u65f6\u957f1400\u5206\u949f\uff0c\u65e8\u5728\u6355\u6349\u9a7e\u9a76\u5458\u72b6\u6001\u7684\u6e10\u53d8\u800c\u975e\u79bb\u6563\u6807\u7b7e\u3002

Result: \u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u591a\u6a21\u6001\u7684\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u603b\u65f6\u957f\u8fbe1400\u5206\u949f\uff0c\u5305\u542b\u591a\u79cd\u751f\u7406\u3001\u884c\u4e3a\u548c\u9a7e\u9a76\u76f8\u5173\u4fe1\u53f7\uff0c\u5e76\u8bb0\u5f55\u4e86\u9a7e\u9a76\u5458\u72b6\u6001\u7684\u6e10\u53d8\u8fc7\u7a0b\u3002\u8be5\u6570\u636e\u96c6\u5c06\u6309\u9700\u63d0\u4f9b\u7ed9\u76f8\u5173\u7814\u7a76\u8005\u3002

Conclusion: \u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u4e14\u5168\u9762\u7684\u591a\u6a21\u6001\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u6709\u671b\u4e3a\u9a7e\u9a76\u5458\u75b2\u52b3\u7814\u7a76\u63d0\u4f9b\u5b9d\u8d35\u8d44\u6e90\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u68c0\u6d4b\u9a7e\u9a76\u5458\u75b2\u52b3\u72b6\u6001\u7684\u8fde\u7eed\u53d8\u5316\u3002

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [40] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u4eceCT/MRI\u56fe\u50cf\u751f\u6210\u5e73\u6ed1\u3001\u7b26\u5408\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u8981\u6c42\u76843D\u4e3b\u52a8\u8109\u8868\u9762\u7f51\u683c\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u624b\u52a8\u5e72\u9884\u7684\u4f9d\u8d56\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u76843D\u4e3b\u52a8\u8109\u6784\u5efa\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5e7f\u6cdb\u7684\u624b\u52a8\u5e72\u9884\uff0c\u5e76\u4e14\u96be\u4ee5\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u3001\u9002\u7528\u4e8eCFD\u5206\u6790\u7684\u8868\u9762\uff0c\u800c\u7cbe\u786e\u76843D\u4e3b\u52a8\u8109\u6a21\u578b\u5bf9\u4e8e\u4e34\u5e8a\u8bca\u65ad\u3001\u672f\u524d\u89c4\u5212\u548cCFD\u6a21\u62df\u81f3\u5173\u91cd\u8981\u3002

Method: AortaDiff\u91c7\u7528\u6269\u6563\u6a21\u578b\u3002\u9996\u5148\uff0c\u4e00\u4e2a\u4f53\u79ef\u5f15\u5bfc\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08CDM\uff09\u4ece\u533b\u5b66\u56fe\u50cf\u4e2d\u8fed\u4ee3\u751f\u6210\u4e3b\u52a8\u8109\u4e2d\u5fc3\u7ebf\u3002\u7136\u540e\uff0c\u6bcf\u4e2a\u4e2d\u5fc3\u7ebf\u70b9\u88ab\u81ea\u52a8\u7528\u4f5c\u63d0\u793a\uff0c\u4ee5\u63d0\u53d6\u76f8\u5e94\u7684\u8840\u7ba1\u8f6e\u5ed3\uff0c\u786e\u4fdd\u51c6\u786e\u7684\u8fb9\u754c\u63cf\u7ed8\u3002\u6700\u540e\uff0c\u5c06\u63d0\u53d6\u7684\u8f6e\u5ed3\u62df\u5408\u4e3a\u5e73\u6ed1\u76843D\u8868\u9762\uff0c\u751f\u6210\u8fde\u7eed\u3001\u4e0eCFD\u517c\u5bb9\u7684\u7f51\u683c\u8868\u793a\u3002

Result: AortaDiff\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5bf9\u5927\u578b\u6807\u8bb0\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u6027\u6700\u5c0f\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684CFD\u517c\u5bb9\u4e3b\u52a8\u8109\u7f51\u683c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\uff0cAortaDiff\u4e5f\u80fd\u6709\u6548\u8fd0\u884c\uff0c\u6210\u529f\u6784\u5efa\u6b63\u5e38\u548c\u75c5\u53d8\uff08\u5982\u52a8\u8109\u7624\u6216\u4e3b\u52a8\u8109\u7f29\u7a84\uff09\u7684\u4e3b\u52a8\u8109\u7f51\u683c\u3002

Conclusion: AortaDiff\u4e3a\u5fc3\u8840\u7ba1\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53ef\u89c6\u5316\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u7cbe\u786e\u3001\u5e73\u6ed1\u76843D\u4e3b\u52a8\u8109\u8868\u9762\uff0c\u652f\u6301CFD\u5206\u6790\uff0c\u5373\u4f7f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8868\u73b0\u51fa\u8272\u3002

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis,
preoperative planning, and computational fluid dynamics (CFD) simulations, as
it enables the estimation of critical hemodynamic parameters such as blood flow
velocity, pressure distribution, and wall shear stress. Existing construction
methods often rely on large annotated training datasets and extensive manual
intervention. While the resulting meshes can serve for visualization purposes,
they struggle to produce geometrically consistent, well-constructed surfaces
suitable for downstream CFD analysis. To address these challenges, we introduce
AortaDiff, a diffusion-based framework that generates smooth aortic surfaces
directly from CT/MRI volumes. AortaDiff first employs a volume-guided
conditional diffusion model (CDM) to iteratively generate aortic centerlines
conditioned on volumetric medical images. Each centerline point is then
automatically used as a prompt to extract the corresponding vessel contour,
ensuring accurate boundary delineation. Finally, the extracted contours are
fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh
representation. AortaDiff offers distinct advantages over existing methods,
including an end-to-end workflow, minimal dependency on large labeled datasets,
and the ability to generate CFD-compatible aorta meshes with high geometric
fidelity. Experimental results demonstrate that AortaDiff performs effectively
even with limited training data, successfully constructing both normal and
pathologically altered aorta meshes, including cases with aneurysms or
coarctation. This capability enables the generation of high-quality
visualizations and positions AortaDiff as a practical solution for
cardiovascular research.

</details>


### [41] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: IConMark\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5\uff0c\u5728AI\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u53ef\u7406\u89e3\u7684\u6982\u5ff5\uff0c\u4ee5\u63d0\u9ad8\u5bf9AI\u751f\u6210\u56fe\u50cf\u7684\u9c81\u68d2\u6027\u8bc6\u522b\u548c\u9632\u7be1\u6539\u80fd\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u968f\u7740\u751f\u6210\u5f0fAI\u548c\u5408\u6210\u5a92\u4f53\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u533a\u5206AI\u751f\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u9632\u6b62\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u5e76\u786e\u4fdd\u6570\u5b57\u771f\u5b9e\u6027\u3002\u4f20\u7edf\u6c34\u5370\u6280\u672f\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u6709\u6548\u6027\u53d7\u635f\u3002

Method: IConMark\u662f\u4e00\u79cd\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u7684\u9c81\u68d2\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5\u3002\u5b83\u4e0d\u4f9d\u8d56\u4e8e\u6dfb\u52a0\u566a\u58f0\u6216\u6270\u52a8\uff0c\u800c\u662f\u5c06\u6709\u610f\u4e49\u7684\u8bed\u4e49\u5c5e\u6027\u878d\u5165AI\u751f\u6210\u56fe\u50cf\u4e2d\uff0c\u4f7f\u5176\u5bf9\u4eba\u7c7b\u53ef\u89e3\u91ca\uff0c\u4ece\u800c\u80fd\u62b5\u6297\u5bf9\u6297\u6027\u64cd\u7eb5\u3002\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u6709\u6c34\u5370\u6280\u672f\uff08\u5982StegaStamp\u548cTrustMark\uff09\u7ed3\u5408\uff0c\u5f62\u6210\u6df7\u5408\u65b9\u6cd5\uff08IConMark+SS\u548cIConMark+TM\uff09\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u9c81\u68d2\u6027\u3002

Result: IConMark\u4e0d\u4ec5\u5bf9\u5404\u79cd\u56fe\u50cf\u589e\u5f3a\u5177\u6709\u9c81\u68d2\u6027\uff0c\u800c\u4e14\u5177\u6709\u4eba\u7c7b\u53ef\u8bfb\u6027\uff0c\u652f\u6301\u624b\u52a8\u9a8c\u8bc1\u6c34\u5370\u3002\u5b83\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002\u4e0e\u6700\u4f73\u57fa\u7ebf\u76f8\u6bd4\uff0cIConMark\u53ca\u5176\u53d8\u4f53\uff08+TM\u548c+SS\uff09\u5728\u6c34\u5370\u68c0\u6d4b\u7684\u5e73\u5747ROC\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUROC\uff09\u5206\u6570\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8610.8%\u300114.5%\u548c15.9%\u3002

Conclusion: IConMark\u4e3a\u53ef\u89e3\u91ca\u7684\u6c34\u5370\u6280\u672f\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u663e\u8457\u63d0\u9ad8\u4e86AI\u751f\u6210\u56fe\u50cf\u6c34\u5370\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u5bf9\u56fe\u50cf\u64cd\u7eb5\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u652f\u6301\u4eba\u5de5\u9a8c\u8bc1\u3002

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [42] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: \u8be5\u8bba\u6587\u63d0\u51fa\u4e86COREVQA\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u62e5\u6324\u56fe\u50cf\u4e0a\u89c6\u89c9\u8574\u6db5\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u662f\u9876\u7ea7VLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e5f\u8fdc\u4f4e\u4e8e80%\u7684\u51c6\u786e\u7387\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u57fa\u51c6\u5f88\u5c11\u6d4b\u8bd5\u6a21\u578b\u51c6\u786e\u5b8c\u6210\u89c6\u89c9\u8574\u6db5\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u6839\u636e\u56fe\u50cf\u63a5\u53d7\u6216\u9a73\u65a5\u4e00\u4e2a\u5047\u8bbe\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u62e5\u6324\u56fe\u50cf\u573a\u666f\u4e2d\u3002

Method: \u7814\u7a76\u8005\u63d0\u51fa\u4e86COREVQA\u57fa\u51c6\uff0c\u5305\u542b5608\u5bf9\u56fe\u50cf\u548c\u5408\u6210\u751f\u6210\u7684\u771f/\u5047\u9648\u8ff0\uff0c\u5176\u4e2d\u56fe\u50cf\u6765\u6e90\u4e8eCrowdHuman\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6fc0\u53d1\u6a21\u578b\u5728\u62e5\u6324\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u8574\u6db5\u63a8\u7406\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u4f73\u7684VLMs\uff0c\u5176\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e80%\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u7684\u8868\u73b0\u5219\u663e\u8457\u66f4\u5dee\uff0839.98%-69.95%\uff09\u3002

Conclusion: \u8fd9\u4e00\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u63ed\u793a\u4e86VLMs\u5728\u5bf9\u62e5\u6324\u573a\u666f\u4e2d\u7279\u5b9a\u7c7b\u578b\u7684\u56fe\u50cf-\u95ee\u9898\u5bf9\u8fdb\u884c\u63a8\u7406\u65f6\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\u3002

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [43] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u578b\u6df1\u5ea6\u5b66\u4e60\u96c6\u6210\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u80a9\u90e8X\u5149\u7247\u4e2d\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u9aa8\u6298\uff0c\u65e8\u5728\u89e3\u51b3\u9aa8\u6298\u6f0f\u8bca\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u80a9\u90e8\u9aa8\u6298\u5728\u6025\u8bca\u548c\u9ad8\u6d41\u91cf\u4e34\u5e8a\u73af\u5883\u4e2d\u5e38\u88ab\u6f0f\u8bca\uff0c\u653e\u5c04\u79d1\u533b\u751f\u6f0f\u8bca\u7387\u53ef\u8fbe10%\u3002AI\u5de5\u5177\u80fd\u6709\u6548\u8f85\u52a9\u65e9\u671f\u68c0\u6d4b\u5e76\u51cf\u5c11\u8bca\u65ad\u5ef6\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u80a9\u90e8X\u5149\u7247\u7684AI\u7cfb\u7edf\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002

Method: \u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u578b\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e8610,000\u5f20\u6807\u6ce8\u7684\u80a9\u90e8X\u5149\u7247\u8fdb\u884c\u8bad\u7ec3\u3002\u91c7\u7528\u7684\u67b6\u6784\u5305\u62ecFaster R-CNN\uff08ResNet50-FPN, ResNeXt\uff09\u3001EfficientDet\u548cRF-DETR\u3002\u4e3a\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\uff0c\u5e94\u7528\u4e86\u8fb9\u754c\u6846\u548c\u5206\u7c7b\u7ea7\u522b\u7684\u96c6\u6210\u6280\u672f\uff0c\u5982Soft-NMS\u3001WBF\u548cNMW\u878d\u5408\u3002

Result: NMW\u96c6\u6210\u65b9\u6cd5\u53d6\u5f97\u4e8695.5%\u7684\u51c6\u786e\u7387\u548c0.9610\u7684F1\u5206\u6570\uff0c\u5728\u6240\u6709\u5173\u952e\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002\u5b83\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u53ec\u56de\u7387\u548c\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u80a9\u90e8X\u5149\u7247\u4e2d\u4e34\u5e8a\u9aa8\u6298\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002

Conclusion: \u57fa\u4e8e\u96c6\u6210\u7684AI\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u5730\u5728X\u5149\u7247\u4e2d\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u80a9\u90e8\u9aa8\u6298\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u3002\u8be5\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u90e8\u7f72\u5c31\u7eea\u6027\u4f7f\u5176\u975e\u5e38\u9002\u5408\u96c6\u6210\u5230\u5b9e\u65f6\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u7528\u4e8e\u5feb\u901f\u7b5b\u67e5\u548c\u5206\u8bca\u652f\u6301\uff0c\u4f46\u76ee\u524d\u4ec5\u9650\u4e8e\u4e8c\u5143\u9aa8\u6298\u68c0\u6d4b\u3002

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [44] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: \u901a\u8fc7\u4f7f\u7528CORONA\u5386\u53f2\u536b\u661f\u56fe\u50cf\u91cd\u65b0\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7814\u7a76\u4eba\u5458\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5df2\u5b8c\u5168\u6539\u53d8\u7684\u666f\u89c2\u4e2d\u81ea\u52a8\u8bc6\u522b\u8003\u53e4\u9057\u5740\u7684\u80fd\u529b\uff0c\u751a\u81f3\u53d1\u73b0\u4e86\u4f20\u7edf\u65b9\u6cd5\u672a\u66fe\u53d1\u73b0\u7684\u65b0\u9057\u5740\u3002


<details>
  <summary>Details</summary>
Motivation: \u7531\u4e8e\u8fd1\u4e94\u5341\u5e74\u5730\u8c8c\u7684\u5f7b\u5e95\u6539\u53d8\uff0c\u5305\u62ec\u8bb8\u591a\u8003\u53e4\u9057\u5740\u7684\u5b8c\u5168\u6bc1\u574f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u8fd9\u4e9b\u6d88\u5931\u7684\u8003\u53e4\u9057\u8ff9\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528AI\u6a21\u578b\u7ed3\u5408\u5386\u53f2\u536b\u661f\u6570\u636e\uff0c\u89e3\u51b3\u5728\u5267\u70c8\u53d8\u5316\u73af\u5883\u4e2d\u8003\u53e4\u9057\u5740\u81ea\u52a8\u8bc6\u522b\u7684\u6311\u6218\u3002

Method: \u7814\u7a76\u4eba\u5458\u4f7f\u7528CORONA\u536b\u661f\u56fe\u50cf\u5bf9\u4e00\u4e2a\u73b0\u6709\u7684\u57fa\u4e8eBing\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u4e86\u518d\u8bad\u7ec3\uff0c\u7814\u7a76\u533a\u57df\u4e3a\u5df4\u683c\u8fbe\u4ee5\u897f\u7684\u963f\u5e03\u683c\u83b1\u5e03\u533a\uff0c\u4f4d\u4e8e\u7f8e\u7d22\u4e0d\u8fbe\u7c73\u4e9a\u4e2d\u90e8\u6d2a\u6cdb\u5e73\u539f\u3002

Result: \u7ed3\u679c\u4ee4\u4eba\u60ca\u559c\uff1a\u9996\u5148\uff0c\u6a21\u578b\u5728\u76ee\u6807\u533a\u57df\u7684\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\uff0c\u56fe\u50cf\u5206\u5272\u7ea7\u522b\u7684IoU\u503c\u8d85\u8fc785%\uff0c\u8003\u53e4\u9057\u5740\u68c0\u6d4b\u7684\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523090%\u3002\u5176\u6b21\uff0c\u518d\u8bad\u7ec3\u7684\u6a21\u578b\u6210\u529f\u8bc6\u522b\u51fa\u56db\u4e2a\u65b0\u7684\u8003\u53e4\u9057\u5740\uff08\u7ecf\u5b9e\u5730\u9a8c\u8bc1\uff09\uff0c\u8fd9\u4e9b\u9057\u5740\u662f\u4f20\u7edf\u6280\u672f\u6b64\u524d\u672a\u80fd\u53d1\u73b0\u7684\u3002

Conclusion: \u7814\u7a76\u8bc1\u5b9e\u4e86\u4f7f\u7528AI\u6280\u672f\u7ed3\u540820\u4e16\u7eaa60\u5e74\u4ee3\u7684CORONA\u56fe\u50cf\uff0c\u5bf9\u4e8e\u53d1\u73b0\u76ee\u524d\u5df2\u4e0d\u53ef\u89c1\u7684\u8003\u53e4\u9057\u5740\u7684\u6709\u6548\u6027\u3002\u8fd9\u662f\u4e00\u4e2a\u7a81\u7834\u6027\u7684\u8fdb\u5c55\uff0c\u5bf9\u4e8e\u7814\u7a76\u53d7\u4eba\u7c7b\u6d3b\u52a8\u5f71\u54cd\u800c\u8003\u53e4\u8bc1\u636e\u9010\u6e10\u6d88\u5931\u7684\u666f\u89c2\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [45] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faCaSTFormer\uff0c\u4e00\u79cd\u56e0\u679c\u65f6\u7a7aTransformer\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u9a7e\u9a76\u610f\u56fe\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9a7e\u9a76\u5458\u884c\u4e3a\u4e0e\u73af\u5883\u80cc\u666f\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u548c\u884c\u4e3a\u53d8\u5f02\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u524d\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\u5728\u51c6\u786e\u5efa\u6a21\u590d\u6742\u7684\u65f6\u7a7a\u76f8\u4e92\u4f9d\u8d56\u6027\u548c\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u53d8\u5f02\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u51c6\u786e\u7684\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u662f\u63d0\u5347\u4eba\u673a\u5171\u9a7e\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u4ea4\u4e92\u6548\u7387\u7684\u5173\u952e\uff0c\u4e5f\u662f\u5b9e\u73b0\u9ad8\u7ea7\u522b\u81ea\u52a8\u9a7e\u9a76\u7684\u57fa\u7840\u3002

Method: \u672c\u6587\u63d0\u51faCaSTFormer\uff0c\u4e00\u4e2a\u56e0\u679c\u65f6\u7a7aTransformer\u3002\u5b83\u5305\u542b\uff1a1) \u4e92\u60e0\u504f\u79fb\u878d\u5408\uff08RSF\uff09\u673a\u5236\uff0c\u7528\u4e8e\u7cbe\u786e\u5bf9\u9f50\u5185\u90e8\u548c\u5916\u90e8\u7279\u5f81\u6d41\uff1b2) \u56e0\u679c\u6a21\u5f0f\u63d0\u53d6\uff08CPE\uff09\u6a21\u5757\uff0c\u7cfb\u7edf\u5730\u6d88\u9664\u865a\u5047\u5173\u8054\u4ee5\u63ed\u793a\u771f\u5b9e\u7684\u56e0\u679c\u4f9d\u8d56\uff1b3) \u7279\u5f81\u5408\u6210\u7f51\u7edc\uff08FSN\uff09\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u51c0\u5316\u540e\u7684\u8868\u793a\u5408\u6210\u4e3a\u8fde\u8d2f\u7684\u65f6\u7a7a\u63a8\u65ad\u3002

Result: CaSTFormer\u5728\u516c\u5f00\u7684Brain4Cars\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b83\u6709\u6548\u6355\u6349\u4e86\u590d\u6742\u7684\u56e0\u679c\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002

Conclusion: CaSTFormer\u901a\u8fc7\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u56e0\u679c\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [46] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86PhyWorldBench\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u65e8\u5728\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u7269\u7406\u5b9a\u5f8b\u7684\u9075\u5b88\u7a0b\u5ea6\uff0c\u5e76\u8bc6\u522b\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u6a21\u62df\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u521b\u5efa\u9ad8\u8d28\u91cf\u3001\u903c\u771f\u7684\u5185\u5bb9\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u51c6\u786e\u6a21\u62df\u7269\u7406\u73b0\u8c61\u7684\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002

Method: \u672c\u6587\u5f15\u5165\u4e86PhyWorldBench\u57fa\u51c6\uff0c\u6db5\u76d6\u4ece\u57fa\u672c\u539f\u7406\uff08\u5982\u7269\u4f53\u8fd0\u52a8\u3001\u80fd\u91cf\u5b88\u6052\uff09\u5230\u590d\u6742\u573a\u666f\uff08\u5982\u521a\u4f53\u4ea4\u4e92\u3001\u4eba\u6216\u52a8\u7269\u8fd0\u52a8\uff09\u7684\u591a\u4e2a\u7269\u7406\u73b0\u8c61\u5c42\u9762\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u201c\u53cd\u7269\u7406\u201d\u7c7b\u522b\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u8fdd\u53cd\u7269\u7406\u5b9a\u5f8b\u6307\u4ee4\u4e0b\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\u5927\u89c4\u6a21\u4eba\u5de5\u8bc4\u4f30\u548c\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u96f6\u6837\u672c\u7269\u7406\u771f\u5b9e\u6027\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5\u3002\u7814\u7a76\u8bc4\u4f30\u4e8612\u4e2a\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff09\uff0c\u4f7f\u7528\u4e861050\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u63d0\u793a\uff08\u5305\u62ec\u57fa\u7840\u3001\u590d\u5408\u548c\u53cd\u7269\u7406\u573a\u666f\uff09\u3002

Result: \u7814\u7a76\u8bc6\u522b\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u9075\u5b88\u771f\u5b9e\u4e16\u754c\u7269\u7406\u65b9\u9762\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002\u901a\u8fc7\u7cfb\u7edf\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u4e0b\u6a21\u578b\u5728\u5404\u79cd\u7269\u7406\u73b0\u8c61\u4e0a\u7684\u8868\u73b0\uff0c\u5f97\u51fa\u4e86\u5173\u4e8e\u5982\u4f55\u5236\u4f5c\u80fd\u589e\u5f3a\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u63d0\u793a\u7684\u9488\u5bf9\u6027\u5efa\u8bae\u3002

Conclusion: PhyWorldBench\u57fa\u51c6\u548c\u8be6\u7ec6\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u6a21\u62df\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u6539\u8fdb\u6a21\u578b\u5728\u7269\u7406\u771f\u5b9e\u6027\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u5efa\u8bae\u3002

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [47] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u6570\u636e\u96c6EvReID\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aTriPro-ReID\u7684\u884c\u4eba\u5c5e\u6027\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u4e8b\u4ef6\u76f8\u673aReID\u6570\u636e\u7a00\u7f3a\u548c\u6027\u80fd\u8bc4\u4f30\u56f0\u96be\u7684\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u7b97\u6cd5\u4e3b\u8981\u5728\u5c0f\u89c4\u6a21\u6216\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u5176\u5b9e\u9645\u8bc6\u522b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u6570\u636e\u7a00\u7f3a\u6027\u3002

Method: 1. \u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6EvReID\uff0c\u5305\u542b118,988\u5bf9\u56fe\u50cf\u548c1200\u4e2a\u884c\u4eba\u8eab\u4efd\uff0c\u6570\u636e\u91c7\u96c6\u6db5\u76d6\u591a\u5b63\u8282\u3001\u573a\u666f\u548c\u5149\u7167\u6761\u4ef6\u30022. \u5728EvReID\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e8615\u79cd\u6700\u5148\u8fdb\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u7b97\u6cd5\uff0c\u5efa\u7acb\u4e86\u57fa\u51c6\u30023. \u63d0\u51fa\u4e86\u4e00\u4e2a\u884c\u4eba\u5c5e\u6027\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6TriPro-ReID\uff0c\u8be5\u6846\u67b6\u6709\u6548\u878d\u5408\u4e86RGB\u5e27\u548c\u4e8b\u4ef6\u6d41\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5145\u5206\u5229\u7528\u884c\u4eba\u5c5e\u6027\u4f5c\u4e3a\u4e2d\u5c42\u8bed\u4e49\u7279\u5f81\u3002

Result: 1. \u6210\u529f\u6784\u5efa\u5e76\u53d1\u5e03\u4e86EvReID\u5927\u89c4\u6a21RGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6\u30022. \u5bf915\u79cd\u6700\u5148\u8fdb\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u30023. \u63d0\u51fa\u7684TriPro-ReID\u6846\u67b6\u5728EvReID\u548cMARS\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63a2\u7d22RGB\u5e27\u548c\u4e8b\u4ef6\u6d41\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5145\u5206\u5229\u7528\u884c\u4eba\u5c5e\u6027\u3002

Conclusion: \u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u5c5e\u6027\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u4e3aRGB-\u4e8b\u4ef6\u884c\u4eba\u91cd\u8bc6\u522b\u9886\u57df\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u6570\u636e\u548c\u57fa\u51c6\u57fa\u7840\uff0c\u5e76\u6709\u6548\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\u3002

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [48] [Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation](https://arxiv.org/abs/2507.13486)
*Debao Huang,Rongjun Qin*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5149\u5ea6\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u70b9\u5173\u8054\u8bef\u5dee\u534f\u65b9\u5dee\u77e9\u9635\u6765\u89e3\u51b3SfM\u548cMVS\u4e24\u9636\u6bb5\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9MVS\u9636\u6bb5\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u81ea\u6821\u51c6\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u4e0e\u673a\u8f7dLiDAR\u4e0d\u540c\uff0c\u5149\u5ea6\u6d4b\u91cf\u70b9\u4e91\u7684\u7cbe\u5ea6\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u573a\u666f\uff0c\u4e14\u5176\u8bef\u5dee\u901a\u8fc7SfM\u548cMVS\u4e24\u9636\u6bb5\u4f20\u64ad\u3002SfM\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5df2\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4f46MVS\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7531\u4e8e\u5176\u975e\u53ef\u5fae\u548c\u591a\u6a21\u6001\u7279\u6027\uff0c\u4ecd\u672a\u5f97\u5230\u89e3\u51b3\u548c\u6807\u51c6\u5316\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u70b9\u5173\u8054\u4e00\u4e2a\u8bef\u5dee\u534f\u65b9\u5dee\u77e9\u9635\u6765\u89e3\u91ca\u5149\u5ea6\u6d4b\u91cf\u4e24\u9636\u6bb5\uff08SfM\u548cMVS\uff09\u7684\u8bef\u5dee\u4f20\u64ad\u3002\u7279\u522b\u5730\uff0c\u4e3a\u4e86\u4f30\u8ba1MVS\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u6821\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u6bcf\u4e2a\u89c6\u56fe\u4e2d\u53ef\u9760\u7684n\u89c6\u56fe\u70b9\uff08n>=6\uff09\u548cMVS\u9636\u6bb5\u7684\u76f8\u5173\u7ebf\u7d22\uff08\u5982\u5339\u914d\u6210\u672c\u503c\uff09\u6765\u56de\u5f52\u89c6\u5dee\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u662f\u81ea\u76d1\u7763\u7684\uff0c\u5e76\u5229\u7528\u76f4\u63a5\u4eceMVS\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u7684\u81ea\u5305\u542b\u53ef\u97603D\u70b9\u3002

Result: \u8be5\u6846\u67b6\u5728\u5404\u79cd\u516c\u5f00\u7684\u673a\u8f7d\u548c\u65e0\u4eba\u673a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u4e0d\u5938\u5927\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fb9\u754c\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002

Conclusion: \u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u6821\u51c6\u65b9\u6cd5\u6765\u91cf\u5316MVS\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u5149\u5ea6\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9886\u57df\u7684\u7a7a\u767d\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u8ba4\u8bc1\u7684\u3001\u9002\u7528\u4e8e\u5404\u79cd\u573a\u666f\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6848\u3002

Abstract: Uncertainty quantification of the photogrammetry process is essential for
providing per-point accuracy credentials of the point clouds. Unlike airborne
LiDAR, which typically delivers consistent accuracy across various scenes, the
accuracy of photogrammetric point clouds is highly scene-dependent, since it
relies on algorithm-generated measurements (i.e., stereo or multi-view stereo).
Generally, errors of the photogrammetric point clouds propagate through a
two-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),
followed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM
stage has been well studied using the first-order statistics of the
reprojection error function, that in the MVS stage remains largely unsolved and
non-standardized, primarily due to its non-differentiable and multi-modal
nature (i.e., from pixel values to geometry). In this paper, we present an
uncertainty quantification framework closing this gap by associating an error
covariance matrix per point accounting for this two-step photogrammetry
process. Specifically, to estimate the uncertainty in the MVS stage, we propose
a novel, self-calibrating method by taking reliable n-view points (n>=6)
per-view to regress the disparity uncertainty using highly relevant cues (such
as matching cost values) from the MVS stage. Compared to existing approaches,
our method uses self-contained, reliable 3D points extracted directly from the
MVS process, with the benefit of being self-supervised and naturally adhering
to error propagation path of the photogrammetry process, thereby providing a
robust and certifiable uncertainty quantification across diverse scenes. We
evaluate the framework using a variety of publicly available airborne and UAV
imagery datasets. Results demonstrate that our method outperforms existing
approaches by achieving high bounding rates without overestimating uncertainty.

</details>


### [49] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse\u662f\u4e00\u4e2a\u7edf\u4e00\u7684V2X\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u4e13\u4e3a\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u5c42\u878d\u5408\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u5206\u8fa8\u7387\u8c03\u6574\uff0c\u5728\u591a\u79cd\u4f20\u611f\u5668\u90e8\u7f72\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u5b9e\u4e16\u754c\u4e2d\u7684V2X\u534f\u4f5c\u611f\u77e5\u7cfb\u7edf\u5e38\u56e0\u6210\u672c\u548c\u90e8\u7f72\u5dee\u5f02\u800c\u9762\u4e34\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u95ee\u9898\uff0c\u8fd9\u7ed9\u7279\u5f81\u878d\u5408\u548c\u611f\u77e5\u53ef\u9760\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86HeCoFuse\u6846\u67b6\uff0c\u91c7\u7528\u4ee5\u4e0b\u65b9\u6cd5\uff1a1) \u5f15\u5165\u5206\u5c42\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u5730\u52a0\u6743\u7279\u5f81\uff0c\u89e3\u51b3\u8de8\u6a21\u6001\u7279\u5f81\u4e0d\u5bf9\u9f50\u548c\u8868\u793a\u8d28\u91cf\u4e0d\u5e73\u8861\u95ee\u9898\uff1b2) \u91c7\u7528\u81ea\u9002\u5e94\u7a7a\u95f4\u5206\u8fa8\u7387\u8c03\u6574\u6a21\u5757\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u878d\u5408\u6548\u679c\uff1b3) \u5b9e\u73b0\u534f\u4f5c\u5b66\u4e60\u7b56\u7565\uff0c\u6839\u636e\u53ef\u7528\u6a21\u6001\u52a8\u6001\u8c03\u6574\u878d\u5408\u7c7b\u578b\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u3002

Result: \u5728TUMTraf-V2X\u6570\u636e\u96c6\u4e0a\uff0cHeCoFuse\u5728\u5b8c\u6574\u4f20\u611f\u5668\u914d\u7f6e\uff08LC+LC\uff09\u4e0b\u5b9e\u73b0\u4e8643.22%\u76843D mAP\uff0c\u6bd4CoopDet3D\u57fa\u7ebf\u9ad8\u51fa1.17%\uff1b\u5728L+LC\u573a\u666f\u4e2d\u8fbe\u5230\u66f4\u9ad8\u768443.38% 3D mAP\uff1b\u5e76\u5728\u4e5d\u79cd\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u4fdd\u630121.74%\u81f343.38%\u76843D mAP\u3002\u8fd9\u4e9b\u7ed3\u679c\u901a\u8fc7CVPR 2025 DriveX\u6311\u6218\u8d5b\u7684\u7b2c\u4e00\u540d\u5f97\u5230\u9a8c\u8bc1\u3002

Conclusion: HeCoFuse\u5728TUM-Traf V2X\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u4f20\u611f\u5668\u90e8\u7f72\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784V2X\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u6311\u6218\u3002

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [50] [Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2507.13857)
*Max van den Hoven,Kishaan Jeeveswaran,Pieter Piscaer,Thijs Wensveen,Elahe Arani,Bahram Zonooz*

Main category: cs.CV

TL;DR: Depth3DLane\u662f\u4e00\u79cd\u65b0\u7684\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u76f8\u673a\u53c2\u6570\u9884\u6d4b\uff0c\u65e0\u9700\u6602\u8d35\u4f20\u611f\u5668\u6216\u9884\u5148\u6807\u5b9a\uff0c\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u76843D\u8f66\u9053\u7ebf\u68c0\u6d4b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u9762\u4e34\u7f3a\u4e4f\u660e\u786e\u7a7a\u95f4\u4fe1\u606f\u3001\u4f9d\u8d56\u6602\u8d35\u6df1\u5ea6\u4f20\u611f\u5668\u6216\u96be\u4ee5\u83b7\u53d6\u7684\u6df1\u5ea6\u771f\u503c\u6570\u636e\u3001\u4ee5\u53ca\u5047\u8bbe\u76f8\u673a\u53c2\u6570\u5df2\u77e5\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u4f17\u5305\u9ad8\u7cbe\u5730\u56fe\u7b49\u573a\u666f\u7684\u5e94\u7528\u3002

Method: Depth3DLane\u662f\u4e00\u4e2a\u53cc\u8def\u5f84\u6846\u67b6\uff1a\u4e00\u6761\u8def\u5f84\u901a\u8fc7\u81ea\u76d1\u7763\u6df1\u5ea6\u7f51\u7edc\u83b7\u53d6\u70b9\u4e91\u8868\u793a\uff08\u9e1f\u77b0\u56fe\u8def\u5f84\uff09\u63d0\u53d6\u660e\u786e\u7a7a\u95f4\u4fe1\u606f\uff1b\u53e6\u4e00\u6761\u8def\u5f84\uff08\u524d\u89c6\u56fe\u8def\u5f84\uff09\u540c\u65f6\u63d0\u53d6\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u5b83\u4f7f\u75283D\u8f66\u9053\u951a\u70b9\u4ece\u4e24\u6761\u8def\u5f84\u91c7\u6837\u7279\u5f81\u4ee5\u63a8\u65ad3D\u8f66\u9053\u51e0\u4f55\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u6269\u5c55\u5230\u9010\u5e27\u9884\u6d4b\u76f8\u673a\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u7406\u8bba\u9a71\u52a8\u7684\u62df\u5408\u8fc7\u7a0b\u4ee5\u589e\u5f3a\u9010\u6bb5\u7684\u7a33\u5b9a\u6027\u3002

Result: Depth3DLane\u5728OpenLane\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u53c2\u6570\u800c\u975e\u771f\u503c\u53c2\u6570\uff0c\u4f7f\u5f97Depth3DLane\u80fd\u591f\u5e94\u7528\u4e8e\u76f8\u673a\u6807\u5b9a\u4e0d\u53ef\u884c\u7684\u573a\u666f\uff0c\u8fd9\u662f\u5148\u524d\u65b9\u6cd5\u65e0\u6cd5\u505a\u5230\u7684\u3002

Conclusion: Depth3DLane\u901a\u8fc7\u96c6\u6210\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u76f8\u673a\u53c2\u6570\u9884\u6d4b\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u65e0\u9700\u6602\u8d35\u4f20\u611f\u5668\u548c\u76f8\u673a\u9884\u6807\u5b9a\u60c5\u51b5\u4e0b\u7684\u9c81\u68d23D\u8f66\u9053\u7ebf\u68c0\u6d4b\u3002

Abstract: Monocular 3D lane detection is essential for autonomous driving, but
challenging due to the inherent lack of explicit spatial information.
Multi-modal approaches rely on expensive depth sensors, while methods
incorporating fully-supervised depth networks rely on ground-truth depth data
that is impractical to collect at scale. Additionally, existing methods assume
that camera parameters are available, limiting their applicability in scenarios
like crowdsourced high-definition (HD) lane mapping. To address these
limitations, we propose Depth3DLane, a novel dual-pathway framework that
integrates self-supervised monocular depth estimation to provide explicit
structural information, without the need for expensive sensors or additional
ground-truth depth data. Leveraging a self-supervised depth network to obtain a
point cloud representation of the scene, our bird's-eye view pathway extracts
explicit spatial information, while our front view pathway simultaneously
extracts rich semantic information. Depth3DLane then uses 3D lane anchors to
sample features from both pathways and infer accurate 3D lane geometry.
Furthermore, we extend the framework to predict camera parameters on a
per-frame basis and introduce a theoretically motivated fitting procedure to
enhance stability on a per-segment basis. Extensive experiments demonstrate
that Depth3DLane achieves competitive performance on the OpenLane benchmark
dataset. Furthermore, experimental results show that using learned parameters
instead of ground-truth parameters allows Depth3DLane to be applied in
scenarios where camera calibration is infeasible, unlike previous methods.

</details>


### [51] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548cSentinel-2\u536b\u661f\u5f71\u50cf\u65f6\u95f4\u5e8f\u5217\u7684\u5168\u975e\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u751c\u83dc\u7530\u7684\u80c1\u8feb\u68c0\u6d4b\u3002


<details>
  <summary>Details</summary>
Motivation: \u536b\u661f\u5f71\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\u6570\u636e\u56e0\u5176\u4e30\u5bcc\u7684\u5149\u8c31\u548c\u65f6\u95f4\u7279\u6027\uff0c\u5728\u519c\u4e1a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528SITS\u6570\u636e\uff0c\u4ee5\u975e\u76d1\u7763\u65b9\u5f0f\u89e3\u51b3\u751c\u83dc\u7530\u7684\u80c1\u8feb\u68c0\u6d4b\u95ee\u9898\u3002

Method: \u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a3D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u6a21\u578b\uff0c\u7528\u4e8e\u4eceSentinel-2\u5f71\u50cf\u5e8f\u5217\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002\u6a21\u578b\u7ed3\u5408\u4e86\u7279\u5b9a\u91c7\u96c6\u65e5\u671f\u7684\u65f6\u5e8f\u7f16\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u751c\u83dc\u7684\u751f\u957f\u52a8\u6001\u3002\u5b66\u4e60\u5230\u7684\u8868\u5f81\u968f\u540e\u7528\u4e8e\u4e0b\u6e38\u805a\u7c7b\u4efb\u52a1\uff0c\u4ee5\u533a\u5206\u53d7\u80c1\u8feb\u548c\u5065\u5eb7\u7684\u7530\u5730\u3002

Result: \u901a\u8fc7\u805a\u7c7b\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u53d7\u80c1\u8feb\u7684\u7530\u5730\u4e0e\u5065\u5eb7\u7684\u7530\u5730\u5206\u79bb\u5f00\u6765\u3002\u6240\u5f00\u53d1\u7684\u80c1\u8feb\u68c0\u6d4b\u7cfb\u7edf\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540c\u5e74\u4efd\u7684\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u751c\u83dc\u80c1\u8feb\u68c0\u6d4b\u5de5\u5177\u3002

Conclusion: \u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u975e\u76d1\u77633D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u751c\u83dc\u80c1\u8feb\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u536b\u661f\u5f71\u50cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u53d7\u80c1\u8feb\u548c\u5065\u5eb7\u7684\u7530\u5730\uff0c\u5e76\u5177\u6709\u8de8\u5e74\u4efd\u5e94\u7528\u7684\u5b9e\u7528\u6027\u3002

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [52] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faDiffusion-FSCIL\uff0c\u4e00\u79cd\u5229\u7528\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u63d0\u53d6\u591a\u5c3a\u5ea6\u6269\u6563\u7279\u5f81\u5e76\u7ed3\u5408\u7279\u5f81\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u65b0\u7c7b\u9002\u5e94\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u8bad\u7ec3\u6570\u636e\u6781\u5ea6\u6709\u9650\u3001\u9700\u8981\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u5e76\u6709\u6548\u5b66\u4e60\u65b0\u4fe1\u606f\u3002

Method: \u4f5c\u8005\u63d0\u51faDiffusion-FSCIL\uff0c\u4f7f\u7528\u4e00\u4e2a\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u4ee5\u4e0b\u80fd\u529b\uff1a1) \u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5e26\u6765\u7684\u751f\u6210\u80fd\u529b\uff1b2) \u591a\u5c3a\u5ea6\u8868\u793a\uff1b3) \u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u5b9e\u73b0\u7684\u8868\u793a\u7075\u6d3b\u6027\u3002\u4e3a\u6700\u5927\u5316\u8868\u793a\u80fd\u529b\uff0c\u63d0\u51fa\u63d0\u53d6\u591a\u4e2a\u4e92\u8865\u7684\u6269\u6563\u7279\u5f81\u4f5c\u4e3a\u6f5c\u5728\u56de\u653e\uff08latent replay\uff09\uff0c\u5e76\u8f85\u4ee5\u5c11\u91cf\u7279\u5f81\u84b8\u998f\u4ee5\u9632\u6b62\u751f\u6210\u504f\u5dee\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u51bb\u7ed3\u9aa8\u5e72\u3001\u6700\u5c0f\u53ef\u8bad\u7ec3\u7ec4\u4ef6\u548c\u6279\u91cf\u5904\u7406\u591a\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u9ad8\u6548\u7387\u3002

Result: \u5728CUB-200\u3001miniImageNet\u548cCIFAR-100\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDiffusion-FSCIL\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u7559\u5148\u524d\u5b66\u4e60\u7c7b\u522b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6709\u6548\u9002\u5e94\u4e86\u65b0\u7c7b\u522b\u3002

Conclusion: Diffusion-FSCIL\u901a\u8fc7\u6709\u6548\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u6210\u529f\u5e94\u5bf9\u4e86\u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5728\u4fdd\u6301\u65e7\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u5b66\u4e60\u65b0\u7c7b\u3002

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [53] [SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM](https://arxiv.org/abs/2507.13527)
*Levi Harris,Md Jayed Hossain,Mufan Qiu,Ruichen Zhang,Pingchuan Ma,Tianlong Chen,Jiaqi Gu,Seth Ariel Tongay,Umberto Celano*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u5f15\u5165\u4e86SparseC-AFM\uff0c\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u4ece\u7a00\u758f\u7684\u5bfc\u7535\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08C-AFM\uff09\u626b\u63cf\u6570\u636e\u4e2d\u5feb\u901f\u51c6\u786e\u5730\u91cd\u5efa\u4e8c\u7ef4\u6750\u6599\uff08\u5982MoS2\uff09\u7684\u7535\u5bfc\u7387\u56fe\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u6570\u636e\u91c7\u96c6\u65f6\u95f4\uff08\u8d85\u8fc711\u500d\uff09\u5e76\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u7eb3\u7c73\u7535\u5b50\u5b66\u4e2d\u4e8c\u7ef4\u6750\u6599\u7684\u65e5\u76ca\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u7535\u6c14\u8868\u5f81\u63d0\u51fa\u4e86\u5feb\u901f\u3001\u9c81\u68d2\u7684\u8ba1\u91cf\u6280\u672f\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u4e2d\u3002\u4f20\u7edf\u7684C-AFM\u6280\u672f\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u7531\u4e8e\u5149\u6805\u626b\u63cf\u8fc7\u7a0b\u5bfc\u81f4\u6570\u636e\u91c7\u96c6\u901f\u5ea6\u6162\u3002

Method: \u7814\u7a76\u8005\u5f00\u53d1\u4e86SparseC-AFM\uff0c\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u7684C-AFM\u626b\u63cf\u6570\u636e\u4e2d\u5feb\u901f\u51c6\u786e\u5730\u91cd\u5efa\u4e8c\u7ef4\u6750\u6599\u7684\u7535\u5bfc\u7387\u56fe\u3002\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u626b\u63cf\u6a21\u5f0f\u3001\u57fa\u5e95\u548c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u9ad8\u50cf\u7d20\u5bc6\u5ea6C-AFM\u56fe\u50cf\u91c7\u96c6\u548c\u624b\u52a8\u53c2\u6570\u63d0\u53d6\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002

Result: SparseC-AFM\u5c06\u6570\u636e\u91c7\u96c6\u65f6\u95f4\u7f29\u77ed\u4e86\u8d85\u8fc711\u500d\uff08\u4f8b\u5982\uff0c\u4ece15\u5206\u949f\u964d\u81f35\u5206\u949f\u4ee5\u4e0b\uff09\uff0c\u540c\u65f6\u80fd\u9ad8\u6548\u63d0\u53d6MoS2\u7684\u5173\u952e\u6750\u6599\u53c2\u6570\uff0c\u5982\u8584\u819c\u8986\u76d6\u7387\u3001\u7f3a\u9677\u5bc6\u5ea6\u4ee5\u53ca\u6676\u5c9b\u8fb9\u754c\u3001\u8fb9\u7f18\u548c\u88c2\u7eb9\u7684\u8bc6\u522b\u3002\u6a21\u578b\u9884\u6d4b\u7684\u6837\u672c\u8868\u73b0\u51fa\u4e0e\u5168\u5206\u8fa8\u7387\u6570\u636e\u9ad8\u5ea6\u76f8\u4f3c\u7684\u7535\u5b66\u7279\u6027\u3002

Conclusion: \u8fd9\u9879\u5de5\u4f5c\u662fAI\u8f85\u52a9\u4e8c\u7ef4\u6750\u6599\u8868\u5f81\u4ece\u5b9e\u9a8c\u5ba4\u7814\u7a76\u5411\u5de5\u4e1a\u5236\u9020\u8f6c\u5316\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u901a\u8fc7\u663e\u8457\u63d0\u9ad8\u901f\u5ea6\u548c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u751f\u4ea7\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002

Abstract: The increasing use of two-dimensional (2D) materials in nanoelectronics
demands robust metrology techniques for electrical characterization, especially
for large-scale production. While atomic force microscopy (AFM) techniques like
conductive AFM (C-AFM) offer high accuracy, they suffer from slow data
acquisition speeds due to the raster scanning process. To address this, we
introduce SparseC-AFM, a deep learning model that rapidly and accurately
reconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM
scans. Our approach is robust across various scanning modes, substrates, and
experimental conditions. We report a comparison between (a) classic flow
implementation, where a high pixel density C-AFM image (e.g., 15 minutes to
collect) is manually parsed to extract relevant material parameters, and (b)
our SparseC-AFM method, which achieves the same operation using data that
requires substantially less acquisition time (e.g., under 5 minutes).
SparseC-AFM enables efficient extraction of critical material parameters in
MoS$_2$, including film coverage, defect density, and identification of
crystalline island boundaries, edges, and cracks. We achieve over 11x reduction
in acquisition time compared to manual extraction from a full-resolution C-AFM
image. Moreover, we demonstrate that our model-predicted samples exhibit
remarkably similar electrical properties to full-resolution data gathered using
classic-flow scanning. This work represents a significant step toward
translating AI-assisted 2D material characterization from laboratory research
to industrial fabrication. Code and model weights are available at
github.com/UNITES-Lab/sparse-cafm.

</details>


### [54] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9891\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\u53ca\u5176\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\uff0c\u4ee5\u63d0\u5347\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u91cd\u5efa\u4e2d\u9ad8\u9891\u7ec6\u8282\u7684\u6062\u590d\u80fd\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349HSI\u7684\u9ad8\u9891\u7ec6\u8282\uff0c\u5bfc\u81f4\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\u3002

Method: \u672c\u6587\u63d0\u51fa\uff1a1) \u9891\u8c31\u6269\u6563\u5148\u9a8c\uff08SDP\uff09\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4ece\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u9690\u5f0f\u5b66\u4e60\u5f97\u5230\uff1b2) \u9891\u8c31\u5148\u9a8c\u6ce8\u5165\u6a21\u5757\uff08SPIM\uff09\uff0c\u7528\u4e8e\u52a8\u6001\u5f15\u5bfc\u6a21\u578b\u6062\u590dHSI\u7ec6\u8282\u3002\u8be5\u65b9\u6cd5\u5728MST\u548cBISRNet\u4e24\u79cd\u4ee3\u8868\u6027HSI\u65b9\u6cd5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u65b9\u6cd5\u6bd4\u73b0\u6709\u7f51\u7edc\u6027\u80fd\u63d0\u5347\u7ea60.5 dB\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u7684\u6027\u80fd\u3002

Conclusion: \u901a\u8fc7\u5b66\u4e60\u5230\u7684\u9891\u8c31\u6269\u6563\u5148\u9a8c\u548c\u52a8\u6001\u6ce8\u5165\u6a21\u5757\uff0c\u672c\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u4e2d\u9ad8\u9891\u7ec6\u8282\u7684\u6062\u590d\u6548\u679c\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u91cd\u5efa\u6027\u80fd\u3002

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [55] [Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising](https://arxiv.org/abs/2507.13530)
*Lukas Baumgärtner,Ronny Bergmann,Roland Herzog,Stephan Schmidt,Manuel Weiß*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: We propose a novel formulation for the second-order total generalized
variation (TGV) of the normal vector on an oriented, triangular mesh embedded
in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued
function, taking values on the unit sphere. Our formulation extends previous
discrete TGV models for piecewise constant scalar data that utilize a
Raviart-Thomas function space. To exctend this formulation to the manifold
setting, a tailor-made tangential Raviart-Thomas type finite element space is
constructed in this work. The new regularizer is compared to existing methods
in mesh denoising experiments.

</details>


### [56] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoFNO\u76843D\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u9ad8\u78c1\u5171\u632f\u8840\u6d41\u6210\u50cf\u6570\u636e\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u4fe1\u566a\u6bd4\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8111\u52a8\u8109\u7624\u7834\u88c2\u5e76\u76f4\u63a5\u8ba1\u7b97\u58c1\u9762\u526a\u5e94\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u78c1\u5171\u632f\u8840\u6d41\u6210\u50cf\u867d\u7136\u80fd\u63d0\u4f9b\u65f6\u95f4\u5206\u8fa8\u7684\u8840\u6d41\u901f\u5ea6\u6d4b\u91cf\uff0c\u4f46\u5176\u4f4e\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u4fe1\u566a\u6bd4\u9650\u5236\u4e86\u5176\u5728\u52a8\u8109\u7624\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u9884\u6d4b\u52a8\u8109\u7624\u7834\u88c2\u548c\u6307\u5bfc\u6cbb\u7597\u65b9\u9762\u3002

Method: \u672c\u6587\u63d0\u51fa\u5c40\u90e8\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08LoFNO\uff09\uff0c\u4e00\u4e2a\u65b0\u9896\u76843D\u67b6\u6784\u3002\u5b83\u901a\u8fc7\u6574\u5408\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u4e0d\u89c4\u5219\u3001\u672a\u89c1\u51e0\u4f55\u5f62\u72b6\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff1b\u5e76\u91c7\u7528\u589e\u5f3a\u578b\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff08EDSR\uff09\u5c42\u8fdb\u884c\u9c81\u68d2\u7684\u4e0a\u91c7\u6837\u3002LoFNO\u7ed3\u5408\u4e86\u51e0\u4f55\u5148\u9a8c\u548c\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u5bf9\u8840\u6d41\u6570\u636e\u8fdb\u884c\u53bb\u566a\u548c\u65f6\u7a7a\u4e0a\u91c7\u6837\u3002

Result: LoFNO\u5728\u901f\u5ea6\u548c\u58c1\u9762\u526a\u5e94\u529b\uff08WSS\uff09\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u63d2\u503c\u6cd5\u548c\u5176\u5b83\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u4e34\u5e8a\u5f71\u50cf\u6570\u636e\u9884\u6d4bWSS\u3002

Conclusion: LoFNO\u901a\u8fc7\u53bb\u566a\u548c\u65f6\u7a7a\u4e0a\u91c7\u6837\u8840\u6d41\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u8111\u8840\u7ba1\u8bca\u65ad\uff0c\u4e3a\u52a8\u8109\u7624\u7834\u88c2\u9884\u6d4b\u548c\u6cbb\u7597\u6307\u5bfc\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [57] [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546)
*Dmitrii Mikhailov,Aleksey Letunovskiy,Maria Kovaleva,Vladimir Arkhipkin,Vladimir Korviakov,Vladimir Polovnikov,Viacheslav Vasilev,Evelina Sidorova,Denis Dimitrov*

Main category: cs.CV

TL;DR: NABLA\u662f\u4e00\u79cd\u65b0\u578b\u7684\u90bb\u57df\u81ea\u9002\u5e94\u5757\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210Transformer\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u7a00\u758f\u6a21\u5f0f\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002


<details>
  <summary>Details</summary>
Motivation: Transformer\u67b6\u6784\u5728\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5168\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u548c\u957f\u6301\u7eed\u65f6\u95f4\u89c6\u9891\u5e8f\u5217\u800c\u8a00\uff0c\u662f\u4e00\u4e2a\u5173\u952e\u7684\u8ba1\u7b97\u74f6\u9888\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86NABLA\uff08Neighborhood Adaptive Block-Level Attention\uff09\uff0c\u4e00\u79cd\u52a8\u6001\u9002\u5e94\u89c6\u9891\u6269\u6563Transformer\uff08DiTs\uff09\u4e2d\u7a00\u758f\u6a21\u5f0f\u7684\u673a\u5236\u3002\u5b83\u5229\u7528\u5757\u7ea7\u6ce8\u610f\u529b\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7a00\u758f\u9a71\u52a8\u9608\u503c\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u81ea\u5b9a\u4e49\u5e95\u5c42\u64cd\u4f5c\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210PyTorch\u7684Flex Attention\u64cd\u4f5c\u3002

Result: \u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cNABLA\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.7\u500d\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u51e0\u4e4e\u4e0d\u5f71\u54cd\u5b9a\u91cf\u6307\u6807\uff08CLIP\u5206\u6570\u3001VBench\u5206\u6570\u3001\u4eba\u7c7b\u8bc4\u4f30\u5206\u6570\uff09\u548c\u89c6\u89c9\u8d28\u91cf\u3002

Conclusion: NABLA\u901a\u8fc7\u81ea\u9002\u5e94\u5757\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210Transformer\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u751f\u6210\u8d28\u91cf\u3002

Abstract: Recent progress in transformer-based architectures has demonstrated
remarkable success in video generation tasks. However, the quadratic complexity
of full attention mechanisms remains a critical bottleneck, particularly for
high-resolution and long-duration video sequences. In this paper, we propose
NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that
dynamically adapts to sparsity patterns in video diffusion transformers (DiTs).
By leveraging block-wise attention with adaptive sparsity-driven threshold,
NABLA reduces computational overhead while preserving generative quality. Our
method does not require custom low-level operator design and can be seamlessly
integrated with PyTorch's Flex Attention operator. Experiments demonstrate that
NABLA achieves up to 2.7x faster training and inference compared to baseline
almost without compromising quantitative metrics (CLIP score, VBench score,
human evaluation score) and visual quality drop. The code and model weights are
available here: https://github.com/gen-ai-team/Wan2.1-NABLA

</details>


### [58] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faCF-SSC\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u65f6\u95f4\u60273D\u8bed\u4e49\u573a\u666f\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u4f2a\u672a\u6765\u5e27\u9884\u6d4b\u548c3D\u611f\u77e5\u67b6\u6784\uff0c\u6709\u6548\u6269\u5c55\u611f\u77e5\u8303\u56f4\uff0c\u89e3\u51b3\u5355\u76eeSSC\u4e2d\u906e\u6321\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u573a\u666f\u8865\u5168\u7cbe\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u5355\u76ee3D\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u65b9\u6cd5\u5728\u5b9e\u9645\u4ea4\u901a\u573a\u666f\u4e2d\uff0c\u65e0\u6cd5\u5145\u5206\u5904\u7406\u5927\u91cf\u88ab\u906e\u6321\u6216\u8d85\u51fa\u76f8\u673a\u89c6\u91ce\u7684\u533a\u57df\uff0c\u8fd9\u662f\u5176\u6839\u672c\u6027\u6311\u6218\u3002

Method: \u63d0\u51faCF-SSC\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u65f6\u95f4\u6027SSC\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4f2a\u672a\u6765\u5e27\u9884\u6d4b\u6765\u6269\u5c55\u6a21\u578b\u7684\u6709\u6548\u611f\u77e5\u8303\u56f4\uff0c\u5e76\u7ed3\u5408\u59ff\u6001\u548c\u6df1\u5ea6\u4fe1\u606f\u5efa\u7acb\u7cbe\u786e\u76843D\u5bf9\u5e94\u5173\u7cfb\u3002\u5b83\u57283D\u7a7a\u95f4\u4e2d\u51e0\u4f55\u4e00\u81f4\u5730\u878d\u5408\u8fc7\u53bb\u3001\u73b0\u5728\u548c\u9884\u6d4b\u7684\u672a\u6765\u5e27\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u51763D\u611f\u77e5\u67b6\u6784\u663e\u5f0f\u5efa\u6a21\u65f6\u7a7a\u5173\u7cfb\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u573a\u666f\u8865\u5168\u3002

Result: \u5728SemanticKITTI\u548cSSCBench-KITTI-360\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCF-SSC\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u6539\u5584\u906e\u6321\u63a8\u7406\u548c\u63d0\u9ad83D\u573a\u666f\u8865\u5168\u7cbe\u5ea6\u65b9\u9762\u3002

Conclusion: CF-SSC\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4fe1\u606f\u548c\u4f2a\u672a\u6765\u5e27\u9884\u6d4b\uff0c\u6210\u529f\u514b\u670d\u4e86\u5355\u76eeSSC\u5728\u5904\u7406\u906e\u6321\u548c\u89c6\u91ce\u5916\u533a\u57df\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e863D\u573a\u666f\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u906e\u6321\u63a8\u7406\u80fd\u529b\u3002

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [59] [LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning](https://arxiv.org/abs/2507.13568)
*Kaihong Wang,Donghyun Kim,Margrit Betke*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLoRA\u589e\u5f3a\u7684\u5408\u6210\u56de\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u51bb\u7ed3\u7684Stable Diffusion\u6a21\u578b\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u9002\u914d\u5668\uff0c\u751f\u6210\u66f4\u5177\u9886\u57df\u7279\u5f02\u6027\u7684\u6837\u672c\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u5408\u6210\u56de\u653e\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u9886\u57df\u7279\u5f02\u6027\u6216\u7ec6\u7c92\u5ea6\u8bed\u4e49\u65f6\uff0c\u751f\u6210\u7684\u6837\u672c\u5f80\u5f80\u4e0e\u771f\u5b9e\u6570\u636e\u4e0d\u7b26\uff0c\u5bfc\u81f4\u5fae\u8c03\u6548\u679c\u4e0d\u4f73\u5e76\u635f\u5bb3\u77e5\u8bc6\u4fdd\u7559\uff0c\u56e0\u4e3a\u751f\u6210\u5668\u672a\u80fd\u5145\u5206\u6355\u6349\u8fd9\u4e9b\u7ec6\u5fae\u5dee\u522b\u3002

Method: \u8be5\u65b9\u6cd5\u5f15\u5165\u4e00\u4e2aLoRA\u589e\u5f3a\u7684\u5408\u6210\u56de\u653e\u6846\u67b6\uff1a1) \u5c06\u4efb\u52a1\u7279\u5b9a\u7684LoRA\u6ce8\u5165\u51bb\u7ed3\u7684Stable Diffusion\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u65b0\u4efb\u52a1\u7684\u89c6\u89c9\u548c\u8bed\u4e49\u6a21\u5f0f\u30022) \u91c7\u7528\u4e24\u9636\u6bb5\u3001\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6837\u672c\u9009\u62e9\u673a\u5236\uff1a\u9996\u5148\uff0c\u6839\u636eVLM\u5728\u5fae\u8c03\u540e\u7684\u7f6e\u4fe1\u5ea6\u5bf9\u771f\u5b9e\u4efb\u52a1\u6570\u636e\u8fdb\u884c\u6392\u5e8f\uff0c\u4ee5\u9009\u62e9\u6700\u5177\u4ee3\u8868\u6027\u7684\u6837\u672c\u6765\u5fae\u8c03LoRA\uff1b\u5176\u6b21\uff0c\u751f\u6210\u5408\u6210\u6837\u672c\u540e\uff0c\u518d\u6b21\u6839\u636e\u7f6e\u4fe1\u5ea6\u9009\u62e9\u5b83\u4eec\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u3002\u8be5\u65b9\u6cd5\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u56de\u653e\u6d41\u7a0b\u4e2d\u3002

Result: \u5728\u591a\u9886\u57df\u4efb\u52a1\u589e\u91cf\u5b66\u4e60(MTIL)\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u5408\u6210\u56de\u653e\u6280\u672f\uff0c\u5728\u53ef\u5851\u6027\u3001\u7a33\u5b9a\u6027\u4ee5\u53ca\u96f6\u6837\u672c\u80fd\u529b\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\u3002

Conclusion: \u901a\u8fc7LoRA\u8fdb\u884c\u751f\u6210\u5668\u9002\u5e94\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u5668\u9002\u5e94\u5bf9VLM\u6301\u7eed\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002

Abstract: Continual learning for vision-language models has achieved remarkable
performance through synthetic replay, where samples are generated using Stable
Diffusion to regularize during finetuning and retain knowledge. However,
real-world downstream applications often exhibit domain-specific nuances and
fine-grained semantics not captured by generators, causing synthetic-replay
methods to produce misaligned samples that misguide finetuning and undermine
retention of prior knowledge. In this work, we propose a LoRA-enhanced
synthetic-replay framework that injects task-specific low-rank adapters into a
frozen Stable Diffusion model, efficiently capturing each new task's unique
visual and semantic patterns. Specifically, we introduce a two-stage,
confidence-based sample selection: we first rank real task data by
post-finetuning VLM confidence to focus LoRA finetuning on the most
representative examples, then generate synthetic samples and again select them
by confidence for distillation. Our approach integrates seamlessly with
existing replay pipelines-simply swap in the adapted generator to boost replay
fidelity. Extensive experiments on the Multi-domain Task Incremental Learning
(MTIL) benchmark show that our method outperforms previous synthetic-replay
techniques, achieving an optimal balance among plasticity, stability, and
zero-shot capability. These results demonstrate the effectiveness of generator
adaptation via LoRA for robust continual learning in VLMs.

</details>


### [60] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u5f00\u653e\u5f0f\u89c6\u9891\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u5f02\u6784\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u8bc4\u4f30\u548c\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u63a8\u7406\u6df1\u5ea6\u548c\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u89c6\u9891-\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08Video-LMMs\uff09\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u65f6\u95f4\u5efa\u6a21\u548c\u5904\u7406\u6a21\u7cca\u6216\u7ec4\u5408\u5f0f\u67e5\u8be2\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u6df1\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002

Method: \u5f15\u5165\u4e00\u79cd\u63d0\u793a-\u54cd\u5e94\u96c6\u6210\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u534f\u8c03\u591a\u4e2a\u5f02\u6784\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u6bcf\u4e2aVLM\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7684\u63a8\u7406\u8def\u5f84\u3002\u4e00\u4e2a\u5916\u90e8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5145\u5f53\u8bc4\u4f30\u5668\u548c\u96c6\u6210\u5668\uff0c\u9009\u62e9\u5e76\u878d\u5408\u6700\u53ef\u9760\u7684\u54cd\u5e94\u3002

Result: \u5728CVRR-ES\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002

Conclusion: \u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u63a8\u7406\u7b56\u7565\uff0c\u65e0\u9700\u6a21\u578b\u518d\u8bad\u7ec3\uff0c\u4e3a\u672a\u6765\u7684\u89c6\u9891-\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08Video-LMM\uff09\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [61] [NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision](https://arxiv.org/abs/2507.13595)
*Tengkai Wang,Weihao Li,Ruikai Cui,Shi Qiu,Nick Barnes*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faNoiseSDF2NoiseSDF\u65b9\u6cd5\uff0c\u5c06Noise2Noise\u8303\u5f0f\u6269\u5c55\u52303D\u795e\u7ecf\u573a\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u566a\u58f0SDF\u8868\u793a\u95f4\u7684MSE\u635f\u5931\uff0c\u76f4\u63a5\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u5e72\u51c0\u7684\u795e\u7ecfSDF\uff0c\u4ece\u800c\u63d0\u9ad8\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002


<details>
  <summary>Details</summary>
Motivation: \u4ece\u4f4e\u8d28\u91cf\u626b\u63cf\u8bbe\u5907\u6355\u83b7\u7684\u70b9\u4e91\u91cd\u5efa\u7cbe\u786e\u7684\u9690\u5f0f\u8868\u9762\u8868\u793a\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u70b9\u4e91\u901a\u5e38\u5305\u542b\u5927\u91cf\u566a\u58f0\uff0c\u5bfc\u81f4\u8868\u9762\u91cd\u5efa\u4e0d\u51c6\u786e\u3002

Method: \u5f15\u5165NoiseSDF2NoiseSDF\u65b9\u6cd5\uff0c\u53d72D\u56fe\u50cfNoise2Noise\u8303\u5f0f\u542f\u53d1\uff0c\u5c06\u5176\u5e94\u7528\u4e8e3D\u795e\u7ecf\u573a\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u4e24\u4e2a\u566a\u58f0SDF\u8868\u793a\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u635f\u5931\uff0c\u5b9e\u73b0\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u76f4\u63a5\u5b66\u4e60\u5e72\u51c0\u7684\u795e\u7ecfSDF\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u9690\u5f0f\u5730\u53bb\u566a\u5e76\u4f18\u5316\u8868\u9762\u4f30\u8ba1\u3002

Result: \u5728ShapeNet\u3001ABC\u3001Famous\u548cReal\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u566a\u58f0\u8f93\u5165\u4e2d\u8fdb\u884c\u8868\u9762\u91cd\u5efa\u7684\u8d28\u91cf\u3002

Conclusion: NoiseSDF2NoiseSDF\u6210\u529f\u5730\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u5e76\u91cd\u5efa\u51fa\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u8868\u9762\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d28\u91cf\u626b\u63cf\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\u3002

Abstract: Reconstructing accurate implicit surface representations from point clouds
remains a challenging task, particularly when data is captured using
low-quality scanning devices. These point clouds often contain substantial
noise, leading to inaccurate surface reconstructions. Inspired by the
Noise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel
method designed to extend this concept to 3D neural fields. Our approach
enables learning clean neural SDFs directly from noisy point clouds through
noisy supervision by minimizing the MSE loss between noisy SDF representations,
allowing the network to implicitly denoise and refine surface estimations. We
evaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the
ShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that
our framework significantly improves surface reconstruction quality from noisy
inputs.

</details>


### [62] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u5206\u6790\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982\u4f55\u89e3\u51b3\u5176\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u89c6\u89c9\u8f93\u5165\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u77db\u76fe\u6570\u636e\u96c6\uff0c\u7814\u7a76\u4eba\u5458\u5b9a\u4f4d\u5e76\u53d1\u73b0\u4e86\u4e00\u7ec4\u5173\u952e\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5b83\u4eec\u63a7\u5236\u7740\u51b2\u7a81\u7684\u89e3\u51b3\u65b9\u5411\uff0c\u5e76\u80fd\u7528\u4e8e\u7cbe\u786e\u7684\u89c6\u89c9\u5f52\u56e0\u3002


<details>
  <summary>Details</summary>
Motivation: VLMs\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u5176\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u4e0e\u5916\u90e8\u4fe1\u606f\u4e4b\u95f4\u5e38\u53d1\u751f\u51b2\u7a81\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u548c\u4e0d\u53ef\u9760\u7684\u54cd\u5e94\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u4e8eVLM\u5982\u4f55\u89e3\u51b3\u8fd9\u4e9b\u8de8\u6a21\u6001\u51b2\u7a81\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002

Method: \u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u53cd\u4e8b\u5b9e\u67e5\u8be2\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6545\u610f\u4e0eVLM\u7684\u5185\u90e8\u5e38\u8bc6\u77e5\u8bc6\u76f8\u77db\u76fe\u3002\u901a\u8fc7\u5bf9logit\u8fdb\u884c\u68c0\u67e5\uff0c\u4ed6\u4eec\u5b9a\u4f4d\u4e86\u63a7\u5236\u51b2\u7a81\u89e3\u51b3\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u901a\u8fc7\u4fee\u6539\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u6765\u5f15\u5bfc\u6a21\u578b\u503e\u5411\u4e8e\u5176\u5185\u90e8\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\u3002

Result: \u7814\u7a76\u53d1\u73b0\u5e76\u5b9a\u4f4d\u4e86\u4e00\u5c0f\u7ec4\u63a7\u5236\u51b2\u7a81\u89e3\u51b3\u7684\u6ce8\u610f\u529b\u5934\u3002\u901a\u8fc7\u4fee\u6539\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\uff0c\u6a21\u578b\u53ef\u4ee5\u88ab\u5f15\u5bfc\u503e\u5411\u4e8e\u5176\u5185\u90e8\u77e5\u8bc6\u6216\u89c6\u89c9\u8f93\u5165\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u7684\u6ce8\u610f\u529b\u80fd\u591f\u7cbe\u786e\u5730\u6307\u5411\u9a71\u52a8\u89c6\u89c9\u8986\u76d6\u7684\u56fe\u50cf\u533a\u57df\uff0c\u5e76\u4e14\u5728\u5f52\u56e0\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u3002

Conclusion: VLMs\u89e3\u51b3\u8de8\u6a21\u6001\u51b2\u7a81\u7684\u673a\u5236\u53ef\u4ee5\u901a\u8fc7\u8bc6\u522b\u548c\u64cd\u7eb5\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u6765\u7406\u89e3\u548c\u63a7\u5236\u3002\u8fd9\u4e9b\u5173\u952e\u7684\u6ce8\u610f\u529b\u5934\u4e0d\u4ec5\u5728\u51b2\u7a81\u89e3\u51b3\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u8fd8\u80fd\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7684\u89c6\u89c9\u5f52\u56e0\u3002

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [63] [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/abs/2507.13599)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08DM\uff09\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7a7a\u95f4\u53d8\u5316\u7684\u7eb9\u7406\u5148\u9a8c\u6765\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u6a21\u7cca\u6a21\u5f0f\u590d\u6742\u7684\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u83b7\u53d6\u5927\u91cf\u7684\u771f\u5b9e\u6a21\u7cca-\u6e05\u6670\u56fe\u50cf\u5bf9\u65e2\u56f0\u96be\u53c8\u6602\u8d35\uff0c\u4f7f\u5f97\u4ece\u975e\u914d\u5bf9\u6570\u636e\u4e2d\u5b66\u4e60\u76f2\u53bb\u6a21\u7cca\u6210\u4e3a\u66f4\u5b9e\u9645\u548c\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5bf9\u6297\u5b66\u4e60\u6765\u5f25\u8865\u6a21\u7cca\u57df\u548c\u6e05\u6670\u57df\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5374\u5ffd\u89c6\u4e86\u771f\u5b9e\u4e16\u754c\u4e2d\u6a21\u7cca\u6a21\u5f0f\u7684\u590d\u6742\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201cours\u201d\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff08DM\uff09\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528DM\u751f\u6210\u5148\u9a8c\u77e5\u8bc6\u4ee5\u6062\u590d\u6a21\u7cca\u56fe\u50cf\u7684\u7eb9\u7406\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7eb9\u7406\u5148\u9a8c\u7f16\u7801\u5668\uff08TPE\uff09\uff0c\u5176\u5f15\u5165\u8bb0\u5fc6\u673a\u5236\u6765\u8868\u793a\u56fe\u50cf\u7eb9\u7406\u5e76\u4e3aDM\u8bad\u7ec3\u63d0\u4f9b\u76d1\u7763\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u751f\u6210\u7684\u7eb9\u7406\u5148\u9a8c\uff0c\u63d0\u51fa\u4e86\u7eb9\u7406\u4f20\u8f93Transformer\u5c42\uff08TTformer\uff09\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u79cd\u65b0\u9896\u7684\u6ee4\u6ce2\u8c03\u5236\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08FM-MSA\uff09\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ee4\u6ce2\u6709\u6548\u53bb\u9664\u7a7a\u95f4\u53d8\u5316\u7684\u6a21\u7cca\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u5c0f\u6ce2\u7684\u5bf9\u6297\u6027\u635f\u5931\u4ee5\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\u7ec6\u8282\u3002

Result: \u5e7f\u6cdb\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u201cours\u201d\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65e0\u76d1\u7763\u53bb\u6a21\u7cca\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002

Conclusion: \u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7a7a\u95f4\u53d8\u5316\u7684\u7eb9\u7406\u5148\u9a8c\u548c\u5229\u7528\u81ea\u9002\u5e94\u6ee4\u6ce2\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u56fe\u50cf\u53bb\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u9886\u5148\u6c34\u5e73\u3002

Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is
difficult and expensive, learning blind image deblurring from unpaired data is
a more practical and promising solution. Unfortunately, dominant approaches
rely heavily on adversarial learning to bridge the gap from blurry domains to
sharp domains, ignoring the complex and unpredictable nature of real-world blur
patterns. In this paper, we propose a novel diffusion model (DM)-based
framework, dubbed \ours, for image deblurring by learning spatially varying
texture prior from unpaired data. In particular, \ours performs DM to generate
the prior knowledge that aids in recovering the textures of blurry images. To
implement this, we propose a Texture Prior Encoder (TPE) that introduces a
memory mechanism to represent the image textures and provides supervision for
DM training. To fully exploit the generated texture priors, we present the
Texture Transfer Transformer layer (TTformer), in which a novel
Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes
spatially varying blurring through adaptive filtering. Furthermore, we
implement a wavelet-based adversarial loss to preserve high-frequency texture
details. Extensive evaluations show that \ours provides a promising
unsupervised deblurring solution and outperforms SOTA methods in widely-used
benchmarks.

</details>


### [64] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u5b9e\u65f6\u89c6\u89c9\u6570\u636e\u4e0e\u6d77\u56fe\u4fe1\u606f\u878d\u5408\u6765\u589e\u5f3a\u6d77\u6d0b\u89c6\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u6d6e\u6807\u68c0\u6d4b\u4e0e\u6d77\u56fe\u6807\u8bb0\u7684\u7cbe\u786e\u5339\u914d\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u63d0\u9ad8\u6d77\u6d0b\u89c6\u89c9\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u6d77\u56fe\u4fe1\u606f\u53e0\u52a0\u5230\u5b9e\u65f6\u89c6\u9891\u6d41\u4e0a\uff0c\u89e3\u51b3\u5728\u52a8\u6001\u548c\u590d\u6742\u6d77\u6d0b\u73af\u5883\u4e2d\u5bfc\u822a\u8f85\u52a9\u7269\uff08\u5982\u6d6e\u6807\uff09\u7684\u51c6\u786e\u68c0\u6d4b\u548c\u6d77\u56fe\u5bf9\u5e94\u5339\u914d\u7684\u6311\u6218\u3002

Method: \u8be5\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u5339\u914d\u68c0\u6d4b\u5230\u7684\u5bfc\u822a\u8f85\u52a9\u7269\uff08\u5982\u6d6e\u6807\uff09\u4e0e\u6d77\u56fe\u6570\u636e\u4e2d\u7684\u5bf9\u5e94\u8868\u793a\uff0c\u5c06\u6d77\u56fe\u6570\u636e\u53e0\u52a0\u5230\u5b9e\u65f6\u89c6\u9891\u6d41\u4e0a\u3002\u6838\u5fc3\u6280\u672f\u662f\u5f15\u5165\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u6d6e\u6807\u67e5\u8be2\u7684\u8fb9\u754c\u6846\u548c\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u56fe\u50cf\u57df\u68c0\u6d4b\u4e0e\u4e16\u754c\u7a7a\u95f4\u6d77\u56fe\u6807\u8bb0\u7684\u76f4\u63a5\u5339\u914d\u3002\u8be5\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u5c04\u7ebf\u6295\u5c04\u6a21\u578b\u548c\u6269\u5c55YOLOv7\u7f51\u7edc\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002

Result: \u5728\u771f\u5b9e\u4e16\u754c\u6d77\u6d0b\u573a\u666f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u548c\u5177\u6709\u6311\u6218\u6027\u73af\u5883\u4e2d\u7269\u4f53\u7684\u5b9a\u4f4d\u548c\u5173\u8054\u51c6\u786e\u6027\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u7684\u5b9e\u65f6\u89c6\u89c9\u6570\u636e\u4e0e\u6d77\u56fe\u4fe1\u606f\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u6d77\u6d0b\u89c6\u89c9\uff0c\u5e76\u5728\u7269\u4f53\u5b9a\u4f4d\u548c\u5173\u8054\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [65] [Efficient Burst Super-Resolution with One-step Diffusion](https://arxiv.org/abs/2507.13607)
*Kento Kawai,Takeru Oba,Kyotaro Tokoro,Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: \u9488\u5bf9\u7a81\u53d1\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u4ea7\u751f\u6a21\u7cca\u7ed3\u679c\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u6548\u91c7\u6837\u5668\u548c\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e00\u6b65\u6269\u6563\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8fd0\u884c\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u751f\u6210\u6e05\u6670\u4e14\u9ad8\u4fdd\u771f\u7684SR\u56fe\u50cf\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u7a81\u53d1LR\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65b9\u6cd5\u4ee5\u786e\u5b9a\u6027\u65b9\u5f0f\u8bad\u7ec3\uff0c\u5bfc\u81f4\u751f\u6210\u7684SR\u56fe\u50cf\u6a21\u7cca\uff0c\u611f\u77e5\u8d28\u91cf\u4e0b\u964d\u3002

Method: \u672c\u7814\u7a76\u91c7\u7528\u6269\u6563\u6a21\u578b\u6765\u91cd\u5efa\u6e05\u6670\u4e14\u9ad8\u4fdd\u771f\u7684SR\u56fe\u50cf\u3002\u4e3a\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\uff0c\u65b9\u6cd5\u7ed3\u5408\u4e86\u9ad8\u9636ODE\u7684\u968f\u673a\u91c7\u6837\u5668\u4ee5\u53ca\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u7684\u4e00\u6b65\u6269\u6563\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u56fe\u50cf\u5931\u771f\u548c\u611f\u77e5\u8d28\u91cf\u8861\u91cf\u7684SR\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u81f3\u57fa\u7ebf\u65b9\u6cd5\u76841.6%\u3002

Conclusion: \u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u91c7\u6837\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a81\u53d1LR\u56fe\u50cfSR\u4e2d\u7684\u6a21\u7cca\u95ee\u9898\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684SR\u56fe\u50cf\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002

Abstract: While burst Low-Resolution (LR) images are useful for improving their Super
Resolution (SR) image compared to a single LR image, prior burst SR methods are
trained in a deterministic manner, which produces a blurry SR image. Since such
blurry images are perceptually degraded, we aim to reconstruct sharp and
high-fidelity SR images by a diffusion model. Our method improves the
efficiency of the diffusion model with a stochastic sampler with a high-order
ODE as well as one-step diffusion using knowledge distillation. Our
experimental results demonstrate that our method can reduce the runtime to 1.6
% of its baseline while maintaining the SR quality measured based on image
distortion and perceptual quality.

</details>


### [66] [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
*Yanan Wang,Julio Vizcarra,Zhi Li,Hao Niu,Mori Kurokawa*

Main category: cs.CV

TL;DR: CoTasks\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u89c6\u9891\u95ee\u9898\u5206\u89e3\u4e3a\u5b9e\u4f53\u7ea7\u522b\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u4e3a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u6ce8\u5165\u4e86\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u611f\u77e5\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u80fd\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u7ea7\u89c6\u9891\u7406\u89e3\u57fa\u7840\u4e0a\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u5728\u7f3a\u4e4f\u7ed3\u6784\u5316\u6807\u6ce8\u7684\u9ad8\u7ea7\u89c6\u9891-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002

Method: CoTasks\u6846\u67b6\u5c06\u73b0\u6709\u6570\u636e\u96c6\uff08\u5982NeXT-QA, STAR\uff09\u4e2d\u7684\u590d\u6742\u89c6\u9891\u95ee\u9898\u5206\u89e3\u4e3a\u56db\u4e2a\u5b9e\u4f53\u7ea7\u522b\u7684\u57fa\u7840\u4efb\u52a1\uff1a\u5e27\u5b9a\u4f4d\u3001\u5b9e\u4f53\u8ffd\u8e2a\u3001\u7a7a\u95f4\u5173\u7cfb\u63d0\u53d6\u548c\u65f6\u95f4\u5173\u7cfb\u63d0\u53d6\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u4e2d\u95f4\u7684CoT\u98ce\u683c\u63a8\u7406\u6b65\u9aa4\u5d4c\u5165\u5230\u6a21\u578b\u8f93\u5165\u4e2d\uff0cCoTasks\u4f7f\u6a21\u578b\u80fd\u591f\u660e\u786e\u5730\u6267\u884c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u65f6\u7a7a\u63a8\u7406\u3002

Result: \u5728NeXT-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoTasks\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff1aLLaVA-video-7B\u7684\u5e73\u5747GPT-4\u8bc4\u4f30\u5206\u6570\u63d0\u9ad8\u4e863.3\u5206\uff0cQwen2.5-VL-3B\u63d0\u9ad8\u4e8617.4\u5206\uff0c\u7279\u522b\u662f\u5728\u56e0\u679c\uff08+14.6\uff09\u3001\u65f6\u95f4\uff08+10.9\uff09\u548c\u63cf\u8ff0\u6027\uff08+48.1\uff09\u5b50\u7c7b\u522b\u4e2d\u83b7\u5f97\u4e86\u5de8\u5927\u63d0\u5347\u3002

Conclusion: CoTasks\u4f5c\u4e3a\u4e00\u79cd\u7ed3\u6784\u5316\u7684CoT\u98ce\u683c\u76d1\u7763\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u5f0f\u89c6\u9891\u63a8\u7406\u80fd\u529b\u3002

Abstract: Despite recent progress in video large language models (VideoLLMs), a key
open challenge remains: how to equip models with chain-of-thought (CoT)
reasoning abilities grounded in fine-grained object-level video understanding.
Existing instruction-tuned models, such as the Qwen and LLaVA series, are
trained on high-level video-text pairs, often lacking structured annotations
necessary for compositional, step-by-step reasoning. We propose CoTasks:
Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that
decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)
into four entity-level foundational tasks: frame localization, entity tracking,
spatial and temporal relation extraction. By embedding these intermediate
CoT-style reasoning steps into the input, CoTasks enables models to explicitly
perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA
benchmark show that CoTasks significantly enhance inference performance:
LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and
Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal
(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the
effectiveness of CoTasks as a structured CoT-style supervision framework for
improving compositional video reasoning.

</details>


### [67] [Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation](https://arxiv.org/abs/2507.13628)
*Masahiro Ogawa,Qi An,Atsushi Yamashita*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faFoELS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5149\u6d41\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u6709\u6548\u4ece\u8fd0\u52a8\u76f8\u673a\u89c6\u89d2\u4e2d\u5206\u79bb\u51fa\u8fd0\u52a8\u548c\u9759\u6001\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5149\u6d41\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u68c0\u6d4b\u8fd0\u52a8\u7269\u4f53\u56f0\u96be\u7684\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u673a\u5668\u4eba\u9886\u57df\uff0c\u4ece\u8fd0\u52a8\u76f8\u673a\u89c6\u89d2\u4e2d\u5206\u79bb\u8fd0\u52a8\u548c\u9759\u6001\u7269\u4f53\u5bf9\u4e8e3D\u91cd\u5efa\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5149\u6d41\uff0c\u5728\u6d89\u53ca\u76f8\u673a\u8fd0\u52a8\u7684\u590d\u6742\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u68c0\u6d4b\u8fd0\u52a8\u7269\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\u3002

Method: FoELS\uff08Focus of Expansion Likelihood and Segmentation\uff09\u65b9\u6cd5\u6574\u5408\u4e86\u5149\u6d41\u548c\u7eb9\u7406\u4fe1\u606f\u3002\u5b83\u9996\u5148\u4ece\u5149\u6d41\u8ba1\u7b97\u81a8\u80c0\u7126\u70b9\uff08FoE\uff09\uff0c\u5e76\u4eceFoE\u8ba1\u7b97\u7684\u5f02\u5e38\u503c\u4e2d\u5f97\u51fa\u521d\u59cb\u8fd0\u52a8\u53ef\u80fd\u6027\u3002\u7136\u540e\uff0c\u5c06\u8be5\u53ef\u80fd\u6027\u4e0e\u57fa\u4e8e\u5206\u5272\u7684\u5148\u9a8c\u4fe1\u606f\u878d\u5408\uff0c\u4ee5\u4f30\u8ba1\u6700\u7ec8\u7684\u8fd0\u52a8\u6982\u7387\u3002

Result: \u8be5\u65b9\u6cd5\u6709\u6548\u5904\u7406\u4e86\u590d\u6742\u7ed3\u6784\u5316\u573a\u666f\u3001\u65cb\u8f6c\u76f8\u673a\u8fd0\u52a8\u548c\u5e73\u884c\u8fd0\u52a8\u7b49\u6311\u6218\u3002\u5728DAVIS 2016\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u89c6\u9891\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\u5176\u6709\u6548\u6027\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002

Conclusion: FoELS\u901a\u8fc7\u878d\u5408\u5149\u6d41\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u6210\u529f\u5730\u4ece\u8fd0\u52a8\u76f8\u673a\u89c6\u89d2\u4e2d\u5206\u79bb\u51fa\u8fd0\u52a8\u548c\u9759\u6001\u7269\u4f53\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u5149\u6d41\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u9879\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002

Abstract: Separating moving and static objects from a moving camera viewpoint is
essential for 3D reconstruction, autonomous navigation, and scene understanding
in robotics. Existing approaches often rely primarily on optical flow, which
struggles to detect moving objects in complex, structured scenes involving
camera motion. To address this limitation, we propose Focus of Expansion
Likelihood and Segmentation (FoELS), a method based on the core idea of
integrating both optical flow and texture information. FoELS computes the focus
of expansion (FoE) from optical flow and derives an initial motion likelihood
from the outliers of the FoE computation. This likelihood is then fused with a
segmentation-based prior to estimate the final moving probability. The method
effectively handles challenges including complex structured scenes, rotational
camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016
dataset and real-world traffic videos demonstrate its effectiveness and
state-of-the-art performance.

</details>


### [68] [EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation](https://arxiv.org/abs/2507.13648)
*Seungjun Moon,Sangjoon Yu,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: EPSilon\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u70b9\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901f\u4e86\u57fa\u4e8eNeRF\u548cSMPL\u7684\u6df7\u5408\u4eba\u4f53\u5934\u50cf\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u6df7\u5408\u8868\u793a\uff08NeRF\u4e0eSMPL\u7f51\u683c\u7ed3\u5408\uff09\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u4f53\u5934\u50cf\uff0c\u4f46\u7531\u4e8e\u57fa\u4e8eSMPL\u8499\u76ae\u6743\u91cd\u7684\u53d8\u5f62\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u7a7a\u65f7\u7a7a\u95f4\u4e2d\u7684\u70b9\u8fdb\u884c\u91c7\u6837\u65f6\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6781\u6162\u3002

Method: EPSilon\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u70b9\u91c7\u6837\u7b56\u7565\uff0c\u5305\u62ec\u4e24\u79cd\u65b9\u6cd5\uff1a\u7a7a\u5c04\u7ebf\u5254\u9664\uff08ERO\uff09\uff0c\u5373\u79fb\u9664\u7a7f\u8fc7\u7a7a\u65f7\u7a7a\u95f4\u7684\u5c04\u7ebf\uff1b\u7a7a\u533a\u95f4\u5254\u9664\uff08EIO\uff09\uff0c\u5373\u7f29\u5c0f\u5c04\u7ebf\u4e0a\u7684\u91c7\u6837\u533a\u95f4\uff0c\u4ec5\u4fdd\u7559\u88ab\u8863\u670d\u6216\u7f51\u683c\u5360\u636e\u7684\u533a\u57df\u3002\u8fd9\u79cd\u7cbe\u7ec6\u7684\u91c7\u6837\u65b9\u6848\u51cf\u5c11\u4e86\u53d8\u5f62\u8ba1\u7b97\u91cf\uff0c\u5e76\u5b9e\u73b0\u4e86\u5355\u9636\u6bb5NeRF\u7ed3\u6784\u3002

Result: \u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cEPSilon\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4ec5\u4f7f\u7528\u4e863.9%\u7684\u91c7\u6837\u70b9\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86\u7ea620\u500d\uff0c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e864\u500d\u3002

Conclusion: EPSilon\u901a\u8fc7\u521b\u65b0\u7684\u9ad8\u6548\u70b9\u91c7\u6837\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6df7\u5408NeRF-SMPL\u6a21\u578b\u5728\u4eba\u4f53\u5934\u50cf\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u74f6\u9888\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002

Abstract: The rapid advancement of neural radiance fields (NeRF) has paved the way to
generate animatable human avatars from a monocular video. However, the sole
usage of NeRF suffers from a lack of details, which results in the emergence of
hybrid representation that utilizes SMPL-based mesh together with NeRF
representation. While hybrid-based models show photo-realistic human avatar
generation qualities, they suffer from extremely slow inference due to their
deformation scheme: to be aligned with the mesh, hybrid-based models use the
deformation based on SMPL skinning weights, which needs high computational
costs on each sampled point. We observe that since most of the sampled points
are located in empty space, they do not affect the generation quality but
result in inference latency with deformation. In light of this observation, we
propose EPSilon, a hybrid-based 3D avatar generation scheme with novel
efficient point sampling strategies that boost both training and inference. In
EPSilon, we propose two methods to omit empty points at rendering; empty ray
omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that
progress through the empty space. Then, EIO narrows down the sampling interval
on the ray, which wipes out the region not occupied by either clothes or mesh.
The delicate sampling scheme of EPSilon enables not only great computational
cost reduction during deformation but also the designation of the important
regions to be sampled, which enables a single-stage NeRF structure without
hierarchical sampling. Compared to existing methods, EPSilon maintains the
generation quality while using only 3.9% of sampled points and achieves around
20 times faster inference, together with 4 times faster training convergence.
We provide video results on https://github.com/seungjun-moon/epsilon.

</details>


### [69] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: \u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u77ed\u671f\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u901a\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u51bb\u7ed3\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u3002


<details>
  <summary>Details</summary>
Motivation: \u9884\u6d4b\u80fd\u529b\u5bf9\u4e8e\u5728\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u4e0a\u89c4\u5212\u6216\u884c\u52a8\u7684\u901a\u7528\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u5176\u901a\u7528\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002

Method: \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u5728\u4efb\u4f55\u51bb\u7ed3\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fd0\u884c\uff1a\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u51bb\u7ed3\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u672a\u6765\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u3001\u7279\u5b9a\u4efb\u52a1\u7684\u8bfb\u51fa\u5668\u8fdb\u884c\u89e3\u7801\u3002\u5f15\u5165\u4e86\u5206\u5e03\u5ea6\u91cf\u6765\u76f4\u63a5\u5728\u4e0b\u6e38\u4efb\u52a1\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u5206\u5e03\u7279\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e5d\u4e2a\u6a21\u578b\u548c\u56db\u4e2a\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002

Result: \u53d1\u73b0\u89c6\u89c9\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u5176\u5728\u77ed\u65f6\u95f4\u8303\u56f4\u5185\u7684\u901a\u7528\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002\u8fd9\u4e00\u8d8b\u52bf\u5728\u5404\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5305\u62ec\u751f\u6210\u6a21\u578b\uff09\u548c\u591a\u4e2a\u62bd\u8c61\u7ea7\u522b\uff08\u4ece\u539f\u59cb\u50cf\u7d20\u5230\u6df1\u5ea6\u3001\u70b9\u8f68\u8ff9\u548c\u7269\u4f53\u8fd0\u52a8\uff09\u4e0a\u5747\u6210\u7acb\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u878d\u5408\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\u5efa\u6a21\u5bf9\u4e8e\u65f6\u95f4\u76f8\u5173\u7684\u89c6\u9891\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [70] [Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration](https://arxiv.org/abs/2507.13663)
*Xingyu Jiang,Ning Gao,Hongkun Dou,Xiuhui Zhang,Xiaoqing Zhong,Yue Deng,Hongjue Li*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPW-FNet\u7684\u65b0\u578b\u9ad8\u6548\u91d1\u5b57\u5854\u5c0f\u6ce2-\u5085\u91cc\u53f6\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u5c0f\u6ce2\u5206\u89e3\u548c\u5085\u91cc\u53f6\u53d8\u6362\u6765\u89e3\u51b3\u56fe\u50cf\u6062\u590d\u4e2dTransformer\u6a21\u578b\u7684\u9ad8\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u548c\u6548\u7387\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u65e5\u76ca\u589e\u957f\u7684\u7cfb\u7edf\u590d\u6742\u6027\u5bf9\u5b9e\u65f6\u5904\u7406\u548c\u5b9e\u9645\u90e8\u7f72\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7b80\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7f51\u7edc\u67b6\u6784\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u6062\u590d\u672c\u8eab\u7684\u5185\u5728\u7279\u6027\u3002

Method: \u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u91d1\u5b57\u5854\u5c0f\u6ce2-\u5085\u91cc\u53f6\u8fed\u4ee3\u7ba1\u9053\uff0c\u5e76\u63d0\u51faPW-FNet\u3002\u5176\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\u5305\u62ec\uff1a1) \u5728\u5757\u95f4\u5c42\u9762\uff0c\u96c6\u6210\u91d1\u5b57\u5854\u5c0f\u6ce2\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7ed3\u6784\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u548c\u591a\u9891\u5e26\u5206\u89e3\uff1b2) \u5728\u5757\u5185\u5c42\u9762\uff0c\u5f15\u5165\u5085\u91cc\u53f6\u53d8\u6362\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u4ee5\u5728\u4fdd\u6301\u5168\u5c40\u5efa\u6a21\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002

Result: PW-FNet\u5728\u56fe\u50cf\u53bb\u96e8\u3001\u53bb\u96e8\u6ef4\u3001\u8d85\u5206\u8fa8\u7387\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u53bb\u96fe\u3001\u53bb\u96ea\u4ee5\u53ca\u6c34\u4e0b/\u4f4e\u5149\u589e\u5f3a\u7b49\u4efb\u52a1\u4e0a\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6062\u590d\u8d28\u91cf\uff0c\u800c\u4e14\u5728\u6548\u7387\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53c2\u6570\u91cf\u3001\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u3002

Conclusion: PW-FNet\u6210\u529f\u5c55\u793a\u4e86\u5c0f\u6ce2-\u5085\u91cc\u53f6\u5904\u7406\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u57fa\u7ebf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8ba1\u7b97\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\u3002

Abstract: Natural image quality is often degraded by adverse weather conditions,
significantly impairing the performance of downstream tasks. Image restoration
has emerged as a core solution to this challenge and has been widely discussed
in the literature. Although recent transformer-based approaches have made
remarkable progress in image restoration, their increasing system complexity
poses significant challenges for real-time processing, particularly in
real-world deployment scenarios. To this end, most existing methods attempt to
simplify the self-attention mechanism, such as by channel self-attention or
state space model. However, these methods primarily focus on network
architecture while neglecting the inherent characteristics of image restoration
itself. In this context, we explore a pyramid Wavelet-Fourier iterative
pipeline to demonstrate the potential of Wavelet-Fourier processing for image
restoration. Inspired by the above findings, we propose a novel and efficient
restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).
Specifically, PW-FNet features two key design principles: 1) at the inter-block
level, integrates a pyramid wavelet-based multi-input multi-output structure to
achieve multi-scale and multi-frequency bands decomposition; and 2) at the
intra-block level, incorporates Fourier transforms as an efficient alternative
to self-attention mechanisms, effectively reducing computational complexity
while preserving global modeling capability. Extensive experiments on tasks
such as image deraining, raindrop removal, image super-resolution, motion
deblurring, image dehazing, image desnowing and underwater/low-light
enhancement demonstrate that PW-FNet not only surpasses state-of-the-art
methods in restoration quality but also achieves superior efficiency, with
significantly reduced parameter size, computational cost and inference time.

</details>


### [71] [MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training](https://arxiv.org/abs/2507.13673)
*Yuechen Xie,Haobo Jiang,Jian Yang,Yigong Zhang,Jin Xie*

Main category: cs.CV

TL;DR: MaskHOI\u662f\u4e00\u79cd\u57fa\u4e8eMAE\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u63a9\u7801\u548cSDF\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u5355\u76eeRGB\u56fe\u50cf\u8fdb\u884c3D\u624b\u7269\u4ea4\u4e92\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u4ece\u5355\u76eeRGB\u56fe\u50cf\u4f30\u8ba13D\u624b\u7269\u4ea4\u4e92\uff08HOI\uff09\u7684\u7cbe\u786e\u5173\u8282\u59ff\u6001\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8eRGB\u56fe\u50cf\u56fa\u6709\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u4ee5\u53ca\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u4e25\u91cd\u7684\u76f8\u4e92\u906e\u6321\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86MaskHOI\u6846\u67b6\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u7531Masked Autoencoder (MAE) \u9a71\u52a8\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u3002\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528MAE\u7684\u63a9\u7801-\u91cd\u5efa\u7b56\u7565\u6765\u63a8\u65ad\u7f3a\u5931\u7684\u7a7a\u95f4\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u4ece\u800c\u4fc3\u8fdb\u51e0\u4f55\u611f\u77e5\u548c\u906e\u6321\u9c81\u68d2\u7684\u8868\u793a\u5b66\u4e60\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u533a\u57df\u7279\u5b9a\u63a9\u7801\u6bd4\u7387\u5206\u914d\uff1a\u4e3a\u624b\u90e8\u533a\u57df\u5206\u914d\u8f83\u4f4e\u7684\u63a9\u7801\u6bd4\u7387\uff0c\u4e3a\u521a\u6027\u7269\u4f53\u5206\u914d\u8f83\u9ad8\u7684\u6bd4\u7387\uff0c\u4ee5\u5e73\u8861\u7279\u5f81\u5b66\u4e60\u96be\u5ea6\uff1b\u5e76\u5f15\u5165\u9aa8\u67b6\u9a71\u52a8\u7684\u624b\u90e8\u63a9\u7801\u5f15\u5bfc\uff0c\u4f18\u5148\u63a9\u76d6\u5173\u952e\u624b\u90e8\u90e8\u4f4d\uff08\u5982\u6307\u5c16\u6216\u6574\u4e2a\u624b\u6307\uff09\uff0c\u4ee5\u6a21\u62df\u771f\u5b9e\u906e\u6321\u6a21\u5f0f\u30022) \u63a9\u7801\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5b66\u4e60\u673a\u5236\uff1a\u901a\u8fc7\u81ea\u63a9\u78013D SDF\u9884\u6d4b\uff0c\u4f7f\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u80fd\u591f\u611f\u77e5\u624b\u548c\u7269\u4f53\u7684\u5168\u5c40\u51e0\u4f55\u7ed3\u6784\uff0c\u8d85\u8d8a2D\u56fe\u50cf\u5e73\u9762\uff0c\u514b\u670d\u5355\u76ee\u8f93\u5165\u7684\u5c40\u9650\u6027\u5e76\u7f13\u89e3\u81ea\u906e\u6321\u95ee\u9898\u3002

Result: \u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMaskHOI\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002

Conclusion: MaskHOI\u901a\u8fc7\u521b\u65b0\u7684MAE\u9a71\u52a8\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u533a\u57df\u7279\u5b9a\u63a9\u7801\u7b56\u7565\u548cSDF\u9a71\u52a8\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u624b\u7269\u4ea4\u4e92\u59ff\u6001\u4f30\u8ba1\u4e2d\u51e0\u4f55\u6a21\u7cca\u548c\u4e25\u91cd\u906e\u6321\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u63d0\u5347\u3002

Abstract: In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of
hands and objects from monocular RGB input remains highly challenging due to
the inherent geometric ambiguity of RGB images and the severe mutual occlusions
that occur during interaction.To address these challenges, we propose MaskHOI,
a novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI
pose estimation. Our core idea is to leverage the masking-then-reconstruction
strategy of MAE to encourage the feature encoder to infer missing spatial and
structural information, thereby facilitating geometric-aware and
occlusion-robust representation learning. Specifically, based on our
observation that human hands exhibit far greater geometric complexity than
rigid objects, conventional uniform masking fails to effectively guide the
reconstruction of fine-grained hand structures. To overcome this limitation, we
introduce a Region-specific Mask Ratio Allocation, primarily comprising the
region-specific masking assignment and the skeleton-driven hand masking
guidance. The former adaptively assigns lower masking ratios to hand regions
than to rigid objects, balancing their feature learning difficulty, while the
latter prioritizes masking critical hand parts (e.g., fingertips or entire
fingers) to realistically simulate occlusion patterns in real-world
interactions. Furthermore, to enhance the geometric awareness of the pretrained
encoder, we introduce a novel Masked Signed Distance Field (SDF)-driven
multimodal learning mechanism. Through the self-masking 3D SDF prediction, the
learned encoder is able to perceive the global geometric structure of hands and
objects beyond the 2D image plane, overcoming the inherent limitations of
monocular input and alleviating self-occlusion issues. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches.

</details>


### [72] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86CSD-VAR\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\uff08VAR\uff09\u7684\u5185\u5bb9-\u98ce\u683c\u5206\u89e3\uff08CSD\uff09\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c3a\u5ea6\u611f\u77e5\u4f18\u5316\u3001SVD\u4fee\u6b63\u548c\u589e\u5f3a\u952e\u503c\u8bb0\u5fc6\u6765\u63d0\u5347\u5185\u5bb9\u4e0e\u98ce\u683c\u7684\u5206\u79bb\u548c\u5185\u5bb9\u4fdd\u7559\uff0c\u5e76\u5f15\u5165\u4e86CSD-100\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u5185\u5bb9-\u98ce\u683c\u5206\u89e3\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6269\u6563\u6a21\u578b\uff0c\u4f46\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\uff08VAR\uff09\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c1a\u672a\u88ab\u63a2\u7d22\u7528\u4e8eCSD\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528VAR\u7684\u9010\u5c3a\u5ea6\u751f\u6210\u8fc7\u7a0b\u6765\u6539\u5584\u5185\u5bb9\u4e0e\u98ce\u683c\u7684\u89e3\u8026\uff0c\u63d0\u9ad8\u89c6\u89c9\u5408\u6210\u7684\u7075\u6d3b\u6027\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86CSD-VAR\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a1) \u5c3a\u5ea6\u611f\u77e5\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5185\u5bb9\u548c\u98ce\u683c\u8868\u793a\u7684\u89e3\u8026\uff1b2) \u57fa\u4e8eSVD\u7684\u6821\u6b63\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u5185\u5bb9\u6cc4\u9732\u5230\u98ce\u683c\u8868\u793a\u4e2d\uff1b3) \u589e\u5f3a\u578b\u952e\u503c\uff08K-V\uff09\u8bb0\u5fc6\uff0c\u4ee5\u63d0\u9ad8\u5185\u5bb9\u8eab\u4efd\u7684\u4fdd\u7559\u3002\u6b64\u5916\uff0c\u4e3a\u8be5\u4efb\u52a1\u5f15\u5165\u4e86CSD-100\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCSD-VAR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5185\u5bb9\u4fdd\u7559\u548c\u98ce\u683c\u5316\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002

Conclusion: CSD-VAR\u6210\u529f\u5730\u5c06VAR\u6a21\u578b\u5e94\u7528\u4e8e\u5185\u5bb9-\u98ce\u683c\u5206\u89e3\u4efb\u52a1\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5185\u5bb9\u4e0e\u98ce\u683c\u89e3\u8026\u548c\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u5408\u6210\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u521b\u4f5c\u7075\u6d3b\u6027\u3002

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [73] [Gaussian kernel-based motion measurement](https://arxiv.org/abs/2507.13693)
*Hongyi Liu,Haifeng Wang*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u89c6\u89c9\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u80fd\u5728\u65e0\u9700\u5927\u91cf\u53c2\u6570\u8c03\u4f18\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u3002


<details>
  <summary>Details</summary>
Motivation: \u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u5bf9\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u6d4b\u91cf\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u73b0\u6709\u89c6\u89c9\u6d4b\u91cf\u65b9\u6cd5\u5728\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\u65f6\uff0c\u8981\u4e48\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u624b\u52a8\u53c2\u6570\u8c03\u4f18\u3002

Method: \u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u9ad8\u65af\u6838\u7684\u8fd0\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u9ad8\u65af\u6838\u4f4d\u7f6e\u6765\u63d0\u53d6\u5e27\u95f4\u8fd0\u52a8\u3002\u5f15\u5165\u4e86\u8fd0\u52a8\u4e00\u81f4\u6027\uff08\u7b26\u5408\u5b9e\u9645\u7ed3\u6784\u6761\u4ef6\uff09\u548c\u8d85\u5206\u8fa8\u7387\u7ea6\u675f\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002

Result: \u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u4e14\u65e0\u9700\u4e3a\u4e0d\u540c\u6d4b\u8bd5\u6837\u672c\u8fdb\u884c\u5b9a\u5236\u5316\u7684\u53c2\u6570\u8bbe\u7f6e\u3002

Conclusion: \u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u6d4b\u91cf\u5728\u4e9a\u50cf\u7d20\u7ea7\u8fd0\u52a8\u6d4b\u91cf\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\u6216\u9700\u8981\u5927\u91cf\u8c03\u4f18\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u9c81\u68d2\u6027\u4e14\u65e0\u9700\u5b9a\u5236\u53c2\u6570\u7684\u8fd0\u52a8\u6d4b\u91cf\u3002

Abstract: The growing demand for structural health monitoring has driven increasing
interest in high-precision motion measurement, as structural information
derived from extracted motions can effectively reflect the current condition of
the structure. Among various motion measurement techniques, vision-based
methods stand out due to their low cost, easy installation, and large-scale
measurement. However, when it comes to sub-pixel-level motion measurement,
current vision-based methods either lack sufficient accuracy or require
extensive manual parameter tuning (e.g., pyramid layers, target pixels, and
filter parameters) to reach good precision. To address this issue, we developed
a novel Gaussian kernel-based motion measurement method, which can extract the
motion between different frames via tracking the location of Gaussian kernels.
The motion consistency, which fits practical structural conditions, and a
super-resolution constraint, are introduced to increase accuracy and robustness
of our method. Numerical and experimental validations show that it can
consistently reach high accuracy without customized parameter setup for
different test samples.

</details>


### [74] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: VLA-Mark\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u7684\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u534f\u8c03\u548c\u52a8\u6001\u673a\u5236\uff0c\u5728\u5d4c\u5165\u53ef\u68c0\u6d4b\u6c34\u5370\u7684\u540c\u65f6\uff0c\u6709\u6548\u4fdd\u6301\u591a\u6a21\u6001\u5185\u5bb9\uff08\u5c24\u5176\u662f\u8bed\u4e49\uff09\u7684\u4e00\u81f4\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u6587\u672c\u6c34\u5370\u65b9\u6cd5\u901a\u8fc7\u6709\u504f\u7684\u4ee4\u724c\u9009\u62e9\u548c\u9759\u6001\u7b56\u7565\uff0c\u7834\u574f\u4e86\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u8fde\u8d2f\u6027\u53d7\u635f\uff0c\u4f7f\u8bed\u4e49\u5173\u952e\u6982\u5ff5\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u65e0\u6cd5\u6ee1\u8db3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u548c\u591a\u6a21\u6001\u8fde\u8d2f\u6027\u7684\u53cc\u91cd\u9700\u6c42\u3002

Method: VLA-Mark\u901a\u8fc7\u8de8\u6a21\u6001\u534f\u8c03\u6765\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u5b83\u6574\u5408\u4e86\u591a\u5c3a\u5ea6\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u5ea6\u91cf\uff0c\u5305\u62ec\u5c40\u90e8\u8865\u4e01\u4eb2\u548c\u529b\u3001\u5168\u5c40\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4ee5\u6307\u5bfc\u6c34\u5370\u6ce8\u5165\uff0c\u4e14\u65e0\u9700\u6a21\u578b\u518d\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u71b5\u654f\u611f\u673a\u5236\u52a8\u6001\u5e73\u8861\u6c34\u5370\u5f3a\u5ea6\u548c\u8bed\u4e49\u4fdd\u7559\uff0c\u5728\u4f4e\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u9636\u6bb5\u4f18\u5148\u8003\u8651\u89c6\u89c9\u57fa\u7840\u3002

Result: \u5b9e\u9a8c\u8868\u660e\uff0cVLA-Mark\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0cPPL\u964d\u4f4e7.4%\uff0cBLEU\u63d0\u9ad826.6%\uff0c\u5e76\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u68c0\u6d4b\u7387\uff0898.8% AUC\uff09\u3002\u8be5\u6846\u67b6\u5bf9\u590d\u8ff0\u548c\u540c\u4e49\u8bcd\u66ff\u6362\u7b49\u653b\u51fb\u8868\u73b0\u51fa96.1%\u7684\u6297\u653b\u51fb\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c-\u89c6\u89c9\u4e00\u81f4\u6027\u3002

Conclusion: VLA-Mark\u4e3a\u8d28\u91cf\u4fdd\u6301\u7684\u591a\u6a21\u6001\u6c34\u5370\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5b83\u80fd\u5728\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u7684\u540c\u65f6\uff0c\u6709\u6548\u7ef4\u62a4\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u591a\u6a21\u6001\u8fde\u8d2f\u6027\u3002

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


### [75] [GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms](https://arxiv.org/abs/2507.13706)
*Ángel F. García-Fernández,Jinhao Gu,Lennart Svensson,Yuxuan Xia,Jan Krejčí,Oliver Kost,Ondřej Straka*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u7b97\u6cd5\u6027\u80fd\u8bc4\u4f30\u7684\u51c6\u5ea6\u91cf\uff0c\u5b83\u4eec\u662fGOSPA\u548cT-GOSPA\u5ea6\u91cf\u7684\u6269\u5c55\uff0c\u5177\u6709\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u4f8b\u5982\u5141\u8bb8\u5bf9\u6f0f\u68c0\u548c\u865a\u8b66\u5bf9\u8c61\u8bbe\u7f6e\u4e0d\u540c\u6210\u672c\uff0c\u4ee5\u53ca\u975e\u5bf9\u79f0\u7684\u5b9a\u4f4d\u6210\u672c\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684GOSPA\u548cT-GOSPA\u5ea6\u91cf\u5728\u60e9\u7f5a\u6f0f\u68c0\u548c\u865a\u8b66\u5bf9\u8c61\u65f6\u7f3a\u4e4f\u7075\u6d3b\u6027\uff08\u4f8b\u5982\uff0c\u5fc5\u987b\u4f7f\u7528\u76f8\u540c\u7684\u6210\u672c\uff09\uff0c\u4e14\u5b9a\u4f4d\u6210\u672c\u8981\u6c42\u5bf9\u79f0\u3002\u5728\u67d0\u4e9bMOT\u5e94\u7528\u4e2d\uff0c\u53ef\u80fd\u9700\u8981\u4e0d\u540c\u7684\u6210\u672c\u8bbe\u7f6e\u548c\u975e\u5bf9\u79f0\u7684\u5b9a\u4f4d\u6210\u672c\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6027\u80fd\u3002

Method: \u672c\u6587\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u7684\u51c6\u5ea6\u91cf\uff1a\u4e00\u79cd\u662f\u5e7f\u4e49\u6700\u4f18\u5b50\u6a21\u5f0f\u5206\u914d\uff08GOSPA\uff09\u5ea6\u91cf\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u8861\u91cf\u5bf9\u8c61\u96c6\u5408\u4e4b\u95f4\u7684\u5dee\u5f02\uff1b\u53e6\u4e00\u79cd\u662f\u8f68\u8ff9GOSPA\uff08T-GOSPA\uff09\u5ea6\u91cf\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u8861\u91cf\u8f68\u8ff9\u96c6\u5408\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u8fd9\u4e9b\u51c6\u5ea6\u91cf\u5141\u8bb8\u5bf9\u6f0f\u68c0\u548c\u865a\u8b66\u5bf9\u8c61\u65bd\u52a0\u4e0d\u540c\u7684\u60e9\u7f5a\u6210\u672c\uff0c\u5e76\u4e14\u5b9a\u4f4d\u6210\u672c\u4e0d\u518d\u8981\u6c42\u5bf9\u79f0\u3002T-GOSPA\u51c6\u5ea6\u91cf\u8fd8\u5305\u62ec\u8f68\u8ff9\u5207\u6362\u6210\u672c\u3002

Result: \u901a\u8fc7\u6a21\u62df\uff0c\u4f7f\u7528T-GOSPA\u51c6\u5ea6\u91cf\u8bc4\u4f30\u4e86\u51e0\u79cd\u8d1d\u53f6\u65afMOT\u7b97\u6cd5\u7684\u6027\u80fd\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u51c6\u5ea6\u91cf\u5728\u60e9\u7f5a\u6f0f\u68c0\u548c\u865a\u8b66\u5bf9\u8c61\u4ee5\u53ca\u5904\u7406\u5b9a\u4f4d\u6210\u672c\u65b9\u9762\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u8fd9\u5728\u67d0\u4e9b\u7279\u5b9a\u5e94\u7528\u4e2d\u7684MOT\u8bc4\u4f30\u4e2d\u53ef\u80fd\u975e\u5e38\u6709\u7528\u3002

Abstract: This paper introduces two quasi-metrics for performance assessment of
multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an
extension of the generalised optimal subpattern assignment (GOSPA) metric and
measures the discrepancy between sets of objects. The other quasi-metric is an
extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy
between sets of trajectories. Similar to the GOSPA-based metrics, these
quasi-metrics include costs for localisation error for properly detected
objects, the number of false objects and the number of missed objects. The
T-GOSPA quasi-metric also includes a track switching cost. Differently from the
GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of
penalising missed and false objects with different costs, and the localisation
costs are not required to be symmetric. These properties can be useful in MOT
evaluation in certain applications. The performance of several Bayesian MOT
algorithms is assessed with the T-GOSPA quasi-metric via simulations.

</details>


### [76] [PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement](https://arxiv.org/abs/2507.13708)
*Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,Koustava Goswami,K. J. Joseph*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoemTale Diffusion\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5\u63d0\u793a\u8bcd\u4f18\u5316\u548c\u6269\u6563\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4fee\u6539\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5bf9\u8bd7\u6b4c\u7b49\u590d\u6742\u521b\u610f\u8bed\u8a00\u7684\u89e3\u91ca\u548c\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86P4I\u8bd7\u6b4c\u6570\u636e\u96c6\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u524d\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u89e3\u91ca\u590d\u6742\u3001\u62bd\u8c61\u6216\u9ad8\u5ea6\u63cf\u8ff0\u6027\u7684\u521b\u610f\u8bed\u8a00\uff08\u7279\u522b\u662f\u8bd7\u6b4c\u4e2d\u5e38\u89c1\u7684\u5c42\u6b21\u5316\u3001\u62bd\u8c61\u548c\u53cc\u91cd\u542b\u4e49\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u4fe1\u606f\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u4e22\u5931\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86PoemTale Diffusion\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u5305\u62ec\uff1a1) \u5c06\u591a\u9636\u6bb5\u63d0\u793a\u8bcd\u4f18\u5316\u5faa\u73af\u96c6\u6210\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ee5\u589e\u5f3a\u8bd7\u6b4c\u6587\u672c\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u4fe1\u606f\u635f\u5931\uff1b2) \u4fee\u6539\u73b0\u6709\u6700\u5148\u8fdb\u6269\u6563\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528\u4e00\u81f4\u7684\u81ea\u6ce8\u610f\u529b\u6280\u672f\u751f\u6210\u591a\u5f20\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u5171\u540c\u4f20\u8fbe\u8bd7\u6b4c\u7684\u542b\u4e49\uff1b3) \u5f15\u5165\u4e86P4I\uff08PoemForImage\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b1111\u9996\u8bd7\u6b4c\uff0c\u7528\u4e8e\u652f\u6301\u8bd7\u6b4c\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002

Result: \u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5176\u5728\u8bd7\u6b4c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u80fd\u591f\u66f4\u597d\u5730\u6355\u83b7\u4fe1\u606f\uff0c\u5e76\u751f\u6210\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u56fe\u50cf\u3002

Conclusion: \u672c\u6587\u4e3a\u8bd7\u6b4c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89d2\uff0c\u901a\u8fc7\u589e\u5f3a\u4fe1\u606f\u6355\u83b7\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u521b\u610f\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002

Abstract: Recent advancements in text-to-image diffusion models have achieved
remarkable success in generating realistic and diverse visual content. A
critical factor in this process is the model's ability to accurately interpret
textual prompts. However, these models often struggle with creative
expressions, particularly those involving complex, abstract, or highly
descriptive language. In this work, we introduce a novel training-free approach
tailored to improve image generation for a unique form of creative language:
poetic verse, which frequently features layered, abstract, and dual meanings.
Our proposed PoemTale Diffusion approach aims to minimise the information that
is lost during poetic text-to-image conversion by integrating a multi stage
prompt refinement loop into Language Models to enhance the interpretability of
poetic texts. To support this, we adapt existing state-of-the-art diffusion
models by modifying their self-attention mechanisms with a consistent
self-attention technique to generate multiple consistent images, which are then
collectively used to convey the poem's meaning. Moreover, to encourage research
in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting
of 1111 poems sourced from multiple online and offline resources. We engaged a
panel of poetry experts for qualitative assessments. The results from both
human and quantitative evaluations validate the efficacy of our method and
contribute a novel perspective to poem-to-image generation with enhanced
information capture in the generated images.

</details>


### [77] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: \u4e00\u9879\u591a\u4e2d\u5fc3\u56de\u987e\u6027\u7814\u7a76\u8868\u660e\uff0c\u4e00\u79cd\u5168\u81ea\u52a8\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\u5728Cobb\u89d2\u6d4b\u91cf\u65b9\u9762\u80fd\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\uff0c\u53ef\u7528\u4e8e\u810a\u67f1\u4fa7\u5f2f\u7684\u4e34\u5e8a\u62a5\u544a\u548c\u5206\u8bca\u3002


<details>
  <summary>Details</summary>
Motivation: \u810a\u67f1\u4fa7\u5f2f\u5f71\u54cd\u9752\u5c11\u5e74\uff0c\u6cbb\u7597\u51b3\u7b56\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684Cobb\u89d2\u6d4b\u91cf\u3002\u7136\u800c\uff0c\u4eba\u5de5\u6d4b\u91cf\u8017\u65f6\u4e14\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u3002

Method: \u7814\u7a76\u5bf9\u6765\u81ea10\u5bb6\u533b\u9662\u7684103\u5f20\u7ad9\u7acb\u4f4d\u5168\u810a\u67f1\u524d\u540e\u4f4dX\u5149\u7247\u8fdb\u884c\u56de\u987e\u6027\u591a\u4e2d\u5fc3\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e00\u79cd\u5168\u81ea\u52a8\u6df1\u5ea6\u5b66\u4e60\u8f6f\u4ef6\uff08Carebot AI Bones\uff09\u3002\u4e24\u540d\u808c\u8089\u9aa8\u9abc\u653e\u5c04\u79d1\u533b\u751f\u72ec\u7acb\u6d4b\u91cf\u4f5c\u4e3a\u53c2\u8003\u3002\u901a\u8fc7Bland-Altman\u5206\u6790\u3001\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u3001\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u3001Pearson\u76f8\u5173\u7cfb\u6570\u548cCohen Kappa\u7cfb\u6570\u8bc4\u4f30AI\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u53ca\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u3002

Result: AI\u4e0e\u653e\u5c04\u79d1\u533b\u751f1\u7684MAE\u4e3a3.89\u5ea6\uff08RMSE 4.77\u5ea6\uff09\uff0c\u504f\u5dee0.70\u5ea6\uff1b\u4e0e\u653e\u5c04\u79d1\u533b\u751f2\u7684MAE\u4e3a3.90\u5ea6\uff08RMSE 5.68\u5ea6\uff09\uff0c\u504f\u5dee2.14\u5ea6\u3002AI\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684Pearson\u76f8\u5173\u7cfb\u6570\u5206\u522b\u4e3a0.906\u548c0.880\uff08\u653e\u5c04\u79d1\u533b\u751f\u95f4\u4e3a0.928\uff09\u3002\u4e25\u91cd\u7a0b\u5ea6\u5206\u7ea7\u7684Cohen Kappa\u7cfb\u6570\u5206\u522b\u4e3a0.51\u548c0.64\uff08\u653e\u5c04\u79d1\u533b\u751f\u95f4\u4e3a0.59\uff09\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u8f6f\u4ef6\u80fd\u591f\u91cd\u73b0\u4e13\u5bb6\u6c34\u5e73\u7684Cobb\u89d2\u6d4b\u91cf\u548c\u5206\u7c7b\u5206\u7ea7\uff0c\u5e76\u5728\u591a\u4e2a\u4e2d\u5fc3\u8868\u73b0\u4e00\u81f4\uff0c\u8868\u660e\u5176\u5728\u7b80\u5316\u810a\u67f1\u4fa7\u5f2f\u62a5\u544a\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5206\u8bca\u65b9\u9762\u5177\u6709\u5b9e\u7528\u6027\u3002

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [78] [Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction](https://arxiv.org/abs/2507.13719)
*Daniele Pannone,Alessia Castronovo,Maurizio Mancini,Gian Luca Foresti,Claudio Piciarelli,Rossana Gabrieli,Muhammad Yasir Bilal,Danilo Avola*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u6d41\u7a0b\uff0c\u4e13\u4e3a\u535a\u7269\u9986\u73af\u5883\u8bbe\u8ba1\uff0c\u80fd\u4ece\u5355\u5f20\u56fe\u50cf\u8bc6\u522b\u827a\u672f\u54c1\u5e76\u751f\u6210\u7cbe\u786e\u76843D\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6570\u5b57\u5185\u5bb9\uff0c\u589e\u5f3a\u535a\u7269\u9986\u53c2\u89c2\u8005\u7684\u53c2\u4e0e\u5ea6\uff0c\u89e3\u51b3\u73b0\u6709\u6280\u672f\u5728\u8bc6\u522b\u827a\u672f\u54c1\u548c\u4ece\u5355\u56fe\u751f\u62103D\u6a21\u578b\u65b9\u9762\u7684\u6311\u6218\u3002

Method: \u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff08GLPN\u7528\u4e8e\u5168\u5c40\u7ed3\u6784\uff0cDepth-Anything\u7528\u4e8e\u5c40\u90e8\u7ec6\u8282\uff09\uff0c\u751f\u6210\u4f18\u5316\u7684\u6df1\u5ea6\u56fe\uff0c\u518d\u5c06\u5176\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u70b9\u4e91\u548c\u7f51\u683c\u3002\u6280\u672f\u4e0a\u5229\u7528\u4e86\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u827a\u672f\u54c1\u4e0d\u89c4\u5219\u8f6e\u5ed3\u548c\u591a\u53d8\u7eb9\u7406\u7684\u6311\u6218\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002

Conclusion: \u8be5\u7cfb\u7edf\u662f\u4e00\u4e2a\u9ad8\u5ea6\u9c81\u68d2\u7684\u5de5\u5177\uff0c\u80fd\u6709\u6548\u5e2e\u52a9\u535a\u7269\u9986\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6570\u5b57\u5185\u5bb9\u63d0\u5347\u8bbf\u5ba2\u4f53\u9a8c\u3002

Abstract: This paper presents an innovative augmented reality pipeline tailored for
museum environments, aimed at recognizing artworks and generating accurate 3D
models from single images. By integrating two complementary pre-trained depth
estimation models, i.e., GLPN for capturing global scene structure and
Depth-Anything for detailed local reconstruction, the proposed approach
produces optimized depth maps that effectively represent complex artistic
features. These maps are then converted into high-quality point clouds and
meshes, enabling the creation of immersive AR experiences. The methodology
leverages state-of-the-art neural network architectures and advanced computer
vision techniques to overcome challenges posed by irregular contours and
variable textures in artworks. Experimental results demonstrate significant
improvements in reconstruction accuracy and visual realism, making the system a
highly robust tool for museums seeking to enhance visitor engagement through
interactive digital content.

</details>


### [79] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faEVS\uff0c\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u89c6\u9891\u5408\u6210\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5230\u56fe\u50cf(T2I)\u548c\u6587\u672c\u5230\u89c6\u9891(T2V)\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u8fd0\u52a8\u6d41\u7545\u6027\uff0c\u5e76\u5b9e\u73b0\u63a8\u7406\u901f\u5ea6\u7684\u663e\u8457\u63d0\u5347\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u6587\u672c\u5230\u89c6\u9891(T2V)\u5408\u6210\u6a21\u578b\u5728\u5b9e\u73b0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u6709\u6548\u8fd0\u52a8\u8868\u793a\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u5c06\u9884\u8bad\u7ec3\u7684T2I\u6a21\u578b\u9002\u914d\u5230\u89c6\u9891\u5e27\u4f1a\u5bfc\u81f4\u5e27\u95f4\u4e0d\u4e00\u81f4\u3001\u95ea\u70c1\u548c\u4f2a\u5f71\u95ee\u9898\u3002

Method: EVS\u5c06T2I\u548cT2V\u6a21\u578b\u7ed3\u5408\u3002\u5b83\u5229\u7528\u8bad\u7ec3\u597d\u7684\u6269\u6563\u5f0fT2I\u6a21\u578b\uff0c\u901a\u8fc7\u53bb\u566a\u6b65\u9aa4\u5c06\u4f4e\u8d28\u91cf\u89c6\u9891\u5e27\u89c6\u4e3a\u5206\u5e03\u5916\u6837\u672c\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002\u540c\u65f6\uff0c\u5b83\u91c7\u7528T2V\u9aa8\u5e72\u7f51\u7edc\u786e\u4fdd\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u5c06T2V\u7684\u65f6\u95f4\u5148\u9a8c\u5c01\u88c5\u5230T2I\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86EVS\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u89c6\u9891\u7684\u56fe\u50cf\u548c\u8fd0\u52a8\u8d28\u91cf\u5747\u5f97\u5230\u663e\u8457\u6539\u5584\u3002\u6b64\u5916\uff0cEVS\u7684\u7ec4\u5408\u8fc7\u7a0b\u4f7f\u5f97\u63a8\u7406\u65f6\u95f4\u63d0\u901f1.6\u500d\u81f34.5\u500d\u3002

Conclusion: EVS\u6210\u529f\u5730\u5229\u7528\u4e86T2I\u548cT2V\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u4e2d\u56fe\u50cf\u8d28\u91cf\u548c\u8fd0\u52a8\u6d41\u7545\u6027\u7684\u6311\u6218\uff0c\u5e76\u5e26\u6765\u4e86\u663e\u8457\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered
considerable attention for their abilities to generate videos from textual
descriptions. However, achieving both high imaging quality and effective motion
representation remains a significant challenge for these T2V models. Existing
approaches often adapt pre-trained text-to-image (T2I) models to refine video
frames, leading to issues such as flickering and artifacts due to
inconsistencies across frames. In this paper, we introduce EVS, a training-free
Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both
visual fidelity and motion smoothness of generated videos. Our approach
utilizes a well-trained diffusion-based T2I model to refine low-quality video
frames by treating them as out-of-distribution samples, effectively optimizing
them with noising and denoising steps. Meanwhile, we employ T2V backbones to
ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior
into the T2I generation process, EVS successfully leverages the strengths of
both types of models, resulting in videos of improved imaging and motion
quality. Experimental results validate the effectiveness of our approach
compared to previous approaches. Our composition process also leads to a
significant improvement of 1.6x-4.5x speedup in inference time. Source codes:
https://github.com/Tonniia/EVS.

</details>


### [80] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: \u9488\u5bf9\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u7528\u6237\u63d0\u95ee\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4ea4\u4e92\u6f84\u6e05\u7684\u91cd\u8981\u6027\u3002\u672c\u6587\u5f15\u5165\u4e86ClearVQA\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u901a\u8fc7\u4ea4\u4e92\u89e3\u51b3\u6b67\u4e49\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u7f3a\u4e4f\u57fa\u51c6\u548cVLM\u4e0d\u503e\u5411\u4e8e\u63d0\u95ee\u7684\u6311\u6218\u3002


<details>
  <summary>Details</summary>
Motivation: \u7528\u6237\u5728VQA\u4e2d\u5e38\u63d0\u51fa\u6a21\u7cca\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u6539\u5199\u95ee\u9898\u89e3\u51b3\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u4e0eVLM\u4ea4\u4e92\u4e2d\u901a\u8fc7\u53cd\u9988\u6f84\u6e05\u6b67\u4e49\u7684\u672c\u8d28\u3002\u5f53\u524d\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u7f3a\u4e4f\u8bc4\u4f30VLM\u4ea4\u4e92\u6f84\u6e05\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4ee5\u53caVLM\u503e\u5411\u4e8e\u56de\u7b54\u800c\u975e\u63d0\u95ee\u3002

Method: \u5f15\u5165\u4e86\u540d\u4e3a\u201cClearVQA\u201d\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u9488\u5bf9VQA\u4e2d\u4e09\u7c7b\u5e38\u89c1\u7684\u6b67\u4e49\uff0c\u5e76\u6db5\u76d6\u4e86\u591a\u79cdVQA\u573a\u666f\u3002

Result: \u6210\u529f\u521b\u5efa\u4e86ClearVQA\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30VLM\u5728\u4ea4\u4e92\u5f0f\u89c6\u89c9\u95ee\u7b54\u4e2d\u89e3\u51b3\u6b67\u4e49\u7684\u80fd\u529b\u3002

Conclusion: \u901a\u8fc7\u5f15\u5165ClearVQA\u57fa\u51c6\uff0c\u4e3a\u89e3\u51b3VQA\u4e2d\u4ea4\u4e92\u5f0f\u6b67\u4e49\u6f84\u6e05\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u4ece\u800c\u63a8\u52a8\u4e86VLM\u5728\u8be5\u9886\u57df\u7684\u53d1\u5c55\u548c\u8bc4\u4f30\u3002

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [81] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u3001\u6a21\u5757\u5316\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\u4e09\u5143\u7ec4\uff08\u539f\u59cb\u56fe\u50cf\u3001\u6307\u4ee4\u3001\u7f16\u8f91\u56fe\u50cf\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u540d\u4e3aNHR-Edit\u7684\u5f00\u653e\u6570\u636e\u96c6\u548cBagel-NHR-Edit\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u751f\u6210\u6a21\u578b\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u52a9\u624b\u9700\u8981\u6570\u767e\u4e07\u4e2a\u9ad8\u8d28\u91cf\u7684\u4e09\u5143\u7ec4\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u4f46\u624b\u52a8\u6316\u6398\u50cf\u7d20\u7ea7\u7cbe\u786e\u7684\u793a\u4f8b\u6781\u5176\u56f0\u96be\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u7f16\u8f91\u90fd\u5fc5\u987b\u6ee1\u8db3\u533a\u57df\u9650\u5236\u3001\u98ce\u683c\u4e00\u81f4\u6027\u3001\u7269\u7406\u5408\u7406\u6027\u548c\u89c6\u89c9\u5438\u5f15\u529b\u7b49\u8981\u6c42\uff0c\u4e14\u7f3a\u4e4f\u53ef\u9760\u7684\u81ea\u52a8\u5316\u7f16\u8f91\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u3002

Method: \u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u52a8\u3001\u6a21\u5757\u5316\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u516c\u5171\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u7ecf\u8fc7\u4efb\u52a1\u8c03\u6574\u7684Gemini\u9a8c\u8bc1\u5668\u76f4\u63a5\u8bc4\u4f30\u6307\u4ee4\u4f9d\u4ece\u6027\u548c\u7f8e\u5b66\uff0c\u65e0\u9700\u5206\u5272\u6216\u63a5\u5730\u6a21\u578b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u53cd\u6f14\uff08Inversion\uff09\u548c\u7ec4\u5408\u81ea\u4e3e\uff08compositional bootstrapping\uff09\u6280\u672f\uff0c\u5c06\u6316\u6398\u7684\u6570\u636e\u96c6\u6269\u5927\u4e86\u7ea62.2\u500d\u3002

Result: \u8be5\u65b9\u6cd5\u81ea\u52a8\u5316\u4e86\u91cd\u590d\u7684\u6807\u6ce8\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u751f\u6210\u3002\u7814\u7a76\u4eba\u5458\u53d1\u5e03\u4e86NHR-Edit\uff0c\u4e00\u4e2a\u5305\u542b35.8\u4e07\u4e2a\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5728\u6700\u5927\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u6240\u6709\u516c\u5f00\u66ff\u4ee3\u65b9\u6848\u3002\u4ed6\u4eec\u8fd8\u53d1\u5e03\u4e86Bagel-NHR-Edit\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u5fae\u8c03Bagel\u6a21\u578b\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6307\u6807\u3002

Conclusion: \u8be5\u7814\u7a76\u901a\u8fc7\u81ea\u52a8\u5316\u9ad8\u4fdd\u771f\u8bad\u7ec3\u6570\u636e\u7684\u751f\u6210\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u8d44\u6e90\u5bc6\u96c6\u578b\u56fe\u50cf\u7f16\u8f91\u7814\u7a76\u7684\u95e8\u69db\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u548c\u9ad8\u8d28\u91cf\u7684\u5f00\u653e\u8d44\u6e90\u3002

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


### [82] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7f6e\u6362\u71b5\uff08PE\uff09\u7684\u591a\u5c3a\u5ea6\u3001\u591a\u65b9\u5411\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408HOG\u548cLBP\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u5f53\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u5148\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u7279\u5f81\u5de5\u7a0b\u4ecd\u626e\u6f14\u5173\u952e\u89d2\u8272\u3002

Method: \u5c06\u7f6e\u6362\u71b5\uff08PE\uff09\u6269\u5c55\u5230\u4e8c\u7ef4\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u591a\u5c3a\u5ea6\u3001\u591a\u65b9\u5411\u7684\u71b5\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u4ee5\u6355\u6349\u56fe\u50cf\u7684\u7a7a\u95f4\u987a\u5e8f\u548c\u590d\u6742\u6027\u3002\u4e3a\u589e\u5f3a\u5224\u522b\u529b\uff0c\u6574\u5408\u4e86HOG\uff08\u6355\u6349\u5f62\u72b6\u548c\u8fb9\u7f18\uff09\u548cLBP\uff08\u7f16\u7801\u5fae\u7eb9\u7406\uff09\u3002\u6700\u7ec8\u751f\u6210780\u7ef4\u7279\u5f81\u96c6\uff0c\u5e76\u4f7f\u7528\u7f51\u683c\u641c\u7d22\u4f18\u5316\u7684\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u8fdb\u884c\u5206\u7c7b\u3002

Result: \u5728Fashion-MNIST\u3001KMNIST\u3001EMNIST\u548cCIFAR-10\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u6df1\u5ea6\u67b6\u6784\u5373\u53ef\u63d0\u4f9b\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0cPE\u4e0eHOG\u548cLBP\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u8ba1\u7b97\u6602\u8d35\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002

Conclusion: \u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u71b5\u7684\u63cf\u8ff0\u7b26\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u56fe\u50cf\u5206\u7c7b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u548c\u901a\u7528\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [83] [SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering](https://arxiv.org/abs/2507.13779)
*Durgesh Singh,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u7684\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u805a\u7c7b\u4e2d\u5fc3\u6765\u589e\u5f3a\u5176\u5728\u4f4e\u76d1\u7763\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u534a\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u901a\u8fc7\u5229\u7528\u6709\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u6570\u636e\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u805a\u7c7b\u5047\u8bbe\uff08\u540c\u7c7b\u6570\u636e\u70b9\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5e94\u805a\u5728\u4e00\u8d77\uff09\u5df2\u88ab\u8bc1\u660e\u5bf9\u6709\u9650\u76d1\u7763\u5b66\u4e60\u6709\u5229\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9690\u5f0f\u5730\u5f3a\u5236\u6267\u884c\u6b64\u5047\u8bbe\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u65b9\u5f0f\u5229\u7528\u805a\u7c7b\u5047\u8bbe\u3002

Method: \u6838\u5fc3\u65b9\u6cd5\u662f\u5f15\u5165\u4e00\u4e2a\u663e\u5f0f\u7684\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u4ee5\u5229\u7528\u76d1\u7763\u6570\u636e\u6765\u8ba1\u7b97\u805a\u7c7b\u4e2d\u5fc3\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u76f4\u63a5\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u7b56\u7565\u3002

Result: \u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u534a\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u76d1\u7763\u60c5\u51b5\u4e0b\u3002\u5b83\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u72ec\u7acb\u7684\u6a21\u578b\u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u6b63\u5219\u5316\u5668\u6765\u63d0\u5347\u6027\u80fd\u3002

Conclusion: \u901a\u8fc7\u663e\u5f0f\u5730\u5f15\u5165\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u5757\u5e76\u5229\u7528\u76d1\u7763\u6570\u636e\u8ba1\u7b97\u805a\u7c7b\u4e2d\u5fc3\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u5347\u534a\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u76d1\u7763\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u3002

Abstract: Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)
enhance the model performance by exploiting information from labeled and
unlabeled data. The clustering assumption has proven advantageous for learning
with limited supervision and states that data points belonging to the same
cluster in a high-dimensional space should be assigned to the same category.
Recent works have utilized different training mechanisms to implicitly enforce
this assumption for the SSL and UDA. In this work, we take a different approach
by explicitly involving a differentiable clustering module which is extended to
leverage the supervised data to compute its centroids. We demonstrate the
effectiveness of our straightforward end-to-end training strategy for SSL and
UDA over extensive experiments and highlight its benefits, especially in low
supervision regimes, both as a standalone model and as a regularizer for
existing approaches.

</details>


### [84] [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/abs/2507.13797)
*Huu-Phu Do,Yu-Wei Chen,Yi-Cheng Liao,Chi-Wei Hsiao,Han-Yang Wang,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: DynFaceRestore\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u5c06\u6a21\u7cca\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u65af\u6a21\u7cca\u56fe\u50cf\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u6269\u6563\u6b65\u957f\u548c\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u76f2\u4eba\u8138\u4fee\u590d\u4e2d\u4fdd\u771f\u5ea6\u548c\u8d28\u91cf\u7684\u5e73\u8861\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u76f2\u4eba\u8138\u4fee\u590d\u65b9\u6cd5\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u91c7\u6837\u6b65\u957f\u548c\u5168\u5c40\u5f15\u5bfc\u5c3a\u5ea6\uff0c\u5047\u8bbe\u9000\u5316\u662f\u5747\u5300\u7684\uff0c\u4e14\u53ef\u80fd\u5b58\u5728\u9000\u5316\u6838\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u6216\u4e0d\u8db3\u7684\u6269\u6563\uff0c\u8fdb\u800c\u9020\u6210\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u3002

Method: DynFaceRestore\u5b66\u4e60\u5c06\u4efb\u610f\u76f2\u9000\u5316\u8f93\u5165\u6620\u5c04\u5230\u9ad8\u65af\u6a21\u7cca\u56fe\u50cf\u3002\u7136\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u6a21\u7cca\u56fe\u50cf\u53ca\u5176\u9ad8\u65af\u6838\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u6a21\u7cca\u56fe\u50cf\u7684\u6269\u6563\u8d77\u59cb\u6b65\u957f\uff0c\u5e76\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5e94\u7528\u95ed\u5f0f\u5f15\u5bfc\u4ee5\u4fdd\u6301\u4fdd\u771f\u5ea6\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u6001\u5f15\u5bfc\u5c3a\u5ea6\u8c03\u8282\u5668\uff0c\u7528\u4e8e\u5728\u5c40\u90e8\u533a\u57df\u8c03\u8282\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u4ee5\u589e\u5f3a\u590d\u6742\u533a\u57df\u7684\u7ec6\u8282\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u8f6e\u5ed3\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002

Result: DynFaceRestore\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u76f2\u4eba\u8138\u4fee\u590d\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002

Conclusion: DynFaceRestore\u901a\u8fc7\u5176\u52a8\u6001\u6269\u6563\u7b56\u7565\uff0c\u6210\u529f\u5e73\u8861\u4e86\u76f2\u4eba\u8138\u4fee\u590d\u4e2d\u7684\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4fee\u590d\u6548\u679c\u3002

Abstract: Blind Face Restoration aims to recover high-fidelity, detail-rich facial
images from unknown degraded inputs, presenting significant challenges in
preserving both identity and detail. Pre-trained diffusion models have been
increasingly used as image priors to generate fine details. Still, existing
methods often use fixed diffusion sampling timesteps and a global guidance
scale, assuming uniform degradation. This limitation and potentially imperfect
degradation kernel estimation frequently lead to under- or over-diffusion,
resulting in an imbalance between fidelity and quality. We propose
DynFaceRestore, a novel blind face restoration approach that learns to map any
blindly degraded input to Gaussian blurry images. By leveraging these blurry
images and their respective Gaussian kernels, we dynamically select the
starting timesteps for each blurry image and apply closed-form guidance during
the diffusion sampling process to maintain fidelity. Additionally, we introduce
a dynamic guidance scaling adjuster that modulates the guidance strength across
local regions, enhancing detail generation in complex areas while preserving
structural fidelity in contours. This strategy effectively balances the
trade-off between fidelity and quality. DynFaceRestore achieves
state-of-the-art performance in both quantitative and qualitative evaluations,
demonstrating robustness and effectiveness in blind face restoration.

</details>


### [85] [GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation](https://arxiv.org/abs/2507.13803)
*Weiqi Yang,Xu Zhou,Jingfu Guan,Hao Du,Tianyu Bai*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faGRAM-MAMBA\u6846\u67b6\uff0c\u5229\u7528Mamba\u6a21\u578b\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408GRAM\u77e9\u9635\u8fdb\u884c\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4f4e\u79e9\u8865\u507f\u7b56\u7565\u5904\u7406\u7f3a\u5931\u6a21\u6001\uff0c\u65e8\u5728\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650IoT\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u9c81\u68d2\u591a\u6a21\u6001\u611f\u77e5\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709IoT\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u9762\u4e34\u9ad8\u6a21\u578b\u590d\u6742\u5ea6\u96be\u4ee5\u90e8\u7f72\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3001\u5355\u5411\u6a21\u6001\u5bf9\u9f50\u5ffd\u7565\u6a21\u6001\u95f4\u5173\u7cfb\u3001\u4ee5\u53ca\u4f20\u611f\u5668\u6570\u636e\u7f3a\u5931\u65f6\u9c81\u68d2\u6027\u5dee\u7b49\u6311\u6218\u3002

Method: GRAM-MAMBA\u6846\u67b6\uff1a1. \u91c7\u7528\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u6a21\u578b\u9ad8\u6548\u5904\u7406\u4f20\u611f\u5668\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u30022. \u5f15\u5165\u4f18\u5316\u7684GRAM\u77e9\u9635\u7b56\u7565\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u4e24\u4e24\u5bf9\u9f50\u30023. \u501f\u9274LoRA\u601d\u60f3\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u4f4e\u79e9\u5c42\u8865\u507f\u7b56\u7565\uff0c\u5728\u6a21\u578b\u9884\u8bad\u7ec3\u540e\u5904\u7406\u7f3a\u5931\u6a21\u6001\uff0c\u901a\u8fc7\u51bb\u7ed3\u6838\u5fc3\u548c\u4e0d\u76f8\u5173\u5c42\uff0c\u4ec5\u5fae\u8c03\u4e0e\u53ef\u7528\u6a21\u6001\u53ca\u878d\u5408\u8fc7\u7a0b\u76f8\u5173\u7684\u5c42\u3002

Result: \u5728SPAWC2021\u5ba4\u5185\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u8bef\u5dee\u4f4e\u4e8e\u57fa\u7ebf\uff0c\u9002\u5e94\u7f3a\u5931\u6a21\u6001\u65f6\u6027\u80fd\u63d0\u534724.5%\uff0c\u4e14\u4ec5\u8bad\u7ec3\u4e0d\u52300.2%\u7684\u53c2\u6570\u3002\u5728USC-HAD\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u8fbe93.55%\uff0c\u603b\u51c6\u786e\u7387(OA)\u8fbe93.81%\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u4f5c\uff1b\u66f4\u65b0\u7b56\u7565\u4f7fF1\u5206\u6570\u63d0\u9ad823%\uff0c\u8bad\u7ec3\u53c2\u6570\u4e0d\u52300.3%\u3002

Conclusion: GRAM-MAMBA\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u5c55\u73b0\u51fa\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u591a\u6a21\u6001\u611f\u77e5\u7684\u5de8\u5927\u6f5c\u529b\u3002

Abstract: Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely
deployed in smart homes, intelligent transport, industrial automation, and
healthcare. However, existing systems often face challenges: high model
complexity hinders deployment in resource-constrained environments,
unidirectional modal alignment neglects inter-modal relationships, and
robustness suffers when sensor data is missing. These issues impede efficient
and robust multimodal perception in real-world IoT settings. To overcome these
limitations, we propose GRAM-MAMBA. This framework utilizes the
linear-complexity Mamba model for efficient sensor time-series processing,
combined with an optimized GRAM matrix strategy for pairwise alignment among
modalities, addressing the shortcomings of traditional single-modality
alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive
low-rank layer compensation strategy to handle missing modalities
post-training. This strategy freezes the pre-trained model core and irrelevant
adaptive layers, fine-tuning only those related to available modalities and the
fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On
the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower
error than baselines; adapting to missing modalities yields a 24.5% performance
boost by training less than 0.2% of parameters. On the USC-HAD human activity
recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),
outperforming prior work; the update strategy increases F1 by 23% while
training less than 0.3% of parameters. These results highlight GRAM-MAMBA's
potential for achieving efficient and robust multimodal perception in
resource-constrained environments.

</details>


### [86] [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/abs/2507.13812)
*Yingying Zhang,Lixiang Ru,Kang Wu,Lei Yu,Lei Liang,Yansheng Li,Jingdong Chen*

Main category: cs.CV

TL;DR: SkySense V2\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u9065\u611f\u57fa\u7840\u6a21\u578b\uff08MM-RSFM\uff09\uff0c\u5b83\u91c7\u7528\u5355\u4e00Transformer\u9aa8\u5e72\u7f51\u7edc\u548c\u4e13\u95e8\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5197\u4f59\u548c\u9065\u611f\u6570\u636e\u7279\u6027\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709MM-RSFM\u901a\u5e38\u4e3a\u6bcf\u79cd\u6a21\u6001\u8bad\u7ec3\u72ec\u7acb\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u53c2\u6570\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002\u6b64\u5916\uff0c\u4e3b\u6d41\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u81ea\u7136\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u672a\u80fd\u5145\u5206\u9002\u5e94\u9065\u611f\u56fe\u50cf\u590d\u6742\u7684\u8bed\u4e49\u5206\u5e03\u7279\u5f81\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86SkySense V2\uff0c\u4e00\u4e2a\u91c7\u7528\u5355\u4e00Transformer\u9aa8\u5e72\u7f51\u7edc\u7684\u7edf\u4e00MM-RSFM\u3002\u8be5\u9aa8\u5e72\u7f51\u7edc\u901a\u8fc7\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u6570\u636e\u7279\u6027\u8bbe\u8ba1\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5f15\u5165\u521b\u65b0\u7684\u81ea\u9002\u5e94\u8865\u4e01\u5408\u5e76\u6a21\u5757\u3001\u53ef\u5b66\u4e60\u7684\u6a21\u6001\u63d0\u793atokens\u4ee5\u5904\u7406\u4e0d\u540c\u5206\u8fa8\u7387\u548c\u6709\u9650\u7279\u5f81\u591a\u6837\u6027\uff0c\u5e76\u6574\u5408\u4e86\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6a21\u5757\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002

Result: SkySense V2\u5728\u6d89\u53ca7\u9879\u4efb\u52a1\u768416\u4e2a\u6570\u636e\u96c6\u4e2d\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u6027\u80fd\u6bd4SkySense\u63d0\u5347\u4e861.8\u4e2a\u70b9\u3002

Conclusion: SkySense V2\u901a\u8fc7\u5176\u7edf\u4e00\u7684\u9aa8\u5e72\u7f51\u7edc\u3001\u521b\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u4ee5\u53ca\u5bf9\u9065\u611f\u6570\u636e\u7279\u6027\u7684\u9002\u5e94\u6027\u8bbe\u8ba1\uff08\u5982\u81ea\u9002\u5e94\u8865\u4e01\u5408\u5e76\u548c\u6a21\u6001\u63d0\u793a\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u9065\u611f\u57fa\u7840\u6a21\u578b\u5728\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002

Abstract: The multi-modal remote sensing foundation model (MM-RSFM) has significantly
advanced various Earth observation tasks, such as urban planning, environmental
monitoring, and natural disaster management. However, most existing approaches
generally require the training of separate backbone networks for each data
modality, leading to redundancy and inefficient parameter utilization.
Moreover, prevalent pre-training methods typically apply self-supervised
learning (SSL) techniques from natural images without adequately accommodating
the characteristics of remote sensing (RS) images, such as the complicated
semantic distribution within a single RS image. In this work, we present
SkySense V2, a unified MM-RSFM that employs a single transformer backbone to
handle multiple modalities. This backbone is pre-trained with a novel SSL
strategy tailored to the distinct traits of RS data. In particular, SkySense V2
incorporates an innovative adaptive patch merging module and learnable modality
prompt tokens to address challenges related to varying resolutions and limited
feature diversity across modalities. In additional, we incorporate the mixture
of experts (MoE) module to further enhance the performance of the foundation
model. SkySense V2 demonstrates impressive generalization abilities through an
extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense
by an average of 1.8 points.

</details>


### [87] [PositionIC: Unified Position and Identity Consistency for Image Customization](https://arxiv.org/abs/2507.13861)
*Junjie Hu,Tianyang Han,Kai Ma,Jialin Gao,Hao Dou,Song Yang,Xianhua He,Jianhui Zhang,Junfeng Luo,Xiaoming Wei,Wenqiang Zhang*

Main category: cs.CV

TL;DR: PositionIC\u662f\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u548c\u8f7b\u91cf\u7ea7\u4f4d\u7f6e\u8c03\u5236\u5c42\uff0c\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u56fe\u50cf\u5b9a\u5236\u4e2d\u7cbe\u7ec6\u7a7a\u95f4\u63a7\u5236\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u96be\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u4e3b\u4f53\u9a71\u52a8\u56fe\u50cf\u5b9a\u5236\u5728\u4fdd\u771f\u5ea6\u65b9\u9762\u8fdb\u5c55\u663e\u8457\uff0c\u4f46\u7f3a\u4e4f\u7cbe\u7ec6\u7684\u5b9e\u4f53\u7ea7\u7a7a\u95f4\u63a7\u5236\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u7f3a\u5c11\u5c06\u8eab\u4efd\u4e0e\u7cbe\u786e\u4f4d\u7f6e\u7ebf\u7d22\u7ed1\u5b9a\u7684\u53ef\u6269\u5c55\u6570\u636e\u96c6\uff0c\u4ece\u800c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002

Method: \u5f15\u5165PositionIC\u6846\u67b6\u4ee5\u5f3a\u5236\u6267\u884c\u4f4d\u7f6e\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff1b\u6784\u5efa\u4e00\u4e2a\u91c7\u7528\u53cc\u5411\u751f\u6210\u8303\u5f0f\u7684\u53ef\u6269\u5c55\u5408\u6210\u7ba1\u9053\uff0c\u4ee5\u6d88\u9664\u4e3b\u4f53\u6f02\u79fb\u5e76\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\uff1b\u8bbe\u8ba1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4f4d\u7f6e\u8c03\u5236\u5c42\uff0c\u7528\u4e8e\u89e3\u8026\u4e3b\u4f53\u95f4\u7684\u7a7a\u95f4\u5d4c\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u72ec\u7acb\u3001\u7cbe\u786e\u7684\u653e\u7f6e\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002

Result: \u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5b9a\u5236\u4efb\u52a1\u4e2d\u80fd\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002

Conclusion: PositionIC\u4e3a\u5f00\u653e\u4e16\u754c\u3001\u591a\u5b9e\u4f53\u573a\u666f\u4e2d\u7684\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u56fe\u50cf\u5b9a\u5236\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u5c06\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002

Abstract: Recent subject-driven image customization has achieved significant
advancements in fidelity, yet fine-grained entity-level spatial control remains
elusive, hindering the broader real-world application. This limitation is
mainly attributed to scalable datasets that bind identity with precise
positional cues are absent. To this end, we introduce PositionIC, a unified
framework that enforces position and identity consistency for multi-subject
customization. We construct a scalable synthesis pipeline that employs a
bidirectional generation paradigm to eliminate subject drift and maintain
semantic coherence. On top of these data, we design a lightweight positional
modulation layer that decouples spatial embeddings among subjects, enabling
independent, accurate placement while preserving visual fidelity. Extensive
experiments demonstrate that our approach can achieve precise spatial control
while maintaining high consistency in image customization task. PositionIC
paves the way for controllable, high-fidelity image customization in
open-world, multi-entity scenarios and will be released to foster further
research.

</details>


### [88] [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/abs/2507.13891)
*Yu Wei,Jiahui Zhang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: \u9488\u5bf9COLMAP-free 3D Gaussian Splatting\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faPCR-GS\uff0c\u901a\u8fc7\u7279\u5f81\u91cd\u6295\u5f71\u6b63\u5219\u5316\u548c\u57fa\u4e8e\u5c0f\u6ce2\u7684\u9891\u7387\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u5efa\u6a21\u548c\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u8d28\u91cf\u3002


<details>
  <summary>Details</summary>
Motivation: COLMAP-free 3D Gaussian Splatting\uff083D-GS\uff09\u5728\u5904\u7406\u5177\u6709\u5267\u70c8\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u9000\u5316\uff0c\u5e76\u5728\u4f4d\u59ff\u4e0e3D-GS\u8054\u5408\u4f18\u5316\u4e2d\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002

Method: \u672c\u6587\u63d0\u51faPCR-GS\uff0c\u4e00\u79cd\u521b\u65b0\u7684COLMAP-free 3DGS\u6280\u672f\uff0c\u901a\u8fc7\u76f8\u673a\u4f4d\u59ff\u534f\u540c\u6b63\u5219\u5316\u5b9e\u73b0\u5353\u8d8a\u76843D\u573a\u666f\u5efa\u6a21\u548c\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u4ece\u4e24\u4e2a\u65b9\u9762\u8fdb\u884c\u6b63\u5219\u5316\uff1a1) \u7279\u5f81\u91cd\u6295\u5f71\u6b63\u5219\u5316\uff1a\u4ece\u76f8\u90bb\u89c6\u56fe\u4e2d\u63d0\u53d6\u89c6\u56fe\u9c81\u68d2\u7684DINO\u7279\u5f81\u5e76\u5bf9\u9f50\u5176\u8bed\u4e49\u4fe1\u606f\u4ee5\u6b63\u5219\u5316\u76f8\u673a\u4f4d\u59ff\u30022) \u57fa\u4e8e\u5c0f\u6ce2\u7684\u9891\u7387\u6b63\u5219\u5316\uff1a\u5229\u7528\u9ad8\u9891\u7ec6\u8282\u5dee\u5f02\u8fdb\u4e00\u6b65\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u4e2d\u7684\u65cb\u8f6c\u77e9\u9635\u3002

Result: \u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684PCR-GS\u5728\u76f8\u673a\u8f68\u8ff9\u5267\u70c8\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u65e0\u4f4d\u59ff3D-GS\u573a\u666f\u5efa\u6a21\u3002

Conclusion: PCR-GS\u901a\u8fc7\u5f15\u5165\u76f8\u673a\u4f4d\u59ff\u534f\u540c\u6b63\u5219\u5316\uff08\u5305\u62ec\u7279\u5f81\u91cd\u6295\u5f71\u548c\u57fa\u4e8e\u5c0f\u6ce2\u7684\u9891\u7387\u6b63\u5219\u5316\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86COLMAP-free 3D-GS\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u91cd\u5efa\u548c\u4f4d\u59ff\u4f30\u8ba1\u7684\u6027\u80fd\u3002

Abstract: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing
attention due to its remarkable performance in reconstructing high-quality 3D
scenes from unposed images or videos. However, it often struggles to handle
scenes with complex camera trajectories as featured by drastic rotation and
translation across adjacent camera views, leading to degraded estimation of
camera poses and further local minima in joint optimization of camera poses and
3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that
achieves superior 3D scene modeling and camera pose estimation via camera pose
co-regularization. PCR-GS achieves regularization from two perspectives. The
first is feature reprojection regularization which extracts view-robust DINO
features from adjacent camera views and aligns their semantic information for
camera pose regularization. The second is wavelet-based frequency
regularization which exploits discrepancy in high-frequency details to further
optimize the rotation matrix in camera poses. Extensive experiments over
multiple real-world scenes show that the proposed PCR-GS achieves superior
pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.

</details>


### [89] [Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection](https://arxiv.org/abs/2507.13899)
*Yujian Mo,Yan Wu,Junqiao Zhao,Jijun Wang,Yinghao Hu,Jun Yan*

Main category: cs.CV

TL;DR: \u8be5\u8bba\u6587\u901a\u8fc7\u878d\u5408DepthAnything\u751f\u6210\u7684\u6df1\u5ea6\u5148\u9a8c\u4fe1\u606f\u6765\u589e\u5f3a\u6fc0\u5149\u96f7\u8fbe\u70b9\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u53cc\u8def\u5f84RoI\u7279\u5f81\u63d0\u53d6\u4e0e\u53cc\u5411\u95e8\u63a7\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6fc0\u5149\u96f7\u8fbe3D\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u7a00\u758f\uff0c\u539f\u59cb\u70b9\u7279\u5f81\uff08\u7279\u522b\u662f\u53cd\u5c04\u7387\uff09\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u5224\u522b\u6027\u5f31\u3002\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5982DepthAnything\u80fd\u63d0\u4f9b\u5bc6\u96c6\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u4f46\u8fd9\u4e9b\u89c6\u89c9\u5148\u9a8c\u5728\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u76843D\u76ee\u6807\u68c0\u6d4b\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002

Method: 1. \u5f15\u5165DepthAnything\u9884\u6d4b\u7684\u6df1\u5ea6\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u4e0e\u539f\u59cb\u6fc0\u5149\u96f7\u8fbe\u5c5e\u6027\u878d\u5408\u4ee5\u4e30\u5bcc\u6bcf\u4e2a\u70b9\u7684\u8868\u793a\u30022. \u63d0\u51fa\u4e00\u4e2a\u70b9\u7ea7\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u6765\u5229\u7528\u589e\u5f3a\u540e\u7684\u70b9\u7279\u5f81\u30023. \u91c7\u7528\u53cc\u8def\u5f84RoI\u7279\u5f81\u63d0\u53d6\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u4f53\u7d20\u7684\u5206\u652f\u7528\u4e8e\u5168\u5c40\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u4e00\u4e2a\u57fa\u4e8e\u70b9\u7684\u5206\u652f\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7ed3\u6784\u7ec6\u8282\u30024. \u5f15\u5165\u53cc\u5411\u95e8\u63a7RoI\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u6709\u6548\u6574\u5408\u4e92\u8865\u7684RoI\u7279\u5f81\uff0c\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u3002

Result: \u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002

Conclusion: \u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DepthAnything\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u76843D\u76ee\u6807\u68c0\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002

Abstract: Recent advances in foundation models have opened up new possibilities for
enhancing 3D perception. In particular, DepthAnything offers dense and reliable
geometric priors from monocular RGB images, which can complement sparse LiDAR
data in autonomous driving scenarios. However, such priors remain underutilized
in LiDAR-based 3D object detection. In this paper, we address the limited
expressiveness of raw LiDAR point features, especially the weak discriminative
capability of the reflectance attribute, by introducing depth priors predicted
by DepthAnything. These priors are fused with the original LiDAR attributes to
enrich each point's representation. To leverage the enhanced point features, we
propose a point-wise feature extraction module. Then, a Dual-Path RoI feature
extraction framework is employed, comprising a voxel-based branch for global
semantic context and a point-based branch for fine-grained structural details.
To effectively integrate the complementary RoI features, we introduce a
bidirectional gated RoI feature fusion module that balances global and local
cues. Extensive experiments on the KITTI benchmark show that our method
consistently improves detection accuracy, demonstrating the value of
incorporating visual foundation model priors into LiDAR-based 3D object
detection.

</details>


### [90] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF\u662f\u4e00\u79cd\u53ef\u6cdb\u5316\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u8f93\u5165\u89c6\u56fe\u4e0b\uff0c\u6e32\u67d3\u4efb\u610f\u89c6\u70b9\u548c\u4efb\u610f\u65f6\u95f4\u7684\u65b0\u9896\u89c6\u56fe\uff0c\u5c24\u5176\u64c5\u957f\u6a21\u62df\u81ea\u7136\u573a\u666f\u7684\u663c\u591c\u53d8\u5316\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709NeRF\u6280\u672f\u5728\u65f6\u95f43D\u573a\u666f\u5efa\u6a21\u65b9\u9762\u63a2\u7d22\u6709\u9650\uff0c\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\uff1b\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\uff0c\u591a\u89c6\u56fe\u6570\u636e\u91c7\u96c6\u6602\u8d35\uff0c\u4e14\u9488\u5bf9\u672a\u89c1\u573a\u666f\u7684\u91cd\u65b0\u4f18\u5316\u6548\u7387\u4f4e\u4e0b\uff1b\u5143\u5b87\u5b99\u7b49\u6570\u5b57\u9886\u57df\u5bf9\u81ea\u7136\u663c\u591c\u8fc7\u6e21\u7684\u6c89\u6d78\u5f0f3D\u73af\u5883\u5efa\u6a21\u9700\u6c42\u8feb\u5207\u3002

Method: \u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u89c6\u56fe\u7acb\u4f53\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u8de8\u4e0d\u540c\u6570\u636e\u96c6\u7684\u89e3\u8026\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6784\u5efa\u4e86\u9690\u5f0f\u5185\u5bb9\u8f90\u5c04\u573a\u8fdb\u884c\u573a\u666f\u8868\u793a\uff0c\u5e76\u80fd\u5728\u4efb\u610f\u65f6\u95f4\u6784\u5efa\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u6700\u7ec8\u901a\u8fc7\u4f53\u6e32\u67d3\u5408\u6210\u65b0\u9896\u89c6\u56fe\u3002

Result: TimeNeRF\u80fd\u591f\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u6e32\u67d3\u65b0\u9896\u89c6\u56fe\uff0c\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u3002\u6700\u663e\u8457\u7684\u662f\uff0c\u5b83\u80fd\u521b\u5efa\u5728\u4e0d\u540c\u65f6\u95f4\u4e4b\u95f4\u5e73\u6ed1\u8fc7\u6e21\u7684\u903c\u771f\u65b0\u9896\u89c6\u56fe\uff0c\u7cbe\u51c6\u6355\u6349\u4ece\u9ece\u660e\u5230\u9ec4\u660f\u7684\u590d\u6742\u81ea\u7136\u573a\u666f\u53d8\u5316\u3002

Conclusion: TimeNeRF\u6210\u529f\u89e3\u51b3\u4e86NeRF\u5728\u65f6\u95f43D\u573a\u666f\u5efa\u6a21\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u6e32\u67d3\u4efb\u610f\u89c6\u70b9\u548c\u4efb\u610f\u65f6\u95f4\u7684\u65b0\u9896\u89c6\u56fe\uff0c\u5e76\u7279\u522b\u64c5\u957f\u6355\u6349\u81ea\u7136\u573a\u666f\u7684\u663c\u591c\u8fc7\u6e21\u3002

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

</details>


### [91] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: DiViD\u662f\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u5730\u89e3\u8026\u89c6\u9891\u4e2d\u7684\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u89c6\u9891\u4e2d\u65e0\u76d1\u7763\u5730\u89e3\u8026\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8eVAE\u548cGAN\u7684\u65b9\u6cd5\u5e38\u5e38\u53d7\u5230\u4fe1\u606f\u6cc4\u6f0f\u548c\u6a21\u7cca\u91cd\u5efa\u7684\u56f0\u6270\u3002

Method: DiViD\u662f\u9996\u4e2a\u7528\u4e8e\u663e\u5f0f\u9759\u6001-\u52a8\u6001\u5206\u89e3\u7684\u7aef\u5230\u7aef\u89c6\u9891\u6269\u6563\u6846\u67b6\u3002\u5176\u5e8f\u5217\u7f16\u7801\u5668\u4ece\u7b2c\u4e00\u5e27\u63d0\u53d6\u5168\u5c40\u9759\u6001token\u548c\u6bcf\u5e27\u52a8\u6001token\uff0c\u5e76\u663e\u5f0f\u5730\u4ece\u8fd0\u52a8\u7f16\u7801\u4e2d\u79fb\u9664\u9759\u6001\u5185\u5bb9\u3002\u5176\u6761\u4ef6DDPM\u89e3\u7801\u5668\u5305\u542b\u4e09\u4e2a\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff1a\u7528\u4e8e\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5171\u4eab\u566a\u58f0\u8c03\u5ea6\u3001\u5728\u65e9\u671f\u65f6\u95f4\u6b65\u6536\u7d27\uff08\u538b\u7f29\u9759\u6001\u4fe1\u606f\uff09\u5e76\u5728\u540e\u671f\u653e\u677e\uff08\u4e30\u5bcc\u52a8\u6001\u4fe1\u606f\uff09\u7684\u65f6\u53d8KL\u74f6\u9888\uff0c\u4ee5\u53ca\u5c06\u5168\u5c40\u9759\u6001token\u8def\u7531\u5230\u6240\u6709\u5e27\u540c\u65f6\u4fdd\u6301\u52a8\u6001token\u5e27\u7279\u5f02\u6027\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u6b63\u4ea4\u6027\u6b63\u5219\u5316\u5668\u8fdb\u4e00\u6b65\u9632\u6b62\u6b8b\u4f59\u7684\u9759\u6001-\u52a8\u6001\u6cc4\u6f0f\u3002

Result: DiViD\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8e\u4ea4\u6362\u7684\u51c6\u786e\u6027\u548c\u4ea4\u53c9\u6cc4\u6f0f\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002\u5b83\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5e8f\u5217\u89e3\u8026\u65b9\u6cd5\uff1a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u57fa\u4e8e\u4ea4\u6362\u7684\u8054\u5408\u51c6\u786e\u6027\uff0c\u5728\u63d0\u9ad8\u52a8\u6001\u8fc1\u79fb\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9759\u6001\u4fdd\u771f\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e86\u5e73\u5747\u4ea4\u53c9\u6cc4\u6f0f\u3002

Conclusion: DiViD\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u89c6\u9891\u4e2d\u9759\u6001\u5916\u89c2\u548c\u52a8\u6001\u8fd0\u52a8\u7684\u65e0\u76d1\u7763\u89e3\u8026\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u51c6\u786e\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u6cc4\u6f0f\u51cf\u5c11\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video
remains a fundamental challenge, often hindered by information leakage and
blurry reconstructions in existing VAE- and GAN-based approaches. We introduce
DiViD, the first end-to-end video diffusion framework for explicit
static-dynamic factorization. DiViD's sequence encoder extracts a global static
token from the first frame and per-frame dynamic tokens, explicitly removing
static content from the motion code. Its conditional DDPM decoder incorporates
three key inductive biases: a shared-noise schedule for temporal consistency, a
time-varying KL-based bottleneck that tightens at early timesteps (compressing
static information) and relaxes later (enriching dynamics), and cross-attention
that routes the global static token to all frames while keeping dynamic tokens
frame-specific. An orthogonality regularizer further prevents residual
static-dynamic leakage. We evaluate DiViD on real-world benchmarks using
swap-based accuracy and cross-leakage metrics. DiViD outperforms
state-of-the-art sequential disentanglement methods: it achieves the highest
swap-based joint accuracy, preserves static fidelity while improving dynamic
transfer, and reduces average cross-leakage.

</details>


### [92] [Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset](https://arxiv.org/abs/2507.13981)
*Sara Abdulaziz,Giacomo D'Amicantonio,Egor Bondarev*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u548cHR-VISPR\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002


<details>
  <summary>Details</summary>
Motivation: \u968f\u7740AI\u9a71\u52a8\u7684\u76d1\u63a7\u6280\u672f\u53d1\u5c55\uff0c\u654f\u611f\u4e2a\u4eba\u6570\u636e\u6536\u96c6\u5f15\u53d1\u4e86\u62c5\u5fe7\u3002\u7814\u7a76\u9700\u8981\u5ba2\u89c2\u7684\u6280\u672f\u6765\u8bc4\u4f30\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u201c\u8bbe\u8ba1\u9690\u79c1\u201d\u7684\u80cc\u666f\u4e0b\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u4ece\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86HR-VISPR\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u751f\u7269\u8bc6\u522b\u3001\u8f6f\u751f\u7269\u8bc6\u522b\u548c\u975e\u751f\u7269\u8bc6\u522b\u6807\u7b7e\uff0c\u7528\u4e8e\u8bad\u7ec3\u53ef\u89e3\u91ca\u7684\u9690\u79c1\u5ea6\u91cf\u3002\u8be5\u6846\u67b6\u88ab\u7528\u4e8e\u8bc4\u4f3011\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u6280\u672f\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002

Result: \u8be5\u6846\u67b6\u80fd\u591f\u6839\u636e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u533a\u5206\u9690\u79c1\u7ea7\u522b\uff0c\u5e76\u63ed\u793a\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u672c\u7814\u7a76\u4ee5\u53caHR-VISPR\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u7ed3\u6784\u5316\u6846\u67b6\u3002

Conclusion: \u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6d1e\u5bdf\u529b\u5f3a\u7684\u5de5\u5177\u548c\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u80cc\u666f\u4e0b\u7684\u89c6\u89c9\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u8bc4\u4f30\uff0c\u5e76\u5f3a\u8c03\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002

Abstract: Recent advances in AI-powered surveillance have intensified concerns over the
collection and processing of sensitive personal data. In response, research has
increasingly focused on privacy-by-design solutions, raising the need for
objective techniques to evaluate privacy protection. This paper presents a
comprehensive framework for evaluating visual privacy-protection methods across
three dimensions: privacy, utility, and practicality. In addition, it
introduces HR-VISPR, a publicly available human-centric dataset with biometric,
soft-biometric, and non-biometric labels to train an interpretable privacy
metric. We evaluate 11 privacy protection methods, ranging from conventional
techniques to advanced deep-learning methods, through the proposed framework.
The framework differentiates privacy levels in alignment with human visual
perception, while highlighting trade-offs between privacy, utility, and
practicality. This study, along with the HR-VISPR dataset, serves as an
insightful tool and offers a structured evaluation framework applicable across
diverse contexts.

</details>


### [93] [DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation](https://arxiv.org/abs/2507.13985)
*Haoran Li,Yuli Tian,Kun Lan,Yong Liao,Lin Wang,Pan Hui,Peng Yuan Zhou*

Main category: cs.CV

TL;DR: DreamScene\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6587\u672c\u6216\u5bf9\u8bdd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u7f16\u8f91\u76843D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u30013D\u4e00\u81f4\u6027\u548c\u7cbe\u7ec6\u63a7\u5236\u65b9\u9762\u7684\u6311\u6218\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u62103D\u573a\u666f\u65f6\uff0c\u9762\u4e34\u81ea\u52a8\u5316\u4e0d\u8db3\u30013D\u4e00\u81f4\u6027\u5dee\u548c\u7cbe\u7ec6\u63a7\u5236\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u6e38\u620f\u3001\u7535\u5f71\u548c\u8bbe\u8ba1\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002

Method: DreamScene\u9996\u5148\u901a\u8fc7\u4e00\u4e2a\u573a\u666f\u89c4\u5212\u6a21\u5757\uff0c\u5229\u7528GPT-4\u63a8\u65ad\u5bf9\u8c61\u8bed\u4e49\u548c\u7a7a\u95f4\u7ea6\u675f\uff0c\u6784\u5efa\u6df7\u5408\u56fe\u3002\u63a5\u7740\uff0c\u57fa\u4e8e\u56fe\u7684\u653e\u7f6e\u7b97\u6cd5\u751f\u6210\u65e0\u78b0\u649e\u7684\u7ed3\u6784\u5316\u5e03\u5c40\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u5f62\u6210\u6a21\u5f0f\u91c7\u6837\uff08FPS\uff09\u901a\u8fc7\u591a\u65f6\u95f4\u6b65\u91c7\u6837\u548c\u91cd\u5efa\u4f18\u5316\u751f\u6210\u5bf9\u8c61\u51e0\u4f55\u3002\u4e3a\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u7cfb\u7edf\u91c7\u7528\u6e10\u8fdb\u5f0f\u76f8\u673a\u91c7\u6837\u7b56\u7565\u3002\u6700\u540e\uff0c\u652f\u6301\u5305\u62ec\u5bf9\u8c61\u79fb\u52a8\u3001\u5916\u89c2\u53d8\u5316\u548c4D\u52a8\u6001\u8fd0\u52a8\u5728\u5185\u7684\u7cbe\u7ec6\u573a\u666f\u7f16\u8f91\u3002

Result: \u5b9e\u9a8c\u8bc1\u660e\uff0cDreamScene\u5728\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002

Conclusion: DreamScene\u4e3a\u5f00\u653e\u57df3D\u5185\u5bb9\u521b\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9ad8\u8d28\u91cf\u3001\u9ad8\u4e00\u81f4\u6027\u548c\u7075\u6d3b\u53ef\u7f16\u8f91\u7684\u7279\u70b9\u3002

Abstract: Generating 3D scenes from natural language holds great promise for
applications in gaming, film, and design. However, existing methods struggle
with automation, 3D consistency, and fine-grained control. We present
DreamScene, an end-to-end framework for high-quality and editable 3D scene
generation from text or dialogue. DreamScene begins with a scene planning
module, where a GPT-4 agent infers object semantics and spatial constraints to
construct a hybrid graph. A graph-based placement algorithm then produces a
structured, collision-free layout. Based on this layout, Formation Pattern
Sampling (FPS) generates object geometry using multi-timestep sampling and
reconstructive optimization, enabling fast and realistic synthesis. To ensure
global consistent, DreamScene employs a progressive camera sampling strategy
tailored to both indoor and outdoor settings. Finally, the system supports
fine-grained scene editing, including object movement, appearance changes, and
4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior
methods in quality, consistency, and flexibility, offering a practical solution
for open-domain 3D content creation. Code and demos are available at
https://dreamscene-project.github.io.

</details>


### [94] [Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations](https://arxiv.org/abs/2507.14010)
*Yong Feng,Xiaolei Zhang,Shijin Feng,Yong Zhao,Yihan Chen*

Main category: cs.CV

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u96a7\u9053\u88c2\u7f1d\u5206\u7c7b\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u7ed3\u5408\u4e86\u89c6\u89c9\u89e3\u91ca\u4ee5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u96a7\u9053\u886c\u780c\u88c2\u7f1d\u662f\u8bc4\u4f30\u96a7\u9053\u5b89\u5168\u72b6\u51b5\u7684\u5173\u952e\u6307\u6807\u3002\u4e3a\u4e86\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u5730\u5bf9\u96a7\u9053\u88c2\u7f1d\u8fdb\u884c\u5206\u7c7b\u548c\u5206\u5272\uff0c\u9700\u8981\u5f00\u53d1\u5148\u8fdb\u7684\u65b9\u6cd5\u3002

Method: \u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u6b65\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\uff1a\u7b2c\u4e00\u6b65\u4f7f\u7528DenseNet-169\u5f00\u53d1\u96a7\u9053\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u4ee5\u7b5b\u9009\u51fa\u542b\u88c2\u7f1d\u56fe\u50cf\uff1b\u7b2c\u4e8c\u6b65\u5219\u57fa\u4e8eDeepLabV3+\u6784\u5efa\u88c2\u7f1d\u5206\u5272\u6a21\u578b\uff0c\u5bf9\u7b2c\u4e00\u6b65\u7b5b\u9009\u51fa\u7684\u56fe\u50cf\u8fdb\u884c\u7cbe\u7ec6\u5206\u5272\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f97\u5206\u52a0\u6743\u89c6\u89c9\u89e3\u91ca\u6280\u672f\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u5185\u90e8\u903b\u8f91\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u7c7b\u6a21\u578b\u7684\u51c6\u786e\u7387\u8fbe\u523092.23%\uff0c\u5e27\u7387\uff08FPS\uff09\u4e3a39.80\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8eCNN\u548cTransformer\u7684\u6a21\u578b\u3002\u5206\u5272\u6a21\u578b\u7684\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4e3a57.01%\uff0cF1\u5206\u6570\u4e3a67.44%\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u540c\u65f6\uff0c\u63d0\u4f9b\u7684\u89c6\u89c9\u89e3\u91ca\u6709\u52a9\u4e8e\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u201c\u9ed1\u7bb1\u201d\u673a\u5236\u3002

Conclusion: \u6240\u5f00\u53d1\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6574\u5408\u4e86\u89c6\u89c9\u89e3\u91ca\uff0c\u4e3a\u5feb\u901f\u3001\u51c6\u786e\u5730\u5b9a\u91cf\u8bc4\u4f30\u96a7\u9053\u5065\u5eb7\u72b6\u51b5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002

Abstract: Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming
to classify and segment tunnel cracks with enhanced accuracy and efficiency,
this study proposes a two-step deep learning-based method. An automatic tunnel
image classification model is developed using the DenseNet-169 in the first
step. The proposed crack segmentation model in the second step is based on the
DeepLabV3+, whose internal logic is evaluated via a score-weighted visual
explanation technique. Proposed method combines tunnel image classification and
segmentation together, so that the selected images containing cracks from the
first step are segmented in the second step to improve the detection accuracy
and efficiency. The superior performances of the two-step method are validated
by experiments. The results show that the accuracy and frames per second (FPS)
of the tunnel crack classification model are 92.23% and 39.80, respectively,
which are higher than other convolutional neural networks (CNN) based and
Transformer based models. Also, the intersection over union (IoU) and F1 score
of the tunnel crack segmentation model are 57.01% and 67.44%, respectively,
outperforming other state-of-the-art models. Moreover, the provided visual
explanations in this study are conducive to understanding the "black box" of
deep learning-based models. The developed two-stage deep learning-based method
integrating visual explanations provides a basis for fast and accurate
quantitative assessment of tunnel health status.

</details>


### [95] [Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model](https://arxiv.org/abs/2507.14013)
*Ji-Yan Wu,Zheng Yong Poh,Anoop C. Patil,Bongsoo Park,Giovanni Volpe,Daisuke Urano*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5149\u8c31\u6210\u50cf\u548c\u6539\u8fdbYOLOv5\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u690d\u7269\u53f6\u7247\u8425\u517b\u7f3a\u4e4f\u7684\u5f02\u5e38\u5206\u5272\uff0c\u5e76\u5728Dice\u548cIoU\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\uff0c\u51c6\u786e\u68c0\u6d4b\u690d\u7269\u53f6\u7247\u8425\u517b\u7f3a\u4e4f\u5bf9\u4e8e\u65e9\u671f\u5e72\u9884\u65bd\u80a5\u3001\u75be\u75c5\u548c\u80c1\u8feb\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002

Method: \u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u4e5d\u901a\u9053\u591a\u5149\u8c31\u8f93\u5165\uff0c\u5e76\u5bf9YOLOv5\u6a21\u578b\u8fdb\u884c\u4e86\u589e\u5f3a\uff0c\u52a0\u5165\u4e86\u57fa\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u5934\u3002\u6a21\u578b\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u66f4\u597d\u5730\u6355\u6349\u7ec6\u5fae\u3001\u7a7a\u95f4\u5206\u5e03\u7684\u75c7\u72b6\u3002\u5b9e\u9a8c\u5728\u53d7\u63a7\u8425\u517b\u80c1\u8feb\u6761\u4ef6\u4e0b\u8fdb\u884c\uff0c\u5e76\u4e0e\u57fa\u7ebfYOLOv5\u8fdb\u884c\u4e86\u6027\u80fd\u5bf9\u6bd4\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfYOLOv5\uff0c\u5e73\u5747Dice\u5206\u6570\u548cIoU\uff08\u4ea4\u5e76\u6bd4\uff09\u63d0\u9ad8\u4e86\u7ea612%\u3002\u8be5\u6a21\u578b\u5728\u68c0\u6d4b\u53f6\u7eff\u7d20\u7f3a\u4e4f\u75c7\u548c\u8272\u7d20\u79ef\u7d2f\u7b49\u6311\u6218\u6027\u75c7\u72b6\u65b9\u9762\u5c24\u5176\u6709\u6548\u3002

Conclusion: \u7ed3\u5408\u591a\u5149\u8c31\u6210\u50cf\u4e0e\u5149\u8c31-\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\u5728\u63a8\u8fdb\u690d\u7269\u8868\u578b\u5206\u6790\u548c\u7cbe\u51c6\u519c\u4e1a\u65b9\u9762\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002

Abstract: Accurate detection of nutrient deficiency in plant leaves is essential for
precision agriculture, enabling early intervention in fertilization, disease,
and stress management. This study presents a deep learning framework for leaf
anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model
with a transformer-based attention head. The model is tailored for processing
nine-channel multispectral input and uses self-attention mechanisms to better
capture subtle, spatially-distributed symptoms. The plants in the experiments
were grown under controlled nutrient stress conditions for evaluation. We carry
out extensive experiments to benchmark the proposed model against the baseline
YOLOv5. Extensive experiments show that the proposed model significantly
outperforms the baseline YOLOv5, with an average Dice score and IoU
(Intersection over Union) improvement of about 12%. In particular, this model
is effective in detecting challenging symptoms like chlorosis and pigment
accumulation. These results highlight the promise of combining multi-spectral
imaging with spectral-spatial feature learning for advancing plant phenotyping
and precision agriculture.

</details>


### [96] [Moodifier: MLLM-Enhanced Emotion-Driven Image Editing](https://arxiv.org/abs/2507.14024)
*Jiarong Ye,Sharon X. Huang*

Main category: cs.CV

TL;DR: \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoodifier\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u578b\u60c5\u611f\u6807\u6ce8\u6570\u636e\u96c6MoodArchive\u548c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMoodifyCLIP\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u60c5\u611f\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u60c5\u611f\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u5728\u521b\u610f\u4ea7\u4e1a\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u60c5\u611f\u7684\u62bd\u8c61\u6027\u548c\u5728\u4e0d\u540c\u8bed\u5883\u4e0b\u7684\u591a\u6837\u8868\u73b0\uff0c\u7cbe\u786e\u64cd\u7eb5\u4ecd\u7136\u5145\u6ee1\u6311\u6218\u3002

Method: \u8be5\u7814\u7a76\u91c7\u7528\u4e09\u90e8\u5206\u96c6\u6210\u65b9\u6cd5\uff1a1. \u5f15\u5165MoodArchive\uff0c\u4e00\u4e2a\u5305\u542b800\u591a\u4e07\u5f20\u56fe\u50cf\u7684\u5c42\u6b21\u5316\u60c5\u611f\u6807\u6ce8\u6570\u636e\u96c6\uff08\u7531LLaVA\u751f\u6210\u5e76\u7ecf\u90e8\u5206\u4eba\u5de5\u9a8c\u8bc1\uff09\uff1b2. \u5f00\u53d1MoodifyCLIP\uff0c\u4e00\u4e2a\u5728MoodArchive\u4e0a\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5c06\u62bd\u8c61\u60c5\u611f\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u89c6\u89c9\u5c5e\u6027\uff1b3. \u63d0\u51faMoodifier\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7f16\u8f91\u6a21\u578b\uff0c\u5229\u7528MoodifyCLIP\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5b9e\u73b0\u7cbe\u786e\u7684\u60c5\u611f\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002

Result: Moodifier\u7cfb\u7edf\u5728\u89d2\u8272\u8868\u60c5\u3001\u65f6\u5c1a\u8bbe\u8ba1\u3001\u73e0\u5b9d\u548c\u5bb6\u5c45\u88c5\u9970\u7b49\u591a\u4e2a\u9886\u57df\u5747\u6709\u6548\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMoodifier\u5728\u60c5\u611f\u51c6\u786e\u6027\u548c\u5185\u5bb9\u4fdd\u7559\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u7f16\u8f91\u3002

Conclusion: \u8be5\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u5c06\u62bd\u8c61\u60c5\u611f\u4e0e\u5177\u4f53\u7684\u89c6\u89c9\u53d8\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u60c5\u611f\u5185\u5bb9\u521b\u4f5c\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u56e2\u961f\u5c06\u516c\u5f00\u53d1\u5e03MoodArchive\u6570\u636e\u96c6\u3001MoodifyCLIP\u6a21\u578b\u4ee5\u53caMoodifier\u4ee3\u7801\u548c\u6f14\u793a\u3002

Abstract: Bridging emotions and visual content for emotion-driven image editing holds
great potential in creative industries, yet precise manipulation remains
challenging due to the abstract nature of emotions and their varied
manifestations across different contexts. We tackle this challenge with an
integrated approach consisting of three complementary components. First, we
introduce MoodArchive, an 8M+ image dataset with detailed hierarchical
emotional annotations generated by LLaVA and partially validated by human
evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned
on MoodArchive to translate abstract emotions into specific visual attributes.
Third, we propose Moodifier, a training-free editing model leveraging
MoodifyCLIP and multimodal large language models (MLLMs) to enable precise
emotional transformations while preserving content integrity. Our system works
across diverse domains such as character expressions, fashion design, jewelry,
and home d\'ecor, enabling creators to quickly visualize emotional variations
while preserving identity and structure. Extensive experimental evaluations
show that Moodifier outperforms existing methods in both emotional accuracy and
content preservation, providing contextually appropriate edits. By linking
abstract emotions to concrete visual changes, our solution unlocks new
possibilities for emotional content creation in real-world applications. We
will release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier
code and demo publicly available upon acceptance.

</details>


### [97] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: \u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuantEIT\u7684\u8d85\u8f7b\u91cf\u7ea7\u91cf\u5b50\u8f85\u52a9\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u6c14\u963b\u6297\u65ad\u5c42\u626b\u63cf\uff08EIT\uff09\u56fe\u50cf\u91cd\u5efa\uff0c\u8be5\u6846\u67b6\u5728\u65e0\u76d1\u7763\u3001\u65e0\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u6781\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u6216\u4e0e\u4e4b\u76f8\u5f53\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: EIT\u4f5c\u4e3a\u4e00\u79cd\u5e8a\u65c1\u6210\u50cf\u6280\u672f\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u6210\u672c\uff0c\u4f46\u5176\u56fa\u6709\u7684\u75c5\u6001\u9006\u95ee\u9898\u5bfc\u81f4\u56fe\u50cf\u91cd\u5efa\u56f0\u96be\u3002\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u867d\u7136\u6709\u524d\u666f\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u7f51\u7edc\u67b6\u6784\u548c\u5927\u91cf\u53c2\u6570\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002

Method: QuantEIT\u6846\u67b6\u5229\u7528\u91cf\u5b50\u8f85\u52a9\u7f51\u7edc\uff08QA-Net\uff09\uff0c\u7ed3\u5408\u5e76\u884c\u7684\u53cc\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u6f5c\u5728\u8868\u793a\uff08\u4f5c\u4e3a\u9690\u5f0f\u975e\u7ebf\u6027\u5148\u9a8c\uff09\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5355\u4e00\u7684\u7ebf\u6027\u5c42\u8fdb\u884c\u7535\u5bfc\u7387\u91cd\u5efa\u3002\u8be5\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u53c2\u6570\u6570\u91cf\u3002\u5176\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\uff0cQuantEIT\u4ee5\u65e0\u76d1\u7763\u3001\u65e0\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u5f0f\u8fd0\u884c\uff0c\u662f\u9996\u6b21\u5c06\u91cf\u5b50\u7535\u8def\u96c6\u6210\u5230EIT\u56fe\u50cf\u91cd\u5efa\u4e2d\u3002

Result: \u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u76842D\u548c3D EIT\u80ba\u90e8\u6210\u50cf\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cQuantEIT\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u75280.2%\u7684\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u3002

Conclusion: QuantEIT\u662fEIT\u56fe\u50cf\u91cd\u5efa\u9886\u57df\u7684\u4e00\u4e2a\u521b\u65b0\u6027\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u91cf\u5b50\u8f85\u52a9\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u91cf\u5b50\u7535\u8def\uff0c\u5728\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728EIT\u5e94\u7528\u4e2d\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7684\u6311\u6218\u3002

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [98] [Training-free Token Reduction for Vision Mamba](https://arxiv.org/abs/2507.14042)
*Qiankun Ma,Ziyao Zhang,Chi Su,Jie Chen,Zhen Song,Hairong Zheng,Wen Gao*

Main category: cs.CV

TL;DR: \u672c\u6587\u63d0\u51faMTR\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684Mamba\u6a21\u578bToken\u88c1\u526a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u91cd\u8981\u6027\u8bc4\u5206\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u4fdd\u6301\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: Vision Mamba\u867d\u5177\u9ad8\u6548\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u4f18\u52bf\uff0c\u4f46\u5176Token\u88c1\u526a\uff08ViT\u4e2d\u5e38\u7528\u6280\u672f\uff09\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709ViT\u7684Token\u88c1\u526a\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eMamba\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u6ce8\u610f\u529b\u673a\u5236\u4e14\u5ffd\u7565Token\u987a\u5e8f\uff0c\u5bfc\u81f4Mamba\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002

Method: \u63d0\u51fa\u4e00\u79cdMamba\u7ed3\u6784\u611f\u77e5\u7684Token\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86MTR\uff08Mamba Token Reduction\uff09\u6846\u67b6\u3002MTR\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u4e0d\u9700\u989d\u5916\u8c03\u53c2\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cdMamba\u6a21\u578b\u4e2d\u3002

Result: MTR\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u5c06\u6027\u80fd\u5f71\u54cd\u964d\u5230\u6700\u4f4e\u3002\u4f8b\u5982\uff0c\u5728Vim-B\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cFLOPs\u51cf\u5c11\u4e86\u7ea640%\uff0c\u800cImageNet\u6027\u80fd\u4ec5\u4e0b\u964d1.6%\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002

Conclusion: MTR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684Mamba Token\u88c1\u526a\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8Vision Mamba\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002

Abstract: Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)
due to its ability to efficiently capture long-range dependencies with linear
computational complexity. While token reduction, an effective compression
technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision
Mamba's efficiency is essential for enabling broader applications. However, we
find that directly applying existing token reduction techniques for ViTs to
Vision Mamba leads to significant performance degradation. This is primarily
because Mamba is a sequence model without attention mechanisms, whereas most
token reduction techniques for ViTs rely on attention mechanisms for importance
measurement and overlook the order of compressed tokens. In this paper, we
investigate a Mamba structure-aware importance score to evaluate token
importance in a simple and effective manner. Building on this score, we further
propose MTR, a training-free \textbf{M}amba \textbf{T}oken \textbf{R}eduction
framework. Without the need for training or additional tuning parameters, our
method can be seamlessly integrated as a plug-and-play component across various
Mamba models. Extensive experiments demonstrate that our approach significantly
reduces computational workload while minimizing performance impact across
various tasks and multiple backbones. Notably, MTR reduces FLOPs by
approximately 40\% on the Vim-B backbone, with only a 1.6\% drop in ImageNet
performance without retraining.

</details>


### [99] [Foundation Models as Class-Incremental Learners for Dermatological Image Classification](https://arxiv.org/abs/2507.14050)
*Mohamed Elkhayat,Mohamed Mahmoud,Jamil Fayyad,Nourhan Bayasi*

Main category: cs.CV

TL;DR: \u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5728\u76ae\u80a4\u75c5\u5206\u7c7b\u4e2d\uff0c\u4f7f\u7528\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u8fdb\u884c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u57fa\u7840\u6a21\u578b\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u4e3aCIL\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u53ef\u8fc1\u79fb\u8868\u793a\uff0c\u4f46\u5728\u76ae\u80a4\u75c5\u5b66\u9886\u57df\uff0c\u5176\u5728\u589e\u91cf\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002

Method: \u7814\u7a76\u8005\u8bc4\u4f30\u4e86\u5728\u5927\u578b\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u76ae\u80a4\u75c5\u5206\u7c7b\u7684CIL\u3002\u4ed6\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5176\u4e2d\u4e3b\u5e72\u7f51\u7edc\u4fdd\u6301\u51bb\u7ed3\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u589e\u91cf\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7MLP\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63a2\u7d22\u4e86\u96f6\u8bad\u7ec3\u573a\u666f\uff0c\u4f7f\u7528\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u7684\u539f\u578b\u6784\u5efa\u7684\u6700\u8fd1\u5747\u503c\u5206\u7c7b\u5668\u3002

Result: \u8be5\u65b9\u6cd5\u5728\u4e0d\u9057\u5fd8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u4e8e\u6b63\u5219\u5316\u3001\u91cd\u653e\u548c\u67b6\u6784\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u539f\u578b\u7684\u53d8\u4f53\u4e5f\u80fd\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u652f\u6301\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u533b\u7597\u5e94\u7528\u4e2d\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002

Abstract: Class-Incremental Learning (CIL) aims to learn new classes over time without
forgetting previously acquired knowledge. The emergence of foundation models
(FM) pretrained on large datasets presents new opportunities for CIL by
offering rich, transferable representations. However, their potential for
enabling incremental learning in dermatology remains largely unexplored. In
this paper, we systematically evaluate frozen FMs pretrained on large-scale
skin lesion datasets for CIL in dermatological disease classification. We
propose a simple yet effective approach where the backbone remains frozen, and
a lightweight MLP is trained incrementally for each task. This setup achieves
state-of-the-art performance without forgetting, outperforming regularization,
replay, and architecture based methods. To further explore the capabilities of
frozen FMs, we examine zero training scenarios using nearest mean classifiers
with prototypes derived from their embeddings. Through extensive ablation
studies, we demonstrate that this prototype based variant can also achieve
competitive results. Our findings highlight the strength of frozen FMs for
continual learning in dermatology and support their broader adoption in real
world medical applications. Our code and datasets are available here.

</details>


### [100] [Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection](https://arxiv.org/abs/2507.14083)
*Sara Abdulaziz,Egor Bondarev*

Main category: cs.CV

TL;DR: \u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u56db\u79cd\u4eba\u4f53\u533f\u540d\u5316\u6280\u672f\uff08\u6a21\u7cca\u3001\u906e\u853d\u3001\u52a0\u5bc6\u3001\u5934\u50cf\u66ff\u6362\uff09\u4e0b\uff0c\u56db\u79cd\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u5728\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5f02\u5e38\u68c0\u6d4b\u5728\u533f\u540d\u5316\u6570\u636e\u4e0b\u4ecd\u53ef\u884c\uff0c\u4e14\u6027\u80fd\u53d7\u7b97\u6cd5\u8bbe\u8ba1\u5f71\u54cd\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u533f\u540d\u5316\u751a\u81f3\u80fd\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u4e86\u76d1\u63a7\u89c6\u9891\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u654f\u611f\u4eba\u4f53\u6570\u636e\u7684\u6536\u96c6\u5f15\u53d1\u4e86\u7d27\u8feb\u7684\u9690\u79c1\u95ee\u9898\u3002

Method: \u5c06\u56db\u79cd\u4eba\u4f53\u533f\u540d\u5316\u6280\u672f\uff08\u6a21\u7cca\u3001\u906e\u853d\u3001\u52a0\u5bc6\u3001\u5934\u50cf\u66ff\u6362\uff09\u5e94\u7528\u4e8eUCF-Crime\u6570\u636e\u96c6\u3002\u7136\u540e\uff0c\u5728\u533f\u540d\u5316\u540e\u7684UCF-Crime\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u56db\u79cd\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff08MGFN\u3001UR-DMU\u3001BN-WVAD\u3001PEL4VAD\uff09\uff0c\u5e76\u4e0e\u65b0\u5174\u7684\u9690\u79c1\u8bbe\u8ba1\uff08privacy-by-design\uff09\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f02\u5e38\u68c0\u6d4b\u5728\u533f\u540d\u5316\u6570\u636e\u4e0b\u4ecd\u7136\u53ef\u884c\uff0c\u4e14\u5176\u6027\u80fd\u53d6\u51b3\u4e8e\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b66\u4e60\u7b56\u7565\u3002\u5728\u67d0\u4e9b\u533f\u540d\u5316\u6a21\u5f0f\uff08\u5982\u52a0\u5bc6\u548c\u906e\u853d\uff09\u4e0b\uff0c\u4e00\u4e9b\u6a21\u578b\u751a\u81f3\u6bd4\u5728\u539f\u59cb\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684AUC\u6027\u80fd\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u7b97\u6cd5\u7ec4\u4ef6\u5bf9\u8fd9\u4e9b\u201c\u566a\u58f0\u6a21\u5f0f\u201d\u7684\u5f3a\u70c8\u54cd\u5e94\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86\u7b97\u6cd5\u5bf9\u533f\u540d\u5316\u7684\u7279\u5b9a\u654f\u611f\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u68c0\u6d4b\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002

Conclusion: \u672c\u7814\u7a76\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u4e3a\u5e73\u8861\u4eba\u4f53\u9690\u79c1\u4e0e\u5f02\u5e38\u68c0\u6d4b\u9700\u6c42\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u529b\u7684\u57fa\u51c6\u548c\u6df1\u5165\u89c1\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6548\u7528\u7075\u6d3b\u6027\u4e4b\u95f4\u5e38\u5e38\u88ab\u5ffd\u89c6\u7684\u6743\u8861\u3002

Abstract: Advancements in deep learning have improved anomaly detection in surveillance
videos, yet they raise urgent privacy concerns due to the collection of
sensitive human data. In this paper, we present a comprehensive analysis of
anomaly detection performance under four human anonymization techniques,
including blurring, masking, encryption, and avatar replacement, applied to the
UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,
BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method
responds to different obfuscation techniques. Experimental results demonstrate
that anomaly detection remains viable under anonymized data and is dependent on
the algorithmic design and the learning strategy. For instance, under certain
anonymization patterns, such as encryption and masking, some models
inadvertently achieve higher AUC performance compared to raw data, due to the
strong responsiveness of their algorithmic components to these noise patterns.
These results highlight the algorithm-specific sensitivities to anonymization
and emphasize the trade-off between preserving privacy and maintaining
detection utility. Furthermore, we compare these conventional anonymization
techniques with the emerging privacy-by-design solutions, highlighting an often
overlooked trade-off between robust privacy protection and utility flexibility.
Through comprehensive experiments and analyses, this study provides a
compelling benchmark and insights into balancing human privacy with the demands
of anomaly detection.

</details>


### [101] [C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs](https://arxiv.org/abs/2507.14095)
*Yung-Hong Sun,Ting-Hung Lin,Jiangang Chen,Hongrui Jiang,Yu Hen Hu*

Main category: cs.CV

TL;DR: C-DOG\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u591a\u76ee\u6807\u5173\u8054\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u8fde\u63a5delta-overlap\u56fe\u5efa\u6a21\u548c\u5bf9\u6781\u51e0\u4f55\uff0c\u5373\u4f7f\u5728\u89c6\u89c9\u4e0d\u53ef\u533a\u5206\u6216\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u9c81\u68d2\u5730\u5173\u80542D\u68c0\u6d4b\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a213D\u91cd\u5efa\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u591a\u89c6\u89d2\u591a\u76ee\u6807\u5173\u8054\u65b9\u6cd5\u4f9d\u8d56\u5916\u89c2\u7279\u5f81\u6216\u51e0\u4f55\u7ea6\u675f\uff08\u5982\u5bf9\u6781\u4e00\u81f4\u6027\uff09\uff0c\u4f46\u5728\u7269\u4f53\u89c6\u89c9\u4e0a\u65e0\u6cd5\u533a\u5206\u6216\u89c2\u6d4b\u6570\u636e\u88ab\u566a\u58f0\u6c61\u67d3\u65f6\u4f1a\u5931\u6548\u3002

Method: C-DOG\u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u4e0d\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\u3002\u5b83\u5c062D\u89c2\u6d4b\u8868\u793a\u4e3a\u56fe\u8282\u70b9\uff0c\u8fb9\u6743\u91cd\u7531\u5bf9\u6781\u4e00\u81f4\u6027\u51b3\u5b9a\u3002\u901a\u8fc7delta-\u90bb\u5c45-\u91cd\u53e0\u805a\u7c7b\u8bc6\u522b\u5f3a\u4e00\u81f4\u6027\u7ec4\uff0c\u5e76\u5bb9\u5fcd\u566a\u58f0\u548c\u90e8\u5206\u8fde\u63a5\u3002\u4e3a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5f15\u5165\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\uff08IQR\uff09\u7684\u6ee4\u6ce2\u548c3D\u53cd\u6295\u5f71\u8bef\u5dee\u51c6\u5219\u6765\u6d88\u9664\u4e0d\u4e00\u81f4\u89c2\u6d4b\u3002

Result: \u5728\u5408\u6210\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cC-DOG\u4f18\u4e8e\u57fa\u4e8e\u51e0\u4f55\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u9ad8\u7269\u4f53\u5bc6\u5ea6\u3001\u65e0\u89c6\u89c9\u7279\u5f81\u548c\u6709\u9650\u76f8\u673a\u91cd\u53e0\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002

Conclusion: C-DOG\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u76843D\u91cd\u5efa\uff0c\u4f5c\u4e3a\u76ee\u6807\u68c0\u6d4b\uff08\u6216\u59ff\u6001\u4f30\u8ba1\uff09\u548c3D\u91cd\u5efa\u4e4b\u95f4\u7684\u4e2d\u95f4\u6a21\u5757\u3002

Abstract: Multi-view multi-object association is a fundamental step in 3D
reconstruction pipelines, enabling consistent grouping of object instances
across multiple camera views. Existing methods often rely on appearance
features or geometric constraints such as epipolar consistency. However, these
approaches can fail when objects are visually indistinguishable or observations
are corrupted by noise. We propose C-DOG, a training-free framework that serves
as an intermediate module bridging object detection (or pose estimation) and 3D
reconstruction, without relying on visual features. It combines connected
delta-overlap graph modeling with epipolar geometry to robustly associate
detections across views. Each 2D observation is represented as a graph node,
with edges weighted by epipolar consistency. A delta-neighbor-overlap
clustering step identifies strongly consistent groups while tolerating noise
and partial connectivity. To further improve robustness, we incorporate
Interquartile Range (IQR)-based filtering and a 3D back-projection error
criterion to eliminate inconsistent observations. Extensive experiments on
synthetic benchmarks demonstrate that C-DOG outperforms geometry-based
baselines and remains robust under challenging conditions, including high
object density, without visual features, and limited camera overlap, making it
well-suited for scalable 3D reconstruction in real-world scenarios.

</details>


### [102] [Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning](https://arxiv.org/abs/2507.14137)
*Shashanka Venkataramanan,Valentinos Pariza,Mohammadreza Salehi,Lukas Knobel,Spyros Gidaris,Elias Ramzi,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: Franca\u662f\u4e00\u4e2a\u5168\u5f00\u6e90\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u4e13\u6709\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u5934\u805a\u7c7b\u6295\u5f71\u5668\u548c\u4f4d\u7f6e\u89e3\u8026\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u805a\u7c7b\u6b67\u4e49\u548c\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u591a\u4e3a\u4e13\u6709\u4e14\u4e0d\u900f\u660e\uff0c\u4e14\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u4e2d\u7684\u805a\u7c7b\u65b9\u6cd5\uff08\u5982Sinkhorn-Knopp\uff09\u672a\u80fd\u89e3\u51b3\u805a\u7c7b\u8bed\u4e49\u7684\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u540c\u65f6\u5bc6\u96c6\u8868\u5f81\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u3002

Method: Franca\u91c7\u7528\u900f\u660e\u8bad\u7ec3\u6d41\u7a0b\uff08\u53d7Web-SSL\u542f\u53d1\uff09\uff0c\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\uff08ImageNet-21K\u548cReLAION-2B\u5b50\u96c6\uff09\u3002\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a1. \u5f15\u5165\u57fa\u4e8e\u5d4c\u5957Matryoshka\u8868\u793a\u7684\u53c2\u6570\u9ad8\u6548\u3001\u591a\u5934\u805a\u7c7b\u6295\u5f71\u5668\uff0c\u4ee5\u6e10\u8fdb\u5f0f\u7ec6\u5316\u7279\u5f81\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u30022. \u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4f4d\u7f6e\u89e3\u8026\u7b56\u7565\uff0c\u663e\u5f0f\u79fb\u9664\u5bc6\u96c6\u8868\u5f81\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u4ee5\u6539\u5584\u8bed\u4e49\u5185\u5bb9\u7684\u7f16\u7801\u3002

Result: Franca\u7684\u6027\u80fd\u4e0eDINOv2\u3001CLIP\u3001SigLIPv2\u7b49\u6700\u5148\u8fdb\u7684\u4e13\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u751a\u81f3\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u8d85\u8d8a\u3002\u591a\u5934\u805a\u7c7b\u6295\u5f71\u5668\u5b9e\u73b0\u4e86\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u3002\u4f4d\u7f6e\u89e3\u8026\u7b56\u7565\u5728\u591a\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u66f4\u5e72\u51c0\u7279\u5f81\u7a7a\u95f4\u7684\u5b9e\u7528\u6027\u3002

Conclusion: Franca\u4e3a\u900f\u660e\u3001\u9ad8\u6027\u80fd\u7684\u89c6\u89c9\u6a21\u578b\u6811\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u4e3aAI\u793e\u533a\u5e26\u6765\u4e86\u66f4\u53ef\u590d\u73b0\u548c\u6cdb\u5316\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5f00\u653e\u4e86\u6570\u636e\u3001\u4ee3\u7801\u548c\u6743\u91cd\u3002

Abstract: We present Franca (pronounced Fran-ka): free one; the first fully open-source
(data, code, weights) vision foundation model that matches and in many cases
surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,
CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training
pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and
a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in
SSL clustering methods. While modern models rely on assigning image features to
large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to
account for the inherent ambiguity in clustering semantics. To address this, we
introduce a parameter-efficient, multi-head clustering projector based on
nested Matryoshka representations. This design progressively refines features
into increasingly fine-grained clusters without increasing the model size,
enabling both performance and memory efficiency. Additionally, we propose a
novel positional disentanglement strategy that explicitly removes positional
biases from dense representations, thereby improving the encoding of semantic
content. This leads to consistent gains on several downstream benchmarks,
demonstrating the utility of cleaner feature spaces. Our contributions
establish a new standard for transparent, high-performance vision models and
open a path toward more reproducible and generalizable foundation models for
the broader AI community. The code and model checkpoints are available at
https://github.com/valeoai/Franca.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [103] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
*Atharva Bhargude,Ishan Gonehal,Chandler Haney,Dave Yoon,Kevin Zhu,Aaron Sandoval,Sean O'Brien,Kaustubh Vinnakota*

Main category: cs.CL

TL;DR: \u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5c11\u91cf\u6837\u672c\u81ea\u9002\u5e94\u8bed\u8a00\u63d0\u793a\u201d\uff08ALP\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408GPT-4o\u548cGemini 1.5 Pro\u7b49\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u7f51\u7edc\u9493\u9c7c\u7f51\u9875\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002


<details>
  <summary>Details</summary>
Motivation: \u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u662f\u91cd\u5927\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u81ea\u9002\u5e94\u7684\u68c0\u6d4b\u6280\u672f\u3002

Method: \u91c7\u7528\u5c11\u91cf\u6837\u672c\u81ea\u9002\u5e94\u8bed\u8a00\u63d0\u793a\uff08ALP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u6784\u5316\u8bed\u4e49\u63a8\u7406\u65b9\u6cd5\uff0c\u6307\u5bfcLLMs\u5206\u6790\u6587\u672c\u6b3a\u9a97\uff0c\u5305\u62ec\u5206\u89e3\u8bed\u8a00\u6a21\u5f0f\u3001\u68c0\u6d4b\u7d27\u6025\u63d0\u793a\u548c\u8bc6\u522b\u9493\u9c7c\u5185\u5bb9\u4e2d\u5e38\u89c1\u7684\u64cd\u7eb5\u6027\u63aa\u8f9e\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u6587\u672c\u3001\u89c6\u89c9\u548cURL\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\u3002

Result: \u5b9e\u9a8c\u8bc1\u660e\uff0cALP\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5206\u6790\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7edc\u9493\u9c7c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002\u5176F1\u5206\u6570\u8fbe\u52300.93\uff0c\u8d85\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408ALP\u7684\u591a\u6a21\u6001LLMs\u6709\u6f5c\u529b\u63a8\u52a8\u7f51\u7edc\u9493\u9c7c\u68c0\u6d4b\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u7f51\u7edc\u9493\u9c7c\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002

Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating
adaptive detection techniques. This study explores few-shot Adaptive Linguistic
Prompting (ALP) in detecting phishing webpages through the multimodal
capabilities of state-of-the-art large language models (LLMs) such as GPT-4o
and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides
LLMs to analyze textual deception by breaking down linguistic patterns,
detecting urgency cues, and identifying manipulative diction commonly found in
phishing content. By integrating textual, visual, and URL-based analysis, we
propose a unified model capable of identifying sophisticated phishing attempts.
Our experiments demonstrate that ALP significantly enhances phishing detection
accuracy by guiding LLMs through structured reasoning and contextual analysis.
The findings highlight the potential of ALP-integrated multimodal LLMs to
advance phishing detection frameworks, achieving an F1-score of 0.93,
surpassing traditional approaches. These results establish a foundation for
more robust, interpretable, and adaptive linguistic-based phishing detection
systems using LLMs.

</details>


### [104] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: PersonaGen\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u591a\u9636\u6bb5\u57fa\u4e8e\u89d2\u8272\u7684\u6761\u4ef6\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u6587\u672c\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u8d28\u91cf\u60c5\u611f\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u60c5\u611f\u8bc6\u522b\u9886\u57df\u9762\u4e34\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u60c5\u611f\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u6311\u6218\u3002\u60c5\u611f\u8868\u8fbe\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u53d7\u4e2a\u4f53\u7279\u8d28\u3001\u793e\u4f1a\u6587\u5316\u80cc\u666f\u548c\u60c5\u5883\u56e0\u7d20\u5f71\u54cd\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u3001\u901a\u7528\u6570\u636e\u7684\u6536\u96c6\u5728\u4f26\u7406\u548c\u5b9e\u8df5\u4e0a\u90fd\u5341\u5206\u56f0\u96be\u3002

Method: PersonaGen\u901a\u8fc7\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u3001\u793e\u4f1a\u6587\u5316\u80cc\u666f\u548c\u8be6\u7ec6\u60c5\u5883\u6784\u5efa\u5206\u5c42\u865a\u62df\u89d2\u8272\uff0c\u5e76\u4ee5\u6b64\u5f15\u5bfcLLM\u751f\u6210\u60c5\u611f\u8868\u8fbe\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u9636\u6bb5\u57fa\u4e8e\u89d2\u8272\u7684\u6761\u4ef6\u4f5c\u7528\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPersonaGen\u5728\u751f\u6210\u591a\u6837\u5316\u3001\u8fde\u8d2f\u4e14\u5177\u6709\u8fa8\u8bc6\u5ea6\u7684\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u901a\u8fc7\u805a\u7c7b\u548c\u5206\u5e03\u6307\u6807\u8bc4\u4f30\u4e86\u8bed\u4e49\u591a\u6837\u6027\uff0c\u901a\u8fc7LLM\u8d28\u91cf\u8bc4\u5206\u8bc4\u4f30\u4e86\u7c7b\u4eba\u5ea6\uff0c\u901a\u8fc7\u4e0e\u771f\u5b9e\u60c5\u611f\u8bed\u6599\u5e93\u6bd4\u8f83\u8bc4\u4f30\u4e86\u771f\u5b9e\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002

Conclusion: PersonaGen\u5c55\u793a\u4e86\u4f5c\u4e3a\u589e\u5f3a\u6216\u66ff\u4ee3\u771f\u5b9e\u4e16\u754c\u60c5\u611f\u6570\u636e\u96c6\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u3001\u8fde\u8d2f\u548c\u53ef\u533a\u5206\u7684\u60c5\u611f\u8868\u8fbe\u3002

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [105] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: SAFT\u662f\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u78c1\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5c06\u56fe\u62d3\u6251\u4fe1\u606f\u6ce8\u5165\u9884\u8bad\u7ec3LLM\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u5728AMR-to-text\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: LLMs\u5728\u5904\u7406\u56fe\u7b49\u7ed3\u6784\u5316\u8f93\u5165\u65f6\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u968f\u610f\u7ebf\u6027\u5316\u56fe\u7ed3\u6784\uff08\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0e\u6807\u51c6LLMs\u4e0d\u517c\u5bb9\u7684\u67b6\u6784\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u6765\u5c06\u56fe\u7ed3\u6784\u4fe1\u606f\u878d\u5165LLMs\u3002

Method: \u5f15\u5165SAFT\uff08Structure-Aware Fine-Tuning\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u8f6c\u6362\u540e\u7684AMR\u56fe\u7684\u78c1\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u751f\u6210\u65b9\u5411\u654f\u611f\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e76\u5c06\u5176\u6295\u5f71\u5230LLM\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u5c06\u56fe\u62d3\u6251\u4fe1\u606f\u6ce8\u5165\u9884\u8bad\u7ec3LLM\uff0c\u800c\u65e0\u9700\u4fee\u6539LLM\u7684\u5e95\u5c42\u67b6\u6784\u3002

Result: SAFT\u5728AMR 3.0\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u6a21\u578bBLEU\u5206\u6570\u63d0\u5347\u4e863.5\u3002\u6027\u80fd\u63d0\u5347\u968f\u7740\u56fe\u590d\u6742\u5ea6\u7684\u589e\u52a0\u800c\u66f4\u4e3a\u663e\u8457\uff0c\u7a81\u51fa\u4e86\u7ed3\u6784\u611f\u77e5\u8868\u793a\u5728\u589e\u5f3aLLM\u6027\u80fd\u65b9\u9762\u7684\u4ef7\u503c\u3002

Conclusion: SAFT\u4e3a\u8fde\u63a5\u7ed3\u6784\u5316\u6570\u636e\u548c\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u901a\u7528\u4e14\u6709\u6548\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u6ce8\u5165\u56fe\u62d3\u6251\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u56fe\u5230\u6587\u672c\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [106] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\uff0c\u5229\u7528NLP\u6280\u672f\u5c06\u65b0\u95fb\u6587\u7ae0\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eMDL\u7684\u56fe\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u6765\u8bc6\u522b\u865a\u5047\u65b0\u95fb\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u5f53\u4eca\u6570\u5b57\u4e16\u754c\u4e2d\uff0c\u865a\u5047\u65b0\u95fb\u7684\u5feb\u901f\u4f20\u64ad\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u91cd\u5927\u95ee\u9898\u3002

Method: \u7814\u7a76\u4f7f\u7528\u4e86\u6765\u81eaKaggle\u7684\u771f\u5b9e\u548c\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\uff0c\u5e76\u8865\u5145\u4e86\u8fd1\u671f\u4e0eCOVID-19\u76f8\u5173\u7684\u771f\u5047\u65b0\u95fb\u6587\u7ae0\u3002\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\u5c06\u65b0\u95fb\u6587\u7ae0\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u56fe\u7ed3\u6784\uff0c\u7136\u540e\u5e94\u7528\u57fa\u4e8e\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u7684\u56fe\u5f02\u5e38\u68c0\u6d4b\uff08GBAD\uff09\u7b97\u6cd5\u8fdb\u884c\u56fe\u6316\u6398\uff0c\u4ee5\u8bc6\u522b\u504f\u79bb\u6b63\u5e38\u6a21\u5f0f\u7684\u5f02\u5e38\u6a21\u5f0f\u3002

Result: \u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u6570\u636e\u96c6\u4e2d\u7684\u89c4\u8303\u6a21\u5f0f\uff0c\u5e76\u968f\u540e\u53d1\u73b0\u504f\u79bb\u8fd9\u4e9b\u65e2\u5b9a\u89c4\u8303\u7684\u5f02\u5e38\u6a21\u5f0f\u3002\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u5728\u5904\u7406\u4e30\u5bcc\u4e0a\u4e0b\u6587\u6570\u636e\u65b9\u9762\u7279\u522b\u6709\u6548\uff0c\u80fd\u591f\u53d1\u73b0\u4f20\u7edf\u6280\u672f\u53ef\u80fd\u5ffd\u7565\u7684\u590d\u6742\u6a21\u5f0f\u3002

Conclusion: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u56fe\u57fa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65b0\u95fb\u6587\u7ae0\u8f6c\u6362\u4e3a\u56fe\u7ed3\u6784\u5e76\u5229\u7528\u56fe\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff0c\u6210\u529f\u5730\u8bc6\u522b\u4e86\u865a\u5047\u65b0\u95fb\u6587\u7ae0\u3002

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [107] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: \u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86PARAM-1\uff0c\u4e00\u4e2a2.9B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u95ee\u9898\u800c\u8bbe\u8ba1\u548c\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u4f9b\u516c\u5e73\u7684\u8bed\u8a00\u8868\u793a\u548c\u6587\u5316\u5bf9\u9f50\u7684\u8bc4\u4f30\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u5bfc\u81f4\u5370\u5ea6\u7b49\u8bed\u8a00\u591a\u6837\u6027\u4e30\u5bcc\u7684\u5730\u533a\u5728\u6570\u636e\u3001\u67b6\u6784\u548c\u4f18\u5316\u8303\u5f0f\u4e0a\u5b58\u5728\u7ed3\u6784\u6027\u4ee3\u8868\u4e0d\u8db3\u3002

Method: PARAM-1\u662f\u4e00\u4e2a2.9B\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668\u3001\u4ec5\u6587\u672c\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u3002\u5b83\u4e13\u6ce8\u4e8e\u5370\u5ea6\u591a\u6837\u6027\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u5370\u5730\u8bed\u548c\u82f1\u8bed\u53cc\u8bed\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5e76\u9075\u5faa\u4e09\u4e2a\u6838\u5fc3\u539f\u5219\uff1a1) \u6307\u793a\u8bed\u79cd\u8bed\u6599\u516c\u5e73\u5206\u914d\uff08\u536025%\uff09\uff1b2) \u901a\u8fc7\u9002\u5e94\u5370\u5ea6\u5f62\u6001\u7ed3\u6784\u7684SentencePiece\u5206\u8bcd\u5668\u5b9e\u73b0\u5206\u8bcd\u516c\u5e73\u6027\uff1b3) \u5728IndicQA\u3001\u4ee3\u7801\u6df7\u5408\u63a8\u7406\u548c\u793e\u4f1a\u8bed\u8a00\u5b66\u9c81\u68d2\u6027\u4efb\u52a1\u4e0a\u8fdb\u884c\u6587\u5316\u5bf9\u9f50\u7684\u8bc4\u4f30\u3002

Result: PARAM-1\u5728\u4f5c\u4e3a\u901a\u7528\u6a21\u578b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e3a\u4ee5\u5370\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u3002

Conclusion: PARAM-1\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5d4c\u5165\u591a\u6837\u6027\u800c\u975e\u540e\u671f\u5bf9\u9f50\uff0c\u4e3a\u516c\u5e73\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u201c\u8bbe\u8ba1\u4f18\u5148\u201d\u7684\u84dd\u56fe\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u8bed\u8a00\u591a\u6837\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [108] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: \u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u4e3b\u9898\u5efa\u6a21\u7ba1\u9053\u91cd\u6784\u4e3a\u57fa\u4e8e\u610f\u89c1\u5355\u5143\uff08\u5305\u542b\u6587\u672c\u548c\u60c5\u611f\u5206\u6570\uff09\u7684\u64cd\u4f5c\uff0c\u63d0\u5347\u4e86\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u6d1e\u5bdf\u7684\u80fd\u529b\uff0c\u5e76\u80fd\u5c06\u4e3b\u9898\u4e0e\u60c5\u611f\u5173\u8054\u5230\u4e1a\u52a1\u6307\u6807\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u6df1\u5ea6\u6d1e\u5bdf\u65b9\u9762\u53ef\u80fd\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5173\u8054\u8bc4\u8bba\u5185\u5bb9\u3001\u60c5\u611f\u4e0e\u4e1a\u52a1\u7ed3\u679c\u65b9\u9762\u3002\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u4e3b\u9898\u5efa\u6a21\uff0c\u4f7f\u5176\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u60c5\u611f\u5e76\u91cf\u5316\u5176\u5bf9\u4e1a\u52a1\u7684\u5f71\u54cd\u3002

Method: \u6838\u5fc3\u65b9\u6cd5\u662f\u91cd\u6784\u4e3b\u9898\u5efa\u6a21\u7ba1\u9053\uff0c\u4f7f\u5176\u57fa\u4e8e\u201c\u610f\u89c1\u5355\u5143\u201d\u8fdb\u884c\u64cd\u4f5c\u3002\u8fd9\u4e9b\u610f\u89c1\u5355\u5143\u662f\u5305\u542b\u76f8\u5173\u6587\u672c\u6458\u5f55\u548c\u5173\u8054\u60c5\u611f\u5206\u6570\u7684\u72ec\u7acb\u9648\u8ff0\uff0c\u5e76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u63d0\u53d6\u3002\u968f\u540e\uff0c\u5bf9\u8fd9\u4e9b\u610f\u89c1\u5355\u5143\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u5c06\u5f97\u51fa\u7684\u4e3b\u9898\u548c\u60c5\u611f\u4e0e\u661f\u7ea7\u8bc4\u5206\u7b49\u4e1a\u52a1\u6307\u6807\u8fdb\u884c\u5173\u8054\u3002

Result: \u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u540e\u7eed\u4e3b\u9898\u5efa\u6a21\u7684\u6027\u80fd\uff0c\u751f\u6210\u4e86\u8fde\u8d2f\u4e14\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\uff0c\u5e76\u6355\u83b7\u4e86\u4e0e\u6bcf\u4e2a\u4e3b\u9898\u76f8\u5173\u8054\u7684\u60c5\u611f\u3002\u901a\u8fc7\u5c06\u4e3b\u9898\u548c\u60c5\u611f\u4e0e\u4e1a\u52a1\u6307\u6807\u5173\u8054\uff0c\u80fd\u591f\u6df1\u5165\u4e86\u89e3\u7279\u5b9a\u5ba2\u6237\u5173\u6ce8\u70b9\u5982\u4f55\u5f71\u54cd\u4e1a\u52a1\u6210\u679c\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u5728\u521b\u5efa\u8fde\u8d2f\u4e3b\u9898\u65b9\u9762\u6709\u6548\uff0c\u5e76\u8bc4\u4f30\u4e86\u6574\u5408\u4e3b\u9898\u548c\u60c5\u611f\u6a21\u6001\u4ee5\u51c6\u786e\u9884\u6d4b\u661f\u7ea7\u8bc4\u5206\u7684\u65b9\u6cd5\u3002

Conclusion: \u901a\u8fc7\u5728\u610f\u89c1\u5355\u5143\u4e0a\u8fd0\u884c\u4e3b\u9898\u5efa\u6a21\u7ba1\u9053\uff0c\u5e76\u7ed3\u5408\u60c5\u611f\u5206\u6790\u548c\u4e1a\u52a1\u6307\u6807\u5173\u8054\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u66f4\u6709\u6548\u5730\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\uff0c\u91cf\u5316\u5ba2\u6237\u53cd\u9988\u5bf9\u4e1a\u52a1\u6210\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [109] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51faBabel\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4ec5\u5229\u7528\u5355\u8bed\u8bed\u6599\u5e93\u589e\u5f3a\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u7684\u98ce\u683c\u4fdd\u771f\u5ea6\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\u3002


<details>
  <summary>Details</summary>
Motivation: \u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u5728\u8de8\u8bed\u8a00\u4ea4\u6d41\u4e2d\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u4fdd\u7559\u6587\u4f53\u7ec6\u5fae\u5dee\u522b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5e76\u884c\u8bed\u6599\u5e93\u6765\u5b9e\u73b0\u98ce\u683c\u4fdd\u7559\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002

Method: Babel\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u57fa\u4e8e\u4e0a\u4e0b\u6587\u5d4c\u5165\u7684\u98ce\u683c\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u8bc6\u522b\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u4e4b\u95f4\u7684\u98ce\u683c\u5dee\u5f02\uff1b2) \u57fa\u4e8e\u6269\u6563\u7684\u98ce\u683c\u5e94\u7528\u5668\uff0c\u7528\u4e8e\u7ea0\u6b63\u98ce\u683c\u4e0d\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002\u8be5\u6846\u67b6\u4f5c\u4e3a\u540e\u5904\u7406\u6a21\u5757\u4e0e\u73b0\u6709NMT\u7cfb\u7edf\u96c6\u6210\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u6216\u5e76\u884c\u98ce\u683c\u6570\u636e\u3002

Result: \u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\uff08\u6cd5\u5f8b\u3001\u6587\u5b66\u3001\u79d1\u5b66\u5199\u4f5c\u3001\u533b\u5b66\u548c\u6559\u80b2\u5185\u5bb9\uff09\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBabel\u80fd\u591f\u4ee588.21%\u7684\u7cbe\u5ea6\u8bc6\u522b\u98ce\u683c\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5c06\u98ce\u683c\u4fdd\u7559\u5ea6\u63d0\u9ad8150%\uff0c\u540c\u65f6\u4fdd\u63010.92\u7684\u9ad8\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5f97\u5206\u3002\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\uff0c\u7ecf\u8fc7Babel\u4f18\u5316\u7684\u7ffb\u8bd1\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u6e90\u6587\u672c\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u3002

Conclusion: Babel\u6846\u67b6\u6709\u6548\u5730\u589e\u5f3a\u4e86NMT\u7684\u98ce\u683c\u4fdd\u771f\u5ea6\uff0c\u4ec5\u9700\u5355\u8bed\u8bed\u6599\u5e93\uff0c\u4f5c\u4e3a\u540e\u5904\u7406\u6a21\u5757\u96c6\u6210\uff0c\u65e0\u9700\u5bf9\u73b0\u6709NMT\u7cfb\u7edf\u8fdb\u884c\u67b6\u6784\u4fee\u6539\u6216\u4f9d\u8d56\u5e76\u884c\u98ce\u683c\u6570\u636e\u3002

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [110] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: \u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u786e\u5b9a\u6027\u5730\u63a7\u5236\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u751f\u6210\u8bed\u8a00\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u786e\u5b9a\u6027\u5730\u63a7\u5236\u591a\u8bed\u8a00LLMs\u7684\u76ee\u6807\u751f\u6210\u8bed\u8a00\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u56e0\u4e3a\u6b64\u65f6\u65e2\u6ca1\u6709\u660e\u786e\u7684\u8bed\u8a00\u63d0\u793a\uff0c\u4e5f\u65e0\u6cd5\u8fdb\u884c\u5fae\u8c03\u3002

Method: \u7814\u7a76\u4eba\u5458\u5728Gemma-2B\u548cGemma-9B\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u4e0a\u5229\u7528\u9884\u8bad\u7ec3\u7684SAE\uff0c\u8bc6\u522b\u51fa\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u3001\u65e5\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u4e4b\u95f4\u6fc0\u6d3b\u5dee\u5f02\u6700\u663e\u8457\u7684\u7279\u5f81\u3002\u901a\u8fc7\u5728Transformer\u5c42\u4e2d\u4fee\u6539\u5355\u4e2aSAE\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u8bed\u8a00\u7684\u63a7\u5236\u3002

Result: \u901a\u8fc7\u4fee\u6539\u5355\u4e2aSAE\u7279\u5f81\uff0c\u5728\u8bed\u8a00\u8f6c\u79fb\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u8fbe90%\u7684\u6210\u529f\u7387\uff08\u901a\u8fc7FastText\u5206\u7c7b\u6d4b\u91cf\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\uff08\u901a\u8fc7LaBSE\u76f8\u4f3c\u5ea6\u6d4b\u91cf\uff09\u3002\u5206\u6790\u8868\u660e\uff0c\u8bed\u8a00\u63a7\u5236\u5728\u4e2d\u540e\u671fTransformer\u5c42\u6700\u6709\u6548\uff0c\u5e76\u7531\u4e0e\u8bed\u8a00\u654f\u611fSAE\u7279\u5f81\u76f8\u5173\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u653e\u5927\u3002

Conclusion: \u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u7a00\u758f\u7279\u5f81\u64cd\u7eb5\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u3001\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u673a\u5236\uff0c\u7528\u4e8e\u53ef\u63a7\u7684\u591a\u8bed\u8a00\u751f\u6210\u3002

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [111] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51faALIGNed-LLM\uff0c\u4e00\u79cd\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u7684\u5d4c\u5165\u4fe1\u606f\u6ce8\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u6a21\u578b\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6613\u4e8e\u4ea7\u751f\u5e7b\u89c9\u662f\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002\u5c06\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u4e3aKGs\u80fd\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u53ef\u9760\u3001\u9886\u57df\u7279\u5b9a\u548c\u6700\u65b0\u7684\u5916\u90e8\u4fe1\u606f\u3002

Method: ALIGNed-LLM\u53d7LLaVA\u542f\u53d1\uff0c\u91c7\u7528\u7cbe\u76ca\u7b56\u7565\u5c06KGs\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff08KGE\uff09\u6a21\u578b\uff08\u5982TransE\uff09\u7684\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u5c42\u6765\u5bf9\u9f50\u5b9e\u4f53\u548c\u6587\u672c\u5d4c\u5165\u3002\u8fd9\u79cd\u5bf9\u9f50\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u533a\u5206\u76f8\u4f3c\u5b9e\u4f53\uff0c\u4ece\u800c\u63d0\u9ad8\u4e8b\u5b9e\u57fa\u7840\u548c\u51cf\u5c11\u5e7b\u89c9\u3002

Result: \u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u4e0d\u540c\u5927\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u6b27\u6d32\u4e00\u5bb6\u5927\u578b\u4e2d\u592e\u94f6\u884c\u7684\u771f\u5b9e\u4e16\u754c\u91d1\u878d\u7528\u4f8b\uff0c\u8be5\u7528\u4f8b\u5bf9\u51c6\u786e\u6027\u548c\u7cbe\u786e\u6027\u8981\u6c42\u5f88\u9ad8\uff0c\u7ed3\u679c\u8868\u660eLLM\u7684\u56de\u7b54\u6709\u4e86\u5b9e\u8d28\u6027\u6539\u5584\u3002

Conclusion: ALIGNed-LLM\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\u6ce8\u5165LLM\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [112] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
*Liang Lin,Zhihao Xu,Xuehai Tang,Shi Liu,Biyu Zhou,Fuqing Zhu,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bba\u6587\u6458\u8981\u653b\u51fb\u201d\uff08PSA\uff09\u7684\u65b0\u578b\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6743\u5a01\u4fe1\u606f\uff08\u5982\u5b66\u672f\u8bba\u6587\uff09\u7684\u4fe1\u4efb\uff0c\u901a\u8fc7\u4f2a\u9020\u8bba\u6587\u6458\u8981\u6765\u5d4c\u5165\u6076\u610f\u6307\u4ee4\uff0c\u6210\u529f\u5bf9\u591a\u79cdLLMs\u8fdb\u884c\u8d8a\u72f1\u3002


<details>
  <summary>Details</summary>
Motivation: \u4ee5\u5f80\u7814\u7a76\u8868\u660eLLMs\u503e\u5411\u4e8e\u4fe1\u4efb\u6743\u5a01\u6765\u6e90\u7684\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u53ef\u80fd\u6027\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\u3002

Method: \u63d0\u51fa\u201c\u8bba\u6587\u6458\u8981\u653b\u51fb\u201d\uff08PSA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u5408\u6210\u6765\u81ea\u653b\u51fb\u6216\u9632\u5fa1\u5bfc\u5411\u7684LLM\u5b89\u5168\u8bba\u6587\u5185\u5bb9\uff0c\u6784\u5efa\u5bf9\u6297\u6027\u63d0\u793a\u6a21\u677f\uff0c\u5e76\u7b56\u7565\u6027\u5730\u5728\u9884\u5b9a\u4e49\u7684\u5c0f\u8282\u4e2d\u690d\u5165\u6709\u5bb3\u67e5\u8be2\u4f5c\u4e3a\u5bf9\u6297\u6027\u8f7d\u8377\u3002

Result: PSA\u5728\u591a\u79cdLLMs\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6f0f\u6d1e\uff0c\u5305\u62ec\u57fa\u7840LLMs\u548c\u63a8\u7406\u6a21\u578b\u5982Deepseek-R1\u3002\u5b83\u5728Claude3.5-Sonnet\u4e0a\u5b9e\u73b0\u4e8697%\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u5728Deepseek-R1\u4e0a\u5b9e\u73b0\u4e8698%\u7684ASR\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u751a\u81f3\u540c\u4e00\u6a21\u578b\u7684\u4e0d\u540c\u7248\u672c\u5728\u9762\u5bf9\u653b\u51fb\u5bfc\u5411\u6216\u9632\u5fa1\u5bfc\u5411\u8bba\u6587\u65f6\uff0c\u8868\u73b0\u51fa\u622a\u7136\u76f8\u53cd\u7684\u8106\u5f31\u6027\u504f\u89c1\u3002

Conclusion: PSA\u662f\u4e00\u79cd\u9ad8\u6548\u7684LLM\u8d8a\u72f1\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u4fe1\u4efb\u6743\u5a01\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u7684\u663e\u8457\u6f0f\u6d1e\u3002\u7814\u7a76\u4e2d\u53d1\u73b0\u7684\u8106\u5f31\u6027\u504f\u89c1\u73b0\u8c61\u4e3a\u672a\u6765\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\u548c\u5b89\u5168\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u6f5c\u5728\u7ebf\u7d22\u3002

Abstract: The safety of large language models (LLMs) has garnered significant research
attention. In this paper, we argue that previous empirical studies demonstrate
LLMs exhibit a propensity to trust information from authoritative sources, such
as academic papers, implying new possible vulnerabilities. To verify this
possibility, a preliminary analysis is designed to illustrate our two findings.
Based on this insight, a novel jailbreaking method, Paper Summary Attack
(\llmname{PSA}), is proposed. It systematically synthesizes content from either
attack-focused or defense-focused LLM safety paper to construct an adversarial
prompt template, while strategically infilling harmful query as adversarial
payloads within predefined subsections. Extensive experiments show significant
vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning
model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on
well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on
Deepseek-R1. More intriguingly, our work has further revealed diametrically
opposed vulnerability bias across different base models, and even between
different versions of the same model, when exposed to either attack-focused or
defense-focused papers. This phenomenon potentially indicates future research
clues for both adversarial methodologies and safety alignment.Code is available
at https://github.com/233liang/Paper-Summary-Attack

</details>


### [113] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
*Siqi Shen,Mehar Singh,Lajanugen Logeswaran,Moontae Lee,Honglak Lee,Rada Mihalcea*

Main category: cs.CL

TL;DR: \u672c\u6587\u8bc4\u4f30\u4e86LLM\u4ef7\u503c\u89c2\u63a2\u6d4b\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u8868\u8fbe\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5bf9\u8f93\u5165\u6270\u52a8\u654f\u611f\uff0c\u4e14\u63a2\u6d4b\u5230\u7684\u4ef7\u503c\u89c2\u4e0e\u6a21\u578b\u884c\u4e3a\u5173\u8054\u6027\u5f31\uff0c\u5f3a\u8c03\u4e86\u8c28\u614e\u4f7f\u7528\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u524dLLM\u4ef7\u503c\u89c2\u63a2\u6d4b\u65b9\u6cd5\uff08\u5982\u591a\u9879\u9009\u62e9\u9898\uff09\u6613\u53d7\u6270\u52a8\u5f71\u54cd\uff0c\u4e14\u4e0d\u6e05\u695a\u63a2\u6d4b\u5230\u7684\u4ef7\u503c\u89c2\u662f\u5426\u771f\u5b9e\u53cd\u6620\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u6a21\u578b\u5bf9\u5b9e\u9645\u884c\u4e3a\u7684\u504f\u597d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002

Method: \u8bc4\u4f30\u4e86\u4e09\u79cd\u5e38\u7528\u63a2\u6d4b\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u8868\u8fbe\u6027\uff0c\u901a\u8fc7\u6539\u53d8\u63d0\u793a\u548c\u9009\u9879\u6765\u6d4b\u8bd5\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u3002\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u4efb\u52a1\uff1a\u4e00\u662f\u7814\u7a76\u4ef7\u503c\u89c2\u5bf9\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0a\u4e0b\u6587\u7684\u54cd\u5e94\u80fd\u529b\uff1b\u4e8c\u662f\u8bc4\u4f30\u63a2\u6d4b\u5230\u7684\u4ef7\u503c\u89c2\u4e0e\u6a21\u578b\u5728\u4ef7\u503c\u76f8\u5173\u573a\u666f\u4e2d\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002

Result: \u6240\u6709\u63a2\u6d4b\u65b9\u6cd5\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u90fd\u8868\u73b0\u51fa\u8f83\u5927\u5dee\u5f02\u3002\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0a\u4e0b\u6587\u5bf9\u81ea\u7531\u6587\u672c\u751f\u6210\u7684\u5f71\u54cd\u5f88\u5c0f\u3002\u6a21\u578b\u63a2\u6d4b\u5230\u7684\u4ef7\u503c\u89c2\u4e0e\u5176\u5bf9\u57fa\u4e8e\u4ef7\u503c\u884c\u4e3a\u7684\u504f\u597d\u4ec5\u6709\u5fae\u5f31\u5173\u8054\u3002

Conclusion: \u9700\u8981\u66f4\u4ed4\u7ec6\u5730\u5ba1\u67e5LLM\u7684\u4ef7\u503c\u89c2\u63a2\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5145\u5206\u8ba4\u8bc6\u5176\u5c40\u9650\u6027\u3002

Abstract: There has been extensive research on assessing the value orientation of Large
Language Models (LLMs) as it can shape user experiences across demographic
groups. However, several challenges remain. First, while the Multiple Choice
Question (MCQ) setting has been shown to be vulnerable to perturbations, there
is no systematic comparison of probing methods for value probing. Second, it is
unclear to what extent the probed values capture in-context information and
reflect models' preferences for real-world actions. In this paper, we evaluate
the robustness and expressiveness of value representations across three widely
used probing strategies. We use variations in prompts and options, showing that
all methods exhibit large variances under input perturbations. We also
introduce two tasks studying whether the values are responsive to demographic
context, and how well they align with the models' behaviors in value-related
scenarios. We show that the demographic context has little effect on the
free-text generation, and the models' values only weakly correlate with their
preference for value-based actions. Our work highlights the need for a more
careful examination of LLM value probing and awareness of its limitations.

</details>


### [114] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
*Matilde Marcolli,Robert C. Berwick*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6570\u5b66\u8bba\u8bc1\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u4e00\u4e2a\u51fd\u6570\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u5c06\u8bcd\u6c47\u9879\u8868\u793a\u4e3a\u51fd\u6570\uff08\u5982\u5c0f\u6ce2\uff09\uff0c\u6784\u5efa\u4efb\u610f\u53e5\u6cd5\u5bf9\u8c61\u7684\u5fe0\u5b9e\u8868\u793a\uff0c\u5e76\u5c06\u5176\u8d4b\u4e88\u7279\u5b9a\u7684\u4ee3\u6570\u7ed3\u6784\uff08\u5982\u57fa\u4e8eRenyi\u71b5\u7684\u534a\u73af\u3001operad\u4ee3\u6570\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u53e5\u6cd5\u6838\u5fc3\u8ba1\u7b97\u7ed3\u6784\uff08\u5982Merge\u64cd\u4f5c\uff09\u7684\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u7684\u53ef\u80fd\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63d0\u4f9b\u4e00\u4e2a\u6784\u9020\u6027\u8bba\u8bc1\uff0c\u4ee5\u5c55\u793a\u53e5\u6cd5\u6838\u5fc3\u8ba1\u7b97\u7ed3\u6784\u5728\u7406\u8bba\u4e0a\u5b9e\u73b0\u795e\u7ecf\u8ba1\u7b97\u7684\u53ef\u80fd\u6027\u3002

Method: \u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u8bcd\u6c47\u9879\u8868\u793a\u4e3a\u51fd\u6570\uff08\u5982\u5c0f\u6ce2\uff09\u5728\u67d0\u4e2a\u51fd\u6570\u7a7a\u95f4\u4e2d\uff1b2) \u5728\u540c\u4e00\u51fd\u6570\u7a7a\u95f4\u4e2d\u6784\u5efa\u4efb\u610f\u53e5\u6cd5\u5bf9\u8c61\u7684\u5fe0\u5b9e\u8868\u793a\uff1b3) \u4f7f\u7528\u7b2c\u4e8cRenyi\u71b5\u8d4b\u4e88\u8be5\u51fd\u6570\u7a7a\u95f4\u4e00\u4e2a\u53ef\u4ea4\u6362\u975e\u7ed3\u5408\u534a\u73af\u7ed3\u6784\uff0c\u4f7f\u5176\u4e0emagma\u7ed3\u6784\u517c\u5bb9\uff1b4) \u6784\u5efa\u4e00\u4e2aoperad\u4e0a\u7684\u4ee3\u6570\uff0c\u5176\u4e2doperad\u4e2d\u7684\u64cd\u4f5c\u6a21\u62df\u5c06\u8f93\u5165\u6ce2\u5f62\u8f6c\u6362\u4e3a\u7f16\u7801\u53e5\u6cd5\u7ed3\u6784\u7684\u7ec4\u5408\u8f93\u51fa\u7684\u7535\u8def\uff1b5) \u901a\u8fc7\u4e00\u4e2a\u4f59\u79ef\u548cHopf\u4ee3\u6570\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5c06Merge\u64cd\u4f5c\u5fe0\u5b9e\u5730\u5b9e\u73b0\u5728\u8fd9\u4e9b\u7535\u8def\u4e0a\uff1b6) \u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u4f53\u7684\u5b9e\u73b0\u6848\u4f8b\uff0c\u5373\u901a\u8fc7\u6b63\u5f26\u6ce2\u4e0a\u7684\u4ea4\u53c9\u9891\u7387\u76f8\u4f4d\u540c\u6b65\u5b9e\u73b0Merge\u3002

Result: \u4e3b\u8981\u7ed3\u679c\u8868\u660e\uff1a1) \u53ef\u4ee5\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u6784\u5efa\u53e5\u6cd5\u5bf9\u8c61\u7684\u5fe0\u5b9e\u8868\u793a\uff1b2) \u8be5\u8868\u793a\u4e0emagma\u7ed3\u6784\u517c\u5bb9\uff0c\u5e76\u5f62\u6210operad\u4e0a\u7684\u4ee3\u6570\uff1b3) Merge\u64cd\u4f5c\u53ef\u4ee5\u5fe0\u5b9e\u5730\u901a\u8fc7\u7535\u8def\u52a8\u4f5c\u5b9e\u73b0\uff1b4) \u63d0\u4f9b\u4e86\u53e5\u6cd5\u6838\u5fc3\u8ba1\u7b97\u7ed3\u6784\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u7684\u7406\u8bba\u53ef\u80fd\u6027\uff1b5) Merge\u53ef\u4ee5\u88ab\u5b9e\u73b0\u4e3a\u4ea4\u53c9\u9891\u7387\u76f8\u4f4d\u540c\u6b65\uff1b6) Merge\u53ef\u4ee5\u88ab\u8868\u793a\u4e3a\u534a\u73af\u7684\u540e\u7ee7\u51fd\u6570\uff0c\u4ece\u800c\u9610\u660e\u4e86\u5176\u4e0e\u7b97\u672f\u540e\u7ee7\u51fd\u6570\u7684\u76f8\u4f3c\u6027\u3002

Conclusion: \u672c\u6587\u901a\u8fc7\u6570\u5b66\u8bba\u8bc1\u548c\u5177\u4f53\u5b9e\u73b0\u6848\u4f8b\uff0c\u4e3a\u53e5\u6cd5\u6838\u5fc3\u8ba1\u7b97\u7ed3\u6784\uff08\u7279\u522b\u662fMerge\u64cd\u4f5c\uff09\u7684\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6784\u9020\u6027\u8bc1\u636e\uff0c\u5e76\u63ed\u793a\u4e86Merge\u4e0e\u57fa\u672c\u7b97\u672f\u64cd\u4f5c\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\u3002

Abstract: We provide a mathematical argument showing that, given a representation of
lexical items as functions (wavelets, for instance) in some function space, it
is possible to construct a faithful representation of arbitrary syntactic
objects in the same function space. This space can be endowed with a
commutative non-associative semiring structure built using the second Renyi
entropy. The resulting representation of syntactic objects is compatible with
the magma structure. The resulting set of functions is an algebra over an
operad, where the operations in the operad model circuits that transform the
input wave forms into a combined output that encodes the syntactic structure.
The action of Merge on workspaces is faithfully implemented as action on these
circuits, through a coproduct and a Hopf algebra Markov chain. The results
obtained here provide a constructive argument showing the theoretical
possibility of a neurocomputational realization of the core computational
structure of syntax. We also present a particular case of this general
construction where this type of realization of Merge is implemented as a cross
frequency phase synchronization on sinusoidal waves. This also shows that Merge
can be expressed in terms of the successor function of a semiring, thus
clarifying the well known observation of its similarities with the successor
function of arithmetic.

</details>


### [115] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
*Mohamed Achref Ben Ammar,Mohamed Taha Bennani*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u7b80\u5316\u677e\u6563\u5bf9\u8bdd\u7684\u4f1a\u8bdd\u56fe\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u7b80\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u5efa\u6a21\u7684\u8bed\u4e49\u6e05\u6670\u5ea6\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7cfb\u7edf\u65e5\u76ca\u666e\u53ca\uff0c\u5206\u6790\u4f1a\u8bdd\u52a8\u6001\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u4e0e\u8fd9\u4e9b\u7cfb\u7edf\u8fdb\u884c\u4ea4\u4e92\u7684\u5404\u79cd\u573a\u666f\u4e2d\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u4f1a\u8bdd\u56fe\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u201c\u51c6\u6a21\u5f0f\u5316\u5bf9\u8bdd\u201d\u7684\u6d41\u7a0b\u548c\u7ed3\u6784\u3002\u6838\u5fc3\u65b9\u6cd5\u662f\u201cFilter & Reconnect\u201d\u56fe\u7b80\u5316\u6280\u672f\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u566a\u97f3\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002\u8be5\u65b9\u6cd5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u56fe\u7b80\u5316\u6280\u672f\u76f8\u7ed3\u5408\u3002

Result: \u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u7b80\u5316\u6280\u672f\uff0c\u8bed\u4e49\u6307\u6807S\u63d0\u9ad8\u4e862.06\u500d\uff0c\u540c\u65f6\u5f3a\u5236\u5b9e\u73b0\u4e860 \u03b4-\u53cc\u66f2\u5ea6\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u786e\u4fdd\u4e86\u4f1a\u8bdd\u5efa\u6a21\u7684\u6700\u4f73\u6e05\u6670\u5ea6\u3002

Conclusion: \u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u6790\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5728\u76d1\u63a7\u804a\u5929\u673a\u5668\u4eba\u3001\u5bf9\u8bdd\u7ba1\u7406\u5de5\u5177\u548c\u7528\u6237\u884c\u4e3a\u5206\u6790\u7b49\u81ea\u52a8\u5316\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002

Abstract: The analysis of conversational dynamics has gained increasing importance with
the rise of large language model-based systems, which interact with users
across diverse contexts. In this work, we propose a novel computational
framework for constructing conversational graphs that capture the flow and
structure of loosely organized dialogues, referred to as quasi-patterned
conversations. We introduce the Filter & Reconnect method, a novel graph
simplification technique that minimizes noise while preserving semantic
coherence and structural integrity of conversational graphs. Through
comparative analysis, we demonstrate that the use of large language models
combined with our graph simplification technique has resulted in semantic
metric S increasing by a factor of 2.06 compared to previous approaches while
simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity,
ensuring optimal clarity in conversation modeling. This work provides a
computational method for analyzing large-scale dialogue datasets, with
practical applications related to monitoring automated systems such as
chatbots, dialogue management tools, and user behavior analytics.

</details>


### [116] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: \u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u884d\u751f\u7684\u505c\u987f\u7279\u5f81\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u795e\u5206\u88c2\u75c7\u8c31\u7cfb\u969c\u788d\u4e2d\u5f62\u5f0f\u601d\u7ef4\u969c\u788d\uff08FTD\uff09\u4e25\u91cd\u7a0b\u5ea6\u7684\u81ea\u52a8\u5316\u9884\u6d4b\u80fd\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684FTD\u4e34\u5e8a\u8bc4\u4f30\u91cf\u8868\u8d44\u6e90\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002\u81ea\u52a8\u8bed\u97f3\u5206\u6790\uff0c\u7279\u522b\u662f\u5229\u7528ASR\u6355\u83b7\u7684\u505c\u987f\u52a8\u6001\uff0c\u4e3a\u5ba2\u89c2\u91cf\u5316\u8a00\u8bed\u7279\u5f81\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u8bc4\u4f30FTD\u4e25\u91cd\u6027\u65b9\u9762\u7684\u6548\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002

Method: \u7814\u7a76\u6574\u5408\u4e86\u505c\u987f\u7279\u5f81\u4e0e\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u6599\u5e93\uff08\u81ea\u7136\u5f55\u97f3\u65e5\u8bb0AVH\u3001\u7ed3\u6784\u5316\u56fe\u7247\u63cf\u8ff0TOPSY\u3001\u68a6\u5883\u53d9\u8ff0PsyCL\uff09\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002\u4f7f\u7528\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u6a21\u578b\u9884\u6d4b\u4e34\u5e8aFTD\u8bc4\u5206\u3002

Result: \u7814\u7a76\u53d1\u73b0\uff0c\u5355\u72ec\u7684\u505c\u987f\u7279\u5f81\u80fd\u6709\u6548\u9884\u6d4bFTD\u4e25\u91cd\u7a0b\u5ea6\u3002\u4e0e\u4ec5\u4f7f\u7528\u8bed\u4e49\u6a21\u578b\u7684\u9884\u6d4b\u76f8\u6bd4\uff0c\u6574\u5408\u505c\u987f\u7279\u5f81\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4f8b\u5982\u5728TOPSY\u6570\u636e\u96c6\u4e0a\uff0c\u6574\u5408\u6a21\u578b\u7684\u76f8\u5173\u6027\u9ad8\u8fbe\u03c1=0.649\uff0c\u4e25\u91cd\u75c5\u4f8b\u68c0\u6d4b\u7684AUC\u8fbe\u523083.71%\uff08\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4ec5\u8bed\u4e49\u6a21\u578b\u7684\u6700\u4f73\u03c1=0.584\uff0cAUC=79.23%\uff09\u3002\u6027\u80fd\u63d0\u5347\u5728\u6240\u6709\u8bed\u5883\u4e2d\u5747\u4fdd\u6301\u4e00\u81f4\uff0c\u5c3d\u7ba1\u505c\u987f\u6a21\u5f0f\u56e0\u6570\u636e\u96c6\u800c\u5f02\u3002

Conclusion: \u7ed3\u5408\u65f6\u95f4\uff08\u505c\u987f\uff09\u548c\u8bed\u4e49\u5206\u6790\u7684\u6846\u67b6\u4e3a\u5b8c\u5584\u7d0a\u4e71\u8a00\u8bed\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u5e76\u63a8\u52a8\u4e86\u7cbe\u795e\u75c5\u5b66\u9886\u57df\u81ea\u52a8\u5316\u8bed\u97f3\u5206\u6790\u7684\u53d1\u5c55\u3002

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [117] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
*Kirill Borodin,Nikita Vasiliev,Vasiliy Kudryavtsev,Maxim Maslov,Mikhail Gorodnichev,Oleg Rogov,Grach Mkrtchian*

Main category: cs.CL

TL;DR: \u672c\u6587\u4ecb\u7ecd\u4e86Balalaika\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b2000\u591a\u5c0f\u65f6\u9ad8\u8d28\u91cf\u4fc4\u8bed\u8bed\u97f3\u7684\u65b0\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u6a21\u578b\u7684\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u9762\u4e34\u5143\u97f3\u5f31\u5316\u3001\u8f85\u97f3\u6e05\u5316\u3001\u53ef\u53d8\u91cd\u97f3\u6a21\u5f0f\u3001\u540c\u5f62\u5f02\u4e49\u8bcd\u6b67\u4e49\u548c\u4e0d\u81ea\u7136\u8bed\u8c03\u7b49\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002

Method: \u5f15\u5165\u4e86Balalaika\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc72000\u5c0f\u65f6\u7684\u5f55\u97f3\u68da\u7ea7\u4fc4\u8bed\u8bed\u97f3\uff0c\u5e76\u9644\u6709\u5168\u9762\u7684\u6587\u672c\u6807\u6ce8\uff08\u5305\u62ec\u6807\u70b9\u548c\u91cd\u97f3\u6807\u8bb0\uff09\u3002\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u6570\u636e\u96c6\u7684\u6784\u5efa\u6d41\u7a0b\u548c\u6807\u6ce8\u65b9\u6cd5\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728Balalaika\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u5728\u73b0\u6709\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002

Conclusion: Balalaika\u6570\u636e\u96c6\u7684\u5f15\u5165\u6709\u6548\u89e3\u51b3\u4e86\u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u7684\u7279\u6709\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u5173\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u4fc4\u8bed\u8bed\u97f3\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002

Abstract: Russian speech synthesis presents distinctive challenges, including vowel
reduction, consonant devoicing, variable stress patterns, homograph ambiguity,
and unnatural intonation. This paper introduces Balalaika, a novel dataset
comprising more than 2,000 hours of studio-quality Russian speech with
comprehensive textual annotations, including punctuation and stress markings.
Experimental results show that models trained on Balalaika significantly
outperform those trained on existing datasets in both speech synthesis and
enhancement tasks. We detail the dataset construction pipeline, annotation
methodology, and results of comparative evaluations.

</details>


### [118] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: \u672c\u7814\u7a76\u901a\u8fc7\u8bed\u8a00\u5b66\u7279\u5f81\u5206\u6790\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba\u7c7b\u6587\u672c\u8bed\u6cd5\u66f4\u7b80\u5355\u3001\u8bed\u4e49\u66f4\u4e30\u5bcc\uff0c\u4e14\u5728\u4e0d\u540c\u9886\u57df\u95f4\u53d8\u5f02\u6027\u66f4\u5927\uff1b\u65b0\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u8d8b\u4e8e\u540c\u8d28\u5316\u3002


<details>
  <summary>Details</summary>
Motivation: \u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u751f\u6210\u7684\u6587\u672c\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\u8d8a\u6765\u8d8a\u96be\u4ee5\u533a\u5206\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6587\u672c\u5206\u7c7b\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u5b66\u7279\u5f81\u6765\u523b\u753b\u8fd9\u4e24\u79cd\u6587\u672c\u7684\u7279\u6027\u3002

Method: \u7814\u7a76\u9009\u53d6\u4e86\u6765\u81ea8\u4e2a\u9886\u57df\u3001\u753111\u79cd\u4e0d\u540cLLM\u751f\u6210\u7684\u6587\u672c\u6570\u636e\u96c6\u3002\u8ba1\u7b97\u4e86\u5f62\u6001\u3001\u53e5\u6cd5\u3001\u8bed\u4e49\u7b49\u591a\u4e2a\u8bed\u8a00\u5b66\u5c42\u9762\u7684\u7279\u5f81\uff08\u5982\u4f9d\u5b58\u957f\u5ea6\u3001\u60c5\u611f\u6027\uff09\uff0c\u5e76\u7ed3\u5408\u4e0d\u540c\u7684\u91c7\u6837\u7b56\u7565\u3001\u91cd\u590d\u63a7\u5236\u548c\u6a21\u578b\u53d1\u5e03\u65e5\u671f\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528\u4e86\u98ce\u683c\u5d4c\u5165\u6765\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u53d8\u5f02\u6027\u3002

Result: \u7edf\u8ba1\u5206\u6790\u663e\u793a\uff0c\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\u503e\u5411\u4e8e\u5c55\u73b0\u66f4\u7b80\u5355\u7684\u53e5\u6cd5\u7ed3\u6784\u548c\u66f4\u591a\u6837\u5316\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u4eba\u7c7b\u548c\u673a\u5668\u6587\u672c\u5728\u4e0d\u540c\u9886\u57df\u90fd\u8868\u73b0\u51fa\u6587\u4f53\u591a\u6837\u6027\uff0c\u4f46\u4eba\u7c7b\u6587\u672c\u5728\u6240\u9009\u7279\u5f81\u4e0a\u7684\u53d8\u5f02\u6027\u66f4\u5927\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8f83\u65b0\u7684\u6a21\u578b\u8f93\u51fa\u7684\u6587\u672c\u53d8\u5f02\u6027\u76f8\u4f3c\uff0c\u8fd9\u8868\u660e\u673a\u5668\u751f\u6210\u6587\u672c\u8d8b\u4e8e\u540c\u8d28\u5316\u3002

Conclusion: \u4eba\u7c7b\u6587\u672c\u5728\u53e5\u6cd5\u7ed3\u6784\u548c\u8bed\u4e49\u591a\u6837\u6027\u4e0a\u4e0e\u673a\u5668\u6587\u672c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u4eba\u7c7b\u6587\u672c\u7684\u98ce\u683c\u53d8\u5f02\u6027\u66f4\u5927\u3002\u65b0\u4e00\u4ee3LLM\u751f\u6210\u7684\u6587\u672c\u98ce\u683c\u8d8b\u4e8e\u540c\u8d28\u5316\uff0c\u8fd9\u4e3a\u6587\u672c\u68c0\u6d4b\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [119] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: \u672c\u6587\u4ecb\u7ecd\u4e86Seed-X\uff0c\u4e00\u4e2a70\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u6570\u636e\u9884\u8bad\u7ec3\u3001\u601d\u7ef4\u94fe\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u7ffb\u8bd1\u80fd\u529b\uff0c\u6027\u80fd\u53ef\u4e0e\u9876\u7ea7\u95ed\u6e90\u6a21\u578b\u5ab2\u7f8e\uff0c\u5e76\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u591a\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u65f6\uff0c\u9762\u4e34\u590d\u6742\u7684\u8bed\u8a00\u6a21\u5f0f\u548c\u81ea\u52a8\u5316\u7ffb\u8bd1\u4e2d\u51fa\u73b0\u7684\u751f\u786c\u7ffb\u8bd1\u7b49\u6311\u6218\u3002

Method: \u8be5\u7814\u7a76\u5f15\u5165\u4e86Seed-X\u6a21\u578b\u5bb6\u65cf\uff08\u5305\u542b\u6307\u4ee4\u548c\u63a8\u7406\u6a21\u578b\uff09\uff0c\u5176\u57fa\u7840\u6a21\u578b\u5728\u5305\u542b28\u79cd\u8bed\u8a00\u7684\u5355\u8bed\u548c\u53cc\u8bed\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u6307\u4ee4\u6a21\u578b\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b9e\u73b0\u7ffb\u8bd1\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u4e00\u6b65\u589e\u5f3a\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u8bed\u8a00\u5bf9\u6cdb\u5316\u80fd\u529b\u3002

Result: Seed-X\u572828\u79cd\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u4e0e\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\uff08\u5305\u62ecGemini-2.5\u548cGPT-4o\uff09\u76f8\u5f53\uff0c\u5e76\u5728\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002

Conclusion: Seed-X\u572870\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u5c06\u7ffb\u8bd1\u80fd\u529b\u63a8\u5411\u6781\u9650\uff0c\u5b9e\u73b0\u4e86\u4e0e\u9876\u7ea7\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002\u7814\u7a76\u56e2\u961f\u5206\u4eab\u4e86\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u53c2\u6570\uff0c\u4ee5\u4fc3\u8fdb\u7ffb\u8bd1\u7814\u7a76\u548c\u5e94\u7528\u7684\u53d1\u5c55\u3002

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [120] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
*Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: CU-ICU\u662f\u4e00\u79cd\u9488\u5bf9ICU\u6570\u636e\u96c6\u5b9a\u5236\u65e0\u76d1\u7763\u6307\u4ee4\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u5fae\u8c03\u548c\u5c11\u91cf\u53c2\u6570\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86ICU\u4efb\u52a1\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6574\u5408\u5230\u533b\u7597\u4fdd\u5065\u7b49\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u72ec\u7279\u7684\u6311\u6218\uff0c\u5305\u62ec\u9886\u57df\u9002\u5e94\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86CU-ICU\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eT5\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u6837\u672c\u63d0\u793a\u548c\u9009\u62e9\u6027\u53c2\u6570\u66f4\u65b0\u7684\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b9a\u5236\u7528\u4e8eICU\u6570\u636e\u96c6\u7684\u65e0\u76d1\u7763\u6307\u4ee4\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u76d1\u7763\u4e0b\u7684\u9ad8\u6548\u9002\u5e94\u3002

Result: CU-ICU\u5728\u65e9\u671f\u8d25\u8840\u75c7\u68c0\u6d4b\u3001\u6b7b\u4ea1\u7387\u9884\u6d4b\u548c\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\u7b49\u5173\u952eICU\u4efb\u52a1\u4e0a\uff0c\u76f8\u8f83\u4e8e\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u6700\u6709\u6548\u7684\u914d\u7f6e\u4e0b\uff0c\u8d25\u8840\u75c7\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe15%\uff0c\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u89e3\u91ca\u7684\u80fd\u529b\u589e\u5f3a\u4e8620%\uff0c\u540c\u65f6\u6a21\u578b\u53c2\u6570\u66f4\u65b0\u5c11\u4e8e1%\u3002

Conclusion: CU-ICU\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4f4e\u5f00\u9500\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4e3a\u771f\u5b9e\u4e16\u754c\u7684ICU\u73af\u5883\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002

Abstract: Integrating large language models into specialized domains like healthcare
presents unique challenges, including domain adaptation and limited labeled
data. We introduce CU-ICU, a method for customizing unsupervised
instruction-finetuned language models for ICU datasets by leveraging the
Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse
fine-tuning approach that combines few-shot prompting with selective parameter
updates, enabling efficient adaptation with minimal supervision. Our evaluation
across critical ICU tasks--early sepsis detection, mortality prediction, and
clinical note generation--demonstrates that CU-ICU consistently improves
predictive accuracy and interpretability over standard fine-tuning methods.
Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and
a 20% enhancement in generating clinically relevant explanations while updating
fewer than 1% of model parameters in its most efficient configuration. These
results establish CU-ICU as a scalable, low-overhead solution for delivering
accurate and interpretable clinical decision support in real-world ICU
environments.

</details>


### [121] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
*Woo-Chan Kim,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKiC\uff08Keyword-inspired Cascade\uff09\u7684\u65b0\u578b\u7ea7\u8054\u6846\u67b6\uff0c\u7528\u4e8e\u6210\u672c\u9ad8\u6548\u7684\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u751f\u6210\u3002KiC\u901a\u8fc7\u8bc4\u4f30\u5f31\u6a21\u578b\u591a\u4e2a\u8f93\u51fa\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u6765\u51b3\u5b9a\u662f\u5426\u5347\u7ea7\u5230\u5f3a\u6a21\u578b\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eAPI\u6210\u672c\u3002


<details>
  <summary>Details</summary>
Motivation: \u9ad8\u6027\u80fd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u901a\u8fc7API\u8bbf\u95ee\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u3002\u73b0\u6709\u7ea7\u8054\u65b9\u6cd5\u867d\u7136\u65e8\u5728\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u4f9d\u8d56\u4e8e\u7cbe\u786e\u6587\u672c\u5339\u914d\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u7531\u5f62\u5f0f\u8f93\u51fa\u6765\u8bf4\u4e0d\u53ef\u9760\uff0c\u96be\u4ee5\u9009\u62e9\u4ee3\u8868\u6027\u54cd\u5e94\u548c\u8bc4\u4f30\u6574\u4f53\u53ef\u9760\u6027\u3002

Method: KiC\u6846\u67b6\u9996\u5148\u4ece\u4e00\u4e2a\u8f83\u5f31\u7684\u6a21\u578b\u751f\u6210\u591a\u4e2a\u8f93\u51fa\u3002\u7136\u540e\uff0c\u5b83\u8bc6\u522b\u5176\u4e2d\u6700\u5177\u4ee3\u8868\u6027\u7684\u7b54\u6848\uff0c\u5e76\u8bc4\u4f30\u5176\u4ed6\u54cd\u5e94\u4e0e\u8be5\u4ee3\u8868\u6027\u7b54\u6848\u7684\u8bed\u4e49\u5bf9\u9f50\u7a0b\u5ea6\u3002\u6839\u636e\u5bf9\u9f50\u7a0b\u5ea6\uff0cKiC\u51b3\u5b9a\u662f\u63a5\u53d7\u5f31\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u8fd8\u662f\u5347\u7ea7\u5230\u66f4\u5f3a\u7684\u6a21\u578b\u3002

Result: \u5728\u4e09\u4e2a\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKiC\u8fbe\u5230\u4e86GPT-4 97.53%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5e73\u5747\u964d\u4f4e\u4e8628.81%\u7684API\u6210\u672c\uff0c\u751a\u81f3\u5728\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86GPT-4\u3002

Conclusion: KiC\u662f\u4e00\u79cd\u6709\u6548\u4e14\u6210\u672c\u9ad8\u6548\u7684\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u5b83\u80fd\u591f\u5728\u4fdd\u6301\u63a5\u8fd1\u6700\u5148\u8fdb\u6a21\u578b\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4eAPI\u6210\u672c\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u80fd\u8d85\u8d8a\u5f3a\u6a21\u578b\u7684\u6027\u80fd\u3002

Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance
across a wide range of natural language processing tasks. However,
high-performing models are typically accessible only via APIs, incurring
substantial inference costs. Cascade methods address this by initially
employing a cheaper model and escalating to a stronger one only when necessary.
Nevertheless, existing cascade approaches struggle to select a reliable
representative response and assess the overall reliability of free-form
outputs, as they rely on exact text matching. To overcome these limitations, we
propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient
free-form text generation. KiC identifies the most representative answer among
multiple outputs from a weaker model and evaluates the semantic alignment of
other responses with it. Based on the degree of alignment, KiC determines
whether to accept the weaker model's output or escalate to a stronger model.
Experiments on three free-form text generation benchmarks show that KiC
achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81
percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [122] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: LoopServe\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u53cc\u9636\u6bb5\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u5728\u7ebf\u7a00\u758f\u5316\u548c\u6e10\u8fdb\u5f0f\u952e\u503c\u538b\u7f29\u6765\u63d0\u9ad8\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\uff0c\u968f\u7740\u5bf9\u8bdd\u5386\u53f2\u53d8\u957f\uff0c\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u5f71\u54cd\u5176\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002\u5f53\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u6216\u57fa\u4e8e\u4f4d\u7f6e\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u9002\u5e94\u591a\u8f6e\u5bf9\u8bdd\u7684\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u6a21\u5f0f\u3002

Method: LoopServe\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u53cc\u9636\u6bb5\u63a8\u7406\u52a0\u901f\u6846\u67b6\u3002\u5b83\u5728\u9884\u586b\u5145\u9636\u6bb5\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6ce8\u610f\u529b\u77e9\u9635\u4e2d\u6700\u91cd\u8981\u90e8\u5206\u8fdb\u884c\u5728\u7ebf\u7a00\u758f\u5316\uff1b\u5728\u89e3\u7801\u9636\u6bb5\u901a\u8fc7\u57fa\u4e8e\u6700\u65b0\u751f\u6210\u8f93\u51fatoken\u81ea\u9002\u5e94\u7ef4\u62a4\u76f8\u5173\u4e14\u9ad8\u6548\u7684\u7f13\u5b58\u6765\u4f7f\u7528\u6e10\u8fdb\u5f0f\u952e\u503c\u538b\u7f29\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b11\u4e2a\u591a\u8f6e\u6570\u636e\u96c6\u7684\u65b0\u57fa\u51c6\uff0c\u4ee5\u53cd\u6620\u771f\u5b9e\u7684\u67e5\u8be2\u4f4d\u7f6e\u548c\u5bf9\u8bdd\u4f9d\u8d56\u6027\u3002

Result: LoopServe\u5728\u5e7f\u6cdb\u7684\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6709\u6548\u6027\uff0c\u5e76\u663e\u8457\u52a0\u901f\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u3002

Conclusion: LoopServe\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u63a8\u7406\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u957f\u4e0a\u4e0b\u6587\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [123] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: \u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7fa4\u7ec4\u63a8\u8350\u7cfb\u7edf\uff08GRS\uff09\u4e2d\u4f5c\u4e3a\u51b3\u7b56\u8005\u548c\u89e3\u91ca\u751f\u6210\u5668\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u63a8\u8350\u7ed3\u679c\u5e38\u4e0e\u52a0\u6027\u6548\u7528\u805a\u5408\uff08ADD\uff09\u76f8\u4f3c\uff0c\u4f46\u89e3\u91ca\u5b58\u5728\u4e0d\u4e00\u81f4\u548c\u6a21\u7cca\u6027\uff0c\u5e76\u5e38\u63d0\u53ca\u989d\u5916\u6807\u51c6\u3002


<details>
  <summary>Details</summary>
Motivation: LLMs\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5cGRS\u4e2d\u7684\u8054\u5408\u51b3\u7b56\u8005\u548c\u89e3\u91ca\u751f\u6210\u5668\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u63a8\u8350\u548c\u89e3\u91ca\u4e0e\u57fa\u4e8e\u793e\u4f1a\u9009\u62e9\u7684\u805a\u5408\u7b56\u7565\u76f8\u6bd4\u7684\u6709\u6548\u6027\u3002

Method: \u901a\u8fc7\u5c06LLM\u751f\u6210\u7684\u63a8\u8350\u548c\u89e3\u91ca\u4e0e\u57fa\u4e8e\u793e\u4f1a\u9009\u62e9\u7684\u805a\u5408\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc4\u4f30\u5176\u8868\u73b0\u3002

Result: LLM\u751f\u6210\u7684\u63a8\u8350\u901a\u5e38\u4e0e\u52a0\u6027\u6548\u7528\u805a\u5408\uff08ADD\uff09\u7b56\u7565\u76f8\u4f3c\u3002\u7136\u800c\uff0c\u89e3\u91ca\u901a\u5e38\u63d0\u53ca\u5e73\u5747\u8bc4\u5206\uff08\u4e0eADD\u76f8\u4f3c\u4f46\u4e0d\u5b8c\u5168\u76f8\u540c\uff09\uff0c\u4e14\u7fa4\u7ec4\u7ed3\u6784\uff08\u7edf\u4e00\u6216\u5206\u6b67\uff09\u5bf9\u63a8\u8350\u6ca1\u6709\u5f71\u54cd\u3002\u6b64\u5916\uff0cLLMs\u5e38\u5728\u89e3\u91ca\u4e2d\u58f0\u79f0\u4f7f\u7528\u4e86\u989d\u5916\u7684\u6807\u51c6\uff0c\u5982\u7528\u6237\u6216\u7269\u54c1\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\u6216\u672a\u5b9a\u4e49\u7684\u6d41\u884c\u5ea6\u6307\u6807/\u9608\u503c\u3002\u89e3\u91ca\u4e2d\u989d\u5916\u7684\u6807\u51c6\u4f9d\u8d56\u4e8e\u7fa4\u7ec4\u573a\u666f\u4e2d\u7684\u8bc4\u5206\u6570\u91cf\uff0c\u8fd9\u8868\u660e\u6807\u51c6\u805a\u5408\u65b9\u6cd5\u5728\u66f4\u5927\u7684\u7269\u54c1\u96c6\u5c3a\u5bf8\u4e0b\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\u3002\u540c\u65f6\uff0c\u4e0d\u4e00\u81f4\u548c\u6a21\u7cca\u7684\u89e3\u91ca\u524a\u5f31\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u5bf9LLMs\u5728GRS\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u4ee5\u53ca\u6807\u51c6\u805a\u5408\u7b56\u7565\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u89e3\u91ca\u4e2d\u989d\u5916\u6807\u51c6\u7684\u5b58\u5728\u8868\u660e\u6807\u51c6\u805a\u5408\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u7269\u54c1\u65f6\u53ef\u80fd\u6548\u7387\u4e0d\u9ad8\u3002\u6b64\u5916\uff0cLLM\u89e3\u91ca\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u6a21\u7cca\u6027\u635f\u5bb3\u4e86\u5176\u5728GRS\u4e2d\u5e94\u7528\u7684\u5173\u952e\u52a8\u673a\u2014\u2014\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [124] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: \u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6cd5\u56fd\u4e0a\u8bc9\u6cd5\u9662\u7684\u513f\u7ae5\u76d1\u62a4\u6743\u5224\u51b3\uff0c\u53d1\u73b0\u4e2a\u522b\u6cd5\u5b98\u7684\u51b3\u7b56\u6a21\u5f0f\u663e\u8457\u5f71\u54cd\u6848\u4ef6\u7ed3\u679c\uff0c\u652f\u6301\u4e86\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u89c2\u70b9\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6cd5\u5b98\u5728\u6cd5\u5f8b\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u68c0\u9a8c\u4e2a\u522b\u6cd5\u5b98\u7684\u51b3\u7b56\u6a21\u5f0f\u662f\u5426\u663e\u8457\u5f71\u54cd\u6848\u4ef6\u7ed3\u679c\uff0c\u6311\u6218\u4e86\u6cd5\u5b98\u662f\u4e2d\u7acb\u53d8\u91cf\u5e76\u7edf\u4e00\u9002\u7528\u6cd5\u5f8b\u7684\u5047\u8bbe\u3002\u8fd9\u4e0e\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49-\u5f62\u5f0f\u4e3b\u4e49\u8fa9\u8bba\u76f8\u5173\u3002

Method: \u7814\u7a76\u4f7f\u7528\u4e8610,306\u4e2a\u6848\u4ef6\u4e2d\u768418,937\u4efd\u513f\u7ae5\u76d1\u62a4\u6743\u5224\u51b3\u4e66\u3002\u91c7\u7528\u6df7\u5408\u9884\u6d4b\u7ba1\u9053\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08RF\u3001XGB\u548cSVC\uff09\u8fdb\u884c\u7ed3\u679c\u9884\u6d4b\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u4e8e\u4e2a\u522b\u6cd5\u5b98\u8fc7\u5f80\u5224\u51b3\u8bad\u7ec3\u7684\u201c\u4e13\u5bb6\u6a21\u578b\u201d\u4e0e\u57fa\u4e8e\u805a\u5408\u6570\u636e\u8bad\u7ec3\u7684\u201c\u666e\u9002\u6a21\u578b\u201d\u3002\u4e3a\u9075\u5b88\u9690\u79c1\u6cd5\uff0c\u6570\u636e\u7ecf\u8fc7\u4e25\u683c\u7684\u533f\u540d\u5316\u5904\u7406\u3002

Result: \u7ed3\u679c\u663e\u793a\uff0c\u4e13\u5bb6\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u59cb\u7ec8\u9ad8\u4e8e\u666e\u9002\u6a21\u578b\u3002\u8868\u73b0\u6700\u4f73\u7684\u4e13\u5bb6\u6a21\u578bF1\u5206\u6570\u9ad8\u8fbe92.85%\uff0c\u800c\u666e\u9002\u6a21\u578b\uff08\u8bad\u7ec3\u6570\u636e\u91cf\u591a20\u81f3100\u500d\uff09\u7684F1\u5206\u6570\u4e3a82.63%\u3002\u8fd9\u8868\u660e\u4e13\u5bb6\u6a21\u578b\u6355\u6349\u5230\u4e86\u7a33\u5b9a\u7684\u4e2a\u4f53\u51b3\u7b56\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u4e0d\u53ef\u8f6c\u79fb\u5230\u5176\u4ed6\u6cd5\u5b98\u3002\u57df\u5185\u548c\u8de8\u57df\u6709\u6548\u6027\u6d4b\u8bd5\u4e3a\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002

Conclusion: \u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0c\u53f8\u6cd5\u8eab\u4efd\u5728\u6cd5\u5f8b\u7ed3\u679c\u4e2d\u626e\u6f14\u7740\u53ef\u8861\u91cf\u7684\u89d2\u8272\uff0c\u4e3a\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5b9e\u8bc1\u652f\u6301\uff0c\u6311\u6218\u4e86\u6cd5\u5b98\u662f\u4e2d\u7acb\u53d8\u91cf\u7684\u4f20\u7edf\u89c2\u5ff5\u3002

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [125] [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)
*Maluna Menke,Thilo Hagendorff*

Main category: cs.CL

TL;DR: \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u9488\u5bf9LGBTQIA+\u7fa4\u4f53\u7684\u504f\u89c1\u3002\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672fLoRA\uff0c\u5728\u7cbe\u9009\u7684QueerNews\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u80fd\u6709\u6548\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u5730\u663e\u8457\u964d\u4f4e\u8fd9\u4e9b\u504f\u89c1\u3002


<details>
  <summary>Details</summary>
Motivation: LLMs\u5728\u8bad\u7ec3\u8bed\u6599\u4e2d\u590d\u5236\u4e86\u6027\u522b\u548c\u6027\u53d6\u5411\u504f\u89c1\uff0c\u5bfc\u81f4\u8f93\u51fa\u6b67\u89c6LGBTQIA+\u7528\u6237\uff0c\u56e0\u6b64\u51cf\u5c11\u8fd9\u4e9b\u504f\u89c1\u81f3\u5173\u91cd\u8981\u3002

Method: \u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\uff1aLoRA\u548c\u8f6f\u63d0\u793a\u8c03\u4f18\uff0c\u4f5c\u4e3a\u5168\u6a21\u578b\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002\u4f7f\u7528WinoQueer\u57fa\u51c6\u6d4b\u8bd5\u91cf\u5316\u4e86\u4e09\u4e2a\u5f00\u6e90LLMs\u7684\u504f\u89c1\uff0c\u5e76\u4f7f\u7528\u7cbe\u9009\u7684QueerNews\u8bed\u6599\u5e93\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002

Result: \u57fa\u7ebf\u504f\u89c1\u5f97\u5206\u9ad8\u8fbe98\uff08\u6ee1\u5206100\uff0c50\u4e3a\u4e2d\u7acb\uff09\u3002LoRA\u5fae\u8c03\uff08<0.1%\u989d\u5916\u53c2\u6570\uff09\u5c06\u504f\u89c1\u5f97\u5206\u964d\u4f4e\u4e86\u591a\u8fbe50\u70b9\uff0c\u5e76\u5c06\u4e2d\u7acb\u6027\u4ece\u51e0\u4e4e0%\u63d0\u9ad8\u523036%\u3002\u8f6f\u63d0\u793a\u8c03\u4f18\uff0810\u4e2a\u865a\u62dftokens\uff09\u4ec5\u5e26\u6765\u5fae\u5c0f\u6539\u8fdb\u3002

Conclusion: LoRA\u80fd\u4ee5\u6700\u5c0f\u7684\u8ba1\u7b97\u6210\u672c\u5e26\u6765\u663e\u8457\u7684\u516c\u5e73\u6027\u63d0\u5347\u3002\u7814\u7a76\u5021\u5bfc\u66f4\u5e7f\u6cdb\u5730\u91c7\u7528\u793e\u533a\u53c2\u4e0e\u7684PEFT\u6280\u672f\uff0c\u521b\u5efa\u66f4\u5927\u7684\u7531\u9177\u513f\u4f5c\u8005\u64b0\u5199\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u5f00\u53d1\u6bd4WinoQueer\u66f4\u4e30\u5bcc\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u540c\u65f6\u8fdb\u884c\u6301\u7eed\u5ba1\u8ba1\u4ee5\u4fdd\u6301LLMs\u7684\u5305\u5bb9\u6027\u3002

Abstract: Large Language Models (LLMs) frequently reproduce the gender- and
sexual-identity prejudices embedded in their training corpora, leading to
outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of
great importance. To achieve this, we evaluate two parameter-efficient
fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt
tuning - as lightweight alternatives to full-model fine-tuning for mitigating
such biases. Using the WinoQueer benchmark, we quantify bias in three
open-source LLMs and observe baseline bias scores reaching up to 98 (out of
100) across a range of queer identities defined by gender and/or sexual
orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%
additional parameters) on a curated QueerNews corpus reduces those scores by up
to 50 points and raises neutrality from virtually 0% to as much as 36%.
Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements.
These findings show that LoRA can deliver meaningful fairness gains with
minimal computation. We advocate broader adoption of community-informed PEFT,
the creation of larger queer-authored corpora, and richer evaluation suites
beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.

</details>


### [126] [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)
*Palash Nandi,Maithili Joshi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: \u672c\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u63d0\u793a\u8bcd\uff08prompt\uff09\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u4e0d\u5f53\u5185\u5bb9\u7684\u751f\u6210\uff0c\u53d1\u73b0VLM\u5728\u591a\u6a21\u6001\u73af\u5883\u4e0b\u5bf9\u6076\u610f\u8f93\u5165\u7684\u8bc6\u522b\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u7279\u5b9a\u7684\u63d0\u793a\u8bcd\u5143\u7d20\u3001\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u751a\u81f3\u65e0\u5bb3\u7684\u56fe\u7247\uff08\u5982\u8868\u60c5\u5305\uff09\u90fd\u80fd\u6210\u529f\u89e6\u53d1\u8d8a\u72f1\u3002


<details>
  <summary>Details</summary>
Motivation: \u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd\u8868\u8ff0\u9ad8\u5ea6\u654f\u611f\uff0c\u5fae\u5c0f\u8f93\u5165\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u622a\u7136\u4e0d\u540c\u3002\u8fd9\u5f15\u51fa\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u63d0\u793a\u8bcd\u654f\u611f\u6027\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\uff1f\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8VLM\u4e2d\u63d0\u793a\u8bcd\u7684\u79bb\u6563\u7ec4\u6210\u90e8\u5206\u5982\u4f55\u5f71\u54cd\u4e0d\u5f53\u5185\u5bb9\u7684\u751f\u6210\u3002

Method: \u7814\u7a76\u5206\u6790\u4e86\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\u5bf9\u6210\u529f\u8d8a\u72f1\u7684\u5f71\u54cd\uff1aa) \u5305\u542b\u8be6\u7ec6\u89c6\u89c9\u4fe1\u606f\uff0cb) \u5b58\u5728\u5bf9\u6297\u6027\u793a\u4f8b\uff0cc) \u4f7f\u7528\u79ef\u6781\u63aa\u8f9e\u7684\u5f00\u5934\u77ed\u8bed\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\uff08\u5c11\u81f3\u4e09\u4e2a\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528VLM\u5185\u90e8\u4e24\u5c42\u4e4b\u95f4\u8df3\u8dc3\u8fde\u63a5\uff08skip-connection\uff09\u7684\u6846\u67b6\u4ee5\u63d0\u9ad8\u8d8a\u72f1\u6210\u529f\u7387\u3002

Result: \u7814\u7a76\u53d1\u73b0\uff0cVLM\u5728\u5355\u6a21\u6001\uff08\u7eaf\u6587\u672c\u6216\u7eaf\u56fe\u50cf\uff09\u8bbe\u7f6e\u4e0b\u80fd\u53ef\u9760\u533a\u5206\u826f\u6027\u548c\u6709\u5bb3\u8f93\u5165\uff0c\u4f46\u5728\u591a\u6a21\u6001\u73af\u5883\u4e0b\uff0c\u8fd9\u79cd\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002\u4e0a\u8ff0\u4e09\u4e2a\u56e0\u7d20\u4e2d\u7684\u6bcf\u4e2a\u56e0\u7d20\u90fd\u80fd\u72ec\u7acb\u89e6\u53d1\u8d8a\u72f1\u3002\u5373\u4f7f\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e5f\u80fd\u4fc3\u4f7f\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u8f93\u51fa\u3002\u6240\u63d0\u51fa\u7684\u8df3\u8dc3\u8fde\u63a5\u6846\u67b6\u80fd\u5927\u5e45\u63d0\u9ad8\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u5373\u4f7f\u4f7f\u7528\u826f\u6027\u56fe\u50cf\u4e5f\u80fd\u594f\u6548\u3002\u6b64\u5916\uff0c\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u5e7d\u9ed8\u6216\u65e0\u5bb3\u7684\u8868\u60c5\u5305\uff0c\u5728\u5f15\u53d1\u6709\u5bb3\u5185\u5bb9\u65b9\u9762\u4e0e\u6709\u6bd2\u89c6\u89c9\u5185\u5bb9\u540c\u6837\u6709\u6548\u3002

Conclusion: \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5b58\u5728\u5fae\u5999\u800c\u590d\u6742\u7684\u6f0f\u6d1e\uff0c\u5373\u4f7f\u662f\u770b\u4f3c\u65e0\u5bb3\u7684\u8f93\u5165\u6216\u5de7\u5999\u7684\u63d0\u793a\u8bcd\u8bbe\u8ba1\u4e5f\u80fd\u88ab\u5229\u7528\u6765\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u3002\u8fd9\u51f8\u663e\u4e86VLM\u5728\u591a\u6a21\u6001\u5b89\u5168\u65b9\u9762\u7684\u6311\u6218\u3002

Abstract: Language models are highly sensitive to prompt formulations - small changes
in input can drastically alter their output. This raises a critical question:
To what extent can prompt sensitivity be exploited to generate inapt content?
In this paper, we investigate how discrete components of prompt design
influence the generation of inappropriate content in Visual Language Models
(VLMs). Specifically, we analyze the impact of three key factors on successful
jailbreaks: (a) the inclusion of detailed visual information, (b) the presence
of adversarial examples, and (c) the use of positively framed beginning
phrases. Our findings reveal that while a VLM can reliably distinguish between
benign and harmful inputs in unimodal settings (text-only or image-only), this
ability significantly degrades in multimodal contexts. Each of the three
factors is independently capable of triggering a jailbreak, and we show that
even a small number of in-context examples (as few as three) can push the model
toward generating inappropriate outputs. Furthermore, we propose a framework
that utilizes a skip-connection between two internal layers of the VLM, which
substantially increases jailbreak success rates, even when using benign images.
Finally, we demonstrate that memes, often perceived as humorous or harmless,
can be as effective as toxic visuals in eliciting harmful content, underscoring
the subtle and complex vulnerabilities of VLMs.

</details>


### [127] [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Xuemeng Song,Tian Gan,Liqiang Nie*

Main category: cs.CL

TL;DR: \u9488\u5bf9\u77ed\u6587\u672c\u805a\u7c7b\u4e2d\u7684\u7a00\u758f\u6027\u3001\u9ad8\u7ef4\u5ea6\u548c\u8ba1\u7b97\u5f3a\u5ea6\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72c4\u5229\u514b\u96f7\u591a\u9879\u5f0f\u6df7\u5408\u6a21\u578b\u7684Gibbs\u91c7\u6837\u7b97\u6cd5\uff08GSDMM\uff09\u53ca\u5176\u6539\u8fdb\u7248\uff08GSDMM+\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u5316\u3001\u8bcd\u6743\u91cd\u8c03\u6574\u548c\u805a\u7c7b\u5408\u5e76\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6548\u7387\u548c\u6548\u679c\u3002


<details>
  <summary>Details</summary>
Motivation: \u77ed\u6587\u672c\u805a\u7c7b\u5728\u793e\u4ea4\u5a92\u4f53\u666e\u53ca\u7684\u80cc\u666f\u4e0b\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u56fa\u6709\u7684\u7a00\u758f\u6027\u3001\u5927\u89c4\u6a21\u548c\u9ad8\u7ef4\u5ea6\u7279\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u5f3a\u5ea6\uff0c\u4f7f\u5f97\u8be5\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86GSDMM\uff08Dirichlet Multinomial Mixture\u6a21\u578b\u7684collapsed Gibbs Sampling\u7b97\u6cd5\uff09\uff0c\u6709\u6548\u5904\u7406\u77ed\u6587\u672c\u7684\u7a00\u758f\u6027\u548c\u9ad8\u7ef4\u5ea6\uff0c\u5e76\u8bc6\u522b\u6bcf\u4e2a\u805a\u7c7b\u7684\u4ee3\u8868\u6027\u8bcd\u6c47\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7248GSDMM+\uff0c\u901a\u8fc7\u51cf\u5c11\u521d\u59cb\u5316\u566a\u58f0\u3001\u57fa\u4e8e\u71b5\u81ea\u9002\u5e94\u8c03\u6574\u8bcd\u6743\u91cd\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u805a\u7c7b\uff0c\u5e76\u91c7\u7528\u7b56\u7565\u6027\u805a\u7c7b\u5408\u5e76\u6765\u4f18\u5316\u805a\u7c7b\u7c92\u5ea6\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u771f\u5b9e\u7c7b\u522b\u5206\u5e03\u5bf9\u9f50\u3002

Result: \u901a\u8fc7\u4e0e\u7ecf\u5178\u548c\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\uff08GSDMM\u548cGSDMM+\uff09\u5177\u6709\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002

Conclusion: \u6240\u63d0\u51fa\u7684GSDMM\u548cGSDMM+\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u77ed\u6587\u672c\u805a\u7c7b\u7684\u6311\u6218\uff0c\u5e76\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u8868\u73b0\u51fa\u8272\u3002

Abstract: Short text clustering has become increasingly important with the popularity
of social media like Twitter, Google+, and Facebook. Existing methods can be
broadly categorized into two paradigms: topic model-based approaches and deep
representation learning-based approaches. This task is inherently challenging
due to the sparse, large-scale, and high-dimensional characteristics of the
short text data. Furthermore, the computational intensity required by
representation learning significantly increases the running time. To address
these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet
Multinomial Mixture model (GSDMM), which effectively handles the sparsity and
high dimensionality of short texts while identifying representative words for
each cluster. Based on several aspects of GSDMM that warrant further
refinement, we propose an improved approach, GSDMM+, designed to further
optimize its performance. GSDMM+ reduces initialization noise and adaptively
adjusts word weights based on entropy, achieving fine-grained clustering that
reveals more topic-related information. Additionally, strategic cluster merging
is employed to refine clustering granularity, better aligning the predicted
distribution with the true category distribution. We conduct extensive
experiments, comparing our methods with both classical and state-of-the-art
approaches. The experimental results demonstrate the efficiency and
effectiveness of our methods. The source code for our model is publicly
available at https://github.com/chehaoa/VEMC.

</details>


### [128] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: \u8be5\u7814\u7a76\u65e8\u5728\u4ece\u79d1\u5b66\u6587\u7ae0\u4e2d\u63d0\u53d6\u5173\u952e\u6982\u5ff5\u548c\u8d21\u732e\uff0c\u4ee5\u95ee\u7b54\uff08QA\uff09\u5bf9\u7684\u5f62\u5f0f\u5448\u73b0\u3002\u5b83\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u53e6\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eKG\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u6587\u7ae0\u7684\u4e3b\u8981\u601d\u60f3\uff0c\u4e14\u5bf9\u5b9e\u4f53\u5173\u7cfb\uff08ER\uff09\u63d0\u53d6\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u81f3\u5173\u91cd\u8981\u3002


<details>
  <summary>Details</summary>
Motivation: \u5b66\u8005\u5728\u9605\u8bfb\u6216\u6574\u5408\u6587\u7ae0\u65f6\uff0c\u9700\u8981\u5feb\u901f\u8bc6\u522b\u548c\u7406\u89e3\u5176\u4e3b\u8981\u601d\u60f3\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u95ee\u7b54\u5bf9\u6765\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u4ee5\u5e2e\u52a9\u5b66\u8005\u5feb\u901f\u638c\u63e1\u6587\u7ae0\u7684\u5173\u952e\u6982\u5ff5\u548c\u8d21\u732e\u3002

Method: \u63d0\u51fa\u4e86\u4e24\u79cdQA\u751f\u6210\u65b9\u6cd5\uff1a1. **\u57fa\u4e8eLLM\u7684\u65b9\u6cd5**\uff1a\u9009\u62e9\u7a81\u51fa\u6bb5\u843d\uff0cLLM\u751f\u6210\u95ee\u9898\uff0c\u6839\u636e\u83b7\u5f97\u6709\u610f\u4e49\u7b54\u6848\u7684\u53ef\u80fd\u6027\u5bf9\u95ee\u9898\u8fdb\u884c\u6392\u540d\uff0c\u7136\u540e\u751f\u6210\u7b54\u6848\u3002\u6b64\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6587\u7ae0\u5185\u5bb9\u30022. **\u57fa\u4e8eKG\u7684\u65b9\u6cd5**\uff1a\u901a\u8fc7\u5728\u79d1\u5b66\u6587\u7ae0\u4e0a\u5fae\u8c03\u5b9e\u4f53\u5173\u7cfb\uff08ER\uff09\u63d0\u53d6\u6a21\u578b\u6765\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002\u7136\u540e\uff0c\u5229\u7528\u4e09\u5143\u7ec4TF-IDF\u7c7b\u5ea6\u91cf\uff08\u8bc4\u4f30\u4e09\u5143\u7ec4\u5728\u6587\u7ae0\u4e2d\u7684\u91cd\u8981\u6027\u4e0e\u5728\u6587\u732e\u4e2d\u7684\u666e\u904d\u6027\uff09\u9009\u62e9\u6700\u76f8\u5173\u7684\u4e09\u5143\u7ec4\u3002\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u6307\u6807\uff0c\u7531\u9886\u57df\u4e13\u5bb6\uff08SMEs\uff09\u8bc4\u4f30\u4e24\u79cd\u65b9\u6cd5\u751f\u6210\u7684\u95ee\u7b54\u5bf9\u7684\u8d28\u91cf\u3002

Result: \u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u6587\u7ae0\u4e2d\u8ba8\u8bba\u7684\u4e3b\u8981\u601d\u60f3\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u79d1\u5b66\u8bed\u6599\u5e93\u4e0a\u5bf9\u5b9e\u4f53\u5173\u7cfb\uff08ER\uff09\u63d0\u53d6\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u5bf9\u4e8e\u4ece\u6b64\u7c7b\u6587\u6863\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u7684\u4e09\u5143\u7ec4\u81f3\u5173\u91cd\u8981\u3002

Conclusion: \u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u751f\u6210\u65b9\u6cd5\u80fd\u6709\u6548\u5e2e\u52a9\u5b66\u8005\u8bc6\u522b\u79d1\u5b66\u6587\u7ae0\u4e2d\u7684\u5173\u952e\u6982\u5ff5\u548c\u8d21\u732e\u3002\u5728\u7279\u5b9a\u9886\u57df\u8bed\u6599\u5e93\u4e0a\u5bf9\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u662f\u63d0\u9ad8\u95ee\u7b54\u8d28\u91cf\u7684\u5173\u952e\u3002

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [129] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
*Lizhi Ma,Tong Zhao,Shuai Zhang,Nirui Song,Hongliang He,Anqi Li,Ran Feng,Huachuan Qiu,Jingsong Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: \u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e2d\u6587\u5fc3\u7406\u54a8\u8be2\u4e2d\u8bed\u8a00\u8868\u8fbe\uff08\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u548c\u8d1f\u9762\u60c5\u7eea\u8bcd\uff09\u4e0e\u6291\u90c1\u548c\u7126\u8651\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u8bcd\u4e0e\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u5448\u6b63\u76f8\u5173\uff0c\u800c\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u4e0e\u5fc3\u7406\u72b6\u51b5\u65e0\u5173\uff0c\u8fd9\u4e0e\u897f\u65b9\u7814\u7a76\u7ed3\u679c\u4e0d\u540c\uff0c\u51f8\u663e\u4e86\u6587\u5316\u80cc\u666f\u7684\u5f71\u54cd\u3002


<details>
  <summary>Details</summary>
Motivation: \u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e2d\u6587\u5fc3\u7406\u54a8\u8be2\u4e92\u52a8\u4e2d\u8bed\u8a00\u8868\u8fbe\uff08\u7279\u522b\u662f\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u548c\u8d1f\u9762\u60c5\u7eea\u8bcd\uff09\u4e0e\u6291\u90c1\u3001\u7126\u8651\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5173\u6ce8\u6587\u5316\u5dee\u5f02\uff08\u96c6\u4f53\u4e3b\u4e49\u7684\u4e2d\u56fd\u8bed\u5883\u4e0e\u4e2a\u4f53\u4e3b\u4e49\u7684\u897f\u65b9\u8bed\u5883\uff09\u5bf9\u8bed\u8a00\u4f7f\u7528\u7684\u5f71\u54cd\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u4e2d\u6587\u8bed\u5883\u4e0b\u7684\u7a7a\u767d\u3002

Method: \u7814\u7a76\u5229\u7528\u4e86\u5305\u542b735\u4e2a\u5728\u7ebf\u54a8\u8be2\u4f1a\u8bdd\u7684\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u8bed\u8a00\u63a2\u7a76\u4e0e\u8bcd\u6c47\u8ba1\u6570\uff08LIWC\uff09\u8f6f\u4ef6\u91cf\u5316\u8bed\u8a00\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u5e7f\u4e49\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002

Result: \u7ed3\u679c\u663e\u793a\uff0c\u8d1f\u9762\u60c5\u7eea\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u4e0e\u6765\u8bbf\u8005\u7684\u6291\u90c1\u548c\u7126\u8651\u72b6\u6001\u4e25\u91cd\u7a0b\u5ea6\u5448\u663e\u8457\u6b63\u76f8\u5173\u3002\u7136\u800c\uff0c\u4e0e\u5148\u524d\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u8bed\u5883\u7684\u7814\u7a76\u7ed3\u679c\u76f8\u53cd\uff0c\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u5e76\u672a\u968f\u5ba2\u6237\u7684\u5fc3\u7406\u72b6\u51b5\u800c\u663e\u8457\u53d8\u5316\u3002

Conclusion: \u7814\u7a76\u7ed3\u8bba\u5f3a\u8c03\u4e86\u6587\u5316\uff08\u5982\u96c6\u4f53\u4e3b\u4e49\u4e0e\u4e2a\u4f53\u4e3b\u4e49\uff09\u548c\u5bf9\u8bdd\u4e92\u52a8\u52a8\u6001\u5bf9\u5fc3\u7406\u5065\u5eb7\u6c9f\u901a\u4e2d\u8bed\u8a00\u4f7f\u7528\u7684\u7ec6\u5fae\u5f71\u54cd\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u4e2d\u6587\u8bed\u5883\u4e0b\u6cbb\u7597\u5b9e\u8df5\u4e2d\u76f8\u5173\u7684\u5fc3\u7406\u8bed\u8a00\u6807\u8bb0\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002

Abstract: This study explores the relationship between linguistic expressions and
psychological states of depression and anxiety within Chinese psycho-counseling
interactions, focusing specifically on the usage of first-person singular
pronouns and negative emotional words. Utilizing a corpus derived from 735
online counseling sessions, the analysis employed a general linear mixed-effect
model to assess linguistic patterns quantified by the Linguistic Inquiry and
Word Count (LIWC) software. Results indicate a significant positive correlation
between the frequency of negative emotional words and the severity of both
depressive and anxious states among clients. However, contrary to prior
findings predominantly derived from English-language contexts, the usage
frequency of first-person singular pronouns did not vary significantly with the
clients' psychological conditions. These outcomes are discussed within the
framework of cultural distinctions between collectivist Chinese contexts and
individualistic Western settings, as well as the interactive dynamics unique to
psycho-counseling conversations. The findings highlight the nuanced influence
of cultural and conversational contexts on language use in mental health
communications, providing insights into psycholinguistic markers relevant to
therapeutic practices in Chinese-speaking populations.

</details>


### [130] [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)
*Eitan Wagner,Renana Keydar,Omri Abend*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4fa6\u63a2\u5c0f\u8bf4\u7684\u6982\u7387\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u516c\u5e73\u6027\u3001\u8fde\u8d2f\u6027\u548c\u60ca\u559c\u5ea6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684\u4fa6\u63a2\u6545\u4e8b\uff0c\u53d1\u73b0LLM\u6545\u4e8b\u5728\u60ca\u559c\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u672a\u80fd\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002


<details>
  <summary>Details</summary>
Motivation: \u6709\u6548\u7684\u6545\u4e8b\u53d9\u8ff0\uff0c\u7279\u522b\u662f\u5728\u4fa6\u63a2\u5c0f\u8bf4\u4e2d\uff0c\u9700\u8981\u5728\u6ee1\u8db3\u8bfb\u8005\u9884\u671f\u548c\u5f15\u5165\u610f\u5916\u53d1\u5c55\u4e4b\u95f4\u53d6\u5f97\u5fae\u5999\u5e73\u8861\uff0c\u5373\u201c\u516c\u5e73\u6027\u201d\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3\u548c\u91cf\u5316\u8fd9\u79cd\u5e73\u8861\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u751f\u6210\u6b64\u7c7b\u6545\u4e8b\u65f6\u7684\u8868\u73b0\u3002

Method: \u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u4fa6\u63a2\u5c0f\u8bf4\u7684\u6982\u7387\u6846\u67b6\uff0c\u5e76\u5728\u6b64\u6846\u67b6\u5185\u6b63\u5f0f\u5b9a\u4e49\u4e86\u201c\u516c\u5e73\u6027\u201d\u53ca\u5176\u76f8\u5173\u6307\u6807\uff08\u5982\u6545\u4e8b\u7684\u8fde\u8d2f\u6027\u548c\u60ca\u559c\u5ea6\uff09\u3002\u968f\u540e\uff0c\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u8bc4\u4f30LLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u751f\u6210\u7684\u4fa6\u63a2\u6545\u4e8b\u3002

Result: \u7ed3\u679c\u663e\u793a\uff0cLLM\u751f\u6210\u7684\u4fa6\u63a2\u6545\u4e8b\u867d\u7136\u53ef\u80fd\u4e0d\u53ef\u9884\u6d4b\uff0c\u4f46\u5b83\u4eec\u666e\u904d\u672a\u80fd\u5e73\u8861\u60ca\u559c\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8fd9\u6781\u5927\u5730\u5f71\u54cd\u4e86\u6545\u4e8b\u7684\u8d28\u91cf\u3002

Conclusion: LLM\u5728\u751f\u6210\u4fa6\u63a2\u5c0f\u8bf4\u65f6\uff0c\u96be\u4ee5\u5728\u6545\u4e8b\u7684\u60ca\u559c\u5ea6\u548c\u5bf9\u8bfb\u8005\u9884\u671f\u7684\u201c\u516c\u5e73\u6027\u201d\u4e4b\u95f4\u53d6\u5f97\u6070\u5f53\u7684\u5e73\u8861\uff0c\u8fd9\u662f\u5bfc\u81f4\u5176\u6545\u4e8b\u8d28\u91cf\u4e0d\u4f73\u7684\u5173\u952e\u56e0\u7d20\u3002

Abstract: Effective storytelling relies on a delicate balance between meeting the
reader's prior expectations and introducing unexpected developments. In the
domain of detective fiction, this tension is known as fair play, which includes
the implicit agreement between the writer and the reader as to the range of
possible resolutions the mystery story may have. In this work, we present a
probabilistic framework for detective fiction that allows us to define desired
qualities. Using this framework, we formally define fair play and design
appropriate metrics for it. Stemming from these definitions is an inherent
tension between the coherence of the story, which measures how much it ``makes
sense'', and the surprise it induces. We validate the framework by applying it
to LLM-generated detective stories. This domain is appealing since we have an
abundance of data, we can sample from the distribution generating the story,
and the story-writing capabilities of LLMs are interesting in their own right.
Results show that while LLM-generated stories may be unpredictable, they
generally fail to balance the trade-off between surprise and fair play, which
greatly contributes to their poor quality.

</details>


### [131] [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)
*Nicolò Brunello,Davide Rigamonti,Andrea Sassella,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: \u672c\u6587\u4ecb\u7ecdInTraVisTo\uff0c\u4e00\u4e2a\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u8ffd\u8e2a\u548c\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5185\u90e8\u7684\u8ba1\u7b97\u8fc7\u7a0b\u548c\u4fe1\u606f\u6d41\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1LLMs\u7684\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u4e0d\u53ef\u9884\u6d4b\u6027\u4ee5\u53ca\u671f\u671b\u884c\u4e3a\u4e0e\u5b9e\u9645\u8f93\u51fa\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4f7f\u5f97LLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u5185\u90e8\u8fd0\u4f5c\u673a\u5236\u3002

Method: \u5f15\u5165InTraVisTo\u5de5\u5177\uff0c\u5b83\u901a\u8fc7\u89e3\u7801\u6a21\u578b\u6bcf\u4e00\u5c42\u7684token\u5d4c\u5165\u6765\u53ef\u89c6\u5316Transformer\u6a21\u578b\u7684\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u5229\u7528Sankey\u56fe\u5c55\u793a\u4e0d\u540c\u5c42\u4e4b\u95f4\u5404\u7ec4\u4ef6\u7684\u4fe1\u606f\u6d41\u3002

Result: InTraVisTo\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u80fd\u591f\u8c03\u67e5\u548c\u8ffd\u8e2aTransformer-based LLM\u4e2d\u6bcf\u4e2atoken\u7684\u751f\u6210\u8ba1\u7b97\u8fc7\u7a0b\u3002

Conclusion: InTraVisTo\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u66f4\u597d\u5730\u7406\u89e3Transformer\u6a21\u578b\u5185\u90e8\u7684\u8ba1\u7b97\uff0c\u4ece\u800c\u63ed\u793aLLMs\u91c7\u7528\u7684\u5185\u90e8\u6a21\u5f0f\u548c\u63a8\u7406\u8fc7\u7a0b\u3002

Abstract: The reasoning capabilities of Large Language Models (LLMs) have increased
greatly over the last few years, as have their size and complexity.
Nonetheless, the use of LLMs in production remains challenging due to their
unpredictable nature and discrepancies that can exist between their desired
behavior and their actual model output. In this paper, we introduce a new tool,
InTraVisTo (Inside Transformer Visualisation Tool), designed to enable
researchers to investigate and trace the computational process that generates
each token in a Transformer-based LLM. InTraVisTo provides a visualization of
both the internal state of the Transformer model (by decoding token embeddings
at each layer of the model) and the information flow between the various
components across the different layers of the model (using a Sankey diagram).
With InTraVisTo, we aim to help researchers and practitioners better understand
the computations being performed within the Transformer model and thus to shed
some light on internal patterns and reasoning processes employed by LLMs.

</details>


### [132] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
*Cole Walsh,Rodica Ivan,Muhammad Zafar Iqbal,Colleen Robb*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\uff08SJTs\uff09\u7684\u5f00\u653e\u5f0f\u56de\u7b54\u4e2d\u63d0\u53d6\u4e0e\u6784\u5ff5\u76f8\u5173\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u8bc4\u5206\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u5b66\u672f\u9879\u76ee\u65e5\u76ca\u91cd\u89c6\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u6765\u8861\u91cf\u3001\u8bc4\u4f30\u548c\u53d1\u5c55\u8fd9\u4e9b\u6280\u80fd\u3002\u4f20\u7edf\u7684\u5f00\u653e\u5f0f\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\uff08SJTs\uff09\u4f9d\u8d56\u4eba\u5de5\u8bc4\u5206\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5b9e\u65bd\u3002\u8fc7\u53bb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u8bc4\u5206\u7cfb\u7edf\u56e0\u6784\u5ff5\u6548\u5ea6\u95ee\u9898\u800c\u5931\u8d25\u3002

Method: \u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4eceSJTs\u56de\u7b54\u4e2d\u63d0\u53d6\u4e0e\u6784\u5ff5\u76f8\u5173\u7684\u7279\u5f81\u3002\u5177\u4f53\u4f7f\u7528Casper SJT\u6765\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002

Result: \u7814\u7a76\u5c55\u793a\u4e86\u8fd9\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u63d0\u53d6\u6784\u5ff5\u76f8\u5173\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u5960\u5b9a\u4e86\u57fa\u7840\u3002

Conclusion: \u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u4e2a\u4eba\u548c\u4e13\u4e1a\u6280\u80fd\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u7684\u64cd\u4f5c\u6311\u6218\u3002

Abstract: Academic programs are increasingly recognizing the importance of personal and
professional skills and their critical role alongside technical expertise in
preparing students for future success in diverse career paths. With this
growing demand comes the need for scalable systems to measure, evaluate, and
develop these skills. Situational Judgment Tests (SJTs) offer one potential
avenue for measuring these skills in a standardized and reliable way, but
open-response SJTs have traditionally relied on trained human raters for
evaluation, presenting operational challenges to delivering SJTs at scale. Past
attempts at developing NLP-based scoring systems for SJTs have fallen short due
to issues with construct validity of these systems. In this article, we explore
a novel approach to extracting construct-relevant features from SJT responses
using large language models (LLMs). We use the Casper SJT to demonstrate the
efficacy of this approach. This study sets the foundation for future
developments in automated scoring for personal and professional skills.

</details>


### [133] [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
*Maciej Jalocha,Johan Hausted Schmidt,William Michelseen*

Main category: cs.CL

TL;DR: \u7f51\u7edc\u5b89\u5168NER\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u6807\u7b7e\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u96be\u4ee5\u6574\u5408\u3002\u672c\u7814\u7a76\u5c1d\u8bd5\u7edf\u4e00\u6807\u7b7e\u5e76\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u63d0\u51fa\u7684\u591a\u5934\u6a21\u578b\u548c\u56fe\u57fa\u8f6c\u79fb\u6a21\u578b\u6539\u8fdb\u6709\u9650\u3002


<details>
  <summary>Details</summary>
Motivation: \u7f51\u7edc\u5b89\u5168\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u7f3a\u4e4f\u6807\u51c6\u5316\u6807\u7b7e\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u96be\u4ee5\u7ed3\u5408\u4f7f\u7528\uff0c\u9650\u5236\u4e86\u6570\u636e\u8d44\u6e90\u7684\u53ef\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u6570\u636e\u8d44\u6e90\u53ef\u7528\u6027\u3002

Method: \u7814\u7a76\u91c7\u7528\u4e86\u7c97\u7c92\u5ea6\u6807\u7b7e\u7edf\u4e00\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528BiLSTM\u6a21\u578b\u8fdb\u884c\u6210\u5bf9\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u3002\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u9884\u6d4b\u7ed3\u679c\uff0c\u63ed\u793a\u9519\u8bef\u3001\u5c40\u9650\u6027\u548c\u6570\u636e\u96c6\u5dee\u5f02\u3002\u4e3a\u89e3\u51b3\u7edf\u4e00\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u591a\u5934\u6a21\u578b\u548c\u57fa\u4e8e\u56fe\u7684\u8fc1\u79fb\u6a21\u578b\u7b49\u66ff\u4ee3\u67b6\u6784\u3002

Result: \u7ed3\u679c\u663e\u793a\uff0c\u5728\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u5e26\u6709\u6743\u91cd\u5171\u4eab\u7684\u591a\u5934\u6a21\u578b\u4ec5\u6bd4\u7edf\u4e00\u8bad\u7ec3\u7565\u6709\u6539\u8fdb\uff0c\u800c\u57fa\u4e8eBERT-base-NER\u7684\u56fe\u57fa\u8fc1\u79fb\u6a21\u578b\u4e0eBERT-base-NER\u76f8\u6bd4\u6ca1\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002

Conclusion: \u7f51\u7edc\u5b89\u5168NER\u9886\u57df\u7684\u6807\u7b7e\u7edf\u4e00\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7b80\u5355\u7684\u6807\u7b7e\u7edf\u4e00\u548c\u63d0\u51fa\u7684\u590d\u6742\u6a21\u578b\uff08\u591a\u5934\u3001\u56fe\u57fa\u8fc1\u79fb\uff09\u672a\u80fd\u6709\u6548\u89e3\u51b3\u6a21\u578b\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002

Abstract: The field of cybersecurity NER lacks standardized labels, making it
challenging to combine datasets. We investigate label unification across four
cybersecurity datasets to increase data resource usability. We perform a
coarse-grained label unification and conduct pairwise cross-dataset evaluations
using BiLSTM models. Qualitative analysis of predictions reveals errors,
limitations, and dataset differences. To address unification limitations, we
propose alternative architectures including a multihead model and a graph-based
transfer model. Results show that models trained on unified datasets generalize
poorly across datasets. The multihead model with weight sharing provides only
marginal improvements over unified training, while our graph-based transfer
model built on BERT-base-NER shows no significant performance gains compared
BERT-base-NER.

</details>


### [134] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
*Matous Volf,Jakub Simko*

Main category: cs.CL

TL;DR: \u672c\u6587\u901a\u8fc7\u6574\u5408\u73b0\u6709\u6570\u636e\u96c6\u5e76\u521b\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u89e3\u51b3\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u6587\u672c\u81ea\u52a8\u5206\u7c7b\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u65b9\u6cd5\u5728\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u6587\u672c\u5206\u7c7b\u4e2d\u521b\u5efa\u4e86\u5b64\u7acb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5904\u7406\u5206\u5e03\u5916\u6587\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\u3002

Method: 1. \u7efc\u5408\u6982\u8ff0\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\uff1b2. \u7f16\u8bd1\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec\u5408\u5e7612\u4e2a\u653f\u6cbb\u503e\u5411\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5e76\u6269\u5c5518\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u4ee5\u521b\u5efa\u65b0\u7684\u653f\u6cbb\u6027\u6570\u636e\u96c6\uff1b3. \u91c7\u7528\u201c\u7559\u4e00\u5165\u201d\u548c\u201c\u7559\u4e00\u51fa\u201d\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5e76\u8bad\u7ec3\u5177\u6709\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u6a21\u578b\u3002

Result: \u901a\u8fc7\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u8bad\u7ec3\u51fa\u4e86\u5177\u6709\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u6a21\u578b\u3002

Conclusion: \u901a\u8fc7\u6784\u5efa\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u91c7\u7528\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u5347\u4e86Transformer\u6a21\u578b\u5728\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002

Abstract: This paper addresses the challenge of automatically classifying text
according to political leaning and politicalness using transformer models. We
compose a comprehensive overview of existing datasets and models for these
tasks, finding that current approaches create siloed solutions that perform
poorly on out-of-distribution texts. To address this limitation, we compile a
diverse dataset by combining 12 datasets for political leaning classification
and creating a new dataset for politicalness by extending 18 existing datasets
with the appropriate label. Through extensive benchmarking with leave-one-in
and leave-one-out methodologies, we evaluate the performance of existing models
and train new ones with enhanced generalization capabilities.

</details>


### [135] [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)
*Carlos Mena,Pol Serra,Jacobo Romero,Abir Messaoudi,Jose Giraldo,Carme Armentano-Oller,Rodolfo Zevallos,Ivan Meza,Javier Hernando*

Main category: cs.CL

TL;DR: \u8be5\u7814\u7a76\u65e8\u5728\u6539\u5584\u8bed\u7801\u8f6c\u6362\uff08CS\uff09\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6027\u80fd\uff0c\u7279\u522b\u9488\u5bf9\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bedCS\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u3001\u62fc\u63a5\u5355\u8bed\u97f3\u9891\u548c\u5229\u7528\u5e26\u8bed\u8a00\u6807\u8bb0\u7684\u771f\u5b9eCS\u6570\u636e\uff0c\u53d1\u73b0\u5c11\u91cf\u5408\u6210\u6570\u636e\u7ed3\u5408\u4e3b\u5bfc\u8bed\u8a00\u6807\u8bb0\u6548\u679c\u6700\u4f73\u3002


<details>
  <summary>Details</summary>
Motivation: \u8bed\u7801\u8f6c\u6362\uff08CS\uff09\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u8bed\u8a00\u76f8\u4f3c\u6027\u7ed9\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5e26\u6765\u6311\u6218\u3002\u7f3a\u4e4f\u4e13\u7528CS\u6570\u636e\u96c6\u9650\u5236\u4e86ASR\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u793e\u4f1a\u4e2d\uff0cCS\u5728\u975e\u6b63\u5f0f\u548c\u6b63\u5f0f\u573a\u5408\u666e\u904d\u5b58\u5728\uff08\u5982\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bedCS\uff09\u3002

Method: \u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e09\u79cd\u7b56\u7565\u6765\u6539\u8fdb\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bedCS\u7684ASR\u6027\u80fd\uff1a1) \u751f\u6210\u5408\u6210CS\u6570\u636e\uff1b2) \u62fc\u63a5\u5355\u8bed\u97f3\u9891\uff1b3) \u5229\u7528\u5e26\u6709\u8bed\u8a00\u6807\u8bb0\u7684\u771f\u5b9eCS\u6570\u636e\u3002\u7814\u7a76\u4ece\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u8bed\u97f3\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6CS\u6570\u636e\uff0c\u5e76\u5bf9OpenAI\u7684Whisper\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5c06\u6a21\u578b\u53d1\u5e03\u5728Hugging Face\u4e0a\u3002

Result: \u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u9002\u91cf\u7684\u5408\u6210CS\u6570\u636e\u4e0e\u4e3b\u5bfc\u8bed\u8a00\u6807\u8bb0\u80fd\u4ea7\u751f\u6700\u4f73\u7684\u8f6c\u5f55\u6027\u80fd\u3002

Conclusion: \u901a\u8fc7\u751f\u6210\u5408\u6210CS\u6570\u636e\u5e76\u7ed3\u5408\u4e3b\u5bfc\u8bed\u8a00\u6807\u8bb0\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8bed\u7801\u8f6c\u6362ASR\u7684\u6027\u80fd\uff0c\u4ece\u800c\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2dCS\u6a21\u5f0f\u7684\u6311\u6218\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002

Abstract: Code-switching (CS), the alternating use of two or more languages, challenges
automatic speech recognition (ASR) due to scarce training data and linguistic
similarities. The lack of dedicated CS datasets limits ASR performance, as most
models rely on monolingual or mixed-language corpora that fail to reflect
real-world CS patterns. This issue is critical in multilingual societies where
CS occurs in informal and formal settings. A key example is Catalan-Spanish CS,
widely used in media and parliamentary speeches. In this work, we improve ASR
for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic
CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data
with language tokens. We extract CS data from Catalan speech corpora and
fine-tune OpenAI's Whisper models, making them available on Hugging Face.
Results show that combining a modest amount of synthetic CS data with the
dominant language token yields the best transcription performance.

</details>


### [136] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: \u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u548c\u672a\u6765AI\u7684\u8bf4\u670d\u529b\u4e3b\u8981\u6765\u6e90\u4e8e\u540e\u8bad\u7ec3\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u800c\u975e\u6a21\u578b\u89c4\u6a21\u6216\u4e2a\u6027\u5316\uff0c\u4e14\u8bf4\u670d\u529b\u63d0\u9ad8\u7684\u540c\u65f6\uff0c\u4e8b\u5b9e\u51c6\u786e\u6027\u4f1a\u7cfb\u7edf\u6027\u4e0b\u964d\u3002


<details>
  <summary>Details</summary>
Motivation: \u4eba\u4eec\u666e\u904d\u62c5\u5fc3\u4f1a\u8bdd\u5f0fAI\u53ef\u80fd\u5f88\u5feb\u5bf9\u4eba\u7c7b\u4fe1\u5ff5\u4ea7\u751f\u524d\u6240\u672a\u6709\u7684\u5f71\u54cd\u3002

Method: \u901a\u8fc7\u4e09\u4e2a\u5927\u89c4\u6a21\u5b9e\u9a8c\uff08N=76,977\uff09\uff0c\u4f7f\u752819\u4e2aLLM\uff08\u5305\u62ec\u4e00\u4e9b\u4e13\u95e8\u4e3a\u8bf4\u670d\u800c\u8fdb\u884c\u540e\u8bad\u7ec3\u7684\u6a21\u578b\uff09\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728707\u4e2a\u653f\u6cbb\u95ee\u9898\u4e0a\u7684\u8bf4\u670d\u529b\uff0c\u5e76\u68c0\u67e5\u4e86466,769\u4e2aLLM\u58f0\u660e\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002

Result: AI\u7684\u8bf4\u670d\u529b\u4e3b\u8981\u6765\u81ea\u540e\u8bad\u7ec3\uff08\u63d0\u5347\u9ad8\u8fbe51%\uff09\u548c\u63d0\u793a\u5de5\u7a0b\uff08\u63d0\u5347\u9ad8\u8fbe27%\uff09\uff0c\u800c\u975e\u4e2a\u6027\u5316\u6216\u6a21\u578b\u89c4\u6a21\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u5229\u7528LLM\u5feb\u901f\u83b7\u53d6\u548c\u7b56\u7565\u6027\u90e8\u7f72\u4fe1\u606f\u7684\u80fd\u529b\u6765\u589e\u5f3a\u8bf4\u670d\u529b\u3002\u4ee4\u4eba\u9707\u60ca\u7684\u662f\uff0c\u5728AI\u8bf4\u670d\u529b\u63d0\u9ad8\u7684\u5730\u65b9\uff0c\u5176\u4e8b\u5b9e\u51c6\u786e\u6027\u7cfb\u7edf\u6027\u5730\u4e0b\u964d\u3002

Conclusion: \u5f53\u524d\u548c\u672a\u6765AI\u7684\u8bf4\u670d\u529b\u4e3b\u8981\u6e90\u4e8e\u540e\u8bad\u7ec3\u548c\u63d0\u793a\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u63d0\u9ad8\u8bf4\u670d\u529b\u7684\u540c\u65f6\uff0c\u4f1a\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u4e8b\u5b9e\u51c6\u786e\u6027\u3002

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


### [137] [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)
*Jan Trienes,Anastasiia Derzhanskaia,Roland Schwarzkopf,Markus Mühling,Jörg Schlötterer,Christin Seifert*

Main category: cs.CL

TL;DR: \u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMarcel\u7684\u8f7b\u91cf\u7ea7\u5f00\u6e90\u5bf9\u8bdd\u4ee3\u7406\uff0c\u65e8\u5728\u5e2e\u52a9\u6f5c\u5728\u5b66\u751f\u89e3\u7b54\u5165\u5b66\u76f8\u5173\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u8f7b\u5927\u5b66\u5de5\u4f5c\u4eba\u5458\u7684\u8d1f\u62c5\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u52a8\u673a\u662f\u4e3a\u6f5c\u5728\u5b66\u751f\u63d0\u4f9b\u5feb\u901f\u3001\u4e2a\u6027\u5316\u7684\u5165\u5b66\u54a8\u8be2\u7b54\u590d\uff0c\u5e76\u51cf\u5c11\u5927\u5b66\u5de5\u4f5c\u4eba\u5458\u7684\u5de5\u4f5c\u91cf\u3002

Method: \u7cfb\u7edf\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u5c06\u7b54\u6848\u57fa\u4e8e\u5927\u5b66\u8d44\u6e90\u4ee5\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4fe1\u3002\u4e3a\u63d0\u9ad8\u68c0\u7d22\u8d28\u91cf\uff0c\u5f15\u5165\u4e86\u4e00\u79cdFAQ\u68c0\u7d22\u5668\uff0c\u5c06\u7528\u6237\u95ee\u9898\u6620\u5c04\u5230\u77e5\u8bc6\u5e93\u6761\u76ee\uff0c\u5141\u8bb8\u7ba1\u7406\u5458\u5f15\u5bfc\u68c0\u7d22\uff0c\u4f18\u4e8e\u6807\u51c6\u5bc6\u96c6/\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u3002\u7cfb\u7edf\u8bbe\u8ba1\u6613\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b66\u672f\u73af\u5883\u4e2d\u90e8\u7f72\u3002

Result: \u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u7cfb\u7edf\u67b6\u6784\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5176\u7ec4\u4ef6\u7684\u6280\u672f\u8bc4\u4f30\uff0c\u5e76\u62a5\u544a\u4e86\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u7ecf\u9a8c\u548c\u89c1\u89e3\u3002

Conclusion: Marcel\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5bf9\u8bdd\u4ee3\u7406\uff0c\u80fd\u4e3a\u5b66\u751f\u63d0\u4f9b\u5feb\u901f\u3001\u4e2a\u6027\u5316\u548c\u53ef\u9a8c\u8bc1\u7684\u5165\u5b66\u4fe1\u606f\uff0c\u5176\u521b\u65b0\u7684FAQ\u68c0\u7d22\u5668\u63d0\u5347\u4e86\u4fe1\u606f\u68c0\u7d22\u8d28\u91cf\uff0c\u4e14\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b66\u672f\u73af\u5883\u3002

Abstract: We present Marcel, a lightweight and open-source conversational agent
designed to support prospective students with admission-related inquiries. The
system aims to provide fast and personalized responses, while reducing workload
of university staff. We employ retrieval-augmented generation to ground answers
in university resources and to provide users with verifiable, contextually
relevant information. To improve retrieval quality, we introduce an FAQ
retriever that maps user questions to knowledge-base entries, allowing
administrators to steer retrieval, and improving over standard dense/hybrid
retrieval strategies. The system is engineered for easy deployment in
resource-constrained academic settings. We detail the system architecture,
provide a technical evaluation of its components, and report insights from a
real-world deployment.

</details>


### [138] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: \u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fae\u8c03\u540e\u4f1a\u653e\u5927\u9996\u4f4d\u504f\u5dee\uff0c\u8fd9\u4f1a\u5f71\u54cd\u591a\u9879\u9009\u62e9\u9898\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u65b0\u6392\u5e8f\u9009\u9879\uff0c\u8be5\u7814\u7a76\u6210\u529f\u5229\u7528\u8fd9\u4e00\u504f\u5dee\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: LLMs\u5b58\u5728\u504f\u89c1\uff0c\u7279\u522b\u662f\u9996\u4f4d\u6548\u5e94\u7b49\u4f4d\u7f6e\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u4f1a\u5f71\u54cd\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002\u5728\u591a\u9879\u9009\u62e9\u9898\uff08MCQA\uff09\u4e2d\uff0c\u7b54\u6848\u9009\u9879\u7684\u987a\u5e8f\u4f1a\u5f71\u54cd\u9884\u6d4b\u7ed3\u679c\uff0c\u4e14\u5fae\u8c03\u53ef\u80fd\u4f1a\u653e\u5927\u8fd9\u79cd\u504f\u89c1\u3002

Method: \u9996\u5148\uff0c\u8bc1\u660e\u5fae\u8c03\u4f1a\u653e\u5927LLMs\u7684\u9996\u4f4d\u504f\u5dee\u3002\u7136\u540e\uff0c\u901a\u8fc7\u6839\u636e\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u65b0\u6392\u5e8f\u54cd\u5e94\u9009\u9879\u6765\u7b56\u7565\u6027\u5730\u5229\u7528\u8fd9\u79cd\u6548\u5e94\uff0c\u4e14\u65e0\u9700\u77e5\u9053\u6b63\u786e\u7b54\u6848\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u9879\u9009\u62e9\u9898\u7684\u6027\u80fd\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u504f\u89c1\uff08\u5982\u9996\u4f4d\u504f\u5dee\uff09\u7684\u53cc\u91cd\u6027\u8d28\uff0c\u5b83\u4eec\u65e2\u662f\u6311\u6218\u4e5f\u662f\u673a\u9047\uff0c\u4e3a\u504f\u5dee\u611f\u77e5\u6a21\u578b\u8bbe\u8ba1\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002

Abstract: Large Language Models (LLMs) have become essential in many Natural Language
Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to
achieve high accuracy. However, like humans, LLMs exhibit biases, particularly
positional biases such as primacy and recency effects, which can influence the
accuracy of the answers. The primacy effect-where items presented first are
more likely to be remembered or selected-plays a key role in Multiple Choice
Question Answering (MCQA), where the order of answer options can affect
prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We
first show that fine-tuning amplifies this bias, probably due to exposure to
human-like patterns. Hence, we strategically leverage this effect by reordering
response options based on semantic similarity to the query, without requiring
knowledge of the correct answer. Our experimental results show that this
approach significantly improves performance in MCQA. More generally, our
findings underscore the dual nature of biases as both challenges and
opportunities, offering insights for bias-aware model design and NLP
applications.

</details>


### [139] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
*Bhishma Dedhia,Yuval Kansal,Niraj K. Jha*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u81ea\u4e0b\u800c\u4e0a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eceKG\u57fa\u5143\u5408\u6210\u4efb\u52a1\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u83b7\u5f97\u6df1\u5ea6\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u5728\u533b\u5b66\u9886\u57df\u5b9e\u73b0\u4e86\u9886\u57df\u7279\u5f02\u6027\u8d85\u667a\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u80fd\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\uff0c\u4f46\u5176\u57fa\u4e8e\u901a\u7528\u8bed\u6599\u5e93\u7684\u81ea\u4e0a\u800c\u4e0b\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u83b7\u53d6\u6df1\u5ea6\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6240\u9700\u7684\u62bd\u8c61\u6982\u5ff5\u3002\u8fd9\u9700\u8981\u4e00\u79cd\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u7ec4\u5408\u7b80\u5355\u7684\u9886\u57df\u6982\u5ff5\u6765\u83b7\u5f97\u4e13\u4e1a\u77e5\u8bc6\u3002

Method: 1. \u5229\u7528\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u63d0\u4f9b\u7ec4\u5408\u7ed3\u6784\uff0c\u5c06\u9886\u57df\u57fa\u5143\u8868\u793a\u4e3a\u8fb9\uff0c\u8def\u5f84\u7f16\u7801\u9ad8\u5c42\u6982\u5ff5\u30022. \u5f00\u53d1\u4e00\u4e2a\u4efb\u52a1\u751f\u6210\u7ba1\u9053\uff0c\u76f4\u63a5\u4eceKG\u57fa\u5143\u5408\u6210\u4efb\u52a1\u30023. \u5728\u751f\u6210\u7684KG-grounded\u8bfe\u7a0b\uff08\u4ee5\u533b\u5b66\u9886\u57df\u4e3a\u4f8b\uff0c\u5305\u542b24,000\u4e2a\u5e26\u6709\u601d\u7ef4\u8f68\u8ff9\u7684\u63a8\u7406\u4efb\u52a1\uff09\u4e0a\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff08QwQ-32B\uff0c\u5f97\u5230QwQ-Med-3\uff09\u30024. \u5f15\u5165\u8bc4\u4f30\u5957\u4ef6ICD-Bench\uff0c\u7528\u4e8e\u91cf\u531615\u4e2a\u533b\u5b66\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002

Result: 1. QwQ-Med-3\u5728ICD-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u30022. QwQ-Med-3\u901a\u8fc7\u5229\u7528\u83b7\u5f97\u7684\u57fa\u5143\uff0c\u5728ICD-Bench\u6700\u56f0\u96be\u7684\u4efb\u52a1\u4e0a\u8fdb\u4e00\u6b65\u62c9\u5927\u4e86\u6027\u80fd\u5dee\u8ddd\u30023. QwQ-Med-3\u5c06\u83b7\u5f97\u7684\u4e13\u4e1a\u77e5\u8bc6\u6210\u529f\u8fc1\u79fb\u5230\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002

Conclusion: \u9886\u57df\u7279\u5f02\u6027\u8d85\u667a\u80fd\u6a21\u578b\u662f\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u4e00\u79cd\u53ef\u884c\u9014\u5f84\u3002\u6587\u7ae0\u8bbe\u60f3\uff0c\u672a\u6765\u7684AGI\u53ef\u80fd\u6e90\u4e8e\u9ad8\u6548\u7684\u9886\u57df\u7279\u5f02\u6027\u8d85\u667a\u80fd\u4ee3\u7406\u7684\u53ef\u7ec4\u5408\u4ea4\u4e92\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u5f3a\u8c03\u5e7f\u6cdb\u4e13\u4e1a\u77e5\u8bc6\u7684\u5355\u4e00\u6a21\u578b\u3002

Abstract: Language models traditionally used for cross-domain generalization have
recently demonstrated task-specific reasoning. However, their top-down training
approach on general corpora is insufficient for acquiring abstractions needed
for deep domain expertise. This may require a bottom-up approach that acquires
expertise by learning to compose simple domain concepts into more complex ones.
A knowledge graph (KG) provides this compositional structure, where domain
primitives are represented as head-relation-tail edges and their paths encode
higher-level concepts. We present a task generation pipeline that synthesizes
tasks directly from KG primitives, enabling models to acquire and compose them
for reasoning. We fine-tune language models on the resultant KG-grounded
curriculum to demonstrate domain-specific superintelligence. While broadly
applicable, we validate our approach in medicine, where reliable KGs exist.
Using a medical KG, we curate 24,000 reasoning tasks paired with thinking
traces derived from diverse medical primitives. We fine-tune the QwQ-32B model
on this curriculum to obtain QwQ-Med-3 that takes a step towards medical
superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify
reasoning abilities across 15 medical domains. Our experiments demonstrate that
QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on
ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired
primitives to widen the performance gap on the hardest tasks of ICD-Bench.
Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3
transfers acquired expertise to enhance the base model's performance. While the
industry's approach to artificial general intelligence (AGI) emphasizes broad
expertise, we envision a future in which AGI emerges from the composable
interaction of efficient domain-specific superintelligent agents.

</details>


### [140] [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)
*Lilit Grigoryan,Nikolay Karpov,Enas Albasiri,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u548c\u6587\u672c\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8eFastConformer\u67b6\u6784\u8bad\u7ec3\u4e86\u4e24\u4e2a\u65b0\u7684ASR\u6a21\u578b\uff1a\u4e00\u4e2a\u9488\u5bf9\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\uff0c\u53e6\u4e00\u4e2a\u662f\u9996\u4e2a\u7edf\u4e00\u7684MSA\u548c\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\uff08CA\uff09\u516c\u5171\u6a21\u578b\u3002\u8fd9\u4e24\u4e2a\u6a21\u578b\u5728\u5404\u81ea\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u590d\u73b0\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u963f\u62c9\u4f2f\u8bed\u662f\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u8bed\u8a00\u4e4b\u4e00\uff0c\u4f46\u5176\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u5f00\u53d1\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u539f\u56e0\u5728\u4e8e\u8bed\u8a00\u7684\u590d\u6742\u6027\u4ee5\u53ca\u516c\u5f00\u53ef\u7528\u6a21\u578b\u7684\u7a00\u7f3a\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8MSA\uff0c\u5bf9\u8bed\u8a00\u5185\u90e8\u53d8\u4f53\u7684\u5173\u6ce8\u8f83\u5c11\u3002

Method: \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u548c\u6587\u672c\u5904\u7406\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u8be5\u8bed\u8a00\u7279\u6709\u7684\u6311\u6218\u3002\u57fa\u4e8eFastConformer\u67b6\u6784\u8bad\u7ec3\u4e86\u4e24\u4e2a\u6a21\u578b\uff1a\u4e00\u4e2a\u4e13\u4e3aMSA\u8bbe\u8ba1\uff0c\u53e6\u4e00\u4e2a\u662f\u9996\u4e2a\u7edf\u4e00\u7684MSA\u548cCA\u516c\u5171\u6a21\u578b\u3002

Result: MSA\u6a21\u578b\u5728\u76f8\u5173\u6570\u636e\u96c6\u4e0a\u521b\u4e0b\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002\u7edf\u4e00\u6a21\u578b\u5728CA\u4e0a\u5b9e\u73b0\u4e86\u5e26\u97f3\u7b26\u7684\u6700\u5148\u8fdb\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86MSA\u7684\u5f3a\u5927\u6027\u80fd\u3002

Conclusion: \u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u901a\u7528\u65b9\u6cd5\u548c\u8bad\u7ec3\u9ad8\u6027\u80fd\u7684FastConformer\u6a21\u578b\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u963f\u62c9\u4f2f\u8bedASR\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728MSA\u548c\u7edf\u4e00MSA/CA\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86SOTA\u6210\u679c\u3002\u6a21\u578b\u7684\u5f00\u6e90\u6709\u52a9\u4e8e\u63d0\u9ad8\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002

Abstract: Despite Arabic being one of the most widely spoken languages, the development
of Arabic Automatic Speech Recognition (ASR) systems faces significant
challenges due to the language's complexity, and only a limited number of
public Arabic ASR models exist. While much of the focus has been on Modern
Standard Arabic (MSA), there is considerably less attention given to the
variations within the language. This paper introduces a universal methodology
for Arabic speech and text processing designed to address unique challenges of
the language. Using this methodology, we train two novel models based on the
FastConformer architecture: one designed specifically for MSA and the other,
the first unified public model for both MSA and Classical Arabic (CA). The MSA
model sets a new benchmark with state-of-the-art (SOTA) performance on related
datasets, while the unified model achieves SOTA accuracy with diacritics for CA
while maintaining strong performance for MSA. To promote reproducibility, we
open-source the models and their training recipes.

</details>


### [141] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: RHYTHM\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\u9884\u6d4b\u548c\u63a8\u7406\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u5c42\u65f6\u95f4\u6807\u8bb0\u5316\u548c\u9884\u8ba1\u7b97\u7684\u63d0\u793a\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\u548c\u8f68\u8ff9\u63a8\u7406\uff0c\u540c\u65f6\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5e8f\u5217\u957f\u5ea6\u8fc7\u957f\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002

Method: RHYTHM\u6846\u67b6\u5c06\u8f68\u8ff9\u5206\u5272\u6210\u6bcf\u65e5\u7247\u6bb5\uff0c\u5e76\u5c06\u5176\u7f16\u7801\u4e3a\u5177\u6709\u5206\u5c42\u6ce8\u610f\u529b\uff08\u6355\u6349\u6bcf\u65e5\u548c\u6bcf\u5468\u4f9d\u8d56\u6027\uff09\u7684\u79bb\u6563\u6807\u8bb0\uff0c\u4ece\u800c\u5927\u5e45\u7f29\u77ed\u5e8f\u5217\u957f\u5ea6\u3002\u901a\u8fc7\u51bb\u7ed3\u7684LLM\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u7684\u63d0\u793a\u5d4c\u5165\u6765\u4e30\u5bcc\u6807\u8bb0\u8868\u793a\uff0c\u589e\u5f3a\u6a21\u578b\u6355\u83b7\u76f8\u4e92\u4f9d\u8d56\u6027\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002

Result: \u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRHYTHM\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e862.4%\uff0c\u5468\u672b\u51c6\u786e\u6027\u63d0\u9ad8\u4e865.0%\uff0c\u8bad\u7ec3\u65f6\u95f4\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u51cf\u5c11\u4e8624.6%\u3002

Conclusion: RHYTHM\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u6210\u529f\u5730\u5c06LLMs\u5e94\u7528\u4e8e\u8be5\u9886\u57df\uff0c\u5e76\u5728\u51c6\u786e\u6027\uff08\u5c24\u5176\u662f\u5728\u5468\u672b\uff09\u548c\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [142] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u8ba4\u77e5\u6210\u5bf9\u6bd4\u8f83\u5206\u7c7b\u6a21\u578b\u9009\u62e9\uff08CPC-CMS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u6863\u7ea7\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u4e13\u5bb6\u5224\u65ad\u52a0\u6743\u8bc4\u4f30\u6807\u51c6\u6765\u9009\u62e9\u6700\u4f73\u5206\u7c7b\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u6587\u6863\u7ea7\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u9009\u62e9\u6700\u4f73\u5206\u7c7b\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u80fd\u7efc\u5408\u8003\u8651\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\u3001F1\u5206\u6570\uff09\u548c\u6548\u7387\uff08\u65f6\u95f4\u6d88\u8017\uff09\u3002

Method: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86CPC-CMS\u6846\u67b6\uff1a1. \u4f7f\u7528\u57fa\u4e8e\u4e13\u5bb6\u5224\u65ad\u7684\u8ba4\u77e5\u6210\u5bf9\u6bd4\u8f83\uff08CPC\uff09\u6765\u8ba1\u7b97\u8bc4\u4f30\u6807\u51c6\uff08\u5305\u62ec\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001\u7279\u5f02\u6027\u3001MCC\u3001Kappa\u548c\u6548\u7387\uff09\u7684\u6743\u91cd\u30022. \u9009\u62e9\u4e86\u591a\u79cd\u57fa\u7ebf\u5206\u7c7b\u6a21\u578b\uff08\u5982\u6734\u7d20\u8d1d\u53f6\u65af\u3001LSVC\u3001\u968f\u673a\u68ee\u6797\u3001LSTM\u3001ALBERT\u7b49\uff09\u30023. \u6784\u5efa\u4e00\u4e2a\u52a0\u6743\u51b3\u7b56\u77e9\u9635\uff0c\u7ed3\u5408\u5206\u7c7b\u8bc4\u4f30\u5206\u6570\u548c\u6807\u51c6\u6743\u91cd\uff0c\u4ee5\u9009\u62e9\u6700\u4f73\u6a21\u578b\u30024. \u4f7f\u7528\u4e09\u4e2a\u793e\u4ea4\u5a92\u4f53\u5f00\u653e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002

Result: \u6a21\u62df\u7ed3\u679c\u663e\u793a\uff1a1. \u5728\u4e0d\u8003\u8651\u65f6\u95f4\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\uff0cALBERT\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u6700\u4f73\u30022. \u5982\u679c\u8003\u8651\u65f6\u95f4\u6d88\u8017\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002

Conclusion: CPC-CMS\u6846\u67b6\u5728\u6587\u6863\u7ea7\u60c5\u611f\u5206\u6790\u6a21\u578b\u9009\u62e9\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df\u7684\u5206\u7c7b\u5e94\u7528\u3002

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [143] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86DENSE\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u533b\u751f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6574\u5408\u5206\u6563\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5177\u6709\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u8fdb\u5c55\u8bb0\u5f55\uff0c\u4ee5\u5f25\u8865\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u8fdb\u5c55\u8bb0\u5f55\u7684\u4e0d\u8db3\u3002


<details>
  <summary>Details</summary>
Motivation: \u8fdb\u5c55\u8bb0\u5f55\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u610f\u4e49\uff0c\u4f46\u5b83\u4eec\u5728\u5927\u578bEHR\u6570\u636e\u96c6\u4e2d\u4e25\u91cd\u4e0d\u8db3\uff08\u4f8b\u5982\uff0cMIMIC-III\u6570\u636e\u96c6\u4e2d\u4ec5\u6709\u7ea68.56%\u7684\u4f4f\u9662\u8bbf\u95ee\u5305\u542b\u8fdb\u5c55\u8bb0\u5f55\uff09\uff0c\u5bfc\u81f4\u60a3\u8005\u7eb5\u5411\u53d9\u8ff0\u5b58\u5728\u7a7a\u767d\u3002

Method: \u7814\u7a76\u5f00\u53d1\u4e86DENSE\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u6a21\u62df\u533b\u751f\u5728\u64b0\u5199\u8fdb\u5c55\u8bb0\u5f55\u65f6\u53c2\u8003\u8fc7\u5f80\u5c31\u8bca\u4fe1\u606f\u7684\u65b9\u5f0f\u3002\u5b83\u5f15\u5165\u4e86\u7ec6\u7c92\u5ea6\u7b14\u8bb0\u5206\u7c7b\u548c\u65f6\u95f4\u5bf9\u9f50\u673a\u5236\uff0c\u5c06\u4e0d\u540c\u5c31\u8bca\u4e2d\u7684\u5f02\u6784\u7b14\u8bb0\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u3001\u6309\u65f6\u95f4\u987a\u5e8f\u6392\u5217\u7684\u8f93\u5165\u3002\u6838\u5fc3\u662f\u5229\u7528\u4e34\u5e8a\u77e5\u60c5\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u4ece\u5f53\u524d\u548c\u4e4b\u524d\u7684\u5c31\u8bca\u4e2d\u8bc6\u522b\u51fa\u65f6\u95f4\u4e0a\u548c\u8bed\u4e49\u4e0a\u76f8\u5173\u7684\u8bc1\u636e\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u8bc1\u636e\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4e34\u5e8a\u8fde\u8d2f\u4e14\u65f6\u95f4\u611f\u77e5\u7684\u8fdb\u5c55\u8bb0\u5f55\u3002

Result: DENSE\u7cfb\u7edf\u751f\u6210\u7684\u8fdb\u5c55\u8bb0\u5f55\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7eb5\u5411\u4fdd\u771f\u5ea6\uff0c\u65f6\u95f4\u5bf9\u9f50\u6bd4\u8fbe\u52301.089\uff0c\u8d85\u8d8a\u4e86\u539f\u59cb\u7b14\u8bb0\u4e2d\u89c2\u5bdf\u5230\u7684\u8fde\u7eed\u6027\u3002

Conclusion: \u8be5\u7cfb\u7edf\u901a\u8fc7\u6062\u590d\u788e\u7247\u5316\u6587\u6863\u7684\u53d9\u8ff0\u8fde\u8d2f\u6027\uff0c\u652f\u6301\u6539\u8fdb\u4e0b\u6e38\u4efb\u52a1\uff0c\u5982\u603b\u7ed3\u3001\u9884\u6d4b\u5efa\u6a21\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\uff0c\u5e76\u4e3a\u5728\u771f\u5b9e\u533b\u7597\u73af\u5883\u4e2d\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7b14\u8bb0\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [144] [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)
*Israt Jahan,Md Tahmid Rahman Laskar,Chun Peng,Jimmy Huang*

Main category: cs.CL

TL;DR: \u672c\u6587\u8bc4\u4f30\u4e86\u591a\u79cd\u95ed\u6e90\u548c\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u751f\u7269\u533b\u5b66\u4efb\u52a1\uff08\u5305\u62ec\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff09\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u5f00\u6e90LLMs\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u4e0e\u95ed\u6e90\u6a21\u578b\u5ab2\u7f8e\u751a\u81f3\u66f4\u4f18\uff0c\u5e76\u63d0\u4f9b\u989d\u5916\u4f18\u52bf\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u6210\u672c\u6548\u76ca\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6837\u5316\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u6700\u9002\u5408\u7279\u5b9a\u5e94\u7528\u7684\u6a21\u578b\u3002

Method: \u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u95ed\u6e90\u548c\u5f00\u6e90LLMs\uff0c\u6d4b\u8bd5\u4efb\u52a1\u6db5\u76d6\u751f\u7269\u533b\u5b66\u6587\u672c\u5206\u7c7b\u3001\u751f\u6210\u3001\u95ee\u7b54\u4ee5\u53ca\u591a\u6a21\u6001\u56fe\u50cf\u5904\u7406\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6ca1\u6709\u5355\u4e00LLM\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u5176\u4ed6\u6a21\u578b\uff1b\u4e0d\u540cLLMs\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u867d\u7136\u4e00\u4e9b\u95ed\u6e90LLMs\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5176\u5f00\u6e90\u5bf9\u5e94\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u53ef\u6bd4\uff08\u6709\u65f6\u751a\u81f3\u66f4\u597d\uff09\u7684\u7ed3\u679c\uff0c\u5e76\u5177\u6709\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u589e\u5f3a\u7684\u9690\u79c1\u6027\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u4e3a\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9009\u62e9\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u6307\u51fa\u5f00\u6e90LLMs\u662f\u5177\u6709\u7ade\u4e89\u529b\u7684\u9009\u62e9\u3002

Abstract: This paper presents a comprehensive evaluation of cost-efficient Large
Language Models (LLMs) for diverse biomedical tasks spanning both text and
image modalities. We evaluated a range of closed-source and open-source LLMs on
tasks such as biomedical text classification and generation, question
answering, and multimodal image processing. Our experimental findings indicate
that there is no single LLM that can consistently outperform others across all
tasks. Instead, different LLMs excel in different tasks. While some
closed-source LLMs demonstrate strong performance on specific tasks, their
open-source counterparts achieve comparable results (sometimes even better),
with additional benefits like faster inference and enhanced privacy. Our
experimental results offer valuable insights for selecting models that are
optimally suited for specific biomedical applications.

</details>


### [145] [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
*Lautaro Estienne,Gabriel Ben Zenou,Nona Naderi,Jackie Cheung,Pablo Piantanida*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u534f\u4f5c\u7406\u6027\u8a00\u8bed\u884c\u4e3a\uff08CRSA\uff09\u6846\u67b6\uff0c\u4e00\u4e2aRSA\u7684\u4fe1\u606f\u8bba\u6269\u5c55\uff0c\u7528\u4e8e\u5efa\u6a21\u591a\u8f6e\u534f\u4f5c\u5bf9\u8bdd\uff0c\u901a\u8fc7\u4f18\u5316\u53d7\u7387\u5931\u771f\u7406\u8bba\u542f\u53d1\u7684\u589e\u76ca\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u534f\u4f5c\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684AI\u8bed\u8a00\u4ee3\u7406\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709AI\u7cfb\u7edf\u5728\u534f\u4f5c\u89d2\u8272\u4e2d\u9700\u8981\u63a8\u7406\u5171\u4eab\u76ee\u6807\u548c\u4fe1\u5ff5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u6d41\u7545\u8bed\u8a00\u3002\u5c3d\u7ba1\u7406\u6027\u8a00\u8bed\u884c\u4e3a\uff08RSA\uff09\u6846\u67b6\u63d0\u4f9b\u4e86\u8bed\u7528\u63a8\u7406\u65b9\u6cd5\uff0c\u4f46\u5176\u73b0\u6709\u6269\u5c55\u5728\u5904\u7406\u591a\u8f6e\u3001\u534f\u4f5c\u573a\u666f\u65f6\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\u3002

Method: \u672c\u6587\u5f15\u5165\u4e86\u534f\u4f5c\u7406\u6027\u8a00\u8bed\u884c\u4e3a\uff08CRSA\uff09\u6846\u67b6\uff0c\u5b83\u662fRSA\u7684\u4e00\u4e2a\u4fe1\u606f\u8bba\uff08IT\uff09\u6269\u5c55\u3002CRSA\u901a\u8fc7\u4f18\u5316\u4e00\u4e2a\u6e90\u81ea\u7387\u5931\u771f\u7406\u8bba\u7684\u589e\u76ca\u51fd\u6570\u6765\u5efa\u6a21\u591a\u8f6e\u5bf9\u8bdd\u3002\u8fd9\u4e2a\u589e\u76ca\u51fd\u6570\u8003\u8651\u4e86\u5bf9\u8bdd\u4e2d\u53cc\u65b9\u4ee3\u7406\u90fd\u62e5\u6709\u79c1\u6709\u4fe1\u606f\u5e76\u6839\u636e\u5bf9\u8bdd\u751f\u6210\u8bdd\u8bed\u7684\u60c5\u51b5\u3002

Result: CRSA\u5728\u6307\u4ee3\u6e38\u620f\u548c\u533b\u5b66\u9886\u57df\u7684\u57fa\u4e8e\u6a21\u677f\u7684\u533b\u60a3\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cCRSA\u6bd4\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u80fd\u4ea7\u751f\u66f4\u4e00\u81f4\u3001\u66f4\u53ef\u89e3\u91ca\u548c\u66f4\u5177\u534f\u4f5c\u6027\u7684\u884c\u4e3a\u3002

Conclusion: CRSA\u4e3a\u5f00\u53d1\u66f4\u5177\u8bed\u7528\u610f\u8bc6\u548c\u793e\u4f1a\u611f\u77e5\u80fd\u529b\u7684\u8bed\u8a00\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002

Abstract: As AI systems take on collaborative roles, they must reason about shared
goals and beliefs-not just generate fluent language. The Rational Speech Act
(RSA) framework offers a principled approach to pragmatic reasoning, but
existing extensions face challenges in scaling to multi-turn, collaborative
scenarios. In this paper, we introduce Collaborative Rational Speech Act
(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn
dialog by optimizing a gain function adapted from rate-distortion theory. This
gain is an extension of the gain model that is maximized in the original RSA
model but takes into account the scenario in which both agents in a
conversation have private information and produce utterances conditioned on the
dialog. We demonstrate the effectiveness of CRSA on referential games and
template-based doctor-patient dialogs in the medical domain. Empirical results
show that CRSA yields more consistent, interpretable, and collaborative
behavior than existing baselines-paving the way for more pragmatic and socially
aware language agents.

</details>


### [146] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: PLABA\u8d5b\u9053\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u4e13\u4e1a\u751f\u7269\u533b\u5b66\u6587\u732e\u6539\u5199\u4e3a\u901a\u4fd7\u6613\u61c2\u8bed\u8a00\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7b80\u6d01\u6027\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002


<details>
  <summary>Details</summary>
Motivation: \u8bed\u8a00\u6a21\u578b\u5728\u5c06\u4e13\u4e1a\u751f\u7269\u533b\u5b66\u6587\u732e\u6539\u7f16\u4e3a\u901a\u4fd7\u8bed\u8a00\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u5176\u4e0d\u53ef\u9884\u6d4b\u6027\u4ee5\u53ca\u8be5\u9886\u57df\u6f5c\u5728\u7684\u9ad8\u98ce\u9669\uff0c\u9700\u8981\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002

Method: \u57282023\u5e74\u548c2024\u5e74\u6587\u672c\u68c0\u7d22\u4f1a\u8bae\uff08TREC\uff09\u4e0a\u4e3e\u529e\u4e86\u751f\u7269\u533b\u5b66\u6458\u8981\u901a\u4fd7\u8bed\u8a00\u6539\u7f16\uff08PLABA\uff09\u8d5b\u9053\u3002\u4efb\u52a1\u5305\u62ec\u6458\u8981\u7684\u5b8c\u6574\u53e5\u7ea7\u91cd\u5199\uff08\u4efb\u52a11\uff09\u4ee5\u53ca\u8bc6\u522b\u548c\u66ff\u6362\u96be\u61c2\u672f\u8bed\uff08\u4efb\u52a12\uff09\u3002\u4efb\u52a11\u5f00\u53d1\u4e86\u56db\u5957\u4e13\u4e1a\u64b0\u5199\u7684\u53c2\u8003\u6587\u672c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\uff0c\u4e24\u9879\u4efb\u52a1\u7684\u63d0\u4ea4\u90fd\u7531\u751f\u7269\u533b\u5b66\u4e13\u5bb6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u4eba\u5de5\u8bc4\u4f30\u3002

Result: \u5171\u670912\u652f\u6765\u81ea12\u4e2a\u56fd\u5bb6\u7684\u56e2\u961f\u53c2\u4e0e\u3002\u4efb\u52a11\u7684\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\uff0c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u53ef\u4e0e\u4eba\u7c7b\u5ab2\u7f8e\uff0c\u4f46\u5728\u7b80\u6d01\u6027\u548c\u7b80\u7ec3\u6027\u65b9\u9762\u4e0d\u8db3\u3002\u81ea\u52a8\u3001\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u76f8\u5173\u6027\u666e\u904d\u4e0d\u4f73\u3002\u4efb\u52a12\u4e2d\uff0c\u7cfb\u7edf\u5728\u8bc6\u522b\u96be\u61c2\u672f\u8bed\u548c\u5206\u7c7b\u66ff\u6362\u65b9\u5f0f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u7136\u800c\uff0c\u5728\u751f\u6210\u66ff\u6362\u5185\u5bb9\u65f6\uff0c\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u4eba\u5de5\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u548c\u7b80\u6d01\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7b80\u7ec3\u6027\u65b9\u9762\u4e0d\u8db3\u3002

Conclusion: PLABA\u8d5b\u9053\u5c55\u793a\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u751f\u7269\u533b\u5b66\u6587\u732e\u6539\u7f16\u4e3a\u9762\u5411\u5927\u4f17\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u4e0d\u8db3\u4ee5\u53ca\u5bf9\u6539\u8fdb\u81ea\u52a8\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u7684\u9700\u6c42\u3002

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [147] [Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms](https://arxiv.org/abs/2507.13455)
*Dean Chen,Armin Pomeroy,Brandon T. Peterson,Will Flanagan,He Kai Lim,Alexandra Stavrakis,Nelson F. SooHoo,Jonathan B. Hopkins,Tyler R. Clites*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u9650\u5236\u96c6\u6210\u5230\u7d27\u51d1\u7684\u9650\u4f4d\u8868\u9762\u4e2d\uff0c\u4e3a\u67d4\u6027\u673a\u6784\u63d0\u4f9b\u8fc7\u8f7d\u4fdd\u62a4\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u673a\u6784\u5904\u4e8e\u5f39\u6027\u8303\u56f4\u5185\u7684\u540c\u65f6\u6700\u5927\u5316\u5176\u5de5\u4f5c\u7a7a\u95f4\u3002


<details>
  <summary>Details</summary>
Motivation: \u67d4\u6027\u673a\u6784\u5728\u7cbe\u5bc6\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u56fa\u6709\u7684\u75b2\u52b3\u548c\u673a\u68b0\u5931\u6548\u95ee\u9898\u963b\u788d\u4e86\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u8d1f\u8f7d\u590d\u6742\u548c\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u3002\u4f20\u7edf\u9650\u4f4d\u8bbe\u8ba1\u5728\u591a\u81ea\u7531\u5ea6\u7a7a\u95f4\u4e2d\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u65e0\u6cd5\u6709\u6548\u5e73\u8861\u5b89\u5168\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u3002

Method: \u5f15\u5165\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u8bbe\u8ba1\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e00\u4e2a\u7d27\u51d1\u7684\u9650\u4f4d\u8868\u9762\u5bf9\u4e2d\u96c6\u6210\u8026\u5408\u7684\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u9650\u5236\u6765\u4fdd\u8bc1\u67d4\u6027\u673a\u6784\u7684\u8fc7\u8f7d\u4fdd\u62a4\u3002\u5177\u4f53\u800c\u8a00\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u548c\u5b9e\u8df5\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u63a5\u89e6\u8868\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u4ee5\u5728\u786e\u4fdd\u673a\u6784\u4fdd\u6301\u5f39\u6027\u72b6\u6001\u7684\u540c\u65f6\u6700\u5927\u5316\u5176\u591a\u81ea\u7531\u5ea6\u5de5\u4f5c\u7a7a\u95f4\u3002

Result: \u8be5\u5408\u6210\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u9aa8\u79d1\u690d\u5165\u7269\u7684\u7b3c\u5f0f\u94f0\u94fe\u673a\u6784\u6848\u4f8b\u7814\u7a76\u3002\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u5bfc\u51fa\u7684\u8bbe\u8ba1\u80fd\u6709\u6548\u9632\u6b62\u75b2\u52b3\u3001\u5c48\u670d\u548c\u5c48\u66f2\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u4fdd\u62a4\u3002

Conclusion: \u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u4e0d\u786e\u5b9a\u8d1f\u8f7d\u4e0b\u8fd0\u884c\u7684\u67d4\u6027\u7cfb\u7edf\u4e2d\u7684\u7cbe\u5bc6\u9650\u4f4d\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u662f\u4f7f\u67d4\u6027\u673a\u6784\u5e94\u7528\u4e8e\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u4e00\u6b65\u3002

Abstract: Compliant mechanisms have significant potential in precision applications due
to their ability to guide motion without contact. However, an inherent
vulnerability to fatigue and mechanical failure has hindered the translation of
compliant mechanisms to real-world applications. This is particularly
challenging in service environments where loading is complex and uncertain, and
the cost of failure is high. In such cases, mechanical hard stops are critical
to prevent yielding and buckling. Conventional hard-stop designs, which rely on
stacking single-DOF limits, must be overly restrictive in multi-DOF space to
guarantee safety in the presence of unknown loads. In this study, we present a
systematic design synthesis method to guarantee overload protection in
compliant mechanisms by integrating coupled multi-DOF motion limits within a
single pair of compact hard-stop surfaces. Specifically, we introduce a
theoretical and practical framework for optimizing the contact surface geometry
to maximize the mechanisms multi-DOF working space while still ensuring that
the mechanism remains within its elastic regime. We apply this synthesis method
to a case study of a caged-hinge mechanism for orthopaedic implants, and
provide numerical and experimental validation that the derived design offers
reliable protection against fatigue, yielding, and buckling. This work
establishes a foundation for precision hard-stop design in compliant systems
operating under uncertain loads, which is a crucial step toward enabling the
application of compliant mechanisms in real-world systems.

</details>


### [148] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: ERR@HRI 2.0\u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u673a\u5236\uff0c\u63a8\u52a8\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u9519\u8bef\u3002


<details>
  <summary>Details</summary>
Motivation: \u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u52a8\u6001\u6027\uff0c\u4f46\u5b83\u4eec\u4ecd\u6613\u4e8e\u51fa\u9519\uff0c\u4f8b\u5982\u8bef\u89e3\u7528\u6237\u610f\u56fe\u3001\u8fc7\u65e9\u6253\u65ad\u6216\u65e0\u6cd5\u54cd\u5e94\u3002\u68c0\u6d4b\u548c\u89e3\u51b3\u8fd9\u4e9b\u6545\u969c\u5bf9\u4e8e\u9632\u6b62\u5bf9\u8bdd\u4e2d\u65ad\u3001\u907f\u514d\u4efb\u52a1\u4e2d\u65ad\u548c\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002

Method: ERR@HRI 2.0\u6311\u6218\u8d5b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b16\u5c0f\u65f6\u7684\u4eba\u673a\u4ea4\u4e92\u6570\u636e\uff0c\u6db5\u76d6\u9762\u90e8\u3001\u8bed\u97f3\u548c\u5934\u90e8\u8fd0\u52a8\u7279\u5f81\u3002\u6bcf\u6bb5\u4ea4\u4e92\u90fd\u6807\u6ce8\u4e86\u673a\u5668\u4eba\u9519\u8bef\u7684\u5b58\u5728\u4e0e\u5426\uff08\u7cfb\u7edf\u89c6\u89d2\uff09\u4ee5\u53ca\u7528\u6237\u7ea0\u6b63\u610f\u56fe\u3002\u53c2\u4e0e\u8005\u9700\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u68c0\u6d4b\u8fd9\u4e9b\u6545\u969c\uff0c\u5e76\u6839\u636e\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8bef\u62a5\u7387\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002

Result: \u8be5\u62bd\u8c61\u63cf\u8ff0\u4e86\u4e00\u4e2a\u6311\u6218\u800c\u975e\u5df2\u5b8c\u6210\u7684\u7814\u7a76\u7ed3\u679c\u3002\u6311\u6218\u7684\u9884\u671f\u7ed3\u679c\u662f\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5e76\u57fa\u51c6\u6d4b\u8bd5\u80fd\u591f\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u68c0\u6d4bLLM\u9a71\u52a8\u673a\u5668\u4eba\u6545\u969c\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6700\u7ec8\u901a\u8fc7\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8bef\u62a5\u7387\u7b49\u6027\u80fd\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002

Conclusion: \u8be5\u6311\u6218\u662f\u5229\u7528\u793e\u4ea4\u4fe1\u53f7\u5206\u6790\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u6545\u969c\u68c0\u6d4b\u7684\u5173\u952e\u4e00\u6b65\uff0c\u65e8\u5728\u63d0\u5347\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u53ef\u9760\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [149] [SCOPE for Hexapod Gait Generation](https://arxiv.org/abs/2507.13539)
*Jim O'Connor,Jay B. Nash,Derin Gezgin,Gary B. Parker*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51faSCOPE\u7b97\u6cd5\uff0c\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u663e\u8457\u538b\u7f29\u516d\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u7684\u8f93\u5165\u7ef4\u5ea6\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u4e86\u8fdb\u5316\u7b97\u6cd5\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u8fdb\u5316\u65b9\u6cd5\u5728\u516d\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u4e2d\uff0c\u968f\u7740\u8f93\u5165\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u5176\u5b66\u4e60\u6709\u6548\u7b56\u7565\u7684\u80fd\u529b\u4f1a\u8fc5\u901f\u4e0b\u964d\u3002\u8fd9\u6e90\u4e8e\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u968f\u53c2\u6570\u6570\u91cf\u589e\u52a0\u800c\u5448\u6307\u6570\u7ea7\u589e\u957f\u7684\u95ee\u9898\u3002

Method: \u5f15\u5165\u7a00\u758f\u4f59\u5f26\u4f18\u5316\u7b56\u7565\u8fdb\u5316\uff08SCOPE\uff09\u7b97\u6cd5\u3002SCOPE\u5229\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u76f4\u63a5\u4ece\u8f93\u5165\u77e9\u9635\u7684\u7279\u5f81\u7cfb\u6570\u4e2d\u5b66\u4e60\u3002\u901a\u8fc7\u622a\u65adDCT\u8fd4\u56de\u7684\u7cfb\u6570\u77e9\u9635\uff0c\u53ef\u4ee5\u5728\u4fdd\u7559\u539f\u59cb\u8f93\u5165\u6700\u9ad8\u80fd\u91cf\u7279\u5f81\u7684\u540c\u65f6\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u516d\u8db3\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u5c06\u65f6\u5e8f\u59ff\u6001\u4fe1\u606f\u8f93\u5165\u8f6c\u6362\u4e3a\u6b65\u6001\u53c2\u6570\u3002

Result: \u4e0e\u53c2\u8003\u7b97\u6cd5\u76f8\u6bd4\uff0cSCOPE\u4f7f\u516d\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u7684\u6548\u7387\u63d0\u9ad8\u4e8620%\u3002\u5b83\u5c06\u65f6\u5e8f\u59ff\u6001\u6570\u636e\u7684\u603b\u8f93\u5165\u5927\u5c0f\u4ece2700\u51cf\u5c11\u523054\uff0c\u964d\u4f4e\u4e8698%\u3002\u6b64\u5916\uff0cSCOPE\u80fd\u591f\u5c06\u8f93\u5165\u538b\u7f29\u5230\u4efb\u4f55\u8f93\u51fa\u5f62\u72b6\uff0c\u53ea\u8981\u6bcf\u4e2a\u8f93\u51fa\u7ef4\u5ea6\u4e0d\u5927\u4e8e\u76f8\u5e94\u7684\u8f93\u5165\u7ef4\u5ea6\u3002

Conclusion: SCOPE\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u538b\u7f29\u8fdb\u5316\u63a7\u5236\u5668\u8f93\u5165\u7684\u5c3a\u5bf8\uff0c\u4ece\u800c\u5e26\u6765\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002

Abstract: Evolutionary methods have previously been shown to be an effective learning
method for walking gaits on hexapod robots. However, the ability of these
algorithms to evolve an effective policy rapidly degrades as the input space
becomes more complex. This degradation is due to the exponential growth of the
solution space, resulting from an increasing parameter count to handle a more
complex input. In order to address this challenge, we introduce Sparse Cosine
Optimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine
Transform (DCT) to learn directly from the feature coefficients of an input
matrix. By truncating the coefficient matrix returned by the DCT, we can reduce
the dimensionality of an input while retaining the highest energy features of
the original input. We demonstrate the effectiveness of this method by using
SCOPE to learn the gait of a hexapod robot. The hexapod controller is given a
matrix input containing time-series information of previous poses, which are
then transformed to gait parameters by an evolved policy. In this task, the
addition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.
SCOPE achieves this result by reducing the total input size of the time-series
pose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of
compressing an input to any output shape, provided that each output dimension
is no greater than the corresponding input dimension. This paper demonstrates
that SCOPE is capable of significantly compressing the size of an input to an
evolved controller, resulting in a statistically significant gain in efficacy.

</details>


### [150] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: \u5c06\u4f4e\u6210\u672cGELLO\u9065\u64cd\u4f5c\u7cfb\u7edf\u6269\u5c55\uff0c\u589e\u52a0\u529b\u53cd\u9988\u548c\u7528\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u529b\u4fe1\u606f\uff0c\u7ed3\u679c\u8868\u660e\u7528\u6237\u504f\u597d\u5ea6\u63d0\u9ad8\u4e14\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684GELLO\u9065\u64cd\u4f5c\u7cfb\u7edf\u4ec5\u652f\u6301\u5173\u8282\u4f4d\u7f6e\u63a7\u5236\uff0c\u7f3a\u4e4f\u529b\u4fe1\u606f\u3002\u7814\u7a76\u52a8\u673a\u662f\u589e\u5f3a\u5176\u80fd\u529b\uff0c\u52a0\u5165\u529b\u4fe1\u606f\u4ee5\u5b9e\u73b0\u529b\u53cd\u9988\uff0c\u5e76\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u3002

Method: 1. \u6269\u5c55GELLO\u9065\u64cd\u4f5c\u7cfb\u7edf\u4ee5\u5305\u542b\u529b\u4fe1\u606f\u30022. \u5b9e\u73b0\u529b\u53cd\u9988\u529f\u80fd\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u611f\u77e5\u73af\u5883\u963b\u529b\u30023. \u5c06\u529b\u4fe1\u606f\u7eb3\u5165\u6570\u636e\u6536\u96c6\u548c\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u30024. \u5728\u914d\u5907Franka Panda\u673a\u68b0\u81c2\u7684GELLO\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u8fd9\u4e9b\u6269\u5c55\u30025. \u8fdb\u884c\u7528\u6237\u7814\u7a76\u30026. \u6bd4\u8f83\u6709\u65e0\u529b\u4fe1\u606f\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002

Result: 1. \u5b9a\u6027\u7ed3\u679c\u663e\u793a\uff0c\u6709\u673a\u5668\u4eba\u7ecf\u9a8c\u7684\u7528\u6237\u66f4\u504f\u597d\u65b0\u7684\u63a7\u5236\u5668\u30022. \u589e\u52a0\u529b\u8f93\u5165\u63d0\u9ad8\u4e86\u5927\u591a\u6570\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002

Conclusion: \u5c06\u529b\u4fe1\u606f\u6574\u5408\u5230GELLO\u9065\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u65e0\u8bba\u662f\u7528\u4e8e\u529b\u53cd\u9988\u8fd8\u662f\u6a21\u4eff\u5b66\u4e60\uff0c\u90fd\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u5e76\u63d0\u9ad8\u4e86\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [151] [Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones](https://arxiv.org/abs/2507.13647)
*Minze Li,Wei Zhao,Ran Chen,Mingqiang Wei*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51faPE-PSO\uff0c\u4e00\u79cd\u589e\u5f3a\u578b\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u6269\u5c55\u81f3\u591a\u65e0\u4eba\u673a\u8702\u7fa4\uff0c\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u5728\u8f68\u8ff9\u8d28\u91cf\u3001\u80fd\u6548\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u7684\u4f18\u8d8a\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u7531\u4e8e\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u5feb\u901f\u81ea\u9002\u5e94\u54cd\u5e94\u7684\u9700\u8981\uff0c\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\u4ecd\u662f\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u7c92\u5b50\u7fa4\u4f18\u5316(PSO)\u65b9\u6cd5\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u5e38\u9762\u4e34\u65e9\u719f\u6536\u655b\u548c\u5ef6\u8fdf\u95ee\u9898\u3002

Method: \u63d0\u51faPE-PSO\uff0c\u901a\u8fc7\u5f15\u5165\u6301\u4e45\u63a2\u7d22\u673a\u5236\u4fdd\u6301\u79cd\u7fa4\u591a\u6837\u6027\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u71b5\u7684\u53c2\u6570\u8c03\u6574\u7b56\u7565\u52a8\u6001\u9002\u5e94\u4f18\u5316\u884c\u4e3a\u3002\u65e0\u4eba\u673a\u8f68\u8ff9\u4f7f\u7528B-spline\u66f2\u7ebf\u5efa\u6a21\u4ee5\u786e\u4fdd\u5e73\u6ed1\u6027\u5e76\u964d\u4f4e\u4f18\u5316\u590d\u6742\u6027\u3002\u4e3a\u652f\u6301\u65e0\u4eba\u673a\u8702\u7fa4\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5(GA)\u4efb\u52a1\u5206\u914d\u548c\u5206\u5e03\u5f0fPE-PSO\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u534f\u8c03\u8f68\u8ff9\u751f\u6210\u3002

Result: \u7efc\u5408\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8f68\u8ff9\u8d28\u91cf\u3001\u80fd\u6e90\u6548\u7387\u3001\u907f\u969c\u548c\u8ba1\u7b97\u65f6\u95f4\u7b49\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edfPSO\u548c\u5176\u4ed6\u57fa\u4e8e\u7fa4\u4f53\u7684\u89c4\u5212\u5668\u3002

Conclusion: \u8fd9\u4e9b\u7ed3\u679c\u8bc1\u5b9e\u4e86PE-PSO\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u5b9e\u65f6\u591a\u65e0\u4eba\u673a\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u548c\u9002\u7528\u6027\u3002

Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.

</details>


### [152] [Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography](https://arxiv.org/abs/2507.13650)
*Yu-Ting Lai,Yasamin Foroutani,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u77b3\u5b54\u900f\u5c04\u548c\u773c\u5185OCT\u63a2\u5934\uff0c\u5b9e\u73b0\u767d\u5185\u969c\u672f\u540e\u7ee7\u53d1\u6027\u767d\u5185\u969c\u7684\u6676\u72b6\u4f53\u56ca\u819c\u6e05\u6d01\uff0c\u63d0\u4f9b\u589e\u5f3a\u7684\u53ef\u89c6\u5316\u548c\u5b9e\u65f6\u5de5\u5177-\u7ec4\u7ec7\u8ddd\u79bb\u53cd\u9988\u3002


<details>
  <summary>Details</summary>
Motivation: \u7ee7\u53d1\u6027\u767d\u5185\u969c\u662f\u767d\u5185\u969c\u624b\u672f\u540e\u5e38\u89c1\u7684\u89c6\u529b\u4e27\u5931\u5e76\u53d1\u75c7\uff0c\u7531\u6b8b\u4f59\u6676\u72b6\u4f53\u7269\u8d28\u5728\u6676\u72b6\u4f53\u56ca\u819c\u4e0a\u589e\u6b96\u5f15\u8d77\u3002\u56ca\u819c\u6e05\u6d01\u662f\u4e00\u79cd\u6f5c\u5728\u6cbb\u7597\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u589e\u5f3a\u5bf9\u6574\u4e2a\u56ca\u819c\u7684\u53ef\u89c6\u5316\u548c\u5bf9\u8584\u819c\u7684\u5de5\u5177\u64cd\u4f5c\u7cbe\u5ea6\u3002

Method: \u8be5\u7cfb\u7edf\u5c06\u6807\u51c6\u77b3\u5b54\u900f\u5c04OCT\u548c\u773c\u5185OCT\u63a2\u5934\u96c6\u6210\u5230\u624b\u672f\u5668\u68b0\u4e0a\uff0c\u7528\u4e8e\u8d64\u9053\u56ca\u819c\u7684\u53ef\u89c6\u5316\u548c\u5b9e\u65f6\u5de5\u5177-\u7ec4\u7ec7\u8ddd\u79bb\u53cd\u9988\u3002\u5229\u7528\u673a\u5668\u4eba\u7cbe\u5ea6\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5bf9\u77b3\u5b54\u548c\u8d64\u9053\u533a\u57df\u8fdb\u884c\u5b8c\u6574\u7684\u56ca\u819c\u6620\u5c04\uff0c\u5e76\u8fdb\u884c\u6298\u5c04\u7387\u548c\u5149\u7ea4\u504f\u79fb\u7684\u539f\u4f4d\u6821\u51c6\u3002

Result: \u56ca\u819c\u6620\u5c04\u7b56\u7565\u901a\u8fc7\u5728\u773c\u90e8\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u4e94\u6b21\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u6784\u5efa\u7684\u56ca\u819c\u6a21\u578b\u5747\u65b9\u6839\u8bef\u5dee\u964d\u4f4e\uff1b\u56ca\u819c\u6e05\u6d01\u7b56\u7565\u5728\u4e09\u53ea\u79bb\u4f53\u732a\u773c\u4e0a\u8fdb\u884c\uff0c\u672a\u9020\u6210\u7ec4\u7ec7\u635f\u4f24\u3002

Conclusion: \u6240\u5f00\u53d1\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u56ca\u819c\u6620\u5c04\u548c\u6e05\u6d01\uff0c\u6709\u671b\u6539\u5584\u7ee7\u53d1\u6027\u767d\u5185\u969c\u7684\u6cbb\u7597\uff0c\u901a\u8fc7\u589e\u5f3a\u53ef\u89c6\u5316\u548c\u64cd\u4f5c\u7cbe\u5ea6\u6765\u89e3\u51b3\u5f53\u524d\u6311\u6218\u3002

Abstract: Secondary cataract is one of the most common complications of vision loss due
to the proliferation of residual lens materials that naturally grow on the lens
capsule after cataract surgery. A potential treatment is capsule cleaning, a
surgical procedure that requires enhanced visualization of the entire capsule
and tool manipulation on the thin membrane. This article presents a robotic
system capable of performing the capsule cleaning procedure by integrating a
standard transpupillary and an intraocular optical coherence tomography probe
on a surgical instrument for equatorial capsule visualization and real-time
tool-to-tissue distance feedback. Using robot precision, the developed system
enables complete capsule mapping in the pupillary and equatorial regions with
in-situ calibration of refractive index and fiber offset, which are still
current challenges in obtaining an accurate capsule model. To demonstrate
effectiveness, the capsule mapping strategy was validated through five
experimental trials on an eye phantom that showed reduced root-mean-square
errors in the constructed capsule model, while the cleaning strategy was
performed in three ex-vivo pig eyes without tissue damage.

</details>


### [153] [A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment](https://arxiv.org/abs/2507.13654)
*Haoran Wang,Yasamin Foroutani,Matthew Nepo,Mercedes Rodriguez,Ji Ma,Jean-Pierre Hubschman,Tsu-Chin Tsao,Jacob Rosen*

Main category: cs.RO

TL;DR: \u8be5\u7814\u7a76\u5728\u6a21\u62df\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u73af\u5883\u4e2d\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u7f29\u653e\u56e0\u5b50\u4e0b\u201c\u5185\u90e8\u63a7\u5236\u201d\u548c\u201c\u5916\u90e8\u63a7\u5236\u201d\u6a21\u5f0f\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u201c\u5185\u90e8\u63a7\u5236\u201d\u5728\u9ad8\u7f29\u653e\u56e0\u5b50\u4e0b\u8868\u73b0\u6700\u4f73\u3002


<details>
  <summary>Details</summary>
Motivation: \u65e8\u5728\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\u548c\u7f29\u653e\u56e0\u5b50\uff0c\u63d0\u9ad8\u672a\u6765\u673a\u5668\u4eba\u8f85\u52a9\u773c\u5185\u624b\u672f\u7684\u6548\u7387\u3001\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u98ce\u9669\u3002

Method: \u4f7f\u7528IRISS\u8fdc\u7a0b\u624b\u672f\u7cfb\u7edf\u63a7\u5236\u53f0\uff0c\u7ed3\u5408VR\u5934\u663e\u6a21\u62df\u663e\u5fae\u955c\u89c6\u56fe\u8fdb\u884c\u773c\u5185\u624b\u672f\u6a21\u62df\u3002\u5b9e\u9a8c\u5bf9\u8c61\u5305\u62ec5\u540d\u8d44\u6df1\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u5916\u79d1\u533b\u751f\u548c5\u540d\u65e0\u624b\u672f\u7ecf\u9a8c\u7684\u5de5\u7a0b\u5e08\uff0c\u6267\u884c\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u5e38\u89c1\u4efb\u52a1\uff0c\u6bd4\u8f83\u201c\u5185\u90e8\u63a7\u5236\u201d\u548c\u201c\u5916\u90e8\u63a7\u5236\u201d\u6a21\u5f0f\u5728\u4e0d\u540c\u7f29\u653e\u56e0\u5b50\uff08\u598220\u621630\uff09\u4e0b\u7684\u8868\u73b0\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u201c\u5185\u90e8\u63a7\u5236\u201d\u65b9\u6cd5\u5728\u8f83\u9ad8\u7f29\u653e\u56e0\u5b50\uff0820\u621630\uff09\u4e0b\u603b\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6700\u4f73\u7f29\u653e\u56e0\u5b50\u53ef\u80fd\u56e0\u4efb\u52a1\u548c\u590d\u6742\u6027\u800c\u5f02\u3002

Conclusion: \u4f18\u5316\u63a7\u5236\u65b9\u6cd5\u548c\u7f29\u653e\u56e0\u5b50\u6709\u671b\u663e\u8457\u63d0\u9ad8\u672a\u6765\u673a\u5668\u4eba\u8f85\u52a9\u773c\u5185\u624b\u672f\u7684\u6548\u7387\u3001\u51c6\u786e\u6027\uff0c\u5e76\u6700\u5927\u9650\u5ea6\u5730\u964d\u4f4e\u98ce\u9669\u3002

Abstract: This paper examines the performance of Inside and Outside Control modes at
various scaling factors in a simulated vitreoretinal surgical setting. The
IRISS teleoperated surgical system's console (cockpit) was adapted to project a
simulated microscope view of an intraocular setup to a virtual reality (VR)
headset. Five experienced vitreoretinal surgeons and five engineers with no
surgical experience used the system to perform tasks common to vitreoretinal
surgery. Experimental results indicate that Inside Control methods at higher
scaling factors (20 or 30) achieved the best performance overall, though the
optimal scaling factor may vary by task and complexity. Optimizing control
methods and scaling factors could lead to improvements in surgical efficiency
and accuracy, as well as minimize risks in future robotic-assisted intraocular
procedures.

</details>


### [154] [Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion](https://arxiv.org/abs/2507.13662)
*Jing Cheng,Yasser G. Alqaham,Zhenyu Gan,Amit K. Sanyal*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u81ea\u9002\u5e94\u817f\u5f0f\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u4e0e\u53d7\u751f\u7269\u542f\u53d1\u626d\u77e9\u5e93\uff08TL\uff09\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u51cf\u5c11\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u9760\u7684\u6b65\u6001\u6267\u884c\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f53\u524d\u673a\u5668\u4eba\u6b65\u6001\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u5728\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u3002\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u4e0d\u540c\u8fd0\u52a8\u573a\u666f\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002

Method: \u8be5\u65b9\u6cd5\u5c06\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u4e0e\u7c7b\u808c\u8089\u8bb0\u5fc6\u7684\u626d\u77e9\u5e93\uff08TL\uff09\u76f8\u7ed3\u5408\u3002\u5b83\u5229\u7528\u5468\u671f\u6027\u6b65\u6001\u7684\u91cd\u590d\u7279\u6027\uff0c\u5e76\u5c06ILC\u6269\u5c55\u5230\u975e\u5468\u671f\u6027\u4efb\u52a1\u3002\u63a7\u5236\u67b6\u6784\u662f\u6570\u636e\u9a71\u52a8\u7684\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u6df7\u5408\u7cfb\u7edf\u8f68\u8ff9\u4f18\u5316\u7684\u7269\u7406\u6a21\u578b\u548c\u5b9e\u65f6\u5b66\u4e60\uff0c\u4ee5\u8865\u507f\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u90e8\u5e72\u6270\u3002\u6838\u5fc3\u8d21\u732e\u662f\u5f00\u53d1\u4e86\u4e00\u4e2a\u901a\u7528\u626d\u77e9\u5e93\uff0c\u7528\u4e8e\u5b58\u50a8\u5b66\u4e60\u5230\u7684\u63a7\u5236\u914d\u7f6e\u6587\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u901f\u5ea6\u3001\u5730\u5f62\u548c\u91cd\u529b\u6761\u4ef6\u7684\u5feb\u901f\u9002\u5e94\u3002\u8be5\u65b9\u6cd5\u5728\u53cc\u8db3\u673a\u5668\u4ebaCassie\u548c\u56db\u8db3\u673a\u5668\u4ebaA1\u4e0a\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u5728\u51e0\u79d2\u5185\u5c06\u5173\u8282\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe85%\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u6267\u884c\u5468\u671f\u6027\u548c\u975e\u5468\u671f\u6027\u6b65\u6001\uff0c\u5305\u62ec\u659c\u5761\u7a7f\u8d8a\u548c\u5730\u5f62\u9002\u5e94\u3002\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5168\u8eab\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u5b66\u4e60\u5230\u7684\u6280\u80fd\u6d88\u9664\u4e86\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u5728\u7ebf\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u5c06\u63a7\u5236\u66f4\u65b0\u901f\u7387\u63d0\u9ad8\u4e8630\u500d\u4ee5\u4e0a\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06ILC\u4e0e\u626d\u77e9\u8bb0\u5fc6\u96c6\u6210\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u6570\u636e\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u817f\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6709\u6548\u6027\u3002

Abstract: This paper presents a scalable and adaptive control framework for legged
robots that integrates Iterative Learning Control (ILC) with a biologically
inspired torque library (TL), analogous to muscle memory. The proposed method
addresses key challenges in robotic locomotion, including accurate trajectory
tracking under unmodeled dynamics and external disturbances. By leveraging the
repetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the
framework enhances accuracy and generalization across diverse locomotion
scenarios. The control architecture is data-enabled, combining a physics-based
model derived from hybrid-system trajectory optimization with real-time
learning to compensate for model uncertainties and external disturbances. A
central contribution is the development of a generalized TL that stores learned
control profiles and enables rapid adaptation to changes in speed, terrain, and
gravitational conditions-eliminating the need for repeated learning and
significantly reducing online computation. The approach is validated on the
bipedal robot Cassie and the quadrupedal robot A1 through extensive simulations
and hardware experiments. Results demonstrate that the proposed framework
reduces joint tracking errors by up to 85% within a few seconds and enables
reliable execution of both periodic and nonperiodic gaits, including slope
traversal and terrain adaptation. Compared to state-of-the-art whole-body
controllers, the learned skills eliminate the need for online computation
during execution and achieve control update rates exceeding 30x those of
existing methods. These findings highlight the effectiveness of integrating ILC
with torque memory as a highly data-efficient and practical solution for legged
locomotion in unstructured and dynamic environments.

</details>


### [155] [SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization](https://arxiv.org/abs/2507.13702)
*Junho Choi,Kihwan Ryoo,Jeewon Kim,Taeyun Kim,Eungchang Lee,Myeongwoo Jeong,Kevin Christiansen Marsim,Hyungtae Lim,Hyun Myung*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaWa-ML\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9-\u60ef\u6027-UWB\u4f20\u611f\u5668\u6570\u636e\uff0c\u901a\u8fc7\u57fa\u4e8eUWB\u7684\u51e0\u4f55\u7ed3\u6784\u611f\u77e5\u4f4d\u59ff\u6821\u6b63\u548c\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u6709\u6548\u51cf\u5c11\u957f\u671f\u6f02\u79fb\u5e76\u63d0\u9ad8\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u5355\u4e2a\u673a\u5668\u4eba\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u95f4\u8ddd\u79bb\u6d4b\u91cf\u7684\u7279\u6027\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\uff0c\u5bfc\u81f4\u957f\u671f\u6f02\u79fb\u8bef\u5dee\u7d2f\u79ef\u4e0d\u53ef\u907f\u514d\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86SaWa-ML\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u57fa\u4e8e\u89c6\u89c9-\u60ef\u6027-UWB\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u4ee5\u4e0b\u4e24\u70b9\u5b9e\u73b0\uff1a1) \u5229\u7528UWB\u4f20\u611f\u5668\u6570\u636e\uff08\u5176\u6d4b\u8ddd\u8bef\u5dee\u4e0d\u968f\u65f6\u95f4\u7d2f\u79ef\uff09\u9996\u5148\u4f30\u8ba1\u673a\u5668\u4eba\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u7136\u540e\u6821\u6b63\u6bcf\u4e2a\u673a\u5668\u4eba\u7684\u4f4d\u7f6e\uff0c\u4ece\u800c\u51cf\u5c11\u957f\u671f\u6f02\u79fb\u8bef\u5dee\u30022) \u6839\u636e\u4f20\u611f\u5668\u6570\u636e\u7279\u6027\u548c\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u8bbe\u8ba1\u81ea\u9002\u5e94\u6743\u91cd\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u4f4d\u59ff\u6821\u6b63\u3002

Result: \u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002

Conclusion: SaWa-ML\u901a\u8fc7\u7ed3\u5408UWB\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u51e0\u4f55\u7ed3\u6784\u611f\u77e5\u4f4d\u59ff\u6821\u6b63\u548c\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u4e2d\u7684\u957f\u671f\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u9ad8\u7cbe\u5ea6\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u3002

Abstract: Multi-robot localization is a crucial task for implementing multi-robot
systems. Numerous researchers have proposed optimization-based multi-robot
localization methods that use camera, IMU, and UWB sensors. Nevertheless,
characteristics of individual robot odometry estimates and distance
measurements between robots used in the optimization are not sufficiently
considered. In addition, previous researches were heavily influenced by the
odometry accuracy that is estimated from individual robots. Consequently,
long-term drift error caused by error accumulation is potentially inevitable.
In this paper, we propose a novel visual-inertial-range-based multi-robot
localization method, named SaWa-ML, which enables geometric structure-aware
pose correction and weight adaptation-based robust multi-robot localization.
Our contributions are twofold: (i) we leverage UWB sensor data, whose range
error does not accumulate over time, to first estimate the relative positions
between robots and then correct the positions of each robot, thus reducing
long-term drift errors, (ii) we design adaptive weights for robot pose
correction by considering the characteristics of the sensor data and
visual-inertial odometry estimates. The proposed method has been validated in
real-world experiments, showing a substantial performance increase compared
with state-of-the-art algorithms.

</details>


### [156] [AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework](https://arxiv.org/abs/2507.13729)
*Yu Yao,Salil Bhatnagar,Markus Mazzola,Vasileios Belagiannis,Igor Gilitschenski,Luigi Palmieri,Simon Razniewski,Marcel Hallgarten*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6765\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u573a\u666f\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u6d4b\u8bd5\u4e2d\u7a00\u6709\u5173\u952e\u573a\u666f\u751f\u6210\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u5728\u6d4b\u8bd5\u548c\u8bc4\u4f30\u7a00\u6709\u5173\u952e\u573a\u666f\u65f6\u9762\u4e34\u6311\u6218\u3002\u7eaf\u7cb9\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff1b\u6570\u636e\u9a71\u52a8\u6a21\u578b\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u7f3a\u4e4f\u7cbe\u7ec6\u63a7\u5236\uff0c\u4ece\u96f6\u5f00\u59cb\u751f\u6210\u65b0\u573a\u666f\u53ef\u80fd\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff1b\u73b0\u6709\u901a\u8fc7\u589e\u5f3a\u539f\u59cb\u573a\u666f\u7684\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u64cd\u4f5c\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002

Method: \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6765\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u573a\u666f\u3002\u5173\u952e\u521b\u65b0\u5728\u4e8e\u91c7\u7528\u4ee3\u7406\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8f93\u51fa\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u5e76\u80fd\u5728\u4f7f\u7528\u8f83\u5c0f\u3001\u7ecf\u6d4e\u578bLLM\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002

Result: \u901a\u8fc7\u5e7f\u6cdb\u7684\u4eba\u5de5\u4e13\u5bb6\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u9075\u5faa\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u589e\u5f3a\u573a\u666f\uff0c\u5176\u8d28\u91cf\u53ef\u4e0e\u624b\u52a8\u521b\u5efa\u7684\u573a\u666f\u76f8\u5ab2\u7f8e\u3002

Conclusion: \u8be5LLM\u4ee3\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u3001\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7a00\u6709\u4ea4\u901a\u573a\u666f\uff0c\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u7684\u9700\u6c42\u3002

Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.

</details>


### [157] [Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery](https://arxiv.org/abs/2507.13787)
*Doina Pisla,Alexandru Pusca,Andrei Caprariu,Adrian Pisla,Bogdan Gherman,Calin Vaida,Damien Chablat*

Main category: cs.RO

TL;DR: \u672c\u6587\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u80f0\u817a\u624b\u672f\u76844\u81ea\u7531\u5ea6\u5e76\u8054\u673a\u5668\u4eba\u67b6\u6784\uff08ATHENA-1\u548cATHENA-2\uff09\uff0c\u901a\u8fc7\u8fd0\u52a8\u5b66\u3001\u6709\u9650\u5143\u4eff\u771f\uff08\u521a\u5ea6\uff09\u548c\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\uff0c\u4ee5\u9009\u51fa\u6700\u7b26\u5408\u8bbe\u8ba1\u8981\u6c42\u7684\u67b6\u6784\u3002


<details>
  <summary>Details</summary>
Motivation: \u5f00\u53d1\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u80f0\u817a\u624b\u672f\u7684\u5e76\u8054\u673a\u5668\u4eba\u3002

Method: \u63d0\u51fa\u4e86\u4e24\u79cd4\u81ea\u7531\u5ea6\u5e76\u8054\u673a\u5668\u4eba\u67b6\u6784ATHENA-1\u548cATHENA-2\uff1b\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u8fd0\u52a8\u5b66\u65b9\u6848\u548c\u6982\u5ff5\u60273D CAD\u6a21\u578b\uff1b\u8fdb\u884c\u4e86\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u4eff\u771f\u4ee5\u8bc4\u4f30\u521a\u5ea6\uff1b\u8fdb\u884c\u4e86\u5de5\u4f5c\u7a7a\u95f4\u5b9a\u91cf\u5206\u6790\u4ee5\u8bc4\u4f30\u533b\u7597\u4efb\u52a1\u7684\u53ef\u7528\u6027\u3002

Result: \u901a\u8fc7FEM\u4eff\u771f\u548c\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u4e24\u79cd\u67b6\u6784\u7684\u521a\u5ea6\u548c\u53ef\u7528\u6027\uff1b\u83b7\u5f97\u4e86\u7528\u4e8e\u9009\u62e9\u7b26\u5408\u8bbe\u8ba1\u6807\u51c6\u7684\u67b6\u6784\u7684\u7ed3\u679c\u3002

Conclusion: \u6839\u636e\u521a\u5ea6\u548c\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\u7ed3\u679c\uff0c\u9009\u62e9\u4e86\u4e00\u79cd\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06\u88ab\u7528\u4e8e\u5f00\u53d1\u624b\u672f\u673a\u5668\u4eba\u7684\u5b9e\u9a8c\u6a21\u578b\u3002

Abstract: This paper focuses on the design of a parallel robot designed for robotic
assisted minimally invasive pancreatic surgery. Two alternative architectures,
called ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are
proposed. Their kinematic schemes are presented, and the conceptual 3D CAD
models are illustrated. Based on these, two Finite Element Method (FEM)
simulations were performed to determine which architecture has the higher
stiffness. A workspace quantitative analysis is performed to further assess the
usability of the two proposed parallel architectures related to the medical
tasks. The obtained results are used to select the architecture which fit the
required design criteria and will be used to develop the experimental model of
the surgical robot.

</details>


### [158] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u63a7\u5236\u969c\u788d\u8bc1\u4e66\uff08CBCs\uff09\u5b66\u4e60\u5b89\u5168\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u5b89\u5168\u5173\u952e\u6570\u636e\u6807\u6ce8\u7684\u9700\u6c42\u3002


<details>
  <summary>Details</summary>
Motivation: \u4ece\u89c6\u89c9\u6570\u636e\u5408\u6210\u5b89\u5168\u63a7\u5236\u5668\u901a\u5e38\u9700\u8981\u5927\u91cf\u76d1\u7763\u6807\u6ce8\u5b89\u5168\u5173\u952e\u6570\u636e\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002

Method: \u8be5\u65b9\u6cd5\u5f15\u5165\u4e00\u4e2a\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u5728\u4e16\u754c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u5230\u7684\u63a7\u5236\u969c\u788d\u8bc1\u4e66\uff08CBCs\uff09\u6765\u5408\u6210\u5b89\u5168\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002\u5b83\u4f7f\u7528\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u8054\u5408\u5b66\u4e60\u4e00\u4e2a\u795e\u7ecf\u969c\u788d\u51fd\u6570\u548c\u4e00\u4e2a\u5b89\u5168\u63a7\u5236\u5668\uff0c\u5e76\u5229\u7528\u73b0\u4ee3\u89c6\u89c9Transformer\u7684\u9884\u6d4b\u80fd\u529b\u8fdb\u884c\u6f5c\u5728\u52a8\u529b\u5b66\u5efa\u6a21\u3002

Result: \u8be5\u6846\u67b6\u80fd\u591f\u5229\u7528\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\uff0c\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5408\u6210\u5b89\u5168\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002

Conclusion: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u89c6\u89c9\u6570\u636e\u5b89\u5168\u63a7\u5236\u5668\u5408\u6210\u4e2d\u6570\u636e\u6807\u6ce8\u6311\u6218\u7684\u65b0\u9896\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e16\u754c\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u63a7\u5236\u969c\u788d\u8bc1\u4e66\u5b9e\u73b0\u3002

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


### [159] [AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery](https://arxiv.org/abs/2507.13903)
*Ziliang Li,Hongming Chen,Yiyang Lin,Biyu Ye,Ximin Lyu*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a7a\u4e2d\u673a\u68b0\u81c2\uff08AM\uff09\u7684\u81ea\u4e3b\u7a7a\u6295\u7cfb\u7edf\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u548c\u5206\u5c42\u6270\u52a8\u8865\u507f\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u73af\u5883\u4e2d\u7a7a\u6295\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u654f\u6377\u6027\uff0c\u6709\u6548\u5e94\u5bf9\u63a7\u5236\u6a21\u5f0f\u5207\u6362\u548c\u7cfb\u7edf\u5ef6\u8fdf\u95ee\u9898\u3002


<details>
  <summary>Details</summary>
Motivation: \u81ea\u4e3b\u822a\u7a7a\u7cfb\u7edf\u5728\u8fd0\u8f93\u548c\u9012\u9001\u4efb\u52a1\u4e2d\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5728\u7a7a\u6295\u4efb\u52a1\u4e2d\u9762\u4e34\u7a81\u7136\u63a7\u5236\u6a21\u5f0f\u5207\u6362\u3001\u56fa\u6709\u7cfb\u7edf\u5ef6\u8fdf\u548c\u63a7\u5236\u8bef\u5dee\u7684\u53cc\u91cd\u6311\u6218\u3002

Method: \u5f15\u5165\u7a7a\u4e2d\u673a\u68b0\u81c2\uff08AM\uff09\u589e\u52a0\u81ea\u7531\u5ea6\uff0c\u4e3b\u52a8\u8865\u507f\u65e0\u4eba\u673a\u8ddf\u8e2a\u8bef\u5dee\uff1b\u5bf9\u629b\u7269\u7ebf\u7740\u9646\u70b9\u65bd\u52a0\u5e73\u6ed1\u8fde\u7eed\u7ea6\u675f\uff0c\u751f\u6210\u5bf9\u6709\u6548\u8f7d\u8377\u91ca\u653e\u65f6\u673a\u4e0d\u654f\u611f\u7684\u7a7a\u6295\u8f68\u8ff9\uff1b\u5c06\u5206\u5c42\u6270\u52a8\u8865\u507f\u7b56\u7565\u6574\u5408\u5230\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u6846\u67b6\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u7cfb\u7edf\u53c2\u6570\u7a81\u53d8\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528NMPC\u7684\u9884\u6d4b\u80fd\u529b\u63d0\u9ad8\u7a7a\u6295\u7cbe\u5ea6\u3002

Result: \u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u7ed3\u679c\u5747\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u7a7a\u6295\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u654f\u6377\u6027\u548c\u7cbe\u5ea6\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7a7a\u4e2d\u673a\u68b0\u81c2\u548cNMPC\u7684\u81ea\u4e3b\u7a7a\u6295\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u6295\u4efb\u52a1\u4e2d\u7684\u63a7\u5236\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u6295\u7684\u654f\u6377\u6027\u548c\u7cbe\u5ea6\u3002

Abstract: Autonomous aerial systems play an increasingly vital role in a wide range of
applications, particularly for transport and delivery tasks in complex
environments. In airdrop missions, these platforms face the dual challenges of
abrupt control mode switching and inherent system delays along with control
errors. To address these issues, this paper presents an autonomous airdrop
system based on an aerial manipulator (AM). The introduction of additional
actuated degrees of freedom enables active compensation for UAV tracking
errors. By imposing smooth and continuous constraints on the parabolic landing
point, the proposed approach generates aerial throwing trajectories that are
less sensitive to the timing of payload release. A hierarchical disturbance
compensation strategy is incorporated into the Nonlinear Model Predictive
Control (NMPC) framework to mitigate the effects of sudden changes in system
parameters, while the predictive capabilities of NMPC are further exploited to
improve the precision of aerial throwing. Both simulation and real-world
experimental results demonstrate that the proposed system achieves greater
agility and precision in airdrop missions.

</details>


### [160] [NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning](https://arxiv.org/abs/2507.13940)
*Qingyi Chen,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6c49\u5bc6\u5c14\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5b66\u4e60\uff08Neural HJR\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\uff08MAMP\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u6311\u6218\uff0c\u5e76\u80fd\u5904\u7406\u9ad8\u7ef4\u590d\u6742\u78b0\u649e\u7ea6\u675f\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\uff08MAMP\uff09\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\u56f0\u5883\uff1a\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u4f9d\u8d56\u9884\u6d4b\u3001\u5951\u7ea6\u6216\u901a\u4fe1\u6765\u786e\u4fdd\u5b89\u5168\uff0c\u800c\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u5219\u96be\u4ee5\u6269\u5c55\u548c\u8fdb\u884c\u5b9e\u65f6\u51b3\u7b56\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002

Method: \u672c\u6587\u5f15\u5165\u4e86\u795e\u7ecf\u6c49\u5bc6\u5c14\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5b66\u4e60\uff08Neural HJR\uff09\u6765\u5904\u7406\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u5e76\u6355\u6349\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u78b0\u649e\u548c\u5b89\u5168\u7ea6\u675f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5b66\u4e60\u5230\u7684HJR\u89e3\u6765\u5b9e\u65f6\u89e3\u51b3MAMP\u4efb\u52a1\u3002

Result: \u8be5\u65b9\u6cd5\u88ab\u8bc1\u660e\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u80fd\u591f\u89e3\u51b3\u5177\u6709\u590d\u6742\u78b0\u649e\u7ea6\u675f\u7684\u9ad8\u7ef4MAMP\u95ee\u9898\u3002\u5b83\u80fd\u6cdb\u5316\u5230\u5404\u79cd\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5305\u62ec\u4e00\u4e2a12\u7ef4\u7684\u53cc\u81c2\u8bbe\u7f6e\uff0c\u5e76\u4e14\u5728\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684MAMP\u4efb\u52a1\u65b9\u9762\u4f18\u4e8e\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u57fa\u4e8eNeural HJR\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MAMP\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5728\u590d\u6742\u9ad8\u7ef4\u573a\u666f\u4e0b\u5b89\u5168\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002

Abstract: Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in
robotics. Despite substantial advancements, existing methods often face a
dilemma. Decentralized algorithms typically rely on predicting the behavior of
other agents, sharing contracts, or maintaining communication for safety, while
centralized approaches struggle with scalability and real-time decision-making.
To address these challenges, we introduce Neural Hamilton-Jacobi Reachability
Learning (HJR) for Decentralized Multi-Agent Motion Planning. Our method
provides scalable neural HJR modeling to tackle high-dimensional configuration
spaces and capture worst-case collision and safety constraints between agents.
We further propose a decentralized trajectory optimization framework that
incorporates the learned HJR solutions to solve MAMP tasks in real-time. We
demonstrate that our method is both scalable and data-efficient, enabling the
solution of MAMP problems in higher-dimensional scenarios with complex
collision constraints. Our approach generalizes across various dynamical
systems, including a 12-dimensional dual-arm setup, and outperforms a range of
state-of-the-art techniques in successfully addressing challenging MAMP tasks.
Video demonstrations are available at https://youtu.be/IZiePX0p1Mc.

</details>


### [161] [A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios](https://arxiv.org/abs/2507.13969)
*Maria Eduarda Silva de Macedo,Ana Paula Chiarelli de Souza,Roberto Silvio Ubertino Rosso Jr.,Yuri Kaszubowski Lopes*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u81ea\u805a\u5408\u65b9\u6cd5\uff0c\u4f7f\u540c\u8d28\u673a\u5668\u4eba\u7fa4\u80fd\u591f\u4ec5\u4f9d\u9760\u89c6\u7ebf\u4f20\u611f\u5668\u5f62\u6210\u7d27\u51d1\u7684\u96c6\u7fa4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5f62\u6210\u5706\u5f62\u6216\u975e\u7d27\u51d1\u961f\u5f62\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u73b0\u4e86\u826f\u597d\u7684\u53ef\u4f38\u7f29\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u673a\u5668\u4eba\u7fa4\u591a\u4efb\u52a1\u81ea\u805a\u5408\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u975e\u7d27\u51d1\u5706\u5f62\u961f\u5f62\u6216\u975e\u5b8c\u5168\u81ea\u4e3b\u7684\u95ee\u9898\uff0c\u4e14\u4e0d\u540c\u7ec4\u4e4b\u95f4\u7684\u52a8\u6001\u76f8\u4e92\u5f71\u54cd\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5bfb\u6c42\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002

Method: \u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a1\u81ea\u805a\u5408\u884c\u4e3a\uff0c\u4f7f\u540c\u8d28\u673a\u5668\u4eba\u7fa4\u80fd\u591f\u5c06\u81ea\u8eab\u5206\u7c7b\u5e76\u5f62\u6210\u4e0d\u540c\u7684\u7d27\u51d1\u96c6\u7fa4\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8e\u673a\u5668\u4eba\u7684\u89c6\u7ebf\u4f20\u611f\u5668\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u6a21\u62df\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u7fa4\u7ec4\u6570\u91cf\u548c\u6bcf\u7ec4\u673a\u5668\u4eba\u6570\u91cf\u914d\u7f6e\u4e0b\u7684\u53ef\u4f38\u7f29\u6027\u3002

Result: \u6240\u63d0\u51fa\u7684\u591a\u4efb\u52a1\u81ea\u805a\u5408\u884c\u4e3a\u5728\u53ef\u4f38\u7f29\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u80fd\u591f\u5b9e\u73b0\u7d27\u51d1\u7684\u961f\u5f62\u3002\u4e0e\u73b0\u6709\u7814\u7a76\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u96c6\u7fa4\u7684\u7d27\u51d1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5176\u4ed6\u7814\u7a76\u4e2d\u76f8\u4f3c\u7684\u96c6\u7fa4\u673a\u5668\u4eba\u6bd4\u4f8b\u3002

Conclusion: \u8be5\u7814\u7a76\u6210\u529f\u5730\u6539\u8fdb\u4e86\u673a\u5668\u4eba\u7fa4\u7684\u591a\u4efb\u52a1\u81ea\u805a\u5408\u884c\u4e3a\uff0c\u4f7f\u5176\u80fd\u591f\u81ea\u4e3b\u5730\u5f62\u6210\u7d27\u51d1\u4e14\u53ef\u4f38\u7f29\u7684\u96c6\u7fa4\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u65b9\u6cd5\u5728\u961f\u5f62\u7d27\u51d1\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002

Abstract: The deployment of simple emergent behaviors in swarm robotics has been
well-rehearsed in the literature. A recent study has shown how self-aggregation
is possible in a multitask approach -- where multiple self-aggregation task
instances occur concurrently in the same environment. The multitask approach
poses new challenges, in special, how the dynamic of each group impacts the
performance of others. So far, the multitask self-aggregation of groups of
robots suffers from generating a circular formation -- that is not fully
compact -- or is not fully autonomous. In this paper, we present a multitask
self-aggregation where groups of homogeneous robots sort themselves into
different compact clusters, relying solely on a line-of-sight sensor. Our
multitask self-aggregation behavior was able to scale well and achieve a
compact formation. We report scalability results from a series of simulation
trials with different configurations in the number of groups and the number of
robots per group. We were able to improve the multitask self-aggregation
behavior performance in terms of the compactness of the clusters, keeping the
proportion of clustered robots found in other studies.

</details>


### [162] [A segmented robot grasping perception neural network for edge AI](https://arxiv.org/abs/2507.13970)
*Casper Bröcheler,Thomas Vroom,Derrick Timmermans,Alan van den Akker,Guangzhi Tang,Charalampos S. Kouzinopoulos,Rico Möckel*

Main category: cs.RO

TL;DR: \u672c\u6587\u5728RISC-V SoC\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u6293\u53d6\u68c0\u6d4b\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u529f\u8017\u5fae\u63a7\u5236\u5668\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u7684\u6f5c\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u590d\u6742\uff0c\u9700\u8981\u7cbe\u786e\u7684\u611f\u77e5\u548c\u63a7\u5236\u3002\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6293\u53d6\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u9700\u8981\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u7684\u5b9e\u65f6\u63a8\u7406\u3002

Method: \u672c\u6587\u5728GAP9 RISC-V\u7247\u4e0a\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u201c\u70ed\u56fe\u5f15\u5bfc\u6293\u53d6\u68c0\u6d4b\u201d\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b6\u81ea\u7531\u5ea6\u6293\u53d6\u59ff\u6001\u3002\u901a\u8fc7\u8f93\u5165\u7ef4\u5ea6\u7f29\u51cf\u3001\u6a21\u578b\u5206\u533a\u548c\u91cf\u5316\u7b49\u786c\u4ef6\u611f\u77e5\u6280\u672f\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u4f18\u5316\u3002

Result: \u5728GraspNet-1Billion\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5b8c\u5168\u7247\u4e0a\u63a8\u7406\u7684\u53ef\u884c\u6027\u3002

Conclusion: \u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u4f4e\u529f\u8017\u5fae\u63a7\u5236\u5668\u5728\u5b9e\u65f6\u3001\u81ea\u4e3b\u64cd\u7eb5\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002

Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.

</details>


### [163] [A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems](https://arxiv.org/abs/2507.14043)
*Genliang Li,Yaxin Cui,Jinyu Su*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7b56\u7565\u6539\u8fdb\u7684\u86c7\u7fa4\u4f18\u5316\u5668\uff08MISO\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u86c7\u7fa4\u4f18\u5316\u5668\uff08SO\uff09\u6536\u655b\u6162\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u51fd\u6570\u6d4b\u8bd5\u548c\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\uff08\u5982\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\uff09\u9a8c\u8bc1\u4e86\u5176\u5353\u8d8a\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u86c7\u7fa4\u4f18\u5316\u5668\uff08SO\uff09\u4f5c\u4e3a\u4e00\u79cd\u6e10\u8fdb\u5f0f\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u5e94\u7528\u4e2d\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u548c\u5e94\u7528\u6f5c\u529b\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86MISO\u7b97\u6cd5\uff0c\u5305\u542b\u4e09\u9879\u4e3b\u8981\u6539\u8fdb\u7b56\u7565\uff1a1) \u57fa\u4e8e\u6b63\u5f26\u51fd\u6570\u7684\u81ea\u9002\u5e94\u968f\u673a\u6270\u52a8\u7b56\u7565\uff0c\u4ee5\u907f\u514d\u5c40\u90e8\u6700\u4f18\uff1b2) \u5f15\u5165\u57fa\u4e8e\u6bd4\u4f8b\u56e0\u5b50\u548c\u9886\u5bfc\u8005\u7684\u81ea\u9002\u5e94Levy\u98de\u884c\u7b56\u7565\uff0c\u8d4b\u4e88\u96c4\u6027\u86c7\u9886\u5bfc\u8005\u98de\u884c\u80fd\u529b\uff0c\u4ee5\u8df3\u51fa\u5c40\u90e8\u6700\u4f18\uff1b3) \u7ed3\u5408\u7cbe\u82f1\u9886\u5bfc\u548c\u5e03\u6717\u8fd0\u52a8\u7684\u4f4d\u7f6e\u66f4\u65b0\u7b56\u7565\uff0c\u4ee5\u52a0\u901f\u6536\u655b\u5e76\u4fdd\u6301\u7cbe\u5ea6\u3002\u7b97\u6cd5\u901a\u8fc730\u4e2aCEC2017\u6d4b\u8bd5\u51fd\u6570\u3001CEC2022\u6d4b\u8bd5\u5957\u4ef6\u3001\u65e0\u4eba\u673a3D\u8def\u5f84\u89c4\u5212\u95ee\u9898\u548c6\u4e2a\u5de5\u7a0b\u8bbe\u8ba1\u95ee\u9898\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u4e0e11\u79cd\u6d41\u884c\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMISO\u5728\u6c42\u89e3\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u7ade\u4e89\u7b97\u6cd5\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002

Conclusion: MISO\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u86c7\u7fa4\u4f18\u5316\u5668\u7684\u7f3a\u9677\uff0c\u5728\u6d4b\u8bd5\u51fd\u6570\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5f3a\u5927\u7ade\u4e89\u529b\u3002

Abstract: Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.

</details>


### [164] [EdgeVLA: Efficient Vision-Language-Action Models](https://arxiv.org/abs/2507.14049)
*Paweł Budzianowski,Wesley Maa,Matthew Freed,Jingxiang Mo,Winston Hsiao,Aaron Xie,Tomasz Młoduchowski,Viraj Tipnis,Benjamin Bolte*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51faEdge VLA (EVLA)\uff0c\u4e00\u79cd\u65b0\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u81ea\u56de\u5f52\u9884\u6d4b\u548c\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u8868\u793a\u80fd\u529b\u3002


<details>
  <summary>Details</summary>
Motivation: \u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u673a\u5668\u4eba\u9886\u57df\u5c55\u73b0\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u5b9e\u73b0\u901a\u7528\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7684\u6f5c\u529b\uff0c\u4f46\u5c06\u8fd9\u4e9b\u6a21\u578b\u90e8\u7f72\u5230\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b58\u5728\u663e\u8457\u6311\u6218\u3002

Method: EVLA\u901a\u8fc7\u4e24\u9879\u5173\u952e\u521b\u65b0\u5b9e\u73b0\u76ee\u6807\uff1a1) \u6d88\u9664\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u9884\u6d4b\u7684\u81ea\u56de\u5f52\u8981\u6c42\uff0c\u4ece\u800c\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u9ad87\u500d\uff1b2) \u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u6548\u7387\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u8bad\u7ec3\u6027\u80fd\u3002

Result: \u65e9\u671f\u7ed3\u679c\u8868\u660e\uff0cEVLA\u5b9e\u73b0\u4e86\u4e0eOpenVLA\u76f8\u5f53\u7684\u8bad\u7ec3\u7279\u6027\uff0c\u540c\u65f6\u5728\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002

Conclusion: EVLA\u6210\u529f\u5730\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86VLA\u6a21\u578b\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u8868\u793a\u80fd\u529b\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u90e8\u7f72\u901a\u7528\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002

Abstract: Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.

</details>


### [165] [Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub](https://arxiv.org/abs/2507.14059)
*Tianyuan Wang,Mark A Post,Mathieu Deremetz*

Main category: cs.RO

TL;DR: \u672c\u6587\u4ecb\u7ecd\u4e86STARFAB\u9879\u76ee\u4e2d\u7684\u79fb\u52a8\u68c0\u67e5\u6a21\u5757\uff08MIM\uff09\uff0c\u4e00\u4e2a\u7528\u4e8e\u8f68\u9053\u81ea\u52a8\u5316\u4ed3\u5e93\u7684\u81ea\u4e3b\u68c0\u67e5\u548c\u7ef4\u62a4\u7cfb\u7edf\uff0c\u65e8\u5728\u652f\u6301\u592a\u7a7a\u786c\u4ef6\u7684\u7ec4\u88c5\u548c\u518d\u5229\u7528\u3002


<details>
  <summary>Details</summary>
Motivation: \u592a\u7a7a\u81ea\u4e3b\u673a\u5668\u4eba\u662f\u201c\u65b0\u592a\u7a7a\u201d\u5546\u4e1a\u751f\u6001\u7cfb\u7edf\uff08\u5305\u62ec\u8f68\u9053\u5185\u786c\u4ef6\u7ec4\u88c5\u548c\u518d\u5229\u7528\uff09\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002STARFAB\u9879\u76ee\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u5730\u9762\u6f14\u793a\u7684\u8f68\u9053\u81ea\u52a8\u5316\u4ed3\u5e93\uff0c\u800c\u5bf9\u5176\u5185\u90e8\u7ec4\u4ef6\u548c\u8bbe\u65bd\u8fdb\u884c\u76d1\u6d4b\u3001\u68c0\u67e5\u548c\u72b6\u6001\u8bc4\u4f30\u662f\u5176\u5b8c\u5168\u81ea\u4e3b\u8fd0\u884c\u7684\u5173\u952e\u9700\u6c42\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e86STARFAB\u79fb\u52a8\u68c0\u67e5\u6a21\u5757\uff08MIM\uff09\u7684\u6982\u5ff5\u548c\u8bbe\u8ba1\u3002MIM\u91c7\u7528\u6807\u51c6\u4e92\u8fde\uff0c\u53ef\u7531\u6b65\u884c\u673a\u68b0\u624b\u643a\u5e26\u4f5c\u4e3a\u72ec\u7acb\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u5e76\u80fd\u6839\u636e\u9700\u8981\u8fdb\u884c\u5b58\u50a8\u548c\u53d6\u56de\u3002\u5b83\u914d\u5907\u4e86\u9ad8\u5206\u8fa8\u7387\u76f8\u673a\u30013D\u8f6e\u5ed3\u4eea\u548c\u70ed\u6210\u50cf\u4f20\u611f\u5668\uff0c\u5e76\u652f\u6301\u5176\u4ed6\u6a21\u5757\u5316\u4f20\u611f\u5668\u3002MIM\u5185\u90e8\u8fd8\u5b58\u50a8\u4e86\u7528\u4e8e\u7ef4\u62a4\u64cd\u4f5c\u7684\u6293\u53d6\u5de5\u5177\u548c\u626d\u77e9\u6273\u624b\u3002\u6587\u7ae0\u8be6\u7ec6\u4ecb\u7ecd\u4e86MIM\u7684\u64cd\u4f5c\u6982\u5ff5\u3001\u673a\u68b0\u548c\u7535\u5b50\u8bbe\u8ba1\u4ee5\u53ca\u7528\u4e8e\u65e0\u635f\u68c0\u6d4b\u7684\u4f20\u611f\u5668\u5305\u3002

Result: \u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86STARFAB\u79fb\u52a8\u68c0\u67e5\u6a21\u5757\uff08MIM\uff09\u4f5c\u4e3a\u8f68\u9053\u81ea\u4e3b\u68c0\u67e5\u548c\u7ef4\u62a4\u7cfb\u7edf\u7684\u6982\u5ff5\u3001\u5176\u673a\u68b0\u548c\u7535\u5b50\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u6240\u4f7f\u7528\u7684\u4f20\u611f\u5668\u5957\u4ef6\u3002\u76ee\u524d\uff0c\u8be5\u7cfb\u7edf\u7684\u5b9e\u65bd\u548c\u6d4b\u8bd5\u4ecd\u5728\u8fdb\u884c\u4e2d\u3002

Conclusion: MIM\u662fSTARFAB\u9879\u76ee\u5b9e\u73b0\u8f68\u9053\u81ea\u52a8\u5316\u4ed3\u5e93\u81ea\u4e3b\u68c0\u67e5\u548c\u7ef4\u62a4\u80fd\u529b\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5bf9\u4e8e\u652f\u6301\u53ef\u6301\u7eed\u7684\u5546\u4e1a\u592a\u7a7a\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u529f\u80fd\u4f20\u611f\u5668/\u5de5\u5177\u5305\u4f7f\u5176\u80fd\u591f\u6709\u6548\u76d1\u6d4b\u548c\u8bc4\u4f30\u592a\u7a7a\u786c\u4ef6\u548c\u8bbe\u65bd\u7684\u72b6\u6001\u3002

Abstract: The use of autonomous robots in space is an essential part of the "New Space"
commercial ecosystem of assembly and re-use of space hardware components in
Earth orbit and beyond. The STARFAB project aims to create a ground
demonstration of an orbital automated warehouse as a hub for sustainable
commercial operations and servicing. A critical part of this fully-autonomous
robotic facility will be the capability to monitor, inspect, and assess the
condition of both the components stored in the warehouse, and the STARFAB
facility itself. This paper introduces ongoing work on the STARFAB Mobile
Inspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it
can be carried by Walking Manipulators (WM) as an independently-mobile robot,
and multiple MIMs can be stored and retrieved as needed for operations on
STARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a
thermal imaging sensor, with the capability to add other modular sensors. A
grasping tool and torque wrench are stored within the modular body for use by
an attached WM for maintenance operations. Implementation and testing is still
ongoing at the time of writing. This paper details the concept of operations
for the MIM as an on-orbit autonomous inspection and maintenance system, the
mechanical and electronic design of the MIM, and the sensors package used for
non-destructive testing.

</details>


### [166] [MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation](https://arxiv.org/abs/2507.14061)
*Nataliya Nechyporenko,Yutong Zhang,Sean Campbell,Alessandro Roncone*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMorphIt\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7403\u5f62\u57fa\u5143\u8fd1\u4f3c\u673a\u5668\u4eba\u5f62\u6001\uff0c\u4ee5\u5e73\u8861\u51e0\u4f55\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\u5176\u5f62\u6001\u8868\u793a\u3002


<details>
  <summary>Details</summary>
Motivation: \u76ee\u524d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5c06\u7269\u7406\u5f62\u6001\u89c6\u4e3a\u56fa\u5b9a\u7ea6\u675f\u800c\u975e\u9002\u5e94\u6027\u8d44\u6e90\uff0c\u5bfc\u81f4\u76f8\u540c\u7684\u521a\u6027\u51e0\u4f55\u8868\u793a\u5fc5\u987b\u670d\u52a1\u4e8e\u8ba1\u7b97\u548c\u7cbe\u5ea6\u8981\u6c42\u5dee\u5f02\u5de8\u5927\u7684\u5e94\u7528\uff0c\u6548\u7387\u4f4e\u4e0b\u3002

Method: MorphIt\u7b97\u6cd5\u91c7\u7528\u81ea\u52a8\u7684\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u53ef\u8c03\u53c2\u6570\u901a\u8fc7\u7403\u5f62\u57fa\u5143\u8fd1\u4f3c\u673a\u5668\u4eba\u5f62\u6001\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u663e\u5f0f\u63a7\u5236\u7269\u7406\u4fdd\u771f\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002

Result: \u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0cMorphIt\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u53d8\u5206\u7403\u96c6\u8fd1\u4f3c\u548c\u81ea\u9002\u5e94\u4e2d\u8f74\u8fd1\u4f3c\uff09\uff0c\u80fd\u7528\u66f4\u5c11\u7684\u7403\u4f53\u5b9e\u73b0\u66f4\u597d\u7684\u7f51\u683c\u8fd1\u4f3c\uff0c\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u78b0\u649e\u68c0\u6d4b\u7cbe\u5ea6\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u6a21\u62df\u4ee5\u53ca\u72ed\u7a84\u7a7a\u95f4\u5bfc\u822a\u65b9\u9762\u7684\u80fd\u529b\u3002

Conclusion: \u901a\u8fc7\u52a8\u6001\u8c03\u6574\u51e0\u4f55\u8868\u793a\u4ee5\u9002\u5e94\u4efb\u52a1\u9700\u6c42\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u5c06\u5176\u7269\u7406\u5b9e\u4f53\u4f5c\u4e3a\u4e00\u79cd\u4e3b\u52a8\u8d44\u6e90\u800c\u975e\u50f5\u5316\u53c2\u6570\u6765\u5229\u7528\u3002\u8fd9\u4e3a\u673a\u5668\u4eba\u5728\u9700\u8981\u6301\u7eed\u5e73\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u53ef\u884c\u6027\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u64cd\u4f5c\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002

Abstract: What if a robot could rethink its own morphological representation to better
meet the demands of diverse tasks? Most robotic systems today treat their
physical form as a fixed constraint rather than an adaptive resource, forcing
the same rigid geometric representation to serve applications with vastly
different computational and precision requirements. We introduce MorphIt, a
novel algorithm for approximating robot morphology using spherical primitives
that balances geometric accuracy with computational efficiency. Unlike existing
approaches that rely on either labor-intensive manual specification or
inflexible computational methods, MorphIt implements an automatic
gradient-based optimization framework with tunable parameters that provides
explicit control over the physical fidelity versus computational cost tradeoff.
Quantitative evaluations demonstrate that MorphIt outperforms baseline
approaches (Variational Sphere Set Approximation and Adaptive Medial-Axis
Approximation) across multiple metrics, achieving better mesh approximation
with fewer spheres and reduced computational overhead. Our experiments show
enhanced robot capabilities in collision detection accuracy, contact-rich
interaction simulation, and navigation through confined spaces. By dynamically
adapting geometric representations to task requirements, robots can now exploit
their physical embodiment as an active resource rather than an inflexible
parameter, opening new frontiers for manipulation in environments where
physical form must continuously balance precision with computational
tractability.

</details>


### [167] [Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation](https://arxiv.org/abs/2507.14099)
*Markus Buchholz,Ignacio Carlucho,Michele Grimaldi,Maria Koskinopoulou,Yvan R. Petillot*

Main category: cs.RO

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u8fd0\u52a8\u89c4\u5212\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\uff08HMS\uff09\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u4ee5\u4f18\u5316\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6027\u548c\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u5148\u524d\u7684\u8fd0\u52a8\u7ecf\u9a8c\uff0c\u4e5f\u96be\u4ee5\u9002\u5e94\u6c34\u4e0b\u73af\u5883\u4e2d\u56fa\u6709\u7684\u5b9e\u65f6\u4e0d\u786e\u5b9a\u6027\u3002

Method: \u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u8fd0\u52a8\u89c4\u5212\u5668\u6846\u67b6\uff0c\u5c06\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\uff08HMS\uff09\u4e0e\u8d1d\u53f6\u65af\u7f51\u7edc\u76f8\u7ed3\u5408\u3002\u5b83\u5728HMS\u5185\u4f7f\u7528\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5305\u542b\u8ddd\u79bb\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u80fd\u8017\u548c\u6267\u884c\u65f6\u95f4\u7684\u590d\u5408\u6210\u672c\u51fd\u6570\u6765\u4f18\u5316\u8def\u5f84\u3002HMS\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6027\u80fd\u3002\u8d1d\u53f6\u65af\u7f51\u7edc\u7528\u4e8e\u6839\u636e\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u548c\u73af\u5883\u6761\u4ef6\u52a8\u6001\u66f4\u65b0\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ece\u800c\u5b8c\u5584\u8def\u5f84\u6210\u529f\u7684\u8054\u5408\u6982\u7387\u3002

Result: \u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u573a\u666f\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u589e\u5f3a\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5c55\u73b0\u4e86\u4f18\u52bf\u3002\u8fd9\u79cd\u6982\u7387\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u80fd\u529b\u3002

Conclusion: \u8be5\u6982\u7387\u65b9\u6cd5\u786e\u4fdd\u4e86\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u9762\u5bf9\u52a8\u6001\u6d77\u6d0b\u6311\u6218\u65f6\u80fd\u591f\u8fdb\u884c\u4f18\u5316\u7684\u8fd0\u52a8\u89c4\u5212\u3002

Abstract: Autonomous motion planning is critical for efficient and safe underwater
manipulation in dynamic marine environments. Current motion planning methods
often fail to effectively utilize prior motion experiences and adapt to
real-time uncertainties inherent in underwater settings. In this paper, we
introduce an Adaptive Heuristic Motion Planner framework that integrates a
Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning
for autonomous underwater manipulation. Our approach employs the Probabilistic
Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite
cost function that accounts for distance, uncertainty, energy consumption, and
execution time. By leveraging HMS, our framework significantly reduces the
search space, thereby boosting computational performance and enabling real-time
planning capabilities. Bayesian Networks are utilized to dynamically update
uncertainty estimates based on real-time sensor data and environmental
conditions, thereby refining the joint probability of path success. Through
extensive simulations and real-world test scenarios, we showcase the advantages
of our method in terms of enhanced performance and robustness. This
probabilistic approach significantly advances the capability of autonomous
underwater robots, ensuring optimized motion planning in the face of dynamic
marine challenges.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [168] [Heatwave-driven air conditioning adoption could increase German electricity demand by 14 GW in the near future](https://arxiv.org/abs/2507.13534)
*Leo Semmelmann,Frederik vom Scheidt*

Main category: eess.SY

TL;DR: \u7814\u7a76\u9884\u6d4b\uff0c\u5728\u672a\u6765\u70ed\u6d6a\u60c5\u666f\u4e0b\uff0c\u5fb7\u56fd\u79fb\u52a8\u7a7a\u8c03\u666e\u53ca\u7387\u7684\u589e\u52a0\u53ef\u80fd\u5bfc\u81f4\u7535\u7f51\u5cf0\u503c\u8d1f\u8377\u663e\u8457\u589e\u957f\uff08\u8d85\u8fc714 GW\uff0c\u537323%\uff09\uff0c\u5c24\u5176\u5728\u57ce\u5e02\u70ed\u70b9\u533a\u57df\uff0c\u5e76\u4e0e\u5149\u4f0f\u53d1\u7535\u4f4e\u8c37\u671f\u91cd\u5408\uff0c\u52a0\u5267\u7535\u7f51\u6311\u6218\u3002


<details>
  <summary>Details</summary>
Motivation: \u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u70ed\u6d6a\u52a0\u5267\uff0c\u63a8\u52a8\u79fb\u52a8\u7a7a\u8c03\u7cfb\u7edf\u5feb\u901f\u666e\u53ca\u3002\u8fd9\u79cd\u5927\u89c4\u6a21\u666e\u53ca\u53ef\u80fd\u5bf9\u7535\u7f51\u548c\u7535\u529b\u7cfb\u7edf\u9020\u6210\u989d\u5916\u538b\u529b\u3002

Method: \u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u65f6\u7a7a\u7c92\u5ea6\uff08\u7cfb\u7edf\u7ea7\u548c\u6bcf\u5e73\u65b9\u516c\u91cc\u7f51\u683c\uff09\u4f30\u7b97\u7a7a\u8c03\u7535\u529b\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5fb7\u56fd\u8fd1\u672a\u6765\u70ed\u6d6a\u60c5\u666f\uff082025\u5e747\u6708\u7c7b\u4f3c\u70ed\u6d6a\uff0c\u7a7a\u8c03\u666e\u53ca\u7387\u4ece19%\u589e\u81f335%\uff09\uff0c\u7ed3\u5408\u6c14\u8c61\u6570\u636e\u3001\u4eba\u53e3\u666e\u67e5\u6570\u636e\u3001\u793e\u4f1a\u4eba\u53e3\u5047\u8bbe\u3001\u51fa\u884c\u6a21\u5f0f\u548c\u6e29\u5ea6\u4f9d\u8d56\u7684\u7a7a\u8c03\u6fc0\u6d3b\u51fd\u6570\uff0c\u5206\u6790\u4e86\u5fb7\u56fd196,428\u4e2a\u4e00\u5e73\u65b9\u516c\u91cc\u7f51\u683c\u7684\u5f71\u54cd\u3002

Result: \u7814\u7a76\u53d1\u73b0\uff0c\u65b0\u8d2d\u79fb\u52a8\u7a7a\u8c03\u7684\u7535\u529b\u9700\u6c42\u53ef\u80fd\u4f7f\u5cf0\u503c\u8d1f\u8377\u589e\u52a0\u8d85\u8fc714 GW\uff0823%\uff09\uff0c\u57ce\u5e02\u70ed\u70b9\u533a\u57df\u6bcf\u5e73\u65b9\u516c\u91cc\u53ef\u8fbe5.8 MW\u3002\u8fd9\u79cd\u9700\u6c42\u7684\u65f6\u95f4\u6a21\u5f0f\u5f62\u6210\u4e00\u4e2a\u660e\u663e\u7684\u4e0b\u5348\u9ad8\u5cf0\uff0c\u4e0e\u5149\u4f0f\u53d1\u7535\u91cf\u8f83\u4f4e\u65f6\u671f\u91cd\u5408\uff0c\u53ef\u80fd\u52a0\u5267\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u6027\u6311\u6218\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e3b\u52a8\u8fdb\u884c\u80fd\u6e90\u7cfb\u7edf\u89c4\u5212\u4ee5\u7ba1\u7406\u65b0\u51fa\u73b0\u7684\u8d1f\u8377\u9ad8\u5cf0\u7684\u7d27\u8feb\u6027\u3002

Abstract: Intensifying heatwaves driven by climate change are accelerating the adoption
of mobile air conditioning (AC) systems. A rapid mass adoption of such AC
systems could create additional stress on electricity grids and the power
system. This study presents a novel method to estimate the electricity demand
from AC systems both at system level and at high temporal and spatial
granularity. We apply the method to a near-future heatwave scenario in Germany
in which household AC adoption increases from current 19% to 35% during a
heatwave similar to the one of July 2025. We analyze the effects for 196,428
grid cells of one square kilometer across Germany, by combining weather data,
census data, socio-demographic assumptions, mobility patterns, and
temperature-dependent AC activation functions. We find that electricity demand
of newly purchased mobile AC systems could increase the peak load by over 14 GW
(23%), with urban hot-spots reaching 5.8 MW per square kilometer. The temporal
pattern creates a pronounced afternoon peak that coincides with lower
photovoltaic generation, potentially exacerbating power system stability
challenges. Our findings underscore the urgency for proactive energy system
planning to manage emerging demand peaks.

</details>


### [169] [MD-OFDM: An Energy-Efficient and Low-PAPR MIMO-OFDM Variant for Resource-Constrained Applications](https://arxiv.org/abs/2507.13623)
*Rahul Gulia*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ef4OFDM\uff08MD-OFDM\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u6bcf\u5b50\u8f7d\u6ce2\u53d1\u5c04\u5929\u7ebf\u9009\u62e9\u6765\u964d\u4f4eMIMO-OFDM\u7684\u5cf0\u5747\u529f\u7387\u6bd4\uff08PAPR\uff09\u548c\u529f\u8017\uff0c\u540c\u65f6\u6539\u5584\u8bef\u7801\u7387\uff08BER\uff09\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u80fd\u6e90\u53d7\u9650\u573a\u666f\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684MIMO-OFDM\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u4f7f\u7528MMSE\u5747\u8861\u5668\u65f6\uff0c\u7531\u4e8e\u591a\u4e2a\u5c04\u9891\uff08RF\uff09\u94fe\u7684\u6fc0\u6d3b\uff0c\u5b58\u5728\u9ad8\u5cf0\u5747\u529f\u7387\u6bd4\uff08PAPR\uff09\u548c\u663e\u8457\u7684\u529f\u8017\u95ee\u9898\u3002

Method: \u63d0\u51fa\u5e76\u6570\u5b66\u5efa\u6a21\u4e86\u591a\u7ef4OFDM\uff08MD-OFDM\uff09\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u6bcf\u5b50\u8f7d\u6ce2\u53d1\u5c04\u5929\u7ebf\u9009\u62e9\u7b56\u7565\uff0c\u5373\u6bcf\u4e2a\u5b50\u8f7d\u6ce2\u4ec5\u6fc0\u6d3b\u4e00\u4e2a\u53d1\u5c04\u5929\u7ebf\u3002\u6587\u7ae0\u63d0\u4f9b\u4e86BER\u3001\u80fd\u91cf\u6548\u7387\uff08EE\uff09\u548cPAPR\u7684\u8be6\u7ec6\u6570\u5b66\u516c\u5f0f\u3002

Result: \u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0eMMSE MIMO\u76f8\u6bd4\uff0cMD-OFDM\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684BER\u548c\u663e\u8457\u66f4\u4f4e\u7684PAPR\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9891\u8c31\u590d\u7528\u51cf\u5c11\uff0c\u5176\u5cf0\u503c\u6574\u4f53\u80fd\u91cf\u6548\u7387\u6709\u6240\u727a\u7272\u3002

Conclusion: MD-OFDM\u7cfb\u7edf\u901a\u8fc7\u964d\u4f4ePAPR\u3001\u529f\u8017\u548c\u6539\u5584BER\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7269\u8054\u7f51\uff08IoT\uff09\u548c\u4f4e\u529f\u8017\u5e7f\u57df\u7f51\uff08LPWAN\uff09\u7b49\u80fd\u6e90\u53d7\u9650\u548c\u6210\u672c\u654f\u611f\u7684\u5e94\u7528\u573a\u666f\u3002

Abstract: Orthogonal Frequency Division Multiplexing (OFDM) combined with
Multiple-Input Multiple-Output (MIMO) techniques forms the backbone of modern
wireless communication systems. While offering high spectral efficiency and
robustness, conventional MIMO-OFDM, especially with complex equalizers like
Minimum Mean Square Error (MMSE), suffers from high Peak-to-Average Power Ratio
(PAPR) and significant power consumption due to multiple active Radio Frequency
(RF) chains. This paper proposes and mathematically models an alternative
system, termed Multi-Dimensional OFDM (MD-OFDM), which employs a per-subcarrier
transmit antenna selection strategy. By activating only one transmit antenna
for each subcarrier, MD-OFDM aims to reduce PAPR, lower power consumption, and
improve Bit Error Rate (BER) performance. We provide detailed mathematical
formulations for BER, Energy Efficiency (EE), and PAPR, and discuss the
suitability of MD-OFDM for various applications, particularly in
energy-constrained and cost-sensitive scenarios such as the Internet of Things
(IoT) and Low-Power Wide Area Networks (LPWAN). Simulation results demonstrate
that MD-OFDM achieves superior BER and significantly lower PAPR compared to
MMSE MIMO, albeit with a trade-off in peak overall energy efficiency due to
reduced spectral multiplexing.

</details>


### [170] [Spacecraft Safe Robust Control Using Implicit Neural Representation for Geometrically Complex Targets in Proximity Operations](https://arxiv.org/abs/2507.13672)
*Hang Zhou,Tao Meng,Kun Wang,Chengrui Shi,Renhao Mao,Weijia Wang,Jiakun Lei*

Main category: eess.SY

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u5b89\u5168\u9c81\u68d2\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u76ee\u6807\u822a\u5929\u5668\u4e0e\u8ffd\u8e2a\u822a\u5929\u5668\u4e4b\u95f4\u7684\u5b89\u5168\u8fd1\u8ddd\u79bb\u64cd\u4f5c\u548c\u907f\u78b0\u3002


<details>
  <summary>Details</summary>
Motivation: \u786e\u4fdd\u822a\u5929\u5668\u8fd1\u8ddd\u79bb\u64cd\u4f5c\uff08\u7279\u522b\u662f\u4e0e\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u76ee\u6807\u822a\u5929\u5668\u4e4b\u95f4\u7684\u907f\u78b0\uff09\u5728\u5b58\u5728\u6270\u52a8\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u6027\u3002

Method: \u8be5\u65b9\u6cd5\u63d0\u51fa\u4e00\u4e2a\u5b89\u5168\u9c81\u68d2\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08\u795e\u7ecfSDF\uff09\u5904\u7406\u4efb\u610f\u76ee\u6807\u51e0\u4f55\u5f62\u72b6\u3002\u795e\u7ecfSDF\u901a\u8fc7\u589e\u5f3a\u9690\u5f0f\u51e0\u4f55\u6b63\u5219\u5316\u548c\u8fc7\u8fd1\u4f3c\u7b56\u7565\u4ece\u70b9\u4e91\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u4ee5\u521b\u5efa\u4fdd\u5b88\u7684\u5b89\u5168\u8fb9\u754c\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u5c42\uff1a1) \u5b89\u5168\u901f\u5ea6\u751f\u6210\u5c42\uff0c\u901a\u8fc7\u4e8c\u9636\u9525\u89c4\u5212\u751f\u6210\u5b89\u5168\u53c2\u8003\u901f\u5ea6\uff0c\u5e76\u5f15\u5165\u5faa\u73af\u4e0d\u7b49\u5f0f\u4ee5\u7f13\u89e3\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\uff1b2) \u5b89\u5168\u9c81\u68d2\u63a7\u5236\u5668\u5c42\uff0c\u96c6\u6210\u6270\u52a8\u89c2\u6d4b\u5668\u548c\u5e73\u6ed1\u5b89\u5168\u6ee4\u6ce2\u5668\u4ee5\u8865\u507f\u4f30\u8ba1\u8bef\u5dee\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u3002

Result: \u5e7f\u6cdb\u7684\u6570\u503c\u6a21\u62df\u548c\u8499\u7279\u5361\u6d1b\u5206\u6790\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\uff0c\u7ed3\u679c\u8868\u660e\u4e0e\u4f20\u7edfCBF\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u88d5\u5ea6\u5e76\u907f\u514d\u4e86\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u822a\u5929\u5668\u8fd1\u8ddd\u79bb\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u907f\u78b0\u6311\u6218\uff0c\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u548c\u6270\u52a8\u4e0b\uff0c\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002

Abstract: This study addresses the challenge of ensuring safe spacecraft proximity
operations, focusing on collision avoidance between a chaser spacecraft and a
complex-geometry target spacecraft under disturbances. To ensure safety in such
scenarios, a safe robust control framework is proposed that leverages implicit
neural representations. To handle arbitrary target geometries without explicit
modeling, a neural signed distance function (SDF) is learned from point cloud
data via a enhanced implicit geometric regularization method, which
incorporates an over-apporximation strategy to create a conservative,
safety-prioritized boundary. The target's surface is implicitly defined by the
zero-level set of the learned neural SDF, while the values and gradients
provide critical information for safety controller design. This neural SDF
representation underpins a two-layer hierarchcial safe robust control
framework: a safe velocity generation layer and a safe robust controller layer.
In the first layer, a second-order cone program is formulated to generate
safety-guaranteed reference velocity by explicitly incorporating the
under-approximation error bound. Furthermore, a circulation inequality is
introduced to mitigate the local minimum issues commonly encountered in control
barrier function (CBF) methods. The second layer features an integrated
disturbance observer and a smooth safety filter explicitly compensating for
estimation error, bolstering robustness to external disturbances. Extensive
numerical simulations and Monte Carlo analysis validate the proposed framework,
demonstrating significantly improved safety margins and avoidance of local
minima compared to conventional CBF approaches.

</details>


### [171] [Minimum Clustering of Matrices Based on Phase Alignment](https://arxiv.org/abs/2507.13678)
*Honghao Wu,Kemi Ding,Li Qiu*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5177\u6709\u76f8\u4f3c\u540c\u6b65\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u6700\u5c0f\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6240\u9700\u7684\u63a7\u5236\u5668\u7c7b\u578b\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u540c\u6b65\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u5b9e\u73b0\u6210\u672c\u3002


<details>
  <summary>Details</summary>
Motivation: \u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u9762\u4e34\u540c\u6b65\u6027\u80fd\u548c\u63a7\u5236\u5668\u5b9e\u73b0\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002\u73b0\u6709\u96c6\u4e2d\u5f0f\u63a7\u5236\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u597d\u4f46\u53ef\u6269\u5c55\u6027\u5dee\u4e14\u5b58\u5728\u5355\u70b9\u6545\u969c\uff0c\u800c\u5206\u5e03\u5f0f\u63a7\u5236\u65b9\u6cd5\u5219\u5bfc\u81f4\u6240\u9700\u63a7\u5236\u5668\u7c7b\u578b\u968f\u7cfb\u7edf\u89c4\u6a21\u7ebf\u6027\u589e\u957f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u964d\u4f4e\u63a7\u5236\u5668\u591a\u6837\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002

Method: \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u76f8\u4f4d\u5bf9\u9f50\u7684\u6846\u67b6\u3002\u5229\u7528\u590d\u6742\u77e9\u9635\u7684\u5185\u5728\u76f8\u4f4d\u7279\u6027\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ea6\u675f\u805a\u7c7b\u95ee\u9898\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u9488\u5bf9\u5c0f\u89c4\u6a21\u7cfb\u7edf\u7684\u9012\u5f52\u7cbe\u786e\u641c\u7d22\u548c\u9488\u5bf9\u5927\u89c4\u6a21\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u3002

Result: \u5c06\u7406\u8bba\u7ed3\u679c\u5e94\u7528\u4e8e\u4e00\u4e2a50\u4e2a\u667a\u80fd\u4f53\u7684\u7f51\u7edc\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002

Conclusion: \u8be5\u5de5\u4f5c\u5c06\u7406\u8bba\u76f8\u4f4d\u5206\u6790\u4e0e\u5b9e\u9645\u63a7\u5236\u7efc\u5408\u76f8\u7ed3\u5408\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6218\u7565\u6027\u805a\u7c7b\u667a\u80fd\u4f53\u6765\u6700\u5c0f\u5316\u63a7\u5236\u5668\u7c7b\u578b\u3002

Abstract: Coordinating multi-agent systems requires balancing synchronization
performance and controller implementation costs. To this end, we classify
agents by their intrinsic properties, enabling each group to be controlled by a
uniform controller and thus reducing the number of unique controller types
required. Existing centralized control methods, despite their capability to
achieve high synchronization performance with fewer types of controllers,
suffer from critical drawbacks such as limited scalability and vulnerability to
single points of failure. On the other hand, distributed control strategies,
where controllers are typically agent-dependent, result in the type of required
controllers increasing proportionally with the size of the system.
  This paper introduces a novel phase-alignment-based framework to minimize the
type of controllers by strategically clustering agents with aligned
synchronization behaviors. Leveraging the intrinsic phase properties of complex
matrices, we formulate a constrained clustering problem and propose a
hierarchical optimization method combining recursive exact searches for
small-scale systems and scalable stochastic approximations for large-scale
networks. This work bridges theoretical phase analysis with practical control
synthesis, offering a cost-effective solution for large-scale multi-agent
systems. The theoretical results applied for the analysis of a 50-agent network
illustrate the effectiveness of the proposed algorithms.

</details>


### [172] [Robust Probability Hypothesis Density Filtering: Theory and Algorithms](https://arxiv.org/abs/2507.13687)
*Ming Lei,Shufan Wu*

Main category: eess.SY

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6781\u5c0f\u6781\u5927\u9c81\u68d2PHD\u6ee4\u6ce2\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u6742\u6ce2\u548c\u76ee\u6807\u4ea4\u4e92\u5e26\u6765\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u591a\u76ee\u6807\u8ddf\u8e2a\uff08MTT\uff09\u5728\u4fe1\u606f\u878d\u5408\u4e2d\u9762\u4e34\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u6742\u6ce2\u5e72\u6270\u548c\u76ee\u6807\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u4e0e\u6548\u7387\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u9ad8\u65af\u6df7\u5408PHD\uff08GM-PHD\uff09\u548c\u57fa\u6570PHD\uff08CPHD\uff09\u6ee4\u6ce2\u5668\u5b58\u5728\u7ec4\u5408\u7206\u70b8\u3001\u5bf9\u53c2\u6570\u654f\u611f\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u7b49\u56fa\u6709\u9650\u5236\u3002

Method: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6781\u5c0f\u6781\u5927\u9c81\u68d2PHD\u6ee4\u6ce2\u6846\u67b6\uff0c\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a1) \u7406\u8bba\u63a8\u5bfc\u7684\u9c81\u68d2GM-PHD\u9012\u5f52\u7b97\u6cd5\uff0c\u5728\u6709\u754c\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b9e\u73b0\u6700\u4f18\u6700\u574f\u60c5\u51b5\u8bef\u5dee\u63a7\u5236\uff1b2) \u81ea\u9002\u5e94\u5b9e\u65f6\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0c\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u8bef\u5dee\u754c\u9650\uff1b3) \u5e7f\u4e49\u91cd\u5c3e\u6d4b\u91cf\u4f3c\u7136\u51fd\u6570\uff0c\u4fdd\u6301\u591a\u9879\u5f0f\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b4) \u9488\u5bf9\u6269\u5c55\u76ee\u6807\u7684\u65b0\u578b\u57fa\u4e8e\u5206\u533a\u7684\u53ef\u4fe1\u5ea6\u52a0\u6743\u65b9\u6cd5\u3002\u7814\u7a76\u8fd8\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86PHD\u89e3\u7684\u552f\u4e00\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e0e\u6807\u51c6GM-PHD\u7684\u7b97\u6cd5\u7b49\u6548\u6027\u3002

Result: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9ad8\u6742\u6ce2\u73af\u5883\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cOSPA\u8bef\u5dee\u964d\u4f4e\u4e8632.4%\uff0c\u57fa\u6570RMSE\u964d\u4f4e\u4e8625.3%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6bcf\u6b6515.3\u6beb\u79d2\u7684\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002

Conclusion: \u8fd9\u9879\u7a81\u7834\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u53ef\u9760\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002

Abstract: Multi-target tracking (MTT) serves as a cornerstone technology in information
fusion, yet faces significant challenges in robustness and efficiency when
dealing with model uncertainties, clutter interference, and target
interactions. Conventional approaches like Gaussian Mixture PHD (GM-PHD) and
Cardinalized PHD (CPHD) filters suffer from inherent limitations including
combinatorial explosion, sensitivity to birth/death process parameters, and
numerical instability. This study proposes an innovative minimax robust PHD
filtering framework with four key contributions: (1) A theoretically derived
robust GM-PHD recursion algorithm that achieves optimal worst-case error
control under bounded uncertainties; (2) An adaptive real-time parameter
adjustment mechanism ensuring stability and error bounds; (3) A generalized
heavy-tailed measurement likelihood function maintaining polynomial
computational complexity; (4) A novel partition-based credibility weighting
method for extended targets. The research not only establishes rigorous
convergence guarantees and proves the uniqueness of PHD solutions, but also
verifies algorithmic equivalence with standard GM-PHD. Experimental results
demonstrate that in high-clutter environments, this method achieves a
remarkable 32.4% reduction in OSPA error and 25.3% lower cardinality RMSE
compared to existing techniques, while maintaining real-time processing
capability at 15.3 milliseconds per step. This breakthrough lays a crucial
foundation for reliable MTT in safety-critical applications.

</details>


### [173] [Safe and Performant Controller Synthesis using Gradient-based Model Predictive Control and Control Barrier Functions](https://arxiv.org/abs/2507.13872)
*Aditya Singh,Aastha Mishra,Manan Tayal,Shishir Kolathaya,Pushpak Jagtap*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6MPC\u548cCBF\u5b89\u5168\u6ee4\u6ce2\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9ad8\u7ef4\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5171\u540c\u4f18\u5316\u6027\u80fd\u548c\u5b89\u5168\uff0c\u751f\u6210\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u9ad8\u6027\u80fd\u7684\u63a7\u5236\u5668\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u517c\u987e\u6027\u80fd\u548c\u5b89\u5168\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBF\uff09\u5728\u540d\u4e49\u63a7\u5236\u5668\u7f3a\u4e4f\u5b89\u5168\u610f\u8bc6\u65f6\u53ef\u80fd\u8fc7\u4e8e\u4fdd\u5b88\uff1b\u800c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u6c42\u89e3\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff08SC-OCPs\uff09\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u96be\u4ee5\u5904\u7406\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u6027\u80fd\u53c8\u80fd\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\u7684\u6709\u6548\u65b9\u6cd5\u3002

Method: \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\n1.  **\u7b2c\u4e00\u9636\u6bb5**\uff1a\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u5c06\u5b89\u5168\u7ea6\u675f\u4f5c\u4e3a\u60e9\u7f5a\u9879\u7eb3\u5165\u6210\u672c\u51fd\u6570\u3002\u8fd9\u4f7f\u5f97\u4f18\u5316\u8fc7\u7a0b\u66f4\u5feb\uff0c\u5e76\u907f\u514d\u4e86\u786c\u7ea6\u675f\u76f8\u5173\u7684\u53ef\u884c\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002\n2.  **\u7b2c\u4e8c\u9636\u6bb5**\uff1a\u4f7f\u7528\u57fa\u4e8e\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBF\uff09\u7684\u4e8c\u6b21\u89c4\u5212\uff08CBF-QP\uff09\u5bf9\u7b2c\u4e00\u9636\u6bb5\u5f97\u5230\u7684\u63a7\u5236\u5668\u8fdb\u884c\u4fee\u6539\uff0c\u4ee5\u5f3a\u5236\u6267\u884c\u786c\u6027\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u4f7f\u504f\u5dee\u6700\u5c0f\u5316\u3002

Result: \u8be5\u65b9\u6cd5\u751f\u6210\u7684\u63a7\u5236\u5668\u65e2\u5177\u6709\u9ad8\u6027\u80fd\uff0c\u53c8\u53ef\u8bc1\u660e\u662f\u5b89\u5168\u7684\u3002\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u4e3a\u590d\u6742\u3001\u9ad8\u7ef4\u81ea\u4e3b\u7cfb\u7edf\u5408\u6210\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u9ad8\u6027\u80fd\u63a7\u5236\u5668\u7684\u80fd\u529b\u3002

Conclusion: \u672c\u6587\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6210\u529f\u5730\u5c06\u57fa\u4e8e\u68af\u5ea6\u7684MPC\u4e0eCBF\u5b89\u5168\u6ee4\u6ce2\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u9ad8\u7ef4\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u6027\u80fd\u4e0e\u5b89\u5168\u5171\u4f18\u5316\u7684\u95ee\u9898\uff0c\u751f\u6210\u4e86\u65e2\u9ad8\u6548\u53c8\u5177\u6709\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\u7684\u63a7\u5236\u5668\u3002

Abstract: Ensuring both performance and safety is critical for autonomous systems
operating in real-world environments. While safety filters such as Control
Barrier Functions (CBFs) enforce constraints by modifying nominal controllers
in real time, they can become overly conservative when the nominal policy lacks
safety awareness. Conversely, solving State-Constrained Optimal Control
Problems (SC-OCPs) via dynamic programming offers formal guarantees but is
intractable in high-dimensional systems. In this work, we propose a novel
two-stage framework that combines gradient-based Model Predictive Control (MPC)
with CBF-based safety filtering for co-optimizing safety and performance. In
the first stage, we relax safety constraints as penalties in the cost function,
enabling fast optimization via gradient-based methods. This step improves
scalability and avoids feasibility issues associated with hard constraints. In
the second stage, we modify the resulting controller using a CBF-based
Quadratic Program (CBF-QP), which enforces hard safety constraints with minimal
deviation from the reference. Our approach yields controllers that are both
performant and provably safe. We validate the proposed framework on two case
studies, showcasing its ability to synthesize scalable, safe, and
high-performance controllers for complex, high-dimensional autonomous systems.

</details>


### [174] [Fixed time convergence guarantees for Higher Order Control Barrier Functions](https://arxiv.org/abs/2507.13888)
*Janani S K,Shishir Kolathaya*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u9ad8\u9636\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08HOCBFs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u786e\u4fdd\u7cfb\u7edf\u5728\u7528\u6237\u6307\u5b9a\u7684\u65f6\u95f4\u5185\u6536\u655b\u5230\u5b89\u5168\u96c6\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684\u9ad8\u9636\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08HOCBFs\uff09\u4ec5\u80fd\u4fdd\u8bc1\u6e10\u8fd1\u5b89\u5168\uff0c\u7f3a\u4e4f\u56fa\u5b9a\u65f6\u95f4\u6536\u655b\u7684\u673a\u5236\uff0c\u8fd9\u5728\u81ea\u52a8\u5bfc\u822a\u7b49\u65f6\u95f4\u654f\u611f\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002

Method: \u901a\u8fc7\u5728\u7279\u5f81\u591a\u9879\u5f0f\u4e2d\u4f7f\u7528\u91cd\u590d\u6839\u6765\u65bd\u52a0\u7ed3\u6784\u5316\u7684\u5fae\u5206\u7ea6\u675f\uff0c\u4ece\u800c\u5f97\u5230\u5177\u6709\u7cbe\u786e\u56fa\u5b9a\u65f6\u95f4\u6536\u655b\u7684\u95ed\u5f0f\u591a\u9879\u5f0f\u89e3\u3002\u6587\u4e2d\u63a8\u5bfc\u4e86\u786e\u4fdd\u524d\u5411\u4e0d\u53d8\u6027\u548c\u56fa\u5b9a\u65f6\u95f4\u53ef\u8fbe\u6027\u7684\u969c\u788d\u51fd\u6570\u53ca\u5176\u5bfc\u6570\u7684\u6761\u4ef6\uff0c\u5e76\u4e3a\u4e8c\u9636\u7cfb\u7edf\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u516c\u5f0f\u3002

Result: \u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u70b9\u8d28\u91cf\u6a21\u578b\u3001\u72ec\u8f6e\u8f66\u6a21\u578b\u548c\u81ea\u884c\u8f66\u6a21\u578b\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709HOCBF\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u4f20\u7edf\u65b9\u6cd5\u5931\u8d25\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e5f\u80fd\u53ef\u9760\u5730\u5728\u671f\u671b\u65f6\u95f4\u5185\u5f3a\u5236\u6536\u655b\u3002

Conclusion: \u8fd9\u9879\u5de5\u4f5c\u4e3a\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6709\u9650\u65f6\u95f4\u5b89\u5168\u4fdd\u8bc1\u7684\u5b9e\u65f6\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u4e14\u9c81\u68d2\u7684\u6846\u67b6\u3002

Abstract: We present a novel method for designing higher-order Control Barrier
Functions (CBFs) that guarantee convergence to a safe set within a
user-specified finite. Traditional Higher Order CBFs (HOCBFs) ensure asymptotic
safety but lack mechanisms for fixed-time convergence, which is critical in
time-sensitive and safety-critical applications such as autonomous navigation.
In contrast, our approach imposes a structured differential constraint using
repeated roots in the characteristic polynomial, enabling closed-form
polynomial solutions with exact convergence at a prescribed time. We derive
conditions on the barrier function and its derivatives that ensure forward
invariance and fixed-time reachability, and we provide an explicit formulation
for second-order systems. Our method is evaluated on three robotic systems - a
point-mass model, a unicycle, and a bicycle model and benchmarked against
existing HOCBF approaches. Results demonstrate that our formulation reliably
enforces convergence within the desired time, even when traditional methods
fail. This work provides a tractable and robust framework for real-time control
with provable finite-time safety guarantees.

</details>


### [175] [A Robust Periodic Controller for Spacecraft Attitude Tracking](https://arxiv.org/abs/2507.13908)
*Frederik Thiele,Felix Biertümpfel,Harald Pfifer*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u7684\u536b\u661f\u5468\u671f\u59ff\u6001\u63a7\u5236\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u52a8\u529b\u5b66\u5468\u671f\u6027\u5b9e\u73b0\u8f68\u9053\u4e0a\u7684\u6052\u5b9a\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u4e3a\u4e86\u5728\u536b\u661f\u8f68\u9053\u4e0a\u5b9e\u73b0\u6052\u5b9a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u8981\u6c42\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5728\u63a7\u5236\u5668\u8bbe\u8ba1\u4e2d\u5145\u5206\u5229\u7528\u536b\u661f\u52a8\u529b\u5b66\u7684\u5468\u671f\u6027\u3002

Method: \u8be5\u65b9\u6cd5\u91c7\u7528\u6df7\u5408\u7075\u654f\u5ea6\u63a7\u5236\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u7684\u52a0\u6743\u65b9\u6848\u3002\u63a7\u5236\u5668\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u7ed3\u6784\u5316\u7ebf\u6027\u65f6\u53d8\u5468\u671f\u8f93\u51fa\u53cd\u9988\u7efc\u5408\u65b9\u6cd5\u8ba1\u7b97\uff0c\u8be5\u65b9\u6cd5\u662f\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5468\u671fH-infinity\u7efc\u5408\u4e2d\u5e38\u89c1\u7684\u7f51\u683c\u8bc4\u4f30\uff0c\u5e76\u4fdd\u8bc1\u4e86\u6700\u4f18\u7684L2\u6027\u80fd\u3002

Result: \u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u88ab\u8bc1\u660e\u5bf9\u5468\u671f\u6027\u536b\u661f\u59ff\u6001\u63a7\u5236\u6709\u6548\uff0c\u63a7\u5236\u5668\u7ed3\u6784\u900f\u660e\u4e14\u6613\u4e8e\u5b9e\u73b0\u3002\u901a\u8fc7\u4e00\u4e2a\u592a\u9633\u80fd\u7535\u7ad9\u536b\u661f\u7684\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002

Conclusion: \u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u9c81\u68d2\u5468\u671f\u6027\u536b\u661f\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u786e\u4fdd\u6574\u4e2a\u8f68\u9053\u4e0a\u7684\u6052\u5b9a\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u63a7\u5236\u5668\u6613\u4e8e\u5b9e\u65bd\u3002

Abstract: This paper presents a novel approach for robust periodic attitude control of
satellites. Respecting the periodicity of the satellite dynamics in the
synthesis allows to achieve constant performance and robustness requirements
over the orbit. The proposed design follows a mixed sensitivity control design
employing a physically motivated weighting scheme. The controller is calculated
using a novel structured linear time-periodic output feedback synthesis with
guaranteed optimal L2-performance. The synthesis poses a convex optimization
problem and avoids grid-wise evaluations of coupling conditions inherent for
classical periodic H-infinity-synthesis. Moreover, the controller has a
transparent and easy to implement structure. A solar power plant satellite is
used to demonstrate the effectiveness of the proposed method for periodic
satellite attitude control.

</details>


### [176] [Identifiability Analysis of a Pseudo-Two-Dimensional Model & Single Particle Model-Aided Parameter Estimation](https://arxiv.org/abs/2507.13931)
*L. D. Couto,K. Haghverdi,F. Guo,K. Trad,G. Mulder*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u8fa8\u8bc6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u51c6\u786e\u5730\u4f30\u8ba1\u4f2a\u4e8c\u7ef4\uff08P2D\uff09\u7535\u6c60\u6a21\u578b\u7684\u53c2\u6570\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u9636\u6a21\u578b\u548cP2D\u6a21\u578b\u5b9e\u73b0\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u3002


<details>
  <summary>Details</summary>
Motivation: \u9700\u8981\u4e00\u79cd\u51c6\u786e\u4e14\u5feb\u901f\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1P2D\u7535\u6c60\u6a21\u578b\u7684\u53c2\u6570\u3002

Method: \u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a1) \u68c0\u67e5\u6570\u636e\u5e76\u6355\u83b7\u7279\u5b9a\u7279\u5f81\u7eb3\u5165\u6a21\u578b\uff1b2) \u5206\u6790P2D\u6a21\u578b\u53c2\u6570\u7684\u53ef\u8fa8\u8bc6\u6027\uff0c\u5e76\u63d0\u51fa\u66ff\u4ee3\u53c2\u6570\u5316\u65b9\u6848\u4ee5\u89e3\u51b3\u6f5c\u5728\u95ee\u9898\uff1b3) \u8003\u8651\u4e0d\u540c\u5de5\u51b5\u6765\u6fc0\u53d1\u4e0d\u540c\u7684\u7535\u6c60\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u4f4e\u9636\u7535\u6c60\u6a21\u578b\u3002\u5728\u4f4e\u7535\u6d41\u6761\u4ef6\u4e0b\uff0c\u5148\u4f7f\u7528\u4f4e\u9636\u6a21\u578b\u8fdb\u884c\u5feb\u901f\u4f30\u8ba1\uff0c\u518d\u7528\u8fd9\u4e9b\u4f30\u8ba1\u503c\u521d\u59cb\u5316P2D\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u8fa8\u8bc6\u3002

Result: \u5728\u4f4e\u7535\u6d41\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u4f4e\u9636\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u7684\u901f\u5ea6\u6bd4\u76f4\u63a5\u4f7f\u7528P2D\u6a21\u578b\u5feb\u81f3\u5c11500\u500d\uff0c\u4f46\u8bef\u5dee\u589e\u52a0\u4e00\u500d\u3002\u5982\u679c\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u53ef\u4ee5\u5c06\u4f4e\u9636\u6a21\u578b\u4f30\u8ba1\u7684\u53c2\u6570\u7528\u4e8e\u521d\u59cb\u5316P2D\u6a21\u578b\uff0c\u4ece\u800c\u5c06P2D\u6a21\u578b\u7684\u8fa8\u8bc6\u65f6\u95f4\u7f29\u77ed\u4e00\u534a\u3002

Conclusion: \u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7075\u6d3b\u7684P2D\u7535\u6c60\u6a21\u578b\u53c2\u6570\u8fa8\u8bc6\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u9636\u6a21\u578b\u7684\u901f\u5ea6\u4f18\u52bf\u548cP2D\u6a21\u578b\u7684\u7cbe\u5ea6\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u51c6\u786e\u7684\u53c2\u6570\u4f30\u8ba1\u3002

Abstract: This contribution presents a parameter identification methodology for the
accurate and fast estimation of model parameters in a pseudo-two-dimensional
(P2D) battery model. The methodology consists of three key elements. First, the
data for identification is inspected and specific features herein that need to
be captured are included in the model. Second, the P2D model is analyzed to
assess the identifiability of the physical model parameters and propose
alternative parameterizations that alleviate possible issues. Finally, diverse
operating conditions are considered that excite distinct battery dynamics which
allows the use of different low-order battery models accordingly. Results show
that, under low current conditions, the use of low-order models achieve
parameter estimates at least 500 times faster than using the P2D model at the
expense of twice the error. However, if accuracy is a must, these estimated
parameters can be used to initialize the P2D model and perform the
identification in half of the time.

</details>


### [177] [Diffraction and Scattering Modeling for Laser Power Beaming in Lunar Environment](https://arxiv.org/abs/2507.13982)
*Yanni Jiwan-Mercier,Barış Dönmez,Güneş Karabulut-Kurt,Sébastien Loranger*

Main category: eess.SY

TL;DR: \u7814\u7a76\u8868\u660e\uff0c\u6708\u7403\u5c18\u57c3\u5bf9\u5149\u5b66\u80fd\u91cf\u675f\u4f20\u8f93\u6548\u7387\u5f71\u54cd\u663e\u8457\uff0c\u4f46\u63d0\u9ad8\u6fc0\u5149\u6e90\u9ad8\u5ea6\u53ef\u6709\u6548\u6539\u5584\u4f20\u8f93\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u5728\u6708\u7403\u4efb\u52a1\u4e2d\u7cbe\u786e\u5c18\u57c3\u5efa\u6a21\u548c\u7cfb\u7edf\u9ad8\u5ea6\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u4e3a\u957f\u671f\u6708\u7403\u4efb\u52a1\uff08\u5c24\u5176\u662f\u5728\u592a\u9633\u5149\u7167\u6709\u9650\u533a\u57df\uff09\u63d0\u4f9b\u53ef\u9760\u80fd\u6e90\u81f3\u5173\u91cd\u8981\u3002\u5149\u5b66\u80fd\u91cf\u675f\uff08OPB\uff09\u662f\u4f20\u7edf\u592a\u9633\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u6708\u7403\u5c18\u57c3\u5bf9\u5149\u675f\u4f20\u64ad\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002

Method: \u5f15\u5165\u4e86\u4e00\u4e2a\u8be6\u7ec6\u7684\u4eff\u771f\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u9759\u7535\u60ac\u6d6e\u6708\u7403\u98ce\u5316\u5c42\u5f15\u8d77\u7684\u884d\u5c04\u548c\u9ad8\u5ea6\u4f9d\u8d56\u6027\u6563\u5c04\u3002\u4e0e\u4ee5\u5f80\u5047\u8bbe\u5747\u5300\u5c18\u57c3\u5c42\u6216\u4ec5\u8ba1\u7b97\u4e2d\u5fc3\u5230\u4e2d\u5fc3\u4f20\u8f93\u635f\u8017\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u672c\u6a21\u578b\u4f7f\u7528\u5e7f\u4e49\u884d\u5c04\u7406\u8bba\u548c\u4ece\u7c92\u5b50\u5bc6\u5ea6\u5bfc\u51fa\u7684\u6298\u5c04\u7387\u68af\u5ea6\u6765\u8bc4\u4f30\u5149\u675f\u5f62\u53d8\u548c\u8870\u51cf\u3002

Result: \u5373\u4f7f\u5728\u5730\u9762\u5230\u5730\u9762\u7684\u573a\u666f\u4e2d\uff0c\u6708\u7403\u5c18\u57c3\u4e5f\u663e\u8457\u964d\u4f4e\u80fd\u91cf\u4f20\u8f93\u6548\u7387\uff1a\u572850\u516c\u91cc\u8ddd\u79bb\u4e0a\uff0c\u65e0\u5c18\u6761\u4ef6\u4e0b\u6548\u7387\u4e3a57%\uff0c\u800c\u6709175\u7eb3\u7c73\u5c18\u57c3\u7c92\u5b50\u65f6\u964d\u81f33.7%\u3002\u5c06\u7c92\u5b50\u5c3a\u5bf8\u589e\u81f3250\u7eb3\u7c73\uff0c\u6709\u6548\u4f20\u8f93\u8ddd\u79bb\u9650\u5236\u572830\u516c\u91cc\u4ee5\u4e0b\uff0c\u6548\u7387\u4ec5\u4e3a6%\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u63d0\u9ad8\u6fc0\u5149\u6e90\u9ad8\u5ea6\u53ef\u6539\u5584\u6548\u7387\uff1a\u5f53\u5149\u6e90\u4f4d\u4e8e\u5730\u9762\u4e0a\u65b912\u7c73\u65f6\uff0c5\u516c\u91cc\u8ddd\u79bb\u53ef\u8fbe91%\u6548\u7387\uff0c50\u516c\u91cc\u8ddd\u79bb\u53ef\u8fbe25%\u6548\u7387\u3002

Conclusion: \u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u6708\u7403\u5149\u5b66\u80fd\u91cf\u675f\u8bbe\u8ba1\u4e2d\u7cfb\u7edf\u9ad8\u5ea6\u548c\u5c18\u57c3\u5efa\u6a21\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u7c92\u5b50\u5c3a\u5bf8\u5206\u5e03\uff08\u5c24\u5176\u662f\u5728\u4eba\u7c7b\u6d3b\u52a8\u6270\u52a8\u73af\u5883\u4e2d\uff09\u5bf9\u4efb\u52a1\u7684\u5173\u952e\u4f5c\u7528\u3002

Abstract: Reliable energy delivery is a critical requirement for
  long-term lunar missions, particularly in regions with limited
  solar access, such as polar craters and during extended lunar
  nights. Optical Power Beaming (OPB) using high-power lasers
  offers a promising alternative to conventional solar power, but
  the effects of suspended lunar dust on beam propagation remain
  poorly understood. This study introduces a detailed simulation
  model that incorporates both diffraction and height-dependent
  scattering by the electrostatically suspended lunar regolith. Un like prior
approaches, which assumed uniform dust layers or
  center-to-center transmission loss, our model uses generalized
  diffraction theory and refractive index gradients derived from
  particle density to assess beam deformation and attenuation. The
  results show that even in ground-to-ground scenarios, lunar dust
  significantly degrades energy transfer efficiency, dropping from
  57% to 3.7% over 50 km in dust-free vs. dusty conditions with
  175 nm particles. Increasing the particle size to 250 nm limits the
  viable transmission range to below 30 km at 6% efficiency. The
  study further demonstrates that raising the laser source height
  can improve efficiency, achieving 91% for a distance of 5 km
  and 25% at 50 km when the source is positioned 12 m above
  ground. These findings underscore the importance of system
  elevation and dust modeling in lunar OPB design and reveal
  the mission-critical role of particle size distribution, especially in
  environments disturbed by human activity.

</details>


### [178] [Smart fault detection in satellite electrical power system](https://arxiv.org/abs/2507.14004)
*Niloofar Nobahari,Alireza Rezaee*

Main category: eess.SY

TL;DR: \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u4f4e\u8f68\u536b\u661f\uff08\u65e0\u59ff\u6001\u63a7\u5236\u7cfb\u7edf\uff09\u6574\u4e2a\u7535\u529b\u7cfb\u7edf\u7684\u6545\u969c\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\u3002


<details>
  <summary>Details</summary>
Motivation: \u536b\u661f\u7535\u529b\u7cfb\u7edf\u7ec4\u4ef6\u6613\u53d1\u751f\u6545\u969c\uff0c\u800c\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u4e2a\u7ec4\u4ef6\u7684\u6545\u969c\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6574\u4e2a\u536b\u661f\u7535\u529b\u7cfb\u7edf\u7684\u5168\u9762\u8bca\u65ad\u65b9\u6848\u3002

Method: \u91c7\u7528\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5229\u7528\u592a\u9633\u8f90\u5c04\u548c\u8868\u9762\u6e29\u5ea6\u7b49\u8f93\u5165\u6570\u636e\u9884\u6d4b\u7535\u6d41\u548c\u8d1f\u8f7d\u8f93\u51fa\u3002\u7ed3\u5408\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548cK-\u8fd1\u90bb\uff08KNN\uff09\u7b49\u673a\u5668\u5b66\u4e60\u6280\u672f\u6709\u6548\u5206\u7c7b\u6545\u969c\u3002

Result: \u8be5\u6a21\u578b\u5728\u8bc6\u522b\u591a\u4e2a\u5b50\u7cfb\u7edf\u6545\u969c\u65b9\u9762\u53d6\u5f97\u4e86\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\u3002

Conclusion: \u8be5\u65b9\u6cd5\u4e3a\u6574\u4e2a\u536b\u661f\u7535\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u5e76\u6709\u52a9\u4e8e\u964d\u4f4e\u4efb\u52a1\u5931\u8d25\u7684\u98ce\u9669\u3002

Abstract: This paper presents an new approach for detecting in the electrical power
system of satellites operating in Low Earth Orbit (LEO) without an Attitude
Determination and Control Subsystem (ADCS). Components of these systems are
prone to faults, such as line-to-line faults in the photovoltaic subsystem,
open circuits, and short circuits in the DC-to-DC converter, as well as ground
faults in batteries. In the previous research has largely focused on detecting
faults in each components, such as photovoltaic arrays or converter systems,
therefore, has been limited attention given to whole electrical power system of
satellite as a whole system. Our approach addresses this gap by utilizing a
Multi-Layer Perceptron (MLP) neural network model, which leverages input data
such as solar radiation and surface temperature to predict current and load
outputs. These machine learning techniques that classifiy use different
approaches like Principal Component Analysis (PCA) and K-Nearest Neighbors
(KNN), to classify faults effectively. The model presented achieves over 99%
accuracy in identifying faults across multiple subsystems, marking a notable
advancement from previous approaches by offering a complete diagnostic solution
for the entire satellite power system. This thorough method boosts system
reliability and helps lower the chances of mission failure

</details>


### [179] [Influence of Cell Position on the Capacity of Retired Batteries: Experimental and Statistical Studies](https://arxiv.org/abs/2507.14020)
*Marwan Hassini,Colette Mintsa-Eya,Eduardo Redondo-Iglesias,Pascal Venet*

Main category: eess.SY

TL;DR: \u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u7535\u52a8\u6c7d\u8f66\u9000\u5f79\u7535\u6c60\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u5e73\u5747\u5065\u5eb7\u5bb9\u91cf\u9ad8\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5206\u6563\u6027\uff0c\u4e14\u6027\u80fd\u4e0e\u7535\u6c60\u5728\u6a21\u5757\u4e2d\u7684\u4f4d\u7f6e\u65e0\u5173\uff0c\u5f3a\u8c03\u4e86\u518d\u5229\u7528\u65f6\u9700\u8003\u8651\u5206\u6563\u6027\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u7ba1\u7406\u7cfb\u7edf\u3002


<details>
  <summary>Details</summary>
Motivation: \u4e86\u89e3\u6c7d\u8f66\u4f7f\u7528\u540e\u7684\u7535\u6c60\u6027\u80fd\u5bf9\u4e8e\u5224\u65ad\u5176\u518d\u5229\u7528\u6f5c\u529b\u81f3\u5173\u91cd\u8981\uff0c\u65e8\u5728\u63a8\u8fdb\u9000\u5f79\u7535\u6c60\u6027\u80fd\u77e5\u8bc6\u7684\u7814\u7a76\u3002

Method: \u6d4b\u8bd5\u4e86\u4ece\u7535\u52a8\u6c7d\u8f66\u4e2d\u63d0\u53d6\u7684\u4e09\u4e2a\u7535\u6c60\u6a21\u5757\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u65b9\u5dee\u5206\u6790\uff08ANOVA\uff09\u5bf9\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002

Result: 36\u4e2a\u9000\u5f79\u7535\u6c60\u8868\u73b0\u51fa\u9ad8\u6c34\u5e73\u7684\u6027\u80fd\uff0c\u5e73\u5747\u5065\u5eb7\u5bb9\u91cf\u4e3a95%\uff0c\u4f46\u5206\u6563\u5ea6\u8fbe2.4%\u3002ANOVA\u5206\u6790\u8868\u660e\uff0c\u7535\u6c60\u6027\u80fd\u4e0e\u5176\u5728\u6a21\u5757\u5185\u90e8\u7684\u4f4d\u7f6e\u4e0d\u76f8\u5173\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u8bc4\u4f30\u9000\u5f79\u7535\u6c60\u5185\u90e8\u7684\u5206\u6563\u6027\uff0c\u5e76\u4e3a\u4e8c\u6b21\u5229\u7528\u7535\u6c60\u5f00\u53d1\u70ed\u7ba1\u7406\u548c\u5747\u8861\u7cfb\u7edf\u3002

Abstract: Understanding how batteries perform after automotive use is crucial to
determining their potential for reuse. This article presents experimental
results aimed at advancing knowledge of retired battery performance. Three
modules extracted from electric vehicles were tested. Their performance was
assessed, and the results were analyzed statistically using analysis of
variance (ANOVA). The 36 retired cells exhibited a high level of performance,
albeit with significant variation. On average, the cells had a 95% state of
health capacity with a dispersion of 2.4%. ANOVA analysis suggests that cell
performance is not correlated with their position inside the module. These
results demonstrate the need to evaluate dispersion within retired batteries
and to develop thermal management and balancing systems for second-life
batteries.

</details>


### [180] [Reference-Free Iterative Learning Model Predictive Control with Neural Certificates](https://arxiv.org/abs/2507.14025)
*Wataru Hashimoto,Kazumune Hashimoto,Masako Kishida,Shigemasa Takai*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u65e0\u53c2\u8003\u8fed\u4ee3\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u57fa\u4e8eCLBF\u7684\u8bc1\u4e66\u51fd\u6570\u6765\u5b9a\u4e49MPC\u7684\u7ec8\u7aef\u96c6\u5408\u548c\u6210\u672c\uff0c\u5e76\u5c06\u5176\u8868\u8ff0\u4e3a\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u63a7\u5236\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u8fed\u4ee3\u5b66\u4e60MPC\u65b9\u6cd5\u5e38\u4f9d\u8d56\u6df7\u5408\u6574\u6570\u89c4\u5212\uff0c\u5bfc\u81f4\u6570\u503c\u56f0\u96be\u548c\u5728\u7ebf\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002

Method: \u8be5\u65b9\u6cd5\u63d0\u51fa\u4e00\u79cd\u65e0\u53c2\u8003\u8fed\u4ee3\u5b66\u4e60MPC\u3002\u5b83\u5229\u7528\u4ece\u8fc7\u53bb\u63a7\u5236\u6267\u884c\u4e2d\u6536\u96c6\u7684\u6570\u636e\u5b66\u4e60\u4e00\u4e2a\u57fa\u4e8e\u63a7\u5236Lyapunov\u969c\u788d\u51fd\u6570\uff08CLBF\uff09\u7684\u8bc1\u4e66\u51fd\u6570\uff0c\u5e76\u7528\u8be5\u51fd\u6570\u5b9a\u4e49\u5f53\u524d\u8fed\u4ee3\u4e2dMPC\u4f18\u5316\u95ee\u9898\u7684\u7ec8\u7aef\u96c6\u5408\u548c\u6210\u672c\u3002MPC\u4f18\u5316\u95ee\u9898\u88ab\u8868\u8ff0\u4e3a\u4e00\u4e2a\u6807\u51c6\u7684\u975e\u7ebf\u6027\u89c4\u5212\uff08NLP\uff09\u3002

Result: \u8be5\u65b9\u6848\u5b9e\u73b0\u4e86MPC\u7ec8\u7aef\u7ec4\u4ef6\u7684\u9010\u6b21\u8fed\u4ee3\u6539\u8fdb\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u8fdb\u884c\u5728\u7ebf\u8ba1\u7b97\uff0c\u5e76\u6ee1\u8db3MPC\u7684\u5173\u952e\u7279\u6027\uff0c\u5305\u62ec\u9012\u5f52\u53ef\u884c\u6027\u548c\u6e10\u8fd1\u7a33\u5b9a\u6027\u3002\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\uff0c\u6027\u80fd\u6210\u672c\u968f\u8fed\u4ee3\u6b21\u6570\u975e\u9012\u589e\u3002\u6570\u503c\u5b9e\u9a8c\uff08\u5305\u62ecPyBullet\u4eff\u771f\uff09\u8bc1\u5b9e\u4e86\u5176\u63a7\u5236\u6027\u80fd\u7684\u8fed\u4ee3\u589e\u5f3a\u548c\u5728\u7ebf\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002

Conclusion: \u6240\u63d0\u51fa\u7684\u65e0\u53c2\u8003\u8fed\u4ee3\u5b66\u4e60MPC\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u57fa\u4e8eCLBF\u7684\u8bc1\u4e66\u51fd\u6570\u548c\u91c7\u7528\u975e\u7ebf\u6027\u89c4\u5212\u8868\u8ff0\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u63a7\u5236\u6027\u80fd\u7684\u6301\u7eed\u63d0\u5347\u548c\u5173\u952eMPC\u5c5e\u6027\u7684\u6ee1\u8db3\u3002

Abstract: In this paper, we propose a novel reference-free iterative learning model
predictive control (MPC). In the proposed method, a certificate function based
on the concept of Control Lyapunov Barrier Function (CLBF) is learned using
data collected from past control executions and used to define the terminal set
and cost in the MPC optimization problem at the current iteration. This scheme
enables the progressive refinement of the MPC's terminal components over
successive iterations. Unlike existing methods that rely on mixed-integer
programming and suffer from numerical difficulties, the proposed approach
formulates the MPC optimization problem as a standard nonlinear program,
enabling more efficient online computation. The proposed method satisfies key
MPC properties, including recursive feasibility and asymptotic stability.
Additionally, we demonstrate that the performance cost is non-increasing with
respect to the number of iterations, under certain assumptions. Numerical
experiments including the simulation with PyBullet confirm that our control
scheme iteratively enhances control performance and significantly improves
online computational efficiency compared to the existing methods.

</details>


### [181] [Physics-guided gated recurrent units for inversion-based feedforward control](https://arxiv.org/abs/2507.14052)
*Mingdao Lin,Max Bolderman,Mircea Lazar*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08PG-GRU\uff09\u7684\u524d\u9988\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6b8b\u5dee\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u8d28\u91cf\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u7684\u63a7\u5236\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u4f20\u7edf\u7684\u57fa\u4e8e\u9006\u6a21\u578b\u7684\u63a7\u5236\u9700\u8981\u7cbe\u786e\u7684\u7cfb\u7edf\u9006\u52a8\u529b\u5b66\u6a21\u578b\u3002\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u662f\u83b7\u53d6\u6b64\u7c7b\u6a21\u578b\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u5176\u9ed1\u7bb1\u7279\u6027\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u5dee\u4e14\u6613\u8fc7\u62df\u5408\u3002\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\uff08PGNNs\uff09\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u7269\u7406\u6a21\u578b\u7ed3\u6784\uff0c\u80fd\u6539\u5584\u8bad\u7ec3\u6536\u655b\u5e76\u4fc3\u8fdb\u7269\u7406\u6a21\u578b\u7684\u5b66\u4e60\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u5c06GRU\u4e0ePGNN\u7ed3\u5408\uff0c\u89e3\u51b3GRU\u5728\u9006\u7cfb\u7edf\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\u3002

Method: \u7814\u7a76\u5c06GRU\u6574\u5408\u5230PGNN\u6846\u67b6\u4e2d\uff0c\u5f62\u6210PG-GRU\u3002\u91c7\u7528\u4e24\u6b65\u6cd5\u8bbe\u8ba1\u524d\u9988\u63a7\u5236\u5668\uff1a\u9996\u5148\uff0c\u5229\u7528\u7a33\u5b9a\u9006\u6280\u672f\u8bbe\u8ba1\u4e00\u4e2a\u7a33\u5b9a\u7684\u7ebf\u6027\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff1b\u7136\u540e\uff0c\u8bad\u7ec3\u4e00\u4e2aGRU\u6765\u8bc6\u522b\u6b8b\u5dee\uff08\u5b9e\u9645\u7cfb\u7edf\u4e0e\u7ebf\u6027\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u7684\u9006\u7cfb\u7edf\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u5728\u53cc\u8d28\u91cf\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u4e0a\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002

Result: \u6240\u63d0\u51fa\u7684PG-GRU\u524d\u9988\u63a7\u5236\u5668\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u7ebf\u6027\u524d\u9988\u548c\u57fa\u4e8e\u9884\u89c8\u7684GRU\u524d\u9988\uff0c\u5176\u79ef\u5206\u7edd\u5bf9\u8bef\u5dee\uff08IAE\uff09\u8868\u73b0\u51fa\u5927\u7ea6\u4e24\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002

Conclusion: \u901a\u8fc7\u5c06GRU\u4e0e\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u7ed3\u5408\uff0c\u5e76\u91c7\u7528\u4e24\u6b65\u6cd5\u8fdb\u884c\u9006\u7cfb\u7edf\u8bc6\u522b\uff0cPG-GRU\u524d\u9988\u63a7\u5236\u5668\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfGRU\u5728\u6a21\u578b\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u63a7\u5236\u7cbe\u5ea6\u3002

Abstract: Inversion-based feedforward control relies on an accurate model that
describes the inverse system dynamics. The gated recurrent unit (GRU), which is
a recent architecture in recurrent neural networks, is a strong candidate for
obtaining such a model from data. However, due to their black-box nature, GRUs
face challenges such as limited interpretability and vulnerability to
overfitting. Recently, physics-guided neural networks (PGNNs) have been
introduced, which integrate the prior physical model structure into the
prediction process. This approach not only improves training convergence, but
also facilitates the learning of a physics-based model. In this work, we
integrate a GRU in the PGNN framework to obtain a PG-GRU, based on which we
adopt a two-step approach to feedforward control design. First, we adopt stable
inversion techniques to design a stable linear model of the inverse dynamics.
Then, a GRU trained on the residual is tailored to inverse system
identification. The resulting PG-GRU feedforward controller is validated by
means of real-life experiments on a two-mass spring-damper system, where it
demonstrates roughly a two-fold improvement compared to the linear feedforward
and a preview-based GRU feedforward in terms of the integral absolute error.

</details>


### [182] [Convex computation of regions of attraction from data using Sums-of-Squares programming](https://arxiv.org/abs/2507.14073)
*Oumayma Khattabi,Matteo Tacchi-Bénard,Sorin Olaru*

Main category: eess.SY

TL;DR: \u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u77e9-\u5e73\u65b9\u548c\u5c42\u6b21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u672a\u77e5\u81ea\u6cbb\u52a8\u529b\u7cfb\u7edf\u7684\u5438\u5f15\u57df\u8fdb\u884c\u5916\u90e8\u8fd1\u4f3c\uff0c\u6210\u529f\u89c4\u907f\u4e86\u5bf9\u7cfb\u7edf\u6a21\u578b\u53ca\u5176\u591a\u9879\u5f0f\u7ed3\u6784\u7684\u4f9d\u8d56\u3002


<details>
  <summary>Details</summary>
Motivation: \u7814\u7a76\u672a\u77e5\u81ea\u6cbb\u52a8\u529b\u7cfb\u7edf\u7684\u5438\u5f15\u57df\uff08ROA\uff09\uff0c\u5e76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5bf9\u7cfb\u7edf\u6a21\u578b\u53ca\u5176\u591a\u9879\u5f0f\u7ed3\u6784\u4fe1\u606f\u7684\u4f9d\u8d56\u548c\u9650\u5236\u3002

Method: \u91c7\u7528\u57fa\u4e8e\u77e9-\u5e73\u65b9\u548c\uff08moment-sum-of-squares, SoS\uff09\u5c42\u6b21\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u80fd\u591f\u7ed5\u8fc7\u5bf9\u7cfb\u7edf\u6a21\u578b\u7684\u663e\u5f0f\u4f9d\u8d56\uff0c\u4ece\u800c\u6446\u8131\u4e86\u5bf9\u5176\u591a\u9879\u5f0f\u7ed3\u6784\u7684\u5e38\u89c4\u7ea6\u675f\uff0c\u5b9e\u73b0\u65b0\u9896\u7684ROA\u5916\u90e8\u8fd1\u4f3c\u3002

Result: \u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6570\u636e\u5bf9\u5b66\u4e60\u5230\u7684\u8fd1\u4f3c\u96c6\u5408\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002

Conclusion: \u8be5\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5206\u6790\u672a\u77e5\u52a8\u529b\u7cfb\u7edf\u5438\u5f15\u57df\u65b9\u9762\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u548c\u6f5c\u529b\u3002

Abstract: The paper concentrates on the analysis of the region of attraction (ROA) for
unknown autonomous dynamical systems. The aim is to explore a data-driven
approach based on moment-sum-of-squares (SoS) hierarchy, which enables novel
RoA outer approximations despite the reduced information on the structure of
the dynamics. The main contribution of this work is bypassing the system model
and, consequently, the recurring constraint on its polynomial structure.
Numerical experimentation showcases the influence of data on learned
approximating sets, offering a promising outlook on the potential of this
method.

</details>


### [183] [Integrating Forecasting Models Within Steady-State Analysis and Optimization](https://arxiv.org/abs/2507.14117)
*Aayushya Agarwal,Larry Pileggi*

Main category: eess.SY

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u65e0\u7f1d\u5d4c\u5165\u7269\u7406\u7535\u529b\u6d41\u548c\u7535\u7f51\u4f18\u5316\u5de5\u5177\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u7535\u7f51\u5bf9\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u6781\u7aef\u5929\u6c14\u548c\u8d1f\u8377\u53d8\u5316\uff09\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u6a21\u578b\u83b7\u5f97\u7075\u654f\u5ea6\u4fe1\u606f\uff0c\u4ece\u800c\u4f18\u5316\u7535\u529b\u8c03\u5ea6\u5e76\u63d0\u9ad8\u7535\u7f51\u53ef\u9760\u6027\u3002


<details>
  <summary>Details</summary>
Motivation: \u6781\u7aef\u5929\u6c14\u53d8\u5316\u548c\u8d1f\u8377\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u4f7f\u5f97\u786e\u5b9a\u5bf9\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\u7684\u7535\u7f51\u8c03\u5ea6\u53d8\u5f97\u56f0\u96be\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u8d1f\u8377\u548c\u53ef\u518d\u751f\u80fd\u6e90\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u5982\u4f55\u5c06\u8fd9\u4e9b\u9884\u6d4b\u53ca\u5176\u7075\u654f\u5ea6\u51c6\u786e\u5730\u6574\u5408\u5230\u7a33\u6001\u5206\u6790\u548c\u51b3\u7b56\u5236\u5b9a\u7b56\u7565\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002

Method: \u8be5\u65b9\u6cd5\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u5f15\u64ce\u65e0\u7f1d\u5d4c\u5165\u57fa\u4e8e\u7269\u7406\u7684\u7535\u529b\u6d41\u548c\u7535\u7f51\u4f18\u5316\u5de5\u5177\u4e2d\u3002\u901a\u8fc7\u5c06\u57fa\u4e8e\u7269\u7406\u7684\u7535\u7f51\u5efa\u6a21\u4e0e\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\uff0c\u76f4\u63a5\u5c06\u8bad\u7ec3\u597d\u7684\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6574\u5408\u5230\u7535\u529b\u6d41\u548c\u7535\u7f51\u4f18\u5316\u7684\u6570\u503c\u65b9\u6cd5\u4e2d\u3002\u5b83\u65e0\u9700\u62df\u5408\u66ff\u4ee3\u8d1f\u8377\u6a21\u578b\uff0c\u800c\u662f\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u83b7\u53d6\u7075\u654f\u5ea6\uff0c\u5e76\u7ed3\u5408\u9884\u6d4b\u8bbe\u5907\uff08\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u83b7\u5f97\uff09\u548c\u7269\u7406\u5b9a\u4e49\u7535\u7f51\u8bbe\u5907\u7684\u7075\u654f\u5ea6\u3002

Result: \u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u7075\u654f\u5ea6\u8ba1\u7b97\u7684\u6539\u8fdb\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7075\u654f\u5ea6\u8bbe\u8ba1\u4e86\u9c81\u68d2\u7684\u7535\u529b\u8c03\u5ea6\uff0c\u4ece\u800c\u5728\u968f\u673a\u5929\u6c14\u4e8b\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u7535\u7f51\u7684\u53ef\u9760\u6027\u3002\u5b83\u8fd8\u80fd\u591f\u8ba1\u7b97\u7cfb\u7edf\u5bf9\u5916\u90e8\u56e0\u7d20\u7684\u7075\u654f\u5ea6\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5206\u6790\uff0c\u4ee5\u63d0\u9ad8\u5728\u8d1f\u8377\u53ef\u53d8\u6027\u548c\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u7535\u7f51\u53ef\u9760\u6027\u3002

Conclusion: \u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u7269\u7406\u7535\u7f51\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u8d1f\u8377\u548c\u5929\u6c14\u4e8b\u4ef6\u7684\u884c\u4e3a\u548c\u7075\u654f\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u9c81\u68d2\u7684\u7535\u529b\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u7535\u7f51\u5728\u8d1f\u8377\u53d8\u5316\u548c\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u652f\u6301\u66f4\u6df1\u5165\u7684\u7cfb\u7edf\u7075\u654f\u5ea6\u5206\u6790\u3002

Abstract: Extreme weather variations and the increasing unpredictability of load
behavior make it difficult to determine power grid dispatches that are robust
to uncertainties. While machine learning (ML) methods have improved the ability
to model uncertainty caused by loads and renewables, accurately integrating
these forecasts and their sensitivities into steady-state analyses and
decision-making strategies remains an open challenge. Toward this goal, we
present a generalized methodology that seamlessly embeds ML-based forecasting
engines within physics-based power flow and grid optimization tools. By
coupling physics-based grid modeling with black-box ML methods, we accurately
capture the behavior and sensitivity of loads and weather events by directly
integrating the inputs and outputs of trained ML forecasting models into the
numerical methods of power flow and grid optimization. Without fitting
surrogate load models, our approach obtains the sensitivities directly from
data to accurately predict the response of forecasted devices to changes in the
grid. Our approach combines the sensitivities of forecasted devices attained
via backpropagation and the sensitivities of physics-defined grid devices. We
demonstrate the efficacy of our method by showcasing improvements in
sensitivity calculations and leveraging them to design a robust power dispatch
that improves grid reliability under stochastic weather events. Our approach
enables the computation of system sensitivities to exogenous factors which
supports broader analyses that improve grid reliability in the presence of load
variability and extreme weather conditions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [184] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: \u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63a2\u7a76\u4e86Vision Mamba\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u5c062D\u56fe\u50cf\u5e8f\u5217\u5316\u4e3a1D\u65f6\uff0c\u56fe\u50cf\u5757\u626b\u63cf\u987a\u5e8f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u5176\u662f\u4e00\u4e2a\u663e\u8457\u4e14\u514d\u8d39\u7684\u8d85\u53c2\u6570\uff0c\u5176\u4e2d\u7b80\u5355\u7684\u6805\u683c\u626b\u63cf\u8def\u5f84\u8868\u73b0\u6700\u4f73\u3002


<details>
  <summary>Details</summary>
Motivation: Vision Mamba\u6a21\u578b\u4ee5\u7ebf\u6027\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0Transformer\u7ea7\u522b\u7684\u6027\u80fd\uff0c\u4f46\u5728\u5c062D\u56fe\u50cf\u5e8f\u5217\u5316\u4e3a1D\u5e8f\u5217\u65f6\uff0c\u56fe\u50cf\u5757\u7684\u626b\u63cf\u987a\u5e8f\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5e38\u88ab\u5ffd\u89c6\u7684\u8bbe\u8ba1\u9009\u62e9\u3002\u5c24\u5176\u5728\u5177\u6709\u5f3a\u89e3\u5256\u5148\u9a8c\u7684\u533b\u5b66\u5f71\u50cf\u4e2d\uff0c\u8fd9\u4e00\u9009\u62e9\u5e76\u975e\u5fae\u4e0d\u8db3\u9053\u3002

Method: \u5f15\u5165\u4e86Multi-Scan 2D (MS2D)\u6a21\u5757\uff0c\u4e00\u4e2a\u65e0\u53c2\u6570\u7684Mamba\u67b6\u6784\u6a21\u5757\uff0c\u7528\u4e8e\u63a2\u7d22\u4e0d\u540c\u7684\u626b\u63cf\u8def\u5f84\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf921\u79cd\u626b\u63cf\u7b56\u7565\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6d89\u53ca\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08BraTS 2020, ISLES 2022, LGG\uff09\u8d85\u8fc770,000\u4e2a\u5207\u7247\u3002\u4f7f\u7528Friedman\u68c0\u9a8c\u5206\u6790\u626b\u63cf\u987a\u5e8f\u7684\u7edf\u8ba1\u663e\u8457\u6027\u3002

Result: \u626b\u63cf\u987a\u5e8f\u662f\u4e00\u4e2a\u7edf\u8ba1\u4e0a\u663e\u8457\u7684\u56e0\u7d20\uff08Friedman\u68c0\u9a8c: $\chi^{2}_{20}=43.9, p=0.0016$\uff09\uff0c\u6027\u80fd\u5dee\u5f02\u9ad8\u8fbe27\u4e2aDice\u70b9\u3002\u7a7a\u95f4\u8fde\u7eed\u7684\u8def\u5f84\uff08\u7b80\u5355\u7684\u6c34\u5e73\u548c\u5782\u76f4\u6805\u683c\u626b\u63cf\uff09\u59cb\u7ec8\u4f18\u4e8e\u4e0d\u8fde\u7eed\u7684\u5bf9\u89d2\u7ebf\u626b\u63cf\u3002

Conclusion: \u626b\u63cf\u987a\u5e8f\u662fMamba\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u4e00\u4e2a\u5f3a\u5927\u4e14\u96f6\u6210\u672c\u7684\u8d85\u53c2\u6570\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6700\u4f73\u8def\u5f84\u5217\u8868\uff0c\u4ee5\u6700\u5927\u5316Mamba\u6a21\u578b\u7684\u6027\u80fd\u3002

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [185] [Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning](https://arxiv.org/abs/2507.13394)
*Akhil John Thomas,Christiaan Boerkamp*

Main category: eess.IV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u5e76\u4f18\u5316\u4e86\u4e00\u4e2a\u57fa\u4e8eDeepLabV3\u7684\u795e\u7ecf\u5206\u5272\u6d41\u7a0b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9608\u503c\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u58f0\u795e\u7ecf\u56fe\u50cf\u7684\u5206\u5272\u7cbe\u5ea6\u3002


<details>
  <summary>Details</summary>
Motivation: \u5728\u533b\u5b66\u6210\u50cf\u4e2d\uff0c\u795e\u7ecf\u5206\u5272\u5bf9\u4e8e\u7cbe\u786e\u8bc6\u522b\u795e\u7ecf\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002

Method: \u91c7\u7528\u4f18\u5316\u7684DeepLabV3\u5206\u5272\u6d41\u7a0b\uff0c\u5e76\u6574\u5408\u4e86\u81ea\u52a8\u5316\u9608\u503c\u5fae\u8c03\u529f\u80fd\u3002\u540c\u65f6\uff0c\u6539\u8fdb\u4e86\u9884\u5904\u7406\u6b65\u9aa4\u5e76\u5b9e\u65bd\u4e86\u53c2\u6570\u4f18\u5316\u3002

Result: \u5728\u8d85\u58f0\u795e\u7ecf\u6210\u50cf\u4e0a\uff0cDice Score\u8fbe\u52300.78\uff0cIoU\u8fbe\u52300.70\uff0c\u50cf\u7d20\u51c6\u786e\u7387\u8fbe\u52300.95\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u81ea\u52a8\u5316\u795e\u7ecf\u68c0\u6d4b\u4e2d\uff0c\u91cf\u8eab\u5b9a\u5236\u7684\u53c2\u6570\u9009\u62e9\u5bf9\u4e8e\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\u3002

Abstract: Nerve segmentation is crucial in medical imaging for precise identification
of nerve structures. This study presents an optimized DeepLabV3-based
segmentation pipeline that incorporates automated threshold fine-tuning to
improve segmentation accuracy. By refining preprocessing steps and implementing
parameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a
Pixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate
significant improvements over baseline models and highlight the importance of
tailored parameter selection in automated nerve detection.

</details>


### [186] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: \u6df1\u5ea6\u5b66\u4e60\u5728\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u9650\u5236\u800c\u7f3a\u4e4f\u6cdb\u5316\u6027\uff0c\u672c\u6587\u56de\u987e\u4e86\u4e00\u79cd\u9886\u57df\u968f\u673a\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u6001\u548c\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5b9e\u65bd\u4e0e\u5e94\u7528\u3002


<details>
  <summary>Details</summary>
Motivation: \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u72ed\u7a84\u8303\u56f4\uff0c\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728MRI\u4e2d\uff0c\u56fe\u50cf\u5916\u89c2\u5dee\u5f02\u5927\uff0c\u8fd9\u4e00\u6311\u6218\u66f4\u4e3a\u7a81\u51fa\u3002

Method: \u91c7\u7528\u9886\u57df\u968f\u673a\u5316\u7b56\u7565\uff1a\u901a\u8fc7\u89e3\u5256\u5206\u5272\u56fe\u751f\u6210\u5177\u6709\u968f\u673a\u5f3a\u5ea6\u548c\u89e3\u5256\u5185\u5bb9\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u7528\u8fd9\u4e9b\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002

Result: \u8be5\u65b9\u6cd5\u4f7f\u6a21\u578b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u51c6\u786e\u5904\u7406\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u56fe\u50cf\u7c7b\u578b\u3002\u5b83\u5728MRI\u3001CT\u3001PET\u3001OCT\u3001\u8d85\u58f0\u3001\u7535\u5b50\u548c\u8367\u5149\u663e\u5fae\u955c\u3001X\u5c04\u7ebf\u663e\u5fae\u65ad\u5c42\u626b\u63cf\u7b49\u591a\u79cd\u6a21\u6001\u548c\u795e\u7ecf\u5f71\u50cf\u4e4b\u5916\u7684\u9886\u57df\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u589e\u5f3a\u4e86\u6297\u8fc7\u62df\u5408\u6027\u3002

Conclusion: \u5408\u6210\u9a71\u52a8\u7684\u8bad\u7ec3\u8303\u5f0f\u867d\u7136\u8ba1\u7b97\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6297\u8fc7\u62df\u5408\u6027\u3002\u8be5\u6280\u672f\u65e8\u5728\u52a0\u901f\u901a\u7528\u5316\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u7684\u5f00\u53d1\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u6ca1\u6709\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u6216\u673a\u5668\u5b66\u4e60\u77e5\u8bc6\u7684\u9886\u57df\u4e13\u5bb6\u4f7f\u7528\u3002

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [187] [BreastSegNet: Multi-label Segmentation of Breast MRI](https://arxiv.org/abs/2507.13604)
*Qihang Li,Jichen Yang,Yaqian Chen,Yuwen Chen,Hanxue Gu,Lars J. Grimm,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86BreastSegNet\uff0c\u4e00\u79cd\u7528\u4e8e\u4e73\u817aMRI\u7684\u591a\u6807\u7b7e\u5206\u5272\u7b97\u6cd5\uff0c\u6db5\u76d6\u4e5d\u79cd\u89e3\u5256\u7ed3\u6784\uff0c\u5e76\u5bf9\u591a\u79cd\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2dnnU-Net ResEncM\u8868\u73b0\u6700\u4f73\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u4e73\u817aMRI\u5206\u5272\u65b9\u6cd5\u8303\u56f4\u6709\u9650\uff0c\u901a\u5e38\u53ea\u5173\u6ce8\u5c11\u6570\u7ed3\u6784\uff08\u5982\u7ea4\u7ef4\u817a\u4f53\u7ec4\u7ec7\u6216\u80bf\u7624\uff09\uff0c\u672a\u80fd\u8986\u76d6\u626b\u63cf\u4e2d\u7684\u6240\u6709\u7ec4\u7ec7\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u5b9a\u91cf\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u3002

Method: \u7814\u7a76\u63d0\u51fa\u4e86BreastSegNet\uff0c\u4e00\u79cd\u7528\u4e8e\u4e73\u817aMRI\u7684\u591a\u6807\u7b7e\u5206\u5272\u7b97\u6cd5\uff0c\u53ef\u5206\u5272\u4e5d\u79cd\u89e3\u5256\u7ed3\u6784\uff1a\u7ea4\u7ef4\u817a\u4f53\u7ec4\u7ec7\u3001\u8840\u7ba1\u3001\u808c\u8089\u3001\u9aa8\u9abc\u3001\u75c5\u53d8\u3001\u6dcb\u5df4\u7ed3\u3001\u5fc3\u810f\u3001\u809d\u810f\u548c\u690d\u5165\u7269\u3002\u4e3a\u6b64\uff0c\u624b\u52a8\u6807\u6ce8\u4e861123\u5f20MRI\u5207\u7247\uff0c\u5e76\u7ecf\u8fc7\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u5e08\u7684\u8be6\u7ec6\u5ba1\u67e5\u548c\u6821\u6b63\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u5305\u62ecU-Net\u3001SwinUNet\u3001UNet++\u3001SAM\u3001MedSAM\u4ee5\u53ca\u5e26\u6709\u591a\u79cdResNet\u7f16\u7801\u5668\u7684nnU-Net\u5728\u5185\u7684\u4e5d\u79cd\u5206\u5272\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002

Result: \u5728\u6240\u6709\u6807\u7b7e\u4e0a\uff0cnnU-Net ResEncM\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747Dice\u5206\u6570\uff080.694\uff09\u3002\u5b83\u5728\u5fc3\u810f\u3001\u809d\u810f\u3001\u808c\u8089\u3001\u7ea4\u7ef4\u817a\u4f53\u7ec4\u7ec7\u548c\u9aa8\u9abc\u4e0a\u7684\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0cDice\u5206\u6570\u8d85\u8fc70.73\uff0c\u5fc3\u810f\u548c\u809d\u810f\u63a5\u8fd10.90\u3002\u6240\u6709\u6a21\u578b\u4ee3\u7801\u548c\u6743\u91cd\u5747\u5df2\u516c\u5f00\u3002

Conclusion: BreastSegNet\uff08\u7279\u522b\u662f\u91c7\u7528nnU-Net ResEncM\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u7684\u4e73\u817aMRI\u591a\u6807\u7b7e\u5206\u5272\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u91cf\u5206\u6790\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u5728\u591a\u79cd\u89e3\u5256\u7ed3\u6784\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5206\u5272\u6027\u80fd\u3002

Abstract: Breast MRI provides high-resolution imaging critical for breast cancer
screening and preoperative staging. However, existing segmentation methods for
breast MRI remain limited in scope, often focusing on only a few anatomical
structures, such as fibroglandular tissue or tumors, and do not cover the full
range of tissues seen in scans. This narrows their utility for quantitative
analysis. In this study, we present BreastSegNet, a multi-label segmentation
algorithm for breast MRI that covers nine anatomical labels: fibroglandular
tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and
implant. We manually annotated a large set of 1123 MRI slices capturing these
structures with detailed review and correction from an expert radiologist.
Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet,
UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among
them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across
all labels. It performs especially well on heart, liver, muscle, FGT, and bone,
with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All
model code and weights are publicly available, and we plan to release the data
at a later date.

</details>


### [188] [Converting T1-weighted MRI from 3T to 7T quality using deep learning](https://arxiv.org/abs/2507.13782)
*Malo Gicquel,Ruoyi Zhao,Anika Wuestefeld,Nicola Spotorno,Olof Strandberg,Kalle Åström,Yu Xiao,Laura EM Wisse,Danielle van Westen,Rik Ossenkoppele,Niklas Mattsson-Carlgren,David Berron,Oskar Hansson,Gabrielle Flood,Jacob Vogel*

Main category: eess.IV

TL;DR: \u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u591f\u4ece3T\u8111\u90e8MRI\u56fe\u50cf\u5408\u6210\u63a5\u8fd17T\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u7cbe\u5ea6\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002


<details>
  <summary>Details</summary>
Motivation: 7T MRI\u6bd43T MRI\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u4fe1\u566a\u6bd4\u3001\u5206\u8fa8\u7387\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\uff0c\u4f46\u5176\u53ef\u53ca\u6027\u8f83\u4f4e\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6280\u672f\uff0c\u5728\u4e0d\u727a\u7272\u53ef\u53ca\u6027\u7684\u524d\u63d0\u4e0b\u83b7\u5f977T\u56fe\u50cf\u7684\u4f18\u52bf\u3002

Method: \u7814\u7a76\u4f7f\u7528\u4e86\u6765\u81eaBioFINDER-2\u7814\u7a76\u7684172\u540d\u53c2\u4e0e\u8005\u7684\u914d\u5bf97T\u548c3T T1\u52a0\u6743\u8111\u90e8MRI\u56fe\u50cf\u3002\u8bad\u7ec3\u4e86\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1a\u4e00\u4e2a\u4e13\u95e8\u7684U-Net\u548c\u4e00\u4e2a\u96c6\u6210\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684U-Net\uff08GAN U-Net\uff09\u3002\u901a\u8fc7\u56fe\u50cf\u5ea6\u91cf\u6307\u6807\u3001\u56db\u4f4d\u76f2\u5ba1MRI\u4e13\u4e1a\u4eba\u5458\u7684\u4e3b\u89c2\u8bc4\u4ef7\u3001\u674f\u4ec1\u6838\u81ea\u52a8\u5206\u5272\u4e0e\u624b\u52a8\u5206\u5272\u7684\u76f8\u4f3c\u6027\u6bd4\u8f83\uff0c\u4ee5\u53ca\u5728\u8ba4\u77e5\u72b6\u6001\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u6765\u8bc4\u4f30\u5408\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u3002

Result: \u7814\u7a76\u5f00\u53d1\u7684\u6a21\u578b\u5728\u56fe\u50cf\u5ea6\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u4e24\u79cd\u5148\u8fdb\u76843T-to-7T\u6a21\u578b\u3002\u76f2\u5ba1\u4e13\u4e1a\u4eba\u5458\u8ba4\u4e3a\u5408\u6210\u76847T\u56fe\u50cf\u5728\u7ec6\u8282\u4e0a\u4e0e\u771f\u5b9e7T\u56fe\u50cf\u76f8\u5f53\uff0c\u4e14\u5728\u4e3b\u89c2\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u771f\u5b9e7T\u56fe\u50cf\uff08\u53ef\u80fd\u662f\u7531\u4e8e\u4f2a\u5f71\u51cf\u5c11\uff09\u3002\u6b64\u5916\uff0c\u5408\u6210\u7684GAN U-Net 7T\u56fe\u50cf\u7684\u674f\u4ec1\u6838\u81ea\u52a8\u5206\u5272\u7ed3\u679c\u6bd4\u7528\u4e8e\u5408\u6210\u7684\u539f\u59cb3T\u56fe\u50cf\u7684\u5206\u5272\u7ed3\u679c\u66f4\u63a5\u8fd1\u624b\u52a8\u5206\u5272\u3002\u5728\u8ba4\u77e5\u72b6\u6001\u9884\u6d4b\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5408\u62107T\u56fe\u50cf\u7684\u8868\u73b0\u4e0e\u771f\u5b9e3T\u56fe\u50cf\u76f8\u4f3c\u3002

Conclusion: \u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u4ece3T\u56fe\u50cf\u751f\u6210\u63a5\u8fd17T\u8d28\u91cf\u7684\u5408\u6210T1\u52a0\u6743\u8111\u90e8\u56fe\u50cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u6539\u5584\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u5272\u6548\u679c\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5177\u6709\u6f5c\u5728\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002

Abstract: Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides
detailed anatomical views, offering better signal-to-noise ratio, resolution
and tissue contrast than 3T MRI, though at the cost of accessibility. We
present an advanced deep learning model for synthesizing 7T brain MRI from 3T
brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172
participants (124 cognitively unimpaired, 48 impaired) from the Swedish
BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:
a specialized U-Net, and a U-Net integrated with a generative adversarial
network (GAN U-Net). Our models outperformed two additional state-of-the-art
3T-to-7T models in image-based evaluation metrics. Four blinded MRI
professionals judged our synthetic 7T images as comparable in detail to real 7T
images, and superior in subjective visual quality to 7T images, apparently due
to the reduction of artifacts. Importantly, automated segmentations of the
amygdalae of synthetic GAN U-Net 7T images were more similar to manually
segmented amygdalae (n=20), than automated segmentations from the 3T images
that were used to synthesize the 7T images. Finally, synthetic 7T images showed
similar performance to real 3T images in downstream prediction of cognitive
status using MRI derivatives (n=3,168). In all, we show that synthetic
T1-weighted brain images approaching 7T quality can be generated from 3T
images, which may improve image quality and segmentation, without compromising
performance in downstream tasks. Future directions, possible clinical use
cases, and limitations are discussed.

</details>


### [189] [Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation](https://arxiv.org/abs/2507.13830)
*Maximilian Rokuss,Benjamin Hamm,Yannick Kirchhoff,Klaus Maier-Hein*

Main category: eess.IV

TL;DR: \u8be5\u7814\u7a76\u9996\u6b21\u516c\u5f00\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc713,000\u4e2a\u6ce8\u91ca\u75c5\u4f8b\u7684\u4e73\u817aMRI\u6570\u636e\u96c6\uff0c\u660e\u786e\u6807\u6ce8\u4e86\u5de6\u53f3\u4e73\u623f\u5206\u5272\u6807\u7b7e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u89e3\u51b3\u4e73\u817aMRI\u5206\u6790\u4e2d\u5b58\u5728\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5373\u7f3a\u4e4f\u5e26\u6709\u660e\u786e\u5de6\u53f3\u4e73\u623f\u5206\u5272\u6807\u7b7e\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u5973\u6027\u5065\u5eb7\u9886\u57df\u9ad8\u7ea7\u5de5\u5177\u7684\u5f00\u53d1\u3002

Method: \u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u4e73\u817aMRI\u6570\u636e\u96c6\uff08\u8d85\u8fc713,000\u4e2a\u5e26\u5de6\u53f3\u4e73\u623f\u5206\u5272\u6807\u7b7e\u7684\u6ce8\u91ca\u75c5\u4f8b\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u9c81\u68d2\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u5de6\u53f3\u4e73\u623f\u5206\u5272\u3002

Result: \u8be5\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u5f25\u8865\u4e86\u4e73\u817aMRI\u5206\u6790\u4e2d\u7684\u4e00\u9879\u5173\u952e\u7a7a\u767d\u3002

Conclusion: \u8fd9\u9879\u5de5\u4f5c\u4e3a\u5973\u6027\u5065\u5eb7\u9886\u57df\u9ad8\u7ea7\u5de5\u5177\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u586b\u8865\u4e86\u4e73\u817aMRI\u5206\u6790\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002

Abstract: We introduce the first publicly available breast MRI dataset with explicit
left and right breast segmentation labels, encompassing more than 13,000
annotated cases. Alongside this dataset, we provide a robust deep-learning
model trained for left-right breast segmentation. This work addresses a
critical gap in breast MRI analysis and offers a valuable resource for the
development of advanced tools in women's health. The dataset and trained model
are publicly available at: www.github.com/MIC-DKFZ/BreastDivider

</details>


### [190] [Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive](https://arxiv.org/abs/2507.13901)
*Lei Xu,Torkel B Brismar*

Main category: eess.IV

TL;DR: \u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aAnatomyArchive\u7684CT\u56fe\u50cf\u5206\u6790\u8f6f\u4ef6\u5305\uff0c\u57fa\u4e8eTotalSegmentator\u6a21\u578b\uff0c\u63d0\u4f9b\u81ea\u52a8\u89e3\u5256\u533a\u57df\u9009\u62e9\u3001\u5f71\u50cf\u7ec4\u5b66\u7279\u5f81\u63d0\u53d6\u3001\u53ef\u89c6\u5316\u4ee5\u53ca\u8f85\u52a9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u7b49\u529f\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u65e8\u5728\u63d0\u5347CT\u56fe\u50cf\u5206\u6790\u7684\u81ea\u52a8\u5316\u548c\u7cbe\u786e\u6027\uff0c\u7279\u522b\u662f\u8eab\u4f53\u6210\u5206\u5206\u6790\uff0c\u5e76\u4e3a\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\u5de5\u5177\u3002

Method: \u5f00\u53d1\u4e86\u57fa\u4e8eTotalSegmentator\u6a21\u578b\u7684AnatomyArchive\u8f6f\u4ef6\u5305\uff0c\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u89e3\u5256\u5206\u5272\u63a9\u819c\u7ba1\u7406\uff0c\u5b9e\u73b0\u4e86\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u81ea\u52a8\u76ee\u6807\u4f53\u79ef\u9009\u62e9\u4e0e\u6392\u9664\u3001\u81ea\u52a8\u8eab\u4f53\u4f53\u79ef\u88c1\u526a\u3001\u624b\u81c2\u68c0\u6d4b\u4e0e\u6392\u9664\u3002\u8be5\u5de5\u5177\u8fd8\u63d0\u4f9b\u9c81\u68d2\u7684\u4f53\u7d20\u7ea7\u5f71\u50cf\u7ec4\u5b66\u7279\u5f81\u63d0\u53d6\u3001\u7279\u5f81\u53ef\u89c6\u5316\u3001\u96c6\u6210\u7edf\u8ba1\u5206\u6790\u5de5\u5177\u94fe\uff0c\u4ee5\u53ca\u57fa\u4e8ePython\u7684GPU\u52a0\u901f\u7535\u5f71\u6e32\u67d3\u529f\u80fd\u3002

Result: \u6210\u529f\u5f00\u53d1\u4e86AnatomyArchive\u8f6f\u4ef6\u5305\uff0c\u5b9e\u73b0\u4e86CT\u56fe\u50cf\u7684\u81ea\u52a8\u89e3\u5256\u533a\u57df\u9009\u62e9\u4e0e\u6392\u9664\u3001\u7cbe\u786e\u7684\u8eab\u4f53\u6210\u5206\u5206\u6790\uff08\u5305\u62ec\u81ea\u52a8\u88c1\u526a\u548c\u624b\u81c2\u6392\u9664\uff09\u3001\u9ad8\u6548\u7684\u5206\u5272\u63a9\u819c\u7ba1\u7406\u3001\u5f3a\u5927\u7684\u5f71\u50cf\u7ec4\u5b66\u7279\u5f81\u63d0\u53d6\u4e0e\u53ef\u89c6\u5316\u3001\u96c6\u6210\u7edf\u8ba1\u5206\u6790\uff0c\u4ee5\u53ca\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u3002\u8be5\u5de5\u5177\u80fd\u591f\u6709\u6548\u8f85\u52a9\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u3002

Conclusion: AnatomyArchive\u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u3001\u9ad8\u6548\u7684CT\u56fe\u50cf\u5206\u6790\u8f6f\u4ef6\u5305\uff0c\u5176\u5f00\u6e90\u4ee3\u7801\u5c06\u53d1\u5e03\u7528\u4e8e\u7814\u7a76\u548c\u6559\u80b2\u76ee\u7684\uff0c\u6709\u671b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002

Abstract: We have developed a novel CT image analysis package named AnatomyArchive,
built on top of the recent full body segmentation model TotalSegmentator. It
provides automatic target volume selection and deselection capabilities
according to user-configured anatomies for volumetric upper- and lower-bounds.
It has a knowledge graph-based and time efficient tool for anatomy segmentation
mask management and medical image database maintenance. AnatomyArchive enables
automatic body volume cropping, as well as automatic arm-detection and
exclusion, for more precise body composition analysis in both 2D and 3D
formats. It provides robust voxel-based radiomic feature extraction, feature
visualization, and an integrated toolchain for statistical tests and analysis.
A python-based GPU-accelerated nearly photo-realistic segmentation-integrated
composite cinematic rendering is also included. We present here its software
architecture design, illustrate its workflow and working principle of
algorithms as well provide a few examples on how the software can be used to
assist development of modern machine learning models. Open-source codes will be
released at https://github.com/lxu-medai/AnatomyArchive for only research and
educational purposes.

</details>


### [191] [Blind Super Resolution with Reference Images and Implicit Degradation Representation](https://arxiv.org/abs/2507.13915)
*Huu-Phu Do,Po-Chih Hu,Hao-Chien Hsueh,Che-Kai Liu,Vu-Hoang Tran,Ching-Chun Huang*

Main category: eess.IV

TL;DR: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f2\u8d85\u5206\u8fa8\u7387\uff08BSR\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u9ad8\u5206\u8fa8\u7387\uff08HR\uff09\u53c2\u8003\u56fe\u50cf\u6765\u5efa\u7acb\u5c3a\u5ea6\u611f\u77e5\u7684\u9000\u5316\u6838\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u8d85\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e0b\u63d0\u5347\u6027\u80fd\u3002


<details>
  <summary>Details</summary>
Motivation: \u4ee5\u5f80\u7684\u76f2\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u76f4\u63a5\u4ece\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u8f93\u5165\u4f30\u8ba1\u9000\u5316\u6838\uff0c\u4f46\u8fd9\u4e9b\u9000\u5316\u6838\u672a\u80fd\u5145\u5206\u8003\u8651\u4e0b\u91c7\u6837\u56e0\u5b50\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u8d85\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e0b\u5e94\u7528\u76f8\u540c\u7684\u9000\u5316\u6838\u4e0d\u5207\u5b9e\u9645\u3002

Method: \u672c\u7814\u7a76\u5c06\u9000\u5316\u6838\u548c\u7f29\u653e\u56e0\u5b50\u89c6\u4e3aBSR\u4efb\u52a1\u7684\u5173\u952e\u8981\u7d20\u3002\u5b83\u5229\u7528\u4e0e\u76ee\u6807LR\u56fe\u50cf\u5185\u5bb9\u65e0\u5173\u7684HR\u53c2\u8003\u56fe\u50cf\uff0c\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u9000\u5316\u8fc7\u7a0b\uff0c\u5e76\u4ee5\u6b64\u751f\u6210\u989d\u5916\u7684LR-HR\u5bf9\u3002\u8fd9\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u8bad\u7ec3\u7a0b\u5e8f\u53ef\u5e94\u7528\u4e8e\u5df2\u8bad\u7ec3\u597d\u7684BSR\u6a21\u578b\u548c\u96f6\u6837\u672cBSR\u65b9\u6cd5\u3002

Result: \u8be5\u65b9\u6cd5\u5728\u4e24\u79cd\u573a\u666f\u4e0b\uff08\u5df2\u8bad\u7ec3\u597d\u7684BSR\u6a21\u578b\u548c\u96f6\u6837\u672cBSR\u65b9\u6cd5\uff09\u5747\u6301\u7eed\u4f18\u4e8e\u4ee5\u5f80\u7684\u65b9\u6cd5\u3002

Conclusion: \u7ed3\u5408\u5bf9\u6a21\u7cca\u6838\u548c\u7f29\u653e\u56e0\u5b50\u7684\u53cc\u91cd\u8003\u8651\u4ee5\u53ca\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u7684\u4f7f\u7528\uff0c\u672c\u65b9\u6cd5\u5728\u76f2\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6709\u6548\u6027\u3002

Abstract: Previous studies in blind super-resolution (BSR) have primarily concentrated
on estimating degradation kernels directly from low-resolution (LR) inputs to
enhance super-resolution. However, these degradation kernels, which model the
transition from a high-resolution (HR) image to its LR version, should account
for not only the degradation process but also the downscaling factor. Applying
the same degradation kernel across varying super-resolution scales may be
impractical. Our research acknowledges degradation kernels and scaling factors
as pivotal elements for the BSR task and introduces a novel strategy that
utilizes HR images as references to establish scale-aware degradation kernels.
By employing content-irrelevant HR reference images alongside the target LR
image, our model adaptively discerns the degradation process. It is then
applied to generate additional LR-HR pairs through down-sampling the HR
reference images, which are keys to improving the SR performance. Our
reference-based training procedure is applicable to proficiently trained blind
SR models and zero-shot blind SR methods, consistently outperforming previous
methods in both scenarios. This dual consideration of blur kernels and scaling
factors, coupled with the use of a reference image, contributes to the
effectiveness of our approach in blind super-resolution tasks.

</details>


### [192] [Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images](https://arxiv.org/abs/2507.13974)
*Jiaqi Lv,Yijie Zhu,Carmen Guadalupe Colin Tenorio,Brinder Singh Chohan,Mark Eastwood,Shan E Ahmed Raza*

Main category: eess.IV

TL;DR: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff08Virchow2\uff09\u548cEfficient-UNet\uff0c\u5b9e\u73b0\u4e86\u9ed1\u8272\u7d20\u7624H&E\u56fe\u50cf\u4e2d\u4e94\u79cd\u7ec4\u7ec7\u7c7b\u522b\u7684\u51c6\u786e\u5206\u5272\uff0c\u5e76\u5728PUMA\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002


<details>
  <summary>Details</summary>
Motivation: \u9ed1\u8272\u7d20\u7624\u7ec4\u7ec7\u5f62\u6001\u7684\u51c6\u786e\u8868\u5f81\u5bf9\u9884\u540e\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4eceH&E\u67d3\u8272\u5168\u73bb\u7247\u56fe\u50cf\u4e2d\u624b\u52a8\u5206\u5272\u7ec4\u7ec7\u533a\u57df\u8d39\u65f6\u8d39\u529b\u4e14\u6613\u53d7\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u53ef\u9760\u7684\u81ea\u52a8\u5316\u7ec4\u7ec7\u5206\u5272\u65b9\u6cd5\u3002

Method: \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u7528\u4e8e\u5206\u5272\u9ed1\u8272\u7d20\u7624H&E\u56fe\u50cf\u4e2d\u7684\u4e94\u79cd\u7ec4\u7ec7\u7c7b\u522b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5728310\u4e07\u5f20\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578bVirchow2\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5c06\u63d0\u53d6\u7684\u7279\u5f81\u4e0e\u539f\u59cbRGB\u56fe\u50cf\u878d\u5408\uff0c\u7136\u540e\u7531\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u5272\u7f51\u7edc\uff08Efficient-UNet\uff09\u5904\u7406\u4ee5\u751f\u6210\u51c6\u786e\u7684\u5206\u5272\u56fe\u3002

Result: \u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728PUMA Grand Challenge\u7684\u7ec4\u7ec7\u5206\u5272\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002

Conclusion: \u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u6574\u5408\u5230\u5206\u5272\u7f51\u7edc\u4e2d\u5177\u6709\u6f5c\u529b\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u52a0\u901f\u8ba1\u7b97\u75c5\u7406\u5b66\u5de5\u4f5c\u6d41\u7a0b\u3002

Abstract: Melanoma is an aggressive form of skin cancer with rapid progression and high
metastatic potential. Accurate characterisation of tissue morphology in
melanoma is crucial for prognosis and treatment planning. However, manual
segmentation of tissue regions from haematoxylin and eosin (H&E) stained
whole-slide images (WSIs) is labour-intensive and prone to inter-observer
variability, this motivates the need for reliable automated tissue segmentation
methods. In this study, we propose a novel deep learning network for the
segmentation of five tissue classes in melanoma H&E images. Our approach
leverages Virchow2, a pathology foundation model trained on 3.1 million
histopathology images as a feature extractor. These features are fused with the
original RGB images and subsequently processed by an encoder-decoder
segmentation network (Efficient-UNet) to produce accurate segmentation maps.
The proposed model achieved first place in the tissue segmentation task of the
PUMA Grand Challenge, demonstrating robust performance and generalizability.
Our results show the potential and efficacy of incorporating pathology
foundation models into segmentation networks to accelerate computational
pathology workflows.

</details>


### [193] [OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models](https://arxiv.org/abs/2507.13993)
*Ningyong Wu,Jinzhi Wang,Wenhong Zhao,Chenzhan Yu,Zhigang Xiu,Duwei Dai*

Main category: eess.IV

TL;DR: OrthoInsight\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u808b\u9aa8\u9aa8\u6298\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u548c\u62a5\u544a\u751f\u6210\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5927\u6a21\u578b\u3002


<details>
  <summary>Details</summary>
Motivation: \u533b\u7597\u5f71\u50cf\u6570\u636e\u91cf\u4e0d\u65ad\u589e\u957f\uff0c\u7279\u522b\u662fCT\u626b\u63cf\u4e2d\u7684\u808b\u9aa8\u9aa8\u6298\u8bca\u65ad\uff0c\u624b\u52a8\u89e3\u91ca\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u6025\u9700\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\u3002

Method: \u63d0\u51faOrthoInsight\u6846\u67b6\uff0c\u6574\u5408YOLOv9\u6a21\u578b\u8fdb\u884c\u9aa8\u6298\u68c0\u6d4b\uff0c\u5229\u7528\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u4e34\u5e8a\u4e0a\u4e0b\u6587\uff0c\u5e76\u5fae\u8c03LLaVA\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bca\u65ad\u62a5\u544a\uff0c\u7ed3\u5408CT\u56fe\u50cf\u89c6\u89c9\u7279\u5f81\u548c\u4e13\u5bb6\u6587\u672c\u6570\u636e\u3002

Result: \u572828,675\u5f20\u5e26\u6ce8\u91ca\u7684CT\u56fe\u50cf\u548c\u4e13\u5bb6\u62a5\u544a\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u8bca\u65ad\u51c6\u786e\u6027\u3001\u5185\u5bb9\u5b8c\u6574\u6027\u3001\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e34\u5e8a\u6307\u5bfc\u4ef7\u503c\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u5f97\u52064.28\uff0c\u4f18\u4e8eGPT-4\u548cClaude-3\u7b49\u6a21\u578b\u3002

Conclusion: \u672c\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u652f\u6301\u653e\u5c04\u79d1\u533b\u751f\uff0c\u6709\u671b\u53d8\u9769\u533b\u7597\u8bca\u65ad\u6d41\u7a0b\u3002

Abstract: The growing volume of medical imaging data has increased the need for
automated diagnostic tools, especially for musculoskeletal injuries like rib
fractures, commonly detected via CT scans. Manual interpretation is
time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep
learning framework for rib fracture diagnosis and report generation. It
integrates a YOLOv9 model for fracture detection, a medical knowledge graph for
retrieving clinical context, and a fine-tuned LLaVA language model for
generating diagnostic reports. OrthoInsight combines visual features from CT
images with expert textual data to deliver clinically useful outputs. Evaluated
on 28,675 annotated CT images and expert reports, it achieves high performance
across Diagnostic Accuracy, Content Completeness, Logical Coherence, and
Clinical Guidance Value, with an average score of 4.28, outperforming models
like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal
learning in transforming medical image analysis and providing effective support
for radiologists.

</details>


### [194] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: \u672c\u6587\u63d0\u51faDeep Dynamic Image Prior (D2IP)\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65e0\u76d1\u7763\u53c2\u6570\u70ed\u542f\u52a8\u3001\u65f6\u95f4\u53c2\u6570\u4f20\u64ad\u548c\u8f7b\u91cf\u7ea7\u91cd\u5efa\u9aa8\u5e72\u7f51\u7edc\uff0c\u663e\u8457\u52a0\u901f\u5e76\u4f18\u5316\u4e863D\u65f6\u95f4\u5e8f\u5217\u65ad\u5c42\u6210\u50cf\u4e2d\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u91cd\u5efa\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u5982Deep Image Prior, DIP\uff09\u5728\u65ad\u5c42\u6210\u50cf\u4e2d\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u4f9d\u8d56\u5927\u91cf\u7f51\u7edc\u53c2\u6570\u8fed\u4ee3\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u590d\u67423D\u6216\u65f6\u95f4\u5e8f\u5217\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002

Method: D2IP\u6846\u67b6\u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u7b56\u7565\uff1a1. \u65e0\u76d1\u7763\u53c2\u6570\u70ed\u542f\u52a8\uff08Unsupervised Parameter Warm-Start, UPWS\uff09\u4ee5\u52a0\u901f\u6536\u655b\uff1b2. \u65f6\u95f4\u53c2\u6570\u4f20\u64ad\uff08Temporal Parameter Propagation, TPP\uff09\u4ee5\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff1b3. \u5b9a\u5236\u5316\u7684\u8f7b\u91cf\u7ea7\u91cd\u5efa\u9aa8\u5e72\u7f51\u7edc3D-FastResUNet\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002

Result: D2IP\u5728\u6a21\u62df\u548c\u4e34\u5e8a\u80ba\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u76843D\u65f6\u95f4\u5e8f\u5217\u7535\u963b\u6297\u65ad\u5c42\u6210\u50cf\uff08tsEIT\uff09\u91cd\u5efa\u3002\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0cD2IP\u5728\u56fe\u50cf\u8d28\u91cf\u4e0a\u8868\u73b0\u66f4\u4f18\uff08\u5e73\u5747MSSIM\u63d0\u9ad824.8%\uff0cERR\u964d\u4f4e8.1%\uff09\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff08\u5feb7.1\u500d\uff09\u3002

Conclusion: D2IP\u6846\u67b6\u901a\u8fc7\u63d0\u9ad8\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u5728\u4e34\u5e8a\u52a8\u6001\u80ba\u90e8\u6210\u50cf\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [195] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: \u672c\u6587\u63d0\u51faUGPL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6e10\u8fdb\u5f0f\u5b66\u4e60\uff0c\u5b9e\u73b0CT\u56fe\u50cf\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u7684\u5206\u6790\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u75c5\u7406\u7279\u5f81\u3002


<details>
  <summary>Details</summary>
Motivation: \u73b0\u6709CT\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u5fae\u5999\u4e14\u7a7a\u95f4\u591a\u6837\u5316\u7684\u75c5\u7406\u7279\u5f81\uff0c\u901a\u5e38\u7edf\u4e00\u5904\u7406\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5176\u68c0\u6d4b\u9700\u8981\u805a\u7126\u5206\u6790\u7684\u5c40\u90e8\u5f02\u5e38\u7684\u80fd\u529b\u3002

Method: UGPL\u91c7\u7528\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u975e\u6781\u5927\u503c\u6291\u5236\u673a\u5236\uff08\u4fdd\u6301\u7a7a\u95f4\u591a\u6837\u6027\uff09\u6307\u5bfc\u4fe1\u606f\u6027\u8865\u4e01\u7684\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u7ec6\u5316\u7b56\u7565\uff0c\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u8fdb\u884c\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u7684\u5206\u6790\u3002

Result: UGPL\u5728\u4e09\u4e2aCT\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u80be\u810f\u5f02\u5e38\u3001\u80ba\u764c\u548cCOVID-19\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0a\u5206\u522b\u63d0\u9ad8\u4e863.29%\u30012.46%\u548c8.08%\u3002\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7ec4\u4ef6\u548c\u5b8c\u6574\u7684\u6e10\u8fdb\u5f0f\u5b66\u4e60\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002

Conclusion: UGPL\u901a\u8fc7\u5176\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u6e10\u8fdb\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u56fe\u50cf\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u5fae\u5999\u548c\u5c40\u90e8\u5316\u7684\u75c5\u7406\u7279\u5f81\uff0c\u5e76\u5728\u591a\u4e2a\u533b\u7597\u6761\u4ef6\u4e0b\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>
