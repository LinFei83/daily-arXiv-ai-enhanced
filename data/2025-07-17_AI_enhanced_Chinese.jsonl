{"id": "2507.11595", "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11595", "abs": "https://arxiv.org/abs/2507.11595", "authors": ["Hengyue Zhao"], "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "comment": null, "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems.", "AI": {"tldr": "探讨AI能否推动人类与自然从支配关系转向相互依存，并通过案例研究展示AI在生态设计中的应用。", "motivation": "研究人类与自然关系是否能从支配转向相互依存，以及AI如何作为中介推动这一转变。", "method": "通过案例研究分析AI在数据、图像识别和生态修复中的应用，并结合原型设计提出AI与植物修复结合的方法。", "result": "AI不仅扩展了创意方法，还重构了生态设计的理论与实践，展示了其在科学、艺术和环保中的潜力。", "conclusion": "AI为可持续技术生态系统提供了研究路径，未来可进一步探索其在生态设计中的应用。"}}
{"id": "2507.11633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11633", "abs": "https://arxiv.org/abs/2507.11633", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "comment": "8 pages, ICML MAS workshop", "summary": "We introduce a modular harness design for LLM agents that composes of\nperception, memory, and reasoning components, enabling a single LLM or VLM\nbackbone to tackle a wide spectrum of multi turn gaming environments without\ndomain-specific engineering. Using classic and modern game suites as\nlow-barrier, high-diversity testbeds, our framework provides a unified workflow\nfor analyzing how each module affects performance across dynamic interactive\nsettings. Extensive experiments demonstrate that the harness lifts gameplay\nperformance consistently over un-harnessed baselines and reveals distinct\ncontribution patterns, for example, memory dominates in long-horizon puzzles\nwhile perception is critical in vision noisy arcades. These findings highlight\nthe effectiveness of our modular harness design in advancing general-purpose\nagent, given the familiarity and ubiquity of games in everyday human\nexperience.", "AI": {"tldr": "论文提出了一种模块化设计的LLM代理框架，包含感知、记忆和推理组件，适用于多轮游戏环境，无需领域特定工程。", "motivation": "通过游戏环境作为测试平台，分析模块化设计对动态交互任务性能的影响，推动通用智能代理的发展。", "method": "采用模块化设计（感知、记忆、推理），使用经典和现代游戏套件进行实验。", "result": "实验表明该框架显著提升游戏性能，不同模块在不同场景下作用不同（如记忆对长期任务关键，感知对视觉噪声环境重要）。", "conclusion": "模块化设计有效提升通用代理性能，游戏环境的多样性和熟悉性为研究提供了理想测试平台。"}}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "AI": {"tldr": "MLLMs作为验证器在复杂任务中存在一致性偏差问题，SGV方法通过自生成先验和条件生成提升验证效果。", "motivation": "解决在无明确成功标准的领域（如计算机使用）中扩展AI验证器能力的挑战。", "method": "提出自接地验证（SGV），通过无条件生成和条件生成两步法优化MLLMs的验证能力。", "result": "SGV使MLLMs验证准确率提升20%，任务完成率提升48%，达到新SOTA。", "conclusion": "SGV有效缓解MLLMs的一致性偏差，显著提升其在复杂任务中的验证性能。"}}
{"id": "2507.11733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11733", "abs": "https://arxiv.org/abs/2507.11733", "authors": ["Srikanth Vemula"], "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making", "comment": null, "summary": "This Study introduces Clarity and Reasoning Interface for Artificial\nIntelligence(ClarifAI), a novel approach designed to augment the transparency\nand interpretability of artificial intelligence (AI) in the realm of improved\ndecision making. Leveraging the Case-Based Reasoning (CBR) methodology and\nintegrating an ontology-driven approach, ClarifAI aims to meet the intricate\nexplanatory demands of various stakeholders involved in AI-powered\napplications. The paper elaborates on ClarifAI's theoretical foundations,\ncombining CBR and ontologies to furnish exhaustive explanation mechanisms. It\nfurther elaborates on the design principles and architectural blueprint,\nhighlighting ClarifAI's potential to enhance AI interpretability across\ndifferent sectors and its applicability in high-stake environments. This\nresearch delineates the significant role of ClariAI in advancing the\ninterpretability of AI systems, paving the way for its deployment in critical\ndecision-making processes.", "AI": {"tldr": "ClarifAI结合案例推理和本体驱动方法，提升AI透明度和可解释性，适用于高风险决策场景。", "motivation": "满足AI应用中各利益相关者对解释性的复杂需求，提升AI系统的透明度和可信度。", "method": "结合案例推理（CBR）和本体驱动方法，设计理论框架和架构蓝图。", "result": "ClarifAI能显著增强AI系统的可解释性，适用于多领域和高风险环境。", "conclusion": "ClarifAI为AI系统的可解释性提供新途径，支持关键决策过程。"}}
{"id": "2507.11726", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11726", "abs": "https://arxiv.org/abs/2507.11726", "authors": ["Ding Lin", "Jianhui Wang", "Tianqiao Zhao", "Meng Yue"], "title": "A Deep Reinforcement Learning Method for Multi-objective Transmission Switching", "comment": "5 pages", "summary": "Transmission switching is a well-established approach primarily applied to\nminimize operational costs through strategic network reconfiguration. However,\nexclusive focus on cost reduction can compromise system reliability. While\nmulti-objective transmission switching can balance cost savings with\nreliability improvements, feasible solutions become exceedingly difficult to\nobtain as system scale grows, due to the inherent nonlinearity and high\ncomputational demands involved. This paper proposes a deep reinforcement\nlearning (DRL) method for multi-objective transmission switching. The method\nincorporates a dueling-based actor-critic framework to evaluate the relative\nimpact of each line switching decision within the action space, which improves\ndecision quality and enhances both system reliability and cost efficiency.\nNumerical studies on the IEEE 118-bus system verify the effectiveness and\nefficiency of the proposed approach compared to two benchmark DRL algorithms.", "AI": {"tldr": "本文提出了一种基于深度强化学习（DRL）的多目标输电线路切换方法，通过改进决策质量平衡成本与可靠性。", "motivation": "传统输电线路切换方法过于关注成本降低，可能损害系统可靠性，且在大规模系统中难以找到可行解。", "method": "采用基于决斗的演员-评论家框架，评估线路切换决策的相对影响，提升决策质量。", "result": "在IEEE 118总线系统上的数值研究验证了方法的有效性和效率。", "conclusion": "该方法在成本和可靠性之间取得了更好的平衡，优于基准DRL算法。"}}
{"id": "2507.11621", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11621", "abs": "https://arxiv.org/abs/2507.11621", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks.", "AI": {"tldr": "该论文提出了一种分层协作的匝道合并控制框架（HCOMC），用于解决混合交通流中的匝道合并问题，结合纵向跟车和横向换道模型，并通过仿真验证其性能。", "motivation": "高速公路匝道合并区域是交通拥堵和事故的常见瓶颈，而基于联网自动驾驶车辆（CAV）的协作控制策略是根本解决方案。然而，由于CAV尚未普及，需要提出适用于混合交通流的控制框架。", "method": "论文扩展了基于智能驾驶员模型的纵向跟车模型和基于五次多项式曲线的横向换道模型，提出了HCOMC框架，包括分层协作规划模型、基于博弈论的自主换道模型和多目标优化模型。", "result": "仿真结果表明，HCOMC在提升车辆群安全性、稳定和加速合并过程、优化交通效率和节省燃油消耗方面具有显著优势。", "conclusion": "HCOMC框架为混合交通流中的匝道合并问题提供了有效的解决方案，尤其在CAV普及率较低的情况下表现出色。"}}
{"id": "2507.11551", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11551", "abs": "https://arxiv.org/abs/2507.11551", "authors": ["Ekaterina Stansfield", "Jennifer A. Mitterer", "Abdulrahman Altahhan"], "title": "Landmark Detection for Medical Images using a General-purpose Segmentation Model", "comment": "13 pages, 8 figures, 2 tables. Submitted to ICONIP 2025", "summary": "Radiographic images are a cornerstone of medical diagnostics in orthopaedics,\nwith anatomical landmark detection serving as a crucial intermediate step for\ninformation extraction. General-purpose foundational segmentation models, such\nas SAM (Segment Anything Model), do not support landmark segmentation out of\nthe box and require prompts to function. However, in medical imaging, the\nprompts for landmarks are highly specific. Since SAM has not been trained to\nrecognize such landmarks, it cannot generate accurate landmark segmentations\nfor diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has\nbeen trained to identify larger anatomical structures, such as organs and their\nparts, and lacks the fine-grained precision required for orthopaedic pelvic\nlandmarks. To address this limitation, we propose leveraging another\ngeneral-purpose, non-foundational model: YOLO. YOLO excels in object detection\nand can provide bounding boxes that serve as input prompts for SAM. While YOLO\nis efficient at detection, it is significantly outperformed by SAM in\nsegmenting complex structures. In combination, these two models form a reliable\npipeline capable of segmenting not only a small pilot set of eight anatomical\nlandmarks but also an expanded set of 72 landmarks and 16 regions with complex\noutlines, such as the femoral cortical bone and the pelvic inlet. By using\nYOLO-generated bounding boxes to guide SAM, we trained the hybrid model to\naccurately segment orthopaedic pelvic radiographs. Our results show that the\nproposed combination of YOLO and SAM yields excellent performance in detecting\nanatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.", "AI": {"tldr": "提出了一种结合YOLO和SAM的混合模型，用于精确分割骨科骨盆X光片中的解剖标志和复杂轮廓。", "motivation": "现有通用分割模型（如SAM和MedSAM）无法直接用于医学影像中的精细解剖标志分割，需解决这一局限性。", "method": "利用YOLO生成边界框作为SAM的输入提示，结合两者优势构建可靠的分割流程。", "result": "模型在分割8个解剖标志的基础上，扩展到72个标志和16个复杂区域，表现优异。", "conclusion": "YOLO与SAM的结合为骨科骨盆X光片的分割提供了高效且精确的解决方案。"}}
{"id": "2507.11582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "研究将大语言模型（LLMs）作为“主观文学评论家”，探讨其在文学评估中的审美偏好和评价模式，发现LLMs具有类似人类批评流派的个体评价特征。", "motivation": "探索LLMs在文学评估中的表现，揭示其是否具备类似人类的主观评价特征，而非仅作为中性基准。", "method": "将十篇日本科幻短篇小说翻译成英文，由六种先进LLMs在七次独立会话中评估，使用主成分分析和聚类技术分析评价一致性和模式。", "result": "发现评价一致性差异显著（α范围1.00至0.35），识别出五种评价模式，且不同故事的评价方差差异高达4.5倍。TF-IDF分析确认每种模型有独特的评价词汇。", "conclusion": "LLMs可能具备类似人类批评流派的个体评价特征，而非中性基准工具，其隐含的价值体系受RLHF影响。"}}
{"id": "2507.11549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11549", "abs": "https://arxiv.org/abs/2507.11549", "authors": ["Wendong Mao", "Mingfan Zhao", "Jianfeng Guan", "Qiwei Dong", "Zhongfeng Wang"], "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search", "comment": null, "summary": "Deformable Attention Transformers (DAT) have shown remarkable performance in\ncomputer vision tasks by adaptively focusing on informative image regions.\nHowever, their data-dependent sampling mechanism introduces irregular memory\naccess patterns, posing significant challenges for efficient hardware\ndeployment. Existing acceleration methods either incur high hardware overhead\nor compromise model accuracy. To address these issues, this paper proposes a\nhardware-friendly optimization framework for DAT. First, a neural architecture\nsearch (NAS)-based method with a new slicing strategy is proposed to\nautomatically divide the input feature into uniform patches during the\ninference process, avoiding memory conflicts without modifying model\narchitecture. The method explores the optimal slice configuration by jointly\noptimizing hardware cost and inference accuracy. Secondly, an FPGA-based\nverification system is designed to test the performance of this framework on\nedge-side hardware. Algorithm experiments on the ImageNet-1K dataset\ndemonstrate that our hardware-friendly framework can maintain have only 0.2%\naccuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA\nshow the proposed method reduces DRAM access times to 18% compared with\nexisting DAT acceleration methods.", "AI": {"tldr": "本文提出了一种硬件友好的优化框架，通过NAS和新的切片策略优化DAT的内存访问模式，同时在FPGA上验证其性能。", "motivation": "DAT的数据依赖性采样机制导致不规则内存访问模式，难以高效部署硬件，现有方法要么硬件开销高，要么牺牲模型精度。", "method": "1. 提出基于NAS和切片策略的方法，自动划分输入特征为均匀块；2. 设计FPGA验证系统。", "result": "在ImageNet-1K上仅损失0.2%精度；FPGA实验显示DRAM访问次数减少至现有方法的18%。", "conclusion": "提出的框架在保持精度的同时显著优化了硬件效率。"}}
{"id": "2507.11737", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11737", "abs": "https://arxiv.org/abs/2507.11737", "authors": ["Chenyu Zhou", "Jingyuan Yang", "Linwei Xin", "Yitian Chen", "Ziyan He", "Dongdong Ge"], "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "comment": null, "summary": "Dynamic programming (DP) is a fundamental method in operations research, but\nformulating DP models has traditionally required expert knowledge of both the\nproblem context and DP techniques. Large Language Models (LLMs) offer the\npotential to automate this process. However, DP problems pose unique challenges\ndue to their inherently stochastic transitions and the limited availability of\ntraining data. These factors make it difficult to directly apply existing\nLLM-based models or frameworks developed for other optimization problems, such\nas linear or integer programming. We introduce DP-Bench, the first benchmark\ncovering a wide range of textbook-level DP problems to enable systematic\nevaluation. We present Dynamic Programming Language Model (DPLM), a\n7B-parameter specialized model that achieves performance comparable to\nstate-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on\nhard problems. Central to DPLM's effectiveness is DualReflect, our novel\nsynthetic data generation pipeline, designed to scale up training data from a\nlimited set of initial examples. DualReflect combines forward generation for\ndiversity and backward generation for reliability. Our results reveal a key\ninsight: backward generation is favored in low-data regimes for its strong\ncorrectness guarantees, while forward generation, though lacking such\nguarantees, becomes increasingly valuable at scale for introducing diverse\nformulations. This trade-off highlights the complementary strengths of both\napproaches and the importance of combining them.", "AI": {"tldr": "论文提出了一种名为DPLM的专门用于动态规划（DP）问题的语言模型，通过DualReflect数据生成方法解决了训练数据不足的问题，并在DP-Bench基准测试中表现优异。", "motivation": "动态规划问题因其随机性和数据稀缺性难以直接应用现有LLM模型，需要一种专门的方法来自动化DP模型构建。", "method": "提出DPLM模型和DualReflect数据生成方法，结合前向生成（多样性）和后向生成（可靠性）以扩展训练数据。", "result": "DPLM在DP-Bench上表现优于现有LLM模型，尤其在复杂问题上。", "conclusion": "前向和后向生成方法的结合是解决数据稀缺问题的有效策略，DPLM展示了在DP问题上的潜力。"}}
{"id": "2507.11749", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11749", "abs": "https://arxiv.org/abs/2507.11749", "authors": ["Jonathan Olivares", "Tyler Depe", "Rakeshkumar Mahto"], "title": "Reconfigurable Battery Systems for Enhanced Fast Charging in Electric Vehicles", "comment": null, "summary": "The adoption of electric vehicles (EVs) is rapidly growing as a key solution\nto reducing greenhouse gas emissions. However, prolonged charging times remain\na significant barrier to widespread EV usage, especially for individuals\nwithout access to fast charging infrastructure. This paper explores the\npotential of reconfigurable battery systems to reduce EV charging times without\ncompromising battery life. We propose innovative battery pack configurations\nthat dynamically adjust the arrangement of cells to optimize charging\nperformance. Simulations were conducted using MATLAB and Simulink to compare\nthe efficiency of various battery configurations, focusing on charging times,\nstate of charge (SOC), voltage, and current under different conditions. The\nresults demonstrate that connecting more batteries in series through\nreconfigurability in battery packs can significantly reduce charging times\nwhile maintaining operational safety. This study offers insights into how\nreconfigurable battery designs can provide a practical solution for faster,\nmore efficient home-based EV charging, making EV ownership more accessible and\nsustainable.", "AI": {"tldr": "研究探讨了可重构电池系统如何在不损害电池寿命的情况下缩短电动汽车充电时间，提出动态调整电池组配置的创新方法。", "motivation": "电动汽车充电时间长是普及的主要障碍，尤其是缺乏快速充电设施的用户。", "method": "提出动态调整电池组配置的创新方法，通过MATLAB和Simulink模拟不同配置的充电效率。", "result": "结果显示，通过可重构性串联更多电池可显著缩短充电时间并保持安全性。", "conclusion": "可重构电池设计为家庭充电提供了更快、更高效的解决方案，提升电动汽车的可持续性。"}}
{"id": "2507.11623", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11623", "abs": "https://arxiv.org/abs/2507.11623", "authors": ["Alan Papalia", "Charles Dawson", "Laurentiu L. Anton", "Norhan Magdy Bayomi", "Bianca Champenois", "Jung-Hoon Cho", "Levi Cai", "Joseph DelPreto", "Kristen Edwards", "Bilha-Catherine Githinji", "Cameron Hickert", "Vindula Jayawardana", "Matthew Kramer", "Shreyaa Raghavan", "David Russell", "Shide Salimi", "Jingnan Shi", "Soumya Sudhakar", "Yanwei Wang", "Shouyi Wang", "Luca Carlone", "Vijay Kumar", "Daniela Rus", "John E. Fernandez", "Cathy Wu", "George Kantor", "Derek Young", "Hanumant Singh"], "title": "A Roadmap for Climate-Relevant Robotics Research", "comment": null, "summary": "Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities.", "AI": {"tldr": "本文提出了一个气候相关机器人研究的路线图，旨在通过机器人技术与气候领域的合作解决高影响力问题。", "motivation": "气候变化是21世纪的重要挑战，机器人社区希望通过贡献技术解决方案来应对这一挑战。", "method": "通过识别机器人技术与气候领域（如能源、建筑环境、交通等）的合作机会，提出具体应用问题和方法。", "result": "路线图明确了机器人技术在能源优化、精准农业、环境监测等领域的应用潜力。", "conclusion": "本文旨在激发机器人社区的新研究方向，推动跨学科合作以应对气候问题。"}}
{"id": "2507.11557", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11557", "abs": "https://arxiv.org/abs/2507.11557", "authors": ["Jiaxu Zheng", "Meiman He", "Xuhui Tang", "Xiong Wang", "Tuoyu Cao", "Tianyi Zeng", "Lichi Zhang", "Chenyu You"], "title": "3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation", "comment": null, "summary": "Magnetic Resonance (MR) imaging plays an essential role in contemporary\nclinical diagnostics. It is increasingly integrated into advanced therapeutic\nworkflows, such as hybrid Positron Emission Tomography/Magnetic Resonance\n(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are\ncritically dependent on accurate estimation of radiation attenuation, which is\ntypically facilitated by synthesizing Computed Tomography (CT) images from MR\nscans to generate attenuation maps. However, existing MR-to-CT synthesis\nmethods for whole-body imaging often suffer from poor spatial alignment between\nthe generated CT and input MR images, and insufficient image quality for\nreliable use in downstream clinical tasks. In this paper, we present a novel 3D\nWavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by\nperforming modality translation in a learned latent space. By incorporating a\nWavelet Residual Module into the encoder-decoder architecture, we enhance the\ncapture and reconstruction of fine-scale features across image and latent\nspaces. To preserve anatomical integrity during the diffusion process, we\ndisentangle structural and modality-specific characteristics and anchor the\nstructural component to prevent warping. We also introduce a Dual Skip\nConnection Attention mechanism within the diffusion model, enabling the\ngeneration of high-resolution CT images with improved representation of bony\nstructures and soft-tissue contrast.", "AI": {"tldr": "提出了一种新型3D小波潜在扩散模型（3D-WLDM），用于从MR图像合成高质量的CT图像，解决了现有方法在空间对齐和图像质量上的不足。", "motivation": "MR成像在临床诊断和治疗中至关重要，但现有MR-to-CT合成方法在空间对齐和图像质量上存在问题，影响了其在临床任务中的可靠性。", "method": "通过在小波潜在空间中进行模态转换，结合小波残差模块和双跳跃连接注意力机制，增强细尺度特征的捕获和重建，同时保持解剖结构的完整性。", "result": "生成的CT图像在骨结构和软组织对比度上表现出更高的分辨率和准确性。", "conclusion": "3D-WLDM为MR-to-CT合成提供了一种有效解决方案，有望提升临床工作流程的效率和准确性。"}}
{"id": "2507.11625", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "论文介绍了MapIQ基准数据集，用于评估多模态大语言模型（MLLMs）在地图视觉问答（Map-VQA）中的表现，涵盖多种地图类型和主题，并分析了模型对地图设计变化的敏感性和鲁棒性。", "motivation": "现有Map-VQA研究主要局限于等值区域图，覆盖的主题和任务有限，因此需要更全面的数据集和评估方法。", "method": "构建MapIQ数据集，包含14,706个问题-答案对，覆盖三种地图类型和六种主题，并评估多种MLLMs在六种视觉分析任务中的表现。", "result": "实验比较了MLLMs的性能与人类基准，并分析了地图设计变化对模型表现的影响。", "conclusion": "研究揭示了MLLMs的鲁棒性和敏感性，提出了改进Map-VQA性能的潜在方向。"}}
{"id": "2507.11550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11550", "abs": "https://arxiv.org/abs/2507.11550", "authors": ["Hyeonseok Jin", "Geonmin Kim", "Kyungbaek Kim"], "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction", "comment": "7 pages", "summary": "Spatio-temporal traffic prediction plays a key role in intelligent\ntransportation systems by enabling accurate prediction in complex urban areas.\nAlthough not only accuracy but also efficiency for scalability is important,\nsome previous methods struggle to capture heterogeneity such as varying traffic\npatterns across regions and time periods. Moreover, Graph Neural Networks\n(GNNs), which are the mainstream of traffic prediction, not only require\npredefined adjacency matrix, but also limit scalability to large-scale data\ncontaining many nodes due to their inherent complexity. To overcome these\nlimitations, we propose Deformable Dynamic Convolution Network (DDCN) for\naccurate yet efficient traffic prediction. Traditional Convolutional Neural\nNetworks (CNNs) are limited in modeling non-Euclidean spatial structures and\nspatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically\napplying deformable filters based on offset. Specifically, DDCN decomposes\ntransformer-style CNN to encoder-decoder structure, and applies proposed\napproaches to the spatial and spatio-temporal attention blocks of the encoder\nto emphasize important features. The decoder, composed of feed-forward module,\ncomplements the output of the encoder. This novel structure make DDCN can\nperform accurate yet efficient traffic prediction. In comprehensive experiments\non four real-world datasets, DDCN achieves competitive performance, emphasizing\nthe potential and effectiveness of CNN-based approaches for spatio-temporal\ntraffic prediction.", "AI": {"tldr": "论文提出了一种名为DDCN的新方法，用于高效且准确的时空交通预测，解决了传统方法的局限性和GNN的复杂性。", "motivation": "现有方法在捕捉时空异质性和处理大规模数据时存在不足，GNN需要预定义邻接矩阵且计算复杂。", "method": "DDCN通过动态应用可变形滤波器，结合编码器-解码器结构，利用空间和时空注意力块强调重要特征。", "result": "在四个真实数据集上的实验表明，DDCN性能优越，验证了CNN方法在时空交通预测中的潜力。", "conclusion": "DDCN是一种高效且准确的时空交通预测方法，克服了传统方法的局限性。"}}
{"id": "2507.11787", "categories": ["cs.AI", "68-68W50"], "pdf": "https://arxiv.org/pdf/2507.11787", "abs": "https://arxiv.org/abs/2507.11787", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial\nintelligence, where the natural behavior of animals and insects is observed and\ntranslated into computer algorithms called swarm computing to solve real-world\nproblems. Due to their effectiveness, they are applied in solving various\ncomputer optimization problems. This survey will review all the latest\ndevelopments in Searching for documents based on semantic similarity using\nSwarm Intelligence algorithms and recommend future research directions.", "AI": {"tldr": "综述了基于群体智能算法的语义相似性文档搜索的最新进展，并提出了未来研究方向。", "motivation": "群体智能因其高效性被广泛应用于优化问题，本文旨在探讨其在语义相似性文档搜索中的应用。", "method": "通过综述最新的群体智能算法，分析其在文档搜索中的实现方法。", "result": "总结了群体智能在语义相似性文档搜索中的有效性和应用潜力。", "conclusion": "群体智能在文档搜索中具有广阔前景，未来需进一步研究其优化与扩展。"}}
{"id": "2507.11849", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11849", "abs": "https://arxiv.org/abs/2507.11849", "authors": ["Tanjim Rahman"], "title": "Mobility Extraction and Analysis of GaN HEMTs for RF Applications Using TCAD and Experimental Data", "comment": "5 pages, 7 figures", "summary": "This paper presents an analysis of GaN high-electron-mobility transistors\n(HEMTs) using both TCAD simulation and experimental characterization. The\nenergy band structure was studied using Nextnano simulation software to observe\ntwo-dimensional electron gas (2DEG) formation and carrier confinement under\nequilibrium conditions. Additionally, I-V and C-V data from fabricated\nresearch-grade GaN HEMTs were analyzed to extract key electrical parameters.\nThe device demonstrated an ON current of 1.9 mA and an OFF current of 0.01 mA,\nindicating a strong ON/OFF current ratio. A subthreshold swing of 80 mV/decade\nand a DIBL of 5 mV/V were observed, confirming good gate control and\nshort-channel suppression. The ON-resistance was 22.72 ohm per micron, with a\nsaturation voltage of 1 V . The peak transconductance was extracted as 0.18 mS\nin the linear region and 0.5 mS in saturation. Field-effect mobility was\ncalculated using the transconductance method, with a maximum value of\napproximately 1200 cm2/V.s at low drain bias. The combined simulation and\nexperimental approach provided comprehensive insight into GaN HEMT behavior,\nenabling a deeper understanding of structure-performance relationships critical\nto advanced transistor design.", "AI": {"tldr": "论文通过TCAD模拟和实验表征分析了GaN HEMTs，研究了能带结构和电学参数，展示了优异的性能指标。", "motivation": "深入理解GaN HEMTs的结构与性能关系，为先进晶体管设计提供支持。", "method": "结合Nextnano模拟软件和实验数据（I-V、C-V）分析，提取关键电学参数。", "result": "器件表现出高ON/OFF电流比（1.9 mA/0.01 mA）、低亚阈值摆幅（80 mV/decade）和高迁移率（1200 cm²/V.s）。", "conclusion": "模拟与实验结合的方法为GaN HEMTs的行为提供了全面见解，有助于优化设计。"}}
{"id": "2507.11716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11716", "abs": "https://arxiv.org/abs/2507.11716", "authors": ["Yifan Xu", "Qianwei Wang", "Jordan Lillie", "Vineet Kamat", "Carol Menassa", "Clive D'Souza"], "title": "CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment", "comment": "13 pages, 10 figures", "summary": "As the global population of people with disabilities (PWD) continues to grow,\nso will the need for mobility solutions that promote independent living and\nsocial integration. Wheelchairs are vital for the mobility of PWD in both\nindoor and outdoor environments. The current SOTA in powered wheelchairs is\nbased on either manually controlled or fully autonomous modes of operation,\noffering limited flexibility and often proving difficult to navigate in\nspatially constrained environments. Moreover, research on robotic wheelchairs\nhas focused predominantly on complete autonomy or improved manual control;\napproaches that can compromise efficiency and user trust. To overcome these\nchallenges, this paper introduces the CoNav Chair, a smart wheelchair based on\nthe Robot Operating System (ROS) and featuring shared control navigation and\nobstacle avoidance capabilities that are intended to enhance navigational\nefficiency, safety, and ease of use for the user. The paper outlines the CoNav\nChair's design and presents a preliminary usability evaluation comparing three\ndistinct navigation modes, namely, manual, shared, and fully autonomous,\nconducted with 21 healthy, unimpaired participants traversing an indoor\nbuilding environment. Study findings indicated that the shared control\nnavigation framework had significantly fewer collisions and performed\ncomparably, if not superior to the autonomous and manual modes, on task\ncompletion time, trajectory length, and smoothness; and was perceived as being\nsafer and more efficient based on user reported subjective assessments of\nusability. Overall, the CoNav system demonstrated acceptable safety and\nperformance, laying the foundation for subsequent usability testing with end\nusers, namely, PWDs who rely on a powered wheelchair for mobility.", "AI": {"tldr": "本文介绍了CoNav Chair，一种基于ROS的智能轮椅，通过共享控制导航和避障功能提升导航效率、安全性和易用性。初步评估显示共享控制模式优于手动和全自动模式。", "motivation": "随着残疾人口增长，现有电动轮椅在灵活性和导航能力上存在不足，需改进以提升独立生活和社会融入。", "method": "基于ROS设计CoNav Chair，比较手动、共享和全自动三种导航模式，通过21名健康参与者的室内实验评估性能。", "result": "共享控制模式碰撞更少，任务完成时间、轨迹长度和平滑度表现优异，用户主观评价更高。", "conclusion": "CoNav Chair在安全和性能上表现良好，为后续残疾用户测试奠定基础。"}}
{"id": "2507.11561", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11561", "abs": "https://arxiv.org/abs/2507.11561", "authors": ["Lucas Erlacher", "Samuel Ruipérez-Campillo", "Holger Michel", "Sven Wellmann", "Thomas M. Sutter", "Ece Ozkan", "Julia E. Vogt"], "title": "Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach", "comment": null, "summary": "Pulmonary hypertension (PH) in newborns is a critical condition characterized\nby elevated pressure in the pulmonary arteries, leading to right ventricular\nstrain and heart failure. While right heart catheterization (RHC) is the\ndiagnostic gold standard, echocardiography is preferred due to its non-invasive\nnature, safety, and accessibility. However, its accuracy highly depends on the\noperator, making PH assessment subjective. While automated detection methods\nhave been explored, most models focus on adults and rely on single-view\nechocardiographic frames, limiting their performance in diagnosing PH in\nnewborns. While multi-view echocardiography has shown promise in improving PH\nassessment, existing models struggle with generalizability. In this work, we\nemploy a multi-view variational autoencoder (VAE) for PH prediction using\nechocardiographic videos. By leveraging the VAE framework, our model captures\ncomplex latent representations, improving feature extraction and robustness. We\ncompare its performance against single-view and supervised learning approaches.\nOur results show improved generalization and classification accuracy,\nhighlighting the effectiveness of multi-view learning for robust PH assessment\nin newborns.", "AI": {"tldr": "该论文提出了一种基于多视角变分自编码器（VAE）的方法，用于新生儿肺动脉高压（PH）的预测，通过多视角学习提高了模型的泛化能力和分类准确性。", "motivation": "新生儿肺动脉高压（PH）的诊断依赖超声心动图，但其准确性受操作者影响且现有自动化方法多针对成人，泛化能力不足。多视角超声心动图虽有望改善PH评估，但现有模型泛化能力有限。", "method": "采用多视角变分自编码器（VAE）框架，从超声心动视频中提取复杂潜在表征，提升特征提取和鲁棒性，并与单视角和监督学习方法对比。", "result": "结果表明，多视角学习方法在泛化能力和分类准确性上优于单视角和监督学习方法。", "conclusion": "多视角学习为新生儿PH的稳健评估提供了有效方法。"}}
{"id": "2507.11634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11634", "abs": "https://arxiv.org/abs/2507.11634", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "研究探讨了在波斯语中使用少样本学习和增量学习方法进行跨语言情感分析，通过多语言预训练模型取得了高准确率。", "motivation": "开发一个能够在数据有限的波斯语中进行情感分析的模型，同时利用高资源语言的知识。", "method": "使用了三种多语言预训练模型（XLM-RoBERTa、mDeBERTa和DistilBERT），并通过少样本和增量学习方法在波斯语数据上进行微调。", "result": "mDeBERTa和XLM-RoBERTa在波斯语情感分析中达到了96%的准确率。", "conclusion": "结合少样本学习、增量学习和多语言预训练模型是有效的。"}}
{"id": "2507.11554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "AI": {"tldr": "Inversion-DPO提出了一种新的对齐框架，通过DDIM反演优化扩散模型，无需奖励建模，显著提升训练精度和效率。", "motivation": "现有对齐方法计算开销大且可能影响模型性能，Inversion-DPO旨在解决这些问题。", "method": "利用DDIM反演将DPO应用于扩散模型，避免奖励建模，直接优化后验采样。", "result": "在文本到图像生成和组合图像生成任务中表现优异，生成高保真且组合一致的图像。", "conclusion": "Inversion-DPO为扩散模型的高效高精度对齐提供了新思路，适用于复杂生成任务。"}}
{"id": "2507.11916", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11916", "abs": "https://arxiv.org/abs/2507.11916", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "comment": null, "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance.", "AI": {"tldr": "论文提出了一种利用GPU并行计算优化深度优先搜索（DFS）的方法，称为成本受限深度优先搜索（CB-DFS），并扩展了Batch IDA*和Batch BTS算法。实验验证了该方法在3x3魔方和4x4滑块拼图上的有效性。", "motivation": "GPU技术的快速发展为经典搜索算法的优化提供了新机会，但目前很少有算法在搜索过程中充分利用GPU。", "method": "提出CB-DFS方法，结合CPU和GPU的并行计算能力，扩展了Batch IDA*和Batch BTS算法，基于AIDA*框架并保持最优性保证。", "result": "在3x3魔方和4x4滑块拼图上的实验表明，GPU计算可以高效批处理。还分析了超参数、神经网络启发式大小和硬件资源对性能的影响。", "conclusion": "CB-DFS方法成功将GPU并行计算引入DFS，显著提升了搜索效率，为未来研究提供了新方向。"}}
{"id": "2507.11872", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11872", "abs": "https://arxiv.org/abs/2507.11872", "authors": ["Wenhan Cao", "Tianyi Zhang", "Shengbo Eben Li"], "title": "Algorithm Design and Comparative Test of Natural Gradient Gaussian Approximation Filter", "comment": null, "summary": "Popular Bayes filters typically rely on linearization techniques such as\nTaylor series expansion and stochastic linear regression to use the structure\nof standard Kalman filter. These techniques may introduce large estimation\nerrors in nonlinear and non-Gaussian systems. This paper overviews a recent\nbreakthrough in filtering algorithm design called \\textit{N}atural\nGr\\textit{a}dient Gaussia\\textit{n} Appr\\textit{o}ximation (NANO) filter and\ncompare its performance over a large class of nonlinear filters. The NANO\nfilter interprets Bayesian filtering as solutions to two distinct optimization\nproblems, which allows to define optimal Gaussian approximation and derive its\ncorresponding extremum conditions. The algorithm design still follows the\ntwo-step structure of Bayes filters. In the prediction step, NANO filter\ncalculates the first two moments of the prior distribution, and this process is\nequivalent to a moment-matching filter. In the update step, natural gradient\ndescent is employed to directly minimize the objective of the update step,\nthereby avoiding errors caused by model linearization. Comparative tests are\nconducted on four classic systems, including the damped linear oscillator,\nsequence forecasting, modified growth model, and robot localization, under\nGaussian, Laplace, and Beta noise to evaluate the NANO filter's capability in\nhandling nonlinearity. Additionally, we validate the NANO filter's robustness\nto data outliers using a satellite attitude estimation example. It is observed\nthat the NANO filter outperforms popular Kalman filters family such as extended\nKalman filter (EKF), unscented Kalman filter (UKF), iterated extended Kalman\nfilter (IEKF) and posterior linearization filter (PLF), while having similar\ncomputational burden.", "AI": {"tldr": "NANO滤波器是一种新型贝叶斯滤波算法，通过自然梯度下降优化高斯近似，避免了传统线性化方法的误差，在非线性非高斯系统中表现优于EKF、UKF等滤波器。", "motivation": "传统贝叶斯滤波器依赖线性化技术（如泰勒展开），在非线性非高斯系统中可能引入较大估计误差。NANO滤波器旨在通过优化问题重新定义高斯近似，减少此类误差。", "method": "NANO滤波器将贝叶斯滤波分解为两个优化问题，预测步计算先验分布的前两阶矩（匹配矩滤波器），更新步使用自然梯度下降直接最小化目标函数，避免模型线性化。", "result": "在阻尼线性振荡器、序列预测、修正增长模型和机器人定位等经典系统中，NANO滤波器在非线性处理能力上优于EKF、UKF、IEKF和PLF，且计算负担相似。", "conclusion": "NANO滤波器通过优化高斯近似和避免线性化，显著提升了非线性非高斯系统中的滤波性能，是一种高效且鲁棒的滤波算法。"}}
{"id": "2507.11770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11770", "abs": "https://arxiv.org/abs/2507.11770", "authors": ["Giang Nguyen", "Mihai Pomarlan", "Sascha Jongebloed", "Nils Leusmann", "Minh Nhat Vu", "Michael Beetz"], "title": "Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies", "comment": "8 pages, 7 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS2025)", "summary": "In robotics, the effective integration of environmental data into actionable\nknowledge remains a significant challenge due to the variety and\nincompatibility of data formats commonly used in scene descriptions, such as\nMJCF, URDF, and SDF. This paper presents a novel approach that addresses these\nchallenges by developing a unified scene graph model that standardizes these\nvaried formats into the Universal Scene Description (USD) format. This\nstandardization facilitates the integration of these scene graphs with robot\nontologies through semantic reporting, enabling the translation of complex\nenvironmental data into actionable knowledge essential for cognitive robotic\ncontrol. We evaluated our approach by converting procedural 3D environments\ninto USD format, which is then annotated semantically and translated into a\nknowledge graph to effectively answer competency questions, demonstrating its\nutility for real-time robotic decision-making. Additionally, we developed a\nweb-based visualization tool to support the semantic mapping process, providing\nusers with an intuitive interface to manage the 3D environment.", "AI": {"tldr": "论文提出了一种统一场景图模型，将多种格式（如MJCF、URDF、SDF）标准化为USD格式，并通过语义标注与机器人本体集成，支持实时机器人决策。", "motivation": "解决机器人领域中环境数据格式多样且不兼容的问题，以提升数据集成和认知机器人控制的效率。", "method": "开发统一场景图模型，将不同格式转换为USD格式，并通过语义标注生成知识图谱。同时开发了基于Web的可视化工具。", "result": "成功将3D环境转换为USD格式，并通过知识图谱回答能力问题，验证了方法的实用性。", "conclusion": "该方法有效解决了数据格式不兼容问题，支持实时机器人决策，并通过可视化工具提升了用户体验。"}}
{"id": "2507.11569", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11569", "abs": "https://arxiv.org/abs/2507.11569", "authors": ["Hanxue Gu", "Yaqian Chen", "Nicholas Konz", "Qihang Li", "Maciej A. Mazurowski"], "title": "Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?", "comment": "3 figures, 9 pages", "summary": "Foundation models, pre-trained on large image datasets and capable of\ncapturing rich feature representations, have recently shown potential for\nzero-shot image registration. However, their performance has mostly been tested\nin the context of rigid or less complex structures, such as the brain or\nabdominal organs, and it remains unclear whether these models can handle more\nchallenging, deformable anatomy. Breast MRI registration is particularly\ndifficult due to significant anatomical variation between patients, deformation\ncaused by patient positioning, and the presence of thin and complex internal\nstructure of fibroglandular tissue, where accurate alignment is crucial.\nWhether foundation model-based registration algorithms can address this level\nof complexity remains an open question. In this study, we provide a\ncomprehensive evaluation of foundation model-based registration algorithms for\nbreast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,\nMedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that\ncapture variations in different years and dates, sequences, modalities, and\npatient disease status (lesion versus no lesion). Our results show that\nfoundation model-based algorithms such as SAM outperform traditional\nregistration baselines for overall breast alignment, especially under large\ndomain shifts, but struggle with capturing fine details of fibroglandular\ntissue. Interestingly, additional pre-training or fine-tuning on medical or\nbreast-specific images in MedSAM and SSLSAM, does not improve registration\nperformance and may even decrease it in some cases. Further work is needed to\nunderstand how domain-specific training influences registration and to explore\ntargeted strategies that improve both global alignment and fine structure\naccuracy. We also publicly release our code at\n\\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.", "AI": {"tldr": "该论文评估了基于基础模型的图像配准算法在乳腺MRI中的表现，发现某些模型（如SAM）在整体对齐上优于传统方法，但在细节处理上仍有不足。", "motivation": "研究基础模型在复杂、可变形解剖结构（如乳腺MRI）中的配准能力，填补现有研究在挑战性场景中的空白。", "method": "评估五种预训练编码器（DINO-v2、SAM、MedSAM、SSLSAM、MedCLIP）在四种乳腺配准任务中的表现。", "result": "SAM等基础模型在整体对齐上表现优异，尤其在域偏移大的情况下，但对纤维腺体组织的细节配准效果不佳。医学专用预训练未提升性能。", "conclusion": "需进一步研究领域特定训练对配准的影响，并开发同时优化全局对齐和细节精度的策略。代码已开源。"}}
{"id": "2507.11661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11661", "abs": "https://arxiv.org/abs/2507.11661", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "title": "Partitioner Guided Modal Learning Framework", "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "PgM框架通过模态分割器将多模态表示分为单模态和配对模态特征，分别学习并重构，提升多模态任务的灵活性和性能。", "motivation": "多模态学习中，单模态和配对模态特征对任务贡献不同，现有方法未充分区分和利用这两种特征。", "method": "提出PgM框架，包含模态分割器、单模态学习器、配对模态学习器和解码器，分别处理两种特征。", "result": "实验证明PgM在四种多模态任务中有效，且能迁移到现有模型。可视化分析揭示了两种特征的贡献。", "conclusion": "PgM通过区分和优化单模态与配对模态特征，提升了多模态学习的灵活性和性能。"}}
{"id": "2507.11558", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11558", "abs": "https://arxiv.org/abs/2507.11558", "authors": ["Changlu Chen", "Yanbin Liu", "Chaoxi Niu", "Ling Chen", "Tianqing Zhu"], "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting", "comment": null, "summary": "Foundation models have achieved remarkable success in natural language\nprocessing and computer vision, demonstrating strong capabilities in modeling\ncomplex patterns. While recent efforts have explored adapting large language\nmodels (LLMs) for time-series forecasting, LLMs primarily capture\none-dimensional sequential dependencies and struggle to model the richer\nspatio-temporal (ST) correlations essential for accurate ST forecasting. In\nthis paper, we present \\textbf{ST-VFM}, a novel framework that systematically\nreprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal\nforecasting. While VFMs offer powerful spatial priors, two key challenges arise\nwhen applying them to ST tasks: (1) the lack of inherent temporal modeling\ncapacity and (2) the modality gap between visual and ST data. To address these,\nST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs\nwith auxiliary ST flow inputs, where the flow encodes lightweight temporal\ndifference signals interpretable as dynamic spatial cues. To effectively\nprocess these dual-branch inputs, ST-VFM introduces two dedicated reprogramming\nstages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token\nAdapter to embed temporal context and align both branches into VFM-compatible\nfeature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral\nCross-Prompt Coordination module, enabling dynamic interaction between branches\nthrough prompt-based conditioning, thus enriching joint representation learning\nwithout modifying the frozen VFM backbone. Extensive experiments on ten\nspatio-temporal datasets show that ST-VFM outperforms state-of-the-art\nbaselines, demonstrating effectiveness and robustness across VFM backbones\n(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong\ngeneral framework for spatio-temporal forecasting.", "AI": {"tldr": "ST-VFM是一个新颖的框架，通过重新编程视觉基础模型（VFMs）来解决时空预测任务中的挑战，包括时间建模能力和模态差异问题。", "motivation": "尽管基础模型在自然语言处理和计算机视觉中表现出色，但在时空预测中，大型语言模型（LLMs）难以捕捉丰富的时空相关性。因此，需要一种方法将视觉基础模型（VFMs）重新编程以适用于时空预测。", "method": "ST-VFM采用双分支架构，结合原始时空输入和辅助时空流输入，并通过预VFM和后VFM两个重新编程阶段处理这些输入。预VFM阶段嵌入时间上下文并对齐分支，后VFM阶段通过提示条件动态交互分支。", "result": "在十个时空数据集上的实验表明，ST-VFM优于现有基线，展示了其有效性和鲁棒性。", "conclusion": "ST-VFM是一个强大的通用框架，适用于时空预测任务，且不依赖修改冻结的VFM主干。"}}
{"id": "2507.11988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11988", "abs": "https://arxiv.org/abs/2507.11988", "authors": ["Yexuan Shi", "Mingyu Wang", "Yunxiang Cao", "Hongjie Lai", "Junjian Lan", "Xin Han", "Yu Wang", "Jie Geng", "Zhenan Li", "Zihao Xia", "Xiang Chen", "Chen Li", "Jian Xu", "Wenbo Duan", "Yuanshuo Zhu"], "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "comment": "14 pages, 1 figures,", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.", "AI": {"tldr": "Aime是一个新型多智能体框架，通过动态反应式规划和执行解决传统规划-执行框架的局限性，显著提升多智能体系统的适应性和任务成功率。", "motivation": "传统规划-执行框架在多智能体系统中存在刚性执行、静态能力和低效通信等问题，限制了系统的适应性和鲁棒性。", "method": "Aime采用动态规划器、动态角色工厂和集中式进度管理模块，实现实时策略调整、按需角色生成和全局状态感知。", "result": "在多个基准测试中，Aime表现优于现有最先进的专用智能体，展示了更高的适应性和任务成功率。", "conclusion": "Aime为多智能体协作提供了更灵活、高效的框架，适用于动态环境中的复杂任务。"}}
{"id": "2507.11924", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11924", "abs": "https://arxiv.org/abs/2507.11924", "authors": ["Hyeongmin Choe", "Soojean Han"], "title": "Advantages of Feedback in Distributed Data-Gathering for Accurate and Power-Efficient State-Estimation", "comment": "Archived version. Related work under further development", "summary": "In distributed target-tracking sensor networks, efficient data gathering\nmethods are necessary to save communication resources and assure information\naccuracy. This paper proposes a Feedback (FB) distributed data-gathering method\nwhich lets the central unit feed information back to the mobile sensors; each\nsensor then uses it to cancel redundant transmissions and reduce communication\ncongestion. We rigorously compare its performance, in terms of mean-squared\nerror (MSE) and cost of power per sensor, against more conventional\nNon-Feedback (NF) architectures by evaluating conditions of feasibility and\nadvantage under different architecture specifications (e.g., communication\ndelay rate, power cost rate, maximum back-off time, sampling period,\nobservation noise). Here, we defined the advantage as the performance gain\nachieved by FB over NF, while FB is said to be feasible if the advantage region\nis nonempty. Our theoretical analyses show that the feasibility of FB depends\nmore on the communication power cost, while the advantage depends on the\nsensors' propagation delay per transmission interval; we derive concrete\nconditions under which these outcomes hold. Using extensive numerical\nsimulations under a variety of settings, we confirm the accuracy of the derived\nconditions, and show that our theoretical results hold even for more complex\nscenarios where the simplifying assumptions no longer hold.", "AI": {"tldr": "论文提出了一种反馈式分布式数据收集方法（FB），通过减少冗余传输和通信拥塞，优化传感器网络的资源利用和信息准确性。", "motivation": "在分布式目标跟踪传感器网络中，高效的数据收集方法对节省通信资源和保证信息准确性至关重要。", "method": "提出FB方法，中央单元向移动传感器反馈信息，以减少冗余传输和通信拥塞，并与传统非反馈（NF）架构进行性能比较。", "result": "理论分析和数值模拟表明，FB的可行性主要取决于通信功耗成本，而其优势则与传感器的传播延迟相关。", "conclusion": "FB在特定条件下优于NF，且理论结果在复杂场景中仍适用。"}}
{"id": "2507.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11840", "abs": "https://arxiv.org/abs/2507.11840", "authors": ["Gaofeng Li", "Ruize Wang", "Peisen Xu", "Qi Ye", "Jiming Chen"], "title": "The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey", "comment": null, "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed.", "AI": {"tldr": "本文综述了机器人灵巧操作的发展历程，从机械编程到具身智能，重点介绍了当前阶段的数据收集和技能学习框架，并总结了三大关键挑战。", "motivation": "实现人类水平的机器人灵巧操作是机器人学的核心目标和关键挑战，AI的发展推动了这一领域的快速进步。", "method": "通过仿真、人类演示和远程操作收集灵巧操作数据，并利用模仿学习和强化学习框架进行技能学习。", "result": "概述了现有数据收集范式和技能学习框架，总结了三大限制发展的关键挑战。", "conclusion": "灵巧机器人操作的未来发展需要解决数据收集和技能学习中的关键挑战。"}}
{"id": "2507.11886", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11886", "abs": "https://arxiv.org/abs/2507.11886", "authors": ["Yifan Gao", "Shaohao Rui", "Haoyang Su", "Jinyi Xiang", "Lianming Wu", "Xiaosong Wang"], "title": "A Composite Alignment-Aware Framework for Myocardial Lesion Segmentation in Multi-sequence CMR Images", "comment": "MICCAI 2025", "summary": "Accurate segmentation of myocardial lesions from multi-sequence cardiac\nmagnetic resonance imaging is essential for cardiac disease diagnosis and\ntreatment planning. However, achieving optimal feature correspondence is\nchallenging due to intensity variations across modalities and spatial\nmisalignment caused by inconsistent slice acquisition protocols. We propose\nCAA-Seg, a composite alignment-aware framework that addresses these challenges\nthrough a two-stage approach. First, we introduce a selective slice alignment\nmethod that dynamically identifies and aligns anatomically corresponding slice\npairs while excluding mismatched sections, ensuring reliable spatial\ncorrespondence between sequences. Second, we develop a hierarchical alignment\nnetwork that processes multi-sequence features at different semantic levels,\ni.e., local deformation correction modules address geometric variations in\nlow-level features, while global semantic fusion blocks enable semantic fusion\nat high levels where intensity discrepancies diminish. We validate our method\non a large-scale dataset comprising 397 patients. Experimental results show\nthat our proposed CAA-Seg achieves superior performance on most evaluation\nmetrics, with particularly strong results in myocardial infarction\nsegmentation, representing a substantial 5.54% improvement over\nstate-of-the-art approaches. The code is available at\nhttps://github.com/yifangao112/CAA-Seg.", "AI": {"tldr": "CAA-Seg是一种两阶段框架，通过选择性切片对齐和分层对齐网络解决多序列心脏MRI中的特征对应问题，显著提升了心肌梗死分割性能。", "motivation": "多序列心脏MRI中强度变化和空间错位导致特征对应困难，影响心肌病变分割的准确性。", "method": "采用选择性切片对齐和分层对齐网络，分别处理空间对应和语义融合问题。", "result": "在397名患者数据集上验证，CAA-Seg在心肌梗死分割中表现优异，比现有方法提升5.54%。", "conclusion": "CAA-Seg有效解决了多序列心脏MRI的分割挑战，为心脏疾病诊断提供了更可靠的工具。"}}
{"id": "2507.11694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11694", "abs": "https://arxiv.org/abs/2507.11694", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Sáez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "Héctor Cerezo-Costas"], "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "ExpliCIT-QA是一个多模态表格问答系统，通过模块化设计提供可解释的答案，并在TableVQA-Bench上验证了其透明性和可解释性。", "motivation": "解决端到端表格问答系统中可解释性不足的问题，适用于金融和医疗等敏感领域。", "method": "采用多模态表格理解、语言推理、自动代码生成、代码执行和自然语言解释的模块化设计。", "result": "在TableVQA-Bench上表现优于基线，提升了透明性和可解释性。", "conclusion": "ExpliCIT-QA为敏感领域的表格问答系统提供了可审计的解决方案。"}}
{"id": "2507.11562", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11562", "abs": "https://arxiv.org/abs/2507.11562", "authors": ["Ozer Can Devecioglu", "Serkan Kiranyaz", "Mehmet Yamac", "Moncef Gabbouj"], "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration", "comment": "6 pages", "summary": "The wide range of deformation artifacts that arise from complex light\npropagation, scattering, and depth-dependent attenuation makes the underwater\nimage restoration to remain a challenging problem. Like other single deep\nregressor networks, conventional GAN-based restoration methods struggle to\nperform well across this heterogeneous domain, since a single generator network\nis typically insufficient to capture the full range of visual degradations. In\norder to overcome this limitation, we propose xOp-GAN, a novel GAN model with\nseveral expert generator networks, each trained solely on a particular subset\nwith a certain image quality. Thus, each generator can learn to maximize its\nrestoration performance for a particular quality range. Once a xOp-GAN is\ntrained, each generator can restore the input image and the best restored image\ncan then be selected by the discriminator based on its perceptual confidence\nscore. As a result, xOP-GAN is the first GAN model with multiple generators\nwhere the discriminator is being used during the inference of the regression\ntask. Experimental results on benchmark Large Scale Underwater Image (LSUI)\ndataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,\nsurpassing all single-regressor models by a large margin even, with reduced\ncomplexity.", "AI": {"tldr": "xOp-GAN是一种新型GAN模型，通过多个专家生成器网络解决水下图像恢复问题，每个生成器专注于特定质量范围的图像恢复，最终由判别器选择最佳结果。", "motivation": "传统单生成器GAN模型难以应对水下图像的复杂退化问题，需要一种能处理异质性退化范围的方法。", "method": "提出xOp-GAN，包含多个专家生成器网络，每个生成器针对特定质量范围的图像进行训练，并由判别器选择最佳恢复结果。", "result": "在LSUI数据集上，xOp-GAN的PSNR达到25.16 dB，显著优于单回归器模型。", "conclusion": "xOp-GAN通过多生成器和判别器的协同工作，显著提升了水下图像恢复的性能。"}}
{"id": "2507.11992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11992", "abs": "https://arxiv.org/abs/2507.11992", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "title": "Understanding visual attention beehind bee-inspired UAV navigation", "comment": null, "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs.", "AI": {"tldr": "论文研究了基于光流感知的强化学习无人机导航策略，发现训练后的智能体主要关注光流不连续区域和大光流区域，行为类似昆虫飞行。", "motivation": "生物系统（如蜜蜂）在有限感知和计算能力下仍能高效飞行和避障，启发研究光流感知在无人机导航中的应用。", "method": "使用强化学习训练智能体仅依赖光流输入在障碍隧道中导航，并分析其注意力模式。", "result": "智能体主要关注光流不连续区域和大光流区域，行为类似昆虫飞行，且策略在独立训练中表现一致。", "conclusion": "该策略可能适用于开发简单明确的无人机控制法则。"}}
{"id": "2507.12031", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12031", "abs": "https://arxiv.org/abs/2507.12031", "authors": ["Fateme Salehi", "Aamir Mahmood", "Sarder Fakhrul Abedin", "Kyi Thar", "Mikael Gidlund"], "title": "Towards Ultra-Reliable 6G in-X Subnetworks: Dynamic Link Adaptation by Deep Reinforcement Learning", "comment": null, "summary": "6G networks are composed of subnetworks expected to meet ultra-reliable\nlow-latency communication (URLLC) requirements for mission-critical\napplications such as industrial control and automation. An often-ignored aspect\nin URLLC is consecutive packet outages, which can destabilize control loops and\ncompromise safety in in-factory environments. Hence, the current work proposes\na link adaptation framework to support extreme reliability requirements using\nthe soft actor-critic (SAC)-based deep reinforcement learning (DRL) algorithm\nthat jointly optimizes energy efficiency (EE) and reliability under dynamic\nchannel and interference conditions. Unlike prior work focusing on average\nreliability, our method explicitly targets reducing burst/consecutive outages\nthrough adaptive control of transmit power and blocklength based solely on the\nobserved signal-to-interference-plus-noise ratio (SINR). The joint optimization\nproblem is formulated under finite blocklength and quality of service\nconstraints, balancing reliability and EE. Simulation results show that the\nproposed method significantly outperforms the baseline algorithms, reducing\noutage bursts while consuming only 18\\% of the transmission cost required by a\nfull/maximum resource allocation policy in the evaluated scenario. The\nframework also supports flexible trade-off tuning between EE and reliability by\nadjusting reward weights, making it adaptable to diverse industrial\nrequirements.", "AI": {"tldr": "提出了一种基于SAC-DRL的链路自适应框架，用于6G网络中URLLC的极端可靠性需求，优化能量效率和可靠性。", "motivation": "解决连续数据包中断问题，以稳定控制环路并确保工业环境安全。", "method": "使用SAC-DRL算法，动态调整发射功率和块长度，基于观测到的SINR。", "result": "显著优于基线算法，减少中断突发，传输成本仅为最大资源分配策略的18%。", "conclusion": "该框架支持灵活调整能量效率与可靠性的权衡，适应多样化的工业需求。"}}
{"id": "2507.11852", "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.11852", "abs": "https://arxiv.org/abs/2507.11852", "authors": ["Mohammed Hassanin", "Mohammad Abu Alsheikh", "Carlos C. N. Kuhn", "Damith Herath", "Dinh Thai Hoang", "Ibrahim Radwan"], "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "comment": "17 pages", "summary": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility.", "AI": {"tldr": "综述分析了自主骑行（AR）技术的现状与挑战，借鉴自动驾驶（AD）技术，指出当前研究的不足并提出未来方向。", "motivation": "随着微出行工具（如电动滑板车和电动自行车）的快速普及，开发可靠的自主骑行技术成为迫切需求。", "method": "通过系统分析AR的核心组件（感知、规划、控制），并借鉴AD技术，识别当前研究的不足。", "result": "发现AR研究存在感知系统不全面、行业和政府支持不足、研究关注度低等问题。", "conclusion": "结合AD技术与AR需求，提出多模态传感器和边缘深度学习等方向，以推动安全高效的AR系统发展。"}}
{"id": "2507.12237", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12237", "abs": "https://arxiv.org/abs/2507.12237", "authors": ["Matthias Wjst"], "title": "Constructed Realities? Technical and Contextual Anomalies in a High-Profile Image", "comment": "32 pages, 8 figures, 36 references", "summary": "This study offers a forensic assessment of a widely circulated photograph\nfeaturing Prince Andrew, Virginia Giuffre, and Ghislaine Maxwell - an image\nthat has played a pivotal role in public discourse and legal narratives.\nThrough analysis of multiple published versions, several inconsistencies are\nidentified, including irregularities in lighting, posture, and physical\ninteraction, which are more consistent with digital compositing than with an\nunaltered snapshot. While the absence of the original negative and a verifiable\naudit trail precludes definitive conclusions, the technical and contextual\nanomalies suggest that the image may have been deliberately constructed.\nNevertheless, without additional evidence, the photograph remains an unresolved\nbut symbolically charged fragment within a complex story of abuse, memory, and\ncontested truth.", "AI": {"tldr": "对一张涉及安德鲁王子、弗吉尼亚·朱弗尔和吉斯莱恩·麦克斯韦的广泛传播照片进行法医分析，发现其可能存在数字合成的迹象。", "motivation": "该照片在公共讨论和法律叙事中起关键作用，但存在不一致性，需验证其真实性。", "method": "通过分析多个已发布版本，检查光线、姿势和肢体互动等不一致性。", "result": "发现技术性和上下文异常，表明照片可能是人为构造的，但无法完全确认。", "conclusion": "照片的真实性未定，但其象征意义在复杂的虐待、记忆和真相争议中仍重要。"}}
{"id": "2507.11742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11742", "abs": "https://arxiv.org/abs/2507.11742", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Ludäscher"], "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "论文提出了一种名为CRABS的方法，通过结合浅层语法分析和LLM，解决Python笔记本中信息流和依赖关系的理解问题。", "motivation": "由于数据科学和机器学习笔记本的复杂性和依赖性问题，直接重新执行笔记本往往不切实际，而现有LLM在理解笔记本时存在幻觉和长上下文挑战。", "method": "CRABS方法结合浅层语法分析（如AST解析）和LLM的零样本学习，捕捉笔记本的上下界估计，并解决剩余歧义。", "result": "在50个Kaggle笔记本上测试，LLM成功解决了98%的歧义，CRABS在信息流和执行依赖识别上分别达到98%和99%的F1分数。", "conclusion": "CRABS方法通过结合语法分析和LLM，显著提高了笔记本信息流和依赖关系的理解准确性。"}}
{"id": "2507.11571", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11571", "abs": "https://arxiv.org/abs/2507.11571", "authors": ["Varun Velankar"], "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation", "comment": null, "summary": "Estimating a person's age from their gait has important applications in\nhealthcare, security and human-computer interaction. In this work, we review\nfifty-nine studies involving over seventy-five thousand subjects recorded with\nvideo, wearable and radar sensors. We observe that convolutional neural\nnetworks produce an average error of about 4.2 years, inertial-sensor models\nabout 4.5 years and multi-sensor fusion as low as 3.4 years, with notable\ndifferences between lab and real-world data. We then analyse sixty-three\nthousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population\ndataset to quantify correlations between age and five key metrics: stride\nlength, walking speed, step cadence, step-time variability and joint-angle\nentropy, with correlation coefficients of at least 0.27. Next, we fine-tune a\nResNet34 model and apply Grad-CAM to reveal that the network attends to the\nknee and pelvic regions, consistent with known age-related gait changes.\nFinally, on a one hundred thousand sample subset of the VersatileGait database,\nwe compare support vector machines, decision trees, random forests, multilayer\nperceptrons and convolutional neural networks, finding that deep networks\nachieve up to 96 percent accuracy while processing each sample in under 0.1\nseconds. By combining a broad meta-analysis with new large-scale experiments\nand interpretable visualizations, we establish solid performance baselines and\npractical guidelines for reducing gait-age error below three years in\nreal-world scenarios.", "AI": {"tldr": "本文通过综述59项研究和分析大规模数据集，评估了基于步态估计年龄的方法，发现多传感器融合误差最低（3.4年），并揭示了步态指标与年龄的相关性。", "motivation": "步态年龄估计在医疗、安全和人机交互中有重要应用，但现有研究缺乏统一的性能基准和实用指南。", "method": "综述59项研究，分析大规模数据集（如OU-ISIR和VersatileGait），使用卷积神经网络（ResNet34）和支持向量机等方法进行实验。", "result": "多传感器融合误差最低（3.4年），步态指标与年龄相关性显著（相关系数≥0.27），深度网络准确率达96%。", "conclusion": "通过综合分析和大规模实验，提出了降低步态年龄估计误差至3年以下的实用指南，并建立了性能基准。"}}
{"id": "2507.12110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12110", "abs": "https://arxiv.org/abs/2507.12110", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "comment": "16 pages, 16 figures", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.", "AI": {"tldr": "论文提出了一种拓扑增强的多智能体强化学习方法（TPE-MARL），用于优化混合交通中联网自动驾驶车辆（CAV）的协作决策，通过压缩高维状态信息和减少搜索空间，显著提升了性能。", "motivation": "解决多智能体强化学习（MARL）中探索-利用权衡的挑战，特别是在混合交通环境下，CAV的协作决策问题。", "method": "构建动态交通流的游戏拓扑张量，压缩高维状态信息；基于QMIX算法，结合访问计数和智能体互信息，建立拓扑增强的MARL框架。", "result": "TPE-MARL在多种交通密度和CAV渗透率下表现出色，平衡了探索与利用，提升了交通效率、安全性、决策平滑性和任务完成度。", "conclusion": "TPE-MARL在混合和全自动驾驶场景中表现出优于或等同于人类驾驶员的决策合理性，具有实际应用潜力。"}}
{"id": "2507.12052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12052", "abs": "https://arxiv.org/abs/2507.12052", "authors": ["Takumi Shinohara", "Karl H. Johansson", "Henrik Sandberg"], "title": "Distributed Resilient State Estimation and Control with Strategically Implemented Security Measures", "comment": null, "summary": "This paper addresses the problem of distributed resilient state estimation\nand control for linear time-invariant systems in the presence of malicious\nfalse data injection sensor attacks and bounded noise. We consider a system\noperator (defender) capable of deploying cybersecurity measures to counteract\nthe sensor compromises. Although such measures enhance resilience against\nadversarial attacks, they may incur substantial costs; hence, it is crucial to\nselect countermeasures to balance resilience gains and cost efficiency\nstrategically. We first demonstrate that the system's resilience against\nattacks is maximized through the appropriate implementation of security\nmeasures, implying that no attacker can execute undetectable sensor attacks.\nBuilding on this analysis, we propose an algorithm that identifies the optimal\nsecurity measure. While determining this measure is NP-hard in general, we also\nderive sufficient conditions under which efficient computation is feasible.\nFurthermore, we develop a distributed resilient state estimation and control\nscheme informed by the optimal security measure and establish conditions that\nguarantee bounded estimation and control errors. Finally, we validate the\nefficacy of our approach via numerical simulations of a vehicle platooning\nscenario.", "AI": {"tldr": "论文研究了分布式弹性状态估计与控制问题，针对线性时不变系统在恶意传感器攻击和有界噪声下的表现，提出了一种优化安全措施的方法。", "motivation": "解决在恶意传感器攻击和有界噪声下，系统如何通过部署网络安全措施实现弹性状态估计与控制，同时平衡成本与效果。", "method": "提出了一种算法，用于识别最优安全措施，并开发了分布式弹性状态估计与控制方案。", "result": "证明了在适当条件下，系统能有效抵御攻击，且算法在特定情况下可高效计算。数值模拟验证了方法的有效性。", "conclusion": "通过优化安全措施，系统能显著提升对攻击的弹性，同时保证估计与控制误差的有界性。"}}
{"id": "2507.11880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11880", "abs": "https://arxiv.org/abs/2507.11880", "authors": ["Jinyuan Liu", "Minglei Fu", "Ling Shi", "Chenguang Yang", "Wenan Zhang"], "title": "A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications", "comment": "37 pages, 33 figures", "summary": "Tethered robots play a pivotal role in specialized environments such as\ndisaster response and underground exploration, where their stable power supply\nand reliable communication offer unparalleled advantages. However, their motion\nplanning is severely constrained by tether length limitations and entanglement\nrisks, posing significant challenges to achieving optimal path planning. To\naddress these challenges, this study introduces CDT-TCS (Convex Dissection\nTopology-based Tethered Configuration Search), a novel algorithm that leverages\nCDT Encoding as a homotopy invariant to represent topological states of paths.\nBy integrating algebraic topology with geometric optimization, CDT-TCS\nefficiently computes the complete set of optimal feasible configurations for\ntethered robots at all positions in 2D environments through a single\ncomputation. Building on this foundation, we further propose three\napplication-specific algorithms: i) CDT-TPP for optimal tethered path planning,\nii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for\ndistance-optimal path planning of untethered robots. All theoretical results\nand propositions underlying these algorithms are rigorously proven and\nthoroughly discussed in this paper. Extensive simulations demonstrate that the\nproposed algorithms significantly outperform state-of-the-art methods in their\nrespective problem domains. Furthermore, real-world experiments on robotic\nplatforms validate the practicality and engineering value of the proposed\nframework.", "AI": {"tldr": "本文提出了一种名为CDT-TCS的新算法，用于解决系留机器人在运动规划中的限制问题，并通过三种应用算法进一步优化路径规划。", "motivation": "系留机器人在特殊环境（如灾难响应和地下探索）中具有稳定供电和可靠通信的优势，但其运动规划受限于系留长度和缠绕风险，亟需高效解决方案。", "method": "CDT-TCS算法利用CDT编码作为同伦不变量表示路径拓扑状态，结合代数拓扑和几何优化，一次性计算2D环境中所有位置的最优可行配置。", "result": "仿真和实验表明，该算法在各自问题领域中显著优于现有方法，并验证了其工程实用性。", "conclusion": "CDT-TCS及其衍生算法为系留和自由机器人的路径规划提供了高效且实用的解决方案。"}}
{"id": "2507.12427", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12427", "abs": "https://arxiv.org/abs/2507.12427", "authors": ["Ashkan Shakarami", "Azade Farshad", "Yousef Yeganeh", "Lorenzo Nicole", "Peter Schuffler", "Stefano Ghidoni", "Nassir Navab"], "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation", "comment": "12 pages, 6 figures", "summary": "We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.", "AI": {"tldr": "UTS是一种基于单元的组织分割框架，通过分类固定大小的32*32图块而非单个像素，减少标注工作量并提高计算效率，同时保持准确性。", "motivation": "解决传统像素级分割方法在组织病理学中标注工作量大和计算效率低的问题。", "method": "提出多级视觉变换器（L-ViT），利用多级特征表示捕捉细粒度形态和全局组织上下文。", "result": "在459个H&E染色区域的386,371个图块上评估，UTS优于U-Net变体和基于变换器的基线方法。", "conclusion": "UTS在减少标注和计算成本的同时，保持了分割准确性，适用于临床任务如肿瘤-间质定量和手术边缘评估。"}}
{"id": "2507.11764", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.11764", "abs": "https://arxiv.org/abs/2507.11764", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "AI Wizards团队在CLEF 2025 CheckThat! Lab任务1中，通过结合情感分数与句子表示，提升了基于Transformer的分类器在主观性检测任务中的表现，尤其在零样本和多语言场景下取得了显著成果。", "motivation": "研究旨在改进新闻文章中句子主观性检测的准确性，特别是在多语言和零样本设置下，以评估模型的泛化能力。", "method": "团队采用基于Transformer的模型（如mDeBERTaV3-base、ModernBERT-base和Llama3.2-1B），并通过集成情感分数（来自辅助模型）与句子表示来增强分类器性能。同时，针对类别不平衡问题，使用了决策阈值校准。", "result": "实验表明，情感特征的集成显著提升了性能，尤其是主观F1分数。团队在希腊语任务中取得了第一名（Macro F1 = 0.51）。", "conclusion": "通过结合情感特征和句子表示，该方法在多语言和零样本主观性检测任务中表现优异，展示了其泛化能力和实用性。"}}
{"id": "2507.11575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11575", "abs": "https://arxiv.org/abs/2507.11575", "authors": ["Victor Caquilpan"], "title": "What cat is that? A re-id model for feral cats", "comment": "Master's project", "summary": "Feral cats exert a substantial and detrimental impact on Australian wildlife,\nplacing them among the most dangerous invasive species worldwide. Therefore,\nclosely monitoring these cats is essential labour in minimising their effects.\nIn this context, the potential application of Re-Identification (re-ID) emerges\nto enhance monitoring activities for these animals, utilising images captured\nby camera traps. This project explores different CV approaches to create a\nre-ID model able to identify individual feral cats in the wild. The main\napproach consists of modifying a part-pose guided network (PPGNet) model,\ninitially used in the re-ID of Amur tigers, to be applicable for feral cats.\nThis adaptation, resulting in PPGNet-Cat, which incorporates specific\nmodifications to suit the characteristics of feral cats images. Additionally,\nvarious experiments were conducted, particularly exploring contrastive learning\napproaches such as ArcFace loss. The main results indicate that PPGNet-Cat\nexcels in identifying feral cats, achieving high performance with a mean\nAverage Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes\nestablish PPGNet-Cat as a competitive model within the realm of re-ID.", "AI": {"tldr": "研究提出了一种基于PPGNet改进的模型PPGNet-Cat，用于通过摄像头图像识别野生野猫个体，取得了高精度结果。", "motivation": "野猫对澳大利亚野生动物造成严重威胁，需要高效监控以减少其影响。", "method": "改进PPGNet模型（原用于识别阿穆尔虎）为PPGNet-Cat，并探索对比学习方法如ArcFace损失。", "result": "PPGNet-Cat表现优异，mAP达0.86，rank-1准确率为0.95。", "conclusion": "PPGNet-Cat是一种高效的野猫识别模型，适用于实际监控需求。"}}
{"id": "2507.12186", "categories": ["cs.AI", "I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12186", "abs": "https://arxiv.org/abs/2507.12186", "authors": ["Edward Kim", "Hanna Kurniawati"], "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint\n  Conference on Artificial Intelligence 2025", "summary": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks.", "AI": {"tldr": "提出了一种新的在线近似POMDP求解器，通过深度采样未来历史并逐步更新策略，性能损失由采样误差的平均值而非最大值决定。", "motivation": "解决在线规划中采样稀疏性问题，提升动态环境下的决策性能。", "method": "采用部分可观测参考策略编程，深度采样未来历史并强制逐步策略更新。", "result": "在动态环境的大规模问题（如直升机紧急场景）中显著优于现有在线基准。", "conclusion": "算法在理论和实验上均表现出色，适用于复杂动态环境。"}}
{"id": "2507.12082", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12082", "abs": "https://arxiv.org/abs/2507.12082", "authors": ["Theofilos Papadopoulos", "Antonios Antonopoulos"], "title": "Inductance Estimation for High-Power Multilayer Rectangle Planar Windings", "comment": "7 pages, 7 figures, IEEE Journal of Emerging and Selected Topics in\n  Industrial Electronics, 2025", "summary": "This paper proposes a simple and accurate monomial-like equation for\nestimating the inductance of Multilayer Rectangle-shaped Planar Windings\n(MLRPWs) for high-frequency, high-power applications. The equation consists of\nthe power product of the geometrical dimensions, raised at individual power\ncoefficients. The coefficients are generated via Multiple Linear Regression\n(MLR), based on a large set of approximately 6,000 simulated windings, with an\n80/20 training/evaluation sample ratio. The resulting mean error value is 0%,\nwith a standard deviation below 1.8%. The accuracy of the inductance estimation\nis confirmed on several experimental samples, with dimensions both within and\noutside the initial training dataset.", "AI": {"tldr": "提出了一种简单且准确的单形式方程，用于估算多层矩形平面绕组（MLRPWs）的电感，适用于高频高功率应用。", "motivation": "为高频高功率应用中的多层矩形平面绕组提供准确的电感估算方法。", "method": "通过多元线性回归（MLR）生成几何尺寸的幂乘积方程，基于约6,000个模拟绕组数据，训练/评估比例为80/20。", "result": "平均误差为0%，标准差低于1.8%，并在实验样本中验证了准确性。", "conclusion": "该方法能够准确估算电感，适用于训练数据集内外的尺寸范围。"}}
{"id": "2507.11889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11889", "abs": "https://arxiv.org/abs/2507.11889", "authors": ["Adnan Abdullah", "Alankrit Gupta", "Vaishnav Ramesh", "Shivali Patel", "Md Jahidul Islam"], "title": "NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy", "comment": "10 pages, V1", "summary": "Adaptive mission control and dynamic parameter reconfiguration are essential\nfor autonomous underwater vehicles (AUVs) operating in GPS-denied,\ncommunication-limited marine environments. However, most current AUV platforms\nexecute static, pre-programmed missions or rely on tethered connections and\nhigh-latency acoustic channels for mid-mission updates, significantly limiting\ntheir adaptability and responsiveness. In this paper, we introduce NemeSys, a\nnovel AUV system designed to support real-time mission reconfiguration through\ncompact optical and magnetoelectric (OME) signaling facilitated by floating\nbuoys. We present the full system design, control architecture, and a semantic\nmission encoding framework that enables interactive exploration and task\nadaptation via low-bandwidth communication. The proposed system is validated\nthrough analytical modeling, controlled experimental evaluations, and\nopen-water trials. Results confirm the feasibility of online mission adaptation\nand semantic task updates, highlighting NemeSys as an online AUV platform for\ngoal-driven adaptive autonomy in dynamic and uncertain underwater environments.", "AI": {"tldr": "NemeSys是一种新型AUV系统，通过光学和磁电信号实现实时任务重配置，支持动态水下任务适应。", "motivation": "当前AUV在GPS缺失和通信受限环境中依赖静态任务或高延迟更新，限制了适应性和响应能力。", "method": "提出NemeSys系统，采用光学和磁电信号及浮标通信，设计控制架构和语义任务编码框架。", "result": "通过建模、实验和开放水域测试验证了在线任务适应和语义更新的可行性。", "conclusion": "NemeSys为动态水下环境中的目标驱动自适应AUV提供了有效解决方案。"}}
{"id": "2507.12432", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12432", "abs": "https://arxiv.org/abs/2507.12432", "authors": ["Andreas Habring", "Martin Holler", "Thomas Pock", "Martin Zach"], "title": "Energy-based models for inverse imaging problems", "comment": null, "summary": "In this chapter we provide a thorough overview of the use of energy-based\nmodels (EBMs) in the context of inverse imaging problems. EBMs are probability\ndistributions modeled via Gibbs densities $p(x) \\propto \\exp{-E(x)}$ with an\nappropriate energy functional $E$. Within this chapter we present a rigorous\ntheoretical introduction to Bayesian inverse problems that includes results on\nwell-posedness and stability in the finite-dimensional and infinite-dimensional\nsetting. Afterwards we discuss the use of EBMs for Bayesian inverse problems\nand explain the most relevant techniques for learning EBMs from data. As a\ncrucial part of Bayesian inverse problems, we cover several popular algorithms\nfor sampling from EBMs, namely the Metropolis-Hastings algorithm, Gibbs\nsampling, Langevin Monte Carlo, and Hamiltonian Monte Carlo. Moreover, we\npresent numerical results for the resolution of several inverse imaging\nproblems obtained by leveraging an EBM that allows for the explicit\nverification of those properties that are needed for valid energy-based\nmodeling.", "AI": {"tldr": "本文综述了基于能量模型（EBMs）在逆成像问题中的应用，包括理论介绍、学习方法、采样算法及数值结果。", "motivation": "探讨EBMs在解决逆成像问题中的潜力，提供理论支持和实际应用验证。", "method": "介绍了EBMs的理论基础，包括贝叶斯逆问题的适定性和稳定性，以及从数据中学习EBMs的技术。讨论了Metropolis-Hastings、Gibbs采样、Langevin Monte Carlo和Hamiltonian Monte Carlo等采样算法。", "result": "通过数值实验验证了EBMs在逆成像问题中的有效性，并展示了其建模所需的关键性质。", "conclusion": "EBMs为逆成像问题提供了强大的建模工具，理论支持和实际应用均证明了其潜力。"}}
{"id": "2507.11809", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11809", "abs": "https://arxiv.org/abs/2507.11809", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "comment": "18 Pages, 13 figures", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "AI": {"tldr": "本文通过可重复性研究探讨了大型语言模型（LLMs）如何处理竞争性事实与反事实信息，重点关注注意力头的作用。", "motivation": "研究旨在复现并调和Ortu等人、Yu、Merullo、Pavlick以及McDougall等人的三项近期研究，这些研究通过机制解释工具探讨了模型学习事实与矛盾上下文信息之间的竞争。", "method": "研究分析了注意力头强度与事实输出比例的关系，评估了关于注意力头抑制机制的竞争性假设，并探讨了这些注意力模式的领域特异性。", "result": "研究发现，促进事实输出的注意力头通过通用的复制抑制而非选择性反事实抑制实现其作用，且注意力头行为具有领域依赖性，较大模型表现出更专业和类别敏感的模式。", "conclusion": "注意力头通过通用机制而非选择性抑制处理竞争信息，且其行为受模型规模和领域影响。"}}
{"id": "2507.11579", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11579", "abs": "https://arxiv.org/abs/2507.11579", "authors": ["Sathvik Chereddy", "John Femiani"], "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation", "comment": "17 pages, 63 figures, Proceedings of the 42nd International\n  Conference on Machine Learning (ICML2025)", "summary": "We present SketchDNN, a generative model for synthesizing CAD sketches that\njointly models both continuous parameters and discrete class labels through a\nunified continuous-discrete diffusion process. Our core innovation is\nGaussian-Softmax diffusion, where logits perturbed with Gaussian noise are\nprojected onto the probability simplex via a softmax transformation,\nfacilitating blended class labels for discrete variables. This formulation\naddresses 2 key challenges, namely, the heterogeneity of primitive\nparameterizations and the permutation invariance of primitives in CAD sketches.\nOur approach significantly improves generation quality, reducing Fr\\'echet\nInception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)\nfrom 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch\ngeneration on the SketchGraphs dataset.", "AI": {"tldr": "SketchDNN提出了一种生成CAD草图的模型，通过统一的连续-离散扩散过程联合建模连续参数和离散类别标签，显著提升了生成质量。", "motivation": "解决CAD草图中原始参数化的异质性和原始元素的排列不变性两大挑战。", "method": "采用Gaussian-Softmax扩散，通过高斯噪声扰动logit并通过softmax变换投影到概率单纯形，实现离散变量的混合类别标签。", "result": "在SketchGraphs数据集上，FID从16.04降至7.80，NLL从84.8降至81.33，达到新的最优性能。", "conclusion": "SketchDNN在CAD草图生成中实现了显著的性能提升，为相关领域提供了新的解决方案。"}}
{"id": "2507.12207", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.12207", "abs": "https://arxiv.org/abs/2507.12207", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "comment": "ICML 2025 CO-Build Workshop Poster", "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.", "AI": {"tldr": "BuildEvo利用大型语言模型（LLM）自动设计高效且可解释的建筑能耗预测启发式方法，结合物理原理和数据，性能优于现有方法。", "motivation": "传统启发式方法精度不足，而高级模型缺乏透明性和泛化能力，忽视物理原理。", "method": "通过进化过程，引导LLM构建和优化启发式方法，结合建筑特征和运行数据的物理洞察。", "result": "BuildEvo在基准测试中达到最优性能，泛化能力更强且预测逻辑透明。", "conclusion": "该研究推动了自动化设计基于物理的鲁棒启发式方法，为复杂能源系统提供可信模型。"}}
{"id": "2507.12163", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12163", "abs": "https://arxiv.org/abs/2507.12163", "authors": ["Pramit Karmakar", "Siddharth B", "Chinmay Murlidhar Kadnur Rao"], "title": "Integrated Switched Capacitor Array and Synchronous Charge Extraction with Adaptive Hybrid MPPT for Piezoelectric Harvesters", "comment": null, "summary": "Energy Harvesting technologies will play a fundamental role in the\ndevelopment of the next generation of electronic systems as well as in\nadvancing the development of sustainable infrastructure. One of the critical\nchallenges in EH is utilizing ambient vibrations to harvest energy. Piezo\nEnergy Harvesting, which uses ambient vibrations, is a promising technology in\nenergy harvesting and a self-powered technology. However, it suffers from\nseveral practical challenges. Some of these challenges include narrow\nbandwidth, non-linearity, and impedance mismatch, among others. This paper\npresents a novel, simulated Piezo Energy Harvesting (PEH) framework that\naddresses some of these challenges. The proposed model is designed to be\nadaptive and effective against the inherent non-linearity of PEH. This detailed\nmodel covers a non-linear piezo, Synchronous Electric Charge Extraction (SECE),\nHybrid Maximum Power Point Tracking (MPPT) and a Switched Capacitor Array\n(SCA). The SECE extracts the maximum charge accumulated on the piezo every time\nthe piezo reaches the mechanical extremum. The Bouc-Wen model has been used to\nestablish nonlinearity in the system. The hybrid MPPT exhibits significant\nimprovement over conventional P&O, while the SCA-tuned system demonstrates\nresilience against variable frequency input.", "AI": {"tldr": "本文提出了一种新型的压电能量收集（PEH）框架，解决了窄带宽、非线性和阻抗不匹配等问题，通过SECE、混合MPPT和SCA等技术提高了效率。", "motivation": "压电能量收集技术虽前景广阔，但面临带宽窄、非线性等挑战，亟需改进。", "method": "采用非线性压电模型、SECE电荷提取、混合MPPT和SCA调谐系统，结合Bouc-Wen模型描述非线性。", "result": "混合MPPT显著优于传统P&O，SCA系统对变频输入表现出鲁棒性。", "conclusion": "提出的PEH框架有效解决了现有问题，为自供电技术发展提供了新思路。"}}
{"id": "2507.11920", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11920", "abs": "https://arxiv.org/abs/2507.11920", "authors": ["Jeongyong Yang", "KwangBin Lee", "SooJean Han"], "title": "Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments", "comment": null, "summary": "Real-time path planning in dense, uncertain environments remains a\nchallenging problem, as predicting the future motions of numerous dynamic\nobstacles is computationally burdensome and unrealistic. To address this, we\nintroduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a\nprediction-based risk-aware path-planning framework which uses a hybrid\ncombination of models to predict local obstacle movement. HyPRAP uses a novel\nPrediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by\neach obstacle, enabling the selective use of predictors based on whether the\nagent prioritizes high predictive accuracy or low computational prediction\noverhead. This selective routing enables the agent to focus on high-risk\nobstacles while ignoring or simplifying low-risk ones, making it suitable for\nenvironments with a large number of obstacles. Moreover, HyPRAP incorporates\nuncertainty quantification through hybrid conformal prediction by deriving\nconfidence bounds simultaneously achieved by multiple predictions across\ndifferent models. Theoretical analysis demonstrates that HyPRAP effectively\nbalances safety and computational efficiency by leveraging the diversity of\nprediction models. Extensive simulations validate these insights for more\ngeneral settings, confirming that HyPRAP performs better compared to single\npredictor methods, and P-CRI performs better over naive proximity-based risk\nassessment.", "AI": {"tldr": "HyPRAP是一种基于混合预测的风险感知路径规划框架，通过选择性使用预测模型和新型碰撞风险指数（P-CRI），在密集动态环境中平衡安全性和计算效率。", "motivation": "解决密集动态环境中实时路径规划的挑战，特别是预测大量动态障碍物未来运动的计算负担和不现实性。", "method": "HyPRAP结合多种预测模型，利用P-CRI评估障碍物风险，选择性使用预测器，并通过混合共形预测量化不确定性。", "result": "理论分析和仿真表明，HyPRAP优于单一预测方法，P-CRI比基于邻近性的风险评估更有效。", "conclusion": "HyPRAP在密集动态环境中实现了安全性和计算效率的平衡，适用于大规模障碍物场景。"}}
{"id": "2507.11967", "categories": ["cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11967", "abs": "https://arxiv.org/abs/2507.11967", "authors": ["Yuchi Ishikawa", "Shota Nakada", "Hokuto Munakata", "Kazuhiro Saito", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos", "comment": "Interspeech 2025", "summary": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked\nAutoencoders (LG-CAV-MAE) to improve audio-visual representation learning.\nLG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual\nmasked autoencoders, enabling the model to learn across audio, visual and text\nmodalities. To train LG-CAV-MAE, we introduce an automatic method to generate\naudio-visual-text triplets from unlabeled videos. We first generate frame-level\ncaptions using an image captioning model and then apply CLAP-based filtering to\nensure strong alignment between audio and captions. This approach yields\nhigh-quality audio-visual-text triplets without requiring manual annotations.\nWe evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an\naudio-visual classification task. Our method significantly outperforms existing\napproaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks\nand a 3.2% improvement for the classification task.", "AI": {"tldr": "提出LG-CAV-MAE模型，通过语言引导提升音频-视觉表示学习，结合文本编码器实现多模态学习，自动生成高质量音频-视觉-文本三元组，显著优于现有方法。", "motivation": "提升音频-视觉表示学习，通过语言引导和多模态结合解决现有方法的局限性。", "method": "集成预训练文本编码器到对比音频-视觉掩码自编码器中，自动生成音频-视觉-文本三元组，使用CLAP过滤确保对齐。", "result": "在检索任务中recall@10提升5.6%，分类任务提升3.2%。", "conclusion": "LG-CAV-MAE在多模态学习任务中表现优异，无需人工标注即可生成高质量数据。"}}
{"id": "2507.11832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11832", "abs": "https://arxiv.org/abs/2507.11832", "authors": ["Yash Ingle", "Pruthwik Mishra"], "title": "ILID: Native Script Language Identification for Indian Languages", "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "AI": {"tldr": "论文提出了一种针对印度语言的语言识别任务，发布了包含23万句子的数据集，并开发了基于机器学习和深度学习的基线模型。", "motivation": "语言识别是NLP的基础任务，尤其在多语言环境中面临噪声、短文本和代码混合的挑战，印度语言因共享脚本和相似性而更具难度。", "method": "构建了包含英语和22种印度官方语言的数据集，并采用机器学习和深度学习的最新方法开发基线模型。", "result": "基线模型在语言识别任务中表现与当前最优模型相当。", "conclusion": "研究为印度语言识别提供了新数据集和强基线模型，推动了该领域的发展。"}}
{"id": "2507.11638", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11638", "abs": "https://arxiv.org/abs/2507.11638", "authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Maryam Mohsin", "Samuel D. Relton"], "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders", "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Effective treatment for rectal cancer relies on accurate lymph node\nmetastasis (LNM) staging. However, radiological criteria based on lymph node\n(LN) size, shape and texture morphology have limited diagnostic accuracy. In\nthis work, we investigate applying a Variational Autoencoder (VAE) as a feature\nencoder model to replace the large pre-trained Convolutional Neural Network\n(CNN) used in existing approaches. The motivation for using a VAE is that the\ngenerative model aims to reconstruct the images, so it directly encodes visual\nfeatures and meaningful patterns across the data. This leads to a disentangled\nand structured latent space which can be more interpretable than a CNN. Models\nare deployed on an in-house MRI dataset with 168 patients who did not undergo\nneo-adjuvant treatment. The post-operative pathological N stage was used as the\nground truth to evaluate model predictions. Our proposed model 'VAE-MLP'\nachieved state-of-the-art performance on the MRI dataset, with cross-validated\nmetrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85\n+/- 0.05. Code is available at:\nhttps://github.com/benkeel/Lymph_Node_Classification_MIUA.", "AI": {"tldr": "使用变分自编码器（VAE）替代传统CNN，提高直肠癌淋巴结转移分期的准确性，在MRI数据集上取得SOTA性能。", "motivation": "现有基于淋巴结大小、形状和纹理的放射学标准诊断准确性有限，VAE能直接编码视觉特征和有意义的数据模式，生成更可解释的潜在空间。", "method": "提出VAE-MLP模型，利用VAE作为特征编码器，在168例未接受新辅助治疗的患者的MRI数据集上进行验证。", "result": "模型在MRI数据集上表现优异，交叉验证指标为AUC 0.86 +/- 0.05，敏感性0.79 +/- 0.06，特异性0.85 +/- 0.05。", "conclusion": "VAE-MLP模型在直肠癌淋巴结转移分期中具有高准确性和可解释性，优于现有方法。"}}
{"id": "2507.12215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12215", "abs": "https://arxiv.org/abs/2507.12215", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "comment": "10 pages, 7 figures", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在空间战略推理（如中国象棋）中的不足，并提出了一种针对中国象棋的训练框架Xiangqi-R1，显著提升了模型表现。", "motivation": "评估LLMs在空间战略推理中的能力，填补其在复杂棋盘游戏中的研究空白。", "method": "采用多阶段训练框架：1）微调合法移动预测；2）融入战略注释；3）通过GRPO强化学习提升推理稳定性。", "result": "Xiangqi-R1在移动合法性和分析准确性上分别提升了18%和22%，优于通用LLMs。", "conclusion": "该研究为在空间复杂领域开发通用战略智能提供了可行路径。"}}
{"id": "2507.12187", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12187", "abs": "https://arxiv.org/abs/2507.12187", "authors": ["Laura Boca de Giuli", "Alessio La Bella", "Riccardo Scattolini"], "title": "Learning, fast and slow: a two-fold algorithm for data-based model adaptation", "comment": null, "summary": "This article addresses the challenge of adapting data-based models over time.\nWe propose a novel two-fold modelling architecture designed to correct\nplant-model mismatch caused by two types of uncertainty. Out-of-domain\nuncertainty arises when the system operates under conditions not represented in\nthe initial training dataset, while in-domain uncertainty results from\nreal-world variability and flaws in the model structure or training process. To\nhandle out-of-domain uncertainty, a slow learning component, inspired by the\nhuman brain's slow thinking process, learns system dynamics under unexplored\noperating conditions, and it is activated only when a monitoring strategy deems\nit necessary. This component consists of an ensemble of models, featuring (i) a\ncombination rule that weights individual models based on the statistical\nproximity between their training data and the current operating condition, and\n(ii) a monitoring algorithm based on statistical control charts that supervises\nthe ensemble's reliability and triggers the offline training and integration of\na new model when a new operating condition is detected. To address in-domain\nuncertainty, a fast learning component, inspired by the human brain's fast\nthinking process, continuously compensates in real time for the mismatch of the\nslow learning model. This component is implemented as a Gaussian process (GP)\nmodel, trained online at each iteration using recent data while discarding\nolder samples. The proposed methodology is tested on a benchmark energy system\nreferenced in the literature, demonstrating that the combined use of slow and\nfast learning components improves model accuracy compared to standard\nadaptation approaches.", "AI": {"tldr": "提出一种新颖的双重建模架构，分别处理域外和域内不确定性，通过慢学习和快学习组件提升模型准确性。", "motivation": "解决数据驱动模型随时间适应性的挑战，特别是由域外和域内不确定性引起的模型不匹配问题。", "method": "慢学习组件处理域外不确定性，采用模型集成和监控策略；快学习组件处理域内不确定性，使用高斯过程模型实时补偿。", "result": "在基准能源系统上测试，双重组件结合使用比标准方法显著提高了模型准确性。", "conclusion": "提出的双重架构有效解决了模型适应性问题，为动态系统的建模提供了新思路。"}}
{"id": "2507.11938", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11938", "abs": "https://arxiv.org/abs/2507.11938", "authors": ["Hao Chen", "Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "comment": "Accepted by IEEE T-RO", "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.", "AI": {"tldr": "论文提出了一种基于相似性匹配的新方法，通过匹配已知物体来指导未知物体的抓取，解决了单视角下抓取的不确定性问题。", "motivation": "传统学习方法对感知噪声和环境变化敏感，性能鲁棒性不足，因此需要一种更通用的抓取方法。", "method": "通过视觉特征匹配已知数据库、利用候选模型的抓取知识规划模仿抓取、局部微调优化抓取质量，并引入多级相似性匹配框架和C-FPFH描述符。", "result": "提出的方法在单视角下实现了对未知物体的鲁棒抓取，并通过多级匹配和新描述符提高了准确性。", "conclusion": "相似性匹配方法为未知物体抓取提供了新视角，显著提升了鲁棒性和通用性。"}}
{"id": "2507.11851", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11851", "abs": "https://arxiv.org/abs/2507.11851", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "comment": null, "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "AI": {"tldr": "提出了一种新框架，通过多令牌预测提高自回归语言模型的推理速度和并行性。", "motivation": "自回归语言模型的顺序生成限制推理速度和并行性，尤其在文本方向和语义确定后。", "method": "结合掩码输入、门控LoRA、轻量采样器、辅助训练损失和推测生成策略。", "result": "代码和数学生成速度提升近5倍，通用聊天和知识任务提升2.5倍，且质量无损。", "conclusion": "新框架显著提升生成效率，同时保持生成质量。"}}
{"id": "2507.11642", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11642", "abs": "https://arxiv.org/abs/2507.11642", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "comment": null, "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "AI": {"tldr": "论文提出了一种基于姿势的心理状态推断方法，特别关注板球运动中攻击性和防守性击球意图的识别，通过运动分析取得了较高的准确率。", "motivation": "姿势推断在诊断疲劳、预防伤害和提升表现方面潜力巨大，但面临数据敏感性问题。体育场景被选为替代方案以积累多样化情绪状态数据。", "method": "利用板球比赛视频，通过姿势分析识别击球意图，结合运动分析和弱监督验证。", "result": "方法在区分攻击性和防守性击球意图上达到75% F1分数和80% AUC-ROC。", "conclusion": "姿势分析能有效推断意图，弱监督验证为解决数据标注限制提供了可能，为体育分析和行为分析开辟了新途径。"}}
{"id": "2507.12259", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12259", "abs": "https://arxiv.org/abs/2507.12259", "authors": ["Lihan Lian", "Yuxin Tong", "Uduak Inyang-Udoh"], "title": "Neural Co-state Regulator: A Data-Driven Paradigm for Real-time Optimal Control with Input Constraints", "comment": null, "summary": "We propose a novel unsupervised learning framework for solving nonlinear\noptimal control problems (OCPs) with input constraints in real-time. In this\nframework, a neural network (NN) learns to predict the optimal co-state\ntrajectory that minimizes the control Hamiltonian for a given system, at any\nsystem's state, based on the Pontryagin's Minimum Principle (PMP).\nSpecifically, the NN is trained to find the norm-optimal co-state solution that\nsimultaneously satisfies the nonlinear system dynamics and minimizes a\nquadratic regulation cost. The control input is then extracted from the\npredicted optimal co-state trajectory by solving a quadratic program (QP) to\nsatisfy input constraints and optimality conditions. We coin the term neural\nco-state regulator (NCR) to describe the combination of the co-state NN and\ncontrol input QP solver. To demonstrate the effectiveness of the NCR, we\ncompare its feedback control performance with that of an expert nonlinear model\npredictive control (MPC) solver on a unicycle model. Because the NCR's training\ndoes not rely on expert nonlinear control solvers which are often suboptimal,\nthe NCR is able to produce solutions that outperform the nonlinear MPC solver\nin terms of convergence error and input trajectory smoothness even for system\nconditions that are outside its original training domain. At the same time, the\nNCR offers two orders of magnitude less computational time than the nonlinear\nMPC.", "AI": {"tldr": "提出一种无监督学习框架NCR，通过神经网络预测最优共态轨迹，结合QP求解器实时解决非线性最优控制问题，性能优于非线性MPC。", "motivation": "解决非线性最优控制问题（OCP）的实时性和输入约束问题，避免依赖专家非线性控制求解器。", "method": "利用神经网络学习最小化控制哈密顿量的最优共态轨迹，通过QP求解器提取满足约束的控制输入。", "result": "NCR在收敛误差和输入轨迹平滑度上优于非线性MPC，计算时间减少两个数量级。", "conclusion": "NCR是一种高效的无监督学习框架，适用于实时非线性最优控制问题。"}}
{"id": "2507.11940", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11940", "abs": "https://arxiv.org/abs/2507.11940", "authors": ["Kanghyun Ryu", "Minjun Sung", "Piyush Gupta", "Jovin D'sa", "Faizan M. Tariq", "David Isele", "Sangjae Bae"], "title": "IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving", "comment": "To be published in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "Motion planning for autonomous vehicles (AVs) in dense traffic is\nchallenging, often leading to overly conservative behavior and unmet planning\nobjectives. This challenge stems from the AVs' limited ability to anticipate\nand respond to the interactive behavior of surrounding agents. Traditional\ndecoupled prediction and planning pipelines rely on non-interactive predictions\nthat overlook the fact that agents often adapt their behavior in response to\nthe AV's actions. To address this, we propose Interaction-Aware Neural\nNetwork-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which\nenables interactive trajectory planning by predicting how surrounding agents\nmay react to each control sequence sampled by MPPI. To improve performance in\nstructured lane environments, we introduce a spline-based prior for the MPPI\nsampling distribution, enabling efficient lane-changing behavior. We evaluate\nIANN-MPPI in a dense traffic merging scenario, demonstrating its ability to\nperform efficient merging maneuvers. Our project website is available at\nhttps://sites.google.com/berkeley.edu/iann-mppi", "AI": {"tldr": "提出了一种交互感知的神经网络增强模型预测路径积分控制（IANN-MPPI），用于自动驾驶车辆在密集交通中的轨迹规划，解决了传统方法过于保守的问题。", "motivation": "自动驾驶车辆在密集交通中的运动规划常因无法预测周围车辆的交互行为而表现保守，传统方法忽略了车辆间的动态响应。", "method": "结合神经网络预测周围车辆对MPPI采样控制序列的反应，并引入基于样条的MPPI采样分布先验，优化车道变换行为。", "result": "在密集交通合并场景中，IANN-MPPI能够高效完成合并操作。", "conclusion": "IANN-MPPI通过交互感知和优化采样分布，显著提升了自动驾驶车辆在复杂交通环境中的规划能力。"}}
{"id": "2507.11862", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11862", "abs": "https://arxiv.org/abs/2507.11862", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "comment": "Accepted to CLNLP 2025", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "AI": {"tldr": "论文研究了跨领域模型迁移、多领域数据融合和样本高效学习在PII识别中的效果，发现法律领域数据对传记文本迁移效果好，医疗领域迁移效果差，融合效果因领域而异，低专业化领域仅需10%训练数据即可实现高质量识别。", "motivation": "研究PII识别的跨领域模型迁移、数据融合和样本高效学习方法，以提升文本匿名化的准确性。", "method": "使用医疗（I2B2）、法律（TAB）和传记（Wikipedia）领域的标注语料库，评估模型在领域内性能、跨领域迁移性、融合和少样本学习四个维度上的表现。", "result": "法律领域数据对传记文本迁移效果好，医疗领域迁移效果差；融合效果因领域而异；低专业化领域仅需10%训练数据即可实现高质量识别。", "conclusion": "跨领域迁移、数据融合和样本高效学习在PII识别中具有潜力，但效果因领域特性而异。"}}
{"id": "2507.11653", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11653", "abs": "https://arxiv.org/abs/2507.11653", "authors": ["Hannah Shafferman", "Annika Thomas", "Jouko Kinnari", "Michael Ricard", "Jose Nino", "Jonathan How"], "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization", "comment": "9 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Global localization is critical for autonomous navigation, particularly in\nscenarios where an agent must localize within a map generated in a different\nsession or by another agent, as agents often have no prior knowledge about the\ncorrelation between reference frames. However, this task remains challenging in\nunstructured environments due to appearance changes induced by viewpoint\nvariation, seasonal changes, spatial aliasing, and occlusions -- known failure\nmodes for traditional place recognition methods. To address these challenges,\nwe propose VISTA (View-Invariant Segmentation-Based Tracking for Frame\nAlignment), a novel open-set, monocular global localization framework that\ncombines: 1) a front-end, object-based, segmentation and tracking pipeline,\nfollowed by 2) a submap correspondence search, which exploits geometric\nconsistencies between environment maps to align vehicle reference frames. VISTA\nenables consistent localization across diverse camera viewpoints and seasonal\nchanges, without requiring any domain-specific training or finetuning. We\nevaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a\n69% improvement in recall over baseline methods. Furthermore, we maintain a\ncompact object-based map that is only 0.6% the size of the most\nmemory-conservative baseline, making our approach capable of real-time\nimplementation on resource-constrained platforms.", "AI": {"tldr": "VISTA是一种基于分割和跟踪的单目全局定位框架，能够在不同视角和季节变化下实现一致定位，显著优于基线方法。", "motivation": "解决无结构环境中因视角变化、季节变化等导致的传统定位方法失效问题。", "method": "结合前端基于对象的分割跟踪管道和子图对应搜索，利用几何一致性对齐参考帧。", "result": "在季节和斜视角航空数据集上，召回率提升69%，地图大小仅为基线方法的0.6%。", "conclusion": "VISTA无需领域特定训练，适用于资源受限平台，实现高效实时定位。"}}
{"id": "2507.11730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11730", "abs": "https://arxiv.org/abs/2507.11730", "authors": ["Maciej Szankin", "Vidhyananth Venkatasamy", "Lihang Ying"], "title": "Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis", "comment": null, "summary": "Outdoor advertisements remain a critical medium for modern marketing, yet\naccurately verifying billboard text visibility under real-world conditions is\nstill challenging. Traditional Optical Character Recognition (OCR) pipelines\nexcel at cropped text recognition but often struggle with complex outdoor\nscenes, varying fonts, and weather-induced visual noise. Recently, multimodal\nVision-Language Models (VLMs) have emerged as promising alternatives, offering\nend-to-end scene understanding with no explicit detection step. This work\nsystematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,\nInternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline\n(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with\nsynthetic weather distortions to simulate realistic degradation. Our results\nreveal that while selected VLMs excel at holistic scene reasoning, lightweight\nCNN pipelines still achieve competitive accuracy for cropped text at a fraction\nof the computational cost-an important consideration for edge deployment. To\nfoster future research, we release our weather-augmented benchmark and\nevaluation code publicly.", "AI": {"tldr": "本文比较了多模态视觉语言模型（VLMs）与传统CNN OCR在户外广告文本识别中的表现，发现轻量级CNN在裁剪文本上仍具竞争力，并发布了天气增强的基准数据集。", "motivation": "户外广告文本识别在复杂场景中仍具挑战性，传统OCR难以应对天气和字体变化，而VLMs提供了端到端解决方案。", "method": "系统评估了Qwen 2.5 VL 3B、InternVL3和SmolVLM2等VLMs与PaddleOCRv4在ICDAR 2015和SVT数据集上的表现，并加入合成天气失真模拟。", "result": "部分VLMs在整体场景理解上表现优异，但轻量级CNN在裁剪文本识别上仍具竞争力且计算成本更低。", "conclusion": "轻量级CNN适合边缘部署，同时发布了天气增强的基准数据集以促进未来研究。"}}
{"id": "2507.12327", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12327", "abs": "https://arxiv.org/abs/2507.12327", "authors": ["Mohamad Charara", "Martin De Montigny", "Nivine Abou Daher", "Hanane Dagdougui", "Antoine Lesage-Landry"], "title": "Mixed-integer Second-Order Cone Programming for Multi-period Scheduling of Flexible AC Transmission System Devices", "comment": "10 pages, 1 figure, submitted to CIGR\\'E 2025 International\n  Symposium, Paper 10998, PS1: System Enhancement, Markets and Regulation", "summary": "With the increasing energy demand and the growing integration of renewable\nsources of energy, power systems face operational challenges such as overloads,\nlosses, and stability concerns, particularly as networks operate near their\ncapacity limits. Flexible alternating current transmission system (FACTS)\ndevices are essential to ensure reliable grid operations and enable the\nefficient integration of renewable energy. This work introduces a mixed-integer\nsecond-order cone programming (MISOCP) model for the multi-period scheduling of\nkey FACTS devices in electric transmission systems. The proposed model\nintegrates four key control mechanisms: (i) on-load tap changers (OLTCs) for\nvoltage regulation via discrete taps; (ii) static synchronous compensators\n(STATCOMs) and (iii) shunt reactors for reactive power compensation; and (iv)\nthyristor-controlled series capacitors (TCSCs) for adjustable impedance and\nflow control. The objective is to minimize active power losses using a limited\nnumber of control actions while meeting physical and operational constraints at\nall times throughout the defined time horizon. To ensure tractability, the\nmodel employs a second-order cone relaxation of the power flow. Device-specific\nconstraints are handled via binary expansion and linearization: OLTCs and shunt\nreactors are modelled with discrete variables, STATCOMs through reactive power\nbounds, and TCSCs using a reformulation-linearization technique (RLT). A\nmulti-period formulation captures the sequential nature of decision making,\nensuring consistency across time steps. The model is evaluated on the IEEE\n9-bus, 30-bus, and RTS96 test systems, demonstrating its ability to reduce\nlosses, with potential applicability to larger-scale grids.", "AI": {"tldr": "提出了一种混合整数二阶锥规划（MISOCP）模型，用于电力传输系统中关键FACTS设备的多周期调度，以最小化有功功率损耗。", "motivation": "随着能源需求增加和可再生能源的集成，电力系统面临过载、损耗和稳定性问题，需要FACTS设备确保电网可靠运行。", "method": "模型整合了四种关键控制机制（OLTCs、STATCOMs、shunt reactors、TCSCs），采用二阶锥松弛处理潮流问题，并通过二进制扩展和线性化处理设备约束。", "result": "在IEEE 9-bus、30-bus和RTS96测试系统上验证，模型能有效减少损耗，适用于大规模电网。", "conclusion": "该模型为FACTS设备的多周期调度提供了可行且高效的解决方案，有助于电网稳定和可再生能源集成。"}}
{"id": "2507.11974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11974", "abs": "https://arxiv.org/abs/2507.11974", "authors": ["Waseem Akram", "Muhayy Ud Din", "Lyes Saad Soud", "Irfan Hussain"], "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming", "comment": null, "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems.", "AI": {"tldr": "综述探讨了生成式人工智能（GAI）在水产养殖中的广泛应用，包括环境监测、机器人技术、疾病诊断等，并分析了其技术架构、实际案例及面临的挑战。", "motivation": "随着水产养殖行业向数据驱动和自动化转型（Aquaculture 4.0），GAI为智能决策提供了新的可能性，本文旨在全面梳理其应用与局限。", "method": "通过综述GAI的基础架构（如扩散模型、Transformer等）、实验系统、试点部署和实际案例，分析其在水产养殖中的技术实现。", "result": "GAI在水下感知、数字孪生建模和自主规划等方面展现出潜力，但面临数据不足、实时性能、可解释性等挑战。", "conclusion": "GAI是推动智能、可持续水产养殖系统的关键工具，但需解决技术和监管问题以实现广泛应用。"}}
{"id": "2507.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11867", "abs": "https://arxiv.org/abs/2507.11867", "authors": ["Xiangyu Yang", "Xinying Qiu"], "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "comment": "Accepted to CLNLP 2025", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "AI": {"tldr": "COLA-GEC框架通过双向知识转移提升语法错误纠正（GEC）和语法可接受性判断（COLA）任务，实现多语言基准上的最优表现。", "motivation": "GEC和COLA任务共享语法知识但独立发展，缺乏协同优化。", "method": "1. 利用GEC数据集增强语法可接受性模型；2. 通过动态损失函数将语法可接受性信号整合到GEC训练中。", "result": "在多语言基准上取得最优结果，但标点错误纠正仍有挑战。", "conclusion": "COLA-GEC为语法建模提供了新方向，未来需进一步优化标点错误纠正。"}}
{"id": "2507.11761", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11761", "abs": "https://arxiv.org/abs/2507.11761", "authors": ["Fan Shi", "Bin Li", "Xiangyang Xue"], "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning", "comment": null, "summary": "Abstract visual reasoning (AVR) enables humans to quickly discover and\ngeneralize abstract rules to new scenarios. Designing intelligent systems with\nhuman-like AVR abilities has been a long-standing topic in the artificial\nintelligence community. Deep AVR solvers have recently achieved remarkable\nsuccess in various AVR tasks. However, they usually use task-specific designs\nor parameters in different tasks. In such a paradigm, solving new tasks often\nmeans retraining the model, and sometimes retuning the model architectures,\nwhich increases the cost of solving AVR problems. In contrast to task-specific\napproaches, this paper proposes a novel Unified Conditional Generative Solver\n(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we\nprove that some well-known AVR tasks can be reformulated as the problem of\nestimating the predictability of target images in problem panels. Then, we\nillustrate that, under the proposed framework, training one conditional\ngenerative model can solve various AVR tasks. The experiments show that with a\nsingle round of multi-task training, UCGS demonstrates abstract reasoning\nability across various AVR tasks. Especially, UCGS exhibits the ability of\nzero-shot reasoning, enabling it to perform abstract reasoning on problems from\nunseen AVR tasks in the testing phase.", "AI": {"tldr": "论文提出了一种统一的生成式求解器UCGS，用于解决多种抽象视觉推理任务，避免了任务特定的设计和参数调整，实现了零样本推理能力。", "motivation": "设计一个统一的框架来解决多种抽象视觉推理任务，减少任务特定设计和重新训练的成本。", "method": "将抽象视觉推理任务重新表述为目标图像的可预测性问题，并训练一个条件生成模型来解决这些任务。", "result": "UCGS通过多任务训练展示了跨任务的抽象推理能力，并具备零样本推理能力。", "conclusion": "UCGS为抽象视觉推理任务提供了一种高效、统一的解决方案，具有实际应用潜力。"}}
{"id": "2507.11892", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11892", "abs": "https://arxiv.org/abs/2507.11892", "authors": ["Yu Liu", "Leyuan Qu", "Hanlei Shi", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition", "comment": null, "summary": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions\nfrom temporally evolving facial movements and plays a critical role in\naffective computing. While recent vision-language approaches have introduced\nsemantic textual descriptions to guide expression recognition, existing methods\nstill face two key limitations: they often underutilize the subtle emotional\ncues embedded in generated text, and they have yet to incorporate sufficiently\neffective mechanisms for filtering out facial dynamics that are irrelevant to\nemotional expression. To address these gaps, We propose GRACE, Granular\nRepresentation Alignment for Cross-modal Emotion recognition that integrates\ndynamic motion modeling, semantic text refinement, and token-level cross-modal\nalignment to facilitate the precise localization of emotionally salient\nspatiotemporal features. Our method constructs emotion-aware textual\ndescriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and\nhighlights expression-relevant facial motion through a motion-difference\nweighting mechanism. These refined semantic and visual signals are aligned at\nthe token level using entropy-regularized optimal transport. Experiments on\nthree benchmark datasets demonstrate that our method significantly improves\nrecognition performance, particularly in challenging settings with ambiguous or\nimbalanced emotion classes, establishing new state-of-the-art (SOTA) results in\nterms of both UAR and WAR.", "AI": {"tldr": "论文提出GRACE方法，通过动态运动建模、语义文本细化和跨模态对齐，提升动态面部表情识别的性能。", "motivation": "现有方法未能充分利用文本中的情感线索，且缺乏有效过滤无关面部动态的机制。", "method": "GRACE结合动态运动建模、语义文本细化（CATE模块）和基于熵正则化最优传输的跨模态对齐。", "result": "在三个基准数据集上显著提升识别性能，特别是在模糊或不平衡情感类别中，达到SOTA结果。", "conclusion": "GRACE通过细粒度跨模态对齐和情感感知文本增强，有效解决了现有方法的局限性。"}}
{"id": "2507.12339", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12339", "abs": "https://arxiv.org/abs/2507.12339", "authors": ["Youssef Ait Si", "Antoine Girard", "Adnane Saoud"], "title": "Symbolic Control: Unveiling Free Robustness Margins", "comment": "9", "summary": "This paper addresses the challenge of ensuring robustness in the presence of\nsystem perturbations for symbolic control techniques. Given a discrete-time\ncontrol system that is related to its symbolic model by an alternating\nsimulation relation. In this paper, we focus on computing the maximum\nrobustness margin under which the symbolic model remains valid for a\nperturbed-version of the discrete-time control system. We first show that\nsymbolic models are inherently equipped with a certain free robustness margins.\nWe then provide constructive procedures to compute uniform and non-uniform\n(state and input dependent) robustness margins. We also show that the tightness\nof the robustness margin depends on the tightness of the reachability technique\nused to compute the symbolic model. We then explain how the computed robustness\nmargin can be used for the sake of controller synthesis. Finally, we present\ntwo illustrative examples to demonstrate the effectiveness of our approach.", "AI": {"tldr": "论文研究了符号控制技术在系统扰动下的鲁棒性问题，提出了计算最大鲁棒性边界的方法，并展示了其在控制器综合中的应用。", "motivation": "解决符号控制技术在系统扰动下鲁棒性不足的问题，确保符号模型在扰动系统中仍有效。", "method": "通过构造性方法计算均匀和非均匀的鲁棒性边界，并分析其与可达性技术的关系。", "result": "证明了符号模型具有固有鲁棒性边界，并提供了计算方法和控制器综合的应用示例。", "conclusion": "提出的方法有效提升了符号控制技术在扰动系统中的鲁棒性，并通过示例验证了其有效性。"}}
{"id": "2507.11991", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11991", "abs": "https://arxiv.org/abs/2507.11991", "authors": ["Juanran Wang", "Marc R. Schlichting", "Mykel J. Kochenderfer"], "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "comment": null, "summary": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller.", "AI": {"tldr": "利用深度生成模型提升自动驾驶车辆在交叉路口的安全性，通过单步去噪扩散模型快速生成潜在碰撞场景，优化决策规划。", "motivation": "高风险交通区域（如交叉路口）是碰撞的主要原因，需提升自动驾驶车辆的安全性。", "method": "训练1000步去噪扩散概率模型生成碰撞传感器噪声序列，并通过生成对抗架构将其蒸馏为单步模型，用于快速推理。", "result": "单步模型在保持采样质量的同时实现快速推理，应用于规划器后显著降低故障率和延迟率。", "conclusion": "单步去噪扩散模型能有效提升自动驾驶车辆在交叉路口的决策鲁棒性。"}}
{"id": "2507.11875", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11875", "abs": "https://arxiv.org/abs/2507.11875", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "AI": {"tldr": "DualReward是一种新颖的强化学习框架，用于自动生成完形填空测试的干扰项，通过双奖励结构和自适应缩放机制提升性能。", "motivation": "传统方法依赖监督学习或静态生成模型，无法动态调整奖励信号。DualReward旨在通过自适应奖励机制更好地平衡从人类示例学习和探索高质量干扰项。", "method": "采用双奖励结构，动态调整奖励信号强度，基于模型性能和置信度。在CLOTH-F和MCQ数据集上评估。", "result": "在CLOTH-F上表现稳定，在MCQ上P@1提升3.48-3.86%，尤其适用于多样化问题和跨域数据。", "conclusion": "DualReward提供了一种灵活框架，有效平衡学习人类示例和生成高质量干扰项，适用于自动化测试生成。"}}
{"id": "2507.11834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11834", "abs": "https://arxiv.org/abs/2507.11834", "authors": ["Peiwen Xia", "Tangfei Liao", "Wei Zhu", "Danhuai Zhao", "Jianjun Ke", "Kaihao Zhang", "Tong Lu", "Tao Wang"], "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning", "comment": "Accepted by ECAI 2025", "summary": "Establishing reliable correspondences between image pairs is a fundamental\ntask in computer vision, underpinning applications such as 3D reconstruction\nand visual localization. Although recent methods have made progress in pruning\noutliers from dense correspondence sets, they often hypothesize consistent\nvisual domains and overlook the challenges posed by diverse scene structures.\nIn this paper, we propose CorrMoE, a novel correspondence pruning framework\nthat enhances robustness under cross-domain and cross-scene variations. To\naddress domain shift, we introduce a De-stylization Dual Branch, performing\nstyle mixing on both implicit and explicit graph features to mitigate the\nadverse influence of domain-specific representations. For scene diversity, we\ndesign a Bi-Fusion Mixture of Experts module that adaptively integrates\nmulti-perspective features through linear-complexity attention and dynamic\nexpert routing. Extensive experiments on benchmark datasets demonstrate that\nCorrMoE achieves superior accuracy and generalization compared to\nstate-of-the-art methods. The code and pre-trained models are available at\nhttps://github.com/peiwenxia/CorrMoE.", "AI": {"tldr": "提出了一种名为CorrMoE的新框架，用于增强跨域和跨场景变化下的图像对应关系修剪鲁棒性。", "motivation": "现有方法在处理密集对应关系时假设视觉域一致，忽视了多样场景结构的挑战。", "method": "采用De-stylization Dual Branch处理域偏移，以及Bi-Fusion Mixture of Experts模块适应场景多样性。", "result": "在基准数据集上表现优于现有方法，具有更高的准确性和泛化能力。", "conclusion": "CorrMoE框架在跨域和跨场景任务中表现出色，代码和预训练模型已开源。"}}
{"id": "2507.11893", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11893", "abs": "https://arxiv.org/abs/2507.11893", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Dezhi Zheng", "Jifeng Dai"], "title": "Spatial Frequency Modulation for Semantic Segmentation", "comment": "Accept by TPAMI 2025", "summary": "High spatial frequency information, including fine details like textures,\nsignificantly contributes to the accuracy of semantic segmentation. However,\naccording to the Nyquist-Shannon Sampling Theorem, high-frequency components\nare vulnerable to aliasing or distortion when propagating through downsampling\nlayers such as strided-convolution. Here, we propose a novel Spatial Frequency\nModulation (SFM) that modulates high-frequency features to a lower frequency\nbefore downsampling and then demodulates them back during upsampling.\nSpecifically, we implement modulation through adaptive resampling (ARS) and\ndesign a lightweight add-on that can densely sample the high-frequency areas to\nscale up the signal, thereby lowering its frequency in accordance with the\nFrequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling\n(MSAU) to demodulate the modulated feature and recover high-frequency\ninformation through non-uniform upsampling This module further improves\nsegmentation by explicitly exploiting information interaction between densely\nand sparsely resampled areas at multiple scales. Both modules can seamlessly\nintegrate with various architectures, extending from convolutional neural\nnetworks to transformers. Feature visualization and analysis confirm that our\nmethod effectively alleviates aliasing while successfully retaining details\nafter demodulation. Finally, we validate the broad applicability and\neffectiveness of SFM by extending it to image classification, adversarial\nrobustness, instance segmentation, and panoptic segmentation tasks. The code is\navailable at\n\\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.", "AI": {"tldr": "提出了一种空间频率调制（SFM）方法，通过调制和解调高频特征，减少下采样中的混叠问题，提升语义分割的细节保留能力。", "motivation": "高频信息对语义分割精度至关重要，但下采样层可能导致其失真或混叠。", "method": "使用自适应重采样（ARS）调制高频特征，并通过多尺度自适应上采样（MSAU）解调，恢复高频信息。", "result": "有效减少混叠并保留细节，适用于多种任务和架构。", "conclusion": "SFM方法在多种视觉任务中表现出广泛的适用性和有效性。"}}
{"id": "2507.12444", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.12444", "abs": "https://arxiv.org/abs/2507.12444", "authors": ["Man Shi", "Vikram Jain", "Antony Joseph", "Maurice Meijer", "Marian Verhelst"], "title": "BitWave: Exploiting Column-Based Bit-Level Sparsity for Deep Learning Acceleration", "comment": "15 pages, 18 figures, 2024 IEEE International Symposium on\n  High-Performance Computer Architecture (HPCA)", "summary": "Bit-serial computation facilitates bit-wise sequential data processing,\noffering numerous benefits, such as a reduced area footprint and\ndynamically-adaptive computational precision. It has emerged as a prominent\napproach, particularly in leveraging bit-level sparsity in Deep Neural Networks\n(DNNs). However, existing bit-serial accelerators exploit bit-level sparsity to\nreduce computations by skipping zero bits, but they suffer from inefficient\nmemory accesses due to the irregular indices of the non-zero bits.\n  As memory accesses typically are the dominant contributor to DNN accelerator\nperformance, this paper introduces a novel computing approach called\n\"bit-column-serial\" and a compatible architecture design named \"BitWave.\"\nBitWave harnesses the advantages of the \"bit-column-serial\" approach,\nleveraging structured bit-level sparsity in combination with dynamic dataflow\ntechniques. This achieves a reduction in computations and memory footprints\nthrough redundant computation skipping and weight compression. BitWave is able\nto mitigate the performance drop or the need for retraining that is typically\nassociated with sparsity-enhancing techniques using a post-training\noptimization involving selected weight bit-flips. Empirical studies conducted\non four deep-learning benchmarks demonstrate the achievements of BitWave: (1)\nMaximally realize 13.25x higher speedup, 7.71x efficiency compared to\nstate-of-the-art sparsity-aware accelerators. (2) Occupying 1.138 mm2 area and\nconsuming 17.56 mW power in 16nm FinFet process node.", "AI": {"tldr": "BitWave是一种新型的位序列计算架构，通过结构化位级稀疏性和动态数据流技术，减少计算和内存占用，显著提升性能。", "motivation": "现有位序列加速器因非零位索引不规则导致内存访问效率低下，影响性能。", "method": "提出“位列序列”计算方法和BitWave架构，结合结构化稀疏性和动态数据流，跳过冗余计算并压缩权重。", "result": "在16nm工艺下，BitWave实现最高13.25倍加速和7.71倍效率提升，面积1.138 mm²，功耗17.56 mW。", "conclusion": "BitWave通过后训练优化和位翻转技术，有效解决了稀疏性增强技术的性能下降问题。"}}
{"id": "2507.12067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12067", "abs": "https://arxiv.org/abs/2507.12067", "authors": ["Xing Tong", "Michele D. Simoni"], "title": "Robust Route Planning for Sidewalk Delivery Robots", "comment": null, "summary": "Sidewalk delivery robots are a promising solution for urban freight\ndistribution, reducing congestion compared to trucks and providing a safer,\nhigher-capacity alternative to drones. However, unreliable travel times on\nsidewalks due to pedestrian density, obstacles, and varying infrastructure\nconditions can significantly affect their efficiency. This study addresses the\nrobust route planning problem for sidewalk robots, explicitly accounting for\ntravel time uncertainty due to varying sidewalk conditions. Optimization is\nintegrated with simulation to reproduce the effect of obstacles and pedestrian\nflows and generate realistic travel times. The study investigates three\ndifferent approaches to derive uncertainty sets, including budgeted,\nellipsoidal, and support vector clustering (SVC)-based methods, along with a\ndistributionally robust method to solve the shortest path (SP) problem. A\nrealistic case study reproducing pedestrian patterns in Stockholm's city center\nis used to evaluate the efficiency of robust routing across various robot\ndesigns and environmental conditions. The results show that, when compared to a\nconventional SP, robust routing significantly enhances operational reliability\nunder variable sidewalk conditions. The Ellipsoidal and DRSP approaches\noutperform the other methods, yielding the most efficient paths in terms of\naverage and worst-case delay. Sensitivity analyses reveal that robust\napproaches consistently outperform the conventional SP, particularly for\nsidewalk delivery robots that are wider, slower, and have more conservative\nnavigation behaviors. These benefits are even more pronounced in adverse\nweather conditions and high pedestrian congestion scenarios.", "AI": {"tldr": "研究针对人行道送货机器人的鲁棒路径规划问题，通过优化与仿真结合，解决因行人密度、障碍物和基础设施条件导致的旅行时间不确定性。", "motivation": "人行道送货机器人是城市货运的潜在解决方案，但旅行时间的不确定性影响其效率，需要鲁棒路径规划方法。", "method": "集成优化与仿真，采用预算、椭球和支持向量聚类（SVC）方法及分布鲁棒方法解决最短路径问题。", "result": "鲁棒路径规划显著提升操作可靠性，椭球和DRSP方法表现最佳。", "conclusion": "鲁棒方法在恶劣天气和高行人密度下效果更显著，尤其适合较宽、较慢且导航保守的机器人。"}}
{"id": "2507.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11878", "abs": "https://arxiv.org/abs/2507.11878", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "title": "LLMs Encode Harmfulness and Refusal Separately", "comment": null, "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "AI": {"tldr": "论文发现LLMs内部对有害性的理解与拒绝行为是两个独立的概念，并提出了一种新的安全机制（Latent Guard）。", "motivation": "研究LLMs是否真正理解有害性，而不仅仅是机械地拒绝指令。", "method": "通过识别有害性方向与拒绝方向的差异，分析LLMs的安全机制。", "result": "发现某些越狱方法通过减少拒绝信号而非改变有害性判断来绕过安全机制。", "conclusion": "LLMs对有害性的内部理解比拒绝行为更稳健，为AI安全研究提供了新视角。"}}
{"id": "2507.11845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11845", "abs": "https://arxiv.org/abs/2507.11845", "authors": ["Kexuan Shi", "Zhuang Qi", "Jingjing Zhu", "Lei Meng", "Yaochen Zhang", "Haibei Huang", "Xiangxu Meng"], "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification", "comment": "Accepted in ChinaMM and recommended to Displays", "summary": "Open-set few-shot image classification aims to train models using a small\namount of labeled data, enabling them to achieve good generalization when\nconfronted with unknown environments. Existing methods mainly use visual\ninformation from a single image to learn class representations to distinguish\nknown from unknown categories. However, these methods often overlook the\nbenefits of integrating rich contextual information. To address this issue,\nthis paper proposes a prototypical augmentation and alignment method, termed\nProtoConNet, which incorporates background information from different samples\nto enhance the diversity of the feature space, breaking the spurious\nassociations between context and image subjects in few-shot scenarios.\nSpecifically, it consists of three main modules: the clustering-based data\nselection (CDS) module mines diverse data patterns while preserving core\nfeatures; the contextual-enhanced semantic refinement (CSR) module builds a\ncontext dictionary to integrate into image representations, which boosts the\nmodel's robustness in various scenarios; and the prototypical alignment (PA)\nmodule reduces the gap between image representations and class prototypes,\namplifying feature distances for known and unknown classes. Experimental\nresults from two datasets verified that ProtoConNet enhances the effectiveness\nof representation learning in few-shot scenarios and identifies open-set\nsamples, making it superior to existing methods.", "AI": {"tldr": "ProtoConNet提出了一种原型增强和对齐方法，通过整合背景信息提升少样本图像分类的性能，优于现有方法。", "motivation": "现有的少样本图像分类方法通常忽略上下文信息的整合，导致性能受限。", "method": "ProtoConNet包含三个模块：基于聚类的数据选择（CDS）、上下文增强语义细化（CSR）和原型对齐（PA），分别用于数据多样性挖掘、上下文整合和特征距离优化。", "result": "在两个数据集上的实验表明，ProtoConNet在少样本场景下提升了表示学习效果，并能有效识别开放集样本。", "conclusion": "ProtoConNet通过整合上下文信息，显著提升了少样本开放集分类的性能。"}}
{"id": "2507.11936", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11936", "abs": "https://arxiv.org/abs/2507.11936", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "title": "A Survey of Deep Learning for Geometry Problem Solving", "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "AI": {"tldr": "本文综述了深度学习在几何问题解决中的应用，包括任务总结、方法回顾、评估指标分析以及未来方向的讨论。", "motivation": "几何问题解决是数学推理的关键领域，涉及教育和人工智能能力评估等多个重要领域，深度学习的发展推动了相关研究。", "method": "通过总结几何问题解决的相关任务、深度学习方法的回顾、评估指标的分析以及挑战和未来方向的讨论。", "result": "提供了深度学习在几何问题解决中的全面参考，并创建了持续更新的论文列表。", "conclusion": "本文旨在促进几何问题解决领域的进一步发展，为研究者和实践者提供实用参考。"}}
{"id": "2507.12433", "categories": ["cs.CV", "cs.SY", "eess.SY", "I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.12433", "abs": "https://arxiv.org/abs/2507.12433", "authors": ["Fahimeh Orvati Nia", "Hai Lin"], "title": "Traffic-Aware Pedestrian Intention Prediction", "comment": "6 pages, 4 figures. Accepted to the American Control Conference (ACC)\n  2025", "summary": "Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.", "AI": {"tldr": "提出了一种基于交通信号感知的时空图卷积网络（TA-STGCN），用于提升行人意图预测的准确性。", "motivation": "现有模型在行人意图预测中常忽略动态交通信号和场景信息，而这对自动驾驶车辆的安全导航至关重要。", "method": "TA-STGCN通过整合动态交通信号状态和边界框大小作为关键特征，捕捉复杂城市环境中的时空依赖性。", "result": "在PIE数据集上，TA-STGCN比基线模型准确率提高了4.75%。", "conclusion": "TA-STGCN有效提升了行人意图预测的准确性，适用于实际应用。"}}
{"id": "2507.12093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12093", "abs": "https://arxiv.org/abs/2507.12093", "authors": ["David Rapado-Rincon", "Gert Kootstra"], "title": "Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards", "comment": "Paper submitted to Smart Agricultural Technology", "summary": "Accurate mapping of individual trees is an important component for precision\nagriculture in orchards, as it allows autonomous robots to perform tasks like\ntargeted operations or individual tree monitoring. However, creating these maps\nis challenging because GPS signals are often unreliable under dense tree\ncanopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)\napproaches struggle in orchards because the repetitive appearance of trees can\nconfuse the system, leading to mapping errors. To address this, we introduce\nTree-SLAM, a semantic SLAM approach tailored for creating maps of individual\ntrees in orchards. Utilizing RGB-D images, our method detects tree trunks with\nan instance segmentation model, estimates their location and re-identifies them\nusing a cascade-graph-based data association algorithm. These re-identified\ntrunks serve as landmarks in a factor graph framework that integrates noisy GPS\nsignals, odometry, and trunk observations. The system produces maps of\nindividual trees with a geo-localization error as low as 18 cm, which is less\nthan 20\\% of the planting distance. The proposed method was validated on\ndiverse datasets from apple and pear orchards across different seasons,\ndemonstrating high mapping accuracy and robustness in scenarios with unreliable\nGPS signals.", "AI": {"tldr": "Tree-SLAM是一种针对果园中单棵树定位的语义SLAM方法，通过RGB-D图像和实例分割模型检测树干，结合GPS、里程计和树干观测，实现高精度地图构建。", "motivation": "果园中单棵树的精准定位对农业机器人任务至关重要，但GPS信号不可靠且传统SLAM方法在重复树冠环境中易出错。", "method": "使用RGB-D图像和实例分割模型检测树干，通过级联图数据关联算法重识别树干，结合因子图框架整合GPS、里程计和树干观测。", "result": "系统在苹果和梨园数据集上验证，定位误差低至18厘米，低于种植距离的20%。", "conclusion": "Tree-SLAM在GPS信号不可靠的果园环境中表现出高精度和鲁棒性。"}}
{"id": "2507.11882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11882", "abs": "https://arxiv.org/abs/2507.11882", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "comment": "ACL 2025 Main Conference paper", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "AI": {"tldr": "论文提出了一个多语言指令跟随基准Marco-Bench-MIF，覆盖30种语言，解决了现有数据集在语言和文化上的局限性。", "motivation": "现有指令跟随评估数据集多为英语或简单机器翻译，限制了多语言场景的适用性。", "method": "通过结合翻译与验证的混合流程，创建了本地化的多语言版本Marco-Bench-MIF。", "result": "评估发现高低资源语言间存在25-35%的准确率差距，模型规模影响性能45-60%，机器翻译数据低估准确率7-22%。", "conclusion": "多语言指令跟随存在挑战，如关键词一致性和跨语言约束遵循，Marco-Bench-MIF为相关研究提供了工具。"}}
{"id": "2507.11900", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11900", "abs": "https://arxiv.org/abs/2507.11900", "authors": ["Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai"], "title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos", "comment": "CompressedVQA-HDR won first place in the FR track of the\n  Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE\n  ICME 2025", "summary": "Video compression is a standard procedure applied to all videos to minimize\nstorage and transmission demands while preserving visual quality as much as\npossible. Therefore, evaluating the visual quality of compressed videos is\ncrucial for guiding the practical usage and further development of video\ncompression algorithms. Although numerous compressed video quality assessment\n(VQA) methods have been proposed, they often lack the generalization capability\nneeded to handle the increasing diversity of video types, particularly high\ndynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an\neffective VQA framework designed to address the challenges of HDR video quality\nassessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the\nbackbone networks for the proposed full-reference (FR) and no-reference (NR)\nVQA models, respectively. For the FR model, we compute deep structural and\ntextural similarities between reference and distorted frames using\nintermediate-layer features extracted from the Swin Transformer as its\nquality-aware feature representation. For the NR model, we extract the global\nmean of the final-layer feature maps from SigLip 2 as its quality-aware\nrepresentation. To mitigate the issue of limited HDR training data, we\npre-train the FR model on a large-scale standard dynamic range (SDR) VQA\ndataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ\nan iterative mixed-dataset training strategy across multiple compressed VQA\ndatasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental\nresults show that our models achieve state-of-the-art performance compared to\nexisting FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place\nin the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand\nChallenge at IEEE ICME 2025. The code is available at\nhttps://github.com/sunwei925/CompressedVQA-HDR.", "AI": {"tldr": "提出CompressedVQA-HDR框架，针对HDR视频质量评估，结合Swin Transformer和SigLip 2网络，在FR和NR模型中表现优异。", "motivation": "现有视频质量评估方法对HDR内容泛化能力不足，需改进。", "method": "FR模型使用Swin Transformer提取特征计算相似性；NR模型使用SigLip 2提取全局特征。通过预训练和微调解决数据不足问题。", "result": "模型在FR和NR任务中表现最佳，FR模型在IEEE ICME 2025挑战赛中夺冠。", "conclusion": "CompressedVQA-HDR框架有效解决了HDR视频质量评估问题，性能优越。"}}
{"id": "2507.11939", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.11939", "abs": "https://arxiv.org/abs/2507.11939", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "PolyChartQA是首个大规模多语言图表问答基准，覆盖10种语言，旨在解决现有图表理解基准的英语中心化问题。", "motivation": "现有图表理解基准主要针对英语，限制了其全球适用性，因此需要开发多语言基准以促进全球包容性。", "method": "采用解耦管道，将图表数据与渲染代码分离，通过翻译数据和重用代码生成多语言图表，并利用先进的LLM翻译和严格质量控制。", "result": "实验显示，英语与其他语言（尤其是非拉丁文字的低资源语言）之间存在显著性能差距。", "conclusion": "PolyChartQA为推进全球包容性视觉语言模型奠定了基础。"}}
{"id": "2507.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12148", "abs": "https://arxiv.org/abs/2507.12148", "authors": ["Xing Tong", "Michele D. Simoni", "Kaj Munhoz Arfvidsson", "Jonas Mårtensson"], "title": "Leveraging Sidewalk Robots for Walkability-Related Analyses", "comment": null, "summary": "Walkability is a key component of sustainable urban development, while\ncollecting detailed data on its related features remains challenging due to the\nhigh costs and limited scalability of traditional methods. Sidewalk delivery\nrobots, increasingly deployed in urban environments, offer a promising solution\nto these limitations. This paper explores how these robots can serve as mobile\ndata collection platforms, capturing sidewalk-level features related to\nwalkability in a scalable, automated, and real-time manner. A sensor-equipped\nrobot was deployed on a sidewalk network at KTH in Stockholm, completing 101\ntrips covering 900 segments. From the collected data, different typologies of\nfeatures are derived, including robot trip characteristics (e.g., speed,\nduration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk\nutilization (e.g., pedestrian density). Their walkability-related implications\nwere investigated with a series of analyses. The results demonstrate that\npedestrian movement patterns are strongly influenced by sidewalk\ncharacteristics, with higher density, reduced width, and surface irregularity\nassociated with slower and more variable trajectories. Notably, robot speed\nclosely mirrors pedestrian behavior, highlighting its potential as a proxy for\nassessing pedestrian dynamics. The proposed framework enables continuous\nmonitoring of sidewalk conditions and pedestrian behavior, contributing to the\ndevelopment of more walkable, inclusive, and responsive urban environments.", "AI": {"tldr": "利用配备传感器的送货机器人收集人行道数据，分析其对步行性的影响。", "motivation": "传统方法收集步行性相关数据成本高且难以扩展，送货机器人提供了一种可扩展的自动化解决方案。", "method": "部署传感器机器人完成101次行程，收集人行道特征数据，分析步行性影响。", "result": "人行道特征（如宽度、密度、表面不平）显著影响行人行为，机器人速度可作为行人动态的代理指标。", "conclusion": "该框架支持持续监测人行道条件和行人行为，有助于打造更宜居的城市环境。"}}
{"id": "2507.11941", "categories": ["cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11941", "abs": "https://arxiv.org/abs/2507.11941", "authors": ["Amos You"], "title": "BlockBPE: Parallel BPE Tokenization", "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE是一种并行GPU实现的BPE算法，优化了高吞吐量的批量推理，比现有CPU实现快2-2.5倍。", "motivation": "现有CPU实现的BPE算法在GPU批量推理中效率低下，BlockBPE旨在解决这一问题。", "method": "BlockBPE通过消除Regex预分词步骤，实现高度并行化的标记合并，复杂度降至O(nd)。", "result": "在高批量推理任务中，BlockBPE的吞吐量比tiktoken高2倍，比HuggingFace Tokenizers高2.5倍。", "conclusion": "BlockBPE显著提升了GPU批量推理的效率，尽管生成质量略有损失，但性能提升显著。"}}
{"id": "2507.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11910", "abs": "https://arxiv.org/abs/2507.11910", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring", "comment": "Accepted at the 28th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "Event-based sensors have emerged as a promising solution for addressing\nchallenging conditions in pedestrian and traffic monitoring systems. Their\nlow-latency and high dynamic range allow for improved response time in\nsafety-critical situations caused by distracted walking or other unusual\nmovements. However, the availability of data covering such scenarios remains\nlimited. To address this gap, we present SEPose -- a comprehensive synthetic\nevent-based human pose estimation dataset for fixed pedestrian perception\ngenerated using dynamic vision sensors in the CARLA simulator. With nearly 350K\nannotated pedestrians with body pose keypoints from the perspective of fixed\ntraffic cameras, SEPose is a comprehensive synthetic multi-person pose\nestimation dataset that spans busy and light crowds and traffic across diverse\nlighting and weather conditions in 4-way intersections in urban, suburban, and\nrural environments. We train existing state-of-the-art models such as RVT and\nYOLOv8 on our dataset and evaluate them on real event-based data to demonstrate\nthe sim-to-real generalization capabilities of the proposed dataset.", "AI": {"tldr": "SEPose是一个基于事件的合成人体姿态估计数据集，用于固定行人感知，填补了真实数据不足的空白。", "motivation": "解决行人监测系统中因数据不足而难以应对挑战性条件的问题。", "method": "利用CARLA模拟器和动态视觉传感器生成SEPose数据集，包含近350K标注行人姿态关键点。", "result": "在真实事件数据上验证了SEPose的模拟到现实泛化能力。", "conclusion": "SEPose为事件传感器在行人监测中的应用提供了有效的数据支持。"}}
{"id": "2507.11947", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11947", "abs": "https://arxiv.org/abs/2507.11947", "authors": ["Geon Park", "Seon Bin Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation", "comment": "6 Pages", "summary": "With recent advancements in text-to-image (T2I) models, effectively\ngenerating multiple instances within a single image prompt has become a crucial\nchallenge. Existing methods, while successful in generating positions of\nindividual instances, often struggle to account for relationship discrepancy\nand multiple attributes leakage. To address these limitations, this paper\nproposes the relation-aware disentangled learning (RaDL) framework. RaDL\nenhances instance-specific attributes through learnable parameters and\ngenerates relation-aware image features via Relation Attention, utilizing\naction verbs extracted from the global prompt. Through extensive evaluations on\nbenchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that\nRaDL outperforms existing methods, showing significant improvements in\npositional accuracy, multiple attributes consideration, and the relationships\nbetween instances. Our results present RaDL as the solution for generating\nimages that consider both the relationships and multiple attributes of each\ninstance within the multi-instance image.", "AI": {"tldr": "本文提出了一种关系感知解耦学习（RaDL）框架，用于解决文本到图像（T2I）模型中多实例生成时的关系差异和属性泄漏问题。", "motivation": "现有方法在多实例图像生成中难以处理实例间的关系差异和多个属性的泄漏问题。", "method": "RaDL通过可学习参数增强实例特定属性，并利用从全局提示中提取的动作动词，通过关系注意力生成关系感知的图像特征。", "result": "在COCO-Position、COCO-MIG和DrawBench等基准测试中，RaDL在位置准确性、多属性考虑和实例关系方面显著优于现有方法。", "conclusion": "RaDL是生成考虑多实例图像中实例关系和多个属性的有效解决方案。"}}
{"id": "2507.12158", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12158", "abs": "https://arxiv.org/abs/2507.12158", "authors": ["Nawshin Mannan Proma", "Gricel Vázquez", "Sepeedeh Shahbeigi", "Arjun Badyal", "Victoria Hodge"], "title": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach", "comment": "6 pages, 6 figures", "summary": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems.", "AI": {"tldr": "本文提出了一种基于系统性情境提取、概率建模和验证的工业自动驾驶车辆安全验证新方法，通过概率模型检查验证安全属性，有效识别高风险情境并提供定量安全保证。", "motivation": "随着工业自动驾驶车辆在安全关键环境中的部署增加，确保其在多样化条件下的安全运行至关重要。", "method": "构建情境覆盖网格，结合概率数据生成概率模型，并通过概率模型检查验证安全属性。", "result": "方法有效识别高风险情境，提供定量安全保证，并支持符合监管标准。", "conclusion": "该方法为自动驾驶系统的稳健部署提供了有力支持。"}}
{"id": "2507.11942", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11942", "abs": "https://arxiv.org/abs/2507.11942", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "comment": "ACL 2025", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "AI": {"tldr": "论文提出了一种动态注意力感知的任务无关提示压缩方法（DAC），通过结合熵和注意力信息，动态感知压缩过程中的熵变化，实现了细粒度的提示压缩。", "motivation": "现有方法主要依赖信息熵作为压缩指标，忽略了注意力关键令牌的重要性以及压缩过程中熵的变化。", "method": "提出DAC方法，动态整合熵和注意力信息，感知压缩过程中的熵变化。", "result": "在多个领域（如LongBench、GSM8K、BBH）的实验表明，DAC在各种任务和大型语言模型上均表现出显著改进。", "conclusion": "DAC方法在任务无关提示压缩中表现出高效性和鲁棒性。"}}
{"id": "2507.11931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11931", "abs": "https://arxiv.org/abs/2507.11931", "authors": ["Jingqian Wu", "Peiqi Duan", "Zongqiang Wang", "Changwei Wang", "Boxin Shi", "Edmund Y. Lam"], "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "comment": null, "summary": "In low-light environments, conventional cameras often struggle to capture\nclear multi-view images of objects due to dynamic range limitations and motion\nblur caused by long exposure. Event cameras, with their high-dynamic range and\nhigh-speed properties, have the potential to mitigate these issues.\nAdditionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,\nfacilitating bright frame synthesis from multiple viewpoints in low-light\nconditions. However, naively using an event-assisted 3D GS approach still faced\nchallenges because, in low light, events are noisy, frames lack quality, and\nthe color tone may be inconsistent. To address these issues, we propose\nDark-EvGS, the first event-assisted 3D GS framework that enables the\nreconstruction of bright frames from arbitrary viewpoints along the camera\ntrajectory. Triplet-level supervision is proposed to gain holistic knowledge,\ngranular details, and sharp scene rendering. The color tone matching block is\nproposed to guarantee the color consistency of the rendered frames.\nFurthermore, we introduce the first real-captured dataset for the event-guided\nbright frame synthesis task via 3D GS-based radiance field reconstruction.\nExperiments demonstrate that our method achieves better results than existing\nmethods, conquering radiance field reconstruction under challenging low-light\nconditions. The code and sample data are included in the supplementary\nmaterial.", "AI": {"tldr": "提出Dark-EvGS框架，结合事件相机和3D高斯泼溅技术，解决低光环境下多视角图像重建问题，并通过实验验证其优越性。", "motivation": "传统相机在低光环境下难以捕捉清晰多视角图像，事件相机和高斯泼溅技术虽能部分解决，但仍面临噪声、低质量和色彩不一致的挑战。", "method": "提出Dark-EvGS框架，采用三重监督学习和色彩匹配模块，确保重建帧的细节和色彩一致性，并创建首个真实数据集。", "result": "实验表明，该方法在低光条件下优于现有方法，成功实现高质量辐射场重建。", "conclusion": "Dark-EvGS为低光环境下的多视角图像重建提供了有效解决方案，并展示了事件相机与高斯泼溅技术的潜力。"}}
{"id": "2507.11959", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11959", "abs": "https://arxiv.org/abs/2507.11959", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "AI": {"tldr": "提出了一种新型的PoT量化框架，用于LLM权重，在极低精度下优于现有方法，并实现更快的推理速度。", "motivation": "LLMs需要大量计算资源，现有PoT量化在GPU上效率不足。", "method": "采用两步后训练算法：初始化量化尺度并校准优化。", "result": "在2-和3-bit格式下表现优于现有整数量化方法，GPU推理速度显著提升。", "conclusion": "新型PoT量化框架在低精度和GPU效率上具有优势。"}}
{"id": "2507.12174", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12174", "abs": "https://arxiv.org/abs/2507.12174", "authors": ["Zhenmin Huang", "Yusen Xie", "Benshan Ma", "Shaojie Shen", "Jun Ma"], "title": "Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties", "comment": null, "summary": "Trajectory planning involving multi-agent interactions has been a\nlong-standing challenge in the field of robotics, primarily burdened by the\ninherent yet intricate interactions among agents. While game-theoretic methods\nare widely acknowledged for their effectiveness in managing multi-agent\ninteractions, significant impediments persist when it comes to accommodating\nthe intentional uncertainties of agents. In the context of intentional\nuncertainties, the heavy computational burdens associated with existing\ngame-theoretic methods are induced, leading to inefficiencies and poor\nscalability. In this paper, we propose a novel game-theoretic interactive\ntrajectory planning method to effectively address the intentional uncertainties\nof agents, and it demonstrates both high efficiency and enhanced scalability.\nAs the underpinning basis, we model the interactions between agents under\nintentional uncertainties as a general Bayesian game, and we show that its\nagent-form equivalence can be represented as a potential game under certain\nminor assumptions. The existence and attainability of the optimal interactive\ntrajectories are illustrated, as the corresponding Bayesian Nash equilibrium\ncan be attained by optimizing a unified optimization problem. Additionally, we\npresent a distributed algorithm based on the dual consensus alternating\ndirection method of multipliers (ADMM) tailored to the parallel solving of the\nproblem, thereby significantly improving the scalability. The attendant\noutcomes from simulations and experiments demonstrate that the proposed method\nis effective across a range of scenarios characterized by general forms of\nintentional uncertainties. Its scalability surpasses that of existing\ncentralized and decentralized baselines, allowing for real-time interactive\ntrajectory planning in uncertain game settings.", "AI": {"tldr": "提出了一种新的博弈论交互轨迹规划方法，有效解决多智能体交互中的意图不确定性，具有高效性和可扩展性。", "motivation": "多智能体交互中的轨迹规划面临意图不确定性的挑战，现有博弈论方法计算负担重且可扩展性差。", "method": "将意图不确定性下的交互建模为贝叶斯博弈，并转化为潜在博弈，提出基于ADMM的分布式算法。", "result": "方法在多种意图不确定性场景下有效，可扩展性优于现有基线，支持实时规划。", "conclusion": "新方法高效解决了意图不确定性下的多智能体轨迹规划问题，具有实际应用潜力。"}}
{"id": "2507.11953", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11953", "abs": "https://arxiv.org/abs/2507.11953", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "comment": "ACL 2025", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "AI": {"tldr": "论文提出IAM框架，通过利用不同规模LLMs间注意力矩阵的高相似性，优化计算和KV缓存使用，实现加速和资源节省。", "motivation": "当前LLMs在长上下文场景下资源消耗大，现有优化方法未充分利用外部信息。", "method": "分析注意力矩阵相似性，提出IAM框架，通过小规模与大规模LLMs间的注意力映射优化计算和KV缓存。", "result": "IAM加速预填充15%，减少KV缓存使用22.1%，性能无明显损失，且与现有方法兼容。", "conclusion": "IAM为提升LLM效率提供了通用且兼容的新工具。"}}
{"id": "2507.11932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11932", "abs": "https://arxiv.org/abs/2507.11932", "authors": ["Mohammad Shahab Sepehri", "Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs", "comment": null, "summary": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.", "AI": {"tldr": "论文介绍了Hyperphantasia基准，用于评估多模态大语言模型（MLLMs）的心理可视化能力，发现其与人类表现存在显著差距。", "motivation": "当前基准主要评估被动视觉感知，缺乏对主动构建视觉模式以支持问题解决能力的评估，而心理可视化是人类认知的核心能力。", "method": "设计了包含四个程序生成任务的Hyperphantasia基准，分为三个难度级别，并评估了最先进模型的性能。", "result": "评估显示MLLMs在心理可视化能力上与人类存在显著差距，部分模型仅能识别视觉模式。", "conclusion": "心理可视化对当前MLLMs仍是一个开放挑战，强化学习可能有助于提升视觉模拟能力。"}}
{"id": "2507.11966", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.11966", "abs": "https://arxiv.org/abs/2507.11966", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "comment": null, "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "AI": {"tldr": "提出了一种两阶段框架，用于在低资源语言对中保留毒性的翻译，并以新加坡英语为例验证其有效性。", "motivation": "在线交流中，标准翻译系统难以处理低资源语言中的俚语、混合语和文化嵌入的有害内容标记，尤其是在毒性内容翻译方面。", "method": "1. 通过人工验证的少样本提示工程，筛选和排序新加坡英语-目标语言的例子；2. 通过直接和回译的语义相似性，优化模型-提示对。", "result": "定量人工评估证实了该框架的有效性和效率。", "conclusion": "该框架不仅提升了翻译质量，还支持低资源环境下的文化敏感内容审核和基准测试，强调了在现实应用中保留社会语言细微差别的重要性。"}}
{"id": "2507.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12194", "abs": "https://arxiv.org/abs/2507.12194", "authors": ["Hongming Shen", "Xun Chen", "Yulin Hui", "Zhenyu Wu", "Wei Wang", "Qiyang Lyu", "Tianchen Deng", "Danwei Wang"], "title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization", "comment": null, "summary": "Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios.", "AI": {"tldr": "提出了一种统一的LGL方法UniLGL，通过编码完整点云信息（几何和材质）到BEV图像，实现空间、材质和传感器类型的统一性。", "motivation": "现有LGL方法仅考虑部分信息或针对同质LiDAR传感器，忽略了统一性。", "method": "将点云编码为空间和强度BEV图像，设计多BEV融合网络提取统一特征，引入视角不变性假设实现传感器类型统一。", "result": "实验验证UniLGL在真实环境中表现优异，支持多种平台应用。", "conclusion": "UniLGL在工业和野外场景中具有广泛适用性。"}}
{"id": "2507.11954", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11954", "abs": "https://arxiv.org/abs/2507.11954", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "comment": "15 pages, 3 figures, 7 tables", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "AI": {"tldr": "论文提出了一种多阶段的基于查询的知识图谱问答框架，用于提升多跳和时间性问题的性能。", "motivation": "大型语言模型在多跳推理和时间性问题上表现不佳，因此需要一种模块化的替代方案。", "method": "采用多阶段的查询生成方法，结合实体链接和谓词匹配的新技术。", "result": "在多跳和时间性问答数据集上表现出色，验证了框架的鲁棒性。", "conclusion": "基于查询的多阶段框架为小型语言模型在多跳和时间性问答中提供了潜力。"}}
{"id": "2507.11955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11955", "abs": "https://arxiv.org/abs/2507.11955", "authors": ["Yuhang Zhang", "Zhengyu Zhang", "Muxin Liao", "Shishun Tian", "Wenbin Zou", "Lu Zhang", "Chen Xu"], "title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation", "comment": "This paper was accepted by IEEE Transactions on Intelligent\n  Transportation Systems", "summary": "Generalizable semantic segmentation aims to perform well on unseen target\ndomains, a critical challenge due to real-world applications requiring high\ngeneralizability. Class-wise prototypes, representing class centroids, serve as\ndomain-invariant cues that benefit generalization due to their stability and\nsemantic consistency. However, this approach faces three challenges. First,\nexisting methods often adopt coarse prototypical alignment strategies, which\nmay hinder performance. Second, naive prototypes computed by averaging source\nbatch features are prone to overfitting and may be negatively affected by\nunrelated source data. Third, most methods treat all source samples equally,\nignoring the fact that different features have varying adaptation difficulties.\nTo address these limitations, we propose a novel framework for generalizable\nsemantic segmentation: Prototypical Progressive Alignment and Reweighting\n(PPAR), leveraging the strong generalization ability of the CLIP model.\nSpecifically, we define two prototypes: the Original Text Prototype (OTP) and\nVisual Text Prototype (VTP), generated via CLIP to serve as a solid base for\nalignment. We then introduce a progressive alignment strategy that aligns\nfeatures in an easy-to-difficult manner, reducing domain gaps gradually.\nFurthermore, we propose a prototypical reweighting mechanism that estimates the\nreliability of source data and adjusts its contribution, mitigating the effect\nof irrelevant or harmful features (i.e., reducing negative transfer). We also\nprovide a theoretical analysis showing the alignment between our method and\ndomain generalization theory. Extensive experiments across multiple benchmarks\ndemonstrate that PPAR achieves state-of-the-art performance, validating its\neffectiveness.", "AI": {"tldr": "PPAR框架通过渐进式对齐和原型重加权，结合CLIP模型，提升了语义分割的泛化能力。", "motivation": "解决现有方法在原型对齐中的不足，如粗糙对齐、原型过拟合及忽略特征适应性差异。", "method": "定义OTP和VTP原型，采用渐进对齐策略和原型重加权机制。", "result": "在多个基准测试中达到最优性能。", "conclusion": "PPAR有效提升了语义分割的泛化能力，验证了其理论合理性。"}}
{"id": "2507.12006", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12006", "abs": "https://arxiv.org/abs/2507.12006", "authors": ["Linwei Chen", "Lin Gu", "Ying Fu"], "title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "comment": "Accepted by ICCV 2025", "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\n\\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.", "AI": {"tldr": "提出了一种基于电路理论的频率动态注意力调制（FDAM）方法，通过注意力反转和频率动态缩放改进ViTs的频率响应，避免细节丢失，提升性能。", "motivation": "ViTs的注意力机制导致低频信息占主导，高频细节丢失，影响性能。", "method": "提出FDAM，包含注意力反转（AttInv）和频率动态缩放（FreqScale），动态调整频率响应。", "result": "在多种模型（如SegFormer、DeiT）和任务（语义分割、目标检测）中性能提升，并在遥感检测中达到SOTA。", "conclusion": "FDAM有效解决了ViTs的频率消失问题，显著提升了视觉任务的性能。"}}
{"id": "2507.12273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12273", "abs": "https://arxiv.org/abs/2507.12273", "authors": ["Luca Garello", "Francesca Cocchella", "Alessandra Sciutti", "Manuel Catalano", "Francesco Rea"], "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot", "comment": null, "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.", "AI": {"tldr": "论文介绍了一款名为Alter-Ego的自主博物馆导览机器人，结合了先进的导航和交互技术，利用大语言模型（LLMs）实现实时问答互动，并通过SLAM技术优化导航。实验结果显示机器人受到欢迎，但也存在一些局限性。", "motivation": "探索自主机器人在文化空间中的应用潜力，提升博物馆参观体验，支持知识获取和可访问性。", "method": "设计并实现Alter-Ego机器人，结合LLMs和SLAM技术，进行实地测试，通过定性和定量分析评估效果。", "result": "机器人受到34名参与者的普遍好评，提升了博物馆体验，但在理解和响应方面存在不足。", "conclusion": "AI驱动的机器人在文化空间具有潜力，但实际部署仍面临技术和交互挑战。"}}
{"id": "2507.11972", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.11972", "abs": "https://arxiv.org/abs/2507.11972", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "comment": null, "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "AI": {"tldr": "研究比较了人类和大型语言模型（LLMs）在阅读理解中的表现，通过图结构分析发现LLMs在语义理解上具有高度一致性。", "motivation": "探索人类和LLMs在语言理解上的差异，并改进对阅读理解过程的深度分析。", "method": "使用基于LLM的AI代理将文本表示为图结构（节点和边），并通过眼动数据验证重要节点和边的注视分布。", "result": "LLMs在图拓扑结构层面表现出高度一致的语言理解能力。", "conclusion": "研究为人类与AI协同学习策略提供了新见解，并验证了图结构分析的有效性。"}}
{"id": "2507.11968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11968", "abs": "https://arxiv.org/abs/2507.11968", "authors": ["Sahid Hossain Mustakim", "S M Jishanul Islam", "Ummay Maria Muna", "Montasir Chowdhury", "Mohammed Jawwadul Islam", "Sadia Ahmmed", "Tashfia Sikder", "Syed Tasdid Azam Dhrubo", "Swakkhar Shatabda"], "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation", "comment": "Accepted as long paper, SVU Workshop at ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.", "AI": {"tldr": "论文提出了一个评估多模态大语言模型（MLLMs）在短视频内容审核中安全性的框架，包括SVMA数据集和ChimeraBreak攻击策略，揭示了模型的显著漏洞。", "motivation": "当前的安全评估多关注单模态攻击，未能全面应对多模态攻击的挑战，因此需要更全面的评估框架。", "method": "提出SVMA数据集和ChimeraBreak攻击策略，同时挑战视觉、听觉和语义推理路径。", "result": "实验显示MLLMs存在显著漏洞，攻击成功率（ASR）高，并揭示了模型对良性或违规内容的分类偏差。", "conclusion": "研究为开发更鲁棒和安全的MLLMs提供了重要见解。"}}
{"id": "2507.12008", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12008", "abs": "https://arxiv.org/abs/2507.12008", "authors": ["Jiawen Wang", "Yinda Chen", "Xiaoyu Liu", "Che Liu", "Dong Liu", "Jianqing Gao", "Zhiwei Xiong"], "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation", "comment": "Accepted by ICML 2025", "summary": "Recent works have correlated Masked Image Modeling (MIM) with consistency\nregularization in Unsupervised Domain Adaptation (UDA). However, they merely\ntreat masking as a special form of deformation on the input images and neglect\nthe theoretical analysis, which leads to a superficial understanding of masked\nreconstruction and insufficient exploitation of its potential in enhancing\nfeature extraction and representation learning. In this paper, we reframe\nmasked reconstruction as a sparse signal reconstruction problem and\ntheoretically prove that the dual form of complementary masks possesses\nsuperior capabilities in extracting domain-agnostic image features. Based on\nthis compelling insight, we propose MaskTwins, a simple yet effective UDA\nframework that integrates masked reconstruction directly into the main training\npipeline. MaskTwins uncovers intrinsic structural patterns that persist across\ndisparate domains by enforcing consistency between predictions of images masked\nin complementary ways, enabling domain generalization in an end-to-end manner.\nExtensive experiments verify the superiority of MaskTwins over baseline methods\nin natural and biological image segmentation. These results demonstrate the\nsignificant advantages of MaskTwins in extracting domain-invariant features\nwithout the need for separate pre-training, offering a new paradigm for\ndomain-adaptive segmentation.", "AI": {"tldr": "论文将掩码图像建模（MIM）重新定义为稀疏信号重建问题，提出MaskTwins框架，通过互补掩码增强特征提取，在无监督域适应（UDA）中实现端到端训练。", "motivation": "现有方法仅将掩码视为图像变形，缺乏理论分析，未能充分利用掩码重建在特征提取和表示学习中的潜力。", "method": "提出MaskTwins框架，通过互补掩码强制预测一致性，提取跨域不变特征。", "result": "实验表明MaskTwins在自然和生物图像分割中优于基线方法。", "conclusion": "MaskTwins无需单独预训练即可提取域不变特征，为域自适应分割提供了新范式。"}}
{"id": "2507.12391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12391", "abs": "https://arxiv.org/abs/2507.12391", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.", "AI": {"tldr": "评估多模态大语言模型在机器人路径规划中的视觉输入效用，发现视觉输入在简单任务中有效，但在复杂任务中表现不佳。", "motivation": "探索视觉输入对多模态大语言模型在机器人路径规划任务中的潜在提升作用。", "method": "通过综合基准测试，比较15种多模态大语言模型在2D网格环境中的路径规划表现，分析文本与文本加视觉输入的差异。", "result": "在简单任务中视觉输入或少量文本提示有一定帮助，但在复杂任务中表现显著下降；大模型表现更好，但视觉模态并非总是优于结构化文本。", "conclusion": "当前模型在空间推理、约束遵循和多模态整合方面存在局限性，需进一步改进。"}}
{"id": "2507.11979", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11979", "abs": "https://arxiv.org/abs/2507.11979", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "AI": {"tldr": "研究探讨了价值相似性对LLM代理之间关系建立的影响，发现价值相似性高的代理表现出更高的信任和亲密感。", "motivation": "探索价值相似性在人工社会中是否像在人类社会中一样影响关系建立。", "method": "通过两个实验：初步实验评估LLM中价值的可控性；主实验生成具有特定价值的代理对并分析其信任和亲密感。", "result": "价值相似性高的代理对表现出更高的相互信任和亲密感。", "conclusion": "LLM代理模拟可作为社会科学理论的有效测试平台，并有助于理解价值如何影响关系建立。"}}
{"id": "2507.11969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11969", "abs": "https://arxiv.org/abs/2507.11969", "authors": ["Zhaohong Huang", "Yuxin Zhang", "Jingjing Xie", "Fei Chao", "Rongrong Ji"], "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models", "comment": null, "summary": "Recent advances in test-time adaptation (TTA) for Vision-Language Models\n(VLMs) have garnered increasing attention, particularly through the use of\nmultiple augmented views of a single image to boost zero-shot generalization.\nUnfortunately, existing methods fail to strike a satisfactory balance between\nperformance and efficiency, either due to excessive overhead of tuning text\nprompts or unstable benefits from handcrafted, training-free visual feature\nenhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),\nan efficient and effective TTA paradigm that incorporates two learnable biases\nduring TTA, unfolded as the global bias and spatial bias. Particularly, the\nglobal bias captures the global semantic features of a test image by learning\nconsistency across augmented views, while spatial bias learns the semantic\ncoherence between regions in the image's spatial visual representation. It is\nworth highlighting that these two sets of biases are directly added to the\nlogits outputed by the pretrained VLMs, which circumvent the full\nbackpropagation through VLM that hinders the efficiency of existing TTA\nmethods. This endows GS-Bias with extremely high efficiency while achieving\nstate-of-the-art performance on 15 benchmark datasets. For example, it achieves\na 2.23% improvement over TPT in cross-dataset generalization and a 2.72%\nimprovement in domain generalization, while requiring only 6.5% of TPT's memory\nusage on ImageNet.", "AI": {"tldr": "GS-Bias是一种高效的测试时适应（TTA）方法，通过全局和空间偏差学习提升视觉语言模型的零样本泛化能力，显著优于现有方法。", "motivation": "现有TTA方法在性能和效率之间难以平衡，要么调整文本提示开销大，要么手工视觉特征增强效果不稳定。", "method": "GS-Bias引入全局偏差和空间偏差，直接添加到预训练模型的输出logits中，避免全反向传播，提高效率。", "result": "在15个基准数据集上达到SOTA性能，例如在跨数据集泛化和领域泛化上分别提升2.23%和2.72%，内存使用仅为TPT的6.5%。", "conclusion": "GS-Bias以高效的方式显著提升了TTA性能，为视觉语言模型的零样本泛化提供了新思路。"}}
{"id": "2507.12012", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12012", "abs": "https://arxiv.org/abs/2507.12012", "authors": ["Matthias Perkonigg", "Nina Bastati", "Ahmed Ba-Ssalamah", "Peter Mesenbrink", "Alexander Goehler", "Miljen Martic", "Xiaofei Zhou", "Michael Trauner", "Georg Langs"], "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease", "comment": null, "summary": "Quantifiable image patterns associated with disease progression and treatment\nresponse are critical tools for guiding individual treatment, and for\ndeveloping novel therapies. Here, we show that unsupervised machine learning\ncan identify a pattern vocabulary of liver tissue in magnetic resonance images\nthat quantifies treatment response in diffuse liver disease. Deep clustering\nnetworks simultaneously encode and cluster patches of medical images into a\nlow-dimensional latent space to establish a tissue vocabulary. The resulting\ntissue types capture differential tissue change and its location in the liver\nassociated with treatment response. We demonstrate the utility of the\nvocabulary on a randomized controlled trial cohort of non-alcoholic\nsteatohepatitis patients. First, we use the vocabulary to compare longitudinal\nliver change in a placebo and a treatment cohort. Results show that the method\nidentifies specific liver tissue change pathways associated with treatment, and\nenables a better separation between treatment groups than established\nnon-imaging measures. Moreover, we show that the vocabulary can predict biopsy\nderived features from non-invasive imaging data. We validate the method on a\nseparate replication cohort to demonstrate the applicability of the proposed\nmethod.", "AI": {"tldr": "无监督机器学习用于识别肝脏磁共振图像中的组织模式，量化弥漫性肝病的治疗反应。", "motivation": "量化与疾病进展和治疗反应相关的图像模式，为个体化治疗和新疗法开发提供工具。", "method": "使用深度聚类网络对医学图像块进行编码和聚类，建立组织词汇表。", "result": "该方法能识别与治疗相关的特定肝脏组织变化路径，并比非成像方法更好地区分治疗组。", "conclusion": "该方法在非侵入性影像数据中预测活检特征，并在独立队列中验证了其适用性。"}}
{"id": "2507.12407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12407", "abs": "https://arxiv.org/abs/2507.12407", "authors": ["Svetlana Levit", "Marc Toussaint"], "title": "Regrasp Maps for Sequential Manipulation Planning", "comment": null, "summary": "We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems.", "AI": {"tldr": "提出了一种基于优化和状态空间抽象的TAMP方法，通过生成和调整regrasp地图来加速复杂环境中的抓取规划。", "motivation": "解决在受限和杂乱环境中需要多次未知位置抓取的操作问题，提高规划效率。", "method": "使用regrasp地图抽象状态空间，为优化求解器提供抓取序列和区域猜测，并通过迭代调整地图和解决子问题实现鲁棒搜索。", "result": "能够高效处理具有挑战性的抓取操作问题。", "conclusion": "提出的方法通过结合状态空间抽象和迭代优化，显著提升了复杂环境中的抓取规划性能。"}}
{"id": "2507.11981", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11981", "abs": "https://arxiv.org/abs/2507.11981", "authors": ["Lukas Ellinger", "Miriam Anschütz", "Georg Groh"], "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "AI": {"tldr": "研究探讨了简化对多义词定义完整性的影响，发现过度简化会降低定义质量，导致误解风险增加。通过微调模型，可以显著提升多义词定义的质量。", "motivation": "大型语言模型（LLMs）在不同目标群体（如儿童或语言学习者）中提供准确的多义词定义时，简化可能导致关键信息丢失，引发误解。", "method": "使用两种新颖的多语言评估数据集，测试了多个LLM模型（如DeepSeek v3、Llama 4 Maverick等），并通过LLM-as-Judge和人工标注评估简化对定义质量的影响。", "result": "简化显著降低了定义的完整性，尤其是忽略了多义性。通过微调Llama 3.1 8B模型，多义词定义质量在所有提示类型中均得到显著提升。", "conclusion": "研究强调在教育NLP中需平衡简洁性与完整性，以确保为所有学习者提供可靠、上下文感知的定义。"}}
{"id": "2507.11980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11980", "abs": "https://arxiv.org/abs/2507.11980", "authors": ["Jiajian Xie", "Shengyu Zhang", "Zhou Zhao", "Fan Wu", "Fei Wu"], "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models", "comment": "21 pages, 8 figures", "summary": "Diffusion Models have shown remarkable proficiency in image and video\nsynthesis. As model size and latency increase limit user experience, hybrid\nedge-cloud collaborative framework was recently proposed to realize fast\ninference and high-quality generation, where the cloud model initiates\nhigh-quality semantic planning and the edge model expedites later-stage\nrefinement. However, excessive cloud denoising prolongs inference time, while\ninsufficient steps cause semantic ambiguity, leading to inconsistency in edge\nmodel output. To address these challenges, we propose EC-Diff that accelerates\ncloud inference through gradient-based noise estimation while identifying the\noptimal point for cloud-edge handoff to maintain generation quality.\nSpecifically, we design a K-step noise approximation strategy to reduce cloud\ninference frequency by using noise gradients between steps and applying cloud\ninference periodically to adjust errors. Then we design a two-stage greedy\nsearch algorithm to efficiently find the optimal parameters for noise\napproximation and edge model switching. Extensive experiments demonstrate that\nour method significantly enhances generation quality compared to edge\ninference, while achieving up to an average $2\\times$ speedup in inference\ncompared to cloud inference. Video samples and source code are available at\nhttps://ec-diff.github.io/.", "AI": {"tldr": "EC-Diff通过梯度噪声估计加速云推理，优化云边切换点以保持生成质量，显著提升推理速度与生成质量。", "motivation": "解决云推理时间过长与边缘模型输出不一致的问题。", "method": "采用K步噪声近似策略减少云推理频率，设计两阶段贪心搜索算法优化参数。", "result": "相比边缘推理显著提升生成质量，推理速度平均提升2倍。", "conclusion": "EC-Diff在生成质量和推理速度上取得平衡，适用于边缘-云协作框架。"}}
{"id": "2507.12017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12017", "abs": "https://arxiv.org/abs/2507.12017", "authors": ["Xiwei Zhang", "Chunjin Yang", "Yiming Xiao", "Runtong Zhang", "Fanman Meng"], "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection", "comment": "8 main-pages, 3 reference-pages, 5 figures, 6 tables", "summary": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain\nto the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB\ndomain as a unified domain and neglect the multiple subdomains within it, such\nas daytime, nighttime, and foggy scenes. We argue that decoupling the\ndomain-invariant (DI) and domain-specific (DS) features across these multiple\nsubdomains is beneficial for RGB-IR domain adaptation. To this end, this paper\nproposes a new SS-DC framework based on a decoupling-coupling strategy. In\nterms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)\nmodule in the aspect of spectral decomposition. Due to the style and content\ninformation being highly embedded in different frequency bands, this module can\ndecouple DI and DS components more accurately and interpretably. A novel filter\nbank-based spectral processing paradigm and a self-distillation-driven\ndecoupling loss are proposed to improve the spectral domain decoupling. In\nterms of coupling, a new spatial-spectral coupling method is proposed, which\nrealizes joint coupling through spatial and spectral DI feature pyramids.\nMeanwhile, this paper introduces DS from decoupling to reduce the domain bias.\nExtensive experiments demonstrate that our method can significantly improve the\nbaseline performance and outperform existing UDAOD methods on multiple RGB-IR\ndatasets, including a new experimental protocol proposed in this paper based on\nthe FLIR-ADAS dataset.", "AI": {"tldr": "论文提出了一种基于解耦-耦合策略的SS-DC框架，用于解决可见光到红外（RGB-IR）的无监督域自适应目标检测问题，通过光谱分解和空间-光谱耦合方法提升性能。", "motivation": "现有方法将RGB域视为统一域，忽略了其多子域（如白天、夜晚、雾天场景）的特性，解耦域不变（DI）和域特定（DS）特征有助于RGB-IR域适应。", "method": "设计了光谱自适应幂等解耦（SAID）模块，通过光谱分解解耦DI和DS特征；提出基于滤波器组的光谱处理范式和解耦损失；采用空间-光谱耦合方法联合DI特征金字塔。", "result": "实验表明，该方法显著提升基线性能，在多个RGB-IR数据集上优于现有UDAOD方法，包括基于FLIR-ADAS数据集的新实验协议。", "conclusion": "通过解耦和耦合策略，SS-DC框架有效提升了RGB-IR域自适应目标检测的性能，验证了多子域特征解耦的重要性。"}}
{"id": "2507.12431", "categories": ["cs.RO", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.12431", "abs": "https://arxiv.org/abs/2507.12431", "authors": ["Connor Burgess", "Kyle Douin", "Amir Kordijazi"], "title": "Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement", "comment": "14 pages, 4 figures", "summary": "The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work\ncell developed to automate the measurement of surface wettability on 3D-printed\nmaterials. Designed for precision, repeatability, and safety, ACAT addresses\nthe limitations of manual contact angle testing by combining programmable\nrobotics, precise liquid dispensing, and a modular software-hardware\narchitecture. The system is composed of three core subsystems: (1) an\nelectrical system including power, control, and safety circuits compliant with\nindustrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software\ncontrol system based on a Raspberry Pi and Python, featuring fault detection,\nGPIO logic, and operator interfaces; and (3) a mechanical system that includes\na 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser\nenclosed within a safety-certified frame. The ACAT enables high-throughput,\nautomated surface characterization and provides a robust platform for future\nintegration into smart manufacturing and materials discovery workflows. This\npaper details the design methodology, implementation strategies, and system\nintegration required to develop the ACAT platform.", "AI": {"tldr": "ACAT是一个自动化机器人系统，用于测量3D打印材料的表面润湿性，解决了手动测试的局限性，具有高精度和可重复性。", "motivation": "手动接触角测试存在精度和效率问题，ACAT旨在通过自动化解决这些问题。", "method": "ACAT由电气系统、软件控制系统和机械系统组成，采用模块化设计，符合工业标准。", "result": "ACAT实现了高通量、自动化的表面表征，为智能制造和材料发现提供了平台。", "conclusion": "ACAT的设计和实现展示了其在自动化表面测试中的潜力，未来可进一步集成到智能制造中。"}}
{"id": "2507.12004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12004", "abs": "https://arxiv.org/abs/2507.12004", "authors": ["Josip Jukić"], "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "comment": null, "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "AI": {"tldr": "该论文针对神经语言模型的数据和参数效率问题，提出了基于表示平滑性的创新优化技术，结合主动学习和参数高效微调，显著提升了模型性能和效率。", "motivation": "解决神经语言模型在数据和参数效率上的挑战，增强模型的鲁棒性和泛化能力。", "method": "1. 分析语言表示的动态特性并提出基于平滑性的正则化策略；2. 结合主动学习和参数高效微调提升效率；3. 利用上下文学习增强弱监督技术。", "result": "实验表明，这些方法在性能、稳定性和效率上显著优于传统方法，尤其在低资源环境中表现突出。", "conclusion": "论文提出的技术为高效、鲁棒的神经语言模型提供了新思路，尤其在减少标注依赖和计算资源方面具有重要价值。"}}
{"id": "2507.11985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11985", "abs": "https://arxiv.org/abs/2507.11985", "authors": ["Jiahao Xia", "Yike Wu", "Wenjian Huang", "Jianguo Zhang", "Jian Zhang"], "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints", "comment": "Accepted to ICCV 2025", "summary": "Part-level features are crucial for image understanding, but few studies\nfocus on them because of the lack of fine-grained labels. Although unsupervised\npart discovery can eliminate the reliance on labels, most of them cannot\nmaintain robustness across various categories and scenarios, which restricts\ntheir application range. To overcome this limitation, we present a more\neffective paradigm for unsupervised part discovery, named Masked Part\nAutoencoder (MPAE). It first learns part descriptors as well as a feature map\nfrom the inputs and produces patch features from a masked version of the\noriginal images. Then, the masked regions are filled with the learned part\ndescriptors based on the similarity between the local features and descriptors.\nBy restoring these masked patches using the part descriptors, they become\nbetter aligned with their part shapes, guided by appearance features from\nunmasked patches. Finally, MPAE robustly discovers meaningful parts that\nclosely match the actual object shapes, even in complex scenarios. Moreover,\nseveral looser yet more effective constraints are proposed to enable MPAE to\nidentify the presence of parts across various scenarios and categories in an\nunsupervised manner. This provides the foundation for addressing challenges\nposed by occlusion and for exploring part similarity across multiple\ncategories. Extensive experiments demonstrate that our method robustly\ndiscovers meaningful parts across various categories and scenarios. The code is\navailable at the project https://github.com/Jiahao-UTS/MPAE.", "AI": {"tldr": "提出了一种名为MPAE的无监督部件发现方法，通过掩码自编码器学习部件描述符，并在复杂场景中鲁棒地发现与实际物体形状匹配的部件。", "motivation": "现有无监督部件发现方法在跨类别和场景中缺乏鲁棒性，限制了应用范围。", "method": "MPAE通过掩码自编码器学习部件描述符和特征图，利用局部特征与描述符的相似性填充掩码区域，从而对齐部件形状。", "result": "MPAE在多种类别和场景中鲁棒地发现了与实际形状匹配的部件，并通过实验验证了其有效性。", "conclusion": "MPAE为无监督部件发现提供了一种更有效的范式，能够应对遮挡和跨类别部件相似性等挑战。"}}
{"id": "2507.12029", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12029", "abs": "https://arxiv.org/abs/2507.12029", "authors": ["Xinhang Wan", "Jiyuan Liu", "Qian Qu", "Suyuan Liu", "Chuyu Zhang", "Fangdi Wang", "Xinwang Liu", "En Zhu", "Kunlun He"], "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery", "comment": null, "summary": "In this paper, we address the problem of novel class discovery (NCD), which\naims to cluster novel classes by leveraging knowledge from disjoint known\nclasses. While recent advances have made significant progress in this area,\nexisting NCD methods face two major limitations. First, they primarily focus on\nsingle-view data (e.g., images), overlooking the increasingly common multi-view\ndata, such as multi-omics datasets used in disease diagnosis. Second, their\nreliance on pseudo-labels to supervise novel class clustering often results in\nunstable performance, as pseudo-label quality is highly sensitive to factors\nsuch as data noise and feature dimensionality. To address these challenges, we\npropose a novel framework named Intra-view and Inter-view Correlation Guided\nMulti-view Novel Class Discovery (IICMVNCD), which is the first attempt to\nexplore NCD in multi-view setting so far. Specifically, at the intra-view\nlevel, leveraging the distributional similarity between known and novel\nclasses, we employ matrix factorization to decompose features into\nview-specific shared base matrices and factor matrices. The base matrices\ncapture distributional consistency among the two datasets, while the factor\nmatrices model pairwise relationships between samples. At the inter-view level,\nwe utilize view relationships among known classes to guide the clustering of\nnovel classes. This includes generating predicted labels through the weighted\nfusion of factor matrices and dynamically adjusting view weights of known\nclasses based on the supervision loss, which are then transferred to novel\nclass learning. Experimental results validate the effectiveness of our proposed\napproach.", "AI": {"tldr": "本文提出了一种名为IICMVNCD的新框架，首次在多视图数据中探索新类发现（NCD），解决了现有方法在单视图数据和伪标签依赖上的局限性。", "motivation": "现有NCD方法主要针对单视图数据，且依赖伪标签导致性能不稳定。本文旨在解决这些问题，并探索多视图数据中的NCD。", "method": "提出IICMVNCD框架，通过矩阵分解在视图内捕获分布一致性，在视图间利用已知类关系指导新类聚类。", "result": "实验验证了该方法的有效性。", "conclusion": "IICMVNCD在多视图NCD中表现优越，为未来研究提供了新方向。"}}
{"id": "2507.12440", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12440", "abs": "https://arxiv.org/abs/2507.12440", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA", "AI": {"tldr": "论文提出利用人类第一视角视频训练视觉-语言-动作（VLA）模型，通过逆运动学和动作重定向将人类动作转化为机器人动作，显著提升了机器人模仿学习的性能。", "motivation": "传统机器人模仿学习依赖真实机器人数据，但数据规模受限。人类视频数据规模大且场景丰富，为解决这一问题提供了可能。", "method": "使用人类第一视角视频训练VLA模型，预测人类手腕和手部动作，通过逆运动学和动作重定向转化为机器人动作，并用少量机器人演示微调模型。", "result": "在Isaac Humanoid Manipulation Benchmark上，EgoVLA模型显著优于基线，验证了人类数据的重要性。", "conclusion": "人类视频数据为机器人模仿学习提供了高效且丰富的解决方案，EgoVLA模型展示了其潜力。"}}
{"id": "2507.12039", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12039", "abs": "https://arxiv.org/abs/2507.12039", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "AI": {"tldr": "论文介绍了一种通用的语言创造力测试，用于评估人类和大型语言模型（LLMs）在生成新词和短语方面的能力。结果显示LLMs在所有评估标准上均优于人类。", "motivation": "研究旨在比较人类和LLMs在语言创造力方面的表现，特别是在生成新词和短语的能力上。", "method": "测试包括多种任务，评估基于词形变化和隐喻语言使用的创造力。使用OCSAI工具自动评估答案的原创性、精细性和灵活性。", "result": "LLMs在所有评估标准上优于人类，并在八项任务中的六项中表现更好。人类更倾向于扩展创造力，而LLMs偏向固定创造力。", "conclusion": "LLMs在语言创造力方面表现出色，但人类和LLMs在创造力类型上存在差异。"}}
{"id": "2507.11986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11986", "abs": "https://arxiv.org/abs/2507.11986", "authors": ["Jaehyun Lee", "Wonhark Park", "Wonsik Shin", "Hyunho Lee", "Hyoung Min Na", "Nojun Kwak"], "title": "Style Composition within Distinct LoRA modules for Traditional Art", "comment": null, "summary": "Diffusion-based text-to-image models have achieved remarkable results in\nsynthesizing diverse images from text prompts and can capture specific artistic\nstyles via style personalization. However, their entangled latent space and\nlack of smooth interpolation make it difficult to apply distinct painting\ntechniques in a controlled, regional manner, often causing one style to\ndominate. To overcome this, we propose a zero-shot diffusion pipeline that\nnaturally blends multiple styles by performing style composition on the\ndenoised latents predicted during the flow-matching denoising process of\nseparately trained, style-specialized models. We leverage the fact that\nlower-noise latents carry stronger stylistic information and fuse them across\nheterogeneous diffusion pipelines using spatial masks, enabling precise,\nregion-specific style control. This mechanism preserves the fidelity of each\nindividual style while allowing user-guided mixing. Furthermore, to ensure\nstructural coherence across different models, we incorporate depth-map\nconditioning via ControlNet into the diffusion framework. Qualitative and\nquantitative experiments demonstrate that our method successfully achieves\nregion-specific style mixing according to the given masks.", "AI": {"tldr": "提出了一种零样本扩散管道，通过在不同风格专用模型的去噪过程中融合潜在空间，实现多风格的自然混合和区域控制。", "motivation": "解决现有扩散模型在风格混合时难以控制区域性和平滑插值的问题。", "method": "在去噪过程中融合不同风格专用模型的潜在空间，利用低噪声潜在空间携带更强风格信息的特点，通过空间掩码实现区域风格控制，并结合ControlNet进行深度图条件化。", "result": "定性和定量实验表明，该方法能根据给定掩码成功实现区域风格混合。", "conclusion": "该方法在多风格混合和区域控制方面表现优异，保留了各风格的保真度。"}}
{"id": "2507.12060", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.12060", "abs": "https://arxiv.org/abs/2507.12060", "authors": ["Kun-Hsiang Lin", "Yu-Wen Tseng", "Kang-Yang Huang", "Jhih-Ciang Wu", "Wen-Huang Cheng"], "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing", "comment": "Accepted by MM'25", "summary": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.", "AI": {"tldr": "InstructFLIP是一种基于视觉语言模型（VLM）的新型指令调优框架，通过文本指导增强跨域泛化能力，显著减少训练冗余。", "motivation": "解决人脸防伪（FAS）中攻击类型语义理解不足和跨域训练冗余的问题。", "method": "结合VLM增强视觉输入感知，采用元域策略学习统一模型，并解耦指令为内容和风格两部分。", "result": "在准确率上超越SOTA模型，显著减少跨域训练冗余。", "conclusion": "InstructFLIP通过文本指导和元域策略有效提升了FAS的泛化能力和效率。"}}
{"id": "2507.12027", "categories": ["cs.CV", "cs.RO", "I.4.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12027", "abs": "https://arxiv.org/abs/2507.12027", "authors": ["Beining Xu", "Siting Zhu", "Hesheng Wang"], "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation", "comment": "8 pages, 2 figures, IROS 2025", "summary": "We propose SGLoc, a novel localization system that directly regresses camera\nposes from 3D Gaussian Splatting (3DGS) representation by leveraging semantic\ninformation. Our method utilizes the semantic relationship between 2D image and\n3D scene representation to estimate the 6DoF pose without prior pose\ninformation. In this system, we introduce a multi-level pose regression\nstrategy that progressively estimates and refines the pose of query image from\nthe global 3DGS map, without requiring initial pose priors. Moreover, we\nintroduce a semantic-based global retrieval algorithm that establishes\ncorrespondences between 2D (image) and 3D (3DGS map). By matching the extracted\nscene semantic descriptors of 2D query image and 3DGS semantic representation,\nwe align the image with the local region of the global 3DGS map, thereby\nobtaining a coarse pose estimation. Subsequently, we refine the coarse pose by\niteratively optimizing the difference between the query image and the rendered\nimage from 3DGS. Our SGLoc demonstrates superior performance over baselines on\n12scenes and 7scenes datasets, showing excellent capabilities in global\nlocalization without initial pose prior. Code will be available at\nhttps://github.com/IRMVLab/SGLoc.", "AI": {"tldr": "SGLoc是一种新颖的定位系统，通过语义信息直接从3D高斯泼溅（3DGS）表示回归相机位姿。", "motivation": "解决无需初始位姿先验的全局定位问题。", "method": "利用2D图像与3D场景表示的语义关系，通过多层级位姿回归策略和语义全局检索算法逐步估计和优化位姿。", "result": "在12scenes和7scenes数据集上表现优于基线方法。", "conclusion": "SGLoc在无初始位姿先验的情况下展示了出色的全局定位能力。"}}
{"id": "2507.12059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12059", "abs": "https://arxiv.org/abs/2507.12059", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "AI": {"tldr": "研究了28个大语言模型（LLMs）在方向推理（CDs）上的能力，发现即使是最新的大推理模型也无法在所有问题上可靠地确定正确方向。", "motivation": "评估LLMs在方向推理任务中的表现，探索其局限性。", "method": "使用基于模板生成的基准测试，测试LLMs在不同场景下确定方向的能力。", "result": "即使新模型也无法在所有问题上可靠地确定正确方向。", "conclusion": "LLMs在方向推理任务中仍有改进空间，需进一步研究。"}}
{"id": "2507.11990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11990", "abs": "https://arxiv.org/abs/2507.11990", "authors": ["Hyun-Jun Jin", "Young-Eun Kim", "Seong-Whan Lee"], "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation", "comment": null, "summary": "Recently, personalized portrait generation with a text-to-image diffusion\nmodel has significantly advanced with Textual Inversion, emerging as a\npromising approach for creating high-fidelity personalized images. Despite its\npotential, current Textual Inversion methods struggle to maintain consistent\nfacial identity due to semantic misalignments between textual and visual\nembedding spaces regarding identity. We introduce ID-EA, a novel framework that\nguides text embeddings to align with visual identity embeddings, thereby\nimproving identity preservation in a personalized generation. ID-EA comprises\ntwo key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned\nAdapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings\nwith a textual ID anchor, refining visual identity embeddings derived from a\nface recognition model using representative text embeddings. Then, the\nID-Adapter leverages the identity-enhanced embedding to adapt the text\ncondition, ensuring identity preservation by adjusting the cross-attention\nmodule in the pre-trained UNet model. This process encourages the text features\nto find the most related visual clues across the foreground snippets. Extensive\nquantitative and qualitative evaluations demonstrate that ID-EA substantially\noutperforms state-of-the-art methods in identity preservation metrics while\nachieving remarkable computational efficiency, generating personalized\nportraits approximately 15 times faster than existing approaches.", "AI": {"tldr": "ID-EA是一种新框架，通过ID驱动的增强器和适配器，改进文本到图像扩散模型中的身份一致性，显著提升个性化肖像生成的质量和效率。", "motivation": "当前Textual Inversion方法在个性化肖像生成中难以保持面部身份一致性，原因是文本与视觉嵌入空间在身份语义上的不对齐。", "method": "ID-EA包含ID-Enhancer和ID-Adapter：前者通过文本ID锚点增强视觉身份嵌入，后者调整预训练UNet模型的交叉注意力模块以保持身份。", "result": "ID-EA在身份保持指标上显著优于现有方法，且计算效率高，生成速度比现有方法快约15倍。", "conclusion": "ID-EA通过改进文本与视觉嵌入的对齐，有效解决了身份一致性问题，为个性化肖像生成提供了高效解决方案。"}}
{"id": "2507.12064", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12064", "abs": "https://arxiv.org/abs/2507.12064", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "comment": null, "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "AI": {"tldr": "论文提出了一种基于模块化风格测量管道的二元AI检测方法，使用spaCy模型进行文本预处理和特征提取，并采用轻量级梯度提升机作为分类器。", "motivation": "探索一种非神经网络的、计算成本低且可解释的方法来检测机器生成的文本。", "method": "使用spaCy模型进行文本预处理（如分词、命名实体识别等），提取数千个特征，并采用轻量级梯度提升机作为分类器。", "result": "通过训练一个包含50万篇机器生成文本的大型语料库，优化分类器参数以提高性能。", "conclusion": "该方法在保持计算效率的同时，提供了一种可解释的AI文本检测解决方案。"}}
{"id": "2507.12083", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12083", "abs": "https://arxiv.org/abs/2507.12083", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Xuesong Chen", "Xu Liu", "Shaojie Shen"], "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics", "comment": "Accepted by ICCV 2025", "summary": "Motion forecasting for on-road traffic agents presents both a significant\nchallenge and a critical necessity for ensuring safety in autonomous driving\nsystems. In contrast to most existing data-driven approaches that directly\npredict future trajectories, we rethink this task from a planning perspective,\nadvocating a \"First Reasoning, Then Forecasting\" strategy that explicitly\nincorporates behavior intentions as spatial guidance for trajectory prediction.\nTo achieve this, we introduce an interpretable, reward-driven intention\nreasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)\nscheme. Our method first encodes traffic agents and scene elements into a\nunified vectorized representation, then aggregates contextual features through\na query-centric paradigm. This enables the derivation of a reward distribution,\na compact yet informative representation of the target agent's behavior within\nthe given scene context via IRL. Guided by this reward heuristic, we perform\npolicy rollouts to reason about multiple plausible intentions, providing\nvaluable priors for subsequent trajectory generation. Finally, we develop a\nhierarchical DETR-like decoder integrated with bidirectional selective state\nspace models to produce accurate future trajectories along with their\nassociated probabilities. Extensive experiments on the large-scale Argoverse\nand nuScenes motion forecasting datasets demonstrate that our approach\nsignificantly enhances trajectory prediction confidence, achieving highly\ncompetitive performance relative to state-of-the-art methods.", "AI": {"tldr": "论文提出了一种基于规划视角的运动预测方法，通过先推理行为意图再预测轨迹的策略，结合逆强化学习和分层解码器，显著提升了预测性能。", "motivation": "自动驾驶系统中，运动预测对安全性至关重要。现有数据驱动方法直接预测轨迹，缺乏对行为意图的显式建模。", "method": "采用逆强化学习推理行为意图，结合查询中心范式聚合场景特征，通过分层解码器生成轨迹及其概率。", "result": "在Argoverse和nuScenes数据集上表现优异，显著提升了预测置信度。", "conclusion": "通过显式建模行为意图，该方法在运动预测任务中取得了竞争性表现。"}}
{"id": "2507.12075", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12075", "abs": "https://arxiv.org/abs/2507.12075", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Lluís Huguet Cabot", "Roberto Navigli"], "title": "BOOKCOREF: Coreference Resolution at Book Scale", "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "AI": {"tldr": "论文提出了一个自动标注管道，创建了首个书籍规模的共指消解基准BOOKCOREF，并展示了其在提升长文档共指消解系统性能上的价值。", "motivation": "现有共指消解基准主要针对中小规模文档，缺乏对长文本（如书籍规模）的评估能力，限制了系统在长文档中的表现评估。", "method": "提出了一种自动标注管道，用于生成高质量的全叙事文本共指消解标注，并基于此创建了BOOKCOREF基准。", "result": "实验表明，该自动标注方法稳健，BOOKCOREF基准使当前长文档共指消解系统性能提升高达+20 CoNLL-F1分。", "conclusion": "BOOKCOREF填补了书籍规模共指消解评估的空白，揭示了当前模型在长文档中的性能不足，并鼓励未来研究改进。"}}
{"id": "2507.11994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11994", "abs": "https://arxiv.org/abs/2507.11994", "authors": ["Jun Yin", "Fei Wu", "Yupeng Ren", "Jisheng Huang", "Qiankun Li", "Heng jin", "Jianhai Fu", "Chanjie Cui"], "title": "SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation", "comment": "IGARSS2025 accepted, Correspondence: fujianhai2024@gmail.com (J.F.),\n  cuichj@mail2.sysu.edu.cn (C.C.)", "summary": "Public remote sensing datasets often face limitations in universality due to\nresolution variability and inconsistent land cover category definitions. To\nharness the vast pool of unlabeled remote sensing data, we propose SAMST, a\nsemi-supervised semantic segmentation method. SAMST leverages the strengths of\nthe Segment Anything Model (SAM) in zero-shot generalization and boundary\ndetection. SAMST iteratively refines pseudo-labels through two main components:\nsupervised model self-training using both labeled and pseudo-labeled data, and\na SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three\nmodules: a Threshold Filter Module for preprocessing, a Prompt Generation\nModule for extracting connected regions and generating prompts for SAM, and a\nLabel Refinement Module for final label stitching. By integrating the\ngeneralization power of large models with the training efficiency of small\nmodels, SAMST improves pseudo-label accuracy, thereby enhancing overall model\nperformance. Experiments on the Potsdam dataset validate the effectiveness and\nfeasibility of SAMST, demonstrating its potential to address the challenges\nposed by limited labeled data in remote sensing semantic segmentation.", "AI": {"tldr": "SAMST是一种半监督语义分割方法，利用Segment Anything Model（SAM）的零样本泛化和边界检测能力，通过迭代优化伪标签，提升遥感数据语义分割的性能。", "motivation": "公共遥感数据集因分辨率差异和土地覆盖类别定义不一致而缺乏普适性，限制了其应用。SAMST旨在利用大量未标记遥感数据，解决标注数据不足的问题。", "method": "SAMST结合监督模型自训练和基于SAM的伪标签优化器，通过阈值过滤、提示生成和标签细化模块迭代优化伪标签。", "result": "在Potsdam数据集上的实验验证了SAMST的有效性和可行性，表明其能显著提升伪标签准确性，从而改善模型性能。", "conclusion": "SAMST通过结合大模型的泛化能力与小模型的训练效率，成功解决了遥感语义分割中标注数据不足的挑战。"}}
{"id": "2507.12107", "categories": ["cs.CV", "cs.AI", "cs.CR", "I.2.6; I.5.4; D.4.6; K.6.5; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.12107", "abs": "https://arxiv.org/abs/2507.12107", "authors": ["Sunpill Kim", "Seunghun Paik", "Chanwoo Hwang", "Minsu Kim", "Jae Hong Seo"], "title": "Non-Adaptive Adversarial Face Generation", "comment": null, "summary": "Adversarial attacks on face recognition systems (FRSs) pose serious security\nand privacy threats, especially when these systems are used for identity\nverification. In this paper, we propose a novel method for generating\nadversarial faces-synthetic facial images that are visually distinct yet\nrecognized as a target identity by the FRS. Unlike iterative optimization-based\napproaches (e.g., gradient descent or other iterative solvers), our method\nleverages the structural characteristics of the FRS feature space. We figure\nout that individuals sharing the same attribute (e.g., gender or race) form an\nattributed subsphere. By utilizing such subspheres, our method achieves both\nnon-adaptiveness and a remarkably small number of queries. This eliminates the\nneed for relying on transferability and open-source surrogate models, which\nhave been a typical strategy when repeated adaptive queries to commercial FRSs\nare impossible. Despite requiring only a single non-adaptive query consisting\nof 100 face images, our method achieves a high success rate of over 93% against\nAWS's CompareFaces API at its default threshold. Furthermore, unlike many\nexisting attacks that perturb a given image, our method can deliberately\nproduce adversarial faces that impersonate the target identity while exhibiting\nhigh-level attributes chosen by the adversary.", "AI": {"tldr": "提出了一种基于特征空间结构特性的对抗性人脸生成方法，无需迭代优化，仅需少量查询即可高效攻击人脸识别系统。", "motivation": "对抗性攻击对人脸识别系统构成严重威胁，现有方法依赖迭代优化或迁移性，效率低且不适用于商业系统。", "method": "利用特征空间中共享属性的子球面结构，生成视觉不同但被识别为目标身份的对抗性人脸。", "result": "仅需100张人脸图像的非自适应查询，对AWS CompareFaces API的成功率超过93%。", "conclusion": "该方法高效且灵活，能生成具有特定属性的对抗性人脸，适用于商业系统攻击。"}}
{"id": "2507.12414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12414", "abs": "https://arxiv.org/abs/2507.12414", "authors": ["Santosh Vasa", "Aditi Ramadwar", "Jnana Rama Krishna Darabattula", "Md Zafar Anwar", "Stanislaw Antol", "Andrei Vatavu", "Thomas Monninger", "Sihao Ding"], "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models", "comment": null, "summary": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.", "AI": {"tldr": "AutoVDC框架利用视觉语言模型自动检测视觉数据集中的错误标注，提升数据质量，验证显示其高效性。", "motivation": "人工标注存在缺陷且成本高，需自动化方法提升数据集质量。", "method": "利用Vision-Language Models (VLMs)自动识别错误标注，并在KITTI和nuImages数据集上验证。", "result": "AutoVDC在错误检测和数据清理实验中表现优异，显著提升数据集可靠性。", "conclusion": "AutoVDC能高效提升自动驾驶大规模数据集的质量和准确性。"}}
{"id": "2507.12079", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12079", "abs": "https://arxiv.org/abs/2507.12079", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "AI": {"tldr": "研究比较了MEGA方法（结合苏格拉底法、CoT推理、简化游戏化和形成性反馈）与传统CoT方法在数学学习中的效果，发现MEGA方法更受学生认可，尤其在解决难题时表现更优。", "motivation": "许多学生在数学学习中遇到困难，导致他们回避数学相关学科。研究旨在探索更有效的教学方法，以提升学生的数学学习体验。", "method": "采用组内设计，随机分配问题给大学生参与者，比较MEGA方法和传统CoT方法的效果。使用GSM8K和MATH数据集样本评估两种LLM（GPT4o和Claude 3.5 Sonnet）。", "result": "MEGA方法在两种数据集上均被学生认为更有利于学习，尤其在MATH数据集中表现显著优于CoT方法（47.5% vs 26.67%）。", "conclusion": "MEGA方法在解释复杂数学问题时效果更佳，为数学教学提供了新的有效工具。"}}
{"id": "2507.12001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12001", "abs": "https://arxiv.org/abs/2507.12001", "authors": ["Hao Li", "Ju Dai", "Feng Zhou", "Kaida Ning", "Lei Li", "Junjun Pan"], "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation", "comment": "ICCV 2025", "summary": "While 3D facial animation has made impressive progress, challenges still\nexist in realizing fine-grained stylized 3D facial expression manipulation due\nto the lack of appropriate datasets. In this paper, we introduce the\nAUBlendSet, a 3D facial dataset based on AU-Blendshape representation for\nfine-grained facial expression manipulation across identities. AUBlendSet is a\nblendshape data collection based on 32 standard facial action units (AUs)\nacross 500 identities, along with an additional set of facial postures\nannotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to\nlearn AU-Blendshape basis vectors for different character styles. AUBlendNet\npredicts, in parallel, the AU-Blendshape basis vectors of the corresponding\nstyle for a given identity mesh, thereby achieving stylized 3D emotional facial\nmanipulation. We comprehensively validate the effectiveness of AUBlendSet and\nAUBlendNet through tasks such as stylized facial expression manipulation,\nspeech-driven emotional facial animation, and emotion recognition data\naugmentation. Through a series of qualitative and quantitative experiments, we\ndemonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D\nfacial animation tasks. To the best of our knowledge, AUBlendSet is the first\ndataset, and AUBlendNet is the first network for continuous 3D facial\nexpression manipulation for any identity through facial AUs. Our source code is\navailable at https://github.com/wslh852/AUBlendNet.git.", "AI": {"tldr": "论文提出了AUBlendSet数据集和AUBlendNet网络，用于基于面部动作单元（AUs）的细粒度3D面部表情操纵。", "motivation": "现有3D面部动画在实现细粒度风格化表情操纵方面存在挑战，主要由于缺乏合适的数据集。", "method": "基于32个标准AUs和500个身份的数据集AUBlendSet，提出AUBlendNet网络，学习不同风格的AU-Blendshape基向量。", "result": "通过实验验证了AUBlendSet和AUBlendNet在风格化表情操纵、语音驱动动画和情感识别数据增强中的有效性。", "conclusion": "AUBlendSet和AUBlendNet在3D面部动画任务中具有重要潜力，是首个基于AUs的连续表情操纵数据集和网络。"}}
{"id": "2507.12188", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12188", "abs": "https://arxiv.org/abs/2507.12188", "authors": ["Shuangli Du", "Siming Yan", "Zhenghao Shi", "Zhenzhen You", "Lu Sun"], "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement", "comment": null, "summary": "Low-light images suffer from complex degradation, and existing enhancement\nmethods often encode all degradation factors within a single latent space. This\nleads to highly entangled features and strong black-box characteristics, making\nthe model prone to shortcut learning. To mitigate the above issues, this paper\nproposes a wavelet-based low-light stereo image enhancement method with feature\nspace decoupling. Our insight comes from the following findings: (1) Wavelet\ntransform enables the independent processing of low-frequency and\nhigh-frequency information. (2) Illumination adjustment can be achieved by\nadjusting the low-frequency component of a low-light image, extracted through\nmulti-level wavelet decomposition. Thus, by using wavelet transform the feature\nspace is decomposed into a low-frequency branch for illumination adjustment and\nmultiple high-frequency branches for texture enhancement. Additionally, stereo\nlow-light image enhancement can extract useful cues from another view to\nimprove enhancement. To this end, we propose a novel high-frequency guided\ncross-view interaction module (HF-CIM) that operates within high-frequency\nbranches rather than across the entire feature space, effectively extracting\nvaluable image details from the other view. Furthermore, to enhance the\nhigh-frequency information, a detail and texture enhancement module (DTEM) is\nproposed based on cross-attention mechanism. The model is trained on a dataset\nconsisting of images with uniform illumination and images with non-uniform\nillumination. Experimental results on both real and synthetic images indicate\nthat our algorithm offers significant advantages in light adjustment while\neffectively recovering high-frequency information. The code and dataset are\npublicly available at: https://github.com/Cherisherr/WDCI-Net.git.", "AI": {"tldr": "提出了一种基于小波变换的低光立体图像增强方法，通过特征空间解耦解决现有方法中特征高度纠缠的问题。", "motivation": "解决低光图像增强中特征空间高度纠缠和黑盒特性导致的捷径学习问题。", "method": "利用小波变换将特征空间分解为低频分支（用于光照调整）和高频分支（用于纹理增强），并设计了高频引导的跨视图交互模块（HF-CIM）和细节纹理增强模块（DTEM）。", "result": "在真实和合成图像上的实验表明，该方法在光照调整和高频信息恢复方面具有显著优势。", "conclusion": "该方法通过小波变换和特征空间解耦，有效提升了低光图像增强的性能。"}}
{"id": "2507.12126", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12126", "abs": "https://arxiv.org/abs/2507.12126", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "comment": null, "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "AI": {"tldr": "本文提出了一种评估大型语言模型（LLM）文本增强的框架，包括可扩展性分析和迭代增强与摘要细化（IASR），验证了其在语义保真度和多样性上的优势。", "motivation": "解决现有文本增强技术在语义保存方面的不足，特别是在大规模或迭代生成时导致的冗余和不稳定性。", "method": "提出两个评估组件：可扩展性分析（衡量语义一致性）和IASR（评估递归释义中的语义漂移）。", "result": "GPT-3.5 Turbo在语义保真度、多样性和生成效率上表现最佳；在BERTopic任务中，主题粒度提升400%且完全消除重叠。", "conclusion": "验证了所提框架在NLP实际应用中对LLM增强的结构化评估价值。"}}
{"id": "2507.12009", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12009", "abs": "https://arxiv.org/abs/2507.12009", "authors": ["Florian David", "Michael Chan", "Elenor Morgenroth", "Patrik Vuilleumier", "Dimitri Van De Ville"], "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli", "comment": "Accepted in International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC) 2025", "summary": "We propose an end-to-end deep neural encoder-decoder model to encode and\ndecode brain activity in response to naturalistic stimuli using functional\nmagnetic resonance imaging (fMRI) data. Leveraging temporally correlated input\nfrom consecutive film frames, we employ temporal convolutional layers in our\narchitecture, which effectively allows to bridge the temporal resolution gap\nbetween natural movie stimuli and fMRI acquisitions. Our model predicts\nactivity of voxels in and around the visual cortex and performs reconstruction\nof corresponding visual inputs from neural activity. Finally, we investigate\nbrain regions contributing to visual decoding through saliency maps. We find\nthat the most contributing regions are the middle occipital area, the fusiform\narea, and the calcarine, respectively employed in shape perception, complex\nrecognition (in particular face perception), and basic visual features such as\nedges and contrasts. These functions being strongly solicited are in line with\nthe decoder's capability to reconstruct edges, faces, and contrasts. All in\nall, this suggests the possibility to probe our understanding of visual\nprocessing in films using as a proxy the behaviour of deep learning models such\nas the one proposed in this paper.", "AI": {"tldr": "提出了一种端到端的深度神经编码-解码模型，用于编码和解码自然刺激下的大脑活动，通过fMRI数据预测视觉皮层活动并重建视觉输入。", "motivation": "研究如何利用深度学习模型理解自然电影刺激下的大脑活动，并探索视觉解码的关键脑区。", "method": "采用时间卷积层处理连续电影帧的输入，以弥补自然电影刺激与fMRI采集之间的时间分辨率差距。", "result": "模型成功预测了视觉皮层及其周围体素的活动，并重建了对应的视觉输入。关键贡献区域包括中枕区、梭状回和距状沟。", "conclusion": "研究表明深度学习模型可以作为理解视觉处理的工具，尤其是在边缘、面部和对比度等特征的重建中。"}}
{"id": "2507.12195", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12195", "abs": "https://arxiv.org/abs/2507.12195", "authors": ["Arkaprabha Basu"], "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "comment": null, "summary": "Modern digitised approaches have dramatically changed the preservation and\nrestoration of cultural treasures, integrating computer scientists into\nmultidisciplinary projects with ease. Machine learning, deep learning, and\ncomputer vision techniques have revolutionised developing sectors like 3D\nreconstruction, picture inpainting,IoT-based methods, genetic algorithms, and\nimage processing with the integration of computer scientists into\nmultidisciplinary initiatives. We suggest three cutting-edge techniques in\nrecognition of the special qualities of Indian monuments, which are famous for\ntheir architectural skill and aesthetic appeal. First is the Fractal\nConvolution methodology, a segmentation method based on image processing that\nsuccessfully reveals subtle architectural patterns within these irreplaceable\ncultural buildings. The second is a revolutionary Self-Sensitive Tile Filling\n(SSTF) method created especially for West Bengal's mesmerising Bankura\nTerracotta Temples with a brand-new data augmentation method called MosaicSlice\non the third. Furthermore, we delve deeper into the Super Resolution strategy\nto upscale the images without losing significant amount of quality. Our methods\nallow for the development of seamless region-filling and highly detailed tiles\nwhile maintaining authenticity using a novel data augmentation strategy within\naffordable costs introducing automation. By providing effective solutions that\npreserve the delicate balance between tradition and innovation, this study\nimproves the subject and eventually ensures unrivalled efficiency and aesthetic\nexcellence in cultural heritage protection. The suggested approaches advance\nthe field into an era of unmatched efficiency and aesthetic quality while\ncarefully upholding the delicate equilibrium between tradition and innovation.", "AI": {"tldr": "论文提出了三种创新方法（Fractal Convolution、SSTF和Super Resolution）用于印度文化遗产的保护与修复，结合计算机视觉和机器学习技术，实现了高效且经济实惠的自动化解决方案。", "motivation": "现代数字化方法为文化遗产保护带来了革命性变化，但印度古迹因其独特的建筑风格和美学价值需要专门的技术支持。本文旨在开发针对性的方法，平衡传统与创新。", "method": "1. Fractal Convolution：基于图像处理的细分方法，揭示建筑细节。2. SSTF：专为Bankura陶庙设计的自敏感瓷砖填充方法，结合MosaicSlice数据增强。3. Super Resolution：提升图像分辨率而不损失质量。", "result": "提出的方法实现了无缝区域填充和高细节瓷砖生成，同时保持真实性和低成本自动化。", "conclusion": "研究为文化遗产保护提供了高效且美观的解决方案，推动了传统与创新的平衡发展。"}}
{"id": "2507.12143", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12143", "abs": "https://arxiv.org/abs/2507.12143", "authors": ["Pavel Šindelář", "Ondřej Bojar"], "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "AI": {"tldr": "ELOQUENT的Sensemaking任务通过三步评估生成模型的文本理解能力：问题生成、回答和评分，发现现有方法在问题质量评估和答案限制上存在问题。", "motivation": "为生成语言模型提供可测试的高层次评估标准，特别是在文本理解能力方面。", "method": "采用三步评估框架（问题生成、回答、评分），使用多种语言和材料来源，结合自动和手动评估。", "result": "发现现有方法在问题质量评估和答案限制上存在问题，LLM在回答任务中表现尚可但受限。", "conclusion": "需要改进问题生成和答案限制的评估策略，LLM-as-a-Judge范式存在误判问题。"}}
{"id": "2507.12022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12022", "abs": "https://arxiv.org/abs/2507.12022", "authors": ["Yuechen Xie", "Jie Song", "Yicheng Shan", "Xiaoyan Zhang", "Yuanyu Wan", "Shengxuming Zhang", "Jiarui Duan", "Mingli Song"], "title": "Dataset Ownership Verification for Pre-trained Masked Models", "comment": "Accepted by ICCV 2025", "summary": "High-quality open-source datasets have emerged as a pivotal catalyst driving\nthe swift advancement of deep learning, while facing the looming threat of\npotential exploitation. Protecting these datasets is of paramount importance\nfor the interests of their owners. The verification of dataset ownership has\nevolved into a crucial approach in this domain; however, existing verification\ntechniques are predominantly tailored to supervised models and contrastive\npre-trained models, rendering them ill-suited for direct application to the\nincreasingly prevalent masked models. In this work, we introduce the inaugural\nmethodology addressing this critical, yet unresolved challenge, termed Dataset\nOwnership Verification for Masked Modeling (DOV4MM). The central objective is\nto ascertain whether a suspicious black-box model has been pre-trained on a\nparticular unlabeled dataset, thereby assisting dataset owners in safeguarding\ntheir rights. DOV4MM is grounded in our empirical observation that when a model\nis pre-trained on the target dataset, the difficulty of reconstructing masked\ninformation within the embedding space exhibits a marked contrast to models not\npre-trained on that dataset. We validated the efficacy of DOV4MM through ten\nmasked image models on ImageNet-1K and four masked language models on\nWikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,\nwith a $p$-value considerably below 0.05, surpassing all prior approaches. Code\nis available at https://github.com/xieyc99/DOV4MM.", "AI": {"tldr": "提出了一种名为DOV4MM的方法，用于验证掩码模型是否使用了特定未标注数据集进行预训练，以保护数据集所有者的权益。", "motivation": "高质量开源数据集对深度学习发展至关重要，但面临被滥用的风险，现有验证技术无法直接适用于掩码模型。", "method": "基于掩码信息在嵌入空间中的重构难度差异，设计DOV4MM方法验证模型是否预训练于目标数据集。", "result": "在ImageNet-1K和WikiText-103上的实验表明，DOV4MM显著优于现有方法，p值远低于0.05。", "conclusion": "DOV4MM为掩码模型的数据集所有权验证提供了有效解决方案，填补了该领域的技术空白。"}}
{"id": "2507.12252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12252", "abs": "https://arxiv.org/abs/2507.12252", "authors": ["Shilin Zhou", "Zhenghua Li"], "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "comment": null, "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "AI": {"tldr": "提出了一种多粒度融合方法，结合了词级和短语级融合的优势，提升了ASR模型在关键词识别上的性能。", "motivation": "现有的端到端ASR模型在一般语音转录上表现良好，但在识别上下文相关关键词（如专有名词或用户特定实体）时表现不佳。", "method": "提出了一种多粒度融合方法，结合了词级和短语级融合的优势，并采用后融合策略将ASR的声学信息与LLM的上下文知识结合。", "result": "在中英文数据集上，该方法在关键词相关指标上达到了最先进的性能，同时保持了非关键词文本的高准确性。", "conclusion": "多粒度融合框架中的词级和短语级组件相互补充，显著提升了性能。代码和模型将公开。"}}
{"id": "2507.12208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12208", "abs": "https://arxiv.org/abs/2507.12208", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "comment": null, "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "AI": {"tldr": "论文提出了行为翻译风格空间（BTSS），用于描述翻译行为模式，并通过分析击键和注视数据揭示隐藏的认知结构。", "motivation": "研究翻译行为背后的认知和情感过程，以更好地理解翻译的动态机制。", "method": "通过记录击键和注视数据，构建多层嵌入的BTSS，作为计算翻译代理的基础。", "result": "BTSS能够模拟翻译过程中的情感、自动化行为和认知的时间动态。", "conclusion": "BTSS为研究翻译行为提供了新视角，并为计算翻译代理的开发奠定了基础。"}}
{"id": "2507.12023", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12023", "abs": "https://arxiv.org/abs/2507.12023", "authors": ["Xu Fan", "Zhihao Wang", "Yuetan Lin", "Yan Zhang", "Yang Xiang", "Hao Li"], "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model", "comment": null, "summary": "Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.", "AI": {"tldr": "MVAR模型通过多变量自回归方法提升空气污染物预测精度，减少对长时窗输入的依赖，并设计新训练范式支持120小时长期预测。", "motivation": "现有研究多关注单一污染物预测，忽略了污染物间的相互作用及其空间响应多样性，需解决多变量预测的实际需求。", "method": "提出MVAR模型，结合多变量自回归训练范式及气象耦合空间变换模块，利用AI气象预报数据。", "result": "实验表明MVAR优于现有方法，验证了其架构有效性。", "conclusion": "MVAR为多变量空气污染物预测提供了高效解决方案，并构建了标准化数据集。"}}
{"id": "2507.12261", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12261", "abs": "https://arxiv.org/abs/2507.12261", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "AI": {"tldr": "Infherno是一个基于LLM代理、代码执行和医疗术语数据库工具的端到端框架，旨在将自由形式的临床笔记转换为结构化FHIR资源，解决现有方法的泛化性和结构一致性问题。", "motivation": "现有方法（如模块化规则系统或指令调优的LLM）在泛化性和结构一致性上表现不佳，因此需要一种更高效的解决方案。", "method": "采用LLM代理、代码执行和医疗术语数据库工具的端到端框架，确保符合FHIR文档模式。", "result": "Infherno在从非结构化文本预测FHIR资源方面表现优异，接近人类基准水平。", "conclusion": "Infherno支持临床数据集成和跨机构互操作性，具有实际应用潜力。"}}
{"id": "2507.12217", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12217", "abs": "https://arxiv.org/abs/2507.12217", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "title": "Towards few-shot isolated word reading assessment", "comment": "Accepted to SLaTE 2025", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "AI": {"tldr": "探索了一种在低资源环境下无需ASR的孤立单词阅读评估方法，通过对比儿童语音与少量成人参考模板，使用自监督学习模型的中间层编码。", "motivation": "解决低资源环境下儿童语音评估的挑战，利用自监督学习模型的潜力。", "method": "采用few-shot方法，将输入儿童语音与成人参考模板比较，使用SSL模型的中间层编码，并探索离散化SSL特征和模板的重心平均。", "result": "在理想化实验中，成人语音表现良好，但儿童语音输入效果显著下降，即使使用儿童模板。", "conclusion": "尽管SSL表示在低资源语音任务中表现良好，但在few-shot分类系统中处理儿童数据时存在局限性。"}}
{"id": "2507.12026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12026", "abs": "https://arxiv.org/abs/2507.12026", "authors": ["Rongtao Xu", "Han Gao", "Mingming Yu", "Dong An", "Shunpeng Chen", "Changwei Wang", "Li Guo", "Xiaodan Liang", "Shibiao Xu"], "title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering", "comment": "Accepted by IROS 2025", "summary": "With the growing need for diverse and scalable data in indoor scene tasks,\nsuch as question answering and dense captioning, we propose 3D-MoRe, a novel\nparadigm designed to generate large-scale 3D-language datasets by leveraging\nthe strengths of foundational models. The framework integrates key components,\nincluding multi-modal embedding, cross-modal interaction, and a language model\ndecoder, to process natural language instructions and 3D scene data. This\napproach facilitates enhanced reasoning and response generation in complex 3D\nenvironments. Using the ScanNet 3D scene dataset, along with text annotations\nfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs\nand 73,000 object descriptions across 1,513 scenes. We also employ various data\naugmentation techniques and implement semantic filtering to ensure high-quality\ndata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms\nstate-of-the-art baselines, with the CIDEr score improving by 2.15\\%.\nSimilarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5\nby 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated\ndatasets will be publicly released to benefit the community, and both can be\naccessed on the https://3D-MoRe.github.io.", "AI": {"tldr": "3D-MoRe是一种新范式，利用基础模型生成大规模3D-语言数据集，显著提升了3D场景任务（如问答和密集标注）的性能。", "motivation": "解决室内场景任务中多样化、可扩展数据的需求。", "method": "整合多模态嵌入、跨模态交互和语言模型解码器，处理自然语言指令和3D场景数据。", "result": "在ScanQA和ScanRefer任务中，CIDEr分数分别提升2.15%和1.84%。", "conclusion": "3D-MoRe在生成高质量数据和支持复杂3D环境任务方面表现出色，代码和数据集将公开。"}}
{"id": "2507.12269", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12269", "abs": "https://arxiv.org/abs/2507.12269", "authors": ["Sybelle Goedicke-Fritz", "Michelle Bous", "Annika Engel", "Matthias Flotho", "Pascal Hirsch", "Hannah Wittig", "Dino Milanovic", "Dominik Mohr", "Mathias Kaspar", "Sogand Nemat", "Dorothea Kerner", "Arno Bücker", "Andreas Keller", "Sascha Meyer", "Michael Zemlin", "Philipp Flotho"], "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants", "comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship", "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.", "AI": {"tldr": "本文提出了一种基于深度学习的模型，利用出生24小时内的胸部X光片预测极低出生体重婴儿的支气管肺发育不良（BPD）结局。", "motivation": "BPD是一种常见的慢性肺病，预防干预措施可能带来严重风险，因此早期预测BPD结局至关重要。", "method": "使用ResNet-50模型，结合渐进层冻结、CutMix增强和线性探测技术，对163名极低出生体重婴儿的胸部X光片进行分析。", "result": "最佳模型在预测中重度BPD时的AUROC为0.78，平衡准确率为0.69，F1分数为0.67。", "conclusion": "研究表明，领域特定的预训练结合渐进冻结和线性探测技术，能够高效且准确地预测BPD结局。"}}
{"id": "2507.12260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12260", "abs": "https://arxiv.org/abs/2507.12260", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "AI": {"tldr": "提出了一种名为T-index的定量测量方法，用于评估翻译文本的翻译特性（translationese），并通过对比微调的语言模型计算。实验表明T-index在跨域设置中具有普适性，且与人类判断高度相关。", "motivation": "现有机器翻译质量评估指标（如BLEU和COMET）未能涵盖翻译特性（translationese），因此需要一种新的定量测量方法。", "method": "使用两个对比微调的语言模型（0.5B参数）计算T-index，并在合成数据集和真实翻译数据集上进行评估。", "result": "T-index能够有效捕捉翻译特性，与人类标注的翻译特性高度相关（Pearson's r = 0.568），且与现有MT质量评估指标相关性低。", "conclusion": "T-index是一种稳健且高效的翻译特性测量方法，可作为现有MT质量评估指标的补充。"}}
{"id": "2507.12049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12049", "abs": "https://arxiv.org/abs/2507.12049", "authors": ["Manuel Barusco", "Francesco Borsatti", "Arianna Stropeni", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "MoViAD: Modular Visual Anomaly Detection", "comment": null, "summary": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.", "AI": {"tldr": "MoViAD是一个模块化库，旨在加速视觉异常检测（VAD）的研究和部署，提供多种场景支持和实用工具。", "motivation": "解决VAD领域异常数据稀缺和无监督训练的挑战，促进研究和实际应用。", "method": "开发MoViAD库，集成先进模型、训练器、数据集和实用工具，支持多种场景和部署需求。", "result": "MoViAD提供高效、灵活的解决方案，支持快速部署和扩展研究。", "conclusion": "MoViAD为VAD领域的研究和部署提供了强大且易用的工具。"}}
{"id": "2507.12295", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "AI": {"tldr": "该论文提出了一个文本异常检测的基准测试，通过多种预训练语言模型的嵌入和多领域数据集，系统评估了嵌入质量对异常检测效果的影响，并开源了工具包。", "motivation": "现有文本异常检测方法缺乏标准化和全面的基准测试，限制了方法的比较和创新。", "method": "利用多种预训练语言模型的嵌入和多领域数据集，结合多种评估指标（如AUROC、AUPRC），系统评估嵌入质量对异常检测的影响。", "result": "研究发现嵌入质量对异常检测效果至关重要，且深度学习模型在LLM嵌入下并未优于传统浅层算法（如KNN、Isolation Forest）。", "conclusion": "该工作为未来文本异常检测研究提供了基准和工具，支持高效模型评估和选择。"}}
{"id": "2507.12308", "categories": ["cs.CL", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.12308", "abs": "https://arxiv.org/abs/2507.12308", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "论文评估了现有代码大语言模型（LLMs）在VHDL代码生成和摘要任务中的表现，发现其性能不足，并提出了一种名为Chain-of-Descriptions（CoDes）的新方法以提升性能。", "motivation": "尽管LLMs在通用代码任务中表现优异，但在硬件描述语言（如VHDL）领域的研究较少，存在性能不足的问题。", "method": "提出CoDes方法，通过生成中间描述步骤并结合原始输入提示来改进LLMs的输出。", "result": "实验表明，CoDes方法在VHDL代码生成和摘要任务中显著优于标准提示策略。", "conclusion": "CoDes不仅提升了VHDL任务的质量，还为未来改进代码LLMs提供了框架。"}}
{"id": "2507.12062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12062", "abs": "https://arxiv.org/abs/2507.12062", "authors": ["Hongxu Ma", "Guanshuo Wang", "Fufu Yu", "Qiong Jia", "Shouhong Ding"], "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning", "comment": "Accepted by ACM MM'25", "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint\nspecific moments and assess clip-wise relevance based on the text query. While\nDETR-based joint frameworks have made significant strides, there remains\nuntapped potential in harnessing the intricate relationships between temporal\nmotion and spatial semantics within video content. In this paper, we propose\nthe Motion-Semantics DETR (MS-DETR), a framework that captures rich\nmotion-semantics features through unified learning for MR/HD tasks. The encoder\nfirst explicitly models disentangled intra-modal correlations within motion and\nsemantics dimensions, guided by the given text queries. Subsequently, the\ndecoder utilizes the task-wise correlation across temporal motion and spatial\nsemantics dimensions to enable precise query-guided localization for MR and\nrefined highlight boundary delineation for HD. Furthermore, we observe the\ninherent sparsity dilemma within the motion and semantics dimensions of MR/HD\ndatasets. To address this issue, we enrich the corpus from both dimensions by\ngeneration strategies and propose contrastive denoising learning to ensure the\nabove components learn robustly and effectively. Extensive experiments on four\nMR/HD benchmarks demonstrate that our method outperforms existing\nstate-of-the-art models by a margin. Our code is available at\nhttps://github.com/snailma0229/MS-DETR.git.", "AI": {"tldr": "MS-DETR框架通过统一学习捕捉视频中的运动-语义特征，提升视频时刻检索和高光检测任务性能。", "motivation": "现有DETR框架在利用视频中的运动与语义关系方面仍有潜力未开发。", "method": "提出MS-DETR框架，通过解耦模态内相关性和任务间相关性，结合生成策略和对比去噪学习。", "result": "在四个基准测试中超越现有最优模型。", "conclusion": "MS-DETR通过运动-语义特征的统一学习，显著提升了任务性能。"}}
{"id": "2507.12318", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12318", "abs": "https://arxiv.org/abs/2507.12318", "authors": ["Samuel Lavoie", "Michael Noukhovitch", "Aaron Courville"], "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "comment": "In submission, 22 pages, 7 tables, 12 figures", "summary": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.", "AI": {"tldr": "论文提出离散潜在码（DLC）作为扩散模型的输入条件表示，提升生成质量和组合性，并在无条件图像生成和文本到图像生成中取得新进展。", "motivation": "研究扩散模型成功的关键在于输入条件表示，理想表示应提升样本保真度、易于生成且具有组合性。", "method": "引入离散潜在码（DLC），一种基于自监督学习的图像表示，通过扩散模型训练提升生成质量。", "result": "DLC在ImageNet上实现无条件图像生成的新SOTA，并能生成超出训练分布的样本。", "conclusion": "DLC为扩散模型提供高效、组合性强的条件表示，拓展了生成能力。"}}
{"id": "2507.12356", "categories": ["cs.CL", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12356", "abs": "https://arxiv.org/abs/2507.12356", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "研究发现阿尔茨海默病（AD）语音感知中存在性别偏见，男性语音更易被识别为AD，尤其在中文语音中。声学分析显示男性语音的shimmer值与AD感知显著相关。", "motivation": "探讨性别偏见在AD语音感知中的影响，以改进AD检测模型的开发。", "method": "通过16名中文听众对中文和希腊语语音的感知实验，结合声学分析。", "result": "男性语音更频繁被识别为AD，shimmer值与AD感知显著相关，语言对AD感知无显著影响。", "conclusion": "性别偏见在AD语音感知中起关键作用，需在模型开发中加以解决，并进一步验证跨语言性能。"}}
{"id": "2507.12087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12087", "abs": "https://arxiv.org/abs/2507.12087", "authors": ["Xiang Yu", "Xinyao Liu", "Guang Liang"], "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association", "comment": null, "summary": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned\nAerial Vehicle (UAV) perspective is a highly challenging computer vision task.\nThe difficulty stems from three main sources: the extreme scarcity of target\nappearance features, the complex motion entanglement caused by the combined\ndynamics of the camera and the targets themselves, and the frequent occlusions\nand identity ambiguity arising from dense flocking behavior. This paper details\nour championship-winning solution in the MVA 2025 \"Finding Birds\" Small\nMulti-Object Tracking Challenge (SMOT4SB), which adopts the\ntracking-by-detection paradigm with targeted innovations at both the detection\nand association levels. On the detection side, we propose a systematic training\nenhancement framework named \\textbf{SliceTrain}. This framework, through the\nsynergy of 'deterministic full-coverage slicing' and 'slice-level stochastic\naugmentation, effectively addresses the problem of insufficient learning for\nsmall objects in high-resolution image training. On the tracking side, we\ndesigned a robust tracker that is completely independent of appearance\ninformation. By integrating a \\textbf{motion direction maintenance (EMA)}\nmechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding\nbox expansion and distance penalty} into the OC-SORT framework, our tracker can\nstably handle irregular motion and maintain target identities. Our method\nachieves state-of-the-art performance on the SMOT4SB public test set, reaching\nan SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness\nand advancement of our framework in solving complex real-world SMOT problems.\nThe source code will be made available at\nhttps://github.com/Salvatore-Love/YOLOv8-SMOT.", "AI": {"tldr": "本文提出了一种针对无人机视角下小型敏捷多目标（如鸟类）跟踪的冠军解决方案，通过创新的检测和关联方法解决了特征稀缺、运动复杂和遮挡问题。", "motivation": "解决无人机视角下小型多目标跟踪的三大挑战：目标外观特征稀缺、相机与目标运动复杂纠缠、以及密集群体行为导致的遮挡和身份模糊。", "method": "采用跟踪-检测范式，提出SliceTrain训练增强框架优化小目标检测，并设计独立于外观的跟踪器，结合运动方向维护和自适应相似性度量。", "result": "在SMOT4SB公开测试集上达到SO-HOTA分数55.205，验证了方法的有效性和先进性。", "conclusion": "该方法在复杂现实SMOT问题中表现出色，代码将开源。"}}
{"id": "2507.12359", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12359", "abs": "https://arxiv.org/abs/2507.12359", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "title": "Cluster Contrast for Unsupervised Visual Representation Learning", "comment": "ICIP 2025", "summary": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised\nvisual representation learning that effectively combines the strengths of\ncontrastive learning and clustering methods. Inspired by recent advancements,\nCueCo is designed to simultaneously scatter and align feature representations\nwithin the feature space. This method utilizes two neural networks, a query and\na key, where the key network is updated through a slow-moving average of the\nquery outputs. CueCo employs a contrastive loss to push dissimilar features\napart, enhancing inter-class separation, and a clustering objective to pull\ntogether features of the same cluster, promoting intra-class compactness. Our\nmethod achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on\nCIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18\nbackbone. By integrating contrastive learning with clustering, CueCo sets a new\ndirection for advancing unsupervised visual representation learning.", "AI": {"tldr": "Cluster Contrast (CueCo) 是一种结合对比学习和聚类方法的无监督视觉表示学习方法，通过同时分散和对齐特征表示，显著提升了分类准确率。", "motivation": "现有的无监督学习方法在特征表示上存在不足，CueCo旨在通过结合对比学习和聚类方法，提升特征的类间分离性和类内紧凑性。", "method": "CueCo使用两个神经网络（查询网络和关键网络），通过对比损失增强类间分离，聚类目标提升类内紧凑性，关键网络通过查询输出的慢速平均更新。", "result": "在CIFAR-10、CIFAR-100和ImageNet-100上分别达到91.40%、68.56%和78.65%的top-1分类准确率。", "conclusion": "CueCo通过结合对比学习和聚类，为无监督视觉表示学习开辟了新方向。"}}
{"id": "2507.12370", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12370", "abs": "https://arxiv.org/abs/2507.12370", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "论文提出了一种多智能体辩论框架，用于提升大型语言模型（LLMs）在模糊用户请求中的检测和解决能力。", "motivation": "LLMs在处理模糊用户请求时存在挑战，需要一种方法来增强其能力。", "method": "采用三种LLM架构（Llama3-8B、Gemma2-9B和Mistral-7B变体）和一个包含多样模糊性的数据集，设计多智能体辩论框架。", "result": "辩论框架显著提升了Llama3-8B和Mistral-7B的性能，Mistral-7B主导的辩论成功率达到76.7%，尤其在复杂模糊性和高效共识方面表现突出。", "conclusion": "辩论框架是增强LLM能力的有效方法，为开发更鲁棒和自适应的语言理解系统提供了重要见解。"}}
{"id": "2507.12092", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12092", "abs": "https://arxiv.org/abs/2507.12092", "authors": ["Nataliia Molchanova", "Alessandro Cagol", "Mario Ocampo-Pineda", "Po-Jui Lu", "Matthias Weigel", "Xinjie Chen", "Erin Beck", "Charidimos Tsagkas", "Daniel Reich", "Colin Vanden Bulcke", "Anna Stolting", "Serena Borrelli", "Pietro Maggi", "Adrien Depeursinge", "Cristina Granziera", "Henning Mueller", "Pedro M. Gordaliza", "Meritxell Bach Cuadra"], "title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis", "comment": null, "summary": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple\nsclerosis (MS), offering high diagnostic specificity and prognostic relevance.\nHowever, their routine clinical integration remains limited due to subtle\nmagnetic resonance imaging (MRI) appearance, challenges in expert annotation,\nand a lack of standardized automated methods. We propose a comprehensive\nmulti-centric benchmark of CL detection and segmentation in MRI. A total of 656\nMRI scans, including clinical trial and research data from four institutions,\nwere acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with\nexpert-consensus annotations. We rely on the self-configuring nnU-Net\nframework, designed for medical imaging segmentation, and propose adaptations\ntailored to the improved CL detection. We evaluated model generalization\nthrough out-of-distribution testing, demonstrating strong lesion detection\ncapabilities with an F1-score of 0.64 and 0.5 in and out of the domain,\nrespectively. We also analyze internal model features and model errors for a\nbetter understanding of AI decision-making. Our study examines how data\nvariability, lesion ambiguity, and protocol differences impact model\nperformance, offering future recommendations to address these barriers to\nclinical adoption. To reinforce the reproducibility, the implementation and\nmodels will be publicly accessible and ready to use at\nhttps://github.com/Medical-Image-Analysis-Laboratory/ and\nhttps://doi.org/10.5281/zenodo.15911797.", "AI": {"tldr": "该论文提出了一种基于nnU-Net框架的自动化方法，用于检测和分割多发性硬化症中的皮质病变，通过多中心MRI数据验证了模型的泛化能力，并公开了代码和模型以促进临床应用。", "motivation": "皮质病变在多发性硬化症中具有重要诊断和预后价值，但由于MRI表现不明显、专家标注困难以及缺乏标准化方法，其临床应用受限。", "method": "使用656份来自四个机构的3T和7T MRI扫描数据，基于nnU-Net框架进行改进，并通过域外测试评估模型泛化能力。", "result": "模型在域内和域外的F1分数分别为0.64和0.5，分析了数据变异性、病变模糊性和协议差异对性能的影响。", "conclusion": "研究为临床应用中数据标准化和模型改进提供了建议，并公开了代码和模型以支持可重复性。"}}
{"id": "2507.12379", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12379", "abs": "https://arxiv.org/abs/2507.12379", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "title": "Probing for Arithmetic Errors in Language Models", "comment": null, "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "AI": {"tldr": "研究探讨了语言模型内部激活是否能用于检测算术错误，通过简单探针解码隐藏状态，训练轻量级错误检测器，并扩展到复杂任务，最终实现选择性重新提示以提高准确性。", "motivation": "探索语言模型内部激活是否能有效检测算术错误，为模型自我修正提供轻量级解决方案。", "method": "从3位数加法控制实验开始，训练探针解码隐藏状态；扩展到GSM8K问题，训练错误检测器并选择性重新提示。", "result": "探针能准确解码模型预测和正确答案，错误检测器准确率超90%；选择性重新提示提高了任务准确性。", "conclusion": "算术错误可通过内部激活预测，简单探针为轻量级自我修正提供了可行路径。"}}
{"id": "2507.12372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12372", "abs": "https://arxiv.org/abs/2507.12372", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "comment": null, "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "AI": {"tldr": "论文探讨了具备网页浏览能力的大语言模型（LLMs）能否通过用户名推断社交媒体用户的 demographics，结果显示其具备一定准确性，但也存在偏见和滥用风险。", "motivation": "传统LLMs依赖静态数据，无法实时获取信息。研究旨在探索LLMs通过网页浏览能力直接获取和分析社交媒体数据的潜力。", "method": "使用合成数据集（48个Twitter账户）和调查数据集（1,384名国际参与者），评估LLMs通过用户名推断用户 demographics 的能力。", "result": "LLMs能够访问社交媒体内容并预测用户 demographics，但分析显示可能存在性别和政治偏见。", "conclusion": "该能力对计算社会科学有益，但也存在滥用风险，建议限制公开应用，仅保留受控研究访问。"}}
{"id": "2507.12095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12095", "abs": "https://arxiv.org/abs/2507.12095", "authors": ["Davide Di Nucci", "Matteo Tomei", "Guido Borghi", "Luca Ciuffreda", "Roberto Vezzani", "Rita Cucchiara"], "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images", "comment": null, "summary": "Accurate 3D reconstruction of vehicles is vital for applications such as\nvehicle inspection, predictive maintenance, and urban planning. Existing\nmethods like Neural Radiance Fields and Gaussian Splatting have shown\nimpressive results but remain limited by their reliance on dense input views,\nwhich hinders real-world applicability. This paper addresses the challenge of\nreconstructing vehicles from sparse-view inputs, leveraging depth maps and a\nrobust pose estimation architecture to synthesize novel views and augment\ntraining data. Specifically, we enhance Gaussian Splatting by integrating a\nselective photometric loss, applied only to high-confidence pixels, and\nreplacing standard Structure-from-Motion pipelines with the DUSt3R architecture\nto improve camera pose estimation. Furthermore, we present a novel dataset\nfeaturing both synthetic and real-world public transportation vehicles,\nenabling extensive evaluation of our approach. Experimental results demonstrate\nstate-of-the-art performance across multiple benchmarks, showcasing the\nmethod's ability to achieve high-quality reconstructions even under constrained\ninput conditions.", "AI": {"tldr": "本文提出了一种从稀疏视角输入重建车辆3D模型的方法，通过结合深度图和鲁棒的姿态估计架构，改进了高斯泼溅技术，并在多个基准测试中取得了最先进的性能。", "motivation": "准确的车辆3D重建对车辆检查、预测性维护和城市规划至关重要，但现有方法依赖密集输入视角，限制了实际应用。", "method": "通过选择性光度损失和DUSt3R架构改进高斯泼溅技术，结合深度图和姿态估计，从稀疏视角输入重建车辆。", "result": "实验结果表明，该方法在多个基准测试中表现优异，即使在输入受限条件下也能实现高质量重建。", "conclusion": "该方法显著提升了稀疏视角输入下的车辆3D重建质量，具有广泛的实际应用潜力。"}}
{"id": "2507.12416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12416", "abs": "https://arxiv.org/abs/2507.12416", "authors": ["Jaehyun Kwak", "Ramahdani Muhammad Izaaz Inhar", "Se-Young Yun", "Sung-Ju Lee"], "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval", "comment": "Accepted to ICML 2025", "summary": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference\nimage and accompanying text describing desired modifications. However, existing\nCIR methods only focus on retrieving the target image and disregard the\nrelevance of other images. This limitation arises because most methods\nemploying contrastive learning-which treats the target image as positive and\nall other images in the batch as negatives-can inadvertently include false\nnegatives. This may result in retrieving irrelevant images, reducing user\nsatisfaction even when the target image is retrieved. To address this issue, we\npropose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which\noptimizes a reward model objective to reduce false negatives. Additionally, we\nintroduce a hard negative sampling strategy that selects images positioned\nbetween two steep drops in relevance scores following the target image, to\neffectively filter false negatives. In order to evaluate CIR models on their\nalignment with human satisfaction, we create Human-Preference FashionIQ\n(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond\ntarget retrieval. Extensive experiments demonstrate that QuRe achieves\nstate-of-the-art performance on FashionIQ and CIRR datasets while exhibiting\nthe strongest alignment with human preferences on the HP-FashionIQ dataset. The\nsource code is available at https://github.com/jackwaky/QuRe.", "AI": {"tldr": "QuRe提出了一种通过硬负采样减少假负例的图像检索方法，提升了用户满意度。", "motivation": "现有CIR方法仅关注目标图像检索，忽视了其他图像的相关性，导致假负例问题。", "method": "采用奖励模型目标和硬负采样策略，选择相关图像以减少假负例。", "result": "在FashionIQ和CIRR数据集上达到最优性能，并与人类偏好高度一致。", "conclusion": "QuRe有效解决了假负例问题，提升了检索质量和用户满意度。"}}
{"id": "2507.12425", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.12425", "abs": "https://arxiv.org/abs/2507.12425", "authors": ["Chandana Cheerla"], "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "comment": null, "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "AI": {"tldr": "本文提出了一种改进的RAG框架，结合混合检索策略和元数据过滤，显著提升了企业数据处理的效果。", "motivation": "企业依赖专有数据进行决策，但现有LLMs和RAG框架在处理结构化数据时存在局限性。", "method": "采用混合检索策略（密集嵌入和BM25）、元数据过滤、语义分块和表格结构保留，优化检索效率。", "result": "实验显示Precision@5提升15%，Recall@5提升13%，Faithfulness等定性指标显著提高。", "conclusion": "框架有效提升了企业任务的准确性和相关性，未来将扩展至多模态数据和代理检索。"}}
{"id": "2507.12103", "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.12103", "abs": "https://arxiv.org/abs/2507.12103", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "comment": "7pages, 4 figures. Accepted to IJCAI 2025", "summary": "Heatwaves pose a significant threat to public health, especially as global\nwarming intensifies. However, current routing systems (e.g., online maps) fail\nto incorporate shade information due to the difficulty of estimating shades\ndirectly from noisy satellite imagery and the limited availability of training\ndata for generative models. In this paper, we address these challenges through\ntwo main contributions. First, we build an extensive dataset covering diverse\nlongitude-latitude regions, varying levels of building density, and different\nurban layouts. Leveraging Blender-based 3D simulations alongside building\noutlines, we capture building shadows under various solar zenith angles\nthroughout the year and at different times of day. These simulated shadows are\naligned with satellite images, providing a rich resource for learning shade\npatterns. Second, we propose the DeepShade, a diffusion-based model designed to\nlearn and synthesize shade variations over time. It emphasizes the nuance of\nedge features by jointly considering RGB with the Canny edge layer, and\nincorporates contrastive learning to capture the temporal change rules of\nshade. Then, by conditioning on textual descriptions of known conditions (e.g.,\ntime of day, solar angles), our framework provides improved performance in\ngenerating shade images. We demonstrate the utility of our approach by using\nour shade predictions to calculate shade ratios for real-world route planning\nin Tempe, Arizona. We believe this work will benefit society by providing a\nreference for urban planning in extreme heat weather and its potential\npractical applications in the environment.", "AI": {"tldr": "论文提出了一种名为DeepShade的扩散模型，用于学习和合成随时间变化的阴影，并结合对比学习捕捉阴影的时间变化规律。通过构建大规模数据集和模拟阴影，改进了阴影图像的生成性能，并应用于实际路线规划。", "motivation": "热浪对公共健康构成重大威胁，但现有路线系统因难以从卫星图像中直接估计阴影且缺乏训练数据，未能纳入阴影信息。", "method": "1. 构建包含多样地理区域和城市布局的数据集，利用Blender模拟建筑阴影；2. 提出DeepShade模型，结合RGB和Canny边缘层，通过对比学习捕捉阴影时间变化。", "result": "模型在生成阴影图像方面表现优异，并成功应用于亚利桑那州坦佩市的实际路线规划。", "conclusion": "该研究为极端高温天气下的城市规划提供了参考，并展示了潜在的实际应用价值。"}}
{"id": "2507.12428", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "AI": {"tldr": "研究发现，通过线性探针分析CoT激活可以更准确地预测最终回答的安全性，优于基于文本的方法。", "motivation": "探讨是否可以利用CoT预测最终回答的对齐风险，以减少有害内容的传播。", "method": "评估多种监控方法，包括人类、大语言模型和文本分类器，分析CoT文本或激活。", "result": "线性探针在CoT激活上的表现优于文本方法，且能在推理早期准确预测安全性。", "conclusion": "轻量级探针可实现实时安全监控和早期干预，适用于不同模型和任务。"}}
{"id": "2507.12451", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12451", "abs": "https://arxiv.org/abs/2507.12451", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "comment": "Accepted as a long paper for ACL 2025 main conference", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "AI": {"tldr": "S2WTM提出了一种新的主题建模方法，通过球面切片Wasserstein距离解决VAE-NTMs中的后验坍塌问题，生成更一致和多样化的主题。", "motivation": "现有的VAE-NTMs在建模高维文本数据的超球面潜在表示时，常因后验坍塌导致潜在表示无效。", "method": "S2WTM采用单位超球面上的先验分布，并利用球面切片Wasserstein距离对齐聚合后验分布与先验。", "result": "实验表明S2WTM优于现有主题模型，生成的主题更一致和多样化，下游任务表现更优。", "conclusion": "S2WTM有效解决了后验坍塌问题，提升了主题建模的性能。"}}
{"id": "2507.12105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12105", "abs": "https://arxiv.org/abs/2507.12105", "authors": ["Yiquan Gao", "Duohui Xu"], "title": "Out-of-distribution data supervision towards biomedical semantic segmentation", "comment": "This paper was published in Proceedings of SPIE Volume 13442 and is\n  reprinted with permission. The official version is available at\n  https://doi.org/10.1117/12.3052988. One personal copy is allowed.\n  Reproduction, distribution, or commercial use is prohibited", "summary": "Biomedical segmentation networks easily suffer from the unexpected\nmisclassification between foreground and background objects when learning on\nlimited and imperfect medical datasets. Inspired by the strong power of\nOut-of-Distribution (OoD) data on other visual tasks, we propose a data-centric\nframework, Med-OoD to address this issue by introducing OoD data supervision\ninto fully-supervised biomedical segmentation with none of the following needs:\n(i) external data sources, (ii) feature regularization objectives, (iii)\nadditional annotations. Our method can be seamlessly integrated into\nsegmentation networks without any modification on the architectures. Extensive\nexperiments show that Med-OoD largely prevents various segmentation networks\nfrom the pixel misclassification on medical images and achieves considerable\nperformance improvements on Lizard dataset. We also present an emerging\nlearning paradigm of training a medical segmentation network completely using\nOoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU\nas test result. We hope this learning paradigm will attract people to rethink\nthe roles of OoD data. Code is made available at\nhttps://github.com/StudioYG/Med-OoD.", "AI": {"tldr": "Med-OoD框架通过引入OoD数据监督，解决了医学图像分割中的像素误分类问题，无需外部数据、特征正则化或额外标注。", "motivation": "医学图像分割网络在有限和不完美的数据集上容易发生前景与背景的误分类，OoD数据在其他视觉任务中表现优异，因此探索其在医学分割中的应用。", "method": "提出Med-OoD框架，将OoD数据监督引入全监督医学分割，无需外部数据、特征正则化或额外标注，且无需修改网络架构。", "result": "实验表明，Med-OoD显著减少了像素误分类，并在Lizard数据集上取得显著性能提升。此外，仅用OoD数据训练的网络达到76.1% mIoU。", "conclusion": "Med-OoD为医学分割提供了新思路，展示了OoD数据的潜力，鼓励重新思考其在学习范式中的作用。"}}
{"id": "2507.12461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12461", "abs": "https://arxiv.org/abs/2507.12461", "authors": ["Trong-Thang Pham", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis", "comment": "ACM MM 2025", "summary": "Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.", "AI": {"tldr": "论文提出RadGazeIntent，一种基于深度学习的模型，用于捕捉放射科医生在解读医学图像时的意图驱动行为。", "motivation": "现有模型未能捕捉放射科医生在图像搜索中的意图，而理解其意图对提高诊断效率至关重要。", "method": "采用基于Transformer的架构，处理眼动数据的时空维度，将细粒度注视特征转化为诊断意图的粗粒度表示。", "result": "RadGazeIntent在三个意图标记数据集（RadSeq、RadExplore、RadHybrid）上均优于基线方法，能准确预测放射科医生的搜索目标。", "conclusion": "RadGazeIntent成功建模了放射科医生的意图驱动行为，为医学图像解读提供了新视角。"}}
{"id": "2507.12466", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12466", "abs": "https://arxiv.org/abs/2507.12466", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "AI": {"tldr": "论文提出了一种名为BETR的显式优化数据选择方法，通过将预训练文档与基准训练示例对齐，显著提升了模型性能和计算效率。", "motivation": "研究动机是探索将数据选择目标显式化（而非隐式通过基准迭代）的效果，以更精准地塑造模型能力。", "method": "方法包括嵌入基准示例和预训练文档到共享空间，基于相似性评分，并训练轻量级分类器预测全语料库的分数。", "result": "实验结果表明，BETR在10项任务中9项表现更优，计算效率提升2.1倍，且适用于不同规模的模型。", "conclusion": "结论是直接匹配预训练数据与目标任务能有效提升模型能力，且数据选择策略需随模型规模调整。"}}
{"id": "2507.12114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12114", "abs": "https://arxiv.org/abs/2507.12114", "authors": ["Yuzhou Ji", "Ke Ma", "Hong Cai", "Anchun Zhang", "Lizhuang Ma", "Xin Tan"], "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance", "comment": null, "summary": "Dynamic driving scene reconstruction is of great importance in fields like\ndigital twin system and autonomous driving simulation. However, unacceptable\ndegradation occurs when the view deviates from the input trajectory, leading to\ncorrupted background and vehicle models. To improve reconstruction quality on\nnovel trajectory, existing methods are subject to various limitations including\ninconsistency, deformation, and time consumption. This paper proposes\nLidarPainter, a one-step diffusion model that recovers consistent driving views\nfrom sparse LiDAR condition and artifact-corrupted renderings in real-time,\nenabling high-fidelity lane shifts in driving scene reconstruction. Extensive\nexperiments show that LidarPainter outperforms state-of-the-art methods in\nspeed, quality and resource efficiency, specifically 7 x faster than\nStreetCrafter with only one fifth of GPU memory required. LidarPainter also\nsupports stylized generation using text prompts such as \"foggy\" and \"night\",\nallowing for a diverse expansion of the existing asset library.", "AI": {"tldr": "LidarPainter是一种基于扩散模型的实时方法，用于从稀疏LiDAR数据和损坏的渲染中恢复一致的驾驶场景视图，支持高质量的车道变换和风格化生成。", "motivation": "动态驾驶场景重建在数字孪生系统和自动驾驶模拟中非常重要，但现有方法在偏离输入轨迹时会出现背景和车辆模型损坏的问题，且存在不一致、变形和耗时等限制。", "method": "提出LidarPainter，一种一步扩散模型，从稀疏LiDAR条件和损坏的渲染中实时恢复一致的驾驶视图。", "result": "实验表明，LidarPainter在速度、质量和资源效率上优于现有方法，比StreetCrafter快7倍，GPU内存仅需五分之一，并支持风格化生成。", "conclusion": "LidarPainter在驾驶场景重建中实现了高保真度和实时性，支持多样化的风格扩展。"}}
{"id": "2507.12123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12123", "abs": "https://arxiv.org/abs/2507.12123", "authors": ["Sergey Linok", "Gleb Naumov"], "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph", "comment": "13 pages, 5 figures, 2 tables", "summary": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects\nusing 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor\nenvironment over a Hierarchical Scene Graph derived from sequences of RGB-D\nframes utilizing a set of open-vocabulary foundation models and sensor data\nprocessing. The hierarchical representation explicitly models spatial relations\nacross floors, rooms, locations, and objects. To effectively address complex\nqueries involving spatial reference to other objects, we integrate the\nhierarchical scene graph with a Large Language Model for multistep reasoning.\nThis integration leverages inter-layer (e.g., room-to-object) and intra-layer\n(e.g., object-to-object) connections, enhancing spatial contextual\nunderstanding. We investigate the semantic and geometry accuracy of\nhierarchical representation on Habitat Matterport 3D Semantic multi-floor\nscenes. Our approach demonstrates efficient scene comprehension and robust\nobject grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates\nstrong potential for applications requiring spatial reasoning and understanding\nof indoor environments. Related materials can be found at\nhttps://github.com/linukc/OVIGo-3DHSG.", "AI": {"tldr": "OVIGo-3DHSG是一种基于3D分层场景图的开放词汇室内物体定位方法，结合了大型语言模型进行多步推理，提升了空间上下文理解能力。", "motivation": "解决复杂查询中涉及的空间参考问题，提升室内环境的语义和几何准确性。", "method": "利用RGB-D帧序列构建分层场景图，结合开放词汇基础模型和传感器数据处理，整合大型语言模型进行多步推理。", "result": "在Habitat Matterport 3D多楼层场景中表现出高效的场景理解和鲁棒的物体定位能力。", "conclusion": "OVIGo-3DHSG在需要空间推理和室内环境理解的应用中具有强大潜力。"}}
{"id": "2507.12125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12125", "abs": "https://arxiv.org/abs/2507.12125", "authors": ["Yi-Kuan Hsieh", "Jun-Wei Hsieh", "Xin Li", "Yu-Ming Chang", "Yu-Chee Tseng"], "title": "Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers", "comment": null, "summary": "Vision Transformer (ViT) has achieved impressive results across various\nvision tasks, yet its high computational cost limits practical applications.\nRecent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning\nunimportant tokens. However, these techniques often sacrifice accuracy by\nindependently pruning query (Q) and key (K) tokens, leading to performance\ndegradation due to overlooked token interactions. To address this limitation,\nwe introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for\nefficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.\nUnlike previous methods that consider only a single direction, our approach\nevaluates each token and its neighbors to decide which tokens to retain by\ntaking token interaction into account. The retained tokens are compressed\nthrough a similarity fusion step, preserving key information while reducing\ncomputational costs. The shared weights of Q/K tokens create a symmetric\nattention matrix, allowing pruning only the upper triangular part for speed up.\nBSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning\nlevels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%\non DeiT-S, while reducing computational overhead by 50%. It achieves 40%\nspeedup with improved accuracy across various ViTs.", "AI": {"tldr": "BSPF-ViT通过联合修剪Q/K令牌并考虑令牌交互，显著降低了ViT的计算成本，同时提升了性能。", "motivation": "ViT的高计算成本限制了实际应用，现有方法因独立修剪Q/K令牌而牺牲了准确性。", "method": "提出块级对称修剪与融合方法（BSPF-ViT），联合优化Q/K令牌修剪，并通过相似性融合保留关键信息。", "result": "在DeiT-T和DeiT-S上分别提升ImageNet分类准确率1.3%和2.0%，计算开销减少50%，速度提升40%。", "conclusion": "BSPF-ViT在降低计算成本的同时显著提升了ViT的性能，适用于各种ViT模型。"}}
{"id": "2507.12135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12135", "abs": "https://arxiv.org/abs/2507.12135", "authors": ["Junyu Lou", "Xiaorui Zhao", "Kexuan Shi", "Shuhang Gu"], "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement", "comment": "Accepted to ICCV 2025", "summary": "Deep learning-based bilateral grid processing has emerged as a promising\nsolution for image enhancement, inherently encoding spatial and intensity\ninformation while enabling efficient full-resolution processing through slicing\noperations. However, existing approaches are limited to linear affine\ntransformations, hindering their ability to model complex color relationships.\nMeanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,\ntraditional MLP-based methods employ globally shared parameters, which is hard\nto deal with localized variations. To overcome these dual challenges, we\npropose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)\nframework. Our approach synergizes the spatial modeling of bilateral grids with\nthe non-linear capabilities of MLPs. Specifically, we generate bilateral grids\ncontaining MLP parameters, where each pixel dynamically retrieves its unique\ntransformation parameters and obtain a distinct MLP for color mapping based on\nspatial coordinates and intensity values. In addition, we propose a novel grid\ndecomposition strategy that categorizes MLP parameters into distinct types\nstored in separate subgrids. Multi-channel guidance maps are used to extract\ncategory-specific parameters from corresponding subgrids, ensuring effective\nutilization of color information during slicing while guiding precise parameter\ngeneration. Extensive experiments on public datasets demonstrate that our\nmethod outperforms state-of-the-art methods in performance while maintaining\nreal-time processing capabilities.", "AI": {"tldr": "提出了一种结合双边网格和MLP的BPAM框架，用于图像增强，解决了现有方法在非线性映射和局部变化处理上的不足。", "motivation": "现有双边网格方法仅支持线性变换，而传统MLP方法无法处理局部变化，BPAM框架旨在结合两者的优势。", "method": "通过生成包含MLP参数的双边网格，动态为每个像素分配独特的变换参数，并提出网格分解策略和通道指导图优化参数生成。", "result": "在公开数据集上，BPAM性能优于现有方法，同时保持实时处理能力。", "conclusion": "BPAM框架有效结合了双边网格的空间建模能力和MLP的非线性映射能力，提升了图像增强效果。"}}
{"id": "2507.12137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12137", "abs": "https://arxiv.org/abs/2507.12137", "authors": ["Jiawei Xu", "Kai Deng", "Zexin Fan", "Shenlong Wang", "Jin Xie", "Jian Yang"], "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving", "comment": "Accepted by ICCV 2025", "summary": "Modeling and rendering dynamic urban driving scenes is crucial for\nself-driving simulation. Current high-quality methods typically rely on costly\nmanual object tracklet annotations, while self-supervised approaches fail to\ncapture dynamic object motions accurately and decompose scenes properly,\nresulting in rendering artifacts. We introduce AD-GS, a novel self-supervised\nframework for high-quality free-viewpoint rendering of driving scenes from a\nsingle log. At its core is a novel learnable motion model that integrates\nlocality-aware B-spline curves with global-aware trigonometric functions,\nenabling flexible yet precise dynamic object modeling. Rather than requiring\ncomprehensive semantic labeling, AD-GS automatically segments scenes into\nobjects and background with the simplified pseudo 2D segmentation, representing\nobjects using dynamic Gaussians and bidirectional temporal visibility masks.\nFurther, our model incorporates visibility reasoning and physically rigid\nregularization to enhance robustness. Extensive evaluations demonstrate that\nour annotation-free model significantly outperforms current state-of-the-art\nannotation-free methods and is competitive with annotation-dependent\napproaches.", "AI": {"tldr": "AD-GS是一种自监督框架，用于从单一日志中高质量渲染驾驶场景，无需手动标注，通过动态高斯和双向时间可见性掩码实现场景分解和动态对象建模。", "motivation": "当前方法依赖昂贵的手动标注或自监督方法无法准确捕捉动态对象运动，导致渲染伪影。", "method": "结合局部感知B样条曲线和全局感知三角函数的学习运动模型，简化伪2D分割自动分解场景，动态高斯和双向时间可见性掩码表示对象。", "result": "在无标注方法中显著优于现有技术，与依赖标注的方法竞争。", "conclusion": "AD-GS提供了一种高效且高质量的自监督驾驶场景渲染解决方案。"}}
{"id": "2507.12138", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12138", "abs": "https://arxiv.org/abs/2507.12138", "authors": ["Michal Heker", "Sefy Kararlitsky", "David Tolpin"], "title": "Neural Human Pose Prior", "comment": "Work in progress", "summary": "We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.", "AI": {"tldr": "提出了一种基于归一化流的数据驱动方法，用于建模人体姿态的神经先验，通过RealNVP学习6D旋转格式的灵活密度分布。", "motivation": "解决现有启发式或低表达能力方法在建模6D旋转流形分布上的不足，提供稳定学习且兼容旋转框架的先验模型。", "method": "利用RealNVP学习6D旋转格式的密度分布，通过反转Gram-Schmidt过程确保稳定学习，保持与旋转框架的兼容性。", "result": "通过定性和定量评估验证了先验模型的有效性，并通过消融研究分析了其影响。", "conclusion": "为人体运动捕捉和重建流程中的姿态先验集成提供了概率基础。"}}
{"id": "2507.12157", "categories": ["cs.CV", "I.2; I.4"], "pdf": "https://arxiv.org/pdf/2507.12157", "abs": "https://arxiv.org/abs/2507.12157", "authors": ["Edwin Arkel Rios", "Fernando Mikael", "Oswin Gosal", "Femiloye Oyerinde", "Hao-Chun Liang", "Bo-Cheng Lai", "Min-Chun Hu"], "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation", "comment": "Main: 10 pages, 2 figures, 4 tables", "summary": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.", "AI": {"tldr": "论文提出了一种无需预训练的高性能细粒度图像识别（FGIR）框架TGDA，通过数据感知增强和弱监督实现，显著提升了低分辨率和高分辨率输入的识别性能。", "motivation": "现有FGIR方法依赖预训练模型，限制了在资源受限环境中的适应性和任务特定架构的发展。", "method": "引入TGDA框架，结合数据感知增强和通过知识蒸馏的细粒度感知教师模型进行弱监督，设计了任务特定架构如LRNets和ViTFS。", "result": "在多个FGIR基准测试中，TGDA框架性能优于或匹配现有预训练方法，尤其在低分辨率输入下，LRNets提升精度23%，参数和计算量大幅减少。", "conclusion": "TGDA展示了无需预训练的高效FGIR潜力，为细粒度视觉系统提供了更灵活的设计路径。"}}
{"id": "2507.12177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12177", "abs": "https://arxiv.org/abs/2507.12177", "authors": ["Zahid Ullah", "Dragan Pamucar", "Jihie Kim"], "title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable\ntool for detecting tumors due to its capability to produce detailed images that\nreveal their presence. However, the accuracy of diagnosis can be compromised\nwhen human specialists evaluate these images. Factors such as fatigue, limited\nexpertise, and insufficient image detail can lead to errors. For example, small\ntumors might go unnoticed, or overlap with healthy brain regions could result\nin misidentification. To address these challenges and enhance diagnostic\nprecision, this study proposes a novel double ensembling framework, consisting\nof ensembled pre-trained deep learning (DL) models for feature extraction and\nensembled fine-tuned hyperparameter machine learning (ML) models to efficiently\nclassify brain tumors. Specifically, our method includes extensive\npreprocessing and augmentation, transfer learning concepts by utilizing various\npre-trained deep convolutional neural networks and vision transformer networks\nto extract deep features from brain MRI, and fine-tune hyperparameters of ML\nclassifiers. Our experiments utilized three different publicly available Kaggle\nMRI brain tumor datasets to evaluate the pre-trained DL feature extractor\nmodels, ML classifiers, and the effectiveness of an ensemble of deep features\nalong with an ensemble of ML classifiers for brain tumor classification. Our\nresults indicate that the proposed feature fusion and classifier fusion improve\nupon the state of the art, with hyperparameter fine-tuning providing a\nsignificant enhancement over the ensemble method. Additionally, we present an\nablation study to illustrate how each component contributes to accurate brain\ntumor classification.", "AI": {"tldr": "提出了一种双集成框架，结合预训练深度学习模型和超参数优化的机器学习模型，以提高脑肿瘤MRI分类的准确性。", "motivation": "MRI诊断脑肿瘤时，人为因素可能导致误诊，需通过自动化方法提升精度。", "method": "采用预训练深度学习模型提取特征，结合超参数优化的机器学习模型进行分类，并进行了数据预处理和增强。", "result": "实验表明，特征融合和分类器融合优于现有方法，超参数优化显著提升性能。", "conclusion": "双集成框架有效提高了脑肿瘤分类的准确性，各组件对结果均有贡献。"}}
{"id": "2507.12201", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.12201", "abs": "https://arxiv.org/abs/2507.12201", "authors": ["Yiqi Tian", "Pengfei Jin", "Mingze Yuan", "Na Li", "Bo Zeng", "Quanzheng Li"], "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models", "comment": null, "summary": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.", "AI": {"tldr": "RODS是一种基于优化的扩散采样方法，通过几何线索检测和修正高风险采样步骤，提升生成模型的鲁棒性和保真度。", "motivation": "扩散模型的采样过程容易因分数近似不准确而产生幻觉，需要一种无需重新训练且低成本的解决方案。", "method": "RODS利用损失景观的几何信息检测和修正高风险采样步骤，优化采样轨迹并自适应调整扰动。", "result": "在AFHQv2、FFHQ和11k-hands数据集上，RODS检测到70%以上的幻觉样本并修正超过25%，且未引入新伪影。", "conclusion": "RODS通过优化视角改进了扩散采样，显著减少了幻觉问题，同时保持了高效性。"}}
{"id": "2507.12232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12232", "abs": "https://arxiv.org/abs/2507.12232", "authors": ["Tao Chen", "Jingyi Zhang", "Decheng Liu", "Chunlei Peng"], "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM", "comment": null, "summary": "Recent studies have utilized visual large language models (VLMs) to answer\nnot only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These\nstudies introduced forgery-related attributes, such as forgery location and\ntype, to construct deepfake VQA datasets and train VLMs, achieving high\naccuracy while providing human-understandable explanatory text descriptions.\nHowever, these methods still have limitations. For example, they do not fully\nleverage face quality-related attributes, which are often abnormal in forged\nfaces, and they lack effective training strategies for forgery-aware VLMs. In\nthis paper, we extend the VQA dataset to create DD-VQA+, which features a\nricher set of attributes and a more diverse range of samples. Furthermore, we\nintroduce a novel forgery detection framework, MGFFD-VLM, which integrates an\nAttribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual\nLarge Language Models (VLMs). Additionally, our framework incorporates\nMulti-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By\ntransforming classification and forgery segmentation results into prompts, our\nmethod not only improves forgery classification but also enhances\ninterpretability. To further boost detection performance, we design multiple\nforgery-related auxiliary losses. Experimental results demonstrate that our\napproach surpasses existing methods in both text-based forgery judgment and\nanalysis, achieving superior accuracy.", "AI": {"tldr": "本文提出了一种新的伪造检测框架MGFFD-VLM，通过扩展数据集和引入多粒度提示学习等方法，显著提升了视觉大语言模型在伪造检测中的性能。", "motivation": "现有方法在利用伪造相关属性时存在不足，如未充分利用人脸质量相关属性，且缺乏有效的训练策略。", "method": "扩展了VQA数据集为DD-VQA+，引入属性驱动的混合LoRA策略、多粒度提示学习和伪造感知训练策略。", "result": "实验结果表明，该方法在文本伪造判断和分析中优于现有方法，准确率更高。", "conclusion": "MGFFD-VLM框架通过改进数据集和训练策略，显著提升了伪造检测的性能和可解释性。"}}
{"id": "2507.12236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12236", "abs": "https://arxiv.org/abs/2507.12236", "authors": ["Felix Nützel", "Mischa Dombrowski", "Bernhard Kainz"], "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models", "comment": "20 pages, 6 figures. To appear in Proc. MIDL 2025 (PMLR)", "summary": "Phrase grounding, i.e., mapping natural language phrases to specific image\nregions, holds significant potential for disease localization in medical\nimaging through clinical reports. While current state-of-the-art methods rely\non discriminative, self-supervised contrastive models, we demonstrate that\ngenerative text-to-image diffusion models, leveraging cross-attention maps, can\nachieve superior zero-shot phrase grounding performance. Contrary to prior\nassumptions, we show that fine-tuning diffusion models with a frozen,\ndomain-specific language model, such as CXR-BERT, substantially outperforms\ndomain-agnostic counterparts. This setup achieves remarkable improvements, with\nmIoU scores doubling those of current discriminative methods. These findings\nhighlight the underexplored potential of generative models for phrase grounding\ntasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),\na novel post-processing technique that aligns text and image biases to identify\nregions of high certainty. BBM refines cross-attention maps, achieving even\ngreater localization accuracy. Our results establish generative approaches as a\nmore effective paradigm for phrase grounding in the medical imaging domain,\npaving the way for more robust and interpretable applications in clinical\npractice. The source code and model weights are available at\nhttps://github.com/Felix-012/generate_to_ground.", "AI": {"tldr": "生成式扩散模型在医学图像短语定位任务中表现优于现有判别式方法，通过跨注意力图和领域特定语言模型微调，性能显著提升。", "motivation": "探索生成式模型在医学图像短语定位中的潜力，以提升疾病定位的准确性和可解释性。", "method": "使用生成式文本到图像扩散模型，结合跨注意力图和领域特定语言模型（如CXR-BERT）微调，并引入Bimodal Bias Merging（BBM）后处理技术。", "result": "性能显著优于现有判别式方法，mIoU分数翻倍，BBM进一步提升了定位准确性。", "conclusion": "生成式模型在医学图像短语定位中具有显著优势，为临床应用提供了更鲁棒和可解释的解决方案。"}}
{"id": "2507.12245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12245", "abs": "https://arxiv.org/abs/2507.12245", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "Calisthenics Skills Temporal Video Segmentation", "comment": "9 pages, 6 figures, In Proceedings of the 19th International Joint\n  Conference on Computer Vision, Imaging and Computer Graphics Theory and\n  Applications - Volume 2", "summary": "Calisthenics is a fast-growing bodyweight discipline that consists of\ndifferent categories, one of which is focused on skills. Skills in calisthenics\nencompass both static and dynamic elements performed by athletes. The\nevaluation of static skills is based on their difficulty level and the duration\nof the hold. Automated tools able to recognize isometric skills from a video by\nsegmenting them to estimate their duration would be desirable to assist\nathletes in their training and judges during competitions. Although the video\nunderstanding literature on action recognition through body pose analysis is\nrich, no previous work has specifically addressed the problem of calisthenics\nskill temporal video segmentation. This study aims to provide an initial step\ntowards the implementation of automated tools within the field of Calisthenics.\nTo advance knowledge in this context, we propose a dataset of video footage of\nstatic calisthenics skills performed by athletes. Each video is annotated with\na temporal segmentation which determines the extent of each skill. We hence\nreport the results of a baseline approach to address the problem of skill\ntemporal segmentation on the proposed dataset. The results highlight the\nfeasibility of the proposed problem, while there is still room for improvement.", "AI": {"tldr": "论文提出了一种自动化工具，用于从视频中识别和分割静态健身技能，以辅助运动员训练和比赛评分。", "motivation": "目前缺乏针对健身技能时间分割的研究，而自动化工具在此领域有潜在应用价值。", "method": "构建了一个包含静态健身技能视频的数据集，并标注了时间分割，随后采用基线方法进行技能分割实验。", "result": "结果表明该问题具有可行性，但仍需改进。", "conclusion": "研究为健身技能自动化分析提供了初步探索，未来有进一步优化的空间。"}}
{"id": "2507.12248", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12248", "abs": "https://arxiv.org/abs/2507.12248", "authors": ["Anida Nezović", "Jalal Romano", "Nada Marić", "Medina Kapo", "Amila Akagić"], "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST", "comment": null, "summary": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.", "AI": {"tldr": "比较Keras、PyTorch和JAX在医学图像分类任务中的性能，使用PathMNIST数据集评估训练效率、分类准确性和推理速度。", "motivation": "尽管深度学习框架在医学图像分类中广泛应用，但不同框架（如Keras、PyTorch和JAX）在性能上的比较研究较少。", "method": "使用PathMNIST数据集，对CNN在不同框架中的实现进行综合分析，评估训练效率、分类准确性和推理速度。", "result": "研究揭示了计算速度和模型准确性之间的权衡。", "conclusion": "结果为医学图像分析的研究者和从业者提供了有价值的参考。"}}
{"id": "2507.12283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12283", "abs": "https://arxiv.org/abs/2507.12283", "authors": ["Zixuan Fu", "Yan Ren", "Finn Carter", "Chenyue Wang", "Ze Niu", "Dacheng Yu", "Emily Davis", "Bo Zhang"], "title": "FADE: Adversarial Concept Erasure in Flow Models", "comment": "Camera Ready", "summary": "Diffusion models have demonstrated remarkable image generation capabilities,\nbut also pose risks in privacy and fairness by memorizing sensitive concepts or\nperpetuating biases. We propose a novel \\textbf{concept erasure} method for\ntext-to-image diffusion models, designed to remove specified concepts (e.g., a\nprivate individual or a harmful stereotype) from the model's generative\nrepertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion\nErasure), combines a trajectory-aware fine-tuning strategy with an adversarial\nobjective to ensure the concept is reliably removed while preserving overall\nmodel fidelity. Theoretically, we prove a formal guarantee that our approach\nminimizes the mutual information between the erased concept and the model's\noutputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable\nDiffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,\nexplicit content, and style erasure tasks from MACE). FADE achieves\nstate-of-the-art concept removal performance, surpassing recent baselines like\nESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.\nNotably, FADE improves the harmonic mean of concept removal and fidelity by\n5--10\\% over the best prior method. We also conduct an ablation study to\nvalidate each component of FADE, confirming that our adversarial and\ntrajectory-preserving objectives each contribute to its superior performance.\nOur work sets a new standard for safe and fair generative modeling by\nunlearning specified concepts without retraining from scratch.", "AI": {"tldr": "提出了一种名为FADE的新方法，用于从文本到图像扩散模型中擦除指定概念，确保隐私和公平性。", "motivation": "扩散模型在图像生成方面表现优异，但可能泄露隐私或传播偏见，需要一种可靠的概念擦除方法。", "method": "结合轨迹感知微调策略和对抗目标，确保概念被可靠移除且模型保真度不受影响。", "result": "FADE在概念移除和图像质量上优于现有方法，提升了5-10%的性能。", "conclusion": "FADE为安全公平的生成建模设定了新标准，无需从头训练即可移除指定概念。"}}
{"id": "2507.12292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12292", "abs": "https://arxiv.org/abs/2507.12292", "authors": ["Antonio Finocchiaro", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation", "comment": "13 pages, 4 figures, In International Conference on Image Analysis\n  and Processing", "summary": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.", "AI": {"tldr": "提出了一种基于深度估计和运动员区域检索的体操技能分类方法，避免了传统姿态估计的高计算成本，显著提升了效率和分类精度。", "motivation": "传统体操技能识别方法依赖姿态估计，计算成本高且复杂，限制了实时应用和移动设备的适用性。", "method": "使用Depth Anything V2进行深度估计，YOLOv10进行运动员定位，通过分割运动员区域而非姿态估计来分类技能。", "result": "方法显著优于基于骨架的方法，推理速度提升38.3倍，深度补丁的分类精度更高（0.837 vs. 0.815）。", "conclusion": "该方法不仅高效且模块化设计灵活，适用于未来增强和实际应用。"}}
{"id": "2507.12336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12336", "abs": "https://arxiv.org/abs/2507.12336", "authors": ["Subin Jeon", "In Cho", "Junyoung Hong", "Seon Joo Kim"], "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors", "comment": null, "summary": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D\nkeypoints estimation that accurately predicts 3D keypoints from a single image.\nWhile previous methods rely on manual annotations or calibrated multi-view\nimages, both of which are expensive to collect, our method enables monocular 3D\nkeypoints estimation using only a collection of single-view images. To achieve\nthis, we leverage powerful geometric priors embedded in a pretrained multi-view\ndiffusion model. In our framework, this model generates multi-view images from\na single image, serving as a supervision signal to provide 3D geometric cues to\nour model. We also use the diffusion model as a powerful 2D multi-view feature\nextractor and construct 3D feature volumes from its intermediate\nrepresentations. This transforms implicit 3D priors learned by the diffusion\nmodel into explicit 3D features. Beyond accurate keypoints estimation, we\nfurther introduce a pipeline that enables manipulation of 3D objects generated\nby the diffusion model. Experimental results on diverse aspects and datasets,\nincluding Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain\ndatasets, highlight the effectiveness of our method in terms of accuracy,\ngeneralization, and its ability to enable manipulation of 3D objects generated\nby the diffusion model from a single image.", "AI": {"tldr": "KeyDiff3D是一个无监督的单目3D关键点估计框架，利用预训练的多视角扩散模型生成几何先验，实现从单张图像预测3D关键点。", "motivation": "现有方法依赖昂贵的手动标注或多视角图像，而KeyDiff3D仅需单视角图像，降低了数据收集成本。", "method": "通过扩散模型生成多视角图像作为监督信号，并提取其2D多视角特征构建3D特征体积，将隐式3D先验转化为显式特征。", "result": "在Human3.6M、Stanford Dogs等数据集上验证了方法的准确性、泛化能力，并支持对扩散模型生成的3D对象进行操控。", "conclusion": "KeyDiff3D提供了一种低成本、高效的单目3D关键点估计方案，并扩展了3D对象操控的可能性。"}}
{"id": "2507.12344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12344", "abs": "https://arxiv.org/abs/2507.12344", "authors": ["Ahmet Oğuz Saltık", "Max Voigt", "Sourav Modak", "Mike Beckworth", "Anthony Stein"], "title": "Improving Lightweight Weed Detection via Knowledge Distillation", "comment": null, "summary": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.", "AI": {"tldr": "论文研究了通道知识蒸馏（CWD）和掩码生成蒸馏（MGD）在轻量级模型中的应用，以提升实时智能喷洒系统中的杂草检测性能，实验表明两种方法均有效提升了模型精度。", "motivation": "杂草检测是精准农业的关键，但资源受限平台上部署高精度模型仍具挑战性，特别是区分视觉相似的杂草种类。", "method": "使用YOLO11x作为教师模型，YOLO11n作为学生模型，通过CWD和MGD方法进行知识蒸馏。", "result": "实验显示，CWD和MGD分别提升了2.5%和1.9%的mAP50，且未增加模型复杂度。", "conclusion": "CWD和MGD是提升深度学习杂草检测精度的有效、高效且实用的方法。"}}
{"id": "2507.12382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12382", "abs": "https://arxiv.org/abs/2507.12382", "authors": ["Kaiwen Huang", "Yi Zhou", "Huazhu Fu", "Yizhe Zhang", "Chen Gong", "Tao Zhou"], "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "comment": "10 pages; 2 figures; Have been accepted by MICCAI 2025", "summary": "Semi-supervised medical image segmentation is a crucial technique for\nalleviating the high cost of data annotation. When labeled data is limited,\ntextual information can provide additional context to enhance visual semantic\nunderstanding. However, research exploring the use of textual data to enhance\nvisual semantic embeddings in 3D medical imaging tasks remains scarce. In this\npaper, we propose a novel text-driven multiplanar visual interaction framework\nfor semi-supervised medical image segmentation (termed Text-SemiSeg), which\nconsists of three main modules: Text-enhanced Multiplanar Representation (TMR),\nCategory-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation\n(DCA). Specifically, TMR facilitates text-visual interaction through planar\nmapping, thereby enhancing the category awareness of visual features. CSA\nperforms cross-modal semantic alignment between the text features with\nintroduced learnable variables and the intermediate layer of visual features.\nDCA reduces the distribution discrepancy between labeled and unlabeled data\nthrough their interaction, thus improving the model's robustness. Finally,\nexperiments on three public datasets demonstrate that our model effectively\nenhances visual features with textual information and outperforms other\nmethods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "AI": {"tldr": "提出了一种文本驱动的多平面视觉交互框架（Text-SemiSeg），用于半监督医学图像分割，通过文本增强视觉语义嵌入，提升模型性能。", "motivation": "解决医学图像标注成本高的问题，探索文本信息在半监督3D医学图像分割中的应用。", "method": "框架包含三个模块：文本增强多平面表示（TMR）、类别感知语义对齐（CSA）和动态认知增强（DCA）。", "result": "在三个公开数据集上验证了模型的有效性，性能优于其他方法。", "conclusion": "Text-SemiSeg通过文本与视觉特征的交互，显著提升了半监督医学图像分割的效果。"}}
{"id": "2507.12396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12396", "abs": "https://arxiv.org/abs/2507.12396", "authors": ["Hayat Ullah", "Abbas Khan", "Arslan Munir", "Hari Kalva"], "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments", "comment": "14 pages", "summary": "Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.", "AI": {"tldr": "论文介绍了两个视觉目标检测基准数据集OD-VIRAT Large和OD-VIRAT Tiny，用于评估复杂环境下的人体监测模型性能，并测试了多种先进目标检测架构。", "motivation": "开发可靠的人体监测系统需要多样化和具有挑战性的数据集，以全面评估模型性能。", "method": "提出两个数据集，包含丰富的边界框和类别标注，并在这些数据集上测试了RETMDET、YOLOX等先进目标检测架构。", "result": "OD-VIRAT Large包含8.7百万标注实例，OD-VIRAT Tiny包含288,901标注实例，实验提供了这些模型在复杂条件下的性能分析。", "conclusion": "该工作为开发更高效和鲁棒的目标检测架构奠定了基础，并提供了对现有模型性能的深入见解。"}}
{"id": "2507.12420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12420", "abs": "https://arxiv.org/abs/2507.12420", "authors": ["Haoyuan Liu", "Hiroshi Watanabe"], "title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization", "comment": null, "summary": "Bounding box regression (BBR) is fundamental to object detection, where the\nregression loss is crucial for accurate localization. Existing IoU-based losses\noften incorporate handcrafted geometric penalties to address IoU's\nnon-differentiability in non-overlapping cases and enhance BBR performance.\nHowever, these penalties are sensitive to box shape, size, and distribution,\noften leading to suboptimal optimization for small objects and undesired\nbehaviors such as bounding box enlargement due to misalignment with the IoU\nobjective. To address these limitations, we propose InterpIoU, a novel loss\nfunction that replaces handcrafted geometric penalties with a term based on the\nIoU between interpolated boxes and the target. By using interpolated boxes to\nbridge the gap between predictions and ground truth, InterpIoU provides\nmeaningful gradients in non-overlapping cases and inherently avoids the box\nenlargement issue caused by misaligned penalties. Simulation results further\nshow that IoU itself serves as an ideal regression target, while existing\ngeometric penalties are both unnecessary and suboptimal. Building on InterpIoU,\nwe introduce Dynamic InterpIoU, which dynamically adjusts interpolation\ncoefficients based on IoU values, enhancing adaptability to scenarios with\ndiverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC\nshow that our methods consistently outperform state-of-the-art IoU-based losses\nacross various detection frameworks, with particularly notable improvements in\nsmall object detection, confirming their effectiveness.", "AI": {"tldr": "论文提出了一种新的损失函数InterpIoU，通过插值框解决现有IoU损失函数在非重叠情况下的问题，并进一步提出Dynamic InterpIoU以动态调整插值系数，显著提升了小物体检测性能。", "motivation": "现有IoU损失函数在非重叠情况下不可导，且手工设计的几何惩罚项对框的形状、大小和分布敏感，导致优化效果不佳，尤其是对小物体检测。", "method": "提出InterpIoU，用插值框的IoU替代手工几何惩罚项，并通过Dynamic InterpIoU动态调整插值系数。", "result": "在COCO、VisDrone和PASCAL VOC数据集上，InterpIoU和Dynamic InterpIoU显著优于现有IoU损失函数，尤其是小物体检测。", "conclusion": "InterpIoU和Dynamic InterpIoU有效解决了现有IoU损失函数的局限性，提升了检测性能，特别是对小物体检测。"}}
{"id": "2507.12426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12426", "abs": "https://arxiv.org/abs/2507.12426", "authors": ["Hayat Ullah", "Muhammad Ali Shafique", "Abbas Khan", "Arslan Munir"], "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition", "comment": "17 pages", "summary": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.", "AI": {"tldr": "提出了一种轻量级视频焦点调制网络DVFL-Net，通过知识蒸馏和时空特征调制，在保持高性能的同时显著降低计算成本。", "motivation": "Transformer模型在视频识别中表现出色但计算成本高，需要一种更高效的解决方案。", "method": "DVFL-Net利用知识蒸馏和时空焦点调制，从大模型中提取知识并压缩为轻量级模型。", "result": "在多个基准测试中表现优异，平衡了性能和效率，适用于实时应用。", "conclusion": "DVFL-Net是一种高效且实用的视频识别解决方案。"}}
{"id": "2507.12441", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12441", "abs": "https://arxiv.org/abs/2507.12441", "authors": ["Yen-Linh Vu", "Dinh-Thang Duong", "Truong-Binh Duong", "Anh-Khoi Nguyen", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Jianhua Xing", "Xingjian Li", "Tianyang Wang", "Ulas Bagci", "Min Xu"], "title": "Describe Anything Model for Visual Question Answering on Text-rich Images", "comment": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025", "summary": "Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.", "AI": {"tldr": "DAM-QA利用区域感知的视觉语言模型（DAM）提升文本密集图像的视觉问答（VQA）性能，通过多区域视图聚合答案，显著优于基线模型。", "motivation": "针对文本密集图像的VQA任务，区域级描述能力对提取细粒度文本信息至关重要，DAM-QA旨在利用DAM的区域感知能力解决这一问题。", "method": "DAM-QA引入多区域视图机制，聚合图像内容的区域级描述，以更有效地识别与文本相关的证据。", "result": "在六个VQA基准测试中，DAM-QA显著优于基线DAM，尤其在DocVQA上提升7+分，且参数更少，性能接近通用视觉语言模型。", "conclusion": "DAM-QA展示了区域感知模型在文本密集VQA任务中的潜力，通过高效集成策略缩小与通用模型的差距。"}}
{"id": "2507.12449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12449", "abs": "https://arxiv.org/abs/2507.12449", "authors": ["Van-Hoang-Anh Phan", "Chi-Tam Nguyen", "Doan-Trung Au", "Thanh-Danh Phan", "Minh-Thien Duong", "My-Ha Le"], "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios", "comment": "7 pages, 6 figures, 4 tables, HSI 2025", "summary": "Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8", "AI": {"tldr": "提出了一种基于摄像头感知和Frenet-Pure Pursuit规划的障碍物避障系统，结合YOLOv11和Depth Anything V2模型，在校园场景中验证了其有效性。", "motivation": "确保自动驾驶车辆的安全需要准确的感知和运动规划，尤其是在复杂环境中避免碰撞。", "method": "使用摄像头感知模块（YOLOv11和Depth Anything V2）进行物体检测和距离估计，结合Frenet-Pure Pursuit规划策略。", "result": "系统在多样化的校园场景中表现出色，能够有效处理各种障碍物。", "conclusion": "提出的系统在自动驾驶车辆的障碍物避障方面具有高效性和实用性。"}}
{"id": "2507.12455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12455", "abs": "https://arxiv.org/abs/2507.12455", "authors": ["Shangpin Peng", "Senqiao Yang", "Li Jiang", "Zhuotao Tian"], "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention", "comment": null, "summary": "Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.", "AI": {"tldr": "SENTINEL框架通过句子级早期干预和领域内偏好学习，显著减少多模态大语言模型中的幻觉现象，性能优于现有方法。", "motivation": "多模态大语言模型在跨模态理解中存在幻觉问题，现有方法成本高或引入数据分布不匹配。", "method": "提出SENTINEL框架，通过迭代采样、验证和分类构建高质量偏好数据，并使用上下文感知偏好损失（C-DPO）训练模型。", "result": "SENTINEL将幻觉减少90%以上，在幻觉和通用能力基准测试中优于现有方法。", "conclusion": "SENTINEL通过早期干预和领域内学习有效解决幻觉问题，具有优越性和泛化能力。"}}
{"id": "2507.12462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12462", "abs": "https://arxiv.org/abs/2507.12462", "authors": ["Yuxi Xiao", "Jianyuan Wang", "Nan Xue", "Nikita Karaev", "Yuri Makarov", "Bingyi Kang", "Xing Zhu", "Hujun Bao", "Yujun Shen", "Xiaowei Zhou"], "title": "SpatialTrackerV2: 3D Point Tracking Made Easy", "comment": "International Conference on Computer Vision, ICCV 2025. Huggingface\n  Demo: https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2, Code:\n  https://github.com/henry123-boy/SpaTrackerV2", "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.", "AI": {"tldr": "SpatialTrackerV2是一种基于单目视频的前馈3D点跟踪方法，通过统一点跟踪、单目深度和相机姿态估计，实现了高性能的3D跟踪。", "motivation": "现有3D跟踪方法依赖模块化流程和现成组件，无法充分利用点跟踪、深度和相机姿态之间的内在联系。", "method": "将世界空间3D运动分解为场景几何、相机自运动和像素级物体运动，采用全可微分和端到端架构，支持跨数据集训练。", "result": "性能优于现有3D跟踪方法30%，与领先的动态3D重建方法精度相当，但速度快50倍。", "conclusion": "SpatialTrackerV2通过联合学习几何和运动，实现了高效且高精度的3D点跟踪。"}}
{"id": "2507.12463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12463", "abs": "https://arxiv.org/abs/2507.12463", "authors": ["Renjie Li", "Ruijie Ye", "Mingyang Wu", "Hao Frank Yang", "Zhiwen Fan", "Hezhen Hu", "Zhengzhong Tu"], "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding", "comment": null, "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.", "AI": {"tldr": "论文提出了一个名为MMHU的大规模人类行为分析基准，包含丰富的注释和多任务评估。", "motivation": "理解人类行为对开发安全驾驶系统至关重要，但目前缺乏全面的评估基准。", "method": "构建了一个包含57k人类运动片段和1.73M帧的数据集，采用人机协作标注流程生成行为描述。", "result": "提供了全面的数据集分析，并针对运动预测、生成和行为问答等多任务进行了基准测试。", "conclusion": "MMHU为人类行为理解提供了广泛的评估工具，填补了研究空白。"}}
{"id": "2507.12464", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.12464", "abs": "https://arxiv.org/abs/2507.12464", "authors": ["Muhammed Furkan Dasdelen", "Hyesu Lim", "Michele Buck", "Katharina S. Götze", "Carsten Marr", "Steffen Schneider"], "title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "comment": "11 pages, 5 figures", "summary": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.", "AI": {"tldr": "稀疏自编码器（SAEs）在医学影像中应用，提出CytoSAE用于血液学分析，发现形态学相关概念并验证其有效性。", "motivation": "医学影像领域缺乏解释性工具，SAEs在视觉领域的成功启发其在医学影像中的应用。", "method": "提出CytoSAE，基于40,000多张外周血单细胞图像训练，适用于多样化和域外数据。", "result": "CytoSAE发现形态学相关概念，验证其有效性，并在AML亚型分类任务中表现优异。", "conclusion": "CytoSAE为医学影像提供可解释性工具，性能媲美现有技术，同时支持亚细胞级解释。"}}
{"id": "2507.12465", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12465", "abs": "https://arxiv.org/abs/2507.12465", "authors": ["Ziang Cao", "Zhaoxi Chen", "Linag Pan", "Ziwei Liu"], "title": "PhysX: Physical-Grounded 3D Asset Generation", "comment": "Project page: https://physx-3d.github.io/", "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.", "AI": {"tldr": "论文提出PhysX，一种物理基础的3D资产生成方法，包括数据集PhysXNet和生成框架PhysXGen，填补了现有3D生成忽略物理属性的空白。", "motivation": "现有3D生成模型主要关注几何和纹理，忽略了物理属性，限制了在仿真和具身AI等物理领域的应用。", "method": "1) 构建PhysXNet数据集，标注物理属性；2) 提出PhysXGen框架，通过双分支结构将物理知识注入预训练的3D结构空间。", "result": "实验验证了PhysXGen的优越性能和泛化能力。", "conclusion": "PhysX为物理基础的3D生成提供了新范式，代码和数据将开源以推动研究。"}}
